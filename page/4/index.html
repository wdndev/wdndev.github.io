<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog - Home of firefly</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="A firefly flying freely in the AI domain.">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/page/4/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="A firefly flying freely in the AI domain.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:author" content="Firefly">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/page/4/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2026-02-08 08:08:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/1.4.%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0%E8%AE%BA%E6%96%87%E7%BB%BC%E8%BF%B0/" title="论文精读 对比学习论文综述">论文精读 对比学习论文综述</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-10-06T16:00:00.000Z" title="Created 2023-10-07 00:00:00">2023-10-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">对比学习在计算机视觉领域的发展历程，4个阶段：

百花齐放：InstDisc（instance discrimination）、CPC、CMC。方法、模型、目标函数、代理任务都还没有统一。
CV双雄：MOCOv1、SimCLRv1、MOCOv2、SimCLRv2、CPC和CMC的延伸工作、SwaV，这个阶段发展非常迅速，以上这些工作间隔时间都很短，ImageNet上的最好成绩，基本上每个月都在被刷新。
不用负样本：BYOL及其后续改进，SimSima把所有方法都归纳总结，融入到SimSima框架之中，算是卷积神经网络做对比学习的总结性工作。
Transformer：MOCOv3、DINO，用Vision Transformer开展工作。对于自监督学习来说，无论是对比学习还是最新的掩码学习，都是用Vision Transformer做的

1.阶段一：百花齐放&#x20;1.1 InstDisc（instance discrimination）
论文名称：Unsupervised Feature Learning via Non-Parametric Instance Discrimin ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/0.Transformer%E7%BB%BC%E8%BF%B0/" title="Transformer综述">Transformer综述</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-09-25T16:00:00.000Z" title="Created 2023-09-26 00:00:00">2023-09-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer综述

论文标题： A Survey of Transformers
论文链接： https://arxiv.org/abs/2106.04554

Transformer 在自然语言处理、计算机视觉、音频处理等许多人工智能领域都取得了巨大的成功，也吸引了学术界和行业研究人员的大量兴趣。到目前为止，已经有各种各样的 Transformer 变体（又名 X-former）被提出，但是，关于这些 Transformer 变体的系统而全面的文献综述仍然缺失。这篇综述对各种 X-former 进行了全面介绍**。**&#x20;
这篇综述首先简要介绍了原版 Transformer，然后提出了一种新的 X-former 分类法。接着从架构修改、预训练、应用三个角度介绍各种 X-former。最后，概述了未来研究的一些潜在方向。
1.引言Transformer 最初是作为机器翻译的 Seq2Seq 模型提出的。后来的工作表明，基于 Transformer 的预训练模型 (PTM) 可以在各种任务上实现 SOTA。因此，Transformer，特别是 PTM，已成为 NLP 中 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/1.3.MoCo/" title="论文精读 MoCo">论文精读 MoCo</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-09-16T16:00:00.000Z" title="Created 2023-09-17 00:00:00">2023-09-17</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">
论文名称：Momentum Contrast for Unsupervised Visual Representation Learning
论文连接：1911.05722.pdf (arxiv.org)

0.基础知识0.1 对比学习对比学习顾名思义就是对比着学习，模型不需要知道图片具体是什么，只需要知道哪些图片类似，哪些不类似。
假设有三张图片，两张是人类，一张是狗，假如这三张图片都通过一个网络，得到了三个特征。如果已经有了一个学习好的特征空间，那么学习到的三个特征就是特征空间里的三个点，我们希望对比学习做到的就是：能把类似图片的特征尽可能的靠近，不类似的图片的特征尽可能的远离。如果能做到，那么我们就学到了一个很好的特征。

对比学习虽然不需要知道图片的标签信息，但还是需要知道哪些图片相似，哪些不相似，才能做模型训练。那为什么对比学习在视觉领域是一个无监督的训练方式呢？
因为在视觉领域，通过设计一些巧妙的代理任务，从而人为的订立一些规则，这些规则可以用来定义哪些图片是相似的，哪些是不相似的。从而可以提供一个监督信号去训练模型，这就是所谓的自监督训练。
0.2 最广泛应用的代理任务 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/1.2.GAN/" title="论文精读 GAN">论文精读 GAN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-09-06T16:00:00.000Z" title="Created 2023-09-07 00:00:00">2023-09-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">GAN Note
最新版本论文：Generative Adversarial Nets (neurips.cc)（建议看这个）
arXiv版本论文：1406.2661.pdf (arxiv.org)（早期写的）
1.标题 + 作者近 5 年，GAN 上头条次数很多，Reddit 里 GAN 很火
使用GAN生成人脸网址：thispersondoesnotexist.com
加州法令：禁止换脸、禁止对政治人物骚操作，说未讲过的话
GAN: 两个网络相互对抗

generative: ML模型分 discriminative(AlexNet, ResNet, Transformer) 和 generative
adversarial: 对抗
nets: networks 简写，非 native speaker 不建议使用简写

2.摘要写作简洁，可直接搬运（wiki, textbook）

创新工作：讲清楚自己是谁？
拓展工作：和别人的区别、创新

本文的 framework ：estimating generative models via an adversarial process； ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.8.MAE/" title="论文精读 MAE">论文精读 MAE</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-09-06T16:00:00.000Z" title="Created 2023-09-07 00:00:00">2023-09-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">8.MAEMasked Autoencoders Are Scalable Vision Learners
MAE：CV版的BERT

论文链接：https://arxiv.org/pdf/2111.06377.pdf
论文代码：https://github.com/facebookresearch/mae
李沐讲解：MAE 论文逐段精读【论文精读】_哔哩哔哩_bilibili

MAE 2021.11.11提交 arxiv
知乎 百万 view; Reddit or Twitter 讨论不多
MAE 很新 —&gt; 如何在读比较新的文章 获取一些新的研究思路？
0.和之前精读论文的关系？Transformer
一个纯基于注意力机制的编码器和解码器
表现比 RNN 架构好，在机器翻译任务

BERT
使用 一个 Transformer 编码器，拓展到更一般的 NLP 的任务
使用了 完型填空 的自监督的训练机制，不需要使用标号，去预测一个句子里面 不见 masked 的词 ，从而获取对文本特征抽取的能力
BERT 极大的扩展了 Transformer 的应用，在一个大规模的、没有标号 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/1.1.GNN/" title="论文精读 GNN">论文精读 GNN</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-08-06T16:00:00.000Z" title="Created 2023-08-07 00:00:00">2023-08-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">文章链接：A Gentle Introduction to Graph Neural Networks (distill.pub)
1.前言图这个数据结构相对于之前讨论的文本（文本是一个序列）、图片（图片是一个矩阵），图相对来说更加复杂一点。
图是一个很一般化的架构，十几年前，研究者提出了针对图的神经网络（图神经网络，GNN），最近它们在能力和表达上都有增强。
图神经网络的实际应用（starting to，图神经网络还是一个比较新的领域，在应用上刚起步）

药物的发现
物理模拟
虚假新闻检测
车流量的检测
推荐系统

本文旨在探索和解释现代的图神经网络

什么数据可以表示成一张图
图和别的数据有什么不同，为什么要做图神经网络，而不是使用最简单的卷积神经网络等
构建了一个GNN，看各模块的具体结构
提供了一个GNN的playground

图神经网络所关注的重点

怎样把所想要的信息表示成向量
这些向量是不是能够通过数据来学到

2.什么是图图是用来表示entity（实体）之间的关系

实体就是一个点（node，顶点）
关系就是一个边（edge）

2.1 图的构成
V：顶点
E：边
U ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/255.1.ChatGPT%20%E7%9B%B8%E5%85%B3%E6%A0%B8%E5%BF%83%E7%AE%97%E6%B3%95/" title="ChatGPT 相关核心算法">ChatGPT 相关核心算法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-07-08T16:00:00.000Z" title="Created 2023-07-09 00:00:00">2023-07-09</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">1.ChatGPT 相关核心算法ChatGPT 的卓越表现得益于其背后多项核心算法的支持和配合。本文将分别介绍作为其实现基础的 Transformer 模型、激发出其所蕴含知识的Prompt/Instruction Tuning 算法、其涌现出的思维链能力、以及确保其与人类意图对齐的基于人类反馈的强化学习算法。
1.基于Transformer的预训练语言模型ChatGPT 强大的基础模型采用 Transformer 架构， Transformer 是一种基于自注意力机制的深度神经网络模型，可以高效并行地处理序列数据。
原始的 Transformer 模型包含两个关键组件：编码器和解码器。编码器用于将输入序列映射到一组中间表示，解码器则将中间表示转换为目标序列。编码器和解码器都由多层的注意力模块和前馈神经网络模块组成。其中自注意力模块可以学习序列中不同位置之间的依赖关系，即在处理每个位置的信息时，模型会考虑序列中其他所有位置上的信息，这种机制使得 Transformer模型能够有效地处理长距离依赖关系。在原始 Transformer 模型基础上，相继衍生出了三类预训练语言模型：编码预训练 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.7.GPT-4/" title="论文精读 GPT-4">论文精读 GPT-4</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-07-06T16:00:00.000Z" title="Created 2023-07-07 00:00:00">2023-07-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">8.GPT-4GPT-4 Technical Report：2303.08774.pdf (arxiv.org)
OpenAI博客：https://openai.com/research/gpt-4
博客基本是99页技术报告的缩略版，2023.3.14发布。
0.疯狂的三月（202303）
03-08 微软发布Visual ChatGPT，聊天时可以用图片，并可以根据文字对图片进行修改
论文：https://arxiv.org/abs/2303.04671
代码：https://github.com/microsoft/visual-chatgpt


03-09 微软宣布将要发布大型多模态模型GPT4
03-09 10亿规模的模型GigaGAN推出
论文：https://arxiv.org/abs/2303.05511


03-13 斯坦福大学推出7B的Alpaca模型
代码：https://github.com/tatsu-lab/stanford_alpaca


03-14 GPT4推出
03-14 谷歌公布PALM模型的API使用
03-14 Anthropic介绍大型语言 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.6.InstructGPT/" title="论文精读 InstructGPT">论文精读 InstructGPT</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-07-02T16:00:00.000Z" title="Created 2023-07-03 00:00:00">2023-07-03</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">6.InstructGPTChatGPT

Chat GPT 既没有发表在 NeurlPS 上面，也没有发表在 EMNLP ，甚至连一篇论文都没有
InstructGPT是微调的GPT-3.5模型

0.前言ChatGPT0.1 ChatGPT 的四个应用官方给出四个使用的场景
1、ChatGPT asks the clarifying questions to debug code

2、ChatGPT initially refuses to answer a question that could be about illegal activities but responds after the user clarifies their intent

ChatGPT 能在安全性上避免进行一些非法的回答


3、ChatGPT is able to understand the reference (“it”) to the subject of the previous question (“fermat’s little theorem”)

ChatGPT 是能够理解上下 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.4.Swin%20Transformer/" title="论文精读 Swin Transformer">论文精读 Swin Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-30T16:00:00.000Z" title="Created 2023-07-01 00:00:00">2023-07-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">4.Swin TransformerSwin transformer: Hierarchical vision transformer using shifted windows
论文链接：https://arxiv.org/pdf/2103.14030.pdf
官方代码库：microsoft/Swin-Transformer
论文解读视频：https://www.bilibili.com/video/BV13L4y1475U
0.Swim Transformer简介Swin Transformer是 ICCV 21的最佳论文，它之所以能有这么大的影响力主要是因为在 ViT 之后，Swin Transformer通过在一系列视觉任务上的强大表现 ，进一步证明了Transformer是可以在视觉领域取得广泛应用的。
更新时间线：

2021年3月传到 arxiv上的
2021年4月份代码库放出
2021年5月12号又放出来了自监督版本的Swin Transformer—moby，从方法上和性能上其实和MoCo v3和DINO都差不多，只是换了个骨干网络
接下来过了一个月，Swin Tra ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.5.GPT_GPT-2_GPT-3/" title="论文精读 GPT、GPT-2、GPT-3">论文精读 GPT、GPT-2、GPT-3</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-30T16:00:00.000Z" title="Created 2023-07-01 00:00:00">2023-07-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">5.GPT，GPT-2，GPT-3大力出奇迹模型

李沐讲解：GPT，GPT-2，GPT-3 论文精读

最近以GPT系列为代表的大语言模型LLM掀起了一阵热潮，许多人惊叹LLM的震撼能力，因此紧跟时代潮流，学习GPT系列论文，加深自己对LLM的理解。总的来说，GPT整体的模型架构都是以Transformer的解码器为模块进行堆叠而成。主要的创新点集中在模型训练策略，还有就是讲究一个大力出奇迹。
0.论文时间轴
GPT：Transformer解码器，在没有标号的大量的文本数据上，训练一个语言模型，来获得预训练模型，后续在子任务上做微调，得到每一个任务所用的分类器。
BERT：Transformer编码器，收集了一个更大的数据集，用来做预训练，效果比GPT好。BERT有两个模型，BERT-base模型与GPT模型参数相差不大，BERT-Large比BERT-base模型大。
GPT-2：原作者吸取教训，收集更大的数据集，训练了一个更大的模型，GPT-2的模型比BERT-large要大。继续使用Transformer的解码器，发现非常适合做Zero Shot，步子跨的太大，效果上不是那么好 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/7.Transformer%E4%B8%AD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81/" title="Transformer中的位置编码">Transformer中的位置编码</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-25T16:00:00.000Z" title="Created 2023-06-26 00:00:00">2023-06-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer中的位置编码原文链接：Transformer Architecture: The Positional Encoding
1.位置编码对任何语言来说，句子中词汇的顺序和位置都是非常重要的。它们定义了语法，从而定义了句子的实际语义。RNN结构本身就涵盖了单词的顺序，RNN按顺序逐字分析句子，这就直接在处理的时候整合了文本的顺序信息。
但Transformer架构抛弃了循环机制，仅采用多头自注意机制。避免了RNN较大的时间成本。并且从理论上讲，它可以捕捉句子中较长的依赖关系。
由于句子中的单词同时流经Transformer的编码器、解码器堆栈，模型本身对每个单词没有任何位置信息的。因此，仍然需要一种方法将单词的顺序整合到模型中。
想给模型一些位置信息，一个方案是在每个单词中添加一条关于其在句子中位置的信息。我们称之为“信息片段”，即位置编码。
第一个可能想到的方法是为每个时间步添加一个$[0,1]$范围内的数字，其中0表示第一个单词，1表示最后一个单词。
但这样会存在一个问题：无法计算出特定范围内有多少个单词。换句话说，时间步长在不同句子中的含义不一致。如下所示：

 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/6.Transformer%E6%9E%B6%E6%9E%84%E7%BB%86%E8%8A%82/" title="Transformer架构细节">Transformer架构细节</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-23T16:00:00.000Z" title="Created 2023-06-24 00:00:00">2023-06-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer架构细节
1.Transformer各个模块的作用（1）Encoder 模块&#x20;
经典的Transformer架构中的Encoder模块包含6个Encoder Block. &#x20;
每个Encoder Block包含两个⼦模块, 分别是多头⾃注意⼒层, 和前馈全连接层. &#x20;
多头⾃注意⼒层采⽤的是⼀种Scaled Dot-Product Attention的计算⽅式, 实验结果表  明, Multi-head可以在更细致的层⾯上提取不同head的特征, ⽐单⼀head提取特征的  效果更佳. &#x20;
前馈全连接层是由两个全连接层组成, 线性变换中间增添⼀个Relu激活函数, 具体的 维度采⽤4倍关系, 即多头⾃注意⼒的d_model=512, 则层内的变换维度d_ff=2048. &#x20;



（2）Decoder 模块 &#x20;
经典的Transformer架构中的Decoder模块包含6个Decoder Block. &#x20;
每个Decoder Block包含3个⼦模块, 分别是多头⾃注意⼒层, Encoder-D ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.3.ViT/" title="论文精读 ViT">论文精读 ViT</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-14T16:00:00.000Z" title="Created 2023-06-15 00:00:00">2023-06-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">3.ViT论文链接：https://arxiv.org/abs/2010.11929
源码链接：https://github.com/rwightman/py
1.标题 + 简介An image is worth 16*16 words
每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches —&gt; an image is worth 16 * 16 words
ViT：过去一年，CV 最有影响力的工作

推翻了 2012 Alexnet 提出的 CNN 在 CV 的统治地位
有足够多的预训练数据，NLP 的 Transformer 搬运到 CV，效果很好
打破 CV 和 NLP 的壁垒，给 CV、多模态 挖坑

ViT效果有多好，CV 任务刷榜，paperwithcode网站&#x20;

霸榜 ImageNet （基于 ViT）
&#x20;COCO ,目标检测（Swin Transformer ICCV 21 best paper：多尺度的 ViT ）的模型

下图中的的四种情况 ViT 都能处理
遮挡、数据分布的偏移（纹理的去除）、鸟头部+对 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/5.Transformer%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B/" title="Transformer中的注意力">Transformer中的注意力</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-09T16:00:00.000Z" title="Created 2023-06-10 00:00:00">2023-06-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer中的注意力本文主要来自：The Illustrated Transformer
1.自注意力假设我们要翻译下边这句话：“The animal didn’t cross the street because it was too tired”。这里it指的是什么？是street还是animal？人理解起来很容易，但是对算法来讲就不那么容易了。
当模型处理it这个词的时候，自注意力会让**it和animal**关联起来。
当模型编码每个位置上的单词的时候，自注意力的作用就是：看一看输入句子中其他位置的单词，试图寻找一种对当前单词更好的编码方式。
如果你熟悉RNNs模型，回想一下RNN如何处理当前时间步的隐藏状态：将之前的隐藏状态与当前位置的输入结合起来。
在Transformer中，自注意力机制也可以将其他相关单词的“理解”融入到我们当前处理的单词中。

可以去Tensor2Tensor ，自己体验一下上图的可视化。动图如下所示：

2.图解注意力计算先画图用向量解释一下自注意力是怎么算的，之后再看一下实际实现中是怎么用矩阵算的。
第一步：计算query、key、va ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/3/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/3/#content-inner">3</a><span class="page-number current">4</span><a class="page-number" href="/page/5/#content-inner">5</a><span class="space">&hellip;</span><a class="page-number" href="/page/63/#content-inner">63</a><a class="extend next" rel="next" href="/page/5/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Firefly</div><div class="author-info__description">A firefly flying freely in the AI domain.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/" title="LLMs公开课 - 5.高效训练&amp;模型压缩">LLMs公开课 - 5.高效训练&amp;模型压缩</a><time datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI/"><span class="card-category-list-name">AI</span><span class="card-category-list-count">414</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/GitHub/"><span class="card-category-list-name">GitHub</span><span class="card-category-list-count">224</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/HotNews/"><span class="card-category-list-name">HotNews</span><span class="card-category-list-count">73</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.32em; color: rgb(18, 39, 197)">DSA</a><a href="/tags/RL/" style="font-size: 1.24em; color: rgb(16, 85, 18)">RL</a><a href="/tags/Transformer/" style="font-size: 1.34em; color: rgb(20, 87, 191)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.26em; color: rgb(172, 26, 190)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.3em; color: rgb(39, 65, 9)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.22em; color: rgb(187, 163, 126)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.17em; color: rgb(45, 108, 80)">CV</a><a href="/tags/GPT/" style="font-size: 1.19em; color: rgb(187, 122, 70)">GPT</a><a href="/tags/PL/" style="font-size: 1.21em; color: rgb(157, 195, 117)">PL</a><a href="/tags/domain/" style="font-size: 1.15em; color: rgb(152, 180, 144)">domain</a><a href="/tags/github/" style="font-size: 1.15em; color: rgb(199, 111, 181)">github</a><a href="/tags/hf/" style="font-size: 1.15em; color: rgb(56, 34, 122)">hf</a><a href="/tags/hot-news/" style="font-size: 1.15em; color: rgb(14, 184, 85)">hot_news</a><a href="/tags/ArXiv/" style="font-size: 1.39em; color: rgb(77, 123, 156)">ArXiv</a><a href="/tags/Domain/" style="font-size: 1.39em; color: rgb(180, 98, 87)">Domain</a><a href="/tags/AI/" style="font-size: 1.45em; color: rgb(57, 191, 138)">AI</a><a href="/tags/GitHub/" style="font-size: 1.43em; color: rgb(179, 120, 1)">GitHub</a><a href="/tags/Trending/" style="font-size: 1.43em; color: rgb(19, 23, 111)">Trending</a><a href="/tags/HuggingFace/" style="font-size: 1.41em; color: rgb(33, 140, 22)">HuggingFace</a><a href="/tags/Papers/" style="font-size: 1.41em; color: rgb(200, 199, 128)">Papers</a><a href="/tags/%E5%BE%AE%E5%8D%9A/" style="font-size: 1.38em; color: rgb(159, 97, 36)">微博</a><a href="/tags/%E7%83%AD%E6%90%9C/" style="font-size: 1.38em; color: rgb(186, 62, 84)">热搜</a><a href="/tags/HotNews/" style="font-size: 1.36em; color: rgb(124, 12, 82)">HotNews</a><a href="/tags/leetcode/" style="font-size: 1.28em; color: rgb(119, 33, 104)">leetcode</a><a href="/tags/algo/" style="font-size: 1.17em; color: rgb(26, 118, 96)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/09/"><span class="card-archive-list-date">September 2023</span><span class="card-archive-list-count">4</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">942</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">47379.1k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2026-02-08T00:07:45.667Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>