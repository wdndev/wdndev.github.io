<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>37.2° Blog - Home of firefly</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="A firefly flying freely in the AI domain.">
<meta property="og:type" content="website">
<meta property="og:title" content="37.2° Blog">
<meta property="og:url" content="https://wdndev.github.io/page/5/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="A firefly flying freely in the AI domain.">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:author" content="Firefly">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/page/5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '37.2° Blog',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-11-02 07:51:42'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/5.Transformer%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B/" title="Transformer中的注意力">Transformer中的注意力</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-09T16:00:00.000Z" title="Created 2023-06-10 00:00:00">2023-06-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer中的注意力本文主要来自：The Illustrated Transformer
1.自注意力假设我们要翻译下边这句话：“The animal didn’t cross the street because it was too tired”。这里it指的是什么？是street还是animal？人理解起来很容易，但是对算法来讲就不那么容易了。
当模型处理it这个词的时候，自注意力会让**it和animal**关联起来。
当模型编码每个位置上的单词的时候，自注意力的作用就是：看一看输入句子中其他位置的单词，试图寻找一种对当前单词更好的编码方式。
如果你熟悉RNNs模型，回想一下RNN如何处理当前时间步的隐藏状态：将之前的隐藏状态与当前位置的输入结合起来。
在Transformer中，自注意力机制也可以将其他相关单词的“理解”融入到我们当前处理的单词中。

可以去Tensor2Tensor ，自己体验一下上图的可视化。动图如下所示：

2.图解注意力计算先画图用向量解释一下自注意力是怎么算的，之后再看一下实际实现中是怎么用矩阵算的。
第一步：计算query、key、va ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/4.Transformer%E4%B8%BA%E5%95%A5%E8%BF%99%E4%B9%88%E6%AC%A2%E8%BF%8E/" title="为何Transformer在计算机视觉中如此受欢迎？">为何Transformer在计算机视觉中如此受欢迎？</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-06T16:00:00.000Z" title="Created 2023-06-07 00:00:00">2023-06-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">转自：为何Transformer在计算机视觉中如此受欢迎？ (msra.cn)
2021-09-24 | 作者：胡瀚
编者按：近一年来，Transformer 在计算机视觉领域所带来的革命性提升，引起了学术界的广泛关注，有越来越多的研究人员投入其中。Transformer 的特点和优势是什么？为什么在计算机领域中 Transformer 可以频频出圈？让我们通过今天的文章来一探究竟吧！

“统一性”是很多学科共同追求的目标，例如在物理学领域，科学家们追求的大统一，就是希望用单独一种理论来解释力与力之间的相互作用。人工智能领域自然也存在着关于“统一性”的目标。在深度学习的浪潮中，人工智能领域已经朝着统一性的目标前进了一大步。比如，一个新的任务基本都会遵循同样的流程对新数据进行预测：收集数据，做标注，定义网络结构，训练网络参数。
但是，在人工智能的不同子领域中，基本建模的方式各种各样，并不统一，例如：在自然语言处理（NLP）领域目前的主导建模网络是 Transformer；计算机视觉（CV）领域很长一段时间的主导网络是卷积神经网络（CNN）；社交网络领域目前的主导网络则是图网络等。
尽管如 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.2.BERT/" title="论文精读 BERT">论文精读 BERT</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-05T16:00:00.000Z" title="Created 2023-06-06 00:00:00">2023-06-06</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">2.BERT论文链接：1810.04805.pdf (arxiv.org)
解读视频：BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili
参考代码：personal/bert_torch at main · wdndev/personal · GitHub
1.题目+作者“BERT” (Devlin 等, 2018, p. 1) 自然语言中中近三年最重要的文章；在计算机视觉里面很早就能够在一个大的数据集（比如说ImageNet）上训练出一个CNN模型，用这个模型可以用来处理一大片的机器视觉任务，来提升他们的性能；但是在自然语言处理里面，在BERT之前一直没有一个深的神经网络使得它训练好之后能够帮处理一大片的NLP任务，在NLP中很多时候还是对每个任务构造自己的神经网路，然后再做训练；BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃；

pre-training：在一个数据集上训练好一个模型，这个模型 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/llms_idx/" title="LLMs 目录">LLMs 目录</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-04T16:00:00.000Z" title="Created 2023-06-05 00:00:00">2023-06-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/LLMs/">LLMs</a></span></div><div class="content">0.LLM八股1.LLMs相关文章
LLMs推理优化技术
主流大语言模型的技术原理细节
LLaMA系列模型架构
ChatGLM系列模型架构
RAG（检索增强生成）技术
大模型Agent技术
大语言模型方法与实践
LLM 推理常见参数
检索增强LLM

2.LLMs相关论文
ZeRO
GPT_GPT-2_GPT-3
InstructGPT
GPT-4
ChatGPT 相关核心算法

3.Transformer相关文章
Transformer综述
The Annotated Transformer最新翻译
Transformer架构解析
Transformer构建语言模型
Transformer为啥这么欢迎
Transformer中的注意力
Transformer架构细节
Transformer中的位置编码

4.Transformer相关论文
Transformer
BERT
ViT
Swim Transformer
MAE

5.清华大模型公开课
视频连接：https://www.bilibili.com/video/BV1UG411p7zv
文档资料：OpenBMB - 让大模型飞入 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/pr_content/" title="论文精读目录">论文精读目录</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-06-04T16:00:00.000Z" title="Created 2023-06-05 00:00:00">2023-06-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">0.如何阅读论文
如何阅读论文

1.Deep Learning
GNN
GAN
MoCo
对比学习论文综述
ELMo
MoE
MoE经典论文简牍

2.Transformer
Transformer
BERT
ViT
Swim Transformer
GPT_GPT-2_GPT-3
InstructGPT
GPT-4
MAE

3.Reinforcement Learning
WU-UCT
Rainbow

4.LLMs
ZeRO

5.Others
ChatGPT 相关核心算法
大模型时代下做科研的四个思路

</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/2.1.Transformer/" title="论文精读 Transformer">论文精读 Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-05-30T16:00:00.000Z" title="Created 2023-05-31 00:00:00">2023-05-31</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">论文地址: https://arxiv.org/pdf/1810.04805.pdf
解析视频：Transformer论文逐段精读【论文精读
Transformer Model：Transformer架构解析
代码链接：personal/transformer · GitHub
1.Transformer
1.摘要1、transduction models”：序列转录模型：给以序列生成另一个序列
2.结论3.导言4.相关工作5.模型
5.1 LayerNorm 和 BatchNorm（1）BatchNorm 简单的 2 维 情况（蓝色）每一行是一个样本 X，每一列是 一个 feature
BatchNorm：每次把一列（1 个 feature）放在一个 mini-batch 里，均值变成 0， 方差变成 1 的标准化。
How：（该列向量 - mini-batch 该列向量的均值）/（mini - batch 该列向量的方差）
训练时：mini-batch 计算均值；
测试时：使用 全局 均值、方差。
BatchNorm 还会学 lambda beta，BatchNorm 可以通过学习 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/3.Transformer%E6%9E%84%E5%BB%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" title="Transformer构建语言模型">Transformer构建语言模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-05-26T16:00:00.000Z" title="Created 2023-05-27 00:00:00">2023-05-27</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer构建语言模型本文主要来自：The Annotated Transformer
Transformer Model：Transformer架构解析
论文地址: https://arxiv.org/pdf/1810.04805.pdf
代码链接：personal/transformer · GitHub
什么是语言模型:
以一个符合语言规律的序列为输入，模型将利用序列间关系等特征，输出一个在所有词汇上的概率分布.这样的模型称为语言模型.
123# 语言模型的训练语料一般来自于文章，对应的源文本和目标文本形如:src1 = &quot;I can do&quot; tgt1 = &quot;can do it&quot;src2 = &quot;can do it&quot;, tgt2 = &quot;do it &lt;eos&gt;&quot;

语言模型能解决哪些问题

根据语言模型的定义，可以在它的基础上完成机器翻译，文本生成等任务，因为我们通过最后输出的概率分布来预测下一个词汇是什么.
语言模型可以判断输入的序列是否为一句完整的话，因为我们可以根据输出的概率 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/2.Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/" title="Transformer架构解析">Transformer架构解析</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-05-24T16:00:00.000Z" title="Created 2023-05-25 00:00:00">2023-05-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">Transformer架构解析本文主要来自：The Annotated Transformer
论文地址: https://arxiv.org/pdf/1810.04805.pdf
代码链接：personal/transformer · GitHub
1.Transformer架构图1.1 Transformer模型的作用基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.
https://www.bilibili.com/video/BV1qh4y1o7UU
1.2 Transformer总体架构
（1）输入部分
源文本嵌入层及其位置编码器
目标文本嵌入层及其位置编码器


（2）输出部分
线性层
softmax层


（3）编码器
由N个编码器层堆叠而成
每个编码器层由两个子层连接结构组成
第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接
第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接


（4）解码器部分:
由N个解码器层堆叠 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/llms/transformer/1.The_Annotated_Transformer%E6%9C%80%E6%96%B0%E7%BF%BB%E8%AF%91/" title="The Annotated Transformer最新翻译">The Annotated Transformer最新翻译</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2023-05-14T16:00:00.000Z" title="Created 2023-05-15 00:00:00">2023-05-15</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Transformer/">Transformer</a></span></div><div class="content">The Annotated Transformer最新翻译
2023版最新The Annotated Transformer翻译
原文地址：http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule
0.Prelims123456789101112131415161718192021222324252627import osfrom os.path import existsimport torchimport torch.nn as nnfrom torch.nn.functional import log_softmax, padimport mathimport copyimport timefrom torch.optim.lr_scheduler import LambdaLRimport pandas as pdimport altair as altfrom torchtext.data.functional import to_map_style_datasetfrom torch.u ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/0.%E5%A6%82%E4%BD%95%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87/" title="如何阅读论文">如何阅读论文</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-12-31T16:00:00.000Z" title="Created 2023-01-01 00:00:00">2023-01-01</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">解读视频：https://www.bilibili.com/video/BV1H44y1t75x/?spm_id_from=333.999.0.0
花三遍，读一篇论文
第一遍（花时最少，做海选）第一遍读论文的时候，需要去关注标题和摘要

读完摘要之后，直接跳到结论这边
读完这三个部分，大致就知道这篇论文是在讲什么东西了

第二遍（对相关论文做以进一步精选）第二遍里面我们就要对整个文章完整过一遍，然后知道每一块到底在干什么东西，我们可以沿着从标题一直往下读到最后，但是这个时候也不需要注意太多的细节，以及一些公式的证明等等。

关注的地方：第二遍阅读的时候，最重要是搞明白那些重要的图和表，都要知道他每一个字在干什么事情 ；作者提出的方法和别人提出的方法是怎么进行对比的？之间差距有多大？这个时候可能你还没有特别搞懂他在干什么。但是不要紧，你可以将不懂的地方标记下来，留到之后第三遍进行阅读
达到的效果：第二遍阅读完之后，你就对整个论文的各个部分，都有一个大概的了解，中间可以把作者引用的别人的相关文献圈出来，比如作者是在某某某的方法上进行了改进，做了哪些改进之类的。这里需要注意的是，如果你发现作 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/3.2.Rainbow/" title="论文精读 Rainbow">论文精读 Rainbow</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-11-24T16:00:00.000Z" title="Created 2022-11-25 00:00:00">2022-11-25</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">RainbowRainbow: Combining Improvements in Deep Reinforcement LearningRainbow: 结合深度强化学习的改进论文地址：https://arxiv.org/abs/1710.02298
摘要深度强化学习社区对DQN算法进行了几项独立改进。然而，尚不清楚这些扩展中的哪一个是互补的，并且可以有效地结合。本文研究了DQN算法的六个扩展，并实证研究了它们的组合。我们的实验表明，在数据效率和最终性能方面，该组合在雅达利2600基准上提供了最先进的性能。我们还提供了详细的消融研究结果，该研究显示了每个部件对整体性能的贡献。
AbstractThe deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully comb ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/paper_reading/3.1.WU_UCT/" title="论文精读 一种简单的蒙特卡洛树搜索并行化方法">论文精读 一种简单的蒙特卡洛树搜索并行化方法</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-10-28T16:00:00.000Z" title="Created 2022-10-29 00:00:00">2022-10-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="content">监控未观察样本: 一种简单的蒙特卡洛树搜索并行化方法Watch the Unobserved: a Sample Approach to Parallelizing Monte Carlo TreeSearch论文地址：https://openreview.net/forum?id=BJlQtJSKDB
Github ：https://github.com/liuanji/WU-UCT
摘要蒙特卡洛树搜索 (MCTS) 算法在许多具有挑战性的基准测试上(例如,围棋等)取得了巨大成功.然而,它们通常需要大量部署,这使得它们的应用成本很高. 此外,由于 MCTS固有的顺序性质,并行化 MCTS 也极具挑战性:每次模拟都严重依赖从先前模拟的数据 (例如,节点访问计数),用于实现有效的探索和利用的权衡. 尽管存在这些困难,我们还是开发了一种算法WU-UCT, 来有效地并行化 MCTS, 它实现了线性加速,并且随着线程数量的增加表现出有限的性能损失. WU-UCT的关键思想是我们引入一组统计数据来跟踪正在进行未结束的模拟样本(称为未观察样本) 的数量. 当我们将最耗时的扩展和模拟步骤并行化时,这 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/easy_rl_exercise/1.%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/" title="1.强化学习基础">1.强化学习基础</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-10-23T16:00:00.000Z" title="Created 2022-10-24 00:00:00">2022-10-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/RL/">RL</a></span></div><div class="content">关键词
强化学习（reinforcement learning，RL）：智能体可以在与复杂且不确定的环境进行交互时，尝试使所获得的奖励最大化的算法。
动作（action）： 环境接收到的智能体基于当前状态的输出。
状态（state）：智能体从环境中获取的状态。
奖励（reward）：智能体从环境中获取的反馈信号，这个信号指定了智能体在某一步采取了某个策略以后是否得到奖励，以及奖励的大小。
探索（exploration）：在当前的情况下，继续尝试新的动作。其有可能得到更高的奖励，也有可能一无所有。
开发（exploitation）：在当前的情况下，继续尝试已知的可以获得最大奖励的过程，即选择重复执行当前动作。
深度强化学习（deep reinforcement learning）：不需要手动设计特征，仅需要输入状态就可以让系统直接输出动作的一个端到端（end-to-end）的强化学习方法。通常使用神经网络来拟合价值函数（value function）或者策略网络（policy network）。
全部可观测（full observability）、完全可观测（fully observed ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/easy_rl_exercise/10.%E6%A8%A1%E4%BB%BF%E5%AD%A6%E4%B9%A0/" title="10.模仿学习">10.模仿学习</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-10-23T16:00:00.000Z" title="Created 2022-10-24 00:00:00">2022-10-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/RL/">RL</a></span></div><div class="content">关键词
模仿学习（imitation learning，IL）：其讨论我们没有奖励或者无法定义奖励但是有与环境进行交互时怎么进行智能体的学习。这与我们平时处理的问题有些类似，因为通常我们无法从环境中得到明确的奖励。模仿学习又被称为示范学习（learning from demonstration）、学徒学习（apprenticeship learning）以及观察学习（learning by watching）等。
行为克隆（behavior cloning）：类似于机器学习中的监督学习，通过收集专家的状态与动作等对应信息，来训练我们的网络。在使用时，输入状态就可以输出对应的动作。
数据集聚合（dataset aggregation）：用来应对在行为克隆中专家提供不到数据的情况，其希望收集专家在各种极端状态下的动作。
逆强化学习（inverse reinforcement learning，IRL）：逆强化学习先找出奖励函数，再用强化学习找出最优演员。这么做是因为我们没有环境中的奖励，但是有专家的示范，使用逆强化学习，我们可以推断专家是因为何种奖励函数才会采取这些动作。有了奖励函数以后就 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/easy_rl_exercise/2.%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B/" title="2.马尔可夫决策过程">2.马尔可夫决策过程</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">Created</span><time datetime="2022-10-23T16:00:00.000Z" title="Created 2022-10-24 00:00:00">2022-10-24</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/RL/">RL</a></span></div><div class="content">关键词
马尔可夫性质（Markov property，MP）：如果某一个过程未来的状态与过去的状态无关，只由现在的状态决定，那么其具有马尔可夫性质。换句话说，一个状态的下一个状态只取决于它的当前状态，而与它当前状态之前的状态都没有关系。
马尔可夫链（Markov chain）： 概率论和数理统计中具有马尔可夫性质且存在于离散的指数集（index set）和状态空间（state space）内的随机过程（stochastic process）。
状态转移矩阵（state transition matrix）：状态转移矩阵类似于条件概率（conditional probability），其表示当智能体到达某状态后，到达其他所有状态的概率。矩阵的每一行描述的是从某节点到达所有其他节点的概率。
马尔可夫奖励过程（Markov reward process，MRP）： 本质是马尔可夫链加上一个奖励函数。在马尔可夫奖励过程中，状态转移矩阵和它的状态都与马尔可夫链的一样，只多了一个奖励函数。奖励函数是一个期望，即在某一个状态可以获得多大的奖励。
范围（horizon）：定义了同一个回合（episod ...</div></div></div><nav id="pagination"><div class="pagination"><a class="extend prev" rel="prev" href="/page/4/#content-inner"><i class="fas fa-chevron-left fa-fw"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><span class="page-number current">5</span><a class="page-number" href="/page/6/#content-inner">6</a><span class="space">&hellip;</span><a class="page-number" href="/page/38/#content-inner">38</a><a class="extend next" rel="next" href="/page/6/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/wdn_icon.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Firefly</div><div class="author-info__description">A firefly flying freely in the AI domain.</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/wdndev"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/wdndev" target="_blank" title="Github"><i class="fab fa-github" style="color: #24292e;"></i></a><a class="social-icon" href="mailto:dongnian.wang@outlook.com" target="_blank" title="Email"><i class="fas fa-envelope" style="color: #24292e;"></i></a><a class="social-icon" href="https://blog.csdn.net/wdnshadow" target="_blank" title="CSDN"><i class="fas fa-rss" style="color: #24292e;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>Announcement</span></div><div class="announcement_content">Welcome to My Personal Blog! <br /> If Not, Please Visit <a target="_blank" rel="noopener" href="https://wdndev.gitee.io/"> <font color=#00BFFF>Gitee Mirror</font></a>.</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/daily/domain/202510/2025-10-13/" title="No title">No title</a><time datetime="2025-11-01T23:46:09.753Z" title="Created 2025-11-02 07:46:09">2025-11-02</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_article/9.%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BALLM/" title="检索增强LLM">检索增强LLM</a><time datetime="2024-01-12T16:00:00.000Z" title="Created 2024-01-13 00:00:00">2024-01-13</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/llms/llms_course/6.%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3%E5%92%8C%E7%94%9F%E6%88%90%E5%A4%A7%E6%A8%A1%E5%9E%8B/" title="LLMs公开课 - 6.文本理解和生成大模型">LLMs公开课 - 6.文本理解和生成大模型</a><time datetime="2024-01-09T16:00:00.000Z" title="Created 2024-01-10 00:00:00">2024-01-10</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>Categories</span>
            <a class="card-more-btn" href="/categories/" title="More">
    <i class="fas fa-angle-right"></i></a>
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/AI/"><span class="card-category-list-name">AI</span><span class="card-category-list-count">222</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Cython/"><span class="card-category-list-name">Cython</span><span class="card-category-list-count">1</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/DSA/"><span class="card-category-list-name">DSA</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/GitHub/"><span class="card-category-list-name">GitHub</span><span class="card-category-list-count">127</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/LLMs/"><span class="card-category-list-name">LLMs</span><span class="card-category-list-count">16</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>Tags</span></div><div class="card-tag-cloud"><a href="/tags/DSA/" style="font-size: 1.34em; color: rgb(191, 100, 132)">DSA</a><a href="/tags/RL/" style="font-size: 1.26em; color: rgb(88, 4, 94)">RL</a><a href="/tags/Transformer/" style="font-size: 1.36em; color: rgb(15, 7, 31)">Transformer</a><a href="/tags/LLMs/" style="font-size: 1.28em; color: rgb(7, 53, 45)">LLMs</a><a href="/tags/PaperReading/" style="font-size: 1.32em; color: rgb(162, 54, 193)">PaperReading</a><a href="/tags/DeepLearning/" style="font-size: 1.24em; color: rgb(103, 170, 183)">DeepLearning</a><a href="/tags/CV/" style="font-size: 1.17em; color: rgb(9, 137, 60)">CV</a><a href="/tags/GPT/" style="font-size: 1.19em; color: rgb(171, 172, 167)">GPT</a><a href="/tags/PL/" style="font-size: 1.21em; color: rgb(165, 39, 35)">PL</a><a href="/tags/domain/" style="font-size: 1.15em; color: rgb(20, 117, 147)">domain</a><a href="/tags/github/" style="font-size: 1.15em; color: rgb(69, 95, 160)">github</a><a href="/tags/hf/" style="font-size: 1.15em; color: rgb(103, 23, 18)">hf</a><a href="/tags/weibo/" style="font-size: 1.15em; color: rgb(55, 98, 78)">weibo</a><a href="/tags/ArXiv/" style="font-size: 1.39em; color: rgb(193, 75, 140)">ArXiv</a><a href="/tags/Domain/" style="font-size: 1.39em; color: rgb(154, 112, 37)">Domain</a><a href="/tags/AI/" style="font-size: 1.45em; color: rgb(92, 85, 64)">AI</a><a href="/tags/GitHub/" style="font-size: 1.43em; color: rgb(178, 33, 4)">GitHub</a><a href="/tags/Trending/" style="font-size: 1.43em; color: rgb(7, 176, 101)">Trending</a><a href="/tags/HuggingFace/" style="font-size: 1.39em; color: rgb(152, 90, 22)">HuggingFace</a><a href="/tags/Papers/" style="font-size: 1.39em; color: rgb(185, 187, 126)">Papers</a><a href="/tags/%E5%BE%AE%E5%8D%9A/" style="font-size: 1.41em; color: rgb(179, 191, 127)">微博</a><a href="/tags/%E7%83%AD%E6%90%9C/" style="font-size: 1.41em; color: rgb(140, 38, 166)">热搜</a><a href="/tags/leetcode/" style="font-size: 1.3em; color: rgb(181, 30, 128)">leetcode</a><a href="/tags/algo/" style="font-size: 1.17em; color: rgb(62, 23, 166)">algo</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>Archives</span><a class="card-more-btn" href="/archives/" title="More">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/11/"><span class="card-archive-list-date">November 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/01/"><span class="card-archive-list-date">January 2024</span><span class="card-archive-list-count">5</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/12/"><span class="card-archive-list-date">December 2023</span><span class="card-archive-list-count">14</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/11/"><span class="card-archive-list-date">November 2023</span><span class="card-archive-list-count">26</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2023/10/"><span class="card-archive-list-date">October 2023</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>Info</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">Article :</div><div class="item-count">565</div></div><div class="webinfo-item"><div class="item-name">Run time :</div><div class="item-count" id="runtimeshow" data-publishDate="2023-05-31T16:00:00.000Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Total Count :</div><div class="item-count">24897.4k</div></div><div class="webinfo-item"><div class="item-name">UV :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">PV :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">Last Push :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-11-01T23:51:22.310Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>