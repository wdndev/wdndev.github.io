<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>ArXiv Domain 2026-01-30 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：ArXiv Domain  LLM Domain Papers1. Recursive Language ModelsWe study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propo">
<meta property="og:type" content="article">
<meta property="og:title" content="ArXiv Domain 2026-01-30">
<meta property="og:url" content="https://wdndev.github.io/daily/domain/202601/2026-01-30/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：ArXiv Domain  LLM Domain Papers1. Recursive Language ModelsWe study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.708Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="ArXiv">
<meta property="article:tag" content="Domain">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/domain/202601/2026-01-30/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'ArXiv Domain 2026-01-30',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">ArXiv Domain 2026-01-30</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.708Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">159.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>543min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="ArXiv Domain 2026-01-30"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://arxiv.org">ArXiv Domain</a></p>
</blockquote>
<h2 id="LLM-Domain-Papers"><a href="#LLM-Domain-Papers" class="headerlink" title="LLM Domain Papers"></a>LLM Domain Papers</h2><h3 id="1-Recursive-Language-Models"><a href="#1-Recursive-Language-Models" class="headerlink" title="1. Recursive Language Models"></a>1. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.24601v2">Recursive Language Models</a></h3><p>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse long-context tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLM-Qwen3-8B, outperforms the underlying Qwen3-8B model by $28.3\%$ on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们研究通过推理时间扩展的视角，使大型语言模型（LLMs）能够处理任意长度的提示。我们提出了递归语言模型（RLMs），这是一种通用的推理范式，将长提示视为外部环境的一部分，并允许LLM以编程方式检查、分解并递归调用自身来处理提示片段。我们发现，RLMs能够成功处理比模型上下文窗口大两个数量级的输入，即使对于较短的提示，在四个不同的长上下文任务中，其性能也显著优于普通的前沿LLM和常见的长上下文结构，同时成本相当。在小规模实验中，我们对首个原生递归语言模型进行了后训练。我们的模型RLM-Qwen3-8B，相对于基础Qwen3-8B模型平均提高了28.3%，甚至在三个长上下文任务中接近普通GPT-5的质量。代码可在 <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现代大语言模型（LLM）在处理“任意长输入”时遭遇的两大瓶颈：</p>
<ol>
<li><p>物理上下文窗口限制<br>即使最新模型（如 GPT-5）的显式窗口已达 272K token，仍无法直接容纳 10M 乃至 100M 级别的长文档、代码库或多跳检索语料。</p>
</li>
<li><p>上下文腐烂（context rot）<br>在窗口内，随着输入变长，模型对早期信息的利用效率急剧下降，表现为任务复杂度越高、衰减越快（图 1）。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Recursive Language Models (RLMs)</strong>——一种<strong>推理时（inference-time）通用框架</strong>，把超长 prompt 视为外部环境变量，让 LLM 在 Python REPL 里用代码“窥视、分解、递归调用自身”处理片段，从而：</p>
<ul>
<li>将有效上下文长度扩展两个数量级（实验到 10M+ token）；</li>
<li>在多项长上下文基准上显著优于直接调用基座模型、摘要式代理、检索增强代理等基线，且成本持平或更低；</li>
<li>无需修改模型参数或重新训练，仅通过推理策略即可落地。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两大脉络，并指出 RLM 与它们的本质区别：</p>
<ol>
<li>长上下文系统（Long-Context LM Systems）</li>
</ol>
<ul>
<li><strong>架构-级扩展</strong>：通过重新训练或改进注意力机制来延长上下文，例如<br>– 线性偏置注意力 Press et al. 2022<br>– 结构化状态空间 Gu et al. 2022<br>– Infini-attention Munkhdalai et al. 2024</li>
<li><strong>外部脚手架（scaffold）</strong>：不改动模型，而在推理阶段压缩或管理上下文<br>– 有损管理：MemWalker（树状导航摘要）、ReSum（迭代压缩）、Claude-Code 式多轮摘要<br>– 显式记忆层级：MemGPT、Mem0、G-Memory 等把上下文拆成“主存/外存”<br><em>区别</em>：RLM 不预设任何摘要、检索或记忆结构，完全由模型在 REPL 里用代码自行决定如何切块、过滤、调用子 LLM，上下文管理被“隐式”地交给模型自己。</li>
</ul>
<ol>
<li>子 LLM 递归调用（Task Decomposition via sub-LM calls）</li>
</ol>
<ul>
<li>人设计流程：AutoGPT、ReAct、Claude sub-agents 等多按固定模板拆任务</li>
<li>模型自主拆：ViperGPT、THREAD、DisCIPL、ReDel、Context-Folding、AgentFold 等让 LLM 自己决定何时再调 LLM，但输入长度仍受基模型窗口限制<br><em>区别</em>：RLM 把“输入提示”本身卸载到外部环境，首次让递归调用可以<strong>符号级操作任意长字符串</strong>，从而突破窗口上限；先前方法只能递归拆“任务”，不能递归拆“输入”。</li>
</ul>
<p>综上，RLM 的核心差异可概括为：</p>
<blockquote>
<p>把提示当成环境变量，而非模型输入；用代码+递归子 LLM 实现<strong>输入长度</strong>的无限外扩，而非仅对任务做逻辑拆解。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“超长提示无法一次性塞进上下文”这一瓶颈重新定义为<strong>推理时计算</strong>问题，并借鉴“核外算法”（out-of-core）思想：数据远大于内存时，通过<strong>按需加载+符号化操作</strong>完成计算。具体方案称为 <strong>Recursive Language Models (RLMs)</strong>，其关键机制如下：</p>
<ol>
<li><p>把提示变成环境变量<br>启动一个 Python REPL，将完整提示  P  作为字符串变量 <code>context</code> 载入内存；模型不再直接接收  P  作为输入，而是接收一段<strong>固定系统提示</strong>+REPL 返回的<strong>截断执行结果</strong>。</p>
</li>
<li><p>用代码“窥视-分解-调用”<br>模型在 REPL 里写 Python 代码，可以</p>
</li>
</ol>
<ul>
<li>用 <code>print</code>/<code>regex</code>/<code>pandas</code> 等快速扫描、过滤、统计；</li>
<li>把 <code>context</code> 切成任意粒度（行、文档、token 块）；</li>
<li>通过内置函数 <code>llm_query(sub_prompt)</code> <strong>递归调用子 LLM</strong>（可以是自身轻量版或其他模型），子调用结果再写回 REPL 变量。</li>
</ul>
<ol>
<li><p>递归子调用不受窗口限制<br>每次 <code>llm_query</code> 只把<strong>当前必要片段</strong>喂给子模型，因此总输入长度可以远超基模型物理窗口；子模型输出被存入变量，供后续代码继续聚合、验证或拼接。</p>
</li>
<li><p>终止与答案返回<br>当代码逻辑判断已收集足够信息，模型执行<br><code>FINAL(answer)</code> 或 <code>FINAL_VAR(variable_name)</code><br>把结果返回给用户，整个轨迹在 REPL 内完成，无需人工设计摘要或检索流程。</p>
</li>
</ol>
<p>通过上述四步，RLM 将“长上下文”转化为<strong>可执行对象</strong>，用<strong>代码控制数据流+递归子 LLM 控制语义流</strong>，从而在 10M token 规模上仍保持高精度，且平均调用成本与基模型持平甚至更低。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 4 类信息密度递增的长上下文任务上展开系统实验，覆盖 32 K–11 M token 区间，对比 2 组前沿模型与 4 种推理策略，并给出成本-性能联合分析。实验设计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>基座模型</td>
<td>GPT-5（闭源）与 Qwen3-Coder-480B-A35B（开源）</td>
</tr>
<tr>
<td>对比方法</td>
<td>① Base 直接调用 ② CodeAct(+BM25) ③ Summary Agent ④ RLM（完整） ⑤ RLM（无递归子调用）</td>
</tr>
<tr>
<td>任务集</td>
<td>① S-NIAH（常数级处理）② BrowseComp-Plus@1K-doc（多跳问答，6–11 M token）③ OOLONG（线性级语义聚合，131 K token）④ OOLONG-Pairs（二次级成对聚合，32 K token）⑤ LongBench-v2 CodeQA（900 k 级代码库理解）</td>
</tr>
<tr>
<td>规模轴</td>
<td>对 S-NIAH/OOLONG/OOLONG-Pairs 按 2¹³…2¹⁸ token 做长度扫描，观察性能衰减曲线</td>
</tr>
<tr>
<td>评价指标</td>
<td>准确率 / F1 / 官方评分函数；每次查询的 API 成本与运行时长（25–95 分位）</td>
</tr>
<tr>
<td>消融与失败分析</td>
<td>① 无递归子调用 ablation② 不同文档规模子集（BrowseComp 20 任务，100–1000 doc）③ 轨迹案例：验证-冗余、过度递归、输出令牌耗尽等负面模式</td>
</tr>
</tbody>
</table>
</div>
<p>核心结果一览（Table 1 &amp; Figure 1）：</p>
<ul>
<li>10 M token 规模下，RLM 仍保持 50–90 % 准确率，而基座模型因超出 272 K 窗口直接失败。</li>
<li>在窗口<strong>之内</strong>的任务，RLM 相对 Base 提升 10–58 % 绝对准确率；OOLONG-Pairs 的 F1 从 &lt;0.1 % 提升到 58 %（GPT-5）/ 23 %（Qwen3）。</li>
<li>中位调用成本：RLM 与 Base 同量级，甚至低 3×；仅在长尾轨迹出现高方差。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><strong>异步与并行化</strong>：当前子 LLM 调用为阻塞式，引入异步 <code>llm_query</code> 与并行 chunk 处理可显著压缩延迟与云成本。</li>
<li><strong>更深递归层数</strong>：实验限递归深度=1（仅子 LLM），探索深度&gt;1 的“子-子”调用能否进一步降低单段上下文压力，及如何避免误差级联。</li>
<li><strong>专用 RLM 预训练/后训练</strong>：用 RLM 轨迹数据（代码+子调用序列）做 SFT/RL，使模型学会更优的切块、过滤、停止决策，减少冗余验证。</li>
<li><strong>动态预算控制</strong>：在 REPL 内暴露“已用 token/费用”变量，让模型自主权衡精度-成本，实现可中断的任意长输入推理。</li>
<li><strong>多模态与工具扩展</strong>：将图像、音频、数据库、搜索引擎也作为环境变量，通过同一套“代码+递归调用”框架统一处理跨模态长上下文。</li>
<li><strong>可验证性提升</strong>：引入形式化验证或符号执行，对代码产生的中间变量进行一致性检查，降低长轨迹中“正确→错误”翻转概率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Recursive Language Models (RLMs)</strong> 提出一种<strong>推理时通用框架</strong>，把超长提示卸载到 Python REPL 环境，让大模型用代码“窥视-分解-递归调用自身”，突破上下文窗口与上下文腐烂双重瓶颈。主要贡献如下：</p>
<ol>
<li><p>问题重新定义<br>将“长上下文”视为<strong>推理时计算</strong>问题，而非单纯扩大窗口。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>提示 → 环境变量 <code>context</code></li>
<li>模型写 Python 代码做过滤、切块、聚合</li>
<li>通过 <code>llm_query()</code> 递归调用子 LLM，片段级处理，结果写回变量</li>
<li>终止时 <code>FINAL()/FINAL_VAR()</code> 返回答案</li>
</ul>
<ol>
<li><p>实验<br>在 32 K–11 M token 的 5 项任务（S-NIAH、BrowseComp-Plus、OOLONG、OOLONG-Pairs、CodeQA）上，RLM 用 GPT-5 与 Qwen3-Coder 均取得<strong>双倍位数绝对准确率提升</strong>，10 M token 仍保持高性能，中位 API 成本与基模型持平或更低。</p>
</li>
<li><p>结论<br>RLMs 是<strong>任务无关、模型无关</strong>的推理范式，无需重训练即可把有效上下文扩展两个数量级，为“无限长输入”提供了一条可扩展、可负担的新轴心。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Recursive%20Language%20Models，链接是https://arxiv.org/pdf/2512.24601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.24601。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Alex L. Zhang, Tim Kraska, Omar Khattab</p>
<p><strong>Categories</strong>: cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.24601v2.pdf">https://arxiv.org/pdf/2512.24601v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.24601v2">https://papers.cool/arxiv/2512.24601v2</a></p>
<p><strong>Published</strong>: 2025-12-31T03:43:41Z</p>
<hr>
<h3 id="2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs"><a href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs" class="headerlink" title="2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs"></a>2. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20861v1">Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</a></h3><p>One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>当前AI系统中最缺失的能力之一是部署后持续学习的能力。实现此类持续学习系统面临诸多挑战，其中之一是用于训练最先进LLMs的基于梯度算法的大量内存需求。进化策略（ES）最近作为传统学习算法的无梯度替代方案重新出现，并在LLMs的特定任务中表现出令人鼓舞的表现。本文对ES进行了全面分析，并特别评估了在训练更新步骤增加时遗忘曲线。首先，我们发现ES在计算预算相当的情况下，在数学和推理任务中能够达到接近GRPO的性能。然而，对持续学习最重要的是，ES的性能提升伴随着对先前能力的显著遗忘，限制了其在线训练模型的适用性。我们还探讨了这种行为背后的原因，并展示了使用ES进行的更新比对应GRPO更新更稀疏，且其$\ell_2$范数远大于相应的GRPO更新，这解释了两种算法之间遗忘曲线的对比。通过这项研究，我们旨在突出像ES这样的无梯度算法中遗忘的问题，并希望激励未来工作来缓解这些问题。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文聚焦“如何让大模型在部署后持续学习”这一核心难题，具体针对以下子问题展开研究：</p>
<ul>
<li>梯度类后训练算法（SFT、RLHF、GRPO 等）内存开销巨大，难以在端侧或在线场景下反复更新权重。</li>
<li>近期复兴的梯度无参算法——进化策略（Evolutionary Strategies, ES）——声称在特定任务上可与 GRPO 媲美，且仅需前向推理，内存占用极低，被视为持续学习的潜在替代方案。</li>
<li>然而，ES 在连续更新过程中是否会“灾难性遗忘”旧能力，此前缺乏系统评估；若遗忘严重，则其“在线学习”优势将失去意义。</li>
</ul>
<p>因此，论文旨在</p>
<ol>
<li>在数学与推理任务上全面对比 ES 与 GRPO 的峰值性能；</li>
<li>量化二者在持续更新过程中的遗忘曲线，判断 ES 是否真正适合部署后持续学习；</li>
<li>从参数更新范数与稀疏度角度解释遗忘差异，为后续改进梯度无参算法提供依据。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 Related Work 中提及并扩展了以下几条研究脉络：</p>
<ol>
<li>进化策略（Evolution Strategies, ES）</li>
</ol>
<ul>
<li>经典理论：Rechenberg 1989、Schwefel 1977、Beyer 1995 奠定  (μ,λ) -ES 框架。</li>
<li>实用变体：CMA-ES（Hansen &amp; Ostermeier 2001）、自然 ES（Wierstra et al. 2011, Sun et al. 2012）。</li>
<li>深度神经演化：Such et al. 2018、Risi &amp; Stanley 2019 在百万参数规模验证 GA/ES 可训练 RL 网络。</li>
<li>大模型时代：Salimans et al. 2017 首次将 ES 扩展到强化学习，展示高度并行与无梯度优势。</li>
<li>近期 LLM 工作：<br>– Malladi et al. 2024、Korotyshova et al. 2025 提出内存高效的 LoRA-ES。<br>– Qiu et al. 2025（与本论文直接对标）在 1–7 B 模型上报告 ES 优于 GRPO，但未系统评估遗忘。</li>
</ul>
<ol>
<li>梯度类后训练算法</li>
</ol>
<ul>
<li>SFT：Wei et al. 2022</li>
<li>RLHF：Ouyang et al. 2022</li>
<li>DPO：Rafailov et al. 2024</li>
<li>GRPO：Shao et al. 2024，通过组内相对奖励省去 Critic，成为本文对比基线。</li>
</ul>
<ol>
<li>灾难性遗忘与持续学习</li>
</ol>
<ul>
<li>Kirkpatrick et al. 2017 提出 EWC，首次在深度学习语境下量化遗忘。</li>
<li>Gupta et al. 2024 发现大规模模型编辑同样会出现“渐进-突发”式遗忘。</li>
<li>Shenfeld et al. 2025 观察到带 KL 正则的在线 RL（包括 GRPO）遗忘更少，为本论文提供参照。</li>
</ul>
<ol>
<li>参数更新稀疏性研究</li>
</ol>
<ul>
<li>Mukherjee et al. 2025 通过掩码分析证明 RLHF 仅微调极小“子网络”，解释其抗遗忘特性；本文沿用其稀疏度定义与分层统计方法，对比 ES 与 GRPO。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一套“新算法”去彻底解决灾难性遗忘，而是通过<strong>系统性实证诊断</strong>揭示 ES 在持续学习场景下的根本缺陷，从而为后续算法设计指明改进方向。具体步骤如下：</p>
<ol>
<li>构建可比实验框架</li>
</ol>
<ul>
<li>统一任务：Countdown、GSM8K、MATH、OlympiadBench。</li>
<li>统一模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct。</li>
<li>统一资源：30 条 rollout / population，200 条训练样本，500 更新步以内。</li>
<li>统一评估：每轮 checkpoint 同时测“新任务准确率”与“旧任务（HellaSwag）准确率”，绘制遗忘曲线。</li>
</ul>
<ol>
<li>量化遗忘现象</li>
</ol>
<ul>
<li>绘制 Pareto 前沿：新任务准确率 vs 旧任务准确率。</li>
<li>统计旧任务性能随更新步的衰减斜率；ES 在 200 步后额外训练几乎不再提升新任务，却持续降低旧任务约 10 个百分点，而 GRPO 基本平稳。</li>
</ul>
<ol>
<li>追溯根本原因</li>
</ol>
<ul>
<li><strong>更新范数</strong>：计算相邻 checkpoint 的 Frobenius 范数</li>
</ul>
<p>|DeltaW|<em>F = |W</em>(t) - W_(t-1)|_F</p>
<p>ES 在 500 步时漂移幅度比 GRPO 大 <strong>3 个数量级</strong>。</p>
<ul>
<li><strong>更新稀疏度</strong>：定义</li>
</ul>
<p>sparsity = #|(DeltaW)_(ij)| &lt; 10^(-6)#params</p>
<p>GRPO 稀疏度 ≈ 95 %，ES 普遍 &lt; 20 %，呈全局密集扰动。</p>
<ul>
<li>关联分析：大范数 + 低稀疏 → 参数空间全局漂移 → 与旧任务表征冲突 → 灾难性遗忘。</li>
</ul>
<ol>
<li>给出改进启示</li>
</ol>
<ul>
<li>梯度无参方法若想用于持续学习，必须<strong>显式约束更新范数</strong>（如裁剪、衰减）或<strong>强制稀疏掩码</strong>（如 LoRA-ES + 动态门控）。</li>
<li>可借鉴 GRPO 的 KL 正则思路，在 ES 的目标中引入与锚定模型的散度惩罚，限制漂移。</li>
</ul>
<p>综上，论文通过“实验量化 → 现象确认 → 根因剖析 → 改进提示”四步，<strong>把 ES 的遗忘风险从经验猜测转化为可度量、可解释的定量结论</strong>，为后续设计“既省内存又不遗忘”的梯度无参算法奠定实证基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计三类实验，全部围绕“ES 能否在持续学习场景下替代 GRPO”这一核心问题展开。实验均在同一硬件（NVIDIA RTX A6000）与同一代码框架（VERL + 自复现 ES）上完成，保证可比性。</p>
<ol>
<li>主实验：ES vs GRPO 峰值性能对照</li>
</ol>
<ul>
<li>模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct</li>
<li>任务：Countdown、GSM8K、MATH、OlympiadBench</li>
<li>协议：每任务 200 训练样本，30 rollout/population，最大 500 更新步，早停验证集不再提升。</li>
<li>指标：验证集最佳准确率（%）</li>
<li>结果：ES 平均落后 GRPO 3–4 个百分点，仅在 Llama-3.2-1B/GSM8K 一项略超；其余均低于 GRPO，反驳了“ES 全面优于 GRPO”的先前提法。</li>
</ul>
<ol>
<li>遗忘曲线实验</li>
</ol>
<ul>
<li>固定模型：Qwen2.5-1.5B-Instruct</li>
<li>新任务：Countdown（训练集 200 条）</li>
<li>旧任务：HellaSwag（全验证集，作为“通用先验能力”探针）</li>
<li>协议：每 50 步保存一个 checkpoint，同时测 Countdown 与 HellaSwag 准确率。</li>
<li>可视化：<br>– Pareto 前沿图：新任务准确率 vs 旧任务准确率<br>– 单指标时序图：HellaSwag 准确率随更新步变化</li>
<li>结果：<br>– ES 在 200 步后新任务收敛，但旧任务仍单调下降，累计跌落 ≈ 10 %。<br>– GRPO 旧任务曲线几乎水平，无明显遗忘。</li>
</ul>
<ol>
<li>更新诊断实验（解释为何遗忘）<br>3.1 更新范数</li>
</ol>
<ul>
<li>计算相邻 checkpoint 参数差 ΔW 的 Frobenius 范数</li>
<li>结果：500 步时 ES 漂移比 GRPO 大 <strong>1000×</strong> 以上，且单调递增。</li>
</ul>
<p>3.2 更新稀疏度</p>
<ul>
<li>定义阈值 τ=1e-6，统计 |ΔW_ij|&lt;τ 的元素比例</li>
<li>分层统计：Attention Q/K/V、WO、MLP、LayerNorm</li>
<li>结果：<br>– GRPO 稀疏度 ≈ 95 %（仅 5 % 参数被显著修改）<br>– ES 稀疏度 &lt; 20 %，几乎全参数被扰动，LayerNorm 稍高但仍远低于 GRPO。</li>
</ul>
<p>3.3 KL-散度关联（附录）</p>
<ul>
<li>计算 π_θ 与 base 模型在 Countdown 与 HellaSwag 上的 KL</li>
<li>结果：ES 的 KL 随步数线性增长，且与旧任务准确率呈显著负相关；GRPO 因显式 KL 惩罚，散度与准确率均无单调关系。</li>
</ul>
<p>通过上述三类实验，论文既验证了 ES 的“性能可比性”，又定量揭示了其“灾难性遗忘”现象，并用范数+稀疏度+KL 三维诊断给出解释。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，均围绕“让梯度无参算法既省内存又不灾难性遗忘”这一核心目标展开：</p>
<ol>
<li>约束型 ES 框架</li>
</ol>
<ul>
<li>将 KL 惩罚或参数距离惩罚显式写入 ES 的目标：</li>
</ul>
<p>θ<em>(t+1) = θ_t + α · (1) / (N) ∑_n Z_n ε_n - β ∇</em>θ D<em>(KL)(π</em>(θ) parallel π_(ref))</p>
<p>需推导无梯度情形下如何高效估计该惩罚项。</p>
<ul>
<li>探索硬约束：每步更新后投影到以  θ<em>(base)  为中心的  ell_2  球或  ell</em>∞  立方体内，限制漂移半径。</li>
</ul>
<ol>
<li>稀疏化 ES 更新</li>
</ol>
<ul>
<li>引入随机掩码：每次只扰动随机子集参数（如 5 %），保持其余固定；掩码可在层、通道或奇异值维度上采样。</li>
<li>学习稀疏掩码：借鉴 RigL / Magnitude Pruning，动态决定“哪些参数值得扰动”，使有效更新稀疏度逼近 GRPO 的 95 % 水平。</li>
</ul>
<ol>
<li>低秩 / 适配器混合 ES</li>
</ol>
<ul>
<li>仅在 LoRA 低秩矩阵（A、B）上应用 ES 扰动，冻结主干；漂移空间从 1.5 B 降至数百万参数，天然减小范数。</li>
<li>比较不同秩（4, 16, 64）对“性能-遗忘”Pareto 前沿的影响，找出秩-遗忘临界值。</li>
</ul>
<ol>
<li>种群级正则</li>
</ol>
<ul>
<li>在种群内部保持“锚点个体”——不扰动的  θ_(base)  副本，将其奖励作为额外基线，惩罚偏离过远的个体。</li>
<li>研究锚点比例（1/30、5/30、10/30）对旧任务保留率的边际效应。</li>
</ul>
<ol>
<li>多任务 ES 演化</li>
</ol>
<ul>
<li>同时把 Countdown + HellaSwag 打包成多目标奖励：</li>
</ul>
<p>R = R<em>(new) + λ R</em>(old)</p>
<p>用演化多目标算法（NSGA-ES）搜索非支配解集，观察是否自然涌现“不遗忘”个体。</p>
<ul>
<li>考察  λ  调度：初期  λ  大，后期逐步减小，让模型先稳固旧能力再提升新能力。</li>
</ul>
<ol>
<li>与参数高效微调正交组合</li>
</ol>
<ul>
<li>将 ES 用于 Prompt-tuning、AdaLoRA、VeRA 等“极轻量”模块，验证遗忘是否随可训参数量线性下降。</li>
<li>对比同一参数预算下“ES-LoRA”与“GRPO-LoRA”的 Pareto 前沿，判断梯度自由在极低秩下是否仍有优势。</li>
</ul>
<ol>
<li>在线 / 流式场景测试</li>
</ol>
<ul>
<li>构建“任务流”：按顺序依次训练 Countdown → GSM8K → MATH → Olympiad，每任务仅 200 样本，周期性回放 5 % 旧数据。</li>
<li>监测 ES 与 GRPO 的“遗忘-学习”折线，评估哪种算法在数据流场景下最终平均性能更高。</li>
</ul>
<ol>
<li>理论侧：ES 漂移上界</li>
</ol>
<ul>
<li>在强凸或 PL 条件下，推导 ES 更新步的期望漂移</li>
</ul>
<p>E[|θ_(t)-θ_0|^2] le f(t, σ, α, d)</p>
<p>并与 SGD/GRPO 的对应上界比较，给出“遗忘-漂移”定量关系。</p>
<ol>
<li>统计稳定性提升</li>
</ol>
<ul>
<li>扩大种群（30 → 128 → 512）与减半学习率，观察遗忘曲线方差是否显著下降；给出“种群大小-遗忘方差”经验公式，为工业部署提供选型依据。</li>
</ul>
<ol>
<li>遗忘评测体系扩展</li>
</ol>
<ul>
<li>引入更多旧任务探针（MMLU、ARC、TruthfulQA）与参数探测方法（LiNGAM、CKA），检查遗忘是否具“任务特异性”或“全局通用性”。</li>
<li>记录层-wise 遗忘速率，找出 ES 最先干扰的表征层，为后续选择性正则提供目标。</li>
</ul>
<p>通过上述探索，可逐步把 ES 从“内存友好但易遗忘”的基线，推向“内存友好且持续稳健”的实用在线学习方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>研究背景</strong><br>大模型部署后需持续学习，但梯度类后训练（SFT、RLHF、GRPO）内存开销大；进化策略（ES）作为梯度无参替代方案近期被重新关注，声称性能可比 GRPO 且极省内存，然而其是否会灾难性遗忘尚缺系统评估。</p>
</li>
<li><p><strong>核心发现</strong></p>
</li>
</ul>
<ol>
<li>在 Countdown、GSM8K、MATH、OlympiadBench 上，ES 峰值准确率仅落后 GRPO 3–4 个百分点，计算步数相近，确认其“性能可比”。</li>
<li>在同一微调 run 内，ES 新任务收敛后旧任务（HellaSwag）准确率仍持续下降约 10 %；GRPO 几乎无遗忘。</li>
<li>诊断表明 ES 更新 Frobenius 范数比 GRPO 大 <strong>1000×</strong>，稀疏度 &lt; 20 %（GRPO ≈ 95 %），全局密集漂移导致旧能力被破坏。</li>
</ol>
<ul>
<li><strong>结论</strong><br>ES 虽提供内存友好的梯度无参路径，但单任务微调即出现显著灾难性遗忘，目前不适合需要保留旧能力的在线学习场景；未来需引入稀疏化、范数约束或 KL 正则等机制以缓解遗忘。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs，链接是https://arxiv.org/pdf/2601.20861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20861。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20861v1.pdf">https://arxiv.org/pdf/2601.20861v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20861v1">https://papers.cool/arxiv/2601.20861v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:59:34Z</p>
<hr>
<h3 id="3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching"><a href="#3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching" class="headerlink" title="3. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching"></a>3. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15942v2">ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</a></h3><p>Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>气候预测存在与气候系统组成部分及其相互作用相关的不确定性。量化这些不确定性的一种典型方法是使用气候模型在不同初始条件下创建重复模拟的集合。由于这些模拟的复杂性，生成此类预测集合在计算上代价高昂。在本工作中，我们提出了ArchesClimate，一种基于深度学习的气候模型仿真器，旨在降低这一成本。ArchesClimate在IPSL-CM6A-LR气候模型的十年重预测数据上进行训练，空间分辨率约为2.5x1.25度。我们按照ArchesWeatherGen的方法训练一个流匹配模型，并将其改编为预测近期气候。一旦训练完成，该模型即可生成一个月的预测状态，并可用于自回归地仿真任意长度的气候模型模拟。我们展示了在长达10年的时间尺度上，这些生成结果是稳定且符合物理规律的。我们还显示，对于多个重要气候变量，ArchesClimate生成的模拟结果可以与IPSL模型互换。本研究表明，气候模型仿真器可能显著降低气候模型模拟的成本。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在以极低计算成本生成气候模式 IPSL-CM6A-LR 的十年尺度概率集合，从而替代传统“多次扰动初值 + 全模式积分”这一昂贵流程，支撑年代际气候变率与不确定性的量化研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>NeuralGCM</strong>（Kochkov et al. 2024）<br>混合物理-深度学习自回归大气集合 emulator，在网络内部注入噪声以产生内部变率。</p>
</li>
<li><p><strong>Spherical DYffusion</strong>（Cachay et al. 2023）<br>基于扩散模型的概率气候 emulator，显式对大气状态分布进行建模。</p>
</li>
<li><p><strong>cBottle</strong>（Brenowitz et al. 2025）<br>生成式扩散模型，学习多源再分析资料的瞬时短期动力，但非自回归，无法延伸为长序列。</p>
</li>
<li><p><strong>ACE / ACE2</strong>（Watt-Meyer et al. 2023, 2024）<br>确定性大气 emulator，可稳定积分千年，但不包含随机性，也未耦合海洋。</p>
</li>
<li><p><strong>LUCIE</strong>（Guan et al. 2024）<br>轻量级非耦合气候 emulator，以 ERA5 为训练数据，侧重高效生成大集合。</p>
</li>
<li><p><strong>DLESyM</strong>（Cresswell-Clay et al. 2025）<br>耦合大气-海洋深度学习地球系统模型，用 ERA5 训练，验证 AI 同时模拟两圈层的可能性。</p>
</li>
<li><p><strong>SamudrACE</strong>（Duncan et al. 2025）<br>将确定性海洋 emulator Samudra 与 ACE2 耦合，实现快速海-气交换模拟。</p>
</li>
<li><p><strong>ACE-SOM2</strong>（Clark et al. 2024）<br>ACE2 与单层海洋耦合，用于探究不同强迫下的气候响应，向更长尺度与不同情景扩展。</p>
</li>
</ul>
<p>这些研究共同构成了“AI 气候模式替代”脉络，为 ArchesClimate 在概率性、十年尺度、海-气耦合与计算效率上的设计提供了直接对比与参考。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 ArchesClimate，一套基于**流匹配（flow matching）**的深度生成框架，以月时间步长、≈2.5°×1.25° 空间分辨率自回归地模拟 IPSL-CM6A-LR 的十年集合。核心思路与步骤如下：</p>
<ol>
<li><p>数据利用<br>采用 IPSL-CM6A-LR 对 CMIP6-DCPP 的 55 组 10 年、10 成员回报试验（≈7×10⁴ 个月），仅选取 24 个关键海-气变量（含 4 层大气、7 个海洋整层量）及 5 种辐射强迫（CO₂、CH₄、N₂O、CFC11eq、SSI）作为训练集。</p>
</li>
<li><p>两阶段架构</p>
</li>
</ol>
<ul>
<li><strong>确定性模型</strong>  f<em>θ ：3D-Swin-U-Net 结构，输入  X_t 、 X</em>(t-δ)  与强迫，输出下月平均态  hat X_(t+δ) 。</li>
<li><strong>生成模型</strong>  g_θ ：同骨干网络，以 flow matching 学习残差分布</li>
</ul>
<p>r<em>(t+δ)= X</em>(t+δ) - f<em>θ(X_t,X</em>(t-δ))σ</p>
<p>训练时从  mathcal N(0,I)  到残差分布建立可逆映射；推理时从随机噪声出发，经  M=12  步 ODE 积分生成样本残差，再加回确定性预测得到完整状态。</p>
<ol>
<li><p>强迫嵌入<br>对每条辐射强迫标量，先经线性嵌入生成 scale/shift 参数，再以<strong>条件层归一化</strong>注入所有 Transformer 块，使模型可在不同强迫情景下动态调整。</p>
</li>
<li><p>自回归集合生成<br>训练完成后，用  X<em>0 、 X</em>(-1)  初始化，交替调用  f<em>θ  与  g</em>θ  逐月外推；每次从独立高斯噪声采样即可产生不同成员，实现 10 年、任意大小集合的“一键”输出。</p>
</li>
<li><p>计算加速<br>模型仅作用于 144×144×45 的粗网格，参数空间约为原模式 1/400；在 4×A100 上 12 min 可生成 10 年 10 成员集合，对应 ≈4 core-h/模拟年，而 IPSL-CM6A-LR 需 ≈1900 core-h/模拟年。</p>
</li>
</ol>
<p>通过“确定性骨架 + 流匹配残差”策略，ArchesClimate 在保持物理一致性、季节-年际变率与低频趋势的同时，把传统气候模式集合成本降低两个数量级，从而解决了“十年尺度概率集合昂贵”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否以极低算力生成与 IPSL-CM6A-LR 统计-物理特性一致的十年集合”这一核心问题，设计了 10 组实验，覆盖统计评分、物理一致性、可交换性、方差调校与长期稳定性等维度。实验按目的可归纳为以下 6 类：</p>
<ol>
<li>基础统计评估</li>
</ol>
<ul>
<li><strong>CRPS 与标准差对比</strong>（表 3）<br>对 1969-79、1979-89、2010-20 三个十年，计算 8 个关键变量（tos、psl、net_flux、thetaot2000 等）的纬度加权 CRPS 与集合标准差，比较 ArchesClimate、IPSL-DCPP 与 Pattern-Scaling 基线。</li>
<li><strong>方差-CRPS 消融</strong>（图 5）<br>对比三种训练配置：<br>– residual-flow（本文方案）<br>– full-deterministic（仅  f<em>θ ）<br>– full-flow（仅  g</em>θ ，无确定性骨架）<br>量化确定性骨架对精度与方差的贡献。</li>
</ul>
<ol>
<li>集合可交换性（rank histogram）</li>
</ol>
<ul>
<li><strong>北大西洋区域 rank histogram</strong>（图 6 左）<br>1969 起报的 10 成员、10 年序列，对 6 个变量逐格点排序，检验 ArchesClimate 成员是否能被视为从 IPSL 分布中独立同分布抽取。</li>
</ul>
<ol>
<li>频谱与变率结构</li>
</ol>
<ul>
<li><strong>时间功率谱（TPS）对比</strong>（图 6 右）<br>对同一北大西洋 10 年异常序列做逐点傅里叶变换，比较月-季-年周期强度，诊断 ArchesClimate 是否欠变率（under-dispersion）。</li>
</ul>
<ol>
<li>长期趋势与遥相关</li>
</ol>
<ul>
<li><strong>线性趋势空间相关</strong>（图 7）<br>计算 5 成员集合的 tos 异常 10 年趋势，逐像素与 IPSL 对照，评估年代际信号在三大洋及北极的相关性。</li>
<li><strong>50 年强迫响应</strong>（附录图 A1）<br>固定 1969 强迫 vs 时变强迫各 rollout 50 年，验证模型能否随辐射强迫漂移而非崩溃。</li>
</ul>
<ol>
<li>区域/季节信号</li>
</ol>
<ul>
<li><strong>热带区域气候态与异常</strong>（图 4）<br>20°S–20°N 平均的季节循环与异常时间序列，对比集合均值与 ±1σ 包络。</li>
<li><strong>北大西洋季节异常空间图</strong>（图 8）<br>MAM/JJA/SON/DJF 四季平均 tos 异常的空间格局对比，检验季节尺度空间相位与强度。</li>
</ul>
<ol>
<li>方差增强与损失设计</li>
</ol>
<ul>
<li><strong>三种增方差策略</strong>（图 9）<br>– 推理初始噪声放大 1.1×<br>– 按变量训练-验证方差差值逐变量缩放噪声<br>– 在 flow-matching 损失中加入梯度与功率谱项（ L<em>(rm total)=L</em>(rm MSE)+0.2L<em>(rm grad)+0.2L</em>(rm PSD) ）<br>比较其对 CRPS-方差权衡的影响。</li>
</ul>
<p>以上实验共同表明：</p>
<ul>
<li>ArchesClimate 的 CRPS 与 IPSL 相当或更低，但方差普遍略小；</li>
<li>加入谱-梯度损失可把方差提升到 IPSL 水平，代价是 CRPS 略增；</li>
<li>rank histogram 在 net_flux、wap 等变量接近均匀，tos、thetaot2000 存在低值偏移；</li>
<li>热带与北大西洋季节空间异常被忠实再现，年代际趋势在三大洋遥相关区相关系数 &gt;0.8；</li>
<li>50 年 rollout 仍能跟随辐射强迫漂移，未见数值漂移或崩溃。</li>
</ul>
<p>因此，论文从统计、物理、频谱、长期稳定性四个角度系统验证了“低成本 AI 集合”替代“高成本模式集合”的可行性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>高分辨率时空降尺度</strong><br>以 ArchesClimate 月尺度 2.5° 输出为条件，训练条件扩散或超分模型，生成 1 km-日级别区域情景，支持极端事件归因与局地影响评估。</p>
</li>
<li><p><strong>多世纪长序列外推</strong><br>将训练数据从 55×10 年扩展至 CMIP6 “historical” 与 SSP 几百年试验，检验对 50-100 年低频变率（AMO、PDO、IPO）与强迫响应的保真度。</p>
</li>
<li><p><strong>增强海洋与冰冻圈变量</strong><br>引入海冰厚度/面积、陆地冰盖、三维盐度与洋流、植被动态，减少北极趋势漂移并提升碳循环一致性。</p>
</li>
<li><p><strong>守恒与物理约束嵌入</strong><br>在损失或网络层显式加入大气静力平衡、柱总水量/能量守恒、海洋热盐守恒，抑制长期能量泄漏。</p>
</li>
<li><p><strong>极端与尾部分布校准</strong><br>采用尾部分位数映射或极端值理论对 flow-matching 输出进行事后校正，提高热浪、干旱、极端降水重现期精度。</p>
</li>
<li><p><strong>单步/少步生成加速</strong><br>引入 Consistency-Model 或 Rectified-Flow 单步采样，把 12 步推理降至 1-2 步，再降一个数量级推理耗时。</p>
</li>
<li><p><strong>跨模式迁移与多模式集合</strong><br>用 IPSL 训练的模型权重初始化，微调至 CESM2、MRI-ESM2 等，实现“零成本”多模式集合加权平均。</p>
</li>
<li><p><strong>初始化信息再利用</strong><br>将海洋同化态（三维温盐）或海冰初值编码为条件向量，评估初始化记忆对 1-5 年预测技巧的贡献。</p>
</li>
<li><p><strong>强迫情景插值与反事实试验</strong><br>在嵌入空间对 CO₂、SSI 做线性或非线性插值，生成未运行过的强迫路径（如 1.5 °C 过冲、火山群喷发），快速筛查气候响应。</p>
</li>
<li><p><strong>谱方差自适应损失</strong><br>按纬度-变量-频段动态加权功率谱损失，缓解年循环“过拟合”而高频变率不足的问题，实现方差-精度自动权衡。</p>
</li>
<li><p><strong>不确定性分解</strong><br>利用大样本（10³-10⁴ 成员）将总不确定性拆分为内部变率、模型结构、参数与强迫，对照传统大集合进行方差分解验证。</p>
</li>
<li><p><strong>在线数据同化闭环</strong><br>将 ArchesClimate 作为“廉价预报核”，与 Ensemble Kalman Filter 结合，实现实时年代际预测系统，每月滚动更新。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>ArchesClimate：用流匹配实现十年尺度概率气候集合的快速生成</strong></p>
<ol>
<li><p>问题<br>传统气候模式需对初值扰动运行数十次才能量化十年内部变率，单成员 10 年 ≈ 1900 core-h，成本高昂。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>数据：IPSL-CM6A-LR 的 DCPP 回报（55×10 年×10 成员，≈7×10⁴ 月），24 个海-气变量 + 5 种辐射强迫。</li>
<li>架构：两阶段<br>– 确定性 Swin-U-Net  f<em>θ  预测下月平均态；<br>– 流匹配生成网络  g</em>θ  学习残差分布，12 步 ODE 从高斯噪声采样。</li>
<li>训练：先训  f<em>θ  再训  g</em>θ ，损失为逐网格 MSE（纬度加权）+ 可选谱/梯度正则。</li>
<li>推理：自回归 rollout，10 年 10 成员在 4×A100 上 12 min 完成（4 core-h/年）。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>统计：CRPS 与 IPSL 相当，方差略低；加入谱损失可把方差提升至 IPSL 水平。</li>
<li>可交换性：net_flux、wap 的 rank histogram 接近均匀，tos/thetaot2000 略偏低值。</li>
<li>频谱：年-季周期准确，月-年际功率略低。</li>
<li>趋势：三大洋年代际趋势空间相关 &gt;0.8，北极因无冰过程快速退相关。</li>
<li>长期：50 年 rollout 仍能跟随辐射强迫漂移，无崩溃。</li>
<li>区域：热带与北大西洋季节异常空间相位与观测一致。</li>
</ul>
<ol>
<li><p>结论<br>ArchesClimate 以 ≈ 1/400 参数规模、1/500 计算成本，生成与 IPSL-CM6A-LR 统计-物理特性高度一致的十年集合，为年代际气候不确定性与极端事件研究提供可负担的“AI 集合引擎”。</p>
</li>
<li><p>展望<br>高分辨率降尺度、多世纪长序列、守恒约束、极端校正、单步采样及跨模式迁移等方向可进一步提升可用性与物理可信度。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ArchesClimate%3A%20Probabilistic%20Decadal%20Ensemble%20Generation%20With%20Flow%20Matching，链接是https://arxiv.org/pdf/2509.15942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.15942。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis</p>
<p><strong>Categories</strong>: physics.ao-ph, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15942v2.pdf">https://arxiv.org/pdf/2509.15942v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15942v2">https://papers.cool/arxiv/2509.15942v2</a></p>
<p><strong>Published</strong>: 2025-09-19T12:53:24Z</p>
<hr>
<h3 id="4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems"><a href="#4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems" class="headerlink" title="4. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems"></a>4. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06052v3">DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</a></h3><p>Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>离散组合问题（DCP）在工业决策和优化中非常常见。然而，尽管针对DCP的约束求解技术已经取得了显著进展，其核心过程——形式化问题，即约束建模，仍需要大量专业知识，并且是广泛应用的瓶颈。为了缓解这一瓶颈，近期研究探索了使用大型语言模型（LLM）将组合问题描述转化为可执行的约束模型。然而，现有的离散约束建模评估数据集通常仅限于小型、同质化或特定领域的问题，无法反映现实世界场景的多样性。本工作通过引入DCP-Bench-Open来填补这一空白，这是一个新型基准，包含来自约束规划（CP）和运筹学（OR）社区的多种知名离散组合问题，并专门针对LLM驱动的约束建模进行了结构化设计。利用该数据集，并考虑不同建模框架的多样性，我们比较评估了LLM在三种不同约束建模系统中的建模能力，这些系统在抽象层次和底层语法上各不相同。值得注意的是，当使用基于Python的高级框架进行建模时，性能显示出更高的表现。此外，我们系统评估了不同LLM在提示式和推理时计算方法下的使用，这进一步提高了准确性，在这一高度挑战性的基准上达到了91%。DCP-Bench-Open已公开提供。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决的问题是如何利用大型语言模型（LLMs）来简化约束编程（Constraint Programming, CP）中的约束建模过程，从而降低约束编程的使用门槛，使其能够被更广泛的非专家用户所接受和应用。具体而言，论文的目标包括：</p>
<ol>
<li><p><strong>解决约束建模的瓶颈问题</strong>：约束编程是一种解决组合优化问题的有效范式，但其核心过程——约束建模——非常复杂，需要深厚的应用领域知识和对建模框架的熟练掌握。这限制了约束编程技术的广泛应用。论文探索使用LLMs作为建模助手，将自然语言描述的组合问题转化为可执行的约束模型，类似于现有的基于LLMs的代码生成助手。</p>
</li>
<li><p><strong>提供一个多样化的基准数据集</strong>：现有的用于评估LLMs在约束建模能力上的数据集往往存在局限性，例如规模小、同质化或特定于某个领域，无法涵盖现实世界场景的多样性。论文通过引入一个新的基准数据集CP-Bench，填补了这一空白。CP-Bench包含了一系列来自CP社区的多样化、知名的组合问题类别，专门用于评估LLMs驱动的CP建模能力。</p>
</li>
<li><p><strong>评估不同约束建模框架对LLMs性能的影响</strong>：论文比较和评估了LLMs在三种不同抽象层次和底层语法的约束建模系统中的建模能力，这些系统包括高级别的MiniZinc语言、基于Python的CPMpy库，以及较低级别的Python接口OR-Tools CP-SAT求解器。通过这种比较，论文旨在了解不同框架对LLMs建模准确性的具体影响。</p>
</li>
<li><p><strong>探索提升LLMs生成有效约束模型能力的方法</strong>：为了提高LLMs生成有效约束模型的能力，论文系统地评估了基于提示（prompt-based）和推理时计算（inference-time compute）方法在约束建模中的应用。这些方法包括从现有基于LLMs的代码生成研究中借鉴而来的技术，如检索增强型上下文学习（Retrieval-Augmented In-Context Learning, RAICL）、重复采样和自我验证等。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了多个与利用大型语言模型（LLMs）进行约束建模相关的研究工作，这些研究主要集中在以下几个方面：</p>
<p>约束编程与大型语言模型的结合</p>
<ul>
<li><strong>NL4Opt Competition</strong>：Ramamonjison等人在NeurIPS 2022竞赛中提出了NL4Opt数据集，这是一个包含简单线性规划（LP）问题的基准，用于评估LLMs将自然语言问题描述转化为优化模型的能力<br>^28^<br>。</li>
<li><strong>Logic Grid Puzzles (LGPs)</strong>：Jabrayilzade和Tekir提出了一个用于解决逻辑网格谜题的数据集，这些谜题可以看作是组合问题的一个特定子集<br>^14^<br>。</li>
<li><strong>Text2Zinc</strong>：Singirikonda等人提出了一个跨领域的数据集，包含MiniZinc模型，主要关注线性规划（LP）和混合整数规划（MIP）问题<br>^31^<br>。</li>
</ul>
<p>大型语言模型在代码生成中的应用</p>
<ul>
<li><strong>Evaluating Large Language Models Trained on Code</strong>：Chen等人研究了LLMs在代码生成方面的表现，特别是在生成Python代码方面的能力<br>^4^<br>。</li>
<li><strong>Automated Repair of Programs from Large Language Models</strong>：Fan等人探讨了LLMs在自动修复程序中的应用，这与LLMs在约束建模中的自我验证和错误检测有相似之处<br>^7^<br>。</li>
</ul>
<p>大型语言模型的推理时计算方法</p>
<ul>
<li><strong>Scaling LLM Test-Time Compute</strong>：Snell等人研究了在测试时增加LLMs的计算量以提高性能的方法，这在论文中被应用于约束建模任务中<br>^32^<br>。</li>
<li><strong>Teaching Large Language Models to Self-Debug</strong>：Chen等人提出了让LLMs进行自我调试的方法，这与论文中探讨的自我验证方法有相似之处<br>^5^<br>。</li>
</ul>
<p>约束编程的其他相关工作</p>
<ul>
<li><strong>Global Constraint Catalogue</strong>：Beldiceanu等人提供了约束编程中全局约束的详细目录，这对于理解约束编程的多样性和复杂性非常重要<br>^1^<br>。</li>
<li><strong>Applications of Combinatorial Optimization</strong>：Paschos的著作提供了组合优化在多个领域的应用实例，强调了约束编程在实际问题中的重要性<br>^24^<br>。</li>
</ul>
<p>这些相关研究为本文提供了背景和基础，同时也指出了现有研究的局限性和改进方向。例如，现有数据集在多样性和复杂性上的不足，以及在不同约束建模框架下LLMs性能的差异等。本文通过引入CP-Bench数据集和系统地评估不同框架和方法，旨在推动这一领域的研究进展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下几个主要步骤来解决如何利用大型语言模型（LLMs）简化约束编程（CP）中的约束建模过程的问题：</p>
<p>1. 构建多样化的基准数据集 CP-Bench</p>
<ul>
<li><strong>数据集来源</strong>：论文从多个来源收集了101个多样化的组合问题，包括CSPLib、CPMpy在线示例库、Håkan Kjellerstrand的在线资源库以及一个基于课程的问题集<br>^21^<br>。</li>
<li><strong>数据集结构</strong>：每个数据集实例都被转化为一个自包含的Python文件，包含元数据、问题描述、输入数据（可选）、模型代码和解决方案打印代码<br>^5^<br>。</li>
<li><strong>评估方法</strong>：论文采用解决方案准确性（solution accuracy）作为评估指标，即通过执行生成的代码来验证其解决方案是否满足真实模型的约束和目标函数<br>^5^<br>。</li>
</ul>
<p>2. 评估不同约束建模框架对LLMs性能的影响</p>
<ul>
<li><strong>选择框架</strong>：论文选择了三个具有不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）<br>^1^<br>。</li>
<li><strong>系统提示（System Prompts）</strong>：为了指导LLMs更有效地进行建模，论文设计了三种不同详细程度的系统提示，从基本的建模任务描述到包含框架文档的详细提示<br>^4.1^<br>。</li>
<li><strong>实验结果</strong>：实验表明，使用Python基础框架（CPMpy和OR-Tools）生成模型的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
</ul>
<p>3. 探索提升LLMs生成有效约束模型能力的方法</p>
<ul>
<li><strong>检索增强型上下文学习（RAICL）</strong>：通过从预构建的数据库中检索与当前问题语义相似的输入-输出对来丰富提示上下文<br>^4.2^<br>。然而，实验结果表明，这种方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>重复采样（Repeated Sampling）</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型<br>^4.3^<br>。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证（Self-Verification）</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性<br>^4.4^<br>。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
</ul>
<p>4. 结合方法以进一步提高性能</p>
<ul>
<li><strong>综合方法</strong>：论文将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>通过上述步骤，论文不仅提供了一个多样化的基准数据集来评估LLMs在约束建模中的能力，还系统地比较了不同建模框架对LLMs性能的影响，并探索了多种方法来提高LLMs生成有效约束模型的能力。这些研究成果为降低约束编程的使用门槛、使其更易于被非专家用户接受和应用提供了有力的支持。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验来评估大型语言模型（LLMs）在约束建模中的表现：</p>
<p>实验一：不同建模框架下的LLMs性能评估</p>
<ul>
<li><strong>目的</strong>：比较LLMs在不同约束建模框架下的性能，以确定哪种框架更适合LLMs生成约束模型。</li>
<li><strong>方法</strong>：选择了三种不同的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。使用CP-Bench数据集中的101个组合问题，对8种不同的LLMs进行了评估<br>^6.1^<br>。</li>
<li><strong>结果</strong>：Python基础框架（CPMpy和OR-Tools）的性能优于领域特定语言（MiniZinc）。具体来说，CPMpy在详细系统提示（Level 3）下达到了最高的准确率<br>^6.2.1^<br>。这表明Python基础框架更适合LLMs生成约束模型。</li>
</ul>
<p>实验二：不同系统提示对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同详细程度的系统提示对LLMs性能的影响。</li>
<li><strong>方法</strong>：设计了三种不同详细程度的系统提示：基本提示（Level 1）、包含建模指南的提示（Level 2）和包含框架文档的详细提示（Level 3）。在三种不同的约束建模框架下，对8种LLMs进行了评估<br>^4.1^<br>。</li>
<li><strong>结果</strong>：提供更详细的系统提示（尤其是包含框架文档的提示）能够提高LLMs的建模准确性<br>^6.2.2^<br>。这表明详细的系统提示有助于LLMs更好地理解和生成约束模型。</li>
</ul>
<p>实验三：推理时计算方法对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同的推理时计算方法对LLMs性能的影响。</li>
<li><strong>方法</strong>：测试了四种不同的推理时计算方法：基线方法（直接评估LLMs生成的第一个模型）、检索增强型上下文学习（RAICL）、重复采样（Repeated Sampling）和自我验证（Self-Verification）。选择了4种LLMs（gpt-4.1-mini、DeepSeek-V3、QwQ-32B、Llama-3.3-70B）进行评估<br>^6.2.3^<br>。</li>
<li><strong>结果</strong>：</li>
<li><strong>RAICL</strong>：在上下文中添加示例对LLMs的性能没有帮助，反而降低了准确性<br>^6.2.3^<br>。</li>
<li><strong>重复采样</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
<li><strong>综合方法</strong>：将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>实验四：错误类型分析</p>
<ul>
<li><strong>目的</strong>：分析LLMs在生成约束模型时所犯错误的类型，以了解不同系统提示对错误的影响。</li>
<li><strong>方法</strong>：将错误分为两类：可检测错误（如代码无法执行、输出解析问题或超时）和建模错误（如生成的模型不可满足或产生错误的解决方案）。对不同系统提示配置下的错误进行了统计<br>^6.2.2^<br>。</li>
<li><strong>结果</strong>：随着系统提示详细程度的增加，可检测错误减少，而建模错误增加<br>^6.2.2^<br>。这表明更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误。</li>
</ul>
<p>这些实验综合评估了LLMs在约束建模中的性能，并探索了不同建模框架、系统提示和推理时计算方法对性能的影响。实验结果为如何利用LLMs简化约束编程的建模过程提供了重要的见解。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文在探索利用大型语言模型（LLMs）进行约束建模方面已经取得了显著的成果，但仍有一些可以进一步探索的点，以推动这一领域的研究进展。以下是一些潜在的研究方向：</p>
<p>1. <strong>多轮交互式建模</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在单次生成模型的准确性上，但在实际应用中，建模过程往往需要多次迭代和用户反馈。</li>
<li><strong>研究方向</strong>：探索多轮交互式建模，允许用户和系统之间进行多次交互，逐步改进和细化模型。这可以通过设计更复杂的对话系统来实现，系统可以根据用户的反馈动态调整模型<br>^7^<br>。</li>
</ul>
<p>2. <strong>工业级问题的应用</strong></p>
<ul>
<li><strong>背景</strong>：现有的基准数据集（如CP-Bench）虽然多样化，但主要来源于学术领域，与实际工业问题仍有差距。工业问题通常涉及更多的数据、更复杂的约束和更详细的描述。</li>
<li><strong>研究方向</strong>：与工业界合作，收集和构建更接近实际应用场景的组合问题数据集。这将有助于评估LLMs在处理更复杂、更实际问题时的表现<br>^24^<br>。</li>
</ul>
<p>3. <strong>模型效率优化</strong></p>
<ul>
<li><strong>背景</strong>：虽然论文主要关注模型的正确性，但在实际应用中，模型的求解效率同样重要。高效的模型可以显著减少求解时间和计算资源的消耗。</li>
<li><strong>研究方向</strong>：探索如何利用LLMs学习生成更高效的约束模型，例如通过引入启发式方法、动态约束调整或模型重构技术<br>^32^<br>。</li>
</ul>
<p>4. <strong>监督式微调</strong></p>
<ul>
<li><strong>背景</strong>：论文中提到，虽然CP-Bench数据集为评估LLMs提供了基础，但目前的数据集主要用于测试，而不是训练。</li>
<li><strong>研究方向</strong>：构建一个包含约束模型和问题描述的训练数据集，用于对LLMs进行监督式微调。这可能需要大量的标注数据，但有望进一步提高LLMs在约束建模任务中的性能<br>^4^<br>。</li>
</ul>
<p>5. <strong>跨领域知识迁移</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在训练过程中接触了大量的文本数据，这些数据可能包含不同领域的知识。然而，如何有效地将这些跨领域的知识迁移到约束建模任务中仍是一个未解决的问题。</li>
<li><strong>研究方向</strong>：研究如何利用LLMs的跨领域知识，通过迁移学习或领域适应技术，提高其在特定约束建模任务中的表现<br>^14^<br>。</li>
</ul>
<p>6. <strong>模型的可解释性</strong></p>
<ul>
<li><strong>背景</strong>：在实际应用中，除了模型的准确性和效率外，模型的可解释性也非常重要。用户需要理解模型是如何工作的，以及为什么会产生特定的解决方案。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的可解释性，例如通过生成详细的解释文本或可视化工具<br>^5^<br>。</li>
</ul>
<p>7. <strong>与其他AI技术的结合</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在约束建模中的应用可以与其他AI技术（如机器学习、强化学习等）相结合，以进一步提高建模和求解的效率和准确性。</li>
<li><strong>研究方向</strong>：研究如何将LLMs与这些技术相结合，例如通过强化学习来优化模型的生成过程，或者利用机器学习模型来预测模型的性能<br>^1^<br>。</li>
</ul>
<p>8. <strong>模型的泛化能力</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在特定的数据集和问题类型上，但实际应用中需要模型具有更强的泛化能力，能够处理各种不同类型的问题。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的泛化能力，例如通过引入更多的数据增强技术、设计更通用的建模框架或开发新的评估指标<br>^28^<br>。</li>
</ul>
<p>这些研究方向不仅有助于进一步提高LLMs在约束建模中的表现，还可能推动约束编程技术在更广泛领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了CP-Bench，这是一个用于评估大型语言模型（LLMs）在约束建模能力上的新基准数据集。文章的核心目标是解决约束编程（CP）中约束建模的瓶颈问题，通过利用LLMs将自然语言描述的组合问题转化为可执行的约束模型，从而降低CP的使用门槛，使其更易于被非专家用户接受和应用。以下是文章的主要内容概述：</p>
<p>背景知识</p>
<ul>
<li><strong>组合问题与约束编程</strong>：组合问题在众多实际应用中普遍存在，如物流、调度和网络设计等。CP作为一种解决问题的范式，通过声明式方法让用户指定解决方案必须满足的约束，而不是详细说明寻找解决方案的具体步骤。然而，将问题描述转化为正式的CP模型的过程复杂，需要深厚的应用领域知识和对建模框架的熟练掌握，这限制了CP技术的广泛应用<br>^1^<br>。</li>
<li><strong>大型语言模型的应用</strong>：近年来，LLMs在自然语言处理和代码生成方面取得了显著进展，为创建能够辅助或自动化建模过程的工具提供了可能。然而，将LLMs应用于约束建模面临着独特挑战，如声明式建模的复杂性、建模选择的组合性质以及验证生成模型正确性的困难<br>^4^<br>。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>CP-Bench数据集</strong>：为了解决现有评估数据集在多样性和复杂性上的不足，作者构建了CP-Bench，包含101个来自CP社区的多样化组合问题。这些问题涵盖了优化和满足问题，具有广泛的决策变量、约束和241种不同的约束类型<br>^5^<br>。</li>
<li><strong>建模框架的比较</strong>：文章比较了三种不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。通过这些框架，作者评估了LLMs在不同条件下的建模能力<br>^1^<br>。</li>
<li><strong>系统提示与推理时计算方法</strong>：为了提高LLMs生成有效约束模型的能力，文章系统地评估了基于提示和推理时计算的方法。这些方法包括不同详细程度的系统提示、检索增强型上下文学习（RAICL）、重复采样和自我验证<br>^4^<br>。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>实验设置</strong>：作者选择了8种不同的LLMs进行评估，这些模型在参数数量、上下文窗口大小、是否包含推理能力等方面存在差异<br>^6.1^<br>。</li>
<li><strong>实验结果</strong>：</li>
<li><strong>建模框架的影响</strong>：实验结果表明，使用Python基础框架（CPMpy和OR-Tools）的LLMs在生成约束模型时的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
<li><strong>系统提示的作用</strong>：更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误增加<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的效果</strong>：重复采样和自我验证方法显著提高了LLMs的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>Python基础框架的优势</strong>：Python基础框架（CPMpy和OR-Tools）更适合LLMs生成约束模型，这可能与LLMs训练过程中接触的大量Python代码有关<br>^6.2.1^<br>。</li>
<li><strong>系统提示的重要性</strong>：详细的系统提示，尤其是包含框架文档的提示，能够提高LLMs的建模准确性<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的有效性</strong>：重复采样和自我验证方法能够显著提高LLMs在约束建模任务中的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>LLMs在约束建模中的潜力与局限</strong>：尽管LLMs在约束建模中展现出了潜力，但仍然需要人类专家的验证，尤其是在关键任务中。文章中最强的性能配置（结合重复采样和自我验证的gpt-4.1-mini模型）达到了70%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>未来工作</p>
<p>文章提出了未来可能的研究方向，包括构建更接近实际应用场景的组合问题数据集、探索多轮交互式建模、研究模型效率优化、进行监督式微调、实现跨领域知识迁移、提高模型的可解释性、结合其他AI技术以及增强模型的泛化能力<br>^7^<br>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling，链接是https://arxiv.org/pdf/2506.06052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06052。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kostis Michailidis, Dimos Tsouros, Tias Guns</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06052v3.pdf">https://arxiv.org/pdf/2506.06052v3.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06052v3">https://papers.cool/arxiv/2506.06052v3</a></p>
<p><strong>Published</strong>: 2025-06-06T12:56:02Z</p>
<hr>
<h3 id="5-When-Flores-Bloomz-Wrong-Cross-Direction-Contamination-in-Machine-Translation-Evaluation"><a href="#5-When-Flores-Bloomz-Wrong-Cross-Direction-Contamination-in-Machine-Translation-Evaluation" class="headerlink" title="5. When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation"></a>5. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20858v1">When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation</a></h3><p>Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to “uncontaminated” languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz’s FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLM）可能受到基准测试污染，从而导致得分被抬高，使记忆能力被误认为是泛化能力。在多语言环境中，这种记忆甚至可能转移到“未污染”的语言中。使用 FLORES-200 翻译基准作为诊断工具，我们研究了两个 7-8B 参数、经过指令调优的多语言 LLM：Bloomz，它在 FLORES 上进行了训练，以及作为未污染对照的 Llama。我们确认了 Bloomz 的 FLORES 污染，并证明机器翻译的污染可能具有跨方向性，由于目标端的记忆，这可能人为地提升在未见翻译方向上的表现。进一步分析显示，尽管对源端进行改写和命名实体替换等各种扰动努力，记忆的参考内容通常仍然被回忆出来。然而，替换命名实体会导致 BLEU 值的一致下降，这表明这是一种有效的检测被污染模型记忆的方法。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文聚焦的核心问题是<strong>多语言机器翻译评测中的“跨方向数据污染”</strong>。具体而言，作者试图回答：</p>
<ul>
<li>当大模型在训练阶段见过某一目标语言的参考译文后，是否会在<strong>从未见过的源→目标翻译方向</strong>上，因<strong>目标侧记忆</strong>而获得虚高的评测分数？</li>
<li>这种“跨方向污染”能否被<strong>源侧扰动</strong>（回译、改写、实体替换）削弱或暴露？</li>
<li>如何设计<strong>简单可控的探测方法</strong>，在缺乏训练数据细节的黑盒场景下，判断模型是否已被污染？</li>
</ul>
<p>简言之，论文把 FLORES-200 这一多语种平行评测集当作“诊断工具”，揭示并量化<strong>目标侧记忆</strong>如何导致<strong>源语言未见的翻译方向</strong>也被人为拔高，进而提出<strong>实体替换</strong>作为轻量级探针来检测污染。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，可按主题归类为“数据污染与记忆化”“机器翻译中的记忆化与幻觉”“多语言/跨语言污染”三条主线。</p>
<p>数据污染与记忆化</p>
<ul>
<li><strong>Magar &amp; Schwartz (2022)</strong>：首次系统提出“数据污染”定义，从记忆到利用的连续谱。</li>
<li><strong>Carlini et al. (2022)</strong>：量化不同规模语言模型的记忆化程度，给出提取攻击方法。</li>
<li><strong>Elangovan et al. (2021)</strong>：揭示 NLP 评测中训练-测试重叠导致的虚高指标。</li>
<li><strong>Zhou et al. (2023)</strong>：提出“不要让 LLM 成为评测作弊者”，强调公开评测集被预训练数据覆盖的风险。</li>
<li><strong>Shi et al. (2024)</strong>：黑盒探测预训练数据是否包含特定测试样本。</li>
<li><strong>Oren et al. (2023)</strong>：在无法访问训练数据的情况下，用统计检验证明测试集泄漏。</li>
<li><strong>Jacovi et al. (2023)</strong>：给出防止评测数据以明文形式泄漏的实践策略。</li>
<li><strong>Jiang et al. (2024)</strong>：通过受控预训练实验，量化污染对下游任务的影响。</li>
<li><strong>Yang et al. (2023)</strong>：用改写样本重思基准可靠性，发现重述仍能被模型识别。</li>
</ul>
<p>机器翻译中的记忆化与幻觉</p>
<ul>
<li><strong>Raunak et al. (2021)</strong>：指出幻觉翻译往往源于训练段落的片段记忆。</li>
<li><strong>Raunak &amp; Menezes (2022)</strong>：提出“抽取式记忆”——模型仅见部分源文即可逐字复现整段目标文。</li>
<li><strong>Raunak et al. (2022)</strong>：发现稀有模式被记忆后，导致长尾翻译错误。</li>
<li><strong>Guerreiro et al. (2023)</strong>：对多语言场景下的幻觉进行系统分类，证实记忆失败会跨语言放大。</li>
<li><strong>Kocyigit et al. (2025)</strong>：大规模受控实验显示，即使少量评测句进入预训练，也足以显著抬升 BLEU。</li>
</ul>
<p>多语言/跨语言污染</p>
<ul>
<li><strong>Yao et al. (2024)</strong>：首次证明污染可以“跨语言屏障”——模型在语言 A 见过评测，导致在语言 B 的同一评测上也表现虚高。</li>
<li><strong>Chowdhury et al. (2020; 2021)</strong>：利用嵌入空间同构检测翻译伪影，间接说明多语言平行语料易造成交叉污染。</li>
</ul>
<p>上述工作共同构成论文的学术背景，但<strong>尚未有人专门探讨“多语言平行评测集里，目标侧记忆对未见源→目标方向的跨方向污染”</strong>，这正是本文填补的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“诊断-验证-探测”三步法，系统暴露并量化跨方向污染，同时给出可落地的探针工具。</p>
<ol>
<li>诊断：设计并行实验</li>
</ol>
<ul>
<li>选用<strong>已确认</strong>在指令微调阶段用过 FLORES 的 Bloomz-7B1 作为“污染组”，<strong>未用</strong> FLORES 的 Llama-3.1-8B-Instruct 作为“干净对照组”。</li>
<li>对 15 种语言的全部 210 个源→目标方向计算 BLEU（表面记忆指标）与 COMET（语义相似度），用“高 BLEU + 高 COMET”区块定位可疑污染方向。</li>
</ul>
<ol>
<li>验证：排除“模型只是更强”的替代解释</li>
</ol>
<ul>
<li>在<strong>外部低资源印地语数据集</strong>（PMIndia、Mann-ki-Baat）上测试 Bloomz；结果 BLEU≈0，证明其并无真实翻译能力，高分仅源于对 FLORES 参考译文的记忆。</li>
<li>通过回译实验把源语替换为<strong>第三语言生成的平行句</strong>（xxxzzz→tam），仍观察到 45–80 BLEU，说明<strong>源语是否“原样”出现并非召回记忆的必要条件</strong>。</li>
</ul>
<ol>
<li>探测：提出轻量级扰动探针</li>
</ol>
<ul>
<li><strong>实体替换</strong>：用 spaCy 抽取 Named Entity，再让 Aya 生成同类型全新实体并回填到源句。</li>
<li>结果：Bloomz 在“全实体替换”后 BLEU 普遍再降 10–20 分，Llama 仅降约 5–10 分；<strong>降幅差异</strong>成为判断模型是否记忆目标的显著信号。</li>
<li>给出可复现的<strong>三步探针脚本</strong>：</li>
</ul>
<ol>
<li>在待测方向上做“实体保留”与“实体替换”两次翻译；</li>
<li>计算 ΔBLEU = BLEU_base − BLEU_replaced；</li>
<li>若 ΔBLEU 显著大于对照模型，即可怀疑该方向存在目标侧记忆污染。</li>
</ol>
<p>通过上述流程，论文不仅<strong>证实跨方向污染真实存在</strong>，还提供了一套<strong>无需训练数据即可在黑盒场景下快速筛查污染</strong>的实用工具。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计并执行了 4 组核心实验，层层递进地验证“跨方向污染”假设并提炼探针方法。所有实验均在单张 A100 80 GB 上完成，总计约 30 GPU 小时。</p>
<p>1. 大范围筛查实验</p>
<p><strong>目的</strong>：定位哪些语言方向存在“高 BLEU + 高 COMET”可疑区块。</p>
<ul>
<li><strong>数据</strong>：FLORES-200 dev 集 997 句 × 15 种语言 → 210 个源→目标方向。</li>
<li><strong>模型</strong>：Bloomz-7B1（污染组） vs. Llama-3.1-8B-Instruct（对照组）。</li>
<li><strong>指标</strong>：sacreBLEU、COMET-22。</li>
<li><strong>结果</strong>：</li>
<li>Bloomz 在 xxx→{eng,fra,mal,ory,por,tam,vie,zho} 出现 40–90 BLEU，COMET&gt;0.8；</li>
<li>Llama 仅在高资源方向出现 30–40 BLEU，且 COMET 与 BLEU 同步偏低。</li>
</ul>
<p>2. 外部集验证实验</p>
<p><strong>目的</strong>：排除“模型只是翻译能力强”的替代解释。</p>
<ul>
<li><strong>数据</strong>：PMIndia、Mann-ki-Baat 低资源印地语平行句（eng→{mal,ory,tam}）。</li>
<li><strong>模型</strong>：Bloomz。</li>
<li><strong>指标</strong>：BLEU。</li>
<li><strong>结果</strong>：BLEU≈0，证实 Bloomz 对非 FLORES 句几乎无翻译能力，高分仅来自记忆。</li>
</ul>
<p>3. 源侧扰动实验</p>
<p>3a 回译扰动</p>
<ul>
<li><strong>流程</strong>：用 Llama 先将 zzz→xxx（zzz∈{asm,bam,por}）生成回译句 xxxzzz，保持与原始目标平行；再让 Bloomz 翻译 xxxzzz→tam。</li>
<li><strong>结果</strong>：</li>
<li>回译质量 20–50 BLEU 时，Bloomz 仍输出 60+ BLEU；</li>
<li>回译质量≈0 时，部分方向仍达 20–60 BLEU，说明<strong>源句是否“原样”并非召回必要条件</strong>。</li>
</ul>
<p>3b 改写扰动</p>
<ul>
<li><strong>流程</strong>：用 Llama 对同一源句做意译（xxxpp），与回译句对比。</li>
<li><strong>结果</strong>：回译句 BLEU 比改写句再高 20–30 分，进一步证明<strong>表面相似度越高越易触发记忆</strong>，但即使低相似度也无法彻底消除召回。</li>
</ul>
<p>4. 实体替换探针实验</p>
<p><strong>目的</strong>：找到可在黑盒场景下快速检测记忆的低成本信号。</p>
<ul>
<li><strong>流程</strong>：</li>
</ul>
<ol>
<li>用 spaCy 抽取英语句中全部实体→Aya 生成同类型全新实体→回填到源句并翻译为目标语；</li>
<li>设置“单实体替换”“全实体替换”两档，对比原始句结果。</li>
</ol>
<ul>
<li><strong>模型</strong>：Bloomz &amp; Llama，六组高资源方向 eng/fra/por/spa/vie/zho→tam。</li>
<li><strong>指标</strong>：ΔBLEU = BLEU_base − BLEU_replaced。</li>
<li><strong>结果</strong>：</li>
<li>Bloomz 全实体替换平均降 15.3 BLEU，峰值 20+；</li>
<li>Llama 仅降 7.2 BLEU；</li>
<li><strong>ΔBLEU 差异显著</strong>，可用作污染探测信号。</li>
</ul>
<p>5. 受控微调复现实验（bonus）</p>
<p><strong>目的</strong>：验证“仅通过 eng↔xxx 双向数据即可产生跨方向污染”这一机制假设。</p>
<ul>
<li><strong>流程</strong>：用 Axolotl 在 FLORES eng↔{11 种语言} 上继续微调 Llama-3.1-8B，直至严重过拟合。</li>
<li><strong>测试</strong>：微调后模型对<strong>未见方向</strong> zzz→yyy 进行推理。</li>
<li><strong>结果</strong>：</li>
<li>BLEU 在大多数未见方向提升 8–16 分；</li>
<li>COMET 反而下降，说明极端记忆拉高 BLEU 却牺牲语义；</li>
<li>再次印证<strong>目标侧记忆是跨方向污染主因</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为论文结论的自然延伸，分为“机制深挖”“探针升级”“场景拓展”三大类，均围绕“跨方向污染”这一核心现象展开。</p>
<p>1. 机制深挖</p>
<ul>
<li><strong>激活追踪</strong><br>用梯度<em>探针</em>或隐状态探针定位“实体→记忆目标”触发路径，验证实体替换降低 BLEU 是否因抑制了特定注意力头或 FFN 关键神经元。</li>
<li><strong>语言层级差异</strong><br>将实验扩展到<strong>屈折度、书写系统、语序</strong>差异更大的语言对，观察跨方向污染是否仍仅由目标侧主导，抑或某些源语结构会放大记忆召回。</li>
<li><strong>训练阶段对比</strong><br>分别用<em>预训练</em>、<em>指令微调</em>、<em>持续预训练</em>三阶段引入 FLORES，量化哪一阶段对跨方向污染贡献最大，从而指导数据清洗时机。</li>
</ul>
<p>2. 探针升级</p>
<ul>
<li><strong>语义级探针</strong><br>当前仅用 BLEU 差值。可引入<em>实体感知 COMET</em> 或<em>实体替换后的 BERTScore 差值</em>，排除“替换后句子本身更难翻译”带来的混淆。</li>
<li><strong>多模态探针</strong><br>若评测集含<strong>图像-文本对</strong>（如 Multi30K），可同步替换图像中的 OCR 文字或视觉实体，检验跨模态记忆是否也存在“目标侧污染”。</li>
<li><strong>自适应阈值</strong><br>建立<em>模型规模-语言资源度-ΔBLEU</em>三维映射，给出“污染临界阈值”在线查询表，使探针无需干净对照模型即可给出统计显著性。</li>
</ul>
<p>3. 场景拓展</p>
<ul>
<li><strong>生成任务</strong><br>将探针迁移到<strong>多语言摘要、问答、对话</strong>任务，验证“目标侧记忆”是否同样导致跨语言虚高。</li>
<li><strong>多引擎评测</strong><br>对同一评测集同时使用<em>NMT、LLM、云 API</em>三类系统，看商业引擎是否也呈现一致的实体替换 ΔBLEU 模式，为第三方评测提供审计工具。</li>
<li><strong>动态去污染</strong><br>基于实体替换思想，构建<strong>对抗性数据增强</strong>流程：在训练侧实时替换实体并保留标签，观察能否在<em>不降下游性能</em>的前提下抑制记忆化。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文主旨</strong><br>用 FLORES-200 机器翻译基准做“诊断探针”，系统揭示并量化<strong>大模型在多语言评测中的跨方向数据污染</strong>：模型只要见过“某目标语言”的参考译文，就能在<strong>从未见过的源→该目标方向</strong>上凭记忆输出高 BLEU，导致虚高评分。</p>
<p><strong>核心发现</strong></p>
<ol>
<li><strong>污染可跨方向</strong>：Bloomz 因微调阶段摄入 FLORES，在 210 个方向中大量出现“80+ BLEU &amp; 0.9+ COMET”区块；对照模型 Llama 无此现象。</li>
<li><strong>目标侧记忆为主</strong>：回译、改写等源侧扰动仍常触发 45–80 BLEU；仅当<strong>替换源句全部命名实体</strong>时，BLEU 才系统性下降 10–20 分，且降幅显著大于干净模型，可用作<strong>黑盒探针</strong>。</li>
<li><strong>机制复现</strong>：用 Llama 仅在 eng↔xxx 双向数据上继续微调，即可让<strong>未见方向</strong>zzz→yyy 的 BLEU 提升 8–16 分，证实跨方向污染可由<strong>纯目标侧记忆</strong>产生。</li>
</ol>
<p><strong>方法论贡献</strong></p>
<ul>
<li>提出<strong>实体替换 ΔBLEU 探针</strong>：无需训练数据，仅比较“原始句”与“实体替换句”的 BLEU 差值，即可快速筛查模型是否记忆了评测参考。</li>
<li>给出<strong>15 语言、210 方向</strong>的 BLEU/COMET 全景热图，为后续研究提供基准对照。</li>
</ul>
<p><strong>实践建议</strong><br>评测多语言模型时，务必：</p>
<ol>
<li>检查<strong>所有翻译方向</strong>而非仅报告→英语；</li>
<li>运行<strong>实体替换探针</strong>，ΔBLEU 异常高即视为污染可疑；</li>
<li>对可疑方向改用<strong>外部独立数据集</strong>复测，避免将记忆当泛化。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是When%20Flores%20Bloomz%20Wrong%3A%20Cross-Direction%20Contamination%20in%20Machine%20Translation%20Evaluation，链接是https://arxiv.org/pdf/2601.20858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20858。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: David Tan, Pinzhen Chen, Josef van Genabith, Koel Dutta Chowdhury</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20858v1.pdf">https://arxiv.org/pdf/2601.20858v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20858v1">https://papers.cool/arxiv/2601.20858v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:21Z</p>
<hr>
<h3 id="6-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models"><a href="#6-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models" class="headerlink" title="6. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models"></a>6. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20856v1">SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</a></h3><p>Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型的能力在复杂推理任务上得到了越来越多的测试，但它们的长远规划能力尚未得到广泛研究。在本工作中，我们对最先进的大型推理模型（LRMs）的规划和长远推理能力进行了系统评估。我们提出了一个基于推箱子（Sokoban）谜题的新型基准测试，该测试经过刻意简化，以将长远规划与状态持久性隔离开来。我们的研究结果显示，当需要超过25步才能达到解决方案时，规划性能会显著下降，这表明前向规划能力存在根本性限制。我们还表明，为LRMs配备规划领域定义语言（PDDL）的解析、验证和求解工具可以带来适度的改进，这表明其架构本身存在限制，可能仅靠测试时的规模扩展无法克服。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统评估当前大型推理模型（Large Reasoning Models, LRMs）在**长程规划（long-horizon planning）<strong>与</strong>长程推理（long-horizon reasoning）**上的能力边界。核心问题可以概括为：</p>
<blockquote>
<p><strong>当解空间深度增加、但分支因子被刻意压到最低时，LRMs 是否仍能维持连贯、可行且最优的动作序列？</strong></p>
</blockquote>
<p>为隔离“深度”与“复杂性”因素，作者构造了极简的一维 Sokoban 走廊谜题：</p>
<ul>
<li>仅 1 个箱子、1 个目标、1 条直线通道；</li>
<li>唯一可变参数是走廊长度 ℓ，它直接决定最优解长度（≈ℓ 步）；</li>
<li>无外部记忆、无中间反馈、无状态验证，完全依赖模型内部表示。</li>
</ul>
<p>通过该基准，论文试图回答以下子问题：</p>
<ol>
<li>模型在无需外部工具时，究竟能稳定规划到多远的 horizon？</li>
<li>当 horizon 超过某一阈值（≈25–30 步）时，失效模式是“搜索错误”还是“更底层的计数/状态跟踪错误”？</li>
<li>若给模型提供 PDDL 解析器与外部求解器（LLM-Modulo 框架），能否弥补其固有缺陷？</li>
<li>上述改进若存在，是否足以克服长程动作表示与空间 grounding 的瓶颈？</li>
</ol>
<p>综上，论文并非追求更复杂的规划域，而是<strong>用极简环境放大“深度”维度</strong>，从而暴露 LRMs 在长序列状态一致性上的结构性局限，并验证“测试时扩展+符号工具”是否足以突破该局限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四大脉络，均围绕“LLM 规划能力”与“Sokoban 作为诊断环境”展开：</p>
<ol>
<li><strong>通用规划基准与 LLM 失效诊断</strong></li>
</ol>
<ul>
<li><strong>PlanBench</strong>（Valmeekam et al., 2022, 2023a, 2025）：PDDL 格式的 BlocksWorld、Logistics 等经典域，发现 LLM 随 horizon 增加迅速退化，且错误集中在无效动作与循环。</li>
<li><strong>8-puzzle 文本版</strong>（Schepanowski &amp; Ling, 2025）：证明即使状态空间仅 181440，LLM 仍因“弱内部状态表示”与“误导式启发搜索”而频繁产生非法移动或提前终止。</li>
<li><strong>字符级计数任务</strong>（Karpathy, 2024；Xu &amp; Ma, 2025；CharBench, Uzan &amp; Pinter, 2025）：揭示“strawberry 有几个 r”类失败源于 tokenization 之外的长度-计数表征缺陷，与本文“走廊计数漂移”论点直接呼应。</li>
</ul>
<ol>
<li><strong>LLM-Modulo / 工具增强规划</strong></li>
</ol>
<ul>
<li><strong>LLM⇄PDDL 管道</strong>（Silver et al., 2022；Oswald et al., 2024；La Malfa et al., 2025）：让模型仅生成 domain 或 problem 文件，由外部求解器完成搜索，显著降低非法动作率。</li>
<li><strong>PlanBench-o1 实验</strong>（Valmeekam et al., 2025）：o1-preview 裸机成功率 10–12%，接入 Fast-Downward 后提升至 43%，但计算成本增加 10× 以上；本文 LLM-Modulo 结果与之趋势一致，进一步指出“即使符号求解，空间量纲错误仍存”。</li>
</ul>
<ol>
<li><strong>Sokoban 作为认知诊断环境</strong></li>
</ol>
<ul>
<li><strong>IPC-2023 Sokoban track</strong>（Taitler et al., 2024）：正式将 Sokoban 纳入国际规划竞赛，强调其“无捷径、无模式化解”特性。</li>
<li><strong>RNN 专用求解器</strong>（Taufeeque et al., 2024；Jolicoeur-Martineau, 2025）：在 1000+ 关卡达到 &gt;95% 成功率，表明非 Transformer 架构可通过循环隐状态掌握空间规则，反衬 LLM 的隐状态短板。</li>
<li><strong>o1-preview 裸机实验</strong>（Valmeekam et al., 2025）：在标准 2D Sokoban 仅 10–12% 成功率，与本文“一维走廊”结果形成对照，说明“简化空间并未显著缓解长程退化”。</li>
</ul>
<ol>
<li><strong>长程推理与“漫游”现象的理论解释</strong></li>
</ol>
<ul>
<li><strong>“Wandering vs. Systematic Exploration”框架</strong>（Lu et al., 2025）：提出 LLM 推理轨迹本质是“无 visited-state 集的随机游走”，在深度线性链上表现为重复循环与计数漂移，与本文 token 上限耗尽、前缀准确率指数下降现象完全一致。</li>
<li><strong>工作记忆容量度量</strong>（Zhang et al., 2024）：用 dual-task 方式证明 LLM 的“有效工作记忆”不超过 7–9 个 token 级状态，与本文 25 步左右断崖式失效点数量级一致。</li>
</ul>
<p>综上，既有研究共同指向：<strong>LLM 的规划瓶颈并非高层搜索策略，而是底层状态计数、空间 grounding 与工作记忆持久化的结构性缺陷</strong>。本文通过“极简走廊”设计，将上述观察从复杂域剥离出来，首次给出可量化、可复现的长程失效阈值。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一种“治愈”长程规划缺陷的新架构，而是设计了一套<strong>极简诊断协议</strong>，通过“控制变量→量化失效→工具增强→对比分析”四步，把问题拆成可测量的现象，从而给出改进方向与上限证据。</p>
<ol>
<li><strong>控制变量：构造 SokoBench 走廊谜题</strong></li>
</ol>
<ul>
<li>仅留“长度 ℓ”一个自由参数，最优解长度 ≈ℓ，分支因子=1。</li>
<li>旋转增广（0°/90°/180°/270°）降低预训练泄露风险。</li>
<li>由此将“长程”与“空间复杂性”解耦，任何失效都可归因于<strong>深度+内部状态跟踪</strong>。</li>
</ul>
<ol>
<li><strong>量化失效：1-shot 裸机实验</strong></li>
</ol>
<ul>
<li>指标：Exact-match Accuracy、Prefix Accuracy、Manhattan Distance、token 消耗曲线。</li>
<li>发现：<br>– 25 ≤ ℓ ≤ 30 是断崖阈值，之后 Accuracy 指数下降。<br>– 错误主因是“计数漂移”与“循环输出”，与 Lu et al. 的“wandering”假说一致。<br>– Token 消耗随 ℓ 线性增加，但正确解的方差远小于错误解，说明失败路径发散。</li>
</ul>
<ol>
<li><strong>工具增强：LLM-Modulo 管道</strong></li>
</ol>
<ul>
<li>给模型只留两项任务：<br>① 把 ASCII 图翻译成 PDDL problem 文件；<br>② 调用外部 Unified-Planning 求解器（Fast-Downward / PyperPlan）。</li>
<li>结果：<br>– GPT-5-mini 裸机 25 步后≈0，工具增强后 100 步仍维持 30–40% Accuracy。<br>– 但垂直走廊显著弱于水平走廊，说明“空间量纲估计”错误未被完全消除。<br>– 7/320 次出现 PDDL 语法错，其余失败均因“problem 文件与真实地图尺寸不符”，即<strong>内部计数偏差外溢到符号描述</strong>。</li>
</ul>
<ol>
<li><strong>对比分析与上限证据</strong></li>
</ol>
<ul>
<li>工具增强仅带来<strong>边际提升</strong>，未能突破 25–30 步的“软上限”，证明缺陷<strong>架构层</strong>而非<strong>搜索层</strong>。</li>
<li>给出明确结论：测试时扩展 + 符号求解器<strong>不足以</strong>克服缺乏“显式状态-转移记忆”的硬伤；未来需引入<br>– 显式 visited-state 缓存，<br>– 可学习的计数/坐标专用模块，<br>– 或直接在隐状态内加入符号接地机制。</li>
</ul>
<p>通过上述四步，论文把“长程规划难”从经验观察转化为<strong>可复现指标</strong>，并用量化的方式说明：<strong>在深度线性链场景下，当前 LRM 的失效主因是内部状态漂移，而非高层搜索策略；外部符号工具只能缓解，不能根除。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计 <strong>两套主实验</strong> 与 <strong>一套旋转对照分析</strong>，全部在公开数据集 SokoBench（ℓ=5–100，步长 5，4 种旋转，共 80 张地图）上完成。实验变量、模型、重复次数与核心指标如下：</p>
<p>实验 1 1-shot 裸机推理（无外部工具）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型</td>
<td>DeepSeek-R1-0528、GPT-5-mini、GPT-oss-120B</td>
</tr>
<tr>
<td>Prompt</td>
<td>系统提示 + 表 1 符号映射 + 1 条 4 步示范解</td>
</tr>
<tr>
<td>重复</td>
<td>每 ℓ×旋转 8 次（nt=8，nr=4，总 640 次/模型）</td>
</tr>
<tr>
<td>输出限制</td>
<td>max_tokens=32768，temperature 默认</td>
</tr>
<tr>
<td>记录量</td>
<td>完整推理 trace、最终 字符串、token 消耗</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong></p>
<ol>
<li>Accuracy(ℓ) — Exact string 匹配率</li>
<li>PrefixAccuracy(ℓ) — 逐字符前缀正确率</li>
<li>Manhattan-Distance(ℓ) — 终端坐标与目标 L1 距离</li>
<li>Token 回归 — 正确/错误解的斜率与 R²</li>
</ol>
<p><strong>关键发现</strong></p>
<ul>
<li>ℓ≈25 处出现指数级断崖；</li>
<li>错误解 token 方差远大于正确解，且常因循环输出顶到上限；</li>
<li>旋转 90°/270°（垂直走廊）普遍更低分，揭示“行计数”难度 &gt;“列计数”。</li>
</ul>
<p>实验 2 LLM-Modulo 工具增强</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>可用模型</td>
<td>仅 GPT-5-mini（成本与工具调用稳定性权衡）</td>
</tr>
<tr>
<td>工具链</td>
<td>自定义 MCP 服务器 → PyperPlan / Fast-Downward</td>
</tr>
<tr>
<td>模型任务</td>
<td>生成 PDDL problem 文件 → 调用 solve_problem → 返回 LURD 解</td>
</tr>
<tr>
<td>重复</td>
<td>nt=4，nr=4，总 320 次</td>
</tr>
<tr>
<td>失败处理</td>
<td>语法或语义错误自动重试，最多 3 轮</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong><br>同上 3 项，但额外记录：</p>
<ul>
<li>PDDL 语法错误率</li>
<li>语义不一致率（problem 与真实地图尺寸不符）</li>
<li>工具调用总 token 数（含推理+PDDL 文本）</li>
</ul>
<p><strong>关键发现</strong></p>
<ul>
<li>100 步处 Accuracy 仍≈0.35，较裸机提升 3×，但远未收敛到 1；</li>
<li>7/320 次语法错；其余失败均因“地图长度数错”导致求解器给出短计划；</li>
<li>垂直走廊性能仍显著低于水平走廊，说明<strong>空间量纲估计偏差未被工具消除</strong>。</li>
</ul>
<p>实验 3 旋转细粒度对照（附录 C）</p>
<ul>
<li>仅展示 GPT-5-mini LLM-Modulo 结果；</li>
<li>将 80 张地图按 4 个旋转角度拆分为 4 条子曲线；</li>
<li>量化同一 ℓ 下 0°/90°/180°/270° 的 Accuracy 差异，证实“垂直 vs 水平”效应稳定存在。</li>
</ul>
<p>补充微观分析</p>
<ul>
<li><strong>Token 线性回归</strong>：正确解斜率 29.8–51.1 token/ℓ，错误解斜率 55.2–86.3 token/ℓ，验证“失败轨迹更长且更发散”。</li>
<li><strong>循环检测</strong>：对顶到 32768 上限的 trace 进行子串重复计数，平均 42% 内容属于重复 L/R 语句，直接支持“wandering”假说。</li>
<li><strong>曼哈顿距离热图</strong>：裸机解常出现“玩家最终落在目标上”的非法状态，进一步佐证内部状态未遵守墙/箱约束。</li>
</ul>
<p>综上，实验由<strong>裸机诊断→工具增强→旋转消融→微观痕迹分析</strong>四层组成，共同量化出 LRM 长程规划失效的阈值、特征与改进上限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SokoBench 的“直接续作”或“横向扩展”，均围绕<strong>把当前暴露的单一缺陷拆成多维、可干预变量</strong>展开，以便下一步真正突破 25–30 步的“软上限”。</p>
<p>1 状态表示缺陷的显式干预</p>
<ul>
<li><strong>可学习坐标嵌入</strong>：在 Transformer 隐状态外，显式维护一个  (x,y)  浮游向量，每一步自回归后通过对比损失与符号坐标对齐，检验“外置坐标”能否阻断计数漂移。</li>
<li><strong>Visited-State Cache</strong>：用外部键-值缓存记录已生成状态哈希，采样时禁止重复输出，测试“循环”是否纯粹由无记忆导致。</li>
<li><strong>Delta 预测 vs 绝对预测</strong>：让模型不再预测绝对位置，而是预测“相对位移 + 边界检查”，降低长链绝对坐标累积误差。</li>
</ul>
<p>2 复杂度维度的渐进叠加</p>
<ul>
<li><strong>多箱走廊</strong>：固定 ℓ=50，逐步增加箱子数  b=1…5 ，引入<strong>死锁检测</strong>需求，观察错误模式从“计数漂移”转向“冲突/死锁”的临界  b 。</li>
<li><strong>单侧/双侧分支</strong>：在走廊中段加入  k=1,2  条“死胡同”侧枝，保持解长度不变但增加<strong>搜索宽度</strong>，量化“分支因子”与“深度”对失效的相对贡献。</li>
<li><strong>动态障碍</strong>：在第  t  步随机出现/消失一堵墙，迫使模型<strong>重规划</strong>，测试工作记忆刷新与快速回溯能力。</li>
</ul>
<p>3 感知-动作接口的消融</p>
<ul>
<li><strong>多种编码</strong>：</li>
<li>行号-列号坐标 <code>(r3,c5)</code></li>
<li>相对距离 <code>(Δx,Δy)</code></li>
<li>图像化 2-D 矩阵（利用 VLRM）<br>对比同一地图在不同编码下的 Accuracy 曲线，找出<strong>最敏感格式</strong>，为后续“最佳接地”提供数据。</li>
<li><strong>稀疏 vs 密集奖励提示</strong>：在推理链中每隔  k  步插入人工“状态校验”句子（“此时箱子应在 x=7”），测量<strong>外部提醒</strong>能把阈值 ℓ 提高多少。</li>
</ul>
<p>4 训练与推理策略</p>
<ul>
<li><strong>课程强化微调</strong>：用增量 ℓ 的课程数据对同一基础模型做轻量级 LoRA 微调，验证“见过更长序列”能否把断崖后移，或仅提升记忆。</li>
<li><strong>Best-of-N + 状态验证器</strong>：在推理阶段采样 N 条计划，用外部 Sokoban 模拟器过滤非法解，再选最短合法解；测量 N=1…100 时的收益曲线，给出“纯采样”上限。</li>
<li><strong>递归子目标分解</strong>：强制模型先在高层输出“分阶段里程碑”（如“先把箱推到 x=30”），再对每个子目标展开低层动作；对比端到端方式，检验“显式分层”能否降低有效深度。</li>
</ul>
<p>5 神经-符号混合上限</p>
<ul>
<li><strong>可微规划器端到端训练</strong>：把 Fast-Downward 的编码层换成可微 GNN，生成计划后通过策略梯度微调，看“符号求解器可微化”能否让模型学会<strong>修正 PDDL 参数错误</strong>。</li>
<li><strong>逆向数据增强</strong>：用最优求解器生成百万级走廊解，再反向生成“错误 PDDL 问题”(坐标偏移 ±1)，训练模型<strong>检测并自纠错</strong>，量化“语法+语义”双重验证的剩余错误率。</li>
</ul>
<p>6 横向迁移与外部效度</p>
<ul>
<li><strong>跨域深度链</strong>：保持“单参数难度”理念，换用 Tower of Hanoi、Sliding-Tile、Monkey-and-Bananas 的一维变体，检验“25–30 步阈值”是否<strong>与域无关</strong>，从而验证其为<strong>通用工作记忆极限</strong>。</li>
<li><strong>真实机器人走廊任务</strong>：把 ASCII 走廊映射到实体 1-D 轨道（机器人推箱），观察语言模型生成的计划在高延迟、低精度执行环境下的<strong>鲁棒性差距</strong>，引入“物理漂移”新变量。</li>
</ul>
<p>7 社会-技术风险</p>
<ul>
<li><strong>预训练数据溯源</strong>：对垂直 vs 水平走廊差异做 n-gram 重叠分析，若水平模板在 CommonCrawl 出现频率显著更高，可解释“方向偏差”部分源于<strong>数据不平衡</strong>而非空间认知。</li>
<li><strong>能力-成本权衡公开仪表盘</strong>：建立在线基准，实时记录不同模型在 ℓ=50 走廊的“成功率 / 美元 / 克 CO₂”三元组，为社区提供<strong>绿色 AI</strong> 视角下的规划能力评价。</li>
</ul>
<p>以上方向遵循“单变量介入-可量化-可复现”原则，既可单独成文，也可组合成新的<strong>多维度 SokoBench v2</strong>，目标是<strong>把“长程规划”从黑盒艺术变成白盒实验科学</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可浓缩为“一个基准、两条曲线、三点证据、四步结论”：</p>
<p>一个基准</p>
<ul>
<li><strong>SokoBench</strong>：极简一维 Sokoban 走廊，仅“长度 ℓ”可变，最优解 ≈ℓ 步；</li>
<li>80 张地图（ℓ=5–100，步长 5，4 旋转），完全公开，用于隔离“深度”与“分支/空间复杂性”。</li>
</ul>
<p>两条性能曲线</p>
<ol>
<li><strong>裸机 LRM</strong>：</li>
</ol>
<ul>
<li>25–30 步处 Accuracy 指数断崖；</li>
<li>token 消耗随 ℓ 线性增加，但错误解方差远大于正确解。</li>
</ul>
<ol>
<li><strong>LLM-Modulo（+PDDL 求解器）</strong>：</li>
</ol>
<ul>
<li>相同阈值仍在，100 步时 Accuracy≈0.35，提升有限；</li>
<li>失败主因：模型生成的 PDDL 问题“长度数错”，外置求解器无法弥补。</li>
</ul>
<p>三点证据</p>
<ol>
<li><strong>计数漂移是主因</strong>：Prefix Accuracy 与 Manhattan Distance 同步下降，循环输出占 token 上限 42%。</li>
<li><strong>状态记忆缺失</strong>：模型无 visited-state 概念，反复 proposing 同一动作，体现“wandering”而非系统搜索。</li>
<li><strong>空间接地缺陷</strong>：垂直走廊显著弱于水平走廊，即使用符号求解器亦未消除，说明“坐标量纲”估计错误根深蒂固。</li>
</ol>
<p>四步结论</p>
<ol>
<li>当前 LRM 的长程规划瓶颈主要在<strong>底层状态跟踪与计数表征</strong>，而非高层搜索策略。</li>
<li>25–30 步是<strong>架构性软上限</strong>，测试时扩展或外接符号求解器<strong>仅能边际改善</strong>，无法突破。</li>
<li>未来需引入<strong>显式坐标记忆、可微符号接地或分层子目标验证</strong>等结构性改进，否则单纯堆算力无益。</li>
<li>SokoBench 以极简环境量化深度失效，为社区提供了<strong>可复现、可干预</strong>的长程规划诊断平台。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2601.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20856。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20856v1.pdf">https://arxiv.org/pdf/2601.20856v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20856v1">https://papers.cool/arxiv/2601.20856v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:00Z</p>
<hr>
<h3 id="7-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images"><a href="#7-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images" class="headerlink" title="7. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images"></a>7. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17934v2">From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</a></h3><p>Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM’s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>像 Segment Anything Model (SAM) 这样的基础模型展示了强大的泛化能力，但由于领域偏移、标签稀缺以及参数高效微调（PEFT）无法利用未标注数据，将它们应用于医学影像仍然十分困难。尽管像 U-Net 这样的传统模型在半监督医学学习中表现出色，它们在辅助 PEFT SAM 方面的潜力却在很大程度上被忽视。我们提出了 SC-SAM，这是一个专家-通用框架，其中 U-Net 提供基于点的提示和伪标签来引导 SAM 的适应，而 SAM 则作为强大的通用监督器来规范 U-Net。这种相互引导形成了双向协同训练循环，使两个模型都能有效利用未标注数据。在前列腺 MRI 和息肉分割基准实验中，我们的方法达到了最先进的结果，优于其他现有的半监督 SAM 变体，甚至超过了像 MedSAM 这样的医学基础模型，突显了专家-通用合作在标签高效医学影像分割中的价值。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对医学图像分割中“基础模型难以利用无标注数据”这一核心矛盾展开。具体而言：</p>
<ul>
<li><p><strong>SAM 在医学域的瓶颈</strong><br>尽管 Segment Anything Model（SAM）具备强大的零样本泛化能力，但面对医学影像时存在显著域偏移；同时，参数高效微调（PEFT）仍依赖少量标注，无法直接从无标注样本学习，导致大量无标签数据被闲置。</p>
</li>
<li><p><strong>传统半监督方案与 SAM 的割裂</strong><br>基于 U-Net 的半监督框架（一致性正则、伪标签等）已证明无标注数据的价值，却与 SAM 的 PEFT 流程脱节；现有尝试或仅把 SAM 当教师，或引入复杂双解码结构，仍未让 SAM 真正“吸收”无标注信息。</p>
</li>
<li><p><strong>目标</strong><br>提出 SC-SAM，让“传统专家”U-Net 与“通用基础模型”SAM 形成双向协同：</p>
</li>
</ul>
<ol>
<li>U-Net 先利用全部数据（含无标注）生成可靠点提示与伪标签，指导 SAM 的 PEFT；</li>
<li>SAM 反过来用其高阶语义输出正则化 U-Net，抑制噪声。<br>通过该“专家-通才”循环，使 SAM 在仅 5 %–10 % 标注比例下即可充分挖掘无标注医学图像，提升分割精度并超越现有半监督 SAM 变体及医学专用基础模型。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四类，均围绕“如何在半监督或低标注场景下让 SAM 及其变体适应医学图像”展开：</p>
<ol>
<li><strong>直接微调 SAM（PEFT-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>Medical SAM Adapter</strong>（Wu et al., MedIA 2025）<br>在 ViT 编码器内插入 Adapter 层，仅用小规模标注数据做参数高效微调，未利用无标签集。</li>
<li><strong>SAM-Med2D</strong>（Cheng et al., arXiv 2023）<br>将 2D 医学切片重构成 RGB 三通道后直接全微调，参数开销大且仍依赖标注。</li>
</ul>
<ol>
<li><strong>双分支/交叉提示一致性（Dual-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>CPC-SAM</strong>（Miao et al., MICCAI 2024）<br>两个独立 mask decoder 互作“教师”，通过交叉提示生成伪标签，但两分支均为 SAM 自身，易受域偏移影响而耦合。</li>
</ul>
<ol>
<li><strong>专家网络为 SAM 提供提示（SP-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>KnowSAM</strong>（Huang et al., IEEE T-MI 2025）<br>用 U-Net+V-Net 生成融合 mask 作为 SAM 的 mask prompt，再以 KL 蒸馏让专家模仿 SAM；专家仅充当“提示生成器”，SAM 并未反向受益。</li>
</ul>
<ol>
<li><strong>传统半监督 CNN 框架</strong></li>
</ol>
<ul>
<li><strong>U-Net 系列</strong>（Ronneberger et al., MICCAI 2015；Zhou et al., 2018；Jha et al., 2019）<br>通过一致性正则、伪标签、Mean Teacher 等手段充分挖掘无标注数据，但无法与 SAM 的 PEFT 流程协同。</li>
</ul>
<p>SC-SAM 与上述工作的本质区别在于：</p>
<ul>
<li>首次让“专家”U-Net 与“通才”SAM 形成<strong>双向协同训练闭环</strong>；</li>
<li>引入 sigmoid ramp-up 缓解早期噪声伪标签对 SAM 的干扰；</li>
<li>既用 U-Net 生成点提示/伪标签指导 SAM，又用 SAM 的语义输出正则化 U-Net，从而真正将无标注数据的信息注入基础模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 SC-SAM，通过“专家-通才”双向协同训练框架，把 U-Net 对无标注数据的利用能力无缝迁移到 SAM 的 PEFT 流程。核心机制分三步：</p>
<ol>
<li><p><strong>专家先行：U-Net 半监督预热</strong><br>用标准半监督策略（一致性正则 + 伪标签）在全部数据上训练 U-Net，使其对标注与无标注图像均产生可靠预测  P^(UNet) 。</p>
</li>
<li><p><strong>通才跟进：SAM 接受 U-Net 的“提示-伪标签”</strong></p>
</li>
</ol>
<ul>
<li>从  P^(UNet)  随机采样 5 前景点 + 5 背景点，生成点提示</li>
</ul>
<p>points = Sample(P^(UNet))</p>
<ul>
<li>将点提示送入 SAM 的 prompt encoder，与图像编码特征一起经 mask decoder 得到预测</li>
</ul>
<p>P^(SAM) = D!(E(X);; P(points))</p>
<ul>
<li>对无标注图像，把 U-Net 的预测当作伪标签，计算无监督损失</li>
</ul>
<p>L<em>(unsup)^(SAM) = L</em>(seg)(P^(SAM), P^(UNet))</p>
<ol>
<li><strong>反向正则：SAM 输出平滑 U-Net</strong><br>同时用 SAM 的预测  P^(SAM)  作为质量更高的伪标签，反向监督 U-Net</li>
</ol>
<p>L<em>(unsup)^(UNet) = L</em>(seg)(P^(UNet), P^(SAM))</p>
<ol>
<li><strong>稳定训练：sigmoid ramp-up</strong><br>为避免早期 U-Net 噪声淹没 SAM，引入时变权重</li>
</ol>
<p>ω(t)= exp!l(-(1-(t) / (T<em>(max)))^2r), &amp; 0le tle T</em>(max)[2pt] 1, &amp; t&gt;T_(max)</p>
<p>总无监督损失</p>
<p>L<em>(unsup) = L</em>(unsup)^(UNet) + ω(t),L_(unsup)^(SAM)</p>
<ol>
<li><strong>联合目标</strong><br>加上标注数据的监督项，整体损失</li>
</ol>
<p>L<em>(total) = L</em>(seg)(P^(UNet), Y) + L<em>(seg)(P^(SAM), Y)</em>(L)<em>(sup) + L</em>(unsup)</p>
<p>通过该闭环，U-Net 提供结构先验与伪标签，SAM 提供高阶语义正则，二者在每次迭代中同步更新，实现无标注信息向基础模型的有效注入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“极低标注比例下能否让 SAM 充分利用无标注医学图像”展开，覆盖前列腺 MRI 与结直肠息肉两个代表性任务，共 5 组实验：</p>
<ol>
<li><strong>主实验：半监督分割精度对比</strong></li>
</ol>
<ul>
<li><strong>数据集</strong></li>
<li>PROMISE12：50 例前列腺 T2 MRI，按 35/5/10 划分 train/val/test，每例按 2D 切片处理。</li>
<li>COLON：CVC-ClinicDB 612 张 + Kvasir 838 张训练，跨域测试五套公开库（CVC-300、CVC-ColonDB、ETIS-LaribPolypDB 等）。</li>
<li><strong>标注比例</strong><br>仅提供 5 % 或 10 % 训练切片作为标注，其余完全无标签。</li>
<li><strong>对照方法</strong></li>
<li>PEFT-SAM 系列：SAM、MedSAM、SAM-Med2D</li>
<li>Dual-SAM 系列：CPC-SAM</li>
<li>SP-SAM 系列：KnowSAM</li>
<li><strong>指标</strong><br>Dice、IoU、HD95、ASD</li>
<li><strong>结果</strong><br>在 5 % 标注下，SC-SAM 在 PROMISE12 上 Dice 83.64，比第二名 KnowSAM 高 5.15；COLON 五套测试集平均 Dice 提升 2–4 个百分点，均达到新 SOTA。</li>
</ul>
<ol>
<li><p><strong>跨域泛化评估</strong><br>COLON 实验即跨域场景：训练集与五套测试集来自不同医院、不同设备。SC-SAM 在未见中心数据上仍保持最高平均 Dice，验证对域偏移的鲁棒性。</p>
</li>
<li><p><strong>可视化对比</strong><br>随机抽取 5 % 与 10 % 标注场景下的前列腺与息肉样本，绘制预测 mask 与 GT 叠加热图。SC-SAM 边界更紧致，背景误报明显减少。</p>
</li>
<li><p><strong>消融实验</strong></p>
</li>
</ol>
<ul>
<li><strong>组件消融</strong><br>去掉 sigmoid ramp-up（ω(t)≡1）→ Dice 从 83.64 降至 36.37，降幅 47 %。</li>
<li><strong>骨干替换</strong><br>将 U-Net 换成 UNet++、ResUNet++、Swin-UNet，在同等设置下 Dice 分别降至 80.14、78.20、70.27，验证 CNN 型专家在半监督场景下优于 ViT 型。</li>
</ul>
<ol>
<li><strong>资源与收敛分析</strong><br>单卡 RTX 3090 Ti 24 GB 即可完成训练；PROMISE12 5 % 标注场景下，SC-SAM 在约 6 k 迭代后收敛，验证集 Dice 即超过最佳对照 2 个百分点，表明样本效率显著提升。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>三维/视频扩展</strong><br>将双向协同机制从 2D 切片推广到 3D 体数据或结肠镜视频，设计时空一致的伪标签生成策略，解决切片间不连续问题。</p>
</li>
<li><p><strong>提示模态扩充</strong><br>除点提示外，引入可学习的稀疏框、文本或解剖先验嵌入，探索混合提示在更小标注比例（≤1 %）下的极限性能。</p>
</li>
<li><p><strong>理论分析</strong><br>对 sigmoid ramp-up 进行误差上界推导，量化早期伪标签噪声对 SAM 梯度方向的影响，给出最优 ramp-up 长度  T_(max)  的解析估计。</p>
</li>
<li><p><strong>更多专家融合</strong><br>采用多专家集成（CNN + Transformer + 物理模型）生成更鲁棒的伪标签，研究不同专家间置信度加权或投票机制，进一步提升伪标签质量。</p>
</li>
<li><p><strong>持续/增量学习</strong><br>当新器官或成像模态陆续加入时，固定 SAM 编码器、仅扩展 Adapter 与专家网络头部，实现无需重训历史数据的持续医学分割。</p>
</li>
<li><p><strong>不确定性驱动采样</strong><br>利用 SAM 或 U-Net 的预测不确定性，主动挑选最有价值的无标注样本进行人工标注，实现从 5 % 到 1 % 标注比例的主动-半监督联合优化。</p>
</li>
<li><p><strong>跨模态协同</strong><br>将同一患者的 MRI、CT、超声等多模态图像共同输入，设计模态无关的提示空间，验证 SC-SAM 在模态缺失情况下的鲁棒性。</p>
</li>
<li><p><strong>轻量化部署</strong><br>对 U-Net 与 Adapter 进行联合量化/剪枝，保持伪标签质量的同时减少显存占用，使框架可在边缘内镜设备上实时运行。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SC-SAM 核心内容速览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>SAM 在医学影像因域偏移+ scarce labels 表现骤降；PEFT 无法直接利用无标注数据，而传统 U-Net 半监督方案又与 SAM 脱节。</td>
</tr>
<tr>
<td>思路</td>
<td>让“专家”U-Net 与“通才”SAM 互为师生，形成双向协同：U-Net 用全量数据生成点提示+伪标签 → 引导 SAM 的 PEFT；SAM 输出高质量 mask → 反向正则化 U-Net。</td>
</tr>
<tr>
<td>技术</td>
<td>1. 点提示采样：5 前景点+5 背景点。 2. 无监督损失： Lunsup=Lseg(P^(UNet),P^(SAM)) + ω(t)Lseg(P^(SAM),P^(UNet)) ，其中  ω(t)  为 sigmoid ramp-up。 3. 联合目标： Ltotal=Lsup+Lunsup 。</td>
</tr>
<tr>
<td>实验</td>
<td>前列腺 MRI（PROMISE12）与结直肠息肉（COLON）在 5 %/10 % 标注比例下均达 SOTA；跨域、消融、可视化与资源分析验证有效性。</td>
</tr>
<tr>
<td>结论</td>
<td>传统 CNN 专家与现代基础模型协同，可在极低标注场景下充分挖掘无标签医学图像，实现标签高效、精度领先的医学分割。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images，链接是https://arxiv.org/pdf/2601.17934，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17934。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</p>
<p><strong>Categories</strong>: cs.CV, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17934v2.pdf">https://arxiv.org/pdf/2601.17934v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17934v2">https://papers.cool/arxiv/2601.17934v2</a></p>
<p><strong>Published</strong>: 2026-01-25T18:13:48Z</p>
<hr>
<h3 id="8-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation"><a href="#8-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation" class="headerlink" title="8. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation"></a>8. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20854v1">Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</a></h3><p>Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>表格数据仍然是生成模型面临的挑战领域。特别是，标准的变分自编码器（VAE）架构通常由多层感知器组成，在处理混合数据类型时，难以建模特征间的关系。相比之下，Transformer通过其注意力机制，更适合捕捉复杂的特征交互。本文实证探讨将变压器集成到VAE不同组件中的影响。我们在OpenML CC18套件中的57个数据集上进行了实验，得出两个主要结论。首先，结果表明，将变换金刚定位为利用潜在和解码器表示，导致保真度与多样性之间的权衡。其次，我们观察到变压器所有组件中连续的模块之间高度相似。特别地，在解码器中，变换器输入与输出之间的关系近似线性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“表格数据生成”中标准变分自编码器（VAE）难以建模异构特征间复杂依赖关系的问题，提出并系统评估一种改进思路：<br><strong>在 VAE 的不同表征阶段（输入、隐空间、输出）引入 Transformer 的自注意力机制，以显式捕获特征间的高阶交互，同时量化这一改动对生成样本保真度（fidelity）与多样性（diversity）的权衡影响。</strong></p>
<p>具体而言，论文试图回答以下核心问题：</p>
<ol>
<li>在表格数据混合类型特征场景下，将 Transformer 置于 VAE 的编码器、隐空间或解码器，会对生成质量产生何种差异？</li>
<li>不同放置策略是否会带来“保真度-多样性”权衡？</li>
<li>Transformer 内部表征在训练后是否真正改变了特征方向，抑或仅起到缩放作用？</li>
</ol>
<p>通过 57 个 OpenML 数据集的实验，论文得出：</p>
<ul>
<li><strong>仅在隐空间与解码器端引入 Transformer 可显著提升多样性，但会降低保真度</strong>；</li>
<li><strong>编码器端 Transformer 对保真度影响最小</strong>；</li>
<li><strong>解码器端 Transformer 在训练后近似恒等映射，其残差分支受层归一化缩放效应主导</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 章“Related Work”中系统回顾了与“表格数据生成”相关的四大类深度生成模型，并单独梳理了“Transformer 在表格数据生成”中的已有尝试。相关研究可归纳如下：</p>
<p>1. 面向表格数据的深度生成模型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型类别</th>
<th>代表方法</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>GAN 系列</td>
<td>CTGAN、CTAB-GAN、CTAB-GAN+</td>
<td>将条件 GAN 扩展到混合类型特征，引入模式特定归一化、分类编码与辅助损失。</td>
</tr>
<tr>
<td>扩散模型</td>
<td>TabDDPM、CoDi、StaSy、TabSyn、TabDiff</td>
<td>针对离散-连续混合特征设计前向/反向过程，或在潜空间执行扩散。</td>
</tr>
<tr>
<td>自回归模型</td>
<td>GReaT、TabMT</td>
<td>把表格行序列化后用 GPT/Transformer 逐列生成；引入掩码语言建模。</td>
</tr>
<tr>
<td>VAE 系列</td>
<td>TVAE、VAEM、GOGGLE</td>
<td>TVAE 直接对 One-Hot 后特征做 VAE；VAEM 两阶段先逐特征 VAE 再建模隐变量关系；GOGGLE 用图神经网络近似变量关系并联合 VAE 训练。</td>
</tr>
<tr>
<td>流匹配</td>
<td>梯度提升流 (Gradient-Boosted Flow)</td>
<td>用梯度提升树拟合流匹配中的速度场，适配异构特征。</td>
</tr>
</tbody>
</table>
</div>
<p>2. Transformer 在表格数据生成中的已有探索</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>Transformer 角色</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabDiff</td>
<td>同时出现在主干去噪网络与输出头</td>
<td>仅作为模型组件，未系统研究不同放置对保真-多样性权衡的影响。</td>
</tr>
<tr>
<td>TabSyn</td>
<td>用在 VAE 的“识别网络”(encoder) 与“生成网络”(decoder) 内部</td>
<td>与本文 ELD-VAE 结构类似，但未进行表征相似性分析，也未揭示解码器 Transformer 近似恒等的现象。</td>
</tr>
<tr>
<td>GReaT、TabMT</td>
<td>把整张表看成序列，用 Transformer 自回归生成</td>
<td>属于“表→文本”范式，与 VAE 的“潜变量+重构”范式不同。</td>
</tr>
<tr>
<td>SAINT、TabTransformer</td>
<td>主要服务于监督学习，而非生成</td>
<td>聚焦判别任务中的特征交互，未评估合成数据质量。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 小结</p>
<p>已有文献验证了 Transformer 对捕获表格特征交互的有效性，但<strong>缺乏在 VAE 框架内、对“编码器-隐空间-解码器”三个不同层级系统性地放置 Transformer 的对比研究</strong>，也<strong>未量化其带来的保真度-多样性权衡与内部表征相似性</strong>。本文正是填补这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“<strong>系统性位置消融 + 多维度量化评估</strong>”的策略，把 Transformer 逐层“插入”到 VAE 的三种表征路径（输入、隐空间、输出），在 57 个 OpenML 表格数据集上完成对照实验，从而回答“<strong>放哪里、有何利弊、为何如此</strong>”三个问题。具体步骤如下：</p>
<p>1. 构建 6 种 VAE 变体（位置消融）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>Transformer 位置</th>
<th>研究目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAE</td>
<td>无</td>
<td>纯 MLP 基线</td>
</tr>
<tr>
<td>E-VAE</td>
<td>仅 encoder</td>
<td>检验“原始特征”层面交互是否有助益</td>
</tr>
<tr>
<td>EL-VAE</td>
<td>encoder + latent</td>
<td>观察隐变量层加入自注意力的影响</td>
</tr>
<tr>
<td>ELD-VAE</td>
<td>encoder + latent + decoder</td>
<td>全路径覆盖，验证累积效应</td>
</tr>
<tr>
<td>LD-VAE</td>
<td>latent + decoder（无 encoder）</td>
<td>剥离 encoder 干扰，聚焦深层表征</td>
</tr>
<tr>
<td>D-VAE</td>
<td>仅 decoder</td>
<td>直接测试“重构空间”交互的作用</td>
</tr>
</tbody>
</table>
</div>
<p>所有变体共享相同的 tokenizer、detokenizer、隐藏层宽度、 latent 维度与训练超参，保证差异仅由 Transformer 位置引起。</p>
<p>2. 统一训练与采样协议</p>
<ul>
<li><strong>数据预处理</strong>：数值特征用 QuantileTransformer→高斯，类别特征 One-Hot；缺失值均值/众数填补；删除零方差特征。</li>
<li><strong>训练</strong>：500 epoch，Adam  10^(-3) ，权重衰减 0.9，batch size 按验证集自动折半调优。</li>
<li><strong>Transformer 超参</strong>：1 头、4 块、128 隐藏维、pre-norm、embedding 维 4。</li>
<li><strong>采样</strong>：训练完成后从先验  zsimN(0,I)  生成，与训练集同规模。</li>
</ul>
<p>3. 三维评估体系（量化利弊）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评估层级</th>
<th>指标</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low-Density</td>
<td>1-way marginals + Pairwise-correlations</td>
<td>快速检验单变量/双变量分布是否失真</td>
</tr>
<tr>
<td>High-Density</td>
<td>α-Precision（保真） vs β-Recall（多样）</td>
<td>精确量化“保真-多样”权衡</td>
</tr>
<tr>
<td>ML-Efficiency</td>
<td>TSTR Utility + ML-Fidelity</td>
<td>测合成数据在下游 XGBoost 任务中的真实价值</td>
</tr>
</tbody>
</table>
</div>
<p>所有指标归一化到<br>0,1<br>，用 Wilcoxon 符号秩检验判断显著性。</p>
<p>4. 表征相似性诊断（解释原因）</p>
<ul>
<li><strong>CKA 线性相似度</strong>：计算 Transformer 块内“输入-输出”余弦相似度，揭示是否近似恒等映射。</li>
<li><strong>残差缩放因子 σ</strong>：逐样本求解  E= σ E ，若  σ≈ 1  则表明自注意力+LayerNorm 仅做尺度变换。</li>
<li><strong>逐块 F-范数变化</strong>：量化 LayerNorm 对幅值的压缩程度，解释为何解码器 Transformer 不改变方向。</li>
</ul>
<p>5. 结论与落地建议</p>
<ul>
<li><strong>权衡曲线</strong>：latent + decoder 引入 Transformer → β-Recall 平均提升 4–7%，但 α-Precision 下降 3–6%；encoder 处 Transformer 对保真度几乎无负面影响。</li>
<li><strong>恒等现象</strong>：解码器 Transformer 在训练后  σ  中位数≈1，CKA&gt;0.95，证实其退化为“带缩放的恒等函数”，主要受 LayerNorm 的平移-缩放效应主导。</li>
<li><strong>实用指南</strong>：</li>
<li>若任务更重视<strong>多样性</strong>（数据增广、隐私扩充），优先选用 LD-VAE 或 ELD-VAE；</li>
<li>若任务更重视<strong>保真</strong>（缺失值插补、发布替代数据），用 E-VAE 甚至纯 MLP-VAE 即可，省去 Transformer 的计算与调参成本。</li>
</ul>
<p>通过上述“<strong>位置消融 → 多维指标 → 表征诊断</strong>”闭环，论文不仅回答了“放哪里”，也解释了“为何这样”，为后续表格生成模型中 Transformer 的取舍与再设计提供了量化依据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“Transformer 在 VAE 不同位置对表格数据生成的影响”共设计并执行了 <strong>三大类、七组、累计 &gt; 2 000 次训练</strong> 的实验，覆盖 57 个 OpenML-CC18 数据集。具体实验矩阵如下：</p>
<p>1. 主实验：6 种架构 × 57 数据集 全面对照</p>
<p><strong>目的</strong>：量化 Transformer 位置对生成质量与下游效用的一致影响。<br><strong>指标</strong>：</p>
<ul>
<li>Low-Density：1-way marginals、Pairwise-correlations</li>
<li>High-Density：α-Precision（保真）、β-Recall（多样）</li>
<li>ML-Efficiency：TSTR Utility、ML-Fidelity<br><strong>统计</strong>：Wilcoxon 符号秩检验（p=0.001）+ 平均分数/平均排名双维度判定显著性。</li>
</ul>
<p>2. fidelity–diversity 细粒度实验</p>
<p>2.1 Forward 序列（VAE→E-VAE→EL-VAE→ELD-VAE）</p>
<p>2.2 Backward 序列（VAE→D-VAE→LD-VAE→ELD-VAE）</p>
<p><strong>分组</strong>：按样本量将 57 数据集切为 small<br>500,1 k)、medium[1 k,5 k)、large[5 k,96 k<br> 三桶。<br><strong>输出</strong>：</p>
<ul>
<li>各序列每步过渡的 Δα、Δβ 随桶大小的变化曲线（论文图 2 &amp; 补充图 1–2）。</li>
<li>证实“latent/decoder 加 Transformer → 保真降、多样升”趋势与数据规模呈单调关系。</li>
</ul>
<p>3. 表征相似性实验</p>
<p>3.1 聚合相似度</p>
<ul>
<li>对 6 种模型，计算每个 Transformer 组件“输入-输出”CKA 相似度，再按桶平均（论文图 3）。</li>
</ul>
<p>3.2 块内相似度</p>
<ul>
<li>在 ELD-VAE 上，抽取 encoder/latent/decoder 各 4 个 block 内部 6 个检查点（e.in→e.n1→e.attn→e.res→e.n2→e.ffn）两两 CKA，绘制热图（论文图 4–5）。</li>
</ul>
<p>3.3 残差缩放因子</p>
<ul>
<li>逐样本求解 σ 使  E=σE ，给出 σ 分布与块编号关系（论文图 6），验证 decoder 近似恒等。</li>
</ul>
<p>3.4 块数敏感性</p>
<ul>
<li>在 churn、adult、credit-approval 三数据集上，把 ELD-VAE 的 Transformer 块数从 4→3→2→1，重复实验：<br>– High-Density 变化（补充图 3）<br>– 表征相似度热图（补充图 4）<br>结论：块数减少不改变“decoder 高相似”现象，仅 latent 层相似度随块数增加而下降。</li>
</ul>
<p>4. 训练稳定性诊断实验</p>
<ul>
<li>对出现 NaN/爆炸损失的 3 个数据集（kc2、jm1、spambase），将隐藏维与 latent 维减半后重训；记录重构/KL 曲线（补充图 5–6），并对比 α/β 前后变化（补充表 2–3），确认超参过大是 instability 主因。</li>
</ul>
<p>5. 重构数据 fidelity–diversity 实验</p>
<ul>
<li>用训练好的 6 模型对真实测试集做前向重构，再以训练集为参照计算 α-Precision、β-Recall（补充表 4）。</li>
<li>结果与“采样生成”趋势一致，进一步排除评估偏差。</li>
</ul>
<p>6. 低维与 ML 效用桶分析（补充第 2 章）</p>
<ul>
<li>对 Marginals、Pairwise、Utility、ML-Fidelity 四项指标，均按 Forward/Backward 序列、三桶样本量给出 Δ 曲线（补充图 1–2），验证“多样性提升≠下游效用提升”现象在所有指标上稳定存在。</li>
</ul>
<p>7. 单数据集结果全表（补充第 7 章）</p>
<ul>
<li>给出 57 数据集上 6 模型 6 项指标的原始分数与排名（补充表 6–8），支持主文“LD-VAE 综合平均第一”结论的可复现性。</li>
</ul>
<p>实验规模小结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集</td>
<td>57</td>
</tr>
<tr>
<td>模型变体</td>
<td>6</td>
</tr>
<tr>
<td>每数据集训练次数</td>
<td>1（主实验）+ 块数敏感 3×4 + 稳定性重训 3</td>
</tr>
<tr>
<td>总训练运行</td>
<td>≈ 6×57 + 3×4×3 + 3 = 363 + 36 + 3 = 402 次完整训练</td>
</tr>
<tr>
<td>生成与评估</td>
<td>每次训练后生成 1×训练集规模样本，并计算 6 项指标</td>
</tr>
<tr>
<td>CKA/σ 计算</td>
<td>在测试集上逐 batch 计算，累计 &gt; 10 7 次向量内积</td>
</tr>
</tbody>
</table>
</div>
<p>通过上述实验矩阵，论文从“质量-效用-机理”三个层面完整回答了 Transformer 放置问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对该工作的直接延伸或深层追问，均具备理论价值与落地潜力：</p>
<p>1. 架构层面：把“退化”变为“有用”</p>
<ul>
<li><p><strong>去 LayerNorm 的 Transformer 块</strong><br>论文揭示解码器 Transformer 近似恒等的核心原因是 LayerNorm 的平移-缩放效应。可尝试</p>
</li>
<li><p>完全移除 LayerNorm，改用 RMSNorm、PowerNorm 或简单可学习标量缩放；</p>
</li>
<li>仅用 Pre-Activation MLP 替代 Transformer 块，对比是否仍能保持多样性增益。<br>目标：在不影响保真度的前提下，让解码器注意力真正改变表征方向。</li>
<li><strong>轻量级注意力替代</strong><br>探索 Linformer、Performer、Cosformer 等线性复杂度注意力，验证“多样性↑保真度↓”规律是否依旧成立，为大规模高维表格（≥10 000 特征）提供可行方案。</li>
</ul>
<p>2. 目标函数层面：重调保真-多样天平</p>
<ul>
<li><strong>动态加权 α-β 损失</strong><br>当前 VAE 仅优化 ELBO。可把 α-Precision、β-Recall 直接纳入损失，构建</li>
</ul>
<p>L<em>(new) = L</em>(ELBO) - λ_1 α + λ_2 β</p>
<p>通过 Pareto 搜索寻找 (λ₁, λ₂) 最优前沿，实现“保真-多样”可控生成。</p>
<ul>
<li><strong>对比式/信息瓶颈正则</strong><br>引入对比损失或信息瓶颈，限制隐变量互信息 I(x;z) 上限，抑制 VAE 本身“过度随机”导致的保真度下降，看能否在 latent-Transformer 场景下仍保持高 β-Recall。</li>
</ul>
<p>3. 数据层面：更复杂的表格形态</p>
<ul>
<li><p><strong>缺失非随机（MNAR）与噪声标签</strong><br>论文仅做简单均值/众数填补。下一步让生成器直接输出“缺失掩码”与“值”两组向量，用 MNAR 模拟机制评估 Transformer 位置对插补偏差的影响。</p>
</li>
<li><p><strong>时序-表格混合</strong><br>将静态表格字段与时间序列拼接，考察 Transformer 在“静态-动态”异构注意力下的表现，拓展到医疗纵向数据、用户行为日志等场景。</p>
</li>
</ul>
<p>4. 评估层面：超越 α-β</p>
<ul>
<li><p><strong>细粒度隐私泄露度量</strong><br>采用 Membership Inference、Attribute Inference 攻击评估不同位置 Transformer 的隐私-效用权衡，验证“多样性↑”是否伴随“泄露↑”。</p>
</li>
<li><p><strong>因果保真度</strong><br>引入因果发现指标（如结构汉明距离 SHD）衡量合成数据是否保持原始变量间的因果边方向，验证注意力机制是否会混淆因果与相关。</p>
</li>
</ul>
<p>5. 训练策略层面：让 Transformer 不再“闲置”</p>
<ul>
<li><p><strong>两阶段训练</strong><br>① 先无 Transformer 训练 VAE 至收敛；② 冻结解码器 MLP，仅微调解码器 Transformer。观察能否打破“恒等”初始化陷阱，使注意力真正学习残差方向。</p>
</li>
<li><p><strong>梯度截断 / 权重标准化</strong><br>对注意力输出做梯度截断或使用 Weight Standardization，缓解 LayerNorm 输入分布漂移，从而提升有效学习率，测试是否减少恒等现象。</p>
</li>
</ul>
<p>6. 跨模型迁移：是否只是 VAE 的“专利”？</p>
<ul>
<li><p><strong>Transformer-in-GAN</strong><br>将同样的“encoder- latent-decoder”放置策略迁移到 CTGAN、TabDDPM 框架，检验保真-多样权衡是否依旧成立，或是否因目标函数不同而逆转。</p>
</li>
<li><p><strong>扩散模型中的注意力位置</strong><br>在 TabDiff 的去噪网络中，分别把 Transformer 放在“时间步嵌入后”“特征嵌入后”“输出头”三段，重复 α-β 实验，看多样性增益是否普遍存在于所有生成范式。</p>
</li>
</ul>
<p>7. 可解释性深挖：注意力到底在“看”什么？</p>
<ul>
<li><p><strong>注意力可视化</strong><br>对 decoder Transformer 的注意力矩阵进行平均，检查是否始终呈现对角或均匀分布，进一步佐证“无结构交互”导致恒等映射的假设。</p>
</li>
<li><p><strong>互信息分解</strong><br>用 MISE 或 CXPlain 对注意力输出与原始特征做互信息分解，定位哪些特征对被过度强化，从而设计稀疏注意力掩码，减少虚假相关。</p>
</li>
</ul>
<p>8. 工程与产品化</p>
<ul>
<li><p><strong>自动架构搜索（NAS）</strong><br>以 α-β 加权分数为奖励，用强化 NAS 搜索“在哪几层、用几块、头数多少”最优，输出 Pareto 前沿供用户按业务场景（金融风控 vs 营销扩量）一键选择。</p>
</li>
<li><p><strong>端侧轻量化</strong><br>结合第 1 点的线性注意力与第 5 点的两阶段训练，构建 &lt;1 MB 的“微型表格生成器”，在移动端实现现场数据增强与隐私释放。</p>
</li>
</ul>
<p>总结</p>
<p>论文首次量化了“Transformer 位置-保真-多样”三角关系，并揭示了解码器恒等退化现象。下一步工作可沿“<strong>架构-目标-数据-评估-训练-跨模型-可解释-工程化</strong>”八个维度展开，把“表格 Transformer 生成”从“能用”推向“好用、可控、可信”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文主旨一句话</strong><br>系统探究“把 Transformer 塞进 VAE 的哪个部位”才能在混合类型表格数据生成中取得最佳“保真-多样”权衡，并揭示解码器 Transformer 训练后近似恒等映射的机理。</p>
<p>1. 背景与动机</p>
<ul>
<li>标准 VAE（全 MLP）难以捕获异构特征间高阶关系。</li>
<li>Transformer 的自注意力在文本、图像领域已验证其交互建模能力，但在表格生成中“放哪里、值不值”缺乏大尺度实证。</li>
</ul>
<p>2. 方法</p>
<ul>
<li><strong>6 个 VAE 变体</strong>：VAE/E-VAE/EL-VAE/ELD-VAE/LD-VAE/D-VAE，仅改变 Transformer 所处位置（encoder / latent / decoder）。</li>
<li><strong>统一设置</strong>：57 个 OpenML-CC18 数据集 → 相同 tokenizer、训练超参、评估流程。</li>
<li><strong>三维评估</strong>：<br>– Low-Density：1-way marginals + pairwise correlations<br>– High-Density：α-Precision（保真） vs β-Recall（多样）<br>– ML-Efficiency：TSTR Utility + ML-Fidelity</li>
<li><strong>表征诊断</strong>：CKA 相似度、残差缩放因子 σ、逐块 F-范数，解释“退化”原因。</li>
</ul>
<p>3. 主要发现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>位置</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>encoder</td>
<td>几乎不损保真，对多样提升微小。</td>
</tr>
<tr>
<td>latent + decoder</td>
<td>多样↑ 4–7%，保真↓ 3–6%；权衡显著且随数据规模单调。</td>
</tr>
<tr>
<td>decoder 单独</td>
<td>最差保真，多样性增益有限。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>机理级结论</strong></p>
<ul>
<li>解码器 Transformer 训练后输入-输出 CKA&gt;0.95，σ≈1，本质为“LayerNorm 缩放后的恒等函数”。</li>
<li>减少块数或移除 LayerNorm 仍无法打破恒等现象，提示需重新设计目标函数或训练策略。</li>
</ul>
<p>4. 实用建议</p>
<ul>
<li><strong>重保真</strong> → 用 E-VAE 或纯 MLP-VAE，省计算。</li>
<li><strong>重多样</strong> → 用 LD-VAE，接受轻微保真损失。</li>
<li><strong>下游 ML 效用</strong> → Transformer 位置无显著优势，多样性↑≠效用↑，需按任务取舍。</li>
</ul>
<p>5. 贡献清单</p>
<p>① 大尺度（57 数据集）位置消融实验，首次量化“保真-多样”权衡。<br>② 揭示解码器 Transformer 恒等退化现象并给出 LayerNorm 根源解释。<br>③ 提供面向表格生成的 Transformer 使用指南与开源训练-评估协议。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Transformer%20Placement%20in%20Variational%20Autoencoders%20for%20Tabular%20Data%20Generation，链接是https://arxiv.org/pdf/2601.20854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20854。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</p>
<p><strong>Categories</strong>: cs.LG, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20854v1.pdf">https://arxiv.org/pdf/2601.20854v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20854v1">https://papers.cool/arxiv/2601.20854v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:54:27Z</p>
<hr>
<h3 id="9-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><a href="#9-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization" class="headerlink" title="9. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization"></a>9. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07972v2">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</a></h3><p>While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型（LLM）在推理和基于主体的问题解决方面取得了显著进步，但当前的评估方法未能充分评估其能力：现有基准要么依赖易被饱和和记忆的封闭式问题，要么依赖缺乏一致性和严谨性的主观比较。在本研究中，我们介绍了HeuriGym，这是一个为评估由大型语言模型生成的组合优化问题启发式算法而设计的代理框架，其特点是目标明确且解空间广阔。HeuriGym 赋能大型语言模型提出启发式方法，通过代码执行获得评估反馈，并迭代完善解决方案。我们评估了涵盖计算机系统、物流和生物学等九个领域的九个最先进模型，揭示了工具使用、规划和自适应推理中持续存在的局限性。为了量化性能，我们提出了质量-产出指数（QYI），这是一个既能反映解法通过率又能体现质量的指标。即使是像GPT-o4-mini-high和Gemini-2.5-Pro这样的顶级模型，QYI分数也仅为0.6，远低于专家的基准1分。我们的开源基准旨在引导大型语言模型在科学和工程领域实现更有效、更现实的问题解决。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决如何有效评估大型语言模型（LLMs）在组合优化问题上的能力。现有的评估方法存在局限性，主要分为两类：基于封闭问题的客观基准和基于人类偏好的主观评估。前者容易受到数据污染和性能饱和的影响，后者则缺乏一致性和严谨性。为了克服这些局限性，作者提出了一个名为HeuriGym的新框架，旨在通过迭代生成、执行和反馈机制来评估LLMs生成的启发式算法在组合优化问题上的表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了与LLMs在组合优化（CO）领域以及LLMs评估相关的研究。以下是相关研究的总结：</p>
<p>LLMs for Combinatorial Optimization</p>
<ul>
<li><strong>NL4Opt Competition</strong>：将自然语言转化为结构化的优化问题，后续工作致力于改进特定领域的模型训练和提示策略，但这些方法依赖于精确求解器，难以扩展。</li>
<li><strong>FunSearch 和 AlphaEvolve</strong>：FunSearch 使用LLMs生成启发式算法，而AlphaEvolve结合进化搜索生成新的启发式算法。但这些方法需要评估数千个候选方案，效率较低。</li>
<li><strong>Metaheuristic Templates</strong>：最近的一些方法通过元启发式模板改进效率，但仍然限制LLMs只填充评分函数，而不是设计完整的算法。</li>
</ul>
<p>Evaluation on LLMs</p>
<ul>
<li><strong>HumanEval、BigCodeBench、LiveCodeBench</strong>：这些基准测试侧重于闭合问题，如数学问题、编程任务等，存在固定答案，容易受到数据污染。</li>
<li><strong>Chatbot Arena、KernelBench</strong>：这些基准测试鼓励多样化的输出，但往往缺乏明确的目标，导致评估不一致。</li>
<li><strong>NPHardEval、GraphArena</strong>：这些基准测试评估小规模NP难问题的精确解，但实际应用中更倾向于使用启发式方法以实现可扩展性。</li>
</ul>
<p>HeuriGym框架通过提供一个开放式的解决方案空间，允许LLMs生成完整的、自包含的优化程序，包括自定义数据结构和端到端的流程，从而更贴近现实世界的CO挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过提出一个名为 <strong>HeuriGym</strong> 的新框架来解决评估 LLMs 在组合优化问题上的能力的问题。HeuriGym 是一个以代理（agentic）为核心的框架，它通过以下方式实现对 LLMs 的全面评估：</p>
<p>1. <strong>框架概述</strong></p>
<p>HeuriGym 框架的核心思想是让 LLMs 生成启发式算法，然后通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。这个过程模拟了实际工程中的工作流程，能够深入评估 LLMs 的多步推理、工具使用和指令遵循能力。</p>
<p>2. <strong>问题描述与提示设计</strong></p>
<ul>
<li><strong>问题描述</strong>：每个基准任务都包含一个结构化的问题描述，包括背景、正式化和输入/输出格式。背景部分介绍优化的上下文和关键术语，正式化部分定义优化目标和约束条件，输入/输出格式部分则明确输入和输出文件的结构。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中，以便 LLMs 学习并改进其输出。</li>
</ul>
<p>3. <strong>反馈循环</strong></p>
<p>HeuriGym 将数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。每次迭代后，生成的解决方案、执行日志、验证结果和评估分数都会被记录并反馈给 LLM，使其能够从过去的尝试中学习并逐步改进输出。</p>
<p>4. <strong>评估指标</strong></p>
<ul>
<li><strong>solves@i</strong>：跟踪 LLM 在 i 次迭代内解决约束问题的能力。这个指标分为三个阶段：执行、解决方案生成和验证。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：这是一个综合指标，结合了解决方案的通过率（Yield）和质量（Quality），通过计算这两个指标的调和平均值来评估 LLM 的整体表现。</li>
</ul>
<p>5. <strong>基准构建</strong></p>
<p>HeuriGym 包含了来自计算机系统、编译器、计算生物学、物流等领域的九个组合优化问题。这些问题被精心挑选，以确保它们具有明确的目标、庞大的解空间、可扩展的数据实例和可复现的专家基线。每个问题都包括一个任务特定的验证器和评估器，用于评估解决方案的通过率和质量。</p>
<p>6. <strong>实验与评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，包括 GPT-o4-mini-high、Gemini-2.5-Pro、LLaMA-4-Maverick 等。实验结果表明，即使是顶级模型，如 GPT-o4-mini-high 和 Gemini-2.5-Pro，在 QYI 评分上也仅为 0.6 左右，远低于专家基线的 1。这揭示了当前 LLMs 在现实世界问题解决中的局限性。</p>
<p>7. <strong>案例研究</strong></p>
<p>论文还通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。虽然 LLMs 能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</p>
<p>8. <strong>未来工作</strong></p>
<p>作者指出，HeuriGym 仍存在一些局限性，如 Python 执行的开销、迭代自优化过程的进一步改进、以及评估指标与现实世界部署之间的差距。未来的工作将包括扩展基准的广度和深度，引入新的问题，并探索更高效的测试时扩展策略。</p>
<p>通过这些方法，HeuriGym 为 LLMs 在组合优化问题上的评估提供了一个全面、严谨且实用的框架，有助于推动 LLMs 在科学和工程领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验：</p>
<p>1. <strong>模型评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，这些模型包括：</p>
<ul>
<li>OpenAI 的 GPT-o4-mini-high</li>
<li>Anthropic 的 Claude-3.7-Sonnet</li>
<li>DeepSeek 的 DeepSeek-V3 和 DeepSeek-R1</li>
<li>Google 的 Gemini-2.5-Flash 和 Gemini-2.5-Pro</li>
<li>Meta 的 LLaMA-3.3 和 LLaMA-4-Maverick</li>
<li>Alibaba 的 Qwen3-235B</li>
</ul>
<p>这些模型在多个领域（如计算机系统、物流、生物学等）的九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</p>
<p>2. <strong>性能评估</strong></p>
<ul>
<li><strong>solves@i 指标</strong>：评估 LLM 在 i 次迭代内解决约束问题的能力。具体来说，solves@i 分为三个阶段：</li>
<li><strong>执行（Execution）</strong>：生成的程序是否能正确编译或解释，并成功执行基本的输入输出操作。</li>
<li><strong>解决方案生成（Solution Generation）</strong>：程序是否在预定义的超时时间内产生非空输出，并符合预期的输出格式。</li>
<li><strong>验证（Verification）</strong>：解决方案是否满足所有问题特定的约束条件。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：综合评估解决方案的通过率（Yield）和质量（Quality）。QYI 是这两个指标的调和平均值，能够全面反映 LLM 的性能。</li>
</ul>
<p>3. <strong>案例研究</strong></p>
<p>作者通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。具体步骤如下：</p>
<ul>
<li><strong>初始迭代</strong>：LLMs 生成基于动态规划（DP）的启发式算法，但未对搜索空间进行剪枝，导致计算时间过长。</li>
<li><strong>后续迭代</strong>：LLMs 逐渐引入剪枝策略，优化算法性能，最终在第五次迭代中达到了最佳的解决方案。</li>
</ul>
<p>4. <strong>消融研究</strong></p>
<ul>
<li><strong>温度（Temperature）</strong>：评估不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率。</li>
<li><strong>少样本演示（Few-shot Demonstrations）</strong>：研究在不同数量的演示示例下模型的性能。结果表明，提供更多的演示示例可以显著提高模型的性能。</li>
<li><strong>反馈轮次（Feedback Rounds）</strong>：研究不同数量的反馈轮次对模型性能的影响。结果表明，适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>5. <strong>错误分析</strong></p>
<p>作者对 LLMs 在生成启发式算法时常见的错误类型进行了分类和分析，包括：</p>
<ul>
<li><strong>API 调用错误</strong>：使用不存在或过时的库调用。</li>
<li><strong>算法逻辑错误</strong>：即使总体方法合理，但实现存在缺陷。</li>
<li><strong>约束误解</strong>：忽略或误解问题的约束条件。</li>
<li><strong>超时错误</strong>：未在给定的约束时间内产生输出。</li>
</ul>
<p>6. <strong>C++ 实验</strong></p>
<p>作者还进行了初步的 C++ 实验，将 LLMs 生成的 Python 代码转换为 C++ 代码，以评估 C++ 在执行速度上的优势。结果显示，即使在第一次迭代中，C++ 解决方案的通过率也高于经过 10 次迭代的 Python 解决方案。</p>
<p>这些实验为评估 LLMs 在组合优化问题上的能力提供了全面的视角，并揭示了当前 LLMs 的优势和局限性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文中提到了一些可以进一步探索的点，以下是一些关键方向：</p>
<p>1. <strong>扩展基准的广度和深度</strong></p>
<ul>
<li><strong>增加问题数量和多样性</strong>：HeuriGym 目前包含九个问题，但随着 LLM 能力的提升，这些基准可能会逐渐饱和。因此，需要不断引入新的、更具挑战性的组合优化问题，特别是来自尚未充分探索的科学领域。</li>
<li><strong>扩大问题规模</strong>：现有问题的数据实例规模有限，未来可以将现有问题扩展到更复杂、更大规模的实例，以更好地评估 LLMs 在大规模问题上的表现。</li>
</ul>
<p>2. <strong>改进测试时扩展策略</strong></p>
<ul>
<li><strong>测试时计算优化</strong>：当前的迭代自优化过程可以被视为一种测试时扩展（Test-time Scaling, TTS）。可以探索如何结合 Best-of-N 采样、束搜索（Beam Search）和进化算法等技术，特别是在增加迭代预算的情况下，以进一步提升 LLMs 的性能。</li>
<li><strong>自验证能力</strong>：利用强大的验证器，HeuriGym 提供了一个自然的平台来研究 LLMs 的自验证能力，这可能是实现更自主的 LLMs 的一个有前景的方向。</li>
</ul>
<p>3. <strong>评估指标与现实世界部署的对齐</strong></p>
<ul>
<li><strong>代理指标与实际性能的差距</strong>：当前的评估指标虽然在初步基准测试中很有用，但在反映现实世界性能方面往往存在不足。特别是在科学领域，解决方案的质量通常需要通过物理实验来验证；在工程领域，如 EDA，质量需要通过耗时的后端综合来确认。如何弥合代理评估和现实世界部署之间的差距，同时管理更长的反馈循环带来的延迟，是一个关键挑战。</li>
</ul>
<p>4. <strong>多语言支持</strong></p>
<ul>
<li><strong>C++ 集成</strong>：目前的实验主要在 Python 上进行，但 C++ 在某些任务中表现出更好的性能。未来可以进一步探索如何将 C++ 更好地集成到框架中，包括处理依赖于特定领域的库和生成高效、功能正确且并行的 C++ 代码。</li>
</ul>
<p>5. <strong>社区贡献和合作</strong></p>
<ul>
<li><strong>开放基准</strong>：HeuriGym 被设计为一个开放的基准，鼓励社区贡献新的问题。未来的工作将包括扩展基准的广度和深度，通过引入新问题和扩展现有问题到更复杂的领域，以保持基准的长期相关性。</li>
<li><strong>跨学科合作</strong>：通过标准化组合问题的格式和可复现的代理工作流程，HeuriGym 可以作为一个共享的测试平台，促进跨学科合作。</li>
</ul>
<p>6. <strong>模型改进</strong></p>
<ul>
<li><strong>迭代改进策略</strong>：研究如何改进 LLMs 的迭代改进策略，使其能够更有效地从反馈中学习并优化解决方案。</li>
<li><strong>模型架构和训练方法</strong>：探索新的模型架构和训练方法，以提高 LLMs 在组合优化问题上的性能。</li>
</ul>
<p>7. <strong>应用领域扩展</strong></p>
<ul>
<li><strong>实际应用</strong>：将 HeuriGym 中的评估方法应用于实际的工业和科学问题，验证其在现实世界中的有效性。</li>
<li><strong>跨领域应用</strong>：探索 LLMs 在其他领域的应用，如医疗、金融等，这些领域同样需要复杂的推理和优化能力。</li>
</ul>
<p>这些方向为未来的研究提供了广阔的空间，有助于推动 LLMs 在组合优化问题上的应用和发展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了一个名为HeuriGym的框架，旨在评估大型语言模型（LLMs）在组合优化问题上生成启发式算法的能力。HeuriGym通过一个代理（agentic）框架，让LLMs提出解决方案，接收执行反馈，并迭代改进，从而全面评估LLMs的多步推理、工具使用和指令遵循能力。该框架通过定义明确目标和广阔解空间的组合优化问题，克服了现有评估方法的局限性，为LLMs的评估提供了一个更加严谨和实用的平台。</p>
<p>背景知识</p>
<ul>
<li><strong>LLMs的进展</strong>：LLMs在复杂推理和基于代理的问题解决方面取得了显著进展，但现有评估方法未能充分评估其能力。传统基准测试依赖于封闭问题，容易受到数据污染和性能饱和的影响；而基于人类偏好的主观评估则缺乏一致性和严谨性。</li>
<li><strong>组合优化问题</strong>：这类问题在多个领域（如计算机系统、科学推理、计算生物学、物流等）中具有基础性地位，适合用于评估LLMs，因为它们需要算法知识、启发式推理、权衡导航和创造性问题解决能力。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>HeuriGym框架</strong>：该框架包括问题描述、提示设计、反馈循环和评估指标。LLMs根据问题描述生成启发式算法，通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。</li>
<li><strong>问题描述</strong>：每个基准任务都包含背景、正式化和输入/输出格式的结构化描述。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中。</li>
<li><strong>反馈循环</strong>：数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。</li>
<li><strong>评估指标</strong>：提出了solves@i和Quality-Yield Index (QYI)两个指标。solves@i跟踪LLMs在i次迭代内解决约束问题的能力，而QYI则综合评估解决方案的通过率和质量。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>模型评估</strong>：对九个最先进的LLMs模型进行了评估，包括GPT-o4-mini-high、Claude-3.7-Sonnet、DeepSeek-V3、DeepSeek-R1、Gemini-2.5-Flash、Gemini-2.5-Pro、LLaMA-3.3、LLaMA-4-Maverick和Qwen3-235B。这些模型在九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</li>
<li><strong>性能评估</strong>：实验结果显示，即使是顶级模型，如GPT-o4-mini-high和Gemini-2.5-Pro，在QYI评分上也仅为0.6左右，远低于专家基线的1。这揭示了当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>案例研究</strong>：通过技术映射问题展示了LLMs的潜力和当前局限性。虽然LLMs能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</li>
<li><strong>消融研究</strong>：研究了不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响，以及少样本演示和反馈轮次对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率；适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>性能差距</strong>：即使是顶级LLMs模型，在组合优化问题上的表现也远低于专家基线，表明当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>迭代改进的重要性</strong>：通过迭代改进，LLMs能够逐步优化其解决方案，但与专家工具相比仍有显著差距。</li>
<li><strong>评估指标的有效性</strong>：solves@i和QYI指标能够有效评估LLMs在组合优化问题上的表现，为未来的LLMs评估提供了新的视角。</li>
<li><strong>未来工作方向</strong>：HeuriGym框架为未来的LLMs评估提供了一个有前景的平台，未来的工作可以包括扩展基准的广度和深度、改进测试时扩展策略、弥合代理评估和现实世界部署之间的差距等。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是HeuriGym%3A%20An%20Agentic%20Benchmark%20for%20LLM-Crafted%20Heuristics%20in%20Combinatorial%20Optimization，链接是https://arxiv.org/pdf/2506.07972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.07972。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07972v2.pdf">https://arxiv.org/pdf/2506.07972v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07972v2">https://papers.cool/arxiv/2506.07972v2</a></p>
<p><strong>Published</strong>: 2025-06-09T17:46:47Z</p>
<hr>
<h3 id="10-Post-Training-Fairness-Control-A-Single-Train-Framework-for-Dynamic-Fairness-in-Recommendation"><a href="#10-Post-Training-Fairness-Control-A-Single-Train-Framework-for-Dynamic-Fairness-in-Recommendation" class="headerlink" title="10. Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation"></a>10. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20848v1">Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</a></h3><p>Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/weixinchen98/Cofair">https://github.com/weixinchen98/Cofair</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管越来越多的努力试图减少推荐系统中的不公平，现有的公平意识方法通常在培训时确定公平要求，并提供有限的培训后灵活性。然而，在现实中，不同利益相关者可能对不同的公平要求进行时间推移，因此针对不同公平要求的重新培训变得困难。为解决这一局限，我们提出了Cofair，一个单一列车框架，实现培训后推荐的公平控制。具体来说，Cofair引入了与公平性条件适配器模块的共享表示层，以生成针对不同公平度水平的用户嵌入，并引入用户级正则化项，保证在这些层级上实现单调公平性提升。我们理论上确立了Cofair上界人口统计平价的对抗目标和正则化项在用户层面强制执行渐进公平性。对多个数据集和骨干模型的全面实验表明，我们的框架在不同层级提供了动态公平性，能够提供与最先进基线相当甚至更好的公平性与准确性曲线，而无需为每个新的公平性要求重新训练。我们的代码公开发布于 <a target="_blank" rel="noopener" href="https://github.com/weixinchen98/Cofair。">https://github.com/weixinchen98/Cofair。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对推荐系统中“训练后公平性不可调”的瓶颈：现有公平性方法在训练阶段就把公平强度固定死，一旦上线后监管部门、平台或用户提出新的公平要求，就必须重新训练整个模型，代价极高。为此，作者提出 Cofair 框架，<strong>仅用一次训练即可在推理阶段按需调节公平强度</strong>，实现“训练后公平性动态控制”，避免反复重训。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线，均围绕“如何在推荐系统里注入公平性”展开，但各自局限明显：</p>
<ol>
<li><strong>公平性注入但无训练后可调能力</strong></li>
</ol>
<ul>
<li>正则/约束类：FOCF、UGF、FairRec、ComFair、FairGo 等，通过加正则项或重排直接优化 DP、EOpp 等指标，训练后公平强度即固化。</li>
<li>对抗表示类：AFRL、Zemel et al. 2013、Madras et al. 2018 等，用对抗网络把敏感信息从用户表示中剔除，同样需为每个公平阈值重训。</li>
<li>理论保证类：Cui et al. 2023、Song et al. 2019、Gupta et al. 2021 等，在训练阶段把不公平度显式约束在 ε 以内，但 ε 一旦变更必须重新求解约束优化。</li>
</ul>
<ol>
<li><strong>“可控”公平性初步探索，但仍无法细粒度调节强度</strong></li>
</ol>
<ul>
<li>属性级控制：Li et al. 2021 SIGIR、AFRL 2024 SIGIR，允许上线后选择“保护哪些敏感属性”，但保护程度（公平等级）仍由训练时超参锁定。</li>
<li>训练阶段可控：同上理论保证类方法，可控范围仅限训练前，推理期无法“旋钮式”微调。</li>
</ul>
<p>综上，<strong>现有工作要么完全重训，要么只能切换属性而不能平滑调节公平强度</strong>；Cofair 首次在单次训练中产出连续公平等级，填补了这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>Cofair 把“训练后公平性可调”转化为<strong>单次训练、多等级输出</strong>的架构-正则联合优化问题，核心手段有三点：</p>
<ol>
<li><strong>共享表示 + 公平等级适配器</strong></li>
</ol>
<ul>
<li>共享层  S  一次性提取跨等级通用用户特征  s_u ；</li>
<li>每个公平等级  t  拥有轻量适配器  P^((t)) ，只学习“该等级需要去除的敏感信息”，推理时按需求切换适配器即可，无需重训主干。</li>
</ul>
<ol>
<li><strong>用户级单调正则</strong><br>引入逐用户 adversarial BCE 损失序列  L_(fair)^((t))(u) ，通过 softplus 惩罚</li>
</ol>
<p>L<em>(reg)=∑_u∑</em>(t=1)^(T-1)softplus!(L<em>(fair)^((t+1))(u)-L</em>(fair)^((t))(u))</p>
<p>保证随着  t  增大，任何用户的公平性不会倒退，实现“旋钮向右转，公平只增不减”。</p>
<ol>
<li><strong>自适应公平权重</strong><br>训练过程中动态调整各等级权重  λ_t ：</li>
</ol>
<p>λ<em>(t+1)=λ_t+eta(1-L</em>(fair)^((t+1))L_(fair)^((t)))</p>
<p>若相邻等级公平提升不足，则自动加大  λ_(t+1) ，无需人工逐格调参。</p>
<p>最终目标函数</p>
<p>min<em>(Theta)max</em>(θ<em>d)(1) / (T)∑</em>(t=1)^T!l(L<em>(rec)^((t))+λ_t L</em>(fair)^((t))r)+β L_(reg)</p>
<p>一次求解后，推理阶段仅通过选取不同  t  即可输出从“高精度-低公平”到“高公平-低偏差”整条帕累托曲线，完成<strong>训练后公平性动态控制</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕 5 个研究问题（RQ1–RQ5）展开，全部在公开数据集 MovieLens-1M 与 Lastfm-360K 上完成，采用 BPR、LightGCN 两种骨干模型，性别为敏感属性。核心实验内容如下：</p>
<ul>
<li><p><strong>RQ1 整体性能</strong><br>与 4 个强基线（ComFair、FairRec、FairGo、AFRL）对比 fairness-accuracy Pareto 曲线。Cofair 仅训练 1 次，基线需为每个公平强度重训。结果：16 组设定中 15 组 Cofair 取得最帕累托高效曲线（p &lt; 0.05），且可覆盖更宽的公平值区间。</p>
</li>
<li><p><strong>RQ2 消融实验</strong><br>依次移除共享表示层（SRL）、公平适配器（FCA）、自适应权重（AWL）、用户级正则（URL）。指标 DP@10 平均劣化 10.13%，验证各组件对可控性与单调提升的必要性。</p>
</li>
<li><p><strong>RQ3 超参数敏感度</strong><br>扫描初始公平系数 λ0、更新步长 η、正则强度 β，统计 5 个公平等级的均值与方差。发现中等 λ0、η、β 可在公平改善幅度与精度稳定性之间取得最佳折中。</p>
</li>
<li><p><strong>RQ4 框架通用性</strong><br>将 Cofair 作为“插件”嵌入到 4 个基线（即“原方法 + Cofair”）。结果：所有增强版在单次训练内即可生成连续公平曲线，且帕累托效率持平或优于原多次重训版本，证明可无缝移植。</p>
</li>
<li><p><strong>RQ5 效率评估</strong><br>记录达到 5 个公平等级所需总训练时间。Cofair 每 epoch 耗时约为最快基线的 2 倍，但仅需 1/5 的 epoch 数，总体训练时间减少约 4 倍，验证其在大规模场景下的可扩展性。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向（按研究价值与可行性排序）：</p>
<ol>
<li><p><strong>多公平准则统一控制</strong><br>目前需更换 adversarial 目标才能从 DP 切换到 EOpp 等。可研究单一模型同时输出“DP-等级”“EOpp-等级”“个体公平-等级”等多维旋钮，实现跨准则联合调控。</p>
</li>
<li><p><strong>连续公平强度</strong><br>现框架离散 t=1… T 。将适配器权重或条件向量改为连续变量 $ τ ∈<br>0,1<br>$，利用超网络或神经 ODE 生成对应表示，实现真正无级调节。</p>
</li>
<li><p><strong>多敏感属性与交叉公平</strong><br>论文仅验证二元性别。可扩展至种族-性别-年龄等多维敏感属性，并显式约束交叉子群（intersectionality）公平，避免“平均公平掩盖局部不公”。</p>
</li>
<li><p><strong>提供者侧公平联动</strong><br>仅控制用户侧 DP/EOpp。可同时引入提供者侧曝光或收益约束，实现“用户-商品”双端公平，并探索两端旋钮之间的耦合效应。</p>
</li>
<li><p><strong>动态数据分布下的在线调节</strong><br>真实场景用户行为与敏感分布随时间漂移。可结合在线适配或强化学习，让公平系数  λ_t  随漂移信号自动微调，保持长期公平性。</p>
</li>
<li><p><strong>轻量化部署</strong><br>适配器虽轻量，但  T  个等级仍占显存。研究权重共享、低秩分解或二值 mask，使得移动端可只保存一套公共参数与  ≤ 1  MB 的增量，实现端侧实时调节。</p>
</li>
<li><p><strong>可解释性接口</strong><br>为每个等级生成自然语言或可视化解释（如“该等级使女性用户 Top-10 曝光占比提升 6%”），帮助监管/运营方理解不同旋钮的业务含义。</p>
</li>
<li><p><strong>公平-收益-成本三目标优化</strong><br>平台关心 GMV、广告收入等指标。将收益损失或运营成本显式建模为第三轴，研究三目标帕累托前沿，支持高层决策。</p>
</li>
<li><p><strong>跨域迁移与冷启动</strong><br>探索把已训练好的公平适配器迁移到新领域或新敏感属性，仅微调少量参数即可快速获得可控制公平性，减少新场景数据需求。</p>
</li>
<li><p><strong>合规审计与鲁棒攻击</strong><br>设计对抗攻击刻意触发公平性退化，检验单调正则的鲁棒性；同时提供可验证的审计接口，满足欧盟 AI Act 等监管“可验证公平”要求。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：既有公平推荐方法把公平强度“焊死”在训练阶段，一旦政策/业务需要不同公平级别就得重训，代价高。</li>
<li><strong>思路</strong>：Cofair 用“单次训练、推理时拨档”取代反复重训，实现<strong>训练后公平性动态控制</strong>。</li>
<li><strong>架构</strong><br>– 共享表示层：提取跨等级通用用户特征，避免重复计算。<br>– 公平等级适配器：每个等级  t  一个轻量网络，仅学习该等级需去除的敏感信息；推理时按需调用。<br>– 用户级单调正则：强制每个用户的公平性随等级提升而<strong>只增不减</strong>，防止个体倒退。<br>– 自适应权重：训练期自动调节各等级公平系数，无需人工逐格调参。</li>
<li><strong>理论</strong>：证明 adversarial 公平损失是 demographic parity 差值的上界，且正则项收敛时必得单调公平改进。</li>
<li><strong>实验</strong>：在 MovieLens-1M、Lastfm-360K 与 BPR/LightGCN 上，一次训练即可生成覆盖多种公平-精度权衡的帕累托曲线，<strong>16 组对比 15 组最优</strong>，总体训练时间减少约 4 倍；可即插即用到现有公平方法，显著提升其灵活性而不降性能。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Post-Training%20Fairness%20Control%3A%20A%20Single-Train%20Framework%20for%20Dynamic%20Fairness%20in%20Recommendation，链接是https://arxiv.org/pdf/2601.20848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20848。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Weixin Chen, Li Chen, Yuhan Zhao</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CY, cs.IR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20848v1.pdf">https://arxiv.org/pdf/2601.20848v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20848v1">https://papers.cool/arxiv/2601.20848v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:48:43Z</p>
<hr>
<h2 id="Agent-Domain-Papers"><a href="#Agent-Domain-Papers" class="headerlink" title="Agent Domain Papers"></a>Agent Domain Papers</h2><h3 id="1-Recursive-Language-Models-1"><a href="#1-Recursive-Language-Models-1" class="headerlink" title="1. Recursive Language Models"></a>1. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.24601v2">Recursive Language Models</a></h3><p>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse long-context tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLM-Qwen3-8B, outperforms the underlying Qwen3-8B model by $28.3\%$ on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们研究通过推理时间扩展的视角，使大型语言模型（LLMs）能够处理任意长度的提示。我们提出了递归语言模型（RLMs），这是一种通用的推理范式，将长提示视为外部环境的一部分，并允许LLM以编程方式检查、分解并递归调用自身来处理提示片段。我们发现，RLMs能够成功处理比模型上下文窗口大两个数量级的输入，即使对于较短的提示，在四个不同的长上下文任务中，其性能也显著优于普通的前沿LLM和常见的长上下文结构，同时成本相当。在小规模实验中，我们对首个原生递归语言模型进行了后训练。我们的模型RLM-Qwen3-8B，相对于基础Qwen3-8B模型平均提高了28.3%，甚至在三个长上下文任务中接近普通GPT-5的质量。代码可在 <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现代大语言模型（LLM）在处理“任意长输入”时遭遇的两大瓶颈：</p>
<ol>
<li><p>物理上下文窗口限制<br>即使最新模型（如 GPT-5）的显式窗口已达 272K token，仍无法直接容纳 10M 乃至 100M 级别的长文档、代码库或多跳检索语料。</p>
</li>
<li><p>上下文腐烂（context rot）<br>在窗口内，随着输入变长，模型对早期信息的利用效率急剧下降，表现为任务复杂度越高、衰减越快（图 1）。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Recursive Language Models (RLMs)</strong>——一种<strong>推理时（inference-time）通用框架</strong>，把超长 prompt 视为外部环境变量，让 LLM 在 Python REPL 里用代码“窥视、分解、递归调用自身”处理片段，从而：</p>
<ul>
<li>将有效上下文长度扩展两个数量级（实验到 10M+ token）；</li>
<li>在多项长上下文基准上显著优于直接调用基座模型、摘要式代理、检索增强代理等基线，且成本持平或更低；</li>
<li>无需修改模型参数或重新训练，仅通过推理策略即可落地。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两大脉络，并指出 RLM 与它们的本质区别：</p>
<ol>
<li>长上下文系统（Long-Context LM Systems）</li>
</ol>
<ul>
<li><strong>架构-级扩展</strong>：通过重新训练或改进注意力机制来延长上下文，例如<br>– 线性偏置注意力 Press et al. 2022<br>– 结构化状态空间 Gu et al. 2022<br>– Infini-attention Munkhdalai et al. 2024</li>
<li><strong>外部脚手架（scaffold）</strong>：不改动模型，而在推理阶段压缩或管理上下文<br>– 有损管理：MemWalker（树状导航摘要）、ReSum（迭代压缩）、Claude-Code 式多轮摘要<br>– 显式记忆层级：MemGPT、Mem0、G-Memory 等把上下文拆成“主存/外存”<br><em>区别</em>：RLM 不预设任何摘要、检索或记忆结构，完全由模型在 REPL 里用代码自行决定如何切块、过滤、调用子 LLM，上下文管理被“隐式”地交给模型自己。</li>
</ul>
<ol>
<li>子 LLM 递归调用（Task Decomposition via sub-LM calls）</li>
</ol>
<ul>
<li>人设计流程：AutoGPT、ReAct、Claude sub-agents 等多按固定模板拆任务</li>
<li>模型自主拆：ViperGPT、THREAD、DisCIPL、ReDel、Context-Folding、AgentFold 等让 LLM 自己决定何时再调 LLM，但输入长度仍受基模型窗口限制<br><em>区别</em>：RLM 把“输入提示”本身卸载到外部环境，首次让递归调用可以<strong>符号级操作任意长字符串</strong>，从而突破窗口上限；先前方法只能递归拆“任务”，不能递归拆“输入”。</li>
</ul>
<p>综上，RLM 的核心差异可概括为：</p>
<blockquote>
<p>把提示当成环境变量，而非模型输入；用代码+递归子 LLM 实现<strong>输入长度</strong>的无限外扩，而非仅对任务做逻辑拆解。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“超长提示无法一次性塞进上下文”这一瓶颈重新定义为<strong>推理时计算</strong>问题，并借鉴“核外算法”（out-of-core）思想：数据远大于内存时，通过<strong>按需加载+符号化操作</strong>完成计算。具体方案称为 <strong>Recursive Language Models (RLMs)</strong>，其关键机制如下：</p>
<ol>
<li><p>把提示变成环境变量<br>启动一个 Python REPL，将完整提示  P  作为字符串变量 <code>context</code> 载入内存；模型不再直接接收  P  作为输入，而是接收一段<strong>固定系统提示</strong>+REPL 返回的<strong>截断执行结果</strong>。</p>
</li>
<li><p>用代码“窥视-分解-调用”<br>模型在 REPL 里写 Python 代码，可以</p>
</li>
</ol>
<ul>
<li>用 <code>print</code>/<code>regex</code>/<code>pandas</code> 等快速扫描、过滤、统计；</li>
<li>把 <code>context</code> 切成任意粒度（行、文档、token 块）；</li>
<li>通过内置函数 <code>llm_query(sub_prompt)</code> <strong>递归调用子 LLM</strong>（可以是自身轻量版或其他模型），子调用结果再写回 REPL 变量。</li>
</ul>
<ol>
<li><p>递归子调用不受窗口限制<br>每次 <code>llm_query</code> 只把<strong>当前必要片段</strong>喂给子模型，因此总输入长度可以远超基模型物理窗口；子模型输出被存入变量，供后续代码继续聚合、验证或拼接。</p>
</li>
<li><p>终止与答案返回<br>当代码逻辑判断已收集足够信息，模型执行<br><code>FINAL(answer)</code> 或 <code>FINAL_VAR(variable_name)</code><br>把结果返回给用户，整个轨迹在 REPL 内完成，无需人工设计摘要或检索流程。</p>
</li>
</ol>
<p>通过上述四步，RLM 将“长上下文”转化为<strong>可执行对象</strong>，用<strong>代码控制数据流+递归子 LLM 控制语义流</strong>，从而在 10M token 规模上仍保持高精度，且平均调用成本与基模型持平甚至更低。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 4 类信息密度递增的长上下文任务上展开系统实验，覆盖 32 K–11 M token 区间，对比 2 组前沿模型与 4 种推理策略，并给出成本-性能联合分析。实验设计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>基座模型</td>
<td>GPT-5（闭源）与 Qwen3-Coder-480B-A35B（开源）</td>
</tr>
<tr>
<td>对比方法</td>
<td>① Base 直接调用 ② CodeAct(+BM25) ③ Summary Agent ④ RLM（完整） ⑤ RLM（无递归子调用）</td>
</tr>
<tr>
<td>任务集</td>
<td>① S-NIAH（常数级处理）② BrowseComp-Plus@1K-doc（多跳问答，6–11 M token）③ OOLONG（线性级语义聚合，131 K token）④ OOLONG-Pairs（二次级成对聚合，32 K token）⑤ LongBench-v2 CodeQA（900 k 级代码库理解）</td>
</tr>
<tr>
<td>规模轴</td>
<td>对 S-NIAH/OOLONG/OOLONG-Pairs 按 2¹³…2¹⁸ token 做长度扫描，观察性能衰减曲线</td>
</tr>
<tr>
<td>评价指标</td>
<td>准确率 / F1 / 官方评分函数；每次查询的 API 成本与运行时长（25–95 分位）</td>
</tr>
<tr>
<td>消融与失败分析</td>
<td>① 无递归子调用 ablation② 不同文档规模子集（BrowseComp 20 任务，100–1000 doc）③ 轨迹案例：验证-冗余、过度递归、输出令牌耗尽等负面模式</td>
</tr>
</tbody>
</table>
</div>
<p>核心结果一览（Table 1 &amp; Figure 1）：</p>
<ul>
<li>10 M token 规模下，RLM 仍保持 50–90 % 准确率，而基座模型因超出 272 K 窗口直接失败。</li>
<li>在窗口<strong>之内</strong>的任务，RLM 相对 Base 提升 10–58 % 绝对准确率；OOLONG-Pairs 的 F1 从 &lt;0.1 % 提升到 58 %（GPT-5）/ 23 %（Qwen3）。</li>
<li>中位调用成本：RLM 与 Base 同量级，甚至低 3×；仅在长尾轨迹出现高方差。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><strong>异步与并行化</strong>：当前子 LLM 调用为阻塞式，引入异步 <code>llm_query</code> 与并行 chunk 处理可显著压缩延迟与云成本。</li>
<li><strong>更深递归层数</strong>：实验限递归深度=1（仅子 LLM），探索深度&gt;1 的“子-子”调用能否进一步降低单段上下文压力，及如何避免误差级联。</li>
<li><strong>专用 RLM 预训练/后训练</strong>：用 RLM 轨迹数据（代码+子调用序列）做 SFT/RL，使模型学会更优的切块、过滤、停止决策，减少冗余验证。</li>
<li><strong>动态预算控制</strong>：在 REPL 内暴露“已用 token/费用”变量，让模型自主权衡精度-成本，实现可中断的任意长输入推理。</li>
<li><strong>多模态与工具扩展</strong>：将图像、音频、数据库、搜索引擎也作为环境变量，通过同一套“代码+递归调用”框架统一处理跨模态长上下文。</li>
<li><strong>可验证性提升</strong>：引入形式化验证或符号执行，对代码产生的中间变量进行一致性检查，降低长轨迹中“正确→错误”翻转概率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Recursive Language Models (RLMs)</strong> 提出一种<strong>推理时通用框架</strong>，把超长提示卸载到 Python REPL 环境，让大模型用代码“窥视-分解-递归调用自身”，突破上下文窗口与上下文腐烂双重瓶颈。主要贡献如下：</p>
<ol>
<li><p>问题重新定义<br>将“长上下文”视为<strong>推理时计算</strong>问题，而非单纯扩大窗口。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>提示 → 环境变量 <code>context</code></li>
<li>模型写 Python 代码做过滤、切块、聚合</li>
<li>通过 <code>llm_query()</code> 递归调用子 LLM，片段级处理，结果写回变量</li>
<li>终止时 <code>FINAL()/FINAL_VAR()</code> 返回答案</li>
</ul>
<ol>
<li><p>实验<br>在 32 K–11 M token 的 5 项任务（S-NIAH、BrowseComp-Plus、OOLONG、OOLONG-Pairs、CodeQA）上，RLM 用 GPT-5 与 Qwen3-Coder 均取得<strong>双倍位数绝对准确率提升</strong>，10 M token 仍保持高性能，中位 API 成本与基模型持平或更低。</p>
</li>
<li><p>结论<br>RLMs 是<strong>任务无关、模型无关</strong>的推理范式，无需重训练即可把有效上下文扩展两个数量级，为“无限长输入”提供了一条可扩展、可负担的新轴心。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Recursive%20Language%20Models，链接是https://arxiv.org/pdf/2512.24601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.24601。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Alex L. Zhang, Tim Kraska, Omar Khattab</p>
<p><strong>Categories</strong>: cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.24601v2.pdf">https://arxiv.org/pdf/2512.24601v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.24601v2">https://papers.cool/arxiv/2512.24601v2</a></p>
<p><strong>Published</strong>: 2025-12-31T03:43:41Z</p>
<hr>
<h3 id="2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-1"><a href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-1" class="headerlink" title="2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs"></a>2. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20861v1">Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</a></h3><p>One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>当前AI系统中最缺失的能力之一是部署后持续学习的能力。实现此类持续学习系统面临诸多挑战，其中之一是用于训练最先进LLMs的基于梯度算法的大量内存需求。进化策略（ES）最近作为传统学习算法的无梯度替代方案重新出现，并在LLMs的特定任务中表现出令人鼓舞的表现。本文对ES进行了全面分析，并特别评估了在训练更新步骤增加时遗忘曲线。首先，我们发现ES在计算预算相当的情况下，在数学和推理任务中能够达到接近GRPO的性能。然而，对持续学习最重要的是，ES的性能提升伴随着对先前能力的显著遗忘，限制了其在线训练模型的适用性。我们还探讨了这种行为背后的原因，并展示了使用ES进行的更新比对应GRPO更新更稀疏，且其$\ell_2$范数远大于相应的GRPO更新，这解释了两种算法之间遗忘曲线的对比。通过这项研究，我们旨在突出像ES这样的无梯度算法中遗忘的问题，并希望激励未来工作来缓解这些问题。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文聚焦“如何让大模型在部署后持续学习”这一核心难题，具体针对以下子问题展开研究：</p>
<ul>
<li>梯度类后训练算法（SFT、RLHF、GRPO 等）内存开销巨大，难以在端侧或在线场景下反复更新权重。</li>
<li>近期复兴的梯度无参算法——进化策略（Evolutionary Strategies, ES）——声称在特定任务上可与 GRPO 媲美，且仅需前向推理，内存占用极低，被视为持续学习的潜在替代方案。</li>
<li>然而，ES 在连续更新过程中是否会“灾难性遗忘”旧能力，此前缺乏系统评估；若遗忘严重，则其“在线学习”优势将失去意义。</li>
</ul>
<p>因此，论文旨在</p>
<ol>
<li>在数学与推理任务上全面对比 ES 与 GRPO 的峰值性能；</li>
<li>量化二者在持续更新过程中的遗忘曲线，判断 ES 是否真正适合部署后持续学习；</li>
<li>从参数更新范数与稀疏度角度解释遗忘差异，为后续改进梯度无参算法提供依据。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 Related Work 中提及并扩展了以下几条研究脉络：</p>
<ol>
<li>进化策略（Evolution Strategies, ES）</li>
</ol>
<ul>
<li>经典理论：Rechenberg 1989、Schwefel 1977、Beyer 1995 奠定  (μ,λ) -ES 框架。</li>
<li>实用变体：CMA-ES（Hansen &amp; Ostermeier 2001）、自然 ES（Wierstra et al. 2011, Sun et al. 2012）。</li>
<li>深度神经演化：Such et al. 2018、Risi &amp; Stanley 2019 在百万参数规模验证 GA/ES 可训练 RL 网络。</li>
<li>大模型时代：Salimans et al. 2017 首次将 ES 扩展到强化学习，展示高度并行与无梯度优势。</li>
<li>近期 LLM 工作：<br>– Malladi et al. 2024、Korotyshova et al. 2025 提出内存高效的 LoRA-ES。<br>– Qiu et al. 2025（与本论文直接对标）在 1–7 B 模型上报告 ES 优于 GRPO，但未系统评估遗忘。</li>
</ul>
<ol>
<li>梯度类后训练算法</li>
</ol>
<ul>
<li>SFT：Wei et al. 2022</li>
<li>RLHF：Ouyang et al. 2022</li>
<li>DPO：Rafailov et al. 2024</li>
<li>GRPO：Shao et al. 2024，通过组内相对奖励省去 Critic，成为本文对比基线。</li>
</ul>
<ol>
<li>灾难性遗忘与持续学习</li>
</ol>
<ul>
<li>Kirkpatrick et al. 2017 提出 EWC，首次在深度学习语境下量化遗忘。</li>
<li>Gupta et al. 2024 发现大规模模型编辑同样会出现“渐进-突发”式遗忘。</li>
<li>Shenfeld et al. 2025 观察到带 KL 正则的在线 RL（包括 GRPO）遗忘更少，为本论文提供参照。</li>
</ul>
<ol>
<li>参数更新稀疏性研究</li>
</ol>
<ul>
<li>Mukherjee et al. 2025 通过掩码分析证明 RLHF 仅微调极小“子网络”，解释其抗遗忘特性；本文沿用其稀疏度定义与分层统计方法，对比 ES 与 GRPO。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一套“新算法”去彻底解决灾难性遗忘，而是通过<strong>系统性实证诊断</strong>揭示 ES 在持续学习场景下的根本缺陷，从而为后续算法设计指明改进方向。具体步骤如下：</p>
<ol>
<li>构建可比实验框架</li>
</ol>
<ul>
<li>统一任务：Countdown、GSM8K、MATH、OlympiadBench。</li>
<li>统一模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct。</li>
<li>统一资源：30 条 rollout / population，200 条训练样本，500 更新步以内。</li>
<li>统一评估：每轮 checkpoint 同时测“新任务准确率”与“旧任务（HellaSwag）准确率”，绘制遗忘曲线。</li>
</ul>
<ol>
<li>量化遗忘现象</li>
</ol>
<ul>
<li>绘制 Pareto 前沿：新任务准确率 vs 旧任务准确率。</li>
<li>统计旧任务性能随更新步的衰减斜率；ES 在 200 步后额外训练几乎不再提升新任务，却持续降低旧任务约 10 个百分点，而 GRPO 基本平稳。</li>
</ul>
<ol>
<li>追溯根本原因</li>
</ol>
<ul>
<li><strong>更新范数</strong>：计算相邻 checkpoint 的 Frobenius 范数</li>
</ul>
<p>|DeltaW|<em>F = |W</em>(t) - W_(t-1)|_F</p>
<p>ES 在 500 步时漂移幅度比 GRPO 大 <strong>3 个数量级</strong>。</p>
<ul>
<li><strong>更新稀疏度</strong>：定义</li>
</ul>
<p>sparsity = #|(DeltaW)_(ij)| &lt; 10^(-6)#params</p>
<p>GRPO 稀疏度 ≈ 95 %，ES 普遍 &lt; 20 %，呈全局密集扰动。</p>
<ul>
<li>关联分析：大范数 + 低稀疏 → 参数空间全局漂移 → 与旧任务表征冲突 → 灾难性遗忘。</li>
</ul>
<ol>
<li>给出改进启示</li>
</ol>
<ul>
<li>梯度无参方法若想用于持续学习，必须<strong>显式约束更新范数</strong>（如裁剪、衰减）或<strong>强制稀疏掩码</strong>（如 LoRA-ES + 动态门控）。</li>
<li>可借鉴 GRPO 的 KL 正则思路，在 ES 的目标中引入与锚定模型的散度惩罚，限制漂移。</li>
</ul>
<p>综上，论文通过“实验量化 → 现象确认 → 根因剖析 → 改进提示”四步，<strong>把 ES 的遗忘风险从经验猜测转化为可度量、可解释的定量结论</strong>，为后续设计“既省内存又不遗忘”的梯度无参算法奠定实证基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计三类实验，全部围绕“ES 能否在持续学习场景下替代 GRPO”这一核心问题展开。实验均在同一硬件（NVIDIA RTX A6000）与同一代码框架（VERL + 自复现 ES）上完成，保证可比性。</p>
<ol>
<li>主实验：ES vs GRPO 峰值性能对照</li>
</ol>
<ul>
<li>模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct</li>
<li>任务：Countdown、GSM8K、MATH、OlympiadBench</li>
<li>协议：每任务 200 训练样本，30 rollout/population，最大 500 更新步，早停验证集不再提升。</li>
<li>指标：验证集最佳准确率（%）</li>
<li>结果：ES 平均落后 GRPO 3–4 个百分点，仅在 Llama-3.2-1B/GSM8K 一项略超；其余均低于 GRPO，反驳了“ES 全面优于 GRPO”的先前提法。</li>
</ul>
<ol>
<li>遗忘曲线实验</li>
</ol>
<ul>
<li>固定模型：Qwen2.5-1.5B-Instruct</li>
<li>新任务：Countdown（训练集 200 条）</li>
<li>旧任务：HellaSwag（全验证集，作为“通用先验能力”探针）</li>
<li>协议：每 50 步保存一个 checkpoint，同时测 Countdown 与 HellaSwag 准确率。</li>
<li>可视化：<br>– Pareto 前沿图：新任务准确率 vs 旧任务准确率<br>– 单指标时序图：HellaSwag 准确率随更新步变化</li>
<li>结果：<br>– ES 在 200 步后新任务收敛，但旧任务仍单调下降，累计跌落 ≈ 10 %。<br>– GRPO 旧任务曲线几乎水平，无明显遗忘。</li>
</ul>
<ol>
<li>更新诊断实验（解释为何遗忘）<br>3.1 更新范数</li>
</ol>
<ul>
<li>计算相邻 checkpoint 参数差 ΔW 的 Frobenius 范数</li>
<li>结果：500 步时 ES 漂移比 GRPO 大 <strong>1000×</strong> 以上，且单调递增。</li>
</ul>
<p>3.2 更新稀疏度</p>
<ul>
<li>定义阈值 τ=1e-6，统计 |ΔW_ij|&lt;τ 的元素比例</li>
<li>分层统计：Attention Q/K/V、WO、MLP、LayerNorm</li>
<li>结果：<br>– GRPO 稀疏度 ≈ 95 %（仅 5 % 参数被显著修改）<br>– ES 稀疏度 &lt; 20 %，几乎全参数被扰动，LayerNorm 稍高但仍远低于 GRPO。</li>
</ul>
<p>3.3 KL-散度关联（附录）</p>
<ul>
<li>计算 π_θ 与 base 模型在 Countdown 与 HellaSwag 上的 KL</li>
<li>结果：ES 的 KL 随步数线性增长，且与旧任务准确率呈显著负相关；GRPO 因显式 KL 惩罚，散度与准确率均无单调关系。</li>
</ul>
<p>通过上述三类实验，论文既验证了 ES 的“性能可比性”，又定量揭示了其“灾难性遗忘”现象，并用范数+稀疏度+KL 三维诊断给出解释。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，均围绕“让梯度无参算法既省内存又不灾难性遗忘”这一核心目标展开：</p>
<ol>
<li>约束型 ES 框架</li>
</ol>
<ul>
<li>将 KL 惩罚或参数距离惩罚显式写入 ES 的目标：</li>
</ul>
<p>θ<em>(t+1) = θ_t + α · (1) / (N) ∑_n Z_n ε_n - β ∇</em>θ D<em>(KL)(π</em>(θ) parallel π_(ref))</p>
<p>需推导无梯度情形下如何高效估计该惩罚项。</p>
<ul>
<li>探索硬约束：每步更新后投影到以  θ<em>(base)  为中心的  ell_2  球或  ell</em>∞  立方体内，限制漂移半径。</li>
</ul>
<ol>
<li>稀疏化 ES 更新</li>
</ol>
<ul>
<li>引入随机掩码：每次只扰动随机子集参数（如 5 %），保持其余固定；掩码可在层、通道或奇异值维度上采样。</li>
<li>学习稀疏掩码：借鉴 RigL / Magnitude Pruning，动态决定“哪些参数值得扰动”，使有效更新稀疏度逼近 GRPO 的 95 % 水平。</li>
</ul>
<ol>
<li>低秩 / 适配器混合 ES</li>
</ol>
<ul>
<li>仅在 LoRA 低秩矩阵（A、B）上应用 ES 扰动，冻结主干；漂移空间从 1.5 B 降至数百万参数，天然减小范数。</li>
<li>比较不同秩（4, 16, 64）对“性能-遗忘”Pareto 前沿的影响，找出秩-遗忘临界值。</li>
</ul>
<ol>
<li>种群级正则</li>
</ol>
<ul>
<li>在种群内部保持“锚点个体”——不扰动的  θ_(base)  副本，将其奖励作为额外基线，惩罚偏离过远的个体。</li>
<li>研究锚点比例（1/30、5/30、10/30）对旧任务保留率的边际效应。</li>
</ul>
<ol>
<li>多任务 ES 演化</li>
</ol>
<ul>
<li>同时把 Countdown + HellaSwag 打包成多目标奖励：</li>
</ul>
<p>R = R<em>(new) + λ R</em>(old)</p>
<p>用演化多目标算法（NSGA-ES）搜索非支配解集，观察是否自然涌现“不遗忘”个体。</p>
<ul>
<li>考察  λ  调度：初期  λ  大，后期逐步减小，让模型先稳固旧能力再提升新能力。</li>
</ul>
<ol>
<li>与参数高效微调正交组合</li>
</ol>
<ul>
<li>将 ES 用于 Prompt-tuning、AdaLoRA、VeRA 等“极轻量”模块，验证遗忘是否随可训参数量线性下降。</li>
<li>对比同一参数预算下“ES-LoRA”与“GRPO-LoRA”的 Pareto 前沿，判断梯度自由在极低秩下是否仍有优势。</li>
</ul>
<ol>
<li>在线 / 流式场景测试</li>
</ol>
<ul>
<li>构建“任务流”：按顺序依次训练 Countdown → GSM8K → MATH → Olympiad，每任务仅 200 样本，周期性回放 5 % 旧数据。</li>
<li>监测 ES 与 GRPO 的“遗忘-学习”折线，评估哪种算法在数据流场景下最终平均性能更高。</li>
</ul>
<ol>
<li>理论侧：ES 漂移上界</li>
</ol>
<ul>
<li>在强凸或 PL 条件下，推导 ES 更新步的期望漂移</li>
</ul>
<p>E[|θ_(t)-θ_0|^2] le f(t, σ, α, d)</p>
<p>并与 SGD/GRPO 的对应上界比较，给出“遗忘-漂移”定量关系。</p>
<ol>
<li>统计稳定性提升</li>
</ol>
<ul>
<li>扩大种群（30 → 128 → 512）与减半学习率，观察遗忘曲线方差是否显著下降；给出“种群大小-遗忘方差”经验公式，为工业部署提供选型依据。</li>
</ul>
<ol>
<li>遗忘评测体系扩展</li>
</ol>
<ul>
<li>引入更多旧任务探针（MMLU、ARC、TruthfulQA）与参数探测方法（LiNGAM、CKA），检查遗忘是否具“任务特异性”或“全局通用性”。</li>
<li>记录层-wise 遗忘速率，找出 ES 最先干扰的表征层，为后续选择性正则提供目标。</li>
</ul>
<p>通过上述探索，可逐步把 ES 从“内存友好但易遗忘”的基线，推向“内存友好且持续稳健”的实用在线学习方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>研究背景</strong><br>大模型部署后需持续学习，但梯度类后训练（SFT、RLHF、GRPO）内存开销大；进化策略（ES）作为梯度无参替代方案近期被重新关注，声称性能可比 GRPO 且极省内存，然而其是否会灾难性遗忘尚缺系统评估。</p>
</li>
<li><p><strong>核心发现</strong></p>
</li>
</ul>
<ol>
<li>在 Countdown、GSM8K、MATH、OlympiadBench 上，ES 峰值准确率仅落后 GRPO 3–4 个百分点，计算步数相近，确认其“性能可比”。</li>
<li>在同一微调 run 内，ES 新任务收敛后旧任务（HellaSwag）准确率仍持续下降约 10 %；GRPO 几乎无遗忘。</li>
<li>诊断表明 ES 更新 Frobenius 范数比 GRPO 大 <strong>1000×</strong>，稀疏度 &lt; 20 %（GRPO ≈ 95 %），全局密集漂移导致旧能力被破坏。</li>
</ol>
<ul>
<li><strong>结论</strong><br>ES 虽提供内存友好的梯度无参路径，但单任务微调即出现显著灾难性遗忘，目前不适合需要保留旧能力的在线学习场景；未来需引入稀疏化、范数约束或 KL 正则等机制以缓解遗忘。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs，链接是https://arxiv.org/pdf/2601.20861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20861。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20861v1.pdf">https://arxiv.org/pdf/2601.20861v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20861v1">https://papers.cool/arxiv/2601.20861v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:59:34Z</p>
<hr>
<h3 id="3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching-1"><a href="#3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching-1" class="headerlink" title="3. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching"></a>3. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15942v2">ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</a></h3><p>Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>气候预测存在与气候系统组成部分及其相互作用相关的不确定性。量化这些不确定性的一种典型方法是使用气候模型在不同初始条件下创建重复模拟的集合。由于这些模拟的复杂性，生成此类预测集合在计算上代价高昂。在本工作中，我们提出了ArchesClimate，一种基于深度学习的气候模型仿真器，旨在降低这一成本。ArchesClimate在IPSL-CM6A-LR气候模型的十年重预测数据上进行训练，空间分辨率约为2.5x1.25度。我们按照ArchesWeatherGen的方法训练一个流匹配模型，并将其改编为预测近期气候。一旦训练完成，该模型即可生成一个月的预测状态，并可用于自回归地仿真任意长度的气候模型模拟。我们展示了在长达10年的时间尺度上，这些生成结果是稳定且符合物理规律的。我们还显示，对于多个重要气候变量，ArchesClimate生成的模拟结果可以与IPSL模型互换。本研究表明，气候模型仿真器可能显著降低气候模型模拟的成本。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在以极低计算成本生成气候模式 IPSL-CM6A-LR 的十年尺度概率集合，从而替代传统“多次扰动初值 + 全模式积分”这一昂贵流程，支撑年代际气候变率与不确定性的量化研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>NeuralGCM</strong>（Kochkov et al. 2024）<br>混合物理-深度学习自回归大气集合 emulator，在网络内部注入噪声以产生内部变率。</p>
</li>
<li><p><strong>Spherical DYffusion</strong>（Cachay et al. 2023）<br>基于扩散模型的概率气候 emulator，显式对大气状态分布进行建模。</p>
</li>
<li><p><strong>cBottle</strong>（Brenowitz et al. 2025）<br>生成式扩散模型，学习多源再分析资料的瞬时短期动力，但非自回归，无法延伸为长序列。</p>
</li>
<li><p><strong>ACE / ACE2</strong>（Watt-Meyer et al. 2023, 2024）<br>确定性大气 emulator，可稳定积分千年，但不包含随机性，也未耦合海洋。</p>
</li>
<li><p><strong>LUCIE</strong>（Guan et al. 2024）<br>轻量级非耦合气候 emulator，以 ERA5 为训练数据，侧重高效生成大集合。</p>
</li>
<li><p><strong>DLESyM</strong>（Cresswell-Clay et al. 2025）<br>耦合大气-海洋深度学习地球系统模型，用 ERA5 训练，验证 AI 同时模拟两圈层的可能性。</p>
</li>
<li><p><strong>SamudrACE</strong>（Duncan et al. 2025）<br>将确定性海洋 emulator Samudra 与 ACE2 耦合，实现快速海-气交换模拟。</p>
</li>
<li><p><strong>ACE-SOM2</strong>（Clark et al. 2024）<br>ACE2 与单层海洋耦合，用于探究不同强迫下的气候响应，向更长尺度与不同情景扩展。</p>
</li>
</ul>
<p>这些研究共同构成了“AI 气候模式替代”脉络，为 ArchesClimate 在概率性、十年尺度、海-气耦合与计算效率上的设计提供了直接对比与参考。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 ArchesClimate，一套基于**流匹配（flow matching）**的深度生成框架，以月时间步长、≈2.5°×1.25° 空间分辨率自回归地模拟 IPSL-CM6A-LR 的十年集合。核心思路与步骤如下：</p>
<ol>
<li><p>数据利用<br>采用 IPSL-CM6A-LR 对 CMIP6-DCPP 的 55 组 10 年、10 成员回报试验（≈7×10⁴ 个月），仅选取 24 个关键海-气变量（含 4 层大气、7 个海洋整层量）及 5 种辐射强迫（CO₂、CH₄、N₂O、CFC11eq、SSI）作为训练集。</p>
</li>
<li><p>两阶段架构</p>
</li>
</ol>
<ul>
<li><strong>确定性模型</strong>  f<em>θ ：3D-Swin-U-Net 结构，输入  X_t 、 X</em>(t-δ)  与强迫，输出下月平均态  hat X_(t+δ) 。</li>
<li><strong>生成模型</strong>  g_θ ：同骨干网络，以 flow matching 学习残差分布</li>
</ul>
<p>r<em>(t+δ)= X</em>(t+δ) - f<em>θ(X_t,X</em>(t-δ))σ</p>
<p>训练时从  mathcal N(0,I)  到残差分布建立可逆映射；推理时从随机噪声出发，经  M=12  步 ODE 积分生成样本残差，再加回确定性预测得到完整状态。</p>
<ol>
<li><p>强迫嵌入<br>对每条辐射强迫标量，先经线性嵌入生成 scale/shift 参数，再以<strong>条件层归一化</strong>注入所有 Transformer 块，使模型可在不同强迫情景下动态调整。</p>
</li>
<li><p>自回归集合生成<br>训练完成后，用  X<em>0 、 X</em>(-1)  初始化，交替调用  f<em>θ  与  g</em>θ  逐月外推；每次从独立高斯噪声采样即可产生不同成员，实现 10 年、任意大小集合的“一键”输出。</p>
</li>
<li><p>计算加速<br>模型仅作用于 144×144×45 的粗网格，参数空间约为原模式 1/400；在 4×A100 上 12 min 可生成 10 年 10 成员集合，对应 ≈4 core-h/模拟年，而 IPSL-CM6A-LR 需 ≈1900 core-h/模拟年。</p>
</li>
</ol>
<p>通过“确定性骨架 + 流匹配残差”策略，ArchesClimate 在保持物理一致性、季节-年际变率与低频趋势的同时，把传统气候模式集合成本降低两个数量级，从而解决了“十年尺度概率集合昂贵”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否以极低算力生成与 IPSL-CM6A-LR 统计-物理特性一致的十年集合”这一核心问题，设计了 10 组实验，覆盖统计评分、物理一致性、可交换性、方差调校与长期稳定性等维度。实验按目的可归纳为以下 6 类：</p>
<ol>
<li>基础统计评估</li>
</ol>
<ul>
<li><strong>CRPS 与标准差对比</strong>（表 3）<br>对 1969-79、1979-89、2010-20 三个十年，计算 8 个关键变量（tos、psl、net_flux、thetaot2000 等）的纬度加权 CRPS 与集合标准差，比较 ArchesClimate、IPSL-DCPP 与 Pattern-Scaling 基线。</li>
<li><strong>方差-CRPS 消融</strong>（图 5）<br>对比三种训练配置：<br>– residual-flow（本文方案）<br>– full-deterministic（仅  f<em>θ ）<br>– full-flow（仅  g</em>θ ，无确定性骨架）<br>量化确定性骨架对精度与方差的贡献。</li>
</ul>
<ol>
<li>集合可交换性（rank histogram）</li>
</ol>
<ul>
<li><strong>北大西洋区域 rank histogram</strong>（图 6 左）<br>1969 起报的 10 成员、10 年序列，对 6 个变量逐格点排序，检验 ArchesClimate 成员是否能被视为从 IPSL 分布中独立同分布抽取。</li>
</ul>
<ol>
<li>频谱与变率结构</li>
</ol>
<ul>
<li><strong>时间功率谱（TPS）对比</strong>（图 6 右）<br>对同一北大西洋 10 年异常序列做逐点傅里叶变换，比较月-季-年周期强度，诊断 ArchesClimate 是否欠变率（under-dispersion）。</li>
</ul>
<ol>
<li>长期趋势与遥相关</li>
</ol>
<ul>
<li><strong>线性趋势空间相关</strong>（图 7）<br>计算 5 成员集合的 tos 异常 10 年趋势，逐像素与 IPSL 对照，评估年代际信号在三大洋及北极的相关性。</li>
<li><strong>50 年强迫响应</strong>（附录图 A1）<br>固定 1969 强迫 vs 时变强迫各 rollout 50 年，验证模型能否随辐射强迫漂移而非崩溃。</li>
</ul>
<ol>
<li>区域/季节信号</li>
</ol>
<ul>
<li><strong>热带区域气候态与异常</strong>（图 4）<br>20°S–20°N 平均的季节循环与异常时间序列，对比集合均值与 ±1σ 包络。</li>
<li><strong>北大西洋季节异常空间图</strong>（图 8）<br>MAM/JJA/SON/DJF 四季平均 tos 异常的空间格局对比，检验季节尺度空间相位与强度。</li>
</ul>
<ol>
<li>方差增强与损失设计</li>
</ol>
<ul>
<li><strong>三种增方差策略</strong>（图 9）<br>– 推理初始噪声放大 1.1×<br>– 按变量训练-验证方差差值逐变量缩放噪声<br>– 在 flow-matching 损失中加入梯度与功率谱项（ L<em>(rm total)=L</em>(rm MSE)+0.2L<em>(rm grad)+0.2L</em>(rm PSD) ）<br>比较其对 CRPS-方差权衡的影响。</li>
</ul>
<p>以上实验共同表明：</p>
<ul>
<li>ArchesClimate 的 CRPS 与 IPSL 相当或更低，但方差普遍略小；</li>
<li>加入谱-梯度损失可把方差提升到 IPSL 水平，代价是 CRPS 略增；</li>
<li>rank histogram 在 net_flux、wap 等变量接近均匀，tos、thetaot2000 存在低值偏移；</li>
<li>热带与北大西洋季节空间异常被忠实再现，年代际趋势在三大洋遥相关区相关系数 &gt;0.8；</li>
<li>50 年 rollout 仍能跟随辐射强迫漂移，未见数值漂移或崩溃。</li>
</ul>
<p>因此，论文从统计、物理、频谱、长期稳定性四个角度系统验证了“低成本 AI 集合”替代“高成本模式集合”的可行性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>高分辨率时空降尺度</strong><br>以 ArchesClimate 月尺度 2.5° 输出为条件，训练条件扩散或超分模型，生成 1 km-日级别区域情景，支持极端事件归因与局地影响评估。</p>
</li>
<li><p><strong>多世纪长序列外推</strong><br>将训练数据从 55×10 年扩展至 CMIP6 “historical” 与 SSP 几百年试验，检验对 50-100 年低频变率（AMO、PDO、IPO）与强迫响应的保真度。</p>
</li>
<li><p><strong>增强海洋与冰冻圈变量</strong><br>引入海冰厚度/面积、陆地冰盖、三维盐度与洋流、植被动态，减少北极趋势漂移并提升碳循环一致性。</p>
</li>
<li><p><strong>守恒与物理约束嵌入</strong><br>在损失或网络层显式加入大气静力平衡、柱总水量/能量守恒、海洋热盐守恒，抑制长期能量泄漏。</p>
</li>
<li><p><strong>极端与尾部分布校准</strong><br>采用尾部分位数映射或极端值理论对 flow-matching 输出进行事后校正，提高热浪、干旱、极端降水重现期精度。</p>
</li>
<li><p><strong>单步/少步生成加速</strong><br>引入 Consistency-Model 或 Rectified-Flow 单步采样，把 12 步推理降至 1-2 步，再降一个数量级推理耗时。</p>
</li>
<li><p><strong>跨模式迁移与多模式集合</strong><br>用 IPSL 训练的模型权重初始化，微调至 CESM2、MRI-ESM2 等，实现“零成本”多模式集合加权平均。</p>
</li>
<li><p><strong>初始化信息再利用</strong><br>将海洋同化态（三维温盐）或海冰初值编码为条件向量，评估初始化记忆对 1-5 年预测技巧的贡献。</p>
</li>
<li><p><strong>强迫情景插值与反事实试验</strong><br>在嵌入空间对 CO₂、SSI 做线性或非线性插值，生成未运行过的强迫路径（如 1.5 °C 过冲、火山群喷发），快速筛查气候响应。</p>
</li>
<li><p><strong>谱方差自适应损失</strong><br>按纬度-变量-频段动态加权功率谱损失，缓解年循环“过拟合”而高频变率不足的问题，实现方差-精度自动权衡。</p>
</li>
<li><p><strong>不确定性分解</strong><br>利用大样本（10³-10⁴ 成员）将总不确定性拆分为内部变率、模型结构、参数与强迫，对照传统大集合进行方差分解验证。</p>
</li>
<li><p><strong>在线数据同化闭环</strong><br>将 ArchesClimate 作为“廉价预报核”，与 Ensemble Kalman Filter 结合，实现实时年代际预测系统，每月滚动更新。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>ArchesClimate：用流匹配实现十年尺度概率气候集合的快速生成</strong></p>
<ol>
<li><p>问题<br>传统气候模式需对初值扰动运行数十次才能量化十年内部变率，单成员 10 年 ≈ 1900 core-h，成本高昂。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>数据：IPSL-CM6A-LR 的 DCPP 回报（55×10 年×10 成员，≈7×10⁴ 月），24 个海-气变量 + 5 种辐射强迫。</li>
<li>架构：两阶段<br>– 确定性 Swin-U-Net  f<em>θ  预测下月平均态；<br>– 流匹配生成网络  g</em>θ  学习残差分布，12 步 ODE 从高斯噪声采样。</li>
<li>训练：先训  f<em>θ  再训  g</em>θ ，损失为逐网格 MSE（纬度加权）+ 可选谱/梯度正则。</li>
<li>推理：自回归 rollout，10 年 10 成员在 4×A100 上 12 min 完成（4 core-h/年）。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>统计：CRPS 与 IPSL 相当，方差略低；加入谱损失可把方差提升至 IPSL 水平。</li>
<li>可交换性：net_flux、wap 的 rank histogram 接近均匀，tos/thetaot2000 略偏低值。</li>
<li>频谱：年-季周期准确，月-年际功率略低。</li>
<li>趋势：三大洋年代际趋势空间相关 &gt;0.8，北极因无冰过程快速退相关。</li>
<li>长期：50 年 rollout 仍能跟随辐射强迫漂移，无崩溃。</li>
<li>区域：热带与北大西洋季节异常空间相位与观测一致。</li>
</ul>
<ol>
<li><p>结论<br>ArchesClimate 以 ≈ 1/400 参数规模、1/500 计算成本，生成与 IPSL-CM6A-LR 统计-物理特性高度一致的十年集合，为年代际气候不确定性与极端事件研究提供可负担的“AI 集合引擎”。</p>
</li>
<li><p>展望<br>高分辨率降尺度、多世纪长序列、守恒约束、极端校正、单步采样及跨模式迁移等方向可进一步提升可用性与物理可信度。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ArchesClimate%3A%20Probabilistic%20Decadal%20Ensemble%20Generation%20With%20Flow%20Matching，链接是https://arxiv.org/pdf/2509.15942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.15942。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis</p>
<p><strong>Categories</strong>: physics.ao-ph, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15942v2.pdf">https://arxiv.org/pdf/2509.15942v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15942v2">https://papers.cool/arxiv/2509.15942v2</a></p>
<p><strong>Published</strong>: 2025-09-19T12:53:24Z</p>
<hr>
<h3 id="4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems-1"><a href="#4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems-1" class="headerlink" title="4. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems"></a>4. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06052v3">DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</a></h3><p>Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>离散组合问题（DCP）在工业决策和优化中非常常见。然而，尽管针对DCP的约束求解技术已经取得了显著进展，其核心过程——形式化问题，即约束建模，仍需要大量专业知识，并且是广泛应用的瓶颈。为了缓解这一瓶颈，近期研究探索了使用大型语言模型（LLM）将组合问题描述转化为可执行的约束模型。然而，现有的离散约束建模评估数据集通常仅限于小型、同质化或特定领域的问题，无法反映现实世界场景的多样性。本文通过引入DCP-Bench-Open来填补这一空白，这是一个新型基准，包含来自约束规划（CP）和运筹学（OR）社区的多种知名离散组合问题，并专门针对LLM驱动的约束建模进行了结构化设计。利用该数据集，并考虑不同建模框架的多样性，我们对三种不同的约束建模系统进行了对比和评估，这些系统在抽象层次和底层语法上各不相同。值得注意的是，结果显示，当使用基于Python的高级框架进行建模时，性能更高。此外，我们系统评估了基于提示和推理时计算方法在不同LLM上的应用，这进一步提高了准确率，在这一高难度基准上最高可达91%。DCP-Bench-Open已公开发布。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决的问题是如何利用大型语言模型（LLMs）来简化约束编程（Constraint Programming, CP）中的约束建模过程，从而降低约束编程的使用门槛，使其能够被更广泛的非专家用户所接受和应用。具体而言，论文的目标包括：</p>
<ol>
<li><p><strong>解决约束建模的瓶颈问题</strong>：约束编程是一种解决组合优化问题的有效范式，但其核心过程——约束建模——非常复杂，需要深厚的应用领域知识和对建模框架的熟练掌握。这限制了约束编程技术的广泛应用。论文探索使用LLMs作为建模助手，将自然语言描述的组合问题转化为可执行的约束模型，类似于现有的基于LLMs的代码生成助手。</p>
</li>
<li><p><strong>提供一个多样化的基准数据集</strong>：现有的用于评估LLMs在约束建模能力上的数据集往往存在局限性，例如规模小、同质化或特定于某个领域，无法涵盖现实世界场景的多样性。论文通过引入一个新的基准数据集CP-Bench，填补了这一空白。CP-Bench包含了一系列来自CP社区的多样化、知名的组合问题类别，专门用于评估LLMs驱动的CP建模能力。</p>
</li>
<li><p><strong>评估不同约束建模框架对LLMs性能的影响</strong>：论文比较和评估了LLMs在三种不同抽象层次和底层语法的约束建模系统中的建模能力，这些系统包括高级别的MiniZinc语言、基于Python的CPMpy库，以及较低级别的Python接口OR-Tools CP-SAT求解器。通过这种比较，论文旨在了解不同框架对LLMs建模准确性的具体影响。</p>
</li>
<li><p><strong>探索提升LLMs生成有效约束模型能力的方法</strong>：为了提高LLMs生成有效约束模型的能力，论文系统地评估了基于提示（prompt-based）和推理时计算（inference-time compute）方法在约束建模中的应用。这些方法包括从现有基于LLMs的代码生成研究中借鉴而来的技术，如检索增强型上下文学习（Retrieval-Augmented In-Context Learning, RAICL）、重复采样和自我验证等。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了多个与利用大型语言模型（LLMs）进行约束建模相关的研究工作，这些研究主要集中在以下几个方面：</p>
<p>约束编程与大型语言模型的结合</p>
<ul>
<li><strong>NL4Opt Competition</strong>：Ramamonjison等人在NeurIPS 2022竞赛中提出了NL4Opt数据集，这是一个包含简单线性规划（LP）问题的基准，用于评估LLMs将自然语言问题描述转化为优化模型的能力<br>^28^<br>。</li>
<li><strong>Logic Grid Puzzles (LGPs)</strong>：Jabrayilzade和Tekir提出了一个用于解决逻辑网格谜题的数据集，这些谜题可以看作是组合问题的一个特定子集<br>^14^<br>。</li>
<li><strong>Text2Zinc</strong>：Singirikonda等人提出了一个跨领域的数据集，包含MiniZinc模型，主要关注线性规划（LP）和混合整数规划（MIP）问题<br>^31^<br>。</li>
</ul>
<p>大型语言模型在代码生成中的应用</p>
<ul>
<li><strong>Evaluating Large Language Models Trained on Code</strong>：Chen等人研究了LLMs在代码生成方面的表现，特别是在生成Python代码方面的能力<br>^4^<br>。</li>
<li><strong>Automated Repair of Programs from Large Language Models</strong>：Fan等人探讨了LLMs在自动修复程序中的应用，这与LLMs在约束建模中的自我验证和错误检测有相似之处<br>^7^<br>。</li>
</ul>
<p>大型语言模型的推理时计算方法</p>
<ul>
<li><strong>Scaling LLM Test-Time Compute</strong>：Snell等人研究了在测试时增加LLMs的计算量以提高性能的方法，这在论文中被应用于约束建模任务中<br>^32^<br>。</li>
<li><strong>Teaching Large Language Models to Self-Debug</strong>：Chen等人提出了让LLMs进行自我调试的方法，这与论文中探讨的自我验证方法有相似之处<br>^5^<br>。</li>
</ul>
<p>约束编程的其他相关工作</p>
<ul>
<li><strong>Global Constraint Catalogue</strong>：Beldiceanu等人提供了约束编程中全局约束的详细目录，这对于理解约束编程的多样性和复杂性非常重要<br>^1^<br>。</li>
<li><strong>Applications of Combinatorial Optimization</strong>：Paschos的著作提供了组合优化在多个领域的应用实例，强调了约束编程在实际问题中的重要性<br>^24^<br>。</li>
</ul>
<p>这些相关研究为本文提供了背景和基础，同时也指出了现有研究的局限性和改进方向。例如，现有数据集在多样性和复杂性上的不足，以及在不同约束建模框架下LLMs性能的差异等。本文通过引入CP-Bench数据集和系统地评估不同框架和方法，旨在推动这一领域的研究进展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下几个主要步骤来解决如何利用大型语言模型（LLMs）简化约束编程（CP）中的约束建模过程的问题：</p>
<p>1. 构建多样化的基准数据集 CP-Bench</p>
<ul>
<li><strong>数据集来源</strong>：论文从多个来源收集了101个多样化的组合问题，包括CSPLib、CPMpy在线示例库、Håkan Kjellerstrand的在线资源库以及一个基于课程的问题集<br>^21^<br>。</li>
<li><strong>数据集结构</strong>：每个数据集实例都被转化为一个自包含的Python文件，包含元数据、问题描述、输入数据（可选）、模型代码和解决方案打印代码<br>^5^<br>。</li>
<li><strong>评估方法</strong>：论文采用解决方案准确性（solution accuracy）作为评估指标，即通过执行生成的代码来验证其解决方案是否满足真实模型的约束和目标函数<br>^5^<br>。</li>
</ul>
<p>2. 评估不同约束建模框架对LLMs性能的影响</p>
<ul>
<li><strong>选择框架</strong>：论文选择了三个具有不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）<br>^1^<br>。</li>
<li><strong>系统提示（System Prompts）</strong>：为了指导LLMs更有效地进行建模，论文设计了三种不同详细程度的系统提示，从基本的建模任务描述到包含框架文档的详细提示<br>^4.1^<br>。</li>
<li><strong>实验结果</strong>：实验表明，使用Python基础框架（CPMpy和OR-Tools）生成模型的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
</ul>
<p>3. 探索提升LLMs生成有效约束模型能力的方法</p>
<ul>
<li><strong>检索增强型上下文学习（RAICL）</strong>：通过从预构建的数据库中检索与当前问题语义相似的输入-输出对来丰富提示上下文<br>^4.2^<br>。然而，实验结果表明，这种方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>重复采样（Repeated Sampling）</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型<br>^4.3^<br>。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证（Self-Verification）</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性<br>^4.4^<br>。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
</ul>
<p>4. 结合方法以进一步提高性能</p>
<ul>
<li><strong>综合方法</strong>：论文将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>通过上述步骤，论文不仅提供了一个多样化的基准数据集来评估LLMs在约束建模中的能力，还系统地比较了不同建模框架对LLMs性能的影响，并探索了多种方法来提高LLMs生成有效约束模型的能力。这些研究成果为降低约束编程的使用门槛、使其更易于被非专家用户接受和应用提供了有力的支持。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验来评估大型语言模型（LLMs）在约束建模中的表现：</p>
<p>实验一：不同建模框架下的LLMs性能评估</p>
<ul>
<li><strong>目的</strong>：比较LLMs在不同约束建模框架下的性能，以确定哪种框架更适合LLMs生成约束模型。</li>
<li><strong>方法</strong>：选择了三种不同的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。使用CP-Bench数据集中的101个组合问题，对8种不同的LLMs进行了评估<br>^6.1^<br>。</li>
<li><strong>结果</strong>：Python基础框架（CPMpy和OR-Tools）的性能优于领域特定语言（MiniZinc）。具体来说，CPMpy在详细系统提示（Level 3）下达到了最高的准确率<br>^6.2.1^<br>。这表明Python基础框架更适合LLMs生成约束模型。</li>
</ul>
<p>实验二：不同系统提示对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同详细程度的系统提示对LLMs性能的影响。</li>
<li><strong>方法</strong>：设计了三种不同详细程度的系统提示：基本提示（Level 1）、包含建模指南的提示（Level 2）和包含框架文档的详细提示（Level 3）。在三种不同的约束建模框架下，对8种LLMs进行了评估<br>^4.1^<br>。</li>
<li><strong>结果</strong>：提供更详细的系统提示（尤其是包含框架文档的提示）能够提高LLMs的建模准确性<br>^6.2.2^<br>。这表明详细的系统提示有助于LLMs更好地理解和生成约束模型。</li>
</ul>
<p>实验三：推理时计算方法对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同的推理时计算方法对LLMs性能的影响。</li>
<li><strong>方法</strong>：测试了四种不同的推理时计算方法：基线方法（直接评估LLMs生成的第一个模型）、检索增强型上下文学习（RAICL）、重复采样（Repeated Sampling）和自我验证（Self-Verification）。选择了4种LLMs（gpt-4.1-mini、DeepSeek-V3、QwQ-32B、Llama-3.3-70B）进行评估<br>^6.2.3^<br>。</li>
<li><strong>结果</strong>：</li>
<li><strong>RAICL</strong>：在上下文中添加示例对LLMs的性能没有帮助，反而降低了准确性<br>^6.2.3^<br>。</li>
<li><strong>重复采样</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
<li><strong>综合方法</strong>：将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>实验四：错误类型分析</p>
<ul>
<li><strong>目的</strong>：分析LLMs在生成约束模型时所犯错误的类型，以了解不同系统提示对错误的影响。</li>
<li><strong>方法</strong>：将错误分为两类：可检测错误（如代码无法执行、输出解析问题或超时）和建模错误（如生成的模型不可满足或产生错误的解决方案）。对不同系统提示配置下的错误进行了统计<br>^6.2.2^<br>。</li>
<li><strong>结果</strong>：随着系统提示详细程度的增加，可检测错误减少，而建模错误增加<br>^6.2.2^<br>。这表明更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误。</li>
</ul>
<p>这些实验综合评估了LLMs在约束建模中的性能，并探索了不同建模框架、系统提示和推理时计算方法对性能的影响。实验结果为如何利用LLMs简化约束编程的建模过程提供了重要的见解。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文在探索利用大型语言模型（LLMs）进行约束建模方面已经取得了显著的成果，但仍有一些可以进一步探索的点，以推动这一领域的研究进展。以下是一些潜在的研究方向：</p>
<p>1. <strong>多轮交互式建模</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在单次生成模型的准确性上，但在实际应用中，建模过程往往需要多次迭代和用户反馈。</li>
<li><strong>研究方向</strong>：探索多轮交互式建模，允许用户和系统之间进行多次交互，逐步改进和细化模型。这可以通过设计更复杂的对话系统来实现，系统可以根据用户的反馈动态调整模型<br>^7^<br>。</li>
</ul>
<p>2. <strong>工业级问题的应用</strong></p>
<ul>
<li><strong>背景</strong>：现有的基准数据集（如CP-Bench）虽然多样化，但主要来源于学术领域，与实际工业问题仍有差距。工业问题通常涉及更多的数据、更复杂的约束和更详细的描述。</li>
<li><strong>研究方向</strong>：与工业界合作，收集和构建更接近实际应用场景的组合问题数据集。这将有助于评估LLMs在处理更复杂、更实际问题时的表现<br>^24^<br>。</li>
</ul>
<p>3. <strong>模型效率优化</strong></p>
<ul>
<li><strong>背景</strong>：虽然论文主要关注模型的正确性，但在实际应用中，模型的求解效率同样重要。高效的模型可以显著减少求解时间和计算资源的消耗。</li>
<li><strong>研究方向</strong>：探索如何利用LLMs学习生成更高效的约束模型，例如通过引入启发式方法、动态约束调整或模型重构技术<br>^32^<br>。</li>
</ul>
<p>4. <strong>监督式微调</strong></p>
<ul>
<li><strong>背景</strong>：论文中提到，虽然CP-Bench数据集为评估LLMs提供了基础，但目前的数据集主要用于测试，而不是训练。</li>
<li><strong>研究方向</strong>：构建一个包含约束模型和问题描述的训练数据集，用于对LLMs进行监督式微调。这可能需要大量的标注数据，但有望进一步提高LLMs在约束建模任务中的性能<br>^4^<br>。</li>
</ul>
<p>5. <strong>跨领域知识迁移</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在训练过程中接触了大量的文本数据，这些数据可能包含不同领域的知识。然而，如何有效地将这些跨领域的知识迁移到约束建模任务中仍是一个未解决的问题。</li>
<li><strong>研究方向</strong>：研究如何利用LLMs的跨领域知识，通过迁移学习或领域适应技术，提高其在特定约束建模任务中的表现<br>^14^<br>。</li>
</ul>
<p>6. <strong>模型的可解释性</strong></p>
<ul>
<li><strong>背景</strong>：在实际应用中，除了模型的准确性和效率外，模型的可解释性也非常重要。用户需要理解模型是如何工作的，以及为什么会产生特定的解决方案。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的可解释性，例如通过生成详细的解释文本或可视化工具<br>^5^<br>。</li>
</ul>
<p>7. <strong>与其他AI技术的结合</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在约束建模中的应用可以与其他AI技术（如机器学习、强化学习等）相结合，以进一步提高建模和求解的效率和准确性。</li>
<li><strong>研究方向</strong>：研究如何将LLMs与这些技术相结合，例如通过强化学习来优化模型的生成过程，或者利用机器学习模型来预测模型的性能<br>^1^<br>。</li>
</ul>
<p>8. <strong>模型的泛化能力</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在特定的数据集和问题类型上，但实际应用中需要模型具有更强的泛化能力，能够处理各种不同类型的问题。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的泛化能力，例如通过引入更多的数据增强技术、设计更通用的建模框架或开发新的评估指标<br>^28^<br>。</li>
</ul>
<p>这些研究方向不仅有助于进一步提高LLMs在约束建模中的表现，还可能推动约束编程技术在更广泛领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了CP-Bench，这是一个用于评估大型语言模型（LLMs）在约束建模能力上的新基准数据集。文章的核心目标是解决约束编程（CP）中约束建模的瓶颈问题，通过利用LLMs将自然语言描述的组合问题转化为可执行的约束模型，从而降低CP的使用门槛，使其更易于被非专家用户接受和应用。以下是文章的主要内容概述：</p>
<p>背景知识</p>
<ul>
<li><strong>组合问题与约束编程</strong>：组合问题在众多实际应用中普遍存在，如物流、调度和网络设计等。CP作为一种解决问题的范式，通过声明式方法让用户指定解决方案必须满足的约束，而不是详细说明寻找解决方案的具体步骤。然而，将问题描述转化为正式的CP模型的过程复杂，需要深厚的应用领域知识和对建模框架的熟练掌握，这限制了CP技术的广泛应用<br>^1^<br>。</li>
<li><strong>大型语言模型的应用</strong>：近年来，LLMs在自然语言处理和代码生成方面取得了显著进展，为创建能够辅助或自动化建模过程的工具提供了可能。然而，将LLMs应用于约束建模面临着独特挑战，如声明式建模的复杂性、建模选择的组合性质以及验证生成模型正确性的困难<br>^4^<br>。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>CP-Bench数据集</strong>：为了解决现有评估数据集在多样性和复杂性上的不足，作者构建了CP-Bench，包含101个来自CP社区的多样化组合问题。这些问题涵盖了优化和满足问题，具有广泛的决策变量、约束和241种不同的约束类型<br>^5^<br>。</li>
<li><strong>建模框架的比较</strong>：文章比较了三种不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。通过这些框架，作者评估了LLMs在不同条件下的建模能力<br>^1^<br>。</li>
<li><strong>系统提示与推理时计算方法</strong>：为了提高LLMs生成有效约束模型的能力，文章系统地评估了基于提示和推理时计算的方法。这些方法包括不同详细程度的系统提示、检索增强型上下文学习（RAICL）、重复采样和自我验证<br>^4^<br>。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>实验设置</strong>：作者选择了8种不同的LLMs进行评估，这些模型在参数数量、上下文窗口大小、是否包含推理能力等方面存在差异<br>^6.1^<br>。</li>
<li><strong>实验结果</strong>：</li>
<li><strong>建模框架的影响</strong>：实验结果表明，使用Python基础框架（CPMpy和OR-Tools）的LLMs在生成约束模型时的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
<li><strong>系统提示的作用</strong>：更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误增加<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的效果</strong>：重复采样和自我验证方法显著提高了LLMs的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>Python基础框架的优势</strong>：Python基础框架（CPMpy和OR-Tools）更适合LLMs生成约束模型，这可能与LLMs训练过程中接触的大量Python代码有关<br>^6.2.1^<br>。</li>
<li><strong>系统提示的重要性</strong>：详细的系统提示，尤其是包含框架文档的提示，能够提高LLMs的建模准确性<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的有效性</strong>：重复采样和自我验证方法能够显著提高LLMs在约束建模任务中的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>LLMs在约束建模中的潜力与局限</strong>：尽管LLMs在约束建模中展现出了潜力，但仍然需要人类专家的验证，尤其是在关键任务中。文章中最强的性能配置（结合重复采样和自我验证的gpt-4.1-mini模型）达到了70%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>未来工作</p>
<p>文章提出了未来可能的研究方向，包括构建更接近实际应用场景的组合问题数据集、探索多轮交互式建模、研究模型效率优化、进行监督式微调、实现跨领域知识迁移、提高模型的可解释性、结合其他AI技术以及增强模型的泛化能力<br>^7^<br>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling，链接是https://arxiv.org/pdf/2506.06052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06052。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kostis Michailidis, Dimos Tsouros, Tias Guns</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06052v3.pdf">https://arxiv.org/pdf/2506.06052v3.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06052v3">https://papers.cool/arxiv/2506.06052v3</a></p>
<p><strong>Published</strong>: 2025-06-06T12:56:02Z</p>
<hr>
<h3 id="5-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models"><a href="#5-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models" class="headerlink" title="5. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models"></a>5. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20856v1">SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</a></h3><p>Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型的能力在复杂推理任务上得到了越来越多的测试，但它们的长远规划能力尚未得到广泛研究。在本工作中，我们对最先进的大型推理模型（LRMs）的规划和长远推理能力进行了系统评估。我们提出了一个基于推箱子（Sokoban）谜题的新型基准测试，该测试经过简化，以将长远规划与状态持久性分离。我们的研究发现，当需要超过25步才能到达解决方案时，规划性能会持续下降，这表明前向规划能力存在根本性限制。我们还表明，为LRMs配备规划领域定义语言（PDDL）的解析、验证和求解工具能够带来适度的改进，这表明其架构存在固有限制，仅靠测试时的扩展可能无法克服这些限制。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统评估当前大型推理模型（Large Reasoning Models, LRMs）在**长程规划（long-horizon planning）<strong>与</strong>长程推理（long-horizon reasoning）**上的能力边界。核心问题可以概括为：</p>
<blockquote>
<p><strong>当解空间深度增加、但分支因子被刻意压到最低时，LRMs 是否仍能维持连贯、可行且最优的动作序列？</strong></p>
</blockquote>
<p>为隔离“深度”与“复杂性”因素，作者构造了极简的一维 Sokoban 走廊谜题：</p>
<ul>
<li>仅 1 个箱子、1 个目标、1 条直线通道；</li>
<li>唯一可变参数是走廊长度 ℓ，它直接决定最优解长度（≈ℓ 步）；</li>
<li>无外部记忆、无中间反馈、无状态验证，完全依赖模型内部表示。</li>
</ul>
<p>通过该基准，论文试图回答以下子问题：</p>
<ol>
<li>模型在无需外部工具时，究竟能稳定规划到多远的 horizon？</li>
<li>当 horizon 超过某一阈值（≈25–30 步）时，失效模式是“搜索错误”还是“更底层的计数/状态跟踪错误”？</li>
<li>若给模型提供 PDDL 解析器与外部求解器（LLM-Modulo 框架），能否弥补其固有缺陷？</li>
<li>上述改进若存在，是否足以克服长程动作表示与空间 grounding 的瓶颈？</li>
</ol>
<p>综上，论文并非追求更复杂的规划域，而是<strong>用极简环境放大“深度”维度</strong>，从而暴露 LRMs 在长序列状态一致性上的结构性局限，并验证“测试时扩展+符号工具”是否足以突破该局限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四大脉络，均围绕“LLM 规划能力”与“Sokoban 作为诊断环境”展开：</p>
<ol>
<li><strong>通用规划基准与 LLM 失效诊断</strong></li>
</ol>
<ul>
<li><strong>PlanBench</strong>（Valmeekam et al., 2022, 2023a, 2025）：PDDL 格式的 BlocksWorld、Logistics 等经典域，发现 LLM 随 horizon 增加迅速退化，且错误集中在无效动作与循环。</li>
<li><strong>8-puzzle 文本版</strong>（Schepanowski &amp; Ling, 2025）：证明即使状态空间仅 181440，LLM 仍因“弱内部状态表示”与“误导式启发搜索”而频繁产生非法移动或提前终止。</li>
<li><strong>字符级计数任务</strong>（Karpathy, 2024；Xu &amp; Ma, 2025；CharBench, Uzan &amp; Pinter, 2025）：揭示“strawberry 有几个 r”类失败源于 tokenization 之外的长度-计数表征缺陷，与本文“走廊计数漂移”论点直接呼应。</li>
</ul>
<ol>
<li><strong>LLM-Modulo / 工具增强规划</strong></li>
</ol>
<ul>
<li><strong>LLM⇄PDDL 管道</strong>（Silver et al., 2022；Oswald et al., 2024；La Malfa et al., 2025）：让模型仅生成 domain 或 problem 文件，由外部求解器完成搜索，显著降低非法动作率。</li>
<li><strong>PlanBench-o1 实验</strong>（Valmeekam et al., 2025）：o1-preview 裸机成功率 10–12%，接入 Fast-Downward 后提升至 43%，但计算成本增加 10× 以上；本文 LLM-Modulo 结果与之趋势一致，进一步指出“即使符号求解，空间量纲错误仍存”。</li>
</ul>
<ol>
<li><strong>Sokoban 作为认知诊断环境</strong></li>
</ol>
<ul>
<li><strong>IPC-2023 Sokoban track</strong>（Taitler et al., 2024）：正式将 Sokoban 纳入国际规划竞赛，强调其“无捷径、无模式化解”特性。</li>
<li><strong>RNN 专用求解器</strong>（Taufeeque et al., 2024；Jolicoeur-Martineau, 2025）：在 1000+ 关卡达到 &gt;95% 成功率，表明非 Transformer 架构可通过循环隐状态掌握空间规则，反衬 LLM 的隐状态短板。</li>
<li><strong>o1-preview 裸机实验</strong>（Valmeekam et al., 2025）：在标准 2D Sokoban 仅 10–12% 成功率，与本文“一维走廊”结果形成对照，说明“简化空间并未显著缓解长程退化”。</li>
</ul>
<ol>
<li><strong>长程推理与“漫游”现象的理论解释</strong></li>
</ol>
<ul>
<li><strong>“Wandering vs. Systematic Exploration”框架</strong>（Lu et al., 2025）：提出 LLM 推理轨迹本质是“无 visited-state 集的随机游走”，在深度线性链上表现为重复循环与计数漂移，与本文 token 上限耗尽、前缀准确率指数下降现象完全一致。</li>
<li><strong>工作记忆容量度量</strong>（Zhang et al., 2024）：用 dual-task 方式证明 LLM 的“有效工作记忆”不超过 7–9 个 token 级状态，与本文 25 步左右断崖式失效点数量级一致。</li>
</ul>
<p>综上，既有研究共同指向：<strong>LLM 的规划瓶颈并非高层搜索策略，而是底层状态计数、空间 grounding 与工作记忆持久化的结构性缺陷</strong>。本文通过“极简走廊”设计，将上述观察从复杂域剥离出来，首次给出可量化、可复现的长程失效阈值。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一种“治愈”长程规划缺陷的新架构，而是设计了一套<strong>极简诊断协议</strong>，通过“控制变量→量化失效→工具增强→对比分析”四步，把问题拆成可测量的现象，从而给出改进方向与上限证据。</p>
<ol>
<li><strong>控制变量：构造 SokoBench 走廊谜题</strong></li>
</ol>
<ul>
<li>仅留“长度 ℓ”一个自由参数，最优解长度 ≈ℓ，分支因子=1。</li>
<li>旋转增广（0°/90°/180°/270°）降低预训练泄露风险。</li>
<li>由此将“长程”与“空间复杂性”解耦，任何失效都可归因于<strong>深度+内部状态跟踪</strong>。</li>
</ul>
<ol>
<li><strong>量化失效：1-shot 裸机实验</strong></li>
</ol>
<ul>
<li>指标：Exact-match Accuracy、Prefix Accuracy、Manhattan Distance、token 消耗曲线。</li>
<li>发现：<br>– 25 ≤ ℓ ≤ 30 是断崖阈值，之后 Accuracy 指数下降。<br>– 错误主因是“计数漂移”与“循环输出”，与 Lu et al. 的“wandering”假说一致。<br>– Token 消耗随 ℓ 线性增加，但正确解的方差远小于错误解，说明失败路径发散。</li>
</ul>
<ol>
<li><strong>工具增强：LLM-Modulo 管道</strong></li>
</ol>
<ul>
<li>给模型只留两项任务：<br>① 把 ASCII 图翻译成 PDDL problem 文件；<br>② 调用外部 Unified-Planning 求解器（Fast-Downward / PyperPlan）。</li>
<li>结果：<br>– GPT-5-mini 裸机 25 步后≈0，工具增强后 100 步仍维持 30–40% Accuracy。<br>– 但垂直走廊显著弱于水平走廊，说明“空间量纲估计”错误未被完全消除。<br>– 7/320 次出现 PDDL 语法错，其余失败均因“problem 文件与真实地图尺寸不符”，即<strong>内部计数偏差外溢到符号描述</strong>。</li>
</ul>
<ol>
<li><strong>对比分析与上限证据</strong></li>
</ol>
<ul>
<li>工具增强仅带来<strong>边际提升</strong>，未能突破 25–30 步的“软上限”，证明缺陷<strong>架构层</strong>而非<strong>搜索层</strong>。</li>
<li>给出明确结论：测试时扩展 + 符号求解器<strong>不足以</strong>克服缺乏“显式状态-转移记忆”的硬伤；未来需引入<br>– 显式 visited-state 缓存，<br>– 可学习的计数/坐标专用模块，<br>– 或直接在隐状态内加入符号接地机制。</li>
</ul>
<p>通过上述四步，论文把“长程规划难”从经验观察转化为<strong>可复现指标</strong>，并用量化的方式说明：<strong>在深度线性链场景下，当前 LRM 的失效主因是内部状态漂移，而非高层搜索策略；外部符号工具只能缓解，不能根除。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计 <strong>两套主实验</strong> 与 <strong>一套旋转对照分析</strong>，全部在公开数据集 SokoBench（ℓ=5–100，步长 5，4 种旋转，共 80 张地图）上完成。实验变量、模型、重复次数与核心指标如下：</p>
<p>实验 1 1-shot 裸机推理（无外部工具）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型</td>
<td>DeepSeek-R1-0528、GPT-5-mini、GPT-oss-120B</td>
</tr>
<tr>
<td>Prompt</td>
<td>系统提示 + 表 1 符号映射 + 1 条 4 步示范解</td>
</tr>
<tr>
<td>重复</td>
<td>每 ℓ×旋转 8 次（nt=8，nr=4，总 640 次/模型）</td>
</tr>
<tr>
<td>输出限制</td>
<td>max_tokens=32768，temperature 默认</td>
</tr>
<tr>
<td>记录量</td>
<td>完整推理 trace、最终 字符串、token 消耗</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong></p>
<ol>
<li>Accuracy(ℓ) — Exact string 匹配率</li>
<li>PrefixAccuracy(ℓ) — 逐字符前缀正确率</li>
<li>Manhattan-Distance(ℓ) — 终端坐标与目标 L1 距离</li>
<li>Token 回归 — 正确/错误解的斜率与 R²</li>
</ol>
<p><strong>关键发现</strong></p>
<ul>
<li>ℓ≈25 处出现指数级断崖；</li>
<li>错误解 token 方差远大于正确解，且常因循环输出顶到上限；</li>
<li>旋转 90°/270°（垂直走廊）普遍更低分，揭示“行计数”难度 &gt;“列计数”。</li>
</ul>
<p>实验 2 LLM-Modulo 工具增强</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>可用模型</td>
<td>仅 GPT-5-mini（成本与工具调用稳定性权衡）</td>
</tr>
<tr>
<td>工具链</td>
<td>自定义 MCP 服务器 → PyperPlan / Fast-Downward</td>
</tr>
<tr>
<td>模型任务</td>
<td>生成 PDDL problem 文件 → 调用 solve_problem → 返回 LURD 解</td>
</tr>
<tr>
<td>重复</td>
<td>nt=4，nr=4，总 320 次</td>
</tr>
<tr>
<td>失败处理</td>
<td>语法或语义错误自动重试，最多 3 轮</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong><br>同上 3 项，但额外记录：</p>
<ul>
<li>PDDL 语法错误率</li>
<li>语义不一致率（problem 与真实地图尺寸不符）</li>
<li>工具调用总 token 数（含推理+PDDL 文本）</li>
</ul>
<p><strong>关键发现</strong></p>
<ul>
<li>100 步处 Accuracy 仍≈0.35，较裸机提升 3×，但远未收敛到 1；</li>
<li>7/320 次语法错；其余失败均因“地图长度数错”导致求解器给出短计划；</li>
<li>垂直走廊性能仍显著低于水平走廊，说明<strong>空间量纲估计偏差未被工具消除</strong>。</li>
</ul>
<p>实验 3 旋转细粒度对照（附录 C）</p>
<ul>
<li>仅展示 GPT-5-mini LLM-Modulo 结果；</li>
<li>将 80 张地图按 4 个旋转角度拆分为 4 条子曲线；</li>
<li>量化同一 ℓ 下 0°/90°/180°/270° 的 Accuracy 差异，证实“垂直 vs 水平”效应稳定存在。</li>
</ul>
<p>补充微观分析</p>
<ul>
<li><strong>Token 线性回归</strong>：正确解斜率 29.8–51.1 token/ℓ，错误解斜率 55.2–86.3 token/ℓ，验证“失败轨迹更长且更发散”。</li>
<li><strong>循环检测</strong>：对顶到 32768 上限的 trace 进行子串重复计数，平均 42% 内容属于重复 L/R 语句，直接支持“wandering”假说。</li>
<li><strong>曼哈顿距离热图</strong>：裸机解常出现“玩家最终落在目标上”的非法状态，进一步佐证内部状态未遵守墙/箱约束。</li>
</ul>
<p>综上，实验由<strong>裸机诊断→工具增强→旋转消融→微观痕迹分析</strong>四层组成，共同量化出 LRM 长程规划失效的阈值、特征与改进上限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SokoBench 的“直接续作”或“横向扩展”，均围绕<strong>把当前暴露的单一缺陷拆成多维、可干预变量</strong>展开，以便下一步真正突破 25–30 步的“软上限”。</p>
<p>1 状态表示缺陷的显式干预</p>
<ul>
<li><strong>可学习坐标嵌入</strong>：在 Transformer 隐状态外，显式维护一个  (x,y)  浮游向量，每一步自回归后通过对比损失与符号坐标对齐，检验“外置坐标”能否阻断计数漂移。</li>
<li><strong>Visited-State Cache</strong>：用外部键-值缓存记录已生成状态哈希，采样时禁止重复输出，测试“循环”是否纯粹由无记忆导致。</li>
<li><strong>Delta 预测 vs 绝对预测</strong>：让模型不再预测绝对位置，而是预测“相对位移 + 边界检查”，降低长链绝对坐标累积误差。</li>
</ul>
<p>2 复杂度维度的渐进叠加</p>
<ul>
<li><strong>多箱走廊</strong>：固定 ℓ=50，逐步增加箱子数  b=1…5 ，引入<strong>死锁检测</strong>需求，观察错误模式从“计数漂移”转向“冲突/死锁”的临界  b 。</li>
<li><strong>单侧/双侧分支</strong>：在走廊中段加入  k=1,2  条“死胡同”侧枝，保持解长度不变但增加<strong>搜索宽度</strong>，量化“分支因子”与“深度”对失效的相对贡献。</li>
<li><strong>动态障碍</strong>：在第  t  步随机出现/消失一堵墙，迫使模型<strong>重规划</strong>，测试工作记忆刷新与快速回溯能力。</li>
</ul>
<p>3 感知-动作接口的消融</p>
<ul>
<li><strong>多种编码</strong>：</li>
<li>行号-列号坐标 <code>(r3,c5)</code></li>
<li>相对距离 <code>(Δx,Δy)</code></li>
<li>图像化 2-D 矩阵（利用 VLRM）<br>对比同一地图在不同编码下的 Accuracy 曲线，找出<strong>最敏感格式</strong>，为后续“最佳接地”提供数据。</li>
<li><strong>稀疏 vs 密集奖励提示</strong>：在推理链中每隔  k  步插入人工“状态校验”句子（“此时箱子应在 x=7”），测量<strong>外部提醒</strong>能把阈值 ℓ 提高多少。</li>
</ul>
<p>4 训练与推理策略</p>
<ul>
<li><strong>课程强化微调</strong>：用增量 ℓ 的课程数据对同一基础模型做轻量级 LoRA 微调，验证“见过更长序列”能否把断崖后移，或仅提升记忆。</li>
<li><strong>Best-of-N + 状态验证器</strong>：在推理阶段采样 N 条计划，用外部 Sokoban 模拟器过滤非法解，再选最短合法解；测量 N=1…100 时的收益曲线，给出“纯采样”上限。</li>
<li><strong>递归子目标分解</strong>：强制模型先在高层输出“分阶段里程碑”（如“先把箱推到 x=30”），再对每个子目标展开低层动作；对比端到端方式，检验“显式分层”能否降低有效深度。</li>
</ul>
<p>5 神经-符号混合上限</p>
<ul>
<li><strong>可微规划器端到端训练</strong>：把 Fast-Downward 的编码层换成可微 GNN，生成计划后通过策略梯度微调，看“符号求解器可微化”能否让模型学会<strong>修正 PDDL 参数错误</strong>。</li>
<li><strong>逆向数据增强</strong>：用最优求解器生成百万级走廊解，再反向生成“错误 PDDL 问题”(坐标偏移 ±1)，训练模型<strong>检测并自纠错</strong>，量化“语法+语义”双重验证的剩余错误率。</li>
</ul>
<p>6 横向迁移与外部效度</p>
<ul>
<li><strong>跨域深度链</strong>：保持“单参数难度”理念，换用 Tower of Hanoi、Sliding-Tile、Monkey-and-Bananas 的一维变体，检验“25–30 步阈值”是否<strong>与域无关</strong>，从而验证其为<strong>通用工作记忆极限</strong>。</li>
<li><strong>真实机器人走廊任务</strong>：把 ASCII 走廊映射到实体 1-D 轨道（机器人推箱），观察语言模型生成的计划在高延迟、低精度执行环境下的<strong>鲁棒性差距</strong>，引入“物理漂移”新变量。</li>
</ul>
<p>7 社会-技术风险</p>
<ul>
<li><strong>预训练数据溯源</strong>：对垂直 vs 水平走廊差异做 n-gram 重叠分析，若水平模板在 CommonCrawl 出现频率显著更高，可解释“方向偏差”部分源于<strong>数据不平衡</strong>而非空间认知。</li>
<li><strong>能力-成本权衡公开仪表盘</strong>：建立在线基准，实时记录不同模型在 ℓ=50 走廊的“成功率 / 美元 / 克 CO₂”三元组，为社区提供<strong>绿色 AI</strong> 视角下的规划能力评价。</li>
</ul>
<p>以上方向遵循“单变量介入-可量化-可复现”原则，既可单独成文，也可组合成新的<strong>多维度 SokoBench v2</strong>，目标是<strong>把“长程规划”从黑盒艺术变成白盒实验科学</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可浓缩为“一个基准、两条曲线、三点证据、四步结论”：</p>
<p>一个基准</p>
<ul>
<li><strong>SokoBench</strong>：极简一维 Sokoban 走廊，仅“长度 ℓ”可变，最优解 ≈ℓ 步；</li>
<li>80 张地图（ℓ=5–100，步长 5，4 旋转），完全公开，用于隔离“深度”与“分支/空间复杂性”。</li>
</ul>
<p>两条性能曲线</p>
<ol>
<li><strong>裸机 LRM</strong>：</li>
</ol>
<ul>
<li>25–30 步处 Accuracy 指数断崖；</li>
<li>token 消耗随 ℓ 线性增加，但错误解方差远大于正确解。</li>
</ul>
<ol>
<li><strong>LLM-Modulo（+PDDL 求解器）</strong>：</li>
</ol>
<ul>
<li>相同阈值仍在，100 步时 Accuracy≈0.35，提升有限；</li>
<li>失败主因：模型生成的 PDDL 问题“长度数错”，外置求解器无法弥补。</li>
</ul>
<p>三点证据</p>
<ol>
<li><strong>计数漂移是主因</strong>：Prefix Accuracy 与 Manhattan Distance 同步下降，循环输出占 token 上限 42%。</li>
<li><strong>状态记忆缺失</strong>：模型无 visited-state 概念，反复 proposing 同一动作，体现“wandering”而非系统搜索。</li>
<li><strong>空间接地缺陷</strong>：垂直走廊显著弱于水平走廊，即使用符号求解器亦未消除，说明“坐标量纲”估计错误根深蒂固。</li>
</ol>
<p>四步结论</p>
<ol>
<li>当前 LRM 的长程规划瓶颈主要在<strong>底层状态跟踪与计数表征</strong>，而非高层搜索策略。</li>
<li>25–30 步是<strong>架构性软上限</strong>，测试时扩展或外接符号求解器<strong>仅能边际改善</strong>，无法突破。</li>
<li>未来需引入<strong>显式坐标记忆、可微符号接地或分层子目标验证</strong>等结构性改进，否则单纯堆算力无益。</li>
<li>SokoBench 以极简环境量化深度失效，为社区提供了<strong>可复现、可干预</strong>的长程规划诊断平台。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2601.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20856。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20856v1.pdf">https://arxiv.org/pdf/2601.20856v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20856v1">https://papers.cool/arxiv/2601.20856v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:00Z</p>
<hr>
<h3 id="6-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images"><a href="#6-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images" class="headerlink" title="6. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images"></a>6. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17934v2">From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</a></h3><p>Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM’s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>像 Segment Anything Model (SAM) 这样的基础模型展示了强大的泛化能力，但由于领域偏移、标签稀缺以及参数高效微调（PEFT）无法利用未标注数据，将它们应用于医学影像仍然十分困难。尽管像 U-Net 这样的传统模型在半监督医学学习中表现出色，它们在辅助 PEFT SAM 方面的潜力却在很大程度上被忽视。我们提出了 SC-SAM，这是一个专家-通用框架，其中 U-Net 提供基于点的提示和伪标签来引导 SAM 的适应，而 SAM 则作为强大的通用监督器来规范 U-Net。这种相互引导形成了双向协同训练循环，使两个模型都能有效利用未标注数据。在前列腺 MRI 和息肉分割基准实验中，我们的方法达到了最先进的结果，优于其他现有的半监督 SAM 变体，甚至超过了像 MedSAM 这样的医学基础模型，突显了专家-通用合作在标签高效医学影像分割中的价值。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对医学图像分割中“基础模型难以利用无标注数据”这一核心矛盾展开。具体而言：</p>
<ul>
<li><p><strong>SAM 在医学域的瓶颈</strong><br>尽管 Segment Anything Model（SAM）具备强大的零样本泛化能力，但面对医学影像时存在显著域偏移；同时，参数高效微调（PEFT）仍依赖少量标注，无法直接从无标注样本学习，导致大量无标签数据被闲置。</p>
</li>
<li><p><strong>传统半监督方案与 SAM 的割裂</strong><br>基于 U-Net 的半监督框架（一致性正则、伪标签等）已证明无标注数据的价值，却与 SAM 的 PEFT 流程脱节；现有尝试或仅把 SAM 当教师，或引入复杂双解码结构，仍未让 SAM 真正“吸收”无标注信息。</p>
</li>
<li><p><strong>目标</strong><br>提出 SC-SAM，让“传统专家”U-Net 与“通用基础模型”SAM 形成双向协同：</p>
</li>
</ul>
<ol>
<li>U-Net 先利用全部数据（含无标注）生成可靠点提示与伪标签，指导 SAM 的 PEFT；</li>
<li>SAM 反过来用其高阶语义输出正则化 U-Net，抑制噪声。<br>通过该“专家-通才”循环，使 SAM 在仅 5 %–10 % 标注比例下即可充分挖掘无标注医学图像，提升分割精度并超越现有半监督 SAM 变体及医学专用基础模型。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四类，均围绕“如何在半监督或低标注场景下让 SAM 及其变体适应医学图像”展开：</p>
<ol>
<li><strong>直接微调 SAM（PEFT-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>Medical SAM Adapter</strong>（Wu et al., MedIA 2025）<br>在 ViT 编码器内插入 Adapter 层，仅用小规模标注数据做参数高效微调，未利用无标签集。</li>
<li><strong>SAM-Med2D</strong>（Cheng et al., arXiv 2023）<br>将 2D 医学切片重构成 RGB 三通道后直接全微调，参数开销大且仍依赖标注。</li>
</ul>
<ol>
<li><strong>双分支/交叉提示一致性（Dual-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>CPC-SAM</strong>（Miao et al., MICCAI 2024）<br>两个独立 mask decoder 互作“教师”，通过交叉提示生成伪标签，但两分支均为 SAM 自身，易受域偏移影响而耦合。</li>
</ul>
<ol>
<li><strong>专家网络为 SAM 提供提示（SP-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>KnowSAM</strong>（Huang et al., IEEE T-MI 2025）<br>用 U-Net+V-Net 生成融合 mask 作为 SAM 的 mask prompt，再以 KL 蒸馏让专家模仿 SAM；专家仅充当“提示生成器”，SAM 并未反向受益。</li>
</ul>
<ol>
<li><strong>传统半监督 CNN 框架</strong></li>
</ol>
<ul>
<li><strong>U-Net 系列</strong>（Ronneberger et al., MICCAI 2015；Zhou et al., 2018；Jha et al., 2019）<br>通过一致性正则、伪标签、Mean Teacher 等手段充分挖掘无标注数据，但无法与 SAM 的 PEFT 流程协同。</li>
</ul>
<p>SC-SAM 与上述工作的本质区别在于：</p>
<ul>
<li>首次让“专家”U-Net 与“通才”SAM 形成<strong>双向协同训练闭环</strong>；</li>
<li>引入 sigmoid ramp-up 缓解早期噪声伪标签对 SAM 的干扰；</li>
<li>既用 U-Net 生成点提示/伪标签指导 SAM，又用 SAM 的语义输出正则化 U-Net，从而真正将无标注数据的信息注入基础模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 SC-SAM，通过“专家-通才”双向协同训练框架，把 U-Net 对无标注数据的利用能力无缝迁移到 SAM 的 PEFT 流程。核心机制分三步：</p>
<ol>
<li><p><strong>专家先行：U-Net 半监督预热</strong><br>用标准半监督策略（一致性正则 + 伪标签）在全部数据上训练 U-Net，使其对标注与无标注图像均产生可靠预测  P^(UNet) 。</p>
</li>
<li><p><strong>通才跟进：SAM 接受 U-Net 的“提示-伪标签”</strong></p>
</li>
</ol>
<ul>
<li>从  P^(UNet)  随机采样 5 前景点 + 5 背景点，生成点提示</li>
</ul>
<p>points = Sample(P^(UNet))</p>
<ul>
<li>将点提示送入 SAM 的 prompt encoder，与图像编码特征一起经 mask decoder 得到预测</li>
</ul>
<p>P^(SAM) = D!(E(X);; P(points))</p>
<ul>
<li>对无标注图像，把 U-Net 的预测当作伪标签，计算无监督损失</li>
</ul>
<p>L<em>(unsup)^(SAM) = L</em>(seg)(P^(SAM), P^(UNet))</p>
<ol>
<li><strong>反向正则：SAM 输出平滑 U-Net</strong><br>同时用 SAM 的预测  P^(SAM)  作为质量更高的伪标签，反向监督 U-Net</li>
</ol>
<p>L<em>(unsup)^(UNet) = L</em>(seg)(P^(UNet), P^(SAM))</p>
<ol>
<li><strong>稳定训练：sigmoid ramp-up</strong><br>为避免早期 U-Net 噪声淹没 SAM，引入时变权重</li>
</ol>
<p>ω(t)= exp!l(-(1-(t) / (T<em>(max)))^2r), &amp; 0le tle T</em>(max)[2pt] 1, &amp; t&gt;T_(max)</p>
<p>总无监督损失</p>
<p>L<em>(unsup) = L</em>(unsup)^(UNet) + ω(t),L_(unsup)^(SAM)</p>
<ol>
<li><strong>联合目标</strong><br>加上标注数据的监督项，整体损失</li>
</ol>
<p>L<em>(total) = L</em>(seg)(P^(UNet), Y) + L<em>(seg)(P^(SAM), Y)</em>(L)<em>(sup) + L</em>(unsup)</p>
<p>通过该闭环，U-Net 提供结构先验与伪标签，SAM 提供高阶语义正则，二者在每次迭代中同步更新，实现无标注信息向基础模型的有效注入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“极低标注比例下能否让 SAM 充分利用无标注医学图像”展开，覆盖前列腺 MRI 与结直肠息肉两个代表性任务，共 5 组实验：</p>
<ol>
<li><strong>主实验：半监督分割精度对比</strong></li>
</ol>
<ul>
<li><strong>数据集</strong></li>
<li>PROMISE12：50 例前列腺 T2 MRI，按 35/5/10 划分 train/val/test，每例按 2D 切片处理。</li>
<li>COLON：CVC-ClinicDB 612 张 + Kvasir 838 张训练，跨域测试五套公开库（CVC-300、CVC-ColonDB、ETIS-LaribPolypDB 等）。</li>
<li><strong>标注比例</strong><br>仅提供 5 % 或 10 % 训练切片作为标注，其余完全无标签。</li>
<li><strong>对照方法</strong></li>
<li>PEFT-SAM 系列：SAM、MedSAM、SAM-Med2D</li>
<li>Dual-SAM 系列：CPC-SAM</li>
<li>SP-SAM 系列：KnowSAM</li>
<li><strong>指标</strong><br>Dice、IoU、HD95、ASD</li>
<li><strong>结果</strong><br>在 5 % 标注下，SC-SAM 在 PROMISE12 上 Dice 83.64，比第二名 KnowSAM 高 5.15；COLON 五套测试集平均 Dice 提升 2–4 个百分点，均达到新 SOTA。</li>
</ul>
<ol>
<li><p><strong>跨域泛化评估</strong><br>COLON 实验即跨域场景：训练集与五套测试集来自不同医院、不同设备。SC-SAM 在未见中心数据上仍保持最高平均 Dice，验证对域偏移的鲁棒性。</p>
</li>
<li><p><strong>可视化对比</strong><br>随机抽取 5 % 与 10 % 标注场景下的前列腺与息肉样本，绘制预测 mask 与 GT 叠加热图。SC-SAM 边界更紧致，背景误报明显减少。</p>
</li>
<li><p><strong>消融实验</strong></p>
</li>
</ol>
<ul>
<li><strong>组件消融</strong><br>去掉 sigmoid ramp-up（ω(t)≡1）→ Dice 从 83.64 降至 36.37，降幅 47 %。</li>
<li><strong>骨干替换</strong><br>将 U-Net 换成 UNet++、ResUNet++、Swin-UNet，在同等设置下 Dice 分别降至 80.14、78.20、70.27，验证 CNN 型专家在半监督场景下优于 ViT 型。</li>
</ul>
<ol>
<li><strong>资源与收敛分析</strong><br>单卡 RTX 3090 Ti 24 GB 即可完成训练；PROMISE12 5 % 标注场景下，SC-SAM 在约 6 k 迭代后收敛，验证集 Dice 即超过最佳对照 2 个百分点，表明样本效率显著提升。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>三维/视频扩展</strong><br>将双向协同机制从 2D 切片推广到 3D 体数据或结肠镜视频，设计时空一致的伪标签生成策略，解决切片间不连续问题。</p>
</li>
<li><p><strong>提示模态扩充</strong><br>除点提示外，引入可学习的稀疏框、文本或解剖先验嵌入，探索混合提示在更小标注比例（≤1 %）下的极限性能。</p>
</li>
<li><p><strong>理论分析</strong><br>对 sigmoid ramp-up 进行误差上界推导，量化早期伪标签噪声对 SAM 梯度方向的影响，给出最优 ramp-up 长度  T_(max)  的解析估计。</p>
</li>
<li><p><strong>更多专家融合</strong><br>采用多专家集成（CNN + Transformer + 物理模型）生成更鲁棒的伪标签，研究不同专家间置信度加权或投票机制，进一步提升伪标签质量。</p>
</li>
<li><p><strong>持续/增量学习</strong><br>当新器官或成像模态陆续加入时，固定 SAM 编码器、仅扩展 Adapter 与专家网络头部，实现无需重训历史数据的持续医学分割。</p>
</li>
<li><p><strong>不确定性驱动采样</strong><br>利用 SAM 或 U-Net 的预测不确定性，主动挑选最有价值的无标注样本进行人工标注，实现从 5 % 到 1 % 标注比例的主动-半监督联合优化。</p>
</li>
<li><p><strong>跨模态协同</strong><br>将同一患者的 MRI、CT、超声等多模态图像共同输入，设计模态无关的提示空间，验证 SC-SAM 在模态缺失情况下的鲁棒性。</p>
</li>
<li><p><strong>轻量化部署</strong><br>对 U-Net 与 Adapter 进行联合量化/剪枝，保持伪标签质量的同时减少显存占用，使框架可在边缘内镜设备上实时运行。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SC-SAM 核心内容速览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>SAM 在医学影像因域偏移+ scarce labels 表现骤降；PEFT 无法直接利用无标注数据，而传统 U-Net 半监督方案又与 SAM 脱节。</td>
</tr>
<tr>
<td>思路</td>
<td>让“专家”U-Net 与“通才”SAM 互为师生，形成双向协同：U-Net 用全量数据生成点提示+伪标签 → 引导 SAM 的 PEFT；SAM 输出高质量 mask → 反向正则化 U-Net。</td>
</tr>
<tr>
<td>技术</td>
<td>1. 点提示采样：5 前景点+5 背景点。 2. 无监督损失： Lunsup=Lseg(P^(UNet),P^(SAM)) + ω(t)Lseg(P^(SAM),P^(UNet)) ，其中  ω(t)  为 sigmoid ramp-up。 3. 联合目标： Ltotal=Lsup+Lunsup 。</td>
</tr>
<tr>
<td>实验</td>
<td>前列腺 MRI（PROMISE12）与结直肠息肉（COLON）在 5 %/10 % 标注比例下均达 SOTA；跨域、消融、可视化与资源分析验证有效性。</td>
</tr>
<tr>
<td>结论</td>
<td>传统 CNN 专家与现代基础模型协同，可在极低标注场景下充分挖掘无标签医学图像，实现标签高效、精度领先的医学分割。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images，链接是https://arxiv.org/pdf/2601.17934，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17934。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</p>
<p><strong>Categories</strong>: cs.CV, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17934v2.pdf">https://arxiv.org/pdf/2601.17934v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17934v2">https://papers.cool/arxiv/2601.17934v2</a></p>
<p><strong>Published</strong>: 2026-01-25T18:13:48Z</p>
<hr>
<h3 id="7-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation"><a href="#7-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation" class="headerlink" title="7. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation"></a>7. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20854v1">Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</a></h3><p>Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>表格数据仍然是生成模型的一个具有挑战性的领域。特别是，标准的变分自编码器（VAE）架构，通常由多层感知器组成，在建模特征之间的关系方面表现不佳，尤其是在处理混合数据类型时。相比之下，Transformer 通过其注意力机制，更适合捕捉复杂的特征交互。在本文中，我们通过实证方法研究了将 Transformer 集成到 VAE 不同组件中的影响。我们在 OpenML CC18 套件中的 57 个数据集上进行了实验，并得出两个主要结论。首先，结果表明，将 Transformer 放置以利用潜在空间和解码器表示会在保真度和多样性之间产生权衡。其次，我们观察到所有组件中的连续 Transformer 块之间具有高度相似性。特别是在解码器中，Transformer 的输入与输出之间的关系大致为线性关系。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“表格数据生成”中标准变分自编码器（VAE）难以建模异构特征间复杂依赖关系的问题，提出并系统评估一种改进思路：<br><strong>在 VAE 的不同表征阶段（输入、隐空间、输出）引入 Transformer 的自注意力机制，以显式捕获特征间的高阶交互，同时量化这一改动对生成样本保真度（fidelity）与多样性（diversity）的权衡影响。</strong></p>
<p>具体而言，论文试图回答以下核心问题：</p>
<ol>
<li>在表格数据混合类型特征场景下，将 Transformer 置于 VAE 的编码器、隐空间或解码器，会对生成质量产生何种差异？</li>
<li>不同放置策略是否会带来“保真度-多样性”权衡？</li>
<li>Transformer 内部表征在训练后是否真正改变了特征方向，抑或仅起到缩放作用？</li>
</ol>
<p>通过 57 个 OpenML 数据集的实验，论文得出：</p>
<ul>
<li><strong>仅在隐空间与解码器端引入 Transformer 可显著提升多样性，但会降低保真度</strong>；</li>
<li><strong>编码器端 Transformer 对保真度影响最小</strong>；</li>
<li><strong>解码器端 Transformer 在训练后近似恒等映射，其残差分支受层归一化缩放效应主导</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 章“Related Work”中系统回顾了与“表格数据生成”相关的四大类深度生成模型，并单独梳理了“Transformer 在表格数据生成”中的已有尝试。相关研究可归纳如下：</p>
<p>1. 面向表格数据的深度生成模型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型类别</th>
<th>代表方法</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>GAN 系列</td>
<td>CTGAN、CTAB-GAN、CTAB-GAN+</td>
<td>将条件 GAN 扩展到混合类型特征，引入模式特定归一化、分类编码与辅助损失。</td>
</tr>
<tr>
<td>扩散模型</td>
<td>TabDDPM、CoDi、StaSy、TabSyn、TabDiff</td>
<td>针对离散-连续混合特征设计前向/反向过程，或在潜空间执行扩散。</td>
</tr>
<tr>
<td>自回归模型</td>
<td>GReaT、TabMT</td>
<td>把表格行序列化后用 GPT/Transformer 逐列生成；引入掩码语言建模。</td>
</tr>
<tr>
<td>VAE 系列</td>
<td>TVAE、VAEM、GOGGLE</td>
<td>TVAE 直接对 One-Hot 后特征做 VAE；VAEM 两阶段先逐特征 VAE 再建模隐变量关系；GOGGLE 用图神经网络近似变量关系并联合 VAE 训练。</td>
</tr>
<tr>
<td>流匹配</td>
<td>梯度提升流 (Gradient-Boosted Flow)</td>
<td>用梯度提升树拟合流匹配中的速度场，适配异构特征。</td>
</tr>
</tbody>
</table>
</div>
<p>2. Transformer 在表格数据生成中的已有探索</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>Transformer 角色</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabDiff</td>
<td>同时出现在主干去噪网络与输出头</td>
<td>仅作为模型组件，未系统研究不同放置对保真-多样性权衡的影响。</td>
</tr>
<tr>
<td>TabSyn</td>
<td>用在 VAE 的“识别网络”(encoder) 与“生成网络”(decoder) 内部</td>
<td>与本文 ELD-VAE 结构类似，但未进行表征相似性分析，也未揭示解码器 Transformer 近似恒等的现象。</td>
</tr>
<tr>
<td>GReaT、TabMT</td>
<td>把整张表看成序列，用 Transformer 自回归生成</td>
<td>属于“表→文本”范式，与 VAE 的“潜变量+重构”范式不同。</td>
</tr>
<tr>
<td>SAINT、TabTransformer</td>
<td>主要服务于监督学习，而非生成</td>
<td>聚焦判别任务中的特征交互，未评估合成数据质量。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 小结</p>
<p>已有文献验证了 Transformer 对捕获表格特征交互的有效性，但<strong>缺乏在 VAE 框架内、对“编码器-隐空间-解码器”三个不同层级系统性地放置 Transformer 的对比研究</strong>，也<strong>未量化其带来的保真度-多样性权衡与内部表征相似性</strong>。本文正是填补这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“<strong>系统性位置消融 + 多维度量化评估</strong>”的策略，把 Transformer 逐层“插入”到 VAE 的三种表征路径（输入、隐空间、输出），在 57 个 OpenML 表格数据集上完成对照实验，从而回答“<strong>放哪里、有何利弊、为何如此</strong>”三个问题。具体步骤如下：</p>
<p>1. 构建 6 种 VAE 变体（位置消融）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>Transformer 位置</th>
<th>研究目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAE</td>
<td>无</td>
<td>纯 MLP 基线</td>
</tr>
<tr>
<td>E-VAE</td>
<td>仅 encoder</td>
<td>检验“原始特征”层面交互是否有助益</td>
</tr>
<tr>
<td>EL-VAE</td>
<td>encoder + latent</td>
<td>观察隐变量层加入自注意力的影响</td>
</tr>
<tr>
<td>ELD-VAE</td>
<td>encoder + latent + decoder</td>
<td>全路径覆盖，验证累积效应</td>
</tr>
<tr>
<td>LD-VAE</td>
<td>latent + decoder（无 encoder）</td>
<td>剥离 encoder 干扰，聚焦深层表征</td>
</tr>
<tr>
<td>D-VAE</td>
<td>仅 decoder</td>
<td>直接测试“重构空间”交互的作用</td>
</tr>
</tbody>
</table>
</div>
<p>所有变体共享相同的 tokenizer、detokenizer、隐藏层宽度、 latent 维度与训练超参，保证差异仅由 Transformer 位置引起。</p>
<p>2. 统一训练与采样协议</p>
<ul>
<li><strong>数据预处理</strong>：数值特征用 QuantileTransformer→高斯，类别特征 One-Hot；缺失值均值/众数填补；删除零方差特征。</li>
<li><strong>训练</strong>：500 epoch，Adam  10^(-3) ，权重衰减 0.9，batch size 按验证集自动折半调优。</li>
<li><strong>Transformer 超参</strong>：1 头、4 块、128 隐藏维、pre-norm、embedding 维 4。</li>
<li><strong>采样</strong>：训练完成后从先验  zsimN(0,I)  生成，与训练集同规模。</li>
</ul>
<p>3. 三维评估体系（量化利弊）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评估层级</th>
<th>指标</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low-Density</td>
<td>1-way marginals + Pairwise-correlations</td>
<td>快速检验单变量/双变量分布是否失真</td>
</tr>
<tr>
<td>High-Density</td>
<td>α-Precision（保真） vs β-Recall（多样）</td>
<td>精确量化“保真-多样”权衡</td>
</tr>
<tr>
<td>ML-Efficiency</td>
<td>TSTR Utility + ML-Fidelity</td>
<td>测合成数据在下游 XGBoost 任务中的真实价值</td>
</tr>
</tbody>
</table>
</div>
<p>所有指标归一化到<br>0,1<br>，用 Wilcoxon 符号秩检验判断显著性。</p>
<p>4. 表征相似性诊断（解释原因）</p>
<ul>
<li><strong>CKA 线性相似度</strong>：计算 Transformer 块内“输入-输出”余弦相似度，揭示是否近似恒等映射。</li>
<li><strong>残差缩放因子 σ</strong>：逐样本求解  E= σ E ，若  σ≈ 1  则表明自注意力+LayerNorm 仅做尺度变换。</li>
<li><strong>逐块 F-范数变化</strong>：量化 LayerNorm 对幅值的压缩程度，解释为何解码器 Transformer 不改变方向。</li>
</ul>
<p>5. 结论与落地建议</p>
<ul>
<li><strong>权衡曲线</strong>：latent + decoder 引入 Transformer → β-Recall 平均提升 4–7%，但 α-Precision 下降 3–6%；encoder 处 Transformer 对保真度几乎无负面影响。</li>
<li><strong>恒等现象</strong>：解码器 Transformer 在训练后  σ  中位数≈1，CKA&gt;0.95，证实其退化为“带缩放的恒等函数”，主要受 LayerNorm 的平移-缩放效应主导。</li>
<li><strong>实用指南</strong>：</li>
<li>若任务更重视<strong>多样性</strong>（数据增广、隐私扩充），优先选用 LD-VAE 或 ELD-VAE；</li>
<li>若任务更重视<strong>保真</strong>（缺失值插补、发布替代数据），用 E-VAE 甚至纯 MLP-VAE 即可，省去 Transformer 的计算与调参成本。</li>
</ul>
<p>通过上述“<strong>位置消融 → 多维指标 → 表征诊断</strong>”闭环，论文不仅回答了“放哪里”，也解释了“为何这样”，为后续表格生成模型中 Transformer 的取舍与再设计提供了量化依据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“Transformer 在 VAE 不同位置对表格数据生成的影响”共设计并执行了 <strong>三大类、七组、累计 &gt; 2 000 次训练</strong> 的实验，覆盖 57 个 OpenML-CC18 数据集。具体实验矩阵如下：</p>
<p>1. 主实验：6 种架构 × 57 数据集 全面对照</p>
<p><strong>目的</strong>：量化 Transformer 位置对生成质量与下游效用的一致影响。<br><strong>指标</strong>：</p>
<ul>
<li>Low-Density：1-way marginals、Pairwise-correlations</li>
<li>High-Density：α-Precision（保真）、β-Recall（多样）</li>
<li>ML-Efficiency：TSTR Utility、ML-Fidelity<br><strong>统计</strong>：Wilcoxon 符号秩检验（p=0.001）+ 平均分数/平均排名双维度判定显著性。</li>
</ul>
<p>2. fidelity–diversity 细粒度实验</p>
<p>2.1 Forward 序列（VAE→E-VAE→EL-VAE→ELD-VAE）</p>
<p>2.2 Backward 序列（VAE→D-VAE→LD-VAE→ELD-VAE）</p>
<p><strong>分组</strong>：按样本量将 57 数据集切为 small<br>500,1 k)、medium[1 k,5 k)、large[5 k,96 k<br> 三桶。<br><strong>输出</strong>：</p>
<ul>
<li>各序列每步过渡的 Δα、Δβ 随桶大小的变化曲线（论文图 2 &amp; 补充图 1–2）。</li>
<li>证实“latent/decoder 加 Transformer → 保真降、多样升”趋势与数据规模呈单调关系。</li>
</ul>
<p>3. 表征相似性实验</p>
<p>3.1 聚合相似度</p>
<ul>
<li>对 6 种模型，计算每个 Transformer 组件“输入-输出”CKA 相似度，再按桶平均（论文图 3）。</li>
</ul>
<p>3.2 块内相似度</p>
<ul>
<li>在 ELD-VAE 上，抽取 encoder/latent/decoder 各 4 个 block 内部 6 个检查点（e.in→e.n1→e.attn→e.res→e.n2→e.ffn）两两 CKA，绘制热图（论文图 4–5）。</li>
</ul>
<p>3.3 残差缩放因子</p>
<ul>
<li>逐样本求解 σ 使  E=σE ，给出 σ 分布与块编号关系（论文图 6），验证 decoder 近似恒等。</li>
</ul>
<p>3.4 块数敏感性</p>
<ul>
<li>在 churn、adult、credit-approval 三数据集上，把 ELD-VAE 的 Transformer 块数从 4→3→2→1，重复实验：<br>– High-Density 变化（补充图 3）<br>– 表征相似度热图（补充图 4）<br>结论：块数减少不改变“decoder 高相似”现象，仅 latent 层相似度随块数增加而下降。</li>
</ul>
<p>4. 训练稳定性诊断实验</p>
<ul>
<li>对出现 NaN/爆炸损失的 3 个数据集（kc2、jm1、spambase），将隐藏维与 latent 维减半后重训；记录重构/KL 曲线（补充图 5–6），并对比 α/β 前后变化（补充表 2–3），确认超参过大是 instability 主因。</li>
</ul>
<p>5. 重构数据 fidelity–diversity 实验</p>
<ul>
<li>用训练好的 6 模型对真实测试集做前向重构，再以训练集为参照计算 α-Precision、β-Recall（补充表 4）。</li>
<li>结果与“采样生成”趋势一致，进一步排除评估偏差。</li>
</ul>
<p>6. 低维与 ML 效用桶分析（补充第 2 章）</p>
<ul>
<li>对 Marginals、Pairwise、Utility、ML-Fidelity 四项指标，均按 Forward/Backward 序列、三桶样本量给出 Δ 曲线（补充图 1–2），验证“多样性提升≠下游效用提升”现象在所有指标上稳定存在。</li>
</ul>
<p>7. 单数据集结果全表（补充第 7 章）</p>
<ul>
<li>给出 57 数据集上 6 模型 6 项指标的原始分数与排名（补充表 6–8），支持主文“LD-VAE 综合平均第一”结论的可复现性。</li>
</ul>
<p>实验规模小结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集</td>
<td>57</td>
</tr>
<tr>
<td>模型变体</td>
<td>6</td>
</tr>
<tr>
<td>每数据集训练次数</td>
<td>1（主实验）+ 块数敏感 3×4 + 稳定性重训 3</td>
</tr>
<tr>
<td>总训练运行</td>
<td>≈ 6×57 + 3×4×3 + 3 = 363 + 36 + 3 = 402 次完整训练</td>
</tr>
<tr>
<td>生成与评估</td>
<td>每次训练后生成 1×训练集规模样本，并计算 6 项指标</td>
</tr>
<tr>
<td>CKA/σ 计算</td>
<td>在测试集上逐 batch 计算，累计 &gt; 10 7 次向量内积</td>
</tr>
</tbody>
</table>
</div>
<p>通过上述实验矩阵，论文从“质量-效用-机理”三个层面完整回答了 Transformer 放置问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对该工作的直接延伸或深层追问，均具备理论价值与落地潜力：</p>
<p>1. 架构层面：把“退化”变为“有用”</p>
<ul>
<li><p><strong>去 LayerNorm 的 Transformer 块</strong><br>论文揭示解码器 Transformer 近似恒等的核心原因是 LayerNorm 的平移-缩放效应。可尝试</p>
</li>
<li><p>完全移除 LayerNorm，改用 RMSNorm、PowerNorm 或简单可学习标量缩放；</p>
</li>
<li>仅用 Pre-Activation MLP 替代 Transformer 块，对比是否仍能保持多样性增益。<br>目标：在不影响保真度的前提下，让解码器注意力真正改变表征方向。</li>
<li><strong>轻量级注意力替代</strong><br>探索 Linformer、Performer、Cosformer 等线性复杂度注意力，验证“多样性↑保真度↓”规律是否依旧成立，为大规模高维表格（≥10 000 特征）提供可行方案。</li>
</ul>
<p>2. 目标函数层面：重调保真-多样天平</p>
<ul>
<li><strong>动态加权 α-β 损失</strong><br>当前 VAE 仅优化 ELBO。可把 α-Precision、β-Recall 直接纳入损失，构建</li>
</ul>
<p>L<em>(new) = L</em>(ELBO) - λ_1 α + λ_2 β</p>
<p>通过 Pareto 搜索寻找 (λ₁, λ₂) 最优前沿，实现“保真-多样”可控生成。</p>
<ul>
<li><strong>对比式/信息瓶颈正则</strong><br>引入对比损失或信息瓶颈，限制隐变量互信息 I(x;z) 上限，抑制 VAE 本身“过度随机”导致的保真度下降，看能否在 latent-Transformer 场景下仍保持高 β-Recall。</li>
</ul>
<p>3. 数据层面：更复杂的表格形态</p>
<ul>
<li><p><strong>缺失非随机（MNAR）与噪声标签</strong><br>论文仅做简单均值/众数填补。下一步让生成器直接输出“缺失掩码”与“值”两组向量，用 MNAR 模拟机制评估 Transformer 位置对插补偏差的影响。</p>
</li>
<li><p><strong>时序-表格混合</strong><br>将静态表格字段与时间序列拼接，考察 Transformer 在“静态-动态”异构注意力下的表现，拓展到医疗纵向数据、用户行为日志等场景。</p>
</li>
</ul>
<p>4. 评估层面：超越 α-β</p>
<ul>
<li><p><strong>细粒度隐私泄露度量</strong><br>采用 Membership Inference、Attribute Inference 攻击评估不同位置 Transformer 的隐私-效用权衡，验证“多样性↑”是否伴随“泄露↑”。</p>
</li>
<li><p><strong>因果保真度</strong><br>引入因果发现指标（如结构汉明距离 SHD）衡量合成数据是否保持原始变量间的因果边方向，验证注意力机制是否会混淆因果与相关。</p>
</li>
</ul>
<p>5. 训练策略层面：让 Transformer 不再“闲置”</p>
<ul>
<li><p><strong>两阶段训练</strong><br>① 先无 Transformer 训练 VAE 至收敛；② 冻结解码器 MLP，仅微调解码器 Transformer。观察能否打破“恒等”初始化陷阱，使注意力真正学习残差方向。</p>
</li>
<li><p><strong>梯度截断 / 权重标准化</strong><br>对注意力输出做梯度截断或使用 Weight Standardization，缓解 LayerNorm 输入分布漂移，从而提升有效学习率，测试是否减少恒等现象。</p>
</li>
</ul>
<p>6. 跨模型迁移：是否只是 VAE 的“专利”？</p>
<ul>
<li><p><strong>Transformer-in-GAN</strong><br>将同样的“encoder- latent-decoder”放置策略迁移到 CTGAN、TabDDPM 框架，检验保真-多样权衡是否依旧成立，或是否因目标函数不同而逆转。</p>
</li>
<li><p><strong>扩散模型中的注意力位置</strong><br>在 TabDiff 的去噪网络中，分别把 Transformer 放在“时间步嵌入后”“特征嵌入后”“输出头”三段，重复 α-β 实验，看多样性增益是否普遍存在于所有生成范式。</p>
</li>
</ul>
<p>7. 可解释性深挖：注意力到底在“看”什么？</p>
<ul>
<li><p><strong>注意力可视化</strong><br>对 decoder Transformer 的注意力矩阵进行平均，检查是否始终呈现对角或均匀分布，进一步佐证“无结构交互”导致恒等映射的假设。</p>
</li>
<li><p><strong>互信息分解</strong><br>用 MISE 或 CXPlain 对注意力输出与原始特征做互信息分解，定位哪些特征对被过度强化，从而设计稀疏注意力掩码，减少虚假相关。</p>
</li>
</ul>
<p>8. 工程与产品化</p>
<ul>
<li><p><strong>自动架构搜索（NAS）</strong><br>以 α-β 加权分数为奖励，用强化 NAS 搜索“在哪几层、用几块、头数多少”最优，输出 Pareto 前沿供用户按业务场景（金融风控 vs 营销扩量）一键选择。</p>
</li>
<li><p><strong>端侧轻量化</strong><br>结合第 1 点的线性注意力与第 5 点的两阶段训练，构建 &lt;1 MB 的“微型表格生成器”，在移动端实现现场数据增强与隐私释放。</p>
</li>
</ul>
<p>总结</p>
<p>论文首次量化了“Transformer 位置-保真-多样”三角关系，并揭示了解码器恒等退化现象。下一步工作可沿“<strong>架构-目标-数据-评估-训练-跨模型-可解释-工程化</strong>”八个维度展开，把“表格 Transformer 生成”从“能用”推向“好用、可控、可信”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文主旨一句话</strong><br>系统探究“把 Transformer 塞进 VAE 的哪个部位”才能在混合类型表格数据生成中取得最佳“保真-多样”权衡，并揭示解码器 Transformer 训练后近似恒等映射的机理。</p>
<p>1. 背景与动机</p>
<ul>
<li>标准 VAE（全 MLP）难以捕获异构特征间高阶关系。</li>
<li>Transformer 的自注意力在文本、图像领域已验证其交互建模能力，但在表格生成中“放哪里、值不值”缺乏大尺度实证。</li>
</ul>
<p>2. 方法</p>
<ul>
<li><strong>6 个 VAE 变体</strong>：VAE/E-VAE/EL-VAE/ELD-VAE/LD-VAE/D-VAE，仅改变 Transformer 所处位置（encoder / latent / decoder）。</li>
<li><strong>统一设置</strong>：57 个 OpenML-CC18 数据集 → 相同 tokenizer、训练超参、评估流程。</li>
<li><strong>三维评估</strong>：<br>– Low-Density：1-way marginals + pairwise correlations<br>– High-Density：α-Precision（保真） vs β-Recall（多样）<br>– ML-Efficiency：TSTR Utility + ML-Fidelity</li>
<li><strong>表征诊断</strong>：CKA 相似度、残差缩放因子 σ、逐块 F-范数，解释“退化”原因。</li>
</ul>
<p>3. 主要发现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>位置</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>encoder</td>
<td>几乎不损保真，对多样提升微小。</td>
</tr>
<tr>
<td>latent + decoder</td>
<td>多样↑ 4–7%，保真↓ 3–6%；权衡显著且随数据规模单调。</td>
</tr>
<tr>
<td>decoder 单独</td>
<td>最差保真，多样性增益有限。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>机理级结论</strong></p>
<ul>
<li>解码器 Transformer 训练后输入-输出 CKA&gt;0.95，σ≈1，本质为“LayerNorm 缩放后的恒等函数”。</li>
<li>减少块数或移除 LayerNorm 仍无法打破恒等现象，提示需重新设计目标函数或训练策略。</li>
</ul>
<p>4. 实用建议</p>
<ul>
<li><strong>重保真</strong> → 用 E-VAE 或纯 MLP-VAE，省计算。</li>
<li><strong>重多样</strong> → 用 LD-VAE，接受轻微保真损失。</li>
<li><strong>下游 ML 效用</strong> → Transformer 位置无显著优势，多样性↑≠效用↑，需按任务取舍。</li>
</ul>
<p>5. 贡献清单</p>
<p>① 大尺度（57 数据集）位置消融实验，首次量化“保真-多样”权衡。<br>② 揭示解码器 Transformer 恒等退化现象并给出 LayerNorm 根源解释。<br>③ 提供面向表格生成的 Transformer 使用指南与开源训练-评估协议。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Transformer%20Placement%20in%20Variational%20Autoencoders%20for%20Tabular%20Data%20Generation，链接是https://arxiv.org/pdf/2601.20854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20854。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</p>
<p><strong>Categories</strong>: cs.LG, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20854v1.pdf">https://arxiv.org/pdf/2601.20854v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20854v1">https://papers.cool/arxiv/2601.20854v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:54:27Z</p>
<hr>
<h3 id="8-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><a href="#8-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization" class="headerlink" title="8. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization"></a>8. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07972v2">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</a></h3><p>While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型（LLM）在推理和基于主体的问题解决方面取得了显著进步，但当前的评估方法未能充分评估其能力：现有基准要么依赖易被饱和和记忆的封闭式问题，要么依赖缺乏一致性和严谨性的主观比较。在本研究中，我们介绍了HeuriGym，这是一个为评估由大型语言模型生成的组合优化问题启发式算法而设计的代理框架，其特点是目标明确且解空间广阔。HeuriGym 赋能大型语言模型提出启发式方法，通过代码执行获得评估反馈，并迭代完善解决方案。我们评估了涵盖计算机系统、物流和生物学等九个领域的九个最先进模型，揭示了工具使用、规划和自适应推理中持续存在的局限性。为了量化性能，我们提出了质量-产出指数（QYI），这是一个既能反映解法通过率又能体现质量的指标。即使是像GPT-o4-mini-high和Gemini-2.5-Pro这样的顶级模型，QYI分数也仅为0.6，远低于专家的基准1分。我们的开源基准旨在引导大型语言模型在科学和工程领域实现更有效、更现实的问题解决。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决如何有效评估大型语言模型（LLMs）在组合优化问题上的能力。现有的评估方法存在局限性，主要分为两类：基于封闭问题的客观基准和基于人类偏好的主观评估。前者容易受到数据污染和性能饱和的影响，后者则缺乏一致性和严谨性。为了克服这些局限性，作者提出了一个名为HeuriGym的新框架，旨在通过迭代生成、执行和反馈机制来评估LLMs生成的启发式算法在组合优化问题上的表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了与LLMs在组合优化（CO）领域以及LLMs评估相关的研究。以下是相关研究的总结：</p>
<p>LLMs for Combinatorial Optimization</p>
<ul>
<li><strong>NL4Opt Competition</strong>：将自然语言转化为结构化的优化问题，后续工作致力于改进特定领域的模型训练和提示策略，但这些方法依赖于精确求解器，难以扩展。</li>
<li><strong>FunSearch 和 AlphaEvolve</strong>：FunSearch 使用LLMs生成启发式算法，而AlphaEvolve结合进化搜索生成新的启发式算法。但这些方法需要评估数千个候选方案，效率较低。</li>
<li><strong>Metaheuristic Templates</strong>：最近的一些方法通过元启发式模板改进效率，但仍然限制LLMs只填充评分函数，而不是设计完整的算法。</li>
</ul>
<p>Evaluation on LLMs</p>
<ul>
<li><strong>HumanEval、BigCodeBench、LiveCodeBench</strong>：这些基准测试侧重于闭合问题，如数学问题、编程任务等，存在固定答案，容易受到数据污染。</li>
<li><strong>Chatbot Arena、KernelBench</strong>：这些基准测试鼓励多样化的输出，但往往缺乏明确的目标，导致评估不一致。</li>
<li><strong>NPHardEval、GraphArena</strong>：这些基准测试评估小规模NP难问题的精确解，但实际应用中更倾向于使用启发式方法以实现可扩展性。</li>
</ul>
<p>HeuriGym框架通过提供一个开放式的解决方案空间，允许LLMs生成完整的、自包含的优化程序，包括自定义数据结构和端到端的流程，从而更贴近现实世界的CO挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过提出一个名为 <strong>HeuriGym</strong> 的新框架来解决评估 LLMs 在组合优化问题上的能力的问题。HeuriGym 是一个以代理（agentic）为核心的框架，它通过以下方式实现对 LLMs 的全面评估：</p>
<p>1. <strong>框架概述</strong></p>
<p>HeuriGym 框架的核心思想是让 LLMs 生成启发式算法，然后通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。这个过程模拟了实际工程中的工作流程，能够深入评估 LLMs 的多步推理、工具使用和指令遵循能力。</p>
<p>2. <strong>问题描述与提示设计</strong></p>
<ul>
<li><strong>问题描述</strong>：每个基准任务都包含一个结构化的问题描述，包括背景、正式化和输入/输出格式。背景部分介绍优化的上下文和关键术语，正式化部分定义优化目标和约束条件，输入/输出格式部分则明确输入和输出文件的结构。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中，以便 LLMs 学习并改进其输出。</li>
</ul>
<p>3. <strong>反馈循环</strong></p>
<p>HeuriGym 将数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。每次迭代后，生成的解决方案、执行日志、验证结果和评估分数都会被记录并反馈给 LLM，使其能够从过去的尝试中学习并逐步改进输出。</p>
<p>4. <strong>评估指标</strong></p>
<ul>
<li><strong>solves@i</strong>：跟踪 LLM 在 i 次迭代内解决约束问题的能力。这个指标分为三个阶段：执行、解决方案生成和验证。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：这是一个综合指标，结合了解决方案的通过率（Yield）和质量（Quality），通过计算这两个指标的调和平均值来评估 LLM 的整体表现。</li>
</ul>
<p>5. <strong>基准构建</strong></p>
<p>HeuriGym 包含了来自计算机系统、编译器、计算生物学、物流等领域的九个组合优化问题。这些问题被精心挑选，以确保它们具有明确的目标、庞大的解空间、可扩展的数据实例和可复现的专家基线。每个问题都包括一个任务特定的验证器和评估器，用于评估解决方案的通过率和质量。</p>
<p>6. <strong>实验与评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，包括 GPT-o4-mini-high、Gemini-2.5-Pro、LLaMA-4-Maverick 等。实验结果表明，即使是顶级模型，如 GPT-o4-mini-high 和 Gemini-2.5-Pro，在 QYI 评分上也仅为 0.6 左右，远低于专家基线的 1。这揭示了当前 LLMs 在现实世界问题解决中的局限性。</p>
<p>7. <strong>案例研究</strong></p>
<p>论文还通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。虽然 LLMs 能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</p>
<p>8. <strong>未来工作</strong></p>
<p>作者指出，HeuriGym 仍存在一些局限性，如 Python 执行的开销、迭代自优化过程的进一步改进、以及评估指标与现实世界部署之间的差距。未来的工作将包括扩展基准的广度和深度，引入新的问题，并探索更高效的测试时扩展策略。</p>
<p>通过这些方法，HeuriGym 为 LLMs 在组合优化问题上的评估提供了一个全面、严谨且实用的框架，有助于推动 LLMs 在科学和工程领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验：</p>
<p>1. <strong>模型评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，这些模型包括：</p>
<ul>
<li>OpenAI 的 GPT-o4-mini-high</li>
<li>Anthropic 的 Claude-3.7-Sonnet</li>
<li>DeepSeek 的 DeepSeek-V3 和 DeepSeek-R1</li>
<li>Google 的 Gemini-2.5-Flash 和 Gemini-2.5-Pro</li>
<li>Meta 的 LLaMA-3.3 和 LLaMA-4-Maverick</li>
<li>Alibaba 的 Qwen3-235B</li>
</ul>
<p>这些模型在多个领域（如计算机系统、物流、生物学等）的九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</p>
<p>2. <strong>性能评估</strong></p>
<ul>
<li><strong>solves@i 指标</strong>：评估 LLM 在 i 次迭代内解决约束问题的能力。具体来说，solves@i 分为三个阶段：</li>
<li><strong>执行（Execution）</strong>：生成的程序是否能正确编译或解释，并成功执行基本的输入输出操作。</li>
<li><strong>解决方案生成（Solution Generation）</strong>：程序是否在预定义的超时时间内产生非空输出，并符合预期的输出格式。</li>
<li><strong>验证（Verification）</strong>：解决方案是否满足所有问题特定的约束条件。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：综合评估解决方案的通过率（Yield）和质量（Quality）。QYI 是这两个指标的调和平均值，能够全面反映 LLM 的性能。</li>
</ul>
<p>3. <strong>案例研究</strong></p>
<p>作者通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。具体步骤如下：</p>
<ul>
<li><strong>初始迭代</strong>：LLMs 生成基于动态规划（DP）的启发式算法，但未对搜索空间进行剪枝，导致计算时间过长。</li>
<li><strong>后续迭代</strong>：LLMs 逐渐引入剪枝策略，优化算法性能，最终在第五次迭代中达到了最佳的解决方案。</li>
</ul>
<p>4. <strong>消融研究</strong></p>
<ul>
<li><strong>温度（Temperature）</strong>：评估不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率。</li>
<li><strong>少样本演示（Few-shot Demonstrations）</strong>：研究在不同数量的演示示例下模型的性能。结果表明，提供更多的演示示例可以显著提高模型的性能。</li>
<li><strong>反馈轮次（Feedback Rounds）</strong>：研究不同数量的反馈轮次对模型性能的影响。结果表明，适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>5. <strong>错误分析</strong></p>
<p>作者对 LLMs 在生成启发式算法时常见的错误类型进行了分类和分析，包括：</p>
<ul>
<li><strong>API 调用错误</strong>：使用不存在或过时的库调用。</li>
<li><strong>算法逻辑错误</strong>：即使总体方法合理，但实现存在缺陷。</li>
<li><strong>约束误解</strong>：忽略或误解问题的约束条件。</li>
<li><strong>超时错误</strong>：未在给定的约束时间内产生输出。</li>
</ul>
<p>6. <strong>C++ 实验</strong></p>
<p>作者还进行了初步的 C++ 实验，将 LLMs 生成的 Python 代码转换为 C++ 代码，以评估 C++ 在执行速度上的优势。结果显示，即使在第一次迭代中，C++ 解决方案的通过率也高于经过 10 次迭代的 Python 解决方案。</p>
<p>这些实验为评估 LLMs 在组合优化问题上的能力提供了全面的视角，并揭示了当前 LLMs 的优势和局限性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文中提到了一些可以进一步探索的点，以下是一些关键方向：</p>
<p>1. <strong>扩展基准的广度和深度</strong></p>
<ul>
<li><strong>增加问题数量和多样性</strong>：HeuriGym 目前包含九个问题，但随着 LLM 能力的提升，这些基准可能会逐渐饱和。因此，需要不断引入新的、更具挑战性的组合优化问题，特别是来自尚未充分探索的科学领域。</li>
<li><strong>扩大问题规模</strong>：现有问题的数据实例规模有限，未来可以将现有问题扩展到更复杂、更大规模的实例，以更好地评估 LLMs 在大规模问题上的表现。</li>
</ul>
<p>2. <strong>改进测试时扩展策略</strong></p>
<ul>
<li><strong>测试时计算优化</strong>：当前的迭代自优化过程可以被视为一种测试时扩展（Test-time Scaling, TTS）。可以探索如何结合 Best-of-N 采样、束搜索（Beam Search）和进化算法等技术，特别是在增加迭代预算的情况下，以进一步提升 LLMs 的性能。</li>
<li><strong>自验证能力</strong>：利用强大的验证器，HeuriGym 提供了一个自然的平台来研究 LLMs 的自验证能力，这可能是实现更自主的 LLMs 的一个有前景的方向。</li>
</ul>
<p>3. <strong>评估指标与现实世界部署的对齐</strong></p>
<ul>
<li><strong>代理指标与实际性能的差距</strong>：当前的评估指标虽然在初步基准测试中很有用，但在反映现实世界性能方面往往存在不足。特别是在科学领域，解决方案的质量通常需要通过物理实验来验证；在工程领域，如 EDA，质量需要通过耗时的后端综合来确认。如何弥合代理评估和现实世界部署之间的差距，同时管理更长的反馈循环带来的延迟，是一个关键挑战。</li>
</ul>
<p>4. <strong>多语言支持</strong></p>
<ul>
<li><strong>C++ 集成</strong>：目前的实验主要在 Python 上进行，但 C++ 在某些任务中表现出更好的性能。未来可以进一步探索如何将 C++ 更好地集成到框架中，包括处理依赖于特定领域的库和生成高效、功能正确且并行的 C++ 代码。</li>
</ul>
<p>5. <strong>社区贡献和合作</strong></p>
<ul>
<li><strong>开放基准</strong>：HeuriGym 被设计为一个开放的基准，鼓励社区贡献新的问题。未来的工作将包括扩展基准的广度和深度，通过引入新问题和扩展现有问题到更复杂的领域，以保持基准的长期相关性。</li>
<li><strong>跨学科合作</strong>：通过标准化组合问题的格式和可复现的代理工作流程，HeuriGym 可以作为一个共享的测试平台，促进跨学科合作。</li>
</ul>
<p>6. <strong>模型改进</strong></p>
<ul>
<li><strong>迭代改进策略</strong>：研究如何改进 LLMs 的迭代改进策略，使其能够更有效地从反馈中学习并优化解决方案。</li>
<li><strong>模型架构和训练方法</strong>：探索新的模型架构和训练方法，以提高 LLMs 在组合优化问题上的性能。</li>
</ul>
<p>7. <strong>应用领域扩展</strong></p>
<ul>
<li><strong>实际应用</strong>：将 HeuriGym 中的评估方法应用于实际的工业和科学问题，验证其在现实世界中的有效性。</li>
<li><strong>跨领域应用</strong>：探索 LLMs 在其他领域的应用，如医疗、金融等，这些领域同样需要复杂的推理和优化能力。</li>
</ul>
<p>这些方向为未来的研究提供了广阔的空间，有助于推动 LLMs 在组合优化问题上的应用和发展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了一个名为HeuriGym的框架，旨在评估大型语言模型（LLMs）在组合优化问题上生成启发式算法的能力。HeuriGym通过一个代理（agentic）框架，让LLMs提出解决方案，接收执行反馈，并迭代改进，从而全面评估LLMs的多步推理、工具使用和指令遵循能力。该框架通过定义明确目标和广阔解空间的组合优化问题，克服了现有评估方法的局限性，为LLMs的评估提供了一个更加严谨和实用的平台。</p>
<p>背景知识</p>
<ul>
<li><strong>LLMs的进展</strong>：LLMs在复杂推理和基于代理的问题解决方面取得了显著进展，但现有评估方法未能充分评估其能力。传统基准测试依赖于封闭问题，容易受到数据污染和性能饱和的影响；而基于人类偏好的主观评估则缺乏一致性和严谨性。</li>
<li><strong>组合优化问题</strong>：这类问题在多个领域（如计算机系统、科学推理、计算生物学、物流等）中具有基础性地位，适合用于评估LLMs，因为它们需要算法知识、启发式推理、权衡导航和创造性问题解决能力。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>HeuriGym框架</strong>：该框架包括问题描述、提示设计、反馈循环和评估指标。LLMs根据问题描述生成启发式算法，通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。</li>
<li><strong>问题描述</strong>：每个基准任务都包含背景、正式化和输入/输出格式的结构化描述。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中。</li>
<li><strong>反馈循环</strong>：数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。</li>
<li><strong>评估指标</strong>：提出了solves@i和Quality-Yield Index (QYI)两个指标。solves@i跟踪LLMs在i次迭代内解决约束问题的能力，而QYI则综合评估解决方案的通过率和质量。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>模型评估</strong>：对九个最先进的LLMs模型进行了评估，包括GPT-o4-mini-high、Claude-3.7-Sonnet、DeepSeek-V3、DeepSeek-R1、Gemini-2.5-Flash、Gemini-2.5-Pro、LLaMA-3.3、LLaMA-4-Maverick和Qwen3-235B。这些模型在九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</li>
<li><strong>性能评估</strong>：实验结果显示，即使是顶级模型，如GPT-o4-mini-high和Gemini-2.5-Pro，在QYI评分上也仅为0.6左右，远低于专家基线的1。这揭示了当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>案例研究</strong>：通过技术映射问题展示了LLMs的潜力和当前局限性。虽然LLMs能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</li>
<li><strong>消融研究</strong>：研究了不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响，以及少样本演示和反馈轮次对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率；适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>性能差距</strong>：即使是顶级LLMs模型，在组合优化问题上的表现也远低于专家基线，表明当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>迭代改进的重要性</strong>：通过迭代改进，LLMs能够逐步优化其解决方案，但与专家工具相比仍有显著差距。</li>
<li><strong>评估指标的有效性</strong>：solves@i和QYI指标能够有效评估LLMs在组合优化问题上的表现，为未来的LLMs评估提供了新的视角。</li>
<li><strong>未来工作方向</strong>：HeuriGym框架为未来的LLMs评估提供了一个有前景的平台，未来的工作可以包括扩展基准的广度和深度、改进测试时扩展策略、弥合代理评估和现实世界部署之间的差距等。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是HeuriGym%3A%20An%20Agentic%20Benchmark%20for%20LLM-Crafted%20Heuristics%20in%20Combinatorial%20Optimization，链接是https://arxiv.org/pdf/2506.07972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.07972。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07972v2.pdf">https://arxiv.org/pdf/2506.07972v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07972v2">https://papers.cool/arxiv/2506.07972v2</a></p>
<p><strong>Published</strong>: 2025-06-09T17:46:47Z</p>
<hr>
<h3 id="9-Post-Training-Fairness-Control-A-Single-Train-Framework-for-Dynamic-Fairness-in-Recommendation"><a href="#9-Post-Training-Fairness-Control-A-Single-Train-Framework-for-Dynamic-Fairness-in-Recommendation" class="headerlink" title="9. Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation"></a>9. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20848v1">Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</a></h3><p>Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/weixinchen98/Cofair">https://github.com/weixinchen98/Cofair</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管越来越多的努力试图减少推荐系统中的不公平，现有的公平意识方法通常在培训时确定公平要求，并提供有限的培训后灵活性。然而，在现实中，不同利益相关者可能对不同的公平要求进行时间推移，因此针对不同公平要求的重新培训变得困难。为解决这一局限，我们提出了Cofair，一个单一列车框架，实现培训后推荐的公平控制。具体来说，Cofair引入了与公平性条件适配器模块的共享表示层，以生成针对不同公平度水平的用户嵌入，并引入用户级正则化项，保证在这些层级上实现单调公平性提升。我们理论上确立了Cofair上界人口统计平价的对抗目标和正则化项在用户层面强制执行渐进公平性。对多个数据集和骨干模型的全面实验表明，我们的框架在不同层级提供了动态公平性，能够提供与最先进基线相当甚至更好的公平性与准确性曲线，而无需为每个新的公平性要求重新训练。我们的代码公开发布于 <a target="_blank" rel="noopener" href="https://github.com/weixinchen98/Cofair。">https://github.com/weixinchen98/Cofair。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对推荐系统中“训练后公平性不可调”的瓶颈：现有公平性方法在训练阶段就把公平强度固定死，一旦上线后监管部门、平台或用户提出新的公平要求，就必须重新训练整个模型，代价极高。为此，作者提出 Cofair 框架，<strong>仅用一次训练即可在推理阶段按需调节公平强度</strong>，实现“训练后公平性动态控制”，避免反复重训。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线，均围绕“如何在推荐系统里注入公平性”展开，但各自局限明显：</p>
<ol>
<li><strong>公平性注入但无训练后可调能力</strong></li>
</ol>
<ul>
<li>正则/约束类：FOCF、UGF、FairRec、ComFair、FairGo 等，通过加正则项或重排直接优化 DP、EOpp 等指标，训练后公平强度即固化。</li>
<li>对抗表示类：AFRL、Zemel et al. 2013、Madras et al. 2018 等，用对抗网络把敏感信息从用户表示中剔除，同样需为每个公平阈值重训。</li>
<li>理论保证类：Cui et al. 2023、Song et al. 2019、Gupta et al. 2021 等，在训练阶段把不公平度显式约束在 ε 以内，但 ε 一旦变更必须重新求解约束优化。</li>
</ul>
<ol>
<li><strong>“可控”公平性初步探索，但仍无法细粒度调节强度</strong></li>
</ol>
<ul>
<li>属性级控制：Li et al. 2021 SIGIR、AFRL 2024 SIGIR，允许上线后选择“保护哪些敏感属性”，但保护程度（公平等级）仍由训练时超参锁定。</li>
<li>训练阶段可控：同上理论保证类方法，可控范围仅限训练前，推理期无法“旋钮式”微调。</li>
</ul>
<p>综上，<strong>现有工作要么完全重训，要么只能切换属性而不能平滑调节公平强度</strong>；Cofair 首次在单次训练中产出连续公平等级，填补了这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>Cofair 把“训练后公平性可调”转化为<strong>单次训练、多等级输出</strong>的架构-正则联合优化问题，核心手段有三点：</p>
<ol>
<li><strong>共享表示 + 公平等级适配器</strong></li>
</ol>
<ul>
<li>共享层  S  一次性提取跨等级通用用户特征  s_u ；</li>
<li>每个公平等级  t  拥有轻量适配器  P^((t)) ，只学习“该等级需要去除的敏感信息”，推理时按需求切换适配器即可，无需重训主干。</li>
</ul>
<ol>
<li><strong>用户级单调正则</strong><br>引入逐用户 adversarial BCE 损失序列  L_(fair)^((t))(u) ，通过 softplus 惩罚</li>
</ol>
<p>L<em>(reg)=∑_u∑</em>(t=1)^(T-1)softplus!(L<em>(fair)^((t+1))(u)-L</em>(fair)^((t))(u))</p>
<p>保证随着  t  增大，任何用户的公平性不会倒退，实现“旋钮向右转，公平只增不减”。</p>
<ol>
<li><strong>自适应公平权重</strong><br>训练过程中动态调整各等级权重  λ_t ：</li>
</ol>
<p>λ<em>(t+1)=λ_t+eta(1-L</em>(fair)^((t+1))L_(fair)^((t)))</p>
<p>若相邻等级公平提升不足，则自动加大  λ_(t+1) ，无需人工逐格调参。</p>
<p>最终目标函数</p>
<p>min<em>(Theta)max</em>(θ<em>d)(1) / (T)∑</em>(t=1)^T!l(L<em>(rec)^((t))+λ_t L</em>(fair)^((t))r)+β L_(reg)</p>
<p>一次求解后，推理阶段仅通过选取不同  t  即可输出从“高精度-低公平”到“高公平-低偏差”整条帕累托曲线，完成<strong>训练后公平性动态控制</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕 5 个研究问题（RQ1–RQ5）展开，全部在公开数据集 MovieLens-1M 与 Lastfm-360K 上完成，采用 BPR、LightGCN 两种骨干模型，性别为敏感属性。核心实验内容如下：</p>
<ul>
<li><p><strong>RQ1 整体性能</strong><br>与 4 个强基线（ComFair、FairRec、FairGo、AFRL）对比 fairness-accuracy Pareto 曲线。Cofair 仅训练 1 次，基线需为每个公平强度重训。结果：16 组设定中 15 组 Cofair 取得最帕累托高效曲线（p &lt; 0.05），且可覆盖更宽的公平值区间。</p>
</li>
<li><p><strong>RQ2 消融实验</strong><br>依次移除共享表示层（SRL）、公平适配器（FCA）、自适应权重（AWL）、用户级正则（URL）。指标 DP@10 平均劣化 10.13%，验证各组件对可控性与单调提升的必要性。</p>
</li>
<li><p><strong>RQ3 超参数敏感度</strong><br>扫描初始公平系数 λ0、更新步长 η、正则强度 β，统计 5 个公平等级的均值与方差。发现中等 λ0、η、β 可在公平改善幅度与精度稳定性之间取得最佳折中。</p>
</li>
<li><p><strong>RQ4 框架通用性</strong><br>将 Cofair 作为“插件”嵌入到 4 个基线（即“原方法 + Cofair”）。结果：所有增强版在单次训练内即可生成连续公平曲线，且帕累托效率持平或优于原多次重训版本，证明可无缝移植。</p>
</li>
<li><p><strong>RQ5 效率评估</strong><br>记录达到 5 个公平等级所需总训练时间。Cofair 每 epoch 耗时约为最快基线的 2 倍，但仅需 1/5 的 epoch 数，总体训练时间减少约 4 倍，验证其在大规模场景下的可扩展性。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向（按研究价值与可行性排序）：</p>
<ol>
<li><p><strong>多公平准则统一控制</strong><br>目前需更换 adversarial 目标才能从 DP 切换到 EOpp 等。可研究单一模型同时输出“DP-等级”“EOpp-等级”“个体公平-等级”等多维旋钮，实现跨准则联合调控。</p>
</li>
<li><p><strong>连续公平强度</strong><br>现框架离散 t=1… T 。将适配器权重或条件向量改为连续变量 $ τ ∈<br>0,1<br>$，利用超网络或神经 ODE 生成对应表示，实现真正无级调节。</p>
</li>
<li><p><strong>多敏感属性与交叉公平</strong><br>论文仅验证二元性别。可扩展至种族-性别-年龄等多维敏感属性，并显式约束交叉子群（intersectionality）公平，避免“平均公平掩盖局部不公”。</p>
</li>
<li><p><strong>提供者侧公平联动</strong><br>仅控制用户侧 DP/EOpp。可同时引入提供者侧曝光或收益约束，实现“用户-商品”双端公平，并探索两端旋钮之间的耦合效应。</p>
</li>
<li><p><strong>动态数据分布下的在线调节</strong><br>真实场景用户行为与敏感分布随时间漂移。可结合在线适配或强化学习，让公平系数  λ_t  随漂移信号自动微调，保持长期公平性。</p>
</li>
<li><p><strong>轻量化部署</strong><br>适配器虽轻量，但  T  个等级仍占显存。研究权重共享、低秩分解或二值 mask，使得移动端可只保存一套公共参数与  ≤ 1  MB 的增量，实现端侧实时调节。</p>
</li>
<li><p><strong>可解释性接口</strong><br>为每个等级生成自然语言或可视化解释（如“该等级使女性用户 Top-10 曝光占比提升 6%”），帮助监管/运营方理解不同旋钮的业务含义。</p>
</li>
<li><p><strong>公平-收益-成本三目标优化</strong><br>平台关心 GMV、广告收入等指标。将收益损失或运营成本显式建模为第三轴，研究三目标帕累托前沿，支持高层决策。</p>
</li>
<li><p><strong>跨域迁移与冷启动</strong><br>探索把已训练好的公平适配器迁移到新领域或新敏感属性，仅微调少量参数即可快速获得可控制公平性，减少新场景数据需求。</p>
</li>
<li><p><strong>合规审计与鲁棒攻击</strong><br>设计对抗攻击刻意触发公平性退化，检验单调正则的鲁棒性；同时提供可验证的审计接口，满足欧盟 AI Act 等监管“可验证公平”要求。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：既有公平推荐方法把公平强度“焊死”在训练阶段，一旦政策/业务需要不同公平级别就得重训，代价高。</li>
<li><strong>思路</strong>：Cofair 用“单次训练、推理时拨档”取代反复重训，实现<strong>训练后公平性动态控制</strong>。</li>
<li><strong>架构</strong><br>– 共享表示层：提取跨等级通用用户特征，避免重复计算。<br>– 公平等级适配器：每个等级  t  一个轻量网络，仅学习该等级需去除的敏感信息；推理时按需调用。<br>– 用户级单调正则：强制每个用户的公平性随等级提升而<strong>只增不减</strong>，防止个体倒退。<br>– 自适应权重：训练期自动调节各等级公平系数，无需人工逐格调参。</li>
<li><strong>理论</strong>：证明 adversarial 公平损失是 demographic parity 差值的上界，且正则项收敛时必得单调公平改进。</li>
<li><strong>实验</strong>：在 MovieLens-1M、Lastfm-360K 与 BPR/LightGCN 上，一次训练即可生成覆盖多种公平-精度权衡的帕累托曲线，<strong>16 组对比 15 组最优</strong>，总体训练时间减少约 4 倍；可即插即用到现有公平方法，显著提升其灵活性而不降性能。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Post-Training%20Fairness%20Control%3A%20A%20Single-Train%20Framework%20for%20Dynamic%20Fairness%20in%20Recommendation，链接是https://arxiv.org/pdf/2601.20848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20848。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Weixin Chen, Li Chen, Yuhan Zhao</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CY, cs.IR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20848v1.pdf">https://arxiv.org/pdf/2601.20848v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20848v1">https://papers.cool/arxiv/2601.20848v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:48:43Z</p>
<hr>
<h3 id="10-Fail-Fast-Win-Big-Rethinking-the-Drafting-Strategy-in-Speculative-Decoding-via-Diffusion-LLMs"><a href="#10-Fail-Fast-Win-Big-Rethinking-the-Drafting-Strategy-in-Speculative-Decoding-via-Diffusion-LLMs" class="headerlink" title="10. Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs"></a>10. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20573v3">Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</a></h3><p>Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM’s speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It “fails fast” by spending minimal compute in hard-to-speculate regions to shrink speculation latency and “wins big” by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.7$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at <a target="_blank" rel="noopener" href="https://github.com/ruipeterpan/failfast">https://github.com/ruipeterpan/failfast</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>扩散大型语言模型（dLLMs）提供快速、并行的令牌生成，但其单独使用存在固有的效率与质量权衡。我们表明，如果谨慎运用，dLLM的属性实际上可以成为利用自回归（AR）验证器进行推测解码的起草者的优势。我们的核心见解是，dLLM通过并行解码的速度极大降低了昂贵的拒绝风险，提供了一种实用机制，有效实现那些（难以捉摸的）冗长草稿，从而通过推测解码实现大幅加速。我们介绍FailFast，一个基于dLLM的推测解码框架，通过动态调整推测长度实现这一方法。它通过在难以投机的地区花费极少的计算来“快速失败”，以减少投机延迟;而在较容易的地区积极延长选秀长度以降低验证延迟（很多情况下，一次投机并接受70个代币！）则“大获全胜”。无需微调，FailFast 实现了增强现实大型语言模型的无损加速，在不同模型和工作负载下，在不同模型和工作负载下，速度可比原版解码提升4.9美元/时间美元，比最优秀的 dLLM 起草器快 1.7 美元\时间美元，EAGLE-3 快达 1.7 美元/时间。我们开源了 FailFast at <a target="_blank" rel="noopener" href="https://github.com/ruipeterpan/failfast。">https://github.com/ruipeterpan/failfast。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>扩散大语言模型（dLLM）作为独立生成器时固有的效率-质量权衡难题</strong>，并将其转化为<strong>投机解码（speculative decoding）场景下的优势</strong>。核心问题与目标可归纳为：</p>
<ol>
<li><p><strong>dLLM 独立生成的根本矛盾</strong><br>并行解码带来低延迟，却因条件独立假设而牺牲 token 间依赖建模，导致质量随并行步长增加而显著下降。</p>
</li>
<li><p><strong>投机解码中“草稿长度 vs. 接受率”两难</strong><br>传统自回归草稿模型每步只能生成 1 个 token，想加长草稿就必须串行更多步，延迟呈线性上升；而固定短草稿又导致验证器频繁调用，内存带宽瓶颈凸显。</p>
</li>
<li><p><strong>序列内部难度差异被静态草稿长度忽视</strong><br>同一序列中“易推测”区域（复制、摘要、简单计算）与“难推测”区域（多步推理、知识检索）并存，静态长度无法在易区域“放大收益”，也难在难区域“及时止损”。</p>
</li>
</ol>
<p>为此，作者提出 <strong>FailFast 框架</strong>，用 dLLM 做草稿器、自回归 LLM 做验证器，通过<strong>动态调节草稿长度</strong>实现：</p>
<ul>
<li><strong>Fail Fast</strong>：在难区域用极低计算预算（通常 1 次前向）迅速放弃，减少无效草稿延迟。</li>
<li><strong>Win Big</strong>：在易区域利用 dLLM 并行能力一次性草稿多达 70 个 token，显著降低验证器调用次数与内存加载开销。</li>
</ul>
<p>最终，在不进行任何微调的情况下，FailFast 在多个模型与任务上取得 <strong>1.9–4.9× 无损加速</strong>，超越现有最佳基线（Fast-dLLM、EAGLE-3 等）。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，可按主题归类为：</p>
<ul>
<li><strong>扩散语言模型（dLLM）</strong></li>
<li><em>Mercury</em> (Khanna et al., 2025)</li>
<li><em>Seed-Diffusion</em> (Song et al., 2025)</li>
<li><em>LLaDA 2.0</em> (Bie et al., 2025)</li>
<li><em>Dream-7B</em> (Ye et al., 2025)</li>
<li><em>Fast-dLLM v1/v2</em> (Wu et al., 2025a;b)</li>
<li><em>Block Diffusion</em> (Arriola et al., 2025)</li>
<li><strong>投机解码（Speculative Decoding）基础框架</strong></li>
<li>Leviathan et al., 2023（经典自回归草稿+并行验证）</li>
<li>Stern et al., 2018（Blockwise Parallel Decoding）</li>
<li><strong>增强草稿质量/接受率</strong></li>
<li><em>Medusa</em> (Cai et al., 2024) – 多头解码</li>
<li><em>EAGLE-3</em> (Li et al., 2025) – 单层可训练草稿器</li>
<li><em>SpecInfer</em> (Miao et al., 2024) – 树形验证</li>
<li><em>Sequoia</em> (Chen et al., 2024) – 鲁棒树验证</li>
<li><em>REST</em> (He et al., 2023) – 检索式草稿</li>
<li><em>Lookahead</em> (Fu et al., 2024) – n-gram 表</li>
<li><em>SpecDiff-1/2</em> (Christopher et al., 2025; Sandler et al., 2025) – 微调 dLLM 草稿器</li>
<li><strong>动态/自适应草稿长度</strong></li>
<li><em>SpecDec++</em> (Huang et al., 2024) – 基于接受率调整长度</li>
<li><em>Online Speculative Decoding</em> (Liu et al., 2023) – 运行时选草稿器</li>
<li><strong>非参数化或轻量草稿</strong></li>
<li><em>SuffixDecoding</em> (Oliaro et al., 2024) – 后缀树匹配</li>
<li><strong>系统与优化</strong></li>
<li>vLLM (Kwon et al., 2023) – 高吞吐推理引擎</li>
<li><em>Prefix Caching</em> (Pan et al., 2025b) – KV 缓存复用</li>
<li><em>Sarathi-Serve</em> (Agrawal et al., 2024) – 吞吐-延迟权衡</li>
</ul>
<p>这些工作共同构成了 FailFast 的对比基线、设计灵感或工程实现基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“<strong>扩散大语言模型（dLLM）</strong>”从“<strong>独立生成器</strong>”重新定位为“<strong>投机草稿器</strong>”，并围绕两大观察设计 <strong>FailFast</strong> 框架，从而一次性解决“草稿延迟-验证延迟-接受率”三角矛盾。具体做法可概括为 <strong>两条原则、一个信号、一套算法</strong>：</p>
<p>1. 两条原则</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原则</th>
<th>目的</th>
<th>实现手段</th>
</tr>
</thead>
<tbody>
<tr>
<td>Fail Fast</td>
<td>在难推测区域最小化草稿延迟</td>
<td>只给 dLLM 1 次前向（单步去噪），快速生成一批 token；即使错误率高也不追加计算，靠验证器兜底。</td>
</tr>
<tr>
<td>Win Big</td>
<td>在易推测区域最大化验证吞吐</td>
<td>只要当前 token 置信度全部高于阈值 τ，就原地再追加 N 个 token；循环扩张，一轮草稿最长可达 Nmax=70，显著减少验证器调用次数。</td>
</tr>
</tbody>
</table>
</div>
<p>2. 一个信号：token-level 置信度</p>
<p>dLLM 虽非自回归，但仍输出每个掩码位置的分布</p>
<p>C<em>i = max</em>(v∈ V) P_i(x_i=v)</p>
<p>用 <strong>Ci≥τ</strong> 作为“区域易推测”的实时代理，无需人工标注或离线 profiling。</p>
<p>3. 一套算法：动态扩张草稿长度</p>
<p>伪代码（与论文 Alg.1 等价）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">每轮初始化 L = 0</span><br><span class="line">while True:</span><br><span class="line">用 dLLM 一次前向推测下 N 个 token</span><br><span class="line">L += N</span><br><span class="line">if 任一 token 置信度 &lt; τ 或 L≥Nmax:</span><br><span class="line">break</span><br><span class="line">提交 L 个 token 给验证器做并行 chunked-prefill</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>扩张步长 N</strong> 与 <strong>最大长度 Nmax</strong> 为唯一超参，跨任务固定即可。</li>
<li>易区域可连续扩张 → 一轮 60+ token；难区域第一次遇到低置信即停止 → 一轮 10 token 以内，实现<strong>长度自适配</strong>。</li>
</ul>
<p>4. 系统级优化</p>
<ul>
<li><strong>超低草稿延迟</strong>：dLLM 并行解码 + 近似 KV-cache（Fast-dLLM 技术）使单步前向仅 0.3–0.5× 自回归草稿延迟。</li>
<li><strong>验证延迟不变但次数锐减</strong>：长草稿把 100+ 轮验证压缩到 70 轮左右，每轮仍是一次 memory-bound 短 prefill，整体验证时间 ↓17%。</li>
<li><strong>无损生成</strong>：验证器为原始自回归 LLM，拒绝点立即回滚，保证分布与贪心/采样解码完全一致。</li>
</ul>
<p>5. 结果总结</p>
<ul>
<li><strong>端到端加速</strong>：1.9–4.9× 超越原生解码；1.2–1.7× 超越 Fast-dLLM；1.1–1.4× 超越 EAGLE-3。</li>
<li><strong>零微调</strong>：直接拿现成 1.5 B dLLM 当草稿器，无需任何梯度更新或蒸馏。</li>
</ul>
<p>通过“<strong>用速度换长度，用长度换验证次数，用验证次数换总延迟</strong>”，论文把 dLLM 的“低质高速”劣势转化为投机场景下的<strong>绝对优势</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕 <strong>“FailFast 能否在无损精度下取得更高端到端加速”</strong> 展开，覆盖模型规模、数据集、硬件、超参、消融五个维度。主要实验内容如下：</p>
<p>1. 端到端加速对比（主表 Table 1）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>取值</th>
</tr>
</thead>
<tbody>
<tr>
<td>目标模型</td>
<td>Qwen2.5-Instruct {7B, 14B, 32B}</td>
</tr>
<tr>
<td>草稿方案</td>
<td>AR-drafter、Fast-dLLM、EAGLE-3、FailFast</td>
</tr>
<tr>
<td>数据集</td>
<td>MATH、AIME、GSM8K、GPQA、HumanEval</td>
</tr>
<tr>
<td>指标</td>
<td>相对原生自回归解码的 wall-clock speedup</td>
</tr>
</tbody>
</table>
</div>
<p>结果：FailFast 平均加速 <strong>4.0×（32B）、3.3×（14B）、2.5×（7B）</strong>，全面高于最佳基线。</p>
<p>2. 延迟拆解（Figure 4 &amp; Table 3）</p>
<ul>
<li>把总延迟拆成 <strong>草稿延迟 + 验证延迟</strong>。</li>
<li>FailFast 在 32B/MATH 上：<br>– 草稿延迟再降 <strong>41%</strong>（vs Fast-dLLM）<br>– 验证延迟降 <strong>17%</strong>（轮次减少 16%）</li>
<li>单轮可接受 <strong>最多 70 token</strong>，而 AR 基线固定 8 token。</li>
</ul>
<p>3. 质量与长度统计（Table 3）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>AR-drafter</th>
<th>Fast-dLLM</th>
<th>FailFast</th>
</tr>
</thead>
<tbody>
<tr>
<td>平均接受率</td>
<td>55%</td>
<td>56%</td>
<td>34%（更低）</td>
</tr>
<tr>
<td>平均草稿长度</td>
<td>7.2</td>
<td>7.2</td>
<td>12.7（更长）</td>
</tr>
<tr>
<td>每轮草稿器前向次数</td>
<td>7.2</td>
<td>5.5</td>
<td>3.3（更少）</td>
</tr>
<tr>
<td>每轮接受 token 数</td>
<td>4.0</td>
<td>4.0</td>
<td>4.6（更高）</td>
</tr>
</tbody>
</table>
</div>
<p>说明：虽然接受率下降，但<strong>单位前向产出更多可用 token</strong>，总轮数减少，最终更快。</p>
<p>4. 超参敏感性（Figure 6）</p>
<ul>
<li>τ∈<br>0.3,0.55<br>性能平坦；τ&gt;0.7 过度保守，τ&lt;0.1 浪费计算。</li>
<li>Nmax∈<br>30,70<br>差异 <3%；>70 验证阶段变为 compute-bound，收益递减。</li>
</ul>
<p>5. 与动态长度基线对比（附录 D）</p>
<ul>
<li><strong>Fast-dLLM+</strong>：对每份数据单独网格搜索最优固定长度，仅比默认长度提升 2.3–3.3%，仍低于 FailFast。</li>
<li><strong>SuffixDecoding</strong>：在 HumanEval/MATH 上几乎无加速（&lt;5%），因重复片段稀少。</li>
<li><strong>EAGLE-3</strong>：在 7B 上 FailFast 再快 <strong>1.1–1.4×</strong>；EAGLE-3 树深只有 8，无法利用长易片段。</li>
</ul>
<p>6. 轨迹可视化（Appendix F）</p>
<p>给出 MATH 题完整解码轨迹：</p>
<ul>
<li>易区域（公式、分数相加）连续 60/60 token 被接受；</li>
<li>难区域（首次出现推理错误）仅 10 token 即停止；</li>
<li>直观展示“fail fast / win big”交替出现。</li>
</ul>
<p>7. 硬件与工程细节</p>
<ul>
<li>2×A6000-48 GB，vLLM + prefix caching，TP=2（目标模型）/TP=1（草稿器）。</li>
<li>温度=0，输出上限 1024 token，batch=1（典型交互场景）。</li>
</ul>
<p>综上，实验从<strong>宏观加速</strong>到<strong>微观轨迹</strong>，从<strong>单变量敏感性</strong>到<strong>强基线对比</strong>，系统验证了 FailFast 的普适性与工程落地价值。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可<strong>直接继承 FailFast 框架</strong>继续深挖，亦可在<strong>理论、系统、应用</strong>层面拓展：</p>
<p>1. 草稿器侧：更“准”或更“小”</p>
<ul>
<li><strong>数据-参数-计算三匹配消融</strong><br>目前 dLLM 与目标模型训练数据不完全一致，可构造<strong>同数据池、同参数规模、同 FLOPs</strong> 的受控实验，厘清性能差距多少来自“扩散 vs 自回归”本身，多少来自数据偏差。</li>
<li><strong>任务特定微调</strong><br>SpecDiff 已证明微调可提升接受率；FailFast 当前零样本。可探索<strong>轻量级 LoRA/Adapter</strong> 在易区域保持 1-step、在难区域启用 2-step 的<strong>混合计算模式</strong>。</li>
<li><strong>超小草稿器</strong><br>尝试 <strong>≤0.5 B</strong> 的纯扩散模型，验证“fail fast”极限：当草稿器容量极低时，是否仍能用<strong>更长草稿+验证器兜底</strong>获得净加速。</li>
</ul>
<p>2. 验证器侧：降低长草稿验证成本</p>
<ul>
<li><strong>分块级联验证</strong><br>70-token 一次性 prefill 仍可能 compute-bound。可设计<strong>两级验证</strong>：先让小模型过滤明显错误，再用大模型复核，保持分布无损。</li>
<li><strong>早期退出验证</strong><br>在 chunked-prefill 内部插入 <strong>“early-reject” 门控</strong>，一旦累积置信度低于阈值即停止剩余计算，减少浪费。</li>
</ul>
<p>3. 长度调度策略：从启发式到学习型</p>
<ul>
<li><strong>强化学习调度器</strong><br>将 τ、N、Nmax 作为策略网络输出，以<strong>实际 wall-clock 奖励</strong>为信号，学习<strong>序列级最优停止/扩张策略</strong>。</li>
<li><strong>难度预测器</strong><br>用小型编码器提前 1-2 句预测“后续 50 token 难度分布”，<strong>全局规划</strong>草稿长度，而非仅看当前已生成 token 置信度。</li>
</ul>
<p>4. 草稿复用与树形扩展</p>
<ul>
<li><strong>Suffix-Reuse+FailFast</strong><br>附录已验证单 query 15% 额外加速。可系统实现<strong>跨轮次后缀匹配+树形拼接</strong>，并配合<strong>哈希缓存</strong>避免重复前向。</li>
<li><strong>Diffusion-Tree</strong><br>利用 dLLM 一次可并行采样多条高置信路径，构造<strong>宽度&gt;1 的扩散树</strong>，在验证阶段用<strong>树形 attention</strong>一次性比对，提高接受 token 表面积。</li>
</ul>
<p>5. 系统级耦合优化</p>
<ul>
<li><strong>草稿-验证流水线重叠</strong><br>当前串行“草稿→验证”。可在 GPU 多流上<strong>异步执行</strong>：上一轮验证同时，下一轮 dLLM 草稿已启动，隐藏草稿延迟。</li>
<li><strong>KV-Caching 跨轮复用</strong><br>被拒绝后缀的 KV 仍可能部分正确，设计<strong>细粒度 KV 掩码复用</strong>，减少重复计算。</li>
</ul>
<p>6. 扩展场景与限制研究</p>
<ul>
<li><strong>高吞吐/大 batch</strong><br>投机解码在 batch↑ 时收益递减。量化 FailFast 的<strong>临界 batch size</strong>，并探索<strong>动态 Nmax 与 batch size 联动</strong>策略。</li>
<li><strong>随机温度场景</strong><br>当前实验温度=0。研究 <strong>τ 与采样温度耦合</strong>机制，保证分布无损的同时维持长草稿优势。</li>
<li><strong>长上下文 &gt;32 k</strong><br>验证器 prefill 成本随上下文长度线性增长。结合<strong>滑动窗口+局部草稿</strong>思想，测试 FailFast 在超长文档生成上的可扩展性。</li>
</ul>
<p>7. 理论分析</p>
<ul>
<li><strong>期望加速公式闭合解</strong><br>给出“dLLM 1-step 接受率 ∝ 区域难度”假设下的<strong>期望轮次数 E<br>R<br>、期望草稿长度 E<br>L
</strong> 闭合形式，指导超参选择。</li>
<li><strong>最优置信阈值 τ</strong>*<br>在“验证成本 vs 草稿错误成本”线性组合下，推导<strong>τ 的最优解析解</strong>，减少网格搜索。</li>
</ul>
<p>8. 与其他新范式融合</p>
<ul>
<li><strong>Diffusion+MoE</strong><br>用稀疏激活 MoE-dLLM 做草稿器，进一步降低每 step 计算量。</li>
<li><strong>Diffusion+SSM/Transformer-替代架构</strong><br>验证 FailFast 是否适用于 Linear Attention、Mamba 等非传统验证器。</li>
</ul>
<p>综上，FailFast 把“<strong>低质高速</strong>”转化为“<strong>长草稿+少验证</strong>”的思路仍处在<strong>早期生态阶段</strong>；从<strong>模型、调度、系统、理论</strong>四线并进，有望再推高 1.5–2× 加速上限，并推广至<strong>更长文本、更高吞吐、更复杂采样</strong>场景。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>FailFast: 用扩散大语言模型 (dLLM) 做投机草稿器，以“长草稿+少验证”实现无损加速</strong></p>
<p>1. 问题背景</p>
<ul>
<li>dLLM 并行解码快但质量低，独立生成时存在<strong>效率-质量权衡</strong>。</li>
<li>投机解码需平衡“草稿越长→潜在加速越大”与“越长→接受率指数下降”的矛盾。</li>
<li>序列内部有<strong>易/难区域</strong>之分，静态草稿长度无法在易区域“放大收益”，也难在难区域“及时止损”。</li>
</ul>
<p>2. 核心观察</p>
<ol>
<li>dLLM 每步可并行产出多 token，<strong>单步前向成本极低</strong>。</li>
<li>dLLM 的<strong>精度提升呈凹性</strong>：增加前向次数对接受率收益迅速递减。</li>
<li>易区域（复制、摘要、简单计算）即使 <strong>1-step 草稿</strong>也足够准确；难区域再多步也常被拒。</li>
</ol>
<p>3. FailFast 框架</p>
<p><strong>两条原则</strong></p>
<ul>
<li><strong>Fail Fast</strong>：难区域仅用 1 次前向，快速放弃，降低草稿延迟。</li>
<li><strong>Win Big</strong>：易区域用 token 置信度 Ci≥τ 作为信号，<strong>循环追加</strong> N 个 token，单轮最长 70+，减少验证轮次。</li>
</ul>
<p><strong>算法</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">while 当前块置信度全≥τ 且 长度&lt;Nmax:</span><br><span class="line">追加 N 个 token（1 次前向）</span><br><span class="line">提交长草稿给验证器并行 chunked-prefill</span><br></pre></td></tr></table></figure>
<p><strong>超参</strong></p>
<ul>
<li>τ∈<br>0.3,0.55<br>、N=10、Nmax=60，<strong>跨任务固定</strong>即可。</li>
</ul>
<p>4. 实验结果</p>
<ul>
<li><strong>加速</strong>：1.9–4.9× 超越原生解码；1.2–1.7× 超越 Fast-dLLM；1.1–1.4× 超越 EAGLE-3。</li>
<li><strong>零微调</strong>：直接拿现成 1.5 B dLLM 当草稿器。</li>
<li><strong>延迟拆解</strong>：草稿延迟再降 41%，验证延迟降 17%；单轮可接受 70 token。</li>
<li><strong>质量无损</strong>：验证器为原始自回归模型，分布完全一致。</li>
</ul>
<p>5. 贡献一句话</p>
<p><strong>把 dLLM 的“低质高速”劣势转化为投机场景下的“长草稿+少验证”优势</strong>，实现<strong>无损、通用、无需训练</strong>的 SOTA 加速。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Fail%20Fast%2C%20Win%20Big%3A%20Rethinking%20the%20Drafting%20Strategy%20in%20Speculative%20Decoding%20via%20Diffusion%20LLMs，链接是https://arxiv.org/pdf/2512.20573，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20573。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Rui Pan, Zhuofu Chen, Hongyi Liu, Arvind Krishnamurthy, Ravi Netravali</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.DC</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20573v3.pdf">https://arxiv.org/pdf/2512.20573v3.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.20573v3">https://papers.cool/arxiv/2512.20573v3</a></p>
<p><strong>Published</strong>: 2025-12-23T18:16:58Z</p>
<hr>
<h2 id="Evaluation-Domain-Papers"><a href="#Evaluation-Domain-Papers" class="headerlink" title="Evaluation Domain Papers"></a>Evaluation Domain Papers</h2><h3 id="1-Recursive-Language-Models-2"><a href="#1-Recursive-Language-Models-2" class="headerlink" title="1. Recursive Language Models"></a>1. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.24601v2">Recursive Language Models</a></h3><p>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse long-context tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLM-Qwen3-8B, outperforms the underlying Qwen3-8B model by $28.3\%$ on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们研究通过推理时间扩展的视角，使大型语言模型（LLMs）能够处理任意长度的提示。我们提出了递归语言模型（RLMs），这是一种通用的推理范式，将长提示视为外部环境的一部分，并允许LLM以编程方式检查、分解并递归调用自身来处理提示片段。我们发现，RLMs能够成功处理比模型上下文窗口大两个数量级的输入，即使对于较短的提示，在四个不同的长上下文任务中，其性能也显著优于普通的前沿LLM和常见的长上下文结构，同时成本相当。在小规模实验中，我们后训练了第一个原生递归语言模型。我们的模型RLM-Qwen3-8B在性能上平均比基础的Qwen3-8B模型高出28.3%，在三项长上下文任务中甚至接近普通GPT-5的质量。代码可在 <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现代大语言模型（LLM）在处理“任意长输入”时遭遇的两大瓶颈：</p>
<ol>
<li><p>物理上下文窗口限制<br>即使最新模型（如 GPT-5）的显式窗口已达 272K token，仍无法直接容纳 10M 乃至 100M 级别的长文档、代码库或多跳检索语料。</p>
</li>
<li><p>上下文腐烂（context rot）<br>在窗口内，随着输入变长，模型对早期信息的利用效率急剧下降，表现为任务复杂度越高、衰减越快（图 1）。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Recursive Language Models (RLMs)</strong>——一种<strong>推理时（inference-time）通用框架</strong>，把超长 prompt 视为外部环境变量，让 LLM 在 Python REPL 里用代码“窥视、分解、递归调用自身”处理片段，从而：</p>
<ul>
<li>将有效上下文长度扩展两个数量级（实验到 10M+ token）；</li>
<li>在多项长上下文基准上显著优于直接调用基座模型、摘要式代理、检索增强代理等基线，且成本持平或更低；</li>
<li>无需修改模型参数或重新训练，仅通过推理策略即可落地。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两大脉络，并指出 RLM 与它们的本质区别：</p>
<ol>
<li>长上下文系统（Long-Context LM Systems）</li>
</ol>
<ul>
<li><strong>架构-级扩展</strong>：通过重新训练或改进注意力机制来延长上下文，例如<br>– 线性偏置注意力 Press et al. 2022<br>– 结构化状态空间 Gu et al. 2022<br>– Infini-attention Munkhdalai et al. 2024</li>
<li><strong>外部脚手架（scaffold）</strong>：不改动模型，而在推理阶段压缩或管理上下文<br>– 有损管理：MemWalker（树状导航摘要）、ReSum（迭代压缩）、Claude-Code 式多轮摘要<br>– 显式记忆层级：MemGPT、Mem0、G-Memory 等把上下文拆成“主存/外存”<br><em>区别</em>：RLM 不预设任何摘要、检索或记忆结构，完全由模型在 REPL 里用代码自行决定如何切块、过滤、调用子 LLM，上下文管理被“隐式”地交给模型自己。</li>
</ul>
<ol>
<li>子 LLM 递归调用（Task Decomposition via sub-LM calls）</li>
</ol>
<ul>
<li>人设计流程：AutoGPT、ReAct、Claude sub-agents 等多按固定模板拆任务</li>
<li>模型自主拆：ViperGPT、THREAD、DisCIPL、ReDel、Context-Folding、AgentFold 等让 LLM 自己决定何时再调 LLM，但输入长度仍受基模型窗口限制<br><em>区别</em>：RLM 把“输入提示”本身卸载到外部环境，首次让递归调用可以<strong>符号级操作任意长字符串</strong>，从而突破窗口上限；先前方法只能递归拆“任务”，不能递归拆“输入”。</li>
</ul>
<p>综上，RLM 的核心差异可概括为：</p>
<blockquote>
<p>把提示当成环境变量，而非模型输入；用代码+递归子 LLM 实现<strong>输入长度</strong>的无限外扩，而非仅对任务做逻辑拆解。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“超长提示无法一次性塞进上下文”这一瓶颈重新定义为<strong>推理时计算</strong>问题，并借鉴“核外算法”（out-of-core）思想：数据远大于内存时，通过<strong>按需加载+符号化操作</strong>完成计算。具体方案称为 <strong>Recursive Language Models (RLMs)</strong>，其关键机制如下：</p>
<ol>
<li><p>把提示变成环境变量<br>启动一个 Python REPL，将完整提示  P  作为字符串变量 <code>context</code> 载入内存；模型不再直接接收  P  作为输入，而是接收一段<strong>固定系统提示</strong>+REPL 返回的<strong>截断执行结果</strong>。</p>
</li>
<li><p>用代码“窥视-分解-调用”<br>模型在 REPL 里写 Python 代码，可以</p>
</li>
</ol>
<ul>
<li>用 <code>print</code>/<code>regex</code>/<code>pandas</code> 等快速扫描、过滤、统计；</li>
<li>把 <code>context</code> 切成任意粒度（行、文档、token 块）；</li>
<li>通过内置函数 <code>llm_query(sub_prompt)</code> <strong>递归调用子 LLM</strong>（可以是自身轻量版或其他模型），子调用结果再写回 REPL 变量。</li>
</ul>
<ol>
<li><p>递归子调用不受窗口限制<br>每次 <code>llm_query</code> 只把<strong>当前必要片段</strong>喂给子模型，因此总输入长度可以远超基模型物理窗口；子模型输出被存入变量，供后续代码继续聚合、验证或拼接。</p>
</li>
<li><p>终止与答案返回<br>当代码逻辑判断已收集足够信息，模型执行<br><code>FINAL(answer)</code> 或 <code>FINAL_VAR(variable_name)</code><br>把结果返回给用户，整个轨迹在 REPL 内完成，无需人工设计摘要或检索流程。</p>
</li>
</ol>
<p>通过上述四步，RLM 将“长上下文”转化为<strong>可执行对象</strong>，用<strong>代码控制数据流+递归子 LLM 控制语义流</strong>，从而在 10M token 规模上仍保持高精度，且平均调用成本与基模型持平甚至更低。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 4 类信息密度递增的长上下文任务上展开系统实验，覆盖 32 K–11 M token 区间，对比 2 组前沿模型与 4 种推理策略，并给出成本-性能联合分析。实验设计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>基座模型</td>
<td>GPT-5（闭源）与 Qwen3-Coder-480B-A35B（开源）</td>
</tr>
<tr>
<td>对比方法</td>
<td>① Base 直接调用 ② CodeAct(+BM25) ③ Summary Agent ④ RLM（完整） ⑤ RLM（无递归子调用）</td>
</tr>
<tr>
<td>任务集</td>
<td>① S-NIAH（常数级处理）② BrowseComp-Plus@1K-doc（多跳问答，6–11 M token）③ OOLONG（线性级语义聚合，131 K token）④ OOLONG-Pairs（二次级成对聚合，32 K token）⑤ LongBench-v2 CodeQA（900 k 级代码库理解）</td>
</tr>
<tr>
<td>规模轴</td>
<td>对 S-NIAH/OOLONG/OOLONG-Pairs 按 2¹³…2¹⁸ token 做长度扫描，观察性能衰减曲线</td>
</tr>
<tr>
<td>评价指标</td>
<td>准确率 / F1 / 官方评分函数；每次查询的 API 成本与运行时长（25–95 分位）</td>
</tr>
<tr>
<td>消融与失败分析</td>
<td>① 无递归子调用 ablation② 不同文档规模子集（BrowseComp 20 任务，100–1000 doc）③ 轨迹案例：验证-冗余、过度递归、输出令牌耗尽等负面模式</td>
</tr>
</tbody>
</table>
</div>
<p>核心结果一览（Table 1 &amp; Figure 1）：</p>
<ul>
<li>10 M token 规模下，RLM 仍保持 50–90 % 准确率，而基座模型因超出 272 K 窗口直接失败。</li>
<li>在窗口<strong>之内</strong>的任务，RLM 相对 Base 提升 10–58 % 绝对准确率；OOLONG-Pairs 的 F1 从 &lt;0.1 % 提升到 58 %（GPT-5）/ 23 %（Qwen3）。</li>
<li>中位调用成本：RLM 与 Base 同量级，甚至低 3×；仅在长尾轨迹出现高方差。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><strong>异步与并行化</strong>：当前子 LLM 调用为阻塞式，引入异步 <code>llm_query</code> 与并行 chunk 处理可显著压缩延迟与云成本。</li>
<li><strong>更深递归层数</strong>：实验限递归深度=1（仅子 LLM），探索深度&gt;1 的“子-子”调用能否进一步降低单段上下文压力，及如何避免误差级联。</li>
<li><strong>专用 RLM 预训练/后训练</strong>：用 RLM 轨迹数据（代码+子调用序列）做 SFT/RL，使模型学会更优的切块、过滤、停止决策，减少冗余验证。</li>
<li><strong>动态预算控制</strong>：在 REPL 内暴露“已用 token/费用”变量，让模型自主权衡精度-成本，实现可中断的任意长输入推理。</li>
<li><strong>多模态与工具扩展</strong>：将图像、音频、数据库、搜索引擎也作为环境变量，通过同一套“代码+递归调用”框架统一处理跨模态长上下文。</li>
<li><strong>可验证性提升</strong>：引入形式化验证或符号执行，对代码产生的中间变量进行一致性检查，降低长轨迹中“正确→错误”翻转概率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Recursive Language Models (RLMs)</strong> 提出一种<strong>推理时通用框架</strong>，把超长提示卸载到 Python REPL 环境，让大模型用代码“窥视-分解-递归调用自身”，突破上下文窗口与上下文腐烂双重瓶颈。主要贡献如下：</p>
<ol>
<li><p>问题重新定义<br>将“长上下文”视为<strong>推理时计算</strong>问题，而非单纯扩大窗口。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>提示 → 环境变量 <code>context</code></li>
<li>模型写 Python 代码做过滤、切块、聚合</li>
<li>通过 <code>llm_query()</code> 递归调用子 LLM，片段级处理，结果写回变量</li>
<li>终止时 <code>FINAL()/FINAL_VAR()</code> 返回答案</li>
</ul>
<ol>
<li><p>实验<br>在 32 K–11 M token 的 5 项任务（S-NIAH、BrowseComp-Plus、OOLONG、OOLONG-Pairs、CodeQA）上，RLM 用 GPT-5 与 Qwen3-Coder 均取得<strong>双倍位数绝对准确率提升</strong>，10 M token 仍保持高性能，中位 API 成本与基模型持平或更低。</p>
</li>
<li><p>结论<br>RLMs 是<strong>任务无关、模型无关</strong>的推理范式，无需重训练即可把有效上下文扩展两个数量级，为“无限长输入”提供了一条可扩展、可负担的新轴心。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Recursive%20Language%20Models，链接是https://arxiv.org/pdf/2512.24601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.24601。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Alex L. Zhang, Tim Kraska, Omar Khattab</p>
<p><strong>Categories</strong>: cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.24601v2.pdf">https://arxiv.org/pdf/2512.24601v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.24601v2">https://papers.cool/arxiv/2512.24601v2</a></p>
<p><strong>Published</strong>: 2025-12-31T03:43:41Z</p>
<hr>
<h3 id="2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-2"><a href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-2" class="headerlink" title="2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs"></a>2. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20861v1">Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</a></h3><p>One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>当前AI系统中最缺失的能力之一是部署后持续学习的能力。实现此类持续学习系统面临诸多挑战，其中之一是用于训练最先进LLMs的基于梯度算法的大量内存需求。进化策略（ES）最近作为传统学习算法的无梯度替代方案重新出现，并在LLMs的特定任务中表现出令人鼓舞的表现。本文对ES进行了全面分析，并特别评估了在训练更新步骤增加时遗忘曲线。首先，我们发现ES在计算预算相当的情况下，在数学和推理任务中能够达到接近GRPO的性能。然而，对持续学习最重要的是，ES的性能提升伴随着对先前能力的显著遗忘，限制了其在线训练模型的适用性。我们还探讨了这种行为背后的原因，并展示了使用ES进行的更新比对应GRPO更新更稀疏，且其$\ell_2$范数远大于相应的GRPO更新，这解释了两种算法之间遗忘曲线的对比。通过这项研究，我们旨在突出像ES这样的无梯度算法中遗忘的问题，并希望激励未来工作来缓解这些问题。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文聚焦“如何让大模型在部署后持续学习”这一核心难题，具体针对以下子问题展开研究：</p>
<ul>
<li>梯度类后训练算法（SFT、RLHF、GRPO 等）内存开销巨大，难以在端侧或在线场景下反复更新权重。</li>
<li>近期复兴的梯度无参算法——进化策略（Evolutionary Strategies, ES）——声称在特定任务上可与 GRPO 媲美，且仅需前向推理，内存占用极低，被视为持续学习的潜在替代方案。</li>
<li>然而，ES 在连续更新过程中是否会“灾难性遗忘”旧能力，此前缺乏系统评估；若遗忘严重，则其“在线学习”优势将失去意义。</li>
</ul>
<p>因此，论文旨在</p>
<ol>
<li>在数学与推理任务上全面对比 ES 与 GRPO 的峰值性能；</li>
<li>量化二者在持续更新过程中的遗忘曲线，判断 ES 是否真正适合部署后持续学习；</li>
<li>从参数更新范数与稀疏度角度解释遗忘差异，为后续改进梯度无参算法提供依据。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 Related Work 中提及并扩展了以下几条研究脉络：</p>
<ol>
<li>进化策略（Evolution Strategies, ES）</li>
</ol>
<ul>
<li>经典理论：Rechenberg 1989、Schwefel 1977、Beyer 1995 奠定  (μ,λ) -ES 框架。</li>
<li>实用变体：CMA-ES（Hansen &amp; Ostermeier 2001）、自然 ES（Wierstra et al. 2011, Sun et al. 2012）。</li>
<li>深度神经演化：Such et al. 2018、Risi &amp; Stanley 2019 在百万参数规模验证 GA/ES 可训练 RL 网络。</li>
<li>大模型时代：Salimans et al. 2017 首次将 ES 扩展到强化学习，展示高度并行与无梯度优势。</li>
<li>近期 LLM 工作：<br>– Malladi et al. 2024、Korotyshova et al. 2025 提出内存高效的 LoRA-ES。<br>– Qiu et al. 2025（与本论文直接对标）在 1–7 B 模型上报告 ES 优于 GRPO，但未系统评估遗忘。</li>
</ul>
<ol>
<li>梯度类后训练算法</li>
</ol>
<ul>
<li>SFT：Wei et al. 2022</li>
<li>RLHF：Ouyang et al. 2022</li>
<li>DPO：Rafailov et al. 2024</li>
<li>GRPO：Shao et al. 2024，通过组内相对奖励省去 Critic，成为本文对比基线。</li>
</ul>
<ol>
<li>灾难性遗忘与持续学习</li>
</ol>
<ul>
<li>Kirkpatrick et al. 2017 提出 EWC，首次在深度学习语境下量化遗忘。</li>
<li>Gupta et al. 2024 发现大规模模型编辑同样会出现“渐进-突发”式遗忘。</li>
<li>Shenfeld et al. 2025 观察到带 KL 正则的在线 RL（包括 GRPO）遗忘更少，为本论文提供参照。</li>
</ul>
<ol>
<li>参数更新稀疏性研究</li>
</ol>
<ul>
<li>Mukherjee et al. 2025 通过掩码分析证明 RLHF 仅微调极小“子网络”，解释其抗遗忘特性；本文沿用其稀疏度定义与分层统计方法，对比 ES 与 GRPO。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一套“新算法”去彻底解决灾难性遗忘，而是通过<strong>系统性实证诊断</strong>揭示 ES 在持续学习场景下的根本缺陷，从而为后续算法设计指明改进方向。具体步骤如下：</p>
<ol>
<li>构建可比实验框架</li>
</ol>
<ul>
<li>统一任务：Countdown、GSM8K、MATH、OlympiadBench。</li>
<li>统一模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct。</li>
<li>统一资源：30 条 rollout / population，200 条训练样本，500 更新步以内。</li>
<li>统一评估：每轮 checkpoint 同时测“新任务准确率”与“旧任务（HellaSwag）准确率”，绘制遗忘曲线。</li>
</ul>
<ol>
<li>量化遗忘现象</li>
</ol>
<ul>
<li>绘制 Pareto 前沿：新任务准确率 vs 旧任务准确率。</li>
<li>统计旧任务性能随更新步的衰减斜率；ES 在 200 步后额外训练几乎不再提升新任务，却持续降低旧任务约 10 个百分点，而 GRPO 基本平稳。</li>
</ul>
<ol>
<li>追溯根本原因</li>
</ol>
<ul>
<li><strong>更新范数</strong>：计算相邻 checkpoint 的 Frobenius 范数</li>
</ul>
<p>|DeltaW|<em>F = |W</em>(t) - W_(t-1)|_F</p>
<p>ES 在 500 步时漂移幅度比 GRPO 大 <strong>3 个数量级</strong>。</p>
<ul>
<li><strong>更新稀疏度</strong>：定义</li>
</ul>
<p>sparsity = #|(DeltaW)_(ij)| &lt; 10^(-6)#params</p>
<p>GRPO 稀疏度 ≈ 95 %，ES 普遍 &lt; 20 %，呈全局密集扰动。</p>
<ul>
<li>关联分析：大范数 + 低稀疏 → 参数空间全局漂移 → 与旧任务表征冲突 → 灾难性遗忘。</li>
</ul>
<ol>
<li>给出改进启示</li>
</ol>
<ul>
<li>梯度无参方法若想用于持续学习，必须<strong>显式约束更新范数</strong>（如裁剪、衰减）或<strong>强制稀疏掩码</strong>（如 LoRA-ES + 动态门控）。</li>
<li>可借鉴 GRPO 的 KL 正则思路，在 ES 的目标中引入与锚定模型的散度惩罚，限制漂移。</li>
</ul>
<p>综上，论文通过“实验量化 → 现象确认 → 根因剖析 → 改进提示”四步，<strong>把 ES 的遗忘风险从经验猜测转化为可度量、可解释的定量结论</strong>，为后续设计“既省内存又不遗忘”的梯度无参算法奠定实证基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计三类实验，全部围绕“ES 能否在持续学习场景下替代 GRPO”这一核心问题展开。实验均在同一硬件（NVIDIA RTX A6000）与同一代码框架（VERL + 自复现 ES）上完成，保证可比性。</p>
<ol>
<li>主实验：ES vs GRPO 峰值性能对照</li>
</ol>
<ul>
<li>模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct</li>
<li>任务：Countdown、GSM8K、MATH、OlympiadBench</li>
<li>协议：每任务 200 训练样本，30 rollout/population，最大 500 更新步，早停验证集不再提升。</li>
<li>指标：验证集最佳准确率（%）</li>
<li>结果：ES 平均落后 GRPO 3–4 个百分点，仅在 Llama-3.2-1B/GSM8K 一项略超；其余均低于 GRPO，反驳了“ES 全面优于 GRPO”的先前提法。</li>
</ul>
<ol>
<li>遗忘曲线实验</li>
</ol>
<ul>
<li>固定模型：Qwen2.5-1.5B-Instruct</li>
<li>新任务：Countdown（训练集 200 条）</li>
<li>旧任务：HellaSwag（全验证集，作为“通用先验能力”探针）</li>
<li>协议：每 50 步保存一个 checkpoint，同时测 Countdown 与 HellaSwag 准确率。</li>
<li>可视化：<br>– Pareto 前沿图：新任务准确率 vs 旧任务准确率<br>– 单指标时序图：HellaSwag 准确率随更新步变化</li>
<li>结果：<br>– ES 在 200 步后新任务收敛，但旧任务仍单调下降，累计跌落 ≈ 10 %。<br>– GRPO 旧任务曲线几乎水平，无明显遗忘。</li>
</ul>
<ol>
<li>更新诊断实验（解释为何遗忘）<br>3.1 更新范数</li>
</ol>
<ul>
<li>计算相邻 checkpoint 参数差 ΔW 的 Frobenius 范数</li>
<li>结果：500 步时 ES 漂移比 GRPO 大 <strong>1000×</strong> 以上，且单调递增。</li>
</ul>
<p>3.2 更新稀疏度</p>
<ul>
<li>定义阈值 τ=1e-6，统计 |ΔW_ij|&lt;τ 的元素比例</li>
<li>分层统计：Attention Q/K/V、WO、MLP、LayerNorm</li>
<li>结果：<br>– GRPO 稀疏度 ≈ 95 %（仅 5 % 参数被显著修改）<br>– ES 稀疏度 &lt; 20 %，几乎全参数被扰动，LayerNorm 稍高但仍远低于 GRPO。</li>
</ul>
<p>3.3 KL-散度关联（附录）</p>
<ul>
<li>计算 π_θ 与 base 模型在 Countdown 与 HellaSwag 上的 KL</li>
<li>结果：ES 的 KL 随步数线性增长，且与旧任务准确率呈显著负相关；GRPO 因显式 KL 惩罚，散度与准确率均无单调关系。</li>
</ul>
<p>通过上述三类实验，论文既验证了 ES 的“性能可比性”，又定量揭示了其“灾难性遗忘”现象，并用范数+稀疏度+KL 三维诊断给出解释。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，均围绕“让梯度无参算法既省内存又不灾难性遗忘”这一核心目标展开：</p>
<ol>
<li>约束型 ES 框架</li>
</ol>
<ul>
<li>将 KL 惩罚或参数距离惩罚显式写入 ES 的目标：</li>
</ul>
<p>θ<em>(t+1) = θ_t + α · (1) / (N) ∑_n Z_n ε_n - β ∇</em>θ D<em>(KL)(π</em>(θ) parallel π_(ref))</p>
<p>需推导无梯度情形下如何高效估计该惩罚项。</p>
<ul>
<li>探索硬约束：每步更新后投影到以  θ<em>(base)  为中心的  ell_2  球或  ell</em>∞  立方体内，限制漂移半径。</li>
</ul>
<ol>
<li>稀疏化 ES 更新</li>
</ol>
<ul>
<li>引入随机掩码：每次只扰动随机子集参数（如 5 %），保持其余固定；掩码可在层、通道或奇异值维度上采样。</li>
<li>学习稀疏掩码：借鉴 RigL / Magnitude Pruning，动态决定“哪些参数值得扰动”，使有效更新稀疏度逼近 GRPO 的 95 % 水平。</li>
</ul>
<ol>
<li>低秩 / 适配器混合 ES</li>
</ol>
<ul>
<li>仅在 LoRA 低秩矩阵（A、B）上应用 ES 扰动，冻结主干；漂移空间从 1.5 B 降至数百万参数，天然减小范数。</li>
<li>比较不同秩（4, 16, 64）对“性能-遗忘”Pareto 前沿的影响，找出秩-遗忘临界值。</li>
</ul>
<ol>
<li>种群级正则</li>
</ol>
<ul>
<li>在种群内部保持“锚点个体”——不扰动的  θ_(base)  副本，将其奖励作为额外基线，惩罚偏离过远的个体。</li>
<li>研究锚点比例（1/30、5/30、10/30）对旧任务保留率的边际效应。</li>
</ul>
<ol>
<li>多任务 ES 演化</li>
</ol>
<ul>
<li>同时把 Countdown + HellaSwag 打包成多目标奖励：</li>
</ul>
<p>R = R<em>(new) + λ R</em>(old)</p>
<p>用演化多目标算法（NSGA-ES）搜索非支配解集，观察是否自然涌现“不遗忘”个体。</p>
<ul>
<li>考察  λ  调度：初期  λ  大，后期逐步减小，让模型先稳固旧能力再提升新能力。</li>
</ul>
<ol>
<li>与参数高效微调正交组合</li>
</ol>
<ul>
<li>将 ES 用于 Prompt-tuning、AdaLoRA、VeRA 等“极轻量”模块，验证遗忘是否随可训参数量线性下降。</li>
<li>对比同一参数预算下“ES-LoRA”与“GRPO-LoRA”的 Pareto 前沿，判断梯度自由在极低秩下是否仍有优势。</li>
</ul>
<ol>
<li>在线 / 流式场景测试</li>
</ol>
<ul>
<li>构建“任务流”：按顺序依次训练 Countdown → GSM8K → MATH → Olympiad，每任务仅 200 样本，周期性回放 5 % 旧数据。</li>
<li>监测 ES 与 GRPO 的“遗忘-学习”折线，评估哪种算法在数据流场景下最终平均性能更高。</li>
</ul>
<ol>
<li>理论侧：ES 漂移上界</li>
</ol>
<ul>
<li>在强凸或 PL 条件下，推导 ES 更新步的期望漂移</li>
</ul>
<p>E[|θ_(t)-θ_0|^2] le f(t, σ, α, d)</p>
<p>并与 SGD/GRPO 的对应上界比较，给出“遗忘-漂移”定量关系。</p>
<ol>
<li>统计稳定性提升</li>
</ol>
<ul>
<li>扩大种群（30 → 128 → 512）与减半学习率，观察遗忘曲线方差是否显著下降；给出“种群大小-遗忘方差”经验公式，为工业部署提供选型依据。</li>
</ul>
<ol>
<li>遗忘评测体系扩展</li>
</ol>
<ul>
<li>引入更多旧任务探针（MMLU、ARC、TruthfulQA）与参数探测方法（LiNGAM、CKA），检查遗忘是否具“任务特异性”或“全局通用性”。</li>
<li>记录层-wise 遗忘速率，找出 ES 最先干扰的表征层，为后续选择性正则提供目标。</li>
</ul>
<p>通过上述探索，可逐步把 ES 从“内存友好但易遗忘”的基线，推向“内存友好且持续稳健”的实用在线学习方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>研究背景</strong><br>大模型部署后需持续学习，但梯度类后训练（SFT、RLHF、GRPO）内存开销大；进化策略（ES）作为梯度无参替代方案近期被重新关注，声称性能可比 GRPO 且极省内存，然而其是否会灾难性遗忘尚缺系统评估。</p>
</li>
<li><p><strong>核心发现</strong></p>
</li>
</ul>
<ol>
<li>在 Countdown、GSM8K、MATH、OlympiadBench 上，ES 峰值准确率仅落后 GRPO 3–4 个百分点，计算步数相近，确认其“性能可比”。</li>
<li>在同一微调 run 内，ES 新任务收敛后旧任务（HellaSwag）准确率仍持续下降约 10 %；GRPO 几乎无遗忘。</li>
<li>诊断表明 ES 更新 Frobenius 范数比 GRPO 大 <strong>1000×</strong>，稀疏度 &lt; 20 %（GRPO ≈ 95 %），全局密集漂移导致旧能力被破坏。</li>
</ol>
<ul>
<li><strong>结论</strong><br>ES 虽提供内存友好的梯度无参路径，但单任务微调即出现显著灾难性遗忘，目前不适合需要保留旧能力的在线学习场景；未来需引入稀疏化、范数约束或 KL 正则等机制以缓解遗忘。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs，链接是https://arxiv.org/pdf/2601.20861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20861。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20861v1.pdf">https://arxiv.org/pdf/2601.20861v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20861v1">https://papers.cool/arxiv/2601.20861v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:59:34Z</p>
<hr>
<h3 id="3-LLMStinger-Jailbreaking-LLMs-using-RL-fine-tuned-LLMs"><a href="#3-LLMStinger-Jailbreaking-LLMs-using-RL-fine-tuned-LLMs" class="headerlink" title="3. LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs"></a>3. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2411.08862v2">LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</a></h3><p>We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了LLMStinger，一种新颖的方法，利用大型语言模型（LLMs）自动生成用于越狱攻击的对抗性后缀。与传统方法不同，传统方法需要复杂的提示工程或白盒访问，LLMStinger使用强化学习（RL）循环来微调攻击者LLM，根据HarmBench基准中有害问题的现有攻击生成新的后缀。我们的方法显著优于现有的红队方法（我们与15种最新方法进行了比较），在LLaMA2-7B-chat上的攻击成功率（ASR）提升了57.2%，在Claude 2上提升了50.3%，这两个模型都以其严格的安全措施而闻名。此外，我们在GPT-3.5上实现了94.97%的ASR，在Gemma-2B-it上实现了99.4%的ASR，展示了LLMStinger在开放源和闭源模型中的稳健性和适应性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文介绍了一种名为LLM STINGER的新方法，旨在破解（jailbreak）经过安全训练的大型语言模型（LLMs）。具体来说，它试图解决以下问题：</p>
<ul>
<li><strong>如何自动生成有效的对抗性后缀（adversarial suffixes）</strong>：传统的破解方法需要复杂的提示工程（prompt engineering）或白盒（white-box）访问模型，这限制了破解尝试的可扩展性。LLM STINGER通过使用强化学习（RL）循环来微调一个攻击者LLM，自动生成针对有害问题的新对抗性后缀，这些后缀能够触发安全训练的LLMs产生未预期或有害的响应。</li>
<li><p><strong>提高对安全训练模型的破解成功率</strong>：尽管现有的后缀攻击方法在某些情况下已被修补，但对这些后缀的修改仍然可以导致成功的破解尝试。LLM STINGER的目标是通过自动化的方式发现新的后缀，以绕过现有的防御机制，从而提高破解成功率。</p>
</li>
<li><p><strong>适应不同的LLMs，包括开源和闭源模型</strong>：LLM STINGER只需要对受害模型进行黑盒（black-box）访问，这使得它能够适用于更广泛的模型，包括那些经过广泛安全训练的开源和闭源LLMs。</p>
</li>
<li><strong>提升攻击方法的效率和适应性</strong>：通过使用强化学习和字符串相似性检查器提供的反馈，LLM STINGER能够更有效地探索攻击向量空间，减少搜索空间，提高生成有效对抗性后缀的效率和成功率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了多个与LLM STINGER相关的研究工作，这些研究主要集中在以下几个方面：</p>
<p>基于梯度的攻击方法</p>
<ul>
<li><strong>GCG (Zou et al. 2023)</strong>：通过优化对抗性后缀的token来最大化生成肯定字符串的对数概率。该方法还扩展到针对多个用户提示（GCG-Multi）和跨多个开源模型（GCG-Transfer）的攻击。</li>
<li><strong>GBDA (Guo et al. 2021)</strong>：使用Gumbel-softmax分布来优化对抗性后缀的token。</li>
<li><strong>UAT (Wallace et al. 2019)</strong>：使用泰勒近似法在token嵌入的梯度周围进行一阶优化，以优化对抗性后缀的token。</li>
</ul>
<p>基于角色扮演和生成的攻击方法</p>
<ul>
<li><strong>TAP (Mehrotra et al. 2023)</strong>：通过树状提示结构和修剪技术自动生成破解提示，仅需要对目标模型进行黑盒访问。</li>
<li><strong>PAIR (Chao et al. 2023)</strong>：使用攻击者LLM生成对抗性提示，并通过角色扮演模板和判断LLM的反馈进行迭代改进。</li>
<li><strong>AutoDAN (Liu et al. 2023)</strong>：通过层次遗传算法生成破解提示，使用选择、交叉和变异来修改提示，并通过适应度评分指导未来的修改。</li>
</ul>
<p>基于优化和搜索的攻击方法</p>
<ul>
<li><strong>PEZ (Wen et al. 2024)</strong>：通过最近邻投影优化对抗性后缀的token。</li>
<li><strong>SFS (Perez et al. 2022)</strong> 和 <strong>ZS (Perez et al. 2022)</strong>：分别通过少样本（few-shot）和零样本（zero-shot）生成测试用例，利用攻击者LLM进行攻击。</li>
<li><strong>TAP-Transfer (Mehrotra et al. 2023)</strong>：在TAP的基础上，将攻击转移到其他LLMs上。</li>
</ul>
<p>基于策略和模板的攻击方法</p>
<ul>
<li><strong>AutoPrompt (AP) (Shin et al. 2020)</strong>：类似于GCG，但使用不同的候选选择策略。</li>
<li><strong>PAP (Zeng et al. 2024)</strong>：通过使攻击者LLM在每次请求中听起来更有说服力来诱导有害行为。</li>
<li><strong>Human Jailbreaks (Shen et al. 2023)</strong>：一组手工制作的破解提示。</li>
</ul>
<p>其他相关研究</p>
<ul>
<li><strong>FuzzLLM (Yao et al. 2024)</strong>：一个通用的模糊测试框架，用于主动识别和评估LLMs中的破解漏洞。</li>
<li><strong>TrustLLM (Sun et al. 2024)</strong>：对LLMs的信任度进行全面研究，提出了跨八个维度的信任度原则，并建立了跨六个维度的基准。</li>
<li><strong>ICAG (Zhou et al. 2024)</strong>：一个新颖的框架，通过迭代对抗游戏在攻击和防御代理之间动态改进防御机制，无需对LLMs进行微调。</li>
</ul>
<p>这些研究为LLM STINGER提供了背景和基础，展示了该领域在对抗性攻击方法上的进展和多样性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过提出LLM STINGER框架来解决破解经过安全训练的大型语言模型（LLMs）的问题。以下是LLM STINGER解决该问题的具体方法和步骤：</p>
<p>1. <strong>强化学习（Reinforcement Learning, RL）微调攻击者LLM</strong></p>
<p>LLM STINGER利用强化学习（特别是近端策略优化，PPO算法）来微调一个攻击者LLM，使其能够自动生成有效的对抗性后缀。这种方法的核心在于通过奖励信号引导攻击者LLM生成更有效的攻击后缀。</p>
<p>2. <strong>自动化生成对抗性后缀</strong></p>
<p>在训练过程中，攻击者LLM以有害问题和已知的对抗性后缀为输入，生成新的类似后缀。这些新生成的后缀被附加到有害问题上，并发送给受害LLM。受害LLM的响应由判断模型评估，判断攻击是否成功。</p>
<p>3. <strong>使用判断模型和字符串相似性检查器提供反馈</strong></p>
<ul>
<li><strong>判断模型（Judgment Model）</strong>：判断模型提供二元反馈，指示攻击是否成功。如果攻击失败，字符串相似性检查器会提供更细致的反馈。</li>
<li><strong>字符串相似性检查器（String Similarity Checker）</strong>：该检查器在token级别提供反馈，评估生成的后缀与已知成功后缀的相似度。这种细粒度的反馈帮助攻击者LLM保留那些已知有效的特征，同时探索新的攻击向量。</li>
</ul>
<p>4. <strong>黑盒访问和适应性</strong></p>
<p>LLM STINGER只需要对受害模型进行黑盒访问，这使得它能够适用于更广泛的模型，包括那些经过广泛安全训练的开源和闭源LLMs。这种方法不依赖于内部模型权重，因此具有更好的适应性和可扩展性。</p>
<p>5. <strong>实验验证和性能提升</strong></p>
<p>通过在HarmBench基准测试中与15种最新的攻击方法进行比较，LLM STINGER在攻击成功率（Attack Success Rate, ASR）上显著优于其他方法。例如，在Claude 2模型上，LLM STINGER实现了+50.3%的ASR提升，而在LLaMA2-7Bchat模型上，ASR提升了+57.2%。</p>
<p>6. <strong>高效搜索和优化</strong></p>
<p>LLM STINGER利用强化学习的反馈机制和字符串相似性检查器的细粒度反馈，高效地探索攻击向量空间，减少搜索空间，提高生成有效对抗性后缀的效率和成功率。</p>
<p>7. <strong>手动验证攻击效果</strong></p>
<p>为了确保LLM STINGER真正突破了模型的防御，而不是仅仅欺骗了判断模型，作者还手动验证了受害模型的输出。</p>
<p>8. <strong>未来工作方向</strong></p>
<p>论文还提出了未来的研究方向，包括支持更多的token级别攻击方法、与更多的攻击策略和受害LLMs进行比较，以及探索额外的反馈机制以增强攻击者LLM的效果。</p>
<p>通过上述方法，LLM STINGER有效地解决了如何自动生成有效的对抗性后缀、提高对安全训练模型的破解成功率、适应不同的LLMs以及提升攻击方法的效率和适应性等问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验来验证LLM STINGER的有效性和性能：</p>
<p>1. <strong>实验设置</strong></p>
<ul>
<li><strong>数据集</strong>：使用HarmBench的标准化行为基准测试集，该数据集包含有害行为的训练集和测试集。</li>
<li><strong>评估指标</strong>：主要评估指标是攻击成功率（Attack Success Rate, ASR），即测试集中经过修改的有害问题能够操纵模型产生未预期或有害响应的比例。</li>
<li><strong>攻击者模型</strong>：选择Gemma作为攻击者LLM，因为它允许系统提示修改，并且在响应上不过于谨慎，适合用于对抗性目的。</li>
<li><strong>判断模型</strong>：使用HarmBench的判断LLM，因为它在手动标记的验证集上表现优于其他判断模型（如GPT-4、Llama-Guard、AdvBench）。</li>
<li><strong>比较方法</strong>：与15种流行的红队攻击方法和HarmBench使用的直接请求（Direct Request, DR）基线进行比较。</li>
</ul>
<p>2. <strong>受害LLMs选择</strong></p>
<ul>
<li><strong>开源模型</strong>：选择了LLaMA-2和Vicuna，因为LLaMA-2以其对GCG和其他攻击的强鲁棒性而闻名，Vicuna是从LLaMA微调而来的。</li>
<li><strong>闭源模型</strong>：选择了GPT-3.5和GPT-4 API，以及Claude API，这些模型在系统和模型级别都进行了广泛的防御训练。</li>
</ul>
<p>3. <strong>实验结果</strong></p>
<ul>
<li><strong>LLaMA2-7Bchat</strong>：LLM STINGER实现了89.3%的ASR，相比次佳方法（32.1%）提升了+57.2%。</li>
<li><strong>Vicuna-7B</strong>：LLM STINGER实现了93.08%的ASR，相比次佳方法（89.9%）提升了+3.18%。</li>
<li><strong>Claude 2</strong>：LLM STINGER实现了52.2%的ASR，相比次佳方法（1.9%）提升了+50.3%。</li>
<li><strong>GPT-3.5 Turbo 0613</strong>：LLM STINGER实现了88.67%的ASR，相比次佳方法（44.3%）提升了+44.37%。</li>
<li><strong>GPT-3.5 Turbo 1106</strong>：LLM STINGER实现了94.97%的ASR，相比次佳方法（56.4%）提升了+38.57%。</li>
<li><strong>GPT-4 Turbo 1106</strong>：LLM STINGER实现了80.50%的ASR，相比次佳方法（21.4%）提升了+59.10%。</li>
<li><strong>Gemma-2Bit</strong>：LLM STINGER实现了99.4%的ASR，展示了其在自身模型上的强大适应性和有效性。</li>
</ul>
<p>4. <strong>手动验证</strong></p>
<p>为了确保LLM STINGER真正突破了模型的防御，而不是仅仅欺骗了判断模型，作者还手动验证了受害模型的输出。</p>
<p>5. <strong>实验细节</strong></p>
<ul>
<li><strong>训练过程</strong>：使用Proximal Policy Optimization (PPO)算法对攻击者LLM进行微调，训练了50个epoch。训练过程中，攻击者LLM以有害问题和七个公开可用的后缀为输入，生成新的后缀。</li>
<li><strong>反馈机制</strong>：如果攻击失败，字符串相似性检查器会提供token级别的反馈，帮助攻击者LLM进行更精确的调整。</li>
<li><strong>硬件配置</strong>：所有实验在高性能CentOS V7集群上进行，配备Intel E52683 v4 Broadwell处理器（2.10 GHz）、2个NVIDIA V100 GPU和64 GiB内存。</li>
</ul>
<p>6. <strong>实验结论</strong></p>
<p>LLM STINGER在多种经过安全训练的LLMs上表现出色，特别是在那些其他攻击方法难以突破的模型上。它只需要对受害模型进行黑盒访问，这使得它能够适用于更广泛的模型，包括开源和闭源LLMs。此外，LLM STINGER的自动化和反馈驱动的攻击生成方法使其能够高效地发现新的攻击后缀，并适应模型的防御机制。</p>
<p>这些实验结果证明了LLM STINGER在破解经过安全训练的LLMs方面的有效性和适应性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文中提到了一些可以进一步探索的方向，这些方向有助于进一步提升LLM STINGER的性能和适应性，同时也可以拓展其应用场景。以下是一些具体的点：</p>
<p>1. <strong>支持更多的token级别攻击方法</strong></p>
<p>目前LLM STINGER主要集中在生成对抗性后缀，但可以探索更多的token级别攻击方法，例如：</p>
<ul>
<li><strong>前缀攻击（Prefix Attacks）</strong>：探索在输入前添加特定字符串以触发有害行为。</li>
<li><strong>嵌入式攻击（Embedded Attacks）</strong>：在输入中嵌入特定模式或token，以绕过安全机制。</li>
<li><strong>多位置攻击（Multi-Position Attacks）</strong>：同时在输入的不同位置插入攻击性内容，增加攻击的复杂性和成功率。</li>
</ul>
<p>2. <strong>与更多攻击策略和受害LLMs进行比较</strong></p>
<ul>
<li><strong>更多攻击策略</strong>：与更多最新的攻击方法进行比较，例如基于深度学习的攻击方法、基于图神经网络的攻击方法等。</li>
<li><strong>更多受害LLMs</strong>：测试更多的开源和闭源LLMs，特别是那些具有不同安全训练策略和防御机制的模型。例如：</li>
<li><strong>多模态模型（Multimodal Models）</strong>：探索LLM STINGER在多模态模型（如视觉-语言模型）上的应用。</li>
<li><strong>最新模型</strong>：随着新的LLMs不断发布，持续更新测试集，确保LLM STINGER能够适应最新的模型。</li>
</ul>
<p>3. <strong>探索额外的反馈机制</strong></p>
<ul>
<li><strong>多模态反馈</strong>：结合文本、图像、音频等多种模态的反馈，以更全面地评估攻击效果。</li>
<li><strong>动态反馈</strong>：引入动态反馈机制，根据受害模型的响应实时调整攻击策略。</li>
<li><strong>人类反馈</strong>：结合人类专家的反馈，进一步优化攻击策略，确保攻击的有效性和安全性。</li>
</ul>
<p>4. <strong>提升攻击的隐蔽性和适应性</strong></p>
<ul>
<li><strong>隐蔽性</strong>：探索如何使生成的攻击后缀更加隐蔽，避免被现有的检测机制发现。例如，通过自然语言生成技术使攻击后缀看起来更自然。</li>
<li><strong>适应性</strong>：进一步提升LLM STINGER对不同模型和安全机制的适应性，使其能够更有效地绕过复杂的防御策略。</li>
</ul>
<p>5. <strong>防御机制的研究</strong></p>
<ul>
<li><strong>对抗性训练</strong>：研究如何将LLM STINGER生成的攻击后缀用于对抗性训练，提升LLMs的安全性。</li>
<li><strong>防御策略评估</strong>：评估现有的防御策略对LLM STINGER攻击的有效性，提出新的防御方法。</li>
</ul>
<p>6. <strong>跨领域应用</strong></p>
<ul>
<li><strong>其他领域</strong>：探索LLM STINGER在其他领域的应用，如网络安全、软件测试、漏洞挖掘等。</li>
<li><strong>跨语言攻击</strong>：研究如何将LLM STINGER应用于不同语言的LLMs，评估其在跨语言环境中的有效性。</li>
</ul>
<p>7. <strong>性能优化</strong></p>
<ul>
<li><strong>计算效率</strong>：优化LLM STINGER的计算效率，减少训练和生成攻击后缀的时间。</li>
<li><strong>资源利用</strong>：探索如何更高效地利用计算资源，例如通过分布式训练和并行化处理。</li>
</ul>
<p>8. <strong>伦理和安全考虑</strong></p>
<ul>
<li><strong>伦理审查</strong>：在开发和使用LLM STINGER时，考虑其潜在的伦理和安全问题，确保其应用符合道德和法律标准。</li>
<li><strong>安全评估</strong>：定期进行安全评估，确保LLM STINGER不会被恶意利用。</li>
</ul>
<p>这些方向不仅有助于提升LLM STINGER的性能和适应性，还可以推动整个领域的发展，为LLMs的安全性和可靠性提供更全面的保障。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文标题</p>
<p>LLM STINGER: Jailbreaking LLMs using RL fine-tuned LLMs</p>
<p>作者</p>
<p>Piyush Jha, Arnav Arora, and Vijay Ganesh<br>Georgia Institute of Technology, USA</p>
<p>摘要</p>
<p>LLM STINGER是一种新颖的方法，利用大型语言模型（LLMs）自动生成对抗性后缀，用于破解（jailbreak）经过安全训练的LLMs。与传统方法相比，LLM STINGER不需要复杂的提示工程或白盒访问，而是通过强化学习（RL）循环微调一个攻击者LLM，生成新的后缀以绕过现有的防御机制。该方法在HarmBench基准测试中显著优于现有的15种最新攻击方法，例如在LLaMA2-7Bchat上ASR提升了+57.2%，在Claude 2上ASR提升了+50.3%。此外，LLM STINGER在GPT-3.5上实现了94.97%的ASR，在Gemma2B-it上实现了99.4%的ASR，展示了其在开源和闭源模型上的鲁棒性和适应性。</p>
<p>研究背景</p>
<p>破解LLMs涉及构造输入，使经过安全训练的模型产生未预期或有害的响应。传统的破解方法需要复杂的提示工程或白盒访问，这些方法不仅耗时，而且难以扩展。LLM STINGER通过自动化生成对抗性后缀，提供了一种更高效、更易于自动化的替代方案。</p>
<p>研究方法</p>
<p>LLM STINGER的核心是一个强化学习循环，使用近端策略优化（PPO）算法微调攻击者LLM。具体步骤如下：</p>
<ol>
<li><strong>输入和生成</strong>：攻击者LLM以有害问题和已知的对抗性后缀为输入，生成新的后缀。</li>
<li><strong>攻击验证</strong>：将生成的后缀附加到有害问题上，发送给受害LLM，判断模型评估攻击是否成功。</li>
<li><strong>反馈机制</strong>：如果攻击失败，字符串相似性检查器提供token级别的反馈，帮助攻击者LLM进行更精确的调整。</li>
<li><strong>微调</strong>：使用PPO算法在50个epoch内微调攻击者LLM，以提高攻击成功率。</li>
</ol>
<p>实验设置</p>
<ul>
<li><strong>数据集</strong>：使用HarmBench的标准化行为基准测试集，评估指标为攻击成功率（ASR）。</li>
<li><strong>攻击者模型</strong>：选择Gemma作为攻击者LLM。</li>
<li><strong>判断模型</strong>：使用HarmBench的判断LLM。</li>
<li><strong>比较方法</strong>：与15种流行的红队攻击方法和直接请求（DR）基线进行比较。</li>
<li><strong>受害LLMs</strong>：选择LLaMA-2、Vicuna、GPT-3.5、GPT-4和Claude等模型。</li>
</ul>
<p>实验结果</p>
<ul>
<li><strong>LLaMA2-7Bchat</strong>：LLM STINGER实现了89.3%的ASR，相比次佳方法（32.1%）提升了+57.2%。</li>
<li><strong>Vicuna-7B</strong>：LLM STINGER实现了93.08%的ASR，相比次佳方法（89.9%）提升了+3.18%。</li>
<li><strong>Claude 2</strong>：LLM STINGER实现了52.2%的ASR，相比次佳方法（1.9%）提升了+50.3%。</li>
<li><strong>GPT-3.5 Turbo 0613</strong>：LLM STINGER实现了88.67%的ASR，相比次佳方法（44.3%）提升了+44.37%。</li>
<li><strong>GPT-3.5 Turbo 1106</strong>：LLM STINGER实现了94.97%的ASR，相比次佳方法（56.4%）提升了+38.57%。</li>
<li><strong>GPT-4 Turbo 1106</strong>：LLM STINGER实现了80.50%的ASR，相比次佳方法（21.4%）提升了+59.10%。</li>
<li><strong>Gemma-2Bit</strong>：LLM STINGER实现了99.4%的ASR，展示了其在自身模型上的强大适应性和有效性。</li>
</ul>
<p>结论</p>
<p>LLM STINGER通过强化学习和细粒度反馈机制，有效地生成了能够绕过复杂防御机制的对抗性后缀。它在多种经过安全训练的LLMs上表现出色，特别是在那些其他攻击方法难以突破的模型上。LLM STINGER只需要对受害模型进行黑盒访问，这使得它能够适用于更广泛的模型，包括开源和闭源LLMs。未来的工作将包括支持更多的token级别攻击方法、与更多攻击策略和受害LLMs进行比较，以及探索额外的反馈机制，以进一步提升其性能和适应性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是LLMStinger%3A%20Jailbreaking%20LLMs%20using%20RL%20fine-tuned%20LLMs，链接是https://arxiv.org/pdf/2411.08862，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2411.08862。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Piyush Jha, Arnav Arora, Vijay Ganesh</p>
<p><strong>Categories</strong>: cs.LG, cs.CR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2411.08862v2.pdf">https://arxiv.org/pdf/2411.08862v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2411.08862v2">https://papers.cool/arxiv/2411.08862v2</a></p>
<p><strong>Published</strong>: 2024-11-13T18:44:30Z</p>
<hr>
<h3 id="4-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching"><a href="#4-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching" class="headerlink" title="4. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching"></a>4. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15942v2">ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</a></h3><p>Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>气候预测存在与气候系统组成部分及其相互作用相关的不确定性。量化这些不确定性的一种典型方法是使用气候模型在不同初始条件下创建重复模拟的集合。由于这些模拟的复杂性，生成此类预测集合在计算上代价高昂。在本工作中，我们提出了ArchesClimate，一种基于深度学习的气候模型仿真器，旨在降低这一成本。ArchesClimate在IPSL-CM6A-LR气候模型的十年重预测数据上进行训练，空间分辨率约为2.5x1.25度。我们按照ArchesWeatherGen的方法训练一个流匹配模型，并将其改编为预测近期气候。一旦训练完成，该模型即可生成一个月的预测状态，并可用于自回归地仿真任意长度的气候模型模拟。我们展示了在长达10年的时间尺度上，这些生成结果是稳定且符合物理规律的。我们还显示，对于多个重要气候变量，ArchesClimate生成的模拟结果与IPSL模型互换性良好。本工作表明，气候模型仿真器有可能显著降低气候模型模拟的成本。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在以极低计算成本生成气候模式 IPSL-CM6A-LR 的十年尺度概率集合，从而替代传统“多次扰动初值 + 全模式积分”这一昂贵流程，支撑年代际气候变率与不确定性的量化研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>NeuralGCM</strong>（Kochkov et al. 2024）<br>混合物理-深度学习自回归大气集合 emulator，在网络内部注入噪声以产生内部变率。</p>
</li>
<li><p><strong>Spherical DYffusion</strong>（Cachay et al. 2023）<br>基于扩散模型的概率气候 emulator，显式对大气状态分布进行建模。</p>
</li>
<li><p><strong>cBottle</strong>（Brenowitz et al. 2025）<br>生成式扩散模型，学习多源再分析资料的瞬时短期动力，但非自回归，无法延伸为长序列。</p>
</li>
<li><p><strong>ACE / ACE2</strong>（Watt-Meyer et al. 2023, 2024）<br>确定性大气 emulator，可稳定积分千年，但不包含随机性，也未耦合海洋。</p>
</li>
<li><p><strong>LUCIE</strong>（Guan et al. 2024）<br>轻量级非耦合气候 emulator，以 ERA5 为训练数据，侧重高效生成大集合。</p>
</li>
<li><p><strong>DLESyM</strong>（Cresswell-Clay et al. 2025）<br>耦合大气-海洋深度学习地球系统模型，用 ERA5 训练，验证 AI 同时模拟两圈层的可能性。</p>
</li>
<li><p><strong>SamudrACE</strong>（Duncan et al. 2025）<br>将确定性海洋 emulator Samudra 与 ACE2 耦合，实现快速海-气交换模拟。</p>
</li>
<li><p><strong>ACE-SOM2</strong>（Clark et al. 2024）<br>ACE2 与单层海洋耦合，用于探究不同强迫下的气候响应，向更长尺度与不同情景扩展。</p>
</li>
</ul>
<p>这些研究共同构成了“AI 气候模式替代”脉络，为 ArchesClimate 在概率性、十年尺度、海-气耦合与计算效率上的设计提供了直接对比与参考。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 ArchesClimate，一套基于**流匹配（flow matching）**的深度生成框架，以月时间步长、≈2.5°×1.25° 空间分辨率自回归地模拟 IPSL-CM6A-LR 的十年集合。核心思路与步骤如下：</p>
<ol>
<li><p>数据利用<br>采用 IPSL-CM6A-LR 对 CMIP6-DCPP 的 55 组 10 年、10 成员回报试验（≈7×10⁴ 个月），仅选取 24 个关键海-气变量（含 4 层大气、7 个海洋整层量）及 5 种辐射强迫（CO₂、CH₄、N₂O、CFC11eq、SSI）作为训练集。</p>
</li>
<li><p>两阶段架构</p>
</li>
</ol>
<ul>
<li><strong>确定性模型</strong>  f<em>θ ：3D-Swin-U-Net 结构，输入  X_t 、 X</em>(t-δ)  与强迫，输出下月平均态  hat X_(t+δ) 。</li>
<li><strong>生成模型</strong>  g_θ ：同骨干网络，以 flow matching 学习残差分布</li>
</ul>
<p>r<em>(t+δ)= X</em>(t+δ) - f<em>θ(X_t,X</em>(t-δ))σ</p>
<p>训练时从  mathcal N(0,I)  到残差分布建立可逆映射；推理时从随机噪声出发，经  M=12  步 ODE 积分生成样本残差，再加回确定性预测得到完整状态。</p>
<ol>
<li><p>强迫嵌入<br>对每条辐射强迫标量，先经线性嵌入生成 scale/shift 参数，再以<strong>条件层归一化</strong>注入所有 Transformer 块，使模型可在不同强迫情景下动态调整。</p>
</li>
<li><p>自回归集合生成<br>训练完成后，用  X<em>0 、 X</em>(-1)  初始化，交替调用  f<em>θ  与  g</em>θ  逐月外推；每次从独立高斯噪声采样即可产生不同成员，实现 10 年、任意大小集合的“一键”输出。</p>
</li>
<li><p>计算加速<br>模型仅作用于 144×144×45 的粗网格，参数空间约为原模式 1/400；在 4×A100 上 12 min 可生成 10 年 10 成员集合，对应 ≈4 core-h/模拟年，而 IPSL-CM6A-LR 需 ≈1900 core-h/模拟年。</p>
</li>
</ol>
<p>通过“确定性骨架 + 流匹配残差”策略，ArchesClimate 在保持物理一致性、季节-年际变率与低频趋势的同时，把传统气候模式集合成本降低两个数量级，从而解决了“十年尺度概率集合昂贵”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否以极低算力生成与 IPSL-CM6A-LR 统计-物理特性一致的十年集合”这一核心问题，设计了 10 组实验，覆盖统计评分、物理一致性、可交换性、方差调校与长期稳定性等维度。实验按目的可归纳为以下 6 类：</p>
<ol>
<li>基础统计评估</li>
</ol>
<ul>
<li><strong>CRPS 与标准差对比</strong>（表 3）<br>对 1969-79、1979-89、2010-20 三个十年，计算 8 个关键变量（tos、psl、net_flux、thetaot2000 等）的纬度加权 CRPS 与集合标准差，比较 ArchesClimate、IPSL-DCPP 与 Pattern-Scaling 基线。</li>
<li><strong>方差-CRPS 消融</strong>（图 5）<br>对比三种训练配置：<br>– residual-flow（本文方案）<br>– full-deterministic（仅  f<em>θ ）<br>– full-flow（仅  g</em>θ ，无确定性骨架）<br>量化确定性骨架对精度与方差的贡献。</li>
</ul>
<ol>
<li>集合可交换性（rank histogram）</li>
</ol>
<ul>
<li><strong>北大西洋区域 rank histogram</strong>（图 6 左）<br>1969 起报的 10 成员、10 年序列，对 6 个变量逐格点排序，检验 ArchesClimate 成员是否能被视为从 IPSL 分布中独立同分布抽取。</li>
</ul>
<ol>
<li>频谱与变率结构</li>
</ol>
<ul>
<li><strong>时间功率谱（TPS）对比</strong>（图 6 右）<br>对同一北大西洋 10 年异常序列做逐点傅里叶变换，比较月-季-年周期强度，诊断 ArchesClimate 是否欠变率（under-dispersion）。</li>
</ul>
<ol>
<li>长期趋势与遥相关</li>
</ol>
<ul>
<li><strong>线性趋势空间相关</strong>（图 7）<br>计算 5 成员集合的 tos 异常 10 年趋势，逐像素与 IPSL 对照，评估年代际信号在三大洋及北极的相关性。</li>
<li><strong>50 年强迫响应</strong>（附录图 A1）<br>固定 1969 强迫 vs 时变强迫各 rollout 50 年，验证模型能否随辐射强迫漂移而非崩溃。</li>
</ul>
<ol>
<li>区域/季节信号</li>
</ol>
<ul>
<li><strong>热带区域气候态与异常</strong>（图 4）<br>20°S–20°N 平均的季节循环与异常时间序列，对比集合均值与 ±1σ 包络。</li>
<li><strong>北大西洋季节异常空间图</strong>（图 8）<br>MAM/JJA/SON/DJF 四季平均 tos 异常的空间格局对比，检验季节尺度空间相位与强度。</li>
</ul>
<ol>
<li>方差增强与损失设计</li>
</ol>
<ul>
<li><strong>三种增方差策略</strong>（图 9）<br>– 推理初始噪声放大 1.1×<br>– 按变量训练-验证方差差值逐变量缩放噪声<br>– 在 flow-matching 损失中加入梯度与功率谱项（ L<em>(rm total)=L</em>(rm MSE)+0.2L<em>(rm grad)+0.2L</em>(rm PSD) ）<br>比较其对 CRPS-方差权衡的影响。</li>
</ul>
<p>以上实验共同表明：</p>
<ul>
<li>ArchesClimate 的 CRPS 与 IPSL 相当或更低，但方差普遍略小；</li>
<li>加入谱-梯度损失可把方差提升到 IPSL 水平，代价是 CRPS 略增；</li>
<li>rank histogram 在 net_flux、wap 等变量接近均匀，tos、thetaot2000 存在低值偏移；</li>
<li>热带与北大西洋季节空间异常被忠实再现，年代际趋势在三大洋遥相关区相关系数 &gt;0.8；</li>
<li>50 年 rollout 仍能跟随辐射强迫漂移，未见数值漂移或崩溃。</li>
</ul>
<p>因此，论文从统计、物理、频谱、长期稳定性四个角度系统验证了“低成本 AI 集合”替代“高成本模式集合”的可行性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>高分辨率时空降尺度</strong><br>以 ArchesClimate 月尺度 2.5° 输出为条件，训练条件扩散或超分模型，生成 1 km-日级别区域情景，支持极端事件归因与局地影响评估。</p>
</li>
<li><p><strong>多世纪长序列外推</strong><br>将训练数据从 55×10 年扩展至 CMIP6 “historical” 与 SSP 几百年试验，检验对 50-100 年低频变率（AMO、PDO、IPO）与强迫响应的保真度。</p>
</li>
<li><p><strong>增强海洋与冰冻圈变量</strong><br>引入海冰厚度/面积、陆地冰盖、三维盐度与洋流、植被动态，减少北极趋势漂移并提升碳循环一致性。</p>
</li>
<li><p><strong>守恒与物理约束嵌入</strong><br>在损失或网络层显式加入大气静力平衡、柱总水量/能量守恒、海洋热盐守恒，抑制长期能量泄漏。</p>
</li>
<li><p><strong>极端与尾部分布校准</strong><br>采用尾部分位数映射或极端值理论对 flow-matching 输出进行事后校正，提高热浪、干旱、极端降水重现期精度。</p>
</li>
<li><p><strong>单步/少步生成加速</strong><br>引入 Consistency-Model 或 Rectified-Flow 单步采样，把 12 步推理降至 1-2 步，再降一个数量级推理耗时。</p>
</li>
<li><p><strong>跨模式迁移与多模式集合</strong><br>用 IPSL 训练的模型权重初始化，微调至 CESM2、MRI-ESM2 等，实现“零成本”多模式集合加权平均。</p>
</li>
<li><p><strong>初始化信息再利用</strong><br>将海洋同化态（三维温盐）或海冰初值编码为条件向量，评估初始化记忆对 1-5 年预测技巧的贡献。</p>
</li>
<li><p><strong>强迫情景插值与反事实试验</strong><br>在嵌入空间对 CO₂、SSI 做线性或非线性插值，生成未运行过的强迫路径（如 1.5 °C 过冲、火山群喷发），快速筛查气候响应。</p>
</li>
<li><p><strong>谱方差自适应损失</strong><br>按纬度-变量-频段动态加权功率谱损失，缓解年循环“过拟合”而高频变率不足的问题，实现方差-精度自动权衡。</p>
</li>
<li><p><strong>不确定性分解</strong><br>利用大样本（10³-10⁴ 成员）将总不确定性拆分为内部变率、模型结构、参数与强迫，对照传统大集合进行方差分解验证。</p>
</li>
<li><p><strong>在线数据同化闭环</strong><br>将 ArchesClimate 作为“廉价预报核”，与 Ensemble Kalman Filter 结合，实现实时年代际预测系统，每月滚动更新。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>ArchesClimate：用流匹配实现十年尺度概率气候集合的快速生成</strong></p>
<ol>
<li><p>问题<br>传统气候模式需对初值扰动运行数十次才能量化十年内部变率，单成员 10 年 ≈ 1900 core-h，成本高昂。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>数据：IPSL-CM6A-LR 的 DCPP 回报（55×10 年×10 成员，≈7×10⁴ 月），24 个海-气变量 + 5 种辐射强迫。</li>
<li>架构：两阶段<br>– 确定性 Swin-U-Net  f<em>θ  预测下月平均态；<br>– 流匹配生成网络  g</em>θ  学习残差分布，12 步 ODE 从高斯噪声采样。</li>
<li>训练：先训  f<em>θ  再训  g</em>θ ，损失为逐网格 MSE（纬度加权）+ 可选谱/梯度正则。</li>
<li>推理：自回归 rollout，10 年 10 成员在 4×A100 上 12 min 完成（4 core-h/年）。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>统计：CRPS 与 IPSL 相当，方差略低；加入谱损失可把方差提升至 IPSL 水平。</li>
<li>可交换性：net_flux、wap 的 rank histogram 接近均匀，tos/thetaot2000 略偏低值。</li>
<li>频谱：年-季周期准确，月-年际功率略低。</li>
<li>趋势：三大洋年代际趋势空间相关 &gt;0.8，北极因无冰过程快速退相关。</li>
<li>长期：50 年 rollout 仍能跟随辐射强迫漂移，无崩溃。</li>
<li>区域：热带与北大西洋季节异常空间相位与观测一致。</li>
</ul>
<ol>
<li><p>结论<br>ArchesClimate 以 ≈ 1/400 参数规模、1/500 计算成本，生成与 IPSL-CM6A-LR 统计-物理特性高度一致的十年集合，为年代际气候不确定性与极端事件研究提供可负担的“AI 集合引擎”。</p>
</li>
<li><p>展望<br>高分辨率降尺度、多世纪长序列、守恒约束、极端校正、单步采样及跨模式迁移等方向可进一步提升可用性与物理可信度。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ArchesClimate%3A%20Probabilistic%20Decadal%20Ensemble%20Generation%20With%20Flow%20Matching，链接是https://arxiv.org/pdf/2509.15942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.15942。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis</p>
<p><strong>Categories</strong>: physics.ao-ph, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15942v2.pdf">https://arxiv.org/pdf/2509.15942v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15942v2">https://papers.cool/arxiv/2509.15942v2</a></p>
<p><strong>Published</strong>: 2025-09-19T12:53:24Z</p>
<hr>
<h3 id="5-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems"><a href="#5-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems" class="headerlink" title="5. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems"></a>5. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06052v3">DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</a></h3><p>Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>离散组合问题（DCP）在工业决策和优化中非常普遍。然而，尽管针对DCP的约束求解技术已经取得了显著进展，其核心过程——即约束建模——仍然需要大量专业知识，并且是限制其广泛应用的瓶颈。为了缓解这一瓶颈，近期研究探索了使用大型语言模型（LLM）将组合问题描述转化为可执行的约束模型。然而，现有的离散约束建模评估数据集通常仅限于小型、同质化或特定领域的问题，无法反映现实世界场景的多样性。本研究通过引入DCP-Bench-Open填补了这一空白，这是一个新型基准，包含来自约束规划（CP）和运筹学（OR）社区的多种知名离散组合问题，并专门针对LLM驱动的约束建模进行了结构化设计。利用该数据集，并考虑不同建模框架的多样性，我们对LLM在三种不同约束建模系统中的建模能力进行了比较和评估，这些系统在抽象层次和底层语法上各不相同。值得注意的是，结果显示在使用基于高层Python框架进行建模时性能更高。此外，我们系统评估了不同LLM在提示式和推理时计算方法下的使用，这进一步提高了准确性，在这一高度挑战性的基准上达到了91%。DCP-Bench-Open已公开提供。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决的问题是如何利用大型语言模型（LLMs）来简化约束编程（Constraint Programming, CP）中的约束建模过程，从而降低约束编程的使用门槛，使其能够被更广泛的非专家用户所接受和应用。具体而言，论文的目标包括：</p>
<ol>
<li><p><strong>解决约束建模的瓶颈问题</strong>：约束编程是一种解决组合优化问题的有效范式，但其核心过程——约束建模——非常复杂，需要深厚的应用领域知识和对建模框架的熟练掌握。这限制了约束编程技术的广泛应用。论文探索使用LLMs作为建模助手，将自然语言描述的组合问题转化为可执行的约束模型，类似于现有的基于LLMs的代码生成助手。</p>
</li>
<li><p><strong>提供一个多样化的基准数据集</strong>：现有的用于评估LLMs在约束建模能力上的数据集往往存在局限性，例如规模小、同质化或特定于某个领域，无法涵盖现实世界场景的多样性。论文通过引入一个新的基准数据集CP-Bench，填补了这一空白。CP-Bench包含了一系列来自CP社区的多样化、知名的组合问题类别，专门用于评估LLMs驱动的CP建模能力。</p>
</li>
<li><p><strong>评估不同约束建模框架对LLMs性能的影响</strong>：论文比较和评估了LLMs在三种不同抽象层次和底层语法的约束建模系统中的建模能力，这些系统包括高级别的MiniZinc语言、基于Python的CPMpy库，以及较低级别的Python接口OR-Tools CP-SAT求解器。通过这种比较，论文旨在了解不同框架对LLMs建模准确性的具体影响。</p>
</li>
<li><p><strong>探索提升LLMs生成有效约束模型能力的方法</strong>：为了提高LLMs生成有效约束模型的能力，论文系统地评估了基于提示（prompt-based）和推理时计算（inference-time compute）方法在约束建模中的应用。这些方法包括从现有基于LLMs的代码生成研究中借鉴而来的技术，如检索增强型上下文学习（Retrieval-Augmented In-Context Learning, RAICL）、重复采样和自我验证等。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了多个与利用大型语言模型（LLMs）进行约束建模相关的研究工作，这些研究主要集中在以下几个方面：</p>
<p>约束编程与大型语言模型的结合</p>
<ul>
<li><strong>NL4Opt Competition</strong>：Ramamonjison等人在NeurIPS 2022竞赛中提出了NL4Opt数据集，这是一个包含简单线性规划（LP）问题的基准，用于评估LLMs将自然语言问题描述转化为优化模型的能力<br>^28^<br>。</li>
<li><strong>Logic Grid Puzzles (LGPs)</strong>：Jabrayilzade和Tekir提出了一个用于解决逻辑网格谜题的数据集，这些谜题可以看作是组合问题的一个特定子集<br>^14^<br>。</li>
<li><strong>Text2Zinc</strong>：Singirikonda等人提出了一个跨领域的数据集，包含MiniZinc模型，主要关注线性规划（LP）和混合整数规划（MIP）问题<br>^31^<br>。</li>
</ul>
<p>大型语言模型在代码生成中的应用</p>
<ul>
<li><strong>Evaluating Large Language Models Trained on Code</strong>：Chen等人研究了LLMs在代码生成方面的表现，特别是在生成Python代码方面的能力<br>^4^<br>。</li>
<li><strong>Automated Repair of Programs from Large Language Models</strong>：Fan等人探讨了LLMs在自动修复程序中的应用，这与LLMs在约束建模中的自我验证和错误检测有相似之处<br>^7^<br>。</li>
</ul>
<p>大型语言模型的推理时计算方法</p>
<ul>
<li><strong>Scaling LLM Test-Time Compute</strong>：Snell等人研究了在测试时增加LLMs的计算量以提高性能的方法，这在论文中被应用于约束建模任务中<br>^32^<br>。</li>
<li><strong>Teaching Large Language Models to Self-Debug</strong>：Chen等人提出了让LLMs进行自我调试的方法，这与论文中探讨的自我验证方法有相似之处<br>^5^<br>。</li>
</ul>
<p>约束编程的其他相关工作</p>
<ul>
<li><strong>Global Constraint Catalogue</strong>：Beldiceanu等人提供了约束编程中全局约束的详细目录，这对于理解约束编程的多样性和复杂性非常重要<br>^1^<br>。</li>
<li><strong>Applications of Combinatorial Optimization</strong>：Paschos的著作提供了组合优化在多个领域的应用实例，强调了约束编程在实际问题中的重要性<br>^24^<br>。</li>
</ul>
<p>这些相关研究为本文提供了背景和基础，同时也指出了现有研究的局限性和改进方向。例如，现有数据集在多样性和复杂性上的不足，以及在不同约束建模框架下LLMs性能的差异等。本文通过引入CP-Bench数据集和系统地评估不同框架和方法，旨在推动这一领域的研究进展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下几个主要步骤来解决如何利用大型语言模型（LLMs）简化约束编程（CP）中的约束建模过程的问题：</p>
<p>1. 构建多样化的基准数据集 CP-Bench</p>
<ul>
<li><strong>数据集来源</strong>：论文从多个来源收集了101个多样化的组合问题，包括CSPLib、CPMpy在线示例库、Håkan Kjellerstrand的在线资源库以及一个基于课程的问题集<br>^21^<br>。</li>
<li><strong>数据集结构</strong>：每个数据集实例都被转化为一个自包含的Python文件，包含元数据、问题描述、输入数据（可选）、模型代码和解决方案打印代码<br>^5^<br>。</li>
<li><strong>评估方法</strong>：论文采用解决方案准确性（solution accuracy）作为评估指标，即通过执行生成的代码来验证其解决方案是否满足真实模型的约束和目标函数<br>^5^<br>。</li>
</ul>
<p>2. 评估不同约束建模框架对LLMs性能的影响</p>
<ul>
<li><strong>选择框架</strong>：论文选择了三个具有不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）<br>^1^<br>。</li>
<li><strong>系统提示（System Prompts）</strong>：为了指导LLMs更有效地进行建模，论文设计了三种不同详细程度的系统提示，从基本的建模任务描述到包含框架文档的详细提示<br>^4.1^<br>。</li>
<li><strong>实验结果</strong>：实验表明，使用Python基础框架（CPMpy和OR-Tools）生成模型的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
</ul>
<p>3. 探索提升LLMs生成有效约束模型能力的方法</p>
<ul>
<li><strong>检索增强型上下文学习（RAICL）</strong>：通过从预构建的数据库中检索与当前问题语义相似的输入-输出对来丰富提示上下文<br>^4.2^<br>。然而，实验结果表明，这种方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>重复采样（Repeated Sampling）</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型<br>^4.3^<br>。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证（Self-Verification）</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性<br>^4.4^<br>。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
</ul>
<p>4. 结合方法以进一步提高性能</p>
<ul>
<li><strong>综合方法</strong>：论文将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>通过上述步骤，论文不仅提供了一个多样化的基准数据集来评估LLMs在约束建模中的能力，还系统地比较了不同建模框架对LLMs性能的影响，并探索了多种方法来提高LLMs生成有效约束模型的能力。这些研究成果为降低约束编程的使用门槛、使其更易于被非专家用户接受和应用提供了有力的支持。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验来评估大型语言模型（LLMs）在约束建模中的表现：</p>
<p>实验一：不同建模框架下的LLMs性能评估</p>
<ul>
<li><strong>目的</strong>：比较LLMs在不同约束建模框架下的性能，以确定哪种框架更适合LLMs生成约束模型。</li>
<li><strong>方法</strong>：选择了三种不同的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。使用CP-Bench数据集中的101个组合问题，对8种不同的LLMs进行了评估<br>^6.1^<br>。</li>
<li><strong>结果</strong>：Python基础框架（CPMpy和OR-Tools）的性能优于领域特定语言（MiniZinc）。具体来说，CPMpy在详细系统提示（Level 3）下达到了最高的准确率<br>^6.2.1^<br>。这表明Python基础框架更适合LLMs生成约束模型。</li>
</ul>
<p>实验二：不同系统提示对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同详细程度的系统提示对LLMs性能的影响。</li>
<li><strong>方法</strong>：设计了三种不同详细程度的系统提示：基本提示（Level 1）、包含建模指南的提示（Level 2）和包含框架文档的详细提示（Level 3）。在三种不同的约束建模框架下，对8种LLMs进行了评估<br>^4.1^<br>。</li>
<li><strong>结果</strong>：提供更详细的系统提示（尤其是包含框架文档的提示）能够提高LLMs的建模准确性<br>^6.2.2^<br>。这表明详细的系统提示有助于LLMs更好地理解和生成约束模型。</li>
</ul>
<p>实验三：推理时计算方法对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同的推理时计算方法对LLMs性能的影响。</li>
<li><strong>方法</strong>：测试了四种不同的推理时计算方法：基线方法（直接评估LLMs生成的第一个模型）、检索增强型上下文学习（RAICL）、重复采样（Repeated Sampling）和自我验证（Self-Verification）。选择了4种LLMs（gpt-4.1-mini、DeepSeek-V3、QwQ-32B、Llama-3.3-70B）进行评估<br>^6.2.3^<br>。</li>
<li><strong>结果</strong>：</li>
<li><strong>RAICL</strong>：在上下文中添加示例对LLMs的性能没有帮助，反而降低了准确性<br>^6.2.3^<br>。</li>
<li><strong>重复采样</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
<li><strong>综合方法</strong>：将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>实验四：错误类型分析</p>
<ul>
<li><strong>目的</strong>：分析LLMs在生成约束模型时所犯错误的类型，以了解不同系统提示对错误的影响。</li>
<li><strong>方法</strong>：将错误分为两类：可检测错误（如代码无法执行、输出解析问题或超时）和建模错误（如生成的模型不可满足或产生错误的解决方案）。对不同系统提示配置下的错误进行了统计<br>^6.2.2^<br>。</li>
<li><strong>结果</strong>：随着系统提示详细程度的增加，可检测错误减少，而建模错误增加<br>^6.2.2^<br>。这表明更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误。</li>
</ul>
<p>这些实验综合评估了LLMs在约束建模中的性能，并探索了不同建模框架、系统提示和推理时计算方法对性能的影响。实验结果为如何利用LLMs简化约束编程的建模过程提供了重要的见解。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文在探索利用大型语言模型（LLMs）进行约束建模方面已经取得了显著的成果，但仍有一些可以进一步探索的点，以推动这一领域的研究进展。以下是一些潜在的研究方向：</p>
<p>1. <strong>多轮交互式建模</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在单次生成模型的准确性上，但在实际应用中，建模过程往往需要多次迭代和用户反馈。</li>
<li><strong>研究方向</strong>：探索多轮交互式建模，允许用户和系统之间进行多次交互，逐步改进和细化模型。这可以通过设计更复杂的对话系统来实现，系统可以根据用户的反馈动态调整模型<br>^7^<br>。</li>
</ul>
<p>2. <strong>工业级问题的应用</strong></p>
<ul>
<li><strong>背景</strong>：现有的基准数据集（如CP-Bench）虽然多样化，但主要来源于学术领域，与实际工业问题仍有差距。工业问题通常涉及更多的数据、更复杂的约束和更详细的描述。</li>
<li><strong>研究方向</strong>：与工业界合作，收集和构建更接近实际应用场景的组合问题数据集。这将有助于评估LLMs在处理更复杂、更实际问题时的表现<br>^24^<br>。</li>
</ul>
<p>3. <strong>模型效率优化</strong></p>
<ul>
<li><strong>背景</strong>：虽然论文主要关注模型的正确性，但在实际应用中，模型的求解效率同样重要。高效的模型可以显著减少求解时间和计算资源的消耗。</li>
<li><strong>研究方向</strong>：探索如何利用LLMs学习生成更高效的约束模型，例如通过引入启发式方法、动态约束调整或模型重构技术<br>^32^<br>。</li>
</ul>
<p>4. <strong>监督式微调</strong></p>
<ul>
<li><strong>背景</strong>：论文中提到，虽然CP-Bench数据集为评估LLMs提供了基础，但目前的数据集主要用于测试，而不是训练。</li>
<li><strong>研究方向</strong>：构建一个包含约束模型和问题描述的训练数据集，用于对LLMs进行监督式微调。这可能需要大量的标注数据，但有望进一步提高LLMs在约束建模任务中的性能<br>^4^<br>。</li>
</ul>
<p>5. <strong>跨领域知识迁移</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在训练过程中接触了大量的文本数据，这些数据可能包含不同领域的知识。然而，如何有效地将这些跨领域的知识迁移到约束建模任务中仍是一个未解决的问题。</li>
<li><strong>研究方向</strong>：研究如何利用LLMs的跨领域知识，通过迁移学习或领域适应技术，提高其在特定约束建模任务中的表现<br>^14^<br>。</li>
</ul>
<p>6. <strong>模型的可解释性</strong></p>
<ul>
<li><strong>背景</strong>：在实际应用中，除了模型的准确性和效率外，模型的可解释性也非常重要。用户需要理解模型是如何工作的，以及为什么会产生特定的解决方案。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的可解释性，例如通过生成详细的解释文本或可视化工具<br>^5^<br>。</li>
</ul>
<p>7. <strong>与其他AI技术的结合</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在约束建模中的应用可以与其他AI技术（如机器学习、强化学习等）相结合，以进一步提高建模和求解的效率和准确性。</li>
<li><strong>研究方向</strong>：研究如何将LLMs与这些技术相结合，例如通过强化学习来优化模型的生成过程，或者利用机器学习模型来预测模型的性能<br>^1^<br>。</li>
</ul>
<p>8. <strong>模型的泛化能力</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在特定的数据集和问题类型上，但实际应用中需要模型具有更强的泛化能力，能够处理各种不同类型的问题。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的泛化能力，例如通过引入更多的数据增强技术、设计更通用的建模框架或开发新的评估指标<br>^28^<br>。</li>
</ul>
<p>这些研究方向不仅有助于进一步提高LLMs在约束建模中的表现，还可能推动约束编程技术在更广泛领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了CP-Bench，这是一个用于评估大型语言模型（LLMs）在约束建模能力上的新基准数据集。文章的核心目标是解决约束编程（CP）中约束建模的瓶颈问题，通过利用LLMs将自然语言描述的组合问题转化为可执行的约束模型，从而降低CP的使用门槛，使其更易于被非专家用户接受和应用。以下是文章的主要内容概述：</p>
<p>背景知识</p>
<ul>
<li><strong>组合问题与约束编程</strong>：组合问题在众多实际应用中普遍存在，如物流、调度和网络设计等。CP作为一种解决问题的范式，通过声明式方法让用户指定解决方案必须满足的约束，而不是详细说明寻找解决方案的具体步骤。然而，将问题描述转化为正式的CP模型的过程复杂，需要深厚的应用领域知识和对建模框架的熟练掌握，这限制了CP技术的广泛应用<br>^1^<br>。</li>
<li><strong>大型语言模型的应用</strong>：近年来，LLMs在自然语言处理和代码生成方面取得了显著进展，为创建能够辅助或自动化建模过程的工具提供了可能。然而，将LLMs应用于约束建模面临着独特挑战，如声明式建模的复杂性、建模选择的组合性质以及验证生成模型正确性的困难<br>^4^<br>。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>CP-Bench数据集</strong>：为了解决现有评估数据集在多样性和复杂性上的不足，作者构建了CP-Bench，包含101个来自CP社区的多样化组合问题。这些问题涵盖了优化和满足问题，具有广泛的决策变量、约束和241种不同的约束类型<br>^5^<br>。</li>
<li><strong>建模框架的比较</strong>：文章比较了三种不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。通过这些框架，作者评估了LLMs在不同条件下的建模能力<br>^1^<br>。</li>
<li><strong>系统提示与推理时计算方法</strong>：为了提高LLMs生成有效约束模型的能力，文章系统地评估了基于提示和推理时计算的方法。这些方法包括不同详细程度的系统提示、检索增强型上下文学习（RAICL）、重复采样和自我验证<br>^4^<br>。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>实验设置</strong>：作者选择了8种不同的LLMs进行评估，这些模型在参数数量、上下文窗口大小、是否包含推理能力等方面存在差异<br>^6.1^<br>。</li>
<li><strong>实验结果</strong>：</li>
<li><strong>建模框架的影响</strong>：实验结果表明，使用Python基础框架（CPMpy和OR-Tools）的LLMs在生成约束模型时的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
<li><strong>系统提示的作用</strong>：更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误增加<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的效果</strong>：重复采样和自我验证方法显著提高了LLMs的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>Python基础框架的优势</strong>：Python基础框架（CPMpy和OR-Tools）更适合LLMs生成约束模型，这可能与LLMs训练过程中接触的大量Python代码有关<br>^6.2.1^<br>。</li>
<li><strong>系统提示的重要性</strong>：详细的系统提示，尤其是包含框架文档的提示，能够提高LLMs的建模准确性<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的有效性</strong>：重复采样和自我验证方法能够显著提高LLMs在约束建模任务中的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>LLMs在约束建模中的潜力与局限</strong>：尽管LLMs在约束建模中展现出了潜力，但仍然需要人类专家的验证，尤其是在关键任务中。文章中最强的性能配置（结合重复采样和自我验证的gpt-4.1-mini模型）达到了70%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>未来工作</p>
<p>文章提出了未来可能的研究方向，包括构建更接近实际应用场景的组合问题数据集、探索多轮交互式建模、研究模型效率优化、进行监督式微调、实现跨领域知识迁移、提高模型的可解释性、结合其他AI技术以及增强模型的泛化能力<br>^7^<br>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling，链接是https://arxiv.org/pdf/2506.06052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06052。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kostis Michailidis, Dimos Tsouros, Tias Guns</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06052v3.pdf">https://arxiv.org/pdf/2506.06052v3.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06052v3">https://papers.cool/arxiv/2506.06052v3</a></p>
<p><strong>Published</strong>: 2025-06-06T12:56:02Z</p>
<hr>
<h3 id="6-When-Flores-Bloomz-Wrong-Cross-Direction-Contamination-in-Machine-Translation-Evaluation"><a href="#6-When-Flores-Bloomz-Wrong-Cross-Direction-Contamination-in-Machine-Translation-Evaluation" class="headerlink" title="6. When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation"></a>6. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20858v1">When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation</a></h3><p>Large language models (LLMs) can be benchmark-contaminated, resulting in inflated scores that mask memorization as generalization, and in multilingual settings, this memorization can even transfer to “uncontaminated” languages. Using the FLORES-200 translation benchmark as a diagnostic, we study two 7-8B instruction-tuned multilingual LLMs: Bloomz, which was trained on FLORES, and Llama as an uncontaminated control. We confirm Bloomz’s FLORES contamination and demonstrate that machine translation contamination can be cross-directional, artificially boosting performance in unseen translation directions due to target-side memorization. Further analysis shows that recall of memorized references often persists despite various source-side perturbation efforts like paraphrasing and named entity replacement. However, replacing named entities leads to a consistent decrease in BLEU, suggesting an effective probing method for memorization in contaminated models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLM）可能受到基准测试污染，从而导致得分被抬高，使记忆能力被误认为是泛化能力。在多语言环境中，这种记忆甚至可能转移到“未污染”的语言中。使用 FLORES-200 翻译基准作为诊断工具，我们研究了两个 7-8B 参数、经过指令调优的多语言 LLM：Bloomz，它在 FLORES 上进行了训练，以及作为未污染对照的 Llama。我们确认了 Bloomz 的 FLORES 污染，并证明机器翻译的污染可能具有跨方向性，由于目标端的记忆，可能人为地提升在未见翻译方向上的表现。进一步分析显示，尽管对源端进行改写和命名实体替换等各种扰动努力，记忆的参考内容通常仍然被召回。然而，替换命名实体会导致 BLEU 分数持续下降，这表明这种方法可以作为检测污染模型中记忆现象的有效手段。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文聚焦的核心问题是<strong>多语言机器翻译评测中的“跨方向数据污染”</strong>。具体而言，作者试图回答：</p>
<ul>
<li>当大模型在训练阶段见过某一目标语言的参考译文后，是否会在<strong>从未见过的源→目标翻译方向</strong>上，因<strong>目标侧记忆</strong>而获得虚高的评测分数？</li>
<li>这种“跨方向污染”能否被<strong>源侧扰动</strong>（回译、改写、实体替换）削弱或暴露？</li>
<li>如何设计<strong>简单可控的探测方法</strong>，在缺乏训练数据细节的黑盒场景下，判断模型是否已被污染？</li>
</ul>
<p>简言之，论文把 FLORES-200 这一多语种平行评测集当作“诊断工具”，揭示并量化<strong>目标侧记忆</strong>如何导致<strong>源语言未见的翻译方向</strong>也被人为拔高，进而提出<strong>实体替换</strong>作为轻量级探针来检测污染。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，可按主题归类为“数据污染与记忆化”“机器翻译中的记忆化与幻觉”“多语言/跨语言污染”三条主线。</p>
<p>数据污染与记忆化</p>
<ul>
<li><strong>Magar &amp; Schwartz (2022)</strong>：首次系统提出“数据污染”定义，从记忆到利用的连续谱。</li>
<li><strong>Carlini et al. (2022)</strong>：量化不同规模语言模型的记忆化程度，给出提取攻击方法。</li>
<li><strong>Elangovan et al. (2021)</strong>：揭示 NLP 评测中训练-测试重叠导致的虚高指标。</li>
<li><strong>Zhou et al. (2023)</strong>：提出“不要让 LLM 成为评测作弊者”，强调公开评测集被预训练数据覆盖的风险。</li>
<li><strong>Shi et al. (2024)</strong>：黑盒探测预训练数据是否包含特定测试样本。</li>
<li><strong>Oren et al. (2023)</strong>：在无法访问训练数据的情况下，用统计检验证明测试集泄漏。</li>
<li><strong>Jacovi et al. (2023)</strong>：给出防止评测数据以明文形式泄漏的实践策略。</li>
<li><strong>Jiang et al. (2024)</strong>：通过受控预训练实验，量化污染对下游任务的影响。</li>
<li><strong>Yang et al. (2023)</strong>：用改写样本重思基准可靠性，发现重述仍能被模型识别。</li>
</ul>
<p>机器翻译中的记忆化与幻觉</p>
<ul>
<li><strong>Raunak et al. (2021)</strong>：指出幻觉翻译往往源于训练段落的片段记忆。</li>
<li><strong>Raunak &amp; Menezes (2022)</strong>：提出“抽取式记忆”——模型仅见部分源文即可逐字复现整段目标文。</li>
<li><strong>Raunak et al. (2022)</strong>：发现稀有模式被记忆后，导致长尾翻译错误。</li>
<li><strong>Guerreiro et al. (2023)</strong>：对多语言场景下的幻觉进行系统分类，证实记忆失败会跨语言放大。</li>
<li><strong>Kocyigit et al. (2025)</strong>：大规模受控实验显示，即使少量评测句进入预训练，也足以显著抬升 BLEU。</li>
</ul>
<p>多语言/跨语言污染</p>
<ul>
<li><strong>Yao et al. (2024)</strong>：首次证明污染可以“跨语言屏障”——模型在语言 A 见过评测，导致在语言 B 的同一评测上也表现虚高。</li>
<li><strong>Chowdhury et al. (2020; 2021)</strong>：利用嵌入空间同构检测翻译伪影，间接说明多语言平行语料易造成交叉污染。</li>
</ul>
<p>上述工作共同构成论文的学术背景，但<strong>尚未有人专门探讨“多语言平行评测集里，目标侧记忆对未见源→目标方向的跨方向污染”</strong>，这正是本文填补的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“诊断-验证-探测”三步法，系统暴露并量化跨方向污染，同时给出可落地的探针工具。</p>
<ol>
<li>诊断：设计并行实验</li>
</ol>
<ul>
<li>选用<strong>已确认</strong>在指令微调阶段用过 FLORES 的 Bloomz-7B1 作为“污染组”，<strong>未用</strong> FLORES 的 Llama-3.1-8B-Instruct 作为“干净对照组”。</li>
<li>对 15 种语言的全部 210 个源→目标方向计算 BLEU（表面记忆指标）与 COMET（语义相似度），用“高 BLEU + 高 COMET”区块定位可疑污染方向。</li>
</ul>
<ol>
<li>验证：排除“模型只是更强”的替代解释</li>
</ol>
<ul>
<li>在<strong>外部低资源印地语数据集</strong>（PMIndia、Mann-ki-Baat）上测试 Bloomz；结果 BLEU≈0，证明其并无真实翻译能力，高分仅源于对 FLORES 参考译文的记忆。</li>
<li>通过回译实验把源语替换为<strong>第三语言生成的平行句</strong>（xxxzzz→tam），仍观察到 45–80 BLEU，说明<strong>源语是否“原样”出现并非召回记忆的必要条件</strong>。</li>
</ul>
<ol>
<li>探测：提出轻量级扰动探针</li>
</ol>
<ul>
<li><strong>实体替换</strong>：用 spaCy 抽取 Named Entity，再让 Aya 生成同类型全新实体并回填到源句。</li>
<li>结果：Bloomz 在“全实体替换”后 BLEU 普遍再降 10–20 分，Llama 仅降约 5–10 分；<strong>降幅差异</strong>成为判断模型是否记忆目标的显著信号。</li>
<li>给出可复现的<strong>三步探针脚本</strong>：</li>
</ul>
<ol>
<li>在待测方向上做“实体保留”与“实体替换”两次翻译；</li>
<li>计算 ΔBLEU = BLEU_base − BLEU_replaced；</li>
<li>若 ΔBLEU 显著大于对照模型，即可怀疑该方向存在目标侧记忆污染。</li>
</ol>
<p>通过上述流程，论文不仅<strong>证实跨方向污染真实存在</strong>，还提供了一套<strong>无需训练数据即可在黑盒场景下快速筛查污染</strong>的实用工具。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计并执行了 4 组核心实验，层层递进地验证“跨方向污染”假设并提炼探针方法。所有实验均在单张 A100 80 GB 上完成，总计约 30 GPU 小时。</p>
<p>1. 大范围筛查实验</p>
<p><strong>目的</strong>：定位哪些语言方向存在“高 BLEU + 高 COMET”可疑区块。</p>
<ul>
<li><strong>数据</strong>：FLORES-200 dev 集 997 句 × 15 种语言 → 210 个源→目标方向。</li>
<li><strong>模型</strong>：Bloomz-7B1（污染组） vs. Llama-3.1-8B-Instruct（对照组）。</li>
<li><strong>指标</strong>：sacreBLEU、COMET-22。</li>
<li><strong>结果</strong>：</li>
<li>Bloomz 在 xxx→{eng,fra,mal,ory,por,tam,vie,zho} 出现 40–90 BLEU，COMET&gt;0.8；</li>
<li>Llama 仅在高资源方向出现 30–40 BLEU，且 COMET 与 BLEU 同步偏低。</li>
</ul>
<p>2. 外部集验证实验</p>
<p><strong>目的</strong>：排除“模型只是翻译能力强”的替代解释。</p>
<ul>
<li><strong>数据</strong>：PMIndia、Mann-ki-Baat 低资源印地语平行句（eng→{mal,ory,tam}）。</li>
<li><strong>模型</strong>：Bloomz。</li>
<li><strong>指标</strong>：BLEU。</li>
<li><strong>结果</strong>：BLEU≈0，证实 Bloomz 对非 FLORES 句几乎无翻译能力，高分仅来自记忆。</li>
</ul>
<p>3. 源侧扰动实验</p>
<p>3a 回译扰动</p>
<ul>
<li><strong>流程</strong>：用 Llama 先将 zzz→xxx（zzz∈{asm,bam,por}）生成回译句 xxxzzz，保持与原始目标平行；再让 Bloomz 翻译 xxxzzz→tam。</li>
<li><strong>结果</strong>：</li>
<li>回译质量 20–50 BLEU 时，Bloomz 仍输出 60+ BLEU；</li>
<li>回译质量≈0 时，部分方向仍达 20–60 BLEU，说明<strong>源句是否“原样”并非召回必要条件</strong>。</li>
</ul>
<p>3b 改写扰动</p>
<ul>
<li><strong>流程</strong>：用 Llama 对同一源句做意译（xxxpp），与回译句对比。</li>
<li><strong>结果</strong>：回译句 BLEU 比改写句再高 20–30 分，进一步证明<strong>表面相似度越高越易触发记忆</strong>，但即使低相似度也无法彻底消除召回。</li>
</ul>
<p>4. 实体替换探针实验</p>
<p><strong>目的</strong>：找到可在黑盒场景下快速检测记忆的低成本信号。</p>
<ul>
<li><strong>流程</strong>：</li>
</ul>
<ol>
<li>用 spaCy 抽取英语句中全部实体→Aya 生成同类型全新实体→回填到源句并翻译为目标语；</li>
<li>设置“单实体替换”“全实体替换”两档，对比原始句结果。</li>
</ol>
<ul>
<li><strong>模型</strong>：Bloomz &amp; Llama，六组高资源方向 eng/fra/por/spa/vie/zho→tam。</li>
<li><strong>指标</strong>：ΔBLEU = BLEU_base − BLEU_replaced。</li>
<li><strong>结果</strong>：</li>
<li>Bloomz 全实体替换平均降 15.3 BLEU，峰值 20+；</li>
<li>Llama 仅降 7.2 BLEU；</li>
<li><strong>ΔBLEU 差异显著</strong>，可用作污染探测信号。</li>
</ul>
<p>5. 受控微调复现实验（bonus）</p>
<p><strong>目的</strong>：验证“仅通过 eng↔xxx 双向数据即可产生跨方向污染”这一机制假设。</p>
<ul>
<li><strong>流程</strong>：用 Axolotl 在 FLORES eng↔{11 种语言} 上继续微调 Llama-3.1-8B，直至严重过拟合。</li>
<li><strong>测试</strong>：微调后模型对<strong>未见方向</strong> zzz→yyy 进行推理。</li>
<li><strong>结果</strong>：</li>
<li>BLEU 在大多数未见方向提升 8–16 分；</li>
<li>COMET 反而下降，说明极端记忆拉高 BLEU 却牺牲语义；</li>
<li>再次印证<strong>目标侧记忆是跨方向污染主因</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为论文结论的自然延伸，分为“机制深挖”“探针升级”“场景拓展”三大类，均围绕“跨方向污染”这一核心现象展开。</p>
<p>1. 机制深挖</p>
<ul>
<li><strong>激活追踪</strong><br>用梯度<em>探针</em>或隐状态探针定位“实体→记忆目标”触发路径，验证实体替换降低 BLEU 是否因抑制了特定注意力头或 FFN 关键神经元。</li>
<li><strong>语言层级差异</strong><br>将实验扩展到<strong>屈折度、书写系统、语序</strong>差异更大的语言对，观察跨方向污染是否仍仅由目标侧主导，抑或某些源语结构会放大记忆召回。</li>
<li><strong>训练阶段对比</strong><br>分别用<em>预训练</em>、<em>指令微调</em>、<em>持续预训练</em>三阶段引入 FLORES，量化哪一阶段对跨方向污染贡献最大，从而指导数据清洗时机。</li>
</ul>
<p>2. 探针升级</p>
<ul>
<li><strong>语义级探针</strong><br>当前仅用 BLEU 差值。可引入<em>实体感知 COMET</em> 或<em>实体替换后的 BERTScore 差值</em>，排除“替换后句子本身更难翻译”带来的混淆。</li>
<li><strong>多模态探针</strong><br>若评测集含<strong>图像-文本对</strong>（如 Multi30K），可同步替换图像中的 OCR 文字或视觉实体，检验跨模态记忆是否也存在“目标侧污染”。</li>
<li><strong>自适应阈值</strong><br>建立<em>模型规模-语言资源度-ΔBLEU</em>三维映射，给出“污染临界阈值”在线查询表，使探针无需干净对照模型即可给出统计显著性。</li>
</ul>
<p>3. 场景拓展</p>
<ul>
<li><strong>生成任务</strong><br>将探针迁移到<strong>多语言摘要、问答、对话</strong>任务，验证“目标侧记忆”是否同样导致跨语言虚高。</li>
<li><strong>多引擎评测</strong><br>对同一评测集同时使用<em>NMT、LLM、云 API</em>三类系统，看商业引擎是否也呈现一致的实体替换 ΔBLEU 模式，为第三方评测提供审计工具。</li>
<li><strong>动态去污染</strong><br>基于实体替换思想，构建<strong>对抗性数据增强</strong>流程：在训练侧实时替换实体并保留标签，观察能否在<em>不降下游性能</em>的前提下抑制记忆化。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文主旨</strong><br>用 FLORES-200 机器翻译基准做“诊断探针”，系统揭示并量化<strong>大模型在多语言评测中的跨方向数据污染</strong>：模型只要见过“某目标语言”的参考译文，就能在<strong>从未见过的源→该目标方向</strong>上凭记忆输出高 BLEU，导致虚高评分。</p>
<p><strong>核心发现</strong></p>
<ol>
<li><strong>污染可跨方向</strong>：Bloomz 因微调阶段摄入 FLORES，在 210 个方向中大量出现“80+ BLEU &amp; 0.9+ COMET”区块；对照模型 Llama 无此现象。</li>
<li><strong>目标侧记忆为主</strong>：回译、改写等源侧扰动仍常触发 45–80 BLEU；仅当<strong>替换源句全部命名实体</strong>时，BLEU 才系统性下降 10–20 分，且降幅显著大于干净模型，可用作<strong>黑盒探针</strong>。</li>
<li><strong>机制复现</strong>：用 Llama 仅在 eng↔xxx 双向数据上继续微调，即可让<strong>未见方向</strong>zzz→yyy 的 BLEU 提升 8–16 分，证实跨方向污染可由<strong>纯目标侧记忆</strong>产生。</li>
</ol>
<p><strong>方法论贡献</strong></p>
<ul>
<li>提出<strong>实体替换 ΔBLEU 探针</strong>：无需训练数据，仅比较“原始句”与“实体替换句”的 BLEU 差值，即可快速筛查模型是否记忆了评测参考。</li>
<li>给出<strong>15 语言、210 方向</strong>的 BLEU/COMET 全景热图，为后续研究提供基准对照。</li>
</ul>
<p><strong>实践建议</strong><br>评测多语言模型时，务必：</p>
<ol>
<li>检查<strong>所有翻译方向</strong>而非仅报告→英语；</li>
<li>运行<strong>实体替换探针</strong>，ΔBLEU 异常高即视为污染可疑；</li>
<li>对可疑方向改用<strong>外部独立数据集</strong>复测，避免将记忆当泛化。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是When%20Flores%20Bloomz%20Wrong%3A%20Cross-Direction%20Contamination%20in%20Machine%20Translation%20Evaluation，链接是https://arxiv.org/pdf/2601.20858，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20858。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: David Tan, Pinzhen Chen, Josef van Genabith, Koel Dutta Chowdhury</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20858v1.pdf">https://arxiv.org/pdf/2601.20858v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20858v1">https://papers.cool/arxiv/2601.20858v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:21Z</p>
<hr>
<h3 id="7-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models"><a href="#7-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models" class="headerlink" title="7. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models"></a>7. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20856v1">SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</a></h3><p>Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型的能力在复杂推理任务上得到了越来越多的测试，但它们的长远规划能力尚未得到广泛研究。在本工作中，我们对最先进的大型推理模型（LRMs）的规划和长远推理能力进行了系统评估。我们提出了一个基于推箱子（Sokoban）谜题的新型基准测试，该测试经过刻意简化，以将长远规划与状态持久性隔离开来。我们的研究结果显示，当需要超过25步才能达到解决方案时，规划性能会显著下降，这表明前向规划能力存在根本性限制。我们还表明，为LRMs配备规划领域定义语言（PDDL）的解析、验证和求解工具可以带来适度的改进，这表明其架构本身存在限制，可能仅靠测试时的规模扩展无法克服。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统评估当前大型推理模型（Large Reasoning Models, LRMs）在**长程规划（long-horizon planning）<strong>与</strong>长程推理（long-horizon reasoning）**上的能力边界。核心问题可以概括为：</p>
<blockquote>
<p><strong>当解空间深度增加、但分支因子被刻意压到最低时，LRMs 是否仍能维持连贯、可行且最优的动作序列？</strong></p>
</blockquote>
<p>为隔离“深度”与“复杂性”因素，作者构造了极简的一维 Sokoban 走廊谜题：</p>
<ul>
<li>仅 1 个箱子、1 个目标、1 条直线通道；</li>
<li>唯一可变参数是走廊长度 ℓ，它直接决定最优解长度（≈ℓ 步）；</li>
<li>无外部记忆、无中间反馈、无状态验证，完全依赖模型内部表示。</li>
</ul>
<p>通过该基准，论文试图回答以下子问题：</p>
<ol>
<li>模型在无需外部工具时，究竟能稳定规划到多远的 horizon？</li>
<li>当 horizon 超过某一阈值（≈25–30 步）时，失效模式是“搜索错误”还是“更底层的计数/状态跟踪错误”？</li>
<li>若给模型提供 PDDL 解析器与外部求解器（LLM-Modulo 框架），能否弥补其固有缺陷？</li>
<li>上述改进若存在，是否足以克服长程动作表示与空间 grounding 的瓶颈？</li>
</ol>
<p>综上，论文并非追求更复杂的规划域，而是<strong>用极简环境放大“深度”维度</strong>，从而暴露 LRMs 在长序列状态一致性上的结构性局限，并验证“测试时扩展+符号工具”是否足以突破该局限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四大脉络，均围绕“LLM 规划能力”与“Sokoban 作为诊断环境”展开：</p>
<ol>
<li><strong>通用规划基准与 LLM 失效诊断</strong></li>
</ol>
<ul>
<li><strong>PlanBench</strong>（Valmeekam et al., 2022, 2023a, 2025）：PDDL 格式的 BlocksWorld、Logistics 等经典域，发现 LLM 随 horizon 增加迅速退化，且错误集中在无效动作与循环。</li>
<li><strong>8-puzzle 文本版</strong>（Schepanowski &amp; Ling, 2025）：证明即使状态空间仅 181440，LLM 仍因“弱内部状态表示”与“误导式启发搜索”而频繁产生非法移动或提前终止。</li>
<li><strong>字符级计数任务</strong>（Karpathy, 2024；Xu &amp; Ma, 2025；CharBench, Uzan &amp; Pinter, 2025）：揭示“strawberry 有几个 r”类失败源于 tokenization 之外的长度-计数表征缺陷，与本文“走廊计数漂移”论点直接呼应。</li>
</ul>
<ol>
<li><strong>LLM-Modulo / 工具增强规划</strong></li>
</ol>
<ul>
<li><strong>LLM⇄PDDL 管道</strong>（Silver et al., 2022；Oswald et al., 2024；La Malfa et al., 2025）：让模型仅生成 domain 或 problem 文件，由外部求解器完成搜索，显著降低非法动作率。</li>
<li><strong>PlanBench-o1 实验</strong>（Valmeekam et al., 2025）：o1-preview 裸机成功率 10–12%，接入 Fast-Downward 后提升至 43%，但计算成本增加 10× 以上；本文 LLM-Modulo 结果与之趋势一致，进一步指出“即使符号求解，空间量纲错误仍存”。</li>
</ul>
<ol>
<li><strong>Sokoban 作为认知诊断环境</strong></li>
</ol>
<ul>
<li><strong>IPC-2023 Sokoban track</strong>（Taitler et al., 2024）：正式将 Sokoban 纳入国际规划竞赛，强调其“无捷径、无模式化解”特性。</li>
<li><strong>RNN 专用求解器</strong>（Taufeeque et al., 2024；Jolicoeur-Martineau, 2025）：在 1000+ 关卡达到 &gt;95% 成功率，表明非 Transformer 架构可通过循环隐状态掌握空间规则，反衬 LLM 的隐状态短板。</li>
<li><strong>o1-preview 裸机实验</strong>（Valmeekam et al., 2025）：在标准 2D Sokoban 仅 10–12% 成功率，与本文“一维走廊”结果形成对照，说明“简化空间并未显著缓解长程退化”。</li>
</ul>
<ol>
<li><strong>长程推理与“漫游”现象的理论解释</strong></li>
</ol>
<ul>
<li><strong>“Wandering vs. Systematic Exploration”框架</strong>（Lu et al., 2025）：提出 LLM 推理轨迹本质是“无 visited-state 集的随机游走”，在深度线性链上表现为重复循环与计数漂移，与本文 token 上限耗尽、前缀准确率指数下降现象完全一致。</li>
<li><strong>工作记忆容量度量</strong>（Zhang et al., 2024）：用 dual-task 方式证明 LLM 的“有效工作记忆”不超过 7–9 个 token 级状态，与本文 25 步左右断崖式失效点数量级一致。</li>
</ul>
<p>综上，既有研究共同指向：<strong>LLM 的规划瓶颈并非高层搜索策略，而是底层状态计数、空间 grounding 与工作记忆持久化的结构性缺陷</strong>。本文通过“极简走廊”设计，将上述观察从复杂域剥离出来，首次给出可量化、可复现的长程失效阈值。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一种“治愈”长程规划缺陷的新架构，而是设计了一套<strong>极简诊断协议</strong>，通过“控制变量→量化失效→工具增强→对比分析”四步，把问题拆成可测量的现象，从而给出改进方向与上限证据。</p>
<ol>
<li><strong>控制变量：构造 SokoBench 走廊谜题</strong></li>
</ol>
<ul>
<li>仅留“长度 ℓ”一个自由参数，最优解长度 ≈ℓ，分支因子=1。</li>
<li>旋转增广（0°/90°/180°/270°）降低预训练泄露风险。</li>
<li>由此将“长程”与“空间复杂性”解耦，任何失效都可归因于<strong>深度+内部状态跟踪</strong>。</li>
</ul>
<ol>
<li><strong>量化失效：1-shot 裸机实验</strong></li>
</ol>
<ul>
<li>指标：Exact-match Accuracy、Prefix Accuracy、Manhattan Distance、token 消耗曲线。</li>
<li>发现：<br>– 25 ≤ ℓ ≤ 30 是断崖阈值，之后 Accuracy 指数下降。<br>– 错误主因是“计数漂移”与“循环输出”，与 Lu et al. 的“wandering”假说一致。<br>– Token 消耗随 ℓ 线性增加，但正确解的方差远小于错误解，说明失败路径发散。</li>
</ul>
<ol>
<li><strong>工具增强：LLM-Modulo 管道</strong></li>
</ol>
<ul>
<li>给模型只留两项任务：<br>① 把 ASCII 图翻译成 PDDL problem 文件；<br>② 调用外部 Unified-Planning 求解器（Fast-Downward / PyperPlan）。</li>
<li>结果：<br>– GPT-5-mini 裸机 25 步后≈0，工具增强后 100 步仍维持 30–40% Accuracy。<br>– 但垂直走廊显著弱于水平走廊，说明“空间量纲估计”错误未被完全消除。<br>– 7/320 次出现 PDDL 语法错，其余失败均因“problem 文件与真实地图尺寸不符”，即<strong>内部计数偏差外溢到符号描述</strong>。</li>
</ul>
<ol>
<li><strong>对比分析与上限证据</strong></li>
</ol>
<ul>
<li>工具增强仅带来<strong>边际提升</strong>，未能突破 25–30 步的“软上限”，证明缺陷<strong>架构层</strong>而非<strong>搜索层</strong>。</li>
<li>给出明确结论：测试时扩展 + 符号求解器<strong>不足以</strong>克服缺乏“显式状态-转移记忆”的硬伤；未来需引入<br>– 显式 visited-state 缓存，<br>– 可学习的计数/坐标专用模块，<br>– 或直接在隐状态内加入符号接地机制。</li>
</ul>
<p>通过上述四步，论文把“长程规划难”从经验观察转化为<strong>可复现指标</strong>，并用量化的方式说明：<strong>在深度线性链场景下，当前 LRM 的失效主因是内部状态漂移，而非高层搜索策略；外部符号工具只能缓解，不能根除。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计 <strong>两套主实验</strong> 与 <strong>一套旋转对照分析</strong>，全部在公开数据集 SokoBench（ℓ=5–100，步长 5，4 种旋转，共 80 张地图）上完成。实验变量、模型、重复次数与核心指标如下：</p>
<p>实验 1 1-shot 裸机推理（无外部工具）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型</td>
<td>DeepSeek-R1-0528、GPT-5-mini、GPT-oss-120B</td>
</tr>
<tr>
<td>Prompt</td>
<td>系统提示 + 表 1 符号映射 + 1 条 4 步示范解</td>
</tr>
<tr>
<td>重复</td>
<td>每 ℓ×旋转 8 次（nt=8，nr=4，总 640 次/模型）</td>
</tr>
<tr>
<td>输出限制</td>
<td>max_tokens=32768，temperature 默认</td>
</tr>
<tr>
<td>记录量</td>
<td>完整推理 trace、最终 字符串、token 消耗</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong></p>
<ol>
<li>Accuracy(ℓ) — Exact string 匹配率</li>
<li>PrefixAccuracy(ℓ) — 逐字符前缀正确率</li>
<li>Manhattan-Distance(ℓ) — 终端坐标与目标 L1 距离</li>
<li>Token 回归 — 正确/错误解的斜率与 R²</li>
</ol>
<p><strong>关键发现</strong></p>
<ul>
<li>ℓ≈25 处出现指数级断崖；</li>
<li>错误解 token 方差远大于正确解，且常因循环输出顶到上限；</li>
<li>旋转 90°/270°（垂直走廊）普遍更低分，揭示“行计数”难度 &gt;“列计数”。</li>
</ul>
<p>实验 2 LLM-Modulo 工具增强</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>可用模型</td>
<td>仅 GPT-5-mini（成本与工具调用稳定性权衡）</td>
</tr>
<tr>
<td>工具链</td>
<td>自定义 MCP 服务器 → PyperPlan / Fast-Downward</td>
</tr>
<tr>
<td>模型任务</td>
<td>生成 PDDL problem 文件 → 调用 solve_problem → 返回 LURD 解</td>
</tr>
<tr>
<td>重复</td>
<td>nt=4，nr=4，总 320 次</td>
</tr>
<tr>
<td>失败处理</td>
<td>语法或语义错误自动重试，最多 3 轮</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong><br>同上 3 项，但额外记录：</p>
<ul>
<li>PDDL 语法错误率</li>
<li>语义不一致率（problem 与真实地图尺寸不符）</li>
<li>工具调用总 token 数（含推理+PDDL 文本）</li>
</ul>
<p><strong>关键发现</strong></p>
<ul>
<li>100 步处 Accuracy 仍≈0.35，较裸机提升 3×，但远未收敛到 1；</li>
<li>7/320 次语法错；其余失败均因“地图长度数错”导致求解器给出短计划；</li>
<li>垂直走廊性能仍显著低于水平走廊，说明<strong>空间量纲估计偏差未被工具消除</strong>。</li>
</ul>
<p>实验 3 旋转细粒度对照（附录 C）</p>
<ul>
<li>仅展示 GPT-5-mini LLM-Modulo 结果；</li>
<li>将 80 张地图按 4 个旋转角度拆分为 4 条子曲线；</li>
<li>量化同一 ℓ 下 0°/90°/180°/270° 的 Accuracy 差异，证实“垂直 vs 水平”效应稳定存在。</li>
</ul>
<p>补充微观分析</p>
<ul>
<li><strong>Token 线性回归</strong>：正确解斜率 29.8–51.1 token/ℓ，错误解斜率 55.2–86.3 token/ℓ，验证“失败轨迹更长且更发散”。</li>
<li><strong>循环检测</strong>：对顶到 32768 上限的 trace 进行子串重复计数，平均 42% 内容属于重复 L/R 语句，直接支持“wandering”假说。</li>
<li><strong>曼哈顿距离热图</strong>：裸机解常出现“玩家最终落在目标上”的非法状态，进一步佐证内部状态未遵守墙/箱约束。</li>
</ul>
<p>综上，实验由<strong>裸机诊断→工具增强→旋转消融→微观痕迹分析</strong>四层组成，共同量化出 LRM 长程规划失效的阈值、特征与改进上限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SokoBench 的“直接续作”或“横向扩展”，均围绕<strong>把当前暴露的单一缺陷拆成多维、可干预变量</strong>展开，以便下一步真正突破 25–30 步的“软上限”。</p>
<p>1 状态表示缺陷的显式干预</p>
<ul>
<li><strong>可学习坐标嵌入</strong>：在 Transformer 隐状态外，显式维护一个  (x,y)  浮游向量，每一步自回归后通过对比损失与符号坐标对齐，检验“外置坐标”能否阻断计数漂移。</li>
<li><strong>Visited-State Cache</strong>：用外部键-值缓存记录已生成状态哈希，采样时禁止重复输出，测试“循环”是否纯粹由无记忆导致。</li>
<li><strong>Delta 预测 vs 绝对预测</strong>：让模型不再预测绝对位置，而是预测“相对位移 + 边界检查”，降低长链绝对坐标累积误差。</li>
</ul>
<p>2 复杂度维度的渐进叠加</p>
<ul>
<li><strong>多箱走廊</strong>：固定 ℓ=50，逐步增加箱子数  b=1…5 ，引入<strong>死锁检测</strong>需求，观察错误模式从“计数漂移”转向“冲突/死锁”的临界  b 。</li>
<li><strong>单侧/双侧分支</strong>：在走廊中段加入  k=1,2  条“死胡同”侧枝，保持解长度不变但增加<strong>搜索宽度</strong>，量化“分支因子”与“深度”对失效的相对贡献。</li>
<li><strong>动态障碍</strong>：在第  t  步随机出现/消失一堵墙，迫使模型<strong>重规划</strong>，测试工作记忆刷新与快速回溯能力。</li>
</ul>
<p>3 感知-动作接口的消融</p>
<ul>
<li><strong>多种编码</strong>：</li>
<li>行号-列号坐标 <code>(r3,c5)</code></li>
<li>相对距离 <code>(Δx,Δy)</code></li>
<li>图像化 2-D 矩阵（利用 VLRM）<br>对比同一地图在不同编码下的 Accuracy 曲线，找出<strong>最敏感格式</strong>，为后续“最佳接地”提供数据。</li>
<li><strong>稀疏 vs 密集奖励提示</strong>：在推理链中每隔  k  步插入人工“状态校验”句子（“此时箱子应在 x=7”），测量<strong>外部提醒</strong>能把阈值 ℓ 提高多少。</li>
</ul>
<p>4 训练与推理策略</p>
<ul>
<li><strong>课程强化微调</strong>：用增量 ℓ 的课程数据对同一基础模型做轻量级 LoRA 微调，验证“见过更长序列”能否把断崖后移，或仅提升记忆。</li>
<li><strong>Best-of-N + 状态验证器</strong>：在推理阶段采样 N 条计划，用外部 Sokoban 模拟器过滤非法解，再选最短合法解；测量 N=1…100 时的收益曲线，给出“纯采样”上限。</li>
<li><strong>递归子目标分解</strong>：强制模型先在高层输出“分阶段里程碑”（如“先把箱推到 x=30”），再对每个子目标展开低层动作；对比端到端方式，检验“显式分层”能否降低有效深度。</li>
</ul>
<p>5 神经-符号混合上限</p>
<ul>
<li><strong>可微规划器端到端训练</strong>：把 Fast-Downward 的编码层换成可微 GNN，生成计划后通过策略梯度微调，看“符号求解器可微化”能否让模型学会<strong>修正 PDDL 参数错误</strong>。</li>
<li><strong>逆向数据增强</strong>：用最优求解器生成百万级走廊解，再反向生成“错误 PDDL 问题”(坐标偏移 ±1)，训练模型<strong>检测并自纠错</strong>，量化“语法+语义”双重验证的剩余错误率。</li>
</ul>
<p>6 横向迁移与外部效度</p>
<ul>
<li><strong>跨域深度链</strong>：保持“单参数难度”理念，换用 Tower of Hanoi、Sliding-Tile、Monkey-and-Bananas 的一维变体，检验“25–30 步阈值”是否<strong>与域无关</strong>，从而验证其为<strong>通用工作记忆极限</strong>。</li>
<li><strong>真实机器人走廊任务</strong>：把 ASCII 走廊映射到实体 1-D 轨道（机器人推箱），观察语言模型生成的计划在高延迟、低精度执行环境下的<strong>鲁棒性差距</strong>，引入“物理漂移”新变量。</li>
</ul>
<p>7 社会-技术风险</p>
<ul>
<li><strong>预训练数据溯源</strong>：对垂直 vs 水平走廊差异做 n-gram 重叠分析，若水平模板在 CommonCrawl 出现频率显著更高，可解释“方向偏差”部分源于<strong>数据不平衡</strong>而非空间认知。</li>
<li><strong>能力-成本权衡公开仪表盘</strong>：建立在线基准，实时记录不同模型在 ℓ=50 走廊的“成功率 / 美元 / 克 CO₂”三元组，为社区提供<strong>绿色 AI</strong> 视角下的规划能力评价。</li>
</ul>
<p>以上方向遵循“单变量介入-可量化-可复现”原则，既可单独成文，也可组合成新的<strong>多维度 SokoBench v2</strong>，目标是<strong>把“长程规划”从黑盒艺术变成白盒实验科学</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可浓缩为“一个基准、两条曲线、三点证据、四步结论”：</p>
<p>一个基准</p>
<ul>
<li><strong>SokoBench</strong>：极简一维 Sokoban 走廊，仅“长度 ℓ”可变，最优解 ≈ℓ 步；</li>
<li>80 张地图（ℓ=5–100，步长 5，4 旋转），完全公开，用于隔离“深度”与“分支/空间复杂性”。</li>
</ul>
<p>两条性能曲线</p>
<ol>
<li><strong>裸机 LRM</strong>：</li>
</ol>
<ul>
<li>25–30 步处 Accuracy 指数断崖；</li>
<li>token 消耗随 ℓ 线性增加，但错误解方差远大于正确解。</li>
</ul>
<ol>
<li><strong>LLM-Modulo（+PDDL 求解器）</strong>：</li>
</ol>
<ul>
<li>相同阈值仍在，100 步时 Accuracy≈0.35，提升有限；</li>
<li>失败主因：模型生成的 PDDL 问题“长度数错”，外置求解器无法弥补。</li>
</ul>
<p>三点证据</p>
<ol>
<li><strong>计数漂移是主因</strong>：Prefix Accuracy 与 Manhattan Distance 同步下降，循环输出占 token 上限 42%。</li>
<li><strong>状态记忆缺失</strong>：模型无 visited-state 概念，反复 proposing 同一动作，体现“wandering”而非系统搜索。</li>
<li><strong>空间接地缺陷</strong>：垂直走廊显著弱于水平走廊，即使用符号求解器亦未消除，说明“坐标量纲”估计错误根深蒂固。</li>
</ol>
<p>四步结论</p>
<ol>
<li>当前 LRM 的长程规划瓶颈主要在<strong>底层状态跟踪与计数表征</strong>，而非高层搜索策略。</li>
<li>25–30 步是<strong>架构性软上限</strong>，测试时扩展或外接符号求解器<strong>仅能边际改善</strong>，无法突破。</li>
<li>未来需引入<strong>显式坐标记忆、可微符号接地或分层子目标验证</strong>等结构性改进，否则单纯堆算力无益。</li>
<li>SokoBench 以极简环境量化深度失效，为社区提供了<strong>可复现、可干预</strong>的长程规划诊断平台。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2601.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20856。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20856v1.pdf">https://arxiv.org/pdf/2601.20856v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20856v1">https://papers.cool/arxiv/2601.20856v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:00Z</p>
<hr>
<h3 id="8-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images"><a href="#8-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images" class="headerlink" title="8. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images"></a>8. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17934v2">From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</a></h3><p>Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM’s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>像 Segment Anything Model (SAM) 这样的基础模型展示了强大的泛化能力，但由于领域偏移、标签稀缺以及参数高效微调（PEFT）无法利用未标注数据，将它们应用于医学影像仍然十分困难。尽管像 U-Net 这样的传统模型在半监督医学学习中表现出色，它们在辅助 PEFT SAM 方面的潜力却在很大程度上被忽视。我们提出了 SC-SAM，这是一个专家-通用框架，其中 U-Net 提供基于点的提示和伪标签来引导 SAM 的适应，而 SAM 则作为强大的通用监督器来规范 U-Net。这种相互引导形成了双向协同训练循环，使两个模型都能有效利用未标注数据。在前列腺 MRI 和息肉分割基准实验中，我们的方法达到了最先进的结果，优于其他现有的半监督 SAM 变体，甚至超过了像 MedSAM 这样的医学基础模型，突显了专家-通用合作在高效标签医学影像分割中的价值。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对医学图像分割中“基础模型难以利用无标注数据”这一核心矛盾展开。具体而言：</p>
<ul>
<li><p><strong>SAM 在医学域的瓶颈</strong><br>尽管 Segment Anything Model（SAM）具备强大的零样本泛化能力，但面对医学影像时存在显著域偏移；同时，参数高效微调（PEFT）仍依赖少量标注，无法直接从无标注样本学习，导致大量无标签数据被闲置。</p>
</li>
<li><p><strong>传统半监督方案与 SAM 的割裂</strong><br>基于 U-Net 的半监督框架（一致性正则、伪标签等）已证明无标注数据的价值，却与 SAM 的 PEFT 流程脱节；现有尝试或仅把 SAM 当教师，或引入复杂双解码结构，仍未让 SAM 真正“吸收”无标注信息。</p>
</li>
<li><p><strong>目标</strong><br>提出 SC-SAM，让“传统专家”U-Net 与“通用基础模型”SAM 形成双向协同：</p>
</li>
</ul>
<ol>
<li>U-Net 先利用全部数据（含无标注）生成可靠点提示与伪标签，指导 SAM 的 PEFT；</li>
<li>SAM 反过来用其高阶语义输出正则化 U-Net，抑制噪声。<br>通过该“专家-通才”循环，使 SAM 在仅 5 %–10 % 标注比例下即可充分挖掘无标注医学图像，提升分割精度并超越现有半监督 SAM 变体及医学专用基础模型。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四类，均围绕“如何在半监督或低标注场景下让 SAM 及其变体适应医学图像”展开：</p>
<ol>
<li><strong>直接微调 SAM（PEFT-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>Medical SAM Adapter</strong>（Wu et al., MedIA 2025）<br>在 ViT 编码器内插入 Adapter 层，仅用小规模标注数据做参数高效微调，未利用无标签集。</li>
<li><strong>SAM-Med2D</strong>（Cheng et al., arXiv 2023）<br>将 2D 医学切片重构成 RGB 三通道后直接全微调，参数开销大且仍依赖标注。</li>
</ul>
<ol>
<li><strong>双分支/交叉提示一致性（Dual-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>CPC-SAM</strong>（Miao et al., MICCAI 2024）<br>两个独立 mask decoder 互作“教师”，通过交叉提示生成伪标签，但两分支均为 SAM 自身，易受域偏移影响而耦合。</li>
</ul>
<ol>
<li><strong>专家网络为 SAM 提供提示（SP-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>KnowSAM</strong>（Huang et al., IEEE T-MI 2025）<br>用 U-Net+V-Net 生成融合 mask 作为 SAM 的 mask prompt，再以 KL 蒸馏让专家模仿 SAM；专家仅充当“提示生成器”，SAM 并未反向受益。</li>
</ul>
<ol>
<li><strong>传统半监督 CNN 框架</strong></li>
</ol>
<ul>
<li><strong>U-Net 系列</strong>（Ronneberger et al., MICCAI 2015；Zhou et al., 2018；Jha et al., 2019）<br>通过一致性正则、伪标签、Mean Teacher 等手段充分挖掘无标注数据，但无法与 SAM 的 PEFT 流程协同。</li>
</ul>
<p>SC-SAM 与上述工作的本质区别在于：</p>
<ul>
<li>首次让“专家”U-Net 与“通才”SAM 形成<strong>双向协同训练闭环</strong>；</li>
<li>引入 sigmoid ramp-up 缓解早期噪声伪标签对 SAM 的干扰；</li>
<li>既用 U-Net 生成点提示/伪标签指导 SAM，又用 SAM 的语义输出正则化 U-Net，从而真正将无标注数据的信息注入基础模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 SC-SAM，通过“专家-通才”双向协同训练框架，把 U-Net 对无标注数据的利用能力无缝迁移到 SAM 的 PEFT 流程。核心机制分三步：</p>
<ol>
<li><p><strong>专家先行：U-Net 半监督预热</strong><br>用标准半监督策略（一致性正则 + 伪标签）在全部数据上训练 U-Net，使其对标注与无标注图像均产生可靠预测  P^(UNet) 。</p>
</li>
<li><p><strong>通才跟进：SAM 接受 U-Net 的“提示-伪标签”</strong></p>
</li>
</ol>
<ul>
<li>从  P^(UNet)  随机采样 5 前景点 + 5 背景点，生成点提示</li>
</ul>
<p>points = Sample(P^(UNet))</p>
<ul>
<li>将点提示送入 SAM 的 prompt encoder，与图像编码特征一起经 mask decoder 得到预测</li>
</ul>
<p>P^(SAM) = D!(E(X);; P(points))</p>
<ul>
<li>对无标注图像，把 U-Net 的预测当作伪标签，计算无监督损失</li>
</ul>
<p>L<em>(unsup)^(SAM) = L</em>(seg)(P^(SAM), P^(UNet))</p>
<ol>
<li><strong>反向正则：SAM 输出平滑 U-Net</strong><br>同时用 SAM 的预测  P^(SAM)  作为质量更高的伪标签，反向监督 U-Net</li>
</ol>
<p>L<em>(unsup)^(UNet) = L</em>(seg)(P^(UNet), P^(SAM))</p>
<ol>
<li><strong>稳定训练：sigmoid ramp-up</strong><br>为避免早期 U-Net 噪声淹没 SAM，引入时变权重</li>
</ol>
<p>ω(t)= exp!l(-(1-(t) / (T<em>(max)))^2r), &amp; 0le tle T</em>(max)[2pt] 1, &amp; t&gt;T_(max)</p>
<p>总无监督损失</p>
<p>L<em>(unsup) = L</em>(unsup)^(UNet) + ω(t),L_(unsup)^(SAM)</p>
<ol>
<li><strong>联合目标</strong><br>加上标注数据的监督项，整体损失</li>
</ol>
<p>L<em>(total) = L</em>(seg)(P^(UNet), Y) + L<em>(seg)(P^(SAM), Y)</em>(L)<em>(sup) + L</em>(unsup)</p>
<p>通过该闭环，U-Net 提供结构先验与伪标签，SAM 提供高阶语义正则，二者在每次迭代中同步更新，实现无标注信息向基础模型的有效注入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“极低标注比例下能否让 SAM 充分利用无标注医学图像”展开，覆盖前列腺 MRI 与结直肠息肉两个代表性任务，共 5 组实验：</p>
<ol>
<li><strong>主实验：半监督分割精度对比</strong></li>
</ol>
<ul>
<li><strong>数据集</strong></li>
<li>PROMISE12：50 例前列腺 T2 MRI，按 35/5/10 划分 train/val/test，每例按 2D 切片处理。</li>
<li>COLON：CVC-ClinicDB 612 张 + Kvasir 838 张训练，跨域测试五套公开库（CVC-300、CVC-ColonDB、ETIS-LaribPolypDB 等）。</li>
<li><strong>标注比例</strong><br>仅提供 5 % 或 10 % 训练切片作为标注，其余完全无标签。</li>
<li><strong>对照方法</strong></li>
<li>PEFT-SAM 系列：SAM、MedSAM、SAM-Med2D</li>
<li>Dual-SAM 系列：CPC-SAM</li>
<li>SP-SAM 系列：KnowSAM</li>
<li><strong>指标</strong><br>Dice、IoU、HD95、ASD</li>
<li><strong>结果</strong><br>在 5 % 标注下，SC-SAM 在 PROMISE12 上 Dice 83.64，比第二名 KnowSAM 高 5.15；COLON 五套测试集平均 Dice 提升 2–4 个百分点，均达到新 SOTA。</li>
</ul>
<ol>
<li><p><strong>跨域泛化评估</strong><br>COLON 实验即跨域场景：训练集与五套测试集来自不同医院、不同设备。SC-SAM 在未见中心数据上仍保持最高平均 Dice，验证对域偏移的鲁棒性。</p>
</li>
<li><p><strong>可视化对比</strong><br>随机抽取 5 % 与 10 % 标注场景下的前列腺与息肉样本，绘制预测 mask 与 GT 叠加热图。SC-SAM 边界更紧致，背景误报明显减少。</p>
</li>
<li><p><strong>消融实验</strong></p>
</li>
</ol>
<ul>
<li><strong>组件消融</strong><br>去掉 sigmoid ramp-up（ω(t)≡1）→ Dice 从 83.64 降至 36.37，降幅 47 %。</li>
<li><strong>骨干替换</strong><br>将 U-Net 换成 UNet++、ResUNet++、Swin-UNet，在同等设置下 Dice 分别降至 80.14、78.20、70.27，验证 CNN 型专家在半监督场景下优于 ViT 型。</li>
</ul>
<ol>
<li><strong>资源与收敛分析</strong><br>单卡 RTX 3090 Ti 24 GB 即可完成训练；PROMISE12 5 % 标注场景下，SC-SAM 在约 6 k 迭代后收敛，验证集 Dice 即超过最佳对照 2 个百分点，表明样本效率显著提升。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>三维/视频扩展</strong><br>将双向协同机制从 2D 切片推广到 3D 体数据或结肠镜视频，设计时空一致的伪标签生成策略，解决切片间不连续问题。</p>
</li>
<li><p><strong>提示模态扩充</strong><br>除点提示外，引入可学习的稀疏框、文本或解剖先验嵌入，探索混合提示在更小标注比例（≤1 %）下的极限性能。</p>
</li>
<li><p><strong>理论分析</strong><br>对 sigmoid ramp-up 进行误差上界推导，量化早期伪标签噪声对 SAM 梯度方向的影响，给出最优 ramp-up 长度  T_(max)  的解析估计。</p>
</li>
<li><p><strong>更多专家融合</strong><br>采用多专家集成（CNN + Transformer + 物理模型）生成更鲁棒的伪标签，研究不同专家间置信度加权或投票机制，进一步提升伪标签质量。</p>
</li>
<li><p><strong>持续/增量学习</strong><br>当新器官或成像模态陆续加入时，固定 SAM 编码器、仅扩展 Adapter 与专家网络头部，实现无需重训历史数据的持续医学分割。</p>
</li>
<li><p><strong>不确定性驱动采样</strong><br>利用 SAM 或 U-Net 的预测不确定性，主动挑选最有价值的无标注样本进行人工标注，实现从 5 % 到 1 % 标注比例的主动-半监督联合优化。</p>
</li>
<li><p><strong>跨模态协同</strong><br>将同一患者的 MRI、CT、超声等多模态图像共同输入，设计模态无关的提示空间，验证 SC-SAM 在模态缺失情况下的鲁棒性。</p>
</li>
<li><p><strong>轻量化部署</strong><br>对 U-Net 与 Adapter 进行联合量化/剪枝，保持伪标签质量的同时减少显存占用，使框架可在边缘内镜设备上实时运行。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SC-SAM 核心内容速览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>SAM 在医学影像因域偏移+ scarce labels 表现骤降；PEFT 无法直接利用无标注数据，而传统 U-Net 半监督方案又与 SAM 脱节。</td>
</tr>
<tr>
<td>思路</td>
<td>让“专家”U-Net 与“通才”SAM 互为师生，形成双向协同：U-Net 用全量数据生成点提示+伪标签 → 引导 SAM 的 PEFT；SAM 输出高质量 mask → 反向正则化 U-Net。</td>
</tr>
<tr>
<td>技术</td>
<td>1. 点提示采样：5 前景点+5 背景点。 2. 无监督损失： Lunsup=Lseg(P^(UNet),P^(SAM)) + ω(t)Lseg(P^(SAM),P^(UNet)) ，其中  ω(t)  为 sigmoid ramp-up。 3. 联合目标： Ltotal=Lsup+Lunsup 。</td>
</tr>
<tr>
<td>实验</td>
<td>前列腺 MRI（PROMISE12）与结直肠息肉（COLON）在 5 %/10 % 标注比例下均达 SOTA；跨域、消融、可视化与资源分析验证有效性。</td>
</tr>
<tr>
<td>结论</td>
<td>传统 CNN 专家与现代基础模型协同，可在极低标注场景下充分挖掘无标签医学图像，实现标签高效、精度领先的医学分割。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images，链接是https://arxiv.org/pdf/2601.17934，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17934。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</p>
<p><strong>Categories</strong>: cs.CV, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17934v2.pdf">https://arxiv.org/pdf/2601.17934v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17934v2">https://papers.cool/arxiv/2601.17934v2</a></p>
<p><strong>Published</strong>: 2026-01-25T18:13:48Z</p>
<hr>
<h3 id="9-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation"><a href="#9-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation" class="headerlink" title="9. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation"></a>9. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20854v1">Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</a></h3><p>Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>表格数据仍然是生成模型的一个具有挑战性的领域。特别是，标准的变分自编码器（VAE）架构，通常由多层感知器组成，在建模特征之间的关系方面表现不佳，尤其是在处理混合数据类型时。相比之下，Transformer 通过其注意力机制，更适合捕捉复杂的特征交互。在本文中，我们通过实证方法研究了将 Transformer 集成到 VAE 不同组件中的影响。我们在 OpenML CC18 套件中的 57 个数据集上进行了实验，并得出两个主要结论。首先，结果表明，将 Transformer 放置以利用潜在空间和解码器表示会在保真度和多样性之间产生权衡。其次，我们观察到所有组件中的连续 Transformer 块之间具有高度相似性。特别是在解码器中，Transformer 的输入与输出之间的关系近似为线性关系。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“表格数据生成”中标准变分自编码器（VAE）难以建模异构特征间复杂依赖关系的问题，提出并系统评估一种改进思路：<br><strong>在 VAE 的不同表征阶段（输入、隐空间、输出）引入 Transformer 的自注意力机制，以显式捕获特征间的高阶交互，同时量化这一改动对生成样本保真度（fidelity）与多样性（diversity）的权衡影响。</strong></p>
<p>具体而言，论文试图回答以下核心问题：</p>
<ol>
<li>在表格数据混合类型特征场景下，将 Transformer 置于 VAE 的编码器、隐空间或解码器，会对生成质量产生何种差异？</li>
<li>不同放置策略是否会带来“保真度-多样性”权衡？</li>
<li>Transformer 内部表征在训练后是否真正改变了特征方向，抑或仅起到缩放作用？</li>
</ol>
<p>通过 57 个 OpenML 数据集的实验，论文得出：</p>
<ul>
<li><strong>仅在隐空间与解码器端引入 Transformer 可显著提升多样性，但会降低保真度</strong>；</li>
<li><strong>编码器端 Transformer 对保真度影响最小</strong>；</li>
<li><strong>解码器端 Transformer 在训练后近似恒等映射，其残差分支受层归一化缩放效应主导</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 章“Related Work”中系统回顾了与“表格数据生成”相关的四大类深度生成模型，并单独梳理了“Transformer 在表格数据生成”中的已有尝试。相关研究可归纳如下：</p>
<p>1. 面向表格数据的深度生成模型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型类别</th>
<th>代表方法</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>GAN 系列</td>
<td>CTGAN、CTAB-GAN、CTAB-GAN+</td>
<td>将条件 GAN 扩展到混合类型特征，引入模式特定归一化、分类编码与辅助损失。</td>
</tr>
<tr>
<td>扩散模型</td>
<td>TabDDPM、CoDi、StaSy、TabSyn、TabDiff</td>
<td>针对离散-连续混合特征设计前向/反向过程，或在潜空间执行扩散。</td>
</tr>
<tr>
<td>自回归模型</td>
<td>GReaT、TabMT</td>
<td>把表格行序列化后用 GPT/Transformer 逐列生成；引入掩码语言建模。</td>
</tr>
<tr>
<td>VAE 系列</td>
<td>TVAE、VAEM、GOGGLE</td>
<td>TVAE 直接对 One-Hot 后特征做 VAE；VAEM 两阶段先逐特征 VAE 再建模隐变量关系；GOGGLE 用图神经网络近似变量关系并联合 VAE 训练。</td>
</tr>
<tr>
<td>流匹配</td>
<td>梯度提升流 (Gradient-Boosted Flow)</td>
<td>用梯度提升树拟合流匹配中的速度场，适配异构特征。</td>
</tr>
</tbody>
</table>
</div>
<p>2. Transformer 在表格数据生成中的已有探索</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>Transformer 角色</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabDiff</td>
<td>同时出现在主干去噪网络与输出头</td>
<td>仅作为模型组件，未系统研究不同放置对保真-多样性权衡的影响。</td>
</tr>
<tr>
<td>TabSyn</td>
<td>用在 VAE 的“识别网络”(encoder) 与“生成网络”(decoder) 内部</td>
<td>与本文 ELD-VAE 结构类似，但未进行表征相似性分析，也未揭示解码器 Transformer 近似恒等的现象。</td>
</tr>
<tr>
<td>GReaT、TabMT</td>
<td>把整张表看成序列，用 Transformer 自回归生成</td>
<td>属于“表→文本”范式，与 VAE 的“潜变量+重构”范式不同。</td>
</tr>
<tr>
<td>SAINT、TabTransformer</td>
<td>主要服务于监督学习，而非生成</td>
<td>聚焦判别任务中的特征交互，未评估合成数据质量。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 小结</p>
<p>已有文献验证了 Transformer 对捕获表格特征交互的有效性，但<strong>缺乏在 VAE 框架内、对“编码器-隐空间-解码器”三个不同层级系统性地放置 Transformer 的对比研究</strong>，也<strong>未量化其带来的保真度-多样性权衡与内部表征相似性</strong>。本文正是填补这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“<strong>系统性位置消融 + 多维度量化评估</strong>”的策略，把 Transformer 逐层“插入”到 VAE 的三种表征路径（输入、隐空间、输出），在 57 个 OpenML 表格数据集上完成对照实验，从而回答“<strong>放哪里、有何利弊、为何如此</strong>”三个问题。具体步骤如下：</p>
<p>1. 构建 6 种 VAE 变体（位置消融）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>Transformer 位置</th>
<th>研究目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAE</td>
<td>无</td>
<td>纯 MLP 基线</td>
</tr>
<tr>
<td>E-VAE</td>
<td>仅 encoder</td>
<td>检验“原始特征”层面交互是否有助益</td>
</tr>
<tr>
<td>EL-VAE</td>
<td>encoder + latent</td>
<td>观察隐变量层加入自注意力的影响</td>
</tr>
<tr>
<td>ELD-VAE</td>
<td>encoder + latent + decoder</td>
<td>全路径覆盖，验证累积效应</td>
</tr>
<tr>
<td>LD-VAE</td>
<td>latent + decoder（无 encoder）</td>
<td>剥离 encoder 干扰，聚焦深层表征</td>
</tr>
<tr>
<td>D-VAE</td>
<td>仅 decoder</td>
<td>直接测试“重构空间”交互的作用</td>
</tr>
</tbody>
</table>
</div>
<p>所有变体共享相同的 tokenizer、detokenizer、隐藏层宽度、 latent 维度与训练超参，保证差异仅由 Transformer 位置引起。</p>
<p>2. 统一训练与采样协议</p>
<ul>
<li><strong>数据预处理</strong>：数值特征用 QuantileTransformer→高斯，类别特征 One-Hot；缺失值均值/众数填补；删除零方差特征。</li>
<li><strong>训练</strong>：500 epoch，Adam  10^(-3) ，权重衰减 0.9，batch size 按验证集自动折半调优。</li>
<li><strong>Transformer 超参</strong>：1 头、4 块、128 隐藏维、pre-norm、embedding 维 4。</li>
<li><strong>采样</strong>：训练完成后从先验  zsimN(0,I)  生成，与训练集同规模。</li>
</ul>
<p>3. 三维评估体系（量化利弊）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评估层级</th>
<th>指标</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low-Density</td>
<td>1-way marginals + Pairwise-correlations</td>
<td>快速检验单变量/双变量分布是否失真</td>
</tr>
<tr>
<td>High-Density</td>
<td>α-Precision（保真） vs β-Recall（多样）</td>
<td>精确量化“保真-多样”权衡</td>
</tr>
<tr>
<td>ML-Efficiency</td>
<td>TSTR Utility + ML-Fidelity</td>
<td>测合成数据在下游 XGBoost 任务中的真实价值</td>
</tr>
</tbody>
</table>
</div>
<p>所有指标归一化到<br>0,1<br>，用 Wilcoxon 符号秩检验判断显著性。</p>
<p>4. 表征相似性诊断（解释原因）</p>
<ul>
<li><strong>CKA 线性相似度</strong>：计算 Transformer 块内“输入-输出”余弦相似度，揭示是否近似恒等映射。</li>
<li><strong>残差缩放因子 σ</strong>：逐样本求解  E= σ E ，若  σ≈ 1  则表明自注意力+LayerNorm 仅做尺度变换。</li>
<li><strong>逐块 F-范数变化</strong>：量化 LayerNorm 对幅值的压缩程度，解释为何解码器 Transformer 不改变方向。</li>
</ul>
<p>5. 结论与落地建议</p>
<ul>
<li><strong>权衡曲线</strong>：latent + decoder 引入 Transformer → β-Recall 平均提升 4–7%，但 α-Precision 下降 3–6%；encoder 处 Transformer 对保真度几乎无负面影响。</li>
<li><strong>恒等现象</strong>：解码器 Transformer 在训练后  σ  中位数≈1，CKA&gt;0.95，证实其退化为“带缩放的恒等函数”，主要受 LayerNorm 的平移-缩放效应主导。</li>
<li><strong>实用指南</strong>：</li>
<li>若任务更重视<strong>多样性</strong>（数据增广、隐私扩充），优先选用 LD-VAE 或 ELD-VAE；</li>
<li>若任务更重视<strong>保真</strong>（缺失值插补、发布替代数据），用 E-VAE 甚至纯 MLP-VAE 即可，省去 Transformer 的计算与调参成本。</li>
</ul>
<p>通过上述“<strong>位置消融 → 多维指标 → 表征诊断</strong>”闭环，论文不仅回答了“放哪里”，也解释了“为何这样”，为后续表格生成模型中 Transformer 的取舍与再设计提供了量化依据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“Transformer 在 VAE 不同位置对表格数据生成的影响”共设计并执行了 <strong>三大类、七组、累计 &gt; 2 000 次训练</strong> 的实验，覆盖 57 个 OpenML-CC18 数据集。具体实验矩阵如下：</p>
<p>1. 主实验：6 种架构 × 57 数据集 全面对照</p>
<p><strong>目的</strong>：量化 Transformer 位置对生成质量与下游效用的一致影响。<br><strong>指标</strong>：</p>
<ul>
<li>Low-Density：1-way marginals、Pairwise-correlations</li>
<li>High-Density：α-Precision（保真）、β-Recall（多样）</li>
<li>ML-Efficiency：TSTR Utility、ML-Fidelity<br><strong>统计</strong>：Wilcoxon 符号秩检验（p=0.001）+ 平均分数/平均排名双维度判定显著性。</li>
</ul>
<p>2. fidelity–diversity 细粒度实验</p>
<p>2.1 Forward 序列（VAE→E-VAE→EL-VAE→ELD-VAE）</p>
<p>2.2 Backward 序列（VAE→D-VAE→LD-VAE→ELD-VAE）</p>
<p><strong>分组</strong>：按样本量将 57 数据集切为 small<br>500,1 k)、medium[1 k,5 k)、large[5 k,96 k<br> 三桶。<br><strong>输出</strong>：</p>
<ul>
<li>各序列每步过渡的 Δα、Δβ 随桶大小的变化曲线（论文图 2 &amp; 补充图 1–2）。</li>
<li>证实“latent/decoder 加 Transformer → 保真降、多样升”趋势与数据规模呈单调关系。</li>
</ul>
<p>3. 表征相似性实验</p>
<p>3.1 聚合相似度</p>
<ul>
<li>对 6 种模型，计算每个 Transformer 组件“输入-输出”CKA 相似度，再按桶平均（论文图 3）。</li>
</ul>
<p>3.2 块内相似度</p>
<ul>
<li>在 ELD-VAE 上，抽取 encoder/latent/decoder 各 4 个 block 内部 6 个检查点（e.in→e.n1→e.attn→e.res→e.n2→e.ffn）两两 CKA，绘制热图（论文图 4–5）。</li>
</ul>
<p>3.3 残差缩放因子</p>
<ul>
<li>逐样本求解 σ 使  E=σE ，给出 σ 分布与块编号关系（论文图 6），验证 decoder 近似恒等。</li>
</ul>
<p>3.4 块数敏感性</p>
<ul>
<li>在 churn、adult、credit-approval 三数据集上，把 ELD-VAE 的 Transformer 块数从 4→3→2→1，重复实验：<br>– High-Density 变化（补充图 3）<br>– 表征相似度热图（补充图 4）<br>结论：块数减少不改变“decoder 高相似”现象，仅 latent 层相似度随块数增加而下降。</li>
</ul>
<p>4. 训练稳定性诊断实验</p>
<ul>
<li>对出现 NaN/爆炸损失的 3 个数据集（kc2、jm1、spambase），将隐藏维与 latent 维减半后重训；记录重构/KL 曲线（补充图 5–6），并对比 α/β 前后变化（补充表 2–3），确认超参过大是 instability 主因。</li>
</ul>
<p>5. 重构数据 fidelity–diversity 实验</p>
<ul>
<li>用训练好的 6 模型对真实测试集做前向重构，再以训练集为参照计算 α-Precision、β-Recall（补充表 4）。</li>
<li>结果与“采样生成”趋势一致，进一步排除评估偏差。</li>
</ul>
<p>6. 低维与 ML 效用桶分析（补充第 2 章）</p>
<ul>
<li>对 Marginals、Pairwise、Utility、ML-Fidelity 四项指标，均按 Forward/Backward 序列、三桶样本量给出 Δ 曲线（补充图 1–2），验证“多样性提升≠下游效用提升”现象在所有指标上稳定存在。</li>
</ul>
<p>7. 单数据集结果全表（补充第 7 章）</p>
<ul>
<li>给出 57 数据集上 6 模型 6 项指标的原始分数与排名（补充表 6–8），支持主文“LD-VAE 综合平均第一”结论的可复现性。</li>
</ul>
<p>实验规模小结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集</td>
<td>57</td>
</tr>
<tr>
<td>模型变体</td>
<td>6</td>
</tr>
<tr>
<td>每数据集训练次数</td>
<td>1（主实验）+ 块数敏感 3×4 + 稳定性重训 3</td>
</tr>
<tr>
<td>总训练运行</td>
<td>≈ 6×57 + 3×4×3 + 3 = 363 + 36 + 3 = 402 次完整训练</td>
</tr>
<tr>
<td>生成与评估</td>
<td>每次训练后生成 1×训练集规模样本，并计算 6 项指标</td>
</tr>
<tr>
<td>CKA/σ 计算</td>
<td>在测试集上逐 batch 计算，累计 &gt; 10 7 次向量内积</td>
</tr>
</tbody>
</table>
</div>
<p>通过上述实验矩阵，论文从“质量-效用-机理”三个层面完整回答了 Transformer 放置问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对该工作的直接延伸或深层追问，均具备理论价值与落地潜力：</p>
<p>1. 架构层面：把“退化”变为“有用”</p>
<ul>
<li><p><strong>去 LayerNorm 的 Transformer 块</strong><br>论文揭示解码器 Transformer 近似恒等的核心原因是 LayerNorm 的平移-缩放效应。可尝试</p>
</li>
<li><p>完全移除 LayerNorm，改用 RMSNorm、PowerNorm 或简单可学习标量缩放；</p>
</li>
<li>仅用 Pre-Activation MLP 替代 Transformer 块，对比是否仍能保持多样性增益。<br>目标：在不影响保真度的前提下，让解码器注意力真正改变表征方向。</li>
<li><strong>轻量级注意力替代</strong><br>探索 Linformer、Performer、Cosformer 等线性复杂度注意力，验证“多样性↑保真度↓”规律是否依旧成立，为大规模高维表格（≥10 000 特征）提供可行方案。</li>
</ul>
<p>2. 目标函数层面：重调保真-多样天平</p>
<ul>
<li><strong>动态加权 α-β 损失</strong><br>当前 VAE 仅优化 ELBO。可把 α-Precision、β-Recall 直接纳入损失，构建</li>
</ul>
<p>L<em>(new) = L</em>(ELBO) - λ_1 α + λ_2 β</p>
<p>通过 Pareto 搜索寻找 (λ₁, λ₂) 最优前沿，实现“保真-多样”可控生成。</p>
<ul>
<li><strong>对比式/信息瓶颈正则</strong><br>引入对比损失或信息瓶颈，限制隐变量互信息 I(x;z) 上限，抑制 VAE 本身“过度随机”导致的保真度下降，看能否在 latent-Transformer 场景下仍保持高 β-Recall。</li>
</ul>
<p>3. 数据层面：更复杂的表格形态</p>
<ul>
<li><p><strong>缺失非随机（MNAR）与噪声标签</strong><br>论文仅做简单均值/众数填补。下一步让生成器直接输出“缺失掩码”与“值”两组向量，用 MNAR 模拟机制评估 Transformer 位置对插补偏差的影响。</p>
</li>
<li><p><strong>时序-表格混合</strong><br>将静态表格字段与时间序列拼接，考察 Transformer 在“静态-动态”异构注意力下的表现，拓展到医疗纵向数据、用户行为日志等场景。</p>
</li>
</ul>
<p>4. 评估层面：超越 α-β</p>
<ul>
<li><p><strong>细粒度隐私泄露度量</strong><br>采用 Membership Inference、Attribute Inference 攻击评估不同位置 Transformer 的隐私-效用权衡，验证“多样性↑”是否伴随“泄露↑”。</p>
</li>
<li><p><strong>因果保真度</strong><br>引入因果发现指标（如结构汉明距离 SHD）衡量合成数据是否保持原始变量间的因果边方向，验证注意力机制是否会混淆因果与相关。</p>
</li>
</ul>
<p>5. 训练策略层面：让 Transformer 不再“闲置”</p>
<ul>
<li><p><strong>两阶段训练</strong><br>① 先无 Transformer 训练 VAE 至收敛；② 冻结解码器 MLP，仅微调解码器 Transformer。观察能否打破“恒等”初始化陷阱，使注意力真正学习残差方向。</p>
</li>
<li><p><strong>梯度截断 / 权重标准化</strong><br>对注意力输出做梯度截断或使用 Weight Standardization，缓解 LayerNorm 输入分布漂移，从而提升有效学习率，测试是否减少恒等现象。</p>
</li>
</ul>
<p>6. 跨模型迁移：是否只是 VAE 的“专利”？</p>
<ul>
<li><p><strong>Transformer-in-GAN</strong><br>将同样的“encoder- latent-decoder”放置策略迁移到 CTGAN、TabDDPM 框架，检验保真-多样权衡是否依旧成立，或是否因目标函数不同而逆转。</p>
</li>
<li><p><strong>扩散模型中的注意力位置</strong><br>在 TabDiff 的去噪网络中，分别把 Transformer 放在“时间步嵌入后”“特征嵌入后”“输出头”三段，重复 α-β 实验，看多样性增益是否普遍存在于所有生成范式。</p>
</li>
</ul>
<p>7. 可解释性深挖：注意力到底在“看”什么？</p>
<ul>
<li><p><strong>注意力可视化</strong><br>对 decoder Transformer 的注意力矩阵进行平均，检查是否始终呈现对角或均匀分布，进一步佐证“无结构交互”导致恒等映射的假设。</p>
</li>
<li><p><strong>互信息分解</strong><br>用 MISE 或 CXPlain 对注意力输出与原始特征做互信息分解，定位哪些特征对被过度强化，从而设计稀疏注意力掩码，减少虚假相关。</p>
</li>
</ul>
<p>8. 工程与产品化</p>
<ul>
<li><p><strong>自动架构搜索（NAS）</strong><br>以 α-β 加权分数为奖励，用强化 NAS 搜索“在哪几层、用几块、头数多少”最优，输出 Pareto 前沿供用户按业务场景（金融风控 vs 营销扩量）一键选择。</p>
</li>
<li><p><strong>端侧轻量化</strong><br>结合第 1 点的线性注意力与第 5 点的两阶段训练，构建 &lt;1 MB 的“微型表格生成器”，在移动端实现现场数据增强与隐私释放。</p>
</li>
</ul>
<p>总结</p>
<p>论文首次量化了“Transformer 位置-保真-多样”三角关系，并揭示了解码器恒等退化现象。下一步工作可沿“<strong>架构-目标-数据-评估-训练-跨模型-可解释-工程化</strong>”八个维度展开，把“表格 Transformer 生成”从“能用”推向“好用、可控、可信”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文主旨一句话</strong><br>系统探究“把 Transformer 塞进 VAE 的哪个部位”才能在混合类型表格数据生成中取得最佳“保真-多样”权衡，并揭示解码器 Transformer 训练后近似恒等映射的机理。</p>
<p>1. 背景与动机</p>
<ul>
<li>标准 VAE（全 MLP）难以捕获异构特征间高阶关系。</li>
<li>Transformer 的自注意力在文本、图像领域已验证其交互建模能力，但在表格生成中“放哪里、值不值”缺乏大尺度实证。</li>
</ul>
<p>2. 方法</p>
<ul>
<li><strong>6 个 VAE 变体</strong>：VAE/E-VAE/EL-VAE/ELD-VAE/LD-VAE/D-VAE，仅改变 Transformer 所处位置（encoder / latent / decoder）。</li>
<li><strong>统一设置</strong>：57 个 OpenML-CC18 数据集 → 相同 tokenizer、训练超参、评估流程。</li>
<li><strong>三维评估</strong>：<br>– Low-Density：1-way marginals + pairwise correlations<br>– High-Density：α-Precision（保真） vs β-Recall（多样）<br>– ML-Efficiency：TSTR Utility + ML-Fidelity</li>
<li><strong>表征诊断</strong>：CKA 相似度、残差缩放因子 σ、逐块 F-范数，解释“退化”原因。</li>
</ul>
<p>3. 主要发现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>位置</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>encoder</td>
<td>几乎不损保真，对多样提升微小。</td>
</tr>
<tr>
<td>latent + decoder</td>
<td>多样↑ 4–7%，保真↓ 3–6%；权衡显著且随数据规模单调。</td>
</tr>
<tr>
<td>decoder 单独</td>
<td>最差保真，多样性增益有限。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>机理级结论</strong></p>
<ul>
<li>解码器 Transformer 训练后输入-输出 CKA&gt;0.95，σ≈1，本质为“LayerNorm 缩放后的恒等函数”。</li>
<li>减少块数或移除 LayerNorm 仍无法打破恒等现象，提示需重新设计目标函数或训练策略。</li>
</ul>
<p>4. 实用建议</p>
<ul>
<li><strong>重保真</strong> → 用 E-VAE 或纯 MLP-VAE，省计算。</li>
<li><strong>重多样</strong> → 用 LD-VAE，接受轻微保真损失。</li>
<li><strong>下游 ML 效用</strong> → Transformer 位置无显著优势，多样性↑≠效用↑，需按任务取舍。</li>
</ul>
<p>5. 贡献清单</p>
<p>① 大尺度（57 数据集）位置消融实验，首次量化“保真-多样”权衡。<br>② 揭示解码器 Transformer 恒等退化现象并给出 LayerNorm 根源解释。<br>③ 提供面向表格生成的 Transformer 使用指南与开源训练-评估协议。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Transformer%20Placement%20in%20Variational%20Autoencoders%20for%20Tabular%20Data%20Generation，链接是https://arxiv.org/pdf/2601.20854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20854。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</p>
<p><strong>Categories</strong>: cs.LG, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20854v1.pdf">https://arxiv.org/pdf/2601.20854v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20854v1">https://papers.cool/arxiv/2601.20854v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:54:27Z</p>
<hr>
<h3 id="10-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><a href="#10-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization" class="headerlink" title="10. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization"></a>10. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07972v2">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</a></h3><p>While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然大型语言模型（LLMs）在推理和基于代理的问题解决方面取得了显著进展，但目前的评估方法未能充分评估它们的能力：现有基准要么依赖容易饱和和记忆化的封闭式问题，要么依赖缺乏一致性和严谨性的主观比较。在本工作中，我们引入了 HeuriGym，一种面向评估由 LLM 生成的组合优化问题启发式算法的代理框架，其特点是目标明确且解空间广阔。HeuriGym 赋能 LLM 提出启发式方案，通过代码执行获得评估反馈，并迭代优化其解决方案。我们在计算机系统、物流和生物学等领域的九个问题上对九种最先进模型进行了评估，揭示了它们在工具使用、规划和自适应推理方面的持续局限。为量化性能，我们提出了质量-收益指数（Quality-Yield Index, QYI），这一指标同时反映了解决方案通过率和质量。即使是最顶尖的模型，如 GPT-4-mini-high 和 Gemini-2.5-Pro，其 QYI 得分也仅为 0.6，远低于专家基准的 1。我们的开源基准旨在引导 LLM 在科学和工程领域实现更高效、更现实的问题解决能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决如何有效评估大型语言模型（LLMs）在组合优化问题上的能力。现有的评估方法存在局限性，主要分为两类：基于封闭问题的客观基准和基于人类偏好的主观评估。前者容易受到数据污染和性能饱和的影响，后者则缺乏一致性和严谨性。为了克服这些局限性，作者提出了一个名为HeuriGym的新框架，旨在通过迭代生成、执行和反馈机制来评估LLMs生成的启发式算法在组合优化问题上的表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了与LLMs在组合优化（CO）领域以及LLMs评估相关的研究。以下是相关研究的总结：</p>
<p>LLMs for Combinatorial Optimization</p>
<ul>
<li><strong>NL4Opt Competition</strong>：将自然语言转化为结构化的优化问题，后续工作致力于改进特定领域的模型训练和提示策略，但这些方法依赖于精确求解器，难以扩展。</li>
<li><strong>FunSearch 和 AlphaEvolve</strong>：FunSearch 使用LLMs生成启发式算法，而AlphaEvolve结合进化搜索生成新的启发式算法。但这些方法需要评估数千个候选方案，效率较低。</li>
<li><strong>Metaheuristic Templates</strong>：最近的一些方法通过元启发式模板改进效率，但仍然限制LLMs只填充评分函数，而不是设计完整的算法。</li>
</ul>
<p>Evaluation on LLMs</p>
<ul>
<li><strong>HumanEval、BigCodeBench、LiveCodeBench</strong>：这些基准测试侧重于闭合问题，如数学问题、编程任务等，存在固定答案，容易受到数据污染。</li>
<li><strong>Chatbot Arena、KernelBench</strong>：这些基准测试鼓励多样化的输出，但往往缺乏明确的目标，导致评估不一致。</li>
<li><strong>NPHardEval、GraphArena</strong>：这些基准测试评估小规模NP难问题的精确解，但实际应用中更倾向于使用启发式方法以实现可扩展性。</li>
</ul>
<p>HeuriGym框架通过提供一个开放式的解决方案空间，允许LLMs生成完整的、自包含的优化程序，包括自定义数据结构和端到端的流程，从而更贴近现实世界的CO挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过提出一个名为 <strong>HeuriGym</strong> 的新框架来解决评估 LLMs 在组合优化问题上的能力的问题。HeuriGym 是一个以代理（agentic）为核心的框架，它通过以下方式实现对 LLMs 的全面评估：</p>
<p>1. <strong>框架概述</strong></p>
<p>HeuriGym 框架的核心思想是让 LLMs 生成启发式算法，然后通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。这个过程模拟了实际工程中的工作流程，能够深入评估 LLMs 的多步推理、工具使用和指令遵循能力。</p>
<p>2. <strong>问题描述与提示设计</strong></p>
<ul>
<li><strong>问题描述</strong>：每个基准任务都包含一个结构化的问题描述，包括背景、正式化和输入/输出格式。背景部分介绍优化的上下文和关键术语，正式化部分定义优化目标和约束条件，输入/输出格式部分则明确输入和输出文件的结构。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中，以便 LLMs 学习并改进其输出。</li>
</ul>
<p>3. <strong>反馈循环</strong></p>
<p>HeuriGym 将数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。每次迭代后，生成的解决方案、执行日志、验证结果和评估分数都会被记录并反馈给 LLM，使其能够从过去的尝试中学习并逐步改进输出。</p>
<p>4. <strong>评估指标</strong></p>
<ul>
<li><strong>solves@i</strong>：跟踪 LLM 在 i 次迭代内解决约束问题的能力。这个指标分为三个阶段：执行、解决方案生成和验证。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：这是一个综合指标，结合了解决方案的通过率（Yield）和质量（Quality），通过计算这两个指标的调和平均值来评估 LLM 的整体表现。</li>
</ul>
<p>5. <strong>基准构建</strong></p>
<p>HeuriGym 包含了来自计算机系统、编译器、计算生物学、物流等领域的九个组合优化问题。这些问题被精心挑选，以确保它们具有明确的目标、庞大的解空间、可扩展的数据实例和可复现的专家基线。每个问题都包括一个任务特定的验证器和评估器，用于评估解决方案的通过率和质量。</p>
<p>6. <strong>实验与评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，包括 GPT-o4-mini-high、Gemini-2.5-Pro、LLaMA-4-Maverick 等。实验结果表明，即使是顶级模型，如 GPT-o4-mini-high 和 Gemini-2.5-Pro，在 QYI 评分上也仅为 0.6 左右，远低于专家基线的 1。这揭示了当前 LLMs 在现实世界问题解决中的局限性。</p>
<p>7. <strong>案例研究</strong></p>
<p>论文还通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。虽然 LLMs 能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</p>
<p>8. <strong>未来工作</strong></p>
<p>作者指出，HeuriGym 仍存在一些局限性，如 Python 执行的开销、迭代自优化过程的进一步改进、以及评估指标与现实世界部署之间的差距。未来的工作将包括扩展基准的广度和深度，引入新的问题，并探索更高效的测试时扩展策略。</p>
<p>通过这些方法，HeuriGym 为 LLMs 在组合优化问题上的评估提供了一个全面、严谨且实用的框架，有助于推动 LLMs 在科学和工程领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验：</p>
<p>1. <strong>模型评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，这些模型包括：</p>
<ul>
<li>OpenAI 的 GPT-o4-mini-high</li>
<li>Anthropic 的 Claude-3.7-Sonnet</li>
<li>DeepSeek 的 DeepSeek-V3 和 DeepSeek-R1</li>
<li>Google 的 Gemini-2.5-Flash 和 Gemini-2.5-Pro</li>
<li>Meta 的 LLaMA-3.3 和 LLaMA-4-Maverick</li>
<li>Alibaba 的 Qwen3-235B</li>
</ul>
<p>这些模型在多个领域（如计算机系统、物流、生物学等）的九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</p>
<p>2. <strong>性能评估</strong></p>
<ul>
<li><strong>solves@i 指标</strong>：评估 LLM 在 i 次迭代内解决约束问题的能力。具体来说，solves@i 分为三个阶段：</li>
<li><strong>执行（Execution）</strong>：生成的程序是否能正确编译或解释，并成功执行基本的输入输出操作。</li>
<li><strong>解决方案生成（Solution Generation）</strong>：程序是否在预定义的超时时间内产生非空输出，并符合预期的输出格式。</li>
<li><strong>验证（Verification）</strong>：解决方案是否满足所有问题特定的约束条件。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：综合评估解决方案的通过率（Yield）和质量（Quality）。QYI 是这两个指标的调和平均值，能够全面反映 LLM 的性能。</li>
</ul>
<p>3. <strong>案例研究</strong></p>
<p>作者通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。具体步骤如下：</p>
<ul>
<li><strong>初始迭代</strong>：LLMs 生成基于动态规划（DP）的启发式算法，但未对搜索空间进行剪枝，导致计算时间过长。</li>
<li><strong>后续迭代</strong>：LLMs 逐渐引入剪枝策略，优化算法性能，最终在第五次迭代中达到了最佳的解决方案。</li>
</ul>
<p>4. <strong>消融研究</strong></p>
<ul>
<li><strong>温度（Temperature）</strong>：评估不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率。</li>
<li><strong>少样本演示（Few-shot Demonstrations）</strong>：研究在不同数量的演示示例下模型的性能。结果表明，提供更多的演示示例可以显著提高模型的性能。</li>
<li><strong>反馈轮次（Feedback Rounds）</strong>：研究不同数量的反馈轮次对模型性能的影响。结果表明，适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>5. <strong>错误分析</strong></p>
<p>作者对 LLMs 在生成启发式算法时常见的错误类型进行了分类和分析，包括：</p>
<ul>
<li><strong>API 调用错误</strong>：使用不存在或过时的库调用。</li>
<li><strong>算法逻辑错误</strong>：即使总体方法合理，但实现存在缺陷。</li>
<li><strong>约束误解</strong>：忽略或误解问题的约束条件。</li>
<li><strong>超时错误</strong>：未在给定的约束时间内产生输出。</li>
</ul>
<p>6. <strong>C++ 实验</strong></p>
<p>作者还进行了初步的 C++ 实验，将 LLMs 生成的 Python 代码转换为 C++ 代码，以评估 C++ 在执行速度上的优势。结果显示，即使在第一次迭代中，C++ 解决方案的通过率也高于经过 10 次迭代的 Python 解决方案。</p>
<p>这些实验为评估 LLMs 在组合优化问题上的能力提供了全面的视角，并揭示了当前 LLMs 的优势和局限性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文中提到了一些可以进一步探索的点，以下是一些关键方向：</p>
<p>1. <strong>扩展基准的广度和深度</strong></p>
<ul>
<li><strong>增加问题数量和多样性</strong>：HeuriGym 目前包含九个问题，但随着 LLM 能力的提升，这些基准可能会逐渐饱和。因此，需要不断引入新的、更具挑战性的组合优化问题，特别是来自尚未充分探索的科学领域。</li>
<li><strong>扩大问题规模</strong>：现有问题的数据实例规模有限，未来可以将现有问题扩展到更复杂、更大规模的实例，以更好地评估 LLMs 在大规模问题上的表现。</li>
</ul>
<p>2. <strong>改进测试时扩展策略</strong></p>
<ul>
<li><strong>测试时计算优化</strong>：当前的迭代自优化过程可以被视为一种测试时扩展（Test-time Scaling, TTS）。可以探索如何结合 Best-of-N 采样、束搜索（Beam Search）和进化算法等技术，特别是在增加迭代预算的情况下，以进一步提升 LLMs 的性能。</li>
<li><strong>自验证能力</strong>：利用强大的验证器，HeuriGym 提供了一个自然的平台来研究 LLMs 的自验证能力，这可能是实现更自主的 LLMs 的一个有前景的方向。</li>
</ul>
<p>3. <strong>评估指标与现实世界部署的对齐</strong></p>
<ul>
<li><strong>代理指标与实际性能的差距</strong>：当前的评估指标虽然在初步基准测试中很有用，但在反映现实世界性能方面往往存在不足。特别是在科学领域，解决方案的质量通常需要通过物理实验来验证；在工程领域，如 EDA，质量需要通过耗时的后端综合来确认。如何弥合代理评估和现实世界部署之间的差距，同时管理更长的反馈循环带来的延迟，是一个关键挑战。</li>
</ul>
<p>4. <strong>多语言支持</strong></p>
<ul>
<li><strong>C++ 集成</strong>：目前的实验主要在 Python 上进行，但 C++ 在某些任务中表现出更好的性能。未来可以进一步探索如何将 C++ 更好地集成到框架中，包括处理依赖于特定领域的库和生成高效、功能正确且并行的 C++ 代码。</li>
</ul>
<p>5. <strong>社区贡献和合作</strong></p>
<ul>
<li><strong>开放基准</strong>：HeuriGym 被设计为一个开放的基准，鼓励社区贡献新的问题。未来的工作将包括扩展基准的广度和深度，通过引入新问题和扩展现有问题到更复杂的领域，以保持基准的长期相关性。</li>
<li><strong>跨学科合作</strong>：通过标准化组合问题的格式和可复现的代理工作流程，HeuriGym 可以作为一个共享的测试平台，促进跨学科合作。</li>
</ul>
<p>6. <strong>模型改进</strong></p>
<ul>
<li><strong>迭代改进策略</strong>：研究如何改进 LLMs 的迭代改进策略，使其能够更有效地从反馈中学习并优化解决方案。</li>
<li><strong>模型架构和训练方法</strong>：探索新的模型架构和训练方法，以提高 LLMs 在组合优化问题上的性能。</li>
</ul>
<p>7. <strong>应用领域扩展</strong></p>
<ul>
<li><strong>实际应用</strong>：将 HeuriGym 中的评估方法应用于实际的工业和科学问题，验证其在现实世界中的有效性。</li>
<li><strong>跨领域应用</strong>：探索 LLMs 在其他领域的应用，如医疗、金融等，这些领域同样需要复杂的推理和优化能力。</li>
</ul>
<p>这些方向为未来的研究提供了广阔的空间，有助于推动 LLMs 在组合优化问题上的应用和发展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了一个名为HeuriGym的框架，旨在评估大型语言模型（LLMs）在组合优化问题上生成启发式算法的能力。HeuriGym通过一个代理（agentic）框架，让LLMs提出解决方案，接收执行反馈，并迭代改进，从而全面评估LLMs的多步推理、工具使用和指令遵循能力。该框架通过定义明确目标和广阔解空间的组合优化问题，克服了现有评估方法的局限性，为LLMs的评估提供了一个更加严谨和实用的平台。</p>
<p>背景知识</p>
<ul>
<li><strong>LLMs的进展</strong>：LLMs在复杂推理和基于代理的问题解决方面取得了显著进展，但现有评估方法未能充分评估其能力。传统基准测试依赖于封闭问题，容易受到数据污染和性能饱和的影响；而基于人类偏好的主观评估则缺乏一致性和严谨性。</li>
<li><strong>组合优化问题</strong>：这类问题在多个领域（如计算机系统、科学推理、计算生物学、物流等）中具有基础性地位，适合用于评估LLMs，因为它们需要算法知识、启发式推理、权衡导航和创造性问题解决能力。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>HeuriGym框架</strong>：该框架包括问题描述、提示设计、反馈循环和评估指标。LLMs根据问题描述生成启发式算法，通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。</li>
<li><strong>问题描述</strong>：每个基准任务都包含背景、正式化和输入/输出格式的结构化描述。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中。</li>
<li><strong>反馈循环</strong>：数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。</li>
<li><strong>评估指标</strong>：提出了solves@i和Quality-Yield Index (QYI)两个指标。solves@i跟踪LLMs在i次迭代内解决约束问题的能力，而QYI则综合评估解决方案的通过率和质量。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>模型评估</strong>：对九个最先进的LLMs模型进行了评估，包括GPT-o4-mini-high、Claude-3.7-Sonnet、DeepSeek-V3、DeepSeek-R1、Gemini-2.5-Flash、Gemini-2.5-Pro、LLaMA-3.3、LLaMA-4-Maverick和Qwen3-235B。这些模型在九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</li>
<li><strong>性能评估</strong>：实验结果显示，即使是顶级模型，如GPT-o4-mini-high和Gemini-2.5-Pro，在QYI评分上也仅为0.6左右，远低于专家基线的1。这揭示了当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>案例研究</strong>：通过技术映射问题展示了LLMs的潜力和当前局限性。虽然LLMs能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</li>
<li><strong>消融研究</strong>：研究了不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响，以及少样本演示和反馈轮次对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率；适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>性能差距</strong>：即使是顶级LLMs模型，在组合优化问题上的表现也远低于专家基线，表明当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>迭代改进的重要性</strong>：通过迭代改进，LLMs能够逐步优化其解决方案，但与专家工具相比仍有显著差距。</li>
<li><strong>评估指标的有效性</strong>：solves@i和QYI指标能够有效评估LLMs在组合优化问题上的表现，为未来的LLMs评估提供了新的视角。</li>
<li><strong>未来工作方向</strong>：HeuriGym框架为未来的LLMs评估提供了一个有前景的平台，未来的工作可以包括扩展基准的广度和深度、改进测试时扩展策略、弥合代理评估和现实世界部署之间的差距等。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是HeuriGym%3A%20An%20Agentic%20Benchmark%20for%20LLM-Crafted%20Heuristics%20in%20Combinatorial%20Optimization，链接是https://arxiv.org/pdf/2506.07972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.07972。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07972v2.pdf">https://arxiv.org/pdf/2506.07972v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07972v2">https://papers.cool/arxiv/2506.07972v2</a></p>
<p><strong>Published</strong>: 2025-06-09T17:46:47Z</p>
<hr>
<h2 id="VLM-Domain-Papers"><a href="#VLM-Domain-Papers" class="headerlink" title="VLM Domain Papers"></a>VLM Domain Papers</h2><h3 id="1-Recursive-Language-Models-3"><a href="#1-Recursive-Language-Models-3" class="headerlink" title="1. Recursive Language Models"></a>1. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.24601v2">Recursive Language Models</a></h3><p>We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse long-context tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLM-Qwen3-8B, outperforms the underlying Qwen3-8B model by $28.3\%$ on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们研究通过推理时间扩展的视角，使大型语言模型（LLMs）能够处理任意长度的提示。我们提出了递归语言模型（RLMs），这是一种通用的推理范式，将长提示视为外部环境的一部分，并允许LLM以编程方式检查、分解并递归调用自身来处理提示片段。我们发现，RLMs能够成功处理比模型上下文窗口大两个数量级的输入，即使对于较短的提示，在四个不同的长上下文任务中，其性能也显著优于普通的前沿LLM和常见的长上下文结构，同时成本相当。在小规模实验中，我们对首个原生递归语言模型进行了后训练。我们的模型RLM-Qwen3-8B，相对于基础Qwen3-8B模型平均提高了28.3%，甚至在三个长上下文任务中接近普通GPT-5的质量。代码可在 <a target="_blank" rel="noopener" href="https://github.com/alexzhang13/rlm">https://github.com/alexzhang13/rlm</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现代大语言模型（LLM）在处理“任意长输入”时遭遇的两大瓶颈：</p>
<ol>
<li><p>物理上下文窗口限制<br>即使最新模型（如 GPT-5）的显式窗口已达 272K token，仍无法直接容纳 10M 乃至 100M 级别的长文档、代码库或多跳检索语料。</p>
</li>
<li><p>上下文腐烂（context rot）<br>在窗口内，随着输入变长，模型对早期信息的利用效率急剧下降，表现为任务复杂度越高、衰减越快（图 1）。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Recursive Language Models (RLMs)</strong>——一种<strong>推理时（inference-time）通用框架</strong>，把超长 prompt 视为外部环境变量，让 LLM 在 Python REPL 里用代码“窥视、分解、递归调用自身”处理片段，从而：</p>
<ul>
<li>将有效上下文长度扩展两个数量级（实验到 10M+ token）；</li>
<li>在多项长上下文基准上显著优于直接调用基座模型、摘要式代理、检索增强代理等基线，且成本持平或更低；</li>
<li>无需修改模型参数或重新训练，仅通过推理策略即可落地。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两大脉络，并指出 RLM 与它们的本质区别：</p>
<ol>
<li>长上下文系统（Long-Context LM Systems）</li>
</ol>
<ul>
<li><strong>架构-级扩展</strong>：通过重新训练或改进注意力机制来延长上下文，例如<br>– 线性偏置注意力 Press et al. 2022<br>– 结构化状态空间 Gu et al. 2022<br>– Infini-attention Munkhdalai et al. 2024</li>
<li><strong>外部脚手架（scaffold）</strong>：不改动模型，而在推理阶段压缩或管理上下文<br>– 有损管理：MemWalker（树状导航摘要）、ReSum（迭代压缩）、Claude-Code 式多轮摘要<br>– 显式记忆层级：MemGPT、Mem0、G-Memory 等把上下文拆成“主存/外存”<br><em>区别</em>：RLM 不预设任何摘要、检索或记忆结构，完全由模型在 REPL 里用代码自行决定如何切块、过滤、调用子 LLM，上下文管理被“隐式”地交给模型自己。</li>
</ul>
<ol>
<li>子 LLM 递归调用（Task Decomposition via sub-LM calls）</li>
</ol>
<ul>
<li>人设计流程：AutoGPT、ReAct、Claude sub-agents 等多按固定模板拆任务</li>
<li>模型自主拆：ViperGPT、THREAD、DisCIPL、ReDel、Context-Folding、AgentFold 等让 LLM 自己决定何时再调 LLM，但输入长度仍受基模型窗口限制<br><em>区别</em>：RLM 把“输入提示”本身卸载到外部环境，首次让递归调用可以<strong>符号级操作任意长字符串</strong>，从而突破窗口上限；先前方法只能递归拆“任务”，不能递归拆“输入”。</li>
</ul>
<p>综上，RLM 的核心差异可概括为：</p>
<blockquote>
<p>把提示当成环境变量，而非模型输入；用代码+递归子 LLM 实现<strong>输入长度</strong>的无限外扩，而非仅对任务做逻辑拆解。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“超长提示无法一次性塞进上下文”这一瓶颈重新定义为<strong>推理时计算</strong>问题，并借鉴“核外算法”（out-of-core）思想：数据远大于内存时，通过<strong>按需加载+符号化操作</strong>完成计算。具体方案称为 <strong>Recursive Language Models (RLMs)</strong>，其关键机制如下：</p>
<ol>
<li><p>把提示变成环境变量<br>启动一个 Python REPL，将完整提示  P  作为字符串变量 <code>context</code> 载入内存；模型不再直接接收  P  作为输入，而是接收一段<strong>固定系统提示</strong>+REPL 返回的<strong>截断执行结果</strong>。</p>
</li>
<li><p>用代码“窥视-分解-调用”<br>模型在 REPL 里写 Python 代码，可以</p>
</li>
</ol>
<ul>
<li>用 <code>print</code>/<code>regex</code>/<code>pandas</code> 等快速扫描、过滤、统计；</li>
<li>把 <code>context</code> 切成任意粒度（行、文档、token 块）；</li>
<li>通过内置函数 <code>llm_query(sub_prompt)</code> <strong>递归调用子 LLM</strong>（可以是自身轻量版或其他模型），子调用结果再写回 REPL 变量。</li>
</ul>
<ol>
<li><p>递归子调用不受窗口限制<br>每次 <code>llm_query</code> 只把<strong>当前必要片段</strong>喂给子模型，因此总输入长度可以远超基模型物理窗口；子模型输出被存入变量，供后续代码继续聚合、验证或拼接。</p>
</li>
<li><p>终止与答案返回<br>当代码逻辑判断已收集足够信息，模型执行<br><code>FINAL(answer)</code> 或 <code>FINAL_VAR(variable_name)</code><br>把结果返回给用户，整个轨迹在 REPL 内完成，无需人工设计摘要或检索流程。</p>
</li>
</ol>
<p>通过上述四步，RLM 将“长上下文”转化为<strong>可执行对象</strong>，用<strong>代码控制数据流+递归子 LLM 控制语义流</strong>，从而在 10M token 规模上仍保持高精度，且平均调用成本与基模型持平甚至更低。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 4 类信息密度递增的长上下文任务上展开系统实验，覆盖 32 K–11 M token 区间，对比 2 组前沿模型与 4 种推理策略，并给出成本-性能联合分析。实验设计如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>基座模型</td>
<td>GPT-5（闭源）与 Qwen3-Coder-480B-A35B（开源）</td>
</tr>
<tr>
<td>对比方法</td>
<td>① Base 直接调用 ② CodeAct(+BM25) ③ Summary Agent ④ RLM（完整） ⑤ RLM（无递归子调用）</td>
</tr>
<tr>
<td>任务集</td>
<td>① S-NIAH（常数级处理）② BrowseComp-Plus@1K-doc（多跳问答，6–11 M token）③ OOLONG（线性级语义聚合，131 K token）④ OOLONG-Pairs（二次级成对聚合，32 K token）⑤ LongBench-v2 CodeQA（900 k 级代码库理解）</td>
</tr>
<tr>
<td>规模轴</td>
<td>对 S-NIAH/OOLONG/OOLONG-Pairs 按 2¹³…2¹⁸ token 做长度扫描，观察性能衰减曲线</td>
</tr>
<tr>
<td>评价指标</td>
<td>准确率 / F1 / 官方评分函数；每次查询的 API 成本与运行时长（25–95 分位）</td>
</tr>
<tr>
<td>消融与失败分析</td>
<td>① 无递归子调用 ablation② 不同文档规模子集（BrowseComp 20 任务，100–1000 doc）③ 轨迹案例：验证-冗余、过度递归、输出令牌耗尽等负面模式</td>
</tr>
</tbody>
</table>
</div>
<p>核心结果一览（Table 1 &amp; Figure 1）：</p>
<ul>
<li>10 M token 规模下，RLM 仍保持 50–90 % 准确率，而基座模型因超出 272 K 窗口直接失败。</li>
<li>在窗口<strong>之内</strong>的任务，RLM 相对 Base 提升 10–58 % 绝对准确率；OOLONG-Pairs 的 F1 从 &lt;0.1 % 提升到 58 %（GPT-5）/ 23 %（Qwen3）。</li>
<li>中位调用成本：RLM 与 Base 同量级，甚至低 3×；仅在长尾轨迹出现高方差。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><strong>异步与并行化</strong>：当前子 LLM 调用为阻塞式，引入异步 <code>llm_query</code> 与并行 chunk 处理可显著压缩延迟与云成本。</li>
<li><strong>更深递归层数</strong>：实验限递归深度=1（仅子 LLM），探索深度&gt;1 的“子-子”调用能否进一步降低单段上下文压力，及如何避免误差级联。</li>
<li><strong>专用 RLM 预训练/后训练</strong>：用 RLM 轨迹数据（代码+子调用序列）做 SFT/RL，使模型学会更优的切块、过滤、停止决策，减少冗余验证。</li>
<li><strong>动态预算控制</strong>：在 REPL 内暴露“已用 token/费用”变量，让模型自主权衡精度-成本，实现可中断的任意长输入推理。</li>
<li><strong>多模态与工具扩展</strong>：将图像、音频、数据库、搜索引擎也作为环境变量，通过同一套“代码+递归调用”框架统一处理跨模态长上下文。</li>
<li><strong>可验证性提升</strong>：引入形式化验证或符号执行，对代码产生的中间变量进行一致性检查，降低长轨迹中“正确→错误”翻转概率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Recursive Language Models (RLMs)</strong> 提出一种<strong>推理时通用框架</strong>，把超长提示卸载到 Python REPL 环境，让大模型用代码“窥视-分解-递归调用自身”，突破上下文窗口与上下文腐烂双重瓶颈。主要贡献如下：</p>
<ol>
<li><p>问题重新定义<br>将“长上下文”视为<strong>推理时计算</strong>问题，而非单纯扩大窗口。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>提示 → 环境变量 <code>context</code></li>
<li>模型写 Python 代码做过滤、切块、聚合</li>
<li>通过 <code>llm_query()</code> 递归调用子 LLM，片段级处理，结果写回变量</li>
<li>终止时 <code>FINAL()/FINAL_VAR()</code> 返回答案</li>
</ul>
<ol>
<li><p>实验<br>在 32 K–11 M token 的 5 项任务（S-NIAH、BrowseComp-Plus、OOLONG、OOLONG-Pairs、CodeQA）上，RLM 用 GPT-5 与 Qwen3-Coder 均取得<strong>双倍位数绝对准确率提升</strong>，10 M token 仍保持高性能，中位 API 成本与基模型持平或更低。</p>
</li>
<li><p>结论<br>RLMs 是<strong>任务无关、模型无关</strong>的推理范式，无需重训练即可把有效上下文扩展两个数量级，为“无限长输入”提供了一条可扩展、可负担的新轴心。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Recursive%20Language%20Models，链接是https://arxiv.org/pdf/2512.24601，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.24601。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Alex L. Zhang, Tim Kraska, Omar Khattab</p>
<p><strong>Categories</strong>: cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.24601v2.pdf">https://arxiv.org/pdf/2512.24601v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.24601v2">https://papers.cool/arxiv/2512.24601v2</a></p>
<p><strong>Published</strong>: 2025-12-31T03:43:41Z</p>
<hr>
<h3 id="2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-3"><a href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-3" class="headerlink" title="2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs"></a>2. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20861v1">Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</a></h3><p>One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>当前AI系统中最缺失的能力之一是部署后持续学习的能力。实现此类持续学习系统面临诸多挑战，其中之一是用于训练最先进LLMs的基于梯度算法的大量内存需求。进化策略（ES）最近作为传统学习算法的无梯度替代方案重新出现，并在LLMs的特定任务中表现出令人鼓舞的表现。本文对ES进行了全面分析，并特别评估了在训练更新步骤增加时遗忘曲线。首先，我们发现ES在计算预算相当的情况下，在数学和推理任务中能够达到接近GRPO的性能。然而，对持续学习最重要的是，ES的性能提升伴随着对先前能力的显著遗忘，限制了其在线训练模型的适用性。我们还探讨了这种行为背后的原因，并展示了使用ES进行的更新比对应GRPO更新更稀疏，且其$\ell_2$范数远大于相应的GRPO更新，这解释了两种算法之间遗忘曲线的对比。通过这项研究，我们旨在突出像ES这样的无梯度算法中遗忘的问题，并希望激励未来工作来缓解这些问题。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文聚焦“如何让大模型在部署后持续学习”这一核心难题，具体针对以下子问题展开研究：</p>
<ul>
<li>梯度类后训练算法（SFT、RLHF、GRPO 等）内存开销巨大，难以在端侧或在线场景下反复更新权重。</li>
<li>近期复兴的梯度无参算法——进化策略（Evolutionary Strategies, ES）——声称在特定任务上可与 GRPO 媲美，且仅需前向推理，内存占用极低，被视为持续学习的潜在替代方案。</li>
<li>然而，ES 在连续更新过程中是否会“灾难性遗忘”旧能力，此前缺乏系统评估；若遗忘严重，则其“在线学习”优势将失去意义。</li>
</ul>
<p>因此，论文旨在</p>
<ol>
<li>在数学与推理任务上全面对比 ES 与 GRPO 的峰值性能；</li>
<li>量化二者在持续更新过程中的遗忘曲线，判断 ES 是否真正适合部署后持续学习；</li>
<li>从参数更新范数与稀疏度角度解释遗忘差异，为后续改进梯度无参算法提供依据。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 Related Work 中提及并扩展了以下几条研究脉络：</p>
<ol>
<li>进化策略（Evolution Strategies, ES）</li>
</ol>
<ul>
<li>经典理论：Rechenberg 1989、Schwefel 1977、Beyer 1995 奠定  (μ,λ) -ES 框架。</li>
<li>实用变体：CMA-ES（Hansen &amp; Ostermeier 2001）、自然 ES（Wierstra et al. 2011, Sun et al. 2012）。</li>
<li>深度神经演化：Such et al. 2018、Risi &amp; Stanley 2019 在百万参数规模验证 GA/ES 可训练 RL 网络。</li>
<li>大模型时代：Salimans et al. 2017 首次将 ES 扩展到强化学习，展示高度并行与无梯度优势。</li>
<li>近期 LLM 工作：<br>– Malladi et al. 2024、Korotyshova et al. 2025 提出内存高效的 LoRA-ES。<br>– Qiu et al. 2025（与本论文直接对标）在 1–7 B 模型上报告 ES 优于 GRPO，但未系统评估遗忘。</li>
</ul>
<ol>
<li>梯度类后训练算法</li>
</ol>
<ul>
<li>SFT：Wei et al. 2022</li>
<li>RLHF：Ouyang et al. 2022</li>
<li>DPO：Rafailov et al. 2024</li>
<li>GRPO：Shao et al. 2024，通过组内相对奖励省去 Critic，成为本文对比基线。</li>
</ul>
<ol>
<li>灾难性遗忘与持续学习</li>
</ol>
<ul>
<li>Kirkpatrick et al. 2017 提出 EWC，首次在深度学习语境下量化遗忘。</li>
<li>Gupta et al. 2024 发现大规模模型编辑同样会出现“渐进-突发”式遗忘。</li>
<li>Shenfeld et al. 2025 观察到带 KL 正则的在线 RL（包括 GRPO）遗忘更少，为本论文提供参照。</li>
</ul>
<ol>
<li>参数更新稀疏性研究</li>
</ol>
<ul>
<li>Mukherjee et al. 2025 通过掩码分析证明 RLHF 仅微调极小“子网络”，解释其抗遗忘特性；本文沿用其稀疏度定义与分层统计方法，对比 ES 与 GRPO。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一套“新算法”去彻底解决灾难性遗忘，而是通过<strong>系统性实证诊断</strong>揭示 ES 在持续学习场景下的根本缺陷，从而为后续算法设计指明改进方向。具体步骤如下：</p>
<ol>
<li>构建可比实验框架</li>
</ol>
<ul>
<li>统一任务：Countdown、GSM8K、MATH、OlympiadBench。</li>
<li>统一模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct。</li>
<li>统一资源：30 条 rollout / population，200 条训练样本，500 更新步以内。</li>
<li>统一评估：每轮 checkpoint 同时测“新任务准确率”与“旧任务（HellaSwag）准确率”，绘制遗忘曲线。</li>
</ul>
<ol>
<li>量化遗忘现象</li>
</ol>
<ul>
<li>绘制 Pareto 前沿：新任务准确率 vs 旧任务准确率。</li>
<li>统计旧任务性能随更新步的衰减斜率；ES 在 200 步后额外训练几乎不再提升新任务，却持续降低旧任务约 10 个百分点，而 GRPO 基本平稳。</li>
</ul>
<ol>
<li>追溯根本原因</li>
</ol>
<ul>
<li><strong>更新范数</strong>：计算相邻 checkpoint 的 Frobenius 范数</li>
</ul>
<p>|DeltaW|<em>F = |W</em>(t) - W_(t-1)|_F</p>
<p>ES 在 500 步时漂移幅度比 GRPO 大 <strong>3 个数量级</strong>。</p>
<ul>
<li><strong>更新稀疏度</strong>：定义</li>
</ul>
<p>sparsity = #|(DeltaW)_(ij)| &lt; 10^(-6)#params</p>
<p>GRPO 稀疏度 ≈ 95 %，ES 普遍 &lt; 20 %，呈全局密集扰动。</p>
<ul>
<li>关联分析：大范数 + 低稀疏 → 参数空间全局漂移 → 与旧任务表征冲突 → 灾难性遗忘。</li>
</ul>
<ol>
<li>给出改进启示</li>
</ol>
<ul>
<li>梯度无参方法若想用于持续学习，必须<strong>显式约束更新范数</strong>（如裁剪、衰减）或<strong>强制稀疏掩码</strong>（如 LoRA-ES + 动态门控）。</li>
<li>可借鉴 GRPO 的 KL 正则思路，在 ES 的目标中引入与锚定模型的散度惩罚，限制漂移。</li>
</ul>
<p>综上，论文通过“实验量化 → 现象确认 → 根因剖析 → 改进提示”四步，<strong>把 ES 的遗忘风险从经验猜测转化为可度量、可解释的定量结论</strong>，为后续设计“既省内存又不遗忘”的梯度无参算法奠定实证基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计三类实验，全部围绕“ES 能否在持续学习场景下替代 GRPO”这一核心问题展开。实验均在同一硬件（NVIDIA RTX A6000）与同一代码框架（VERL + 自复现 ES）上完成，保证可比性。</p>
<ol>
<li>主实验：ES vs GRPO 峰值性能对照</li>
</ol>
<ul>
<li>模型：Qwen2.5-1.5B-Instruct、Llama-3.2-1B-Instruct</li>
<li>任务：Countdown、GSM8K、MATH、OlympiadBench</li>
<li>协议：每任务 200 训练样本，30 rollout/population，最大 500 更新步，早停验证集不再提升。</li>
<li>指标：验证集最佳准确率（%）</li>
<li>结果：ES 平均落后 GRPO 3–4 个百分点，仅在 Llama-3.2-1B/GSM8K 一项略超；其余均低于 GRPO，反驳了“ES 全面优于 GRPO”的先前提法。</li>
</ul>
<ol>
<li>遗忘曲线实验</li>
</ol>
<ul>
<li>固定模型：Qwen2.5-1.5B-Instruct</li>
<li>新任务：Countdown（训练集 200 条）</li>
<li>旧任务：HellaSwag（全验证集，作为“通用先验能力”探针）</li>
<li>协议：每 50 步保存一个 checkpoint，同时测 Countdown 与 HellaSwag 准确率。</li>
<li>可视化：<br>– Pareto 前沿图：新任务准确率 vs 旧任务准确率<br>– 单指标时序图：HellaSwag 准确率随更新步变化</li>
<li>结果：<br>– ES 在 200 步后新任务收敛，但旧任务仍单调下降，累计跌落 ≈ 10 %。<br>– GRPO 旧任务曲线几乎水平，无明显遗忘。</li>
</ul>
<ol>
<li>更新诊断实验（解释为何遗忘）<br>3.1 更新范数</li>
</ol>
<ul>
<li>计算相邻 checkpoint 参数差 ΔW 的 Frobenius 范数</li>
<li>结果：500 步时 ES 漂移比 GRPO 大 <strong>1000×</strong> 以上，且单调递增。</li>
</ul>
<p>3.2 更新稀疏度</p>
<ul>
<li>定义阈值 τ=1e-6，统计 |ΔW_ij|&lt;τ 的元素比例</li>
<li>分层统计：Attention Q/K/V、WO、MLP、LayerNorm</li>
<li>结果：<br>– GRPO 稀疏度 ≈ 95 %（仅 5 % 参数被显著修改）<br>– ES 稀疏度 &lt; 20 %，几乎全参数被扰动，LayerNorm 稍高但仍远低于 GRPO。</li>
</ul>
<p>3.3 KL-散度关联（附录）</p>
<ul>
<li>计算 π_θ 与 base 模型在 Countdown 与 HellaSwag 上的 KL</li>
<li>结果：ES 的 KL 随步数线性增长，且与旧任务准确率呈显著负相关；GRPO 因显式 KL 惩罚，散度与准确率均无单调关系。</li>
</ul>
<p>通过上述三类实验，论文既验证了 ES 的“性能可比性”，又定量揭示了其“灾难性遗忘”现象，并用范数+稀疏度+KL 三维诊断给出解释。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，均围绕“让梯度无参算法既省内存又不灾难性遗忘”这一核心目标展开：</p>
<ol>
<li>约束型 ES 框架</li>
</ol>
<ul>
<li>将 KL 惩罚或参数距离惩罚显式写入 ES 的目标：</li>
</ul>
<p>θ<em>(t+1) = θ_t + α · (1) / (N) ∑_n Z_n ε_n - β ∇</em>θ D<em>(KL)(π</em>(θ) parallel π_(ref))</p>
<p>需推导无梯度情形下如何高效估计该惩罚项。</p>
<ul>
<li>探索硬约束：每步更新后投影到以  θ<em>(base)  为中心的  ell_2  球或  ell</em>∞  立方体内，限制漂移半径。</li>
</ul>
<ol>
<li>稀疏化 ES 更新</li>
</ol>
<ul>
<li>引入随机掩码：每次只扰动随机子集参数（如 5 %），保持其余固定；掩码可在层、通道或奇异值维度上采样。</li>
<li>学习稀疏掩码：借鉴 RigL / Magnitude Pruning，动态决定“哪些参数值得扰动”，使有效更新稀疏度逼近 GRPO 的 95 % 水平。</li>
</ul>
<ol>
<li>低秩 / 适配器混合 ES</li>
</ol>
<ul>
<li>仅在 LoRA 低秩矩阵（A、B）上应用 ES 扰动，冻结主干；漂移空间从 1.5 B 降至数百万参数，天然减小范数。</li>
<li>比较不同秩（4, 16, 64）对“性能-遗忘”Pareto 前沿的影响，找出秩-遗忘临界值。</li>
</ul>
<ol>
<li>种群级正则</li>
</ol>
<ul>
<li>在种群内部保持“锚点个体”——不扰动的  θ_(base)  副本，将其奖励作为额外基线，惩罚偏离过远的个体。</li>
<li>研究锚点比例（1/30、5/30、10/30）对旧任务保留率的边际效应。</li>
</ul>
<ol>
<li>多任务 ES 演化</li>
</ol>
<ul>
<li>同时把 Countdown + HellaSwag 打包成多目标奖励：</li>
</ul>
<p>R = R<em>(new) + λ R</em>(old)</p>
<p>用演化多目标算法（NSGA-ES）搜索非支配解集，观察是否自然涌现“不遗忘”个体。</p>
<ul>
<li>考察  λ  调度：初期  λ  大，后期逐步减小，让模型先稳固旧能力再提升新能力。</li>
</ul>
<ol>
<li>与参数高效微调正交组合</li>
</ol>
<ul>
<li>将 ES 用于 Prompt-tuning、AdaLoRA、VeRA 等“极轻量”模块，验证遗忘是否随可训参数量线性下降。</li>
<li>对比同一参数预算下“ES-LoRA”与“GRPO-LoRA”的 Pareto 前沿，判断梯度自由在极低秩下是否仍有优势。</li>
</ul>
<ol>
<li>在线 / 流式场景测试</li>
</ol>
<ul>
<li>构建“任务流”：按顺序依次训练 Countdown → GSM8K → MATH → Olympiad，每任务仅 200 样本，周期性回放 5 % 旧数据。</li>
<li>监测 ES 与 GRPO 的“遗忘-学习”折线，评估哪种算法在数据流场景下最终平均性能更高。</li>
</ul>
<ol>
<li>理论侧：ES 漂移上界</li>
</ol>
<ul>
<li>在强凸或 PL 条件下，推导 ES 更新步的期望漂移</li>
</ul>
<p>E[|θ_(t)-θ_0|^2] le f(t, σ, α, d)</p>
<p>并与 SGD/GRPO 的对应上界比较，给出“遗忘-漂移”定量关系。</p>
<ol>
<li>统计稳定性提升</li>
</ol>
<ul>
<li>扩大种群（30 → 128 → 512）与减半学习率，观察遗忘曲线方差是否显著下降；给出“种群大小-遗忘方差”经验公式，为工业部署提供选型依据。</li>
</ul>
<ol>
<li>遗忘评测体系扩展</li>
</ol>
<ul>
<li>引入更多旧任务探针（MMLU、ARC、TruthfulQA）与参数探测方法（LiNGAM、CKA），检查遗忘是否具“任务特异性”或“全局通用性”。</li>
<li>记录层-wise 遗忘速率，找出 ES 最先干扰的表征层，为后续选择性正则提供目标。</li>
</ul>
<p>通过上述探索，可逐步把 ES 从“内存友好但易遗忘”的基线，推向“内存友好且持续稳健”的实用在线学习方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>研究背景</strong><br>大模型部署后需持续学习，但梯度类后训练（SFT、RLHF、GRPO）内存开销大；进化策略（ES）作为梯度无参替代方案近期被重新关注，声称性能可比 GRPO 且极省内存，然而其是否会灾难性遗忘尚缺系统评估。</p>
</li>
<li><p><strong>核心发现</strong></p>
</li>
</ul>
<ol>
<li>在 Countdown、GSM8K、MATH、OlympiadBench 上，ES 峰值准确率仅落后 GRPO 3–4 个百分点，计算步数相近，确认其“性能可比”。</li>
<li>在同一微调 run 内，ES 新任务收敛后旧任务（HellaSwag）准确率仍持续下降约 10 %；GRPO 几乎无遗忘。</li>
<li>诊断表明 ES 更新 Frobenius 范数比 GRPO 大 <strong>1000×</strong>，稀疏度 &lt; 20 %（GRPO ≈ 95 %），全局密集漂移导致旧能力被破坏。</li>
</ol>
<ul>
<li><strong>结论</strong><br>ES 虽提供内存友好的梯度无参路径，但单任务微调即出现显著灾难性遗忘，目前不适合需要保留旧能力的在线学习场景；未来需引入稀疏化、范数约束或 KL 正则等机制以缓解遗忘。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Evolutionary%20Strategies%20lead%20to%20Catastrophic%20Forgetting%20in%20LLMs，链接是https://arxiv.org/pdf/2601.20861，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20861。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20861v1.pdf">https://arxiv.org/pdf/2601.20861v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20861v1">https://papers.cool/arxiv/2601.20861v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:59:34Z</p>
<hr>
<h3 id="3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching-2"><a href="#3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching-2" class="headerlink" title="3. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching"></a>3. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.15942v2">ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</a></h3><p>Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>气候预测存在与气候系统组成部分及其相互作用相关的不确定性。量化这些不确定性的典型方法是利用气候模型在不同初始条件下创建重复模拟的集合。由于这些模拟的复杂性，生成此类投影集合计算成本高。在本研究中，我们介绍了基于深度学习的气候模型模拟器ArchesClimate，旨在降低这一成本。ArchesClimate 基于 IPSL-CM6A-LR 气候模型的十年后投影进行训练，空间分辨率约为 2.5x1.25 度。我们训练一个基于ArchesWeatherGen的流量匹配模型，并对其进行调整以预测近期气候。训练完成后，模型能在一个月的预备时间内生成状态，并可用于自动回归模拟任意长度的气候模型模拟。我们证明，这些世代在长达10年内保持稳定且身体稳定。我们还展示了对于若干重要气候变量，ArchesClimate生成的仿真与IPSL模型可互换。这项研究表明，气候模型仿真器有望显著降低气候模型模拟的成本。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在以极低计算成本生成气候模式 IPSL-CM6A-LR 的十年尺度概率集合，从而替代传统“多次扰动初值 + 全模式积分”这一昂贵流程，支撑年代际气候变率与不确定性的量化研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>NeuralGCM</strong>（Kochkov et al. 2024）<br>混合物理-深度学习自回归大气集合 emulator，在网络内部注入噪声以产生内部变率。</p>
</li>
<li><p><strong>Spherical DYffusion</strong>（Cachay et al. 2023）<br>基于扩散模型的概率气候 emulator，显式对大气状态分布进行建模。</p>
</li>
<li><p><strong>cBottle</strong>（Brenowitz et al. 2025）<br>生成式扩散模型，学习多源再分析资料的瞬时短期动力，但非自回归，无法延伸为长序列。</p>
</li>
<li><p><strong>ACE / ACE2</strong>（Watt-Meyer et al. 2023, 2024）<br>确定性大气 emulator，可稳定积分千年，但不包含随机性，也未耦合海洋。</p>
</li>
<li><p><strong>LUCIE</strong>（Guan et al. 2024）<br>轻量级非耦合气候 emulator，以 ERA5 为训练数据，侧重高效生成大集合。</p>
</li>
<li><p><strong>DLESyM</strong>（Cresswell-Clay et al. 2025）<br>耦合大气-海洋深度学习地球系统模型，用 ERA5 训练，验证 AI 同时模拟两圈层的可能性。</p>
</li>
<li><p><strong>SamudrACE</strong>（Duncan et al. 2025）<br>将确定性海洋 emulator Samudra 与 ACE2 耦合，实现快速海-气交换模拟。</p>
</li>
<li><p><strong>ACE-SOM2</strong>（Clark et al. 2024）<br>ACE2 与单层海洋耦合，用于探究不同强迫下的气候响应，向更长尺度与不同情景扩展。</p>
</li>
</ul>
<p>这些研究共同构成了“AI 气候模式替代”脉络，为 ArchesClimate 在概率性、十年尺度、海-气耦合与计算效率上的设计提供了直接对比与参考。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 ArchesClimate，一套基于**流匹配（flow matching）**的深度生成框架，以月时间步长、≈2.5°×1.25° 空间分辨率自回归地模拟 IPSL-CM6A-LR 的十年集合。核心思路与步骤如下：</p>
<ol>
<li><p>数据利用<br>采用 IPSL-CM6A-LR 对 CMIP6-DCPP 的 55 组 10 年、10 成员回报试验（≈7×10⁴ 个月），仅选取 24 个关键海-气变量（含 4 层大气、7 个海洋整层量）及 5 种辐射强迫（CO₂、CH₄、N₂O、CFC11eq、SSI）作为训练集。</p>
</li>
<li><p>两阶段架构</p>
</li>
</ol>
<ul>
<li><strong>确定性模型</strong>  f<em>θ ：3D-Swin-U-Net 结构，输入  X_t 、 X</em>(t-δ)  与强迫，输出下月平均态  hat X_(t+δ) 。</li>
<li><strong>生成模型</strong>  g_θ ：同骨干网络，以 flow matching 学习残差分布</li>
</ul>
<p>r<em>(t+δ)= X</em>(t+δ) - f<em>θ(X_t,X</em>(t-δ))σ</p>
<p>训练时从  mathcal N(0,I)  到残差分布建立可逆映射；推理时从随机噪声出发，经  M=12  步 ODE 积分生成样本残差，再加回确定性预测得到完整状态。</p>
<ol>
<li><p>强迫嵌入<br>对每条辐射强迫标量，先经线性嵌入生成 scale/shift 参数，再以<strong>条件层归一化</strong>注入所有 Transformer 块，使模型可在不同强迫情景下动态调整。</p>
</li>
<li><p>自回归集合生成<br>训练完成后，用  X<em>0 、 X</em>(-1)  初始化，交替调用  f<em>θ  与  g</em>θ  逐月外推；每次从独立高斯噪声采样即可产生不同成员，实现 10 年、任意大小集合的“一键”输出。</p>
</li>
<li><p>计算加速<br>模型仅作用于 144×144×45 的粗网格，参数空间约为原模式 1/400；在 4×A100 上 12 min 可生成 10 年 10 成员集合，对应 ≈4 core-h/模拟年，而 IPSL-CM6A-LR 需 ≈1900 core-h/模拟年。</p>
</li>
</ol>
<p>通过“确定性骨架 + 流匹配残差”策略，ArchesClimate 在保持物理一致性、季节-年际变率与低频趋势的同时，把传统气候模式集合成本降低两个数量级，从而解决了“十年尺度概率集合昂贵”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否以极低算力生成与 IPSL-CM6A-LR 统计-物理特性一致的十年集合”这一核心问题，设计了 10 组实验，覆盖统计评分、物理一致性、可交换性、方差调校与长期稳定性等维度。实验按目的可归纳为以下 6 类：</p>
<ol>
<li>基础统计评估</li>
</ol>
<ul>
<li><strong>CRPS 与标准差对比</strong>（表 3）<br>对 1969-79、1979-89、2010-20 三个十年，计算 8 个关键变量（tos、psl、net_flux、thetaot2000 等）的纬度加权 CRPS 与集合标准差，比较 ArchesClimate、IPSL-DCPP 与 Pattern-Scaling 基线。</li>
<li><strong>方差-CRPS 消融</strong>（图 5）<br>对比三种训练配置：<br>– residual-flow（本文方案）<br>– full-deterministic（仅  f<em>θ ）<br>– full-flow（仅  g</em>θ ，无确定性骨架）<br>量化确定性骨架对精度与方差的贡献。</li>
</ul>
<ol>
<li>集合可交换性（rank histogram）</li>
</ol>
<ul>
<li><strong>北大西洋区域 rank histogram</strong>（图 6 左）<br>1969 起报的 10 成员、10 年序列，对 6 个变量逐格点排序，检验 ArchesClimate 成员是否能被视为从 IPSL 分布中独立同分布抽取。</li>
</ul>
<ol>
<li>频谱与变率结构</li>
</ol>
<ul>
<li><strong>时间功率谱（TPS）对比</strong>（图 6 右）<br>对同一北大西洋 10 年异常序列做逐点傅里叶变换，比较月-季-年周期强度，诊断 ArchesClimate 是否欠变率（under-dispersion）。</li>
</ul>
<ol>
<li>长期趋势与遥相关</li>
</ol>
<ul>
<li><strong>线性趋势空间相关</strong>（图 7）<br>计算 5 成员集合的 tos 异常 10 年趋势，逐像素与 IPSL 对照，评估年代际信号在三大洋及北极的相关性。</li>
<li><strong>50 年强迫响应</strong>（附录图 A1）<br>固定 1969 强迫 vs 时变强迫各 rollout 50 年，验证模型能否随辐射强迫漂移而非崩溃。</li>
</ul>
<ol>
<li>区域/季节信号</li>
</ol>
<ul>
<li><strong>热带区域气候态与异常</strong>（图 4）<br>20°S–20°N 平均的季节循环与异常时间序列，对比集合均值与 ±1σ 包络。</li>
<li><strong>北大西洋季节异常空间图</strong>（图 8）<br>MAM/JJA/SON/DJF 四季平均 tos 异常的空间格局对比，检验季节尺度空间相位与强度。</li>
</ul>
<ol>
<li>方差增强与损失设计</li>
</ol>
<ul>
<li><strong>三种增方差策略</strong>（图 9）<br>– 推理初始噪声放大 1.1×<br>– 按变量训练-验证方差差值逐变量缩放噪声<br>– 在 flow-matching 损失中加入梯度与功率谱项（ L<em>(rm total)=L</em>(rm MSE)+0.2L<em>(rm grad)+0.2L</em>(rm PSD) ）<br>比较其对 CRPS-方差权衡的影响。</li>
</ul>
<p>以上实验共同表明：</p>
<ul>
<li>ArchesClimate 的 CRPS 与 IPSL 相当或更低，但方差普遍略小；</li>
<li>加入谱-梯度损失可把方差提升到 IPSL 水平，代价是 CRPS 略增；</li>
<li>rank histogram 在 net_flux、wap 等变量接近均匀，tos、thetaot2000 存在低值偏移；</li>
<li>热带与北大西洋季节空间异常被忠实再现，年代际趋势在三大洋遥相关区相关系数 &gt;0.8；</li>
<li>50 年 rollout 仍能跟随辐射强迫漂移，未见数值漂移或崩溃。</li>
</ul>
<p>因此，论文从统计、物理、频谱、长期稳定性四个角度系统验证了“低成本 AI 集合”替代“高成本模式集合”的可行性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>高分辨率时空降尺度</strong><br>以 ArchesClimate 月尺度 2.5° 输出为条件，训练条件扩散或超分模型，生成 1 km-日级别区域情景，支持极端事件归因与局地影响评估。</p>
</li>
<li><p><strong>多世纪长序列外推</strong><br>将训练数据从 55×10 年扩展至 CMIP6 “historical” 与 SSP 几百年试验，检验对 50-100 年低频变率（AMO、PDO、IPO）与强迫响应的保真度。</p>
</li>
<li><p><strong>增强海洋与冰冻圈变量</strong><br>引入海冰厚度/面积、陆地冰盖、三维盐度与洋流、植被动态，减少北极趋势漂移并提升碳循环一致性。</p>
</li>
<li><p><strong>守恒与物理约束嵌入</strong><br>在损失或网络层显式加入大气静力平衡、柱总水量/能量守恒、海洋热盐守恒，抑制长期能量泄漏。</p>
</li>
<li><p><strong>极端与尾部分布校准</strong><br>采用尾部分位数映射或极端值理论对 flow-matching 输出进行事后校正，提高热浪、干旱、极端降水重现期精度。</p>
</li>
<li><p><strong>单步/少步生成加速</strong><br>引入 Consistency-Model 或 Rectified-Flow 单步采样，把 12 步推理降至 1-2 步，再降一个数量级推理耗时。</p>
</li>
<li><p><strong>跨模式迁移与多模式集合</strong><br>用 IPSL 训练的模型权重初始化，微调至 CESM2、MRI-ESM2 等，实现“零成本”多模式集合加权平均。</p>
</li>
<li><p><strong>初始化信息再利用</strong><br>将海洋同化态（三维温盐）或海冰初值编码为条件向量，评估初始化记忆对 1-5 年预测技巧的贡献。</p>
</li>
<li><p><strong>强迫情景插值与反事实试验</strong><br>在嵌入空间对 CO₂、SSI 做线性或非线性插值，生成未运行过的强迫路径（如 1.5 °C 过冲、火山群喷发），快速筛查气候响应。</p>
</li>
<li><p><strong>谱方差自适应损失</strong><br>按纬度-变量-频段动态加权功率谱损失，缓解年循环“过拟合”而高频变率不足的问题，实现方差-精度自动权衡。</p>
</li>
<li><p><strong>不确定性分解</strong><br>利用大样本（10³-10⁴ 成员）将总不确定性拆分为内部变率、模型结构、参数与强迫，对照传统大集合进行方差分解验证。</p>
</li>
<li><p><strong>在线数据同化闭环</strong><br>将 ArchesClimate 作为“廉价预报核”，与 Ensemble Kalman Filter 结合，实现实时年代际预测系统，每月滚动更新。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>ArchesClimate：用流匹配实现十年尺度概率气候集合的快速生成</strong></p>
<ol>
<li><p>问题<br>传统气候模式需对初值扰动运行数十次才能量化十年内部变率，单成员 10 年 ≈ 1900 core-h，成本高昂。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>数据：IPSL-CM6A-LR 的 DCPP 回报（55×10 年×10 成员，≈7×10⁴ 月），24 个海-气变量 + 5 种辐射强迫。</li>
<li>架构：两阶段<br>– 确定性 Swin-U-Net  f<em>θ  预测下月平均态；<br>– 流匹配生成网络  g</em>θ  学习残差分布，12 步 ODE 从高斯噪声采样。</li>
<li>训练：先训  f<em>θ  再训  g</em>θ ，损失为逐网格 MSE（纬度加权）+ 可选谱/梯度正则。</li>
<li>推理：自回归 rollout，10 年 10 成员在 4×A100 上 12 min 完成（4 core-h/年）。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>统计：CRPS 与 IPSL 相当，方差略低；加入谱损失可把方差提升至 IPSL 水平。</li>
<li>可交换性：net_flux、wap 的 rank histogram 接近均匀，tos/thetaot2000 略偏低值。</li>
<li>频谱：年-季周期准确，月-年际功率略低。</li>
<li>趋势：三大洋年代际趋势空间相关 &gt;0.8，北极因无冰过程快速退相关。</li>
<li>长期：50 年 rollout 仍能跟随辐射强迫漂移，无崩溃。</li>
<li>区域：热带与北大西洋季节异常空间相位与观测一致。</li>
</ul>
<ol>
<li><p>结论<br>ArchesClimate 以 ≈ 1/400 参数规模、1/500 计算成本，生成与 IPSL-CM6A-LR 统计-物理特性高度一致的十年集合，为年代际气候不确定性与极端事件研究提供可负担的“AI 集合引擎”。</p>
</li>
<li><p>展望<br>高分辨率降尺度、多世纪长序列、守恒约束、极端校正、单步采样及跨模式迁移等方向可进一步提升可用性与物理可信度。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ArchesClimate%3A%20Probabilistic%20Decadal%20Ensemble%20Generation%20With%20Flow%20Matching，链接是https://arxiv.org/pdf/2509.15942，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.15942。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis</p>
<p><strong>Categories</strong>: physics.ao-ph, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.15942v2.pdf">https://arxiv.org/pdf/2509.15942v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.15942v2">https://papers.cool/arxiv/2509.15942v2</a></p>
<p><strong>Published</strong>: 2025-09-19T12:53:24Z</p>
<hr>
<h3 id="4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems-2"><a href="#4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems-2" class="headerlink" title="4. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems"></a>4. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.06052v3">DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</a></h3><p>Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>离散组合问题（DCP）在工业决策和优化中非常普遍。然而，尽管针对DCP的约束求解技术已经显著进步，但将问题形式化的核心过程，即约束建模，仍然需要大量专业知识，并且是广泛应用的瓶颈。为了缓解这一瓶颈，近期研究探索了使用大型语言模型（LLM）将组合问题描述转化为可执行的约束模型。然而，现有的离散约束建模评估数据集通常仅限于小型、同质化或特定领域的问题，这无法反映现实世界场景的多样性。本研究通过引入DCP-Bench-Open来填补这一空白，这是一个新型基准，包含来自约束规划（CP）和运筹学（OR）社区的多种知名离散组合问题，并专门针对LLM驱动的约束建模进行了结构化设计。利用该数据集，并考虑不同建模框架的多样性，我们对LLM在三种不同约束建模系统中的建模能力进行了比较和评估，这些系统在抽象层次和底层语法上各不相同。值得注意的是，结果显示在使用基于高层次Python框架建模时性能更高。此外，我们系统评估了不同LLM在提示式和推理时计算方法下的使用，这进一步提高了准确性，在这一高度挑战性的基准上达到了91%。DCP-Bench-Open已公开提供。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决的问题是如何利用大型语言模型（LLMs）来简化约束编程（Constraint Programming, CP）中的约束建模过程，从而降低约束编程的使用门槛，使其能够被更广泛的非专家用户所接受和应用。具体而言，论文的目标包括：</p>
<ol>
<li><p><strong>解决约束建模的瓶颈问题</strong>：约束编程是一种解决组合优化问题的有效范式，但其核心过程——约束建模——非常复杂，需要深厚的应用领域知识和对建模框架的熟练掌握。这限制了约束编程技术的广泛应用。论文探索使用LLMs作为建模助手，将自然语言描述的组合问题转化为可执行的约束模型，类似于现有的基于LLMs的代码生成助手。</p>
</li>
<li><p><strong>提供一个多样化的基准数据集</strong>：现有的用于评估LLMs在约束建模能力上的数据集往往存在局限性，例如规模小、同质化或特定于某个领域，无法涵盖现实世界场景的多样性。论文通过引入一个新的基准数据集CP-Bench，填补了这一空白。CP-Bench包含了一系列来自CP社区的多样化、知名的组合问题类别，专门用于评估LLMs驱动的CP建模能力。</p>
</li>
<li><p><strong>评估不同约束建模框架对LLMs性能的影响</strong>：论文比较和评估了LLMs在三种不同抽象层次和底层语法的约束建模系统中的建模能力，这些系统包括高级别的MiniZinc语言、基于Python的CPMpy库，以及较低级别的Python接口OR-Tools CP-SAT求解器。通过这种比较，论文旨在了解不同框架对LLMs建模准确性的具体影响。</p>
</li>
<li><p><strong>探索提升LLMs生成有效约束模型能力的方法</strong>：为了提高LLMs生成有效约束模型的能力，论文系统地评估了基于提示（prompt-based）和推理时计算（inference-time compute）方法在约束建模中的应用。这些方法包括从现有基于LLMs的代码生成研究中借鉴而来的技术，如检索增强型上下文学习（Retrieval-Augmented In-Context Learning, RAICL）、重复采样和自我验证等。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了多个与利用大型语言模型（LLMs）进行约束建模相关的研究工作，这些研究主要集中在以下几个方面：</p>
<p>约束编程与大型语言模型的结合</p>
<ul>
<li><strong>NL4Opt Competition</strong>：Ramamonjison等人在NeurIPS 2022竞赛中提出了NL4Opt数据集，这是一个包含简单线性规划（LP）问题的基准，用于评估LLMs将自然语言问题描述转化为优化模型的能力<br>^28^<br>。</li>
<li><strong>Logic Grid Puzzles (LGPs)</strong>：Jabrayilzade和Tekir提出了一个用于解决逻辑网格谜题的数据集，这些谜题可以看作是组合问题的一个特定子集<br>^14^<br>。</li>
<li><strong>Text2Zinc</strong>：Singirikonda等人提出了一个跨领域的数据集，包含MiniZinc模型，主要关注线性规划（LP）和混合整数规划（MIP）问题<br>^31^<br>。</li>
</ul>
<p>大型语言模型在代码生成中的应用</p>
<ul>
<li><strong>Evaluating Large Language Models Trained on Code</strong>：Chen等人研究了LLMs在代码生成方面的表现，特别是在生成Python代码方面的能力<br>^4^<br>。</li>
<li><strong>Automated Repair of Programs from Large Language Models</strong>：Fan等人探讨了LLMs在自动修复程序中的应用，这与LLMs在约束建模中的自我验证和错误检测有相似之处<br>^7^<br>。</li>
</ul>
<p>大型语言模型的推理时计算方法</p>
<ul>
<li><strong>Scaling LLM Test-Time Compute</strong>：Snell等人研究了在测试时增加LLMs的计算量以提高性能的方法，这在论文中被应用于约束建模任务中<br>^32^<br>。</li>
<li><strong>Teaching Large Language Models to Self-Debug</strong>：Chen等人提出了让LLMs进行自我调试的方法，这与论文中探讨的自我验证方法有相似之处<br>^5^<br>。</li>
</ul>
<p>约束编程的其他相关工作</p>
<ul>
<li><strong>Global Constraint Catalogue</strong>：Beldiceanu等人提供了约束编程中全局约束的详细目录，这对于理解约束编程的多样性和复杂性非常重要<br>^1^<br>。</li>
<li><strong>Applications of Combinatorial Optimization</strong>：Paschos的著作提供了组合优化在多个领域的应用实例，强调了约束编程在实际问题中的重要性<br>^24^<br>。</li>
</ul>
<p>这些相关研究为本文提供了背景和基础，同时也指出了现有研究的局限性和改进方向。例如，现有数据集在多样性和复杂性上的不足，以及在不同约束建模框架下LLMs性能的差异等。本文通过引入CP-Bench数据集和系统地评估不同框架和方法，旨在推动这一领域的研究进展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下几个主要步骤来解决如何利用大型语言模型（LLMs）简化约束编程（CP）中的约束建模过程的问题：</p>
<p>1. 构建多样化的基准数据集 CP-Bench</p>
<ul>
<li><strong>数据集来源</strong>：论文从多个来源收集了101个多样化的组合问题，包括CSPLib、CPMpy在线示例库、Håkan Kjellerstrand的在线资源库以及一个基于课程的问题集<br>^21^<br>。</li>
<li><strong>数据集结构</strong>：每个数据集实例都被转化为一个自包含的Python文件，包含元数据、问题描述、输入数据（可选）、模型代码和解决方案打印代码<br>^5^<br>。</li>
<li><strong>评估方法</strong>：论文采用解决方案准确性（solution accuracy）作为评估指标，即通过执行生成的代码来验证其解决方案是否满足真实模型的约束和目标函数<br>^5^<br>。</li>
</ul>
<p>2. 评估不同约束建模框架对LLMs性能的影响</p>
<ul>
<li><strong>选择框架</strong>：论文选择了三个具有不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）<br>^1^<br>。</li>
<li><strong>系统提示（System Prompts）</strong>：为了指导LLMs更有效地进行建模，论文设计了三种不同详细程度的系统提示，从基本的建模任务描述到包含框架文档的详细提示<br>^4.1^<br>。</li>
<li><strong>实验结果</strong>：实验表明，使用Python基础框架（CPMpy和OR-Tools）生成模型的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
</ul>
<p>3. 探索提升LLMs生成有效约束模型能力的方法</p>
<ul>
<li><strong>检索增强型上下文学习（RAICL）</strong>：通过从预构建的数据库中检索与当前问题语义相似的输入-输出对来丰富提示上下文<br>^4.2^<br>。然而，实验结果表明，这种方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>重复采样（Repeated Sampling）</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型<br>^4.3^<br>。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证（Self-Verification）</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性<br>^4.4^<br>。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
</ul>
<p>4. 结合方法以进一步提高性能</p>
<ul>
<li><strong>综合方法</strong>：论文将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>通过上述步骤，论文不仅提供了一个多样化的基准数据集来评估LLMs在约束建模中的能力，还系统地比较了不同建模框架对LLMs性能的影响，并探索了多种方法来提高LLMs生成有效约束模型的能力。这些研究成果为降低约束编程的使用门槛、使其更易于被非专家用户接受和应用提供了有力的支持。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验来评估大型语言模型（LLMs）在约束建模中的表现：</p>
<p>实验一：不同建模框架下的LLMs性能评估</p>
<ul>
<li><strong>目的</strong>：比较LLMs在不同约束建模框架下的性能，以确定哪种框架更适合LLMs生成约束模型。</li>
<li><strong>方法</strong>：选择了三种不同的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。使用CP-Bench数据集中的101个组合问题，对8种不同的LLMs进行了评估<br>^6.1^<br>。</li>
<li><strong>结果</strong>：Python基础框架（CPMpy和OR-Tools）的性能优于领域特定语言（MiniZinc）。具体来说，CPMpy在详细系统提示（Level 3）下达到了最高的准确率<br>^6.2.1^<br>。这表明Python基础框架更适合LLMs生成约束模型。</li>
</ul>
<p>实验二：不同系统提示对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同详细程度的系统提示对LLMs性能的影响。</li>
<li><strong>方法</strong>：设计了三种不同详细程度的系统提示：基本提示（Level 1）、包含建模指南的提示（Level 2）和包含框架文档的详细提示（Level 3）。在三种不同的约束建模框架下，对8种LLMs进行了评估<br>^4.1^<br>。</li>
<li><strong>结果</strong>：提供更详细的系统提示（尤其是包含框架文档的提示）能够提高LLMs的建模准确性<br>^6.2.2^<br>。这表明详细的系统提示有助于LLMs更好地理解和生成约束模型。</li>
</ul>
<p>实验三：推理时计算方法对LLMs性能的影响</p>
<ul>
<li><strong>目的</strong>：评估不同的推理时计算方法对LLMs性能的影响。</li>
<li><strong>方法</strong>：测试了四种不同的推理时计算方法：基线方法（直接评估LLMs生成的第一个模型）、检索增强型上下文学习（RAICL）、重复采样（Repeated Sampling）和自我验证（Self-Verification）。选择了4种LLMs（gpt-4.1-mini、DeepSeek-V3、QwQ-32B、Llama-3.3-70B）进行评估<br>^6.2.3^<br>。</li>
<li><strong>结果</strong>：</li>
<li><strong>RAICL</strong>：在上下文中添加示例对LLMs的性能没有帮助，反而降低了准确性<br>^6.2.3^<br>。</li>
<li><strong>重复采样</strong>：通过增加温度参数值，生成多个候选模型，并通过解决方案多数投票来选择最可靠的模型。这种方法在所有测试的LLMs上都提高了性能，平均准确率提高了约10%<br>^6.2.3^<br>。</li>
<li><strong>自我验证</strong>：允许LLMs迭代地评估和修正其生成的模型，以提高模型的正确性。自我验证也显著提高了准确性，与重复采样方法的效果相当<br>^6.2.3^<br>。</li>
<li><strong>综合方法</strong>：将重复采样和自我验证方法结合起来，进一步提高了LLMs在约束建模任务中的性能。在最佳配置下，gpt-4.1-mini模型达到了71%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>实验四：错误类型分析</p>
<ul>
<li><strong>目的</strong>：分析LLMs在生成约束模型时所犯错误的类型，以了解不同系统提示对错误的影响。</li>
<li><strong>方法</strong>：将错误分为两类：可检测错误（如代码无法执行、输出解析问题或超时）和建模错误（如生成的模型不可满足或产生错误的解决方案）。对不同系统提示配置下的错误进行了统计<br>^6.2.2^<br>。</li>
<li><strong>结果</strong>：随着系统提示详细程度的增加，可检测错误减少，而建模错误增加<br>^6.2.2^<br>。这表明更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误。</li>
</ul>
<p>这些实验综合评估了LLMs在约束建模中的性能，并探索了不同建模框架、系统提示和推理时计算方法对性能的影响。实验结果为如何利用LLMs简化约束编程的建模过程提供了重要的见解。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文在探索利用大型语言模型（LLMs）进行约束建模方面已经取得了显著的成果，但仍有一些可以进一步探索的点，以推动这一领域的研究进展。以下是一些潜在的研究方向：</p>
<p>1. <strong>多轮交互式建模</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在单次生成模型的准确性上，但在实际应用中，建模过程往往需要多次迭代和用户反馈。</li>
<li><strong>研究方向</strong>：探索多轮交互式建模，允许用户和系统之间进行多次交互，逐步改进和细化模型。这可以通过设计更复杂的对话系统来实现，系统可以根据用户的反馈动态调整模型<br>^7^<br>。</li>
</ul>
<p>2. <strong>工业级问题的应用</strong></p>
<ul>
<li><strong>背景</strong>：现有的基准数据集（如CP-Bench）虽然多样化，但主要来源于学术领域，与实际工业问题仍有差距。工业问题通常涉及更多的数据、更复杂的约束和更详细的描述。</li>
<li><strong>研究方向</strong>：与工业界合作，收集和构建更接近实际应用场景的组合问题数据集。这将有助于评估LLMs在处理更复杂、更实际问题时的表现<br>^24^<br>。</li>
</ul>
<p>3. <strong>模型效率优化</strong></p>
<ul>
<li><strong>背景</strong>：虽然论文主要关注模型的正确性，但在实际应用中，模型的求解效率同样重要。高效的模型可以显著减少求解时间和计算资源的消耗。</li>
<li><strong>研究方向</strong>：探索如何利用LLMs学习生成更高效的约束模型，例如通过引入启发式方法、动态约束调整或模型重构技术<br>^32^<br>。</li>
</ul>
<p>4. <strong>监督式微调</strong></p>
<ul>
<li><strong>背景</strong>：论文中提到，虽然CP-Bench数据集为评估LLMs提供了基础，但目前的数据集主要用于测试，而不是训练。</li>
<li><strong>研究方向</strong>：构建一个包含约束模型和问题描述的训练数据集，用于对LLMs进行监督式微调。这可能需要大量的标注数据，但有望进一步提高LLMs在约束建模任务中的性能<br>^4^<br>。</li>
</ul>
<p>5. <strong>跨领域知识迁移</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在训练过程中接触了大量的文本数据，这些数据可能包含不同领域的知识。然而，如何有效地将这些跨领域的知识迁移到约束建模任务中仍是一个未解决的问题。</li>
<li><strong>研究方向</strong>：研究如何利用LLMs的跨领域知识，通过迁移学习或领域适应技术，提高其在特定约束建模任务中的表现<br>^14^<br>。</li>
</ul>
<p>6. <strong>模型的可解释性</strong></p>
<ul>
<li><strong>背景</strong>：在实际应用中，除了模型的准确性和效率外，模型的可解释性也非常重要。用户需要理解模型是如何工作的，以及为什么会产生特定的解决方案。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的可解释性，例如通过生成详细的解释文本或可视化工具<br>^5^<br>。</li>
</ul>
<p>7. <strong>与其他AI技术的结合</strong></p>
<ul>
<li><strong>背景</strong>：LLMs在约束建模中的应用可以与其他AI技术（如机器学习、强化学习等）相结合，以进一步提高建模和求解的效率和准确性。</li>
<li><strong>研究方向</strong>：研究如何将LLMs与这些技术相结合，例如通过强化学习来优化模型的生成过程，或者利用机器学习模型来预测模型的性能<br>^1^<br>。</li>
</ul>
<p>8. <strong>模型的泛化能力</strong></p>
<ul>
<li><strong>背景</strong>：当前的研究主要集中在特定的数据集和问题类型上，但实际应用中需要模型具有更强的泛化能力，能够处理各种不同类型的问题。</li>
<li><strong>研究方向</strong>：探索如何提高LLMs生成的约束模型的泛化能力，例如通过引入更多的数据增强技术、设计更通用的建模框架或开发新的评估指标<br>^28^<br>。</li>
</ul>
<p>这些研究方向不仅有助于进一步提高LLMs在约束建模中的表现，还可能推动约束编程技术在更广泛领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了CP-Bench，这是一个用于评估大型语言模型（LLMs）在约束建模能力上的新基准数据集。文章的核心目标是解决约束编程（CP）中约束建模的瓶颈问题，通过利用LLMs将自然语言描述的组合问题转化为可执行的约束模型，从而降低CP的使用门槛，使其更易于被非专家用户接受和应用。以下是文章的主要内容概述：</p>
<p>背景知识</p>
<ul>
<li><strong>组合问题与约束编程</strong>：组合问题在众多实际应用中普遍存在，如物流、调度和网络设计等。CP作为一种解决问题的范式，通过声明式方法让用户指定解决方案必须满足的约束，而不是详细说明寻找解决方案的具体步骤。然而，将问题描述转化为正式的CP模型的过程复杂，需要深厚的应用领域知识和对建模框架的熟练掌握，这限制了CP技术的广泛应用<br>^1^<br>。</li>
<li><strong>大型语言模型的应用</strong>：近年来，LLMs在自然语言处理和代码生成方面取得了显著进展，为创建能够辅助或自动化建模过程的工具提供了可能。然而，将LLMs应用于约束建模面临着独特挑战，如声明式建模的复杂性、建模选择的组合性质以及验证生成模型正确性的困难<br>^4^<br>。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>CP-Bench数据集</strong>：为了解决现有评估数据集在多样性和复杂性上的不足，作者构建了CP-Bench，包含101个来自CP社区的多样化组合问题。这些问题涵盖了优化和满足问题，具有广泛的决策变量、约束和241种不同的约束类型<br>^5^<br>。</li>
<li><strong>建模框架的比较</strong>：文章比较了三种不同抽象层次和接口类型的约束建模框架：MiniZinc（高级别、领域特定语言）、CPMpy（高级别、基于Python的库）和OR-Tools CP-SAT求解器的Python接口（低级别）。通过这些框架，作者评估了LLMs在不同条件下的建模能力<br>^1^<br>。</li>
<li><strong>系统提示与推理时计算方法</strong>：为了提高LLMs生成有效约束模型的能力，文章系统地评估了基于提示和推理时计算的方法。这些方法包括不同详细程度的系统提示、检索增强型上下文学习（RAICL）、重复采样和自我验证<br>^4^<br>。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>实验设置</strong>：作者选择了8种不同的LLMs进行评估，这些模型在参数数量、上下文窗口大小、是否包含推理能力等方面存在差异<br>^6.1^<br>。</li>
<li><strong>实验结果</strong>：</li>
<li><strong>建模框架的影响</strong>：实验结果表明，使用Python基础框架（CPMpy和OR-Tools）的LLMs在生成约束模型时的准确性高于使用领域特定语言（MiniZinc）的模型。此外，提供更详细的系统提示（尤其是包含框架文档的提示）能够进一步提高LLMs的建模准确性<br>^6.2.1^<br>。</li>
<li><strong>系统提示的作用</strong>：更详细的系统提示有助于LLMs生成更可执行的代码，但也可能导致逻辑上的建模错误增加<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的效果</strong>：重复采样和自我验证方法显著提高了LLMs的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>Python基础框架的优势</strong>：Python基础框架（CPMpy和OR-Tools）更适合LLMs生成约束模型，这可能与LLMs训练过程中接触的大量Python代码有关<br>^6.2.1^<br>。</li>
<li><strong>系统提示的重要性</strong>：详细的系统提示，尤其是包含框架文档的提示，能够提高LLMs的建模准确性<br>^6.2.2^<br>。</li>
<li><strong>推理时计算方法的有效性</strong>：重复采样和自我验证方法能够显著提高LLMs在约束建模任务中的性能，而RAICL方法在这种情况下并不有效<br>^6.2.3^<br>。</li>
<li><strong>LLMs在约束建模中的潜力与局限</strong>：尽管LLMs在约束建模中展现出了潜力，但仍然需要人类专家的验证，尤其是在关键任务中。文章中最强的性能配置（结合重复采样和自我验证的gpt-4.1-mini模型）达到了70%的准确率<br>^6.2.3^<br>。</li>
</ul>
<p>未来工作</p>
<p>文章提出了未来可能的研究方向，包括构建更接近实际应用场景的组合问题数据集、探索多轮交互式建模、研究模型效率优化、进行监督式微调、实现跨领域知识迁移、提高模型的可解释性、结合其他AI技术以及增强模型的泛化能力<br>^7^<br>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是CP-Bench%3A%20Evaluating%20Large%20Language%20Models%20for%20Constraint%20Modelling，链接是https://arxiv.org/pdf/2506.06052，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.06052。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kostis Michailidis, Dimos Tsouros, Tias Guns</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.06052v3.pdf">https://arxiv.org/pdf/2506.06052v3.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.06052v3">https://papers.cool/arxiv/2506.06052v3</a></p>
<p><strong>Published</strong>: 2025-06-06T12:56:02Z</p>
<hr>
<h3 id="5-FreeFix-Boosting-3D-Gaussian-Splatting-via-Fine-Tuning-Free-Diffusion-Models"><a href="#5-FreeFix-Boosting-3D-Gaussian-Splatting-via-Fine-Tuning-Free-Diffusion-Models" class="headerlink" title="5. FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models"></a>5. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20857v1">FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models</a></h3><p>Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>神经辐射场（Neural Radiance Fields）和三维高斯散点（3D Gaussian Splatting）推进了新视角合成的发展，但仍依赖于密集输入，并且在外推视角下常常性能下降。近期的方法利用生成模型，如扩散模型，为合成过程提供额外的监督，但在泛化能力和保真度之间存在权衡：对扩散模型进行微调以去除伪影可以提升保真度，但存在过拟合风险；而无需微调的方法则能保持较强的泛化能力，但通常保真度较低。我们提出了 FreeFix，这是一种无需微调的方法，通过利用预训练的图像扩散模型增强外推渲染，推动了这一权衡的边界。我们提出了一种交错的 2D-3D 精细化策略，展示了图像扩散模型可用于一致性精细化，而无需依赖高成本的视频扩散模型。此外，我们更深入地研究了用于 2D 精细化的引导信号，并提出了一种逐像素置信度掩码用于识别不确定区域以进行有针对性的改进。跨多个数据集的实验表明，FreeFix 提升了多帧一致性，性能可与基于微调的方法相比甚至超越，同时保持了较强的泛化能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“外推视角渲染质量退化”这一 3D Gaussian Splatting（3DGS）的固有问题提出解决方案。具体而言：</p>
<ul>
<li>3DGS 在训练视角稀疏或外推视角（远离训练分布）时会出现明显伪影，降低合成图像的真实感与几何保真度。</li>
<li>现有方法要么引入手工正则项，要么借助扩散模型（DM）提供额外监督；后者若进行微调可提升保真度，但会牺牲泛化性并带来高昂成本；若保持微调-free 则泛化性强，却常因缺乏有效引导而效果不足。</li>
</ul>
<p>因此，论文旨在<strong>在不微调任何扩散模型的前提下</strong>，利用预训练图像 DM 显著提升 3DGS 外推视角的渲染质量，同时保持跨场景泛化能力，并确保多帧一致性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 FreeFix 直接相关的研究可归纳为三条主线，均围绕“提升 Novel View Synthesis（NVS）质量”展开，并特别关注 3D Gaussian Splatting（3DGS）在外推视角下的伪影问题。</p>
<ol>
<li>基于几何/物理先验的 NVS 正则化</li>
</ol>
<ul>
<li>引入外部传感器：LiDAR 或 RGB-D 深度图作为弱监督，如 Splatam<br>8<br>、Pings<br>21<br>、Spurfies<br>23<br>。</li>
<li>利用场景结构假设：地面平面<br>5, 10, 52<br>、天空穹顶<br>4, 43<br>、室内正交墙面<br>48<br>等手工先验，代表工作 PlanarRF<br>33<br>、MonoSDF<br>48<br>、AutoSplat<br>10<br>。<br>共同点：无需生成模型，但先验固定、无“幻觉”能力，对外推区域伪影抑制有限。</li>
</ul>
<ol>
<li>基于生成式先验的 NVS 增强<br>2-a) 需要微调扩散模型（DM）</li>
</ol>
<ul>
<li>驾驶场景专用：StreetCrafter<br>41<br>（需 LiDAR）、ReconDreamer<br>20<br>、STAG-1<br>35<br>等，在自建驾驶数据集上微调视频 DM。</li>
<li>通用场景：Difix3D+<br>37<br>将 SD-Turbo 在混合 3D 数据集 DLV3D 上微调，兼顾多域但成本高昂。<br>特点：保真度高，泛化性受限于微调域，数据与算力开销大。</li>
</ul>
<p>2-b) 微调-free 的零样本 DM 利用</p>
<ul>
<li>视频 DM：ViewExtrapolator<br>16<br>、NVS-Solver<br>45<br>直接把外推图像作为去噪引导，依赖时序注意力保证一致性，但计算重、易糊。</li>
<li>图像 DM：IP-Adapter<br>44<br>等可接受图像提示，却非为 NVS 设计，多帧一致性差。<br>特点：泛化性强，但缺乏有效引导，质量通常低于微调方法。</li>
</ul>
<ol>
<li>不确定性/置信度估计在 3D 重建中的应用</li>
</ol>
<ul>
<li>FisherRF<br>7<br>、PUP-3DGS<br>6<br>将 Fisher 信息用于 NeRF/3DGS 的主动视角选择或剪枝，提供像素级不确定性。<br>FreeFix 借鉴其思路，首次将“数值稳定的置信度”而非原始不确定性作为扩散去噪的像素级引导权重，实现伪影区域精准重绘。</li>
</ul>
<p>综上，FreeFix 与上述研究的最大区别在于：</p>
<ul>
<li>不微调任何 DM，保持 web-scale 预训练泛化性；</li>
<li>仅用图像 DM，通过“交错 2D–3D refine + 多级置信度掩码”实现视频级一致性；</li>
<li>把 Fisher 信息导出的<strong>数值稳定置信度</strong>而非传统 opacity/uncertainty 作为去噪引导，填补微调-free 方法在保真度上的差距。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>FreeFix 将“外推视角伪影修复”转化为<strong>微调-free 的交错 2D–3D 联合优化</strong>问题，核心手段可概括为三条：</p>
<ol>
<li>交错 2D–3D Refinement（无需视频 DM）<br>沿外推轨迹  T_(ext)=V_0^e,…,V_m^e  顺序处理：</li>
</ol>
<ul>
<li>2D 阶段：用图像扩散模型（IDM）对当前视角渲染图  hat I<em>i^e  去噪，得到精炼帧  hat I</em>(i,f)^e ；</li>
<li>3D 阶段：把  hat I<em>(i,f)^e  与历史精炼帧集合  F</em>(i-1) 、原始训练帧  S_(train)  一起作为监督，微调 3DGS 参数得到  G_i ；</li>
<li>下一视角再基于更新后的  G_i  渲染，循环往复。<br>该过程使“已修复”内容即时写回 3D，后续 2D 去噪始终面对<strong>更一致</strong>的渲染输入，从而用纯图像 DM 实现多帧连贯。</li>
</ul>
<ol>
<li>多级像素置信度引导（Certainty-Guided Denoising）</li>
</ol>
<ul>
<li>将 Fisher 信息矩阵的迹渲染为“不确定性” overline C_(V;G) ，取其指数互补得到数值稳定的<strong>确定性</strong></li>
</ul>
<p>C^(γ<em>c)</em>(V;G)=exp!l(-γ<em>c,overline C</em>(V;G)r),quad γ_c∈γ_1,…,γ_k</p>
<ul>
<li>与 opacity 乘积获得置信度掩码  M^(γ<em>c)</em>(V;G) ；早期去噪用较小  γ_c  保持整体结构，后期用较大  γ_c  精准抑制伪影。</li>
<li>去噪预测按掩码混合：</li>
</ul>
<p>x_(t,g)^0=M_codot x^r_0+(1-M_c)odot x_t^0</p>
<p>高置信区域强制保留原始渲染细节，低置信区域允许 DM 自由重绘。</p>
<ol>
<li>整体引导与颜色仿射矫正</li>
</ol>
<ul>
<li>对天空、地面等弱纹理区，置信度普遍低，仅用  M_c  易导致帧间漂移。FreeFix 在<strong>最早去噪步</strong>额外引入全局结构提示：</li>
</ul>
<p>x<em>(t,g)^0=M_codot x^r_0+(1-M_c)odotl(β M</em>α x^r<em>0+(1-β M</em>α)x_t^0r)</p>
<p>其中  M_α  为渲染不透明度， β  控制整体引导强度，保证弱纹理区一致性。</p>
<ul>
<li>3D 微调阶段对每帧精炼图学习可优化仿射矩阵  A_f∈mathbb R^(3×3),,A_b∈mathbb R^(3×1) ，抵消 DM 轻微色偏，防止多轮累积发灰/发糊。</li>
</ul>
<p>通过“<strong>交错更新 + 像素级置信引导 + 全局结构保真</strong>”三管齐下，FreeFix 在不微调任何扩散模型的前提下，把外推视角的 PSNR、SSIM、LPIPS 同时推向 SOTA，并在多数据集上持平甚至超越需微调的同类方法。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>LLFF</strong>、<strong>Mip-NeRF 360</strong> 与 <strong>Waymo</strong> 三个公开数据集上进行了系统实验，覆盖前向、物体中心与街景三种典型场景，具体工作如下：</p>
<ol>
<li>主实验：与现有方法的全面对比<br>1.1 微调-free 基线</li>
</ol>
<ul>
<li>ViewExtrapolator（视频 DM，opacity 引导）</li>
<li>NVS-Solver（视频 DM，纯 2D 渲染，无 3DGS）</li>
</ul>
<p>1.2 微调-based 基线</p>
<ul>
<li>Difix3D+（SD-Turbo 在 DLV3D 上微调）</li>
<li>StreetCrafter（街景专用，需 LiDAR 微调）</li>
</ul>
<p>1.3 评估指标</p>
<ul>
<li>LLFF / Mip-NeRF：PSNR、SSIM、LPIPS</li>
<li>Waymo（无 GT）：KID</li>
</ul>
<p>1.4 结果</p>
<ul>
<li>FreeFix（Flux 主干）在 12 个场景里 <strong>9 项第一</strong>，其余场景与最佳微调方法差距 &lt;0.3 dB；</li>
<li>在 Waymo 上 KID 平均 <strong>0.147</strong>，低于 StreetCrafter 的 0.157 与 Difix3D+ 的 0.143；</li>
<li>定性对比显示 FreeFix 伪影更少、纹理更清晰，图 5、图 7、图 9 给出多组视觉样例。</li>
</ul>
<ol>
<li>消融实验<br>2.1 扩散模型主干对比</li>
</ol>
<ul>
<li>同用 SVD：FreeFix + Certainty 比 ViewExtrapolator + Opacity 在 Garden 场景 PSNR ↑1.17 dB。</li>
<li>同框架下：Flux &gt; SDXL &gt; SVD，验证先进 IDM 即可带来额外增益，无需视频 DM。</li>
</ul>
<p>2.2 核心模块消融（Garden 场景）</p>
<ul>
<li>原始 Flux：19.23 / 0.390 / 0.389</li>
<li>+置信度引导：19.32 / 0.435 / 0.349</li>
<li>+交错 refine：19.65 / 0.517 / 0.293</li>
<li>+整体引导：19.72 / 0.520 / 0.287<br>逐步添加各组件均带来稳定提升，证实三者互补。</li>
</ul>
<p>2.3 不确定性 vs 确定性掩码<br>直接用 1−M¯ 作为引导，Garden 场景 PSNR 降至 19.30，且视觉出现明显帧间不一致（图 10）。</p>
<p>2.4 颜色仿射矩阵<br>去掉仿射变换 PSNR 略升 0.3 dB，但 SSIM↓0.003、LPIPS↑0.03，并产生漂浮伪影（图 12），主观质量下降。</p>
<ol>
<li>扩展分析</li>
</ol>
<ul>
<li>单场景量化：表 4–6 给出 LLFF、Mip-NeRF、Waymo 共 21 个序列的逐项指标，FreeFix 在 17 个序列取得 Top-1。</li>
<li>与 NVS-Solver 的侧面对比：图 11 显示 NVS-Solver 因深度估计不准出现重影，FreeFix 借助 3DGS 几何一致性避免该问题。</li>
<li>训练/测试视角可视化：补充图 8 展示了外推轨迹设计，确保测试相机明显偏离训练分布。</li>
</ul>
<p>综上，实验从<strong>量化指标、视觉质量、模块消融、替代掩码、颜色矫正、不同主干</strong>六个维度验证了 FreeFix 的有效性，证明其仅用预训练图像扩散模型即可达到乃至超越现有微调方法的修复质量。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下问题在 FreeFix 框架内尚未完全解决，可作为后续工作的直接切入点：</p>
<ol>
<li>置信度估计的泛化与自适应</li>
</ol>
<ul>
<li>当前 γc 需人工设定多级 schedule，能否在外推过程中<strong>在线估计</strong>场景相关 γc 或直接用可学习网络预测？</li>
<li>除 Fisher 信息外，可探索渲染-图像一致性误差、光流重投影误差等多源不确定性融合，提升对<strong>非高斯分布伪影</strong>的敏感度。</li>
</ul>
<ol>
<li>更紧密的 2D-3D 联合优化</li>
</ol>
<ul>
<li>交错 refine 采用“2D 去噪 → 3D 拟合”两阶段循环，能否将梯度从 2D DM 直接反向传播至 3D 高斯参数，实现<strong>端到端可微</strong>的单次优化？</li>
<li>引入可微分 VAE 或 latent-nerf 思路，把去噪 latent space 与 3DGS 的渲染 latent 空间对齐，减少多次 VAE 编解码带来的信息损失。</li>
</ul>
<ol>
<li>动态场景与时间一致性</li>
</ol>
<ul>
<li>FreeFix 目前针对静态场景，若输入为动态 3DGS（4D Gaussian Splatting），需同时处理<strong>几何形变与外观变化</strong>。可将置信度扩展为时空体素，或引入可变形运动先验。</li>
<li>探索轻量级视频 DM 的<strong>早期层特征</strong>作为时间一致性损失，而非直接用 heavy VDM 推理。</li>
</ul>
<ol>
<li>失败案例的鲁棒性</li>
</ol>
<ul>
<li>当外推视角出现“大面积不可见区域”或“严重遮挡”时，置信度本身也失效。可引入<strong>基于语言-视觉大模型的语义幻觉模块</strong>，在完全无纹理区域生成合理内容，再反向约束 3DGS。</li>
<li>设计<strong>置信度阈值自动停机</strong>机制：若平均置信度低于某值，则放弃该帧 3D 更新，避免污染全局模型。</li>
</ul>
<ol>
<li>计算效率与内存优化</li>
</ol>
<ul>
<li>当前 3DGS 每帧都进行 1000-1500 步 refine，耗时明显。可研究：<br>– 仅对“高不确定高斯”子集进行局部更新；<br>– 引入稀疏-密集双路径：稀疏路径快速定位伪影区域，密集路径仅在这些区域做高斯增删与优化。</li>
<li>多级 γc 需要多次前向 DM，能否通过<strong>单步扩散蒸馏</strong>（如 SD-Turbo、Consistency Model）将多级引导压缩为 1-2 步，实现实时修复。</li>
</ul>
<ol>
<li>跨模态条件扩展</li>
</ol>
<ul>
<li>当前仅使用 RGB 渲染作为引导，若同时有<strong>稀疏 LiDAR、语义分割、法向图</strong>，可把它们纳入置信度计算，形成多模态不确定度场。</li>
<li>探索<strong>文本-提示修复</strong>：用户用自然语言指出“移除天空的模糊塔影”，通过跨模态 DM 将文本指令转化为区域置信度下降，实现交互式外推编辑。</li>
</ul>
<ol>
<li>理论分析</li>
</ol>
<ul>
<li>FreeFix 的交错迭代可视为<strong>非凸 2D-3D 交替优化</strong>，需研究其收敛条件与误差累积上界，为“多少步 refine 足够”提供理论指导。</li>
<li>置信度引导与扩散采样分布之间的<strong>贝叶斯最优性</strong>：给定渲染似然与 DM 先验，推导最优混合权重 Mc，而非经验线性插值。</li>
</ul>
<p>通过上述方向的深入，可望在<strong>自动化程度、动态场景、实时性能、交互编辑</strong>等维度进一步释放 FreeFix 的潜力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：3D Gaussian Splatting 在外推视角渲染时伪影严重；现有扩散模型方案要么微调后失去泛化性，要么无微调但缺乏有效引导，保真度不足。</li>
<li><p><strong>思路</strong>：提出 FreeFix——<strong>完全无需微调</strong>的图像扩散模型增强框架，通过“2D-3D 交错精炼 + 像素级置信度引导”在保持泛化的同时提升外推质量。</p>
</li>
<li><p><strong>关键方法</strong></p>
</li>
</ul>
<ol>
<li>沿外推轨迹顺序执行：</li>
</ol>
<ul>
<li>2D 阶段用 IDM 去噪，高置信区域保留原始渲染，低置信区域允许重绘；</li>
<li>3D 阶段将精炼图像即时写回 3DGS，后续视角基于更新后模型渲染，实现多帧一致。</li>
</ul>
<ol>
<li>引入<strong>数值稳定的确定性掩码</strong>（Fisher 信息互补形式）替代传统 opacity/uncertainty，按去噪阶段动态切换多级 γc，精准定位伪影。</li>
<li>早期步额外加入全局结构引导，并对每帧学习可优化仿射矩阵消除 DM 色偏，防止误差累积发灰。</li>
</ol>
<ul>
<li><strong>实验</strong></li>
<li>在 LLFF、Mip-NeRF 360、Waymo 上对比微调/无微调共 5 条基线；FreeFix 用 SDXL/Flux 主干取得 <strong>21 场景里 17 项第一</strong>，平均 PSNR↑1-2 dB，KID/SSIM/LPIPS 均持平或优于最佳微调方法。</li>
<li>消融验证：交错 refine、置信度引导、全局引导、颜色仿射四模块逐项提升；确定性掩码显著优于不确定性掩码。</li>
<li><strong>结论</strong>：FreeFix 首次证明<strong>仅依赖预训练图像扩散模型、无需任何微调</strong>，即可将 3DGS 外推渲染质量推至与微调方法相当甚至更高的水平，同时保持跨场景泛化能力。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是FreeFix%3A%20Boosting%203D%20Gaussian%20Splatting%20via%20Fine-Tuning-Free%20Diffusion%20Models，链接是https://arxiv.org/pdf/2601.20857，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20857。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hongyu Zhou, Zisen Shao, Sheng Miao, Pan Wang, Dongfeng Bai, Bingbing Liu, Yiyi Liao</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20857v1.pdf">https://arxiv.org/pdf/2601.20857v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20857v1">https://papers.cool/arxiv/2601.20857v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:03Z</p>
<hr>
<h3 id="6-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models-1"><a href="#6-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models-1" class="headerlink" title="6. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models"></a>6. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20856v1">SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</a></h3><p>Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型的能力在复杂推理任务上得到了越来越多的测试，但它们的长远规划能力尚未得到广泛研究。在本工作中，我们对最先进的大型推理模型（LRMs）的规划和长远推理能力进行了系统评估。我们提出了一个基于推箱子（Sokoban）谜题的新型基准测试，该测试经过刻意简化，以将长远规划与状态持久性隔离开来。我们的研究结果显示，当需要超过25步才能达到解决方案时，规划性能会显著下降，这表明前向规划能力存在根本性限制。我们还表明，为LRMs配备规划领域定义语言（PDDL）的解析、验证和求解工具可以带来适度的改进，这表明其架构本身存在限制，可能仅靠测试时的规模扩展无法克服。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统评估当前大型推理模型（Large Reasoning Models, LRMs）在**长程规划（long-horizon planning）<strong>与</strong>长程推理（long-horizon reasoning）**上的能力边界。核心问题可以概括为：</p>
<blockquote>
<p><strong>当解空间深度增加、但分支因子被刻意压到最低时，LRMs 是否仍能维持连贯、可行且最优的动作序列？</strong></p>
</blockquote>
<p>为隔离“深度”与“复杂性”因素，作者构造了极简的一维 Sokoban 走廊谜题：</p>
<ul>
<li>仅 1 个箱子、1 个目标、1 条直线通道；</li>
<li>唯一可变参数是走廊长度 ℓ，它直接决定最优解长度（≈ℓ 步）；</li>
<li>无外部记忆、无中间反馈、无状态验证，完全依赖模型内部表示。</li>
</ul>
<p>通过该基准，论文试图回答以下子问题：</p>
<ol>
<li>模型在无需外部工具时，究竟能稳定规划到多远的 horizon？</li>
<li>当 horizon 超过某一阈值（≈25–30 步）时，失效模式是“搜索错误”还是“更底层的计数/状态跟踪错误”？</li>
<li>若给模型提供 PDDL 解析器与外部求解器（LLM-Modulo 框架），能否弥补其固有缺陷？</li>
<li>上述改进若存在，是否足以克服长程动作表示与空间 grounding 的瓶颈？</li>
</ol>
<p>综上，论文并非追求更复杂的规划域，而是<strong>用极简环境放大“深度”维度</strong>，从而暴露 LRMs 在长序列状态一致性上的结构性局限，并验证“测试时扩展+符号工具”是否足以突破该局限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四大脉络，均围绕“LLM 规划能力”与“Sokoban 作为诊断环境”展开：</p>
<ol>
<li><strong>通用规划基准与 LLM 失效诊断</strong></li>
</ol>
<ul>
<li><strong>PlanBench</strong>（Valmeekam et al., 2022, 2023a, 2025）：PDDL 格式的 BlocksWorld、Logistics 等经典域，发现 LLM 随 horizon 增加迅速退化，且错误集中在无效动作与循环。</li>
<li><strong>8-puzzle 文本版</strong>（Schepanowski &amp; Ling, 2025）：证明即使状态空间仅 181440，LLM 仍因“弱内部状态表示”与“误导式启发搜索”而频繁产生非法移动或提前终止。</li>
<li><strong>字符级计数任务</strong>（Karpathy, 2024；Xu &amp; Ma, 2025；CharBench, Uzan &amp; Pinter, 2025）：揭示“strawberry 有几个 r”类失败源于 tokenization 之外的长度-计数表征缺陷，与本文“走廊计数漂移”论点直接呼应。</li>
</ul>
<ol>
<li><strong>LLM-Modulo / 工具增强规划</strong></li>
</ol>
<ul>
<li><strong>LLM⇄PDDL 管道</strong>（Silver et al., 2022；Oswald et al., 2024；La Malfa et al., 2025）：让模型仅生成 domain 或 problem 文件，由外部求解器完成搜索，显著降低非法动作率。</li>
<li><strong>PlanBench-o1 实验</strong>（Valmeekam et al., 2025）：o1-preview 裸机成功率 10–12%，接入 Fast-Downward 后提升至 43%，但计算成本增加 10× 以上；本文 LLM-Modulo 结果与之趋势一致，进一步指出“即使符号求解，空间量纲错误仍存”。</li>
</ul>
<ol>
<li><strong>Sokoban 作为认知诊断环境</strong></li>
</ol>
<ul>
<li><strong>IPC-2023 Sokoban track</strong>（Taitler et al., 2024）：正式将 Sokoban 纳入国际规划竞赛，强调其“无捷径、无模式化解”特性。</li>
<li><strong>RNN 专用求解器</strong>（Taufeeque et al., 2024；Jolicoeur-Martineau, 2025）：在 1000+ 关卡达到 &gt;95% 成功率，表明非 Transformer 架构可通过循环隐状态掌握空间规则，反衬 LLM 的隐状态短板。</li>
<li><strong>o1-preview 裸机实验</strong>（Valmeekam et al., 2025）：在标准 2D Sokoban 仅 10–12% 成功率，与本文“一维走廊”结果形成对照，说明“简化空间并未显著缓解长程退化”。</li>
</ul>
<ol>
<li><strong>长程推理与“漫游”现象的理论解释</strong></li>
</ol>
<ul>
<li><strong>“Wandering vs. Systematic Exploration”框架</strong>（Lu et al., 2025）：提出 LLM 推理轨迹本质是“无 visited-state 集的随机游走”，在深度线性链上表现为重复循环与计数漂移，与本文 token 上限耗尽、前缀准确率指数下降现象完全一致。</li>
<li><strong>工作记忆容量度量</strong>（Zhang et al., 2024）：用 dual-task 方式证明 LLM 的“有效工作记忆”不超过 7–9 个 token 级状态，与本文 25 步左右断崖式失效点数量级一致。</li>
</ul>
<p>综上，既有研究共同指向：<strong>LLM 的规划瓶颈并非高层搜索策略，而是底层状态计数、空间 grounding 与工作记忆持久化的结构性缺陷</strong>。本文通过“极简走廊”设计，将上述观察从复杂域剥离出来，首次给出可量化、可复现的长程失效阈值。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出一种“治愈”长程规划缺陷的新架构，而是设计了一套<strong>极简诊断协议</strong>，通过“控制变量→量化失效→工具增强→对比分析”四步，把问题拆成可测量的现象，从而给出改进方向与上限证据。</p>
<ol>
<li><strong>控制变量：构造 SokoBench 走廊谜题</strong></li>
</ol>
<ul>
<li>仅留“长度 ℓ”一个自由参数，最优解长度 ≈ℓ，分支因子=1。</li>
<li>旋转增广（0°/90°/180°/270°）降低预训练泄露风险。</li>
<li>由此将“长程”与“空间复杂性”解耦，任何失效都可归因于<strong>深度+内部状态跟踪</strong>。</li>
</ul>
<ol>
<li><strong>量化失效：1-shot 裸机实验</strong></li>
</ol>
<ul>
<li>指标：Exact-match Accuracy、Prefix Accuracy、Manhattan Distance、token 消耗曲线。</li>
<li>发现：<br>– 25 ≤ ℓ ≤ 30 是断崖阈值，之后 Accuracy 指数下降。<br>– 错误主因是“计数漂移”与“循环输出”，与 Lu et al. 的“wandering”假说一致。<br>– Token 消耗随 ℓ 线性增加，但正确解的方差远小于错误解，说明失败路径发散。</li>
</ul>
<ol>
<li><strong>工具增强：LLM-Modulo 管道</strong></li>
</ol>
<ul>
<li>给模型只留两项任务：<br>① 把 ASCII 图翻译成 PDDL problem 文件；<br>② 调用外部 Unified-Planning 求解器（Fast-Downward / PyperPlan）。</li>
<li>结果：<br>– GPT-5-mini 裸机 25 步后≈0，工具增强后 100 步仍维持 30–40% Accuracy。<br>– 但垂直走廊显著弱于水平走廊，说明“空间量纲估计”错误未被完全消除。<br>– 7/320 次出现 PDDL 语法错，其余失败均因“problem 文件与真实地图尺寸不符”，即<strong>内部计数偏差外溢到符号描述</strong>。</li>
</ul>
<ol>
<li><strong>对比分析与上限证据</strong></li>
</ol>
<ul>
<li>工具增强仅带来<strong>边际提升</strong>，未能突破 25–30 步的“软上限”，证明缺陷<strong>架构层</strong>而非<strong>搜索层</strong>。</li>
<li>给出明确结论：测试时扩展 + 符号求解器<strong>不足以</strong>克服缺乏“显式状态-转移记忆”的硬伤；未来需引入<br>– 显式 visited-state 缓存，<br>– 可学习的计数/坐标专用模块，<br>– 或直接在隐状态内加入符号接地机制。</li>
</ul>
<p>通过上述四步，论文把“长程规划难”从经验观察转化为<strong>可复现指标</strong>，并用量化的方式说明：<strong>在深度线性链场景下，当前 LRM 的失效主因是内部状态漂移，而非高层搜索策略；外部符号工具只能缓解，不能根除。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计 <strong>两套主实验</strong> 与 <strong>一套旋转对照分析</strong>，全部在公开数据集 SokoBench（ℓ=5–100，步长 5，4 种旋转，共 80 张地图）上完成。实验变量、模型、重复次数与核心指标如下：</p>
<p>实验 1 1-shot 裸机推理（无外部工具）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型</td>
<td>DeepSeek-R1-0528、GPT-5-mini、GPT-oss-120B</td>
</tr>
<tr>
<td>Prompt</td>
<td>系统提示 + 表 1 符号映射 + 1 条 4 步示范解</td>
</tr>
<tr>
<td>重复</td>
<td>每 ℓ×旋转 8 次（nt=8，nr=4，总 640 次/模型）</td>
</tr>
<tr>
<td>输出限制</td>
<td>max_tokens=32768，temperature 默认</td>
</tr>
<tr>
<td>记录量</td>
<td>完整推理 trace、最终 字符串、token 消耗</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong></p>
<ol>
<li>Accuracy(ℓ) — Exact string 匹配率</li>
<li>PrefixAccuracy(ℓ) — 逐字符前缀正确率</li>
<li>Manhattan-Distance(ℓ) — 终端坐标与目标 L1 距离</li>
<li>Token 回归 — 正确/错误解的斜率与 R²</li>
</ol>
<p><strong>关键发现</strong></p>
<ul>
<li>ℓ≈25 处出现指数级断崖；</li>
<li>错误解 token 方差远大于正确解，且常因循环输出顶到上限；</li>
<li>旋转 90°/270°（垂直走廊）普遍更低分，揭示“行计数”难度 &gt;“列计数”。</li>
</ul>
<p>实验 2 LLM-Modulo 工具增强</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>可用模型</td>
<td>仅 GPT-5-mini（成本与工具调用稳定性权衡）</td>
</tr>
<tr>
<td>工具链</td>
<td>自定义 MCP 服务器 → PyperPlan / Fast-Downward</td>
</tr>
<tr>
<td>模型任务</td>
<td>生成 PDDL problem 文件 → 调用 solve_problem → 返回 LURD 解</td>
</tr>
<tr>
<td>重复</td>
<td>nt=4，nr=4，总 320 次</td>
</tr>
<tr>
<td>失败处理</td>
<td>语法或语义错误自动重试，最多 3 轮</td>
</tr>
</tbody>
</table>
</div>
<p><strong>观测指标</strong><br>同上 3 项，但额外记录：</p>
<ul>
<li>PDDL 语法错误率</li>
<li>语义不一致率（problem 与真实地图尺寸不符）</li>
<li>工具调用总 token 数（含推理+PDDL 文本）</li>
</ul>
<p><strong>关键发现</strong></p>
<ul>
<li>100 步处 Accuracy 仍≈0.35，较裸机提升 3×，但远未收敛到 1；</li>
<li>7/320 次语法错；其余失败均因“地图长度数错”导致求解器给出短计划；</li>
<li>垂直走廊性能仍显著低于水平走廊，说明<strong>空间量纲估计偏差未被工具消除</strong>。</li>
</ul>
<p>实验 3 旋转细粒度对照（附录 C）</p>
<ul>
<li>仅展示 GPT-5-mini LLM-Modulo 结果；</li>
<li>将 80 张地图按 4 个旋转角度拆分为 4 条子曲线；</li>
<li>量化同一 ℓ 下 0°/90°/180°/270° 的 Accuracy 差异，证实“垂直 vs 水平”效应稳定存在。</li>
</ul>
<p>补充微观分析</p>
<ul>
<li><strong>Token 线性回归</strong>：正确解斜率 29.8–51.1 token/ℓ，错误解斜率 55.2–86.3 token/ℓ，验证“失败轨迹更长且更发散”。</li>
<li><strong>循环检测</strong>：对顶到 32768 上限的 trace 进行子串重复计数，平均 42% 内容属于重复 L/R 语句，直接支持“wandering”假说。</li>
<li><strong>曼哈顿距离热图</strong>：裸机解常出现“玩家最终落在目标上”的非法状态，进一步佐证内部状态未遵守墙/箱约束。</li>
</ul>
<p>综上，实验由<strong>裸机诊断→工具增强→旋转消融→微观痕迹分析</strong>四层组成，共同量化出 LRM 长程规划失效的阈值、特征与改进上限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SokoBench 的“直接续作”或“横向扩展”，均围绕<strong>把当前暴露的单一缺陷拆成多维、可干预变量</strong>展开，以便下一步真正突破 25–30 步的“软上限”。</p>
<p>1 状态表示缺陷的显式干预</p>
<ul>
<li><strong>可学习坐标嵌入</strong>：在 Transformer 隐状态外，显式维护一个  (x,y)  浮游向量，每一步自回归后通过对比损失与符号坐标对齐，检验“外置坐标”能否阻断计数漂移。</li>
<li><strong>Visited-State Cache</strong>：用外部键-值缓存记录已生成状态哈希，采样时禁止重复输出，测试“循环”是否纯粹由无记忆导致。</li>
<li><strong>Delta 预测 vs 绝对预测</strong>：让模型不再预测绝对位置，而是预测“相对位移 + 边界检查”，降低长链绝对坐标累积误差。</li>
</ul>
<p>2 复杂度维度的渐进叠加</p>
<ul>
<li><strong>多箱走廊</strong>：固定 ℓ=50，逐步增加箱子数  b=1…5 ，引入<strong>死锁检测</strong>需求，观察错误模式从“计数漂移”转向“冲突/死锁”的临界  b 。</li>
<li><strong>单侧/双侧分支</strong>：在走廊中段加入  k=1,2  条“死胡同”侧枝，保持解长度不变但增加<strong>搜索宽度</strong>，量化“分支因子”与“深度”对失效的相对贡献。</li>
<li><strong>动态障碍</strong>：在第  t  步随机出现/消失一堵墙，迫使模型<strong>重规划</strong>，测试工作记忆刷新与快速回溯能力。</li>
</ul>
<p>3 感知-动作接口的消融</p>
<ul>
<li><strong>多种编码</strong>：</li>
<li>行号-列号坐标 <code>(r3,c5)</code></li>
<li>相对距离 <code>(Δx,Δy)</code></li>
<li>图像化 2-D 矩阵（利用 VLRM）<br>对比同一地图在不同编码下的 Accuracy 曲线，找出<strong>最敏感格式</strong>，为后续“最佳接地”提供数据。</li>
<li><strong>稀疏 vs 密集奖励提示</strong>：在推理链中每隔  k  步插入人工“状态校验”句子（“此时箱子应在 x=7”），测量<strong>外部提醒</strong>能把阈值 ℓ 提高多少。</li>
</ul>
<p>4 训练与推理策略</p>
<ul>
<li><strong>课程强化微调</strong>：用增量 ℓ 的课程数据对同一基础模型做轻量级 LoRA 微调，验证“见过更长序列”能否把断崖后移，或仅提升记忆。</li>
<li><strong>Best-of-N + 状态验证器</strong>：在推理阶段采样 N 条计划，用外部 Sokoban 模拟器过滤非法解，再选最短合法解；测量 N=1…100 时的收益曲线，给出“纯采样”上限。</li>
<li><strong>递归子目标分解</strong>：强制模型先在高层输出“分阶段里程碑”（如“先把箱推到 x=30”），再对每个子目标展开低层动作；对比端到端方式，检验“显式分层”能否降低有效深度。</li>
</ul>
<p>5 神经-符号混合上限</p>
<ul>
<li><strong>可微规划器端到端训练</strong>：把 Fast-Downward 的编码层换成可微 GNN，生成计划后通过策略梯度微调，看“符号求解器可微化”能否让模型学会<strong>修正 PDDL 参数错误</strong>。</li>
<li><strong>逆向数据增强</strong>：用最优求解器生成百万级走廊解，再反向生成“错误 PDDL 问题”(坐标偏移 ±1)，训练模型<strong>检测并自纠错</strong>，量化“语法+语义”双重验证的剩余错误率。</li>
</ul>
<p>6 横向迁移与外部效度</p>
<ul>
<li><strong>跨域深度链</strong>：保持“单参数难度”理念，换用 Tower of Hanoi、Sliding-Tile、Monkey-and-Bananas 的一维变体，检验“25–30 步阈值”是否<strong>与域无关</strong>，从而验证其为<strong>通用工作记忆极限</strong>。</li>
<li><strong>真实机器人走廊任务</strong>：把 ASCII 走廊映射到实体 1-D 轨道（机器人推箱），观察语言模型生成的计划在高延迟、低精度执行环境下的<strong>鲁棒性差距</strong>，引入“物理漂移”新变量。</li>
</ul>
<p>7 社会-技术风险</p>
<ul>
<li><strong>预训练数据溯源</strong>：对垂直 vs 水平走廊差异做 n-gram 重叠分析，若水平模板在 CommonCrawl 出现频率显著更高，可解释“方向偏差”部分源于<strong>数据不平衡</strong>而非空间认知。</li>
<li><strong>能力-成本权衡公开仪表盘</strong>：建立在线基准，实时记录不同模型在 ℓ=50 走廊的“成功率 / 美元 / 克 CO₂”三元组，为社区提供<strong>绿色 AI</strong> 视角下的规划能力评价。</li>
</ul>
<p>以上方向遵循“单变量介入-可量化-可复现”原则，既可单独成文，也可组合成新的<strong>多维度 SokoBench v2</strong>，目标是<strong>把“长程规划”从黑盒艺术变成白盒实验科学</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可浓缩为“一个基准、两条曲线、三点证据、四步结论”：</p>
<p>一个基准</p>
<ul>
<li><strong>SokoBench</strong>：极简一维 Sokoban 走廊，仅“长度 ℓ”可变，最优解 ≈ℓ 步；</li>
<li>80 张地图（ℓ=5–100，步长 5，4 旋转），完全公开，用于隔离“深度”与“分支/空间复杂性”。</li>
</ul>
<p>两条性能曲线</p>
<ol>
<li><strong>裸机 LRM</strong>：</li>
</ol>
<ul>
<li>25–30 步处 Accuracy 指数断崖；</li>
<li>token 消耗随 ℓ 线性增加，但错误解方差远大于正确解。</li>
</ul>
<ol>
<li><strong>LLM-Modulo（+PDDL 求解器）</strong>：</li>
</ol>
<ul>
<li>相同阈值仍在，100 步时 Accuracy≈0.35，提升有限；</li>
<li>失败主因：模型生成的 PDDL 问题“长度数错”，外置求解器无法弥补。</li>
</ul>
<p>三点证据</p>
<ol>
<li><strong>计数漂移是主因</strong>：Prefix Accuracy 与 Manhattan Distance 同步下降，循环输出占 token 上限 42%。</li>
<li><strong>状态记忆缺失</strong>：模型无 visited-state 概念，反复 proposing 同一动作，体现“wandering”而非系统搜索。</li>
<li><strong>空间接地缺陷</strong>：垂直走廊显著弱于水平走廊，即使用符号求解器亦未消除，说明“坐标量纲”估计错误根深蒂固。</li>
</ol>
<p>四步结论</p>
<ol>
<li>当前 LRM 的长程规划瓶颈主要在<strong>底层状态跟踪与计数表征</strong>，而非高层搜索策略。</li>
<li>25–30 步是<strong>架构性软上限</strong>，测试时扩展或外接符号求解器<strong>仅能边际改善</strong>，无法突破。</li>
<li>未来需引入<strong>显式坐标记忆、可微符号接地或分层子目标验证</strong>等结构性改进，否则单纯堆算力无益。</li>
<li>SokoBench 以极简环境量化深度失效，为社区提供了<strong>可复现、可干预</strong>的长程规划诊断平台。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SokoBench%3A%20Evaluating%20Long-Horizon%20Planning%20and%20Reasoning%20in%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2601.20856，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20856。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20856v1.pdf">https://arxiv.org/pdf/2601.20856v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20856v1">https://papers.cool/arxiv/2601.20856v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:56:00Z</p>
<hr>
<h3 id="7-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images-1"><a href="#7-From-Specialist-to-Generalist-Unlocking-SAM’s-Learning-Potential-on-Unlabeled-Medical-Images-1" class="headerlink" title="7. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images"></a>7. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17934v2">From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</a></h3><p>Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM’s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>像 Segment Anything Model (SAM) 这样的基础模型展示了强大的泛化能力，但由于领域偏移、标签稀缺以及参数高效微调（PEFT）无法利用未标注数据，将它们应用于医学影像仍然十分困难。尽管像 U-Net 这样的传统模型在半监督医学学习中表现出色，它们在辅助 PEFT SAM 方面的潜力却在很大程度上被忽视。我们提出了 SC-SAM，这是一个专家-通用框架，其中 U-Net 提供基于点的提示和伪标签来引导 SAM 的适应，而 SAM 则作为强大的通用监督器来规范 U-Net。这种相互引导形成了双向协同训练循环，使两个模型都能有效利用未标注数据。在前列腺 MRI 和息肉分割基准实验中，我们的方法达到了最先进的结果，优于其他现有的半监督 SAM 变体，甚至超过了像 MedSAM 这样的医学基础模型，突显了专家-通用合作在标签高效医学影像分割中的价值。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/vnlvi2k3/SC-SAM">https://github.com/vnlvi2k3/SC-SAM</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对医学图像分割中“基础模型难以利用无标注数据”这一核心矛盾展开。具体而言：</p>
<ul>
<li><p><strong>SAM 在医学域的瓶颈</strong><br>尽管 Segment Anything Model（SAM）具备强大的零样本泛化能力，但面对医学影像时存在显著域偏移；同时，参数高效微调（PEFT）仍依赖少量标注，无法直接从无标注样本学习，导致大量无标签数据被闲置。</p>
</li>
<li><p><strong>传统半监督方案与 SAM 的割裂</strong><br>基于 U-Net 的半监督框架（一致性正则、伪标签等）已证明无标注数据的价值，却与 SAM 的 PEFT 流程脱节；现有尝试或仅把 SAM 当教师，或引入复杂双解码结构，仍未让 SAM 真正“吸收”无标注信息。</p>
</li>
<li><p><strong>目标</strong><br>提出 SC-SAM，让“传统专家”U-Net 与“通用基础模型”SAM 形成双向协同：</p>
</li>
</ul>
<ol>
<li>U-Net 先利用全部数据（含无标注）生成可靠点提示与伪标签，指导 SAM 的 PEFT；</li>
<li>SAM 反过来用其高阶语义输出正则化 U-Net，抑制噪声。<br>通过该“专家-通才”循环，使 SAM 在仅 5 %–10 % 标注比例下即可充分挖掘无标注医学图像，提升分割精度并超越现有半监督 SAM 变体及医学专用基础模型。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四类，均围绕“如何在半监督或低标注场景下让 SAM 及其变体适应医学图像”展开：</p>
<ol>
<li><strong>直接微调 SAM（PEFT-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>Medical SAM Adapter</strong>（Wu et al., MedIA 2025）<br>在 ViT 编码器内插入 Adapter 层，仅用小规模标注数据做参数高效微调，未利用无标签集。</li>
<li><strong>SAM-Med2D</strong>（Cheng et al., arXiv 2023）<br>将 2D 医学切片重构成 RGB 三通道后直接全微调，参数开销大且仍依赖标注。</li>
</ul>
<ol>
<li><strong>双分支/交叉提示一致性（Dual-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>CPC-SAM</strong>（Miao et al., MICCAI 2024）<br>两个独立 mask decoder 互作“教师”，通过交叉提示生成伪标签，但两分支均为 SAM 自身，易受域偏移影响而耦合。</li>
</ul>
<ol>
<li><strong>专家网络为 SAM 提供提示（SP-SAM 系列）</strong></li>
</ol>
<ul>
<li><strong>KnowSAM</strong>（Huang et al., IEEE T-MI 2025）<br>用 U-Net+V-Net 生成融合 mask 作为 SAM 的 mask prompt，再以 KL 蒸馏让专家模仿 SAM；专家仅充当“提示生成器”，SAM 并未反向受益。</li>
</ul>
<ol>
<li><strong>传统半监督 CNN 框架</strong></li>
</ol>
<ul>
<li><strong>U-Net 系列</strong>（Ronneberger et al., MICCAI 2015；Zhou et al., 2018；Jha et al., 2019）<br>通过一致性正则、伪标签、Mean Teacher 等手段充分挖掘无标注数据，但无法与 SAM 的 PEFT 流程协同。</li>
</ul>
<p>SC-SAM 与上述工作的本质区别在于：</p>
<ul>
<li>首次让“专家”U-Net 与“通才”SAM 形成<strong>双向协同训练闭环</strong>；</li>
<li>引入 sigmoid ramp-up 缓解早期噪声伪标签对 SAM 的干扰；</li>
<li>既用 U-Net 生成点提示/伪标签指导 SAM，又用 SAM 的语义输出正则化 U-Net，从而真正将无标注数据的信息注入基础模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 SC-SAM，通过“专家-通才”双向协同训练框架，把 U-Net 对无标注数据的利用能力无缝迁移到 SAM 的 PEFT 流程。核心机制分三步：</p>
<ol>
<li><p><strong>专家先行：U-Net 半监督预热</strong><br>用标准半监督策略（一致性正则 + 伪标签）在全部数据上训练 U-Net，使其对标注与无标注图像均产生可靠预测  P^(UNet) 。</p>
</li>
<li><p><strong>通才跟进：SAM 接受 U-Net 的“提示-伪标签”</strong></p>
</li>
</ol>
<ul>
<li>从  P^(UNet)  随机采样 5 前景点 + 5 背景点，生成点提示</li>
</ul>
<p>points = Sample(P^(UNet))</p>
<ul>
<li>将点提示送入 SAM 的 prompt encoder，与图像编码特征一起经 mask decoder 得到预测</li>
</ul>
<p>P^(SAM) = D!(E(X);; P(points))</p>
<ul>
<li>对无标注图像，把 U-Net 的预测当作伪标签，计算无监督损失</li>
</ul>
<p>L<em>(unsup)^(SAM) = L</em>(seg)(P^(SAM), P^(UNet))</p>
<ol>
<li><strong>反向正则：SAM 输出平滑 U-Net</strong><br>同时用 SAM 的预测  P^(SAM)  作为质量更高的伪标签，反向监督 U-Net</li>
</ol>
<p>L<em>(unsup)^(UNet) = L</em>(seg)(P^(UNet), P^(SAM))</p>
<ol>
<li><strong>稳定训练：sigmoid ramp-up</strong><br>为避免早期 U-Net 噪声淹没 SAM，引入时变权重</li>
</ol>
<p>ω(t)= exp!l(-(1-(t) / (T<em>(max)))^2r), &amp; 0le tle T</em>(max)[2pt] 1, &amp; t&gt;T_(max)</p>
<p>总无监督损失</p>
<p>L<em>(unsup) = L</em>(unsup)^(UNet) + ω(t),L_(unsup)^(SAM)</p>
<ol>
<li><strong>联合目标</strong><br>加上标注数据的监督项，整体损失</li>
</ol>
<p>L<em>(total) = L</em>(seg)(P^(UNet), Y) + L<em>(seg)(P^(SAM), Y)</em>(L)<em>(sup) + L</em>(unsup)</p>
<p>通过该闭环，U-Net 提供结构先验与伪标签，SAM 提供高阶语义正则，二者在每次迭代中同步更新，实现无标注信息向基础模型的有效注入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“极低标注比例下能否让 SAM 充分利用无标注医学图像”展开，覆盖前列腺 MRI 与结直肠息肉两个代表性任务，共 5 组实验：</p>
<ol>
<li><strong>主实验：半监督分割精度对比</strong></li>
</ol>
<ul>
<li><strong>数据集</strong></li>
<li>PROMISE12：50 例前列腺 T2 MRI，按 35/5/10 划分 train/val/test，每例按 2D 切片处理。</li>
<li>COLON：CVC-ClinicDB 612 张 + Kvasir 838 张训练，跨域测试五套公开库（CVC-300、CVC-ColonDB、ETIS-LaribPolypDB 等）。</li>
<li><strong>标注比例</strong><br>仅提供 5 % 或 10 % 训练切片作为标注，其余完全无标签。</li>
<li><strong>对照方法</strong></li>
<li>PEFT-SAM 系列：SAM、MedSAM、SAM-Med2D</li>
<li>Dual-SAM 系列：CPC-SAM</li>
<li>SP-SAM 系列：KnowSAM</li>
<li><strong>指标</strong><br>Dice、IoU、HD95、ASD</li>
<li><strong>结果</strong><br>在 5 % 标注下，SC-SAM 在 PROMISE12 上 Dice 83.64，比第二名 KnowSAM 高 5.15；COLON 五套测试集平均 Dice 提升 2–4 个百分点，均达到新 SOTA。</li>
</ul>
<ol>
<li><p><strong>跨域泛化评估</strong><br>COLON 实验即跨域场景：训练集与五套测试集来自不同医院、不同设备。SC-SAM 在未见中心数据上仍保持最高平均 Dice，验证对域偏移的鲁棒性。</p>
</li>
<li><p><strong>可视化对比</strong><br>随机抽取 5 % 与 10 % 标注场景下的前列腺与息肉样本，绘制预测 mask 与 GT 叠加热图。SC-SAM 边界更紧致，背景误报明显减少。</p>
</li>
<li><p><strong>消融实验</strong></p>
</li>
</ol>
<ul>
<li><strong>组件消融</strong><br>去掉 sigmoid ramp-up（ω(t)≡1）→ Dice 从 83.64 降至 36.37，降幅 47 %。</li>
<li><strong>骨干替换</strong><br>将 U-Net 换成 UNet++、ResUNet++、Swin-UNet，在同等设置下 Dice 分别降至 80.14、78.20、70.27，验证 CNN 型专家在半监督场景下优于 ViT 型。</li>
</ul>
<ol>
<li><strong>资源与收敛分析</strong><br>单卡 RTX 3090 Ti 24 GB 即可完成训练；PROMISE12 5 % 标注场景下，SC-SAM 在约 6 k 迭代后收敛，验证集 Dice 即超过最佳对照 2 个百分点，表明样本效率显著提升。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>三维/视频扩展</strong><br>将双向协同机制从 2D 切片推广到 3D 体数据或结肠镜视频，设计时空一致的伪标签生成策略，解决切片间不连续问题。</p>
</li>
<li><p><strong>提示模态扩充</strong><br>除点提示外，引入可学习的稀疏框、文本或解剖先验嵌入，探索混合提示在更小标注比例（≤1 %）下的极限性能。</p>
</li>
<li><p><strong>理论分析</strong><br>对 sigmoid ramp-up 进行误差上界推导，量化早期伪标签噪声对 SAM 梯度方向的影响，给出最优 ramp-up 长度  T_(max)  的解析估计。</p>
</li>
<li><p><strong>更多专家融合</strong><br>采用多专家集成（CNN + Transformer + 物理模型）生成更鲁棒的伪标签，研究不同专家间置信度加权或投票机制，进一步提升伪标签质量。</p>
</li>
<li><p><strong>持续/增量学习</strong><br>当新器官或成像模态陆续加入时，固定 SAM 编码器、仅扩展 Adapter 与专家网络头部，实现无需重训历史数据的持续医学分割。</p>
</li>
<li><p><strong>不确定性驱动采样</strong><br>利用 SAM 或 U-Net 的预测不确定性，主动挑选最有价值的无标注样本进行人工标注，实现从 5 % 到 1 % 标注比例的主动-半监督联合优化。</p>
</li>
<li><p><strong>跨模态协同</strong><br>将同一患者的 MRI、CT、超声等多模态图像共同输入，设计模态无关的提示空间，验证 SC-SAM 在模态缺失情况下的鲁棒性。</p>
</li>
<li><p><strong>轻量化部署</strong><br>对 U-Net 与 Adapter 进行联合量化/剪枝，保持伪标签质量的同时减少显存占用，使框架可在边缘内镜设备上实时运行。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SC-SAM 核心内容速览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>SAM 在医学影像因域偏移+ scarce labels 表现骤降；PEFT 无法直接利用无标注数据，而传统 U-Net 半监督方案又与 SAM 脱节。</td>
</tr>
<tr>
<td>思路</td>
<td>让“专家”U-Net 与“通才”SAM 互为师生，形成双向协同：U-Net 用全量数据生成点提示+伪标签 → 引导 SAM 的 PEFT；SAM 输出高质量 mask → 反向正则化 U-Net。</td>
</tr>
<tr>
<td>技术</td>
<td>1. 点提示采样：5 前景点+5 背景点。 2. 无监督损失： Lunsup=Lseg(P^(UNet),P^(SAM)) + ω(t)Lseg(P^(SAM),P^(UNet)) ，其中  ω(t)  为 sigmoid ramp-up。 3. 联合目标： Ltotal=Lsup+Lunsup 。</td>
</tr>
<tr>
<td>实验</td>
<td>前列腺 MRI（PROMISE12）与结直肠息肉（COLON）在 5 %/10 % 标注比例下均达 SOTA；跨域、消融、可视化与资源分析验证有效性。</td>
</tr>
<tr>
<td>结论</td>
<td>传统 CNN 专家与现代基础模型协同，可在极低标注场景下充分挖掘无标签医学图像，实现标签高效、精度领先的医学分割。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是From%20Specialist%20to%20Generalist%3A%20Unlocking%20SAM%27s%20Learning%20Potential%20on%20Unlabeled%20Medical%20Images，链接是https://arxiv.org/pdf/2601.17934，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17934。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</p>
<p><strong>Categories</strong>: cs.CV, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17934v2.pdf">https://arxiv.org/pdf/2601.17934v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17934v2">https://papers.cool/arxiv/2601.17934v2</a></p>
<p><strong>Published</strong>: 2026-01-25T18:13:48Z</p>
<hr>
<h3 id="8-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation-1"><a href="#8-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation-1" class="headerlink" title="8. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation"></a>8. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20854v1">Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</a></h3><p>Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>表格数据仍然是生成模型的一个具有挑战性的领域。特别是，标准的变分自编码器（VAE）架构，通常由多层感知器组成，在建模特征之间的关系方面表现不佳，尤其是在处理混合数据类型时。相比之下，Transformer 通过其注意力机制，更适合捕捉复杂的特征交互。在本文中，我们通过实证方法研究了将 Transformer 集成到 VAE 不同组件中的影响。我们在 OpenML CC18 套件中的 57 个数据集上进行了实验，并得出两个主要结论。首先，结果表明，将 Transformer 放置以利用潜在空间和解码器表示会在保真度和多样性之间产生权衡。其次，我们观察到所有组件中的连续 Transformer 块之间具有高度相似性。特别是在解码器中，Transformer 的输入与输出之间的关系近似为线性关系。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“表格数据生成”中标准变分自编码器（VAE）难以建模异构特征间复杂依赖关系的问题，提出并系统评估一种改进思路：<br><strong>在 VAE 的不同表征阶段（输入、隐空间、输出）引入 Transformer 的自注意力机制，以显式捕获特征间的高阶交互，同时量化这一改动对生成样本保真度（fidelity）与多样性（diversity）的权衡影响。</strong></p>
<p>具体而言，论文试图回答以下核心问题：</p>
<ol>
<li>在表格数据混合类型特征场景下，将 Transformer 置于 VAE 的编码器、隐空间或解码器，会对生成质量产生何种差异？</li>
<li>不同放置策略是否会带来“保真度-多样性”权衡？</li>
<li>Transformer 内部表征在训练后是否真正改变了特征方向，抑或仅起到缩放作用？</li>
</ol>
<p>通过 57 个 OpenML 数据集的实验，论文得出：</p>
<ul>
<li><strong>仅在隐空间与解码器端引入 Transformer 可显著提升多样性，但会降低保真度</strong>；</li>
<li><strong>编码器端 Transformer 对保真度影响最小</strong>；</li>
<li><strong>解码器端 Transformer 在训练后近似恒等映射，其残差分支受层归一化缩放效应主导</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 章“Related Work”中系统回顾了与“表格数据生成”相关的四大类深度生成模型，并单独梳理了“Transformer 在表格数据生成”中的已有尝试。相关研究可归纳如下：</p>
<p>1. 面向表格数据的深度生成模型</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型类别</th>
<th>代表方法</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>GAN 系列</td>
<td>CTGAN、CTAB-GAN、CTAB-GAN+</td>
<td>将条件 GAN 扩展到混合类型特征，引入模式特定归一化、分类编码与辅助损失。</td>
</tr>
<tr>
<td>扩散模型</td>
<td>TabDDPM、CoDi、StaSy、TabSyn、TabDiff</td>
<td>针对离散-连续混合特征设计前向/反向过程，或在潜空间执行扩散。</td>
</tr>
<tr>
<td>自回归模型</td>
<td>GReaT、TabMT</td>
<td>把表格行序列化后用 GPT/Transformer 逐列生成；引入掩码语言建模。</td>
</tr>
<tr>
<td>VAE 系列</td>
<td>TVAE、VAEM、GOGGLE</td>
<td>TVAE 直接对 One-Hot 后特征做 VAE；VAEM 两阶段先逐特征 VAE 再建模隐变量关系；GOGGLE 用图神经网络近似变量关系并联合 VAE 训练。</td>
</tr>
<tr>
<td>流匹配</td>
<td>梯度提升流 (Gradient-Boosted Flow)</td>
<td>用梯度提升树拟合流匹配中的速度场，适配异构特征。</td>
</tr>
</tbody>
</table>
</div>
<p>2. Transformer 在表格数据生成中的已有探索</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>Transformer 角色</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>TabDiff</td>
<td>同时出现在主干去噪网络与输出头</td>
<td>仅作为模型组件，未系统研究不同放置对保真-多样性权衡的影响。</td>
</tr>
<tr>
<td>TabSyn</td>
<td>用在 VAE 的“识别网络”(encoder) 与“生成网络”(decoder) 内部</td>
<td>与本文 ELD-VAE 结构类似，但未进行表征相似性分析，也未揭示解码器 Transformer 近似恒等的现象。</td>
</tr>
<tr>
<td>GReaT、TabMT</td>
<td>把整张表看成序列，用 Transformer 自回归生成</td>
<td>属于“表→文本”范式，与 VAE 的“潜变量+重构”范式不同。</td>
</tr>
<tr>
<td>SAINT、TabTransformer</td>
<td>主要服务于监督学习，而非生成</td>
<td>聚焦判别任务中的特征交互，未评估合成数据质量。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 小结</p>
<p>已有文献验证了 Transformer 对捕获表格特征交互的有效性，但<strong>缺乏在 VAE 框架内、对“编码器-隐空间-解码器”三个不同层级系统性地放置 Transformer 的对比研究</strong>，也<strong>未量化其带来的保真度-多样性权衡与内部表征相似性</strong>。本文正是填补这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“<strong>系统性位置消融 + 多维度量化评估</strong>”的策略，把 Transformer 逐层“插入”到 VAE 的三种表征路径（输入、隐空间、输出），在 57 个 OpenML 表格数据集上完成对照实验，从而回答“<strong>放哪里、有何利弊、为何如此</strong>”三个问题。具体步骤如下：</p>
<p>1. 构建 6 种 VAE 变体（位置消融）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>Transformer 位置</th>
<th>研究目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAE</td>
<td>无</td>
<td>纯 MLP 基线</td>
</tr>
<tr>
<td>E-VAE</td>
<td>仅 encoder</td>
<td>检验“原始特征”层面交互是否有助益</td>
</tr>
<tr>
<td>EL-VAE</td>
<td>encoder + latent</td>
<td>观察隐变量层加入自注意力的影响</td>
</tr>
<tr>
<td>ELD-VAE</td>
<td>encoder + latent + decoder</td>
<td>全路径覆盖，验证累积效应</td>
</tr>
<tr>
<td>LD-VAE</td>
<td>latent + decoder（无 encoder）</td>
<td>剥离 encoder 干扰，聚焦深层表征</td>
</tr>
<tr>
<td>D-VAE</td>
<td>仅 decoder</td>
<td>直接测试“重构空间”交互的作用</td>
</tr>
</tbody>
</table>
</div>
<p>所有变体共享相同的 tokenizer、detokenizer、隐藏层宽度、 latent 维度与训练超参，保证差异仅由 Transformer 位置引起。</p>
<p>2. 统一训练与采样协议</p>
<ul>
<li><strong>数据预处理</strong>：数值特征用 QuantileTransformer→高斯，类别特征 One-Hot；缺失值均值/众数填补；删除零方差特征。</li>
<li><strong>训练</strong>：500 epoch，Adam  10^(-3) ，权重衰减 0.9，batch size 按验证集自动折半调优。</li>
<li><strong>Transformer 超参</strong>：1 头、4 块、128 隐藏维、pre-norm、embedding 维 4。</li>
<li><strong>采样</strong>：训练完成后从先验  zsimN(0,I)  生成，与训练集同规模。</li>
</ul>
<p>3. 三维评估体系（量化利弊）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>评估层级</th>
<th>指标</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>Low-Density</td>
<td>1-way marginals + Pairwise-correlations</td>
<td>快速检验单变量/双变量分布是否失真</td>
</tr>
<tr>
<td>High-Density</td>
<td>α-Precision（保真） vs β-Recall（多样）</td>
<td>精确量化“保真-多样”权衡</td>
</tr>
<tr>
<td>ML-Efficiency</td>
<td>TSTR Utility + ML-Fidelity</td>
<td>测合成数据在下游 XGBoost 任务中的真实价值</td>
</tr>
</tbody>
</table>
</div>
<p>所有指标归一化到<br>0,1<br>，用 Wilcoxon 符号秩检验判断显著性。</p>
<p>4. 表征相似性诊断（解释原因）</p>
<ul>
<li><strong>CKA 线性相似度</strong>：计算 Transformer 块内“输入-输出”余弦相似度，揭示是否近似恒等映射。</li>
<li><strong>残差缩放因子 σ</strong>：逐样本求解  E= σ E ，若  σ≈ 1  则表明自注意力+LayerNorm 仅做尺度变换。</li>
<li><strong>逐块 F-范数变化</strong>：量化 LayerNorm 对幅值的压缩程度，解释为何解码器 Transformer 不改变方向。</li>
</ul>
<p>5. 结论与落地建议</p>
<ul>
<li><strong>权衡曲线</strong>：latent + decoder 引入 Transformer → β-Recall 平均提升 4–7%，但 α-Precision 下降 3–6%；encoder 处 Transformer 对保真度几乎无负面影响。</li>
<li><strong>恒等现象</strong>：解码器 Transformer 在训练后  σ  中位数≈1，CKA&gt;0.95，证实其退化为“带缩放的恒等函数”，主要受 LayerNorm 的平移-缩放效应主导。</li>
<li><strong>实用指南</strong>：</li>
<li>若任务更重视<strong>多样性</strong>（数据增广、隐私扩充），优先选用 LD-VAE 或 ELD-VAE；</li>
<li>若任务更重视<strong>保真</strong>（缺失值插补、发布替代数据），用 E-VAE 甚至纯 MLP-VAE 即可，省去 Transformer 的计算与调参成本。</li>
</ul>
<p>通过上述“<strong>位置消融 → 多维指标 → 表征诊断</strong>”闭环，论文不仅回答了“放哪里”，也解释了“为何这样”，为后续表格生成模型中 Transformer 的取舍与再设计提供了量化依据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“Transformer 在 VAE 不同位置对表格数据生成的影响”共设计并执行了 <strong>三大类、七组、累计 &gt; 2 000 次训练</strong> 的实验，覆盖 57 个 OpenML-CC18 数据集。具体实验矩阵如下：</p>
<p>1. 主实验：6 种架构 × 57 数据集 全面对照</p>
<p><strong>目的</strong>：量化 Transformer 位置对生成质量与下游效用的一致影响。<br><strong>指标</strong>：</p>
<ul>
<li>Low-Density：1-way marginals、Pairwise-correlations</li>
<li>High-Density：α-Precision（保真）、β-Recall（多样）</li>
<li>ML-Efficiency：TSTR Utility、ML-Fidelity<br><strong>统计</strong>：Wilcoxon 符号秩检验（p=0.001）+ 平均分数/平均排名双维度判定显著性。</li>
</ul>
<p>2. fidelity–diversity 细粒度实验</p>
<p>2.1 Forward 序列（VAE→E-VAE→EL-VAE→ELD-VAE）</p>
<p>2.2 Backward 序列（VAE→D-VAE→LD-VAE→ELD-VAE）</p>
<p><strong>分组</strong>：按样本量将 57 数据集切为 small<br>500,1 k)、medium[1 k,5 k)、large[5 k,96 k<br> 三桶。<br><strong>输出</strong>：</p>
<ul>
<li>各序列每步过渡的 Δα、Δβ 随桶大小的变化曲线（论文图 2 &amp; 补充图 1–2）。</li>
<li>证实“latent/decoder 加 Transformer → 保真降、多样升”趋势与数据规模呈单调关系。</li>
</ul>
<p>3. 表征相似性实验</p>
<p>3.1 聚合相似度</p>
<ul>
<li>对 6 种模型，计算每个 Transformer 组件“输入-输出”CKA 相似度，再按桶平均（论文图 3）。</li>
</ul>
<p>3.2 块内相似度</p>
<ul>
<li>在 ELD-VAE 上，抽取 encoder/latent/decoder 各 4 个 block 内部 6 个检查点（e.in→e.n1→e.attn→e.res→e.n2→e.ffn）两两 CKA，绘制热图（论文图 4–5）。</li>
</ul>
<p>3.3 残差缩放因子</p>
<ul>
<li>逐样本求解 σ 使  E=σE ，给出 σ 分布与块编号关系（论文图 6），验证 decoder 近似恒等。</li>
</ul>
<p>3.4 块数敏感性</p>
<ul>
<li>在 churn、adult、credit-approval 三数据集上，把 ELD-VAE 的 Transformer 块数从 4→3→2→1，重复实验：<br>– High-Density 变化（补充图 3）<br>– 表征相似度热图（补充图 4）<br>结论：块数减少不改变“decoder 高相似”现象，仅 latent 层相似度随块数增加而下降。</li>
</ul>
<p>4. 训练稳定性诊断实验</p>
<ul>
<li>对出现 NaN/爆炸损失的 3 个数据集（kc2、jm1、spambase），将隐藏维与 latent 维减半后重训；记录重构/KL 曲线（补充图 5–6），并对比 α/β 前后变化（补充表 2–3），确认超参过大是 instability 主因。</li>
</ul>
<p>5. 重构数据 fidelity–diversity 实验</p>
<ul>
<li>用训练好的 6 模型对真实测试集做前向重构，再以训练集为参照计算 α-Precision、β-Recall（补充表 4）。</li>
<li>结果与“采样生成”趋势一致，进一步排除评估偏差。</li>
</ul>
<p>6. 低维与 ML 效用桶分析（补充第 2 章）</p>
<ul>
<li>对 Marginals、Pairwise、Utility、ML-Fidelity 四项指标，均按 Forward/Backward 序列、三桶样本量给出 Δ 曲线（补充图 1–2），验证“多样性提升≠下游效用提升”现象在所有指标上稳定存在。</li>
</ul>
<p>7. 单数据集结果全表（补充第 7 章）</p>
<ul>
<li>给出 57 数据集上 6 模型 6 项指标的原始分数与排名（补充表 6–8），支持主文“LD-VAE 综合平均第一”结论的可复现性。</li>
</ul>
<p>实验规模小结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>数量</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集</td>
<td>57</td>
</tr>
<tr>
<td>模型变体</td>
<td>6</td>
</tr>
<tr>
<td>每数据集训练次数</td>
<td>1（主实验）+ 块数敏感 3×4 + 稳定性重训 3</td>
</tr>
<tr>
<td>总训练运行</td>
<td>≈ 6×57 + 3×4×3 + 3 = 363 + 36 + 3 = 402 次完整训练</td>
</tr>
<tr>
<td>生成与评估</td>
<td>每次训练后生成 1×训练集规模样本，并计算 6 项指标</td>
</tr>
<tr>
<td>CKA/σ 计算</td>
<td>在测试集上逐 batch 计算，累计 &gt; 10 7 次向量内积</td>
</tr>
</tbody>
</table>
</div>
<p>通过上述实验矩阵，论文从“质量-效用-机理”三个层面完整回答了 Transformer 放置问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对该工作的直接延伸或深层追问，均具备理论价值与落地潜力：</p>
<p>1. 架构层面：把“退化”变为“有用”</p>
<ul>
<li><p><strong>去 LayerNorm 的 Transformer 块</strong><br>论文揭示解码器 Transformer 近似恒等的核心原因是 LayerNorm 的平移-缩放效应。可尝试</p>
</li>
<li><p>完全移除 LayerNorm，改用 RMSNorm、PowerNorm 或简单可学习标量缩放；</p>
</li>
<li>仅用 Pre-Activation MLP 替代 Transformer 块，对比是否仍能保持多样性增益。<br>目标：在不影响保真度的前提下，让解码器注意力真正改变表征方向。</li>
<li><strong>轻量级注意力替代</strong><br>探索 Linformer、Performer、Cosformer 等线性复杂度注意力，验证“多样性↑保真度↓”规律是否依旧成立，为大规模高维表格（≥10 000 特征）提供可行方案。</li>
</ul>
<p>2. 目标函数层面：重调保真-多样天平</p>
<ul>
<li><strong>动态加权 α-β 损失</strong><br>当前 VAE 仅优化 ELBO。可把 α-Precision、β-Recall 直接纳入损失，构建</li>
</ul>
<p>L<em>(new) = L</em>(ELBO) - λ_1 α + λ_2 β</p>
<p>通过 Pareto 搜索寻找 (λ₁, λ₂) 最优前沿，实现“保真-多样”可控生成。</p>
<ul>
<li><strong>对比式/信息瓶颈正则</strong><br>引入对比损失或信息瓶颈，限制隐变量互信息 I(x;z) 上限，抑制 VAE 本身“过度随机”导致的保真度下降，看能否在 latent-Transformer 场景下仍保持高 β-Recall。</li>
</ul>
<p>3. 数据层面：更复杂的表格形态</p>
<ul>
<li><p><strong>缺失非随机（MNAR）与噪声标签</strong><br>论文仅做简单均值/众数填补。下一步让生成器直接输出“缺失掩码”与“值”两组向量，用 MNAR 模拟机制评估 Transformer 位置对插补偏差的影响。</p>
</li>
<li><p><strong>时序-表格混合</strong><br>将静态表格字段与时间序列拼接，考察 Transformer 在“静态-动态”异构注意力下的表现，拓展到医疗纵向数据、用户行为日志等场景。</p>
</li>
</ul>
<p>4. 评估层面：超越 α-β</p>
<ul>
<li><p><strong>细粒度隐私泄露度量</strong><br>采用 Membership Inference、Attribute Inference 攻击评估不同位置 Transformer 的隐私-效用权衡，验证“多样性↑”是否伴随“泄露↑”。</p>
</li>
<li><p><strong>因果保真度</strong><br>引入因果发现指标（如结构汉明距离 SHD）衡量合成数据是否保持原始变量间的因果边方向，验证注意力机制是否会混淆因果与相关。</p>
</li>
</ul>
<p>5. 训练策略层面：让 Transformer 不再“闲置”</p>
<ul>
<li><p><strong>两阶段训练</strong><br>① 先无 Transformer 训练 VAE 至收敛；② 冻结解码器 MLP，仅微调解码器 Transformer。观察能否打破“恒等”初始化陷阱，使注意力真正学习残差方向。</p>
</li>
<li><p><strong>梯度截断 / 权重标准化</strong><br>对注意力输出做梯度截断或使用 Weight Standardization，缓解 LayerNorm 输入分布漂移，从而提升有效学习率，测试是否减少恒等现象。</p>
</li>
</ul>
<p>6. 跨模型迁移：是否只是 VAE 的“专利”？</p>
<ul>
<li><p><strong>Transformer-in-GAN</strong><br>将同样的“encoder- latent-decoder”放置策略迁移到 CTGAN、TabDDPM 框架，检验保真-多样权衡是否依旧成立，或是否因目标函数不同而逆转。</p>
</li>
<li><p><strong>扩散模型中的注意力位置</strong><br>在 TabDiff 的去噪网络中，分别把 Transformer 放在“时间步嵌入后”“特征嵌入后”“输出头”三段，重复 α-β 实验，看多样性增益是否普遍存在于所有生成范式。</p>
</li>
</ul>
<p>7. 可解释性深挖：注意力到底在“看”什么？</p>
<ul>
<li><p><strong>注意力可视化</strong><br>对 decoder Transformer 的注意力矩阵进行平均，检查是否始终呈现对角或均匀分布，进一步佐证“无结构交互”导致恒等映射的假设。</p>
</li>
<li><p><strong>互信息分解</strong><br>用 MISE 或 CXPlain 对注意力输出与原始特征做互信息分解，定位哪些特征对被过度强化，从而设计稀疏注意力掩码，减少虚假相关。</p>
</li>
</ul>
<p>8. 工程与产品化</p>
<ul>
<li><p><strong>自动架构搜索（NAS）</strong><br>以 α-β 加权分数为奖励，用强化 NAS 搜索“在哪几层、用几块、头数多少”最优，输出 Pareto 前沿供用户按业务场景（金融风控 vs 营销扩量）一键选择。</p>
</li>
<li><p><strong>端侧轻量化</strong><br>结合第 1 点的线性注意力与第 5 点的两阶段训练，构建 &lt;1 MB 的“微型表格生成器”，在移动端实现现场数据增强与隐私释放。</p>
</li>
</ul>
<p>总结</p>
<p>论文首次量化了“Transformer 位置-保真-多样”三角关系，并揭示了解码器恒等退化现象。下一步工作可沿“<strong>架构-目标-数据-评估-训练-跨模型-可解释-工程化</strong>”八个维度展开，把“表格 Transformer 生成”从“能用”推向“好用、可控、可信”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文主旨一句话</strong><br>系统探究“把 Transformer 塞进 VAE 的哪个部位”才能在混合类型表格数据生成中取得最佳“保真-多样”权衡，并揭示解码器 Transformer 训练后近似恒等映射的机理。</p>
<p>1. 背景与动机</p>
<ul>
<li>标准 VAE（全 MLP）难以捕获异构特征间高阶关系。</li>
<li>Transformer 的自注意力在文本、图像领域已验证其交互建模能力，但在表格生成中“放哪里、值不值”缺乏大尺度实证。</li>
</ul>
<p>2. 方法</p>
<ul>
<li><strong>6 个 VAE 变体</strong>：VAE/E-VAE/EL-VAE/ELD-VAE/LD-VAE/D-VAE，仅改变 Transformer 所处位置（encoder / latent / decoder）。</li>
<li><strong>统一设置</strong>：57 个 OpenML-CC18 数据集 → 相同 tokenizer、训练超参、评估流程。</li>
<li><strong>三维评估</strong>：<br>– Low-Density：1-way marginals + pairwise correlations<br>– High-Density：α-Precision（保真） vs β-Recall（多样）<br>– ML-Efficiency：TSTR Utility + ML-Fidelity</li>
<li><strong>表征诊断</strong>：CKA 相似度、残差缩放因子 σ、逐块 F-范数，解释“退化”原因。</li>
</ul>
<p>3. 主要发现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>位置</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>encoder</td>
<td>几乎不损保真，对多样提升微小。</td>
</tr>
<tr>
<td>latent + decoder</td>
<td>多样↑ 4–7%，保真↓ 3–6%；权衡显著且随数据规模单调。</td>
</tr>
<tr>
<td>decoder 单独</td>
<td>最差保真，多样性增益有限。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>机理级结论</strong></p>
<ul>
<li>解码器 Transformer 训练后输入-输出 CKA&gt;0.95，σ≈1，本质为“LayerNorm 缩放后的恒等函数”。</li>
<li>减少块数或移除 LayerNorm 仍无法打破恒等现象，提示需重新设计目标函数或训练策略。</li>
</ul>
<p>4. 实用建议</p>
<ul>
<li><strong>重保真</strong> → 用 E-VAE 或纯 MLP-VAE，省计算。</li>
<li><strong>重多样</strong> → 用 LD-VAE，接受轻微保真损失。</li>
<li><strong>下游 ML 效用</strong> → Transformer 位置无显著优势，多样性↑≠效用↑，需按任务取舍。</li>
</ul>
<p>5. 贡献清单</p>
<p>① 大尺度（57 数据集）位置消融实验，首次量化“保真-多样”权衡。<br>② 揭示解码器 Transformer 恒等退化现象并给出 LayerNorm 根源解释。<br>③ 提供面向表格生成的 Transformer 使用指南与开源训练-评估协议。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Transformer%20Placement%20in%20Variational%20Autoencoders%20for%20Tabular%20Data%20Generation，链接是https://arxiv.org/pdf/2601.20854，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.20854。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</p>
<p><strong>Categories</strong>: cs.LG, cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20854v1.pdf">https://arxiv.org/pdf/2601.20854v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20854v1">https://papers.cool/arxiv/2601.20854v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:54:27Z</p>
<hr>
<h3 id="9-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization-1"><a href="#9-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization-1" class="headerlink" title="9. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization"></a>9. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2506.07972v2">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</a></h3><p>While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型（LLM）在推理和基于主体的问题解决方面取得了显著进步，但当前的评估方法未能充分评估其能力：现有基准要么依赖易被饱和和记忆的封闭式问题，要么依赖缺乏一致性和严谨性的主观比较。在本研究中，我们介绍了HeuriGym，这是一个为评估由大型语言模型生成的组合优化问题启发式算法而设计的代理框架，其特点是目标明确且解空间广阔。HeuriGym 赋能大型语言模型提出启发式方法，通过代码执行获得评估反馈，并迭代完善解决方案。我们评估了涵盖计算机系统、物流和生物学等九个领域的九个最先进模型，揭示了工具使用、规划和自适应推理中持续存在的局限性。为了量化性能，我们提出了质量-产出指数（QYI），这是一个既能反映解法通过率又能体现质量的指标。即使是像GPT-o4-mini-high和Gemini-2.5-Pro这样的顶级模型，QYI分数也仅为0.6，远低于专家的基准1分。我们的开源基准旨在引导大型语言模型在科学和工程领域实现更有效、更现实的问题解决。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决如何有效评估大型语言模型（LLMs）在组合优化问题上的能力。现有的评估方法存在局限性，主要分为两类：基于封闭问题的客观基准和基于人类偏好的主观评估。前者容易受到数据污染和性能饱和的影响，后者则缺乏一致性和严谨性。为了克服这些局限性，作者提出了一个名为HeuriGym的新框架，旨在通过迭代生成、执行和反馈机制来评估LLMs生成的启发式算法在组合优化问题上的表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了与LLMs在组合优化（CO）领域以及LLMs评估相关的研究。以下是相关研究的总结：</p>
<p>LLMs for Combinatorial Optimization</p>
<ul>
<li><strong>NL4Opt Competition</strong>：将自然语言转化为结构化的优化问题，后续工作致力于改进特定领域的模型训练和提示策略，但这些方法依赖于精确求解器，难以扩展。</li>
<li><strong>FunSearch 和 AlphaEvolve</strong>：FunSearch 使用LLMs生成启发式算法，而AlphaEvolve结合进化搜索生成新的启发式算法。但这些方法需要评估数千个候选方案，效率较低。</li>
<li><strong>Metaheuristic Templates</strong>：最近的一些方法通过元启发式模板改进效率，但仍然限制LLMs只填充评分函数，而不是设计完整的算法。</li>
</ul>
<p>Evaluation on LLMs</p>
<ul>
<li><strong>HumanEval、BigCodeBench、LiveCodeBench</strong>：这些基准测试侧重于闭合问题，如数学问题、编程任务等，存在固定答案，容易受到数据污染。</li>
<li><strong>Chatbot Arena、KernelBench</strong>：这些基准测试鼓励多样化的输出，但往往缺乏明确的目标，导致评估不一致。</li>
<li><strong>NPHardEval、GraphArena</strong>：这些基准测试评估小规模NP难问题的精确解，但实际应用中更倾向于使用启发式方法以实现可扩展性。</li>
</ul>
<p>HeuriGym框架通过提供一个开放式的解决方案空间，允许LLMs生成完整的、自包含的优化程序，包括自定义数据结构和端到端的流程，从而更贴近现实世界的CO挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过提出一个名为 <strong>HeuriGym</strong> 的新框架来解决评估 LLMs 在组合优化问题上的能力的问题。HeuriGym 是一个以代理（agentic）为核心的框架，它通过以下方式实现对 LLMs 的全面评估：</p>
<p>1. <strong>框架概述</strong></p>
<p>HeuriGym 框架的核心思想是让 LLMs 生成启发式算法，然后通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。这个过程模拟了实际工程中的工作流程，能够深入评估 LLMs 的多步推理、工具使用和指令遵循能力。</p>
<p>2. <strong>问题描述与提示设计</strong></p>
<ul>
<li><strong>问题描述</strong>：每个基准任务都包含一个结构化的问题描述，包括背景、正式化和输入/输出格式。背景部分介绍优化的上下文和关键术语，正式化部分定义优化目标和约束条件，输入/输出格式部分则明确输入和输出文件的结构。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中，以便 LLMs 学习并改进其输出。</li>
</ul>
<p>3. <strong>反馈循环</strong></p>
<p>HeuriGym 将数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。每次迭代后，生成的解决方案、执行日志、验证结果和评估分数都会被记录并反馈给 LLM，使其能够从过去的尝试中学习并逐步改进输出。</p>
<p>4. <strong>评估指标</strong></p>
<ul>
<li><strong>solves@i</strong>：跟踪 LLM 在 i 次迭代内解决约束问题的能力。这个指标分为三个阶段：执行、解决方案生成和验证。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：这是一个综合指标，结合了解决方案的通过率（Yield）和质量（Quality），通过计算这两个指标的调和平均值来评估 LLM 的整体表现。</li>
</ul>
<p>5. <strong>基准构建</strong></p>
<p>HeuriGym 包含了来自计算机系统、编译器、计算生物学、物流等领域的九个组合优化问题。这些问题被精心挑选，以确保它们具有明确的目标、庞大的解空间、可扩展的数据实例和可复现的专家基线。每个问题都包括一个任务特定的验证器和评估器，用于评估解决方案的通过率和质量。</p>
<p>6. <strong>实验与评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，包括 GPT-o4-mini-high、Gemini-2.5-Pro、LLaMA-4-Maverick 等。实验结果表明，即使是顶级模型，如 GPT-o4-mini-high 和 Gemini-2.5-Pro，在 QYI 评分上也仅为 0.6 左右，远低于专家基线的 1。这揭示了当前 LLMs 在现实世界问题解决中的局限性。</p>
<p>7. <strong>案例研究</strong></p>
<p>论文还通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。虽然 LLMs 能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</p>
<p>8. <strong>未来工作</strong></p>
<p>作者指出，HeuriGym 仍存在一些局限性，如 Python 执行的开销、迭代自优化过程的进一步改进、以及评估指标与现实世界部署之间的差距。未来的工作将包括扩展基准的广度和深度，引入新的问题，并探索更高效的测试时扩展策略。</p>
<p>通过这些方法，HeuriGym 为 LLMs 在组合优化问题上的评估提供了一个全面、严谨且实用的框架，有助于推动 LLMs 在科学和工程领域的应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文中进行了以下实验：</p>
<p>1. <strong>模型评估</strong></p>
<p>作者对九个最先进的 LLM 模型进行了评估，这些模型包括：</p>
<ul>
<li>OpenAI 的 GPT-o4-mini-high</li>
<li>Anthropic 的 Claude-3.7-Sonnet</li>
<li>DeepSeek 的 DeepSeek-V3 和 DeepSeek-R1</li>
<li>Google 的 Gemini-2.5-Flash 和 Gemini-2.5-Pro</li>
<li>Meta 的 LLaMA-3.3 和 LLaMA-4-Maverick</li>
<li>Alibaba 的 Qwen3-235B</li>
</ul>
<p>这些模型在多个领域（如计算机系统、物流、生物学等）的九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</p>
<p>2. <strong>性能评估</strong></p>
<ul>
<li><strong>solves@i 指标</strong>：评估 LLM 在 i 次迭代内解决约束问题的能力。具体来说，solves@i 分为三个阶段：</li>
<li><strong>执行（Execution）</strong>：生成的程序是否能正确编译或解释，并成功执行基本的输入输出操作。</li>
<li><strong>解决方案生成（Solution Generation）</strong>：程序是否在预定义的超时时间内产生非空输出，并符合预期的输出格式。</li>
<li><strong>验证（Verification）</strong>：解决方案是否满足所有问题特定的约束条件。</li>
<li><strong>Quality-Yield Index (QYI)</strong>：综合评估解决方案的通过率（Yield）和质量（Quality）。QYI 是这两个指标的调和平均值，能够全面反映 LLM 的性能。</li>
</ul>
<p>3. <strong>案例研究</strong></p>
<p>作者通过一个案例研究（技术映射问题）展示了 LLMs 的潜力和当前局限性。具体步骤如下：</p>
<ul>
<li><strong>初始迭代</strong>：LLMs 生成基于动态规划（DP）的启发式算法，但未对搜索空间进行剪枝，导致计算时间过长。</li>
<li><strong>后续迭代</strong>：LLMs 逐渐引入剪枝策略，优化算法性能，最终在第五次迭代中达到了最佳的解决方案。</li>
</ul>
<p>4. <strong>消融研究</strong></p>
<ul>
<li><strong>温度（Temperature）</strong>：评估不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率。</li>
<li><strong>少样本演示（Few-shot Demonstrations）</strong>：研究在不同数量的演示示例下模型的性能。结果表明，提供更多的演示示例可以显著提高模型的性能。</li>
<li><strong>反馈轮次（Feedback Rounds）</strong>：研究不同数量的反馈轮次对模型性能的影响。结果表明，适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>5. <strong>错误分析</strong></p>
<p>作者对 LLMs 在生成启发式算法时常见的错误类型进行了分类和分析，包括：</p>
<ul>
<li><strong>API 调用错误</strong>：使用不存在或过时的库调用。</li>
<li><strong>算法逻辑错误</strong>：即使总体方法合理，但实现存在缺陷。</li>
<li><strong>约束误解</strong>：忽略或误解问题的约束条件。</li>
<li><strong>超时错误</strong>：未在给定的约束时间内产生输出。</li>
</ul>
<p>6. <strong>C++ 实验</strong></p>
<p>作者还进行了初步的 C++ 实验，将 LLMs 生成的 Python 代码转换为 C++ 代码，以评估 C++ 在执行速度上的优势。结果显示，即使在第一次迭代中，C++ 解决方案的通过率也高于经过 10 次迭代的 Python 解决方案。</p>
<p>这些实验为评估 LLMs 在组合优化问题上的能力提供了全面的视角，并揭示了当前 LLMs 的优势和局限性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文中提到了一些可以进一步探索的点，以下是一些关键方向：</p>
<p>1. <strong>扩展基准的广度和深度</strong></p>
<ul>
<li><strong>增加问题数量和多样性</strong>：HeuriGym 目前包含九个问题，但随着 LLM 能力的提升，这些基准可能会逐渐饱和。因此，需要不断引入新的、更具挑战性的组合优化问题，特别是来自尚未充分探索的科学领域。</li>
<li><strong>扩大问题规模</strong>：现有问题的数据实例规模有限，未来可以将现有问题扩展到更复杂、更大规模的实例，以更好地评估 LLMs 在大规模问题上的表现。</li>
</ul>
<p>2. <strong>改进测试时扩展策略</strong></p>
<ul>
<li><strong>测试时计算优化</strong>：当前的迭代自优化过程可以被视为一种测试时扩展（Test-time Scaling, TTS）。可以探索如何结合 Best-of-N 采样、束搜索（Beam Search）和进化算法等技术，特别是在增加迭代预算的情况下，以进一步提升 LLMs 的性能。</li>
<li><strong>自验证能力</strong>：利用强大的验证器，HeuriGym 提供了一个自然的平台来研究 LLMs 的自验证能力，这可能是实现更自主的 LLMs 的一个有前景的方向。</li>
</ul>
<p>3. <strong>评估指标与现实世界部署的对齐</strong></p>
<ul>
<li><strong>代理指标与实际性能的差距</strong>：当前的评估指标虽然在初步基准测试中很有用，但在反映现实世界性能方面往往存在不足。特别是在科学领域，解决方案的质量通常需要通过物理实验来验证；在工程领域，如 EDA，质量需要通过耗时的后端综合来确认。如何弥合代理评估和现实世界部署之间的差距，同时管理更长的反馈循环带来的延迟，是一个关键挑战。</li>
</ul>
<p>4. <strong>多语言支持</strong></p>
<ul>
<li><strong>C++ 集成</strong>：目前的实验主要在 Python 上进行，但 C++ 在某些任务中表现出更好的性能。未来可以进一步探索如何将 C++ 更好地集成到框架中，包括处理依赖于特定领域的库和生成高效、功能正确且并行的 C++ 代码。</li>
</ul>
<p>5. <strong>社区贡献和合作</strong></p>
<ul>
<li><strong>开放基准</strong>：HeuriGym 被设计为一个开放的基准，鼓励社区贡献新的问题。未来的工作将包括扩展基准的广度和深度，通过引入新问题和扩展现有问题到更复杂的领域，以保持基准的长期相关性。</li>
<li><strong>跨学科合作</strong>：通过标准化组合问题的格式和可复现的代理工作流程，HeuriGym 可以作为一个共享的测试平台，促进跨学科合作。</li>
</ul>
<p>6. <strong>模型改进</strong></p>
<ul>
<li><strong>迭代改进策略</strong>：研究如何改进 LLMs 的迭代改进策略，使其能够更有效地从反馈中学习并优化解决方案。</li>
<li><strong>模型架构和训练方法</strong>：探索新的模型架构和训练方法，以提高 LLMs 在组合优化问题上的性能。</li>
</ul>
<p>7. <strong>应用领域扩展</strong></p>
<ul>
<li><strong>实际应用</strong>：将 HeuriGym 中的评估方法应用于实际的工业和科学问题，验证其在现实世界中的有效性。</li>
<li><strong>跨领域应用</strong>：探索 LLMs 在其他领域的应用，如医疗、金融等，这些领域同样需要复杂的推理和优化能力。</li>
</ul>
<p>这些方向为未来的研究提供了广阔的空间，有助于推动 LLMs 在组合优化问题上的应用和发展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>本文介绍了一个名为HeuriGym的框架，旨在评估大型语言模型（LLMs）在组合优化问题上生成启发式算法的能力。HeuriGym通过一个代理（agentic）框架，让LLMs提出解决方案，接收执行反馈，并迭代改进，从而全面评估LLMs的多步推理、工具使用和指令遵循能力。该框架通过定义明确目标和广阔解空间的组合优化问题，克服了现有评估方法的局限性，为LLMs的评估提供了一个更加严谨和实用的平台。</p>
<p>背景知识</p>
<ul>
<li><strong>LLMs的进展</strong>：LLMs在复杂推理和基于代理的问题解决方面取得了显著进展，但现有评估方法未能充分评估其能力。传统基准测试依赖于封闭问题，容易受到数据污染和性能饱和的影响；而基于人类偏好的主观评估则缺乏一致性和严谨性。</li>
<li><strong>组合优化问题</strong>：这类问题在多个领域（如计算机系统、科学推理、计算生物学、物流等）中具有基础性地位，适合用于评估LLMs，因为它们需要算法知识、启发式推理、权衡导航和创造性问题解决能力。</li>
</ul>
<p>研究方法</p>
<ul>
<li><strong>HeuriGym框架</strong>：该框架包括问题描述、提示设计、反馈循环和评估指标。LLMs根据问题描述生成启发式算法，通过代码执行环境接收反馈，并基于这些反馈迭代改进解决方案。</li>
<li><strong>问题描述</strong>：每个基准任务都包含背景、正式化和输入/输出格式的结构化描述。</li>
<li><strong>提示设计</strong>：系统提示包括机器配置、可用库和任务特定约束，而用户提示则在初始迭代中提供问题描述和代码骨架。随着迭代的进行，反馈数据被附加到提示中。</li>
<li><strong>反馈循环</strong>：数据集分为演示集和评估集。演示集用于在迭代过程中提供基于示例的反馈，而评估集则在模型稳定后用于最终性能测试。</li>
<li><strong>评估指标</strong>：提出了solves@i和Quality-Yield Index (QYI)两个指标。solves@i跟踪LLMs在i次迭代内解决约束问题的能力，而QYI则综合评估解决方案的通过率和质量。</li>
</ul>
<p>实验</p>
<ul>
<li><strong>模型评估</strong>：对九个最先进的LLMs模型进行了评估，包括GPT-o4-mini-high、Claude-3.7-Sonnet、DeepSeek-V3、DeepSeek-R1、Gemini-2.5-Flash、Gemini-2.5-Pro、LLaMA-3.3、LLaMA-4-Maverick和Qwen3-235B。这些模型在九个组合优化问题上进行了测试，以评估它们在工具使用、多步规划、指令遵循和迭代改进方面的能力。</li>
<li><strong>性能评估</strong>：实验结果显示，即使是顶级模型，如GPT-o4-mini-high和Gemini-2.5-Pro，在QYI评分上也仅为0.6左右，远低于专家基线的1。这揭示了当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>案例研究</strong>：通过技术映射问题展示了LLMs的潜力和当前局限性。虽然LLMs能够通过迭代改进启发式算法，但与专家工具相比，它们的解决方案在质量和效率上仍有显著差距。</li>
<li><strong>消融研究</strong>：研究了不同温度设置（T = 0.0, 0.5, 1.0）对模型性能的影响，以及少样本演示和反馈轮次对模型性能的影响。结果显示，较高的温度可以提高解决方案的多样性，但可能会降低通过率；适度的反馈轮次可以提高解决方案的质量，但过多的反馈可能会导致性能下降。</li>
</ul>
<p>关键结论</p>
<ul>
<li><strong>性能差距</strong>：即使是顶级LLMs模型，在组合优化问题上的表现也远低于专家基线，表明当前LLMs在现实世界问题解决中的局限性。</li>
<li><strong>迭代改进的重要性</strong>：通过迭代改进，LLMs能够逐步优化其解决方案，但与专家工具相比仍有显著差距。</li>
<li><strong>评估指标的有效性</strong>：solves@i和QYI指标能够有效评估LLMs在组合优化问题上的表现，为未来的LLMs评估提供了新的视角。</li>
<li><strong>未来工作方向</strong>：HeuriGym框架为未来的LLMs评估提供了一个有前景的平台，未来的工作可以包括扩展基准的广度和深度、改进测试时扩展策略、弥合代理评估和现实世界部署之间的差距等。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是HeuriGym%3A%20An%20Agentic%20Benchmark%20for%20LLM-Crafted%20Heuristics%20in%20Combinatorial%20Optimization，链接是https://arxiv.org/pdf/2506.07972，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2506.07972。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</p>
<p><strong>Categories</strong>: cs.LG, cs.AI, cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2506.07972v2.pdf">https://arxiv.org/pdf/2506.07972v2.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2506.07972v2">https://papers.cool/arxiv/2506.07972v2</a></p>
<p><strong>Published</strong>: 2025-06-09T17:46:47Z</p>
<hr>
<h3 id="10-C3Box-A-CLIP-based-Class-Incremental-Learning-Toolbox"><a href="#10-C3Box-A-CLIP-based-Class-Incremental-Learning-Toolbox" class="headerlink" title="10. C3Box: A CLIP-based Class-Incremental Learning Toolbox"></a>10. <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.20852v1">C3Box: A CLIP-based Class-Incremental Learning Toolbox</a></h3><p>Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at <a target="_blank" rel="noopener" href="https://github.com/LAMDA-CL/C3Box">https://github.com/LAMDA-CL/C3Box</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>传统机器学习系统通常为静态数据分布设计，而静态数据分布在从不断变化的数据流中学习时容易出现灾难性遗忘。阶级增量学习（CIL）通过使学习系统能够在保留既有知识的同时持续学习新类别来应对这一挑战。随着预训练模型（PTM）如CLIP的兴起，利用其强大的泛化和语义对齐能力已成为CIL中一个有前景的方向。然而，现有基于CLIP的CIL方法常常分散在不同的代码库中，依赖于不一致的配置，阻碍了公平的比较、可重复性和实际应用。因此，我们提出C3Box（基于CLIP的Class-inCremental学习工具箱），这是一个模块化且全面的Python工具箱。C3Box 将代表性的传统 CIL 方法、基于 ViT 的 CIL 方法以及最先进的基于 CLIP 的 CIL 方法整合到一个统一的基于 CLIP 的框架中。通过继承 PyCIL 的简化设计，C3Box 提供了基于 JSON 的配置和标准化的执行流水线。这种设计实现了低工程开销的可重复实验，使C3Box成为持续学习研究的可靠基准平台。C3Box 旨在用户友好，仅依赖广泛使用的开源库，并支持主要作系统。代码可在 <a target="_blank" rel="noopener" href="https://github.com/LAMDA-CL/C3Box">https://github.com/LAMDA-CL/C3Box</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“CLIP 驱动的类增量学习（CLIP-based Class-Incremental Learning, CIL）”研究中的三大痛点，提出统一解决方案：</p>
<ol>
<li><p>实现碎片化<br>现有 CLIP-CIL 方法散落在不同代码库，接口、配置、数据划分与评估指标均不统一，导致横向对比困难、复现性差。</p>
</li>
<li><p>实验协议缺失<br>缺乏标准化的增量阶段划分、超参数设定与评价流程，不同论文结果难以直接对照，阻碍社区对方法优劣的客观判断。</p>
</li>
<li><p>工程门槛高<br>将各类 CLIP 适应策略嵌入增量框架需要大量接口开发与模块拼接，研究者需投入额外工程成本，延缓算法迭代。</p>
</li>
</ol>
<p>为此，作者发布 C3Box——一个模块化、纯开源、跨平台的 Python 工具箱，把传统 CIL、ViT-CIL 与最前沿的 CLIP-CIL 方法整合到同一 CLIP 底座下，通过单一 JSON 配置文件驱动实验，实现“零代码改动”的公平评测与可复现研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>C3Box 论文在背景与实验部分共引用/对比了 40 余篇工作，可归纳为 4 条主线。以下按“方法类别 → 代表性文献”梳理，并给出其在 C3Box 中的角色（已集成、作为 baseline 或被引用讨论）。</p>
<ol>
<li>传统（CNN 时代）CIL 方法</li>
</ol>
<ul>
<li>iCaRL ‑ Rebuffi et al., CVPR 2017</li>
<li>FOSTER ‑ Wang et al., ECCV 2022</li>
<li>MEMO ‑ Zhou et al., ICLR 2023<br>→ 作为“非 PTM”对照组被集成，用于验证 CLIP 底座带来的增益。</li>
</ul>
<ol>
<li>ViT/提示型 CIL 方法</li>
</ol>
<ul>
<li>L2P ‑ Wang et al., CVPR 2022</li>
<li>DualPrompt ‑ Wang et al., ECCV 2022</li>
<li>CODA-Prompt ‑ Smith et al., CVPR 2023</li>
<li>EASE ‑ Zhou et al., CVPR 2024</li>
<li>SimpleCIL ‑ Zhou et al., IJCV 2025</li>
<li>APER（含 Adapter/SSF/VPT 等 PEFT 变体）‑ Zhou et al., IJCV 2025</li>
<li>TUNA ‑ Wang et al., arXiv 2025<br>→ 全部集成至 C3Box，构成“ViT-based”方法簇，与 CLIP 方法进行同底座对比。</li>
</ul>
<ol>
<li>CLIP 预训练模型及零样本基线</li>
</ol>
<ul>
<li>CLIP ‑ Radford et al., ICML 2021</li>
<li>OpenCLIP ‑ Ilharco et al., 2021；Cherti et al., CVPR 2023<br>→ 提供两种官方权重（OpenAI &amp; LAION-400M），并作为 ZS-CLIP 与 Fine-tune 两个极端 baseline。</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Hao Sun, Da-Wei Zhou</p>
<p><strong>Categories</strong>: cs.LG, cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.20852v1.pdf">https://arxiv.org/pdf/2601.20852v1.pdf</a></p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.20852v1">https://papers.cool/arxiv/2601.20852v1</a></p>
<p><strong>Published</strong>: 2026-01-28T18:52:36Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/domain/202601/2026-01-30/">https://wdndev.github.io/daily/domain/202601/2026-01-30/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/ArXiv/">ArXiv</a><a class="post-meta__tags" href="/tags/Domain/">Domain</a><a class="post-meta__tags" href="/tags/AI/">AI</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/domain/202507/2025-07-14/" title="ArXiv Domain 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">ArXiv Domain 2025-07-14</div></div></a></div><div><a href="/daily/domain/202507/2025-07-15/" title="ArXiv Domain 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">ArXiv Domain 2025-07-15</div></div></a></div><div><a href="/daily/domain/202507/2025-07-16/" title="ArXiv Domain 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">ArXiv Domain 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM-Domain-Papers"><span class="toc-text">LLM Domain Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Recursive-Language-Models"><span class="toc-text">1. Recursive Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs"><span class="toc-text">2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching"><span class="toc-text">3. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems"><span class="toc-text">4. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-When-Flores-Bloomz-Wrong-Cross-Direction-Contamination-in-Machine-Translation-Evaluation"><span class="toc-text">5. When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models"><span class="toc-text">6. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-From-Specialist-to-Generalist-Unlocking-SAM%E2%80%99s-Learning-Potential-on-Unlabeled-Medical-Images"><span class="toc-text">7. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation"><span class="toc-text">8. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><span class="toc-text">9. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Post-Training-Fairness-Control-A-Single-Train-Framework-for-Dynamic-Fairness-in-Recommendation"><span class="toc-text">10. Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Agent-Domain-Papers"><span class="toc-text">Agent Domain Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Recursive-Language-Models-1"><span class="toc-text">1. Recursive Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-1"><span class="toc-text">2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching-1"><span class="toc-text">3. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems-1"><span class="toc-text">4. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models"><span class="toc-text">5. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-From-Specialist-to-Generalist-Unlocking-SAM%E2%80%99s-Learning-Potential-on-Unlabeled-Medical-Images"><span class="toc-text">6. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation"><span class="toc-text">7. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><span class="toc-text">8. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Post-Training-Fairness-Control-A-Single-Train-Framework-for-Dynamic-Fairness-in-Recommendation"><span class="toc-text">9. Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Fail-Fast-Win-Big-Rethinking-the-Drafting-Strategy-in-Speculative-Decoding-via-Diffusion-LLMs"><span class="toc-text">10. Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluation-Domain-Papers"><span class="toc-text">Evaluation Domain Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Recursive-Language-Models-2"><span class="toc-text">1. Recursive Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-2"><span class="toc-text">2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-LLMStinger-Jailbreaking-LLMs-using-RL-fine-tuned-LLMs"><span class="toc-text">3. LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching"><span class="toc-text">4. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems"><span class="toc-text">5. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-When-Flores-Bloomz-Wrong-Cross-Direction-Contamination-in-Machine-Translation-Evaluation"><span class="toc-text">6. When Flores Bloomz Wrong: Cross-Direction Contamination in Machine Translation Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models"><span class="toc-text">7. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-From-Specialist-to-Generalist-Unlocking-SAM%E2%80%99s-Learning-Potential-on-Unlabeled-Medical-Images"><span class="toc-text">8. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation"><span class="toc-text">9. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization"><span class="toc-text">10. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VLM-Domain-Papers"><span class="toc-text">VLM Domain Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Recursive-Language-Models-3"><span class="toc-text">1. Recursive Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Evolutionary-Strategies-lead-to-Catastrophic-Forgetting-in-LLMs-3"><span class="toc-text">2. Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-ArchesClimate-Probabilistic-Decadal-Ensemble-Generation-With-Flow-Matching-2"><span class="toc-text">3. ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-DCP-Bench-Open-Evaluating-LLMs-for-Constraint-Modelling-of-Discrete-Combinatorial-Problems-2"><span class="toc-text">4. DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-FreeFix-Boosting-3D-Gaussian-Splatting-via-Fine-Tuning-Free-Diffusion-Models"><span class="toc-text">5. FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-SokoBench-Evaluating-Long-Horizon-Planning-and-Reasoning-in-Large-Language-Models-1"><span class="toc-text">6. SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-From-Specialist-to-Generalist-Unlocking-SAM%E2%80%99s-Learning-Potential-on-Unlabeled-Medical-Images-1"><span class="toc-text">7. From Specialist to Generalist: Unlocking SAM’s Learning Potential on Unlabeled Medical Images</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Exploring-Transformer-Placement-in-Variational-Autoencoders-for-Tabular-Data-Generation-1"><span class="toc-text">8. Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-HeuriGym-An-Agentic-Benchmark-for-LLM-Crafted-Heuristics-in-Combinatorial-Optimization-1"><span class="toc-text">9. HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-C3Box-A-CLIP-based-Class-Incremental-Learning-Toolbox"><span class="toc-text">10. C3Box: A CLIP-based Class-Incremental Learning Toolbox</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>