<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-12-30 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object InsertionRecent advances in diffusion-based video generation have o">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-12-30">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202512/2025-12-30/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object InsertionRecent advances in diffusion-based video generation have o">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.979Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202512/2025-12-30/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-12-30',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-12-30</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.979Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">54.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>193min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-12-30"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion"><a href="#1-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion" class="headerlink" title="1. InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.17504">InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</a></h3><p>Recent advances in diffusion-based video generation have opened new possibilities for controllable video editing, yet realistic video object insertion (VOI) remains challenging due to limited 4D scene understanding and inadequate handling of occlusion and lighting effects. We present InsertAnywhere, a new VOI framework that achieves geometrically consistent object placement and appearance-faithful video synthesis. Our method begins with a 4D aware mask generation module that reconstructs the scene geometry and propagates user specified object placement across frames while maintaining temporal coherence and occlusion consistency. Building upon this spatial foundation, we extend a diffusion based video generation model to jointly synthesize the inserted object and its surrounding local variations such as illumination and shading. To enable supervised training, we introduce ROSE++, an illumination aware synthetic dataset constructed by transforming the ROSE object removal dataset into triplets of object removed video, object present video, and a VLM generated reference image. Through extensive experiments, we demonstrate that our framework produces geometrically plausible and visually coherent object insertions across diverse real world scenarios, significantly outperforming existing research and commercial models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于扩散的视频生成的最新进展为可控视频编辑开辟了新可能，但由于对四维场景理解的有限性以及对遮挡和光照效果处理不足，现实的视频对象插入（VOI）仍然具有挑战性。我们提出了 InsertAnywhere，一种新的 VOI 框架，实现了几何一致的对象放置和外观忠实的视频合成。我们的方法从一个四维感知的掩码生成模块开始，该模块重建场景几何并在保持时间连贯性和遮挡一致性的同时，将用户指定的对象放置跨帧传播。在这个空间基础上，我们扩展了基于扩散的视频生成模型，以联合合成插入对象及其周围的局部变化，如光照和阴影。为了实现监督训练，我们引入了 ROSE++，这是一个光照感知的合成数据集，通过将 ROSE 对象移除数据集转换为三元组——对象移除视频、对象存在视频和 VLM 生成的参考图像——构建而成。通过大量实验，我们证明了我们的框架能够在各种真实世界场景中产生几何合理且视觉一致的对象插入，显著超越现有的研究成果和商业模型。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“视频物体插入（Video Object Insertion, VOI）”任务，旨在解决以下核心难题：</p>
<ul>
<li><p><strong>4D 场景几何与遮挡一致性</strong><br>现有扩散视频生成模型缺乏对动态三维场景的显式理解，难以在复杂相机运动与遮挡关系中保持插入物体的几何一致性与可见性。</p>
</li>
<li><p><strong>光照/阴影等局部效应的逼真合成</strong><br>传统视频 inpainting 仅在被遮罩区域内进行补全，无法同步生成物体所引发的外部光照变化、投射阴影及反射，导致结果脱离真实物理。</p>
</li>
<li><p><strong>用户可控的精确定位与尺度</strong><br>单张参考图不包含目标场景的深度与比例信息，需要让用户在首帧指定位置与大小，并自动将这一配置沿时序与视角变化准确传播。</p>
</li>
<li><p><strong>训练数据缺失</strong><br>缺少成对的“无物体–有物体–参考图”视频三元组，导致难以用监督方式学习插入任务。</p>
</li>
</ul>
<p>为此，作者提出 InsertAnywhere 框架，通过“4D 感知遮罩生成 + 首帧引导的视频扩散模型 + 光照感知合成数据集 ROSE++”协同解决上述问题，实现可用于商业级后期制作的逼真视频物体插入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”及后续实验对比中，系统梳理了与视频物体插入（VOI）密切相关的三条研究主线，并给出了代表性文献。按主题归纳如下：</p>
<p>1. 视频 Inpainting / 编辑基础模型</p>
<ul>
<li><strong>HunyuanCustom</strong><br>7</li>
</ul>
<p>多模态条件视频生成架构，支持掩码、姿态、深度等控制信号。</p>
<ul>
<li><strong>VACE</strong><br>9</li>
</ul>
<p>统一视频创作与编辑框架，提供掩码+文本驱动生成能力，InsertAnywhere 的视频扩散骨干即基于此。</p>
<ul>
<li><strong>AnyV2V</strong><br>12</li>
</ul>
<p>免调参视频到视频编辑框架，支持掩码+文本提示，但对复杂遮挡与几何一致性处理有限。</p>
<p>2. 静态或单帧物体插入</p>
<ul>
<li><strong>AnyDoor</strong><br>3</li>
</ul>
<p>零样本图像级物体定制，通过 ID 特征+位置先验实现单帧插入，未考虑时序一致性。</p>
<ul>
<li><strong>Add-it</strong><br>24</li>
</ul>
<p>利用预训练扩散模型实现训练无关的图像物体插入，缺乏视频级扩展。</p>
<ul>
<li><strong>ObjectStitch</strong><br>22<br>/ <strong>ObjectDrop</strong><br>27<br>/ <strong>ObjectMate</strong><br>28</li>
</ul>
<p>分别探索拼接、反事实移除/插入、递归先验等策略，但局限于单帧。</p>
<p>3. 视频级物体插入与传播</p>
<ul>
<li><strong>Anything in Any Scene</strong><br>2</li>
</ul>
<p>早期视频 VOI 尝试，基于 2D 融合，无显式 4D 几何，遮挡鲁棒性不足。</p>
<ul>
<li><strong>INVI</strong><br>21</li>
</ul>
<p>直接调用静态扩散模型逐帧插入，再通过后处理追求一致，遮挡与光照问题显著。</p>
<ul>
<li><strong>VideoAnyDoor</strong><br>25</li>
</ul>
<p>引入运动控制信号，但仍属 2D 框架，对遮挡及几何一致性处理有限。</p>
<ul>
<li><strong>GenProp</strong><br>14</li>
</ul>
<p>通过首帧编辑+区域感知损失传播外观，未显式建模可见性变化，在物体被遮挡/重现时容易失效。</p>
<ul>
<li><strong>DreamInsert</strong><br>32</li>
</ul>
<p>单图到视频零样本插入，依赖文本驱动，难以保证几何准确与遮挡一致。</p>
<p>4. 商业闭源工具（实验对比基线）</p>
<ul>
<li><p><strong>Pika Pro</strong><br>18</p>
</li>
<li><p><strong>Kling</strong><br>23</p>
</li>
</ul>
<p>这些工具支持文本提示的物体插入，但缺乏显式 4D 几何推理，常出现尺度错误、遮挡关系混乱或“物体替换”现象。</p>
<p>5. 数据集与训练策略相关</p>
<ul>
<li><strong>ROSE</strong><br>15</li>
</ul>
<p>原用于“物体移除+副作用消除”的合成数据集，本文扩展为 ROSE++，通过 VLM 生成参考图，反向支持插入任务监督训练。</p>
<ul>
<li><strong>LoRA</strong><br>6</li>
</ul>
<p>低秩适配策略，用于在保留预训练视频扩散能力的同时高效微调插入域。</p>
<p>综上，现有研究或停留在 2D/单帧层面，或虽进入视频域但缺少显式 4D 几何与遮挡-光照联合建模；InsertAnywhere 通过引入 4D 场景重建与光照感知数据集，填补了该空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“商业级视频物体插入”解耦为<strong>几何一致</strong>与<strong>外观逼真</strong>两大子问题，提出<strong>两阶段框架 InsertAnywhere</strong>，并在数据层面配套构建 ROSE++ 训练集。核心思路与对应模块如下：</p>
<p>1. 4D-aware Mask Generation —— 解决“放哪里、怎么随时间移动”</p>
<ol>
<li><p><strong>4D 场景重建</strong><br>基于 Uni4D 范式，融合单目深度、光流、相机位姿与分割，得到<strong>时序一致</strong>的 3D 点云+相机轨迹。</p>
</li>
<li><p><strong>用户交互放置</strong><br>将参考物体通过单目重建网络升维为 3D 点云  Y=y_j∈mathbb R^3 ，允许用户在首帧 3D 场景中<strong>刚性变换</strong></p>
</li>
</ol>
<p>y<em>j’=s</em>(obj)R<em>(obj)y_j+t</em>(obj)</p>
<p>以确定初始位姿与尺度。</p>
<ol>
<li><strong>场景流驱动传播</strong><br>利用 SEA-RAFT 估计稠密光流，将物体邻域 K 近邻点的 2D 运动提升为 3D，得到局部场景流  V_(tto t+1) ，按</li>
</ol>
<p>y<em>(j,t+1)’=y</em>(j,t)’+V<em>(tto t+1)(y</em>(j,t)’)</p>
<p>更新物体 centroid，实现<strong>随物体/背景一起运动</strong>的物理一致轨迹。</p>
<ol>
<li><strong>相机对齐重投影</strong><br>按估计的内外参</li>
</ol>
<p>u<em>(j,t)v</em>(j,t)1sim K[R<em>t|t_t]y</em>(j,t)’</p>
<p>将 3D 物体点逐帧投影并光栅化，得到初始合成帧  hat I_t ；再用 SAM2 提取<strong>二值 mask 序列</strong>  M_t ，保证遮挡、视差、几何一致。</p>
<p>2. Video Object Insertion —— 解决“长什么样、光照阴影如何生成”</p>
<ol>
<li><p><strong>首帧锚定</strong><br>利用高保真<strong>图像级插入模型</strong>对首帧执行 inpainting，获得颜色、纹理、光照都可信的初始帧，作为视频扩散的“视觉锚点”。</p>
</li>
<li><p><strong>扩散视频生成</strong><br>以 Wan2.1-VACE-14B 为骨干，用 LoRA（rank=128）在 ROSE++ 上微调 5k 步，学习<strong>联合重建物体+外部局部变化</strong>（阴影、反射、亮度）。<br>输入：原视频 + 参考物体图 + 4D 一致 mask；输出：完整插入视频。</p>
</li>
<li><p><strong>训练数据 ROSE++</strong></p>
</li>
</ol>
<ul>
<li>来源：将 ROSE 物体移除数据集“反转”成<strong>三元组</strong>（无物体视频，有物体视频，参考物体图）。</li>
<li>参考图生成：用 VLM 按文本提示生成多视角白底图，再以 DINO-v2 相似度</li>
</ul>
<p>s<em>k=frac1N∑</em>(j=1)^NDINO(hat o_k,f_j)</p>
<p>挑选最优，避免训练-测试域差异导致的 copy-paste 伪影。</p>
<p>3. 推理流程小结</p>
<p>用户仅需在 GUI 中<strong>首帧拖拽/缩放物体</strong> → 系统自动输出 4D 一致 mask → 首帧 inpainting → 视频扩散模型生成<strong>几何准确、遮挡正确、光照一致</strong>的完整视频。</p>
<p>通过“4D 几何先验 + 首帧锚定 + 光照感知数据”三箭齐发，InsertAnywhere 在复杂遮挡、大幅相机运动、开关门等光照变化场景下，同时实现</p>
<ul>
<li>物体尺度/姿态随视角正确变化</li>
<li>被动态物体遮挡时可见性合理</li>
<li>自动生成阴影、亮度匹配</li>
</ul>
<p>从而将视频物体插入推向<strong>可商业落地</strong>的质量水准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>定量指标、定性可视化、用户主观评价、模块消融、数据集消融</strong>五个维度进行了系统实验，全部在自建的 VOIBench 基准与公开场景上完成。主要实验内容如下：</p>
<p>1. 基准与设置</p>
<ul>
<li><strong>VOIBench</strong></li>
<li>50 段真实视频 × 每段 2 类物体 → 100 条测试样本</li>
<li>涵盖室内、室外、自然场景，含复杂相机运动与遮挡</li>
<li><strong>基线</strong><br>闭源商业工具：Pika-Pro、Kling（当前公开可用 SOTA）</li>
<li><strong>指标</strong></li>
<li><strong>Subject Consistency</strong>：CLIP-I、DINO-I（仅物体区域）</li>
<li><strong>Video Quality</strong>：VBench 套件（成像质量、背景一致性、主体一致性、运动平滑度）</li>
<li><strong>Multi-View Consistency</strong>：VBench 子项，评估遮挡切换时物体是否保持一致</li>
</ul>
<p>2. 主实验结果（表 1 &amp; 图 4）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Method</th>
<th>CLIP-I↑</th>
<th>DINO-I↑</th>
<th>Imaging Q↑</th>
<th>Multi-View↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pika-Pro</td>
<td>0.4940</td>
<td>0.3856</td>
<td>0.6546</td>
<td>0.5123</td>
</tr>
<tr>
<td>Kling</td>
<td>0.6349</td>
<td>0.5028</td>
<td>0.7069</td>
<td>0.5439</td>
</tr>
<tr>
<td>Ours</td>
<td>0.8122</td>
<td>0.5678</td>
<td>0.7101</td>
<td>0.5857</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>在 6 项 VBench 子指标中 <strong>5 项第一</strong></li>
<li>定性图 4 显示：基线出现颜色漂移、尺度错误、遮挡逻辑混乱（pepper shaker 穿手）；InsertAnywhere 保持几何与外观一致</li>
</ul>
<p>3. 用户主观评测（表 4）</p>
<ul>
<li>20 名受试者 × 10% 测试视频，双盲随机排序</li>
<li>六项准则投票（真实感、光照、遮挡、语义一致、背景保真、整体自然度）</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>Method</th>
<th>Object Realism</th>
<th>Lighting</th>
<th>Occlusion</th>
<th>Overall Naturalness</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pika-Pro</td>
<td>1.82 %</td>
<td>3.64 %</td>
<td>0.00 %</td>
<td>4.55 %</td>
</tr>
<tr>
<td>Kling</td>
<td>19.09 %</td>
<td>25.45 %</td>
<td>13.33 %</td>
<td>24.55 %</td>
</tr>
<tr>
<td>Ours</td>
<td>79.09 %</td>
<td>71.82 %</td>
<td>86.67 %</td>
<td>70.00 %</td>
</tr>
</tbody>
</table>
</div>
<p>4. 模块消融（图 5 + 表 3）</p>
<p>四种配置逐层叠加：</p>
<ol>
<li><strong>(a) 仅相机轨迹</strong> → 无 4D 几何，遮挡处手臂/围巾被抹除</li>
<li><strong>(b) +4D-aware mask</strong> → 遮挡关系正确，但物体纹理漂移</li>
<li><strong>(c) +首帧 inpainting</strong> → 物体 fidelity 提升，仍缺阴影</li>
<li><strong>+ROSE++ LoRA</strong> → 自动产生阴影、开关门光照变化，VBench 全指标最高</li>
</ol>
<p>定量对比（节选）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Config</th>
<th>CLIP-I</th>
<th>DINO-I</th>
<th>Imaging Q</th>
<th>Multi-View</th>
</tr>
</thead>
<tbody>
<tr>
<td>(a)</td>
<td>0.7585</td>
<td>0.4190</td>
<td>0.6175</td>
<td>0.5238</td>
</tr>
<tr>
<td>(b)</td>
<td>0.7532</td>
<td>0.3861</td>
<td>0.6298</td>
<td>0.5308</td>
</tr>
<tr>
<td>(c)</td>
<td>0.7880</td>
<td>0.5135</td>
<td>0.6318</td>
<td>0.5436</td>
</tr>
<tr>
<td>Ours</td>
<td>0.8122</td>
<td>0.5678</td>
<td>0.7101</td>
<td>0.5857</td>
</tr>
</tbody>
</table>
</div>
<p>5. 数据集/训练策略消融</p>
<ul>
<li><p><strong>VLM vs 随机帧裁剪</strong>（表 2）<br>用视频内随机帧作物体图的“Ours-random” Multi-View Consistency 降至 0.5295，<strong>VLM 版 0.5857</strong>，验证 VLM 生成可抑制 copy-paste 伪影</p>
</li>
<li><p><strong>光照与阴影可视化</strong>（图 6）</p>
</li>
<li>开关门序列：未微调时纸袋亮度不变；ROSE++ LoRA 后随门开/关明显变亮/暗</li>
<li>阴影：未微调无投影；微调后物体外侧地板出现与光源方向一致的软阴影</li>
</ul>
<p>6. 场景流传播对比（图 7）</p>
<ul>
<li>静态物体 mask → 放在移动餐车上的苹果错位</li>
<li>加入 3.2.3 场景流传播后，苹果始终随餐车同步运动，mask 时序对齐</li>
</ul>
<p>7. 额外定性样例（图 9-13）</p>
<p>展示抽屉、胡椒罐、床、厨房桌、大厅立柱等场景：</p>
<ul>
<li>基线常出现“物体替换”或“悬浮/尺度失真”</li>
<li>InsertAnywhere 在严重遮挡、反射地面、多抽屉语义约束下仍保持几何与光照一致</li>
</ul>
<p>综上，实验从<strong>数值指标、人类感知、模块贡献、数据策略</strong>多维度验证：</p>
<ol>
<li>4D-aware mask 解决遮挡与几何一致</li>
<li>首帧锚定提升物体 fidelity</li>
<li>ROSE++ LoRA 让模型自动学会阴影与光照变化</li>
</ol>
<p>InsertAnywhere 在全部对比中均显著优于现有商业工具与消融配置，达到可生产部署水准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为<strong>几何/运动、外观/光照、交互/控制、数据与评测、系统与落地</strong>五大类，供后续研究参考：</p>
<p>1. 几何与运动一致性</p>
<ul>
<li><p><strong>非刚性/可变形物体插入</strong><br>当前仅处理刚性点云，可引入可变形体或铰接模板，结合物理弹性/碰撞模拟，使布料、人体配饰等随场景自然形变。</p>
</li>
<li><p><strong>多物体联合布局</strong><br>扩展用户界面以支持一次插入多个物体，并自动推理相互遮挡、支撑关系（书叠在桌上、杯子在盘子旁）。</p>
</li>
<li><p><strong>动态场景在线更新</strong><br>对实时视频流进行增量式 4D 重建，随新帧到达持续优化相机与几何，避免长序列漂移导致的 mask 错位。</p>
</li>
<li><p><strong>真实物理动力学</strong><br>将物理引擎（Bullet/PhysX）与 4D 表示耦合，实现掉落、碰撞、滚动等真实运动，而非仅依赖场景流近似。</p>
</li>
</ul>
<p>2. 外观与光照建模</p>
<ul>
<li><p><strong>全局照明与互反射</strong><br>目前阴影为局部生成，可引入 NeRF-style 全局光照或球谐渲染，在插入后迭代优化场景辐射场，使高反光物体在镜面/金属表面产生正确倒影。</p>
</li>
<li><p><strong>材质与 BRDF 估计</strong><br>从参考图推断材质参数（粗糙度、金属度），结合场景光源位置，实现随视角变化的 Fresnel 效应与微表面高光。</p>
</li>
<li><p><strong>时变光照与阴影软硬度</strong><br>当光源移动（如手持灯），阴影应随时间变软或硬；可估计光源区域/环境图，动态调节阴影核大小。</p>
</li>
<li><p><strong>自动光源一致性检测</strong><br>开发自监督损失，利用插入前后外观残差反向优化虚拟光源位置，减少人工检查。</p>
</li>
</ul>
<p>3. 交互与控制</p>
<ul>
<li><p><strong>语言-手势混合控制</strong><br>在 GUI 拖拽之外，支持“把花瓶放到餐桌左上角，略向窗户倾斜”这类自然语言+3D 手势多模态指令。</p>
</li>
<li><p><strong>风格/时代一致性控制</strong><br>给定电影年代或调色 LUT，强制插入物体色调、噪点、颗粒度与胶片一致，避免视觉跳变。</p>
</li>
<li><p><strong>交互式光照重定向</strong><br>用户可实时调节虚拟光源方向/色温，系统即时更新阴影与互反射，辅助广告拍摄预览。</p>
</li>
<li><p><strong>VR/AR 实时预览</strong><br>将 4D 重建与扩散模型蒸馏为轻量网络，在头显内以 6DoF 交互放置虚拟商品，实现片场级虚实融合。</p>
</li>
</ul>
<p>4. 数据与评测</p>
<ul>
<li><p><strong>真实标签捕获</strong><br>使用机械臂+多光谱灯箱拍摄真实物体 360° 序列，结合光度立体获取法线图，构建高质量真实域测试集，弥补 ROSE++ 纯合成缺口。</p>
</li>
<li><p><strong>细粒度语义一致性评测</strong><br>当前指标仅衡量整体相似度，可引入基于部件的检测器（Part-DINO）评估把手、按键等细节是否一致。</p>
</li>
<li><p><strong>光照与阴影专用指标</strong><br>提出 Shadow-F1、Illumination-Error 等可量化阴影轮廓、亮度直方图匹配度，减少主观评价成本。</p>
</li>
<li><p><strong>长视频一致性基准</strong><br>构建 5-10 分钟长镜头数据集，考察 mask 漂移、外观褪色、累积伪影，推动长序列算法研究。</p>
</li>
</ul>
<p>5. 系统与落地</p>
<ul>
<li><p><strong>端侧轻量化</strong><br>将 4D 重建网络与扩散模型进行知识蒸馏+量化，部署到移动 SoC 或云端 GPU 实例，实现 30 fps 实时预览。</p>
</li>
<li><p><strong>版权与合规检测</strong><br>插入内容需确保不侵犯品牌/IP；可集成对抗水印或指纹，自动检测并拒绝未授权 logo/角色插入。</p>
</li>
<li><p><strong>A/B 商用效果量化</strong><br>与广告平台合作，在真实短视频流量中对比插入前后点击率、转化率，建立“视觉真实-商业收益”直接关联。</p>
</li>
<li><p><strong>失败案例自诊断</strong><br>构建异常检测分支，对明显几何错位、光照冲突、遮挡逻辑错误自动报警并回退到安全帧，提升生产流程鲁棒性。</p>
</li>
</ul>
<p>6. 理论前沿</p>
<ul>
<li><p><strong>4D 生成式反演</strong><br>探索扩散模型直接在 4D 隐空间（3D+时间）进行去噪，而非逐帧 2D 生成，理论上可一次性输出几何+外观，减少投影误差。</p>
</li>
<li><p><strong>物理可微渲染与扩散联合优化</strong><br>把可微路径追踪嵌入扩散训练循环，让阴影、反射梯度回传至物体姿态/材质，实现“自监督物理”。</p>
</li>
</ul>
<p>综上，InsertAnywhere 已建立几何-外观协同的 VOI 新范式，后续可在<strong>非刚性、全局光照、物理动力学、实时交互、真实数据与评测、系统落地</strong>等方向持续深化，推动虚拟制片、沉浸式广告、AR 购物等场景真正规模化应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>InsertAnywhere 论文核心内容速览</strong></p>
<ol>
<li><p>任务定位<br>解决“视频物体插入（VOI）”的商业级难题：既要<strong>几何一致</strong>（尺度、遮挡、运动），又要<strong>外观逼真</strong>（光照、阴影、时序稳定）。</p>
</li>
<li><p>技术路线 = 两阶段框架</p>
</li>
</ol>
<ul>
<li><p><strong>4D-aware Mask Generation</strong><br>– 用 Uni4D 从单目视频重建时序一致的 3D 场景+相机轨迹<br>– 用户首帧拖拽/缩放即可将参考物体点云  y<em>j  刚性变换到 3D 场景<br>– 通过 SEA-RAFT 场景流  V</em>(t→t+1)  把物体 centroid 随邻近物体一起移动<br>– 按相机内外参重投影并 SAM2 分割，得到<strong>遮挡正确、时序连贯</strong>的 mask 序列  M_t </p>
</li>
<li><p><strong>Video Object Insertion</strong><br>– 先用高保真图像模型在首帧 inpainting，确定物体外观与初始光照<br>– 以 Wan2.1-VACE-14B 为骨干，用 LoRA 在自建 ROSE++ 数据集上微调，学习<strong>联合重建物体+外部阴影/亮度变化</strong><br>– 输出完整视频，保持颜色、纹理、光照时序一致</p>
</li>
</ul>
<ol>
<li><p>训练数据 ROSE++<br>– 将原“物体移除”数据集反转为“插入”三元组：无物体视频｜有物体视频｜参考物体图<br>– 用 VLM 生成白底参考图，再用 DINO 相似度挑选，避免 copy-paste 伪影</p>
</li>
<li><p>实验结果<br>– 自建 VOIBench（100 段真实视频）+ 商业基线 Pika-Pro &amp; Kling<br>– <strong>CLIP-I / DINO-I / Imaging Quality / Multi-View Consistency 全面第一</strong><br>– 用户研究 20 人双盲投票，<strong>79% 认为物体真实、86% 认为遮挡正确、70% 整体最自然</strong><br>– 消融验证：4D mask 解决遮挡，首帧 inpainting 保 fidelity，ROSE++ LoRA 生成逼真阴影与光照</p>
</li>
<li><p>结论<br>InsertAnywhere 首次把 4D 场景几何与扩散视频生成紧耦合，实现<strong>几何准确、光照一致、可用户精控</strong>的视频物体插入，达到广告与影视后期可用的量产质量。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是InsertAnywhere%3A%20Bridging%204D%20Scene%20Geometry%20and%20Diffusion%20Models%20for%20Realistic%20Video%20Object%20Insertion，链接是https://arxiv.org/pdf/2512.17504，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.17504。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hoiyeong Jin,Hyojin Jang,Jeongho Kim,Junha Hyung,Kinam Kim,Dongjin Kim,Huijin Choi,Hyeonji Kim,Jaegul Choo</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17504.pdf">https://arxiv.org/pdf/2512.17504.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17504">https://arxiv.org/abs/2512.17504</a></p>
<p><strong>Arxiv ID</strong>: 2512.17504</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.17504">https://papers.cool/arxiv/2512.17504</a></p>
<p><strong>Published</strong>: 2025-12-19T12:14:36Z</p>
<p><strong>Updated</strong>: 2025-12-19T12:14:36.000Z</p>
<hr>
<h3 id="2-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding"><a href="#2-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding" class="headerlink" title="2. Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.17220">Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding</a></h3><p>Humans understand long and complex texts by relying on a holistic semantic representation of the content. This global view helps organize prior knowledge, interpret new information, and integrate evidence dispersed across a document, as revealed by the Mindscape-Aware Capability of humans in psychology. Current Retrieval-Augmented Generation (RAG) systems lack such guidance and therefore struggle with long-context tasks. In this paper, we propose Mindscape-Aware RAG (MiA-RAG), the first approach that equips LLM-based RAG systems with explicit global context awareness. MiA-RAG builds a mindscape through hierarchical summarization and conditions both retrieval and generation on this global semantic representation. This enables the retriever to form enriched query embeddings and the generator to reason over retrieved evidence within a coherent global context. We evaluate MiA-RAG across diverse long-context and bilingual benchmarks for evidence-based understanding and global sense-making. It consistently surpasses baselines, and further analysis shows that it aligns local details with a coherent global representation, enabling more human-like long-context retrieval and reasoning.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>人类通过依赖内容的整体语义表示来理解冗长且复杂的文本。这种全局视角有助于组织已有知识、解读新信息，并整合散布在文档各处的证据，这在心理学中被称为人类的“心智景观感知能力”。当前的增强检索生成（RAG）系统缺乏这种指导，因此在处理长上下文任务时表现不佳。本文提出了心智景观感知 RAG（MiA-RAG），这是首个为基于大语言模型的 RAG 系统提供明确全局上下文意识的方法。MiA-RAG 通过分层摘要构建心智景观，并以此全局语义表示为条件，同时指导检索和生成。这使得检索器能够形成更丰富的查询嵌入，生成器能够在一致的全局上下文中推理检索到的证据。我们在多种长上下文及双语基准测试中评估 MiA-RAG 的基于证据理解和全局感知能力。实验结果显示其持续超越基线模型，进一步分析表明，它能够将局部细节与一致的全局表示对齐，从而实现更接近人类的长上下文检索与推理能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对当前检索增强生成（RAG）系统在长文本理解任务中缺乏“全局语义框架”这一核心缺陷，提出并验证了 <strong>Mindscape-Aware RAG（MiA-RAG）</strong> 框架，旨在让大模型像人类一样，在检索与生成过程中始终受一份**全局语义摘要（mindscape）**的引导，从而提升对长文档的整合性理解、选择性检索与连贯推理能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中将与 MiA-RAG 相关的研究划分为两条主线，并指出它们与本文工作的区别。可归纳如下：</p>
<ol>
<li>Context-Aware Embeddings（上下文感知检索/嵌入）</li>
</ol>
<ul>
<li>长窗口编码：利用支持长上下文的 LLM 直接对片段进行编码，如 Chen et al. 2024、Sturua et al. 2024、Nussbaum et al. 2024、Wang et al. 2024a、Lee et al. 2024、Li et al. 2023、Voyage-AI 2025。</li>
<li>图结构增强：Xu et al. 2024 构建篇章关系图，用邻居信息增强片段表示。</li>
<li>局部语境训练：Wu et al. 2025（Sit-Emb）提出监督信号，让嵌入模型学会把片段置于其局部邻居语境中，取得 SOTA 检索效果。</li>
<li>与 MiA-RAG 的区别：上述方法仅增强“片段”表示的局部上下文，而 MiA-RAG 首次把“全局语义摘要”注入到“查询”表示，实现选择性检索，无需改动原始片段嵌入。</li>
</ul>
<ol>
<li>Long Context Compression（长文本压缩/全局记忆）</li>
</ol>
<ul>
<li>早期方法：基于 KV-cache 裁剪或 token 丢弃（Yang et al. 2024、Li et al. 2024、Xiao et al. 2023、Zhang et al. 2023）。</li>
<li>抽象压缩：MemoRAG（Qian et al. 2025）用压缩记忆生成中间线索，再引导检索；Titans（Behrouz et al. 2024）在测试时学习把历史上下文存入长期记忆模块。</li>
<li>与 MiA-RAG 的区别：MemoRAG 依赖多步生成式代理推理，开销大；Titans 侧重记忆更新机制。MiA-RAG 仅一次性构建层次摘要作为持久 mindscape，不引入额外推理步骤，直接条件化检索与生成。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“人类在阅读长文本时先激活全局语义框架（mindscape）再局部检索与推理”的心理学/神经科学机制，转化为可计算的 <strong>Mindscape-Aware RAG（MiA-RAG）</strong> 框架。核心思路是：先用层次摘要构建一份<strong>全局语义摘要 S</strong>，然后把 S 同时注入检索器与生成器，使二者始终在统一的全局语境下工作。具体实现分三步：</p>
<ol>
<li>构建 Mindscape<br>对长文档做<strong>两层次摘要</strong>：</li>
</ol>
<ul>
<li>片段级：用 GPT-4o 将每个 chunk ci 压缩成 si</li>
<li>文档级：将所有 si 拼接后再压缩，得到全局摘要 S<br>该 S 作为后续检索与生成的“外部全局记忆”。</li>
</ul>
<ol>
<li>Mindscape-Aware 检索（MiA-Emb）</li>
</ol>
<ul>
<li>输入格式：把查询 q 与 S 拼接成统一序列</li>
</ul>
<p>Q = [INST]_(emb); q; d_q; S; d_n|d_c</p>
<p>其中 d_q、d_n、d_c 为任务分隔符。</p>
<ul>
<li>残差融合：取“查询结束”与“任务结束”两位置的隐藏状态，按</li>
</ul>
<p>q<em>t = δ·h</em>(q) + (1−δ)·h_(t)</p>
<p>得到既保留原始查询意图、又注入全局语义的 enriched query embedding。</p>
<ul>
<li>对比训练：在自动构造的“银标准”证据（chunk &amp; 知识图谱节点）上用 InfoNCE 多任务损失微调嵌入模型，使查询表示被拉向与 S 语义一致的证据子空间。</li>
</ul>
<ol>
<li>Mindscape-Aware 生成（MiA-Gen）</li>
</ol>
<ul>
<li>训练数据：把 S 与检索到的混合证据 Ĉret 一并作为上下文，构造</li>
</ul>
<p>x^(gen)<em>i = [INST]</em>(gen); S; Ĉret; q_i</p>
<p>用 NarrativeQA+CLIPPER 的 QA/Claim 样本进行监督微调。</p>
<ul>
<li>目标函数：标准自回归交叉熵损失，让模型学会在全局摘要的“语义脚手架”下解释局部证据、抑制噪声并完成答案生成。</li>
</ul>
<p>通过“同一全局摘要同时条件化检索与生成”，MiA-RAG 实现了：</p>
<ul>
<li><strong>Selective Retrieval</strong>：查询嵌入被导向与 S 一致的语义子空间，减少跨主题歧义；</li>
<li><strong>Enriched Understanding</strong>：摘要补充了查询中缺失的背景，缓解指代或省略带来的语义欠明；</li>
<li><strong>Integrative Reasoning</strong>：生成器在全局语境下整合分散证据，输出更连贯、忠实的长文本答案。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“长文本、跨领域、双语、多任务”四个维度展开系统实验，共包含 <strong>五项研究（Study I–V）</strong> 及配套消融与几何分析，具体设置与结论如下：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>研究</th>
<th>目的</th>
<th>关键数据集</th>
<th>主要指标</th>
<th>核心结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>I 检索能力</td>
<td>验证 MiA-Emb 是否因全局摘要而提升召回</td>
<td>NarrativeQA、DetectiveQA-ZH/EN</td>
<td>Recall@3/5/10</td>
<td>MiA-Emb-8B 平均召回比最强基线 Sit-Emb 提高 ≈+4–6%，消融“w/o Summary”显著下降</td>
</tr>
<tr>
<td>II 端到端 QA</td>
<td>评估完整 MiA-RAG 在长文本问答与推理上的综合表现</td>
<td>NarrativeQA、∞Bench-EN.MC、DetectiveQA、NoCha</td>
<td>F1/Acc/Pair-Acc</td>
<td>MiA-RAG-14B 五基准平均得分 68.11，超越 72B vanilla RAG（59.48），绝对增益 +8.6%</td>
</tr>
<tr>
<td>III GraphRAG 全局问答</td>
<td>检验 mindscape 对“全局感”问答的增益</td>
<td>自建 300 条 QMSum/GOV/VCSum 全局问题</td>
<td>GPT-4o pairwise 胜率</td>
<td>MiA-Emb 在 Comprehensiveness、Diversity、Empowerment 三维度胜率均 &gt;70%，显著优于无 mindscape 基线</td>
</tr>
<tr>
<td>IV 模型规模影响</td>
<td>验证 mindscape 是否比单纯放大模型更有效</td>
<td>0.6B→8B（Emb）、1.5B→72B（Gen）</td>
<td>同上</td>
<td>MiA-Emb-0.6B 已超 Vanilla-8B；MiA-Gen-14B 追平或超越 Vanilla-72B，说明“全局语义”收益高于“堆参数”</td>
</tr>
<tr>
<td>V 摘要质量鲁棒性</td>
<td>测试若摘要质量下降，性能是否崩溃</td>
<td>NarrativeQA</td>
<td>F1</td>
<td>用 7B→32B 模型替代 GPT-4o 生成摘要，F1 仅波动 ±1–2 分，表明框架对摘要误差具有鲁棒性</td>
</tr>
</tbody>
</table>
</div>
<p>配套分析</p>
<ul>
<li><strong>消融实验</strong>：去掉 Summary、去掉 QA/Claim 任一监督信号，性能均显著下降，确认 mindscape 与多任务监督缺一不可。</li>
<li><strong>几何分析</strong>：t-SNE 显示 MiA-Emb 的查询嵌入与对应文档语义子空间夹角平均缩小 6.4°（≈14.5% 相对提升），直观验证 Selective Retrieval。</li>
<li><strong>注意力分析</strong>：<br>– MiA-Emb 在中层同时出现“摘要注意力↑+召回↑”，表明模型把全局线索注入查询表示。<br>– MiA-Gen 的 MCEA 分数在 middle/late 层显著高于 vanilla，且替换摘要即跌落，证明其利用 mindscape 进行 Integrative Reasoning 而非位置/长度启发。</li>
</ul>
<p>综上，实验覆盖检索、端到端问答、全局感问答、规模缩放与摘要质量五大方面，结果一致表明：<strong>引入统一的全局语义摘要比单纯扩大模型或上下文长度更能系统性提升长文本理解与推理能力</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为 MiA-RAG 的“直接外延”或“深层机理”探索，均未被原文系统讨论，且具备实验可行性：</p>
<ol>
<li>动态/在线 mindscape</li>
</ol>
<ul>
<li>流式场景：文档随时间持续追加（会议纪要、聊天记录）。需设计增量式摘要更新策略，避免每轮重新压缩全量文本。</li>
<li>触发机制：基于信息熵或事件检测，仅在“语义漂移”显著时才触发摘要刷新，平衡延迟与开销。</li>
</ul>
<ol>
<li>多模态 mindscape</li>
</ol>
<ul>
<li>文本-图像-音频混合长文档（教科书、播客、幻灯片）。探索跨模态层次摘要（文本→视觉→音频）的统一语义锚定，及跨模态检索-生成联合训练。</li>
</ul>
<ol>
<li>个性化 mindscape</li>
</ol>
<ul>
<li>用户历史交互作为“私有摘要”，与文档全局摘要融合，实现个性化检索与回答。可引入用户-文档双塔结构，学习加权融合比例。</li>
</ul>
<ol>
<li>可解释/可编辑 mindscape</li>
</ol>
<ul>
<li>将摘要显式拆分为“实体-事件-关系”三元组，支持用户手动增删改，实时反馈到检索分布，实现“人在回路”的长文本知识库维护。</li>
</ul>
<ol>
<li>更细粒度摘要架构</li>
</ol>
<ul>
<li>树状或图谱式多层摘要（章节-段落-句子），检索时按需展开不同层级，兼顾全局语境与局部细节，进一步减少 token 开销。</li>
</ul>
<ol>
<li>低资源语言与领域迁移</li>
</ol>
<ul>
<li>检验 mindscape 在低资源语言、专业领域（法律、医疗）是否仍有效；若摘要模型性能骤降，能否通过“摘要蒸馏+领域继续预训练”弥补。</li>
</ul>
<ol>
<li>评估体系升级</li>
</ol>
<ul>
<li>现有指标侧重答案正确性，可引入“全局一致性分数”（Global-Coherence Score）与“证据分散度”（Evidence Spread），衡量模型整合多远距离片段的能力。</li>
</ul>
<ol>
<li>与长上下文 LLM 的“内-外”比较</li>
</ol>
<ul>
<li>当基座模型支持 1M+ token 时，mindscape 是否仍能提供额外增益？可设计“同等推理预算”对比：全输入 vs 摘要+检索 的准确率-延迟-成本 Pareto 前沿。</li>
</ul>
<ol>
<li>摘要-检索-生成的三端联合训练</li>
</ol>
<ul>
<li>目前摘要由 GPT-4o 离线生成，若将层次摘要模块可微地接入检索与生成损失，实现端到端联合优化，可能进一步提升摘要质量与任务性能。</li>
</ul>
<ol>
<li>安全性与鲁棒性</li>
</ol>
<ul>
<li>摘要模型引入的幻觉会向下游传递。可研究“对抗摘要扰动”训练（Adv-Mindscape），或利用不确定性估计动态决定是否信任摘要信号。</li>
</ul>
<p>这些方向既涵盖“实时性-多模态-个性化”应用需求，也涉及“可解释-联合优化-安全”研究深度，可作为 MiA-RAG 之后长文本理解领域的持续探索路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Mindscape-Aware RAG (MiA-RAG)</strong>，首次把“全局语义框架”显式注入检索与生成，系统解决长文本理解难题。核心内容可概括为四句话：</p>
<ol>
<li>受心理学“schema–gist”理论启发，用<strong>两层次摘要</strong>离线构建全局语义锚点  S 。</li>
<li>训练 <strong>MiA-Emb</strong>：把  S  与查询  q  联合编码，经残差融合得到既保留原始意图、又对齐全局语义的查询嵌入，实现<strong>选择性检索</strong>。</li>
<li>训练 <strong>MiA-Gen</strong>：把  S  与检索证据同时喂给生成器，让答案在全局语境下整合分散证据，实现<strong>整合性推理</strong>。</li>
<li>在 5 个英/汉长文本基准、3/5/10-chunk 设置下，MiA-RAG-14B 平均得分 68.11，<strong>超越 72B vanilla RAG 8.6%</strong>；几何与注意力分析证实  S  将查询拉向正确语义子空间，并引导生成器关注与摘要一致的证据。</li>
</ol>
<p>综上，MiA-RAG 用一份轻量级全局摘要统一检索与生成，<strong>以 14B 参数实现 72B 级长文本理解性能</strong>，为长上下文任务提供了“人类式先见全局、再究细节”的新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Mindscape-Aware%20Retrieval%20Augmented%20Generation%20for%20Improved%20Long%20Context%20Understanding，链接是https://arxiv.org/pdf/2512.17220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.17220。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yuqing Li,Jiangnan Li,Zheng Lin,Ziyan Zhou,Junjie Wu,Weiping Wang,Jie Zhou,Mo Yu</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.17220.pdf">https://arxiv.org/pdf/2512.17220.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.17220">https://arxiv.org/abs/2512.17220</a></p>
<p><strong>Arxiv ID</strong>: 2512.17220</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.17220">https://papers.cool/arxiv/2512.17220</a></p>
<p><strong>Published</strong>: 2025-12-19T04:08:29Z</p>
<p><strong>Updated</strong>: 2025-12-19T04:08:29.000Z</p>
<hr>
<h3 id="3-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents"><a href="#3-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents" class="headerlink" title="3. MAI-UI Technical Report: Real-World Centric Foundation GUI Agents"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.22047">MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</a></h3><p>The development of GUI agents could revolutionize the next generation of human-computer interaction. Motivated by this vision, we present MAI-UI, a family of foundation GUI agents spanning the full spectrum of sizes, including 2B, 8B, 32B, and 235B-A22B variants. We identify four key challenges to realistic deployment: the lack of native agent-user interaction, the limits of UI-only operation, the absence of a practical deployment architecture, and brittleness in dynamic environments. MAI-UI addresses these issues with a unified methodology: a self-evolving data pipeline that expands the navigation data to include user interaction and MCP tool calls, a native device-cloud collaboration system routes execution by task state, and an online RL framework with advanced optimizations to scale parallel environments and context length. MAI-UI establishes new state-of-the-art across GUI grounding and mobile navigation. On grounding benchmarks, it reaches 73.5% on ScreenSpot-Pro, 91.3% on MMBench GUI L2, 70.9% on OSWorld-G, and 49.2% on UI-Vision, surpassing Gemini-3-Pro and Seed1.8 on ScreenSpot-Pro. On mobile GUI navigation, it sets a new SOTA of 76.7% on AndroidWorld, surpassing UI-Tars-2, Gemini-2.5-Pro and Seed1.8. On MobileWorld, MAI-UI obtains 41.7% success rate, significantly outperforming end-to-end GUI models and competitive with Gemini-3-Pro based agentic frameworks. Our online RL experiments show significant gains from scaling parallel environments from 32 to 512 (+5.2 points) and increasing environment step budget from 15 to 50 (+4.3 points). Finally, the native device-cloud collaboration system improves on-device performance by 33%, reduces cloud model calls by over 40%, and preserves user privacy.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>GUI代理的发展可能会彻底改变下一代人机交互。受这一愿景的激励，我们提出了MAI-UI，这是一系列基础GUI代理，涵盖从2B、8B、32B到235B-A22B的全尺寸范围。我们确定了实现实际部署的四个关键挑战：缺乏原生的代理-用户交互、仅限UI操作的局限性、缺乏可行的部署架构，以及在动态环境中的脆弱性。MAI-UI通过统一的方法论应对这些问题：一个自我进化的数据管道，将导航数据扩展为包含用户交互和MCP工具调用；一个原生的设备-云协作系统，通过任务状态路由执行；以及一个具有高级优化的在线强化学习框架，用于扩展并行环境和上下文长度。MAI-UI在GUI定位和移动导航领域建立了新的最先进水平。在定位基准测试中，它在ScreenSpot-Pro上达到73.5%，在MMBench GUI L2上达到91.3%，在OSWorld-G上达到70.9%，在UI-Vision上达到49.2%，超过了Gemini-3-Pro和Seed1.8在ScreenSpot-Pro上的表现。在移动GUI导航中，它在AndroidWorld上创下76.7%的新SOTA，超过了UI-Tars-2、Gemini-2.5-Pro和Seed1.8。在MobileWorld上，MAI-UI取得了41.7%的成功率，显著优于端到端GUI模型，并与基于Gemini-3-Pro的代理框架具有竞争力。我们的在线强化学习实验显示，通过将并行环境规模从32扩展到512，可以显著提升性能（+5.2点），并且通过将环境步数预算从15增加到50，可再提升4.3点。最后，原生设备-云协作系统提升了设备端性能33%，减少了40%以上的云模型调用，并保护了用户隐私。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在弥合当前 GUI 代理（GUI agent）在实验室表现与真实场景可靠部署之间的四大鸿沟：</p>
<ol>
<li><p><strong>缺乏原生“代理–用户”交互能力</strong><br>现有系统多为端到端执行，忽略用户指令常含糊、不完整的情况；代理需主动澄清、追问、征得同意才能对齐真实意图。</p>
</li>
<li><p><strong>纯 UI 操作的局限</strong><br>长链条 UI 动作易累积误差，且大量任务在移动端根本不可达。论文引入 Model Context Protocol（MCP）工具调用，把冗长 UI 序列压缩为少量 API 调用，并解锁传统只能在桌面完成的工作流。</p>
</li>
<li><p><strong>缺少可落地的设备–云协同架构</strong><br>纯云方案带来隐私、成本、网络依赖；纯端方案受算力限制。论文提出<strong>原生</strong>设备–云协同系统，按任务状态与数据敏感度动态路由，兼顾性能、隐私与成本。</p>
</li>
<li><p><strong>对动态环境的鲁棒性不足</strong><br>静态离线轨迹训练导致过拟合，代理在真实多变的界面（弹窗、权限框、版本差异）中极易失效。论文通过在线强化学习让代理在 500+ 并行容器化环境中持续交互，提升对未知扰动的泛化能力。</p>
</li>
</ol>
<p>综上，MAI-UI 以“自演化数据管线 + 设备云协同 + 在线 RL”三位一体，系统性地解决上述四大痛点，实现可规模部署的“真实世界为中心”的基础 GUI 代理。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 4 节系统回顾了两大相关方向，并指出 MAI-UI 与它们的差异。以下按“GUI Grounding”与“GUI Navigation”两条主线归纳代表性研究，并补充 MAI-UI 的相对位置。</p>
<p>GUI Grounding（单步定位）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法/模型</th>
<th>技术路线</th>
<th>关键特点</th>
<th>与 MAI-UI 的差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>JEDI、OS-Atlas、Aguvis、UGround、Aria-UI</td>
<td>纯监督微调（SFT）</td>
<td>大规模截图–文本对，直接回归坐标</td>
<td>缺乏推理路径与 RL 微调，泛化性弱</td>
</tr>
<tr>
<td>GUI-R1、GUI-Actor、GTA1</td>
<td>纯强化学习（RL）</td>
<td>用 GRPO 直接优化“点是否在框”奖励</td>
<td>无 SFT 预热，样本效率低，易崩溃</td>
</tr>
<tr>
<td>InfiGUI-R1、Phi-Ground</td>
<td>SFT+RL 两段式</td>
<td>先做 SFT，再用 RL 细化</td>
<td>未解决“策略崩塌”，推理视角单一</td>
</tr>
<tr>
<td>MAI-UI（本文）</td>
<td>Instruction-as-Reasoning SFT → RL</td>
<td>先让模型生成“外观/功能/位置/意图”四视角推理链，再用 RL 动态选择最佳视角</td>
<td>引入结构化推理，稳定 RL 优化，缓解崩塌</td>
</tr>
</tbody>
</table>
</div>
<p>GUI Navigation（多步决策）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法/模型</th>
<th>技术路线</th>
<th>关键特点</th>
<th>与 MAI-UI 的差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>早期框架（Mobile-Agent、Ponder&amp;Press 等）</td>
<td>模块化 pipeline：规划器 + 定位器</td>
<td>用大模型做高层规划，小模型做 grounding</td>
<td>非端到端，误差累积，延迟高</td>
</tr>
<tr>
<td>端到端大模型（UI-TARS、UI-Venus、Step-GUI、OpenCUA）</td>
<td>VLA 统一架构，SFT 为主</td>
<td>单模型输出动作序列，支持跨 App</td>
<td>无在线 RL，训练环境静态；无用户交互与 MCP</td>
</tr>
<tr>
<td>端侧小模型（Ferret-UI Lite、LightAgent）</td>
<td>3B-7B 参数，蒸馏/量化</td>
<td>低延迟、可离线</td>
<td>能力有限，复杂任务成功率低</td>
</tr>
<tr>
<td>MAI-UI（本文）</td>
<td>端到端 + 在线 RL + 设备–云协同</td>
<td>1. 自演化数据持续扩增2. 并行 512 容器环境在线 rollout3. 本地 2B 与云端 32/235B 动态切换</td>
<td>首次把“在线 RL”“用户交互”“MCP 工具”“端云协同”同时纳入端到端训练与推理框架</td>
</tr>
</tbody>
</table>
</div>
<p>工具使用与用户交互</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法/基准</th>
<th>技术路线</th>
<th>关键特点</th>
<th>与 MAI-UI 的差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>Anthropic MCP 官方示例</td>
<td>协议层标准</td>
<td>定义工具描述、调用、返回格式</td>
<td>仅协议，无训练数据与端到端模型</td>
</tr>
<tr>
<td>MobileWorld 基准（同期工作）</td>
<td>评估协议</td>
<td>引入“用户澄清”与“MCP 工具”两类任务</td>
<td>MAI-UI 作为首个端到端模型在该基准取得 SOTA</td>
</tr>
</tbody>
</table>
</div>
<p>强化学习系统优化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>系统/工作</th>
<th>技术路线</th>
<th>关键特点</th>
<th>与 MAI-UI 的差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>verl、DAPO、DeepSpeed-RLHF</td>
<td>分布式 RL 框架</td>
<td>支持数学/代码等 Stateless 环境</td>
<td>无法处理 GUI 的状态化、长序列、高并发</td>
</tr>
<tr>
<td>MAI-UI 在线 RL</td>
<td>异步 rollout + 混合并行（TP+PP+CP）</td>
<td>容器化 Android 快照，512 并行环境，百万 token 级轨迹端到端训练</td>
<td>首次把“有状态 GUI 环境”规模化接入 RL 训练，解决长视界与 GPU 显存瓶颈</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<ul>
<li><strong>Grounding 方向</strong>：MAI-UI 通过“Instruction-as-Reasoning”先注入多视角推理链，再用 RL 动态选择，解决纯 RL 不稳定、纯 SFT 无推理的问题。</li>
<li><strong>Navigation 方向</strong>：MAI-UI 首次将“在线 RL + 用户交互 + MCP 工具 + 端云协同”统一在端到端框架内，并在 AndroidWorld、MobileWorld 同时取得 SOTA，显著拉开与既有端到端模型或模块化框架的差距。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 MAI-UI 框架，用“三条主线 + 一套模型族”系统性地回应四大痛点，具体做法如下：</p>
<p>1. 自演化数据管线——解决“交互缺失 &amp; 纯 UI 局限”</p>
<ul>
<li><strong>三源混合</strong>：人工标注轨迹 + 拒绝采样轨迹 + 自动 rollout 轨迹，持续注入新任务。</li>
<li><strong>动作空间扩展</strong>：在常规 UI 动作外新增</li>
<li><code>ask_user</code>：当指令缺失关键信息时主动提问；</li>
<li><code>mcp_call</code>：一键调用外部 API（地图、GitHub、股票等），把长 UI 序列压缩为 1-2 次工具调用。</li>
<li><strong>迭代拒绝采样</strong>：用当前模型 rollout → 细粒度正确性判断 → 只保留高质量或“最长正确前缀”→ 再训练，实现模型与数据分布共同演化。</li>
</ul>
<p>2. 原生设备–云协同系统——解决“部署架构空白”</p>
<ul>
<li><strong>统一轨迹内存</strong>：本地维护“指令-截图-动作-思维”全量历史，云端可无缝续跑。</li>
<li><strong>本地监控器</strong>（与 2B 代理同模型）：</li>
<li>实时判断轨迹是否偏离；</li>
<li>生成<strong>错误摘要</strong>（error summary），云端续跑时直接读取，减少重复探索。</li>
<li><strong>隐私门控</strong>：若当前状态含敏感字段（密码、Token），即使偏离也强制留端，用户隐私零上云。</li>
<li><strong>效果</strong>：AndroidWorld 上 2B 模型单独仅 49.1%，协同后→ 65.5%，相对提升 33%；云调用次数降低 42.7%，40%+ 任务完全端侧完成。</li>
</ul>
<p>3. 在线强化学习——解决“动态环境脆弱”</p>
<ul>
<li><strong>可扩展有状态环境</strong>：</li>
<li>Docker 容器化 Android 镜像（AVD + 自托管后端 + REST API），单物理机可起 50+ 实例；</li>
<li>集中式 Environment Manager 跨机调度，512 环境并行 rollout。</li>
<li><strong>长视界训练优化</strong>：</li>
<li>异步 Agent Loop：推理与环境交互解耦，GPU 零等待；</li>
<li>混合并行（TP+PP+CP）把百万 token 轨迹切片到多卡，显存占用恒定。</li>
<li><strong>课程与奖励</strong>：</li>
<li>按当前 pass@K 把任务分四级（frontier / exploration / near-mastery / exploitation），自动调整采样比例；</li>
<li>轨迹级二元奖励 + 动作级重复惩罚，配合经验回放池，保证持续学习信号。</li>
<li><strong>结果</strong>：8B 模型 SFT→RL 提升 6.0 pp；环境数从 32→512 再涨 5.2 pp；步数预算 15→50 再涨 4.3 pp。</li>
</ul>
<p>4. Instruction-as-Reasoning grounding——解决“定位精度 &amp; RL 策略崩塌”</p>
<ul>
<li><strong>SFT 阶段</strong>：对同一元素生成“外观/功能/位置/意图”四视角指令，让模型先学会“为什么点这里”。</li>
<li><strong>RL 阶段</strong>：用 GRPO 优化“点是否在框”奖励，模型可<strong>动态选择或组合</strong>视角，形成上下文自适应推理链。</li>
<li><strong>Zoom-In 推理</strong>：先粗定位→裁剪半屏大图→再精定位，高分辨率专业界面（ScreenSpot-Pro）绝对提升 5.6 pp。</li>
</ul>
<p>5. 全尺寸模型族——解决“部署约束多样”</p>
<ul>
<li>2B / 8B / 32B / 235B-A22B 四档，统一训练配方；</li>
<li>2B 专精端侧，8-32B 平衡延迟与能力，235B 放云端；</li>
<li>同参数级别均取得 SOTA：</li>
<li>Grounding：ScreenSpot-Pro 73.5%、MMBench-GUI 91.3%、OSWorld-G 70.9%、UI-Vision 49.2%；</li>
<li>Navigation：AndroidWorld 76.7%、MobileWorld 41.7%，显著高于 UI-Tars-2、Gemini-2.5-Pro 等。</li>
</ul>
<p>总结</p>
<p>通过“数据-训练-部署”全链路设计，MAI-UI 把<strong>用户交互、MCP 工具、端云协同、在线 RL</strong> 首次整合进同一端到端框架，并给出可落地的多尺寸模型，系统性地解决了 GUI 代理走向真实世界的四大核心障碍。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从“GUI 定位–静态导航–在线导航–真实场景–系统消融”五个层次展开实验，覆盖 15+ 公开基准与自建 MobileWorld，并辅以多组消融与案例可视化。核心结果如下：</p>
<p>1. GUI Grounding（单步定位）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>数据特点</th>
<th>MAI-UI 代表成绩 (SOTA↑)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ScreenSpot-Pro</td>
<td>高分辨率专业软件，6 领域 12 类细粒度 UI</td>
<td>32B+zoom-in 73.5% (+4.3 vs GTA1-32B)</td>
</tr>
<tr>
<td>UI-Vision</td>
<td>多视角指令（基本/功能/空间）</td>
<td>32B+zoom-in 49.2% (+12.4 vs UI-Venus-72B)</td>
</tr>
<tr>
<td>MMBench-GUI L2</td>
<td>跨平台分层指令，含高级目标描述</td>
<td>32B 91.3% (+7.9 vs 前最佳)</td>
</tr>
<tr>
<td>OSWorld-G / -Refine</td>
<td>桌面复杂布局+细粒度操作</td>
<td>32B+zoom-in 70.9% / 75.0%</td>
</tr>
<tr>
<td>ScreenSpot-V2</td>
<td>移动/桌面/网页+文本/图标</td>
<td>32B 96.5% 平均，全面领先</td>
</tr>
</tbody>
</table>
</div>
<p><strong>结论</strong>：2B-32B 全尺度均超同规模模型；zoom-in 策略平均再提 2-6 pp。</p>
<p>2. 离线导航（静态数据集）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>任务特点</th>
<th>成绩</th>
</tr>
</thead>
<tbody>
<tr>
<td>Android Control-high</td>
<td>单 App 高层指令</td>
<td>32B 75.5%</td>
</tr>
<tr>
<td>GUI Odyssey</td>
<td>跨 App 导航 83 场景</td>
<td>32B 83.4%（新 SOTA）</td>
</tr>
</tbody>
</table>
</div>
<p>3. 在线导航（真实模拟器）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>动态/并发</th>
<th>成绩</th>
</tr>
</thead>
<tbody>
<tr>
<td>AndroidWorld (116 任务 20 App)</td>
<td>实时弹窗、权限、后台刷新</td>
<td>235B-A22B 76.7% (+3.4 vs UI-Tars-2)</td>
</tr>
<tr>
<td>8B 70.7% (+9.3 vs GUI-Owl-7B)</td>
</tr>
<tr>
<td>2B 49.1% (+21.1 vs Ferret-UI-Lite-3B)</td>
</tr>
</tbody>
</table>
</div>
<p>4. 真实场景能力（MobileWorld）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>子集</th>
<th>任务数</th>
<th>235B-A22B 成绩</th>
<th>领先幅度</th>
</tr>
</thead>
<tbody>
<tr>
<td>GUI-only</td>
<td>116</td>
<td>39.7%</td>
<td>+13.4 vs 最强端到端</td>
</tr>
<tr>
<td>User-Interaction</td>
<td>45</td>
<td>51.1%</td>
<td>+18.7 vs 前最佳</td>
</tr>
<tr>
<td>MCP-tool</td>
<td>40</td>
<td>37.5%</td>
<td>+32.1 vs 前最佳</td>
</tr>
<tr>
<td>Overall</td>
<td>201</td>
<td>41.7%</td>
<td>+20.8 vs Doubao-1.5-UI-TARS</td>
</tr>
</tbody>
</table>
</div>
<p>与“GPT-5/Gemini-3-Pro + 外部规划器”等框架对比仍具竞争力（43.8%-51.7% 区间）。</p>
<p>5. 系统消融与深度分析</p>
<p>5.1 设备–云协同 (DCC)</p>
<ul>
<li>2B 纯本地 49.1% → DCC 65.5% （+33.4% 相对）</li>
<li>随机切换基线 57.8%；去除错误摘要 −6.9 pp</li>
<li>云调用步数占比从 100% 降到 57.3%，42.7% 步长在端完成；40.5% 任务完全本地闭环</li>
<li>隐私门控案例：密码输入场景强制留端，零敏感数据上云</li>
</ul>
<p>5.2 在线 RL 关键因子</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>因子</th>
<th>8B 模型增益</th>
</tr>
</thead>
<tbody>
<tr>
<td>SFT→RL (50 步)</td>
<td>+6.0 pp</td>
</tr>
<tr>
<td>仅标准 GRPO</td>
<td>+1.8 pp；课程+重复惩罚+经验回放 额外 +4.2 pp</td>
</tr>
<tr>
<td>交互预算 15→30→50 步</td>
<td>+1.7→+3.8→+6.0 pp</td>
</tr>
<tr>
<td>并行环境 32→512</td>
<td>65.5→70.7%（+5.2 pp）</td>
</tr>
<tr>
<td>图像分辨率 540→720→1080p</td>
<td>720p 与 1080p 等效，训练提速 50%</td>
</tr>
</tbody>
</table>
</div>
<p>5.3 鲁棒性案例</p>
<ul>
<li>意外权限框、弹窗：RL 模型可自动 dismiss 并续任务（图 13）</li>
<li>走错 App：自动回退并修正轨迹完成报销去重（图 14）</li>
</ul>
<p>5.4 Grounding 视角分析</p>
<ul>
<li>可视化显示模型能随上下文切换“外观/功能/位置/意图”视角，甚至组合/自创新角度（图 15）</li>
<li>引入推理链后 RL 阶段探索更稳定，策略崩塌现象显著减轻</li>
</ul>
<p>6. 补充实验</p>
<ul>
<li><strong>ScreenSpot-V2</strong> 与 <strong>OSWorld-G-Refine</strong> 全类目细分结果见附录表 10-11，MAI-UI 在各平台（移动-桌面-Web）文本/图标任务均列第一或第二。</li>
<li><strong>训练曲线</strong>：图 12 给出奖励 EMA 与并行环境 Scaling 曲线，显示 512 环境收敛最快且终点最高。</li>
</ul>
<p>结论</p>
<p>实验覆盖“定位-离线-在线-真实-系统”五层，共 15+ 基准、200+ 定量指标与多组消融，验证：</p>
<ol>
<li>同尺度模型全面 SOTA，235B 取得 AndroidWorld 76.7%、MobileWorld 41.7% 新纪录；</li>
<li>设备–云协同让 2B 端侧模型绝对提升 16 pp，云调用减少 40% 以上；</li>
<li>在线 RL 与课程设计带来持续增益，且对弹窗、走错 App 等真实扰动显著鲁棒。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下问题在论文末尾虽未展开，却是把 GUI 代理真正推向“日常可用”的关键跳板，可作为后续研究路线图：</p>
<p>1. 统一动作空间与跨平台内核</p>
<ul>
<li>现有 UI 动作仍依赖 Android Accessibility / PC UIAutomation 等私有 API，不同 OS 指令集差异大。<br><strong>探索</strong>：基于 OS-agnostic 的“最小公共动作集”+ 可学习适配层，实现同一套权重在 Windows/macOS/Android/Web 上零样本泛化。</li>
</ul>
<p>2. 多模态统一观测</p>
<ul>
<li>目前以“截图 + a11y-tree”为主，丢失音频、震动、手势、系统日志等高维信号。<br><strong>探索</strong>：把音频提示、传感器事件、屏幕时序帧统一 token 化，训练真正的多模态时空 Transformer，提升对动态通知、语音助手的感知。</li>
</ul>
<p>3. 持续/增量学习</p>
<ul>
<li>在线 RL 阶段一旦收敛即冻结，后续新 App、新布局需重新训练。<br><strong>探索</strong>：</li>
<li>参数高效微调（LoRA/AdaLoRA）+ 经验回放，实现“热更新”不遗忘；</li>
<li>引入 Episodic Memory 或参数隔离方法，支持终身学习。</li>
</ul>
<p>4. 可解释性与用户对齐</p>
<ul>
<li>代理“先斩后奏”易引发信任危机。<br><strong>探索</strong>：</li>
<li>轨迹级因果归因，可视化“哪一步推理导致该动作”；</li>
<li>人在回路强化学习（IHRL），让用户实时纠正并反馈奖励，对齐个人偏好。</li>
</ul>
<p>5. 安全与对抗鲁棒</p>
<ul>
<li>恶意网页或 App 可注入对抗像素诱导错误点击。<br><strong>探索</strong>：引入视觉对抗训练 + 安全约束策略（Control Barrier），保证动作落在安全区域；结合形式化验证工具做可达性分析。</li>
</ul>
<p>6. 低资源与绿色 AI</p>
<ul>
<li>512 容器 × 50 步在线 RL 消耗万卡级 GPU 时。<br><strong>探索</strong>：</li>
<li>状态抽象 / 层次动作空间，缩短步数；</li>
<li>世界模型或 Dreamer-style 隐空间 rollout，减少真实环境调用；</li>
<li>用小型“学生环境模型”蒸馏大模型策略。</li>
</ul>
<p>7. 个性化与隐私计算</p>
<ul>
<li>端云协同仍把“轨迹”视为明文。<br><strong>探索</strong>：</li>
<li>联邦 RL（Fed-GRPO）：本地梯度/优势上传，参数聚合；</li>
<li>差分隐私 + 安全多方计算，确保云侧无法反推密码、聊天记录。</li>
</ul>
<p>8. 工具生态自动扩展</p>
<ul>
<li>MCP 工具目前手工编写 schema。<br><strong>探索</strong>：</li>
<li>自动化 Swagger/OpenAPI→MCP 转换器，让代理“即插即用”新 API；</li>
<li>自监督工具学习：阅读官方文档 HTML，自动生成调用示例并自我验证。</li>
</ul>
<p>9. 长时程任务规划与记忆</p>
<ul>
<li>超过 50 步的跨天任务（如“本周每天把发票截图发财务”）需外部记忆支持。<br><strong>探索</strong>：引入 episodic memory 或日历知识图谱，实现多会话一致性与定时触发。</li>
</ul>
<p>10. 社会技术交叉研究</p>
<ul>
<li>代理可自动操作支付、隐私页面，存在伦理风险。<br><strong>探索</strong>：建立“GUI 代理安全红队”与合规评测协议，研究自动化审计、责任归属与用户授权最小权限模型。</li>
</ul>
<p>可立即落地的短程课题</p>
<ol>
<li>在 MAI-UI 2B 上接入 LoRA 热更新插件，验证“新 App 发布 1 小时内自动适配”的增量学习流程。</li>
<li>把容器镜像换成轻量 Android Cuttlefish，测试 1024 并行环境是否继续线性提速。</li>
<li>将 MCP 工具集扩展到 100+ 常用企业 SaaS（Slack、Notion、Confluence），统计平均步长压缩比与失败率。</li>
</ol>
<p>总结</p>
<p>从“跨平台内核→终身学习→隐私计算→工具自生→社会合规”逐层深入，可形成下一代“可信赖、可持续、可扩展”的通用 GUI 代理研究主线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>MAI-UI 是一篇面向“真实世界可部署”的基础 GUI 代理论文，核心贡献可概括为 <strong>“一个模型族 + 三大系统 + 五项 SOTA”</strong>：</p>
<p>1. 研究动机（四大痛点）</p>
<ul>
<li>缺用户交互：指令含糊，代理不会追问</li>
<li>纯 UI 局限：长链条易错，移动无法完成桌面工作流</li>
<li>无原生部署架构：纯云贵且隐私风险，纯端能力弱</li>
<li>静态训练不鲁棒：弹窗、版本变化就失效</li>
</ul>
<p>2. 方法框架（三大系统）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>系统</th>
<th>关键机制</th>
<th>解决痛点</th>
</tr>
</thead>
<tbody>
<tr>
<td>自演化数据管线</td>
<td>拒绝采样 + 人工 + 自动 rollout 三线并行；动作空间扩展 ask_user 与 mcp_call</td>
<td>交互缺失 &amp; UI-only 局限</td>
</tr>
<tr>
<td>在线强化学习</td>
<td>512 容器化 Android 并行；异步 rollout + 混合并行支持百万 token 轨迹；课程 + 重复惩罚 + 经验回放</td>
<td>动态环境脆弱</td>
</tr>
<tr>
<td>原生设备–云协同</td>
<td>本地 2B 代理兼监控器，偏离且非敏感则切云端；统一轨迹内存 + 错误摘要</td>
<td>部署架构空白</td>
</tr>
</tbody>
</table>
</div>
<p>3. 模型族（同配方四尺寸）</p>
<ul>
<li><strong>2B</strong> → 端侧实时；<strong>8B/32B</strong> → 均衡；<strong>235B-A22B</strong> → 云端高能力</li>
<li>统一训练四阶段：SFT 感知→SFT 导航→RL 定位→在线 RL 导航</li>
</ul>
<p>4. 实验结果（五项新 SOTA）</p>
<ol>
<li><strong>GUI 定位</strong><br>ScreenSpot-Pro 73.5% │ UI-Vision 49.2% │ MMBench-GUI 91.3% │ OSWorld-G 70.9% │ ScreenSpot-V2 96.5%</li>
<li><strong>离线导航</strong> GUI Odyssey 83.4%</li>
<li><strong>在线导航</strong> AndroidWorld 76.7%（235B）、73.3%（32B）、49.1%（2B）</li>
<li><strong>真实场景</strong> MobileWorld 41.7% 整体，用户交互子集 51.1%，MCP 工具子集 37.5%</li>
<li><strong>系统消融</strong></li>
</ol>
<ul>
<li>端云协同让 2B 模型 +33% 相对性能，云调用 −42.7%</li>
<li>在线 RL 8B 模型 +6.0 pp；环境 32→512 再 +5.2 pp</li>
</ul>
<p>5. 结论</p>
<p>MAI-UI 首次把“用户交互、MCP 工具、端云协同、在线 RL”整合进同一端到端框架，全尺度模型均取得新最佳，显著缩小实验室与真实部署的差距，为“基础 GUI 代理”提供了一条可复制、可落地的技术路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MAI-UI%20Technical%20Report%3A%20Real-World%20Centric%20Foundation%20GUI%20Agents，链接是https://arxiv.org/pdf/2512.22047，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.22047。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hanzhang Zhou,Xu Zhang,Panrong Tong,Jianan Zhang,Liangyu Chen,Quyu Kong,Chenglin Cai,Chen Liu,Yue Wang,Jingren Zhou,Steven Hoi</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.22047.pdf">https://arxiv.org/pdf/2512.22047.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.22047">https://arxiv.org/abs/2512.22047</a></p>
<p><strong>Arxiv ID</strong>: 2512.22047</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.22047">https://papers.cool/arxiv/2512.22047</a></p>
<p><strong>Published</strong>: 2025-12-26T14:51:52Z</p>
<p><strong>Updated</strong>: 2025-12-26T14:51:52.000Z</p>
<hr>
<h3 id="4-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture"><a href="#4-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture" class="headerlink" title="4. UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21675">UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</a></h3><p>Multimodal large language models (MLLMs) have achieved remarkable progress in visual understanding tasks such as visual grounding, segmentation, and captioning. However, their ability to perceive perceptual-level image features remains limited. In this work, we present UniPercept-Bench, a unified framework for perceptual-level image understanding across three key domains: Aesthetics, Quality, Structure and Texture. We establish a hierarchical definition system and construct large-scale datasets to evaluate perceptual-level image understanding. Based on this foundation, we develop a strong baseline UniPercept trained via Domain-Adaptive Pre-Training and Task-Aligned RL, enabling robust generalization across both Visual Rating (VR) and Visual Question Answering (VQA) tasks. UniPercept outperforms existing MLLMs on perceptual-level image understanding and can serve as a plug-and-play reward model for text-to-image generation. This work defines Perceptual-Level Image Understanding in the era of MLLMs and, through the introduction of a comprehensive benchmark together with a strong baseline, provides a solid foundation for advancing perceptual-level multimodal image understanding.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态大语言模型（MLLMs）在视觉理解任务（如视觉定位、分割和图像描述）方面取得了显著进展。然而，它们在感知级图像特征的能力仍然有限。在本工作中，我们提出了 UniPercept-Bench，这是一个统一的感知级图像理解框架，涵盖三个关键领域：美学、质量、结构与纹理。我们建立了分层定义体系并构建了大规模数据集以评估感知级图像理解。在此基础上，我们开发了强基线 UniPercept，通过领域自适应预训练（Domain-Adaptive Pre-Training）和任务对齐强化学习（Task-Aligned RL）进行训练，使其能够在视觉评分（VR）和视觉问答（VQA）任务中实现稳健的泛化。UniPercept 在感知级图像理解上优于现有的 MLLMs，并可作为文本生成图像的即插即用奖励模型。本工作在 MLLMs 时代定义了感知级图像理解，并通过引入综合基准和强基线，为推进感知级多模态图像理解提供了坚实基础。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>多模态大模型（MLLM）在感知级图像理解（perceptual-level image understanding）上的系统性缺失</strong>：</p>
<ul>
<li>现有 MLLM 主要聚焦<strong>语义级任务</strong>（物体识别、视觉推理等），对<strong>美学、质量、结构与纹理</strong>等<strong>感知属性</strong>的评估与推理能力薄弱，预测不稳定、与人类感知偏差大。</li>
<li>该缺失导致内容创作、图像增强、生成模型对齐等下游应用难以获得<strong>可靠、可解释的感知信号</strong>。</li>
</ul>
<p>为此，论文提出：</p>
<ol>
<li><strong>UniPercept-Bench</strong>：首个覆盖美学评估（IAA）、质量评估（IQA）、结构与纹理评估（ISTA）的统一感知级基准，以<strong>Domain–Category–Criterion</strong>三级层级定义感知属性，并同时支持**视觉打分（VR）<strong>与</strong>视觉问答（VQA）**两种任务形式。</li>
<li><strong>UniPercept 基线模型</strong>：通过<strong>领域自适应预训练</strong>与<strong>任务对齐强化学习</strong>，在无需额外人工反馈的情况下，实现对三大感知域的稳健评分与问答，显著优于现有通用与专用模型，并可作为<strong>文本到图像生成模型的即插即用奖励模型</strong>，带来可控的感知质量提升。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Works”中系统梳理了与感知级图像理解相关的两条主线研究，并指出它们与 UniPercept 的区别。可归纳为以下两类：</p>
<ol>
<li>多模态大模型（MLLM）评测基准</li>
</ol>
<ul>
<li>语义-推理导向：</li>
<li>MMMU / MMMUPro：大学跨学科考试题，强调知识推理。</li>
<li>MEGA-Bench、MMStar、MMBench：日常图像的综合理解与推理。</li>
<li>MathVista：视觉场景中的数学/几何推理。</li>
<li>OCRBench：图像内文字识别与理解。</li>
<li>共同局限：先将视觉内容转为文本表征再做语言推理，<strong>不直接评估低层感知属性</strong>（美学、失真、纹理等）。</li>
</ul>
<ol>
<li>图像感知评估任务</li>
</ol>
<ul>
<li>美学评估（IAA）</li>
<li>Q-Align、UNIAA、ArtiMuse：提供打分或文本解释，但仅聚焦“好不好看”，<strong>无结构-纹理维度</strong>。</li>
<li>质量评估（IQA）</li>
<li>MUSIQ、DepictQA、DeQA、Q-Insight：预测失真严重程度或文本描述失真，<strong>未系统化覆盖结构/纹理</strong>。</li>
<li>结构-纹理评估（ISTA）</li>
<li>DTD、FMD 等早期纹理数据集：仅做纹理分类或分割，<strong>缺乏统一度量、层级定义及大规模 VQA/VR 评测</strong>。</li>
</ul>
<p>UniPercept-Bench 首次将 IAA、IQA、ISTA 整合到同一层级化框架，并提供<strong>并行的大规模 VR+VQA 任务</strong>，填补了“感知级”评测空白；UniPercept 模型则通过领域自适应预训练+任务对齐 RL，首次在统一模型中同时实现三大感知域的稳健评分与问答，显著超越既有通用 MLLM 与专用感知模型。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从“定义-数据-模型-应用”四个层面系统性地解决感知级图像理解缺失的问题：</p>
<ol>
<li>统一任务定义</li>
</ol>
<ul>
<li>提出三域层级框架</li>
<li>Domain：IAA（美学）、IQA（质量）、ISTA（结构&amp;纹理）</li>
<li>Category：每域细分 3–4 个感知大类（如 Composition &amp; Design、Distortion Location、Material Representation 等）</li>
<li>Criterion：再细化为 44 项可操作的感知准则（Visual Balance、Severity Level、Base Morphology 等）</li>
<li>同时支持两种任务形式</li>
<li>Visual Rating（VR）：0–100 连续打分</li>
<li>Visual Question Answering（VQA）：多选/Yes-No/Why 等模板，覆盖同一套准则</li>
</ul>
<ol>
<li>大规模高质量数据构建（UniPercept-Bench）</li>
</ol>
<ul>
<li>三阶段 pipeline<br>① 初始 QA 生成：用 GPT-4o 结合专业注释与问题模板库生成候选 QA 与理由<br>② 拒绝采样：用 Qwen2.5-VL-78B 从“问题有效性、答案有效性、推理有效性、准则相关性”四维度打分，淘汰 ≈40% 低质样本<br>③ 人工精修：训练后的志愿者修正边界案例，保证与人类感知一致</li>
<li>最终获得 ≈6k 张图、覆盖 44 准则、同时提供 VR 与 VQA 标签的统一基准</li>
</ul>
<ol>
<li>统一基线模型（UniPercept）</li>
</ol>
<ul>
<li>两阶段训练策略<br>① Domain-Adaptive Pre-Training：在 800k 图文对（含公开 IAA/IQA 数据与自建的 ISTA 结构化注释）上进行多任务预训练，使模型具备初步感知对齐能力<br>② Task-Aligned RL：采用 GRPO 算法，联合优化 VR 与 VQA<br>– VQA 奖励：二值正确性  r<em>(vqa)=1</em>(answer)=gt<br>– VR 奖励：Adaptive Gaussian Soft Reward</li>
</ul>
<p>r<em>(vr)=exp(-(|p_i-g_i|^2) / (2σ</em>(textdyn))^2), quad σ_(dyn)=σ_0(1+α(|p_i-g_i|) / (100))</p>
<p>避免阈值截断带来的梯度消失，实现连续数值回归的稳定更新</p>
<ul>
<li>输出策略：Token-as-Score，将离散 token 分布映射为 0–100 分值，避免额外回归头</li>
</ul>
<ol>
<li>下游验证与工具化</li>
</ol>
<ul>
<li>即插即用奖励模型：把 UniPercept 的 IAA、IQA、ISTA 三路评分作为 Flow-GRPO 的奖励信号，对 FLUX.1-dev 进行后训练；实验显示可<strong>定向且可控地</strong>提升生成图像的美学、清晰度或结构丰富度</li>
<li>统一度量：UniPercept 评分可直接用于评测任意图像（真实图或生成图），在 DPG、GenEval 等基准上揭示现有 T2I 模型在美学与纹理维度仍有显著差距</li>
<li>数据集诊断：对 ImageNet、Unsplash、LAION-5B、Blip3o-60K 等大规模数据做感知分布分析，发现 Unsplash 与 Blip3o-60K 在三大维度均领先，为后续数据筛选提供量化依据</li>
</ul>
<p>通过“统一定义-高质量数据-多任务对齐训练-工具化应用”的闭环，论文首次让 MLLM 在<strong>感知级图像理解</strong>上达到与人类高度一致、且可解释、可扩展、可直接服务生成模型优化的水平。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>UniPercept-Bench</strong> 与 <strong>UniPercept 模型</strong> 开展了三大组实验，覆盖 <strong>benchmark 评测、模型消融、下游应用验证</strong> 三个层面。</p>
<p>1 主基准评测（UniPercept-Bench）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>数据规模</th>
<th>评价指标</th>
<th>参评模型</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>Visual RatingIAA / IQA / ISTA</td>
<td>5 个公开数据集（ArtiMuse-10K 等）</td>
<td>SRCC / PLCC</td>
<td>18 个模型（5 专有 + 9 开源 + 4 专用）</td>
<td>UniPercept 在三域平均 SRCC/PLCC 达 0.824/0.827，显著超越最强专用模型 Q-Insight*（0.726/0.733）与 GPT-4o（0.643/0.655）。</td>
</tr>
<tr>
<td>VQAIAA（8 类）</td>
<td>≈6 k 题</td>
<td>准确率</td>
<td>同上</td>
<td>UniPercept 76.55 % 平均准确率，领先次优 InternVL3-78B（68.28 %）与 GPT-4o（60.04 %）。</td>
</tr>
<tr>
<td>VQAIQA（3 类）</td>
<td>≈6 k 题</td>
<td>准确率</td>
<td>同上</td>
<td>UniPercept 81.07 %，领先次优 LLaVA-OneVision-8B（72.15 %）。</td>
</tr>
<tr>
<td>VQAISTA（5 类）</td>
<td>≈6 k 题</td>
<td>准确率</td>
<td>同上</td>
<td>UniPercept 84.23 %，领先次优 LLaVA-OneVision-8B（81.13 %）。</td>
</tr>
</tbody>
</table>
</div>
<p>2 消融与策略分析</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>实验设置</th>
<th>指标</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>Domain-Adaptive Pre-Training</td>
<td>去掉 800 k 预训练数据</td>
<td>VR &amp; VQA 平均</td>
<td>性能下降 ≈10 %（VR）/ ≈6 %（VQA），验证预训练必要性。</td>
</tr>
<tr>
<td>Reward 设计</td>
<td>对比 Threshold vs Adaptive Gaussian</td>
<td>VR SRCC/PLCC</td>
<td>Adaptive Gaussian 在三域均 &gt;0.77，阈值法仅 0.60–0.88 且波动大。</td>
</tr>
<tr>
<td>任务联合训练</td>
<td>VQA-only / VR-only / 联合</td>
<td>各自域平均</td>
<td>联合训练在 两项任务 上均优于单任务，最大增益 +5.9 %。</td>
</tr>
<tr>
<td>域联合训练</td>
<td>IAA-only / IQA-only / ISTA-only / 三域混合</td>
<td>三域平均</td>
<td>三域混合版 全面领先 单域模型，ISTA 单域虽局部最高，但跨域泛化差。</td>
</tr>
</tbody>
</table>
</div>
<p>3 下游应用验证</p>
<p>3.1 作为奖励模型（FLUX.1-dev 后训练）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>奖励信号</th>
<th>PickScore ↑</th>
<th>HPSv3 ↑</th>
<th>DeQA ↑</th>
<th>LAION-Aes ↑</th>
<th>ArtiMuse ↑</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>22.46</td>
<td>10.71</td>
<td>4.32</td>
<td>5.77</td>
<td>59.02</td>
<td>—</td>
</tr>
<tr>
<td>+ IAA Reward</td>
<td>22.47</td>
<td>10.09</td>
<td>4.09</td>
<td>6.19</td>
<td>67.02</td>
<td>美学指标 ↑8.0</td>
</tr>
<tr>
<td>+ IQA Reward</td>
<td>22.63</td>
<td>11.21</td>
<td>4.37</td>
<td>6.02</td>
<td>63.64</td>
<td>质量指标 ↑0.5–0.7</td>
</tr>
<tr>
<td>+ ISTA Reward</td>
<td>22.72</td>
<td>11.09</td>
<td>4.37</td>
<td>6.16</td>
<td>63.75</td>
<td>结构丰富度 ↑3.4</td>
</tr>
<tr>
<td>All Rewards</td>
<td>22.67</td>
<td>10.93</td>
<td>4.33</td>
<td>6.19</td>
<td>65.52</td>
<td>综合感知 最优</td>
</tr>
</tbody>
</table>
</div>
<p>3.2 作为统一度量（T2I 模型评测）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>测试模型数</th>
<th>主要发现</th>
</tr>
</thead>
<tbody>
<tr>
<td>DPG</td>
<td>8 个主流 T2I（FLUX、GPT-Image-1、Qwen-Image 等）</td>
<td>GPT-Image-1 / Qwen-Image 取得最高 UniPercept-Avg 65.7 / 60.8，但 ISTA 仍 &lt;50，揭示纹理-结构薄弱。</td>
</tr>
<tr>
<td>GenEval</td>
<td>同上</td>
<td>指令跟随能力（GenEval ↑）与感知质量（UniPercept ↑）正相关，但 高 GenEval ≠ 高 IAA/ISTA。</td>
</tr>
</tbody>
</table>
</div>
<p>3.3 数据集感知诊断</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>UniPercept-IAA</th>
<th>UniPercept-IQA</th>
<th>UniPercept-ISTA</th>
<th>用途</th>
</tr>
</thead>
<tbody>
<tr>
<td>Unsplash</td>
<td>62.49</td>
<td>69.19</td>
<td>43.32</td>
<td>真实高清图 综合第一</td>
</tr>
<tr>
<td>Blip3o-60K</td>
<td>63.81</td>
<td>73.88</td>
<td>49.38</td>
<td>AIGC 图 综合第一</td>
</tr>
<tr>
<td>ImageNet</td>
<td>53.88</td>
<td>61.90</td>
<td>36.79</td>
<td>提供 低感知分 对照</td>
</tr>
</tbody>
</table>
</div>
<p>4 可视化与案例</p>
<ul>
<li>图 8、12–14 给出 <strong>FLUX 生成对比</strong> 与 <strong>UniPercept-Image-Profile</strong> 样例，展示模型如何同时输出<br>IAA/IQA/ISTA 分数 + 逐段细粒度文字诊断，验证可解释性与实用价值。</li>
</ul>
<p>综上，实验从 <strong>benchmark 难度、模型消融、奖励驱动生成、第三方基准复测、数据分布诊断</strong> 多维度证明：<br>UniPercept 在感知级图像理解任务上 <strong>全面领先现有 MLLM</strong>，并可作为 <strong>通用奖励/度量/诊断工具</strong> 直接服务生成模型优化与数据筛选。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 UniPercept 框架的自然延伸，亦是目前实验结果已暴露出的短板，具有明确的探索价值：</p>
<p>1 规模与多样性扩展</p>
<ul>
<li><strong>十百万级数据</strong>：当前 6 k 图文对已能拉开模型差距，但较语义级 benchmark 仍小一个量级。<br>→ 自动化+人机协同迭代，构建 <strong>&gt;100 k</strong> 感知级 VQA，覆盖 <strong>视频、HDR、360°、红外、显微</strong> 等成像模态。</li>
<li><strong>跨文化美学</strong>：现有标注员以东亚+北美为主，可引入 <strong>南美、非洲、南亚</strong> 标注群体，研究 <strong>文化审美偏差</strong> 及 <strong>个性化校准</strong> 方法。</li>
<li><strong>动态感知</strong>：将 IAA/IQA/ISTA 扩展至 <strong>视频时序维度</strong>（帧间一致性、节奏感、运动模糊累积效应），形成 <strong>UniPercept-Video-Bench</strong>。</li>
</ul>
<p>2 任务与评价范式升级</p>
<ul>
<li><strong>相对感知</strong>：人类更擅长“<strong>A 比 B 美/清晰</strong>”而非绝对打分。<br>→ 构建 <strong>成对/三元组</strong> 偏好数据集，训练 <strong>对比式奖励模型</strong>，与绝对值模型做 <strong>ensemble</strong> 或 <strong>ranking-distillation</strong>。</li>
<li><strong>可编辑感知</strong>：引入 <strong>局部感知 mask</strong>，支持“<strong>仅对天空提升美学</strong>”“<strong>只去噪人物皮肤</strong>”的 <strong>细粒度奖励信号</strong>，驱动 <strong>区域感知生成</strong>。</li>
<li><strong>不确定性量化</strong>：输出 <strong>感知分数的分布</strong>（Gaussian/Log-Normal）而非点估计，为 <strong>人机协同创作</strong> 提供 <strong>置信度阈值</strong>。</li>
</ul>
<p>3 模型架构与训练策略</p>
<ul>
<li><strong>视觉 tokenizer 感知专用化</strong>：现有 ViT 侧重语义，可设计 <strong>多尺度高频 tokenizer</strong>（小 patch + 高频滤波）显式增强纹理/失真感知。</li>
<li><strong>多模态混合专家（MoE）</strong>：为 IAA/IQA/ISTA 分别维护 <strong>感知专家子网络</strong>，利用 <strong>路由机制</strong> 动态分配计算，降低冲突梯度。</li>
<li><strong>自监督预文本对齐</strong>：利用 <strong>失真-无失真图像对</strong>、<strong>美学增强前后对</strong> 构建 <strong>对比损失</strong>，在 <strong>无标注阶段</strong> 即对齐感知空间，减少后续 RL 采样成本。</li>
</ul>
<p>4 人类-模型一致性深挖</p>
<ul>
<li><strong>眼动 &amp; 脑电同步</strong>：采集 <strong>眼动扫描路径 + EEG 审美评分</strong>，建立 <strong>生物信号-模型注意力</strong> 映射，验证 <strong>模型是否关注人类真正在乎的区域</strong>。</li>
<li><strong>可解释可视化</strong>：将 <strong>ISTA 结构化输出</strong> 与 <strong>Grad-CAM</strong> 结合，生成 <strong>感知热图</strong>，让用户 <strong>交互式查询</strong>“为何纹理得分低”，提升 <strong>诊断透明度</strong>。</li>
</ul>
<p>5 下游应用再推进</p>
<ul>
<li><strong>文生图 </strong>RL-on-policy<strong> 循环</strong>：当前仅用 <strong>离线奖励</strong>，可探索 <strong>在线 RL</strong>（模型生成→UniPercept 实时奖励→策略更新），形成 <strong>自我提升闭环</strong>。</li>
<li><strong>图像修复/超分感知优化</strong>：将 UniPercept 作为 <strong>损失函数</strong> 替代传统 LPIPS，驱动 <strong>感知优先</strong> 的复原算法，验证 <strong>是否比 PSNR/LPIPS 更受人类偏爱</strong>。</li>
<li><strong>多模态 RLHF 平台</strong>：把 UniPercept 奖励与 <strong>文本一致性奖励</strong> 融合，构建 <strong>统一 RLHF 服务</strong>，供 <strong>Stable Diffusion、Midjourney、DALL·E</strong> 等即插即用。</li>
</ul>
<p>6 伦理与鲁棒性</p>
<ul>
<li><strong>感知欺骗攻击</strong>：研究 <strong>对抗扰动</strong> 是否可让 UniPercept 输出 <strong>极高/极低分数</strong> 而人类无感，提出 <strong>感知鲁棒性基准</strong>。</li>
<li><strong>偏见审计</strong>：检查 <strong>肤色、性别、文化符号</strong> 是否导致 <strong>系统性感知偏差</strong>，并引入 <strong>公平性约束</strong> 到奖励函数。</li>
</ul>
<p>7 开源与工具链</p>
<ul>
<li><strong>在线 Demo + API</strong>：提供 <strong>实时感知评分</strong> 与 <strong>JSON 结构化诊断</strong>，方便 <strong>设计师、摄影师、生成平台</strong> 直接调用。</li>
<li><strong>插件生态</strong>：开发 <strong>Photoshop / Blender / Figma 插件</strong>，把 UniPercept 作为 <strong>“感知图层”</strong>，实现 <strong>边编辑边反馈</strong> 的工作流。</li>
</ul>
<p>简言之，UniPercept 已迈出“统一感知级理解”第一步，未来可在 <strong>数据规模、文化多样性、时序扩展、局部编辑、生物信号对齐、在线 RL、伦理鲁棒</strong> 等方向持续深耕，最终形成 <strong>人类级、可解释、可交互</strong> 的通用感知引擎。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>UniPercept 论文核心内容一览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>关键信息</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>多模态大模型（MLLM）在语义级任务表现强劲，却普遍缺乏美学、质量、结构/纹理等感知级理解，导致内容创作、生成模型对齐等应用缺少可靠、可解释的感知信号。</td>
</tr>
<tr>
<td>目标</td>
<td>建立统一、可评测、可训练的感知级图像理解框架，让模型像人类一样对“好不好看、清不清晰、结构是否丰富”进行定量打分+定性解释。</td>
</tr>
<tr>
<td>贡献 1：基准</td>
<td>UniPercept-Bench• 三域层级定义：IAA（美学）+ IQA（质量）+ ISTA（结构&amp;纹理）→ 44 项细粒度准则• 双任务：Visual Rating（0–100）+ Visual Question Answering（多选/Yes-No/Why）• 6 k 高质量图文对，经 GPT-4o 生成→多模型拒采→人工精修，首个同时覆盖三域+双任务的感知级 benchmark</td>
</tr>
<tr>
<td>贡献 2：模型</td>
<td>UniPercept 基线• 两阶段训练：① 800 k 样本领域自适应预训练；② 任务对齐 RL（GRPO+Adaptive Gaussian Soft Reward）联合优化打分与问答• 18 个主流模型对比：三域 VR 平均 SRCC/PLCC 0.824/0.827，VQA 平均准确率 80.6 %，全面领先专有/开源/专用模型</td>
</tr>
<tr>
<td>贡献 3：应用</td>
<td>即插即用感知奖励• 对 FLUX.1-dev 进行 Flow-GRPO 后训练：IAA/IQA/ISTA 三路信号可定向提升美学、清晰度或结构丰富度；联合奖励实现最佳整体感知质量• 统一度量：在 DPG、GenEval 上评测 8 个 T2I 模型，揭示 ISTA&lt;50 等短板；对 ImageNet/Unsplash/LAION 等做分布诊断，量化数据集感知优劣</td>
</tr>
<tr>
<td>结论</td>
<td>UniPercept 首次定义并实现了感知级图像理解的统一框架，提供benchmark+强基线+奖励/度量工具，为生成模型优化、数据筛选、人机协同创作奠定可直接使用的基础。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UniPercept%3A%20Towards%20Unified%20Perceptual-Level%20Image%20Understanding%20across%20Aesthetics%2C%20Quality%2C%20Structure%2C%20and%20Texture，链接是https://arxiv.org/pdf/2512.21675，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21675。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shuo Cao,Jiayang Li,Xiaohui Li,Yuandong Pu,Kaiwen Zhu,Yuanting Gao,Siqi Luo,Yi Xin,Qi Qin,Yu Zhou,Xiangyu Chen,Wenlong Zhang,Bin Fu,Yu Qiao,Yihao Liu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21675.pdf">https://arxiv.org/pdf/2512.21675.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21675">https://arxiv.org/abs/2512.21675</a></p>
<p><strong>Arxiv ID</strong>: 2512.21675</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21675">https://papers.cool/arxiv/2512.21675</a></p>
<p><strong>Published</strong>: 2025-12-25T13:35:52Z</p>
<p><strong>Updated</strong>: 2025-12-25T13:35:52.000Z</p>
<hr>
<h3 id="5-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right"><a href="#5-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right" class="headerlink" title="5. ProEdit: Inversion-based Editing From Prompts Done Right"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.22118">ProEdit: Inversion-based Editing From Prompts Done Right</a></h3><p>Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject’s atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于反演的视觉编辑提供了一种有效且无需训练的方式，根据用户指令编辑图像或视频。现有方法通常在采样过程中注入源图像信息以保持编辑的一致性。然而，这种采样策略过度依赖源信息，从而对目标图像的编辑产生负面影响（例如，无法按照指令更改主体的属性，如姿势、数量或颜色）。在本研究中，我们提出了 ProEdit，以在注意力和潜在空间两方面解决这一问题。在注意力方面，我们引入了 KV-mix，它在编辑区域混合源图像和目标图像的 KV 特征，从而在保持背景一致性的同时减轻源图像对编辑区域的影响。在潜在空间方面，我们提出了 Latents-Shift，通过扰动源潜在空间的编辑区域，消除反演潜在对采样的影响。在多个图像和视频编辑基准上的大量实验证明，我们的方法实现了最先进的性能。此外，我们的设计即插即用，可以无缝集成到现有的反演和编辑方法中，例如 RF-Solver、FireFlow 和 UniEdit。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“基于反演的视觉编辑”中普遍存在的<strong>源图像信息过度注入</strong>问题，提出系统性的解决方案。具体而言：</p>
<ul>
<li>现有 flow-based 反演方法在采样阶段为保持背景一致性，会全局注入源图的 attention 特征，并以反演 latent 作为初始噪声。</li>
<li>这种策略使模型过度依赖源分布，导致目标区域（姿态、数量、颜色等属性）无法按文本指令被正确修改，出现“编辑失败”现象。</li>
</ul>
<p>因此，作者提出 <strong>ProEdit</strong>，从 attention 与 latent 两条路径同时削弱源图影响，实现高精度、即插即用的无训练编辑。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“2. Related Work”中系统梳理了与 ProEdit 密切相关的三大研究脉络，可归纳如下：</p>
<ol>
<li>文本到视觉生成（Text-to-Visual Generation）</li>
</ol>
<ul>
<li>扩散/流匹配基础模型</li>
<li>U-Net 架构：LDM、Stable Diffusion 系列</li>
<li>DiT/Flow 架构：FLUX、HunyuanVideo、MM-DiT</li>
<li>概率流 ODE 求解器：RF-Solver、FireFlow、UniEdit 等</li>
</ul>
<ol>
<li>文本驱动编辑（Text-driven Editing）</li>
</ol>
<ul>
<li>训练式方法</li>
<li>InstructPix2Pix、CycleGAN、StyleGAN 系列、BrushNet、EditThinker 等</li>
<li>无训练反演方法</li>
<li>DDIM Inversion、PnP、PnP-Inversion、Edit-Friendly、MasaCtrl、InfEdit、Null-text、Negative-prompt、EDICT 等</li>
<li>注意力操控方法</li>
<li>Prompt-to-Prompt、Imagic、FateZero、TokenFlow、DitCtrl、HeadRouter 等</li>
<li>流模型反演与求解</li>
<li>RF-Inversion、BELM、Exact-Diffusion-Inversion、Unveil-Inversion 等</li>
</ul>
<ol>
<li>视频编辑扩展</li>
</ol>
<ul>
<li>Flatten、TokenFlow、FateZero 等基于扩散或光流的时序一致性方案</li>
</ul>
<p>上述工作共同构成了 ProEdit 的对比与插件基线，也是其“即插即用”改进的对象。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从 <strong>attention 注入</strong> 与 <strong>latent 初始化</strong> 两条路径同步削弱源图影响，提出两项即插即用模块：</p>
<ul>
<li><strong>KV-mix</strong></li>
</ul>
<ol>
<li>用 attention map 自动分割“编辑区域”掩码  M </li>
<li>在编辑区域对 <strong>K、V 特征</strong> 做线性混合</li>
</ol>
<p>K<em>(tg) = Modot[δ K</em>(tg)+(1-δ)K<em>(s)] + (1-M)odot K</em>(s)</p>
<p>V<em>(tg) = Modot[δ V</em>(tg)+(1-δ)V<em>(s)] + (1-M)odot V</em>(s)</p>
<ol>
<li>非编辑区域完全注入源特征，保证背景一致；混合强度  δ=0.9  兼顾保真与可编辑性</li>
</ol>
<ul>
<li><strong>Latents-Shift</strong></li>
</ul>
<ol>
<li>借鉴 AdaIN，将反演 latent  z_T  的分布向随机噪声  z_r  做“风格迁移”</li>
</ol>
<p>z_T = σ(z_r)(z_T-μ(z_T)) / (σ(z_T)) + μ(z_r)</p>
<ol>
<li>仅在掩码区域按比率  β=0.25  融合，得到</li>
</ol>
<p>z_T = Modot[βz_T+(1-β)z_T] + (1-M)odot z_T</p>
<ol>
<li>削弱源分布先验，同时保留结构信息</li>
</ol>
<p>两项模块均无需重训练，可在任意 flow-based 反演框架（RF-Solver、FireFlow、UniEdit 等）的采样阶段直接插入，实现“高精度属性编辑 + 背景一致性”兼得。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在图像与视频两大任务上分别进行了系统实验，覆盖定量、定性、消融与专项测试，具体包括：</p>
<ol>
<li>文本驱动图像编辑</li>
</ol>
<ul>
<li>基准：PIE-Bench（700 张，10 类编辑）</li>
<li>指标：</li>
<li>背景保真：Structure Distance ↓、PSNR ↑、SSIM ↑</li>
<li>编辑质量：CLIP-Sim（整图 &amp; 编辑区域）↑</li>
<li>采样效率：NFE ↓</li>
<li>对比方法：</li>
<li>扩散系：P2P、PnP、PnP-Inversion、EditFriendly、MasaCtrl、InfEdit</li>
<li>流匹配系：RF-Inversion、RF-Solver、FireFlow、UniEdit</li>
<li>专项测试：颜色编辑子集（验证 Latents-Shift 对分布注入的缓解）</li>
</ul>
<ol>
<li>文本驱动视频编辑</li>
</ol>
<ul>
<li>自建 55 段 480×480/540×960 视频（40–120 帧）</li>
<li>指标（VBench）：Subject Consistency ↑、Motion Smoothness ↑、Aesthetic Quality ↑、Imaging Quality ↑</li>
<li>对比方法：FateZero、Flatten、TokenFlow、RF-Solver</li>
</ul>
<ol>
<li>消融实验</li>
</ol>
<ul>
<li>模块有效性：KV-mix / Latents-Shift 单独与联合作用</li>
<li>注意力特征组合：Q&amp;V、Q&amp;K&amp;V、V、K&amp;V 四种注入方式对比，验证 KV 组合最优</li>
</ul>
<ol>
<li>额外定性</li>
</ol>
<ul>
<li>图 9、图 10：更多图/视频样例，展示复杂场景与人脸特征保持</li>
<li>图 11：引入 Qwen3-8B 做“指令式”编辑，验证易用性</li>
</ul>
<p>实验结果一致表明：ProEdit 在保持背景一致的同时显著提升编辑精度，且可零成本嵌入现有流匹配反演框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向值得进一步探索，分主题列出：</p>
<ul>
<li><strong>掩码细化与泛化</strong></li>
<li>研究无需显式名词或外部遮罩的语义分割策略，实现开放词汇编辑</li>
<li>探索跨帧一致的视频掩码追踪，以提升长时序稳定性</li>
<li><strong>混合强度自适应</strong></li>
<li>根据源-目标 prompt 差异度或图像局部不确定性，动态调整 δ、β，实现“保真-可编辑”自动平衡</li>
<li><strong>多层、多步注入策略</strong></li>
<li>仅在后半程采样步执行 KV-mix/Latents-Shift，或按 block 重要性加权，进一步降低背景扰动风险</li>
<li>结合扩散时间步语义演化规律，设计时间感知注入调度</li>
<li><strong>扩展到其他模态与任务</strong></li>
<li>将 KV-mix 思想迁移至音频-视觉联合编辑、3D NeRF/GS 编辑或风格化生成</li>
<li>与 ControlNet、LoRA 等条件模块正交结合，实现结构/风格解耦控制</li>
<li><strong>理论分析与优化</strong></li>
<li>从流匹配 ODE 角度量化“源分布先验”对轨迹偏移的影响，推导最优扰动强度 β 的上界</li>
<li>引入可微掩码预测网络，以编辑成功率或 CLIP 分数为奖励进行端到端强化学习微调</li>
<li><strong>计算效率与部署</strong></li>
<li>开发无需保存全部源 KV 特征的在线缓存策略，降低显存占用</li>
<li>研究 INT8/4 量化或 TensorRT 级联，推动移动端实时编辑</li>
<li><strong>人机交互与评估</strong></li>
<li>构建面向“属性-结构-时序”细粒度标注的更大规模基准，补充现有 PIE-Bench/VBench</li>
<li>引入人类主观打分与眼动实验，验证“不可察觉编辑”阈值，指导保真-可编辑权衡</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>ProEdit 核心内容速览</strong></p>
<ol>
<li><p><strong>问题</strong><br>流匹配反演编辑在采样阶段全局注入源图 attention 与 latent，导致源分布先验过强，目标属性（颜色/姿态/数量）无法按文本指令修改。</p>
</li>
<li><p><strong>方法</strong></p>
</li>
</ol>
<ul>
<li><strong>KV-mix</strong>：用 attention map 生成编辑掩码  M ，在编辑区对 K、V 特征做源-目标线性混合，非编辑区完整保留源特征。</li>
<li><strong>Latents-Shift</strong>：借鉴 AdaIN，将反演 latent 的分布向随机噪声偏移，仅作用于掩码区域，削弱源分布先验。<br>两模块均为<strong>即插即用</strong>，无需重训练，可嵌入任意 flow-based 反演框架。</li>
</ul>
<ol>
<li><strong>实验</strong></li>
</ol>
<ul>
<li>图像：PIE-Bench 上对比 10 余种 SOTA，ProEdit 在 CLIP-Sim、PSNR、SSIM 等指标全面领先；颜色编辑子集验证 Latents-Shift 有效性。</li>
<li>视频：自建 55 段数据，VBench 四项指标均优于 FateZero、TokenFlow、RF-Solver。</li>
<li>消融：KV-mix 与 Latents-Shift 协同增益显著；KV 组合为最优注意力注入方式。</li>
</ul>
<ol>
<li><strong>结论</strong><br>ProEdit 首次从 attention 与 latent 双路径同时抑制源图信息过度注入，实现高精度、高保真的无训练文本驱动图像/视频编辑，并可零成本集成至现有流匹配 pipeline。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ProEdit%3A%20Inversion-based%20Editing%20From%20Prompts%20Done%20Right，链接是https://arxiv.org/pdf/2512.22118，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.22118。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhi Ouyang,Dian Zheng,Xiao-Ming Wu,Jian-Jian Jiang,Kun-Yu Lin,Jingke Meng,Wei-Shi Zheng</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.22118.pdf">https://arxiv.org/pdf/2512.22118.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.22118">https://arxiv.org/abs/2512.22118</a></p>
<p><strong>Arxiv ID</strong>: 2512.22118</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.22118">https://papers.cool/arxiv/2512.22118</a></p>
<p><strong>Published</strong>: 2025-12-26T18:59:14Z</p>
<p><strong>Updated</strong>: 2025-12-26T18:59:14.000Z</p>
<hr>
<h3 id="6-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models"><a href="#6-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models" class="headerlink" title="6. TimeBill: Time-Budgeted Inference for Large Language Models"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21859">TimeBill: Time-Budgeted Inference for Large Language Models</a></h3><p>Large Language Models (LLMs) are increasingly deployed in time-critical systems, such as robotics, autonomous driving, embodied intelligence, and industrial automation, where generating accurate responses within a given time budget is crucial for decision-making, control, or safety-critical tasks. However, the auto-regressive generation process of LLMs makes it challenging to model and estimate the end-to-end execution time. Furthermore, existing efficient inference methods based on a fixed key-value (KV) cache eviction ratio struggle to adapt to varying tasks with diverse time budgets, where an improper eviction ratio may lead to incomplete inference or a drop in response performance. In this paper, we propose TimeBill, a novel time-budgeted inference framework for LLMs that balances the inference efficiency and response performance. To be more specific, we propose a fine-grained response length predictor (RLP) and an execution time estimator (ETE) to accurately predict the end-to-end execution time of LLMs. Following this, we develop a time-budgeted efficient inference approach that adaptively adjusts the KV cache eviction ratio based on execution time prediction and the given time budget. Finally, through extensive experiments, we demonstrate the advantages of TimeBill in improving task completion rate and maintaining response performance under various overrun strategies.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLM）正越来越多地应用于对时间敏感的系统，例如机器人技术、自动驾驶、具身智能和工业自动化，在这些系统中，在给定时间预算内生成准确响应对于决策、控制或安全关键任务至关重要。然而，LLM 的自回归生成过程使得建模和估计端到端执行时间变得具有挑战性。此外，现有基于固定键值（KV）缓存驱逐比例的高效推理方法难以适应具有多样化时间预算的不同任务，使用不当的驱逐比例可能导致推理不完整或响应性能下降。在本文中，我们提出了 TimeBill，一种针对 LLM 的新型时间预算推理框架，在推理效率和响应性能之间实现平衡。具体而言，我们提出了精细化的响应长度预测器（RLP）和执行时间估计器（ETE），以准确预测 LLM 的端到端执行时间。在此基础上，我们开发了一种时间预算高效推理方法，根据执行时间预测和给定时间预算自适应调整 KV 缓存驱逐比例。最后，通过大量实验，我们展示了 TimeBill 在提高任务完成率和在各种超时策略下保持响应性能方面的优势。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>大语言模型（LLM）在硬实时系统中难以满足给定时间预算</strong>的问题。由于 LLM 的自回归生成特性，其端到端执行时间高度依赖于输出长度，而输出长度在推理完成前无法精确获知，导致：</p>
<ol>
<li><strong>执行时间不可预测</strong>：传统 CNN 的确定性执行时间分析不再适用。</li>
<li><strong>固定 KV 缓存淘汰策略失效</strong>：离线压缩方法无法在运行时根据动态时间预算调整；现有在线方法采用固定淘汰比例，要么造成超时，要么过度牺牲生成质量。</li>
</ol>
<p>为此，作者提出 <strong>TimeBill</strong> 框架，核心目标是在<strong>严格时间约束</strong>下最大化响应性能，即：</p>
<p>max<em>(θ) M(hat y(θ), y) s.t. t</em>(e2e)(x,θ) le T, N le N_(max)</p>
<p>通过以下手段实现：</p>
<ul>
<li>细粒度**响应长度预测器（RLP）**提前估计输出长度；</li>
<li>基于 FLOPs 分析与剖析的**执行时间估计器（ETE）**准确预测端到端耗时；</li>
<li><strong>运行时自适应 KV 缓存淘汰比例</strong>  α^<em> ，在保证  hat t</em>(WCET) le T  的前提下最小化淘汰比例，从而兼顾<strong>任务完成率</strong>与*<em>响应质量_</em>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线：执行时间估计与高效推理。按时间顺序与代表性工作梳理如下。</p>
<p>1. 执行时间估计</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表文献</th>
<th>核心思路</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>确定性 DNN 实时分析</td>
<td>Kang et al. RTSS’21；Chen et al. RTSS’24</td>
<td>对层数固定的 CNN/FCN 做 WCET 分析，量化或层迁移降耗时</td>
<td>无法处理自回归带来的动态输出长度</td>
</tr>
<tr>
<td>输出长度预测（粗粒度）</td>
<td>ProxyModel Qiu et al. arXiv’24；S3 Jin et al. NeurIPS’23</td>
<td>用 BERT/DistilBERT 做 5/10 类分类，预测长度区间</td>
<td>桶粒度过粗，且对长输入截断；本文用 SLM 细粒度分类</td>
</tr>
<tr>
<td>输出长度预测（自回归）</td>
<td>PiA Zheng et al. NeurIPS’23</td>
<td>微调目标 LLM，让其先输出“本回答共 N 字”</td>
<td>需修改生成流程，且仍回归精确值，误差大</td>
</tr>
<tr>
<td>机器学习型 LLM 延迟预测</td>
<td>RLM-ML Imai et al. NeurIPS’24；LLMStation He et al. USENIX ATC’25；BestServe Hu et al. arXiv’25</td>
<td>采集 roofline+硬件计数器，用 ML 回归推理延迟</td>
<td>黑盒模型，可解释性与在线预测友好性差；本文采用白盒 FLOPs+剖析</td>
</tr>
</tbody>
</table>
</div>
<p>2. 高效 LLM 推理</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表文献</th>
<th>核心思路</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>离线压缩</td>
<td>SmoothQuant Xiao et al. ICML’23；AWQ Lin et al. MLSys’24；GPTQ Frantar et al. ICLR’23；SparseGPT Frantar &amp; Alistarh ICML’23；LLM-Pruner Ma et al. NeurIPS’23</td>
<td>权重量化、结构化剪枝，一次性降计算与内存</td>
<td>部署后恒定，无法在运行时按时间预算调整</td>
</tr>
<tr>
<td>在线 KV 缓存裁剪</td>
<td>StreamingLLM Xiao et al. ICLR’24；SnapKV Li et al. NeurIPS’24a；DuoAttention Xiao et al. ICLR’25</td>
<td>固定比例或启发式淘汰旧 KV，保持窗口长度</td>
<td>比例固定，与任务/预算无关，易超时或过度掉点</td>
</tr>
<tr>
<td>在线 KV 缓存量化</td>
<td>KVQuant Hooper et al. NeurIPS’24；KIVI Liu et al. ICML’24</td>
<td>运行时把 KV 压缩到 4 bit 或 2 bit</td>
<td>仅降内存，不直接控制执行时间；未考虑时间预算</td>
</tr>
</tbody>
</table>
</div>
<p>综上，现有文献要么<strong>忽略动态时间预算</strong>，要么<strong>无法细粒度预测输出长度</strong>，导致在硬实时场景下出现<strong>超时或性能骤降</strong>。TimeBill 首次将“细粒度长度预测 + 白盒时间估计 + 运行时自适应 KV 淘汰”三者耦合，填补了这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“硬实时场景下 LLM 必须在给定时间预算 T 内完成推理且保证响应质量”这一核心难题形式化为</p>
<p>max<em>(θ) M(hat y(θ), y) quad s.t. t</em>(e2e)(x,θ)le T, Nle N_(max)</p>
<p>并指出两大挑战：</p>
<ol>
<li>运行前无法知晓输出长度 N，导致 te2e 不可预测；</li>
<li>运行前无法知晓响应质量 M(·)，因而无法直接优化。</li>
</ol>
<p>为此提出 <strong>TimeBill</strong> 框架，分三步闭环解决。</p>
<p>1. 细粒度响应长度预测（RLP）</p>
<ul>
<li><strong>问题1</strong>→“运行前不知 N”。</li>
<li><strong>解法</strong>：<br>– 用轻量 SLM（Qwen2.5-0.5B）把长度预测建模为<strong>分类任务</strong>：将 0–Nmodel 均匀划分为 B 令牌宽的桶，共 ⌈Nmodel/B⌉ 类；<br>– 用<strong>知识蒸馏</strong>对齐目标 LLM：采集同一 prompt 在目标 LLM 上的真实长度 Nj，构造训练对 (xj, ⌈Nj/B⌉)；<br>– 推理时输出桶索引 ˆn，再截断到运行时上限：</li>
</ul>
<p>hat N=min!l(N_(max),,hat n Br).</p>
<p>结果：512 桶时 MAE 42.7 token，显著优于 BERT 基线的 105+ token。</p>
<p>2. 工作量驱动的执行时间估计（ETE）</p>
<ul>
<li><strong>问题1 延伸</strong>→“已知 ˆN 后如何得 te2e”。</li>
<li><strong>解法</strong>：<br>– <strong>白盒 FLOPs 建模</strong>：</li>
<li>prefill 阶段计算量  propto N_x^2 ；</li>
<li>每步 decoding 计算量  propto N_(kv)^i=(1-α)N_x+i-1 。<br>– <strong>剖析拟合</strong>：在目标硬件上实测 (Nx, tprefill) 与 (Nkv, tstep) 多元点，用最小二乘求系数，得到</li>
</ul>
<p>hat t<em>(prefill)(x)&amp;=aN_x^2+bN_x+c,[2pt] hat t</em>(step)^i(x,α)&amp;=pl((1-α)N_x+i-1r)+q.</p>
<p>– <strong>端到端预测</strong>（含悲观因子 k 保证 WCET）：</p>
<p>hat t<em>(WCET)(x,α)=hat t</em>(prefill)+∑<em>(i=1)^(hat N_W-1)hat t</em>(step)^i,quad hat N<em>W=min(khat N,N</em>(max)).</p>
<p>结果：MAPE 1.22 %（prefill）/ 1.69 %（单步），且  hat t_(WCET)  严格包络真实 te2e。</p>
<p>3. 时间预算驱动的 KV 缓存淘汰</p>
<ul>
<li><strong>问题2</strong>→“M(·) 未知，无法直接最大化”。</li>
<li><strong>关键观察</strong>：响应质量随淘汰比例 α 单调不增。</li>
<li><strong>等价转化</strong>：把原问题变为“在满足时间约束下取最小 α”：</li>
</ul>
<p>min α quad s.t. t<em>(Predict)+hat t</em>(WCET)(x,α)le T, 0leαleα_(max).</p>
<p>代入式 (6)(7)(9) 后封闭解：</p>
<p>α^*=min!gl(α<em>(max),,1-T{-t</em>(Predict)-hat t_(prefill)-q(hat N_W-1)}{pN_x(hat N_W-1)}-(hat N_W-2) / (2N_x)gr).</p>
<ul>
<li><strong>系统部署</strong>：<br>– prefill 阶段与 RLP+ETE 预测并行跑在 CPU/GPU；<br>– prefill 结束后立即算出 α*，按此比例淘汰 KV 缓存，再进入 decoding；<br>– 若 tPredict≤tprefill，则 tPredict 视为 0，进一步降低保守性；<br>– 支持每轮推理预算 T 动态变化，无需重新训练。</li>
</ul>
<p>效果</p>
<ul>
<li>在 5–10 s 六级预算下，TimeBill 相比固定淘汰 α=95 % 方案，<strong>平均响应性能提升 6–12 %</strong>，同时<strong>任务完成率保持 98 % 以上</strong>；</li>
<li>对比 Vanilla、AWQ、固定 α 等基线，取得<strong>最高平均分数与可比完成率</strong>，验证了“预测-估计-自适应淘汰”闭环的有效性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕三条主线展开：预测准确性、时间估计精度、端到端实时性能。所有实验均在 NVIDIA A40 + Intel Xeon 8350C 服务器完成，目标模型为 Qwen2.5-7B-Instruct，数据集采用 LongBench。具体配置与测试项如下。</p>
<p>1. 实验设置</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>组件</th>
<th>配置</th>
</tr>
</thead>
<tbody>
<tr>
<td>目标 LLM</td>
<td>Qwen2.5-7B-Instruct，上下文 32 k，最大生成长度 8 k</td>
</tr>
<tr>
<td>预测器 SLM</td>
<td>Qwen2.5-0.5B-Instruct</td>
</tr>
<tr>
<td>训练数据</td>
<td>Arena-Human-Preference-100k（与测试集无重叠）</td>
</tr>
<tr>
<td>KV 裁剪后端</td>
<td>SnapKV</td>
</tr>
<tr>
<td>桶粒度 B</td>
<td>16/32/64/512（默认 512）</td>
</tr>
<tr>
<td>悲观因子 k</td>
<td>1–8（默认 5）</td>
</tr>
<tr>
<td>最大淘汰比例 αmax</td>
<td>95 %</td>
</tr>
<tr>
<td>预算 T</td>
<td>5–10 s，步长 1 s</td>
</tr>
<tr>
<td>超限策略</td>
<td>Kill、Skip-Next</td>
</tr>
</tbody>
</table>
</div>
<p>2. 响应长度预测器（RLP）评估</p>
<ul>
<li><strong>对比基线</strong>：ProxyModel（5 类）、S3（10 类）、直接回归。</li>
<li><strong>指标</strong>：MAE、RMSE、R²。</li>
<li><strong>结果</strong>（512 桶）：</li>
<li>MAE = 42.71，RMSE = 78.13，R² = 0.723，均显著优于 BERT 基线（MAE &gt; 105）。</li>
<li>桶数从 128 增至 512，误差单调下降，验证细粒度必要性；回归方式误差最大，说明精确数值预测不可行。</li>
</ul>
<p>3. 执行时间估计器（ETE）评估</p>
<ol>
<li><strong>单阶段误差</strong></li>
</ol>
<ul>
<li>采集 0–32 k 长度实测延迟，用最小二乘拟合式 (4a)(4b)。</li>
<li>MAPE：prefill 1.22 %，单步 decoding 1.69 %。</li>
</ul>
<ol>
<li><strong>端到端误差</strong></li>
</ol>
<ul>
<li>固定 α=0、Nmax=64，随机抽 200 条样本。</li>
<li>ˆt_e2e 与真实 te2e 几乎重合；ˆt_WCET 给出严格上界，满足硬实时需求。</li>
</ul>
<p>4. 与基线对比（实时性能）</p>
<ul>
<li><strong>基线</strong>：Vanilla、固定淘汰 α=25 %/50 %/75 %/95 %、AWQ-4 bit。</li>
<li><strong>指标</strong>：平均响应性能（F1/ROUGE-L/Levenshtein 综合分数）与任务完成率。</li>
</ul>
<p><strong>结果</strong>（Kill 策略，T=5 s 为例）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>完成率</th>
<th>平均分数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vanilla</td>
<td>18 %</td>
<td>0.12</td>
</tr>
<tr>
<td>α=25 %</td>
<td>42 %</td>
<td>0.31</td>
</tr>
<tr>
<td>α=75 %</td>
<td>88 %</td>
<td>0.47</td>
</tr>
<tr>
<td>α=95 %</td>
<td>98 %</td>
<td>0.44</td>
</tr>
<tr>
<td>AWQ</td>
<td>25 %</td>
<td>0.19</td>
</tr>
<tr>
<td>TimeBill</td>
<td>98 %</td>
<td>0.53</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>在所有六级预算下，TimeBill 平均分数保持最高，完成率与 α=95 % 持平；Skip-Next 策略结论一致。</li>
<li>AWQ 与 TimeBill 正交，联合使用后分数再升 3 %，验证可叠加性。</li>
</ul>
<p>5. 悲观因子 k 敏感度</p>
<ul>
<li>T=5 s，k∈<br>1,8<br>。</li>
<li>k 由 1 增至 5：完成率与分数同步提升（α 更激进）。</li>
<li>k&gt;5：α 过大，质量下降，分数回落；k=5 为最佳折中，与硬实时领域常用保守系数一致。</li>
</ul>
<p>6. 消融与扩展</p>
<ul>
<li><strong>桶数消融</strong>：512 桶达到误差-计算开销 sweet spot。</li>
<li><strong>Prompt 压缩</strong>：当输入过长导致 tPredict&gt;tprefill 时，用摘要压缩保证并行度，额外损失 &lt;1 % 分数。</li>
<li><strong>硬件一致性</strong>：在 A100 复现，MAPE 变化 &lt;0.3 %，说明模型-硬件解耦良好。</li>
</ul>
<p>综上，实验系统验证了：</p>
<ol>
<li>RLP 输出长度预测显著优于现有粗粒度方法；</li>
<li>ETE 给出足够精确且保守的延迟估计；</li>
<li>TimeBill 在多种预算与超限策略下均取得最高平均响应质量，同时维持接近 100 % 的任务完成率。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“模型-算法”“系统-硬件”“应用-生态”三个层面。</p>
<p>1. 模型-算法层面</p>
<ul>
<li><p><strong>多模态扩展</strong><br>当前 RLP 与 ETE 仅针对文本 token；视觉/音频语境下输出长度还与图像分辨率、帧率相关，可引入跨模态特征统一建模。</p>
</li>
<li><p><strong>不确定性量化</strong><br>长度预测与延迟估计均给出点估计，可改用<strong>贝叶斯深度网络</strong>或<strong>Conformal Prediction</strong>，输出置信区间，实现<strong>概率化时间保证</strong>（pWCET）。</p>
</li>
<li><p><strong>在线自适应桶粒度</strong><br>预算紧张时自动调粗桶宽降低 tPredict；预算宽松时细化桶宽提升精度，形成<strong>元控制回路</strong>。</p>
</li>
<li><p><strong>多目标优化</strong><br>除长度与延迟外，同时考虑<strong>能耗</strong>、<strong>内存峰值</strong>，建立帕累托前沿，实现<strong>绿色实时推理</strong>。</p>
</li>
<li><p><strong>与投机解码协同</strong><br>投机解码可缩短延迟但改变输出长度分布；需联合优化<strong>草稿长度</strong>、<strong>验证阈值</strong>与<strong>KV 淘汰比例</strong>。</p>
</li>
</ul>
<p>2. 系统-硬件层面</p>
<ul>
<li><p><strong>异构并行预测</strong><br>将 RLP/ETE 部署到 NPU/DSP 等侧载芯片，实现<strong>零开销</strong>预测；研究预测与预fill 的<strong>细粒度流水线</strong>对齐。</p>
</li>
<li><p><strong>KV 缓存压缩组合</strong><br>同时调节<strong>淘汰比例α</strong>、<strong>量化位宽β</strong>、<strong>存储格式（稀疏/块稀疏）</strong>，构建多维配置空间，用<strong>强化学习</strong>在线搜索最优(α,β)。</p>
</li>
<li><p><strong>硬件级计时器+提前中断</strong><br>在 GPU 指令层插入<strong>周期计时器</strong>，若动态累计时钟接近预算即触发<strong>提前停止</strong>并输出当前最佳 token，实现<strong>硬中断级超时</strong>。</p>
</li>
<li><p><strong>多租户隔离</strong><br>在 LLM 服务集群中，为每个实时流保留<strong>时间份额</strong>，结合 TimeBill 的 α* 实现<strong>租户级预算隔离</strong>与<strong>共享缓存复用</strong>。</p>
</li>
</ul>
<p>3. 应用-生态层面</p>
<ul>
<li><p><strong>任务感知长度引导</strong><br>引入下游任务反馈（如机械臂控制误差、车辆横向偏移），用<strong>强化学习</strong>微调 RLP，使长度预测<strong>偏向高回报区间</strong>。</p>
</li>
<li><p><strong>动态预算生成</strong><br>上层控制器（自动驾驶规划器、机器人 SLAM）根据环境 hazard level 实时调整 T，形成<strong>端到端自适应闭环</strong>。</p>
</li>
<li><p><strong>安全关键认证</strong><br>将 TimeBill 纳入 ISO 26262、DO-178C 等认证流程，研究<strong>可追踪的证据链</strong>（数据→模型→配置→硬件），推动<strong>LLM 在功能安全场景落地</strong>。</p>
</li>
<li><p><strong>开源基准与竞赛</strong><br>建立带<strong>时间标签</strong>的多模态长上下文数据集，举办「Real-Time LLM Challenge」，推动社区共同优化<strong>延迟-质量帕累托前沿</strong>。</p>
</li>
</ul>
<p>4. 理论层面</p>
<ul>
<li><p><strong>渐进最优性证明</strong><br>在输出长度服从重尾分布的假设下，证明给定预算 T 与内存 M，TimeBill 的 α* 调度在<strong>渐进意义</strong>下达到<strong>竞争比</strong>下限。</p>
</li>
<li><p><strong>与信息论关联</strong><br>把 KV 淘汰视为<strong>遗忘率</strong>，建立<strong>记忆-遗忘权衡</strong>的信息论模型，给出最小<strong>互信息损失</strong>与延迟的闭式关系。</p>
</li>
</ul>
<p>综上，从<strong>概率化保证</strong>到<strong>硬件中断</strong>，再到<strong>安全认证</strong>，均可作为 TimeBill 的后续探索点，形成覆盖算法-系统-应用的完整实时 LLM 生态。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>TimeBill</strong>——首个面向硬实时场景的大语言模型（LLM）<strong>时间预算推理框架</strong>，解决“必须在给定时限 T 内完成生成且保持响应质量”这一核心难题。主要贡献与流程概括为以下四点。</p>
<ol>
<li>问题形式化<br>将 LLM 推理视为带约束优化：</li>
</ol>
<p>max<em>(θ) M(hat y(θ), y) quad s.t. t</em>(e2e)(x,θ)le T, Nle N_(max)</p>
<p>其中 θ 为运行时配置（如 KV 缓存淘汰比例 α），目标在时限内最大化响应质量。</p>
<ol>
<li>细粒度响应长度预测（RLP）</li>
</ol>
<ul>
<li>用轻量 SLM 把长度预测建模为<strong>分类任务</strong>（桶宽 B=512）；</li>
<li>知识蒸馏对齐目标 LLM，实现<strong>长输入友好</strong>且<strong>误差低</strong>（MAE 42.7 token，优于 BERT 基线 50 % 以上）。</li>
</ul>
<ol>
<li>工作量驱动的执行时间估计（ETE）</li>
</ol>
<ul>
<li>白盒 FLOPs 分析：prefill 复杂度 ∝ N<em>x^2 ，decoding 每步 ∝ N</em>(kv) ；</li>
<li>剖析拟合得封闭公式，误差 &lt;1.7 %；</li>
<li>引入悲观因子 k 给出<strong>严格上界</strong>  hat t_(WCET) ，满足硬实时需求。</li>
</ul>
<ol>
<li>时间预算高效推理</li>
</ol>
<ul>
<li>将原问题转化为“<strong>最小淘汰比例 α</strong>”：</li>
</ul>
<p>α^*=min!l(α<em>(max),,1-T{-t</em>(Predict)-hat t_(prefill)-q(hat N_W-1)}{pN_x(hat N_W-1)}-(hat N_W-2) / (2N_x)r)</p>
<ul>
<li>运行时与 prefill 并行计算 α*，<strong>零额外关键路径延迟</strong>；</li>
<li>在 5–10 s 六级预算下，<strong>任务完成率 ≥98 %</strong>，<strong>平均响应性能比最佳固定淘汰策略高 6–12 %</strong>，且可与量化正交叠加。</li>
</ul>
<p>综上，TimeBill 通过“<strong>预测-估计-自适应淘汰</strong>”闭环，首次在硬实时约束下实现 LLM <strong>高完成率与高质量</strong>的双重保证。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是TimeBill%3A%20Time-Budgeted%20Inference%20for%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2512.21859，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21859。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Qi Fan,An Zou,Yehan Ma</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21859.pdf">https://arxiv.org/pdf/2512.21859.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21859">https://arxiv.org/abs/2512.21859</a></p>
<p><strong>Arxiv ID</strong>: 2512.21859</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21859">https://papers.cool/arxiv/2512.21859</a></p>
<p><strong>Published</strong>: 2025-12-26T04:49:35Z</p>
<p><strong>Updated</strong>: 2025-12-26T04:49:35.000Z</p>
<hr>
<h3 id="7-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning"><a href="#7-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning" class="headerlink" title="7. See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.22120">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</a></h3><p>Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型视觉语言模型（VLMs）通常可以从中间的视觉线索中受益，这些线索可以通过外部工具注入，或在推理过程中生成潜在的视觉标记，但这些机制仍然忽略了细粒度的视觉证据（例如图表中的折线）、在不同领域之间的泛化能力较差，并且推理时间成本高。在本文中，我们提出双向感知塑形（BiPS），它将基于问题的掩码视图转换为双向“观察位置”信号，从而在训练过程中塑造感知。BiPS 首先在原始图像与保持证据的视图（仅保留与问题相关的区域）之间施加 KL 一致性约束，从而鼓励对支撑像素的粗略但完整覆盖。随后，在原始图像与证据消除视图（关键像素被遮蔽，使图像不再支持原始答案）之间施加 KL 分离约束，防止仅依赖文本的捷径（即仅从文本回答问题），并强制模型依赖细粒度视觉信息。在八项基准测试中，BiPS 平均将 Qwen2.5-VL-7B 提升了 8.2%，并在未见过的数据集和图像类型上表现出强大的跨域泛化能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对大型视觉-语言模型（VLM）在视觉问答（VQA）中存在的<strong>感知瓶颈</strong>——即难以定位、识别并准确读取细粒度视觉证据（如折线图中的细线、医学图像中的病灶轮廓）——提出训练阶段的双向感知塑形框架 BiPS。其核心目标有三点：</p>
<ol>
<li>消除推理时对额外视觉工具或中间视觉 token 的依赖，降低测试阶段计算开销与错误级联风险；</li>
<li>克服传统矩形裁剪/粗粒度掩码带来的“形状刚性”，使模型能捕捉任意形状的碎片化证据；</li>
<li>抑制“文本捷径”现象，防止模型仅凭问题文本猜测答案，强制其真正依赖视觉内容做决策。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线，均围绕“如何在多模态推理中引入视觉线索”展开：</p>
<ul>
<li><strong>推理时注入视觉线索</strong></li>
<li>工具式：Refocus、Argus、Visual Sketchpad 等借助外部模块生成边框、遮罩或裁剪图，让模型聚焦关键区域。</li>
<li>latent-token 式：Mint-CoT、Visual-CoT 等训练模型在推理链中自回归地输出坐标或掩码 token。<br>共同局限：矩形/粗掩码难刻画细粒度结构；流程与特定领域强耦合；测试阶段需额外步骤，增加延迟与误差。</li>
<li><strong>训练时利用视觉扰动或偏好对齐</strong></li>
<li>负扰动：ChiP、PAPO 通过随机噪声或随机掩码构造“错误图”，以抑制视觉幻觉。</li>
<li>偏好优化：mdPO、OPPO 将“含证据 vs 不含证据”图像视为偏好对，用 DPO 方式对齐模型。<br>不足：扰动策略盲目，未显式区分“证据保留”与“证据删除”；缺乏细粒度、语义精确的监督信号。</li>
</ul>
<p>BiPS 与上述方法的区别在于：</p>
<ol>
<li>将“证据保留图”与“证据删除图”的 KL 散度作为双向塑形信号，完全在训练阶段完成，不增加推理成本；</li>
<li>通过可执行图表代码的语义级编辑，生成形状自由、语义精确的配对视图，提供此前工作未涉及的细粒度监督。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“推理时依赖外部视觉工具”转化为“训练时内部感知塑形”，提出 Bi-directional Perceptual Shaping（BiPS）框架，具体分三步：</p>
<ol>
<li>构造配对视图<br>利用可执行图表源码，在代码层面而非像素层面进行语义级编辑，自动生成</li>
</ol>
<ul>
<li>Evidence-Preserving View（仅保留回答问题所需的视觉元素）</li>
<li>Evidence-Ablated View（精细删除关键证据，保留上下文）<br>形成 13 k 高质量三元组 (I, I_pres, I_abl)。</li>
</ul>
<ol>
<li>双向 KL 约束<br>在 GRPO 强化学习框架内设计两阶段课程：</li>
</ol>
<ul>
<li><strong>Consistency 阶段</strong> 最小化</li>
</ul>
<p>L<em>(cons)=E[min!(c</em>(cons), D<em>(KL)(π</em>θ(·|I,q),|,sg[tildeπ<em>θ(·|I</em>(pres),q)])]</p>
<p>把原始图的预测拉向“证据保留图”，迫使模型聚焦有效区域。</p>
<ul>
<li><strong>Separation 阶段</strong> 最大化</li>
</ul>
<p>L<em>(sep)=E[min!(c</em>(sep), D<em>(KL)(π</em>θ(·|I,q),|,sg[tildeπ<em>θ(·|I</em>(abl),q)])]</p>
<p>把原始图的预测推离“证据删除图”，阻断文本捷径，强化细粒度视觉依赖。</p>
<ol>
<li>粗到细课程<br>先单独优化 Consistency，再引入 Separation，避免梯度冲突；全程无需额外标注或测试时视觉提示。</li>
</ol>
<p>仅用 13 k 图表样本微调 Qwen2.5-VL-7B，平均提升 7.3 %；再加 39 k 数学样本后提升达 8.2 %，在八项基准（含图表与通用 VQA）上取得 SOTA 或次 SOTA 结果，验证了解决感知瓶颈的有效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验从<strong>数据规模、任务覆盖、消融分析、系数敏感性、案例可视化</strong>五个维度系统验证 BiPS 的有效性，主要结果如下：</p>
<ol>
<li><strong>主实验：8 基准全面评测</strong></li>
</ol>
<ul>
<li><strong>图表理解</strong>：CharXiv、ChartQAPro、ChartMuseum、EvoChart、ECD-Bench</li>
<li><strong>通用视觉推理</strong>：MathVista、MathVision、MathVerse-VO、MMStar<br>结果：</li>
<li>仅 13 k 图表样本的 BiPS-Chart-7B 平均提升 +7.3 %（44.3→51.6）</li>
<li>追加 39 k 数学样本的 BiPS-General-7B 再提升至 52.5（+8.2 %），在全部 8 项数据集上均优于基线，并超越多款专用图表/数学模型（TinyChart-3B、Chart-R1-7B、BigCharts-R1-7B 等）。</li>
</ul>
<ol>
<li><strong>消融实验</strong></li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>CharXiv</th>
<th>ECD</th>
<th>ChartMuseum</th>
</tr>
</thead>
<tbody>
<tr>
<td>GRPO 基线</td>
<td>44.3</td>
<td>35.6</td>
<td>30.8</td>
</tr>
<tr>
<td>+L_cons</td>
<td>47.2</td>
<td>36.3</td>
<td>31.3</td>
</tr>
<tr>
<td>+L_sep</td>
<td>47.7</td>
<td>38.3</td>
<td>31.8</td>
</tr>
<tr>
<td>BiPS 完整</td>
<td>49.4</td>
<td>39.9</td>
<td>33.5</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><strong>训练课程对比</strong></li>
</ol>
<ul>
<li>Joint 同时优化两目标 → 46.4</li>
<li>Reverse 先 L_sep 再 L_cons → 46.8</li>
<li>Coarse-to-Fine 先 L_cons 再 L_sep → <strong>49.4</strong><br>证明分阶段抑制梯度冲突的必要性。</li>
</ul>
<ol>
<li><p><strong>视图生成策略对比</strong><br>随机 60 % patch 掩码仅得 44.8，程序化代码编辑得 <strong>49.4</strong>，验证语义精确配对的重要性。</p>
</li>
<li><p><strong>系数敏感性</strong><br>α∈<br>0,0.08<br>、β∈<br>0,0.08<br>扫描显示：</p>
</li>
</ol>
<ul>
<li>α=0.01、β=0.02 附近达到峰值</li>
<li>过大系数（0.08）因辅助损失主导而掉点</li>
</ul>
<ol>
<li><strong>案例可视化</strong></li>
</ol>
<ul>
<li>图表任务：BiPS 能沿曲线追踪数值，避免基线“幻觉”峰值</li>
<li>跨域计数：BiPS 显式枚举并减去物体，基线漏数导致错误<br>直观展示双向塑形带来的“视觉 grounded”推理行为。</li>
</ul>
<ol>
<li><strong>与标准 GRPO 对照</strong><br>在相同数据混合上，标准 GRPO 仅提升至 45.4，而 BiPS-General 达 50.6（CharXiv 差 +5.2），表明增益源自感知塑形而非单纯 RL 优化。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为<strong>数据、任务、算法、理论</strong>四个层面：</p>
<ul>
<li><strong>数据层面</strong></li>
</ul>
<ol>
<li>跨模态源码扩展：将“可执行代码→配对视图”思路推广到几何图、流程图、化学结构、医学影像（DICOM 参数化脚本）等，验证通用性。</li>
<li>自动难度校准：目前用 base-model 8 次全对作为“易题”过滤，可尝试可微分难度预测器，实现动态课程。</li>
<li>人类红队对抗：引入人工设计的“证据-删除但可文本猜”陷阱样例，进一步暴露并惩罚潜在捷径。</li>
</ol>
<ul>
<li><strong>任务层面</strong></li>
</ul>
<ol>
<li>视频时序证据：将 I_pres/I_abl 拓展为帧级管状区域，研究双向塑形对长视频因果推理的影响。</li>
<li>多图对比：如科学文献组图，构造跨子图证据保留/删除，考察模型对跨图引用、趋势对比的感知鲁棒性。</li>
<li>生成式任务：把 BiPS 的 KL 目标迁移到图像描述、文本-到-图像生成，检验能否抑制幻觉并提升事实一致性。</li>
</ol>
<ul>
<li><strong>算法层面</strong></li>
</ul>
<ol>
<li>可学习视图生成：目前依赖 LLM 代码编辑器，可训练一个轻量级“证据分割-代码编辑”联合网络，实现端到端优化。</li>
<li>自适应系数：引入元梯度或强化控制器，在训练步层面动态调整 α、β，避免手工调参。</li>
<li>更多散度选择：探索 JS 散度、Wasserstein 或 f-散度族，分析不同度量对细粒度感知的影响。</li>
<li>双向约束的博弈视角：把 L_cons 与 L_sep 视为两人零和博弈，用纳什均衡求解器替代两阶段课程，可能进一步缓解梯度冲突。</li>
</ol>
<ul>
<li><strong>理论与分析</strong></li>
</ul>
<ol>
<li>感知瓶颈量化：建立“视觉证据熵”或“答案-视觉互信息”指标，定量衡量模型对关键像素的依赖度，验证 BiPS 是否真正提升视觉信息利用率。</li>
<li>捷径学习动态追踪：记录训练过程中文本先验与视觉先验的权重变化，揭示分离约束何时、如何打破文本主导。</li>
<li>泛化边界：基于 VC 维或 PAC-Bayes，推导双向 KL 约束对 OOD 泛化的理论保证，指导系数选择。</li>
</ol>
<p>通过上述扩展，可系统检验 BiPS 的极限与适用范围，并推动“训练时内部感知塑形”成为多模态推理的新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：大型视觉-语言模型在 VQA 中常被无关像素干扰，且难以聚焦折线、病灶等细粒度证据；现有推理时工具或中间 token 方案存在形状刚性、领域耦合、测试开销大三大缺陷。</li>
<li><p><strong>思路</strong>：把“推理时视觉提示”转为“训练时感知信号”，用<strong>证据保留图</strong>与<strong>证据删除图</strong>的 KL 散度双向塑形模型，使其既关注关键区域又阻断文本捷径，且<strong>零推理开销</strong>。</p>
</li>
<li><p><strong>方法</strong>：</p>
</li>
</ul>
<ol>
<li>基于可执行图表代码自动合成 13 k 精确配对视图（保留/删除问题相关元素）。</li>
<li>在 GRPO 框架内实施两阶段课程：</li>
</ol>
<ul>
<li>Consistency 阶段最小化  D<em>(KL)(π</em>θ(·|I,q)|π<em>θ(·|I</em>(pres),q)) ，强制聚焦。</li>
<li>Separation 阶段最大化  D<em>(KL)(π</em>θ(·|I,q)|π<em>θ(·|I</em>(abl),q)) ，强制视觉依赖。</li>
<li><strong>结果</strong>：</li>
<li>仅用 13 k 图表样本，Qwen2.5-VL-7B 在 8 项基准平均提升 7.3 %；再加 39 k 数学样本后提升至 8.2 %，超越多款专用模型。</li>
<li>消融、课程、系数、案例实验均验证双向塑形与程序化视图生成的有效性。</li>
<li><strong>贡献</strong>：提出“训练时双向感知塑形”新范式，实现数据高效、细粒度感知、无测试延迟的多模态推理增强。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是See%20Less%2C%20See%20Right%3A%20Bi-directional%20Perceptual%20Shaping%20For%20Multimodal%20Reasoning，链接是https://arxiv.org/pdf/2512.22120，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.22120。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shuoshuo Zhang,Yizhen Zhang,Jingjing Fu,Lei Song,Jiang Bian,Yujiu Yang,Rui Wang</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.22120.pdf">https://arxiv.org/pdf/2512.22120.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.22120">https://arxiv.org/abs/2512.22120</a></p>
<p><strong>Arxiv ID</strong>: 2512.22120</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.22120">https://papers.cool/arxiv/2512.22120</a></p>
<p><strong>Published</strong>: 2025-12-26T18:59:47Z</p>
<p><strong>Updated</strong>: 2025-12-26T18:59:47.000Z</p>
<hr>
<h3 id="8-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding"><a href="#8-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding" class="headerlink" title="8. Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21643">Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</a></h3><p>Weather modeling requires both accurate prediction and mechanistic interpretation, yet existing methods treat these goals in isolation, separating generation from understanding. To address this gap, we present Omni-Weather, the first multimodal foundation model that unifies weather generation and understanding within a single architecture. Omni-Weather integrates a radar encoder for weather generation tasks, followed by unified processing using a shared self-attention mechanism. Moreover, we construct a Chain-of-Thought dataset for causal reasoning in weather generation, enabling interpretable outputs and improved perceptual quality. Extensive experiments show Omni-Weather achieves state-of-the-art performance in both weather generation and understanding. Our findings further indicate that generative and understanding tasks in the weather domain can mutually enhance each other. Omni-Weather also demonstrates the feasibility and value of unifying weather generation and understanding.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>天气建模既需要准确的预测，也需要机制性的解释，但现有的方法将这两个目标孤立对待，生成与理解相分离。为了解决这一差距，我们提出了Omni-Weather，这是第一个多模态基础模型，在单一架构中统一了天气生成和理解。Omni-Weather集成了用于天气生成任务的雷达编码器，然后通过共享的自注意力机制进行统一处理。此外，我们构建了一个用于天气生成因果推理的思维链数据集，实现了可解释的输出并提高了感知质量。大量实验证明，Omni-Weather在天气生成和理解方面均达到了最先进的性能。我们的研究进一步表明，天气领域的生成任务和理解任务可以相互促进。Omni-Weather还展示了统一天气生成和理解的可行性与价值。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决气象领域中长期存在的“生成”与“理解”任务割裂的问题。现有方法将短时预报（nowcasting）、雷达反演等生成任务与雷达图像/序列的质量评估、因果解释等理解任务分离建模，导致：</p>
<ul>
<li>预报模型缺乏可解释性，无法给出物理机制层面的诊断；</li>
<li>理解模型仅能对已有结果进行后验评价，无法直接生成或改进预报场；</li>
<li>两套独立系统难以共享表征，限制了跨任务知识迁移与数据效率。</li>
</ul>
<p>为此，作者提出统一的多模态基础模型 Omni-Weather，首次在单一架构内联合建模气象生成与理解，使预报精度与机理解释相互增强，并通过引入面向天气因果推理的 Chain-of-Thought 数据集，实现可解释的生成式预报。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，均指向“生成-理解割裂”这一核心痛点：</p>
<ol>
<li>纯生成模型</li>
</ol>
<ul>
<li>短时预报：PreDiff（latent diffusion）、DiffCast（residual diffusion）、CasCast（cascade CNN）</li>
<li>雷达反演：DiffSR（卫星→雷达反射率）、WeatherGFM（in-context 通用预报-反演）</li>
<li>气候尺度：ClimaX、FengWu（Transformer 全球预报）<br>共性：仅输出物理场，不提供可解释诊断。</li>
</ul>
<ol>
<li>纯理解模型</li>
</ol>
<ul>
<li>文本分析：ClimateBERT、ClimateNLP（气候风险文本分类）</li>
<li>多模态问答：WeatherQA（20 张大气图像→灾害影响区域）、RadarQA（雷达+数值预报→专家质量报告）<br>共性：只能评价或描述，无法生成或修正预报场。</li>
</ul>
<ol>
<li>通用多模态“统一”框架（视觉-语言-生成）</li>
</ol>
<ul>
<li>Transfusion、LMFusion、MetaMorph、MetaQuery、BLIP3-o、BAGEL、Lumina-DiMOO<br>共性：在通用图文领域实现生成-理解一体化，但未触及气象物理约束与因果可解释性。</li>
</ul>
<p>Omni-Weather 首次将上述 3 类思路引入气象领域，通过共享 Transformer 骨干与天气因果 CoT 数据，把生成任务（nowcasting、inversion）与理解任务（QA、质量诊断）统一为序列到序列范式，填补“生成-理解割裂”空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>Omni-Weather</strong>，通过三项关键设计将“生成”与“理解”统一在同一基础模型中，解决气象领域长期割裂的问题：</p>
<p>1. 统一任务建模：序列到序列范式</p>
<p>将所有气象任务抽象为同一映射形式：</p>
<p>T: X arrow Y</p>
<ul>
<li><strong>生成任务</strong>（如雷达外推、卫星→雷达反演）：输入为雷达或卫星序列，输出为未来雷达场或反演场。</li>
<li><strong>理解任务</strong>（如图像/序列诊断）：输入为雷达图像或序列，输出为自然语言描述或结构化评估。</li>
</ul>
<p>通过任务提示（task-specific prompt）控制目标类型，实现<strong>单模型多任务切换</strong>。</p>
<p>2. 统一架构：共享主干 + 模态专用编码器</p>
<p>架构如图 3 所示，核心组件包括：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>Text Encoder</td>
<td>编码任务提示，统一条件空间</td>
</tr>
<tr>
<td>Understanding Encoder</td>
<td>处理单帧/序列雷达图像，用于理解任务</td>
</tr>
<tr>
<td>Generation Encoder</td>
<td>处理卫星或雷达序列，用于生成任务</td>
</tr>
<tr>
<td>Radar Sequence Encoder（EarthFormer）</td>
<td>提取时序动态，用于雷达外推</td>
</tr>
<tr>
<td>Shared Self-Attention</td>
<td>统一建模跨模态、跨任务表示</td>
</tr>
<tr>
<td>VAE Decoder</td>
<td>输出生成任务中的雷达场</td>
</tr>
<tr>
<td>Text Decoder</td>
<td>输出理解任务中的自然语言</td>
</tr>
</tbody>
</table>
</div>
<p>所有任务共享同一 Transformer 主干，<strong>生成与理解信号在表示层互补增强</strong>。</p>
<p>3. 因果推理增强：Chain-of-Thought 数据集与训练机制</p>
<p>为解决黑盒问题，构建<strong>天气因果推理数据集</strong>：</p>
<ul>
<li><strong>因果要素分类</strong>：</li>
<li>时序因子（运动方向、速度、旋转中心）</li>
<li>感知因子（形态、强度、位置）</li>
<li>直接结果（强度演变）</li>
<li>深层结果（面积变化、组织演变）</li>
<li><strong>三阶段构建流程</strong>：</li>
</ul>
<ol>
<li>GPT-4o 提取属性（JSON 格式）</li>
<li>GPT-o3 生成连贯推理文本（无列表、无机制臆测）</li>
<li>自动校验结构、因果对齐与术语一致性</li>
</ol>
<p>最终获得 <strong>4k 条 nowcasting CoT</strong> 与 <strong>4k 条 inversion CoT</strong>。</p>
<p>训练与推理一体化</p>
<ul>
<li><strong>训练阶段</strong>：<br>联合优化生成损失与理解损失，CoT 文本作为辅助监督，迫使模型同时学习“预测”与“解释”：</li>
</ul>
<p>L = ∑<em>(t ∈ T)</em>(gen) λ<em>t |y_t - y_t|_2^2 + ∑</em>(t ∈ T)<em>(under) λ_t ∑</em>(k=1)^(n<em>t) -log p_psi(y</em>(t,k) | y<em>(t,&lt;k), f</em>θ(X_t))</p>
<ul>
<li><strong>推理阶段</strong>：<br>将 CoT 提示与任务指令拼接，引导模型输出<strong>带因果链的预测结果</strong>，实现可解释生成。</li>
</ul>
<p>效果总结</p>
<ul>
<li><strong>生成侧</strong>：CRPS ↓15%，LPIPS ↓25%，高阈值 CSI ↑20%，感知质量显著提升。</li>
<li><strong>理解侧</strong>：在 RadarQA 基准上，Miss/False Alarm 准确率提升 20–25 个百分点，动态一致性 ↑10 分。</li>
<li><strong>相互增益</strong>：联合训练相比单任务训练，理解准确率 ↑5–8 分，生成 CSI ↑3–4 分，验证“生成-理解闭环”有效性。</li>
</ul>
<p>通过上述设计，Omni-Weather 首次在<strong>单一模型</strong>中实现气象生成与理解的统一，兼顾<strong>预报精度</strong>与<strong>机理解释</strong>，为可解释气象智能奠定基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>SEVIR</strong> 与 <strong>RadarQA-70K</strong> 基准上开展了三类实验，覆盖生成、理解、消融与泛化，共涉及 <strong>6 个任务、20 余项指标、8 个强 baseline</strong>，并辅以 <strong>200/4000 样本的细粒度消融与可视化案例</strong>。核心实验一览如下：</p>
<p>1. 主实验：统一生成 vs. 专用 SOTA</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>Baseline（专用模型）</th>
<th>评价指标</th>
<th>Omni-Weather 结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>Radar Nowcasting</td>
<td>EarthFormer、DiffCast、CasCast</td>
<td>CSI-M、CSI-P4/16、CRPS、SSIM、LPIPS、Radar Score</td>
<td>CRPS ↓15 %，LPIPS ↓25 %，CSI-M 持平或↑， perceptual 指标全面领先</td>
</tr>
<tr>
<td>Radar Inversion</td>
<td>UNet、ViT、WeatherGFM</td>
<td>CSI@16/74/160/181/219、RMSE、Radar Score</td>
<td>高阈值 CSI ↑20 %，RMSE 最低，Radar Score 2.42 vs 2.28（WeatherGFM）</td>
</tr>
<tr>
<td>Radar Image Understanding</td>
<td>Claude-3.5-Sonnet、Gemini-2.5-Pro、GPT-4o、RadarQA</td>
<td>Accuracy(Miss/FAR/Sharp/High-Val)、Rouge-L、BERTScore、GPT4-Score</td>
<td>Miss Acc 92.2 %（+25 ↑）、FAR Acc 88.7 %（+23 ↑），Overall 64.3 %（+2.8 ↑）</td>
</tr>
<tr>
<td>Radar Sequence Understanding</td>
<td>同上</td>
<td>Accuracy(Dyn./Cum./High-Val)、Rouge-L、BERTScore、GPT4-Score</td>
<td>Dyn. Consistency 64.1 %（+10.7 ↑），Overall 61.8 %（+5.0 ↑）</td>
</tr>
</tbody>
</table>
</div>
<p>2. 消融实验：验证三大设计必要性</p>
<p>2.1 联合训练 vs. 单任务</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>设置</th>
<th>Understanding Acc ↑</th>
<th>Generation CSI-M ↑</th>
<th>RMSE ↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>Understand-only</td>
<td>81.95 / 54.34</td>
<td>—</td>
<td>—</td>
</tr>
<tr>
<td>Generate-only</td>
<td>—</td>
<td>0.303 / 0.323</td>
<td>0.590 / 19.01</td>
</tr>
<tr>
<td>Joint (U+G)</td>
<td>86.65 / 59.58</td>
<td>0.338 / 0.347</td>
<td>0.514 / 17.11</td>
</tr>
</tbody>
</table>
</div>
<p>2.2 因果推理 (CoT) 消融</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>CoT Fine-tune</th>
<th>Thinking Inference</th>
<th>CSI-M ↑</th>
<th>CRPS ↓</th>
<th>LPIPS ↓</th>
<th>GPT4-Score ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>✗</td>
<td>✗</td>
<td>0.347</td>
<td>0.023</td>
<td>0.182</td>
<td>—</td>
</tr>
<tr>
<td>✓</td>
<td>✗</td>
<td>0.335</td>
<td>0.023</td>
<td>0.163</td>
<td>7.82</td>
</tr>
<tr>
<td>✓</td>
<td>✓</td>
<td>0.335</td>
<td>0.023</td>
<td>0.163</td>
<td>7.82</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>说明：CoT 显著降低 LPIPS（-10 %），GPT4-Score 提升 86 %，CSI 略降但仍在误差允许范围，实现“感知-解释”双赢。</p>
</blockquote>
<p>2.3 数据混合比例</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>通用数据占比</th>
<th>CSI-mean ↑</th>
<th>SSIM ↑</th>
<th>PSNR ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>0 %</td>
<td>0.2358</td>
<td>0.7528</td>
<td>21.42</td>
</tr>
<tr>
<td>30 %</td>
<td>0.2501</td>
<td>0.6866</td>
<td>19.67</td>
</tr>
<tr>
<td>50 %</td>
<td>0.2478</td>
<td>0.6823</td>
<td>19.15</td>
</tr>
<tr>
<td>70 %</td>
<td>0.1386</td>
<td>0.6187</td>
<td>16.66</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>30 % 通用数据即可带来确定性指标提升，过多反而稀释气象特征。</p>
</blockquote>
<p>2.4 雷达时序编码器 vs. vanilla VAE</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编码器</th>
<th>CSI-mean ↑</th>
<th>CSI-P16 ↑</th>
<th>SSIM ↑</th>
<th>PSNR ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAE</td>
<td>0.2358</td>
<td>0.4356</td>
<td>0.7528</td>
<td>21.42</td>
</tr>
<tr>
<td>EarthFormer</td>
<td>0.3471</td>
<td>0.5390</td>
<td>0.7621</td>
<td>23.22</td>
</tr>
</tbody>
</table>
</div>
<p>3. 定性实验：可视化与案例对比</p>
<ul>
<li><strong>Radar Inversion</strong>：Omni-Weather 在高值区结构更紧凑，CoT 推理可追溯“卫星冷池→雷达高VIL”因果链。</li>
<li><strong>Radar Nowcasting</strong>：相比 CasCast/DiffCast，风暴边缘更锐利，<think> 段落给出“东移-无旋转-强度维持”的可解释论证。</li>
<li><strong>Understanding</strong>：输出专家级报告，自动给出四维评分（Miss/FAR/Sharp/High-Val）与区域误差归因，与人工评估一致。</li>
</ul>
<p>4. 推理尺度与效率</p>
<ul>
<li>全实验在 <strong>8×H200</strong> 节点完成，20 k 步训练，256×256 输入，≈256 visual tokens/图。</li>
<li>推理时支持 <strong>CFG=2</strong> 最优平衡，生成 12 帧 VIL 仅需 <strong>≈180 ms</strong>（单张 H200）。</li>
</ul>
<p>5. 局限与后续验证</p>
<ul>
<li>尚未覆盖 <strong>中期预报、台风路径</strong> 等更长尺度任务；</li>
<li>通用 VAE 无法直接适配，需领域化微调。</li>
</ul>
<p>综上，实验从 <strong>精度-感知-解释-效率</strong> 四维度系统验证了 Omni-Weather 的统一架构有效性，并量化展示生成与理解任务 <strong>相互增益</strong> 的机制。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对 Omni-Weather 的直接延伸或深层突破，均围绕“统一生成-理解”范式在气象领域的未解问题展开：</p>
<p>1. 时空尺度外推</p>
<ul>
<li><strong>中长期天气</strong>（3–15 d）：引入层次化时间金字塔或频域分解，缓解误差累积。</li>
<li><strong>次季节-季节预测</strong>（S2S）：耦合海温、积雪、MJO 等慢变变量，探索生成-理解联合约束下的可解释性预报。</li>
<li><strong>区域-全球嵌套</strong>：研究统一模型在可变分辨率球面图上的 zero-shot 泛化，解决台风路径、季风爆发等行星尺度现象。</li>
</ul>
<p>2. 物理一致性约束</p>
<ul>
<li><strong>隐式微分方程嵌入</strong>：将动力学先验（如浅水方程、热力学守恒）作为正则项或神经网络隐式层，降低物理违背率。</li>
<li><strong>可微分数值模式耦合</strong>：把谱模式或格点模式的前向梯度接入生成分支，实现“数据-物理”双驱动。</li>
<li><strong>守恒律量化评价</strong>：新增质量、能量、涡度守恒指标，与 CSI、LPIPS 并列，引导模型学习真正的物理守恒。</li>
</ul>
<p>3. 多模态扩展</p>
<ul>
<li><strong>雷达-卫星-闪电-地面自动站-模式场五模态对齐</strong>：构建任意→任意映射（如闪电→雷达、模式→卫星），验证统一 tokenizer 的跨传感器能力。</li>
<li><strong>文本-视觉-图表混合推理</strong>：引入地面观测报告、社交媒体灾情文本，实现“人-机”混合增强预报。</li>
<li><strong>声音模态探索</strong>：利用雷声、风噪等声学信号反演对流强度，拓展多模态边界。</li>
</ul>
<p>4. 因果与可解释性深化</p>
<ul>
<li><strong>反事实生成</strong>：给定“无地形”“无城市热岛”等反事实条件，生成对应雷达场，量化特定因子贡献。</li>
<li><strong>干预-响应曲线</strong>：系统性地扰动温度、湿度初值，观测生成输出的梯度，绘制因果响应图。</li>
<li><strong>人类可编辑推理</strong>：支持预报员在<think>链中插入“人工干预节点”，实时重定向生成轨迹，实现交互式预报。</li>
</ul>
<p>5. 高效架构与部署</p>
<ul>
<li><strong>连续时间隐空间模型</strong>：用神经 ODE 或 DiT (Diffusion Transformer) 替代离散帧 VAE，实现任意步长插值与可变时效输出。</li>
<li><strong>移动端轻量化</strong>：蒸馏至 ≤1 B 参数，支持边缘 GPU/TPU 实时推理，服务基层台站。</li>
<li><strong>在线增量学习</strong>：设计记忆回放与正则平衡策略，使模型在新年份、新区域数据上持续更新而不遗忘。</li>
</ul>
<p>6. 极端事件与灾害链</p>
<ul>
<li><strong>快速增强台风、爆发性气旋</strong>等小样本极端：引入元学习与外部记忆，提高稀有模式命中率。</li>
<li><strong>级联灾害理解</strong>：一次模式调用同时生成降水→径流→城市内涝→交通中断的跨领域序列，并给出结构化风险报告。</li>
<li><strong>社会韧性评估</strong>：将经济、人口暴露数据接入理解分支，输出“物理-社会”复合损失指数，支持决策。</li>
</ul>
<p>7. 评价基准与社区共建</p>
<ul>
<li><strong>Omni-Weather-Bench</strong>：开源覆盖 0–15 d、多传感器、多语言问答的统一评测套件，包含物理守恒、因果可解释性新指标。</li>
<li><strong>人类-模型协同评分</strong>：引入预报员盲评与模型互评机制，建立“气象图灵测试”排行榜。</li>
<li><strong>持续学习赛道</strong>：每年发布新区域、新极端案例，鼓励社区在不重训全量模型前提下快速适应。</li>
</ul>
<p>8. 气候变率与变化</p>
<ul>
<li><strong>升温情景 downscaling</strong>：输入 SSP-RCP 情景的大尺度场，生成高分辨率区域极端降水变化，并输出归因文字。</li>
<li><strong>年代际振荡可解释性</strong>：利用 CoT 链追踪 AMO、PDO 位相转换对局地干旱-洪涝交替的影响路径。</li>
</ul>
<p>这些方向既可直接继承 Omni-Weather 的“共享主干+CoT”框架，又需在<strong>物理嵌入、多模态对齐、因果推理、高效部署</strong>四个维度上突破现有技术栈，为构建下一代“可解释、可交互、可信任”的气象基础模型提供持续动力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Omni-Weather</strong>，首个将<strong>天气生成</strong>（nowcasting、雷达反演）与<strong>天气理解</strong>（图像/序列诊断、问答）统一在<strong>单一多模态基础模型</strong>中的框架，解决气象领域长期存在的“生成-理解割裂”问题。核心内容可概括为四点：</p>
<p>1. 统一范式</p>
<ul>
<li>所有任务形式化为 <strong>序列到序列映射</strong></li>
</ul>
<p>T: X arrow Y</p>
<p>通过任务提示控制输出类型：</p>
<ul>
<li><strong>生成</strong>：10 帧 VIL → 12 帧 VIL；卫星双红外 → VIL 场</li>
<li><strong>理解</strong>：单帧或序列雷达 → 自然语言评估（Miss/FAR/Sharp/High-Val 等）</li>
</ul>
<p>2. 统一架构</p>
<ul>
<li><strong>共享 Transformer 主干</strong></li>
<li><strong>模态专用编码器</strong>：</li>
<li>Understanding Encoder（图像/序列）</li>
<li>Generation Encoder（卫星/雷达）</li>
<li>Radar Sequence Encoder（EarthFormer，注入时序动态）</li>
<li><strong>双解码器</strong>：VAE 解码生成场，Text 解码诊断文本<br>→ 单模型即可切换任务，表征层共享互补</li>
</ul>
<p>3. 因果可解释</p>
<ul>
<li>构建 <strong>天气 Chain-of-Thought 数据集</strong>（4k 外推 + 4k 反演）</li>
<li>先验属性：形态、运动、强度、旋转等</li>
<li>三阶段：GPT-4o 标注 → GPT-o3 生成连贯推理 → 自动质检</li>
<li><strong>训练</strong>：强制模型同时输出<think>推理链与最终预测</li>
<li><strong>推理</strong>：提示+链式思考，实现“预测+解释”端到端</li>
</ul>
<p>4. 实验效果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>指标提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>Radar Nowcasting</td>
<td>CRPS ↓15 %，LPIPS ↓25 %， perceptual 领先</td>
</tr>
<tr>
<td>Radar Inversion</td>
<td>高阈值 CSI ↑20 %，RMSE 最低</td>
</tr>
<tr>
<td>Image/Sequence Understanding</td>
<td>Miss/FAR Acc ↑20–25 分，Dynamic Consistency ↑10 分</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>联合训练 &gt; 单任务</strong>：生成与理解相互增益</li>
<li><strong>CoT 推理</strong>：感知质量↑，像素误差微降，实现“ fidelity-interpretability ”折中</li>
<li><strong>数据混合</strong>：30 % 通用数据即可增强确定性指标</li>
</ul>
<p>结论</p>
<p>Omni-Weather 首次验证“生成-理解统一”在气象领域的可行性与互补性，为<strong>可解释、多任务、通用气象基础模型</strong>提供新范式。代码与数据集已开源。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Omni-Weather%3A%20Unified%20Multimodal%20Foundation%20Model%20for%20Weather%20Generation%20and%20Understanding，链接是https://arxiv.org/pdf/2512.21643，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21643。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhiwang Zhou,Yuandong Pu,Xuming He,Yidi Liu,Yixin Chen,Junchao Gong,Xiang Zhuang,Wanghan Xu,Qinglong Cao,Shixiang Tang,Yihao Liu,Wenlong Zhang,Lei Bai</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21643.pdf">https://arxiv.org/pdf/2512.21643.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21643">https://arxiv.org/abs/2512.21643</a></p>
<p><strong>Arxiv ID</strong>: 2512.21643</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21643">https://papers.cool/arxiv/2512.21643</a></p>
<p><strong>Published</strong>: 2025-12-25T12:08:09Z</p>
<p><strong>Updated</strong>: 2025-12-25T12:08:09.000Z</p>
<hr>
<h3 id="9-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search"><a href="#9-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search" class="headerlink" title="9. InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.18745">InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</a></h3><p>The ability for AI agents to “think with images” requires a sophisticated blend of reasoning and perception. However, current open multimodal agents still largely fall short on the reasoning aspect crucial for real-world tasks like analyzing documents with dense charts/diagrams and navigating maps. To address this gap, we introduce O3-Bench, a new benchmark designed to evaluate multimodal reasoning with interleaved attention to visual details. O3-Bench features challenging problems that require agents to piece together subtle visual information from distinct image areas through multi-step reasoning. The problems are highly challenging even for frontier systems like OpenAI o3, which only obtains 40.8% accuracy on O3-Bench. To make progress, we propose InSight-o3, a multi-agent framework consisting of a visual reasoning agent (vReasoner) and a visual search agent (vSearcher) for which we introduce the task of generalized visual search — locating relational, fuzzy, or conceptual regions described in free-form language, beyond just simple objects or figures in natural images. We then present a multimodal LLM purpose-trained for this task via reinforcement learning. As a plug-and-play agent, our vSearcher empowers frontier multimodal models (as vReasoners), significantly improving their performance on a wide range of benchmarks. This marks a concrete step towards powerful o3-like open systems. Our code and dataset can be found at <a target="_blank" rel="noopener" href="https://github.com/m-Just/InSight-o3">https://github.com/m-Just/InSight-o3</a> .</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>人工智能代理“通过图像进行思考”的能力需要推理与感知的复杂结合。然而，目前的开放多模态代理在推理方面仍然存在很大不足，而推理对于现实世界任务（如分析包含密集图表/示意图的文档和导航地图）至关重要。为了解决这一差距，我们推出了 O3-Bench，这是一个用于评估多模态推理的新基准，特点是对视觉细节进行交错关注。O3-Bench 包含具有挑战性的问题，要求代理通过多步骤推理，将来自不同图像区域的细微视觉信息拼凑在一起。这些问题即使对于前沿系统如 OpenAI o3 也非常具有挑战性，后者在 O3-Bench 上的准确率仅为 40.8%。为了取得进展，我们提出了 InSight-o3，这是一个由视觉推理代理（vReasoner）和视觉搜索代理（vSearcher）组成的多代理框架，并为其引入了通用视觉搜索任务——不仅限于自然图像中的简单物体或图形，还需根据自由语言描述定位关系性、模糊或概念性区域。随后，我们展示了一个为该任务通过强化学习专门训练的多模态大语言模型（LLM）。作为即插即用的代理，我们的 vSearcher 可以赋能前沿多模态模型（作为 vReasoners），显著提升它们在各种基准测试上的表现。这标志着迈向强大 o3 类开放系统的具体一步。我们的代码和数据集可在 <a target="_blank" rel="noopener" href="https://github.com/m-Just/InSight-o3">https://github.com/m-Just/InSight-o3</a>  获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对当前开放多模态模型在“用图像思考”任务中推理能力薄弱、难以完成真实场景下的复杂视觉-语言协同问题（如高密度图表/地图分析）这一核心痛点，提出并解决以下关键问题：</p>
<ol>
<li><p>评估基准缺失<br>现有 benchmark 侧重单步感知或粗粒度识别，缺乏对“跨区域证据聚合 + 多跳推理”的系统性评测。为此，作者构建 O3-BENCH，用高分辨率、高信息密度的复合图表与数字地图，迫使模型必须交替执行视觉搜索与推理才能作答。</p>
</li>
<li><p>单模型瓶颈<br>将感知与推理耦合在一个 MLLM 内，导致高分辨率图像上下文过长、搜索与推理信号相互干扰。论文提出 INSIGHT-O3 双代理框架：</p>
</li>
</ol>
<ul>
<li>vReasoner 专责高层推理与任务分解；</li>
<li>vSearcher 专责基于自由语言描述的广义视觉搜索（可定位关系型、模糊或概念区域）。</li>
</ul>
<ol>
<li>广义视觉搜索的训练难题<br>传统视觉搜索仅面向自然图像中的具体物体。本文将搜索空间拓展到任意图像类型（地图、海报、截图等）与自由形式描述，并通过混合强化学习（in-loop + out-of-loop RL）训练 vSearcher，使其能精准定位语言指定的抽象区域，并以“即插即用”方式赋能不同 vReasoner。</li>
</ol>
<p>综上，论文同时解决了“如何评测复杂视觉推理”“如何分解推理-感知负载”“如何训练可泛化的视觉搜索代理”三大问题，从而向开放版 o3-like 系统迈出实质性一步。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 Related Work（第 2 页与附录 A）中系统梳理了四条研究脉络，可归纳为以下四类相关研究：</p>
<p>1. 多模态评测基准</p>
<ul>
<li><strong>经典 VQA 数据集</strong></li>
<li>VQA-v2、COCO-QA、GQA、VizWiz、ScienceQA、MMBench、MME、MathVista 等</li>
<li>侧重整体图像或显著物体识别，现代 MLLM 已接近饱和。</li>
<li><strong>高分辨率 / 细粒度基准</strong></li>
<li>V★-Bench、Tree-Bench、VisualProbe、HR-Bench、MME-RealWorld</li>
<li>强调高分辨率、小目标、单区域查找，但问题多为单跳或轻推理。</li>
<li><strong>O3-BENCH 的差异化</strong></li>
<li>首次针对“复合图表 + 数字地图”设计多跳问答，要求跨布局证据聚合与轻量级计算，平均分辨率 4600×3900，GPT-5-mini 仅 39 % 准确率，显著难于上述基准。</li>
</ul>
<p>2. 多模态推理模型</p>
<ul>
<li><strong>RL 驱动的大语言模型</strong></li>
<li>DeepSeek-R1（GRPO）→ 产生长链式思维、自反演。</li>
<li><strong>视觉-语言 RL 扩展</strong></li>
<li>Vision-R1、MM-Eureka、VLM-R1、Skywork-R1V、InternVL3.5、Keye-VL1.5 等</li>
<li>采用 GRPO 或级联 RL 提升 STEM 推理，但仍以文本链式思考为主，视觉侧仅提供 OCR/整体图特征。</li>
<li><strong>INSIGHT-O3 的差异化</strong></li>
<li>首次把“视觉搜索”从推理流中解耦，用专用代理在像素空间执行多轮裁剪与定位，再反哺推理链。</li>
</ul>
<p>3. 分层代理与工具使用框架</p>
<ul>
<li><strong>早期 LLM 编排</strong></li>
<li>Socratic Models、HuggingGPT、VisProg、ViperGPT</li>
<li>用 LLM 生成代码或调用专家模型，完成视觉-语言组合任务。</li>
<li><strong>现代多代理系统</strong></li>
<li>CAMEL、MetaGPT、AutoGen、HALO、AgentOrchestra、HYDRA</li>
<li>引入角色分工、动态规划、强化式协调，但视觉侧多依赖现成检测/分割 API，未涉及“自由语言描述→模糊区域定位”的广义搜索。</li>
<li><strong>INSIGHT-O3 的差异化</strong></li>
<li>提出“vSearcher”专职像素级搜索，支持关系型、模糊或概念描述；通过 RL 与 vReasoner 协同训练，实现可插拔增强。</li>
</ul>
<p>4. 视觉搜索与交互式感知</p>
<ul>
<li><strong>单轮、单区域搜索</strong></li>
<li>V★、Pixel-Reasoner、DeepEyes、Mini-o3、Chain-of-Focus、OpenThinker</li>
<li>聚焦自然图像中的显著物体，一次定位即回答。</li>
<li><strong>代码式多轮操作</strong></li>
<li>PyVision、Active-o3、RoboRefer 等</li>
<li>生成代码执行裁剪/旋转/放大，但仍以“找得到物体”为目标，缺乏跨区域证据聚合。</li>
<li><strong>INSIGHT-O3 的差异化</strong></li>
<li>把搜索任务从推理任务完全解耦；支持任意图像类型与自由语言描述；通过混合 RL（IoU 监督 + 推理结果监督）训练，实现多跳、多区域、高分辨率场景下的精准定位。</li>
</ul>
<p>综上，本文在基准、模型架构、训练算法三大层面均与现有研究形成互补或跃迁，填补了“开放系统如何实现 o3-like 复杂视觉推理”这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“三位一体”策略——新基准、新框架、新训练法——系统性地解决“开放模型难以完成复杂视觉推理”这一核心问题。具体技术路线如下：</p>
<p>1. 建立高难度评测基准 O3-BENCH</p>
<ul>
<li><strong>数据构造</strong></li>
<li>图表：从 MME-RealWorld 与互联网收集≥8 布局的高密度图表 117 张。</li>
<li>地图：人工采集 87 张场馆级高清数字地图（≥4K×4K）。</li>
<li><strong>问题设计</strong></li>
<li>全部六选项多跳选择题，平均 2.4 个目标布局，需跨区检索+轻量计算。</li>
<li>自动预生成 + 人工重写 + 三模型难度过滤，确保 GPT-5-mini 仅 39 % 准确率。</li>
</ul>
<p><strong>作用</strong>：为后续模型优化提供明确、可度量的优化目标。</p>
<p>2. 提出双代理框架 INSIGHT-O3</p>
<ul>
<li><strong>角色分离</strong></li>
<li>vReasoner（任意前沿 MLLM）：负责高层规划、算术、世界知识、最终答案。</li>
<li>vSearcher（可插拔 7B 模型）：专责“广义视觉搜索”——根据自然语言描述返回精准 bounding box，可调用图像裁剪工具。</li>
<li><strong>协作流程</strong></li>
</ul>
<ol>
<li>vReasoner 分解问题 → 生成 region_description</li>
<li>vSearcher 定位 → 返回裁剪图</li>
<li>vReasoner 基于新证据继续推理，循环直至可作答</li>
</ol>
<p><strong>作用</strong>：把交织在一起的感知-推理负载解耦，降低单模型上下文长度与优化难度。</p>
<p>3. 设计混合强化学习训练算法</p>
<p>目标：仅训练 vSearcher，使其与任意 vReasoner 即插即用。</p>
<ul>
<li><strong>Out-of-loop RL</strong></li>
<li>数据：10 K 预生成（InfographicVQA + 布局检测 + GPT-5-nano 描述）。</li>
<li>奖励： r = I<em>(n_rm tool)&gt;0 · (λ</em>(rm format) r<em>(rm format) + λ</em>(rm IoU) r_(rm IoU)) ，直接以 IoU 监督定位精度。</li>
<li><strong>In-loop RL</strong></li>
<li>数据：15 K 合成拼贴图，vReasoner 实时生成搜索请求。</li>
<li>奖励：伪 IoU $r_(rm IoU) = I<br>s = c = 1<br>，其中 s 为 vReasoner 对 crop 的有用性评分， c$ 为最终答案正确性。</li>
<li><strong>优势估计与目标</strong></li>
<li>Out-of-loop 采用 GRPO 组内标准化；In-loop 采用全局标准化。</li>
<li>统一 PPO-clip 目标，加入 KL 正则防止偏离初始策略。</li>
</ul>
<p><strong>作用</strong>：让 vSearcher 同时学会“定位准”和“对推理有用”，且与具体 vReasoner 无关。</p>
<p>4. 实验验证“即插即用”效果</p>
<ul>
<li><strong>跨模型提升</strong></li>
<li>GPT-5-mini + InSight-o3-vS：O3-BENCH 39.0 % → 61.5 %。</li>
<li>Gemini-2.5-Flash：49.8 % → 58.0 %；V★-Bench 80.1 % → 87.6 %。</li>
<li><strong>跨分辨率鲁棒</strong></li>
<li>训练 3.2 M 像素，测试 0.8 M–12.8 M 像素，性能下降 &lt; 3 %。</li>
<li><strong>消融分析</strong></li>
<li>去掉 in-loop 或 out-of-loop 任一组件，平均性能下降 1–3 %。</li>
<li>去掉工具调用奖励、vReasoner 反馈或结果监督，均出现可见下降。</li>
</ul>
<p>5. 释放失败案例与改进路径</p>
<p>论文公开典型错误：vReasoner 幻觉、视觉误读、搜索委托不当等，并指出随着开放模型工具调用与推理能力增强，INSIGHT-O3 框架可继续放大收益。</p>
<p>综上，论文通过“高难度基准→双代理分工→混合 RL 训练→即插即用验证”的完整闭环，首次在开放模型上实现了接近 o3 的“用图像思考”能力，为后续社区复现与扩展提供了可落地的路线图。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕「O3-BENCH 难度验证」「INSIGHT-O3 有效性」「消融与超参」「跨模型/跨分辨率泛化」「失败诊断」五个维度，共执行了 4 组主实验 + 7 类辅助实验，全部结果均基于 3 次随机种子平均（除非特别说明）。</p>
<p>1 主实验 1：O3-BENCH 难度与布局必要性验证</p>
<ul>
<li><strong>被测模型</strong>：GPT-5-mini、Qwen2.5-VL-7B</li>
<li><strong>变量</strong>：仅原图 vs 原图+人工标注的「目标布局裁剪」</li>
<li><strong>结论</strong>：</li>
<li>GPT-5-mini 39.0 % → 67.5 %（↑28.5 pp）</li>
<li>Qwen2.5-VL-7B 27.4 % → 35.4 %（↑8 pp）<br>证明基准的确需要「主动定位+跨区推理」，而非单看整体图。</li>
</ul>
<p>2 主实验 2：INSIGHT-O3 跨域提升测试</p>
<ul>
<li><strong>vReasoner 列表</strong>：GPT-4o、GPT-5-nano、GPT-5-mini、Gemini-2.5-Flash、Qwen3-VL-32B</li>
<li><strong>vSearcher 变量</strong>：无 vS / 原始 Qwen2.5-VL-7B / InSight-o3-vS</li>
<li><strong>覆盖基准</strong>：O3-BENCH、V★-Bench、Tree-Bench、VisualProbe-Hard、HR-Bench4K、MME-RW-Lite</li>
<li><strong>核心结果</strong>（表 1 汇总）：</li>
<li>GPT-5-mini 平均 53.7 % → 64.9 %（+11.2 pp），其中 O3-BENCH +22.5 pp。</li>
<li>Gemini-2.5-Flash 平均 53.4 % → 60.7 %（+7.3 pp）。</li>
<li>小模型 GPT-5-nano 在 VisualProbe-Hard 从 21.7 % → 31.4 %（+9.7 pp）。<br><strong>结论</strong>：InSight-o3-vS 可插拔且跨模型、跨基准稳定提升。</li>
</ul>
<p>3 主实验 3：训练/测试分辨率与调用次数敏感性</p>
<ul>
<li><strong>训练分辨率</strong>：1280² vs 3500²</li>
<li><strong>测试分辨率</strong>：1280² ~ 3500²</li>
<li><strong>指标</strong>：6 项基准平均 + 单样本平均 vSearcher 调用次数</li>
<li><strong>结论</strong>（表 2、表 4）：</li>
<li>训练-测试分辨率不一致时性能波动 ≤ 1.5 pp，显示鲁棒。</li>
<li>输入像素 0.8 M → 12.8 M，V★-Bench 仅 85.3 % ~ 89.4 % 小范围波动；O3-BENCH 56 % ~ 62 %，调用次数 2.8 → 2.6 呈下降趋势（高分辨率一次看清）。</li>
</ul>
<p>4 主实验 4：消融与奖励设计消融</p>
<ul>
<li><strong>因子</strong>：In-loop / Out-of-loop、工具条件奖励、vReasoner 反馈、结果监督、全局归一化</li>
<li><strong>结论</strong>（表 3、表 5）：</li>
<li>完整混合 RL 平均 63.2 %；去掉任一分量下降 1.0 ~ 2.6 pp。</li>
<li>仅 In-loop 训练步耗时 + 470 %，性能仍低于混合方案。</li>
<li>训练曲线（图 3）显示：先学会调用工具 → IoU 与 vReasoner 准确率同步上升，验证奖励设计合理性。</li>
</ul>
<p>5 辅助实验 A：开放模型适配验证</p>
<ul>
<li><strong>Qwen3-VL-32B 自当 vReasoner</strong>，对比原始模型与 +InSight-o3-vS：<br>O3-BENCH 60.4 % → 69.7 %（+9.3 pp），证明框架对开源模型同样有效。</li>
</ul>
<p>6 辅助实验 B：调用质量人工评分</p>
<ul>
<li>随机抽取 60 个搜索回合，由三名标注员对返回 crop 评 High/Medium/Low：</li>
<li>InSight-o3-vS High 率 68 %，Medium 28 %；</li>
<li>原始 Qwen2.5-VL-7B High 率 27 %，Low 率 38 %。<br>直观说明 RL 训练显著提升定位精度与上下文相关性。</li>
</ul>
<p>7 辅助实验 C：失败案例剖析</p>
<ul>
<li>归类 120 个错误样本：</li>
<li>81 % 归因于 vReasoner 幻觉或计算错误；</li>
<li>14 % 为 vSearcher 定位偏差；</li>
<li>5 % 为工具调用格式违规。<br>指明下一步应优先提升 vReasoner 的“看图”一致性。</li>
</ul>
<p>8 辅助实验 D：与 MME-RealWorld 对比</p>
<ul>
<li>同取 chart 子集：</li>
<li>GPT-5-mini 在 MME-RW 82.4 % vs O3-BENCH 38.2 %；</li>
<li>平均 vSearch 步数 1.1 vs 3.1；</li>
<li>平均回答长度 730 字 vs 1940 字。<br>量化证明 O3-BENCH 显著更难、更需要多跳搜索。</li>
</ul>
<p>综上，实验从「基准难度→模型提升→训练机制→超参敏感→失败分析」全链路验证：</p>
<ol>
<li>O3-BENCH 确实构成新的挑战；</li>
<li>INSIGHT-O3 以即插即用方式显著、一致且鲁棒地增强现有前沿多模态模型；</li>
<li>混合 RL 与双代理分工是取得提升的关键。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可直接延续本文成果，也可作为独立课题展开，分为“基准-数据”“模型-架构”“训练-算法”“评测-分析”四条线，供后续研究参考：</p>
<p>1 基准与数据扩展</p>
<ul>
<li><p><strong>自然图像多跳推理子集</strong><br>将 O3-BENCH 的“图表+地图”双域扩展到自然场景（街景、航拍、医学影像），构造需要跨物体、跨时间帧、跨模态（可见光-红外）证据聚合的问题，补齐“数字域-自然域”空白。</p>
</li>
<li><p><strong>动态/时序图像推理</strong><br>引入视频或 GIF 图表（如动态仪表盘、实时监控屏），测试模型对“时序+空间”双维度变化的多跳追踪能力。</p>
</li>
<li><p><strong>多语言与低资源场景</strong><br>当前问题与 OCR 均以英文为主。可构建中文、多语种的图表/地图，考察模型在混合语言、低资源语言下的视觉-语义对齐。</p>
</li>
<li><p><strong>可解释性标注升级</strong><br>在现有“目标布局+步骤说明”基础上，增加“推理链中每一步所依赖的像素级掩码”与“反事实掩码”（ablation mask），便于后续做细粒度可解释性评测。</p>
</li>
</ul>
<p>2 模型与架构方向</p>
<ul>
<li><p><strong>双向协同训练</strong><br>本文仅训练 vSearcher，vReasoner frozen。下一步可设计“双端 RL”或“互蒸馏”：vReasoner 的搜索请求质量与 vSearcher 的定位精度互为环境，联合优化，缓解 credit assignment 难题。</p>
</li>
<li><p><strong>多模态搜索工具箱</strong><br>除裁剪外，引入箭头绘制、高亮、放大、旋转、OCR 触发、图表重渲染等离散工具，让 vSearcher 成为“视觉工具调用代理”，支持更复杂的图像编辑与信息提取。</p>
</li>
<li><p><strong>统一搜索-推理模型</strong><br>以 InSight-o3-vS 为视觉专家，采用 MoE 或 early-fusion 方式把搜索专家并入大型多模态基座模型，实现单模型端到端“思考+搜索”，减少多代理通信延迟。</p>
</li>
<li><p><strong>内存与层级推理</strong><br>引入 episodic memory 存储已访问过的区域特征，支持“返回上一级”“对比两区域”等高级操作，迈向更接近人类视觉导航的层级认知。</p>
</li>
</ul>
<p>3 训练与算法创新</p>
<ul>
<li><p><strong>课程强化学习</strong><br>由“单目标+大目标”逐步到“多目标+小目标+跨图推理”，用课程式奖励减少初期探索稀疏性，提升样本效率。</p>
</li>
<li><p><strong>对比式搜索奖励</strong><br>不仅与 GT box 计算 IoU，同时与“混淆区域”计算对比距离，鼓励模型学习更具判别性的视觉-语义表示。</p>
</li>
<li><p><strong>在线人类偏好对齐</strong><br>收集人工对“搜索结果是否有助于后续推理”的偏好标签，采用 DPO 或 KTO 对齐 vSearcher，使其更符合人类指令风格。</p>
</li>
<li><p><strong>多任务正则</strong><br>同时优化搜索、OCR、图表计算、地图路径规划等多头目标，通过梯度掩码与任务路由，防止灾难性遗忘并提升泛化。</p>
</li>
</ul>
<p>4 评测与分析深化</p>
<ul>
<li><p><strong>细粒度错误 Taxonomy</strong><br>建立“搜索缺失-搜索冗余-推理误用-计算错误-幻觉”五维标签体系，对 10 K 级样本进行标注，形成公开错误诊断工具包，驱动针对性改进。</p>
</li>
<li><p><strong>可扩展性压力测试</strong><br>将单图 4K 提升到 8K-16K，或一次输入 5-10 张高分辨率图，考察系统随着信息密度线性增加时的性能衰减曲线，验证上下文长度与工具调用上限。</p>
</li>
<li><p><strong>对抗与鲁棒性评测</strong><br>在图像中加入扰动（高斯噪声、压缩、水印遮挡）或语言描述中植入同音/近义词干扰，测试搜索-推理链的鲁棒性。</p>
</li>
<li><p><strong>人机协作范式</strong><br>引入“人-代理-工具”闭环，允许人类在任意搜索回合纠正或提示，量化“人类成本-性能提升”曲线，为实际部署提供 ROI 依据。</p>
</li>
</ul>
<p>5 应用与系统落地</p>
<ul>
<li><p><strong>文档理解 SaaS</strong><br>将框架封装为可调 API，支持上传 PDF/幻灯片/报表后自动提取跨页指标、生成财务分析报告，与现有 RAG 方案对比商业效果。</p>
</li>
<li><p><strong>地图导航助手</strong><br>与真实地图厂商合作，在景区/校园/医院室内地图场景开放内测，收集真实用户查询日志，持续 RL 微调。</p>
</li>
<li><p><strong>教育场景可视化辅导</strong><br>针对 STEM 教材中的复杂插图（电路图、几何证明、统计图）提供分步视觉讲解，评估学生知识点掌握度提升。</p>
</li>
</ul>
<p>总结</p>
<p>InSight-O3 首次验证了“开放模型 + 专用视觉搜索代理”在超高分辨率、多跳推理场景的可行性。后续工作可从“更全数据域、更深协同训练、更细评测维度、更广落地场景”四个层面继续突破，推动开放多模态系统真正迈向 o3-level 的“用图像思考”能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心内容速览</p>
<p>1. 研究动机</p>
<ul>
<li>现有开放多模态模型在”用图像思考”任务上推理薄弱，难以处理高密度图表/地图等真实场景</li>
<li>主流基准侧重单跳感知，缺少对”跨区域证据+多跳推理”的评测</li>
<li>OpenAI o3 的闭源优势亟需开放系统复现</li>
</ul>
<p>2. 贡献总览</p>
<ul>
<li><strong>O3-BENCH</strong>：345 例高分辨率图表/地图多跳选择题，GPT-5-mini 仅 39%</li>
<li><strong>INSIGHT-O3</strong> 双代理框架：</li>
<li>vReasoner：任意前沿 MLLM，负责高层推理</li>
<li>vSearcher：7B 专用视觉搜索代理，支持自由语言描述定位</li>
<li><strong>混合 RL 训练</strong>：预生成 IoU 监督 + 在线推理结果监督，即插即用提升多模型 7-22 pp</li>
</ul>
<p>3. 技术要点</p>
<ul>
<li>基准构造：布局检测→自动问答生成→人工精修→三模型难度过滤</li>
<li>搜索任务广义化：支持关系、模糊、概念区域；任意图像类型</li>
<li>奖励设计： r = I<em>(n_rm tool)&gt;0 · (λ</em>(rm format) r<em>(rm format) + λ</em>(rm IoU) r_(rm IoU)) ，辅以 outcome 监督防止幻觉</li>
<li>协同流程：Reasoner 分解问题→描述区域→Searcher 返回裁剪→循环直至答案</li>
</ul>
<p>4. 主要结果</p>
<ul>
<li>GPT-5-mini + InSight-o3-vS：O3-BENCH 39%→61.5%，六基准平均 +11.2 pp</li>
<li>Gemini-2.5-Flash：同等设置 +7.3 pp，跨模型泛化显著</li>
<li>分辨率 0.8 M-12.8 M 像素性能波动 &lt;3%，训练-测试分辨率不一致仍鲁棒</li>
<li>消融：缺 in-loop 或 out-of-loop 下降 1-2.6 pp；奖励组件均贡献正值</li>
</ul>
<p>5. 结论与展望</p>
<p>INSIGHT-O3 通过”高难度基准+感知-推理解耦+混合 RL 训练”首次在开放模型上实现接近 o3 的复杂视觉推理能力，为构建通用多模态智能体提供了可扩展、即插即用的技术路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是InSight-o3%3A%20Empowering%20Multimodal%20Foundation%20Models%20with%20Generalized%20Visual%20Search，链接是https://arxiv.org/pdf/2512.18745，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.18745。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kaican Li,Lewei Yao,Jiannan Wu,Tiezheng Yu,Jierun Chen,Haoli Bai,Lu Hou,Lanqing Hong,Wei Zhang,Nevin L. Zhang</p>
<p><strong>Categories</strong>: cs.CV,cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.18745.pdf">https://arxiv.org/pdf/2512.18745.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.18745">https://arxiv.org/abs/2512.18745</a></p>
<p><strong>Arxiv ID</strong>: 2512.18745</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.18745">https://papers.cool/arxiv/2512.18745</a></p>
<p><strong>Published</strong>: 2025-12-21T14:23:07Z</p>
<p><strong>Updated</strong>: 2025-12-21T14:23:07.000Z</p>
<hr>
<h3 id="10-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents"><a href="#10-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents" class="headerlink" title="10. SWE-RM: Execution-free Feedback For Software Engineering Agents"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21919">SWE-RM: Execution-free Feedback For Software Engineering Agents</a></h3><p>Execution-based feedback like unit testing is widely used in the development of coding agents through test-time scaling (TTS) and reinforcement learning (RL). This paradigm requires scalable and reliable collection of unit test cases to provide accurate feedback, and the resulting feedback is often sparse and cannot effectively distinguish between trajectories that are both successful or both unsuccessful. In contrast, execution-free feedback from reward models can provide more fine-grained signals without depending on unit test cases. Despite this potential, execution-free feedback for realistic software engineering (SWE) agents remains underexplored. Aiming to develop versatile reward models that are effective across TTS and RL, however, we observe that two verifiers with nearly identical TTS performance can nevertheless yield very different results in RL. Intuitively, TTS primarily reflects the model’s ability to select the best trajectory, but this ability does not necessarily generalize to RL. To address this limitation, we identify two additional aspects that are crucial for RL training: classification accuracy and calibration. We then conduct comprehensive controlled experiments to investigate how to train a robust reward model that performs well across these metrics. In particular, we analyze the impact of various factors such as training data scale, policy mixtures, and data source composition. Guided by these investigations, we introduce SWE-RM, an accurate and robust reward model adopting a mixture-of-experts architecture with 30B total parameters and 3B activated during inference. SWE-RM substantially improves SWE agents on both TTS and RL performance. For example, it increases the accuracy of Qwen3-Coder-Flash from 51.6% to 62.0%, and Qwen3-Coder-Max from 67.0% to 74.6% on SWE-Bench Verified using TTS, achieving new state-of-the-art performance among open-source models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于执行的反馈（如单元测试）在通过测试时扩展（TTS）和强化学习（RL）开发编码代理时被广泛使用。这种范式需要可扩展且可靠的单元测试用例收集以提供准确的反馈，但得到的反馈通常是稀疏的，无法有效区分同为成功或同为失败的轨迹。相比之下，来自奖励模型的无执行反馈可以在不依赖单元测试用例的情况下提供更细粒度的信号。尽管有这种潜力，但针对现实软件工程（SWE）代理的无执行反馈仍然未被充分探索。然而，在旨在开发在TTS和RL中都有效的通用奖励模型时，我们发现两个在TTS性能几乎相同的验证器在RL中仍可能产生非常不同的结果。直观上，TTS主要反映模型选择最佳轨迹的能力，但这种能力不一定能推广到RL。为了解决这一限制，我们确定了对RL训练至关重要的两个额外方面：分类准确性和校准性。随后，我们进行了全面的控制实验，研究如何训练在这些指标上表现良好的稳健奖励模型。特别是，我们分析了训练数据规模、策略混合和数据源组成等各种因素的影响。在这些研究的指导下，我们引入了SWE-RM，一个准确且稳健的奖励模型，采用30B总参数、推理时激活3B参数的专家混合架构。SWE-RM显著提升了SWE代理在TTS和RL上的性能。例如，在SWE-Bench Verified测试中，它将Qwen3-Coder-Flash的准确率从51.6%提升至62.0%，将Qwen3-Coder-Max的准确率从67.0%提升至74.6%，在开源模型中创下新的最先进性能。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“如何为软件工程（SWE）智能体训练一个既适用于测试时扩展（TTS）又适用于强化学习（RL）的通用、无执行（execution-free）奖励模型”这一核心问题。具体而言，作者观察到：</p>
<ul>
<li>执行型反馈（如单元测试）只能给出稀疏的 0/1 信号，难以区分同为成功或同为失败的轨迹，且高质量测试用例难以规模化获取；</li>
<li>现有的无执行奖励模型主要被用来提升 TTS，但 TTS 表现相近的两个验证器在 RL 中可能表现迥异，说明 TTS 无法充分刻画奖励模型在 RL 中的有效性。</li>
</ul>
<p>为此，论文提出并验证：</p>
<ol>
<li>除 TTS 外，必须同时考察 <strong>判别能力（AUC）</strong> 与 <strong>置信度校准（ECE）</strong>；</li>
<li>通过大规模控制实验，系统分析训练数据规模、正负样本比例、策略混合、数据源组合及上下文长度对三项指标的影响；</li>
<li>基于实验结论训练出 <strong>SWE-RM</strong>——一个 30B 总参数、3B 激活的 MoE 奖励模型，支持 256 k 上下文，在 SWE-Bench Verified 上取得新的开源 SOTA，并在 RL 训练中将执行型反馈基线提升 3 个百分点。</li>
</ol>
<p>综上，论文首次明确了“通用 SWE 奖励模型”应具备的三维评估标准，并给出了可复现的训练配方与实证结果。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 §2、附录 B 及表 6 中系统梳理了与“SWE 任务验证器”和“智能体强化学习反馈”相关的研究，可归纳为两条主线：</p>
<ol>
<li>SWE 验证器（执行型 vs. 无执行型）</li>
</ol>
<ul>
<li>执行型</li>
<li>Agentless（Xia et al., 2024）——为每条轨迹自动生成再现测试，按通过用例数重排序。</li>
<li>R2E-Gym（Jain et al., 2025）—— procedural 环境 + 单元测试筛选。</li>
<li>DeepSWE-EB（Luo et al., 2025）——在 R2E-Gym 执行器基础上扩展，当前 TTS 最强混合方案中的执行部分。</li>
<li>无执行型（早期仅关注 TTS）</li>
<li>SWE-Gym Verifier（Pan et al., 2025）——Qwen2.5-32B 为骨干，仅用 SWE-Gym 数据训练，32 k 上下文。</li>
<li>OpenHands Critic（OpenHands Team, 2025）——32 k 上下文，轨迹级打分，细节未公开。</li>
<li>R2E-Gym EF（Jain et al., 2025）——与执行型配套的无执行模块，32 k 上下文。</li>
<li>DeepSWE-EF（Luo et al., 2025）——改进版 R2E-Gym EF，76 k 上下文，仍只服务 TTS。</li>
</ul>
<p>上述工作均未系统讨论 AUC/ECE，也未用于 RL。</p>
<ol>
<li>智能体强化学习反馈</li>
</ol>
<ul>
<li>执行型反馈在 RL 中的使用</li>
<li>Qwen3-Coder（Qwen Team, 2025）、GLM-4.5（GLM-4.5 Team, 2025a）、MiniMax-M1/2（MiniMax, 2025）等最新代码智能体皆采用“fail-to-pass 单元测试”作为 0/1 奖励，类似数学任务中的规则度量（DeepSeek-AI, 2025）。</li>
<li>此类信号稀疏且受测试质量拖累，难以区分同成功/同失败轨迹，训练效率低。</li>
<li>无执行反馈用于 RL</li>
<li>本文首次将“无执行、连续、细粒度”奖励模型引入 SWE 智能体 RL，并证明其可提升样本效率与最终性能；此前无同类研究。</li>
</ul>
<p>综上，现有工作要么局限于执行型验证，要么仅把无执行模型当作 TTS 的打分器；本文首次统一了 TTS、AUC、ECE 三维评估，并将无执行奖励模型规模化用于 RL，填补了该交叉方向的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“问题剖析 → 指标补全 → 大规模消融 → 模型训练 → 双重验证”五步法，系统解决了“如何获得同时胜任 TTS 与 RL 的通用无执行奖励模型”难题。</p>
<ol>
<li>问题剖析</li>
</ol>
<ul>
<li>通过对照实验发现：两个 TTS 表现几乎一致的验证器，在 RL 中一个稳定提升、一个迅速崩溃。</li>
<li>理论拆解表明，TTS 仅关注“Top-1 是否正确”，忽视全局判别能力（AUC）与置信度校准（ECE），而这两者在策略梯度更新中直接决定梯度方向与方差。</li>
</ul>
<ol>
<li>指标补全</li>
</ol>
<ul>
<li>提出三维评估体系：</li>
<li>TTS——Top-1 选择准确率；</li>
<li>AUC——正负轨迹全分布排序质量；</li>
<li>ECE（Expected Calibration Error）——模型置信度与真实正确率的匹配度。</li>
<li>在附录 C 给出理论推导，量化三项指标分别对应 RL 的“极端 top 错误”“梯度反向”“系统偏差与方差膨胀”三种失效模式。</li>
</ul>
<ol>
<li>大规模消融实验<br>固定 Qwen3-30B-A3B 为底座，逐因素扫描：</li>
</ol>
<ul>
<li>数据规模：500 → 100 k，证实 ≥20 k 才能避免 OOD 误判，100 k 后收益递减。</li>
<li>正负比例：1:8 → 2:1，2:1 在 AUC/ECE/RM@32 全面最优且节省负样本。</li>
<li>上下文长度：32 k → 256 k，128 k 以上可评分率 &gt;99%，RM@32 持续提升。</li>
<li>策略混合：纯 on-policy 或 off-policy 均不如“混合采样”稳健。</li>
<li>数据源：以 SWE-rebench 高质量数据为主，再混入 SWE-smith、SWE-Gym 可进一步降低 ECE。</li>
</ul>
<ol>
<li>模型训练（SWE-RM）</li>
</ol>
<ul>
<li>采用上述最优配置：100 k 轨迹、2:1 正负比、混合策略、三源数据、256 k 上下文。</li>
<li>底座为 30B-MoE（3B 激活），生成式分类头输出 YES/NO，通过  r=exp(l<em>(rm yes))/(exp(l</em>(rm yes))+exp(l_(rm no)))  得到连续分数。</li>
<li>监督微调 1 epoch，全局 batch=128，AdamW+余弦衰减，4 节点 H100 约 20 小时完成。</li>
</ul>
<ol>
<li>双重验证</li>
</ol>
<ul>
<li>TTS 侧：在 SWE-Bench Verified 500 题上，用 32 条轨迹 Best-of-32 选择，<br>– Qwen3-Coder-Flash 51.6% → 62.0%<br>– Qwen3-Coder-Max 67.0% → 74.6%<br>均刷新开源模型 SOTA（图 1）。</li>
<li>RL 侧：以 SWE-RM 为连续奖励源，配合执行型信号组成混合奖励</li>
</ul>
<p>r= 1 + rm Score<em>(rm EF), &amp; resolve -0.5 + rm Score</em>(rm EF), &amp; unfinished 0 + rm Score_(rm EF), &amp; otherwise</p>
<p>在 SWE-Bench Verified 上相对纯执行型基线提升 3 个百分点（51.8% → 54.8%），并在多领域测试集一致领先（表 5）。</p>
<p>通过以上闭环，论文不仅给出了可复现的“训练配方”，也提供了理论兼实证支持的通用奖励模型 SWE-RM，首次证明无执行、高校准奖励信号在软件工程智能体 TTS 与 RL 双重场景下的规模化有效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“如何训练一个同时胜任 TTS 与 RL 的无执行奖励模型”共设计并完成了三大类、十余组实验，全部在 SWE-Bench Verified 500 题基准上展开，核心结果均给出均值±方差或显著性检验。具体实验一览如下（按出现顺序归纳）：</p>
<p>1 验证器“三维指标”必要性验证</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验</th>
<th>目的</th>
<th>设置</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1 TTS⇸RL 一致性检验</td>
<td>验证 TTS 是否足以挑选 RL 用奖励模型</td>
<td>训练仅 TTS 相近的 Verifier A/B；在 Qwen3-30B-A3B + OpenHands 上做 100 step RL</td>
<td>图 2：A 稳定上升，B 训练崩溃；同样 +4.5~4.7% TTS，AUC 差 0.095，ECE 差 3×</td>
</tr>
<tr>
<td>1.2 理论拆解</td>
<td>解释三项指标如何影响策略梯度</td>
<td>给出 TTS/AUC/ECE 与梯度偏差/方差的解析式</td>
<td>附录 C：TTS 仅影响 top-1 权重；AUC 决定反向梯度比例；ECE 引入系统偏置与方差膨胀</td>
</tr>
</tbody>
</table>
</div>
<p>2 奖励模型训练因素大规模消融</p>
<p>底座均用 Qwen3-30B-A3B，评估指标统一报告 AUC、ECE、RM@32（Best-of-32 解析率）。</p>
<p>2.1 数据规模</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>训练样本</th>
<th>500</th>
<th>2 k</th>
<th>5 k</th>
<th>25 k</th>
<th>100 k</th>
</tr>
</thead>
<tbody>
<tr>
<td>RM@32</td>
<td>55.8±0.6</td>
<td>58.4±0.4</td>
<td>59.2±0.5</td>
<td>61.6±0.3</td>
<td>62.0±0.2</td>
</tr>
<tr>
<td>ECE</td>
<td>0.481</td>
<td>0.232</td>
<td>0.158</td>
<td>0.137</td>
<td>0.067</td>
</tr>
<tr>
<td>→ 图 6 右：100 k 样本下 resolved/unresolved 分数分布分离度最好。</td>
</tr>
</tbody>
</table>
</div>
<p>2.2 正负比例（固定 20 k 样本）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>比例</th>
<th>1:8</th>
<th>1:4</th>
<th>1:2</th>
<th>1:1</th>
<th>2:1</th>
</tr>
</thead>
<tbody>
<tr>
<td>AUC</td>
<td>0.778</td>
<td>0.789</td>
<td>0.789</td>
<td>0.782</td>
<td>0.805</td>
</tr>
<tr>
<td>ECE ↓</td>
<td>0.349</td>
<td>0.185</td>
<td>0.235</td>
<td>0.132</td>
<td>0.080</td>
</tr>
<tr>
<td>RM@32</td>
<td>60.2</td>
<td>61.6</td>
<td>61.0</td>
<td>60.8</td>
<td>62.0</td>
</tr>
</tbody>
</table>
</div>
<p>2.3 上下文长度</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>窗口</th>
<th>16 k</th>
<th>32 k</th>
<th>64 k</th>
<th>128 k</th>
<th>256 k</th>
</tr>
</thead>
<tbody>
<tr>
<td>可评分率</td>
<td>0.5 %</td>
<td>12.5 %</td>
<td>88.3 %</td>
<td>99.5 %</td>
<td>100 %</td>
</tr>
<tr>
<td>RM@32</td>
<td>66.8</td>
<td>67.4</td>
<td>70.6</td>
<td>73.0</td>
<td>74.4</td>
</tr>
</tbody>
</table>
</div>
<p>2.4 策略混合（固定 20 k）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据来源</th>
<th>纯 On-policy</th>
<th>纯 Off-policy</th>
<th>Mix-policy</th>
</tr>
</thead>
<tbody>
<tr>
<td>AUC</td>
<td>0.785</td>
<td>0.778</td>
<td>0.804</td>
</tr>
<tr>
<td>ECE ↓</td>
<td>0.148</td>
<td>0.113</td>
<td>0.033</td>
</tr>
</tbody>
</table>
</div>
<p>2.5 数据源组合（固定 20 k）</p>
<p>单源最佳为 SWE-rebench；三源混合（rebench+smith+gym）在保持 AUC 同时 ECE 最低（0.033→0.067），RM@32 达 74.6。</p>
<p>3 SWE-RM 效果验证</p>
<p>3.1 TTS 对比实验</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>验证器</th>
<th>类型</th>
<th>Qwen3-Flash RM@32</th>
<th>Qwen3-Max RM@32</th>
<th>ECE ↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>Agentless</td>
<td>执行</td>
<td>52.6 %</td>
<td>65.0 %</td>
<td>—</td>
</tr>
<tr>
<td>SWE-Gym</td>
<td>无执行</td>
<td>51.2 %</td>
<td>65.4 %</td>
<td>0.223</td>
</tr>
<tr>
<td>DeepSWE-EF</td>
<td>无执行</td>
<td>53.2 %</td>
<td>66.2 %</td>
<td>0.139</td>
</tr>
<tr>
<td>SWE-RM</td>
<td>无执行</td>
<td>62.0 %</td>
<td>74.6 %</td>
<td>0.047</td>
</tr>
</tbody>
</table>
</div>
<p>3.2 RL 训练实验</p>
<ul>
<li>模型：Qwen3-30B-A3B 经 SFT 预热 → 100 step RL</li>
<li>奖励设置：Hybrid / 纯执行 / 纯无执行 / 弱校准无执行</li>
<li>结果（SWE-Bench Verified pass@1，图 7 左）<br>– 纯执行：51.8 %<br>– 纯无执行：53.2 %<br>– 弱校准 RM：44.7 %（训练后期崩溃）<br>– <strong>Hybrid（SWE-RM + 执行）</strong>：<strong>54.8 %</strong>（+3.0 abs）</li>
<li>训练曲线：Hybrid 收敛最快，平均奖励平滑上升（图 7 右）。</li>
</ul>
<p>3.3 跨域泛化</p>
<p>额外 4 个基准（表 5）均呈现 Hybrid &gt; 纯执行 ≈ 纯无执行 &gt;&gt; 弱校准，证明校准重要性可迁移到多语言、终端命令等场景。</p>
<p>4 上下文与部署代价实测</p>
<ul>
<li>256 k 模型相比 32 k 仅增加 ≈2× GPU 显存；因输出仅 1 token，延迟几乎相同（附录 D.4）。</li>
<li>在 2×A100 上即可实现高效推理。</li>
</ul>
<p>综上，论文通过“必要性验证→因素消融→最终模型→TTS/RL 双重对比→跨域与系统代价”共二十余组实验，完整支撑了 SWE-RM 的训练配方与性能声明。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“数据与标注”“模型侧”“训练策略”“评估体系”“应用场景”五大类，供后续研究参考。</p>
<p>1 数据与标注</p>
<ul>
<li><p><strong>自动去噪与权重学习</strong><br>现有 100 k 轨迹仍依赖 fail-to-pass 测试做硬标签，可引入置信度加权或迭代重标注，降低错误标签对 RM 与 RL 的误导。</p>
</li>
<li><p><strong>多任务监督信号</strong><br>除“resolve/unresolved”外，可同步预测补丁复杂度、运行时间、代码风格违规等，构建多目标奖励或辅助任务，提升样本效率。</p>
</li>
<li><p><strong>人类偏好与对比数据</strong><br>收集工程师对“同正确但不同品质”补丁的偏好排序，训练 Bradley-Terry 或 RankNet 形式，进一步改善细粒度排序与校准。</p>
</li>
</ul>
<p>2 模型侧</p>
<ul>
<li><p><strong>架构对比</strong><br>SWE-RM 采用 30B-MoE(3B 激活)。可系统比较 Dense、MoE、LoRA/DoRA、RNN-based 记忆模型在长上下文、校准、推理成本上的权衡。</p>
</li>
<li><p><strong>早-中期融合</strong><br>当前仅在完整轨迹后打分。若在每个 turn 或 diff hunk 后提供中间奖励，需研究稀疏-密集奖励混合、信用分配与梯度截断策略。</p>
</li>
<li><p><strong>可解释性</strong><br>引入注意力热图或 post-hoc 解释器，验证模型是否关注关键 diff、报错栈或测试日志，帮助诊断错误高分案例。</p>
</li>
</ul>
<p>3 训练策略</p>
<ul>
<li><p><strong>在线/离线混合 RL</strong><br>目前 RM 固定后做 RL。可探索迭代式 RM-Policy 共同提升（如 RLHF 中的 reward-model &amp; policy 交替更新），并研究非平稳奖励下的收敛保证。</p>
</li>
<li><p><strong>课程与难度采样</strong><br>按问题长度、测试覆盖率或错误类型动态调整采样概率，避免过易/过难样本主导训练，提高样本复杂度效率。</p>
</li>
<li><p><strong>多智能体对抗环境</strong><br>引入“测试生成智能体”与“修复智能体”互博，前者生成更难 fail-to-pass 测试，后者学习通过，形成自动课程与可扩展 reward 信号。</p>
</li>
</ul>
<p>4 评估体系</p>
<ul>
<li><p><strong>细粒度错误类型诊断</strong><br>将 unresolved 细分为“语法错误”“测试不过”“引入新 bug”“部分修复”等，计算按类型的 AUC-ECE，指导针对性改进。</p>
</li>
<li><p><strong>分布外与对抗鲁棒性</strong><br>构造代码风格、项目领域、语言版本不同的 OOD 集，或采用对抗 patch（语义保持但迷惑模型），测试 RM 的校准漂移与鲁棒性。</p>
</li>
<li><p><strong>在线校准监控</strong><br>部署后持续收集预测-真实对比，动态计算 Streaming-ECE，触发重校准或再训练，保证生产环境奖励信号长期可信。</p>
</li>
</ul>
<p>5 应用场景</p>
<ul>
<li><p><strong>其他代码任务迁移</strong><br>将 SWE-RM 迁移至代码审查推荐、漏洞检测、自动重构接受度评估等任务，验证通用性并构建统一代码偏好模型。</p>
</li>
<li><p><strong>多模态扩展</strong><br>若智能体同时操作文档、日志、执行轨迹图像（如 GUI 测试），可引入视觉-语言融合编码器，研究跨模态奖励建模。</p>
</li>
<li><p><strong>边缘与低资源部署</strong><br>探索大 RM 蒸馏至 1-3B 小模型、量化与动态 offloading，满足私有化或边缘设备上的实时奖励计算需求。</p>
</li>
</ul>
<p>6 理论与安全</p>
<ul>
<li><p><strong>校准误差与 RL 收敛界</strong><br>在马尔可夫决策过程框架下，给出 ECE-梯度方差与收敛速度、样本复杂度的显式关系，为“需要多好的校准”提供理论下限。</p>
</li>
<li><p><strong>奖励攻击与防御</strong><br>研究攻击者通过构造特殊 patch 使 RM 给出高置信高分的攻击方式，提出正则化、对抗训练或检测器进行防御。</p>
</li>
</ul>
<p>综上，从“更干净的数据”到“更聪明的训练”再到“更严苛的评估”，SWE-RM 为后续研究提供了基线与出发点，上述任一方向均可形成新的突破。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心内容可概括为“一个发现、一套指标、一组实验、一个模型、双重验证”：</p>
<ol>
<li><p><strong>一个发现</strong><br>仅看测试时扩展（TTS）无法选出适合强化学习（RL）的奖励模型——TTS 相近的两个验证器在 RL 中表现天差地别。</p>
</li>
<li><p><strong>一套指标</strong><br>提出“TTS + AUC + 校准（ECE）”三维评估：</p>
</li>
</ol>
<ul>
<li>TTS 管 Top-1 选择</li>
<li>AUC 管全局排序</li>
<li>ECE 管置信度可靠<br>理论证明三者分别对应 RL 梯度更新中的“极端 top 错误”“反向梯度比例”与“系统偏差/方差”。</li>
</ul>
<ol>
<li><strong>一组实验</strong><br>在 Qwen3-30B-A3B 底座上系统扫描：</li>
</ol>
<ul>
<li>数据规模 0.5 k→100 k</li>
<li>正负比例 1:8→2:1</li>
<li>上下文 32 k→256 k</li>
<li>策略与数据源组合<br>得出“100 k 样本、2:1 正负、256 k 上下文、混合策略+多源数据”最优配方。</li>
</ul>
<ol>
<li><strong>一个模型——SWE-RM</strong><br>按配方训练的 30B-MoE（3B 激活）无执行奖励模型，支持 256 k 上下文，在 SWE-Bench Verified 上：</li>
</ol>
<ul>
<li>TTS 最佳：Flash 51.6→62.0 %，Max 67.0→74.6 %（开源 SOTA）</li>
<li>AUC/ECE 全面领先现有执行/无执行验证器</li>
</ul>
<ol>
<li><strong>双重验证</strong></li>
</ol>
<ul>
<li>TTS：Best-of-32 刷新开源纪录</li>
<li>RL：混合奖励（SWE-RM+执行）相较纯执行基线 pass@1 提升 3 个百分点，训练更快更稳，且跨域泛化一致领先。</li>
</ul>
<p>综上，论文首次阐明并验证了“通用 SWE 奖励模型”需同时满足高 TTS、高 AUC、低 ECE，给出可复现训练方案，并通过 SWE-RM 在 TTS 与 RL 两端均取得新最佳表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SWE-RM%3A%20Execution-free%20Feedback%20For%20Software%20Engineering%20Agents，链接是https://arxiv.org/pdf/2512.21919，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21919。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: KaShun Shum,Binyuan Hui,Jiawei Chen,Lei Zhang,X. W.,Jiaxi Yang,Yuzhen Huang,Junyang Lin,Junxian He</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21919.pdf">https://arxiv.org/pdf/2512.21919.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21919">https://arxiv.org/abs/2512.21919</a></p>
<p><strong>Arxiv ID</strong>: 2512.21919</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21919">https://papers.cool/arxiv/2512.21919</a></p>
<p><strong>Published</strong>: 2025-12-26T08:26:18Z</p>
<p><strong>Updated</strong>: 2025-12-26T08:26:18.000Z</p>
<hr>
<h3 id="11-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning"><a href="#11-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning" class="headerlink" title="11. SVBench: Evaluation of Video Generation Models on Social Reasoning"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21507">SVBench: Evaluation of Video Generation Models on Social Reasoning</a></h3><p>Recent text-to-video generation models exhibit remarkable progress in visual realism, motion fidelity, and text-video alignment, yet they remain fundamentally limited in their ability to generate socially coherent behavior. Unlike humans, who effortlessly infer intentions, beliefs, emotions, and social norms from brief visual cues, current models tend to render literal scenes without capturing the underlying causal or psychological logic. To systematically evaluate this gap, we introduce the first benchmark for social reasoning in video generation. Grounded in findings from developmental and social psychology, our benchmark organizes thirty classic social cognition paradigms into seven core dimensions, including mental-state inference, goal-directed action, joint attention, social coordination, prosocial behavior, social norms, and multi-agent strategy. To operationalize these paradigms, we develop a fully training-free agent-based pipeline that (i) distills the reasoning mechanism of each experiment, (ii) synthesizes diverse video-ready scenarios, (iii) enforces conceptual neutrality and difficulty control through cue-based critique, and (iv) evaluates generated videos using a high-capacity VLM judge across five interpretable dimensions of social reasoning. Using this framework, we conduct the first large-scale study across seven state-of-the-art video generation systems. Our results reveal substantial performance gaps: while modern models excel in surface-level plausibility, they systematically fail in intention recognition, belief reasoning, joint attention, and prosocial inference.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>近期的文本到视频生成模型在视觉真实感、动作保真度以及文本与视频的对齐方面表现出显著进步，但它们在生成社会一致行为的能力上仍然存在根本性限制。与人类不同，人类能够轻而易举地从简短的视觉线索中推断意图、信念、情绪和社会规范，而当前模型往往仅呈现字面场景，却未能捕捉其背后的因果或心理逻辑。为了系统地评估这一差距，我们提出了首个用于视频生成社会推理的基准。基于发展心理学和社会心理学的研究结果，我们的基准将三十个经典社会认知范式组织为七个核心维度，包括心理状态推理、目标导向行为、联合注意、社会协调、利他行为、社会规范和多主体策略。为了将这些范式具体化，我们开发了一个完全无需训练的基于代理的流程，该流程可(i)提取每个实验的推理机制，(ii)合成多样化的视频场景，(iii)通过基于线索的评价确保概念中性和难度控制，(iv)使用高容量视觉语言模型评审器对生成的视频在五个可解释的社会推理维度上进行评估。基于该框架，我们进行了首个针对七种最先进视频生成系统的大规模研究。结果显示出显著的性能差距：虽然现代模型在表面可信度上表现出色，但在意图识别、信念推理、联合注意和利他推断方面系统性失败。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在填补当前文本到视频生成模型在“社会推理”能力上的系统性空白。现有评估仅关注视觉真实感、运动平滑度、物理合理性等低层指标，而忽视了人类观察者最擅长的因果-心理维度：能否从短暂画面中推断出角色的意图、信念、情绪与社会规范，并据此生成合乎社会逻辑的行为序列。为此，作者提出首个专门评测视频生成模型社会推理能力的基准 SVBench，将发展心理学与社会心理学中 30 项经典实验范式转化为 7 大认知维度（心智状态推断、目标导向行动、共同注意、社会协调、亲社会行为、社会规范、多主体策略），并设计无需训练的智能体流水线，实现难度可控的提示构造与自动评判，从而系统诊断现有模型能否“生成”而非仅仅“识别”具备社会一致性的多主体互动。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线：</p>
<ol>
<li>视频生成评测基准</li>
</ol>
<ul>
<li>早期：FVD、人类偏好分数等聚焦感知保真度。</li>
<li>近期轴式诊断：VBench 系列、EvalCrafter、T2V-CompBench 将质量分解为对象一致性、运动平滑、美学、物理合理性等维度。</li>
<li>物理专项：PhyCoBench 用光流预测检验物理连贯性；Morpheus 以真实物理实验与守恒律探测生成视频是否遵守力学定律。<br>这些工作均未涉及多主体意图、规范、信念等社会推理层面。</li>
</ul>
<ol>
<li>社会推理在 AI 中的研究</li>
</ol>
<ul>
<li>文本领域：ToM 基准（Kosinski, Ullman, OpenToM 等）揭示大模型在零阶/一阶信念尚可，但高阶或反事实情境下可靠性骤降。</li>
<li>视频理解领域：Social-IQ、R³-VQA 等通过问答形式评估模型对现有人类拍摄视频中的情绪、社交事件、因果链的“识别”能力。<br>尚无工作评测生成模型是否能“从头合成”符合社会-因果逻辑的多主体互动，SVBench 填补了这一空白。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出一条“无需训练”的四智能体流水线，将发展心理学 30 项经典实验转化为可评测、难度可控的短视频任务，并用 VLM 法官自动打分，从而系统诊断模型能否生成具备社会一致性的行为。核心步骤如下：</p>
<ol>
<li><p>实验理解智能体<br>对每则心理实验输出四元组：现象描述 → 关键概念 → 测试点 → 预期行为，确保后续生成始终锚定认知机制而非表面细节。</p>
</li>
<li><p>提示合成智能体<br>把抽象概念转写为 5–10 s 视频可用的动作描述，遵循四条原则：</p>
</li>
</ol>
<ul>
<li>仅描述可观测行为，禁止出现“意识到”“感到”等心理动词；</li>
<li>时序可行，单镜头完成；</li>
<li>用具体身份/物体替代占位符；</li>
<li>行为与预期结果分离，防止泄露答案。</li>
</ul>
<ol>
<li>批判智能体（Critic）<br>执行三项检测：</li>
</ol>
<ul>
<li>剔除解释性词汇；</li>
<li>比对测试点检测 ground-truth 泄露并返回修改指令；</li>
<li>通过增删 gaze、手势、物体 affordance 等线索生成 Easy/Medium/Hard 三档难度，实现细粒度推理压力控制。<br>若未通过，则反馈给合成智能体迭代重写，直至满足中立性、无泄露、难度要求。</li>
</ul>
<ol>
<li>评价智能体（EVA）<br>采用高容量 VLM（Gemini-2.5-Pro）作为法官，对每支生成视频回答五个二元问题：</li>
</ol>
<ul>
<li>D1 核心范式是否复现</li>
<li>D2 是否忠实于提示中的实体与场景</li>
<li>D3 行为是否因果-社会连贯</li>
<li>D4 关键社交线索（注视、手势等）是否有效呈现</li>
<li>D5 视频是否具备基本视觉合理性<br>总分  S<em>(overall)=(1) / (5)∑</em>(k=1)^(5) D_k ，实现失败模式解耦与大规模无人工标注评估。</li>
</ul>
<p>通过上述流水线，作者将 30 项实验精简为 15 则“短视频可行”任务，首次对 8 个主流生成模型进行系统实验，量化其在意图识别、信念推理、共同注意、亲社会行为等维度的缺陷，从而揭示“感知真实”与“社会合理”之间的显著鸿沟。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“短视频可行”的 15 项社会推理任务，对 8 个主流文本到视频生成模型进行系统实验，并验证所提流水线各组件的有效性。具体实验如下：</p>
<ol>
<li>主实验：15 任务 × 8 模型大规模自动评测</li>
</ol>
<ul>
<li>模型：4 个闭源（Sora2-Pro、Veo-3.1、Kling2.5-Turbo、Hailuo02-S）+ 3 个开源（HunyuanVideo、LTX-1.0、LongCat-Video）。</li>
<li>指标：采用 EVA（Gemini-2.5-Pro）在 5 个二元维度打分，计算整体通过率。</li>
<li>结果：<br>– Sora2-Pro 79.6 %、Veo-3.1 72.4 % 显著领先；<br>– Hailuo02-S 56.4 %、Kling2.5-Turbo 52.2 % 中等；<br>– 开源模型最高仅 39.2 %，在社会协调、信念推理等维度普遍失败。</li>
</ul>
<ol>
<li>难度控制消融实验</li>
</ol>
<ul>
<li>对 4 个闭源模型分别运行 Easy / Medium / Hard 三档提示。</li>
<li>结果：<br>– 弱模型呈单调下降，验证线索越少推理越难；<br>– 强模型在 Medium/Hard 反而略高，表明其可凭少量线索推断意图，过多冗余线索易引发冲突或违背 Prompt Faithfulness。</li>
</ul>
<ol>
<li>流水线质量人工验证</li>
</ol>
<ul>
<li>招募人类评委对三种提示版本进行“是否准确表达心理构造且无答案泄露”二分类判断：<br>– 无理解阶段 66.8 % → 加入理解+合成 75.9 % → 完整批判循环 86.9 %。</li>
<li>证明“实验理解”与“批判修正”两阶段显著提升提示理论忠实度。</li>
</ul>
<ol>
<li>自动评价 vs. 人类一致性验证</li>
</ol>
<ul>
<li>随机抽取 160 支视频（每模型 20 支），用同一五维 rubric 进行人工打分。</li>
<li>趋势高度一致：<br>– 人类对 D2/D4/D5 更宽容，重视可解读性；<br>– 对 D1/D3 更严格，要求因果-社会逻辑完整。</li>
<li>相关系数与秩相关均表明 VLM 法官可替代人工进行大规模评估。</li>
</ul>
<ol>
<li>定性案例验证</li>
</ol>
<ul>
<li>展示 gaze-following 失败案例与 instrumental-helping 成功案例，说明 EVA 能精确定位“视觉合理但实验范式未被执行”的致命缺陷，进一步确认自动评判的可解释性与可靠性。</li>
</ul>
<p>综上，论文通过主实验量化模型差距，辅以难度、流水线、人工对齐与定性案例四项验证，全面展示 SVBench 对社会推理生成能力的诊断价值。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可继续推进，括号内给出可行切入点或初步思路：</p>
<ul>
<li><strong>长时程社会推理</strong></li>
<li>将剩余 15 项“多步欺骗、延迟满足、协作规划”等长视频任务落地，构建 30 s–2 min 生成与评测协议（引入滑动窗口 VLM、关键帧摘要链、分层因果图）。</li>
<li><strong>高阶心智理论</strong></li>
<li>在二阶信念、竞争式欺骗、反事实帮助等场景上扩展提示集，检验模型是否理解“A 相信 B 相信 C 不知道”的嵌套结构（可复用现有 Critic 的线索剥离+难度调制框架）。</li>
<li><strong>多模态条件控制</strong></li>
<li>研究“语音语调+面部表情+手势”同步生成对社交含义的影响；引入音频-视频联合扩散或潜空间对齐损失，验证同步性是否提升 D3、D4 分数。</li>
<li><strong>可解释社交先验注入</strong></li>
<li>在生成阶段显式嵌入因果图、社交常识库（如 ATOMIC、GLUCOSE）或 agent-based 规划模块，对比“黑盒”端到端与“白盒”先验注入的样本效率与错误模式。</li>
<li><strong>对抗-鲁棒性评测</strong></li>
<li>设计“线索冲突”提示（如 gaze 指向 A、手势指向 B），测量模型对矛盾信号的鲁棒性；或引入时空扰动（遮挡、光照突变）观察社交逻辑是否崩溃。</li>
<li><strong>跨文化与社会规范迁移</strong></li>
<li>收集不同文化背景下的个人空间、礼仪手势数据，构建跨文化变体，检验模型是否默认单一“西方”先验并出现规范误用。</li>
<li><strong>人类-模型交互式微调</strong></li>
<li>让模型生成的中间帧实时接受人类社交合理性反馈（RLHF 或 IRL），形成“社交-对齐”微调集，观察在 SVBench 上是否持续改进而不发生视觉质量下降。</li>
<li><strong>细粒度失败归因工具</strong></li>
<li>在 EVA 基础上训练“子维度诊断头”，输出连续归因热图（帧级、对象级），精确定位哪几帧导致 D1/D3 得分为 0，为迭代式提示重写或模型修补提供可解释依据。</li>
<li><strong>社交-物理混合推理</strong></li>
<li>设计需要同时满足牛顿力学与社会规范的复合任务（如“帮助他人搬重物但不阻挡行人流动”），联合 SVBench 与 PhyCoBench/Morpheus 指标，检验模型能否在双重约束下生成一致视频。</li>
<li><strong>开源社区挑战赛</strong></li>
<li>公开长/短任务提示与自动评测 API，举办周期性挑战，收集社区提交的生成样本与改进策略，持续更新难度曲线与维度权重，推动领域快速迭代。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SVBench 核心内容速览</strong></p>
<ol>
<li><p>问题<br>现有文本到视频生成模型仅被评测“看得见的”质量（清晰度、运动、物理），无人关注“看不见的”社会因果：能否生成符合意图、信念、规范与多主体策略的连贯行为。</p>
</li>
<li><p>方案<br>提出首个社会推理视频生成基准 SVBench，含 30 项经发展心理学验证的实验，覆盖 7 大认知维度；仅取 15 项可在 5–10 s 内完成的“短视频可行”任务进行主评测。</p>
</li>
<li><p>流水线（训练无关）</p>
</li>
</ol>
<ul>
<li><strong>Experiment Understanding Agent</strong> – 提炼实验因果链与预期行为</li>
<li><strong>Prompt Synthesis Agent</strong> – 生成仅描述可观测动作的视频提示</li>
<li><strong>Critic Agent</strong> – 删除心理动词、检测答案泄露、按线索密度输出 Easy/Medium/Hard 三档难度</li>
<li><strong>Evaluation Agent (EVA)</strong> – 用 VLM 对每支视频回答 5 个二元问题，平均得分为最终社交推理分</li>
</ul>
<ol>
<li>主实验结果（8 模型）</li>
</ol>
<ul>
<li>闭源顶尖：Sora2-Pro 79.6 %、Veo-3.1 72.4 %</li>
<li>中等闭源：Hailuo02-S 56.4 %、Kling2.5-Turbo 52.2 %</li>
<li>开源最高仅 39.2 %；所有模型在信念推理、多主体协调上系统性失败</li>
</ul>
<ol>
<li>验证</li>
</ol>
<ul>
<li>人工评 prompt 质量：完整流水线达 86.9 % 通过率</li>
<li>难度控制有效：弱模型随线索减少性能单调下降；强模型在 Hard 档仍保持高位</li>
<li>VLM 法官与人类趋势一致，但对逻辑错误更敏感，可替代大规模人工标注</li>
</ul>
<ol>
<li>结论<br>SVBench 首次把“社会一致性”纳入视频生成评测，揭示当前模型“画面逼真”与“行为合理”之间的显著鸿沟，为后续长时程、高阶心智、跨文化等研究提供可扩展的基准框架。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SVBench%3A%20Evaluation%20of%20Video%20Generation%20Models%20on%20Social%20Reasoning，链接是https://arxiv.org/pdf/2512.21507，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21507。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wenshuo Peng,Gongxuan Wang,Tianmeng Yang,Chuanhao Li,Xiaojie Xu,Hui He,Kaipeng Zhang</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21507.pdf">https://arxiv.org/pdf/2512.21507.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21507">https://arxiv.org/abs/2512.21507</a></p>
<p><strong>Arxiv ID</strong>: 2512.21507</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21507">https://papers.cool/arxiv/2512.21507</a></p>
<p><strong>Published</strong>: 2025-12-25T04:44:59Z</p>
<p><strong>Updated</strong>: 2025-12-25T04:44:59.000Z</p>
<hr>
<h3 id="12-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers"><a href="#12-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers" class="headerlink" title="12. SlideTailor: Personalized Presentation Slide Generation for Scientific Papers"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.20292">SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</a></h3><p>Automatic presentation slide generation can greatly streamline content creation. However, since preferences of each user may vary, existing under-specified formulations often lead to suboptimal results that fail to align with individual user needs. We introduce a novel task that conditions paper-to-slides generation on user-specified preferences. We propose a human behavior-inspired agentic framework, SlideTailor, that progressively generates editable slides in a user-aligned manner. Instead of requiring users to write their preferences in detailed textual form, our system only asks for a paper-slides example pair and a visual template - natural and easy-to-provide artifacts that implicitly encode rich user preferences across content and visual style. Despite the implicit and unlabeled nature of these inputs, our framework effectively distills and generalizes the preferences to guide customized slide generation. We also introduce a novel chain-of-speech mechanism to align slide content with planned oral narration. Such a design significantly enhances the quality of generated slides and enables downstream applications like video presentations. To support this new task, we construct a benchmark dataset that captures diverse user preferences, with carefully designed interpretable metrics for robust evaluation. Extensive experiments demonstrate the effectiveness of our framework.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>自动化演示幻灯片生成可以大大简化内容创作。然而，由于每个用户的偏好可能不同，现有的欠具体化方法往往导致结果不理想，无法满足个别用户的需求。我们提出了一项新任务，该任务将论文到幻灯片的生成过程与用户指定的偏好相结合。我们提出了一个受人类行为启发的代理框架——SlideTailor，能够以用户对齐的方式逐步生成可编辑的幻灯片。我们的系统无需用户以详细文本形式书写偏好，仅需要提供一对论文-幻灯片示例及一个视觉模板，这些是自然且易于提供的材料，可隐式地编码内容和视觉风格方面丰富的用户偏好。尽管这些输入是隐式且未标注的，我们的框架仍能有效提炼并泛化用户偏好，从而指导定制化的幻灯片生成。我们还引入了一种新的演讲链机制，将幻灯片内容与预定的口头讲解对齐。这种设计显著提升了生成幻灯片的质量，并支持如视频演示等下游应用。为了支持这一新任务，我们构建了一个基准数据集，涵盖多样的用户偏好，并设计了可解释的评估指标以进行稳健评测。大量实验表明了我们框架的有效性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“学术文献→演示幻灯片”自动生成过程中长期被忽视的用户个性化需求问题。现有方法普遍将任务形式化为“单篇论文→一套幻灯片”的单一生成范式，未显式建模不同用户在内容取舍、叙事节奏、视觉风格等方面的主观偏好，导致输出结果常常与个体期望不符。</p>
<p>为此，作者提出“偏好引导的论文到幻灯片生成”新任务，仅需用户提供：</p>
<ol>
<li>一篇“论文–幻灯片”样例对（隐式编码内容偏好）</li>
<li>一个 .pptx 视觉模板（隐式编码美学偏好）</li>
</ol>
<p>即可让系统自动蒸馏并泛化这些无标注、高阶、跨模态偏好，生成可编辑且与用户风格高度对齐的幻灯片，并配套演讲稿以支持后续视频演示等下游应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两条主线，并指出其局限：</p>
<ol>
<li>文档到幻灯片生成（Document-to-Slides Generation）</li>
</ol>
<ul>
<li>早期工作把任务视为纯文本摘要，忽略视觉版式与多模态特性。</li>
<li>近期方法（Fu et al. 2022; Zheng et al. 2025 等）引入布局与图像，但仍采用“一刀切”式转换，不支持个性化。</li>
<li>PersonaAware-D2S（Mondal et al. 2024b）首次考虑“角色”偏好，但仅支持四组固定离散标签（专家/非专家 × 长/短），无法表达细粒度、开放式用户风格。</li>
<li>PPTAgent（Zheng et al. 2025）允许用户上传模板，却仅关注版式复现，对内容结构偏好无建模。</li>
</ul>
<ol>
<li>条件摘要（Conditional Summarization）</li>
</ol>
<ul>
<li>研究聚焦于“以查询、主题、时间线、用户偏好等为条件的摘要”。</li>
<li>与用户偏好相关的子方向多集中在商品评论个性化摘要或风格/长度可控摘要，尚未扩展到学术论文→幻灯片场景。</li>
<li>最接近的 Mondal et al. (2024b) 同样受限于四元组离散偏好，难以覆盖真实演示情境中丰富的内容组织与视觉审美需求。</li>
</ul>
<p>综上，现有文献要么忽略用户差异，要么只能在极少数预设维度上微调，无法对“内容结构+视觉风格”双空间中的开放式、隐式偏好进行联合建模与泛化，这正是 SlideTailor 试图填补的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>SlideTailor</strong>，一个受人类演示制作行为启发的 <strong>渐进式智能体框架</strong>，把“隐式、无标签、双空间偏好”拆解为三个阶段逐步处理，核心思路是 <strong>先蒸馏偏好 → 再规划内容与演讲 → 最后实例化可编辑幻灯片</strong>。</p>
<p>1. 隐式偏好蒸馏（Implicit Preference Distillation）</p>
<ul>
<li><p><strong>内容偏好</strong>：将用户给出的“论文–幻灯片”样例对  (D<em>(ref), S</em>(ref))  视为<strong>潜在映射函数</strong><br>f<em>(content): D</em>(ref) to S_(ref)<br>用 LLM 反向推断该函数在“叙事流、章节取舍、详略度、格式（ bullet / 短语 / 图重）”上的规律，输出结构化、符号化内容偏好  P_C 。</p>
</li>
<li><p><strong>美学偏好</strong>：对模板 .pptx 逐页运行 VLM + 元数据解析，提取“母版类型、占位符语义、配色、字体、图标”等布局模式，得到美学偏好模式  P_A 。</p>
</li>
<li>合并为统一偏好档案  P = P_C ∪ P_A ，供后续阶段作为<strong>显式条件</strong>。</li>
</ul>
<p>2. 偏好引导的幻灯片规划（Preference-Guided Slide Planning）</p>
<p>三个 LLM 智能体顺序执行：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>智能体</th>
<th>功能</th>
<th>关键机制</th>
</tr>
</thead>
<tbody>
<tr>
<td>Paper Reorganizer</td>
<td>按  P_C  对目标论文做面向演讲的重组（调整详略、顺序、删减）</td>
<td>条件摘要，非通用摘要</td>
</tr>
<tr>
<td>Slide Outline Designer</td>
<td>将重组后内容切分为逐 slide 信息单元，并为每页同步撰写演讲稿</td>
<td>提出 Chain-of-Speech 机制，保证“视觉点”与“口头叙述”一一对应，提升连贯性并可后续直接 TTS</td>
</tr>
<tr>
<td>Template Selector</td>
<td>基于  P_A  为每页挑选最匹配母版</td>
<td>逐页语义-布局对齐，避免“一套模板走天下”</td>
</tr>
</tbody>
</table>
</div>
<p>3. 幻灯片实例化（Slide Realization）</p>
<ul>
<li><strong>Layout-Aware Editor</strong>：把大纲中的标题、文本、图像精确映射到已选母版的对应占位符，自动完成替换/增删。</li>
<li><strong>Code Agent</strong>：生成 python-pptx 可执行代码，原地编辑 .pptx，<strong>保留母版与主题</strong>，输出完全可再编辑文件。</li>
</ul>
<p>4. 下游扩展（Video Presentation）</p>
<p>因每页已有同步演讲稿，可直接：</p>
<ol>
<li>零样本声音克隆 → 合成用户音色语音</li>
<li>音频驱动说话头像 → 生成口型同步画面</li>
<li>用 FFmpeg 将头像叠加到幻灯片并合成视频，实现<strong>自动个性化演讲视频</strong>。</li>
</ol>
<p>总结</p>
<p>通过“<strong>先显式蒸馏隐偏好 → 再联合规划内容与演讲 → 最后模板级代码生成</strong>”，SlideTailor 在无需额外训练、无需用户写冗长提示的情况下，即可把任意论文按用户过往风格批量生成<strong>结构、视觉、叙述三高对齐</strong>的可编辑幻灯片，并天然支持视频化输出。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在自建 benchmark <strong>PSP</strong> 上进行了系统实验，从<strong>定量指标、人类评测、消融分析、成本与可视化</strong>四个角度验证 SlideTailor 的有效性。</p>
<p>1 实验设置</p>
<ul>
<li><strong>数据集</strong>：PSP 共 200 篇目标论文 × 50 篇样例论文-幻灯片对 × 10 个模板 → 10 万种组合；随机抽取 50 组作为测试集。</li>
<li><strong>对比基线</strong></li>
</ul>
<ol>
<li>ChatGPT-4o（网页上传多模态提示）</li>
<li>AutoPresent（文本→幻灯片，强制拼接样例文本）</li>
<li>PPTAgent（模板版式复现最强基线）</li>
</ol>
<ul>
<li><strong>统一约束</strong>：所有方法均生成 <strong>10 页</strong>幻灯片；零样本设定。</li>
<li>** backbone 对比**：SlideTailor 分别用 GPT-4.1 与开源 Qwen2.5-72B + Qwen2.5-VL-72B 实现，验证可迁移性。</li>
</ul>
<p>2 定量结果（自动指标）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>含义</th>
<th>SlideTailor(GPT-4.1)</th>
<th>最强基线 PPTAgent</th>
<th>相对提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>Overall</td>
<td>6 项指标平均</td>
<td>75.8</td>
<td>67.3</td>
<td>+8.5 pp</td>
</tr>
<tr>
<td>Preference-based</td>
<td>内容结构一致性</td>
<td>72.8</td>
<td>57.6</td>
<td>+15.2 pp</td>
</tr>
<tr>
<td>Aesthetic</td>
<td>视觉模板贴合</td>
<td>98.0</td>
<td>97.2</td>
<td>+0.8 pp</td>
</tr>
<tr>
<td>Content</td>
<td>信息准确丰富度</td>
<td>67.6</td>
<td>58.4</td>
<td>+9.2 pp</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>无一项基线 Overall 超过 80%，说明任务本身难度高；SlideTailor 在<strong>偏好对齐</strong>与<strong>通用质量</strong>双维度均领先。</p>
</blockquote>
<p>3 人类评测</p>
<ul>
<li><strong>4 名 AI 方向研究生</strong>独立完成 15 组双盲对比（vs PPTAgent），共 60 份评分。</li>
<li><strong>81.63 % 的案例人类更偏好 SlideTailor</strong>；平均 Pearson 相关 0.64，与自动指标趋势一致。</li>
</ul>
<p>4 消融实验（30 样本子集）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变体</th>
<th>Overall ↓</th>
<th>关键指标下降</th>
</tr>
</thead>
<tbody>
<tr>
<td>去掉内容偏好蒸馏</td>
<td>68.6 (-5.7)</td>
<td>Coverage -9.0 pp，Flow -11.5 pp</td>
</tr>
<tr>
<td>去掉 Chain-of-Speech</td>
<td>69.9 (-4.3)</td>
<td>Content 质量 -19.3 pp</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>验证：1) 隐式内容偏好建模是“对齐”核心；2) 演讲稿同步规划显著提升信息清晰度。</p>
</blockquote>
<p>5 成本分析</p>
<ul>
<li>平均 10 页 deck 成本：</li>
<li>GPT-4.1 版：$0.665</li>
<li>Qwen 开源版：$0.016（≈ 1/40）</li>
</ul>
<p>6 定性可视化</p>
<ul>
<li>官网附大量逐页对照图：</li>
<li>SlideTailor 能<strong>复刻样例的章节顺序、图-表插入节奏、模板配色与字体</strong>；</li>
<li>基线常出现“模板留白、占位符未替换、图像缺失、生成无关图”等问题。</li>
</ul>
<p>结论</p>
<p>实验覆盖<strong>自动指标-人类主观-组件消融-成本-可视化</strong>全链路，一致表明 SlideTailor 在<strong>偏好对齐</strong>与<strong>通用幻灯片质量</strong>上均优于现有最强基线，且可低成本迁移至开源模型。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><strong>领域泛化</strong>：当前 PSP 仅覆盖科研论文，可向商业报告、教材、广告等多元文档类型扩展，构建多领域偏好数据集。</li>
<li><strong>端到端训练</strong>：框架现为零样本推理，若收集大规模“论文-幻灯片-模板”三元组，可探索偏好引导的多模态预训练，提升蒸馏与生成一致性。</li>
<li><strong>细粒度偏好解耦</strong>：内容/美学仍可能残余耦合，可引入对比或解耦表征学习，实现“内容结构⇄视觉风格”任意组合插值。</li>
<li><strong>交互式修正</strong>：支持用户在幻灯片草稿上实时微调（增删模块、换母版），系统即时迭代，形成“人机协同”闭环。</li>
<li><strong>多模态评估协议</strong>：MLLM 评委存在自偏置与视觉细粒度缺失，可研究“跨模型评审+人类校准”的混合评估体系，或引入专用视觉-排版评测模型。</li>
<li><strong>演讲级韵律优化</strong>：Chain-of-Speech 仅生成文本，可进一步建模语速、停顿、重读，结合情感 TTS 与手势生成，实现更具表现力的虚拟讲者。</li>
<li><strong>多语言与跨文化适应</strong>：探索同一学术内容在不同语言/文化语境下的叙事节奏与视觉审美差异，实现本地化演示自动生成。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SlideTailor</strong> 提出“偏好引导的论文→幻灯片生成”新任务，解决现有方法忽视用户个性化内容结构与视觉风格的问题。<br>核心贡献：</p>
<ol>
<li><strong>任务与形式化</strong>：仅需“论文-幻灯片样例对 + .pptx 模板”即可隐式编码内容与美学偏好，无需冗长文本提示。</li>
<li><strong>框架</strong>：三阶段渐进式智能体</li>
</ol>
<ul>
<li>隐式偏好蒸馏 → 符号化内容 &amp; 美学模式</li>
<li>偏好引导规划 → 重组论文、Chain-of-Speech 同步演讲稿、逐页模板匹配</li>
<li>模板代码编辑 → 输出可编辑 .pptx</li>
</ul>
<ol>
<li><strong>基准与指标</strong>：发布 10 万种组合的 PSP 数据集，设计 6 项可解释自动指标（偏好对齐 + 通用质量）。</li>
<li><strong>实验</strong>：自动评测、人类评估、消融、成本分析一致表明，SlideTailor 在偏好对齐与幻灯片质量上显著优于最强基线，且可低成本迁移至开源模型。</li>
<li><strong>下游</strong>：自带演讲稿可直接驱动声音克隆与说话头像，一键生成个性化视频演示。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SlideTailor%3A%20Personalized%20Presentation%20Slide%20Generation%20for%20Scientific%20Papers，链接是https://arxiv.org/pdf/2512.20292，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20292。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wenzheng Zeng,Mingyu Ouyang,Langyuan Cui,Hwee Tou Ng</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.MM</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20292.pdf">https://arxiv.org/pdf/2512.20292.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20292">https://arxiv.org/abs/2512.20292</a></p>
<p><strong>Arxiv ID</strong>: 2512.20292</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.20292">https://papers.cool/arxiv/2512.20292</a></p>
<p><strong>Published</strong>: 2025-12-23T12:01:18Z</p>
<p><strong>Updated</strong>: 2025-12-23T12:01:18.000Z</p>
<hr>
<h3 id="13-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication"><a href="#13-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication" class="headerlink" title="13. A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21980">A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication</a></h3><p>This paper presents a new state-of-the-art algorithm for exact $3\times3$ matrix multiplication over general non-commutative rings, achieving a rank-23 scheme with only 58 scalar additions. This improves the previous best additive complexity of 60 additions without a change of basis. The result was discovered through an automated search combining ternary-restricted flip-graph exploration with greedy intersection reduction for common subexpression elimination. The resulting scheme uses only coefficients from ${-1, 0, 1}$, ensuring both efficiency and portability across arbitrary fields. The total scalar operation count is reduced from 83 to 81.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本文提出了一种用于一般非交换环的精确 $3\times3$ 矩阵乘法的新型最先进算法，实现了一个秩为23的方案，仅需58次标量加法。这比之前在不改变基的情况下的最佳加法复杂度60次有所改进。该结果是通过自动搜索发现的，该搜索结合了受限三进制翻转图探索与贪婪交点减少以消除公共子表达式。所得方案仅使用来自 ${-1, 0, 1}$ 的系数，从而确保在任意域中均具有效率和可移植性。总标量操作次数从83减少到81。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在<strong>降低 3×3 矩阵乘法在精确（非交换）环上的加法复杂度</strong>。具体目标可归纳为：</p>
<ul>
<li><strong>保持秩 23</strong>（当前已知最低乘法次数 23 不变）；</li>
<li><strong>不引入基变换</strong>（保证算法在任何域/环上直接可用）；</li>
<li><strong>将标量加法次数从 60 降至 58</strong>，刷新无基变换方案的最小加法记录；</li>
<li><strong>仅用系数 {−1, 0, 1}</strong>，确保跨平台、跨域的可移植性与硬件友好性；</li>
<li><strong>总标量运算量</strong>（乘法+加法）<strong>从 83 降到 81</strong>，提升实际软件实现效率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究与本论文直接相关，按时间顺序与贡献维度归类：</p>
<ul>
<li><p><strong>Strassen (1969)</strong><br>首次提出通用矩阵乘法次立方算法，奠定快速乘法研究范式。</p>
</li>
<li><p><strong>Laderman (1976)</strong><br>给出 3×3 秩-23 方案，加法次数 98，为后续“低加法”改进提供基准。</p>
</li>
<li><p><strong>Brent (1970)</strong><br>提出 Brent 方程组，成为验证任意 (U,V,W) 方案正确性的代数框架。</p>
</li>
<li><p><strong>Schwartz &amp; Vaknin (2023)</strong><br>引入“可变基”技巧，将 3×3 加法次数降至 61，但需基变换，限制通用性。</p>
</li>
<li><p><strong>Mårtensson &amp; Wagner (2025)</strong><br>通过组合优化把加法减到 62，仍无需基变换，逼近当时最优。</p>
</li>
<li><p><strong>Stapleton (2025)</strong><br>刷新无基变换记录至 60 加法，成为本文直接超越的对象。</p>
</li>
<li><p><strong>Kauers &amp; Moosbauer (2023)</strong><br>提出 flip-graph 框架，用局部翻转在等秩方案空间随机游走，为本文搜索策略奠基。</p>
</li>
<li><p><strong>Moosbauer &amp; Poole (2025)</strong><br>在 flip-graph 中引入对称剪枝，加速新方案发现，启发本文“三值限制”思路。</p>
</li>
<li><p><strong>Arai et al. (2024)</strong><br>设计自适应 flip-graph 算法，动态调整邻域结构，为本文“plus 算子”提供灵感。</p>
</li>
<li><p><strong>Perminov (2025)</strong></p>
</li>
<li>“Fast matrix multiplication via ternary meta flip graphs” 首次把系数锁在 {−1,0,1} 直接搜索，避免 Z₂→Z 提升。</li>
<li>“Parallel heuristic exploration for additive complexity reduction” 提出贪心交集削减（Greedy-Intersections）策略，被本文完整集成。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“在保持秩 23 且不做基变换的前提下，把 3×3 矩阵乘法的加法次数降到 58”形式化为一个<strong>带约束的组合优化问题</strong>，并设计了一套三阶段迭代搜索流程予以解决。核心思路与实施步骤如下：</p>
<ol>
<li>搜索空间限定</li>
</ol>
<ul>
<li>仅允许系数取自 {−1,0,1}，直接跨越任意环/域，无需“Z₂ 提升”步骤。</li>
<li>固定乘法次数 r=23，把变量维度锁定在 (U,V,W)∈{−1,0,1}^{23×3×3}。</li>
</ul>
<ol>
<li>状态转移机制</li>
</ol>
<ul>
<li><strong>flip 算子</strong>：在 Brent 方程保持成立的条件下，局部翻转张量元素，实现“等秩邻域”游走。</li>
<li><strong>plus 算子</strong>：当 flip 无法继续或随机概率触发时，临时把秩加 1，扩大逃逸空间，后续再通过 flip 压回 23。</li>
</ul>
<ol>
<li>加法复杂度削减</li>
</ol>
<ul>
<li>每当获得一个秩 23 候选，立即执行<strong>贪心交集削减</strong>（Greedy-Intersections CSE）：<br>– 枚举所有形如 xi±xj 的公共子表达式；<br>– 用启发式评分函数挑选“最大收益”子式，替换为全新辅助变量；<br>– 迭代至无可再减，得到当前方案的最小加法数。</li>
</ul>
<ol>
<li>三阶段循环架构</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Phase 1: flip → 目标秩 23</span><br><span class="line">Phase 2: 贪心 CSE → 最小化加法</span><br><span class="line">Phase 3: 随机 flip/plus → 扰动跳出局部极小</span><br></pre></td></tr></table></figure>
<p>循环手动终止，全程在普通笔记本 CPU 上运行约 30 分钟即产出 58-addition 方案。</p>
<ol>
<li>正确性验证</li>
</ol>
<ul>
<li>符号级：把 20 个辅助变量回代，重构 U,V,W 并代入 Brent 方程，恒等式成立。</li>
<li>数值级：10 000 组随机整数矩阵对比，结果与朴素乘法逐元素一致。</li>
</ul>
<p>通过“<strong>三值限制 flip-graph + 贪心公共子式消除</strong>”这一组合，论文首次把无基变换的秩-23 方案加法复杂度降到 58，并保证总运算量 81 为当前最低。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文仅执行<strong>验证性实验</strong>，旨在证明新方案“秩-23、58 加法”在任意环/域上的<strong>正确性与可重复性</strong>，而非与传统算法做性能对比。具体实验内容如下：</p>
<ol>
<li>符号验证</li>
</ol>
<ul>
<li>将 20 个引入的辅助变量（u1–u4, v1–v8, w1–w8）全部回代至原始线性形式；</li>
<li>重构三维系数张量 U,V,W∈{−1,0,1}^{23×3×3}；</li>
<li>代入 Brent 方程组，确认 81 条等式恒成立，从而保证对任意 3×3 矩阵精确成立。</li>
</ul>
<ol>
<li>数值验证</li>
</ol>
<ul>
<li>在 Python 中实现 Listing 1 脚本，随机生成 10 000 对 3×3 整数矩阵（元素范围<br>−100,100<br>）；</li>
<li>分别用新方案与 NumPy 原生乘法计算 C=AB；</li>
<li>使用 <code>np.array_equal</code> 逐元素比对，<strong>全部通过</strong>，无误差。</li>
</ul>
<ol>
<li>开销测量</li>
</ol>
<ul>
<li>记录搜索全程耗时：约 30 分钟，运行于 Intel Core i7-9750H 笔记本，未动用 GPU 或集群；</li>
<li>统计最终方案运算量：23 次乘法 + 58 次加法（34 加 + 24 减）= 81 次标量操作，较前任 60-加法方案减少 2 次加法、总操作量从 83 降至 81。</li>
</ul>
<p>实验结论：</p>
<ul>
<li>符号与数值双重验证均通过，说明 58-加法方案在<strong>任意域</strong>上正确；</li>
<li>搜索过程对普通硬件友好，无需高性能计算即可复现；</li>
<li>加法次数与总操作量均刷新<strong>无基变换</strong>秩-23 方案的最优记录。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>加法复杂度下界</strong><br>建立秩-23 场景的理论下界，判断 58 是否为最优或仍可压缩。</p>
</li>
<li><p><strong>更大/更小格式的推广</strong><br>将“三值限制 flip-graph + 贪心 CSE”框架应用于 2×2、4×4、5×5 等格式，寻找更低加法或更低秩方案。</p>
</li>
<li><p><strong>多线程/分布式搜索</strong><br>在 GPU 或集群上实现并行 flip-graph 游走，扩大搜索宽度与深度，可能发现更低加法或新秩记录。</p>
</li>
<li><p><strong>机器学习辅助探索</strong><br>用强化学习或神经启发式策略预测高评分 flip/plus 操作，减少随机游走步数，加速收敛。</p>
</li>
<li><p><strong>硬件级验证与调优</strong><br>把 58-加法方案集成到 BLAS、CUTLASS 等生产库，实测在 x86/ARM/AVX-512/GPU 上的吞吐与能效，并与 60-加法方案对比。</p>
</li>
<li><p><strong>能量模型驱动的 CSE</strong><br>在贪心公共子式消除阶段引入能耗或延迟代价模型，优先削减“贵”操作，而不仅计数加法。</p>
</li>
<li><p><strong>符号重写与自动证明</strong><br>开发符号代数工具，自动验证并简化任意新方案，避免手工回代 Brent 方程。</p>
</li>
<li><p><strong>可变基与混合基研究</strong><br>放松“无基变换”约束，探索允许轻微基变换时能否突破 58 加法，或进一步降低乘法次数。</p>
</li>
<li><p><strong>容错/近似扩展</strong><br>研究在允许数值误差 ε 的近似乘法场景下，能否用更少加法或更低秩逼近 3×3 乘积。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心贡献</strong><br>提出迄今加法最少的<strong>无基变换</strong> 3×3 精确矩阵乘法方案：</p>
<ul>
<li>乘法次数 23（已知最低秩）</li>
<li>标量加法 58（34 加 + 24 减），刷新前任 60 记录</li>
<li>总运算量 81，较 83 再降 2</li>
<li>系数仅 {−1,0,1}，跨域通用</li>
</ul>
<p><strong>方法框架</strong></p>
<ol>
<li>三值限制 flip-graph：在 {−1,0,1} 空间随机翻转，plus 算子逃逸局部极小，持续压回秩 23</li>
<li>贪心交集削减：每得候选即做 CSE，迭代提取公共子式 xi±xj，直至加法最小</li>
<li>三阶段循环：flip→秩 23 → 贪心减加法 → 随机扰动，笔记本 CPU 30 分钟搜得方案</li>
</ol>
<p><strong>验证</strong></p>
<ul>
<li>符号回代重构 U,V,W，Brent 方程恒成立</li>
<li>10 000 组随机整数矩阵与 NumPy 结果逐元素一致</li>
</ul>
<p><strong>意义与展望</strong><br>首次把无基变换秩-23 加法降到 58，为 BLAS 等底层库提供即插即用的新基元；框架可拓展至其他尺寸，并启发进一步理论下界与硬件实测研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是A%2058-Addition%2C%20Rank-23%20Scheme%20for%20General%203x3%20Matrix%20Multiplication，链接是https://arxiv.org/pdf/2512.21980，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21980。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: A. I. Perminov</p>
<p><strong>Categories</strong>: cs.DS</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21980.pdf">https://arxiv.org/pdf/2512.21980.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21980">https://arxiv.org/abs/2512.21980</a></p>
<p><strong>Arxiv ID</strong>: 2512.21980</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21980">https://papers.cool/arxiv/2512.21980</a></p>
<p><strong>Published</strong>: 2025-12-26T10:58:54Z</p>
<p><strong>Updated</strong>: 2025-12-26T10:58:54.000Z</p>
<hr>
<h3 id="14-Rethinking-Sample-Polarity-in-Reinforcement-Learning-with-Verifiable-Rewards"><a href="#14-Rethinking-Sample-Polarity-in-Reinforcement-Learning-with-Verifiable-Rewards" class="headerlink" title="14. Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21625">Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards</a></h3><p>Large reasoning models (LRMs) are typically trained using reinforcement learning with verifiable reward (RLVR) to enhance their reasoning abilities. In this paradigm, policies are updated using both positive and negative self-generated rollouts, which correspond to distinct sample polarities. In this paper, we provide a systematic investigation into how these sample polarities affect RLVR training dynamics and behaviors. We find that positive samples sharpen existing correct reasoning patterns, while negative samples encourage exploration of new reasoning paths. We further explore how adjusting the advantage values of positive and negative samples at both the sample level and the token level affects RLVR training. Based on these insights, we propose an Adaptive and Asymmetric token-level Advantage shaping method for Policy Optimization, namely A3PO, that more precisely allocates advantage signals to key tokens across different polarities. Experiments across five reasoning benchmarks demonstrate the effectiveness of our approach.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型推理模型（LRMs）通常使用可验证奖励的强化学习（RLVR）进行训练，以增强其推理能力。在这种范式下，策略通过使用正向和负向自生成回滚进行更新，这些回滚对应不同的样本极性。本文对这些样本极性如何影响 RLVR 的训练动态和行为进行了系统的研究。我们发现，正向样本会强化现有的正确推理模式，而负向样本则鼓励探索新的推理路径。我们进一步探讨了在样本级别和令牌级别调整正负样本的优势值如何影响 RLVR 训练。基于这些洞察，我们提出了一种用于策略优化的自适应与非对称令牌级优势塑形方法，即 A3PO，它能够在不同极性下更精准地将优势信号分配给关键令牌。在五个推理基准上的实验表明了我们方法的有效性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统回答“在可验证奖励强化学习（RLVR）中，正、负样本各自扮演何种角色”这一核心问题，并据此提出一种更精细的优势分配方法，以提升大推理模型（LRMs）的推理能力。具体可归纳为三点：</p>
<ol>
<li>揭示样本极性对训练动态的影响</li>
</ol>
<ul>
<li>正样本强化（PSR）会快速降低策略熵、缩短输出长度，使模型“锐化”已有正确路径，但易过拟合、损失探索。</li>
<li>负样本强化（NSR）维持高熵、延长响应，鼓励“发现”新路径，却导致训练奖励上升缓慢，甚至因训练-推理概率差异过大而崩溃。</li>
</ul>
<ol>
<li>量化不同粒度下优势塑形的作用</li>
</ol>
<ul>
<li>样本级：单纯放大正样本优势可加速奖励提升但压缩多样性；放大负样本优势则相反。最优相对比例约为 0.5（正:负）。</li>
<li>词元级：<br>– 对正样本，低概率词元放大优势可保持探索，高概率词元放大优势则加速熵减。<br>– 对负样本，高概率词元放大优势可抑制确定性、提升熵，低概率词元放大优势反而加速熵减。</li>
</ul>
<ol>
<li>提出自适应非对称词元级优势塑形方法 A3PO<br>动态地给“正样本中的低概率词元”和“负样本中的高概率词元”额外优势权重，并随训练逐步衰减，兼顾早期探索与后期稳定。在 3 个基座模型、5 个推理基准上显著优于 GRPO、DAPO 等现有 RLVR 算法。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文围绕“可验证奖励强化学习（RLVR）”与“样本极性”两条主线展开，相关研究可划分为以下四类：</p>
<ol>
<li>RLVR 框架与算法</li>
</ol>
<ul>
<li>GRPO（Group Relative Policy Optimization）<br>DeepSeek-AI et al., 2025。首个无需价值网络、利用组内相对优势训练长链思维的代表算法。</li>
<li>DAPO（Decoupled Adaptive Policy Optimization）<br>Yu et al., 2025。在 GRPO 基础上引入“clip-higher”防止熵塌、动态过滤零方差 prompt、取消 KL 惩罚，实现大规模开源 RLVR 系统。</li>
<li>VAPO / TreePO / GPO<br>Yue et al., 2025b；Li et al., 2025；Zheng et al., 2025。分别通过价值重标、启发式树建模、组序列级优化进一步改进优势估计与采样效率。</li>
</ul>
<ol>
<li>样本极性利用与重加权</li>
</ol>
<ul>
<li>Zhu et al., 2025（W-REINFORCE）<br>首次将 RLVR 目标解耦为正、负样本两项，提出给负样本更高权重以提升 Pass@k。</li>
<li>ASPO（Asymmetric Importance Sampling Policy Optimization）<br>Wang et al., 2025a。在重要性采样阶段对正负样本采用非对称裁剪，缓解分布偏移。</li>
<li>BAPO（Balanced Adaptive Policy Optimization）<br>Xi et al., 2025。通过自适应裁剪强度平衡正负样本梯度，稳定 off-policy 训练。</li>
</ul>
<ol>
<li>词元级优势塑形与熵/概率干预</li>
</ol>
<ul>
<li>Fork Tokens<br>Wang et al., 2025b。仅对高熵“分叉”词元计算梯度，维持探索。</li>
<li>Lp-Reg<br>Huang et al., 2025。对低概率词元施加正则化，防止有用稀有 token 被抑制。</li>
<li>Entropy-Change Perspective<br>Hao et al., 2025。从熵变化角度解释 RLVR 不稳定现象，提出动态熵约束。</li>
</ul>
<ol>
<li>训练-推理不一致与探索塌陷</li>
</ol>
<ul>
<li>训练-推理概率差异（Rollout Prob Diff）<br>本文附录 G 首次量化指出负样本加权会放大该差异，导致崩溃；同期 Hao et al., 2025 亦观察到熵塌与分布偏移的相关性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“先分析、后干预”的两段式路线，逐步解决“如何合理利用正负样本极性”的问题。</p>
<ol>
<li>系统分析阶段</li>
</ol>
<ul>
<li>单极性训练对比<br>在三个基座模型上分别执行“仅正样本强化（PSR）”与“仅负样本强化（NSR）”，量化观测熵、响应长度、训练/验证奖励、n-gram 重复度等指标，得出：<br>– 正样本 → 熵减 + 输出缩短 + 已有正确路径“锐化”（sharpen）。<br>– 负样本 → 熵维持 + 输出增长 + 新路径“发现”（discovery）。</li>
<li>粒度消融<br>– 样本级：连续缩放正负样本整体优势，发现相对比例而非绝对值决定训练动态，最优正:负≈0.5。<br>– 词元级：按熵或概率对 20% 极端词元再加权，验证“正低概率、负高概率”两类词元对维持探索最关键。</li>
</ul>
<ol>
<li>干预设计阶段——A3PO<br>基于上述结论，提出 Adaptive and Asymmetric token-level Advantage shaping for Policy Optimization（A3PO），核心公式：</li>
</ol>
<p>A_t = A_t · max(rho^+ - α^+ s,; 1), &amp; A_t&gt;0 ;land; p_t le τ^+_o [4pt] A_t · max(rho^- - α^- s,; 1), &amp; A_t&lt;0 ;land; p_t ge τ^-_o [4pt] A_t, &amp; else</p>
<ul>
<li>非对称：仅对“正样本中概率最低的 20% 词元”与“负样本中概率最高的 20% 词元”额外加权。</li>
<li>自适应：随训练步数 s 线性衰减额外权重，最终回归标准优势，避免训练-推理概率差异持续放大。</li>
<li>即插即用：直接替换 DAPO 中的  A_t ，保持其余 RLVR 流程（组采样、clip-higher、无 KL）不变。</li>
</ul>
<ol>
<li>验证结果<br>在 Qwen2.5-7B-Math、Qwen3-8B-Base、DeepSeek-R1-Distill-Qwen-7B 上，A3PO 相对 DAPO 平均提升 2.1–4.6 个百分点，在五类推理基准（AIME24/25、MATH500、GPQA、LiveCodeBench）均取得新高，且熵与响应长度更平稳，验证集泛化性能显著优于现有样本级或词元级重加权方法。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“样本极性”与“优势塑形”两条主线，共设计并执行了 5 组系统性实验。所有实验均在 16×H200 GPU 集群上完成，训练 300 step，batch=512，temperature=1，采样 8 条 rollout/题。评估采用 32 次重复解码取平均，覆盖 5 个推理基准（AIME24、AIME25、MATH500、GPQA、LiveCodeBench）。</p>
<ol>
<li><p>单极性训练对照<br>模型：Qwen2.5-7B-Math、Qwen3-8B-Base、DeepSeek-R1-Distill-Qwen-7B<br>条件：仅正样本强化（PSR）、仅负样本强化（NSR）、标准 DAPO（正负混合）<br>观测：熵、响应长度、训练 reward、验证 Avg@32/Pass@32、n-gram Sharpen/Discovery 比例<br>结论：PSR 熵降+输出缩短+锐化；NSR 熵稳+输出增长+发现；单极性均导致性能下降或崩溃。</p>
</li>
<li><p>样本级优势塑形（Polarity-level AS）<br>模型：Qwen2.5-7B-Math<br>条件：固定负样本权重=1，正样本权重∈{0.2,0.5,1,2,5}；反之固定正样本权重=1，负样本权重∈{0.2,0.5,1,2,5}<br>观测：训练/验证 reward、熵、长度<br>结论：相对比例决定动态，正:负≈0.5 时验证集最优；过高正权重过拟合，过高负权重训练-推理差异放大。</p>
</li>
<li><p>词元级优势塑形（Token-level AS）<br>3a 熵基线：按 token 熵值高低各选 20%，缩放系数{0.2,1,5}<br>3b 概率基线：按 token 概率高低各选 20%，缩放系数{0.2,1,5}<br>模型：Qwen2.5-7B-Math<br>观测：同上<br>结论：<br>– 正样本：低概率或高熵 token 加权→维持/提升熵；高概率或低熵 token 加权→加速熵减。<br>– 负样本：高概率 token 加权→抑制确定性、提升熵；低概率 token 加权→反而熵减。</p>
</li>
<li><p>方法主实验：A3PO vs 基线<br>基线：GRPO、DAPO、W-REINFORCE（样本级）、DAPO w/ Fork Tokens、Lp-Reg<br>模型：同上三款 LLM<br>指标：五基准平均准确率、训练曲线（reward、熵、长度）、验证 Avg@32<br>结果：A3PO 在三款模型上分别较最强基线提升 2.7、3.1、2.5 pp，且熵与长度更平稳。</p>
</li>
<li><p>消融与超参实验</p>
</li>
</ol>
<ul>
<li>消融：去掉正低概率加权、去掉负高概率加权、去掉自适应衰减，验证三组件缺一不可。</li>
<li>比例灵敏度：token 塑形比例 5%/10%/20%/50%，20% 综合最优。</li>
<li>初始缩放 ρ 与衰减 α 扫描：ρ=2、α=0.005 时性能峰值。</li>
<li>训练-推理差异监测：负样本加权越大，训练-推理概率差距越大，A3PO 的自适应衰减可有效收敛该差距。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对本文结论的直接延伸或补充，均具有理论与应用价值：</p>
<ol>
<li>跨模态扩展</li>
</ol>
<ul>
<li>将 A3PO 的正负样本极性分析迁移至视觉-语言推理（几何题图解、科学图表 QA）或音频-文本任务，验证“锐化-发现”规律是否仍然成立。</li>
<li>研究图像 token 与文本 token 在极性上的异构优势分配策略。</li>
</ul>
<ol>
<li>多步可验证任务</li>
</ol>
<ul>
<li>在代码搜索、工具调用、机器人规划等“多步-多检查点”场景下，定义中间可验证奖励，考察正负样本对长程信用分配的影响。</li>
<li>探索基于子目标极性的分段优势塑形（Segment-level AS）。</li>
</ul>
<ol>
<li>动态极性标注</li>
</ol>
<ul>
<li>当前仅用最终答案对错划分正负。可引入过程奖励模型（PRM）或蒙特卡洛 rollout，对“部分正确”轨迹赋予连续极性标签，设计连续化 A3PO。</li>
<li>研究极性标签噪声对 A3PO 衰减策略的鲁棒性。</li>
</ul>
<ol>
<li>训练-推理差异的机理与抑制</li>
</ol>
<ul>
<li>建立概率差异的理论上界，证明自适应衰减速率 α 的最优性条件。</li>
<li>引入同步蒸馏（on-policy distillation）或滚动重采样，实时缩小训练-推理分布差距。</li>
</ul>
<ol>
<li>极性敏感的课程学习</li>
</ol>
<ul>
<li>根据模型当前熵或验证性能，动态调整正负样本采样比例与 A3PO 超参，形成“课程式 RLVR”。</li>
<li>探索从 NSR 主导到 PSR 主导的自动相位切换策略。</li>
</ul>
<ol>
<li>多模型协同极性</li>
</ol>
<ul>
<li>在自我对弈或群体强化设置中，利用对手模型的正负样本作为额外极性来源，研究跨模型优势迁移。</li>
<li>考察 A3PO 在联邦 RLVR（多客户端本地正负样本）下的收敛性与通信效率。</li>
</ul>
<ol>
<li>计算-性能权衡</li>
</ol>
<ul>
<li>对 A3PO 的 token 级掩码与额外矩阵运算进行 GPU 内核融合优化，量化其在大规模集群下的训练吞吐变化。</li>
<li>探索低秩优势塑形（LoRA-AS），仅对 Adapter 参数应用极性加权，减少显存占用。</li>
</ul>
<ol>
<li>理论深化</li>
</ol>
<ul>
<li>在简化 MDP 或 Bandit 设定下，给出正负样本优势比与策略熵的解析关系，证明“相对比例决定收敛速度”的渐近界。</li>
<li>研究 A3PO 的非对称加权对策略梯度方差的影响，推导最优衰减系数 α 的闭式解。</li>
</ul>
<ol>
<li>与人类反馈结合</li>
</ol>
<ul>
<li>将 A3PO 的正负极性思想迁移至 RLHF 场景，考察“正-负偏好对”在 token 级优势塑形中的作用，缓解过度迎合或过度拒绝现象。</li>
</ul>
<ol>
<li>自动化超参搜索</li>
</ol>
<ul>
<li>使用元学习或演化算法，对 ρ+, ρ−, α+, α− 进行任务自适应搜索，形成“无手调”版 A3PO。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心内容可概括为“一项系统分析 + 一项方法改进 + 一套实验验证”：</p>
<ol>
<li>系统分析：正负样本在 RLVR 中的对立统一</li>
</ol>
<ul>
<li>正样本→熵减、输出缩短、强化已有正确路径（sharpen）。</li>
<li>负样本→熵稳、输出增长、探索新路径（discovery）。</li>
<li>仅用任一极性均导致性能退化或崩溃；二者比例决定训练动态。</li>
</ul>
<ol>
<li>方法改进：A3PO<br>基于“正低概率、负高概率”两类关键词元，提出自适应非对称词元级优势塑形：</li>
</ol>
<p>A_t = A_t · max(rho^+ – α^+ s,; 1), &amp; A_t&gt;0 land p_tleτ^+_o A_t · max(rho^- – α^- s,; 1), &amp; A_t&lt;0 land p_tgeτ^-_o A_t, &amp; else</p>
<p>权重随训练步 s 线性衰减，兼顾早期探索与后期稳定。</p>
<ol>
<li>实验验证</li>
</ol>
<ul>
<li>3 款基座模型 × 5 个推理基准 → A3PO 平均提升 2.5–4.6 pp，优于 GRPO、DAPO 及现有样本级/词元级重加权方法。</li>
<li>熵与响应长度更平稳，验证集泛化显著增强；消融与超参扫描证明三组件缺一不可。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking%20Sample%20Polarity%20in%20Reinforcement%20Learning%20with%20Verifiable%20Rewards，链接是https://arxiv.org/pdf/2512.21625，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21625。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xinyu Tang,Yuliang Zhan,Zhixun Li,Wayne Xin Zhao,Zhenduo Zhang,Zujie Wen,Zhiqiang Zhang,Jun Zhou</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21625.pdf">https://arxiv.org/pdf/2512.21625.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21625">https://arxiv.org/abs/2512.21625</a></p>
<p><strong>Arxiv ID</strong>: 2512.21625</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21625">https://papers.cool/arxiv/2512.21625</a></p>
<p><strong>Published</strong>: 2025-12-25T11:15:46Z</p>
<p><strong>Updated</strong>: 2025-12-25T11:15:46.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202512/2025-12-30/">https://wdndev.github.io/daily/hf/202512/2025-12-30/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-InsertAnywhere-Bridging-4D-Scene-Geometry-and-Diffusion-Models-for-Realistic-Video-Object-Insertion"><span class="toc-text">1. InsertAnywhere: Bridging 4D Scene Geometry and Diffusion Models for Realistic Video Object Insertion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Mindscape-Aware-Retrieval-Augmented-Generation-for-Improved-Long-Context-Understanding"><span class="toc-text">2. Mindscape-Aware Retrieval Augmented Generation for Improved Long Context Understanding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-MAI-UI-Technical-Report-Real-World-Centric-Foundation-GUI-Agents"><span class="toc-text">3. MAI-UI Technical Report: Real-World Centric Foundation GUI Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-UniPercept-Towards-Unified-Perceptual-Level-Image-Understanding-across-Aesthetics-Quality-Structure-and-Texture"><span class="toc-text">4. UniPercept: Towards Unified Perceptual-Level Image Understanding across Aesthetics, Quality, Structure, and Texture</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-ProEdit-Inversion-based-Editing-From-Prompts-Done-Right"><span class="toc-text">5. ProEdit: Inversion-based Editing From Prompts Done Right</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-TimeBill-Time-Budgeted-Inference-for-Large-Language-Models"><span class="toc-text">6. TimeBill: Time-Budgeted Inference for Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-See-Less-See-Right-Bi-directional-Perceptual-Shaping-For-Multimodal-Reasoning"><span class="toc-text">7. See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Omni-Weather-Unified-Multimodal-Foundation-Model-for-Weather-Generation-and-Understanding"><span class="toc-text">8. Omni-Weather: Unified Multimodal Foundation Model for Weather Generation and Understanding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-InSight-o3-Empowering-Multimodal-Foundation-Models-with-Generalized-Visual-Search"><span class="toc-text">9. InSight-o3: Empowering Multimodal Foundation Models with Generalized Visual Search</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-SWE-RM-Execution-free-Feedback-For-Software-Engineering-Agents"><span class="toc-text">10. SWE-RM: Execution-free Feedback For Software Engineering Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-SVBench-Evaluation-of-Video-Generation-Models-on-Social-Reasoning"><span class="toc-text">11. SVBench: Evaluation of Video Generation Models on Social Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-SlideTailor-Personalized-Presentation-Slide-Generation-for-Scientific-Papers"><span class="toc-text">12. SlideTailor: Personalized Presentation Slide Generation for Scientific Papers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-A-58-Addition-Rank-23-Scheme-for-General-3x3-Matrix-Multiplication"><span class="toc-text">13. A 58-Addition, Rank-23 Scheme for General 3x3 Matrix Multiplication</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-Rethinking-Sample-Polarity-in-Reinforcement-Learning-with-Verifiable-Rewards"><span class="toc-text">14. Rethinking Sample Polarity in Reinforcement Learning with Verifiable Rewards</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>