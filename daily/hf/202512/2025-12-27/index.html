<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-12-27 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Latent Implicit Visual ReasoningWhile Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-12-27">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202512/2025-12-27/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Latent Implicit Visual ReasoningWhile Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.977Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202512/2025-12-27/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-12-27',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-12-27</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.977Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">28.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>100min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-12-27"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Latent-Implicit-Visual-Reasoning"><a href="#1-Latent-Implicit-Visual-Reasoning" class="headerlink" title="1. Latent Implicit Visual Reasoning"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21218">Latent Implicit Visual Reasoning</a></h3><p>While Large Multimodal Models (LMMs) have made significant progress, they remain largely text-centric, relying on language as their core reasoning modality. As a result, they are limited in their ability to handle reasoning tasks that are predominantly visual. Recent approaches have sought to address this by supervising intermediate visual steps with helper images, depth maps, or image crops. However, these strategies impose restrictive priors on what “useful” visual abstractions look like, add heavy annotation costs, and struggle to generalize across tasks. To address this critical limitation, we propose a task-agnostic mechanism that trains LMMs to discover and use visual reasoning tokens without explicit supervision. These tokens attend globally and re-encode the image in a task-adaptive way, enabling the model to extract relevant visual information without hand-crafted supervision. Our approach outperforms direct fine-tuning and achieves state-of-the-art results on a diverse range of vision-centric tasks — including those where intermediate abstractions are hard to specify — while also generalizing to multi-task instruction tuning.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然大型多模态模型（LMMs）取得了显著进展，但它们仍然以文本为中心，将语言作为核心推理方式。因此，它们在处理以视觉为主的推理任务时能力有限。近期的一些方法试图通过使用辅助图像、深度图或图像裁剪来监督中间视觉步骤以解决这一问题。然而，这些策略对“有用”的视觉抽象形式施加了限制性先验，同时增加了大量标注成本，并且在跨任务泛化方面存在困难。为了解决这一关键限制，我们提出了一种任务无关的机制，使LMMs能够在没有显式监督的情况下发现和使用视觉推理标记。这些标记进行全局关注，并以任务自适应的方式重新编码图像，使模型能够在无需手工监督的情况下提取相关视觉信息。我们的方法优于直接微调，并在各类以视觉为中心的任务上取得了最先进的结果——包括那些难以指定中间抽象的任务——同时还能泛化到多任务指令调优。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>大型多模态模型（LMM）在视觉中心任务中表现受限</strong>的核心问题。具体而言：</p>
<ul>
<li><strong>语言偏见</strong>：现有 LMM 以文本为推理主轴，视觉信息仅被一次性投影到语言空间，导致复杂视觉推理（如拼图、计数、视觉相似度判断）难以用纯文本表达。</li>
<li><strong>显式监督瓶颈</strong>：先前方法依赖人工设计的中间视觉监督（边界框、深度图、裁剪图等），带来高昂标注成本，且对“有用”视觉抽象的先验假设往往与模型最优需求不一致，难以泛化到缺乏明确中间表征的任务。</li>
</ul>
<p>为此，作者提出<strong>Latent Implicit Visual Reasoning (LIVR)</strong>，让模型在无额外标注的情况下，自主发现并利用<strong>隐式视觉推理令牌（latent tokens）</strong>，通过视觉瓶颈机制重新编码图像，实现任务自适应的视觉抽象，从而提升视觉中心任务的性能与泛化能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为三大类，并指出各自局限：</p>
<ol>
<li><strong>纯文本视觉推理</strong></li>
</ol>
<ul>
<li>链式思维（CoT）及其多模态扩展：LLaVA-CoT、Visual-RFT、Vision-R1、R1-VL 等</li>
<li>共性：中间推理完全用文本描述，难以表达空间-结构化的视觉抽象。</li>
</ul>
<ol>
<li><strong>交错视觉推理</strong></li>
</ol>
<ul>
<li><strong>视觉令牌复用</strong>：Visual CoT、Argus、VGR、UV-COT<br>– 依赖显式边界框或裁剪，表达能力受限于原始输入令牌，需人工监督。</li>
<li><strong>视觉中间表征</strong>：MVoT、CoT-VLA、Aurora、Mirage<br>– 生成或注入中间图像/深度/嵌入，需任务相关的显式监督，标注成本高，且人类定义的抽象未必最优。</li>
</ul>
<ol>
<li><strong>隐空间推理</strong></li>
</ol>
<ul>
<li>Coconut、Think Before You Speak<br>– 仅在语言模型内部做隐状态迭代，未针对视觉-语言融合场景设计。</li>
<li>Latent Visual Reasoning、Mirage<br>– 虽引入隐令牌，但仍依赖显式中间目标监督。</li>
</ul>
<p>LIVR 与上述方向的区别在于：<strong>无需任何任务特定的中间标注，完全通过任务损失端到端地学习视觉隐令牌</strong>，实现任务无关、即插即用的视觉推理增强。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>Latent Implicit Visual Reasoning（LIVR）</strong>，通过两项关键设计让 LMM 在无额外标注的条件下自主发现并利用视觉推理令牌：</p>
<ul>
<li><p><strong>Latent Tokens</strong><br>向词表引入  K  个可学习的特殊令牌  L=l_1,…,l_K ，拼接到提示之后。这些令牌不参与自回归生成，仅作为模型内部可自由调用的“视觉计算单元”。</p>
</li>
<li><p><strong>Visual Bottlenecking</strong><br>在训练阶段 1 修改注意力掩码：</p>
</li>
<li><p>答案令牌只能关注提示令牌与  L ，<strong>完全屏蔽对原始图像令牌的直接访问</strong>；</p>
</li>
<li>提示令牌同样无法访问图像令牌。<br>由此，视觉信息必须<strong>先被压缩到  L  中</strong>，再流向答案，迫使  L  成为任务相关的视觉抽象瓶颈。阶段 2 恢复标准掩码，允许答案同时利用原始图像与已训练的  L ，实现联合推理。</li>
</ul>
<p>两阶段均只以<strong>最终答案的负对数似然</strong>为损失，无需任何中间监督。通过该机制，模型端到端地学会“何时、如何”用  L  重新编码图像，从而突破语言偏见与人工监督限制，在多项视觉中心任务上取得 SOTA。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>9 项视觉中心任务</strong> 上系统评估 LIVR，实验分为 <strong>单任务微调</strong>、<strong>多任务微调</strong> 与 <strong>消融/对照</strong> 三大块，核心结果如下：</p>
<ol>
<li>单任务微调（每任务 1 k 训练样本）</li>
</ol>
<ul>
<li>背骨：Qwen2.5-VL-3B、Qwen3-VL-4B、LLaVA-OneVision-1.5-4B</li>
<li>对比：zero-shot、直接 SFT、LIVR（K=16，4 epoch 瓶颈 +6 epoch 联合）</li>
<li>结果：LIVR 平均提升 <strong>+6.24 %、+3.43 %、+5.60 %</strong>；在拼图、功能对应、艺术风格、视觉相似度等抽象任务上增益最大（最高 +13 %）。</li>
</ul>
<ol>
<li>多任务微调（6 任务混合，共 6 k 样本）</li>
</ol>
<ul>
<li>背骨：Qwen3-VL-4B</li>
<li>对比：直接 SFT vs LIVR（2+3 epoch）</li>
<li>结果：LIVR 在所有 6 项上均优于 SFT，平均 <strong>+2.77 %</strong>，验证方法<strong>任务无关</strong>且可无缝迁移到多任务场景。</li>
</ul>
<ol>
<li>与显式监督方法对比</li>
</ol>
<ul>
<li>任务：BLINK-Jigsaw、Visual Spatial Planning</li>
<li>对手：Mirage（需合成 helper image）</li>
<li>结果：LIVR 无需任何中间图像，仍分别 <strong>+19.4 %、+20 %</strong> 超越 Mirage。</li>
</ul>
<ol>
<li>消融与控制实验</li>
</ol>
<ul>
<li><strong>Latent 是否真被使用</strong>：去掉 latent 令牌，LIVR 准确率显著下降；对照组“仅加令牌无瓶颈”几乎不受影响，证明瓶颈训练是令牌生效的关键。</li>
<li><strong>组件必要性</strong>：同时需要“专用 latent 令牌”+“瓶颈掩码”，缺一即明显掉点。</li>
<li><strong>结构超参</strong>：令牌数 K=16 最佳；阶段比例 4:6 最优；令牌放提示后 &gt; 放提示前；不共享嵌入 &gt; 共享嵌入。</li>
<li><strong>注意力可视化</strong>：latent-to-image 注意力集中在任务相关区域（对应点、需计数物体、拼图边缘、亮度对比点等），且 t-SNE 显示 latent 与 image 令牌分布高度重叠，表明其确实编码了视觉特征。</li>
</ul>
<p>综上，实验覆盖从低层（对应、反射率）到高层（风格、相似度）视觉推理，<strong>一致验证 LIVR 在无需额外标注的前提下，持续优于直接微调与显式监督方法</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>可解释性</strong><br>当前 latent token 的表征以向量形式存在，缺乏人类可读语义。可引入离散化（VQ-VAE 风格码本）或稀疏激活约束，配合 probing 任务，把隐空间映射到可解释的视觉属性或文本描述。</p>
</li>
<li><p><strong>容量与规模</strong><br>仅探索了 4–32 个令牌、3B–4B 参数模型。继续放大 K 或采用分层 latent（粗→细）结构，并在 7B–70B 量级上验证，观察性能是否随规模继续提升或出现饱和。</p>
</li>
<li><p><strong>任务与数据多样性</strong><br>目前聚焦 9 项感知任务。将 LIVR 扩展到视频时空推理、机器人 VLA、医学影像、自动驾驶等域外场景，检验瓶颈机制是否仍能有效提炼关键视觉信息。</p>
</li>
<li><p><strong>零样本/少样本迁移</strong><br>现阶段仍需任务内微调。可结合 prompt-tuning 或 adapter 技术，冻结主模型，仅训练少量任务特定参数，实现跨任务快速迁移，降低新任务适应成本。</p>
</li>
<li><p><strong>与测试时扩展结合</strong><br>把 latent token 作为“测试时计算”资源：通过迭代刷新 latent（类似 Coconut 的连续思维）或蒙特卡洛投票，让模型在推理阶段动态分配更多视觉计算步，进一步提升难题准确率。</p>
</li>
<li><p><strong>统一的多模态 latent 空间</strong><br>将文本、音频、触觉等模态也纳入同一套 latent 令牌，研究跨模态瓶颈是否自动产生对齐表征，为真正“模态无关”推理提供基础。</p>
</li>
<li><p><strong>理论分析</strong><br>从信息论角度量化瓶颈掩码迫使图像信息流经 latent 时的压缩率与保真度，建立“视觉比特-任务性能” trade-off 曲线，为后续架构设计提供指导。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Latent Implicit Visual Reasoning（LIVR）</strong> 提出一种<strong>无需额外标注</strong>即可增强大型多模态模型视觉推理能力的通用方法。核心思想是：</p>
<ol>
<li>向输入追加 <strong>K 个可学习的 latent token</strong>；</li>
<li>在训练阶段 1 采用<strong>视觉瓶颈掩码</strong>：答案与提示令牌均无法直接访问图像，迫使所有视觉信息先压缩到 latent token；</li>
<li>阶段 2 恢复标准注意力，让答案同时利用原始图像与已训练的 latent token；</li>
<li>全程仅使用<strong>最终答案的交叉熵损失</strong>，无中间监督。</li>
</ol>
<p>在 9 项视觉中心任务（拼图、计数、对应、风格、相似度等）上，LIVR 相对直接微调平均提升 <strong>3.4–6.2 %</strong>，多任务联合训练亦全面领先；与需合成 helper image 的 Mirage 相比，<strong>零中间标注</strong>仍领先 <strong>+19–20 %</strong>。消融实验证实瓶颈机制与专用 latent 令牌缺一不可，注意力可视化显示 latent 自动聚焦任务关键区域。该方法为<strong>即插即用、任务无关</strong>的视觉推理增强提供了新路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent%20Implicit%20Visual%20Reasoning，链接是https://arxiv.org/pdf/2512.21218，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21218。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kelvin Li,Chuyi Shang,Leonid Karlinsky,Rogerio Feris,Trevor Darrell,Roei Herzig</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21218.pdf">https://arxiv.org/pdf/2512.21218.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21218">https://arxiv.org/abs/2512.21218</a></p>
<p><strong>Arxiv ID</strong>: 2512.21218</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21218">https://papers.cool/arxiv/2512.21218</a></p>
<p><strong>Published</strong>: 2025-12-24T14:59:49Z</p>
<p><strong>Updated</strong>: 2025-12-24T14:59:49.000Z</p>
<hr>
<h3 id="2-Emergent-temporal-abstractions-in-autoregressive-models-enable-hierarchical-reinforcement-learning"><a href="#2-Emergent-temporal-abstractions-in-autoregressive-models-enable-hierarchical-reinforcement-learning" class="headerlink" title="2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.20605">Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</a></h3><p>Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term “internal RL”, enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>在下一个 token 预测上进行预训练并通过强化学习 (RL) 进行微调的大规模自回归模型，在许多问题领域取得了前所未有的成功。在强化学习过程中，这些模型通过一次生成一个 token 来进行探索。然而，逐 token 采样动作可能导致学习效率极低，尤其是在奖励稀疏的情况下。在这里，我们展示了可以通过在自回归模型的内部表示中进行操作和探索来克服这个问题。具体来说，为了发现时间上抽象的动作，我们引入了一种高阶的非因果序列模型，其输出控制基础自回归模型的残差流激活。在具有层次结构的网格世界和基于 MuJoCo 的任务中，我们发现高阶模型学会将长激活序列块压缩到内部控制器上。关键是，每个控制器都执行一个在长时间尺度上展开的、行为上有意义的动作序列，并伴随着一个学习到的终止条件，从而将多个控制器随时间组合能够在新任务上实现高效探索。我们表明，直接的内部控制器强化，这一过程我们称之为“内部 RL”，能够在标准 RL 微调失败的情况下从稀疏奖励中学习。我们的结果展示了在自回归模型中潜在动作生成和强化的优势，表明内部 RL 是在基础模型中实现层次化 RL 的一个有前景的途径。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决的核心问题是：</p>
<blockquote>
<p><strong>在稀疏奖励（sparse-reward）环境下，基于自回归模型（autoregressive models）进行强化学习（RL）时，因逐token采样动作导致的探索效率极低的问题。</strong></p>
</blockquote>
<p>具体而言，尽管大规模自回归模型（如Transformer）在预训练阶段通过“下一token预测”任务学到了丰富的行为先验，但在RL微调阶段，若仍仅在原始动作空间（token级别）进行探索，则难以在需要<strong>长期规划</strong>和<strong>组合泛化</strong>的任务中获得奖励信号，导致学习失败。</p>
<p>为此，论文提出：</p>
<ul>
<li><strong>在模型内部表示空间中行动与探索</strong>，而非在原始动作空间；</li>
<li>通过引入一个<strong>高阶非因果序列模型（metacontroller）</strong>，控制基础自回归模型的<strong>残差流（residual stream）激活</strong>；</li>
<li>自动发现<strong>时间抽象动作（temporally-abstract actions）</strong>，即具有长期时间跨度、可复用的子目标（subgoals）；</li>
<li>提出一种名为**内部强化学习（internal RL）**的新范式，直接在抽象动作空间中进行RL，从而显著提升在稀疏奖励、层次化任务中的学习效率与成功率。</li>
</ul>
<p>总结如下：</p>
<ul>
<li><strong>问题</strong>：token级探索在稀疏奖励任务中效率极低；</li>
<li><strong>方法</strong>：在自模型内部表示中探索，学习时间抽象动作；</li>
<li><strong>目标</strong>：实现高效、层次化的强化学习，解决标准RL无法学习的任务。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，可视为相关工作的代表，按主题分类列出：</p>
<ul>
<li><strong>自回归模型与表示学习</strong></li>
<li>Vaswani et al. (2017) Attention is all you need</li>
<li>Kaplan et al. (2020) Scaling laws for neural language models</li>
<li>Xie et al. (2022) An explanation of in-context learning as implicit Bayesian inference</li>
<li>von Oswald et al. (2023) Uncovering mesa-optimization algorithms in transformers</li>
<li><strong>层次强化学习与选项框架</strong></li>
<li>Sutton, Precup &amp; Singh (1999) Between MDPs and semi-MDPs</li>
<li>Bacon, Harb &amp; Precup (2017) The option-critic architecture</li>
<li>Pateria et al. (2021) Hierarchical reinforcement learning: a comprehensive survey</li>
<li><strong>无监督时序抽象发现</strong></li>
<li>Kipf et al. (2019) CompILE: compositional imitation learning and execution</li>
<li>Jiang et al. (2022) Learning options via compression</li>
<li>Kim et al. (2019) Variational temporal abstraction</li>
<li><strong>模型内部干预与表示工程</strong></li>
<li>Zou et al. (2023) Representation engineering</li>
<li>Turner et al. (2023) Steering language models with activation engineering</li>
<li>Meng et al. (2022) Locating and editing factual associations in GPT</li>
<li><strong>连续控制与残差空间学习</strong></li>
<li>Lillicrap et al. (2016) Continuous control with deep reinforcement learning</li>
<li>Hafner et al. (2025) Mastering diverse control tasks through world models</li>
<li><strong>基于潜变量模型的推理与规划</strong></li>
<li>Schmidhuber (2015) On learning to think</li>
<li>LeCun (2022) A path towards autonomous machine intelligence (JEPA)</li>
<li>Kong et al. (2025) Latent thought models with variational Bayes inference-time computation</li>
<li><strong>机制可解释性与线性表示假说</strong></li>
<li>Nanda et al. (2023) Emergent linear representations in world models</li>
<li>Park et al. (2024) The linear representation hypothesis and the geometry of LLMs</li>
<li>Lindsey et al. (2025) On the biology of a large language model</li>
</ul>
<p>这些研究共同构成了本文提出的“内部强化学习”框架的理论与算法背景。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过<strong>四步递进式策略</strong>解决“稀疏奖励下自回归模型逐 token 探索效率低”的核心问题。每一步均对应一个技术模块，且后一步以前一步的发现为前提。</p>
<p>1. 验证：自回归模型内部已涌现<strong>时序抽象表示</strong></p>
<ul>
<li><p><strong>做法</strong><br>仅用行为克隆（下一 token 预测）预训练 Transformer/SSM，然后</p>
</li>
<li><p>线性探针（linear probing）</p>
</li>
<li>因果干预（causal intervention）<br>检验中间激活是否编码了“子目标”信息。</li>
<li><strong>结论</strong><br>中间层残差流  e_(t,l)  可线性解码出当前子目标  g_t ；且<strong>插入一个低秩线性控制器</strong></li>
</ul>
<p>e<em>(t,l) arrow e</em>(t,l) + U^((g)) e_(t,l)</p>
<p>即可让模型执行对应子目标，无需重新训练。</p>
<p>2. 监督式控制：用“子目标→控制器”实现<strong>组合泛化</strong></p>
<ul>
<li><p><strong>做法</strong><br>为每个子目标  g  单独学习一个控制器矩阵  U^((g)) ；在测试时按任务顺序依次激活  U^((g)) 。</p>
</li>
<li><p><strong>结果</strong><br>在<strong>更长、子目标顺序未见过</strong>的任务上仍取得高成功率，证明</p>
</li>
<li><p>抽象动作已线性嵌入残差空间</p>
</li>
<li>可通过<strong>时序组合</strong>实现零样本长度/组合泛化。</li>
</ul>
<p>3. 无监督发现：用**元控制器（metacontroller）**自动分割抽象动作</p>
<p>不再依赖子目标标签，引入<strong>双层生成模型</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>功能</th>
<th>关键公式</th>
</tr>
</thead>
<tbody>
<tr>
<td>序列嵌入器</td>
<td>用整段未来激活  e<em>(1:T)  产生上下文向量  s(e</em>(1:T))</td>
<td>非因果，提供后验信息</td>
</tr>
<tr>
<td>编码器</td>
<td>每步提出潜在控制器码  tilde z_t sim mathcal N(μ_t,Sigma_t)</td>
<td>近似 $p(z_t</td>
</tr>
<tr>
<td>切换单元</td>
<td>输出门控  β_t∈[0,1]</td>
<td>决定何时切换子目标</td>
</tr>
<tr>
<td>整合单元</td>
<td>z<em>t = β_t tilde z_t + (1-β_t)z</em>(t-1)</td>
<td>实现稀疏切换</td>
</tr>
<tr>
<td>解码器（超网络）</td>
<td>产生线性控制器  U<em>t = f</em>(hyp)(z_t)</td>
<td>残差干预  hat e<em>(t,l)=e</em>(t,l)+U<em>t e</em>(t,l)</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>训练目标</strong><br>在冻结的自回归模型上最小化正则化负对数似然</li>
</ul>
<p>mathcal L(φ)=∑<em>t -log p</em>(θ,φ)(a<em>t|o</em>(1:t)) + α D_(KL)![mathcal N(μ_t,Sigma_t)|mathcal N(0,I)]</p>
<ul>
<li><strong>效果</strong><br>β_t  自发趋于<strong>准二进制</strong>且<strong>与子目标切换时刻对齐</strong>，自动恢复真实分段结构；<br>采样  zsimmathcal N(0,I)  即可生成可解释的“去红色”抽象动作，且跨配置、跨时刻泛化。</li>
</ul>
<p>4. 内部强化学习：在<strong>抽象动作空间</strong>直接做 RL</p>
<p>将预训练自回归模型 + 元控制器整体视为环境，只在<strong>潜在码  z </strong> 层执行 RL：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>要素</th>
<th>原始 RL</th>
<th>内部 RL</th>
</tr>
</thead>
<tbody>
<tr>
<td>动作</td>
<td>原始动作  a_t</td>
<td>潜在抽象码  z_m （维度  ll  动作空间）</td>
</tr>
<tr>
<td>时间尺度</td>
<td>每步</td>
<td>每段子目标（切换信号  β_tgeθ  时才更新）</td>
</tr>
<tr>
<td>观测</td>
<td>原始观测  o_t</td>
<td>残差激活  e_(t,l)</td>
</tr>
<tr>
<td>探索空间</td>
<td>$T×</td>
<td>A</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><p><strong>算法</strong><br>用轻量级 SSM 策略  π(z<em>m|e</em>(1:t))  采样  z<em>m ，直到切换信号触发再采样新  z</em>(m+1) ；<br>优势估计采用<strong>无价值函数</strong>的批次归一化回报。</p>
</li>
<li><p><strong>结果</strong><br>在<strong>仅终点奖励</strong>的层次任务上，内部 RL 的样本效率比标准 RL 高出数个数量级，而</p>
</li>
<li><p>无抽象切换（ β_tequiv 1 ）</p>
</li>
<li>共训练（不冻结自回归模型）</li>
<li>CompILE 等基线<br>均无法持续获得奖励信号。</li>
</ul>
<p>总结：四步闭环解决方案</p>
<ol>
<li><strong>表示验证</strong> → 证明残差流已编码时序抽象</li>
<li><strong>线性控制</strong> → 展示可直接“拨动”子目标</li>
<li><strong>无监督发现</strong> → 自动学习抽象动作与稀疏切换</li>
<li><strong>内部 RL</strong> → 在压缩的抽象空间高效探索与利用</li>
</ol>
<p>由此，论文将“逐 token 探索”转化为“<strong>在模型内部抽象动作空间中的层次决策</strong>”，在稀疏奖励、长时域、组合泛化任务上取得 orders-of-magnitude 的效率提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“内部强化学习是否能在稀疏奖励、层次化任务中显著优于标准 RL”这一核心假设，设计了<strong>三大实验板块、共 7 组具体实验</strong>。所有实验均在两个具有<strong>组合式子目标结构</strong>的环境上完成：</p>
<ul>
<li><strong>离散 GridWorld-PinPad</strong>（7×7 网格，4 动作，访问指定颜色序列）</li>
<li><strong>连续 Ant-PinPad</strong>（MuJoCo 四足机器人，8 维扭矩，进入指定颜色区域）</li>
</ul>
<p>板块一：表示涌现验证（无干预，仅分析）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验</th>
<th>目的</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>E1. 线性探针</td>
<td>检查中间激活是否编码子目标</td>
<td>中层残差流  e_(t,l)  对当前子目标  g_t  的线性解码准确率 &gt;50%，显著高于输入层</td>
</tr>
<tr>
<td>E2. 因果干预</td>
<td>验证“插入线性控制器即可驱动子目标”</td>
<td>在中间层插入  U^((g))  后，模型能零样本完成更长、顺序未见过的任务，成功率 &gt;80%</td>
</tr>
</tbody>
</table>
</div>
<p>板块二：无监督时序抽象发现（自监督训练元控制器）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验</th>
<th>目的</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>E3. 切换对齐</td>
<td>元控制器是否自动学到“子目标切换时刻”</td>
<td>切换门  β_t  呈准二进制且与真实子目标变更时刻重合，F1 &gt; 0.95</td>
</tr>
<tr>
<td>E4. 抽象动作质量</td>
<td>潜在码  z  是否编码可迁移的“去某颜色”意图</td>
<td>在同一颜色不同布局、不同切换时刻注入对应  z ，成功率分别从 23%→36%、10%→24%</td>
</tr>
<tr>
<td>E5. 率-失真曲线</td>
<td>为何必须冻结预训练模型</td>
<td>冻结模型在曲线中出现水平平台（★点），对应子目标对齐；共训练模型则平台消失，趋于退化单切换解</td>
</tr>
</tbody>
</table>
</div>
<p>板块三：内部强化学习（稀疏奖励微调）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验</th>
<th>目的</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>E6. 主实验</td>
<td>内部 RL vs 基线</td>
<td>在仅终点奖励=1的任务上，内部 RL 在 10^5 回合内达到 &gt;70% 成功率；标准 RL、CompILE、共训练、强制每步切换等基线百万回合仍≈0</td>
</tr>
<tr>
<td>E7. 方差分析</td>
<td>为何内部 RL 更快</td>
<td>理论推导显示：内部 RL 的策略梯度方差与抽象段数 M 成比例，而标准 RL 方差**与原始步数 T 及动作维度</td>
</tr>
</tbody>
</table>
</div>
<p>补充实验（附录）</p>
<ul>
<li><strong>超参数鲁棒性</strong>：训练步数、权重衰减、观测辅助损失、专家次优性对抽象表示的影响。</li>
<li><strong>对数坐标曲线</strong>：更清晰地展示基线早期“撞运气”成功但无法利用的现象。</li>
<li><strong>可视化</strong>：残差注入后机器人轨迹实时转向，验证抽象动作即开即用。</li>
</ul>
<p>实验覆盖的核心问题</p>
<ol>
<li>自回归模型<strong>是否</strong>内部编码了时序抽象？</li>
<li>编码后<strong>能否</strong>被线性控制器直接利用？</li>
<li>无监督情况下<strong>能否</strong>自动发现这些抽象并正确分段？</li>
<li>在<strong>稀疏奖励</strong>下，相比标准 RL 是否<strong>数量级更快</strong>？</li>
</ol>
<p>全部实验答案均为<strong>肯定</strong>，形成从“表示涌现”到“控制利用”的完整证据链。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对本文框架的<strong>直接延伸</strong>或<strong>深层扩展</strong>，均围绕“内部 RL + 时序抽象”这一核心范式，分为<strong>理论、算法、模型、系统、应用</strong>五大类。</p>
<p>1. 理论层面</p>
<ul>
<li><p><strong>抽象动作的最优粒度</strong><br>给定任务分布与模型容量，是否存在<strong>最优段数 M* 与维数 n_z*</strong> 使样本复杂度最小？可用率-失真理论给出<strong>信息-效率权衡下界</strong>。</p>
</li>
<li><p><strong>内部控制器表达能力</strong><br>线性残差干预的<strong>可逼近定理</strong>：对任意策略类 Π，何时存在低秩矩阵 {U} 使得残差闭环策略密度任意接近 Π？</p>
</li>
<li><p><strong>信用分配方差的一般界</strong><br>将附录 E.2 的方差对比推广至<strong>任意抽象空间</strong>，得到</p>
</li>
</ul>
<p>mathbb V[PG<em>(∫ernal)] le (M) / (T)·(n_z) / (|A|)· mathbb V[PG</em>(raw)]</p>
<p>并验证该界在更复杂环境中的紧性。</p>
<p>2. 算法层面</p>
<ul>
<li><p><strong>自适应切换阈值 β_th</strong><br>目前 β_th 为超参；可令元控制器输出<strong>可调阈值</strong> β_th∼π_β，使 RL 自动在“探索新子目标”与“持续利用”间权衡。</p>
</li>
<li><p><strong>多层级嵌套抽象</strong><br>将本文“两段式”扩展为<strong>k 级层次</strong>：高层 z^(1) 控制中段切换，低层 z^(2) 控制微技能，形成<strong>递归元控制器</strong>。</p>
</li>
<li><p><strong>在线抽象动作发现</strong><br>当前元控制器仅在离线数据训练；可在内部 RL 阶段增加<strong>在线变分更新</strong>，使抽象动作随策略改进而演化，避免表示-策略失配。</p>
</li>
</ul>
<p>3. 模型与架构</p>
<ul>
<li><p><strong>更大规模语言模型</strong><br>将内部 RL 应用于 7B+ LLM：</p>
</li>
<li><p>是否仍保持线性可控制性？</p>
</li>
<li>是否需要<strong>稀疏混合专家（MoE）</strong> 作为控制器超网络以避免参数爆炸？</li>
<li><p><strong>视觉-语言-行动模型</strong><br>用 VLM 作为基础自回归模型，元控制器在<strong>视觉-语言残差流</strong>上操作，解决<strong>长期视觉规划</strong>任务（如“把红色块放入最远的抽屉”）。</p>
</li>
<li><p><strong>连续-离散混合抽象</strong><br>同时输出<strong>离散子目标 ID</strong> 与<strong>连续参数向量</strong>（如“走到 (x,y)”），引入<strong>混合潜在变量</strong>与相应 RL 算法。</p>
</li>
</ul>
<p>4. 系统与实现</p>
<ul>
<li><strong>高效残差流干预</strong><br>开发<strong>CUDA kernel</strong> 实现</li>
</ul>
<p>e arrow e + Ue</p>
<p>的<strong>低秩融合</strong>，在 70B 模型上做到&lt;1% 延迟开销，使内部 RL 可<strong>实时在线</strong>运行。</p>
<ul>
<li><p><strong>分布式内部 RL</strong><br>将“基础模型 + 元控制器”封装为<strong>黑箱环境</strong>，抽象动作作为<strong>RPC 接口</strong>，实现<strong>多卡并行</strong>采样，解决大规模连续控制延迟瓶颈。</p>
</li>
<li><p><strong>与现有 RL 库兼容的封装</strong><br>提供 Gymnasium/PettingZoo 风格封装，使任何标准 RL 算法（SAC、DrQ、Dreamer）可直接把<strong>z 空间</strong>当作动作空间使用。</p>
</li>
</ul>
<p>5. 应用与评估</p>
<ul>
<li><p><strong>数学推理</strong><br>在 GSM8K、MATH 等数据集上，把“证明步骤”视为子目标，用内部 RL 学习<strong>步级规划策略</strong>，对比 Chain-of-Thought 与树搜索方法。</p>
</li>
<li><p><strong>对话策略</strong><br>用 LLM 作为基础模型，元控制器输出<strong>对话目标</strong>（共情、说服、收集信息），在<strong>众包人类反馈</strong>下学习长期对话策略，避免逐 token 毒性或离题。</p>
</li>
<li><p><strong>机器人长时域操作</strong><br>在真实厨房环境中，把“拿杯子→接水→递给人”作为子目标序列，验证内部 RL 在<strong>稀疏人类奖励</strong>下是否比现有分层 RL（HIRO、DAC）样本效率更高。</p>
</li>
<li><p><strong>科学发现工作流</strong><br>将“假设生成→实验设计→数据分析”建模为抽象动作，以<strong>实验成功率</strong>为稀疏奖励，用内部 RL 自动发现<strong>新实验协议</strong>。</p>
</li>
</ul>
<p>6. 风险与理解</p>
<ul>
<li><p><strong>可解释性</strong><br>开发<strong>自动语言化</strong>抽象动作工具：给定  z ，用可解释性方法生成自然语言描述（如“向左上方移动以避开障碍”），实现<strong>人机共享策略空间</strong>。</p>
</li>
<li><p><strong>安全与对齐</strong><br>研究内部控制器是否可能触发<strong>训练分布外行为</strong>；设计<strong>约束型元控制器</strong>（如把  U  投影到安全子空间）以保证干预后输出仍符合人类价值观。</p>
</li>
</ul>
<p>一句话总结</p>
<p>从“线性控制”到“多层抽象”，从“离线发现”到“在线演进”，从“玩具环境”到“真实世界”，内部强化学习才刚刚打开<strong>在模型内部思考与行动</strong>的大门。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心贡献一句话</strong></p>
<blockquote>
<p>证明“冻结的自回归模型残差流已编码时序抽象，用无监督元控制器自动发现这些抽象，再于潜在抽象空间直接做 RL”，可在稀疏奖励、层次化任务上实现数量级的样本效率提升。</p>
</blockquote>
<p>1. 问题</p>
<ul>
<li>大模型 RL 微调仍逐 token 探索，遇<strong>稀疏奖励</strong>时信号几乎为零，难以学会需多步正确组合的任务。</li>
</ul>
<p>2. 关键观察</p>
<ul>
<li>仅做下一 token 预测的自回归模型（Transformer/SSM）<strong>内部已涌现线性可解码的子目标表示</strong>。</li>
<li>在中间层插入<strong>低秩线性矩阵</strong>  U  即可让模型执行对应子目标，无需重训练。</li>
</ul>
<p>3. 方法框架（四步）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>步骤</th>
<th>名称</th>
<th>关键输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>① 表示验证</td>
<td>线性探针 + 因果干预</td>
<td>确认残差流  e_(t,l)  编码子目标且可被线性控制器驱动</td>
</tr>
<tr>
<td>② 监督控制</td>
<td>每子目标学一个  U^((g))</td>
<td>零样本完成更长、未见过顺序的任务</td>
</tr>
<tr>
<td>③ 无监督发现</td>
<td>元控制器 = 非因果编码器 + 切换门  β_t  + 超网络解码器</td>
<td>自动恢复真实子目标分段，潜在码  z  可跨布局迁移</td>
</tr>
<tr>
<td>④ 内部 RL</td>
<td>把“基础模型+元控制器”当环境，动作空间 =  z</td>
<td>在稀疏奖励任务上 10⁵ 回合达 &gt;70% 成功率，标准 RL 百万回合≈0</td>
</tr>
</tbody>
</table>
</div>
<p>4. 实验规模</p>
<ul>
<li><strong>环境</strong>：离散 GridWorld-PinPad + 连续 MuJoCo-Ant-PinPad，均含组合式子目标。</li>
<li><strong>对比</strong>：标准 RL、GRPO、CompILE、共训练、强制每步切换等 5 条基线全部失败。</li>
<li><strong>指标</strong>：成功率、样本复杂度、切换对齐 F1、率-失真平台、策略梯度方差理论界。</li>
</ul>
<p>5. 结论与意义</p>
<ul>
<li>自回归“下一 token 预测”已隐含地学到<strong>时序抽象</strong>，无需人工定义。</li>
<li><strong>线性残差干预</strong>足以驱动长期行为，实现组合泛化。</li>
<li><strong>内部 RL</strong> 首次在冻结大模型内部表示空间完成层次强化学习，样本效率提升数个量级，为“让模型在内部思考与规划”提供了可扩展的新范式。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Emergent%20temporal%20abstractions%20in%20autoregressive%20models%20enable%20hierarchical%20reinforcement%20learning，链接是https://arxiv.org/pdf/2512.20605，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.20605。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Seijin Kobayashi,Yanick Schimpf,Maximilian Schlegel,Angelika Steger,Maciej Wolczyk,Johannes von Oswald,Nino Scherrer,Kaitlin Maile,Guillaume Lajoie,Blake A. Richards,Rif A. Saurous,James Manyika,Blaise Agüera y Arcas,Alexander Meulemans,João Sacramento</p>
<p><strong>Categories</strong>: cs.LG,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.20605.pdf">https://arxiv.org/pdf/2512.20605.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.20605">https://arxiv.org/abs/2512.20605</a></p>
<p><strong>Arxiv ID</strong>: 2512.20605</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.20605">https://papers.cool/arxiv/2512.20605</a></p>
<p><strong>Published</strong>: 2025-12-23T18:51:50Z</p>
<p><strong>Updated</strong>: 2025-12-23T18:51:50.000Z</p>
<hr>
<h3 id="3-Spatia-Video-Generation-with-Updatable-Spatial-Memory"><a href="#3-Spatia-Video-Generation-with-Updatable-Spatial-Memory" class="headerlink" title="3. Spatia: Video Generation with Updatable Spatial Memory"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.15716">Spatia: Video Generation with Updatable Spatial Memory</a></h3><p>Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model’s ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现有的视频生成模型由于视频信号的稠密、高维特性，在保持长期空间和时间一致性方面存在困难。为克服这一限制，我们提出了 Spatia，一种空间记忆感知的视频生成框架，它通过显式保留三维场景点云作为持久空间记忆。Spatia 在这种空间记忆的条件下迭代生成视频片段，并通过视觉 SLAM 持续更新该记忆。这种动态-静态解耦设计在保持模型生成逼真动态实体能力的同时，提升了整个生成过程的空间一致性。此外，Spatia 还支持如显式相机控制和三维感知交互编辑等应用，为可扩展的、基于记忆的视频生成提供了一个几何学上扎实的框架。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>长时域视频生成中的空间-时间一致性难题</strong>。现有视频生成模型因视频信号高维、稠密，难以像大语言模型那样直接缓存全部历史信息，导致：</p>
<ul>
<li>在分钟甚至小时级生成中，<strong>空间结构漂移</strong>（同一场景再次出现时几何不一致）；</li>
<li><strong>动态-静态耦合</strong>：动态实体与静态背景互相干扰，难以分别建模；</li>
<li><strong>相机控制与交互编辑</strong>缺乏显式几何依据，控制不稳定。</li>
</ul>
<p>为此，作者提出 Spatia，核心思想是：</p>
<blockquote>
<p>维护一个<strong>可更新的 3D 场景点云作为显式空间记忆</strong>，在迭代生成过程中：</p>
<ol>
<li>用该点云对新生成片段进行<strong>空间条件化</strong>，保证跨视角几何一致；</li>
<li>通过视觉 SLAM 将新内容<strong>反投影更新</strong>到点云，实现“记住”曾访问区域。</li>
</ol>
</blockquote>
<p>从而首次在<strong>保持动态实体生成能力的同时</strong>，实现长序列、可交互、可相机控制的<strong>空间一致性视频生成</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四大类，均与“长时域一致、可控制、可记忆的视频生成”密切相关：</p>
<ol>
<li>视频生成基础模型</li>
</ol>
<ul>
<li>早期 UNet 隐扩散：Stable Video Diffusion、VideoCrafter1/2</li>
<li>扩散 Transformer：DiT、HunyuanVideo、Allegro、CogVideoX、Wan2.x、Open-Sora 系列</li>
<li>自回归长视频：StreamingT2V、FIFO-Diffusion、InfinityStar、SkyReels-V2</li>
</ul>
<ol>
<li>显式相机/几何控制</li>
</ol>
<ul>
<li>隐式编码：AnimateDiff（LoRA 轨迹）、CameraCtrl、I2VControl-Camera、Direct-a-Video</li>
<li>3D-aware 条件：TrajectoryCrafter、ViewCrafter、Gen3C、Omnicam；它们用点云/深度/Plücker 坐标作为显式信号，但<strong>不维护持久记忆</strong></li>
</ul>
<ol>
<li>长时记忆与场景级一致性</li>
</ol>
<ul>
<li>静态世界生成：WonderJourney、WonderWorld、InvisibleStitch、FlexWorld、Voyager——只能生成静态 explorable scene，<strong>无法处理动态实体</strong></li>
<li>视图缓存：Context-as-Memory（按 FoV 重叠检索帧）、VMem（surfel-indexed view memory）——仍属“帧缓存”，无全局 3D 几何</li>
<li>4D/GS 流式重建：Mega、DynaGS-stream，聚焦重建而非生成</li>
</ul>
<ol>
<li>三维几何估计（供空间记忆使用）</li>
</ol>
<ul>
<li>成对+全局对齐：Dust3R、Must3R、Mast3R</li>
<li>单向前馈：MapAnything、VGGSfM、VGG-T、Fast3R、PI3——可一次性输出全局一致点云与相机位姿，为 Spatia 提供实时更新源</li>
</ul>
<p>综上，现有方法要么<strong>只能生成短时动态视频</strong>而缺乏持久空间记忆，要么<strong>只能生成静态 explorable scene</strong>而排斥动态实体；Spatia 首次将“可更新 3D 点云记忆”与“动态-静态解耦生成”结合，填补了长时域、空间一致、可交互视频生成的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“长时域空间-时间一致的视频生成”重新形式化为**“以可更新 3D 点云为持久记忆的条件生成问题”**，并给出两阶段迭代算法：</p>
<ol>
<li>记忆条件化生成<br>输入：文本指令 + 当前 3D 场景点云 + 已生成片段<br>步骤：</li>
</ol>
<ul>
<li>按用户指定相机路径，将点云渲染成<strong>2D 投影视频</strong>（含深度/颜色）</li>
<li>用视觉 SLAM 对已生成帧与点云做<strong>空间重叠检索</strong>，挑出 K 张最相关参考帧</li>
<li>将投影视频、参考帧、已生成片段、文本全部 token 化，送入<strong>多模态 Flow-Matching Transformer</strong></li>
<li>网络以“投影视频+参考帧”作为显式空间先验，生成新片段，保证<strong>同一场景再次出现时几何一致</strong></li>
</ul>
<ol>
<li>记忆更新<br>新生成帧反投影到 3D，与旧点云融合：<strong>静态区域追加/更新，动态区域被 SAM2 掩膜剔除</strong>，实现动态-静态解耦。更新后的点云作为下一次迭代的“持久记忆”。</li>
</ol>
<p>通过反复执行 1→2，Spatia 在<strong>不增加序列长度</strong>的情况下，把“看过”的空间结构固化到外部点云，从而</p>
<ul>
<li>抑制长序列几何漂移</li>
<li>支持任意时刻重新访问同一地点</li>
<li>允许用户提前编辑点云（增删物体、改颜色），实现 3D-aware 交互编辑</li>
<li>相机轨迹直接作用在点云，得到显式、几何精确的运动控制</li>
</ul>
<p>实验表明，该机制在 WorldScore 与 RealEstate 基准上同时提升<strong>静态结构一致性</strong>与<strong>动态内容质量</strong>，且随着生成片段增多，指标不衰减。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>视觉质量</strong>与<strong>记忆机制有效性</strong>两条主线展开实验，具体设置与结果如下：</p>
<ol>
<li>视觉质量评测<br>1.1 WorldScore 基准（3000 组文本/图像到视频）</li>
</ol>
<ul>
<li>对比组别<br>– 静态场景生成模型：WonderJourney、InvisibleStitch、WonderWorld、Voyager<br>– 基础视频生成模型：VideoCrafter2、EasyAnimate、Allegro、CogVideoX-I2V、Vchitect-2.0、LTX-Video、Wan2.1</li>
<li>指标：Static/Dynamic World Score、Camera Ctrl、Object Ctrl、Content Align、3D Const、Photo Const、Style Const、Subject Quality、Motion Acc/Mag/Smooth 等</li>
<li>结果：Spatia 平均得分 69.73（↑+3.6 over 最强 baseline Voyager），动态指标与静态指标同时领先，证实**“既保持空间一致，又不牺牲动态质量”**。</li>
</ul>
<p>1.2 RealEstate 测试集（100 段 720P 视频，首帧条件）</p>
<ul>
<li>指标：PSNR、SSIM、LPIPS（与真实视频比较）</li>
<li>结果：Spatia PSNR 18.58 dB、SSIM 0.646、LPIPS 0.254，三项均优于 VMem、ViewCrafter、FlexWorld、Voyager 等。</li>
</ul>
<ol>
<li>记忆机制有效性评测<br>2.1 闭环一致性（WorldScore 子集 100 样本）</li>
</ol>
<ul>
<li>任务：以初始图像生成一段相机“出去再回来”的闭环视频，比较最后一帧与首帧的偏差</li>
<li>指标：PSNRC、SSIMC、LPIPSC、Match Accuracy（RoMa 稠密匹配归一化）</li>
<li>结果：Spatia 19.38 dB / 0.579 / 0.213 / 0.698，大幅领先 Voyager、FlexWorld、ViewCrafter，验证<strong>空间记忆能“记住”曾访问视角</strong>。</li>
</ul>
<p>2.2 长时域漂移测试（RealEstate，2/4/6 段迭代）</p>
<ul>
<li>每两段做“左→右→回原位”相机往复，共 6 段 432 帧</li>
<li>指标：Camera Control 得分 + 闭环一致性指标</li>
<li>结果：随着段数增加，Wan2.2 的 PSNRC 从 13.00 dB 降到 10.74 dB；Spatia 维持在 18+ dB，<strong>无可见漂移</strong>。</li>
</ul>
<ol>
<li>消融实验<br>3.1 记忆组件消融（WorldScore 闭环）</li>
</ol>
<ul>
<li>仅投影视频 / 仅参考帧 / 两者皆用</li>
<li>结果：二者缺一不可，联合使用 Camera Control 得分从 58→84，PSNRC 从 15.55→19.38。</li>
</ul>
<p>3.2 参考帧数量 K</p>
<ul>
<li>K=1,3,5,7</li>
<li>结果：K=7 时 Match Acc 最高（0.698），再增加无收益。</li>
</ul>
<p>3.3 点云密度（RealEstate）</p>
<ul>
<li>体素边长 d=0.01,0.03,0.05,0.07 m</li>
<li>结果：d≤0.01 m 视觉质量最佳；d 增大可降存储，但 PSNR 下降 2.5 dB。</li>
</ul>
<ol>
<li>定性可视化</li>
</ol>
<ul>
<li>长时域对比：展示“w/o Ref.”、“w/o Scene”、Spatia 完整模型在 400+ 帧上的空间记忆点云，完整模型无明显几何漂移。</li>
<li>闭环生成：相机回到原点，末帧与首帧几乎重合。</li>
<li>动态-静态解耦：同一静态点云记忆下，生成不同人物/车辆动作。</li>
<li>3D-Aware 编辑：在点云中删除沙发、增加椅子、改变颜色，生成视频精确反映相应变化。</li>
</ul>
<p>综上，实验从<strong>短片段质量</strong>到<strong>长序列一致性</strong>、从<strong>定量指标</strong>到<strong>用户交互编辑</strong>，系统验证了 Spatia 在“持久空间记忆”这一核心设计上的有效性与可扩展性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可进一步挖掘，分主题列出：</p>
<p>1. 记忆表征升级</p>
<ul>
<li><strong>分层语义-几何混合记忆</strong><br>当前仅稠密点云。可引入实例级对象编码或神经辐射场（NeRF/Gaussian Splatting）作为高层语义-几何联合记忆，实现“物体-场景”双粒度一致性。</li>
<li><strong>可压缩-可检索矢量记忆</strong><br>借鉴 KV-Cache 压缩 / Memory Transformer，把点云或隐特征压缩为固定大小 token，支持<strong>小时级</strong>甚至<strong>天级</strong>持续生成而内存不爆炸。</li>
<li><strong>动态区域时序记忆</strong><br>现做法把动态实体完全剔除。可额外维护一条“动态体素-轨迹缓存”，让重复出现的角色/车辆保持<strong>动作与身份一致</strong>。</li>
</ul>
<p>2. 长程控制与规划</p>
<ul>
<li><strong>语言驱动的长程剧本生成</strong><br>结合大模型脚本规划，让系统根据自然语言自动输出多段相机路径与情节，实现“一句话生成一部连续剧”。</li>
<li><strong>闭环重拍与自动修复</strong><br>当检测到闭环误差 &gt; 阈值时，触发<strong>反向重生成</strong>（类似 Diffusion Revision），自动修复漂移区域，理论上可把误差降到零。</li>
<li><strong>多智能体交互</strong><br>在记忆内维护多个可移动对象体，支持<strong>实时交互式游戏</strong>或<strong>机器人仿真</strong>（如走进房间打开抽屉，物体状态持久保存）。</li>
</ul>
<p>3. 几何-外观联合优化</p>
<ul>
<li><strong>在线 BA + 光照估计</strong><br>当前用 MapAnything 做单次 pose+点云。可在迭代过程中加入<strong>捆集调整（BA）</strong>与<strong>光照/曝光补偿</strong>，抑制光照跳变。</li>
<li><strong>外观-几何解耦渲染条件</strong><br>把点云颜色与光照拆分为 albedo + shading，生成网络只条件于 albedo，可更鲁棒地应对昼夜、天气、风格变化。</li>
<li><strong>可变分辨率点云</strong><br>根据相机距离自适应增减点密度，近景高密、远景低密，兼顾质量与存储。</li>
</ul>
<p>4. 模型与系统效率</p>
<ul>
<li><strong>流式扩散架构</strong><br>目前仍用固定长度 clip（81/72 帧）。可探索<strong>流式 DiT</strong> 或<strong>因果 3D CNN</strong>，实现“边生成边输出”，降低延迟。</li>
<li><strong>端侧轻量化</strong><br>点云更新 + 扩散生成联合推理显存占用大。可研究：</li>
<li>低秩-稀疏混合投影</li>
<li>8-bit 点云量化</li>
<li>边缘端只存点云，云端做生成，实现移动端实时交互。</li>
<li><strong>多卡并行点云更新</strong><br>长序列下点云规模可达千万级，可引入<strong>空间哈希并行融合</strong>，提升迭代速度。</li>
</ul>
<p>5. 数据与评测</p>
<ul>
<li><strong>长时域一致基准缺失</strong><br>现有 WorldScore、RealEstate 仅数十秒。需构建<strong>分钟级</strong>带精确 pose 与实例标注的数据集，并设计“重访率、闭环误差、动态 ID 一致性”等新指标。</li>
<li><strong>自动标注管线</strong><br>利用 SLAM + 实例分割 + 追踪，对任意长视频自动产出“静态点云 + 动态轨迹 + 相机 pose”伪标签，降低人工采集成本。</li>
<li><strong>物理合理性评测</strong><br>引入刚体碰撞、地面支撑、光照一致等物理度量，防止“视觉上连贯但物理上荒谬”。</li>
</ul>
<p>6. 交互与创作应用</p>
<ul>
<li><strong>VR/AR 实时世界生成</strong><br>把 Spatia 嵌入头显，用户边走边“说出”修改（删除/添加物体），系统实时更新点云并生成匹配视频，实现<strong>即时可塑的沉浸式世界</strong>。</li>
<li><strong>Storyboard-to-Movie</strong><br>给定分镜草图+文字，自动规划多段相机路径与角色动作，输出<strong>多镜头连贯影片</strong>，为影视预览降本。</li>
<li><strong>机器人任务数据合成</strong><br>利用持久记忆保证“同一场景多次采样一致”，为 Embodied AI 提供<strong>可重复、可标注</strong>的长时仿真环境。</li>
</ul>
<p>综上，从<strong>记忆形态、控制层级、几何优化、系统效率、数据基准到交互应用</strong>，Spatia 为“具备持久 3D 记忆的视频生成”打开了多条可深入的研究路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Spatia</strong>——首个“可更新 3D 点云记忆”驱动的长时域视频生成框架，核心思想是把<strong>静态场景固化为持久点云</strong>，迭代地“条件于记忆生成 → 用新帧更新记忆”，实现动态-静态解耦、跨视角几何一致、显式相机控制与 3D 交互编辑。主要贡献与结果如下：</p>
<ol>
<li><p>问题定位<br>现有视频模型因高维稠密信号难以缓存历史，导致长序列空间漂移、同一场景再次出现时几何不一致，且缺乏显式几何控制。</p>
</li>
<li><p>方法框架</p>
</li>
</ol>
<ul>
<li><strong>两阶段迭代</strong>：<br>① 按用户文本+相机路径，将当前 3D 点云渲染成 2D 投影视频，并检索历史参考帧，共同条件化 Flow-Matching Transformer 生成新片段；<br>② 用视觉 SLAM 把新帧反投影，更新点云（动态实体被 SAM2 掩膜剔除），形成闭环记忆。</li>
<li><strong>动态-静态解耦</strong>：记忆只保存静态几何，生成网络负责动态实体，兼顾一致性与真实运动。</li>
<li><strong>3D-aware 控制</strong>：相机轨迹直接作用于点云，实现几何精确的运动与交互式场景编辑（增删改物体）。</li>
</ul>
<ol>
<li>实验验证</li>
</ol>
<ul>
<li><strong>WorldScore</strong>（3k 样本）：平均得分 69.73，领先最强 baseline 3.6 分，静态/动态指标双优。</li>
<li><strong>RealEstate</strong>（100 段）：PSNR 18.58 dB、SSIM 0.646、LPIPS 0.254，全面超越 VMem、ViewCrafter、Voyager 等。</li>
<li><strong>闭环一致性</strong>：相机“出去-返回”100 组，末帧-首帧 PSNR 19.38 dB，Match Accuracy 0.698，漂移最小。</li>
<li><strong>长时域</strong>：生成长达 6 段 432 帧，PSNR 仍保持 18+ dB，而 Wan2.2 已降至 10.7 dB。</li>
<li><strong>消融</strong>：投影视频与参考帧缺一不可；K=7 参考帧饱和；点云密度 0.01 m 为质量-存储最佳折中。</li>
</ul>
<ol>
<li>意义<br>Spatia 首次将“显式 3D 点云记忆”嵌入视频扩散模型，在<strong>不牺牲动态质量</strong>的前提下，实现分钟级空间一致、可相机控制、可交互编辑的长视频生成，为世界模型、游戏、 embodied AI 等应用提供了几何可靠、可扩展的记忆驱动生成范式。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Spatia%3A%20Video%20Generation%20with%20Updatable%20Spatial%20Memory，链接是https://arxiv.org/pdf/2512.15716，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.15716。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jinjing Zhao,Fangyun Wei,Zhening Liu,Hongyang Zhang,Chang Xu,Yan Lu</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.15716.pdf">https://arxiv.org/pdf/2512.15716.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.15716">https://arxiv.org/abs/2512.15716</a></p>
<p><strong>Arxiv ID</strong>: 2512.15716</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.15716">https://papers.cool/arxiv/2512.15716</a></p>
<p><strong>Published</strong>: 2025-12-17T18:59:59Z</p>
<p><strong>Updated</strong>: 2025-12-17T18:59:59.000Z</p>
<hr>
<h3 id="4-Schoenfeld’s-Anatomy-of-Mathematical-Reasoning-by-Language-Models"><a href="#4-Schoenfeld’s-Anatomy-of-Mathematical-Reasoning-by-Language-Models" class="headerlink" title="4. Schoenfeld’s Anatomy of Mathematical Reasoning by Language Models"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.19995">Schoenfeld’s Anatomy of Mathematical Reasoning by Language Models</a></h3><p>Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld’s Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型越来越多地暴露出推理痕迹，但其底层认知结构和步骤仍难以在表面统计之外被识别和分析。我们采用Schoenfeld的情节理论作为一种归纳式、中等规模的观察视角，并引入ThinkARM（模型推理解剖学），这是一个可扩展框架，可将推理痕迹明确抽象为功能性推理步骤，如分析、探索、实施、验证等。当应用于不同模型的数学问题求解时，这种抽象揭示了可重复的思维动态，以及推理模型与非推理模型之间的结构差异，而这些在代币级别视角下是不可见的。我们进一步呈现了两个诊断性案例研究，显示探索功能作为与正确性相关的关键分支步骤，以及以效率为导向的方法会选择性地抑制评估反馈步骤，而非统一缩短回应时间。综合来看，我们的结果表明，情节级别的表示使推理步骤明确化，从而能够系统地分析现代语言模型中推理是如何构建、稳定和变化的。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“如何系统性地揭示并比较不同大语言模型在数学推理过程中的内在认知结构”这一问题。现有评估主要关注最终答案正确率、token 长度等结果指标，无法回答：</p>
<ul>
<li>模型在推理链中到底经历了哪些功能阶段（理解、探索、执行、验证等）？</li>
<li>不同模型或不同推理范式的“思考动力学”有何差异？</li>
<li>哪些中间步骤与正确性高度相关，哪些只是冗余“过度思考”？</li>
<li>效率优化方法究竟裁剪了哪些认知环节？</li>
</ul>
<p>为此，作者将 Schoenfeld 的 Episode Theory 扩展到 LLM 场景，提出自动化的 ThinkARM 框架，把 41 万条句子级推理痕迹映射为 8 类功能 episode，从而首次在大规模、多模型、可复现的层面上，对机器推理的“解剖结构”进行定量刻画与诊断。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在认知科学、大模型推理、高效推理与推理分析四条线上梳理了相关研究，核心文献可归纳如下（按出现顺序）：</p>
<ol>
<li>数学问题解决的认知理论</li>
</ol>
<ul>
<li>Bloom 认知目标分类学：Krathwohl (2002)</li>
<li>Polya 四阶段模型：Pólya (1945)</li>
<li>Mason 等人“Thinking Mathematically”框架：Mason et al. (2010)</li>
<li>Schoenfeld Episode Theory（原始）：Schoenfeld (1985)</li>
<li>Greenes 元认知模型：Greenes (1995)</li>
<li>Kuzle 动态几何中的元认知行为：Kuzle (2013)</li>
</ul>
<ol>
<li>大模型推理增强</li>
</ol>
<ul>
<li>链式思维 CoT：Wei et al. (2023)</li>
<li>强化学习激励推理：DeepSeek-R1, Shao et al. (2024)</li>
<li>自奖励修正：Xiong et al. (2025)</li>
<li>测试时计算扩展：Snell et al. (2024), OpenAI o1 系列 (2024a,b,c)</li>
<li>过程奖励：Cui et al. (2025)</li>
<li>数据精选与指令调优：Li et al. (2024a,b; 2025a)</li>
</ul>
<ol>
<li>高效/压缩推理</li>
</ol>
<ul>
<li>过度思考现象：Chen et al. (2025b), Fan et al. (2025)</li>
<li>L1 长度约束 RL：Aggarwal &amp; Welleck (2025a,b)</li>
<li>ThinkPrune 分段裁剪：Hou et al. (2025)</li>
<li>动态分配计算：Arora &amp; Zanette (2025)</li>
<li>自适应长度惩罚：Xiang et al. (2025)</li>
<li>长度奖励塑形：Liu et al. (2025)</li>
</ul>
<ol>
<li>推理痕迹的结构与诊断</li>
</ol>
<ul>
<li>忠实度与偏差：Lanham et al. (2023), Wu et al. (2025)</li>
<li>句子级因果重要性：Bogdan et al. (2025)</li>
<li>图视角 CoT 结构：Feng et al. (2025b)</li>
<li>认知 episode 首次引入 LLM：Li et al. (2025c)</li>
<li>认知基础与 LLM 行为映射：Kargupta et al. (2025)</li>
</ul>
<p>这些研究共同构成了 ThinkARM 的理论与实验背景，但此前尚无工作将 Schoenfeld 框架大规模自动化并用于跨模型、跨效率方法的系统对比。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“理论-数据-模型-诊断”四步流水线，把“黑箱”推理链转化为可解释、可量化的 episode 级动力学：</p>
<ol>
<li><p>理论映射<br>以 Schoenfeld Episode Theory 为骨架，增补 Answer episode，形成 8 类功能标签：Read/Analyze/Plan/Implement/Explore/Verify/Monitor/Answer，实现“认知原子”定义。</p>
</li>
<li><p>大规模语料构建</p>
</li>
</ol>
<ul>
<li>从 Omni-MATH 分层采样 100 题，覆盖多领域难度。</li>
<li>15 个模型（含开源推理、蒸馏、闭源非推理、高效变体）生成 1 500 条完整回复，共 41 万句。</li>
<li>人工标注 7 067 句作为金标准，评估 GPT-4.1、GPT-5、Gemini-2.5 等自动标注器，最终选用 GPT-5（κ=0.83）完成全量 sentence-level 标注。</li>
</ul>
<ol>
<li>自动化框架 ThinkARM</li>
</ol>
<ul>
<li>分段→批处理→JSON 输出 rationale+标签，保证上下文一致性。</li>
<li>输出三类特征：<br>– 全局统计（总 token、思考比例）<br>– episode 强度（8 维 token 占比）<br>– 8×8 转移矩阵（64 维动力学）</li>
</ul>
<ol>
<li>系统诊断与案例研究</li>
</ol>
<ul>
<li>时序“心跳”模式：三阶段（初始化→执行→收敛）在所有推理模型中稳定出现。</li>
<li>模型对比：非推理模型 65 % token 集中在 Implement；推理模型 Analyze/Explore/Verify 显著激活，且存在高频 Explore↔Monitor/Verify 反馈环。</li>
<li>正确性诊断：Lasso 逻辑回归显示，Exp→Mon、Exp→Ana 等“不确定性回流”转移与正确率强正相关（β=+0.41），而持续 Explore 无后续收敛为失败信号（β=−0.54）。</li>
<li>效率方法剖析：L1、ThinkPrune 通过剪断 Verify 循环实现压缩，而 Arora&amp;Zanette 保留拓扑结构仅缩短长度，揭示“同样更短、行为大不同”。</li>
</ul>
<p>通过上述步骤，论文首次把数学推理痕迹拆解为可复现、可度量、可干预的 episode 级动力学，为模型行为比较、错误诊断与高效推理提供了统一框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否用 episode 级视角系统刻画并诊断 LLM 数学推理”这一核心问题，设计并执行了三大类、共 6 项实验：</p>
<ol>
<li>标注质量与可扩展性验证<br>1.1 人工金标准构建</li>
</ol>
<ul>
<li>从 100 题中分层抽 9 题，人工标注 7 067 句，κ=0.81 达到研究级一致性。<br>1.2 自动标注器选拔</li>
<li>候选模型：GPT-4.1、GPT-5、Gemini-2.5-Flash/Pro</li>
<li>指标：Accuracy / Cohen’s κ（推理 vs 非推理分组）</li>
<li>结果：GPT-5 综合 κ=0.83 胜出，被选为后续全量标注引擎。</li>
</ul>
<ol>
<li>Episode 模式发现实验<br>2.1 词汇-语义可分离性</li>
</ol>
<ul>
<li>对 41 万句做 token 频次统计，生成 8 张词云。</li>
<li>结果：Analyze 含“coprime、boundary”等概念词，Implement 含变量名与数值，Verify 含“wrong、double-check”等评价词，表明 episode 标签捕获到语义差异。</li>
</ul>
<p>2.2 时序动力学</p>
<ul>
<li>把每条回复归一化到 0-100 % 进度，25 等分桶，计算各 episode 相对频率。</li>
<li>结果：<br>– 三阶段“心跳”稳定出现：早期 Read/Analyze/Plan 指数衰减；中期 Implement 钟形峰值；后期 Verify 单调上升，Monitor U 型回弹。<br>– 7 个开源推理模型曲线高度重合，说明模式具有跨模型普适性。</li>
</ul>
<p>2.3 边际分配与转移拓扑</p>
<ul>
<li>统计 15 模型 8 类 episode 的 token 占比与 8×8 转移矩阵。</li>
<li>结果：<br>– 非推理模型 Implement 占比 ≥60 %；推理模型 Analyze+Explore+Verify 总和 ≈50 %。<br>– 推理模型高频循环：Exp→Mon、Ver→Exp、Mon→Ana；非推理模型以线性 Read→Plan→Implement→Answer 为主。<br>– 蒸馏系列 1.5 B–32 B 分配与教师 DeepSeek-R1 高度一致（ρ&gt;0.95），表明蒸馏可传递认知结构。</li>
</ul>
<ol>
<li>下游诊断案例研究<br>3.1 正确性预测实验</li>
</ol>
<ul>
<li>数据：5 个开源推理模型共 500 条轨迹，按正确与否平衡采样。</li>
<li>特征：全局统计 + 8 维 episode 占比 + 64 维转移频次。</li>
<li>模型：Lasso 逻辑回归（λ 经交叉验证）。</li>
<li>指标：系数显著性与符号。</li>
<li>结果：<br>– 正相关最强：Exp→Mon β=+0.41、Exp→Ana β=+0.31、Mon→Ana β=+0.28。<br>– 负相关最强：Explore 占比 β=−0.54、Exp→Ver β=−0.45、Exp→Ans β=−0.41。<br>– 解释：能把“探索”及时拉回监控或重新分析者更易对；持续空转或过早验证者易错。</li>
</ul>
<p>3.2 高效方法行为对比实验</p>
<ul>
<li>基线：R1-Distill-Qwen-1.5B</li>
<li>对比：L1、ThinkPrune、Arora&amp;Zanette 三种高效策略</li>
<li>指标：episode 占比变化、MI 排序被抑制的前 10 条 N-gram 模式</li>
<li>结果：<br>– L1 与 ThinkPrune 把 Verify 预算砍至 7 % 与 8 %，Analyze 也显著下降；Arora&amp;Zanette 保持 Verify≈10 %、Analyze≈20 %，更接近基线。<br>– MI 分析：L1 最高抑制 0.376 的 V-N-V 验证循环；Arora&amp;Zanette 最大 MI 仅 0.104，拓扑保留度最高。</li>
</ul>
<p>通过上述实验，论文既验证了 ThinkARM 的可靠性，又用 episode 视角首次定量揭示了“正确-错误”“冗余-高效”背后的认知结构差异。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 ThinkARM 框架的“下一步”，既包括理论扩展，也涵盖工程与伦理视角：</p>
<ul>
<li><strong>跨领域迁移</strong></li>
<li>将 episode 标注从数学推广到物理、化学、编程、法律、医学诊断等需要多步推理的领域，验证 8 类标签是否仍具可分离性与预测力。</li>
<li>研究领域专属 episode（如“实验”“模拟”“判例检索”）是否需要增补到基础八元组。</li>
<li><strong>细粒度时间轴与层级化结构</strong></li>
<li>子句级或 token 级标注，捕捉“一步内多认知活动并存”现象；结合句法树或注意力分布，构建层级 episode 图。</li>
<li>引入连续时间模型（点过程、神经 ODE）替代离散转移矩阵，刻画思考“停留时间”与突发加速。</li>
<li><strong>动态干预与可控生成</strong></li>
<li>在解码阶段植入“episode 控制器”：当 Explore 占比过高或 Verify 循环缺失时，触发回退或强制插入验证，实时降低错误率。</li>
<li>强化学习奖励不再以 token 长度为约束，而以 episode 比例或拓扑相似度为优化目标，实现“结构感知压缩”。</li>
<li><strong>认知科学反向验证</strong></li>
<li>采集人类做题的语音/眼动/击键数据，用同一套 episode 体系标注，比较机器与人类的阶段分布、循环频率、错误模式，检验 LLM 是否再现真实认知路径。</li>
<li>利用 EEG-fMRI 等神经影像，验证人类在 Explore→Monitor 转换时是否出现与模型注意力变化相似的脑区激活模式。</li>
<li><strong>多语言与文化差异</strong></li>
<li>考察不同语言（中文、英文、法文）推理链的 episode 分布：东方数学教育强调“审题”是否导致 Read/Analyze 占比更高；不同文化是否影响 Monitor 的表达形式与频率。</li>
<li><strong>高效推理的“结构-性能”帕累托前沿</strong></li>
<li>以 episode 转移拓扑为搜索空间，用进化算法或贝叶斯优化寻找“最短 yet 保结构”的压缩策略，建立 token 成本 vs 正确率 vs 认知完整度三维前沿。</li>
<li>分析是否存在“最小可验证环”(Minimal Verifiable Loop) 作为任何正确解都无法再压缩的拓扑下限。</li>
<li><strong>错误定位与可解释纠错</strong></li>
<li>当诊断模型预测“即将错误”时，可视化高负贡献的转移（如 Exp→Ans），自动生成解释：“模型在探索后未验证直接给出答案，建议插入 Verify 环节”。</li>
<li>结合反事实生成：仅重写错误轨迹中的关键 episode 句子，观察能否以最小改动翻转预测结果。</li>
<li><strong>安全与伦理视角</strong></li>
<li>研究 episode 分布是否会被恶意 prompt 操控（例如诱导持续 Explore 以耗尽推理预算的“减速攻击”）。</li>
<li>检验高效方法在压缩 Verify/Monitor 后，模型对社会偏见、事实错误的自我审查能力是否同步下降。</li>
<li><strong>开源基础设施</strong></li>
<li>发布多领域、多语言、带人工复核的 episode 标注数据集，配套统一评测协议（类似 GLUE/SuperGLUE 的“Episode Benchmark”）。</li>
<li>提供即插即用的 episode 标注 API 与可视化工具包，支持实时分析 HuggingFace 任意生成模型。</li>
</ul>
<p>通过上述探索，可把 ThinkARM 从“数学推理解剖镜”升级为“通用认知诊断与可控生成平台”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>一句话总结</strong><br>本文提出自动化的 <strong>ThinkARM</strong> 框架，用认知科学的 <strong>Schoenfeld Episode Theory</strong> 把 41 万句数学推理痕迹映射成 8 类功能 episode，首次在大规模、多模型场景下揭示并诊断了 LLM 推理的“解剖结构”与“动力学差异”。</p>
<p>核心内容提要</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>理论</td>
<td>将人类数学问题解决框架（Read/Analyze/Plan/Implement/Explore/Verify/Monitor/Answer）迁移到 LLM，作为中间层抽象。</td>
</tr>
<tr>
<td>数据</td>
<td>100 道 Omni-MATH 题 × 15 个模型 → 1 500 条轨迹 → 41 万句；人工标注 7 k 句金标准。</td>
</tr>
<tr>
<td>自动化</td>
<td>GPT-5 做 sentence-level 标注（κ=0.83），实现可扩展流水线。</td>
</tr>
<tr>
<td>普遍模式</td>
<td>三阶段“心跳”：早期抽象分析→中期符号执行→后期验证收敛，跨模型稳定出现。</td>
</tr>
<tr>
<td>模型差异</td>
<td>非推理模型 60 %+ token 为 Implement；推理模型均衡分配 Analyze/Explore/Verify，且存在高频 Explore↔Monitor/Verify 反馈环。</td>
</tr>
<tr>
<td>正确性诊断</td>
<td>Lasso 回归显示，把 Explore→Monitor/Analyze 的“不确定性回流”做对的轨迹更易正确；持续 Explore 无收敛是失败信号。</td>
</tr>
<tr>
<td>高效方法剖析</td>
<td>L1/ThinkPrune 剪掉 Verify 循环致拓扑断裂；Arora&amp;Zanette 保留结构仅缩短长度，揭示“同样更短、行为大不同”。</td>
</tr>
</tbody>
</table>
</div>
<p>贡献一句话</p>
<p>ThinkARM 让“黑箱”推理链首次拥有<strong>可解释、可度量、可干预</strong>的 episode 级 X 光片，为模型比较、错误诊断与高效压缩提供统一认知视角。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Schoenfeld%27s%20Anatomy%20of%20Mathematical%20Reasoning%20by%20Language%20Models，链接是https://arxiv.org/pdf/2512.19995，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.19995。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ming Li,Chenrui Fan,Yize Cheng,Soheil Feizi,Tianyi Zhou</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19995.pdf">https://arxiv.org/pdf/2512.19995.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19995">https://arxiv.org/abs/2512.19995</a></p>
<p><strong>Arxiv ID</strong>: 2512.19995</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.19995">https://papers.cool/arxiv/2512.19995</a></p>
<p><strong>Published</strong>: 2025-12-23T02:44:25Z</p>
<p><strong>Updated</strong>: 2025-12-23T02:44:25.000Z</p>
<hr>
<h3 id="5-How-Much-3D-Do-Video-Foundation-Models-Encode"><a href="#5-How-Much-3D-Do-Video-Foundation-Models-Encode" class="headerlink" title="5. How Much 3D Do Video Foundation Models Encode?"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.19949">How Much 3D Do Video Foundation Models Encode?</a></h3><p>Videos are continuous 2D projections of 3D worlds. After training on large video data, will global 3D understanding naturally emerge? We study this by quantifying the 3D understanding of existing Video Foundation Models (VidFMs) pretrained on vast video data. We propose the first model-agnostic framework that measures the 3D awareness of various VidFMs by estimating multiple 3D properties from their features via shallow read-outs. Our study presents meaningful findings regarding the 3D awareness of VidFMs on multiple axes. In particular, we show that state-of-the-art video generation models exhibit a strong understanding of 3D objects and scenes, despite not being trained on any 3D data. Such understanding can even surpass that of large expert models specifically trained for 3D tasks. Our findings, together with the 3D benchmarking of major VidFMs, provide valuable observations for building scalable 3D models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视频是三维世界的连续二维投影。在大量视频数据上训练后，全球性的三维理解会自然产生吗？我们通过量化现有视频基础模型（VidFMs）在海量视频数据上预训练的三维理解来研究这个问题。我们提出了第一个模型无关的框架，通过浅层读出从其特征中估计多个三维属性，来衡量各种VidFMs的三维感知能力。我们的研究在多个方面呈现了VidFMs三维感知的有意义发现。特别是，我们表明，尽管最先进的视频生成模型没有经过任何三维数据训练，但它们对三维物体和场景表现出强烈的理解能力。这种理解甚至可以超过专门为三维任务训练的大型专家模型。我们的研究结果，以及对主要VidFMs的三维基准测试，为构建可扩展的三维模型提供了宝贵的观察依据。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在回答一个核心问题：<br><strong>仅通过大规模 2D 视频数据训练得到的视频基础模型（VidFM）在内部究竟“理解”了多少 3D 结构？</strong></p>
<p>具体而言，论文试图量化并系统评估 VidFM 对 3D 世界的感知程度，解决以下子问题：</p>
<ul>
<li><strong>是否存在 3D 感知</strong>：VidFM 能否在无需任何 3D 监督或后优化的情况下，直接通过冻结特征提取出一致的 3D 点云、深度图与相机位姿？</li>
<li><strong>感知强度</strong>：其 3D 感知能力与专门用 3D 数据训练的专家模型相比如何？</li>
<li><strong>影响因素</strong>：时间推理、3D 微调、模型规模怎样影响 3D 感知？</li>
<li><strong>信息位置</strong>：在扩散模型中，哪一层、哪一时间步的特征最具 3D 感知？</li>
<li><strong>实用价值</strong>：在 3D 数据稀缺场景下，VidFM 特征能否替代传统图像特征，提升前馈式 3D 重建性能？</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Works”中将相关研究划分为三大主线，并指出各自与本文工作的区别。可归纳如下：</p>
<p>1. 视频基础模型（VidFMs）</p>
<ul>
<li><strong>自监督/对比学习</strong>：VideoMAE、V-JEPA、VideoCLIP 等通过掩码重构或对比目标学习时空表征。</li>
<li><strong>生成式扩散模型</strong>：Sora、CogVideoX、Open-Sora、WAN 等在大规模文本-视频对上训练，强调像素级生成质量，但内部表征是否蕴含 3D 结构未被系统研究。<br>→ <strong>区别</strong>：本文不关注生成质量，而是<strong>首次模型无关地探测冻结特征的 3D 感知能力</strong>。</li>
</ul>
<p>2. 从视频重建 3D</p>
<ul>
<li><strong>经典多视角几何</strong>：SfM、MVS 依赖特征匹配，对弱纹理、宽基线敏感。</li>
<li><strong>数据驱动前馈方法</strong>：DUST3R、MASt3R、Fast3R 等 Transformer 直接回归点云/深度，但仍需 3D 标注且难以 scaling。</li>
<li><strong>借助视频先验的微调/优化</strong>：</li>
<li>3D 控制：CameraCtrl、AC3D、CamCo 等在扩散模型中注入相机条件。</li>
<li>联合输出 3D：Matrix3D、Geo4D、Wonderland、ReconX 等微调视频扩散模型，同时生成深度/点云。</li>
<li>后优化或显式 3D 记忆：Gen3C、V3D、ViewCrafter、SpatialCrafter 等用视频模型做帧外插后再做 3D 融合。<br>→ <strong>区别</strong>：上述方法均需<strong>额外 3D 监督、后优化或任务特定工程</strong>。本文<strong>无需微调或优化</strong>，仅用<strong>冻结特征+浅层探针</strong>量化 3D 感知。</li>
</ul>
<p>3. 视觉基础模型 3D 感知量化</p>
<ul>
<li><strong>语义/一致性代理</strong>：3D-VQA、多视角识别、深度排序、Blink、SAT、Lexicon3D、WorldScore 等用语义或 2.5D 任务间接评估。</li>
<li><strong>稠密探针</strong>：</li>
<li>Probe3D 对<strong>图像</strong>模型用稠密头估计深度/法向。</li>
<li>Feat2GS 将<strong>图像</strong>特征转为 3D Gaussian Splatting。<br>→ <strong>区别</strong>：</li>
</ul>
<ol>
<li>目标模型不同——本文专注<strong>视频</strong>模型；</li>
<li>任务更直接——同时估计<strong>点云、深度、相机位姿</strong>；</li>
<li>揭示间接指标（如多视角一致性）可能<strong>高估或低估真实 3D 感知</strong>，需用直接 3D 任务评估。</li>
</ol>
<p>总结表（markdown）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>研究方向</th>
<th>代表工作</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频自监督/生成</td>
<td>VideoMAE, V-JEPA, Sora, WAN, CogVideoX</td>
<td>仅关注生成或判别性能，未系统量化 3D 感知</td>
</tr>
<tr>
<td>经典 3D 重建</td>
<td>SfM, MVS</td>
<td>依赖特征匹配，难 scaling</td>
</tr>
<tr>
<td>数据驱动前馈 3D</td>
<td>DUST3R, Fast3R</td>
<td>需 3D 标注，未利用视频先验</td>
</tr>
<tr>
<td>视频+3D 微调</td>
<td>CameraCtrl, Matrix3D, ReconX</td>
<td>需 3D 监督或后优化，非模型无关</td>
</tr>
<tr>
<td>图像 3D 探针</td>
<td>Probe3D, Feat2GS</td>
<td>仅针对图像模型，任务较间接</td>
</tr>
</tbody>
</table>
</div>
<p>因此，本文填补了“<strong>模型无关、纯冻结特征、直接 3D 任务</strong>”这一评估空白，为后续构建可扩展的 3D 世界模型提供基准与洞察。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出一个<strong>两阶段、模型无关的探测框架</strong>，在不微调任何视频基础模型（VidFM）参数的前提下，用<strong>浅层前馈探针</strong>从冻结的时空特征中直接回归 3D 属性，从而量化其 3D 感知强度。具体流程如下：</p>
<p>1. 冻结特征提取</p>
<ul>
<li><strong>输入</strong>：短视频片段  V∈R^(T× 3× H× W) 。</li>
<li><strong>扩散模型</strong>（WAN、CogVideoX 等）：</li>
<li>选取<strong>早-但-非-首个</strong>去噪时间步  τ  与<strong>中间层</strong>隐藏态作为特征图  F_t∈R^(C× H_f× W_f) 。</li>
<li>空文本嵌入，首帧作为图像条件。</li>
<li><strong>自监督/3D 专家</strong>（V-JEPA、DINOv2、Fast3R）：</li>
<li>标准前向，取<strong>最后一层</strong>空间特征。</li>
<li><strong>长视频处理</strong>：对固定短窗口模型按 stride 切分，每 chunk 复用首帧作参考，并维护帧-到-特征索引表  π(t) 。</li>
</ul>
<p>2. 浅层 3D 探针训练（唯一可优化部分）</p>
<p>2.1 数据构造</p>
<ul>
<li>每段视频抽 4 帧：帧 0 为参考，另 3 帧间隔  ≥ 5  帧。</li>
<li>用外部 VGGT 模型为全序列生成<strong>伪真值</strong>：</li>
<li>稠密点云  X_t∈R^(H× W× 3) （坐标系对齐到帧 0）</li>
<li>深度图  D_t∈R^(H× W) </li>
<li>相机位姿  P_t∈ SE(3) </li>
<li>保存置信度图用于加权损失。</li>
</ul>
<p>2.2 探针架构</p>
<ul>
<li><strong>4 层交替注意力 Transformer</strong>（VGGT-like，但极浅）：</li>
</ul>
<ol>
<li><strong>帧内自注意</strong>：同一帧 token 交互</li>
<li><strong>跨帧注意</strong>：不同帧同一位置 token 交互</li>
</ol>
<ul>
<li><strong>三个只读头</strong>（全部参数仅 4 层+3 头）：</li>
<li>Point Head：DPT 解码 → 点云  X_t </li>
<li>Depth Head：DPT 解码 → 深度  D_t </li>
<li>Camera Head：全局池化+MLP → 相对位姿  P_t </li>
</ul>
<p>2.3 多任务损失</p>
<p>L= λ<em>(±ap)L</em>(±ap)<em>(confidence-weighted )ell_2 on points + λ</em>(depth)L<em>(depth)</em>(confidence-weighted )ell<em>2 on depth + λ</em>(cam)L<em>(cam)</em>(Huber on )SE(3)<br> 全局尺度通过 Umeyama 对齐后计算误差，排除尺度歧义。</p>
<p>3. 评估协议</p>
<ul>
<li><strong>数据集</strong>：CO3Dv2（物体-centric）、DL3DV（大场景）。</li>
<li><strong>指标</strong>：</li>
<li>点云误差（×10 缩放便于阅读）</li>
<li>深度  ell_2  误差</li>
<li>相机 AUC@5°/30°（旋转+翻译同时满足阈值的比例曲线下面积）</li>
</ul>
<p>4. 系统性实验</p>
<p>4.1 Extent（感知强度）</p>
<ul>
<li>WAN2.1-14B 在 CO3Dv2 上仅次于 Fast3R；在 DL3DV（Fast3R 未训练域）<strong>全面反超</strong>。</li>
<li>Open-Sora2.0 紧随其后，证明<strong>前沿视频生成模型已内建强 3D 感知</strong>。</li>
</ul>
<p>4.2 Factor 1（时间推理）</p>
<ul>
<li>逐帧 DINOv2 深度尚可，但全局点云/位姿<strong>显著劣于</strong>任何视频模型 → <strong>时间交互是形成全局 3D 理解的关键</strong>。</li>
</ul>
<p>4.3 Factor 2（3D 微调）</p>
<ul>
<li>Aether（CogVideoX+3D 微调）在 DL3DV 上提升，但在 CO3Dv2 上略降 → <strong>3D 微调可能域内增益、域外退化</strong>。</li>
</ul>
<p>4.4 Factor 3（模型规模）</p>
<ul>
<li>WAN 1.3B→14B：点云误差 ↓23%；<br>CogVideoX 2B→5B：点云误差 ↑2% → <strong>参数增加≠必然更强 3D 感知</strong>，数据质量更重要。</li>
</ul>
<p>4.5 Localization（信息位置）</p>
<ul>
<li><strong>中间层 + 早-但-非-首个时间步</strong>在所有扩散模型中一致最优；<br>太浅层特征未形成，太深层被 RGB 合成任务“挤压”；太早步噪声不足，太晚步噪声过度。</li>
</ul>
<p>4.6 Implication（实用价值）</p>
<ul>
<li>用冻结 WAN2.1-14B 特征替换 VGGT 的 DINO 特征，训练数据<strong>不到 10%</strong> 即可超越原模型全数据性能，尤其在 DL3DV 复杂场景 → <strong>VidFM 特征在 3D 数据稀缺场景更具优势</strong>。</li>
</ul>
<p>5. 结论</p>
<p>通过<strong>统一探针+冻结特征+直接 3D 任务</strong>，论文首次量化并验证了：</p>
<blockquote>
<p>仅依赖 2D 视频大规模训练的前沿生成模型，已自发习得<strong>可媲美甚至超越 3D 专家</strong>的通用 3D 感知，且该感知可被<strong>即插即用的浅层解码器</strong>直接利用。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“视频基础模型（VidFM）到底编码了多少 3D 信息”这一核心问题，设计并执行了<strong>三大板块、十余项实验</strong>，覆盖定量基准、消融分析、数据缩放与一致性诊断。所有实验均基于同一套<strong>冻结特征 + 浅层探针</strong>协议，确保模型无关、可复现。</p>
<p>1. 主基准实验（Sec 4.2）</p>
<p><strong>目的</strong>：横向比较不同类别模型的 3D 感知强度。<br><strong>数据集</strong>：CO3Dv2（11k 视频，物体-centric）、DL3DV（6k 视频，复杂场景）。<br><strong>被测模型</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表模型</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像逐帧</td>
<td>DINOv2</td>
</tr>
<tr>
<td>自监督视频</td>
<td>V-JEPA</td>
</tr>
<tr>
<td>视频扩散（无 3D 监督）</td>
<td>CogVideoX、Open-Sora2.0、WAN2.1-14B</td>
</tr>
<tr>
<td>视频扩散（3D 微调）</td>
<td>Aether</td>
</tr>
<tr>
<td>3D 专家</td>
<td>Fast3R</td>
</tr>
</tbody>
</table>
</div>
<p><strong>指标</strong>：</p>
<ul>
<li>点云误差（↓）</li>
<li>深度 ℓ₂ 误差（↓）</li>
<li>相机位姿 AUC@5°/30°（↑）</li>
</ul>
<p><strong>关键结论</strong>：</p>
<ul>
<li>WAN2.1-14B 在 DL3DV 上<strong>全面超越</strong> Fast3R；CO3Dv2 上仅次于 Fast3R。</li>
<li>Open-Sora2.0 稳定第二梯队。</li>
<li>逐帧 DINOv2 深度尚可，但全局点云/位姿<strong>崩溃</strong>。</li>
</ul>
<p>2. 消融实验（Sec 4.3 &amp; Supp A）</p>
<p>2.1 模型规模缩放</p>
<ul>
<li><strong>WAN 系列</strong>：1.3B → 14B，点云误差 ↓23%。</li>
<li><strong>CogVideoX 系列</strong>：2B → 5B，点云误差 ↑2%（数据质量而非参数量主导）。</li>
</ul>
<p>2.2 扩散层-时间步双变量消融</p>
<ul>
<li>固定 3 层网络深度 + 4 个时间步，<strong>一致最优</strong>：<br><strong>中间层 + 早-但-非-首个时间步</strong>（τ≈0.2–0.3）。</li>
</ul>
<p>2.3 探针容量鲁棒性（Supp A）</p>
<ul>
<li>把通道数 1024→512，<strong>相对排序不变</strong>，结论稳健。</li>
</ul>
<p>3. 数据缩放实验（Sec 4.4 &amp; Supp B）</p>
<p><strong>目的</strong>：验证“3D 数据稀缺时，VidFM 特征是否更值得”。</p>
<p><strong>设置</strong>：</p>
<ul>
<li>基线：原版 VGGT（DINO 特征，端到端训练，100% 3D 数据）。</li>
<li>对比：VidFM-VGGT（冻结 WAN2.1-14B 特征，仅替换特征分支，其余相同）。</li>
</ul>
<p><strong>协议</strong>：</p>
<ul>
<li>在 CO3Dv2、DL3DV 上分别采样 {1%, 3%, 10%, 30%, 100%} 的 3D 训练视频。</li>
<li>训练至收敛，评估相同探针指标。</li>
</ul>
<p><strong>结果</strong>（Supp B 图 6）：</p>
<ul>
<li><strong>&lt;10% 数据</strong>，VidFM-VGGT 已<strong>全面超越</strong>原版 100% 数据性能。</li>
<li>在更复杂的 DL3DV 上差距更显著 → <strong>VidFM 特征对“数据+算力”双重稀缺场景极具价值</strong>。</li>
</ul>
<p>4. 多视角一致性 vs. 3D 探针诊断（Supp C）</p>
<p><strong>目的</strong>：检验“特征跨视角一致性”能否作为 3D 感知的代理指标。</p>
<p><strong>方法</strong>：</p>
<ul>
<li>用真值 3D 重投影生成跨视角对应点。</li>
<li>在特征空间做最近邻匹配，计算像素距离 → <strong>Cross-view Correspondence Error</strong>。</li>
<li>与本文 3D 探针误差散点对比。</li>
</ul>
<p><strong>发现</strong>（Supp 图 7）：</p>
<ul>
<li><strong>同一家族（视频扩散）内</strong>，两者正相关；</li>
<li><strong>跨家族不可比</strong>：DINOv2 一致性最好，但 3D 探针误差很高；Fast3R 一致性一般，却探针误差最低。<br>→ <strong>单独用多视角一致性会误判 3D 感知强度</strong>，需直接 3D 任务评估。</li>
</ul>
<p>5. 定性可视化</p>
<ul>
<li>CO3Dv2（图 3）：WAN、Open-Sora2.0、Fast3R 保留细结构（卡车抓手、椅腿）。</li>
<li>DL3DV（图 4）：DINOv2 出现整块漂移/断裂；WAN2.1-14B 点云最锐利、边界最清晰。</li>
</ul>
<p>实验全景速览表（markdown）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验板块</th>
<th>子实验</th>
<th>关键变量</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>主基准</td>
<td>横向对比</td>
<td>模型类别</td>
<td>视频生成模型 3D 感知≈甚至&gt;3D 专家</td>
</tr>
<tr>
<td>消融</td>
<td>模型规模</td>
<td>参数量</td>
<td>WAN 14B 显著↑，CogVideoX 5B 略↓</td>
</tr>
<tr>
<td>消融</td>
<td>层-时间步</td>
<td>层索引 + τ</td>
<td>中间层 + 早-非-首步最优</td>
</tr>
<tr>
<td>消融</td>
<td>探针容量</td>
<td>通道数</td>
<td>512 vs 1024 结论不变</td>
</tr>
<tr>
<td>数据缩放</td>
<td>3D 数据比例</td>
<td>{1,3,10,30,100}%</td>
<td>&lt;10% 数据即可超 DINO-100%</td>
</tr>
<tr>
<td>一致性诊断</td>
<td>多视角一致性</td>
<td>对应误差</td>
<td>与 3D 探针误差跨家族不线性相关</td>
</tr>
<tr>
<td>可视化</td>
<td>定性</td>
<td>—</td>
<td>视频生成模型细节/边界更清晰</td>
</tr>
</tbody>
</table>
</div>
<p>综上，论文通过<strong>由粗到细、由定量到定性、由主基准到诊断分析</strong>的多维实验，系统回答了“VidFM 到底编码了多少 3D”这一核心问题，并提供了可直接复用的评估协议与基准结果。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可进一步探索的方向按“数据-模型-任务-评测”四象限归纳，均围绕<strong>如何让视频基础模型（VidFM）成为可扩展的 3D 世界编码器</strong>这一终极命题展开。</p>
<p>1. 数据层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键问题</th>
<th>可能路径</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1 训练数据溯源</td>
<td>究竟是“视频数量”“场景多样性”还是“相机运动分布”主导 3D 感知？</td>
<td>构建受控子集：固定场景数，仅改变相机运动幅度或视频长度，重新预训练并探测。</td>
</tr>
<tr>
<td>1.2 动态-静态解耦</td>
<td>当前伪真值来自静态或慢速视频，运动模糊/动态物体会否误导探针？</td>
<td>引入高速相机+事件相机数据，用运动分割掩码只对静态区域计算 3D 损失。</td>
</tr>
<tr>
<td>1.3 合成-真实混合</td>
<td>纯合成数据可无限生成 3D 真值，但域迁移未知</td>
<td>在合成游戏视频上预训练，用 GAN-based 纹理随机化+Real-to-Sim 微调，探测 3D 感知迁移曲线。</td>
</tr>
</tbody>
</table>
</div>
<p>2. 模型层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键问题</th>
<th>可能路径</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.1 时空 patch 粒度</td>
<td>更大 patch→更长上下文，但是否损失精细 3D？</td>
<td>采用可变形 patch 或 foveated attention，在深度不连续区域自动提高空间分辨率。</td>
</tr>
<tr>
<td>2.2 显式 3D 归纳偏置</td>
<td>纯 Transformer 能否持续 scaling？</td>
<td>将 cross-frame attention 替换为 cost-volume 或 epipolar attention，仅对极线区域计算匹配。</td>
</tr>
<tr>
<td>2.3 多模态条件</td>
<td>文本-视频-相机参数联合训练能否提升 3D 一致性？</td>
<td>引入“相机运动描述”作为额外文本条件，构建文本↔相机运动一致性对比损失。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 任务与监督层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键问题</th>
<th>可能路径</th>
</tr>
</thead>
<tbody>
<tr>
<td>3.1 零样本度量重建</td>
<td>当前仍需 3D 伪真值，能否彻底零样本？</td>
<td>利用扩散模型自身的“视频-帧自重构”误差作为 3D 一致性奖励，进行零样本强化学习。</td>
</tr>
<tr>
<td>3.2 4D 动态建模</td>
<td>探针目前只重建静态点云，如何建模运动物体？</td>
<td>在探针中新增“运动头”预测场景流或刚性/非刚性运动分割，联合训练。</td>
</tr>
<tr>
<td>3.3 跨场景 SLAM 支持</td>
<td>VidFM 能否作为实时 SLAM 的通用特征前端？</td>
<td>把探针改为递归式，维护全局特征图，用 Neural Radiance Field 做在线闭环检测。</td>
</tr>
</tbody>
</table>
</div>
<p>4. 评测与可解释性层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键问题</th>
<th>可能路径</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1 因果干预探测</td>
<td>探针高绩效是否依赖“数据集捷径”（如车床视频固定旋转）？</td>
<td>在测试时随机打乱帧顺序或屏蔽相机运动相关 token，观察误差增幅以量化因果依赖。</td>
</tr>
<tr>
<td>4.2 特征谱分析</td>
<td>3D 信息具体分布在哪些通道？</td>
<td>对冻结特征进行 ICA/PCA，找出与深度/位姿最高相关的前 k 个成分，可视化其激活热图。</td>
</tr>
<tr>
<td>4.3 跨模态一致性</td>
<td>文本描述“绕物体 360°”是否与生成视频的真实 3D 一致？</td>
<td>构建 Text-to-Video 3D 一致性 benchmark：用文本控制相机轨迹，再用探针估计位姿，与文本解析出的轨迹比对。</td>
</tr>
</tbody>
</table>
</div>
<p>5. 系统与落地层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键问题</th>
<th>可能路径</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.1 端侧实时化</td>
<td>14B 模型无法部署到 AR/VR 头显</td>
<td>用探针中间特征做知识蒸馏，训练 &lt;1B 的“视频-3D 小模型”，在移动端 NPU 运行。</td>
</tr>
<tr>
<td>5.2 闭环数据引擎</td>
<td>能否用 VidFM+探针自动标注新视频，再反哺训练？</td>
<td>建立“探针标注→人工抽检→不确定性加权”主动学习循环，持续扩大高质量 3D 视频数据集。</td>
</tr>
</tbody>
</table>
</div>
<p>一句话总结</p>
<blockquote>
<p><strong>把 VidFM 从“会生成”推向“懂 3D”，需要同时在数据配方、模型结构、监督信号、评测维度四端发力，最终让视频模型成为可扩展、可解释、可落地的 3D 世界基础编码器。</strong></p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>一句话总结</strong><br>仅用 2D 视频训练的前沿视频生成模型，已自发习得<strong>可媲美甚至超越 3D 专家</strong>的通用 3D 感知；本文提出<strong>冻结特征+浅层探针</strong>框架，首次系统量化该能力并验证其在<strong>3D 数据稀缺场景</strong>下的实用价值。</p>
<p><strong>核心内容速览</strong></p>
<ol>
<li><p><strong>研究问题</strong><br>大规模视频数据能否让模型<strong>不依赖 3D 标注</strong>就内建全局 3D 结构理解与 ego-motion 估计？</p>
</li>
<li><p><strong>方法框架</strong></p>
</li>
</ol>
<ul>
<li><strong>两阶段</strong>：冻结 VidFM 提特征 → 4 层 Transformer 探针直接回归<strong>点云、深度、相机位姿</strong>。</li>
<li><strong>模型无关</strong>：同一探针架构适用于生成式、自监督、3D 专家等各类主干。</li>
<li><strong>零后优化</strong>：探针以外<strong>无任何微调或迭代优化</strong>。</li>
</ul>
<ol>
<li><strong>实验规模</strong></li>
</ol>
<ul>
<li>基准：CO3Dv2（物体）、DL3DV（场景）共 17k 视频。</li>
<li>对比：7 类模型，3 项误差指标+2 项 AUC。</li>
<li>消融：层-时间步、模型规模、探针容量、数据比例、多视角一致性。</li>
</ul>
<ol>
<li><strong>主要发现</strong></li>
</ol>
<ul>
<li><strong>Extent</strong>：WAN2.1-14B 在 DL3DV 全面超 Fast3R；Open-Sora2.0 紧随其后。</li>
<li><strong>Factor</strong>：时间推理必不可少；3D 微调可能域外退化；参数量≠必然更好。</li>
<li><strong>Localization</strong>：中间层 + 早-非-首时间步特征最具 3D 信息。</li>
<li><strong>Implication</strong>：用 WAN 特征替换 DINO，<strong>&lt;10% 3D 数据</strong>即可超越原版 100% 数据性能。</li>
</ul>
<ol>
<li><strong>贡献清单</strong></li>
</ol>
<ul>
<li>首个<strong>模型无关的 VidFM 3D 感知量化协议</strong>与公开基准。</li>
<li>揭示前沿视频生成器已<strong>内生强 3D 先验</strong>，可低成本迁移到下游 3D 任务。</li>
<li>提供即插即用特征与探针权重，推动<strong>视频-3D 一体化</strong>研究。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是How%20Much%203D%20Do%20Video%20Foundation%20Models%20Encode%3F，链接是https://arxiv.org/pdf/2512.19949，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.19949。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zixuan Huang,Xiang Li,Zhaoyang Lv,James M. Rehg</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19949.pdf">https://arxiv.org/pdf/2512.19949.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19949">https://arxiv.org/abs/2512.19949</a></p>
<p><strong>Arxiv ID</strong>: 2512.19949</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.19949">https://papers.cool/arxiv/2512.19949</a></p>
<p><strong>Published</strong>: 2025-12-23T00:38:52Z</p>
<p><strong>Updated</strong>: 2025-12-23T00:38:52.000Z</p>
<hr>
<h3 id="6-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation"><a href="#6-VA-π-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation" class="headerlink" title="6. VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.19680">VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</a></h3><p>Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at <a target="_blank" rel="noopener" href="https://github.com/Lil-Shake/VA-Pi">https://github.com/Lil-Shake/VA-Pi</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>自回归（AR）视觉生成依赖于分词器将图像映射到离散序列，并从中还原图像。然而，分词器的训练目标是从真实标记重建干净的图像，而AR生成器仅优化标记的可能性。这种不匹配会导致生成的标记序列解码成低质量图像，因为没有来自像素空间的直接监督。我们提出了VA-$π$，一个轻量级的后训练框架，它通过理论上合理的像素空间目标直接优化AR模型。VA-$π$将生成器-分词器对齐问题表述为变分优化，导出了一个证据下界（ELBO），统一了像素重建和自回归建模。为了在离散标记空间下优化，VA-$π$引入了一种基于强化学习的对齐策略，将AR生成器视为策略，并使用像素空间重建质量作为其内在奖励。奖励通过在教师强制下预测的标记序列重建原始图像的效果来衡量，为模型提供直接的像素级指导，而无需昂贵的自由游走采样。ELBO的正则项则作为自然的正则器，保持标记的分布一致性。VA-$π$实现了现有AR生成器的快速适配，无需重新训练分词器或外部奖励模型。仅使用1%的ImageNet-1K数据和25分钟调优，它将在LlamaGen-XXL上将FID从14.36降至7.65，IS从86.55提升至116.70，同时在GenEval的文本到图像任务中也取得了显著提升，无论是视觉生成模型（LlamaGen: 从0.306提升至0.339）还是统一多模态模型（Janus-Pro: 从0.725提升至0.744）。代码可在<a target="_blank" rel="noopener" href="https://github.com/Lil-Shake/VA-Pi">https://github.com/Lil-Shake/VA-Pi</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>自回归（AR）视觉生成模型中“令牌级优化目标”与“像素级图像质量”之间的错位</strong>这一根本问题。具体而言：</p>
<ul>
<li>现有 AR 生成流程分两阶段：</li>
</ul>
<ol>
<li>视觉 tokenizer 将图像编码为离散令牌并训练以重建干净图像；</li>
<li>AR 模型仅在令牌序列上最大化似然，<strong>缺乏像素空间监督</strong>。</li>
</ol>
<ul>
<li>结果：AR 采样出的高似然令牌序列经 tokenizer 解码后，往往落在图像流形之外，产生<strong>结构错乱或感知质量低下的“离流形”图像</strong>。</li>
<li>以往方法仅通过加噪或改进 tokenizer 来缓解，但<strong>未直接对齐令牌分布与像素空间</strong>，甚至因过度平滑而降低重建锐度。</li>
</ul>
<p>因此，论文提出 <strong>VA-π</strong>：<br>将 AR 生成器与 tokenizer 的像素重建目标<strong>统一到一个变分 ELBO 框架</strong>，利用强化学习在<strong>不重新训练 tokenizer、不依赖外部奖励模型</strong>的前提下，直接优化 AR 策略，使采样令牌序列在像素空间重建质量最大化，从而<strong>根治离流形序列的产生</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大脉络，均围绕“视觉自回归生成”与“策略/奖励优化”展开：</p>
<ol>
<li>自回归视觉生成</li>
</ol>
<ul>
<li>离散令牌化框架：VQ-VAE、VQGAN、LlamaGen、MaskGIT、Chameleon、UniTok 等，通过量化图像 token 使 AR 建模可行。</li>
<li>序列建模改进：随机顺序生成（RandAR、RANDAR）、多尺度预测（VAR）、无向量量化（FSQ）等，缓解一维排序导致的结构损失。</li>
</ul>
<ol>
<li>Tokenizer-Generator 对齐</li>
</ol>
<ul>
<li>生成器侧正则：noisy-context/scheduled sampling（ReAR、Re-tokenization）降低暴露偏差，但未直接优化像素误差。</li>
<li>Tokenizer 侧鲁棒：Post-Tokenizer、FlexTok、Alitok 等让解码器适应 AR 采样分布，却仅“容忍”离流形 token，无法阻止其产生，且易过平滑。</li>
</ul>
<ol>
<li>强化学习用于视觉生成</li>
</ol>
<ul>
<li>扩散模型 RL 微调：DPOK、T2I-R1、Step-DPO，借助人类偏好或CLIP 奖励改善文本-图像对齐。</li>
<li>AR 模型 RL 微调：AR-GRPO、SimpleAR 以外部奖励（CLIP、HPS）为优化目标，需额外 rollout 与参考模型，计算开销大。</li>
</ul>
<p>VA-π 与上述工作的本质区别：</p>
<ul>
<li>不依赖外部奖励，而是<strong>将 tokenizer 的像素重建误差本身作为内在奖励</strong>；</li>
<li>通过<strong>变分 ELBO</strong> 统一“像素重建”与“令牌似然”，用轻量级 GRPO 实现高效后训练；</li>
<li>首次在<strong>不重新训练 tokenizer、不增加推理成本</strong>的条件下，直接对齐 AR 策略与图像流形。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“令牌-像素错位”重新表述为<strong>隐变量模型下的最大似然估计问题</strong>，并推导出一个可直接优化的证据下界（ELBO）。整个解决方案分四步，形成 VA-π 框架：</p>
<ol>
<li>变分目标：把离散令牌序列 x 视为图像 I 的隐变量，定义</li>
</ol>
<p>p(I;θ,φ)=∑<em>x p</em>φ(I|x)π_θ(x)</p>
<p>借助 Jensen 不等式得到 ELBO：</p>
<p>log p(I)ge mathbb E<em>(q_psi,θ)(x|I)[log p</em>φ(I|x)]-rm KL(q<em>(psi,θ)(x|I)|π</em>θ(x))</p>
<ul>
<li>重建项：迫使 teacher-forcing 采样出的 x 经 tokenizer 解码后高保真还原 I；</li>
<li>先验正则项：令 AR 的 free-running 分布 πθ(x) 与 teacher-forcing 分布保持接近，抑制暴露偏差。</li>
</ul>
<ol>
<li>正则化实现<br>将 KL 正则转化为“带噪前缀的下一令牌预测”交叉熵损失：</li>
</ol>
<p>L<em>(rm prior)(θ)=-frac1N∑</em>(t=1)^N logπ<em>θ(x^<em>_t|tilde x^</em></em>(&lt;t)),quad tilde x^<em>sim K_xi(·|x^</em>)</p>
<p>仅需对 ground-truth token 做随机替换即可端到端训练，无需额外存储参考模型。</p>
<ol>
<li>重建奖励的 RL 优化<br>重建项不可导（量化+采样），故把“负重建误差”当作策略奖励：</li>
</ol>
<p>R(x,tilde x^<em>)=-l[L</em>(rm MSE)(hat I,I)+λ<em>p L_p(hat I,I)r],quad hat I=D(x), xsimπ</em>θ(·|tilde x^_)</p>
<p>采用 GRPO 对每组 8 条 teacher-forcing 轨迹计算组内优势，做 clipped importance sampling 更新，同时用同一批数据计算  L_(rm prior)  作为 KL 惩罚。全部梯度一次性回传，无需额外 rollout。</p>
<ol>
<li>整体算法<br>目标函数：</li>
</ol>
<p>J<em>(sf VA-)π(θ)=mathbb El[frac1G∑</em>(i=1)^Gmin!l(rho<em>i A_i,rm clip(rho_i,1!±!ε)A_ir)r]-β L</em>(rm prior)(θ)</p>
<p>其中  A_i  由  R(x_i,tilde x^*)  组内标准化得到。训练仅更新 AR 参数 θ，tokenizer 冻结；数据量≈1 % 原预训练集，8×A100 上 25 min 完成。</p>
<p>通过“ELBO 统一目标 + 像素级重建奖励 + 轻量 RL”三位一体，VA-π 在不改变推理管线的前提下，把 AR 生成器直接对齐到图像流形，显著降低 FID 并提升 IS 与文本对齐度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在**类条件生成（C2I）<strong>与</strong>文本条件生成（T2I）**两大任务上系统验证 VA-π 的有效性，并辅以消融与可视化分析。具体实验如下：</p>
<ol>
<li>类条件 ImageNet-1K 生成</li>
</ol>
<ul>
<li>基线：LlamaGen-XL（775 M）与 LlamaGen-XXL（1.4 B）</li>
<li>对比方法：<br>– AR-GRPO（外部奖励 RL）<br>– 仅后训练 tokenizer（100 步 / 10 k 步）<br>– STE 梯度回传微调 AR</li>
<li>指标：FID↓、IS↑、Precision↑、Recall↑（384→256 像素，50 k 验证集）</li>
<li>结果（无 CFG）：<br>– XXL 基线 FID 14.36 → VA-π 7.65（−47 %），IS 86.55 → 116.70（+35 %）<br>– 训练时间仅 25 min，较 AR-GRPO 快 7.5×，较 STE 快 15×</li>
<li>结果（w/ CFG）：XXL 取得 2.28 FID、273.53 IS，均优于现有最佳。</li>
</ul>
<ol>
<li>文本条件 GenEval 基准</li>
</ol>
<ul>
<li>模型：LlamaGen-XL（T2I 专用）与 Janus-Pro-1B（统一多模态）</li>
<li>指标：GenEval 六项子任务（位置、颜色、属性绑定、计数、单/双物体）及 Overall↑；额外报告 CLIP↑、HPS v2↑（DrawBench 提示）</li>
<li>结果：<br>– LlamaGen-XL：Overall 0.306 → 0.339（+0.033），优于 AR-GRPO 的 0.324<br>– Janus-Pro-1B：Overall 0.725 → 0.744（+0.019），属性绑定与双物体提升最显著<br>– 均未使用外部文本对齐奖励，证明像素级对齐已带来语义增益。</li>
</ul>
<ol>
<li>消融实验</li>
</ol>
<ul>
<li>奖励组成：仅 LMSE、仅 Lp、两者结合、再叠加 Lprior；显示缺失正则时 FID 飙升至 38+，三者联合最佳（FID 7.65）。</li>
<li>正则强度 β：CE 正则优于 KL；β=0.1 时 FID/IS 最佳，过强（1.0）会过平滑。</li>
<li>上下文噪声 ξ：T2I 任务中 ξ=0.5 在 GenEval 六项均最高，无噪声或过高均下降。</li>
</ul>
<ol>
<li>可视化与统计</li>
</ol>
<ul>
<li>KDE 与 t-SNE：VA-π 生成图像嵌入分布明显向真实流形靠拢。</li>
<li>学习曲线：100–500 步内奖励单调上升，训练稳定。</li>
<li>定性对比：VA-π 图像纹理更锐利、结构更准确， tokenizer 后训练则过度平滑（见图 6–7）。</li>
</ul>
<p>综上，实验覆盖<strong>不同规模模型、两种条件生成任务、多项指标与组件消融</strong>，一致表明 VA-π 在<strong>数据量≈1 %、计算时间≈25 min</strong> 的极端受限条件下，仍能显著提升视觉保真度与语义对齐度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可进一步挖掘，分为“理论-算法”“模型-规模”“场景-应用”三类，均围绕 VA-π 的轻量级变分 RL 范式展开：</p>
<p>理论-算法层面</p>
<ol>
<li><p>** tighter 的下界**<br>当前 ELBO 仅使用单步 teacher-forcing 作为后验；可引入多步或 learned proposal 网络，构造 Importance-Weighted ELBO 或扩散式迭代 refine，进一步收紧对数似然估计。</p>
</li>
<li><p><strong>连续-离散混合 latent</strong><br>完全离散 tokens 导致高维求和不可行。若将局部 patch 表征为连续向量、全局布局保持离散索引，可借鉴 VDVAE / DiffVQVAE 思想，使 KL 项可解析计算，减少 RL 方差。</p>
</li>
<li><p><strong>自适应奖励塑形</strong><br>目前奖励为固定 LMSE+LPIPS。可在线估计生成图像的 Rényi 熵或 SWD 距离，动态调整 λp，实现“探索-利用”自动平衡；也可引入对比式奖励，用 ImageNet 特征 bank 做无监督质量打分。</p>
</li>
<li><p><strong>方差缩减与 off-policy 升级</strong><br>GRPO 仅组内 baseline；可引入 critic（例如轻量 ViT）估计状态值，改用 GAE 或 PPO-EMA，或采用 Decision Transformer 形式的离线 RL，利用大规模未标注图像做 off-policy 预更新。</p>
</li>
</ol>
<p>模型-规模层面</p>
<ol>
<li><p><strong>Tokenizer-Generator 联合微调</strong><br>VA-π 冻结 tokenizer 以保证稳定。若采用 Straight-Through Gumbel-Softmax 或 DiffQ 可微量化，可在 ELBO 框架内同时更新 E、Q、D，实现端到端对齐；需设计梯度掩码避免 codebook 崩溃。</p>
</li>
<li><p><strong>Scaling Law 与模型大小</strong><br>目前最大 1.4 B。可在 7 B–30 B 视觉 AR 模型上验证：随着参数增加，VA-π 是否仍保持“1 % 数据+分钟级”收敛？亦或需重新调整 β、ξ 超参，探索新的 scaling coefficient。</p>
</li>
<li><p><strong>多分辨率 / 任意长宽比</strong><br>固定 384² 训练。将 ELBO 推广到可变长度序列，需对重建奖励做分辨率归一化，并引入 FPN-或 DiT-style 位置编码，考察 VA-π 对高清/全景图像的泛化能力。</p>
</li>
</ol>
<p>场景-应用层面</p>
<ol>
<li><p><strong>视频自回归生成</strong><br>将图像 tokenizer 扩展为 3D-VQGAN，帧序列作为时空 token。ELBO 重建项可加入时间一致性损失（RAFT、Flow-LPIPS），用 VA-π 解决“帧间闪烁”与“动态失真”问题。</p>
</li>
<li><p><strong>多模态统一大模型</strong><br>Janus-Pro 仅文本→图像。可推广到文本→视频、图像→文本、交错图文续写等任务，把不同模态输出均视为离散 token，统一用 ELBO 对齐到各自模态的像素/词级重建空间，实现“一个对齐目标，全模态受益”。</p>
</li>
<li><p><strong>可控生成与组合式提示</strong><br>在奖励函数里加入特定可微指标（Depth-MSE、Face-ID、CLIP-方向余弦），实现无训练语料条件下的“零样本风格化”“姿态驱动生成”；也可与 Layout-GAN 结合，把 bbox/关键点作为条件输入，考察 VA-π 对细粒度控制的收敛速度。</p>
</li>
<li><p><strong>数据高效领域适配</strong><br>目前用 1 % ImageNet/LAION-COCO。可极端到 1000 张甚至 100 张风格图片（如动漫、医学影像），研究 β-ξ 协同调度、数据增强与提示模板策略，验证 VA-π 在版权敏感或私有数据下的隐私友好微调。</p>
</li>
<li><p><strong>在线人机协同优化</strong><br>将人类在环反馈（点击、排序、轻量编辑）实时转换为像素级奖励，用 VA-π 做增量更新，实现“生成-反馈-改进”分钟级闭环，探索个性化创作助手场景。</p>
</li>
</ol>
<p>总结</p>
<p>VA-π 把“像素级对齐”转化为<strong>可微正则 + 强化奖励</strong>的轻量框架，为视觉 AR 模型提供了新的后训练范式。未来工作可在<strong>更紧的变分界、更大规模参数、更复杂模态与更极端数据场景</strong>四个维度继续推进，兼顾理论深度与落地广度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：自回归（AR）视觉生成只做“令牌似然”优化，与 tokenizer 的像素重建目标错位，导致采样序列常离图像流形，解码后视觉质量差。</li>
<li><strong>思路</strong>：把令牌序列视为图像的离散隐变量，推导像素级证据下界（ELBO），将“重建 fidelity”与“令牌先验”统一为可优化目标。</li>
<li><strong>方法（VA-π）</strong>：</li>
</ul>
<ol>
<li>用 teacher-forcing 采样作为后验，ELBO 分解为<br>– 重建项：负 MSE+LPIPS 奖励，<br>– 先验正则：带噪前缀的下一令牌交叉熵。</li>
<li>重建项不可导→转为策略奖励，采用 GRPO 做轻量 RL；全部梯度一次性回传，tokenizer 冻结。</li>
</ol>
<ul>
<li><strong>结果</strong>：仅 1 % ImageNet 数据、25 min 微调，LlamaGen-XXL FID 14.36→7.65，IS 86.55→116.70；文本生成 GenEval 提升 0.306→0.339，Janus-Pro 亦受益。无需外部奖励模型或昂贵 rollout，训练成本降低 86 %。</li>
<li><strong>意义</strong>：给出“像素感知对齐”的通用后训练范式，可快速移植到任意 AR 视觉/多模态模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VA-%24%CF%80%24%3A%20Variational%20Policy%20Alignment%20for%20Pixel-Aware%20Autoregressive%20Generation，链接是https://arxiv.org/pdf/2512.19680，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.19680。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xinyao Liao,Qiyuan He,Kai Xu,Xiaoye Qu,Yicong Li,Wei Wei,Angela Yao</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.19680.pdf">https://arxiv.org/pdf/2512.19680.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.19680">https://arxiv.org/abs/2512.19680</a></p>
<p><strong>Arxiv ID</strong>: 2512.19680</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.19680">https://papers.cool/arxiv/2512.19680</a></p>
<p><strong>Published</strong>: 2025-12-22T18:54:30Z</p>
<p><strong>Updated</strong>: 2025-12-22T18:54:30.000Z</p>
<hr>
<h3 id="7-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training"><a href="#7-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training" class="headerlink" title="7. GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.13043">GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</a></h3><p>Multi-turn reinforcement learning (RL) for multi-modal agents built upon vision-language models (VLMs) is hampered by sparse rewards and long-horizon credit assignment. Recent methods densify the reward by querying a teacher that provides step-level feedback, e.g., Guided Thought Reinforcement (GTR) and On-Policy Distillation, but rely on costly, often privileged models as the teacher, limiting practicality and reproducibility. We introduce GTR-Turbo, a highly efficient upgrade to GTR, which matches the performance without training or querying an expensive teacher model. Specifically, GTR-Turbo merges the weights of checkpoints produced during the ongoing RL training, and then uses this merged model as a “free” teacher to guide the subsequent RL via supervised fine-tuning or soft logit distillation. This design removes dependence on privileged VLMs (e.g., GPT or Gemini), mitigates the “entropy collapse” observed in prior work, and keeps training stable. Across diverse visual agentic tasks, GTR-Turbo improves the accuracy of the baseline model by 10-30% while reducing wall-clock training time by 50% and compute cost by 60% relative to GTR.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于视觉-语言模型（VLM）的多模态智能体的多轮强化学习（RL）受到奖励稀疏和长期信用分配的限制。近期的方法通过查询提供逐步反馈的教师来密集化奖励，例如 Guided Thought Reinforcement（GTR）和 On-Policy Distillation，但这些方法依赖成本高昂且通常具有特权的教师模型，限制了其实用性和可复现性。我们提出了 GTR-Turbo，这是 GTR 的高效升级版，无需训练或查询昂贵的教师模型即可达到相同性能。具体而言，GTR-Turbo 将在进行中的 RL 训练过程中产生的检查点权重进行合并，然后使用合并后的模型作为“免费”教师，通过监督微调或软 logits 蒸馏指导后续的 RL。这一设计消除了对特权 VLM（例如 GPT 或 Gemini）的依赖，缓解了先前工作中观察到的“熵塌陷”问题，并保持了训练的稳定性。在各种视觉智能任务中，GTR-Turbo 提高了基线模型 10-30% 的准确率，同时相较于 GTR 将墙钟训练时间减少 50%，计算成本降低 60%。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>多轮强化学习（RL）训练视觉-语言模型（VLM）智能体时因奖励稀疏与长程信用分配困难导致的“思维崩溃”（thought collapse）</strong>问题，同时<strong>消除对昂贵、高门槛外部教师模型（如 GPT-4o、Gemini）的依赖</strong>。<br>具体而言，既有方法 Guided Thought Reinforcement（GTR）通过引入外部 VLM 教师为每一步推理提供稠密反馈，显著缓解崩溃现象，但带来了<strong>高昂的 API 调用成本、训练时间拉长、模型不可访问或数据安全受限</strong>等 scalability 瓶颈。</p>
<p>GTR-Turbo 的核心贡献是**“把 RL 训练过程中自然产生的历史 checkpoint 合并成一位免费、自给自足的教师”**，用合并权重后的模型替代昂贵外部教师，为当前智能体提供两种形式的逐步推理指导：</p>
<ol>
<li>基于 SFT 的在线模仿学习；</li>
<li>基于反向 KL 散度的软 logit 蒸馏。</li>
</ol>
<p>由此，在<strong>不训练、不调用任何外部大模型</strong>的前提下，实现与 GTR 相当甚至更高的最终性能，同时<strong>训练时间缩短 50%，计算开销降低 60%</strong>，为复杂视觉交互任务中的 VLM 智能体后训练提供了一种<strong>高效、可复现、完全自包含</strong>的新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大主线，均与“多轮 VLM 智能体强化学习”“过程级稠密奖励”以及“模型权重合并”直接相关。以下按主题梳理代表性文献（仅列关键出处，按发表年排序）。</p>
<p>1. 多轮 LLM/VLM 智能体强化学习</p>
<ul>
<li><strong>PPO/GRPO/DAPO 系列</strong></li>
<li>Schulman et al. 2017 —— 原始 PPO 算法。</li>
<li>Shao et al. 2024 (DeepSeek-Math) —— GRPO，针对长链推理的组相对优化。</li>
<li>Yu et al. 2025 (DAPO) —— 大规模 LLM 强化学习系统，强调稳定与效率。</li>
<li><strong>纯 RL 后训练 VLM 决策智能体</strong></li>
<li>Zhai et al. 2025 (RL4VLM) —— 首次将原始 PPO 直接用于 VLM 视觉决策，提供基准框架。</li>
<li>Wei et al. 2025 (GTR) —— 引入“思维修正”机制，用外部 GPT-4o 提供逐步指导，缓解思维崩溃。</li>
<li><strong>并行/后续多轮 RL 系统</strong></li>
<li>Wang et al. 2025 (RAGEN) —— 分析 LLM 智能体在多轮 RL 中的自演化行为。</li>
<li>Fu et al. 2025 (AREAL) —— 异步大规模语言推理 RL 系统。</li>
<li>Li et al. 2025 —— GUI 智能体多轮 RL 的解耦训练与自适应数据策划。</li>
<li>Wang et al. 2025 (VaGen) —— 引入世界模型提供未来信息，辅助信用分配。</li>
</ul>
<p>2. 过程级稠密奖励 / 过程监督</p>
<ul>
<li><strong>过程奖励模型（PRM）</strong></li>
<li>Uesato et al. 2022；Lightman et al. 2023 —— 用人工标注训练逐步奖励模型。</li>
<li><strong>自动过程标签/LLM-as-a-Judge</strong></li>
<li>Gao et al. 2024 —— 用 LLM 生成自然语言批评信号。</li>
<li>Xia et al. 2024 —— 数学领域自动过程标签。</li>
<li><strong>隐式/无标过程奖励</strong></li>
<li>Yuan et al. 2024 —— 无需过程标签即可分解最终奖励。</li>
<li>Cui et al. 2025 (Process Reinforcement via Implicit Rewards) —— 隐式过程奖励。</li>
<li><strong>信用分配与熵崩溃分析</strong></li>
<li>Cui et al. 2025 (The Entropy Mechanism of RL for Reasoning LLMs) —— 解释熵崩溃现象，与本文“思维崩溃”呼应。</li>
</ul>
<p>3. 模型权重合并（Model Merging）</p>
<ul>
<li><strong>基础方法</strong></li>
<li>Ilharco et al. 2023 (Task Arithmetic) —— 任务向量加减。</li>
<li>Matena &amp; Raffel 2022 (Fisher Merging) —— 用 Fisher 信息加权平均。</li>
<li>Yadav et al. 2023 (TIES) —— 修剪-符号选举-选择性平均，缓解参数冲突。</li>
<li>Yu et al. 2024 (DARE) —— 随机丢弃+重缩放，实现多任务模型融合。</li>
<li><strong>Checkpoint 时间集成 / 指数平均</strong></li>
<li>Huang et al. 2017 (Snapshot Ensembles) —— 周期保存 checkpoint 做集成。</li>
<li>Li et al. 2025 (Temporal Sampling for Forgotten Reasoning) —— 重采样历史 checkpoint 对抗遗忘。</li>
<li>Li et al. 2025 (Model Merging in Pre-training) —— 预训练阶段大模型 checkpoint 合并。</li>
</ul>
<p>小结</p>
<ul>
<li><strong>RL4VLM</strong> 提供“纯稀疏奖励+RL”基线；</li>
<li><strong>GTR</strong> 引入“外部教师逐步指导”但代价高昂；</li>
<li><strong>过程监督/PRM/隐式奖励</strong> 系列尝试稠密化奖励；</li>
<li><strong>TIES/DARE/EMA</strong> 等合并技术为 GTR-Turbo 的“免费教师”奠定方法论基础。</li>
</ul>
<p>GTR-Turbo 将后两条技术路线结合，首次<strong>把 checkpoint 合并用作在线教师</strong>，在无需任何外部模型条件下实现高效、可扩展的多轮 VLM 智能体强化学习。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 GTR-Turbo 框架，把“训练过程中自然产生的历史 checkpoint”变成一位<strong>零额外训练、零 API 调用</strong>的教师，用两种轻量级方式向当前智能体提供逐步推理反馈，从而<strong>同时解决稀疏奖励导致的思维崩溃与外部教师昂贵不可扩展</strong>两大痛点。核心思路与实现步骤如下：</p>
<p>1. 免费教师的产生：Checkpoint 合并</p>
<ul>
<li>每轮 PPO 更新后保存权重，维护一个<strong>滚动缓存</strong>  C=π<em>θ^((1)),dots,π</em>θ^((k-1)) 。</li>
<li>采用 <strong>TIES-Merging</strong>（Trim-Elect-Select）对缓存模型做参数级合并：</li>
</ul>
<ol>
<li>修剪：仅保留幅度 top-20 % 的参数更新，消除冗余噪声；</li>
<li>符号选举：对同一参数在多 checkpoint 中的正负方向投票，确定“共识符号”；</li>
<li>选择性平均：只合并与共识符号一致的参数，抑制冲突。</li>
</ol>
<ul>
<li>得到<strong>零推理成本</strong>的“合并教师”  π_(merged)^((k)) ，经验证其性能与稳定性均优于当前训练模型（Figure 2）。</li>
</ul>
<p>2. 思路一：SFT 式在线模仿（GTR-Turbo-SFT）</p>
<ul>
<li>并行跑两条前向：<br>– 训练模型  π<em>θ  生成 <code>(thought, action)</code>；<br>– 合并教师  π</em>(merged)  生成参考 <code>thought*</code>。</li>
<li>把 <code>(obs, thought*)</code> 对存入<strong>轻量级 thought 缓存</strong>  D ；PPO 更新时额外最小化</li>
</ul>
<p>L<em>(SFT) = -∑_l logπ</em>θ!(th^<em>[l]mid obs, th^</em>[&lt;l])</p>
<p>与原始 PPO 损失联合优化。</p>
<ul>
<li>作用：在<strong>早期知识注入阶段</strong>快速复制教师的高质量推理模式，抑制崩溃。</li>
</ul>
<p>3. 思路二：软 Logit 蒸馏（GTR-Turbo-KL）</p>
<ul>
<li>当智能体已具备基础能力后，<strong>过度强制的硬模仿会限制探索</strong>。改为在奖励层注入<strong>反向 KL 散度</strong>：</li>
</ul>
<p>RevKL(π<em>θ,π</em>(merged);th) = E<em>l![logπ</em>θ(th[l]midth[&lt;l]){π_(merged)(th[l]midth[&lt;l])}]</p>
<ul>
<li>将 <strong>负 KL 作为附加奖励</strong>（clip 负值保证非负），与环境稀疏奖励一起送入 PPO：</li>
</ul>
<p>r’ = r - β·RevKL,quad β=1</p>
<ul>
<li>仅<strong>一次前向</strong>即可计算整句 KL，无需额外缓存，显存占用更低；同时保留 token 级概率分布信息，<strong>几乎不可被 hack</strong>。</li>
</ul>
<p>4. 训练流程伪代码（KL 版示例）</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">for k = 1…K</span><br><span class="line">π_merged ← TIES-Merge(C) // 免费教师</span><br><span class="line">B ← ∅</span><br><span class="line">while |B| &lt; batch_size</span><br><span class="line">(th, a) ← πθ(o)</span><br><span class="line">RevKL ← KL(πθ(th), π_merged(th))</span><br><span class="line">r ← env.step(a) − β·RevKL // 合并奖励</span><br><span class="line">B ← B ∪ (o, a, r, o&#x27;)</span><br><span class="line">θ ← PPO-Update(B)</span><br><span class="line">C ← C ∪ θ // 更新缓存</span><br></pre></td></tr></table></figure>
<p>5. 效果与开销</p>
<ul>
<li><p><strong>Points24 &amp; ALFWorld</strong> 两项视觉决策任务上，GTR-Turbo-KL 相比原始 GTR：<br>– 成功率提升 <strong>+9 %</strong>；<br>– 训练时间 <strong>−50 %</strong>；<br>– 额外开销 <strong>−60 %</strong>（零 API 费，仅多 1 张 GPU 做教师推理）。</p>
</li>
<li><p><strong>消融验证</strong>：<br>– TIES 合并 &gt; 线性平均；<br>– 仅指导 thought、不指导 action 时探索最充分；<br>– KL-clip 估计器在稳定性与峰值性能间折中最佳；<br>– EMA 权重 α=0.5 可进一步提升早期收敛。</p>
</li>
</ul>
<p>结论</p>
<p>GTR-Turbo 用“历史 checkpoint 合并”这一<strong>零成本操作</strong>取代昂贵外部教师，通过<strong>SFT 早期知识注入 + KL 后期稳定探索</strong>的两段式策略，在完全自包含、可本地部署的条件下，同时<strong>消除思维崩溃</strong>与<strong>可扩展性瓶颈</strong>，为复杂多轮视觉智能体强化学习提供了实用新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在两大视觉-语言智能体基准上开展了系统实验，覆盖<strong>训练曲线、最终性能、开销对比与消融分析</strong>四个层次，具体设置与结果如下。</p>
<p>1. 实验环境</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>环境</th>
<th>特点</th>
<th>奖励</th>
<th>训练步数</th>
</tr>
</thead>
<tbody>
<tr>
<td>Points24</td>
<td>纯视觉扑克牌识别→算术推理，10+ 步，需细粒度数字/运算符选择</td>
<td>稀疏：完成 24 得 +10，非法 −1，其余 0</td>
<td>30 000</td>
</tr>
<tr>
<td>ALFWorld</td>
<td>家庭场景多步 embodied 任务，50+ 步，20+ 类动作，仅 RGB 图像输入（文本描述被移除）</td>
<td>稀疏：子目标 +1，最终目标 +50，非法 −1</td>
<td>20 000</td>
</tr>
</tbody>
</table>
</div>
<p>2. 主实验：性能对比</p>
<p>2.1 Points24</p>
<ul>
<li><p><strong>训练曲线</strong>（Figure 4）<br>RL4VLM → 思维崩溃，成功率跌至 0；<br>GTR（GPT-4o 教师）→ 初期快速提升，后期停滞；<br><strong>GTR-Turbo-KL</strong> → 全程稳定上升，最终成功率 <strong>53.5 %</strong>，<strong>超 GTR 9 %</strong>。</p>
</li>
<li><p><strong>终局指标</strong>（Table 2）</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>SR(%)</th>
<th>Episode Return</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o + Tool</td>
<td>13.5</td>
<td>−3.59</td>
</tr>
<tr>
<td>GTR</td>
<td>44.5</td>
<td>0.53</td>
</tr>
<tr>
<td>GTR-Turbo-KL</td>
<td>53.5</td>
<td>2.39</td>
</tr>
</tbody>
</table>
</div>
<p>2.2 ALFWorld</p>
<ul>
<li><p><strong>训练曲线</strong>（Figure 5）<br>RL4VLM 再次崩溃；GTR 靠外部知识早期占优；<strong>GTR-Turbo-KL 仅依赖自我探索</strong>，最终成功率 <strong>15 %</strong>，与 GTR 持平。</p>
</li>
<li><p><strong>分任务成功率</strong>（Table 3）<br>在 Pick、Clean、Heat 等 7 类子任务上，GTR-Turbo-KL 与 GTR 差距 &lt;1 %，但<strong>零 API 调用</strong>。</p>
</li>
</ul>
<p>3. 开销对比（Table 4）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>环境</th>
<th>方法</th>
<th>成功率</th>
<th>训练时间</th>
<th>额外开销</th>
</tr>
</thead>
<tbody>
<tr>
<td>Points24</td>
<td>GTR (GPT-4o)</td>
<td>41 %</td>
<td>191 h</td>
<td>307 USD</td>
</tr>
<tr>
<td>GTR-Turbo-KL</td>
<td>54 %</td>
<td>89 h</td>
<td>115 USD（仅 1 张 GPU 租赁）</td>
</tr>
<tr>
<td>ALFWorld</td>
<td>GTR (GPT-4o)</td>
<td>16 %</td>
<td>164 h</td>
<td>146 USD</td>
</tr>
<tr>
<td>GTR-Turbo-KL</td>
<td>15 %</td>
<td>78 h</td>
<td>101 USD</td>
</tr>
</tbody>
</table>
</div>
<p>→ 训练时间 <strong>−50 %</strong>，额外费用 <strong>−60 %</strong>。</p>
<p>4. 消融实验</p>
<p>4.1 合并算法有效性（Figure 6）</p>
<ul>
<li><strong>TIES vs 线性平均</strong><br>TIES 成功率 <strong>&gt; 线性平均 8 %</strong>，验证去除参数冲突的必要性。</li>
</ul>
<p>4.2 指导范围（Figure 7）</p>
<ul>
<li>仅指导 <strong>thought</strong> → 最佳；</li>
<li>指导 <strong>thought+action</strong> → 探索受限，性能降 10 %。</li>
</ul>
<p>4.3 KL 估计方法（Figure 8）</p>
<ul>
<li>负值 clip、abs、K3、forward-KL 均有效；</li>
<li><strong>clip 法</strong> 稳定性与峰值兼顾，最终 SR <strong>&gt; 其他估计器 3-5 %</strong>。</li>
</ul>
<p>4.4 权重分配策略（Figure 9）</p>
<ul>
<li><strong>SMA（均匀平均）</strong> 已强劲；</li>
<li><strong>EMA-α=0.5</strong> 可再提升早期收敛；α=0.2 或 0.8 均因偏差或过短记忆而下降。</li>
</ul>
<p>5. 扩展验证</p>
<ul>
<li><strong>Qwen3-VL-8B</strong> 直接在 ALFWorld 上运行 GTR-Turbo-KL，无需 SFT 初始化，成功率 <strong>&gt; Qwen2.5-VL-32B 4× 大模型</strong>（Figure 12），证明框架对更强基模型的兼容性。</li>
</ul>
<p>结论</p>
<p>实验全面覆盖<strong>性能、效率、成本、组件敏感性</strong>；GTR-Turbo 在<strong>零外部教师</strong>条件下取得<strong>SOTA 或持平效果</strong>，同时<strong>训练时间减半、开销减 60 %</strong>，验证了“合并 checkpoint 做免费教师”这一核心设想的有效性与可扩展性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为<strong>理论、算法、系统与应用</strong>四个层面。</p>
<p>1. 理论层面</p>
<ul>
<li><strong>收敛性与误差界</strong><br>给出合并教师  π_(merged)  与最优策略  π^*  的性能差距上界，量化 TIES 合并误差、EMA 衰减系数  α  对收敛速度的影响。</li>
<li><strong>模式寻求 vs 模式覆盖</strong><br>反向 KL 的“模式寻求”特性在何种 MDP 结构下可能欠探索？能否用  α -divergence 或  f -divergence 族自适应切换寻求/覆盖行为。</li>
<li><strong>熵崩溃阈值</strong><br>建立“思维熵”与稀疏奖励之间的相变模型，预测何时必须引入外部知识，何时可完全自给。</li>
</ul>
<p>2. 算法层面</p>
<ul>
<li><strong>自适应合并周期</strong><br>当前每轮都合并；可学习动态触发条件（性能停滞、KL 散度突增等），减少 GPU 峰值占用。</li>
<li><strong>分层或模块级合并</strong><br>仅合并 MLP、attention 或 thought-head 等子模块，保留任务相关特异性，缓解多任务负迁移。</li>
<li><strong>多教师集成</strong><br>维护多个不同  α  的 EMA 教师，按 uncertainty 或梯度方向加权，实现“集成蒸馏”降低方差。</li>
<li><strong>KL 奖励塑形自动化</strong><br>用元梯度在线调节  β  系数，平衡环境奖励与蒸馏奖励，避免手工调参。</li>
<li><strong>前向-反向 KL 混合</strong><br>前期用 forward KL 鼓励覆盖，后期切换 reverse KL 精准对齐，或采用  λ -schedule 连续插值。</li>
</ul>
<p>3. 系统与规模</p>
<ul>
<li><strong>更大模型</strong><br>在 30 B+ 规模验证合并教师是否仍优于外部大模型；研究参数量与合并缓存大小之间的 Scaling Law。</li>
<li><strong>多模态大模型（MLLM）→ 多智能体</strong><br>将 GTR-Turbo 扩展到多智能体协作场景，每个 agent 维护本地合并教师，再周期性全局平均。</li>
<li><strong>异构教师</strong><br>允许缓存来自不同架构（CNN、 diffusion planner、 symbolic solver）的权重，研究异构模型合并的上界与协议。</li>
<li><strong>端-云协同</strong><br>边缘设备只保留轻量 EMA 教师，云端定期执行 TIES 合并后下发，实现低功耗在线演化。</li>
</ul>
<p>4. 应用与场景</p>
<ul>
<li><strong>无奖环境</strong><br>在完全没有环境奖励、只有人类二值成功标签的任务（如复杂 GUI 操作）中，探索纯 KL 蒸馏能否自我提升。</li>
<li><strong>连续控制</strong><br>把 thought 定义为高层子目标序列，动作为低层连续扭矩，验证合并教师是否适用于分层强化学习。</li>
<li><strong>真实机器人</strong><br>在实体抓取、导航任务中测试：合并教师能否降低真实交互次数，解决 sim-to-real 差距。</li>
<li><strong>安全与对齐</strong><br>研究合并过程中是否会放大早期 checkpoint 的偏见或有害行为，引入安全约束合并（Safe-TIES）。</li>
<li><strong>数据隐私场景</strong><br>联邦 RL 下各客户端仅共享权重差值，用 TIES 合并生成全局教师，实现“私有数据不出域”的自演化。</li>
</ul>
<p>5. 评估与基准</p>
<ul>
<li><strong>新任务套件</strong><br>构建更长周期（&gt;100 步）、部分可观测、奖励更稀疏的基准，检验方法极限。</li>
<li><strong>可解释性工具</strong><br>可视化合并前后 attention 模式的变化，理解“教师知识”在几何空间中的迁移路径。</li>
<li><strong>自动化度量</strong><br>提出“思维多样性指数”“合并教师-学生一致性曲线”等自动指标，替代人工看曲线调参。</li>
</ul>
<p>一句话总结</p>
<p>GTR-Turbo 把“自己过去的历史”变成老师，但<strong>如何更聪明地合并、何时合并、合并谁、合并后如何理论保证</strong>，仍是值得长期深挖的开放问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>GTR-Turbo</strong>，一种<strong>零外部教师、零 API 调用</strong>的多轮视觉-语言智能体强化学习框架，核心思想是：</p>
<blockquote>
<p><strong>把 RL 训练过程中自然产生的历史 checkpoint 合并成一个“免费教师”</strong>，用其持续为当前智能体提供逐步推理反馈，从而<strong>同时解决稀疏奖励导致的“思维崩溃”与昂贵外部模型带来的可扩展性瓶颈</strong>。</p>
</blockquote>
<p>1. 关键动机</p>
<ul>
<li>现有 GTR 方法依赖 GPT-4o 等昂贵模型在线修正思维，<strong>训练慢、开销高、模型不可访问</strong>。</li>
<li>稀疏奖励下， vanilla PPO 易出现<strong>思维崩溃</strong>（输出重复、模板化，性能骤降）。</li>
</ul>
<p>2. 方法概览</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>步骤</th>
<th>操作</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>① 缓存</td>
<td>每轮 PPO 后保存权重</td>
<td>获得滚动 checkpoint 池</td>
</tr>
<tr>
<td>② 合并</td>
<td>TIES（修剪-符号选举-选择性平均）</td>
<td>消除参数冲突，得到更强、更稳定的 π_merged</td>
</tr>
<tr>
<td>③ 指导</td>
<td>两条可选路径： - SFT：模仿教师 thought 令牌（早期快速注入知识） - KL：负反向 KL 散度作为附加奖励（后期稳定探索）</td>
<td>全程无需外部模型，零 API 费用</td>
</tr>
</tbody>
</table>
</div>
<p>3. 主要结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>环境</th>
<th>指标</th>
<th>GTR</th>
<th>GTR-Turbo-KL</th>
<th>提升/节省</th>
</tr>
</thead>
<tbody>
<tr>
<td>Points24</td>
<td>成功率</td>
<td>44.5 %</td>
<td>53.5 %</td>
<td>↑ 9 %</td>
</tr>
<tr>
<td>ALFWorld</td>
<td>成功率</td>
<td>16 %</td>
<td>15 %</td>
<td>持平</td>
</tr>
<tr>
<td>训练时间</td>
<td>两者</td>
<td>191 h / 164 h</td>
<td>89 h / 78 h</td>
<td>↓ 50 %</td>
</tr>
<tr>
<td>额外开销</td>
<td>API 费</td>
<td>307 USD / 146 USD</td>
<td>115 USD / 101 USD</td>
<td>↓ 60 %</td>
</tr>
</tbody>
</table>
</div>
<p>4. 消融亮点</p>
<ul>
<li>TIES 合并 &gt; 线性平均；仅指导 <strong>thought</strong> 保留探索；KL-clip 估计器最稳定。</li>
<li>无需任何外部教师即可<strong>超越原 GTR 性能</strong>，且<strong>训练时间减半、成本减 60 %</strong>。</li>
</ul>
<p>5. 一句话总结</p>
<p>GTR-Turbo 用“自己过去的历史”免费造出一位持续进化的教师，<strong>让 VLM 智能体在稀疏奖励、长轮次、复杂视觉环境中实现高效、自给自足、可扩展的强化学习</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是GTR-Turbo%3A%20Merged%20Checkpoint%20is%20Secretly%20a%20Free%20Teacher%20for%20Agentic%20VLM%20Training，链接是https://arxiv.org/pdf/2512.13043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.13043。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tong Wei,Yijun Yang,Changhao Zhang,Junliang Xing,Yuanchun Shi,Zongqing Lu,Deheng Ye</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.13043.pdf">https://arxiv.org/pdf/2512.13043.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.13043">https://arxiv.org/abs/2512.13043</a></p>
<p><strong>Arxiv ID</strong>: 2512.13043</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.13043">https://papers.cool/arxiv/2512.13043</a></p>
<p><strong>Published</strong>: 2025-12-15T07:11:56Z</p>
<p><strong>Updated</strong>: 2025-12-15T07:11:56.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202512/2025-12-27/">https://wdndev.github.io/daily/hf/202512/2025-12-27/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Latent-Implicit-Visual-Reasoning"><span class="toc-text">1. Latent Implicit Visual Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Emergent-temporal-abstractions-in-autoregressive-models-enable-hierarchical-reinforcement-learning"><span class="toc-text">2. Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Spatia-Video-Generation-with-Updatable-Spatial-Memory"><span class="toc-text">3. Spatia: Video Generation with Updatable Spatial Memory</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Schoenfeld%E2%80%99s-Anatomy-of-Mathematical-Reasoning-by-Language-Models"><span class="toc-text">4. Schoenfeld’s Anatomy of Mathematical Reasoning by Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-How-Much-3D-Do-Video-Foundation-Models-Encode"><span class="toc-text">5. How Much 3D Do Video Foundation Models Encode?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-VA-%CF%80-Variational-Policy-Alignment-for-Pixel-Aware-Autoregressive-Generation"><span class="toc-text">6. VA-π: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-GTR-Turbo-Merged-Checkpoint-is-Secretly-a-Free-Teacher-for-Agentic-VLM-Training"><span class="toc-text">7. GTR-Turbo: Merged Checkpoint is Secretly a Free Teacher for Agentic VLM Training</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>