<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-07-18 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. A Survey of Context Engineering for Large Language ModelsThe performance of Large Language Models (LLMs) is fundamentally determined by the contextual informa">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-07-18">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202507/2025-07-18/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. A Survey of Context Engineering for Large Language ModelsThe performance of Large Language Models (LLMs) is fundamentally determined by the contextual informa">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:09.822Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202507/2025-07-18/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-07-18',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-07-18</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:09.822Z" title="Updated 2025-11-02 07:46:09">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">9.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>42min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-07-18"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-A-Survey-of-Context-Engineering-for-Large-Language-Models"><a href="#1-A-Survey-of-Context-Engineering-for-Large-Language-Models" class="headerlink" title="1. A Survey of Context Engineering for Large Language Models"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13334">A Survey of Context Engineering for Large Language Models</a></h3><p>The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1300 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLM）的性能从根本上取决于推理过程中提供的上下文信息。本调查介绍了上下文工程，这是一门超越简单提示设计的正式学科，涵盖了LLM信息有效载荷的系统优化。我们提出了一个全面的分类法，将上下文工程分解为其基本组件以及将它们集成到智能系统中的复杂实现。我们首先研究了基础组件：上下文检索和生成、上下文处理和上下文管理。然后，我们探索这些组件是如何在架构上集成以创建复杂的系统实现的：检索增强生成（RAG）、存储系统和工具集成推理以及多代理系统。通过对1300多篇研究论文的系统分析，我们的调查不仅为该领域建立了技术路线图，还揭示了一个关键的研究差距：模型能力之间存在根本的不对称。虽然当前的模型在高级上下文工程的增强下，在理解复杂上下文方面表现出了非凡的能力，但在生成同样复杂的长篇输出方面却存在明显的局限性。解决这一差距是未来研究的首要任务。最终，这项调查为研究人员和工程师推进情境感知人工智能提供了一个统一的框架。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Lingrui Mei,Jiayu Yao,Yuyao Ge,Yiwei Wang,Baolong Bi,Yujun Cai,Jiazhi Liu,Mingyu Li,Zhong-Zhi Li,Duzhen Zhang,Chenlin Zhou,Jiayi Mao,Tianze Xia,Jiafeng Guo,Shenghua Liu</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13334.pdf">https://arxiv.org/pdf/2507.13334.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13334">https://arxiv.org/abs/2507.13334</a></p>
<p><strong>Arxiv ID</strong>: 2507.13334</p>
<p><strong>Published</strong>: 2025-07-17T17:50:36Z</p>
<p><strong>Updated</strong>: 2025-07-17T17:50:36.000Z</p>
<hr>
<h3 id="2-VisionThink-Smart-and-Efficient-Vision-Language-Model-via-Reinforcement-Learning"><a href="#2-VisionThink-Smart-and-Efficient-Vision-Language-Model-via-Reinforcement-Learning" class="headerlink" title="2. VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13348">VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</a></h3><p>Recent advancements in vision-language models (VLMs) have improved performance by increasing the number of visual tokens, which are often significantly longer than text tokens. However, we observe that most real-world scenarios do not require such an extensive number of visual tokens. While the performance drops significantly in a small subset of OCR-related tasks, models still perform accurately in most other general VQA tasks with only 1/4 resolution. Therefore, we propose to dynamically process distinct samples with different resolutions, and present a new paradigm for visual token compression, namely, VisionThink. It starts with a downsampled image and smartly decides whether it is sufficient for problem solving. Otherwise, the model could output a special token to request the higher-resolution image. Compared to existing Efficient VLM methods that compress tokens using fixed pruning ratios or thresholds, VisionThink autonomously decides whether to compress tokens case by case. As a result, it demonstrates strong fine-grained visual understanding capability on OCR-related tasks, and meanwhile saves substantial visual tokens on simpler tasks. We adopt reinforcement learning and propose the LLM-as-Judge strategy to successfully apply RL to general VQA tasks. Moreover, we carefully design a reward function and penalty mechanism to achieve a stable and reasonable image resize call ratio. Extensive experiments demonstrate the superiority, efficiency, and effectiveness of our method. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/dvlab-research/VisionThink">https://github.com/dvlab-research/VisionThink</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视觉语言模型（VLM）的最新进展通过增加视觉标记的数量来提高性能，视觉标记的长度通常比文本标记长得多。然而，我们观察到，大多数现实世界的场景不需要如此大量的视觉标记。虽然在一小部分OCR相关任务中性能显著下降，但模型在大多数其他一般VQA任务中仍能准确执行，分辨率仅为1/4。因此，我们建议动态处理具有不同分辨率的不同样本，并提出了一种新的视觉令牌压缩范式，即VisionThink。它从降采样图像开始，巧妙地决定它是否足以解决问题。否则，模型可以输出一个特殊的令牌来请求更高分辨率的图像。与使用固定修剪比率或阈值压缩令牌的现有高效VLM方法相比，VisionThink自主决定是否逐案压缩令牌。因此，它在OCR相关任务上表现出强大的细粒度视觉理解能力，同时在更简单的任务上节省了大量的视觉符号。我们采用强化学习，并提出LLM作为判断策略，将RL成功应用于一般VQA任务。此外，我们精心设计了一个奖励函数和惩罚机制，以实现稳定合理的图像调整调用率。大量的实验证明了我们的方法的优越性、效率和有效性。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/dvlab-research/VisionThink">https://github.com/dvlab-research/VisionThink</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Senqiao Yang,Junyi Li,Xin Lai,Bei Yu,Hengshuang Zhao,Jiaya Jia</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13348.pdf">https://arxiv.org/pdf/2507.13348.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13348">https://arxiv.org/abs/2507.13348</a></p>
<p><strong>Arxiv ID</strong>: 2507.13348</p>
<p><strong>Published</strong>: 2025-07-17T17:59:55Z</p>
<p><strong>Updated</strong>: 2025-07-17T17:59:55.000Z</p>
<hr>
<h3 id="3-π-3-Scalable-Permutation-Equivariant-Visual-Geometry-Learning"><a href="#3-π-3-Scalable-Permutation-Equivariant-Visual-Geometry-Learning" class="headerlink" title="3. π^3: Scalable Permutation-Equivariant Visual Geometry Learning"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13347">π^3: Scalable Permutation-Equivariant Visual Geometry Learning</a></h3><p>We introduce $\pi^3$, a feed-forward neural network that offers a novel approach to visual geometry reconstruction, breaking the reliance on a conventional fixed reference view. Previous methods often anchor their reconstructions to a designated viewpoint, an inductive bias that can lead to instability and failures if the reference is suboptimal. In contrast, $\pi^3$ employs a fully permutation-equivariant architecture to predict affine-invariant camera poses and scale-invariant local point maps without any reference frames. This design makes our model inherently robust to input ordering and highly scalable. These advantages enable our simple and bias-free approach to achieve state-of-the-art performance on a wide range of tasks, including camera pose estimation, monocular/video depth estimation, and dense point map reconstruction. Code and models are publicly available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了一种前馈神经网络，它提供了一种新的视觉几何重建方法，打破了对传统固定参考视图的依赖。以前的方法通常将重建锚定在指定的视点上，如果参考值不是最优的，这种归纳偏差可能会导致不稳定和失败。相比之下，$\pi^3$采用完全置换等变架构来预测仿射不变的相机姿态和缩放不变的局部点图，而不需要任何参考帧。这种设计使我们的模型对输入排序具有固有的鲁棒性，并且具有高度的可扩展性。这些优势使我们的简单无偏方法能够在各种任务上实现最先进的性能，包括相机姿态估计、单目/视频深度估计和密集点图重建。代码和模型是公开的。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Yifan Wang,Jianjun Zhou,Haoyi Zhu,Wenzheng Chang,Yang Zhou,Zizun Li,Junyi Chen,Jiangmiao Pang,Chunhua Shen,Tong He</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13347.pdf">https://arxiv.org/pdf/2507.13347.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13347">https://arxiv.org/abs/2507.13347</a></p>
<p><strong>Arxiv ID</strong>: 2507.13347</p>
<p><strong>Published</strong>: 2025-07-17T17:59:53Z</p>
<p><strong>Updated</strong>: 2025-07-17T17:59:53.000Z</p>
<hr>
<h3 id="4-The-Imitation-Game-Turing-Machine-Imitator-is-Length-Generalizable-Reasoner"><a href="#4-The-Imitation-Game-Turing-Machine-Imitator-is-Length-Generalizable-Reasoner" class="headerlink" title="4. The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13332">The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner</a></h3><p>Length generalization, the ability to solve problems of longer sequences than those observed during training, poses a core challenge of Transformer-based large language models (LLM). Although existing studies have predominantly focused on data-driven approaches for arithmetic operations and symbolic manipulation tasks, these approaches tend to be task-specific with limited overall performance. To pursue a more general solution, this paper focuses on a broader case of reasoning problems that are computable, i.e., problems that algorithms can solve, thus can be solved by the Turing Machine. From this perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to improve the length generalization ability of LLMs. TAIL synthesizes chain-of-thoughts (CoT) data that imitate the execution process of a Turing Machine by computer programs, which linearly expands the reasoning steps into atomic states to alleviate shortcut learning and explicit memory fetch mechanism to reduce the difficulties of dynamic and long-range data access in elementary operations. To validate the reliability and universality of TAIL, we construct a challenging synthetic dataset covering 8 classes of algorithms and 18 tasks. Without bells and whistles, TAIL significantly improves the length generalization ability as well as the performance of Qwen2.5-7B on various tasks using only synthetic data, surpassing previous methods and DeepSeek-R1. The experimental results reveal that the key concepts in the Turing Machine, instead of the thinking styles, are indispensable for TAIL for length generalization, through which the model exhibits read-and-write behaviors consistent with the properties of the Turing Machine in their attention layers. This work provides a promising direction for future research in the learning of LLM reasoning from synthetic data.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>长度泛化，即解决比训练过程中观察到的序列更长的问题的能力，是基于Transformer的大型语言模型（LLM）的核心挑战。尽管现有的研究主要集中在算术运算和符号操作任务的数据驱动方法上，但这些方法往往是针对特定任务的，整体性能有限。为了寻求更通用的解决方案，本文关注的是更广泛的可计算推理问题，即算法可以解决的问题，因此可以通过图灵机来解决。从这个角度来看，本文提出了图灵机器模仿学习（TAIL）来提高LLM的长度泛化能力。TAIL通过计算机程序合成模仿图灵机执行过程的思维链（CoT）数据，将推理步骤线性扩展到原子状态，以减轻快捷学习和显式内存提取机制，从而降低基本操作中动态和远程数据访问的难度。为了验证TAIL的可靠性和通用性，我们构建了一个具有挑战性的合成数据集，涵盖8类算法和18个任务。在没有花哨功能的情况下，TAIL显著提高了长度泛化能力，以及Qwen2.5-7B在仅使用合成数据的各种任务上的性能，超越了之前的方法和DeepSeek-R1。实验结果表明，在TAIL进行长度泛化时，图灵机中的关键概念而不是思维方式是必不可少的，通过这些概念，模型在其注意层中表现出与图灵机属性一致的读写行为。这项工作为未来从合成数据中学习LLM推理的研究提供了一个有前景的方向。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Zhouqi Hua,Wenwei Zhang,Chengqi Lyu,Yuzhe Gu,Songyang Gao,Kuikun Liu,Kai Chen</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13332.pdf">https://arxiv.org/pdf/2507.13332.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13332">https://arxiv.org/abs/2507.13332</a></p>
<p><strong>Arxiv ID</strong>: 2507.13332</p>
<p><strong>Published</strong>: 2025-07-17T17:50:07Z</p>
<p><strong>Updated</strong>: 2025-07-17T17:50:07.000Z</p>
<hr>
<h3 id="5-AnyCap-Project-A-Unified-Framework-Dataset-and-Benchmark-for-Controllable-Omni-modal-Captioning"><a href="#5-AnyCap-Project-A-Unified-Framework-Dataset-and-Benchmark-for-Controllable-Omni-modal-Captioning" class="headerlink" title="5. AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.12841">AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning</a></h3><p>Controllable captioning is essential for precise multimodal alignment and instruction following, yet existing models often lack fine-grained control and reliable evaluation protocols. To address this gap, we present the AnyCap Project, an integrated solution spanning model, dataset, and evaluation. We introduce AnyCapModel (ACM), a lightweight plug-and-play framework that enhances the controllability of existing foundation models for omni-modal captioning without retraining the base model. ACM reuses the original captions from base models while incorporating user instructions and modality features to generate improved captions. To remedy the data scarcity in controllable multimodal captioning, we build AnyCapDataset (ACD), covering three modalities, 28 user-instruction types, and 300\,k high-quality data entries. We further propose AnyCapEval, a new benchmark that provides more reliable evaluation metrics for controllable captioning by decoupling content accuracy and stylistic fidelity. ACM markedly improves caption quality across a diverse set of base models on AnyCapEval. Notably, ACM-8B raises GPT-4o\’s content scores by 45\% and style scores by 12\%, and it also achieves substantial gains on widely used benchmarks such as MIA-Bench and VidCapBench.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>可控字幕对于精确的多模式对齐和指令遵循至关重要，但现有的模型往往缺乏细粒度的控制和可靠的评估协议。为了解决这一差距，我们提出了AnyCap项目，这是一个跨越模型、数据集和评估的集成解决方案。我们介绍了AnyCapModel（ACM），这是一个轻量级的即插即用框架，它增强了现有全模态字幕基础模型的可控性，而无需重新训练基础模型。ACM重用基础模型中的原始字幕，同时结合用户指令和模态特征来生成改进的字幕。为了解决可控多模式字幕中的数据稀缺问题，我们构建了AnyCapDataset（ACD），涵盖了三种模式、28种用户指令类型和300k个高质量数据条目。我们进一步提出了AnyCapEval，这是一个新的基准，通过将内容准确性和风格保真度解耦，为可控字幕提供了更可靠的评估指标。ACM显著提高了AnyCapEval上各种基础模型的字幕质量。值得注意的是，ACM-8B将GPT-4o的内容得分提高了45%，风格得分提高了12%，并且在MIA Bench和VidCapBench等广泛使用的基准测试中也取得了长足的进步。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Yiming Ren,Zhiqiang Lin,Yu Li,Gao Meng,Weiyun Wang,Junjie Wang,Zicheng Lin,Jifeng Dai,Yujiu Yang,Wenhai Wang,Ruihang Chu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12841.pdf">https://arxiv.org/pdf/2507.12841.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12841">https://arxiv.org/abs/2507.12841</a></p>
<p><strong>Arxiv ID</strong>: 2507.12841</p>
<p><strong>Published</strong>: 2025-07-17T07:04:05Z</p>
<p><strong>Updated</strong>: 2025-07-17T07:04:05.000Z</p>
<hr>
<h3 id="6-Diffuman4D-4D-Consistent-Human-View-Synthesis-from-Sparse-View-Videos-with-Spatio-Temporal-Diffusion-Models"><a href="#6-Diffuman4D-4D-Consistent-Human-View-Synthesis-from-Sparse-View-Videos-with-Spatio-Temporal-Diffusion-Models" class="headerlink" title="6. Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13344">Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models</a></h3><p>This paper addresses the challenge of high-fidelity view synthesis of humans with sparse-view videos as input. Previous methods solve the issue of insufficient observation by leveraging 4D diffusion models to generate videos at novel viewpoints. However, the generated videos from these models often lack spatio-temporal consistency, thus degrading view synthesis quality. In this paper, we propose a novel sliding iterative denoising process to enhance the spatio-temporal consistency of the 4D diffusion model. Specifically, we define a latent grid in which each latent encodes the image, camera pose, and human pose for a certain viewpoint and timestamp, then alternately denoising the latent grid along spatial and temporal dimensions with a sliding window, and finally decode the videos at target viewpoints from the corresponding denoised latents. Through the iterative sliding, information flows sufficiently across the latent grid, allowing the diffusion model to obtain a large receptive field and thus enhance the 4D consistency of the output, while making the GPU memory consumption affordable. The experiments on the DNA-Rendering and ActorsHQ datasets demonstrate that our method is able to synthesize high-quality and consistent novel-view videos and significantly outperforms the existing approaches. See our project page for interactive demos and video results: <a target="_blank" rel="noopener" href="https://diffuman4d.github.io/">https://diffuman4d.github.io/</a> .</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本文解决了以稀疏视图视频为输入的人类高保真视图合成的挑战。以前的方法通过利用4D扩散模型在新的视点生成视频来解决观察不足的问题。然而，这些模型生成的视频往往缺乏时空一致性，从而降低了视图合成质量。本文提出了一种新的滑动迭代去噪方法，以提高4D扩散模型的时空一致性。具体来说，我们定义了一个潜在网格，其中每个潜在网格对特定视点和时间戳的图像、相机姿态和人体姿态进行编码，然后用滑动窗口沿空间和时间维度交替对潜在网格进行去噪，最后从相应的去噪延迟中解码目标视点的视频。通过迭代滑动，信息在潜在网格中充分流动，使扩散模型获得较大的接收场，从而增强输出的4D一致性，同时使GPU内存消耗负担得起。在DNA渲染和ActorsHQ数据集上的实验表明，我们的方法能够合成高质量和一致的新颖视图视频，并且明显优于现有的方法。有关交互式演示和视频结果，请参阅我们的项目页面：<a target="_blank" rel="noopener" href="https://diffuman4d.github.io/">https://diffuman4d.github.io/</a> .</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Yudong Jin,Sida Peng,Xuan Wang,Tao Xie,Zhen Xu,Yifan Yang,Yujun Shen,Hujun Bao,Xiaowei Zhou</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13344.pdf">https://arxiv.org/pdf/2507.13344.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13344">https://arxiv.org/abs/2507.13344</a></p>
<p><strong>Arxiv ID</strong>: 2507.13344</p>
<p><strong>Published</strong>: 2025-07-17T17:59:17Z</p>
<p><strong>Updated</strong>: 2025-07-17T17:59:17.000Z</p>
<hr>
<h3 id="7-RiemannLoRA-A-Unified-Riemannian-Framework-for-Ambiguity-Free-LoRA-Optimization"><a href="#7-RiemannLoRA-A-Unified-Riemannian-Framework-for-Ambiguity-Free-LoRA-Optimization" class="headerlink" title="7. RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.12142">RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization</a></h3><p>Low-Rank Adaptation (LoRA) has become a widely adopted standard for parameter-efficient fine-tuning of large language models (LLMs), significantly reducing memory and computational demands. However, challenges remain, including finding optimal initialization strategies or mitigating overparametrization in low-rank matrix factorization. In this work, we propose a novel approach that addresses both of the challenges simultaneously within a unified framework. Our method treats a set of fixed-rank LoRA matrices as a smooth manifold. Considering adapters as elements on this manifold removes overparametrization, while determining the direction of the fastest loss decrease along the manifold provides initialization. Special care is taken to obtain numerically stable and computationally efficient implementation of our method, using best practices from numerical linear algebra and Riemannian optimization. Experimental results on LLM and diffusion model architectures demonstrate that RiemannLoRA consistently improves both convergence speed and final performance over standard LoRA and its state-of-the-art modifications.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>低秩自适应（LoRA）已成为大型语言模型（LLM）参数高效微调的广泛采用的标准，显著降低了内存和计算需求。然而，挑战仍然存在，包括找到最优的初始化策略或减轻低秩矩阵分解中的过度参数化。在这项工作中，我们提出了一种在统一框架内同时解决这两个挑战的新方法。我们的方法将一组固定秩LoRA矩阵视为平滑流形。将适配器视为该流形上的元素可以消除过度参数化，同时确定沿流形损失减少最快的方向可以提供初始化。特别注意使用数值线性代数和黎曼优化的最佳实践来获得我们方法的数值稳定和计算高效的实现。LLM和扩散模型架构的实验结果表明，RiemannLoRA在收敛速度和最终性能方面始终优于标准LoRA及其最先进的修改。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Vladimir Bogachev,Vladimir Aletov,Alexander Molozhavenko,Denis Bobkov,Vera Soboleva,Aibek Alanov,Maxim Rakhuba</p>
<p><strong>Categories</strong>: cs.LG,cs.CL,cs.NA,math.DG,math.NA,68T07,65F55,53Z50</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12142.pdf">https://arxiv.org/pdf/2507.12142.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12142">https://arxiv.org/abs/2507.12142</a></p>
<p><strong>Arxiv ID</strong>: 2507.12142</p>
<p><strong>Published</strong>: 2025-07-16T11:17:12Z</p>
<p><strong>Updated</strong>: 2025-07-16T11:17:12.000Z</p>
<hr>
<h3 id="8-FantasyPortrait-Enhancing-Multi-Character-Portrait-Animation-with-Expression-Augmented-Diffusion-Transformers"><a href="#8-FantasyPortrait-Enhancing-Multi-Character-Portrait-Animation-with-Expression-Augmented-Diffusion-Transformers" class="headerlink" title="8. FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.12956">FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers</a></h3><p>Producing expressive facial animations from static images is a challenging task. Prior methods relying on explicit geometric priors (e.g., facial landmarks or 3DMM) often suffer from artifacts in cross reenactment and struggle to capture subtle emotions. Furthermore, existing approaches lack support for multi-character animation, as driving features from different individuals frequently interfere with one another, complicating the task. To address these challenges, we propose FantasyPortrait, a diffusion transformer based framework capable of generating high-fidelity and emotion-rich animations for both single- and multi-character scenarios. Our method introduces an expression-augmented learning strategy that utilizes implicit representations to capture identity-agnostic facial dynamics, enhancing the model’s ability to render fine-grained emotions. For multi-character control, we design a masked cross-attention mechanism that ensures independent yet coordinated expression generation, effectively preventing feature interference. To advance research in this area, we propose the Multi-Expr dataset and ExprBench, which are specifically designed datasets and benchmarks for training and evaluating multi-character portrait animations. Extensive experiments demonstrate that FantasyPortrait significantly outperforms state-of-the-art methods in both quantitative metrics and qualitative evaluations, excelling particularly in challenging cross reenactment and multi-character contexts. Our project page is <a target="_blank" rel="noopener" href="https://fantasy-amap.github.io/fantasy-portrait/">https://fantasy-amap.github.io/fantasy-portrait/</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>从静态图像中制作富有表现力的面部动画是一项具有挑战性的任务。先前依赖于显式几何先验（如面部地标或3DMM）的方法在交叉重现中经常出现伪影，难以捕捉微妙的情绪。此外，现有的方法缺乏对多角色动画的支持，因为来自不同个体的驱动特征经常相互干扰，使任务复杂化。为了应对这些挑战，我们提出了FantasyPortrait，这是一个基于扩散变换器的框架，能够为单角色和多角色场景生成高保真度和情感丰富的动画。我们的方法引入了一种表情增强学习策略，该策略利用隐式表示来捕捉与身份无关的面部动态，增强了模型渲染细粒度情绪的能力。对于多字符控制，我们设计了一种掩码交叉注意力机制，确保独立但协调的表达式生成，有效防止特征干扰。为了推进这一领域的研究，我们提出了Multi-Expr数据集和ExprBench，它们是专门为训练和评估多角色肖像动画而设计的数据集和基准。大量实验表明，FantasyPortrait在定量指标和定性评估方面明显优于最先进的方法，特别是在具有挑战性的交叉重现和多字符上下文中表现出色。我们的项目页面是<a target="_blank" rel="noopener" href="https://fantasy-amap.github.io/fantasy-portrait/">https://fantasy-amap.github.io/fantasy-portrait/</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Qiang Wang,Mengchao Wang,Fan Jiang,Yaqi Fan,Yonggang Qi,Mu Xu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12956.pdf">https://arxiv.org/pdf/2507.12956.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12956">https://arxiv.org/abs/2507.12956</a></p>
<p><strong>Arxiv ID</strong>: 2507.12956</p>
<p><strong>Published</strong>: 2025-07-17T09:50:43Z</p>
<p><strong>Updated</strong>: 2025-07-17T09:50:43.000Z</p>
<hr>
<h3 id="9-MindJourney-Test-Time-Scaling-with-World-Models-for-Spatial-Reasoning"><a href="#9-MindJourney-Test-Time-Scaling-with-World-Models-for-Spatial-Reasoning" class="headerlink" title="9. MindJourney: Test-Time Scaling with World Models for Spatial Reasoning"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.12508">MindJourney: Test-Time Scaling with World Models for Spatial Reasoning</a></h3><p>Spatial reasoning in 3D space is central to human cognition and indispensable for embodied tasks such as navigation and manipulation. However, state-of-the-art vision-language models (VLMs) struggle frequently with tasks as simple as anticipating how a scene will look after an egocentric motion: they perceive 2D images but lack an internal model of 3D dynamics. We therefore propose MindJourney, a test-time scaling framework that grants a VLM with this missing capability by coupling it to a controllable world model based on video diffusion. The VLM iteratively sketches a concise camera trajectory, while the world model synthesizes the corresponding view at each step. The VLM then reasons over this multi-view evidence gathered during the interactive exploration. Without any fine-tuning, our MindJourney achieves over an average 8% performance boost on the representative spatial reasoning benchmark SAT, showing that pairing VLMs with world models for test-time scaling offers a simple, plug-and-play route to robust 3D reasoning. Meanwhile, our method also improves upon the test-time inference VLMs trained through reinforcement learning, which demonstrates the potential of our method that utilizes world models for test-time scaling.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>三维空间中的空间推理是人类认知的核心，对于导航和操纵等具体任务来说是不可或缺的。然而，最先进的视觉语言模型（VLM）经常难以完成像预测自我中心运动后场景的样子这样简单的任务：它们感知2D图像，但缺乏3D动态的内部模型。因此，我们提出了MindJourney，这是一个测试时间缩放框架，通过将VLM与基于视频扩散的可控世界模型耦合，赋予VLM这种缺失的能力。VLM迭代地绘制了一个简洁的相机轨迹，而世界模型在每一步都合成了相应的视图。然后，VLM对交互式探索过程中收集的多视图证据进行了推理。在没有任何微调的情况下，我们的MindJourney在代表性的空间推理基准SAT上实现了平均8%以上的性能提升，这表明将VLM与世界模型配对以进行测试时间扩展为稳健的3D推理提供了一条简单、即插即用的途径。同时，我们的方法还改进了通过强化学习训练的测试时间推理VLM，这证明了我们的方法利用世界模型进行测试时间缩放的潜力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Yuncong Yang,Jiageng Liu,Zheyuan Zhang,Siyuan Zhou,Reuben Tan,Jianwei Yang,Yilun Du,Chuang Gan</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12508.pdf">https://arxiv.org/pdf/2507.12508.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12508">https://arxiv.org/abs/2507.12508</a></p>
<p><strong>Arxiv ID</strong>: 2507.12508</p>
<p><strong>Published</strong>: 2025-07-16T17:59:36Z</p>
<p><strong>Updated</strong>: 2025-07-16T17:59:36.000Z</p>
<hr>
<h3 id="10-AbGen-Evaluating-Large-Language-Models-in-Ablation-Study-Design-and-Evaluation-for-Scientific-Research"><a href="#10-AbGen-Evaluating-Large-Language-Models-in-Ablation-Study-Design-and-Evaluation-for-Scientific-Research" class="headerlink" title="10. AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13300">AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research</a></h3><p>We introduce AbGen, the first benchmark designed to evaluate the capabilities of LLMs in designing ablation studies for scientific research. AbGen consists of 1,500 expert-annotated examples derived from 807 NLP papers. In this benchmark, LLMs are tasked with generating detailed ablation study designs for a specified module or process based on the given research context. Our evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a significant performance gap between these models and human experts in terms of the importance, faithfulness, and soundness of the ablation study designs. Moreover, we demonstrate that current automated evaluation methods are not reliable for our task, as they show a significant discrepancy when compared to human assessment. To better investigate this, we develop AbGen-Eval, a meta-evaluation benchmark designed to assess the reliability of commonly used automated evaluation systems in measuring LLM performance on our task. We investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for future research on developing more effective and reliable LLM-based evaluation systems for complex scientific tasks.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了AbGen，这是第一个旨在评估LLM在设计科学研究消融研究方面能力的基准。AbGen由来自807篇NLP论文的1500个专家注释示例组成。在此基准测试中，LLM的任务是根据给定的研究背景，为指定的模块或过程生成详细的消融研究设计。我们对DeepSeek-R1-0528和o4-mini等领先LLM的评估突显了这些模型与人类专家在消融研究设计的重要性、可信度和合理性方面存在显著的性能差距。此外，我们证明，目前的自动评估方法对我们的任务来说并不可靠，因为与人工评估相比，它们显示出明显的差异。为了更好地研究这一点，我们开发了AbGen Eval，这是一个元评估基准，旨在评估常用自动评估系统在测量LLM任务绩效方面的可靠性。我们在AbGen Eval上研究了各种LLM作为法官系统，为未来为复杂科学任务开发更有效、更可靠的基于LLM的评估系统的研究提供了见解。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Yilun Zhao,Weiyuan Chen,Zhijian Xu,Manasi Patwardhan,Yixin Liu,Chengye Wang,Lovekesh Vig,Arman Cohan</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13300.pdf">https://arxiv.org/pdf/2507.13300.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13300">https://arxiv.org/abs/2507.13300</a></p>
<p><strong>Arxiv ID</strong>: 2507.13300</p>
<p><strong>Published</strong>: 2025-07-17T17:09:22Z</p>
<p><strong>Updated</strong>: 2025-07-17T17:09:22.000Z</p>
<hr>
<h3 id="11-Voxtral"><a href="#11-Voxtral" class="headerlink" title="11. Voxtral"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13264">Voxtral</a></h3><p>We present Voxtral Mini and Voxtral Small, two multimodal audio chat models. Voxtral is trained to comprehend both spoken audio and text documents, achieving state-of-the-art performance across a diverse range of audio benchmarks, while preserving strong text capabilities. Voxtral Small outperforms a number of closed-source models, while being small enough to run locally. A 32K context window enables the model to handle audio files up to 40 minutes in duration and long multi-turn conversations. We also contribute three benchmarks for evaluating speech understanding models on knowledge and trivia. Both Voxtral models are released under Apache 2.0 license.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍Voxtral Mini和Voxtral Small两种多模式音频聊天模式。Voxtral经过培训，能够理解口语音频和文本文档，在各种音频基准测试中实现最先进的性能，同时保持强大的文本功能。Voxtral Small的性能优于许多闭源模型，同时足够小，可以在本地运行。32K上下文窗口使该模型能够处理长达40分钟的音频文件和长时间的多回合对话。我们还贡献了三个基准，用于评估关于知识和琐事的语音理解模型。这两个Voxtral模型都是在Apache 2.0许可证下发布的。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Alexander H. Liu,Andy Ehrenberg,Andy Lo,Clément Denoix,Corentin Barreau,Guillaume Lample,Jean-Malo Delignon,Khyathi Raghavi Chandu,Patrick von Platen,Pavankumar Reddy Muddireddy,Sanchit Gandhi,Soham Ghosh,Srijan Mishra,Thomas Foubert,Abhinav Rastogi,Adam Yang,Albert Q. Jiang,Alexandre Sablayrolles,Amélie Héliou,Amélie Martin,Anmol Agarwal,Antoine Roux,Arthur Darcet,Arthur Mensch,Baptiste Bout,Baptiste Rozière,Baudouin De Monicault,Chris Bamford,Christian Wallenwein,Christophe Renaudin,Clémence Lanfranchi,Darius Dabert,Devendra Singh Chaplot,Devon Mizelle,Diego de las Casas,Elliot Chane-Sane,Emilien Fugier,Emma Bou Hanna,Gabrielle Berrada,Gauthier Delerce,Gauthier Guinet,Georgii Novikov,Guillaume Martin,Himanshu Jaju,Jan Ludziejewski,Jason Rute,Jean-Hadrien Chabran,Jessica Chudnovsky,Joachim Studnia,Joep Barmentlo,Jonas Amar,Josselin Somerville Roberts,Julien Denize,Karan Saxena,Karmesh Yadav,Kartik Khandelwal,Kush Jain,Lélio Renard Lavaud,Léonard Blier,Lingxiao Zhao,Louis Martin,Lucile Saulnier,Luyu Gao,Marie Pellat,Mathilde Guillaumin,Mathis Felardos,Matthieu Dinot,Maxime Darrin,Maximilian Augustin,Mickaël Seznec,Neha Gupta,Nikhil Raghuraman,Olivier Duchenne,Patricia Wang,Patryk Saffer,Paul Jacob,Paul Wambergue,Paula Kurylowicz,Philomène Chagniot,Pierre Stock,Pravesh Agrawal,Rémi Delacourt,Romain Sauvestre,Roman Soletskyi,Sagar Vaze,Sandeep Subramanian,Saurabh Garg,Shashwat Dalal,Siddharth Gandhi,Sumukh Aithal,Szymon Antoniak,Teven Le Scao,Thibault Schueller,Thibaut Lavril,Thomas Robert,Thomas Wang,Timothée Lacroix,Tom Bewley,Valeriia Nemychnikova,Victor Paltz,Virgile Richard,Wen-Ding Li,William Marshall,Xuanyu Zhang,Yihan Wan,Yunhao Tang</p>
<p><strong>Categories</strong>: cs.SD,cs.AI,eess.AS</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13264.pdf">https://arxiv.org/pdf/2507.13264.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13264">https://arxiv.org/abs/2507.13264</a></p>
<p><strong>Arxiv ID</strong>: 2507.13264</p>
<p><strong>Published</strong>: 2025-07-17T16:17:37Z</p>
<p><strong>Updated</strong>: 2025-07-17T16:17:37.000Z</p>
<hr>
<h3 id="12-Teach-Old-SAEs-New-Domain-Tricks-with-Boosting"><a href="#12-Teach-Old-SAEs-New-Domain-Tricks-with-Boosting" class="headerlink" title="12. Teach Old SAEs New Domain Tricks with Boosting"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.12990">Teach Old SAEs New Domain Tricks with Boosting</a></h3><p>Sparse Autoencoders have emerged as powerful tools for interpreting the internal representations of Large Language Models, yet they often fail to capture domain-specific features not prevalent in their training corpora. This paper introduces a residual learning approach that addresses this feature blindness without requiring complete retraining. We propose training a secondary SAE specifically to model the reconstruction error of a pretrained SAE on domain-specific texts, effectively capturing features missed by the primary model. By summing the outputs of both models during inference, we demonstrate significant improvements in both LLM cross-entropy and explained variance metrics across multiple specialized domains. Our experiments show that this method efficiently incorporates new domain knowledge into existing SAEs while maintaining their performance on general tasks. This approach enables researchers to selectively enhance SAE interpretability for specific domains of interest, opening new possibilities for targeted mechanistic interpretability of LLMs.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>稀疏自编码器已经成为解释大型语言模型内部表示的强大工具，但它们往往无法捕捉到训练语料库中不常见的领域特定特征。本文介绍了一种残差学习方法，该方法在不需要完全重新训练的情况下解决了这种特征盲。我们建议专门训练一个二级SAE，以模拟特定领域文本上预训练SAE的重建误差，有效地捕捉主模型遗漏的特征。通过在推理过程中将两个模型的输出相加，我们证明了LLM交叉熵和跨多个专门领域的解释方差度量的显著改进。我们的实验表明，该方法有效地将新的领域知识整合到现有的SAE中，同时保持其在一般任务上的性能。这种方法使研究人员能够有选择地提高SAE对特定感兴趣领域的可解释性，为LLM的有针对性的机械可解释性开辟了新的可能性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Nikita Koriagin,Yaroslav Aksenov,Daniil Laptev,Gleb Gerasimov,Nikita Balagansky,Daniil Gavrilov</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12990.pdf">https://arxiv.org/pdf/2507.12990.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12990">https://arxiv.org/abs/2507.12990</a></p>
<p><strong>Arxiv ID</strong>: 2507.12990</p>
<p><strong>Published</strong>: 2025-07-17T10:57:49Z</p>
<p><strong>Updated</strong>: 2025-07-17T10:57:49.000Z</p>
<hr>
<h3 id="13-FLEXITOKENS-Flexible-Tokenization-for-Evolving-Language-Models"><a href="#13-FLEXITOKENS-Flexible-Tokenization-for-Evolving-Language-Models" class="headerlink" title="13. FLEXITOKENS: Flexible Tokenization for Evolving Language Models"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.12720">FLEXITOKENS: Flexible Tokenization for Evolving Language Models</a></h3><p>Language models (LMs) are challenging to adapt to new data distributions by simple finetuning. This is due to the rigidity of their subword tokenizers, which typically remain unchanged during adaptation. This inflexibility often leads to inefficient tokenization, causing overfragmentation of out-of-distribution domains, unseen languages, or scripts. In this work, we develop byte-level LMs with learnable tokenizers to make tokenization adaptive. Our models include a submodule that learns to predict boundaries between the input byte sequence, encoding it into variable-length segments. Existing tokenizer-free methods train this boundary predictor using an auxiliary loss that enforces a fixed compression rate across the training corpus, introducing a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective that enables significantly greater flexibility during adaptation. Evaluating across multiple multilingual benchmarks, morphologically diverse tasks, and domains, we demonstrate that FLEXITOKENS consistently reduces token over-fragmentation and achieves up to 10\% improvements on downstream task performance compared to subword and other gradient-based tokenizers. Code and data for our experiments will be released at <a target="_blank" rel="noopener" href="https://github.com/owos/flexitokens">https://github.com/owos/flexitokens</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>语言模型（LM）很难通过简单的微调来适应新的数据分布。这是由于它们的子词标记器的刚性，在适应过程中通常保持不变。这种缺乏灵活性通常会导致低效的标记化，导致分发域外、看不见的语言或脚本的过度碎片化。在这项工作中，我们开发了具有可学习标记器的字节级LM，使标记化具有自适应性。我们的模型包括一个子模块，该子模块学习预测输入字节序列之间的边界，并将其编码为可变长度段。现有的无标记器方法使用辅助损失来训练这个边界预测器，该损失在训练语料库中强制执行固定的压缩率，引入了一种新的刚性。我们提出了FLEXITOKENS，这是一种简化的训练目标，在适应过程中具有更大的灵活性。在多个多语言基准测试、形态多样的任务和域中进行评估后，我们证明FLEXITOKENS始终如一地减少了令牌过度碎片化，与子词和其他基于梯度的令牌化器相比，下游任务性能提高了10%。我们实验的代码和数据将在<a target="_blank" rel="noopener" href="https://github.com/owos/flexitokens">https://github.com/owos/flexitokens</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.12720.pdf">https://arxiv.org/pdf/2507.12720.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.12720">https://arxiv.org/abs/2507.12720</a></p>
<p><strong>Arxiv ID</strong>: 2507.12720</p>
<p><strong>Published</strong>: 2025-07-17T01:55:41Z</p>
<p><strong>Updated</strong>: 2025-07-17T01:55:41.000Z</p>
<hr>
<h3 id="14-TLB-VFI-Temporal-Aware-Latent-Brownian-Bridge-Diffusion-for-Video-Frame-Interpolation"><a href="#14-TLB-VFI-Temporal-Aware-Latent-Brownian-Bridge-Diffusion-for-Video-Frame-Interpolation" class="headerlink" title="14. TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.04984">TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation</a></h3><p>Video Frame Interpolation (VFI) aims to predict the intermediate frame $I_n$ (we use n to denote time in videos to avoid notation overload with the timestep $t$ in diffusion models) based on two consecutive neighboring frames $I_0$ and $I_1$. Recent approaches apply diffusion models (both image-based and video-based) in this task and achieve strong performance. However, image-based diffusion models are unable to extract temporal information and are relatively inefficient compared to non-diffusion methods. Video-based diffusion models can extract temporal information, but they are too large in terms of training scale, model size, and inference time. To mitigate the above issues, we propose Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation (TLB-VFI), an efficient video-based diffusion model. By extracting rich temporal information from video inputs through our proposed 3D-wavelet gating and temporal-aware autoencoder, our method achieves 20% improvement in FID on the most challenging datasets over recent SOTA of image-based diffusion models. Meanwhile, due to the existence of rich temporal information, our method achieves strong performance while having 3times fewer parameters. Such a parameter reduction results in 2.3x speed up. By incorporating optical flow guidance, our method requires 9000x less training data and achieves over 20x fewer parameters than video-based diffusion models. Codes and results are available at our project page: <a target="_blank" rel="noopener" href="https://zonglinl.github.io/tlbvfi_page">https://zonglinl.github.io/tlbvfi_page</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视频帧插值（VFI）旨在基于两个连续的相邻帧$I_0$和$I_1$预测中间帧$I_n$（我们使用n表示视频中的时间，以避免扩散模型中时间步长$t$的符号过载）。最近的方法在这项任务中应用了扩散模型（基于图像和基于视频），并取得了很好的性能。然而，基于图像的扩散模型无法提取时间信息，与非扩散方法相比效率相对较低。基于视频的扩散模型可以提取时间信息，但在训练规模、模型大小和推理时间方面都太大。为了缓解上述问题，我们提出了一种基于视频帧插值的时间感知潜在布朗桥扩散（TLB-VFI），这是一种高效的基于视频的扩散模型。通过我们提出的3D小波门控和时间感知自动编码器从视频输入中提取丰富的时间信息，我们的方法在最具挑战性的数据集上的FID比最近基于图像的扩散模型的SOTA提高了20%。同时，由于存在丰富的时间信息，我们的方法在参数减少3倍的情况下实现了很强的性能。这样的参数减少会使速度提高2.3倍。通过结合光流引导，我们的方法需要的训练数据比基于视频的扩散模型少9000倍，参数也少20倍以上。代码和结果可在我们的项目页面上找到：<a target="_blank" rel="noopener" href="https://zonglinl.github.io/tlbvfi_page">https://zonglinl.github.io/tlbvfi_page</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Zonglin Lyu,Chen Chen</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.04984.pdf">https://arxiv.org/pdf/2507.04984.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.04984">https://arxiv.org/abs/2507.04984</a></p>
<p><strong>Arxiv ID</strong>: 2507.04984</p>
<p><strong>Published</strong>: 2025-07-07T13:25:32Z</p>
<p><strong>Updated</strong>: 2025-07-07T13:25:32.000Z</p>
<hr>
<h3 id="15-Automating-Steering-for-Safe-Multimodal-Large-Language-Models"><a href="#15-Automating-Steering-for-Safe-Multimodal-Large-Language-Models" class="headerlink" title="15. Automating Steering for Safe Multimodal Large Language Models"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.13255">Automating Steering for Safe Multimodal Large Language Models</a></h3><p>Recent progress in Multimodal Large Language Models (MLLMs) has unlocked powerful cross-modal reasoning abilities, but also raised new safety concerns, particularly when faced with adversarial multimodal inputs. To improve the safety of MLLMs during inference, we introduce a modular and adaptive inference-time intervention technology, AutoSteer, without requiring any fine-tuning of the underlying model. AutoSteer incorporates three core components: (1) a novel Safety Awareness Score (SAS) that automatically identifies the most safety-relevant distinctions among the model’s internal layers; (2) an adaptive safety prober trained to estimate the likelihood of toxic outputs from intermediate representations; and (3) a lightweight Refusal Head that selectively intervenes to modulate generation when safety risks are detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical benchmarks demonstrate that AutoSteer significantly reduces the Attack Success Rate (ASR) for textual, visual, and cross-modal threats, while maintaining general abilities. These findings position AutoSteer as a practical, interpretable, and effective framework for safer deployment of multimodal AI systems.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态大型语言模型（MLLM）的最新进展释放了强大的跨模态推理能力，但也引发了新的安全问题，特别是在面对对抗性多模态输入时。为了提高MLLM在推理过程中的安全性，我们引入了一种模块化和自适应的推理时间干预技术AutoSteer，而不需要对底层模型进行任何微调。AutoSteer包含三个核心组件：（1）新的安全意识评分（SAS），可自动识别模型内部层中与安全最相关的区别；（2）经过训练的自适应安全探测器，用于估计中间表示的有毒输出的可能性；以及（3）轻型拒绝头，当检测到安全风险时，选择性地干预以调节发电。在LLaVA OV和变色龙上进行的各种安全关键基准测试表明，AutoSteer显著降低了文本、视觉和跨模式威胁的攻击成功率（ASR），同时保持了一般能力。这些发现将AutoSteer定位为一个实用、可解释和有效的框架，用于更安全地部署多模式人工智能系统。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.IR,cs.LG,cs.MM</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.13255.pdf">https://arxiv.org/pdf/2507.13255.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.13255">https://arxiv.org/abs/2507.13255</a></p>
<p><strong>Arxiv ID</strong>: 2507.13255</p>
<p><strong>Published</strong>: 2025-07-17T16:04:55Z</p>
<p><strong>Updated</strong>: 2025-07-17T16:04:55.000Z</p>
<hr>
<h3 id="16-Einstein-Fields-A-Neural-Perspective-To-Computational-General-Relativity"><a href="#16-Einstein-Fields-A-Neural-Perspective-To-Computational-General-Relativity" class="headerlink" title="16. Einstein Fields: A Neural Perspective To Computational General Relativity"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.11589">Einstein Fields: A Neural Perspective To Computational General Relativity</a></h3><p>We introduce Einstein Fields, a neural representation that is designed to compress computationally intensive four-dimensional numerical relativity simulations into compact implicit neural network weights. By modeling the \emph{metric}, which is the core tensor field of general relativity, Einstein Fields enable the derivation of physical quantities via automatic differentiation. However, unlike conventional neural fields (e.g., signed distance, occupancy, or radiance fields), Einstein Fields are \emph{Neural Tensor Fields} with the key difference that when encoding the spacetime geometry of general relativity into neural field representations, dynamics emerge naturally as a byproduct. Einstein Fields show remarkable potential, including continuum modeling of 4D spacetime, mesh-agnosticity, storage efficiency, derivative accuracy, and ease of use. We address these challenges across several canonical test beds of general relativity and release an open source JAX-based library, paving the way for more scalable and expressive approaches to numerical relativity. Code is made available at <a target="_blank" rel="noopener" href="https://github.com/AndreiB137/EinFields">https://github.com/AndreiB137/EinFields</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了Einstein Fields，这是一种神经表示，旨在将计算密集型四维数值相对论模拟压缩为紧凑的隐式神经网络权重。通过对广义相对论的核心张量场emph{metric}进行建模，爱因斯坦场能够通过自动微分来推导物理量。然而，与传统的神经场（例如，带符号的距离、占用或辐射场）不同，爱因斯坦场是{神经张量场}，其关键区别在于，当将广义相对论的时空几何编码为神经场表示时，动力学自然会作为副产品出现。爱因斯坦场显示出非凡的潜力，包括4D时空的连续建模、网格不可知性、存储效率、导数精度和易用性。我们在广义相对论的几个规范测试台上解决了这些挑战，并发布了一个基于JAX的开源库，为更具可扩展性和表现力的数值相对论方法铺平了道路。代码可在以下网址获得<a target="_blank" rel="noopener" href="https://github.com/AndreiB137/EinFields">https://github.com/AndreiB137/EinFields</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed</p>
</div></details>
<p><strong>Authors</strong>: Sandeep Suresh Cranganore,Andrei Bodnar,Arturs Berzins,Johannes Brandstetter</p>
<p><strong>Categories</strong>: cs.LG,gr-qc</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.11589.pdf">https://arxiv.org/pdf/2507.11589.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.11589">https://arxiv.org/abs/2507.11589</a></p>
<p><strong>Arxiv ID</strong>: 2507.11589</p>
<p><strong>Published</strong>: 2025-07-15T14:55:39Z</p>
<p><strong>Updated</strong>: 2025-07-15T14:55:39.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202507/2025-07-18/">https://wdndev.github.io/daily/hf/202507/2025-07-18/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-A-Survey-of-Context-Engineering-for-Large-Language-Models"><span class="toc-text">1. A Survey of Context Engineering for Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-VisionThink-Smart-and-Efficient-Vision-Language-Model-via-Reinforcement-Learning"><span class="toc-text">2. VisionThink: Smart and Efficient Vision Language Model via Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-%CF%80-3-Scalable-Permutation-Equivariant-Visual-Geometry-Learning"><span class="toc-text">3. π^3: Scalable Permutation-Equivariant Visual Geometry Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-The-Imitation-Game-Turing-Machine-Imitator-is-Length-Generalizable-Reasoner"><span class="toc-text">4. The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-AnyCap-Project-A-Unified-Framework-Dataset-and-Benchmark-for-Controllable-Omni-modal-Captioning"><span class="toc-text">5. AnyCap Project: A Unified Framework, Dataset, and Benchmark for Controllable Omni-modal Captioning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Diffuman4D-4D-Consistent-Human-View-Synthesis-from-Sparse-View-Videos-with-Spatio-Temporal-Diffusion-Models"><span class="toc-text">6. Diffuman4D: 4D Consistent Human View Synthesis from Sparse-View Videos with Spatio-Temporal Diffusion Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-RiemannLoRA-A-Unified-Riemannian-Framework-for-Ambiguity-Free-LoRA-Optimization"><span class="toc-text">7. RiemannLoRA: A Unified Riemannian Framework for Ambiguity-Free LoRA Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-FantasyPortrait-Enhancing-Multi-Character-Portrait-Animation-with-Expression-Augmented-Diffusion-Transformers"><span class="toc-text">8. FantasyPortrait: Enhancing Multi-Character Portrait Animation with Expression-Augmented Diffusion Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-MindJourney-Test-Time-Scaling-with-World-Models-for-Spatial-Reasoning"><span class="toc-text">9. MindJourney: Test-Time Scaling with World Models for Spatial Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-AbGen-Evaluating-Large-Language-Models-in-Ablation-Study-Design-and-Evaluation-for-Scientific-Research"><span class="toc-text">10. AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Voxtral"><span class="toc-text">11. Voxtral</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Teach-Old-SAEs-New-Domain-Tricks-with-Boosting"><span class="toc-text">12. Teach Old SAEs New Domain Tricks with Boosting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-FLEXITOKENS-Flexible-Tokenization-for-Evolving-Language-Models"><span class="toc-text">13. FLEXITOKENS: Flexible Tokenization for Evolving Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-TLB-VFI-Temporal-Aware-Latent-Brownian-Bridge-Diffusion-for-Video-Frame-Interpolation"><span class="toc-text">14. TLB-VFI: Temporal-Aware Latent Brownian Bridge Diffusion for Video Frame Interpolation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Automating-Steering-for-Safe-Multimodal-Large-Language-Models"><span class="toc-text">15. Automating Steering for Safe Multimodal Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-Einstein-Fields-A-Neural-Perspective-To-Computational-General-Relativity"><span class="toc-text">16. Einstein Fields: A Neural Perspective To Computational General Relativity</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>