<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-07-31 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual SegmentationReferring audio-visual segmentation (RAVS) has recently seen significant adv">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-07-31">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202507/2025-07-31/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual SegmentationReferring audio-visual segmentation (RAVS) has recently seen significant adv">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:09.835Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202507/2025-07-31/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-07-31',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-07-31</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:09.835Z" title="Updated 2025-11-02 07:46:09">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">6.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>20min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-07-31"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Towards-Omnimodal-Expressions-and-Reasoning-in-Referring-Audio-Visual-Segmentation"><a href="#1-Towards-Omnimodal-Expressions-and-Reasoning-in-Referring-Audio-Visual-Segmentation" class="headerlink" title="1. Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2507.22886">Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation</a></h3><p>Referring audio-visual segmentation (RAVS) has recently seen significant advancements, yet challenges remain in integrating multimodal information and deeply understanding and reasoning about audiovisual content. To extend the boundaries of RAVS and facilitate future research in this field, we propose Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS stands out with three key innovations: (1) 8 types of multimodal expressions that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on understanding audio content beyond just detecting their presence; and (3) the inclusion of complex reasoning and world knowledge in expressions. Furthermore, we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the challenges of multimodal reasoning and fine-grained understanding of audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and perform reasoning-based segmentation. Extensive experiments show that OISA outperforms existing methods on OmniAVS and achieves competitive results on other related tasks.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近，参考音频-视觉分割（RAVS）领域取得了显著进展，但在整合多模态信息及深入理解和推理音视频内容方面仍面临挑战。为了扩展RAVS的边界并促进该领域的未来研究，我们提出了全模态参考音频-视觉分割（OmniAVS），这是一个包含2098个视频和59458个多模态引用表达的新数据集。OmniAVS以三项关键创新而脱颖而出： (1) 8种多模态表达，灵活结合文本、语音、声音和视觉线索； (2) 强调理解音频内容，超越单纯检测其存在； (3) 包括复杂推理和世界知识在表达中。此外，我们介绍了全模态指令分割助手（OISA），以解决OmniAVS中多模态推理和音视频内容细致理解的挑战。OISA利用多模态大语言模型（MLLM）来理解复杂线索并进行基于推理的分割。广泛的实验表明，OISA在OmniAVS上优于现有方法，并在其他相关任务中获得了竞争性结果。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p><strong>A</strong>: 这篇论文试图解决以下问题：</p>
<ol>
<li><p><strong>多模态信息融合与理解</strong>：在现有的音频视觉分割（Referring Audio-Visual Segmentation，RAVS）任务中，多模态信息（如文本、语音、声音、图像）的融合和深度理解存在挑战。以往的研究和数据集在处理音频信息时，往往仅限于检测声音的存在，而缺乏对声音内容的深入理解。例如，仅识别声音的响度或发生时间，而未能理解声音所传达的具体含义（如咳嗽声可能暗示生病）。</p>
</li>
<li><p><strong>复杂推理与世界知识的应用</strong>：在音频视觉场景中，需要模型具备复杂推理和世界知识的应用能力，以实现更高级别的感知和理解。然而，现有的数据集和方法在这方面存在不足，缺乏对音频内容的推理和解释能力。例如，现有的Ref-AVS数据集中的表达式主要关注声音的基本特性，而未能涉及更复杂的推理任务。</p>
</li>
<li><p><strong>多模态输入的灵活性</strong>：现有的RAVS数据集在支持多模态输入方面存在局限性，通常只支持单一或孤立的模态（如仅文本或仅音频）。这限制了模型在实际应用中的灵活性和交互能力。为了更好地模拟现实世界中的复杂场景，需要一个能够支持多种模态组合的数据集，以促进更灵活的多模态交互。</p>
</li>
<li><p><strong>多模态推理和分割的基准模型</strong>：鉴于OmniAVS数据集的独特性和复杂性，需要开发一个能够处理多模态输入并进行推理的基准模型。现有的模型无法直接应用于这种新兴领域，因此需要设计一个新的模型来应对这些挑战。</p>
</li>
</ol>
<p>为了解决这些问题，论文提出了OmniAVS数据集和Omnimodal Instructed Segmentation Assistant（OISA）模型。OmniAVS数据集通过引入多种多模态表达式和复杂的推理任务，推动了音频视觉场景中多模态信息的深度理解和推理能力的发展。而OISA模型则通过多模态大语言模型（MLLM）来理解和处理这些复杂的多模态输入，并执行基于推理的分割任务。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p><strong>A</strong>: 论文中提到了以下相关研究：</p>
<p>音频视觉场景感知</p>
<ul>
<li><strong>多模态融合</strong>：研究如何整合音频和视觉信息以模拟人类的多模态感知能力，如在事件检测、机器人操作、目标定位、动作识别和目标分割等任务中。例如，Music-AVQA [35] 和 AVQA [72] 主要关注问答任务，而 Ref-AVS [68] 则专注于音频视觉分割。</li>
<li><strong>Ref-AVS [68]</strong>：一个音频视觉分割数据集，但其对音频信息的利用有限，主要关注声音的基本特性，如声音的发生、强度和时间序列，而缺乏对声音内容的复杂推理。</li>
</ul>
<p>基于推理的感知</p>
<ul>
<li><strong>多模态大语言模型（MLLM）</strong>：随着 MLLM 的发展，研究从基于简单语义的感知转向基于世界知识或复杂推理的感知。例如，GPT4RoI [82] 通过在 MLLM 上添加边界框来解决需要推理的区域级复杂任务；DetGPT [53] 将预训练的 MLLM 与开放词汇目标检测器结合，实现基于自然语言指令的推理目标检测；LISA [33] 使用 MLLM 输出 [SEG] token，然后通过掩码头生成掩码，在需要复杂推理的复杂分割场景中表现出色。</li>
<li><strong>推理视频目标分割</strong>：VideoLISA [3] 和 VISA [70] 将 LISA 的能力扩展到推理视频目标分割，通过全面的时间建模实现这一目标。这些工作为 OISA 提供了灵感，OISA 将推理能力扩展到音频视觉上下文中。</li>
</ul>
<p>音频视觉分割</p>
<ul>
<li><strong>Open-Vocabulary Audio-Visual Semantic Segmentation [24]</strong>：研究开放词汇音频视觉语义分割，关注如何将语言指令与音频视觉内容相结合以实现分割。</li>
<li><strong>Ref-AVS Bench [68]</strong>：一个音频视觉分割基准数据集，支持音频和文本表达式，但缺乏复杂推理和解释。</li>
</ul>
<p>音频视觉问答</p>
<ul>
<li><strong>AVQA [72]</strong>：一个音频视觉问答数据集，关注如何结合音频和视觉信息回答问题。</li>
<li><strong>Music-AVQA [35]</strong>：一个专注于音乐场景的音频视觉问答数据集。</li>
</ul>
<p>音频视觉目标定位</p>
<ul>
<li><strong>Egocentric Audio-Visual Object Localization [30]</strong>：研究如何在以自我为中心的场景中定位音频视觉目标。</li>
</ul>
<p>音频视觉事件定位</p>
<ul>
<li><strong>Audio-Visual Event Localization in Unconstrained Videos [62]</strong>：研究如何在无约束视频中定位音频视觉事件。</li>
</ul>
<p>音频视觉语义分割</p>
<ul>
<li><strong>Stepping Stones: A Progressive Training Strategy for AudioVisual Semantic Segmentation [47]</strong>：提出了一种逐步训练策略，用于音频视觉语义分割。</li>
</ul>
<p>音频视觉问答</p>
<ul>
<li><strong>AVQA: A Dataset for Audio-Visual Question Answering on Videos [72]</strong>：一个音频视觉问答数据集，关注如何结合音频和视觉信息回答问题。</li>
</ul>
<p>音频视觉分割模型</p>
<ul>
<li><strong>VideoLLaMA 2: Advancing SpatialTemporal Modeling and Audio Understanding in VideoLLMs [15]</strong>：提出了一种视频语言模型，用于增强视频中的时空建模和音频理解能力。</li>
<li><strong>video-SALMONN: Speech-Enhanced Audio-Visual Large Language Models [58]</strong>：一种增强型音频视觉大语言模型，通过语音增强来提升音频视觉理解能力。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p><strong>A</strong>: 论文通过以下方法来解决上述问题：</p>
<p>1. 提出 OmniAVS 数据集</p>
<ul>
<li><strong>多模态表达式</strong>：OmniAVS 数据集包含 2,104 个视频和 61,095 个多模态表达式，支持 8 种不同类型的多模态表达式，这些表达式可以灵活组合文本、语音、声音和图像。这使得数据集能够支持更复杂的多模态交互和推理任务。</li>
<li><strong>深度理解音频内容</strong>：与以往的数据集（如 Ref-AVS）不同，OmniAVS 强调对音频内容的深度理解，而不仅仅是检测声音的存在。例如，表达式“Who is most likely to be sick?” 要求模型理解咳嗽声可能暗示生病，而不仅仅是识别声音的存在。</li>
<li><strong>复杂推理和解释</strong>：OmniAVS 数据集包含需要复杂推理的表达式，并为这些表达式提供详细的解释。这促使模型不仅能够进行分割，还能解释其决策过程。例如，对于表达式“Who is most likely to be sick?”，模型需要理解咳嗽声与生病之间的关系，并提供相应的解释。</li>
</ul>
<p>2. 提出 OISA 模型</p>
<ul>
<li><strong>多模态大语言模型（MLLM）</strong>：OISA 基于多模态大语言模型（MLLM），能够处理文本、语音、声音和图像等多种模态输入。MLLM 的强大语言理解和推理能力使得 OISA 能够处理复杂的多模态表达式。</li>
<li><strong>音频视觉交错（Audio-Visual Interleaving）</strong>：为了实现音频和视频的精确对齐，OISA 采用了音频视觉交错策略。该策略将音频 tokens 分成多个片段，并将这些片段交错插入到视频 tokens 中，从而实现音频和视频的同步，而无需引入额外的参数。</li>
<li><strong>查询传播（Query Propagation）</strong>：在分割过程中，OISA 采用查询传播机制来处理视频中的动态运动。这种方法允许查询在每一帧上在线更新，从而更好地捕捉目标对象的运动轨迹，避免了单一查询在处理快速运动目标时可能出现的 ID 切换问题。</li>
<li><strong>掩码头（Mask Head）</strong>：OISA 使用了一个轻量级的掩码头，结合 ViT-Adapter 提取的多尺度特征，通过像素解码器和掩码解码器生成最终的分割掩码。这种设计既保证了分割的准确性，又提高了模型的效率。</li>
</ul>
<p>3. 实验验证</p>
<ul>
<li><strong>数据集划分</strong>：OmniAVS 数据集被划分为训练集和测试集，包含 1,864 个训练视频和 240 个测试视频，以及相应的表达式。</li>
<li><strong>评估指标</strong>：采用 J &amp; F 作为分割评估指标，其中 J 衡量区域相似性（IoU），F 评估轮廓准确性。对于不包含目标对象的表达式，J &amp; F 的值根据预测是否为空进行设置。此外，还采用 METEOR 作为文本解释生成的评估指标。</li>
<li><strong>训练细节</strong>：在训练过程中，OISA 使用了多个数据集进行联合训练，包括语义分割数据集、指代分割数据集、指代视频分割数据集和音频视觉分割数据集。通过 LoRA 技术对 LLM 进行微调，并训练掩码头的所有参数，同时冻结其他参数。</li>
<li><strong>实验结果</strong>：OISA 在 OmniAVS 数据集上取得了 41.1% 的平均 J &amp; F 分数，显著优于其他现有方法。在不同模态组合的分割任务中，OISA 均表现出色，尤其是在包含多种模态的表达式（如文本+声音+图像）上，取得了最高的分割分数。此外，OISA 在文本解释生成任务上也取得了 21.7% 的 METEOR 分数，表明其在理解表达式并生成合理解释方面具有优势。</li>
</ul>
<p>通过这些方法，论文不仅解决了多模态信息融合和深度理解的问题，还提升了模型在复杂推理任务中的表现，并提高了模型在实际应用中的灵活性和交互能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p><strong>A</strong>: 论文中进行了以下实验：</p>
<p>1. 数据集划分与评估指标</p>
<ul>
<li><strong>数据集划分</strong>：OmniAVS 数据集被划分为训练集和测试集，包含 1,864 个训练视频和 240 个测试视频，以及相应的表达式。</li>
<li><strong>评估指标</strong>：采用 J &amp; F 作为分割评估指标，其中 J 衡量区域相似性（IoU），F 评估轮廓准确性。对于不包含目标对象的表达式，J &amp; F 的值根据预测是否为空进行设置。此外，还采用 METEOR 作为文本解释生成的评估指标。</li>
</ul>
<p>2. 训练细节</p>
<ul>
<li><strong>音频文本对齐</strong>：使用自动语音识别（ASR）数据集和音频字幕数据集对音频编码器 MLP 进行训练，以对齐音频特征与 LLM 空间。</li>
<li><strong>多数据集联合训练</strong>：使用多个数据集进行联合训练，包括语义分割数据集（如 ADE20K、COCO-Stuff）、指代分割数据集（如 RefCOCO、RefCOCO+、RefCOCOg）、指代视频分割数据集（如 Refer-YouTube-VOS、MeViS、ReVOS）和音频视觉分割数据集（如 Ref-AVS Bench、OmniAVS）。通过 LoRA 技术对 LLM 进行微调，并训练掩码头的所有参数，同时冻结其他参数。</li>
</ul>
<p>3. 消融研究</p>
<ul>
<li><p><strong>音频视觉融合策略</strong>：</p>
<ul>
<li><strong>注意力机制</strong>：使用简单的交叉注意力块，将图像 tokens 作为 key，音频 tokens 作为 query/value。</li>
<li><strong>加权求和</strong>：将音频 tokens 填充到与视觉 tokens 相同的长度，然后进行加权求和。</li>
<li><strong>音频视觉交错（AVI）</strong>：将音频 tokens 分成多个片段，并将这些片段交错插入到视频 tokens 中，实现音频和视频的同步。</li>
<li><strong>AVI + Concat</strong>：在音频视觉交错的基础上，将原始音频 tokens 附加到交错序列的末尾，以补充完整的音频 tokens。</li>
</ul>
<p>实验结果表明，音频视觉交错（AVI）策略显著提高了模型性能，特别是在需要精确音频视觉对齐的任务中。</p>
</li>
<li><p><strong>掩码头设计</strong>：</p>
<ul>
<li><strong>单 token 分割所有帧（OTSA）</strong>：使用同一个 [SEG] token 对每帧进行独立分割。</li>
<li><strong>查询传播（QP）</strong>：允许查询在每一帧上在线更新，以更好地捕捉目标对象的运动轨迹。</li>
<li><strong>Mask2Former（M2F）</strong>：使用 Mask2Former 作为掩码头，提高了模型的效率。</li>
</ul>
<p>实验结果表明，查询传播（QP）机制显著提高了分割质量，特别是在处理动态运动目标时。</p>
</li>
</ul>
<p>4. OmniAVS 基准测试</p>
<ul>
<li><strong>与现有方法的比较</strong>：在 OmniAVS 数据集上，OISA 与其他现有方法（如 LMPM、EEMC、MUTR、LISA-7B、LISA-13B）进行了比较。OISA 在所有模态组合上均取得了最高的 J &amp; F 分数，平均分数为 41.1%，显著优于其他方法。在文本解释生成任务上，OISA 也取得了 21.7% 的 METEOR 分数，表明其在理解表达式并生成合理解释方面具有优势。</li>
<li><strong>不同模态组合的性能分析</strong>：OISA 在包含多种模态的表达式（如文本+声音+图像）上取得了最高的分割分数，表明多模态输入可以显著提高模型的性能。</li>
</ul>
<p>5. 相关任务的测试</p>
<ul>
<li><strong>Ref-AVS 数据集</strong>：OISA 在 Ref-AVS 数据集上取得了 58.0% 的 J &amp; F 分数，显著优于其他现有方法，表明其在音频视觉分割任务上的强大性能。</li>
<li><strong>指代图像分割</strong>：OISA 在 refCOCO 系列数据集上取得了 74.2%、78.3% 和 72.0% 的 cIoU 分数，显著优于其他现有方法，表明其在指代图像分割任务上的强大性能。</li>
<li><strong>指代视频分割</strong>：OISA 在 MeViS、R-YTVOS、R-DAVIS17 和 ReVOS 数据集上取得了 43.2%、62.1%、65.2% 和 47.3% 的 J &amp; F 分数，显著优于其他现有方法，表明其在指代视频分割任务上的强大性能。</li>
</ul>
<p>6. 定性结果与失败案例分析</p>
<ul>
<li><strong>成功案例</strong>：展示了 OISA 在处理复杂音频视觉场景时的成功案例，如在对话场景中识别关键对话并进行推理，以及在多模态表达式中理解图像和声音内容。</li>
<li><strong>失败案例</strong>：分析了 OISA 在处理复杂声音场景时的失败案例，如在多个对象同时发声时的音频事件解耦问题，表明在音频理解方面仍有改进空间。</li>
</ul>
<p>通过这些实验，论文验证了 OISA 模型在多模态音频视觉分割任务中的有效性和优越性，并指出了未来研究的方向。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p><strong>A</strong>: 论文提出了多个未来工作的方向，以下是一些可以进一步探索的点：</p>
<p>1. 更高效的音频视觉融合方法</p>
<ul>
<li><strong>学习更鲁棒的音频视觉表示</strong>：当前的音频视觉融合方法虽然在一定程度上实现了音频和视觉信息的对齐，但仍然存在改进空间。可以探索更先进的融合技术，例如通过自监督学习或对比学习来学习更鲁棒的音频视觉表示。</li>
<li><strong>多模态特征融合</strong>：研究如何更有效地结合音频、视觉和文本特征，以提高模型对复杂场景的理解能力。例如，可以探索多模态特征融合的新架构或新的融合策略。</li>
</ul>
<p>2. 音频事件解耦</p>
<ul>
<li><strong>音频事件解耦</strong>：在多个对象同时发声的情况下，如何解耦不同的音频事件是一个关键问题。可以研究音频事件解耦技术，例如通过深度聚类或语音分离技术来分离不同的音频源，从而提高模型对音频内容的理解能力。</li>
<li><strong>音频场景分析</strong>：进一步探索音频场景分析技术，以更好地理解音频信号中的语义信息。例如，可以研究如何结合音频信号的时频特征和语义特征，以实现更准确的音频事件识别。</li>
</ul>
<p>3. 多模态表达式的有效组合</p>
<ul>
<li><strong>多模态表达式的联合表示</strong>：研究如何为多模态表达式创建更有效的联合表示，以更好地利用不同模态之间的互补信息。例如，可以探索多模态表达式的嵌入方法或跨模态融合技术。</li>
<li><strong>多模态交互</strong>：研究如何设计更自然的多模态交互方式，使模型能够更好地理解用户的意图。例如，可以探索多轮对话系统，允许用户通过语音、文本和图像等多种方式与模型进行交互。</li>
</ul>
<p>4. 更先进的分割模型</p>
<ul>
<li><strong>处理复杂场景</strong>：当前的分割模型在处理复杂场景（如遮挡、消失和重新出现）时仍存在挑战。可以研究更先进的分割模型，例如通过引入注意力机制或图神经网络来提高模型对复杂场景的处理能力。</li>
<li><strong>实时分割</strong>：探索实时音频视觉分割技术，以满足实际应用中的实时性要求。例如，可以研究轻量级的分割模型或优化模型的计算效率。</li>
</ul>
<p>5. 更大规模的音频视觉数据集</p>
<ul>
<li><strong>预训练</strong>：在更大的音频视觉数据集上预训练 OISA 模型，以进一步提高其泛化能力。可以探索如何构建更大规模的音频视觉数据集，并研究如何在这些数据集上进行有效的预训练。</li>
<li><strong>跨模态预训练</strong>：研究跨模态预训练技术，以提高模型在不同模态之间的迁移能力。例如，可以探索如何在音频、视觉和文本数据之间进行跨模态预训练。</li>
</ul>
<p>6. 交互式和对话式能力</p>
<ul>
<li><strong>多轮交互</strong>：开发更交互式和对话式的模型，使模型能够与用户进行多轮交互，以澄清和细化用户的指代表达式。例如，可以研究多轮对话系统的设计和实现。</li>
<li><strong>用户反馈</strong>：研究如何利用用户反馈来改进模型的性能。例如，可以探索如何通过用户反馈来调整模型的预测结果，从而提高模型的准确性和用户满意度。</li>
</ul>
<p>这些方向不仅可以进一步提升模型在音频视觉分割任务中的性能，还可以为多模态交互和理解的研究提供新的思路和方法。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>A</strong>: 本文的核心内容是提出了一个名为OmniAVS（OmniAVS）的新型数据集和一个基于多模态大语言模型（MLLM）的基准模型Omnimodal Instructed Segmentation Assistant（OISA），旨在推动音频视觉分割（RAVS）领域的发展，特别是在多模态信息融合、深度理解和复杂推理方面。</p>
<p>背景知识</p>
<p>RAVS是一个新兴领域，目标是利用音频和视觉线索，根据指代表达式（如文本）在音频视觉场景中分割出目标对象。尽管该领域已经取得了一定的进展，但现有的数据集和模型在多模态信息融合、音频内容的深度理解以及复杂推理方面仍存在局限性。例如，Ref-AVS数据集主要关注声音的基本特性，而缺乏对声音内容的复杂推理。</p>
<p>OmniAVS数据集</p>
<p>OmniAVS数据集包含2,104个视频和61,095个多模态指代表达式，支持8种不同类型的多模态表达式，包括文本、语音、声音和图像的组合。该数据集的三个主要创新点如下：</p>
<ol>
<li><strong>多模态表达式</strong>：支持文本、语音、声音和图像的灵活组合，提供了更丰富的交互界面。</li>
<li><strong>深度理解音频内容</strong>：强调对音频内容的深度理解，而不仅仅是检测声音的存在。例如，通过表达式“Who is most likely to be sick?”来推断谁可能生病了。</li>
<li><strong>复杂推理和解释</strong>：包含需要复杂推理的表达式，并为这些表达式提供详细的解释，促使模型不仅能够进行分割，还能解释其决策过程。</li>
</ol>
<p>OISA模型</p>
<p>OISA是一个基于MLLM的基准模型，能够处理多模态输入并执行基于推理的分割任务。模型的主要组件和策略包括：</p>
<ul>
<li><strong>多模态大语言模型（MLLM）</strong>：用于理解文本、语音、声音和图像等多种模态输入。</li>
<li><strong>音频视觉交错（Audio-Visual Interleaving）</strong>：将音频tokens分成多个片段，并将这些片段交错插入到视频tokens中，实现音频和视频的精确对齐。</li>
<li><strong>查询传播（Query Propagation）</strong>：允许查询在每一帧上在线更新，以更好地捕捉目标对象的运动轨迹，避免了单一查询在处理快速运动目标时可能出现的ID切换问题。</li>
<li><strong>掩码头（Mask Head）</strong>：结合ViT-Adapter提取的多尺度特征，通过像素解码器和掩码解码器生成最终的分割掩码。</li>
</ul>
<p>实验</p>
<p>实验部分验证了OISA模型在OmniAVS数据集上的性能，并与其他现有方法进行了比较。主要结果如下：</p>
<ul>
<li><strong>OmniAVS基准测试</strong>：OISA在OmniAVS数据集上取得了41.1%的平均J &amp; F分数，显著优于其他现有方法。在文本解释生成任务上，OISA取得了21.7%的METEOR分数，表明其在理解表达式并生成合理解释方面具有优势。</li>
<li><strong>不同模态组合的性能</strong>：OISA在包含多种模态的表达式（如文本+声音+图像）上取得了最高的分割分数，表明多模态输入可以显著提高模型的性能。</li>
<li><strong>相关任务的测试</strong>：OISA在Ref-AVS、refCOCO系列和ReVOS等数据集上也取得了优异的性能，证明了其在多个相关任务上的泛化能力。</li>
</ul>
<p>结论</p>
<p>本文通过提出OmniAVS数据集和OISA模型，推动了RAVS领域的发展，特别是在多模态信息融合、深度理解和复杂推理方面。OISA模型在多个数据集上的优异表现证明了其有效性和优越性。未来的工作将探索更高效的音频视觉融合方法、音频事件解耦、多模态表达式的有效组合以及更先进的分割模型，以进一步提升模型在音频视觉分割任务中的性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p><strong>A</strong>: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.moonshot.cn/_prefill_chat?prefill_prompt= 我们要讨论的论文是Towards%20Omnimodal%20Expressions%20and%20Reasoning%20in%20Referring%20Audio-Visual%20Segmentation，链接是  https://arxiv.org/pdf/2507.22886  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2507.22886  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kaining Ying,Henghui Ding,Guanquan Jie,Yu-Gang Jiang</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2507.22886.pdf">https://arxiv.org/pdf/2507.22886.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2507.22886">https://arxiv.org/abs/2507.22886</a></p>
<p><strong>Arxiv ID</strong>: 2507.22886</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2507.22886">https://papers.cool/arxiv/2507.22886</a></p>
<p><strong>Published</strong>: 2025-07-30T17:59:31Z</p>
<p><strong>Updated</strong>: 2025-07-30T17:59:31.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202507/2025-07-31/">https://wdndev.github.io/daily/hf/202507/2025-07-31/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Towards-Omnimodal-Expressions-and-Reasoning-in-Referring-Audio-Visual-Segmentation"><span class="toc-text">1. Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>