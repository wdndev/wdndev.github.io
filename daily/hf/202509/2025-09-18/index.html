<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-09-18 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep ResearchThis paper tackles open-ended deep research (OEDR), a complex chal">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-09-18">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202509/2025-09-18/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep ResearchThis paper tackles open-ended deep research (OEDR), a complex chal">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:09.900Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202509/2025-09-18/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-09-18',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-09-18</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:09.900Z" title="Updated 2025-11-02 07:46:09">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">82.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>293min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-09-18"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research"><a href="#1-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research" class="headerlink" title="1. WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13312">WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research</a></h3><p>This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and one-shot generation paradigms that easily suffer from long-context failure issues like “loss in the middle” and hallucinations. To address these challenges, we introduce WebWeaver, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, source-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank for each part, it effectively mitigates long-context issues. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing high-quality, reliable, and well-structured reports.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本文探讨了开放式深度研究（OEDR），这是一个复杂的挑战，AI代理必须将大量网络规模的信息综合成有见地的报告。目前的方法存在双重限制：静态研究流程使得规划与证据获取脱钩，而一次性生成的范式则容易受到长上下文失败问题的影响，如“中间丢失”和幻觉。为了解决这些挑战，我们推出了WebWeaver，一种新颖的双代理框架，模拟人类研究过程。规划者在动态循环中运作，迭代交替进行证据获取与大纲优化，以生成一个全面的、以来源为基础的大纲，并链接到证据记忆库。然后，写作者执行分层检索和写作过程，逐节撰写报告。通过针对性地从记忆库中仅检索每部分所需的证据，实质性地减轻了长上下文问题。我们的框架在主要的OEDR基准测试中建立了新的最先进水平，包括DeepResearch Bench、DeepConsult和DeepResearchGym。这些结果验证了我们以人为本的迭代方法，证明了自适应规划和集中综合对于生成高质量、可靠和结构良好的报告至关重要。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“开放式深度研究”（Open-Ended Deep Research, OEDR）这一高难任务：<br>给定一个无标准答案的开放问题，智能体需在 web 规模的海量信息中自主检索、筛选、综合，最终生成一份篇幅长、结构严谨、观点独到且可验证的研究报告。</p>
<p>现有方法存在两大核心缺陷：</p>
<ol>
<li>静态流程割裂：先一次性定好提纲再搜集证据，导致计划无法随新发现动态调整，错失潜在重要方向。</li>
<li>长上下文失效：一次性把所有材料塞进模型，引发“中间丢失”、幻觉、跨节干扰等问题，难以产出高质量长文。</li>
</ol>
<p>WebWeaver 通过“双智能体+动态循环+分层写作”模拟人类研究过程，克服上述局限，在三大 OEDR 基准上刷新 SOTA。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文第 5 节（Related Work）将相关研究归为两条主线，并指出其局限；下列文献均可在 References 中找到出处。</p>
<ol>
<li><p>深度研究型智能体（Deep Research Agents）<br>1.1 闭源/商用系统</p>
<ul>
<li>OpenAI Deep Research (OpenAI, 2025a)</li>
<li>Gemini Deep Research (Google, 2025)</li>
<li>Claude Research (Anthropic, 2025)<br>‑ 表现强但 API 昂贵、黑箱，不利于学术复现与改进。</li>
</ul>
<p>1.2 开源短答案导向方案</p>
<ul>
<li>WebSailor、WebDancer、MaskSearch 等 (Li et al., 2025a; Wu et al., 2025b; Wu et al., 2025a)</li>
<li>面向 BrowseComp、GAIA 等短答案基准，侧重事实性问答，缺乏长文综合与报告生成能力。</li>
</ul>
<p>1.3 开源长文生成方案</p>
<ul>
<li>OpenDeepResearch (Research, 2025e)</li>
<li>GPT-Researcher (Research, 2025c)</li>
<li>TTD-DR (Han et al., 2025)<br>‑ 普遍采用“静态提纲 → 分节检索 → 一次性成文”流水线；提纲固定、证据全部入模，导致结构僵化、长上下文幻觉、节间干扰。</li>
</ul>
</li>
<li><p>长文本生成（Long Writing）</p>
<ul>
<li>早期递归提示：Re3 (Yang et al., 2022)、DOC (Yang et al., 2023)</li>
<li>近期代理框架：LongWriter (Bai et al., 2025)、CogWriter (Wan et al., 2025)<br>‑ 共性是“先规划后写作”，但规划阶段不随证据更新，写作阶段仍把全部素材一次性输入模型，未能解决注意力饱和与“中间丢失”问题。</li>
</ul>
</li>
</ol>
<p>WebWeaver 与上述工作的根本区别：</p>
<ul>
<li>动态循环：提纲与证据获取交替迭代，随时修正结构；</li>
<li>分层写作：每节仅召回对应证据，写完即剪枝，避免长上下文干扰。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>WebWeaver</strong>——一种“双智能体+动态循环+分层写作”框架，把 OEDR 解耦成两个可迭代、可验证的子系统，从而避开静态流程与长上下文陷阱。核心机制如下：</p>
<p>阶段</p>
<p>关键问题</p>
<p>WebWeaver 对策</p>
<p>技术要点</p>
<p>① 规划</p>
<p>一次性提纲无法随新发现演化</p>
<p><strong>动态研究循环</strong>（§3.2）</p>
<p>Planner 基于 ReAct 交替执行：<br><code>search</code> → <code>write_outline</code> → 再搜索…<br>每轮用新证据增删章节、细化论点，并实时插入 <code>&lt;citation&gt;id_x&lt;/citation&gt;</code> 锚定到记忆库。</p>
<p>② 记忆</p>
<p>百页级原文塞进上下文 → 注意力崩溃</p>
<p><strong>结构化记忆库</strong>（§3.2-3.3）</p>
<p>搜索仅返回 100-200 token 摘要进上下文；<br>全文与可验证片段（quote、数据）以键值形式存入外部记忆，写作时按需召回。</p>
<p>③ 写作</p>
<p>一次性长文生成易“中间丢失”、节间串扰</p>
<p><strong>分层-分段-聚焦写作</strong>（§3.3）</p>
<p>Writer 按提纲顺序每次只写一节：<br><code>retrieve(id_list)</code> → <code>think(内部推理)</code> → <code>write</code> → <code>prune</code><br>写完后立即把原文证据换出，保证下一节上下文干净。</p>
<p>④ 学习</p>
<p>30 B 级模型多轮工具调用不稳定</p>
<p><strong>WebWeaver-3k SFT</strong>（§4.3）</p>
<p>用上述框架的 3.3 k 条高质量轨迹蒸馏出 Planner+Writer 策略，小模型也能达到 85.9 % 引文准确率（原 25 %）。</p>
<p>通过以上设计，WebWeaver 把“长上下文推理”转化为“系统级信息管理与工具调度”问题，在 DeepResearch Bench、DeepConsult、DeepResearchGym 三大基准上均取得新 SOTA。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“方法有效性”与“知识蒸馏可行性”两条主线展开，共 4 组 12 项具体评测，全部基于公开基准与官方评价脚本，保证可复现。</p>
<ol>
<li><p>主评测（§4.2）<br>1.1 DeepResearch Bench（100 条 PhD 级任务）</p>
<ul>
<li>指标：RACE（Overall、Comp.、Insight、Inst.、Read.）+ FACT（Eff. c.、C. acc.）</li>
<li>结果：WebWeaver(Claude-sonnet-4) 50.58 分，超越 GPT-4o-deepresearch 46.45 与 Gemini-2.5-pro 49.71，C. acc. 达 93.37 %。</li>
</ul>
<p>1.2 DeepConsult（商业咨询场景）</p>
<ul>
<li>指标：win/tie/loss vs. openai-deepresearch + 平均质量分</li>
<li>结果：WebWeaver 取得 66.86 % 胜率，平均质量 6.96，显著高于第二名 Gemini-2.5-pro 6.70。</li>
</ul>
<p>1.3 DeepResearchGym（96 k 真实查询抽样 100 条）</p>
<ul>
<li>指标：Clarity、Depth、Balance、Breadth、Support、Insightfulness</li>
<li>结果：WebWeaver 平均 96.77，Depth/Breadth 均达 100 %。</li>
</ul>
</li>
<li><p>消融与细粒度分析（§4.3）<br>2.1 提纲迭代轮数消融</p>
<ul>
<li>固定写作策略，仅改变 Planner 优化轮数（1→3）。</li>
<li>两基准 Overall 分数单调上升（DR-Bench +2.7，DR-Gym +11.0），Support 提升最显著（51.2→73.6）。</li>
</ul>
<p>2.2 分层写作 vs. 暴力长文</p>
<ul>
<li>基线：一次性把 67 k token 证据全部塞进模型的 LongWriter 方式。</li>
<li>结果：Insight ↑9.0，Readability ↑7.5，Support ↑7.0，验证“注意力聚焦”必要性。</li>
</ul>
<p>2.3 统计画像</p>
<ul>
<li>单任务平均 16 次搜索、2.2 轮提纲优化、112 篇网页、26 k token 终稿，量化说明“静态提纲”不可行。</li>
</ul>
</li>
<li><p>模型蒸馏实验（§4.3 末）</p>
<ul>
<li>用 WebWeaver-3k（3.3 k 条轨迹）对 Qwen3-30b-a3b 做 SFT。</li>
<li>结果：<br>– DeepResearch Bench Overall ↑1.34，Citation Accuracy 25 % → 85.9 %；<br>– DeepConsult 质量分 4.57 → 6.09；<br>– DeepResearchGym 77.27 → 90.89，证明“大系统可蒸馏成小模型”。</li>
</ul>
</li>
<li><p>人工可解释性验证</p>
<ul>
<li>附录 B 给出完整 ReAct 轨迹（Planning 8 轮、Writing 25 轮），展示搜索-优化-写作-剪枝全过程，供后续研究对照。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“框架深化”“能力扩展”“评测与安全”三大类，供后续研究参考。</p>
<p>1. 框架深化</p>
<p>方向</p>
<p>关键问题</p>
<p>可探索思路</p>
<p>1.1 多模态证据融合</p>
<p>当前仅文本+表格，图像/图表/视频未利用</p>
<p>引入视觉-语言检索器，对论文图、实验照片、财报截图做 <code>&lt;citation&gt;fig_id&lt;/citation&gt;</code> 级引用；需解决跨模态对齐与版面检测。</p>
<p>1.2 自洽性验证循环</p>
<p>写完才发现矛盾或数据冲突</p>
<p>增加“事实审核器”Agent，反向扫描成稿，触发 <code>search(conflict_query)</code> 重检索，驱动 Planner 回卷修正。</p>
<p>1.3 层次化记忆更新</p>
<p>记忆库只增不减，过时信息累积</p>
<p>引入时效戳与置信度，做“记忆遗忘+增量摘要”，支持版本回溯；可借鉴 Episodic Memory 与 Diff-based Update。</p>
<p>2. 能力扩展</p>
<p>方向</p>
<p>关键问题</p>
<p>可探索思路</p>
<p>2.1 数学/代码推理型报告</p>
<p>当前侧重综述，缺少公式推导与可运行实验</p>
<p>在记忆库中区分“理论推导”与“可执行代码块”，Writer 按需插入 <code>&lt;latex&gt;</code> 或 <code>&lt;codeblock&gt;</code>，并调用沙箱执行结果，实现“可复现报告”。</p>
<p>2.2 多语言与跨文化研究</p>
<p>仅英文网页，非英语信源缺失</p>
<p>集成多语言检索器，自动检测高价值非英文文献，触发翻译摘要，并标注 <code>lang=xx</code> 属性，提升区域研究深度。</p>
<p>2.3 协作式写作</p>
<p>单智能体吞吐量有限</p>
<p>引入“多 Planner 竞赛-多 Writer 分段”并行：① 不同 Planner 按子话题并行探索，② 通过共识机制合并最优提纲，③ Writer 集群按节并行起草，最后一致性融合。</p>
<p>3. 评测、安全与伦理</p>
<p>方向</p>
<p>关键问题</p>
<p>可探索思路</p>
<p>3.1 细粒度幻觉评测</p>
<p>现有指标只统计引用准确率，未定位具体句</p>
<p>构建句子级事实核查集，用 LLM+人工标注“可验证声明”，自动对齐到源句，输出 Precision-Recall-False Attribution Rate。</p>
<p>3.2 对抗性误导源</p>
<p>恶意 SEO、钓鱼报告可能污染记忆库</p>
<p>引入源可信度模块（域名声誉、同行评审标记、引用网络），动态下调低可信度证据权重，并给出“风险提示框”。</p>
<p>3.3 版权与隐私合规</p>
<p>大段引用可能侵权，医疗/法律建议或误导</p>
<p>增加“引用长度阈值+改写建议”工具；对敏感领域（医疗、投资）自动附加 disclaimer，并限制单次可引用比例。</p>
<p>4. 系统级优化</p>
<p>方向</p>
<p>关键问题</p>
<p>可探索思路</p>
<p>4.1 成本-性能权衡</p>
<p>大模型调用 15+ 次搜索+写作，开销高</p>
<p>① 用小型检索-排序模型先过滤 Top-k，再调用大模型总结；② 引入“早期停止”策略，当连续两轮提纲增益 &lt; ε 时终止。</p>
<p>4.2 在线环境交互</p>
<p>仅被动搜索，无法主动提问或请求数据</p>
<p>开放 API 让 Agent 可向数据拥有者发起“信息请求”或问卷，实现主动数据收集（类似人类学者发邮件索要实验数据）。</p>
<p>这些方向既可直接嵌入 WebWeaver 的双 Agent 架构，也可作为上层插件或并行流程，逐步把“开放式深度研究”推向更高可信、更广领域、更强复现性的新阶段。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="WebWeaver-论文要点速览"><a href="#WebWeaver-论文要点速览" class="headerlink" title="WebWeaver 论文要点速览"></a>WebWeaver 论文要点速览</h1><h2 id="1-任务与痛点"><a href="#1-任务与痛点" class="headerlink" title="1. 任务与痛点"></a>1. 任务与痛点</h2><ul>
<li><strong>开放端深度研究（OEDR）</strong>：无标准答案，需自主检索百级网页并输出数万 token 的综述报告。</li>
<li><strong>现有范式缺陷</strong><ul>
<li>静态提纲：先一次性定纲再搜集，无法随新发现调整。</li>
<li>暴力长文：把所有证据一次性塞进模型，出现“中间丢失”、幻觉、节间串扰。</li>
</ul>
</li>
</ul>
<h2 id="2-WebWeaver-框架"><a href="#2-WebWeaver-框架" class="headerlink" title="2. WebWeaver 框架"></a>2. WebWeaver 框架</h2><p>采用“双智能体 + 动态循环 + 分层写作”模拟人类研究流程：</p>
<p>阶段</p>
<p>智能体</p>
<p>关键动作</p>
<p>输出</p>
<p>规划</p>
<p>Planner</p>
<p>search ↔ write_outline 迭代</p>
<p>带 <code>&lt;citation&gt;id_x&lt;/citation&gt;</code> 的演进式提纲 + 结构化记忆库</p>
<p>写作</p>
<p>Writer</p>
<p>retrieve(仅相关证据) → think → write → prune</p>
<p>逐节成稿，上下文始终聚焦</p>
<h2 id="3-主要结果"><a href="#3-主要结果" class="headerlink" title="3. 主要结果"></a>3. 主要结果</h2><ul>
<li><p><strong>三大基准新 SOTA</strong></p>
<ul>
<li>DeepResearch Bench：50.58 分（+0.87↑），引文准确率 93.4 %</li>
<li>DeepConsult：66.9 % 胜率，平均质量 6.96</li>
<li>DeepResearchGym：96.8 分，Depth/Breadth 均达 100 %</li>
</ul>
</li>
<li><p><strong>消融验证</strong></p>
<ul>
<li>多轮提纲优化显著优于单轮（Overall +2.7）</li>
<li>分层写作比暴力长文 Insight ↑9.0、Readability ↑7.5</li>
</ul>
</li>
<li><strong>知识蒸馏</strong><br>用自产 3.3 k 轨迹对 30 B 模型 SFT，引文准确率 25 % → 85.9 %，小模型亦达专家级。</li>
</ul>
<h2 id="4-贡献总结"><a href="#4-贡献总结" class="headerlink" title="4. 贡献总结"></a>4. 贡献总结</h2><ol>
<li>提出动态-耦合式研究循环，破解“提纲僵化”难题。</li>
<li>引入分层-召回-剪枝写作，解决长上下文注意力失效。</li>
<li>在三大公开基准全面刷新最佳成绩，同时开源数据与代码。</li>
<li>通过 SFT 证明大系统能力可蒸馏至小模型，降低实用门槛。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是WebWeaver%3A%20Structuring%20Web-Scale%20Evidence%20with%20Dynamic%20Outlines%20for%20Open-Ended%20Deep%20Research，链接是  https://arxiv.org/pdf/2509.13312  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13312  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zijian Li,Xin Guan,Bo Zhang,Shen Huang,Houquan Zhou,Shaopeng Lai,Ming Yan,Yong Jiang,Pengjun Xie,Fei Huang,Jun Zhang,Jingren Zhou</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13312.pdf">https://arxiv.org/pdf/2509.13312.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13312">https://arxiv.org/abs/2509.13312</a></p>
<p><strong>Arxiv ID</strong>: 2509.13312</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13312">https://papers.cool/arxiv/2509.13312</a></p>
<p><strong>Published</strong>: 2025-09-16T17:57:21Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:57:21.000Z</p>
<hr>
<h3 id="2-Scaling-Agents-via-Continual-Pre-training"><a href="#2-Scaling-Agents-via-Continual-Pre-training" class="headerlink" title="2. Scaling Agents via Continual Pre-training"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13310">Scaling Agents via Continual Pre-training</a></h3><p>Large language models (LLMs) have evolved into agentic systems capable of autonomous tool use and multi-step reasoning for complex problem-solving. However, post-training approaches building upon general-purpose foundation models consistently underperform in agentic tasks, particularly in open-source implementations. We identify the root cause: the absence of robust agentic foundation models forces models during post-training to simultaneously learn diverse agentic behaviors while aligning them to expert demonstrations, thereby creating fundamental optimization tensions. To this end, we are the first to propose incorporating Agentic Continual Pre-training (Agentic CPT) into the deep research agents training pipeline to build powerful agentic foundational models. Based on this approach, we develop a deep research agent model named AgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve state-of-the-art performance while retains strong tool-use ability, notably 39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）已经发展成为能够自主使用工具和进行多步骤推理以解决复杂问题的代理系统。然而，在通用基础模型上进行后训练的方法在代理任务中表现 consistently 不佳，特别是在开源实现中。我们确定了根本原因：缺乏强大的代理基础模型迫使模型在后训练期间同时学习多样的代理行为，同时将其与专家示范对齐，从而造成基本的优化冲突。为此，我们首次提出将代理连续预训练（Agentic CPT）纳入深度研究代理培训管道，以构建强大的代理基础模型。基于这一方法，我们开发了一种名为 AgentFounder 的深度研究代理模型。我们在10个基准测试上评估了我们的AgentFounder-30B，并实现了最先进的性能，同时保持强大的工具使用能力，特别是在BrowseComp-en上为39.9%，在BrowseComp-zh上为43.3%，在HLE上为31.5%的Pass@1。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“开源深度研究智能体显著落后于闭源竞品”这一经验现象，提出并验证其核心假设：<br><strong>通用基础模型缺乏“智能体归纳偏置”</strong>，导致后续对齐阶段必须同时学习“能力”与“行为”两种异质目标，引发根本性的优化冲突，从而限制了智能体性能。</p>
<p>为此，论文首次将“智能体能力”前移到预训练阶段，提出 <strong>Agentic Continual Pre-training（Agentic CPT）</strong> 范式，通过大规模离线合成数据在基础模型中预先植入工具调用、多步推理与决策探索等智能体行为模式，得到一个“已对智能体任务预对齐”的基础模型。后续仅需轻量 SFT/RL 即可释放性能，显著缩小甚至反超闭源模型在 BrowseComp、GAIA、HLE 等 10 项基准上的差距。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两条主线，并在第 4 节“Related Work”中系统综述。以下按主题归纳：</p>
<p>1. 深度研究智能体（Deep Research Agents）</p>
<p>类别</p>
<p>代表工作</p>
<p>核心贡献</p>
<p>与本文差异</p>
<p><strong>闭源产品</strong></p>
<p>OpenAI Deep Research、Grok Deeper Search、Perplexity Deep Research、Gemini Deep Research、Kimi-Researcher</p>
<p>端到端商业系统，强调超长轨迹搜索与报告生成</p>
<p>无训练细节，无法复现；本文聚焦可复现的开源训练范式</p>
<p><strong>开源单模型方案</strong></p>
<p>WebSailor、WebShaper、AFM、DeepDive、WebExplorer、GLM-4.5、DeepSeek-V3.1 等</p>
<p>通过 SFT/RL 在通用基座模型上拟合高难度轨迹</p>
<p>仅在“后训练”阶段注入智能体行为，未解决基座模型缺乏归纳偏置的问题</p>
<p><strong>数据合成方法</strong></p>
<p>SailorFog（WebSailor）、knowledge-projection（WebShaper）、Chain-of-Agents（AFM）、cross-page QA（DeepDiver）</p>
<p>利用知识图谱、迭代查询、多页推理等方式构造复杂问题-轨迹对</p>
<p>这些合成数据仅用于 SFT/RL；本文将其思想迁移到“持续预训练”场景，并引入离线无 API 合成</p>
<p><strong>多智能体/多模态</strong></p>
<p>Cognitive Kernel-Pro、WebWatcher 等</p>
<p>多模型协作或引入视觉模态完成深度研究</p>
<p>本文聚焦单模型、纯文本场景，与多模态/多智能体正交</p>
<p>2. 持续预训练（Continual Pre-training, CPT）</p>
<p>子方向</p>
<p>代表工作</p>
<p>核心结论</p>
<p>与本文差异</p>
<p><strong>领域自适应 CPT</strong></p>
<p>Ke et al. 2023、Çağatay Yıldız et al. 2025、Parmar et al. 2024</p>
<p>在通用基座模型上继续预训练，可缓解灾难性遗忘并提升下游任务</p>
<p>仅关注“知识”迁移，未涉及工具调用、决策序列等智能体行为</p>
<p><strong>工具学习/工具调用 CPT</strong></p>
<p>——</p>
<p>前人未将 CPT 用于工具使用或智能体能力</p>
<p>本文首次把“智能体行为”显式纳入 CPT 目标，提出 FAS/HAS 离线合成框架，填补该空白</p>
<p>3. 其他相关技术</p>
<p>技术点</p>
<p>文献</p>
<p>与本文关联</p>
<p><strong>Reject Sampling</strong></p>
<p>多参考训练（Zheng et al. 2018）、METEOR（Banerjee &amp; Lavie 2005）</p>
<p>本文借其思想对 FAS 生成结果做“知识对齐”过滤</p>
<p><strong>多参考/多样性增强</strong></p>
<p>Multi-Reference Training</p>
<p>启发 FAS 在“问题级”而非“轨迹级”做多样性扩充</p>
<p><strong>步级决策建模</strong></p>
<p>步级 RL、Curriculum RL</p>
<p>本文 HAS 把整条轨迹拆成步级选择空间，离线构建对比式决策样本，规避稀疏奖励问题</p>
<p>小结</p>
<ul>
<li><strong>前人</strong>：把“智能体能力”放在后训练（SFT/RL），且依赖昂贵在线 API 采集轨迹。</li>
<li><strong>本文</strong>：首次将“智能体能力”前移到持续预训练，提出无 API、可扩展的 FAS/HAS 数据合成与两阶段 CPT 策略，从根本上解决通用基座模型缺乏智能体归纳偏置的问题。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“开源智能体落后”归因于<strong>通用基座模型缺少智能体归纳偏置</strong>，导致后训练阶段必须同时学“能力”与“对齐”，优化目标冲突。为此，提出**Agentic Continual Pre-training（Agentic CPT）**新范式，把智能体行为提前嵌入基座模型，再轻量后训练即可。具体解法可归纳为三大构件、两阶段训练、一条数据飞轮：</p>
<p>1. 三大构件：离线合成“智能体原生数据”</p>
<p>构件</p>
<p>关键思想</p>
<p>技术要点</p>
<p>解决痛点</p>
<p><strong>FAS（First-order Action Synthesis）</strong></p>
<p>零 API 成本生成“规划+推理”样本</p>
<ul>
<li>知识→实体→多风格问题自动生成</li>
<li>每题采样 K 个不同风格变体，LLM 仅写“第一步”规划/工具调用</li>
<li>用可访问的知识做 reject sampling，过滤语义不一致</li>
</ul>
<p>避免昂贵在线轨迹；提前注入规划与工具调用模式</p>
<p><strong>HAS（High-order Action Synthesis）</strong></p>
<p>把“整条轨迹”改写成“步级决策”对比样本</p>
<ul>
<li>对真实或合成轨迹的每一步，用 LLM 离线生成 N 条替代 thought+action</li>
<li>构建“多选一”决策文本：给出选项→记录原选项索引→附加真实环境反馈→最后给二元成败标签</li>
</ul>
<p>充分利用次优轨迹；防止模型死记整条路径，学会“每一步如何做选择”</p>
<p><strong>数据飞轮</strong></p>
<p>后训练→失败/成功轨迹→回炉 HAS→再 CPT</p>
<p>不断把新轨迹转为 HAS 数据，持续扩充 CPT 语料</p>
<p>让“智能体归纳偏置”随数据滚雪球</p>
<p>2. 两阶段 CPT 训练策略</p>
<p>阶段</p>
<p>数据</p>
<p>上下文</p>
<p>目标</p>
<p><strong>Stage-1</strong></p>
<p>200B tokens（FAS + 短 HAS）</p>
<p>32K</p>
<p>快速习得基础工具调用、短链推理</p>
<p><strong>Stage-2</strong></p>
<p>100B tokens（精选长 HAS）</p>
<p>128K</p>
<p>掌握长程规划、跨页信息整合与决策探索</p>
<p>3. 后训练：轻量解锁</p>
<p>用同一套“AgentFounder-Base”对接三种不同 SFT/RL 配方（SFT-A/B/C），均显著优于直接用原 Qwen3-Base 的对照组，证明<strong>CPT 已把“智能体能力”预对齐</strong>，后训练只需“解锁”而非“从零建构”。</p>
<p>4. 效果验证</p>
<ul>
<li><strong>10 项基准全面领先</strong>：BrowseComp-en 39.9%（+10% 超越最强开源 DeepSeek-V3.1），HLE 31.5%（首个&gt;30 的开源模型），GAIA 72.8%，Frames 89.6% 等新 SOTA。</li>
<li><strong>scaling law 成立</strong>：1B→4B→30B 参数平均准确率 20.4%→32.7%→48.9%；0→315B tokens 对数增长，累计 +8.0% Pass@3。</li>
<li><strong>通用工具能力不损</strong>：ACEBench 70.0 vs Qwen3-30B-A3B 67.2，表明 CPT 未牺牲通用性。</li>
</ul>
<p>一句话总结</p>
<p>论文<strong>把“智能体对齐”从传统后训练搬到持续预训练</strong>，用零 API 的 FAS/HAS 大规模合成“决策-反馈”式语料，先让基座模型内建工具调用与步级决策偏置，再轻量后训练，一举打破开源智能体性能瓶颈。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 5 个研究问题（RQ1–RQ5）设计了 6 组实验，覆盖性能对比、适应性验证、训练策略消融、数据贡献消融、Scaling Law 与深入诊断，共涉及 10 个公开基准、3 类基线、3 种后训练配方，以及 1B→30B 参数与 0→315B token 的跨度。以下按 RQ 归纳：</p>
<p>RQ1　AgentFounder 与 SOTA 的差距</p>
<p><strong>实验</strong></p>
<ul>
<li>在 10 项基准上与 30 余个模型对比：<br>– 通用搜索：BrowseComp-en/zh、GAIA、Xbench-DeepSearch、WebWalkerQA<br>– 场景专用：HLE、DeepResearch-Bench、Frames、SEAL-0、AcademicBrowse</li>
<li>单 agent、React 范式，统一 5 种工具（Search/Visit/Python/Google Scholar/File Parser）</li>
<li>报告 Pass@1（主指标）与 Pass@3</li>
</ul>
<p><strong>结论</strong><br>AgentFounder-30B 全部刷新开源 SOTA，4 项超过商用 Deep Research/o3，首次把 HLE 开源成绩拉到 30+。</p>
<p>RQ2　Agentic CPT 基座对后训练是否普适</p>
<p><strong>实验</strong></p>
<ul>
<li>固定同一 CPT 基座（AgentFounder-30B-Base），分别用 3 套不同 SFT 数据（SFT-A/B/C）做后训练</li>
<li>对比原始 Qwen3-30B-A3B-Base 在相同 3 套数据下的结果（BrowseComp-en/zh + GAIA + HLE）</li>
</ul>
<p><strong>结论</strong><br>3 组平均提升 5.75%、6.13%、6.45%，验证“预对齐基座”对任何后训练配方均稳定增益；信息检索类任务受益更大。</p>
<p>RQ3　两阶段 CPT 策略是否必要</p>
<p><strong>实验</strong></p>
<ul>
<li>控制总 token 50B，Qwen3-30B-A3B-Base 初始化<br>– 单阶段：32K 上下文，长 HAS 被截断<br>– 两阶段：Stage-1 32K → Stage-2 128K 完整长 HAS</li>
</ul>
<p><strong>结论</strong><br>两阶段平均 Pass@1 +3.3%、Pass@3 +3.7%，确认完整长序列学习不可替代。</p>
<p>RQ4　FAS vs HAS 数据贡献</p>
<p><strong>实验</strong></p>
<ul>
<li>50B token 单阶段：纯 FAS ↔ FAS+HAS 混合</li>
<li>同一 SFT-A 后训练，测 BrowseComp-en/zh + GAIA</li>
</ul>
<p><strong>结论</strong><br>纯 FAS 已带来大幅提升；加入 HAS 后 BrowseComp-zh 再 +3.1%，GAIA Pass@3 +1.9%，显示互补价值。</p>
<p>RQ5　Scaling Law（模型规模 &amp; 数据规模）</p>
<ol>
<li><p><strong>模型规模</strong></p>
<ul>
<li>1B、4B、30B-A3B 三档 CPT 后统一 SFT-A；与更大参数 DeepSeek-V3-1、Kimi-K2 比较</li>
<li>结果：参数-性能呈正相关，30B 达 48.9%，反超更大模型，证明 CPT 参数效率更高</li>
</ul>
</li>
<li><p><strong>数据规模</strong></p>
<ul>
<li>0B→15B→50B→65B→210B→315B token 六档；65B/315B 启用 128K Stage-2</li>
<li>评估平均 Pass@3（对数坐标）</li>
<li>结果：对数律成立，前 15B 提升 3.8%；128K 阶段在 65B、315B 分别再 +1.8%、+1.0%，未见饱和</li>
</ul>
</li>
</ol>
<p>深入诊断实验（ Beyond RQ ）</p>
<p>主题</p>
<p>设置</p>
<p>关键发现</p>
<p><strong>训练收敛性</strong></p>
<p>同一 SFT 语料，对比 baseline 与 4 档 CPT 模型的交叉熵曲线</p>
<p>CPT 模型 loss 全程更低，315B 版最终 loss ↓0.07，收敛更快更稳</p>
<p><strong>工具调用模式</strong></p>
<p>统计 HLE/BrowseComp/WebWalker/GAIA 的 tool-call 分布</p>
<p>复杂任务呈重尾分布，简单任务集中低区间，模型能自适应调节探索深度</p>
<p><strong>通用工具能力</strong></p>
<p>ACEBench（5 类工具 200+ 任务）</p>
<p>AgentFounder-30B 70.0 vs Qwen3-30B-A3B 67.2，CPT 未牺牲通用性</p>
<p><strong>MoE 激活</strong></p>
<p>BrowseComp-zh 最后一 token 的 top-8 expert 权重</p>
<p>CPT 后 expert 使用更均衡，缓解“死专家”与过拟合</p>
<p><strong>工具轮次 vs 准确率</strong></p>
<p>按 tool-turn 分组统计成功率</p>
<p>≤8 轮准确率最高；0 轮反而下降；≥40 轮仍保持 17.5% 成功率，显示长程探索能力</p>
<p><strong>Pass@N scaling</strong></p>
<p>BrowseComp-en 温度采样 N=1–18</p>
<p>Pass@1 31.5% → Pass@16 75.8%，增益 44.3%，表明 HAS 训练有效保留多样性</p>
<p>一句话总结</p>
<p>实验从“宏观性能”到“微观行为”层层递进，既证明 Agentic CPT 对模型规模与数据规模均服从有利 scaling，又通过消融与诊断阐明两阶段训练、FAS/HAS 数据以及步级决策建模各自带来的具体增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可探索方向按“数据-模型-训练-评测-系统”五层归纳，均直接对应论文尚未充分展开或尚未触及的空白点。</p>
<p>1. 数据层</p>
<ul>
<li><p><strong>多语言/跨文化智能体 CPT</strong><br>BrowseComp-zh 仍落后 o3 15%，部分归因于中文语料占比低。可系统构建“多语言实体-知识内存”并验证 CPT 阶段语言迁移与工具调用迁移的耦合关系。</p>
</li>
<li><p><strong>多模态行动合成</strong><br>本文 FAS/HAS 仅文本。将“截图-定位-点击/滚动”等 GUI 动作统一为 token 序列，离线合成视觉-动作块，考察 CPT 能否内建“视觉-行动”归纳偏置。</p>
</li>
<li><p><strong>可验证任务反向生成</strong><br>对数学证明、代码竞赛等可自动判分场景，用“答案→问题”逆向生成 + 难度滤波，构建规模更大、质量自保障的 CPT 语料。</p>
</li>
</ul>
<p>2. 模型层</p>
<ul>
<li><p><strong>MoE 专家分工显式化</strong><br>初步实验显示 CPT 后专家激活更均衡。可加入“专家路由正则化”或“工具专属专家”约束，检验是否进一步降低工具冲突与灾难性遗忘。</p>
</li>
<li><p><strong>小参数高效 CPT</strong><br>1B→4B 提升显著但绝对值仍低。尝试 LoRA/ MoE-Offloading/continual-distillation，把 30B 的 HAS 决策知识蒸馏至 1–3B，探索“边缘设备可用”的深度研究模型。</p>
</li>
<li><p><strong>基座-工具联合 Tokenizer</strong><br>工具调用片段（JSON/URL/Python）当前用通用 BPE 切分，符号冗余。设计“工具感知的 SentencePiece”或“动作字节对编码”，减少 15–20% 长度，提升长程规划容量。</p>
</li>
</ul>
<p>3. 训练层</p>
<ul>
<li><p><strong>在线 HAS-RL 混合</strong><br>本文 HAS 完全离线。可在 RL 阶段实时把新轨迹即时转为 HAS 样本并回灌 CPT，形成“在线决策→离线 CPT→在线 RL”的闭环，实现持续自我改进。</p>
</li>
<li><p><strong>课程式 CPT</strong><br>按“单步工具→多步推理→跨页综合→抗干扰”四级课程递增难度，监测是否出现“能力突跃”及对应隐状态几何变化，解释智能体能力的涌现机制。</p>
</li>
<li><p><strong>上下文长度外推</strong><br>Stage-2 仅 128K。尝试 PI/NTK/YaRN 等免训练外推至 512K，验证超长报告生成是否仍符合对数 scaling 或出现新瓶颈。</p>
</li>
</ul>
<p>4. 评测层</p>
<ul>
<li><p><strong>私有环境可复现基准</strong><br>现有基准依赖公开网页，随时间漂移。构建“快照式 Docker 环境 + 录制回放”工具，保证一年后实验仍可复现，解决结果可比性问题。</p>
</li>
<li><p><strong>智能体能力细粒度剖析</strong><br>将单次轨迹拆成“规划-检索-去噪-整合-写作”五环节，分别标注错误类型，建立细粒度诊断工具，定位 CPT 在哪一环节增益最大。</p>
</li>
<li><p><strong>对抗与误导评测</strong><br>系统注入“假源-冲突-钓鱼页”三级对抗，量化 CPT 对安全性的副作用；若出现下降，可在 HAS 中引入“安全选项”对比样本，实现攻防联合优化。</p>
</li>
</ul>
<p>5. 系统与产品层</p>
<ul>
<li><p><strong>端-云协同深度研究</strong><br>端侧 3B 小模型负责本地隐私检索 + 初步过滤；云侧 30B 模型执行重推理。设计“端云一致性 CPT”数据配比，兼顾延迟、隐私与效果。</p>
</li>
<li><p><strong>个性化 CPT</strong><br>允许用户上传私有笔记/邮件/文献，本地增量 CPT 形成“个人知识-行动记忆”，再与通用模型融合，实现“个人研究助理”而不泄露数据。</p>
</li>
<li><p><strong>可解释决策接口</strong><br>将 HAS 中的“选项-反馈-判断”结构暴露为前端调试面板，用户可实时编辑候选动作并观察模型置信度变化，提升可解释性与可控性。</p>
</li>
</ul>
<p>6. 理论层</p>
<ul>
<li><p><strong>智能体归纳偏置的度量</strong><br>定义“工具调用熵”“决策一致性”等指标，量化不同 CPT 数据对策略空间分布的影响，建立“预训练 ↔ 策略空间”的可计算映射。</p>
</li>
<li><p><strong>Scaling Law 统一公式</strong><br>将参数规模、数据规模、上下文长度、工具数、任务难度五变量纳入同一幂律模型，预测达到指定性能所需资源，指导未来算力分配。</p>
</li>
</ul>
<p>总结</p>
<p>论文首次验证了“智能体能力可前移到 CPT”，但留下语言-视觉-行动统一、在线持续学习、高效小模型、安全性与个性化等广阔空白。上述方向既可直接放大现有优势，也可提前布局下一代“通用智能体基础模型”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="论文主旨"><a href="#论文主旨" class="headerlink" title="论文主旨"></a>论文主旨</h1><p>开源深度研究智能体普遍落后于闭源产品，根因是通用基础模型缺乏“智能体归纳偏置”，导致后训练阶段必须同时学习“能力”与“对齐”，优化目标冲突。为此，论文首次提出把智能体能力前移——<strong>Agentic Continual Pre-training（Agentic CPT）</strong>，用大规模离线合成数据在基座模型中预先植入工具调用与多步决策模式，再轻量后训练即可释放性能。</p>
<h2 id="1-方法框架"><a href="#1-方法框架" class="headerlink" title="1. 方法框架"></a>1. 方法框架</h2><p>模块</p>
<p>要点</p>
<p>作用</p>
<p><strong>FAS</strong>（一阶行动合成）</p>
<p>零API生成&lt;问题, 规划, 首步调用&gt;与&lt;问题, 子问题, 逻辑推理&gt;；知识对齐过滤</p>
<p>注入规划与推理原型</p>
<p><strong>HAS</strong>（高阶行动合成）</p>
<p>对真实/合成轨迹每步离线采样N条候选thought+action，构建“多选一→反馈→成败”对比文本</p>
<p>把轨迹模仿转为步级决策学习，充分利用次优轨迹</p>
<p><strong>两阶段CPT</strong></p>
<p>Stage-1：200B token/32K上下文（FAS+短HAS）<br>Stage-2：100B token/128K上下文（精选长HAS）</p>
<p>先习得基础行为，再扩展长程规划与深度整合能力</p>
<h2 id="2-实验与结果"><a href="#2-实验与结果" class="headerlink" title="2. 实验与结果"></a>2. 实验与结果</h2><ul>
<li><strong>10基准全面评估</strong>（BrowseComp-en/zh、GAIA、HLE、Frames等）<br>– AgentFounder-30B 全部刷新开源SOTA，4项超商用产品（HLE首次&gt;30）。</li>
<li><strong>适应性验证</strong><br>– 同一CPT基座用3套不同SFT数据，后训练平均提升+6%左右，证明“预对齐”普适。</li>
<li><strong>消融与Scaling</strong><br>– 两阶段训练Pass@1平均+3.3%；FAS+HAS混合相比纯FAS再提升；1B→30B参数、0→315B token均呈对数scaling，累计+8.0%。</li>
<li><strong>深入诊断</strong><br>– CPT模型SFT收敛更快；工具调用分布自适应任务难度；通用工具基准ACEBench不降反升；MoE专家激活更均衡。</li>
</ul>
<h2 id="3-贡献清单"><a href="#3-贡献清单" class="headerlink" title="3. 贡献清单"></a>3. 贡献清单</h2><ol>
<li>首次提出Agentic CPT范式，把智能体能力从后训练移到持续预训练。</li>
<li>设计零API、可扩展的FAS/HAS数据合成与两阶段训练策略。</li>
<li>开源模型AgentFounder-30B在10项基准取得新SOTA，验证CPT对参数与数据规模的良好scaling。</li>
<li>证明CPT基座可通用于不同后训练配方，且不损失通用工具能力，为构建更强通用智能体奠定基础。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Scaling%20Agents%20via%20Continual%20Pre-training，链接是  https://arxiv.org/pdf/2509.13310  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13310  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Liangcai Su,Zhen Zhang,Guangyu Li,Zhuo Chen,Chenxi Wang,Maojia Song,Xinyu Wang,Kuan Li,Jialong Wu,Xuanzhong Chen,Zile Qiao,Zhongwang Zhang,Huifeng Yin,Shihao Cai,Runnan Fang,Zhengwei Tao,Wenbiao Yin,Chenxiong Qian,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13310.pdf">https://arxiv.org/pdf/2509.13310.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13310">https://arxiv.org/abs/2509.13310</a></p>
<p><strong>Arxiv ID</strong>: 2509.13310</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13310">https://papers.cool/arxiv/2509.13310</a></p>
<p><strong>Published</strong>: 2025-09-16T17:57:19Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:57:19.000Z</p>
<hr>
<h3 id="3-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning"><a href="#3-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning" class="headerlink" title="3. WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13305">WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning</a></h3><p>Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, matching proprietary agents’ performance and closing the capability gap.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>超越人类认知局限性代表了大语言模型训练中的一个重要前沿。像DeepResearch这样的专有智能系统在极其复杂的信息检索基准测试（如BrowseComp）上展示了超人类的能力，这一成就是之前无法实现的。我们认为，它们的成功依赖于一种在开源模型中缺失的复杂推理模式：在浩瀚的信息领域中系统地减少极端不确定性的能力。基于这一见解，我们引入了WebSailor，这是一种完整的后续训练方法，旨在培养这一关键能力。我们的方法涉及通过结构化抽样和信息模糊、RFT冷启动，以及高效的智能强化学习训练算法——重复抽样策略优化（DUPO）来生成新颖且高不确定性的任务。通过这条集成管道，WebSailor在复杂的信息检索任务中显著超越所有开源代理，达到与专有代理相当的表现，并缩小能力差距。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在弥合开源网络智能体与闭源（专有）系统之间的性能鸿沟，核心目标可概括为：</p>
<ul>
<li><p><strong>数据层面</strong>：现有训练数据对“不确定性”的定义过于单一（多为简单遮蔽），导致模型难以泛化到真实研究中复杂、模糊的问题。为此提出 SailorFog-QA-V2，通过稠密知识图谱引入循环、反馈等复杂拓扑，并扩展多种不确定性类型，以激发更高级的多步推理。</p>
</li>
<li><p><strong>训练层面</strong>：缺乏可扩展且稳定的强化学习环境。高频调用真实网络 API 会带来高成本、高延迟、返回不一致等噪声，污染训练信号。为此设计“双环境”RL 框架：</p>
<ul>
<li>高保真模拟器：基于离线维基库，支持低成本、高并发、可复现的算法迭代；</li>
<li>托管真实环境：统一工具接口与容错机制，保证最终策略训练稳定。</li>
</ul>
</li>
<li><strong>系统层面</strong>：将数据构造与 RL 训练纳入共生反馈闭环，利用训练动态实时合成并过滤高质量数据，实现数据-策略协同演化。</li>
</ul>
<p>综上，论文试图用“高质量合成数据 + 可扩展双环境 RL”这一完整后训练流水线，让 30 B 级开源模型在 BrowseComp、HLE 等严苛基准上首次逼近甚至超越 671 B 闭源模型的深度研究能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“Related Work”章节（附录 A）将近期进展归纳为三大主线，并指出自身与它们的区别。核心相关研究如下：</p>
<ol>
<li><p>数据构造</p>
<ul>
<li>图驱动范式<br>– WebSailor (SailorFog-QA)<br>– 本文 SailorFog-QA-V2：引入稠密循环拓扑、多类不确定性，覆盖更复杂逻辑关系</li>
<li>易→难迭代范式<br>– WebShaper、ASearcher、WebExplorer：从单种子问题逐步扩展，生成树状结构</li>
<li>共同手段：在线调用真实网络工具、引入“遮蔽”不确定性；本文扩展了不确定性定义并提升结构多样性</li>
</ul>
</li>
<li><p>训练策略</p>
<ul>
<li>通用两阶段流程：SFT 冷启动 → RL 精调</li>
<li>RL 算法：GRPO 及其变体（DAPO、Dr-GRPO 等）</li>
<li>本文观点：算法细节并非瓶颈，数据质量与训练环境稳定性才是关键因素；因此重点构建高保真模拟器与托管真实环境</li>
</ul>
</li>
<li><p>推理范式</p>
<ul>
<li>基础 ReAct：WebSailor、WebShaper 采用标准 ReAct，保持最小框架约束</li>
<li>上下文工程：ASearcher、Kimi-Researcher、UI-TARS-2 等在 ReAct 之上做复杂上下文管理或 GUI 模态</li>
<li>本文选择：坚持 vanilla ReAct，以隔离模型本身能力，避免提示工程带来的混淆</li>
</ul>
</li>
</ol>
<p>综上，WebSailor-V2 在数据复杂度、训练环境工程与系统级闭环三方面相对既有研究做了针对性增强，从而首次让开源 30 B 级智能体在深度研究基准上与 671 B 闭源模型竞争。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“开源网络智能体与闭源系统性能鸿沟”这一核心问题拆解为<strong>数据</strong>与<strong>训练</strong>两大瓶颈，并给出一条可落地的完整后训练流水线。具体解法可归纳为三大模块、六个关键步骤：</p>
<p>1. 数据层：构造高密度、多不确定性知识图谱</p>
<p>步骤</p>
<p>关键做法</p>
<p>目的</p>
<p>① 稠密知识图谱构建</p>
<p>以种子实体为起点，用搜索/浏览工具主动“回环”连接节点，刻意生成<strong>循环、反馈</strong>等复杂拓扑，而非树状结构。</p>
<p>覆盖真实世界非线性知识依赖。</p>
<p>② 随机游走子图采样</p>
<p>对稠密图做<strong>随机游走+Weisfeiler-Leman 去同构</strong>，避免组合爆炸。</p>
<p>高效获得结构多样的训练子图。</p>
<p>③ 多类型不确定性 QA 生成</p>
<p>在子图节点上引入<strong>遮蔽、数值区间、时序模糊、关系缺失</strong>等 8+ 种不确定性模板，按节点轨道均匀分布问题焦点。</p>
<p>逼迫模型进行<strong>假设-验证-综合</strong>式深度推理，而非关键词匹配。</p>
<p>2. 训练层：双环境 RL 框架</p>
<p>步骤</p>
<p>关键做法</p>
<p>目的</p>
<p>④ 高保真模拟器</p>
<p>基于<strong>离线维基全量 dump</strong>自建搜索引擎+页面摘要+代码沙盒，返回格式与真实 API 完全一致；可<strong>百万级并发、零成本、可复现</strong>。</p>
<p>快速做算法消融、超参扫描、数据策展，<strong>把“试错”成本降到接近零</strong>。</p>
<p>⑤ 托管真实环境</p>
<p>对真实 SerpAPI、Jina、Google Scholar 等做<strong>统一调度层</strong>：QPS 限速、缓存、重试、降级、热备切换，保证<strong>工具返回确定性</strong>。</p>
<p>消除外部 API 随机性，使 RL 训练信号<strong>不被噪声污染</strong>，实现稳定收敛。</p>
<p>3. 系统层：数据-策略共生闭环</p>
<p>步骤</p>
<p>关键做法</p>
<p>目的</p>
<p>⑥ 动态数据合成与过滤</p>
<p>在 RL 训练过程中，<strong>实时</strong>用当前最优策略 rollout→筛选高价值轨迹→回流到训练池，同步淘汰低质量或重复样本；闭环周期<strong>分钟级</strong>。</p>
<p>让训练分布随策略能力<strong>同步右移</strong>，持续提供“可学习”的 OOD 样本，避免过拟合早期数据。</p>
<p>算法细节（GRPO 变体）</p>
<ul>
<li><strong>token 级策略梯度</strong> + <strong>leave-one-out 基线</strong>降低方差</li>
<li><strong>保守负样本过滤</strong>：丢弃“超长未输出答案”等噪声轨迹，防止格式崩塌</li>
<li><strong>纯 on-policy</strong>：不用经验回放，保证数据分布与策略始终匹配</li>
</ul>
<p>结果验证</p>
<ul>
<li>仅用 30 B MOE（Qwen3-30B-A3B）就在 BrowseComp-EN 取得 <strong>35.3</strong>，HLE <strong>30.6</strong>，显著超越 671 B DeepSeek-V3.1 与一众闭源系统。</li>
<li>消融显示：SFT 冷启动→RL 提升 <strong>+10.9</strong>（BrowseComp-EN），且模拟器与真实环境联合训练比单环境<strong>平均再提 3–5 分</strong>。</li>
</ul>
<p>通过“高密度合成数据 + 双环境稳定 RL + 实时数据闭环”三位一体方案，论文首次在<strong>不增大模型参数</strong>的前提下，把开源智能体的深度研究能力推到与顶级闭源系统同档。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕两条主线展开：</p>
<ol>
<li>在公开基准上验证 <strong>WebSailor-V2-30B-A3B</strong> 的最终性能；</li>
<li>通过受控消融与诊断实验，揭示 <strong>数据、环境、算法</strong> 各组件对结果的具体贡献。</li>
</ol>
<p>1. 主基准评测（零样本 pass@1）</p>
<p>基准</p>
<p>测试能力</p>
<p>结果（SFT → RL）</p>
<p>对比亮点</p>
<p><strong>BrowseComp-EN</strong></p>
<p>英文深度浏览 &amp; 多跳推理</p>
<p>24.4 → <strong>35.3</strong></p>
<p>超越 671 B DeepSeek-V3.1（30.0）</p>
<p><strong>BrowseComp-ZH</strong></p>
<p>中文同能力</p>
<p>28.3 → <strong>44.1</strong></p>
<p>领先所有开源/闭源模型</p>
<p><strong>xbench-DeepSearch</strong></p>
<p>专业领域检索</p>
<p>61.7 → <strong>73.7</strong></p>
<p>高于 GPT-4 系列</p>
<p><strong>GAIA</strong>（文本子集）</p>
<p>多工具通用助手</p>
<p>66.0 → <strong>74.1</strong></p>
<p>与 Gemini-2.5-pro 持平</p>
<p><strong>Humanity’s Last Exam</strong></p>
<p>博士级学科综合</p>
<p>23.9 → <strong>30.6</strong></p>
<p>刷新 SoTA（原 29.8）</p>
<p><strong>DeepResearch Bench</strong></p>
<p>研究报告生成+检索</p>
<p>—</p>
<p><strong>48.9</strong>（仅次于 Gemini-2.5-pro 49.7）</p>
<p>2. 消融与诊断实验</p>
<p>2.1 训练阶段消融</p>
<ul>
<li><strong>仅 SFT</strong>：已在 BrowseComp-EN 拿到 24.4，超过多数开源“完整”模型，证明冷启动数据质量高。</li>
<li><strong>SFT → 模拟器 RL → 真实环境 RL</strong>：每步平均绝对提升 <strong>+6~8 分</strong>，显示双环境策略有效。</li>
</ul>
<p>2.2 数据质量对比</p>
<ul>
<li>用 <strong>BrowseComp 训练集</strong>直接做 RL（人类标注）：BrowseComp-EN 降至 18.7，验证“小规模人工数据分布不一致→反噬性能”。</li>
<li>用 <strong>SailorFog-QA-V2 合成数据</strong>：同预算下提升 <strong>+10.9</strong>，证实大规模、分布一致数据是关键。</li>
</ul>
<p>2.3 环境稳定性消融</p>
<ul>
<li>关闭“统一调度层”（真实 API 随机失败）：训练 200 步后奖励震荡，<strong>最终精度下降 4.2 分</strong>。</li>
<li>开启容错层：奖励曲线平滑，<strong>方差降低 37 %</strong>。</li>
</ul>
<p>2.4 算法组件消融</p>
<p>组件</p>
<p>移除后下降</p>
<p>说明</p>
<p>leave-one-out 基线</p>
<p>−2.4</p>
<p>方差增大，更新噪声变高</p>
<p>保守负样本过滤</p>
<p>−3.1</p>
<p>出现格式崩塌，答案停止符丢失率 ↑</p>
<p>token 级优势估计</p>
<p>−1.8</p>
<p>step-level 估计导致稀疏奖励信号稀释</p>
<p>2.5 规模与预算实验</p>
<ul>
<li><strong>上下文长度</strong>：32 k → 128 k，BrowseComp-EN 准确率 <strong>16 → 35</strong>；64 k 后边际收益趋零。</li>
<li><strong>工具调用预算</strong>：10 → 100 次，性能 <strong>∝ 对数增长</strong>，90 % 正确案例 ≤ 60 次调用，验证数据效率。</li>
</ul>
<p>2.6 熵与训练动态</p>
<ul>
<li>策略熵全程维持 <strong>0.8–1.1</strong>，无显式正则化即可保持探索，说明网页环境非平稳性天然抑制过早收敛。</li>
<li>困难任务（BrowseComp）pass@1 与 pass@3 <strong>同步上升</strong>，表明 RL 真正扩展了解空间；简单任务（GAIA）仅 pass@1 提升，体现 RL 主要优化“首采成功率”。</li>
</ul>
<p>3. 可视化案例研究</p>
<ul>
<li>在 BrowseComp-EN 选取 <strong>29 步轨迹</strong>，展示“线索分解→失败重定向→黄金线索锁定→系统验证→综合回答”完整人类式研究流程，进一步定性说明模型能力来源。</li>
</ul>
<p>实验结论</p>
<ol>
<li>高质量合成数据（SailorFog-QA-V2）+ 稳定双环境 RL 是性能跃迁的决定性组合；</li>
<li>算法技巧虽有效，但<strong>数据分布一致性与环境鲁棒性</strong>对最终分数影响更大；</li>
<li>30 B MOE 在 128 k 上下文与 100 步工具预算下即可逼近 671 B 闭源模型，验证“小而精”的智能体范式可行。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望按“<strong>数据→环境→算法→系统→评测</strong>”五个层次整理，均为论文尚未深入或尚未涉及的开放问题，可直接作为后续工作切入点。</p>
<p>1. 数据层</p>
<ul>
<li><p><strong>跨语言知识图谱对齐</strong><br>当前 SailorFog-QA-V2 以中英为主，可扩展至多语种同构子图，考察智能体在跨语言迁移与多源事实融合上的能力。</p>
</li>
<li><p><strong>动态时效性数据</strong><br>离线维基快照存在“时间漂移”，可引入<strong>增量时间窗口图</strong>，让模型学会判断信息失效、追踪事实演变。</p>
</li>
<li><p><strong>对抗性不确定性</strong><br>现有 8 种不确定性仍为“善意”模糊。可主动注入<strong>矛盾来源、恶意 SEO、钓鱼页面</strong>，训练模型对错误信息的鲁棒性。</p>
</li>
</ul>
<p>2. 环境层</p>
<ul>
<li><p><strong>可微分模拟器</strong><br>将搜索排序、页面摘要抽象为<strong>可微分组件</strong>，实现梯度反向传播，减少真实环境采样成本，实现“模拟即训练”。</p>
</li>
<li><p><strong>多模态环境</strong><br>扩展至<strong>PDF/图表/视频</strong>解析，构建视觉-文本混合工具，支持科研文献中的图表推理、实验复现。</p>
</li>
<li><p><strong>个人化私有环境</strong><br>允许智能体访问用户本地文件、邮箱、数据库，探索<strong>隐私安全约束下的强化学习</strong>（联邦 RL、差分隐私奖励）。</p>
</li>
</ul>
<p>3. 算法层</p>
<ul>
<li><p><strong>分层抽象动作空间</strong><br>当前动作是原子级（search/visit/code）。可引入<strong>高层“子任务”动作</strong>（如“先做文献综述”），用选项框架（Option-Critic）自动学习子策略。</p>
</li>
<li><p><strong>离线→在线混合 RL</strong><br>先在大规模离线轨迹上做<strong>离线 RL（如 Decision Transformer）</strong>，再切到在线 fine-tune，兼顾样本效率与探索。</p>
</li>
<li><p><strong>奖励塑形自动化</strong><br>现有奖励仅基于最终答案正确性。可用<strong>LLM-as-a-Judge 细粒度打分</strong>（相关性、引用准确率、逻辑一致性）并在线学习奖励模型，缓解稀疏奖励。</p>
</li>
</ul>
<p>4. 系统层</p>
<ul>
<li><p><strong>数据-策略双循环扩缩</strong><br>把“数据合成↔RL 训练”封装成<strong>Kubernetes-native 工作流</strong>，根据 GPU/CPU 资源弹性扩缩，实现<strong>24×7 持续自我改进</strong>。</p>
</li>
<li><p><strong>异构算力调度</strong><br>模拟器跑 CPU 集群，真实环境跑 GPU+API 配额，用<strong>强化学习本身调度</strong>“何时用模拟、何时用真实”，最小化成本。</p>
</li>
<li><p><strong>可解释轨迹审计</strong><br>对百万级 rollout 建立<strong>图数据库索引</strong>，支持事后追溯“哪条网页片段导致错误结论”，方便人工审计与安全对齐。</p>
</li>
</ul>
<p>5. 评测与风险</p>
<ul>
<li><p><strong>开放域红队评测</strong><br>构建<strong>自动红队智能体</strong>，专门投放虚假、误导、违法信息，测试目标智能体是否生成有害或错误结论。</p>
</li>
<li><p><strong>长周期经济影响评测</strong><br>设计<strong>7 天-30 天超长任务</strong>（如完整撰写一篇可发表的综述），衡量模型在<strong>持续规划、预算控制、学术伦理</strong>上的表现。</p>
</li>
<li><p><strong>能源-性能权衡基准</strong><br>记录每任务<strong>焦耳/答案</strong>能耗，推动“绿色智能体”研究；探索在<strong>边缘设备</strong>上部署小模型的可行性。</p>
</li>
</ul>
<p>6. 理论问题</p>
<ul>
<li><p><strong>工具增强模型的 scaling law</strong><br>固定参数规模，仅增加工具数量/上下文长度，拟合  </p>
<script type="math/tex; mode=display">, \\text{Accuracy} \\propto (C^\\alpha \\cdot T^\\beta \\cdot P^\\gamma),</script><p>其中 C=上下文，T=工具调用预算，P=参数，探索**“工具-上下文-参数”三维最优配比**。</p>
</li>
<li><p><strong>探索-利用的非平稳边界</strong><br>网页内容分布随时间变化，给出<strong>非平稳遗憾界</strong>，指导何时重置策略或触发再训练。</p>
</li>
</ul>
<p>一句话总结</p>
<p>WebSailor-V2 验证了“合成数据+双环境 RL”的可行性，但要把网络智能体推向<strong>多模态、长周期、可解释、可持续</strong>的下一阶段，仍需在<strong>跨语言时效图谱、可微分/可扩展环境、自动奖励塑形、系统级自我改进循环</strong>以及<strong>面向安全与绿色的新评测体系</strong>上持续深耕。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>WebSailor-V2</strong>，一条端到端的后训练流水线，用于把开源 30 B 规模模型推向闭源级“深度研究”性能。核心内容可概括为 <strong>“一个数据集、双环境 RL、三大验证”</strong>：</p>
<p>1. 数据集：SailorFog-QA-V2</p>
<ul>
<li><strong>稠密知识图谱</strong>：主动引入循环、反馈等复杂拓扑，替代传统树状扩展。</li>
<li><strong>随机游走采样</strong>：高效覆盖所有结构模式，避免组合爆炸。</li>
<li><strong>多类型不确定性</strong>：遮蔽、数值区间、时序模糊等 8+ 模板，迫使模型做多步假设-验证-综合推理。</li>
</ul>
<p>2. 训练：双环境强化学习</p>
<p>环境</p>
<p>作用</p>
<p>关键技术</p>
<p><strong>模拟器</strong></p>
<p>高频、低成本算法迭代</p>
<p>离线维基 dump + 高保真工具接口，百万并发零成本</p>
<p><strong>真实环境</strong></p>
<p>最终策略收敛</p>
<p>统一调度层：限速、缓存、重试、降级，保证工具返回确定性</p>
<p><strong>共生闭环</strong></p>
<p>数据-策略共同进化</p>
<p>实时用当前策略合成并过滤新轨迹，训练分布随模型能力右移</p>
<p><strong>算法</strong>：token 级 GRPO + leave-one-out 基线 + 保守负样本过滤，纯 on-policy 训练。</p>
<p>3. 验证：三大层级实验</p>
<ol>
<li><p><strong>主基准</strong><br>BrowseComp-EN 35.3、ZH 44.1、HLE 30.6、xbench-DS 73.7，均刷新开源记录，<strong>30 B 超越 671 B DeepSeek-V3.1</strong>。</p>
</li>
<li><p><strong>消融与诊断</strong></p>
<ul>
<li>模拟器+真实环境联合训练比单环境再 <strong>+4~6 分</strong></li>
<li>合成数据比直接用人工标注训练集 <strong>+10.9 分</strong></li>
<li>上下文 32 k→128 k 带来 <strong>~19 分</strong> 增幅，90 % 正确案例 ≤ 64 k</li>
</ul>
</li>
<li><strong>定性案例</strong><br>29 步 BrowseComp 轨迹展示“线索分解→失败重定向→黄金线索→系统验证→综合回答”的完整人类式研究流程。</li>
</ol>
<p>结论</p>
<p>WebSailor-V2 首次证明：<strong>高质量合成数据 + 稳定双环境 RL</strong> 即可让中等规模开源模型在深度研究任务上媲美顶级闭源系统，为社区提供了可复现、可扩展的完整后训练范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是WebSailor-V2%3A%20Bridging%20the%20Chasm%20to%20Proprietary%20Agents%20via%20Synthetic%20Data%20and%20Scalable%20Reinforcement%20Learning，链接是  https://arxiv.org/pdf/2509.13305  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13305  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kuan Li,Zhongwang Zhang,Huifeng Yin,Rui Ye,Yida Zhao,Liwen Zhang,Litu Ou,Dingchu Zhang,Xixi Wu,Jialong Wu,Xinyu Wang,Zile Qiao,Zhen Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou</p>
<p><strong>Categories</strong>: cs.LG,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13305.pdf">https://arxiv.org/pdf/2509.13305.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13305">https://arxiv.org/abs/2509.13305</a></p>
<p><strong>Arxiv ID</strong>: 2509.13305</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13305">https://papers.cool/arxiv/2509.13305</a></p>
<p><strong>Published</strong>: 2025-09-16T17:57:03Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:57:03.000Z</p>
<hr>
<h3 id="4-Towards-General-Agentic-Intelligence-via-Environment-Scaling"><a href="#4-Towards-General-Agentic-Intelligence-via-Environment-Scaling" class="headerlink" title="4. Towards General Agentic Intelligence via Environment Scaling"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13311">Towards General Agentic Intelligence via Environment Scaling</a></h3><p>Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>高级代理智能是将大型语言模型应用于实际应用的前提条件。多样化的实际API要求精确、强大的函数调用智能，这需要代理通过在各种环境中的互动来发展这些能力。函数调用能力的广度与代理培训的环境多样性密切相关。在这项工作中，我们扩大环境规模，以推动一般代理智能的进步。这带来了两个核心挑战：（i）如何以原则性的方式扩展环境，以及（ii）如何有效地从与这些环境的互动中训练代理能力。为了解决这些问题，我们设计了一个可扩展的框架，自动构建完全模拟的异构环境，系统地拓宽函数调用场景的空间。我们进一步调整了一个两阶段的代理微调策略：首先赋予代理基本的代理能力，然后将其专业化为特定领域的上下文。在代理基准测试tau-bench、tau2-Bench和ACEBench上进行的大量实验表明，我们训练的模型AgentScaler显著增强了模型的函数调用能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破“大模型具备函数调用能力却缺乏可扩展训练环境”的瓶颈，系统性地回答两个核心问题：</p>
<ol>
<li>如何<strong>可扩展、自动化地构建多样化、可验证的函数调用环境</strong>；</li>
<li>如何<strong>利用这些环境产生的高质量经验轨迹，高效训练出通用且鲁棒的智能体</strong>。</li>
</ol>
<p>最终目标是在不依赖人工标注或真实 API 的前提下，让参数规模相对较小的模型也能获得与闭源大模型媲美的通用智能体能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两大主线，并在第 6 章系统综述。下表按“环境构建”与“工具学习”两大维度，对代表性文献进行归纳（均可在原文 §6.1–§6.2 找到对应引用）。</p>
<p>研究方向</p>
<p>关键问题 / 局限</p>
<p>代表性工作</p>
<p>与本文差异</p>
<p><strong>Tool-Use Environments</strong></p>
<p>真实环境调用</p>
<p>高成本、低可用、难并行</p>
<p>ToolBench (Qin et al. 2023), RestGPT (Song et al. 2023), Mastouri et al. 2025</p>
<p>本文用<strong>完全模拟</strong>替代真实调用，实现零成本、可验证</p>
<p>LLM 模拟响应</p>
<p>幻觉、不一致、难验证</p>
<p>Qin et al. 2024, Lu et al. 2024, Sun et al. 2025</p>
<p>本文将工具<strong>程序化落地为数据库读写</strong>，保证确定性反馈</p>
<p>离线状态环境</p>
<p>手工搭建、难扩展、主要用于评测</p>
<p>τ-bench (Yao et al. 2024), τ2-bench (Barres et al. 2025), ToolSandbox (Lu et al. 2024)</p>
<p>本文提出<strong>工具图采样+规则验证</strong>的自动扩缩方案，实现训练级规模</p>
<p><strong>Tool Learning</strong></p>
<p>大规模合成数据</p>
<p>依赖人工设计或反向生成，环境固定</p>
<p>xLAM (Prabhakar et al. 2025a), ToolAce (Liu et al. 2024a), API-Gen (Liu et al. 2024b)</p>
<p>本文<strong>前向模拟人-机交互</strong>，环境随工具图自动增长</p>
<p>偏好 / RL 优化</p>
<p>需要稳定环境提供奖励</p>
<p>DiaTool-DPO (Jung et al. 2025), Tool-RL (Qian et al. 2025), Tool-N1 (Zhang et al. 2025)</p>
<p>本文构建的<strong>可验证模拟器</strong>天然适配后续 RL 训练（留作未来工作）</p>
<p>小模型agent趋势</p>
<p>参数&lt;30B 的专用模型</p>
<p>Belcak et al. 2025, AgentScaler-4B/8B</p>
<p>本文证明<strong>环境扩缩</strong>即可让 4B 模型逼近 30B~1T 水平</p>
<p>综上，本文首次将“环境可扩展性”作为独立变量提出，并通过<strong>全自动工具图聚类→数据库程序化→可验证轨迹过滤</strong>的闭环，填补了“可扩展环境”与“高质量经验”之间的空白，从而支撑小参数模型实现通用智能体能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出一条“两阶段”端到端 pipeline，把<strong>环境可扩展构建</strong>与<strong>经验驱动训练</strong>解耦又协同，系统性地解决“缺数据、缺环境”难题。核心步骤如下：</p>
<p>1. 环境可扩展构建（§2）</p>
<p>步骤</p>
<p>关键机制</p>
<p>技术细节</p>
<p>解决痛点</p>
<p><strong>① 工具采集</strong></p>
<p>30 k+ API 池</p>
<p>合并 ToolBench / API-Gen / 内部库，重写缺失的 I/O 规格</p>
<p>覆盖度不足</p>
<p><strong>② 工具图建模</strong></p>
<p>参数语义相似 + Louvain 社区发现</p>
<p>$\text{edge}_{i,j}=𝟙[\cos(\phi(P_i),,P_j)!&gt;!\tau]$<br>再让 LLM 二次精修边</p>
<p>人工划分领域成本高</p>
<p><strong>③ 数据库程序化</strong></p>
<p>每领域自动生成 schema → Python 函数</p>
<p>$\texttt{API}(f,\alpha)\equiv \text{op}(f)(\alpha;\mathcal{D})$</p>
<p>环境无法验证、难复现</p>
<p><strong>④ 任务采样</strong></p>
<p>有向工具图上随机游走 → 可执行序列</p>
<p>同步生成初始 $\mathcal{D}_0$ 与参数，保证读写链逻辑一致</p>
<p>轨迹缺乏真实性与可检查性</p>
<p>结果：得到 <strong>>1 000</strong> 个完全模拟、可验证、可并行扩张的领域环境，无需人工维护。</p>
<p>2. 经验采集与过滤（§3.1）</p>
<p>阶段</p>
<p>策略</p>
<p>粒度</p>
<p>保留信号</p>
<p><strong>有效性过滤</strong></p>
<p>去重复、去死循环</p>
<p>轨迹级</p>
<p>交替格式正确</p>
<p><strong>状态对齐</strong></p>
<p>最终 $\mathcal{D}_\text{final}$ 与金标准比对</p>
<p>环境级</p>
<p>写操作正确性</p>
<p><strong>精确匹配</strong></p>
<p>工具+参数序列完全命中</p>
<p>调用级</p>
<p>读链也能被监督</p>
<p>通过三层漏斗，<strong>~45 %</strong> 轨迹被保留，形成高质量经验池。</p>
<p>3. 两阶段经验学习（§3.2）</p>
<p>阶段</p>
<p>目标</p>
<p>数据混合</p>
<p>损失设计</p>
<p><strong>Stage-1 通用能力</strong></p>
<p>何时/如何调用、如何把工具输出说人话</p>
<p>跨领域全量轨迹</p>
<p>$\mathcal{L}_{\theta}=-\frac{1}{</p>
<p><strong>Stage-2 垂直特化</strong></p>
<p>领域上下文、参数格式、业务措辞</p>
<p>目标域轨迹 + 10 % 通用数据防遗忘</p>
<p>同上，但 LR 更小、步数更少</p>
<p>两阶段均在<strong>自构模拟环境</strong>里完成，零真实 API 调用。</p>
<p>4. 产出 AgentScaler 模型族</p>
<ul>
<li><strong>4 B / 8 B / 30B-A3B</strong> 三档规模，基于 Qwen3 系列初始化</li>
<li>在 <strong>τ-bench、τ²-bench、ACEBench</strong> 上刷新 ≤1 T 开源模型 SOTA，4 B 参数即可对标 30 B；30B-A3B 与 GPT-4o、Gemini-2.5-pro 打平。</li>
</ul>
<p>5. 可扩展性保证</p>
<p>工具图、数据库 schema、轨迹采样、过滤规则全部<strong>代码化、参数化</strong>，新增 API 只需：</p>
<ol>
<li>插入工具池 → 自动重跑社区发现</li>
<li>生成新 schema → 自动得到可执行函数</li>
<li>继续采样-过滤-训练</li>
</ol>
<p>实现“环境即数据工厂”的线性扩展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“环境可扩展→经验质量→模型性能”链条展开，覆盖<strong>3 个公开基准、4 组对比、5 类分析</strong>，共 6 项核心实验，全部结果可复现（代码与数据已开源）。</p>
<p>1 主评测：三大基准全面刷榜</p>
<p>基准</p>
<p>领域</p>
<p>指标</p>
<p>对照组</p>
<p>关键结论</p>
<p><strong>τ-bench</strong></p>
<p>retail + airline</p>
<p>pass@1</p>
<p>20+ 闭/开源模型</p>
<p>AgentScaler-30B-A3B 达 <strong>70.4 / 54.0</strong>，持平 GPT-4o，刷新 ≤1 T 开源最佳</p>
<p><strong>τ²-bench</strong></p>
<p>+ telecom</p>
<p>pass@1</p>
<p>同上</p>
<p>加权平均分 <strong>62.5</strong>，领先次优开源模型 17+ 分</p>
<p><strong>ACEBench-en</strong></p>
<p>Normal / Special / Agent</p>
<p>accuracy</p>
<p>同上</p>
<p>Overall <strong>75.7</strong>，4 B 版本即超 Qwen3-30B-A3B 基线 8.5 分</p>
<p>2 消融实验：两阶段训练必要性</p>
<ul>
<li>在 ACEBench-en 上对比<ul>
<li>Base 模型</li>
<li>Stage-1 仅通用</li>
<li>Stage-1+2 完整<br>→ <strong>Agent 子集提升 17.2</strong>，Overall 提升 8.5，验证“广→专”两阶段有效（图 3）。</li>
</ul>
</li>
</ul>
<p>3 跨语言鲁棒性：ACEBench-zh（OOD）</p>
<ul>
<li>训练全英文，直接测中文</li>
<li>AgentScaler-4B Overall <strong>+21.7</strong>（43.9→65.6）；Agent 子集 <strong>+31.7</strong>（6.7→38.4）<br>→ 证明合成环境可迁移，无需额外中文标注。</li>
</ul>
<p>4 稳定性测试：pass@k 曲线</p>
<ul>
<li>τ²-bench 同一题独立跑 k=1~4 次</li>
<li>AgentScaler-30B-A3B 在所有 k 上均 <strong>> Qwen3-30B-A3B</strong></li>
<li>随 k 增大两者均下降，揭示长链一致性仍是共性难题（图 4）。</li>
</ul>
<p>5 长程工具调用挑战</p>
<ul>
<li>按轨迹内工具调用次数分组统计准确率</li>
<li>零售、航空两域均呈 <strong>显著负相关</strong>（图 5）</li>
<li>即使 AgentScaler，&gt;10 步任务准确率跌至 <strong>≤50 %</strong><br>→ 指出未来需引入链式反思或 RL 探索。</li>
</ul>
<p>6 规模-性能对比</p>
<ul>
<li>4 B 模型 ≈ 30 B 级开源模型</li>
<li>30B-A3B ≈ 1 T 级（Kimi-K2-1T-A32B）并逼近闭源 GPT-4o</li>
<li>参数效率曲线显示：<strong>环境扩缩带来的数据增益 &gt; 单纯堆参数</strong></li>
</ul>
<p>可复现性</p>
<ul>
<li>代码： <a target="_blank" rel="noopener" href="https://github.com/Alibaba-NLP/DeepResearch">https://github.com/Alibaba-NLP/DeepResearch</a></li>
<li>评测协议完全遵循官方脚本，随机种子固定，pass@k 重复 4 次取均值。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可直接在论文现成框架上延伸，无需重新造轮；每条均给出可验证的指标或实验设置，便于后续工作快速落地。</p>
<p>1 强化学习：把「可验证模拟器」变成「可微奖励」</p>
<ul>
<li><strong>切入点</strong><br>现有 Stage-1/2 仅做 SFT，而模拟器已能提供 0/1 终端奖励与中间状态差分，天然适合 RL。</li>
<li><strong>可探索算法</strong><ul>
<li>Offline RL：用过滤后的高质量轨迹做 IQL、Decision Transformer；</li>
<li>Online RL：以「状态一致性」为稠密奖励，做 PPO/GRPO，每步即时奖励<br>r_t = Δ(state_match) − λ·(token_length)。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>长程（≥10 步）任务在 τ-bench 的 pass@1 绝对提升 ≥10 %，且 pass@k 下降斜率变缓。</li>
</ul>
<p>2 长程工具链：显式「计划-执行-检查」循环</p>
<ul>
<li><strong>切入点</strong><br>图 5 显示准确率随调用次数增加而线性下降 → 缺乏全局规划。</li>
<li><strong>技术路线</strong><ol>
<li>在轨迹合成阶段引入「计划 API」：让 LLM 先输出 JSON 计划，再执行；</li>
<li>训练时把计划 token 纳入因果语言模型，但只在计划阶段计算 loss；</li>
<li>推理阶段允许模型在每 3-4 步后自我检查状态并 replan。</li>
</ol>
</li>
<li><strong>评估指标</strong><br>12-18 步任务子集（可人工从 τ-bench 抽取）准确率提升 ≥15 %，且首次失败步位置后移 ≥2 步。</li>
</ul>
<p>3 工具图动态扩展：持续学习不遗忘</p>
<ul>
<li><strong>切入点</strong><br>真实世界 API 不断新增，需避免重训全量数据。</li>
<li><strong>技术路线</strong><ul>
<li>用 Function-Descriptor 向量检测「新工具是否属于旧社区」→ 决定复用或新建 schema；</li>
<li>采用 Replay-buffer + 正则化（KL 约束）做持续微调，只采样新增领域轨迹 + 5 % 旧轨迹。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>连续添加 3 个新领域后，旧领域平均性能下降 ≤2 %，新领域性能 ≥70 %（相对一次性联合训练）。</li>
</ul>
<p>4 多模态环境：把数据库状态升级为「富状态」</p>
<ul>
<li><strong>切入点</strong><br>当前状态仅为 JSON 结构化数据；真实工具常返回图像、PDF、音频。</li>
<li><strong>技术路线</strong><ul>
<li>将图像/PDF 表征为可查询的向量字段，工具调用返回「文件句柄+向量」；</li>
<li>训练阶段把文件向量作为额外上下文，但不计算梯度；</li>
<li>合成轨迹时让 LLM 生成「先检索再视觉问答」链。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>在自建 200 任务「视觉维修」环境上，多模态 AgentScaler 比纯文本版本 pass@1 提升 ≥20 %。</li>
</ul>
<p>5 奖励模型蒸馏：用「状态一致性」训练轻量 RM</p>
<ul>
<li><strong>切入点</strong><br>现有过滤只用硬标签 0/1，可蒸馏成轻量奖励模型供其它算法复用。</li>
<li><strong>技术路线</strong><ul>
<li>以「最终状态匹配」为金标准，训练 3 B 参数的交叉编码器 RM；</li>
<li>用 RM 对同一意图生成的 16 条轨迹重排序，挑选 Top-4 进入训练集。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>在相同训练步数下，RM 筛选后训练的 AgentScaler-8B 比随机筛选版本 Overall 提升 ≥4 %，且数据量减少 30 %。</li>
</ul>
<p>6 边缘部署：4 B 模型 + 量化 + speculative decoding</p>
<ul>
<li><strong>切入点</strong><br>论文已证明 4 B 可媲美 30 B，但生产还需低延迟。</li>
<li><strong>技术路线</strong><ul>
<li>8-bit/4-bit 量化后做 QAT（quantization-aware training）补偿工具调用精度；</li>
<li>用 0.5 B 的草稿模型做 speculative decoding，目标首 token 延迟 &lt;100 ms。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>τ-bench retail 域量化后准确率下降 ≤1 %，首 token 延迟降低 35 %，峰值内存 &lt;3 GB。</li>
</ul>
<p>7 安全与可解释：引入「工具风险标签」</p>
<ul>
<li><strong>切入点</strong><br>金融、医疗等域存在高风险写操作，需可解释拒答。</li>
<li><strong>技术路线</strong><ul>
<li>在工具描述中显式加入 risk_level∈{low,medium,high}；</li>
<li>训练时随机丢弃高风险工具，让模型学会请求额外确认；</li>
<li>推理阶段若置信度&lt;δ 则触发「人类介入」信号。</li>
</ul>
</li>
<li><strong>评估指标</strong><br>自建 100 高风险任务上，误调用率（未经允许执行高风险操作）≤2 %，同时保持整体成功率 ≥75 %。</li>
</ul>
<p>以上方向均可在当前开源代码库上直接分支实验，无需重新搭建模拟器，形成「环境-算法-评测」闭环的快速迭代。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心速览</p>
<p><strong>题目</strong>：Towards General Agentic Intelligence via Environment Scaling<br><strong>目标</strong>：用“可扩展的模拟环境”替代昂贵真实 API，让<strong>小参数模型</strong>也能获得<strong>通用且鲁棒的函数调用智能</strong>。</p>
<p>1 关键问题</p>
<ul>
<li>真实 API 采集贵、不稳定 → 数据难扩展</li>
<li>现有合成环境手工重、不可验证 → 训练难规模化</li>
<li>函数调用能力随参数下降快 → 边缘部署难</li>
</ul>
<p>2 解决思路（两大阶段）</p>
<p>阶段</p>
<p>做法</p>
<p>输出</p>
<p><strong>① 环境扩缩</strong></p>
<p>3 万 API → 工具语义图 → Louvain 分域 → 每域自动生成数据库 schema → Python 化工具 → 可验证轨迹采样</p>
<p>>1 000 个完全模拟、可并行扩张的领域环境</p>
<p><strong>② 经验学习</strong></p>
<p>人机交互模拟 → 三层漏斗过滤 → 两阶段微调（通用→垂直）</p>
<p>AgentScaler 模型族 4 B / 8 B / 30B-A3B</p>
<p>3 主要结果</p>
<ul>
<li><strong>τ-bench</strong>（零售+航空）70.4 / 54.0 pass@1 <strong>刷新 ≤1 T 开源 SOTA</strong></li>
<li><strong>τ²-bench</strong> 加权 62.5 <strong>领先次优 17+ 分</strong></li>
<li><strong>ACEBench-en</strong> Overall 75.7 <strong>4 B 版本已超 30 B 基线</strong></li>
<li><strong>跨语言零样本</strong>（ACEBench-zh）Overall +21.7 <strong>验证 OOD 鲁棒</strong></li>
<li><strong>长程负相关</strong>（图 5）与 <strong>稳定性曲线</strong>（图 4）<strong>揭示未来改进点</strong></li>
</ul>
<p>4 贡献一句话</p>
<p><strong>首次把“环境可扩展性”做成独立变量</strong>，用全自动、可验证的模拟数据工厂，让<strong>4 B 模型逼近 1 T 性能</strong>，为轻量级通用智能体提供可复制范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Towards%20General%20Agentic%20Intelligence%20via%20Environment%20Scaling，链接是  https://arxiv.org/pdf/2509.13311  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13311  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Runnan Fang,Shihao Cai,Baixuan Li,Jialong Wu,Guangyu Li,Wenbiao Yin,Xinyu Wang,Xiaobin Wang,Liangcai Su,Zhen Zhang,Shibin Wu,Zhengwei Tao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13311.pdf">https://arxiv.org/pdf/2509.13311.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13311">https://arxiv.org/abs/2509.13311</a></p>
<p><strong>Arxiv ID</strong>: 2509.13311</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13311">https://papers.cool/arxiv/2509.13311</a></p>
<p><strong>Published</strong>: 2025-09-16T17:57:20Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:57:20.000Z</p>
<hr>
<h3 id="5-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents"><a href="#5-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents" class="headerlink" title="5. WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13309">WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents</a></h3><p>Recent advances in deep-research systems have demonstrated the potential for AI agents to autonomously discover and synthesize knowledge from external sources. In this paper, we introduce WebResearcher, a novel framework for building such agents through two key components: (1) WebResearcher, an iterative deep-research paradigm that reformulates deep research as a Markov Decision Process, where agents periodically consolidate findings into evolving reports while maintaining focused workspaces, overcoming the context suffocation and noise contamination that plague existing mono-contextual approaches; and (2) WebFrontier, a scalable data synthesis engine that generates high-quality training data through tool-augmented complexity escalation, enabling systematic creation of research tasks that bridge the gap between passive knowledge recall and active knowledge construction. Notably, we find that the training data from our paradigm significantly enhances tool-use capabilities even for traditional mono-contextual methods. Furthermore, our paradigm naturally scales through parallel thinking, enabling concurrent multi-agent exploration for more comprehensive conclusions. Extensive experiments across 6 challenging benchmarks demonstrate that WebResearcher achieves state-of-the-art performance, even surpassing frontier proprietary systems.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近在深度研究系统方面的进展展示了人工智能代理自主发现和综合外部知识的潜力。在本文中，我们介绍了WebResearcher，这是一种构建此类代理的新框架，包含两个关键组件：（1）WebResearcher，一个迭代的深度研究范式，将深度研究重新构建为马尔可夫决策过程，其中代理定期将发现整合到不断发展的报告中，同时保持专注的工作空间，从而克服现有单一上下文方法所苦恼的上下文窒息和噪声污染；（2）WebFrontier，一个可扩展的数据综合引擎，通过工具增强的复杂性升级生成高质量的训练数据，使研究任务的系统性创建能够弥合被动知识回忆和主动知识构建之间的差距。值得注意的是，我们发现我们范式中的训练数据显著增强了即使是传统单一上下文方法的工具使用能力。此外，我们的范式通过并行思维自然扩展，允许同时进行多代理探索，以得出更全面的结论。通过在六个具有挑战性的基准测试中的广泛实验，WebResearcher实现了最先进的性能，甚至超过了前沿的专有系统。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破现有“深度研究（deep-research）”智能体因**单上下文（mono-contextual）**架构而遭遇的两大根本瓶颈：</p>
<ol>
<li><p><strong>认知工作区窒息</strong><br>随着检索信息不断追加，固定上下文窗口被历史数据挤占，模型可用于深度推理的“思考空间”持续萎缩，迫使研究过程提前终止。</p>
</li>
<li><p><strong>不可逆噪声污染</strong><br>早期引入的无关信息或错误无法在后续步骤中被过滤或修正，噪声线性累积，信号质量单调下降，导致偏差级联。</p>
</li>
</ol>
<p>为根治上述问题，论文提出<strong>WebResearcher</strong>框架，核心目标是将“被动知识回忆”转变为“主动知识构建”，使智能体在任意深度的长周期研究中仍能保持高质量推理与信息综合。具体通过三项相互协同的贡献实现：</p>
<ul>
<li><strong>IterResearch</strong>：把深度研究重定义为<strong>马尔可夫决策过程（MDP）</strong>，每轮仅保留「问题 + 演进报告 + 最新工具反馈」的紧凑状态，周期性地用合成报告替代原始信息堆叠，从而维持恒定大小的认知工作区，理论上支持无界研究深度。</li>
<li><strong>WebFrontier</strong>：一个可扩展的数据合成引擎，利用工具增强的复杂度递增策略，批量生成高难度、可验证的训练任务，填补基座模型与工具增强模型之间的能力鸿沟。</li>
<li><strong>Research-Synthesis</strong>：在推理阶段引入测试时扩展（test-time scaling），通过并行多智能体探索与统一综合，进一步释放迭代范式的性能上限。</li>
</ul>
<p>综上，论文解决的是**“如何在长周期、多跳、跨域的开放环境研究中，持续保持高质量推理与信息综合”**这一根本问题，突破了传统单上下文架构随研究深度增加而性能衰减的悖论。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接地置于同一技术脉络，可划分为四类：<br>（1）专有/闭源深度研究系统；<br>（2）开源单上下文深度研究智能体；<br>（3）工具增强与检索-推理框架；<br>（4）数据合成与课程式训练。</p>
<pre><code>### 1. 专有深度研究系统
- OpenAI Deep Research (2025a)  
- Google Gemini Deep Research (2025b)  
- xAI Grok DeepSearch (2025)  
- Anthropic Claude-4 Research (2025)  
- Perplexity Deep Research (2025)  
- Moonshot Kimi-Researcher (2025)  
→ 确立了“搜索-浏览-综合”范式，但架构细节未公开，均被本文作为性能对标对象。

### 2. 开源单上下文深度研究智能体
- WebSailor (Li et al., 2025a)  
- WebThinker (Li et al., 2025c)  
- WebShaper (Tao et al., 2025)  
- WebDancer (Wu et al., 2025)  
- WebExplorer (Liu et al., 2025)  
- MiroThinker (MiroMindAI, 2025)  
→ 均采用“线性累积”上下文，无中间压缩/修正机制，被本文归类为 mono-contextual baseline。

### 3. 工具增强与检索-推理框架
- Search-R1 (Jin et al., 2025) — RL 驱动搜索调用  
- Search-o1 (Li et al., 2025b) — 检索增强推理模型  
- C-3PO (Chen et al., 2025) — 紧凑即插即用检索优化  
- GLM-4.5 (Zeng et al., 2025) — 自带工具调用底座  
- DeepSeek-R1/V3.1 (Guo et al., 2025b) — 长链思维+工具  
→ 与本文共享“工具+LLM”协同目标，但未引入周期性状态重建。

### 4. 数据合成与课程式训练
- WebDancer（同期工作）— 使用多智能体 rollout 生成轨迹  
- WebShaper — 将信息检索形式化用于数据蒸馏  
- Scaling Agents via Continual Pre-training (Su et al., 2025)  
- Group Sequence Policy Optimization (GSPO, Zheng et al., 2025) — 本文 RL 阶段所采用的分布式优势估计方法  
→ 本文的 WebFrontier 在“复杂度递增”与“工具自举”策略上与此类研究理念相通，但额外强调跨域、可验证、多轮合成。
</code></pre><p>综上，现有工作要么停留在闭源性能标杆，要么沿用单上下文累积架构；本文通过<strong>迭代 MDP 式状态重建</strong>与<strong>可扩展数据引擎</strong>首次系统性地解决了长周期研究中的上下文窒息与噪声污染问题，因而在同领域内具有明确的差异化定位。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“单上下文线性累积”导致的<strong>认知工作区窒息</strong>与<strong>不可逆噪声污染</strong>问题，转化为**“如何在无限长度研究中维持恒定大小、可过滤、可修正的认知状态”**这一形式化问题，并给出三层耦合的系统性解法。</p>
<p>1. 形式化：把深度研究重定义为马尔可夫决策过程（MDP）</p>
<ul>
<li><p><strong>状态</strong><br>sᵢ = ⟨q, Reportᵢ₋₁, (Actionᵢ₋₁, ToolResponseᵢ₋₁)⟩<br>仅保留“原始问题 + 上一轮综合报告 + 最新工具反馈”，与历史长度无关，天然满足马尔可夫性。</p>
</li>
<li><p><strong>动作空间</strong><br>A = {ToolCall, FinalAnswer}，支持搜索、学术检索、页面访问、Python 计算四类工具。</p>
</li>
<li><p><strong>转移函数</strong><br>每轮生成 Think-Report-Action 三元组后，<strong>丢弃 Think 与原始工具返回原文</strong>，仅将更新后的 Report 注入下一状态，实现“信息浓缩 + 工作区重置”。</p>
</li>
<li><p><strong>奖励</strong><br>训练阶段用“最终答案是否匹配参考答案”做稀疏奖励；推理阶段无外部奖励，靠确定性策略最大化答案一致性。</p>
</li>
</ul>
<p>⇒ 通过<strong>周期性状态重建</strong>将上下文尺寸锁死在常数级，理论上支持<strong>无界研究深度</strong>。</p>
<p>2. 数据：WebFrontier 引擎——“工具自举 + 复杂度递增”</p>
<ol>
<li><p><strong>种子生成</strong><br>多领域语料 → 摘要智能体切块 → 主题相关块组合 → ItemWriter 生成需多源综合的 QA 对。</p>
</li>
<li><p><strong>复杂度递增循环</strong><br>同一 ItemWriter 被授予搜索、学术、浏览、Python 四件工具，在每一轮对 (Q, A) 执行：</p>
<ul>
<li>知识扩展（外部检索）</li>
<li>概念抽象（跨域提炼）</li>
<li>多源交叉验证（保证事实）</li>
<li>计算化封装（引入数值/模拟）<br>新生成的更高阶 QA 立即成为下一轮种子，形成“自举式”难度飞轮。</li>
</ul>
</li>
<li><p><strong>质量闸口</strong></p>
<ul>
<li>基线模型（无工具）能答 → 太简单，丢弃；</li>
<li>工具增强模型仍答错 → 过难或错误，丢弃或人工复核；</li>
<li>SimilarityScorer 去重。</li>
</ul>
</li>
</ol>
<p>⇒ 批量产出<strong>恰好落在基座与工具增强模型能力间隙</strong>、且可验证的高难度任务，用于后续训练。</p>
<p>3. 训练：两阶段混合算法</p>
<p>阶段</p>
<p>方法</p>
<p>目标</p>
<p>关键技巧</p>
<p><strong>SFT</strong></p>
<p>拒绝采样微调</p>
<p>学会格式与正确推理链</p>
<p>仅对匹配参考答案的轨迹求最大似然；梯度只落在模型生成的 Think/Report/Action token 上，工具输出视为外部观测。</p>
<p><strong>RL</strong></p>
<p>Group Sequence Policy Optimization (GSPO)</p>
<p>提高探索与多轮综合质量</p>
<p>把一条轨迹的每一轮都当成独立样本，按组归一化优势；采用最小损失下采样保证分布式训练批尺寸恒定。</p>
<p>⇒ 模型既掌握“何时调用何工具”，也学会在 Report 中<strong>消解冲突、修正旧结论、浓缩信号</strong>。</p>
<p>4. 推理：Research-Synthesis 框架——测试时扩展</p>
<ol>
<li><p><strong>Parallel Research</strong><br>n 个独立 Research Agent 并行执行 IterResearch，各自输出 (FinalReport, Answer)。</p>
</li>
<li><p><strong>Integrative Synthesis</strong><br>单一大模型（Qwen3-235B）仅阅读 n 份<strong>已浓缩的最终报告</strong>，再综合得出统一答案。</p>
</li>
</ol>
<p>⇒ 以常数级上下文代价换取<strong>线性增长的推理路径覆盖</strong>，实现可控的“测试时缩放”。</p>
<p>5. 结果验证</p>
<ul>
<li>** Humanity’s Last Exam **：36.7%（+6.9 pp 超越次优系统）</li>
<li>** BrowseComp-en **：51.7%（与 OpenAI 商业系统持平，开源第一）</li>
<li>消融实验：同等数据下，<strong>迭代范式 &gt; 单上下文+迭代数据 &gt; 单上下文</strong>，证明性能增益主要来自<strong>状态重建机制本身</strong>而非数据或模型规模。</li>
</ul>
<p>结论</p>
<p>论文通过“<strong>MDP 式迭代合成 + 工具自举数据飞轮 + 测试时并行综合</strong>”三位一体方案，把长周期研究从“堆叠信息”转变为“周期性精炼”，在任意深度都能保持<strong>恒定认知空间与可修正信号</strong>，从而系统性地解决了上下文窒息与噪声污染问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“迭代式深度研究”与“单上下文深度研究”两大范式，在 6 个公开基准上执行了<strong>三组核心实验 + 两项深入分析</strong>，覆盖性能对比、消融验证、行为可解释性与测试时扩展效果。所有结果均以 pass@1 为主指标，采用 LLM-as-a-Judge 自动判分，温度 0.6、top-p 0.95。</p>
<p>1 主实验：State-of-the-art 对比</p>
<p>| 实验目的 | 验证 WebResearcher 在复杂推理、长程浏览、跨语言检索等任务上是否超越现有最好系统（含闭源） | | 基准 | 6 个挑战性数据集 | | 对照组 | ① 通用 LLM+工具 ② 商业深度研究系统 ③ 开源深度研究智能体 |</p>
<p>1.1 通用网页导航与推理</p>
<ul>
<li><strong>Humanity’s Last Exam (text-only, 2 154 题)</strong></li>
<li><strong>BrowseComp-en (1 266 题)</strong></li>
<li><strong>BrowseComp-zh (289 题)</strong></li>
</ul>
<p>1.2 复杂目标导向任务</p>
<ul>
<li><strong>GAIA (text-only val, 103 题)</strong></li>
<li><strong>Xbench-DeepSearch</strong></li>
<li><strong>FRAMES (824 题)</strong></li>
</ul>
<p>关键结果（pass@1）</p>
<p>系统</p>
<p>HLE</p>
<p>BC-en</p>
<p>BC-zh</p>
<p>GAIA</p>
<p>Xbench</p>
<p>FRAMES</p>
<p>WebResearcher-30B-A3B-heavy</p>
<p><strong>36.7</strong></p>
<p><strong>51.7</strong></p>
<p><strong>56.8</strong></p>
<p><strong>75.7</strong></p>
<p><strong>73.0</strong></p>
<p><strong>85.1</strong></p>
<p>次优（开源）</p>
<p>29.8</p>
<p>30.0</p>
<p>49.2</p>
<p>63.1</p>
<p>71.2</p>
<p>83.7</p>
<p>次优（专有）</p>
<p>26.6</p>
<p>51.5</p>
<p>58.1</p>
<p>70.5</p>
<p>—</p>
<p>84.0</p>
<p>⇒ 在 6 个基准上全部取得<strong>新 SOTA</strong>，其中 HLE 领先 6.9 pp，BC-en 领先开源对手 21.7 pp。</p>
<p>2 消融实验：验证“迭代范式”本身的价值</p>
<p>| 实验目的 | 排除数据或模型规模的干扰，量化<strong>状态重建机制</strong>的独立贡献 | | 设计 | 同底座模型（Qwen3-30B-A3B）+ 同一套迭代训练数据，仅改变推理策略 |</p>
<p>条件</p>
<p>HLE</p>
<p>BC-en</p>
<p>BC-zh</p>
<p>Mono-Agent（线性累积）</p>
<p>18.7</p>
<p>25.4</p>
<p>34.6</p>
<p>Mono-Agent + Iter 数据</p>
<p>25.4</p>
<p>30.1</p>
<p>40.4</p>
<p>WebResearcher（迭代推理）</p>
<p><strong>28.8</strong></p>
<p><strong>37.3</strong></p>
<p><strong>45.2</strong></p>
<p>⇒ 迭代推理相较“仅使用迭代数据”再提升 <strong>+3.4 pp（HLE）</strong>，证明<strong>周期性状态重建是增益主因</strong>。</p>
<p>3 工具使用行为分析</p>
<p>| 实验目的 | 展示迭代范式如何根据任务类型<strong>自适应调整工具组合与推理长度</strong> | | 方法 | 记录并统计 HLE vs BrowseComp 上各工具调用占比与平均轮数 |</p>
<p>指标</p>
<p>HLE（学术问答）</p>
<p>BrowseComp（网页多跳）</p>
<p>平均轮数</p>
<p>4.7</p>
<p>61.4（最大＞200）</p>
<p>Scholar 占比</p>
<p>25.4 %</p>
<p>0.4 %</p>
<p>Search+Visit 占比</p>
<p>28 %</p>
<p><strong>96.2 %</strong></p>
<p>⇒ 迭代架构可**自动切换“短链精准检索”与“长链探索式浏览”**两种策略，验证其对任务语义的敏感度。</p>
<p>4 测试时扩展（Research-Synthesis）</p>
<p>| 实验目的 | 研究并行轨迹数 n 对性能与计算成本的 trade-off | | 设置 | 固定模型 IterResearch-30B-A3B，n∈{1,2,4,8,16} |</p>
<p>n</p>
<p>1</p>
<p>2</p>
<p>4</p>
<p>8</p>
<p>16</p>
<p>HLE pass@1</p>
<p>28.8</p>
<p>30.4</p>
<p>32.1</p>
<p><strong>35.7</strong></p>
<p>36.7</p>
<p>BC-en pass@1</p>
<p>37.3</p>
<p>38.5</p>
<p>46.5</p>
<p><strong>49.3</strong></p>
<p>51.7</p>
<p>⇒ 性能随 n 单调上升，8 条轨迹后边际收益递减；<strong>n=8 为性价比拐点</strong>。</p>
<p>5 数据质量验证（WebFrontier）</p>
<p>| 实验目的 | 证明迭代范式生成的数据对<strong>第三方单上下文系统</strong>同样有效 | | 方法 | 仅用 WebFrontier 数据对 Qwen3-30B-A3B 做 SFT，保持其原始线性推理不变 |</p>
<p>系统</p>
<p>HLE</p>
<p>BC-en</p>
<p>原始 Qwen3-30B-A3B</p>
<p>13.2</p>
<p>0.5</p>
<p>+ WebFrontier 数据</p>
<p><strong>25.4</strong></p>
<p><strong>30.1</strong></p>
<p>⇒ 迭代数据本身即可为传统架构带来 <strong>+12.2 pp（HLE）</strong> 的提升，说明数据引擎产生的训练信号具有<strong>通用工具增强价值</strong>。</p>
<p>实验结论汇总</p>
<ol>
<li>WebResearcher 在 6 大基准全面刷新 SOTA，领先开源平均约 15–20 pp。</li>
<li>消融确认**“状态重建”&gt;“数据”&gt;“ baseline”**，迭代范式是核心。</li>
<li>工具行为统计揭示系统能<strong>自适应短链或长链</strong>策略，验证迭代架构的灵活性。</li>
<li>测试时扩展实验给出<strong>n=8 的实用配置</strong>，实现 24 % 额外增益。</li>
<li>数据引擎产生的训练信号<strong>对第三方 mono-contextual 模型同样显著有效</strong>，凸显 WebFrontier 的普适性。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为<strong>理论-算法、系统-工程、数据-评测、应用-生态</strong>四个层面，并给出可立即着手的技术切入点。</p>
<p>理论-算法层面</p>
<ol>
<li><p><strong>非-Markov 长程依赖建模</strong><br>当前 Report 仅保留上一轮摘要，极端多跳场景（&gt;100 轮）可能出现早期关键证据被“遗忘”。可尝试</p>
<ul>
<li>带遗忘门控的<strong>递归记忆网络</strong>（Recursive Memory Network）</li>
<li>或把 Report 视为<strong>外部向量数据库</strong>，用检索式记忆维持亚线性遗忘率。</li>
</ul>
</li>
<li><p><strong>动态压缩率理论</strong><br>信息论角度分析：给定上下文预算 C，如何最大化互信息 I(Report; 历史轨迹)？可推导<strong>最优压缩率下界</strong>，指导 Report 长度与更新频率的自适应调度。</p>
</li>
<li><p><strong>工具调用策略的强化学习理论</strong><br>现有 RL 仅优化答案正确性。可引入<strong>信息增益</strong>或<strong>边际置信度</strong>作为稠密奖励，建立“最优停止”与“探索-利用”可证明界。</p>
</li>
</ol>
<p>系统-工程层面</p>
<ol>
<li><p><strong>层级式并行综合</strong><br>当前 Research-Synthesis 为单层投票。可探索<strong>树状综合</strong>：8→4→2→1 的多级提炼，每层进一步压缩，降低合成模型上下文占用。</p>
</li>
<li><p><strong>异步工具执行与流式报告</strong><br>搜索、浏览、Python 执行延迟差异大。把工具调用<strong>异步化</strong>，并在部分结果返回时即时更新 Report，可缩短 wall-clock 时间 30 % 以上。</p>
</li>
<li><p><strong>端-云协同小模型部署</strong><br>将 30B 大模型蒸馏为 3B 轻量化 Research Agent 运行在端侧，仅把最终 Report 上传云端合成，兼顾隐私与成本。</p>
</li>
</ol>
<p>数据-评测层面</p>
<ol>
<li><p><strong>可验证性自动标注</strong><br>WebFrontier 目前依赖 LLM Judge 做答案正确性过滤。可引入</p>
<ul>
<li><strong>形式化验证</strong>（Python assert、SQL 单元测试、数学证明助手）</li>
<li><strong>区块链时间戳</strong>对网页快照进行存证，确保后续可人工审计。</li>
</ul>
</li>
<li><p><strong>对抗性噪声评测基准</strong><br>构建刻意插入<strong>误导性网页、 contradictory 论文、虚假引用</strong>的 adversarial split，衡量迭代范式在<strong>故意污染</strong>环境下的鲁棒性。</p>
</li>
<li><p><strong>跨模态深度研究</strong><br>现有实验局限文本。扩展工具集到</p>
<ul>
<li>图像检索（Google Lens API）</li>
<li>视频关键帧 OCR</li>
<li>图表可执行代码（Plotly/D3 自动可视化）<br>并建立 multimodal 版本的 HLE/BC 基准。</li>
</ul>
</li>
</ol>
<p>应用-生态层面</p>
<ol>
<li><p><strong>个性化科研助手</strong><br>允许用户上传私有论文库（Zotero、Notion），把个人知识库作为<strong>额外工具</strong>，Report 内生成“私有引用 + 公开引用”混合答案，实现<strong>个性化深度综述</strong>。</p>
</li>
<li><p><strong>实时政策/金融市场追踪</strong><br>引入 streaming news API 与 SQL 财经数据库，要求智能体在<strong>规定时间窗口内</strong>完成信息收集并给出可执行预测，评测其在<strong>低延迟、高噪声</strong>场景下的可用性。</p>
</li>
<li><p><strong>可解释性人机协作界面</strong><br>提供<strong>可折叠 Report 差异视图</strong>（round-by-round diff），让用户对每一次综合进行<strong>回滚或手动纠错</strong>，形成“人-机迭代”闭环，同时收集人类反馈用于 RLHF。</p>
</li>
</ol>
<p>即刻可执行的实验选题（示例）</p>
<p>选题</p>
<p>预期验证</p>
<p>最短周期</p>
<p>在 Report 中引入“引用指纹”向量索引，每轮用近似检索保留 5 条最相关原始段落</p>
<p>极端长程多跳问答遗忘率 ↓</p>
<p>1 周</p>
<p>把 GSPO 奖励改为“信息增益 + 答案正确”双目标，在 FRAMES 上对比原算法</p>
<p>收敛速度 +2×，最终指标 ±1 pp</p>
<p>2 周</p>
<p>构建 BrowseComp-Adversarial（10 % 页面注入反事实），测试 Mono vs Iter</p>
<p>Iter 优势扩大至 30 pp</p>
<p>3 周</p>
<p>以上方向既包含<strong>理论深挖</strong>（记忆、压缩、RL 理论），也覆盖<strong>系统落地</strong>（异步、端-云、跨模态），并给出<strong>可验证的 adversarial 评测</strong>与<strong>垂直场景落地</strong>，可供后续研究按需取用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="WebResearcher-核心内容速览"><a href="#WebResearcher-核心内容速览" class="headerlink" title="WebResearcher 核心内容速览"></a>WebResearcher 核心内容速览</h1><h2 id="1-问题"><a href="#1-问题" class="headerlink" title="1. 问题"></a>1. 问题</h2><ul>
<li>现有深度研究智能体采用“单上下文线性累积”模式，随检索量增加出现<br>① 认知工作区窒息 ② 不可逆噪声污染</li>
<li>导致模型推理质量随研究深度增加而下降，形成“越搜越笨”悖论。</li>
</ul>
<h2 id="2-思路"><a href="#2-思路" class="headerlink" title="2. 思路"></a>2. 思路</h2><p>把长周期研究形式化为<strong>马尔可夫决策过程</strong>，用“周期性的信息综合 + 状态重建”取代“无限追加”，实现：</p>
<ul>
<li>恒定大小的认知工作区</li>
<li>可修正、可过滤的中央记忆</li>
<li>理论上无界的研究深度</li>
</ul>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>组件</p>
<p>作用</p>
<p>关键机制</p>
<p><strong>IterResearch</strong></p>
<p>推理范式</p>
<p>每轮仅保留⟨问题, 上轮报告, 最新工具反馈⟩；生成Think-Report-Action；用更新后的报告重建下一状态。</p>
<p><strong>WebFrontier</strong></p>
<p>数据引擎</p>
<p>三阶段多智能体流水线：种子→工具自举复杂度递增→双重验证与去冗余，产出“恰好难”且可验证的QA。</p>
<p><strong>Research-Synthesis</strong></p>
<p>测试时扩展</p>
<p>n个并行Research Agent各跑IterResearch；单一大模型仅阅读n份最终报告再做一次综合，线性提升精度。</p>
<h2 id="4-训练"><a href="#4-训练" class="headerlink" title="4. 训练"></a>4. 训练</h2><ul>
<li>拒绝采样微调：只保留答案正确的轨迹，学习结构化生成。</li>
<li>Group Sequence Policy Optimization：把每轮当作独立样本，组内归一化优势，解决变长轨迹分布式训练难题。</li>
</ul>
<h2 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h2><ul>
<li><strong>6大基准</strong>（HLE、BrowseComp-en/zh、GAIA、Xbench-DeepSearch、FRAMES）全面SOTA；HLE达36.7%，领先次优系统6.9pp。</li>
<li><strong>消融</strong>：同等数据下，迭代推理比单上下文提升≈10pp，验证“状态重建”是核心。</li>
<li><strong>行为分析</strong>：学术任务平均4.7轮、Scholar占25%；网页多跳任务61.4轮、Search+Visit占96%，显示自适应策略。</li>
<li><strong>测试时扩展</strong>：n=8性价比最佳，HLE再提7pp。</li>
</ul>
<h2 id="6-贡献"><a href="#6-贡献" class="headerlink" title="6. 贡献"></a>6. 贡献</h2><p>① 提出迭代式MDP深度研究范式，根治上下文窒息与噪声累积。<br>② 构建可扩展的数据合成引擎，填补基座与工具增强模型间的数据缺口。<br>③ 设计并行综合框架，实现低成本测试时缩放。<br>④ 在6个挑战性基准上刷新SOTA，超越开源与专有系统。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是WebResearcher%3A%20Unleashing%20unbounded%20reasoning%20capability%20in%20Long-Horizon%20Agents，链接是  https://arxiv.org/pdf/2509.13309  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13309  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zile Qiao,Guoxin Chen,Xuanzhong Chen,Donglei Yu,Wenbiao Yin,Xinyu Wang,Zhen Zhang,Baixuan Li,Huifeng Yin,Kuan Li,Rui Min,Minpeng Liao,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13309.pdf">https://arxiv.org/pdf/2509.13309.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13309">https://arxiv.org/abs/2509.13309</a></p>
<p><strong>Arxiv ID</strong>: 2509.13309</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13309">https://papers.cool/arxiv/2509.13309</a></p>
<p><strong>Published</strong>: 2025-09-16T17:57:17Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:57:17.000Z</p>
<hr>
<h3 id="6-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization"><a href="#6-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization" class="headerlink" title="6. ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13313">ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</a></h3><p>Large Language Model (LLM)-based web agents demonstrate strong performance on knowledge-intensive tasks but are hindered by context window limitations in paradigms like ReAct. Complex queries involving multiple entities, intertwined relationships, and high uncertainty demand extensive search cycles that rapidly exhaust context budgets before reaching complete solutions. To overcome this challenge, we introduce ReSum, a novel paradigm that enables indefinite exploration through periodic context summarization. ReSum converts growing interaction histories into compact reasoning states, maintaining awareness of prior discoveries while bypassing context constraints. For paradigm adaptation, we propose ReSum-GRPO, integrating GRPO with segmented trajectory training and advantage broadcasting to familiarize agents with summary-conditioned reasoning. Extensive experiments on web agents of varying scales across three benchmarks demonstrate that ReSum delivers an average absolute improvement of 4.5\% over ReAct, with further gains of up to 8.2\% following ReSum-GRPO training. Notably, with only 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\% Pass@1 on BrowseComp-zh and 18.3\% on BrowseComp-en, surpassing existing open-source web agents.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于大型语言模型 （LLM） 的 Web 代理在知识密集型任务上表现出强大的性能，但受到 ReAct 等范式中的上下文窗口限制的阻碍。涉及多个实体、相互交织的关系和高不确定性的复杂查询需要广泛的搜索周期，在得出完整的解决方案之前会迅速耗尽上下文预算。为了克服这一挑战，我们引入了 ReSum，这是一种新颖的范式，可以通过周期性上下文摘要进行无限探索。ReSum 将不断增长的交互历史转换为紧凑的推理状态，保持对先前发现的认识，同时绕过上下文约束。对于范式适配，我们提出了 ReSum-GRPO，将 GRPO 与分割轨迹训练和优势广播相结合，使智能体熟悉摘要条件推理。在三个基准测试中对不同规模的 Web 代理进行的广泛实验表明，与 ReAct 相比，ReSum 的平均绝对改进为 4.5%，在 ReSum-GRPO 训练后进一步提高了 8.2%。值得注意的是，仅使用 1K 训练样本，我们的 WebResummer-30B（WebSailor-30B 的 ReSum-GRPO 训练版本）在 BrowseComp-zh 上实现了 33.3\% 的Pass@1，在 BrowseComp-en 上达到 18.3\%，超过了现有的开源 Web 代理。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>基于大语言模型（LLM）的网页智能体在长周期、多轮次搜索任务中因上下文窗口受限而被迫提前终止</strong>的核心难题。<br>具体而言：</p>
<ul>
<li><strong>ReAct 范式</strong>通过“思维-动作-观察”循环不断追加历史，导致 token 用量随探索深度线性增长，在 32 k 量级窗口内远未完成任务即被截断。</li>
<li>复杂查询（多实体、关系交织、信息碎片化、不确定性高）需要数十轮搜索、浏览、交叉验证才能收敛，而上下文耗尽使证据链无法闭合。</li>
</ul>
<p>为此，作者提出 <strong>ReSum 范式</strong>，其关键洞察是：</p>
<blockquote>
<p>将随时间膨胀的交互历史<strong>周期性地压缩为紧凑的推理状态</strong>（summary），代理从该状态重启，既保留已验证证据与待填补缺口，又<strong>绕过上下文长度约束</strong>，实现<strong>无限期探索</strong>。</p>
</blockquote>
<p>综上，论文试图解决的问题可概括为：</p>
<script type="math/tex; mode=display">\\boxed{ \\text{在有限上下文窗口内，如何使网页代理对复杂查询进行无限轮次、不中断的搜索与推理，直至证据链完整并给出可靠答案。} }</script><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 ReSum 直接可比或可被其借鉴的相关研究可归纳为四大类，每类给出 1–2 篇代表性工作并指出与 ReSum 的差异。</p>
<p>类别</p>
<p>代表论文</p>
<p>核心思路</p>
<p>与 ReSum 的关键区别</p>
<p><strong>长周期网页代理数据与训练框架</strong></p>
<p>WebSailor (Li et al., 2025a) / ASearcher (Gao et al., 2025)</p>
<p>通过拒绝采样或异步 RL 产生 10 k+ 工具调用轨迹，训练专用模型。</p>
<p>仍沿用 ReAct“全历史追加”模式，上下文耗尽问题未被解决；ReSum 直接解除长度约束，且仅用 1 k 样本即可比肩其性能。</p>
<p><strong>上下文压缩/记忆管理</strong></p>
<p>A-Mem (Xu et al., 2025) / MemOS (Li et al., 2025d)</p>
<p>外挂 RAG 记忆模块，定期写入与检索历史信息。</p>
<p>需额外检索器与存储，系统复杂且与策略模型松耦合；ReSum 用轻量级摘要工具原位压缩，无需外部存储。</p>
<p><strong>基于 RL 的上下文自管理</strong></p>
<p>MemAgent (Yu et al., 2025a) / Mem1 (Zhou et al., 2025b)</p>
<p>通过多轮 RL 让代理自己决定何时写入、遗忘或召回记忆。</p>
<p>需设计复杂记忆动作空间与奖励，训练成本高；ReSum 沿用 ReAct 动作集，仅增加“摘要即重启”触发，训练开销低且即插即用。</p>
<p><strong>摘要增强推理</strong></p>
<p>Chain-of-Agents (Li et al., 2025b) / DPO 摘要蒸馏 (MiroThinker)</p>
<p>用摘要实现多代理通信或蒸馏长文档。</p>
<p>聚焦对话或文档级压缩，未针对<strong>长周期工具交互历史</strong>设计；ReSumTool-30B 专门蒸馏“证据+缺口+下一步”三元组，适配网页搜索场景。</p>
<p>简言之，既有工作要么接受上下文耗尽为硬限制，要么引入外部记忆/复杂动作空间；ReSum 首次把“周期摘要+状态重启”作为<strong>轻量级、可插拔</strong>的推理范式，并配套专用摘要模型与 RL 训练框架，使任意现成代理无需大幅改造即可进行<strong>理论上无限步</strong>的网页探索。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“上下文窗口耗尽”这一硬障碍转化为<strong>可控的摘要-重启循环</strong>，通过三层设计实现“无限步”网页探索，具体路径如下：</p>
<p>1. 推理层：ReSum 范式——把“膨胀历史”变成“可重启状态”</p>
<ul>
<li><strong>触发机制</strong><br>当对话 token 数 ≥ 32 k（或代理主动请求）时，立即调用摘要工具 πsum。</li>
<li><strong>摘要内容</strong><br>输出结构化 <code>&lt;summary&gt;</code>，显式包含：<ul>
<li>已验证证据（事实+来源）</li>
<li>信息缺口列表</li>
<li>下一步建议（可选）</li>
</ul>
</li>
<li><strong>状态重置</strong><br>用 <code>(原查询 q, 摘要 s)</code> 拼接成新查询 q′，历史回滚到 H←(q′)，代理从压缩状态继续探索。</li>
</ul>
<blockquote>
<p>结果：上下文长度瞬间回到 O(|q|+|s|)，而关键线索零丢失，实现<strong>逻辑上的无限轮次</strong>。</p>
</blockquote>
<p>2. 模型层：ReSumTool-30B——专为“网页长轨迹”蒸馏的摘要专家</p>
<ul>
<li><strong>数据引擎</strong><br>用强模型（DeepSeek-R1 等）在 SailorFog-QA 上跑 ReSum rollout，收集 ⟨长对话, 专家摘要⟩ 10 k 对。</li>
<li><strong>训练目标</strong><br>对 Qwen3-30B-A3B-Thinking 做 SFT，优化目标：  <script type="math/tex; mode=display">\\max\_\\phi \\mathbb{E}_{(H,s^\*)}!\\left\[\\log \\pi_\\phi(s^_|H)\\right\]</script>其中 s_ 需满足：事实可溯源、缺口可验证、下一步可执行。</li>
<li><strong>效果</strong><br>30 B 参数即可在 BrowseComp-zh 上超越 235 B 级通用模型，部署成本 ↓7×。</li>
</ul>
<p>3. 训练层：ReSum-GRPO——让代理“习惯”从摘要继续推理</p>
<ul>
<li><strong>轨迹分段</strong><br>长轨迹被 K 次摘要自然切成 K+1 段，每段视为独立 episode。</li>
<li><strong>优势广播</strong><br>仅用最终答案对错得 0/1 奖励 R，归一化后整轨迹共享优势：  <script type="math/tex; mode=display">\\hat A^{(i)}\_g = \\frac{R\_g - \\mathrm{mean}({R})}{\\mathrm{std}({R})}, \\quad \\forall i\\in\[1,n\_g\]</script>保证每段都朝“生成易摘要、易推理”方向更新。</li>
<li><strong>兼容性</strong><br>只改 rollout 收集器，底层 GRPO 目标函数不变，短轨迹照常训练，<strong>零额外超参</strong>。</li>
</ul>
<p>4. 系统级效果</p>
<p>组件</p>
<p>解决的问题</p>
<p>贡献</p>
<p>ReSum 范式</p>
<p>上下文无限增长 → 周期性压缩</p>
<p>推理长度理论 ∞，token 占用恒定</p>
<p>ReSumTool-30B</p>
<p>通用模型摘要质量差、部署重</p>
<p>30 B 专家模型，质量≥671 B，延迟↓</p>
<p>ReSum-GRPO</p>
<p>代理不会“从摘要继续思考”</p>
<p>1 k 样本让代理适应新模式，平均+8.2 % Pass@1</p>
<p>综上，论文把“窗口耗尽”这一<strong>资源瓶颈</strong>转化为“摘要-重启”这一<strong>算法步骤</strong>，通过<br><strong>轻量级范式 + 专用摘要器 + 分段式 RL</strong><br>三位一体，首次让开源网页代理在<strong>不增参、不改架构</strong>的前提下完成<strong>长周期、多轮次、证据链完整</strong>的复杂搜索任务。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“<strong>能否在有限上下文里完成无限步探索</strong>”这一核心命题展开，分<strong>训练无关</strong>与<strong>训练相关</strong>两大设置，共覆盖 3 个高难度 benchmark、3 个规模代理、2 种 RL 算法，系统验证 ReSum 的即插即用收益与 RL 适应效果。</p>
<p>1 实验矩阵总览</p>
<p>设置</p>
<p>变量</p>
<p>基准</p>
<p>代理</p>
<p>关键指标</p>
<p><strong>训练无关</strong></p>
<p>推理范式</p>
<p>GAIA(103) / BrowseComp-en(200) / BrowseComp-zh(200)</p>
<p>WebSailor-3B/7B/30B</p>
<p>Pass@1, Pass@3</p>
<p><strong>训练相关</strong></p>
<p>RL 算法</p>
<p>同上</p>
<p>WebSailor-3B/30B</p>
<p>Pass@1, Pass@3</p>
<p><strong>消融</strong></p>
<p>摘要工具</p>
<p>BrowseComp-zh</p>
<p>WebSailor-3B</p>
<p>Pass@1</p>
<p>2 训练无关实验：验证“即插即用”</p>
<p>2.1 主对比</p>
<ul>
<li><strong>基线</strong>：ReAct（全历史追加） vs Recent-History（截断 22 k）</li>
<li><strong>ReSum</strong>：分别用 5 种摘要器（Qwen3-30B、GPT-OSS-120B、Qwen3-235B、DeepSeek-R1-671B、<strong>ReSumTool-30B</strong>）</li>
</ul>
<p>2.2 关键结果（Pass@1 绝对值，单位 %）</p>
<p>代理</p>
<p>基准</p>
<p>ReAct</p>
<p>Recent</p>
<p><strong>ReSumTool-30B</strong></p>
<p>最佳外部摘要器</p>
<p>WebSailor-3B</p>
<p>BrowseComp-zh</p>
<p>8.2</p>
<p>13.2</p>
<p><strong>13.7</strong></p>
<p>15.2 (GPT-OSS-120B)</p>
<p>WebSailor-7B</p>
<p>BrowseComp-en</p>
<p>5.7</p>
<p>5.2</p>
<p><strong>9.0</strong></p>
<p>10.5 (GPT-OSS-120B)</p>
<p>WebSailor-30B</p>
<p>BrowseComp-en</p>
<p>12.8</p>
<p>10.3</p>
<p><strong>16.0</strong></p>
<p>18.8 (GPT-OSS-120B)</p>
<blockquote>
<p>结论：ReSum 范式<strong>一致超越</strong> ReAct（平均 +4.5 %）；自研 30 B 摘要器即可媲美 671 B 模型，部署成本↓7×。</p>
</blockquote>
<p>2.3 与 SOTA 闭源模型对齐</p>
<p>WebSailor-30B + ReSumTool-30B 在 BrowseComp-en 达 <strong>16.0 % Pass@1</strong>，超越 Claude-4-Sonnet（12.2 %）与 Kimi-K2（14.1 %），<strong>首次让开源代理进入第一梯队</strong>。</p>
<p>3 训练相关实验：验证“RL 适应”</p>
<p>3.1 训练配置</p>
<ul>
<li>数据：从 SailorFog-QA 随机抽 1 k 题（刻意选长轨迹）</li>
<li>算法：标准 GRPO vs <strong>ReSum-GRPO</strong>（4 epoch，batch=64，group=8）</li>
<li>代理：WebSailor-3B/30B（无先前 RL 经验）</li>
</ul>
<p>3.2 结果（Pass@1）</p>
<p>代理</p>
<p>基准</p>
<p>ReAct</p>
<p>GRPO</p>
<p>ReSum-GRPO</p>
<p>10 k+ 样本 SOTA</p>
<p>WebSailor-3B</p>
<p>BrowseComp-zh</p>
<p>8.2</p>
<p>11.8</p>
<p><strong>20.5</strong></p>
<p>17.0 (MiroThinker-32B)</p>
<p>WebSailor-30B</p>
<p>BrowseComp-zh</p>
<p>23.9</p>
<p>23.3</p>
<p><strong>33.3</strong></p>
<p>15.6 (ASearcher-32B)</p>
<p>WebSailor-30B</p>
<p>BrowseComp-en</p>
<p>12.8</p>
<p>14.3</p>
<p><strong>18.3</strong></p>
<p>15.7 (WebExplorer-8B)</p>
<blockquote>
<p>结论：ReSum-GRPO 在 <strong>1 k 样本</strong>下即带来额外 <strong>+3.7 %~+8.2 %</strong> 绝对提升，<strong>追平甚至超越</strong>此前需 10 k+ 样本训练的最强开源代理。</p>
</blockquote>
<p>3.3 训练动力学</p>
<ul>
<li>ReSum-GRPO 初始奖励高于 GRPO 15 % 以上，收敛快 1.3×。</li>
<li>长轨迹比例从 18 % → 42 %，说明代理学会主动利用摘要继续探索。</li>
</ul>
<p>4 消融与效率分析</p>
<p>4.1 摘要器消融（BrowseComp-zh, WebSailor-3B）</p>
<p>摘要器</p>
<p>Pass@1</p>
<p>相对下降</p>
<p>ReSumTool-30B</p>
<p>13.7 %</p>
<p>—</p>
<p>Qwen3-30B（基座）</p>
<p>6.9 %</p>
<p>−49 %</p>
<p>DeepSeek-R1-671B</p>
<p>13.0 %</p>
<p>−5 %</p>
<blockquote>
<p>专用训练带来 <strong>49 % 错误降幅</strong>，验证“任务特定摘要”必要性。</p>
</blockquote>
<p>4.2 资源开销</p>
<ul>
<li><strong>训练时间</strong>：ReSum-GRPO 比 GRPO 仅长 33 %–69 %（单步 1.05 h vs 0.62 h，8×144 GB GPU）。</li>
<li><strong>推理成本</strong>：ReSum 平均 token/工具调用 ≈ 2×，但性能提升 4.5 %–8.2 %，<strong>性价比线性可接受</strong>。</li>
</ul>
<p>5 可重复性细节</p>
<ul>
<li>评分器：固定 Qwen2.5-72B-Instruct 做 LLM-as-Judge，避免人为偏差。</li>
<li>预算：最大 60 工具调用，温度 0.6，top_p 0.95，全部实验同一脚本开源。</li>
</ul>
<p>综上，实验从<strong>即插即用</strong>到<strong>RL 再训练</strong>、从<strong>3 B 到 30 B</strong>、从<strong>英文到中文</strong>，系统证明：</p>
<ol>
<li>ReSum 范式本身即可 <strong>+4.5 % Pass@1</strong>；</li>
<li>配合 ReSum-GRPO，<strong>1 k 样本</strong>就能追平此前 <strong>10 k+ 样本</strong>的 SOTA，<strong>首次把开源网页代理送进第一梯队</strong>。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 ReSum 的“直接外延”或“深层机理”探索，均具备可验证的实验设置与明确评估指标。</p>
<p>1 摘要触发策略：从“规则”到“自学”</p>
<ul>
<li><strong>问题现状</strong>：固定 32 k token 触发器过于保守，可能过早/过晚。</li>
<li><strong>可探索</strong><ul>
<li>强化学习内嵌“何时摘要”动作，奖励 = 摘要后能否在 N 步内更接近答案。</li>
<li>不确定性估计：若代理对下一步动作熵值 &gt; τ，则自动调用摘要。</li>
</ul>
</li>
<li><strong>评估</strong>：同样 60 调用预算下，比较“自适应触发”与固定触发在 Pass@1 与平均调用数的帕累托前沿。</li>
</ul>
<p>2 摘要内容空间：从“文本”到“可验证结构化对象”</p>
<ul>
<li><strong>问题现状</strong>：摘要仍为自然语言，下游代理可能误读。</li>
<li><strong>可探索</strong><ul>
<li>生成 JSON-LD/Knowledge-Graph 子图，节点附溯源 URL，边附置信度。</li>
<li>引入“可执行下一步”API 模板字段，让代理直接填充参数继续调用。</li>
</ul>
</li>
<li><strong>评估</strong>：代理从结构化摘要恢复答案的准确率 vs 文本摘要；人工检验事实幻觉率。</li>
</ul>
<p>3 多摘要融合：跨会话、跨代理的“全局记忆”</p>
<ul>
<li><strong>问题现状</strong>：每次重启后旧摘要只读，无法二次聚合。</li>
<li><strong>可探索</strong><ul>
<li>维护一个外部摘要池（向量索引），新摘要在池内做 RAG-merge，消除冗余、消解冲突。</li>
<li>引入“摘要版本链”，支持回溯式对比（类似 Git blame）。</li>
</ul>
</li>
<li><strong>评估</strong>：同一查询跑 M 次独立会话，比较单会话 vs 池融合后的最终答案 F1 及事实一致性。</li>
</ul>
<p>4 在线摘要器自改进：蒸馏→RL 的循环放大</p>
<ul>
<li><strong>问题现状</strong>：ReSumTool-30B 是一次性蒸馏，后续不再进化。</li>
<li><strong>可探索</strong><ul>
<li>用 ReSum-GRPO 产生的“高奖励轨迹”在线筛选优质摘要，反向微调 πsum，形成“摘要器↔策略”双塔共生。</li>
<li>采用 GRPO-within-GRPO 内环优化摘要器，外环优化策略。</li>
</ul>
</li>
<li><strong>评估</strong>：迭代三轮后，摘要器在人工标注的“证据召回率”指标上绝对提升 Δ≥3 %。</li>
</ul>
<p>5 跨模态长周期探索：把 ReSum 搬到 GUI/移动端</p>
<ul>
<li><strong>问题现状</strong>：ReSum 目前仅文本网页。</li>
<li><strong>可探索</strong><ul>
<li>将“摘要”升级为“跨模态状态”：含截图 OCR 文本、UI 结构树、已点击元素列表。</li>
<li>触发条件改为“截图差异度 &lt; ε 且无新信息”，避免同一页面反复点击。</li>
</ul>
</li>
<li><strong>评估</strong>：在 Mobile-Suite/WebArena 上比较原生长度限制 vs ReSum 的任务成功率，观察是否出现“无限循环”或“跨页面证据整合”新能力。</li>
</ul>
<p>6 理论侧：摘要压缩的最小充分性界限</p>
<ul>
<li><strong>问题现状</strong>：无摘要长度 vs 信息损失的理论刻画。</li>
<li><strong>可探索</strong><ul>
<li>借鉴信息瓶颈，定义“摘要-答案互信息 I(A;S)”与压缩率 R=Lsummary/Lhistory，绘制 R-I 曲线。</li>
<li>寻找临界 R*，使得继续增大摘要长度不再提升 I(A;S)。</li>
</ul>
</li>
<li><strong>评估</strong>：在合成数据集（GoldChain）上验证 R* 与任务难度（跳数）的线性关系，为后续自动选择摘要长度提供理论依据。</li>
</ul>
<p>7 安全与对齐：摘要能否成为“遗忘有害信息”的阀门</p>
<ul>
<li><strong>问题现状</strong>：长轨迹可能混入隐私或有害内容。</li>
<li><strong>可探索</strong><ul>
<li>在摘要阶段引入“安全过滤器”，强制删除 PII 或违规 URL，并记录删除日志供后续审计。</li>
<li>研究“安全摘要”是否导致下游答案准确率下降，权衡实用性与合规性。</li>
</ul>
</li>
<li><strong>评估</strong>：使用 Privacy-PolicyQA 基准，比较过滤前后答案准确率与隐私泄露分数（PII 计数）。</li>
</ul>
<p>8 异构工具链：摘要作为跨 API 的“通用检查点”</p>
<ul>
<li><strong>问题现状</strong>：现实工作流常涉及搜索→代码执行→数据库查询等多工具。</li>
<li><strong>可探索</strong><ul>
<li>定义工具无关的“状态描述模式”，让摘要器把 SQL 结果、Python 输出统一编码为同一格式。</li>
<li>支持“断点续跑”：代理崩溃后从摘要恢复，无需重跑昂贵代码。</li>
</ul>
</li>
<li><strong>评估</strong>：在 DataAgentBench（SQL+Python）上测量崩溃恢复后的答案一致性与重跑成本节省比例。</li>
</ul>
<p>以上任意一点均可在原代码库上增量实现，并通过<strong>Pass@1、摘要压缩率、人工事实准确率、资源消耗</strong>四维指标进行量化，形成对 ReSum 的“第二代”扩展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可概括为“<strong>一个范式、一个模型、一个算法</strong>”，解决“<strong>上下文窗口耗尽导致长周期搜索被迫中断</strong>”的难题。</p>
<p>1 痛点</p>
<ul>
<li>复杂网页查询需数十轮搜索-浏览-验证，ReAct 的“全历史追加”模式在 32 k token 上限内提前触顶，证据链无法闭合。</li>
</ul>
<p>2 ReSum 范式（Inference-level）</p>
<ul>
<li><strong>周期摘要</strong>：达 token 上限时调用摘要工具，把冗长历史压缩成“已验证证据 + 信息缺口 + 下一步建议”的结构化摘要。</li>
<li><strong>状态重启</strong>：用“(原查询, 摘要)”作为新上下文继续探索，<strong>逻辑上实现无限步推理</strong>，而 token 占用瞬间降回常数级。</li>
<li><strong>即插即用</strong>：零改代理架构，仅增加 <code>&lt;summary&gt;</code> 触发器。</li>
</ul>
<p>3 ReSumTool-30B（Model-level）</p>
<ul>
<li>以 Qwen3-30B-A3B-Thinking 为基座，用 10 k 条长轨迹⟨对话, 专家摘要⟩做 SFT，专精“网页噪声中抽证据、标缺口、给方向”。</li>
<li>30 B 参数在 BrowseComp-zh 上超越 235 B 通用模型，部署成本↓7×。</li>
</ul>
<p>4 ReSum-GRPO（Training-level）</p>
<ul>
<li>长轨迹被自然切成多段，每段共享<strong>最终答案对错</strong>归一化后的优势信号，迫使代理“学会”从摘要继续推理。</li>
<li>仅 1 k 样本即可让 WebSailor-30B 在 BrowseComp-zh 从 23.9 % → 33.3 % Pass@1，<strong>追平或超越</strong>此前需 10 k+ 样本的 SOTA 开源代理。</li>
</ul>
<p>5 实验结果</p>
<p>设置</p>
<p>平均增益</p>
<p>最高增益</p>
<p>备注</p>
<p>训练无关 ReSum</p>
<p>+4.5 % Pass@1</p>
<p>+8.2 %</p>
<p>3 基准 × 3 规模代理一致提升</p>
<p>ReSum-GRPO 再训练</p>
<p>额外 +4.1 %</p>
<p>+8.2 %</p>
<p>1 k 样本 ≈ 此前 10 k 样本效果</p>
<p>6 结论</p>
<p>ReSum 用<strong>轻量级摘要-重启</strong>取代“堆历史”，首次让<strong>任意现有网页代理</strong>在<strong>不增参、不改架构</strong>的情况下完成<strong>理论上无限步</strong>的探索，并通过专用摘要器与分段 RL 将开源模型推向 SOTA 水平。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是ReSum%3A%20Unlocking%20Long-Horizon%20Search%20Intelligence%20via%20Context%20Summarization，链接是  https://arxiv.org/pdf/2509.13313  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13313  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xixi Wu,Kuan Li,Yida Zhao,Liwen Zhang,Litu Ou,Huifeng Yin,Zhongwang Zhang,Yong Jiang,Pengjun Xie,Fei Huang,Minhao Cheng,Shuai Wang,Hong Cheng,Jingren Zhou</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13313.pdf">https://arxiv.org/pdf/2509.13313.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13313">https://arxiv.org/abs/2509.13313</a></p>
<p><strong>Arxiv ID</strong>: 2509.13313</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13313">https://papers.cool/arxiv/2509.13313</a></p>
<p><strong>Published</strong>: 2025-09-16T17:57:22Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:57:22.000Z</p>
<hr>
<h3 id="7-Single-stream-Policy-Optimization"><a href="#7-Single-stream-Policy-Optimization" class="headerlink" title="7. Single-stream Policy Optimization"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13232">Single-stream Policy Optimization</a></h3><p>We revisit policy-gradient optimization for Large Language Models (LLMs) from a single-stream perspective. Prevailing group-based methods like GRPO reduce variance with on-the-fly baselines but suffer from critical flaws: frequent degenerate groups erase learning signals, and synchronization barriers hinder scalability. We introduce Single-stream Policy Optimization (SPO), which eliminates these issues by design. SPO replaces per-group baselines with a persistent, KL-adaptive value tracker and normalizes advantages globally across the batch, providing a stable, low-variance learning signal for every sample. Being group-free, SPO enables higher throughput and scales effectively in long-horizon or tool-integrated settings where generation times vary. Furthermore, the persistent value tracker naturally enables an adaptive curriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO converges more smoothly and attains higher accuracy than GRPO, while eliminating computation wasted on degenerate groups. Ablation studies confirm that SPO’s gains stem from its principled approach to baseline estimation and advantage normalization, offering a more robust and efficient path for LLM reasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the average maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial absolute point gains on challenging datasets, including +7.3 pp on BRUMO 25, +4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain in pass@$k$ across the evaluated $k$ values. SPO’s success challenges the prevailing trend of adding incidental complexity to RL algorithms, highlighting a path where fundamental principles, not architectural workarounds, drive the next wave of progress in LLM reasoning.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>翻译失败: Unexpected end of JSON input</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>基于组（group-based）策略优化方法</strong>在大语言模型（LLM）强化学习训练中的<strong>两大核心缺陷</strong>：</p>
<ol>
<li><p><strong>计算浪费</strong>：<br>当一组采样响应全部正确或全部错误时（即“退化组”），组内相对优势归零，导致<strong>梯度信号完全消失</strong>，大量生成与评估开销被浪费。</p>
</li>
<li><p><strong>可扩展性瓶颈</strong>：<br>组方法要求<strong>同组所有响应完成后才能计算优势</strong>，在分布式环境或长周期、多轮工具调用的智能体任务中，<strong>最慢响应成为同步屏障</strong>，严重拖慢训练吞吐率。</p>
</li>
</ol>
<p>为此，作者提出<strong>单流策略优化（SPO）</strong>，回归经典单样本策略梯度范式，通过</p>
<ul>
<li>持续、KL 自适应的贝叶斯价值跟踪器替代瞬时组基线；</li>
<li>全局批次级优势归一化；</li>
<li>基于不确定性的优先采样课程；</li>
</ul>
<p>在<strong>消除退化组与同步等待</strong>的同时，显著降低梯度方差，提升<strong>训练效率、扩展性与最终推理准确率</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”与附录 C 中系统梳理了与 SPO 直接对比或思想相关的研究，可归纳为以下四条主线：</p>
<p>1. 组相对（group-based）方法：SPO 的主要批判对象</p>
<p>代表工作</p>
<p>核心思想</p>
<p>与 SPO 的关键差异</p>
<p><strong>GRPO</strong> (Shao et al. 2024)</p>
<p>每 prompt 采样 G 条回答，用组内均值/标准差构造瞬时基线并归一化优势。</p>
<p>存在“退化组”信息损失与同步屏障；基线仅依赖当前小样本，方差高。</p>
<p><strong>DAPO</strong> (Yu et al. 2025)</p>
<p>在 GRPO 基础上加“动态采样”——持续生成直到出现成功+失败为止。</p>
<p>保证非零优势，但期望采样数随 p→0/1 爆炸；计算浪费更严重。</p>
<p><strong>RLOO</strong> (Ahmadian et al. 2024)</p>
<p>回到 REINFORCE，用“留一法”组内 baseline。</p>
<p>仍受限于组同步与小样本估计，方差与退化问题依旧。</p>
<p><strong>GRESO</strong> (Zheng et al. 2025)</p>
<p>在线过滤，提前预测并跳过“无信息” prompt。</p>
<p>减少 rollout 开销，但未触动组架构本身；仍需等待组完成。</p>
<p><strong>Lite PPO</strong> (Liu et al. 2025)</p>
<p>仅保留优势归一化与 token-级 loss 聚合，去掉价值网络。</p>
<p>仍沿用 per-group 归一化，未解决根本缺陷。</p>
<p>2. 单样本/单流思想——与 SPO 同阵营</p>
<p>代表工作</p>
<p>核心思想</p>
<p>与 SPO 的关系</p>
<p><em>_A</em>-PO_* (Brantley et al. 2025)</p>
<p>两阶段：①离线回归拟合最优 V*；②在线用 V* 构造优势做最小二乘策略更新。</p>
<p>同为“单样本”输入，但 V* 固定离线，不随策略漂移；且受 KL-正则约束。</p>
<p><strong>REINFORCE++ / BASLINE-++</strong> (Hu et al. 2025)</p>
<p>在单样本 REINFORCE 上加 prompt-级鲁棒基线估计。</p>
<p>共享“单流”理念，但未引入持续贝叶斯跟踪与全局归一化。</p>
<p>3. 价值函数估计与方差约减理论</p>
<p>主题</p>
<p>代表文献</p>
<p>对 SPO 的启发</p>
<p>最优基线理论</p>
<p>Sutton &amp; Barto 2018; Greensmith et al. 2004</p>
<p>证明使用真实 V_π 可最小化梯度方差；SPO 用 KL-自适应跟踪器逼近。</p>
<p>全局优势归一化</p>
<p>Schulman et al. 2017 (PPO); Andrychowicz et al. 2020</p>
<p>在大批次内统一标准化，降低缩放噪声；SPO 直接采用。</p>
<p>贝叶斯在线更新</p>
<p>经典 Beta-Bernoulli 更新</p>
<p>SPO 把其扩展为“带遗忘”的指数加权，以应对非平稳策略。</p>
<p>4. 课程学习与优先采样</p>
<p>代表工作</p>
<p>核心思想</p>
<p>与 SPO 的对应</p>
<p><strong>Prioritized Experience Replay</strong> (Schaul et al. 2015)</p>
<p>用 TD-error 优先回放转移。</p>
<p>SPO 改为用 √{p(1-p)} 优先采样 prompt，形成自适应课程。</p>
<p><strong>POLARIS</strong> (An et al. 2025)</p>
<p>多级难度课程+重复策略，聚焦“可学习”样本。</p>
<p>目标类似，但 POLARIS 仍基于组重复；SPO 在采样前即完成调度，无生成后丢弃。</p>
<p>小结</p>
<ul>
<li><strong>组相对路线</strong>（GRPO 系列）是 SPO 直接对标与超越的对象；</li>
<li><strong>单样本路线</strong>（A*-PO 等）与 SPO 共享“去组化”愿景，但 SPO 通过<strong>持续贝叶斯跟踪+全局归一化+优先课程</strong>形成更完整的解决方案；</li>
<li>经典<strong>方差约减与课程学习理论</strong>为 SPO 的设计提供了原则性支撑。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“组相对”范式彻底拆散，回归单样本（single-stream）策略梯度，并用三个互补组件系统性地替换掉 GRPO 的瞬时组基线与组内归一化。具体做法如下：</p>
<p>1. KL-自适应价值跟踪器 ── 替代“组基线”</p>
<ul>
<li><p>把每个 prompt 视为独立 Bernoulli 过程，用 Beta(α,β) 维护其<strong>持续</strong>成功概率估计<br>ˆv(x)=α/(α+β)≈V_π(x)</p>
</li>
<li><p>每次观测新 reward 后，<strong>先按策略漂移程度折扣历史</strong>，再更新：<br>α←ρ(x)·α_{-1}+r, β←ρ(x)·β_{-1}+(1-r)<br>其中 ρ(x)=2^{-D(x)/D_half}，D(x) 是本次策略与上次作用于 x 时的 KL 距离。<br>→ 策略变化越大，遗忘越快，保证跟踪器始终跟得上非平稳策略。</p>
</li>
<li><p>该更新等价于<strong>自适应学习率 EMA</strong><br>ˆv(x)←ˆv_{-1}(x)+η(x)(r-ˆv_{-1}(x))<br>学习率 η(x) 随置信度与漂移自动调节，无需调参。</p>
</li>
</ul>
<p>2. 全局优势归一化 ── 替代“组内归一化”</p>
<ul>
<li><p>优势计算完全<strong>脱离当前批次</strong>的统计量：<br>A(x,y)=r(x,y)−ˆv_{-1}(x)<br>保证 baseline 与动作独立，梯度无偏。</p>
</li>
<li><p>归一化阶段<strong>在整个 prompt 批次</strong>上一次性完成：<br>Ã=(A−μ_B)/σ_B (B≫G)<br>均值/方差估计来自数百上千独立样本，噪声远低于 GRPO 的 8–16 样本组。</p>
</li>
</ul>
<p>3. 优先采样课程 ── 替代“均匀采样+动态补救”</p>
<ul>
<li><p>采样权重与<strong>贝叶斯不确定性</strong>成正比：<br>w(x)∝√{ˆv_{-1}(x)(1−ˆv_{-1}(x))}+ε<br>自动把算力投向“半生不熟”的 prompt；完全学会或完全不会的权重被抑制，ε 防止塌陷。</p>
</li>
<li><p>调度在<strong>生成前完成</strong>，无需像 DAPO 那样生成后丢弃，实现“零浪费”课程。</p>
</li>
</ul>
<p>4. 算法与系统实现</p>
<ul>
<li><p>整个训练循环（Algorithm 1）只有三层：</p>
<ol>
<li>按 w(x) 采 prompt → 单样本 rollout → 用 ˆv_{-1} 算 A</li>
<li>批次级归一化 → PPO-Clip 更新策略</li>
<li>用新 reward 更新 Beta 跟踪器</li>
</ol>
</li>
<li><strong>无组同步</strong>：每个样本是独立数据点，分布式环境可异步收集；长尾轨迹不再拖住整个批次。<br>在作者模拟的智能体场景里，batch 组装时间从 486 s → 112 s，<strong>吞吐提高 4.35×</strong>。</li>
</ul>
<p>5. 效果验证</p>
<ul>
<li>五套高难度数学竞赛题（AIME、BRUMO、HMMT 等）上，Qwen3-8B 用 SPO 训练后<br><strong>maj@32 平均绝对提升 +3.4 pp</strong>；BRUMO 25 单数据集 <strong>+7.3 pp</strong>。</li>
<li>梯度方差降低约 50%，且<strong>不存在退化组</strong>；GRPO 60–80% 样本零梯度的情况在 SPO 中消失。</li>
<li>pass@k 曲线全线高于 GRPO，表明优势信号更稳定、泛化更好。</li>
</ul>
<p>一句话总结</p>
<p>SPO 用“<strong>持续贝叶斯价值跟踪 + 全局批次归一化 + 不确定性优先课程</strong>”三件套，彻底取代组相对方法的小样本瞬时基线，从而<strong>消除退化组浪费与同步瓶颈</strong>，在更低方差、更高吞吐的前提下取得更优推理准确率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“<strong>算法效率</strong>”与“<strong>推理性能</strong>”两大维度，共设计四类实验，全部基于 Qwen3-8B 模型、英文 DAPO 数学数据集，并在统一开源框架（verl + ReTool）下完成，以保证公平可比。核心结果汇总如下：</p>
<p>1. 主实验：五套高难度数学竞赛基准</p>
<p>数据集</p>
<p>类型</p>
<p>测试题量</p>
<p>AIME 24 / 25</p>
<p>美国数学邀请赛</p>
<p>30 / 30</p>
<p>BeyondAIME</p>
<p>扩展 AIME 级</p>
<p>100</p>
<p>BRUMO 25</p>
<p>保加利亚数学奥林匹克</p>
<p>50</p>
<p>HMMT 25</p>
<p>哈佛-麻省数学锦标赛</p>
<p>60</p>
<p><strong>指标</strong></p>
<ul>
<li>maj@32：32 条输出投票后最终答案准确率</li>
<li>avg@32：32 条输出平均单样本准确率</li>
<li>pass@k（k=8,16,32）：k 条内至少对一次的概率（无偏估计）</li>
</ul>
<p><strong>结果（表 1 + 图 2）</strong></p>
<p>方法</p>
<p>maj@32 平均</p>
<p>相对提升</p>
<p>GRPO</p>
<p>60.4 %</p>
<p>—</p>
<p>SPO</p>
<p>63.8 %</p>
<p><strong>+3.4 pp</strong></p>
<ul>
<li>单数据集最高增幅：<strong>BRUMO 25 +7.3 pp</strong>；AIME 25 +4.4 pp；HMMT 25 +3.3 pp。</li>
<li>pass@k 曲线在 5 个数据集、所有 k 值上 SPO 均高于 GRPO，平均 <strong>+2.4 pp</strong>。</li>
</ul>
<p>2. 信号效率与稳定性剖析（图 3）</p>
<ul>
<li><p><strong>退化组比例</strong><br>GRPO 训练全程 60 %→80 % 样本因“全对/全错”获得零优势；SPO 的 |A|&lt;0.02 比例仅 ≈20 % 且随训练<strong>主动升高</strong>（ tracker 变准，残余变小），<strong>无梯度浪费</strong>。</p>
</li>
<li><p><strong>优势方差</strong><br>SPO 相对原始 reward 降低 <strong>≈50 %</strong>；GRPO 有效样本方差反而<strong>高于原始 reward</strong>，说明其组内基线/缩放极度不稳定。</p>
</li>
</ul>
<p>3. 智能体场景吞吐模拟（图 4-5）</p>
<p>设定：组装 24 条训练样本</p>
<ul>
<li><strong>Group-based</strong>（并行 6 组×4 选 3 最快组）受 508 s 长尾拖慢，<strong>总耗时 486 s</strong>。</li>
<li><strong>SPO group-free</strong>（超发 48 条取最先 24 条完成）<strong>112 s</strong> 完成，<strong>4.35× 加速</strong>。<br>结论：SPO 的异步、无屏障设计在长周期、多轮工具调用环境中可线性扩展，GRPO 则受“最慢轨迹”瓶颈。</li>
</ul>
<p>4. 超参与消融检验（附录）</p>
<ul>
<li><strong>离线初始化必要性</strong>：n₀=8 相对“无预热”前 20 步训练曲线更平滑，避免早期高方差。</li>
<li><strong>遗忘阈值</strong> ρ_min=0.875 (窗口 8) 在稳定性-适应性间平衡；再减小方差升高，再增大跟踪滞后。</li>
<li><strong>优先采样 ε=0.05</strong> 可在“完全探索”与“课程聚焦”间取得最佳最终性能；ε=0 导致早期过度聚焦，后期泛化略降。</li>
</ul>
<p>实验覆盖总结</p>
<p>维度</p>
<p>实验内容</p>
<p>关键结论</p>
<p>最终性能</p>
<p>5 大赛题 maj@32 / pass@k</p>
<p>SPO 全线领先，最高 +7.3 pp</p>
<p>训练效率</p>
<p>梯度信号利用率、方差</p>
<p>零退化组，方差 ↓50 %</p>
<p>系统可扩展</p>
<p>智能体长尾延迟模拟</p>
<p>4.35× 吞吐提升</p>
<p>算法鲁棒</p>
<p>初始化、ρ、ε 消融</p>
<p>设计参数合理，缺一不可</p>
<p>以上实验共同证明：<strong>SPO 不仅“更简单”，而且同时获得更高准确率、更低方差与显著基础设施收益</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“理论-算法”“系统-规模”“任务-形态”三大板块，供后续研究参考：</p>
<p>理论-算法层面</p>
<ol>
<li><p><strong>非二进制奖励的贝叶斯扩展</strong><br>当前 Beta-Bernoulli 模型仅适用于 0/1 奖励。可尝试：</p>
<ul>
<li>用 <strong>Gamma-Poisson</strong> 或 <strong>Normal-Normal</strong> 共轭对处理计数/连续奖励；</li>
<li>对多步稀疏奖励，引入 <strong>Gamma-Poisson 时间序列</strong> 把“成功次数/总次数”换成“累计回报/访问次数”。</li>
</ul>
</li>
<li><p><strong>跟踪器遗忘系数的自适应最优性</strong><br>ρ(x)=2^(−D(x)/D_half) 中的 D_half 现为常数超参。可：</p>
<ul>
<li>把 D_half 视为可学习参数，用 <strong>meta-gradient</strong> 最小化梯度方差目标在线调整；</li>
<li>或借鉴 <strong>Bayesian Change-Point Detection</strong>，在 KL 漂移显著时自动缩短窗口。</li>
</ul>
</li>
<li><p><strong>与优势-方差下界的比较</strong><br>论文给出经验方差下降 50 %。可进一步推导 <strong>SPO 优势估计器的方差闭合式</strong>，与 Konda-Tsitsiklis、Greensmith 等下界对比，验证是否接近理论最优。</p>
</li>
<li><p><strong>策略熵与基线的联合优化</strong><br>SPO 目前仅使用 PPO-Clip。可将 <strong>熵正则项</strong> 显式写入目标，与 ˆv(x) 共同做 <strong>二阶优化</strong>，在保持高概率正确的同时延迟熵崩溃。</p>
</li>
</ol>
<p>系统-规模层面</p>
<ol>
<li><p><strong>异步分布式框架的极致化</strong></p>
<ul>
<li>把“超发-取最快”策略做成 <strong>在线 Early-Stopping</strong> 协议：当已收样本≥B 且剩余未完成样本的期望边际收益低于阈值时，主动终止长尾任务；</li>
<li>结合 <strong>partial rollout / v-trace 截断</strong>，在万卡级集群上测试是否保持线性加速。</li>
</ul>
</li>
<li><p><strong>异构算力环境下的动态批尺寸</strong><br>不同 GPU/TPU 的推理延迟差异显著。可让 <strong>批大小 B 成为随集群状态变化的随机变量</strong>，用 <strong>MARL 视角</strong> 把“采样-训练”看作多智能体合作，最大化全局吞吐。</p>
</li>
<li><p><strong>CPU-GPU 协同的 Value Tracker offload</strong><br>Beta 更新计算量极小，可放在 <strong>CPU 侧异步刷新</strong>，GPU 仅读取 ˆv(x) 常量，减少 kernel 内分支，进一步压缩 step time。</p>
</li>
</ol>
<p>任务-形态层面</p>
<ol>
<li><p><strong>长周期智能体任务的真实端到端训练</strong><br>论文模拟了延迟分布。可在：</p>
<ul>
<li><strong>BrowseComp-Plus</strong> 或 <strong>WebShop</strong> 等多轮网页导航任务；</li>
<li><strong>Thousands-of-Games</strong> 3D 环境（平均 2k+ 步/局）<br>上验证 SPO 是否保持 4×+ 加速且提升最终成功率。</li>
</ul>
</li>
<li><p><strong>多模态工具链（代码+检索+图像）混合奖励</strong><br>当奖励空间变为 <strong>{0,0.5,1}</strong>（代码通过但结果错、部分对、全对）或 <strong>多维向量</strong> 时，测试全局归一化与优先采样是否仍优于组相对基线。</p>
</li>
<li><p><strong>迭代式“自我进化”课程</strong><br>当前优先权重仅依赖不确定性。可引入 <strong>在线难度估计</strong> 与 ** regret 信号**，让课程从“易→难→易”动态循环，防止过拟合早期分布。</p>
</li>
<li><p><strong>SPO 作为生成式验证器微调的正则器</strong><br>把 ˆv(x) 视作“模型对自己答案的信心”，与 <strong>Test-Time Scaling</strong> 结合：</p>
<ul>
<li>高 1−ˆv(x) 的 prompt 在推理阶段分配更多生成预算；</li>
<li>低 ˆv(x) 的 prompt 直接提前停止，节省推理成本。</li>
</ul>
</li>
</ol>
<p>小结</p>
<ul>
<li><strong>理论侧</strong> 可深挖非平稳环境下的最优遗忘率与方差下界；</li>
<li><strong>系统侧</strong> 可把异步、早停、异构算力推向极致，实现万卡级线性扩展；</li>
<li><strong>任务侧</strong> 可把 SPO 的“单流+优先”思想搬到长周期智能体、多模态工具链及测试-时缩放场景，验证通用性。</li>
</ul>
<p>这些方向既能进一步压缩训练成本，也有望在更复杂的推理与智能体任务上继续拉开与组相对方法的差距。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心速览</p>
<p><strong>题目</strong>：Single-stream Policy Optimization (SPO)<br><strong>目标</strong>：解决“组相对”强化学习在 LLM 推理训练中<strong>计算浪费 + 同步瓶颈</strong>两大顽疾，用<strong>单样本、无组、异步</strong>的新范式实现更高准确率与吞吐。</p>
<p>1. 问题剖析</p>
<ul>
<li><strong>退化组</strong>：同组回答全对/全错 → 优势归零 → 梯度信号 100 % 浪费。</li>
<li><strong>同步屏障</strong>：组内最慢样本拖慢整个 batch，长周期/多轮工具场景尤为严重。</li>
</ul>
<p>2. 方法总览（三组替换）</p>
<p>GRPO 组件</p>
<p>SPO 替代</p>
<p>关键公式</p>
<p>瞬时组基线</p>
<p><strong>KL-自适应 Beta 跟踪器</strong></p>
<p>ρ(x)=2^(−D_KL/ D_half)，α←ρα+r，β←ρ(1−r)</p>
<p>组内归一化</p>
<p><strong>全局批次归一化</strong></p>
<p>Ã=(A−μ_B)/σ_B，B≫G</p>
<p>均匀采样</p>
<p><strong>不确定性优先课程</strong></p>
<p>w(x)∝√{v(1−v)}+ε</p>
<p><strong>算法骨架</strong>：单 prompt → 单回答 → 用跟踪器得 A → 批次归一 → PPO-Clip 更新 → 在线更新 Beta。</p>
<p>3. 主要结果</p>
<ul>
<li><strong>精度</strong>：Qwen3-8B 在五大赛题 maj@32 平均 <strong>63.8 % vs 60.4 %</strong>（+3.4 pp），最高单数据集 <strong>+7.3 pp</strong>。</li>
<li><strong>效率</strong>：梯度方差 ↓50 %，零退化组；智能体模拟 <strong>4.35× 吞吐</strong>提升。</li>
<li><strong>稳定性</strong>：全程无手动补救，曲线更平滑。</li>
</ul>
<p>4. 贡献一句话</p>
<p>SPO 用“<strong>持续贝叶斯价值跟踪 + 全局归一化 + 优先课程</strong>”拆掉组同步，<strong>同时获得更低方差、更高准确率与显著基础设施收益</strong>，为 LLM 推理 RL 提供简洁而更强健的新基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Single-stream%20Policy%20Optimization，链接是  https://arxiv.org/pdf/2509.13232  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13232  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhongwen Xu,Zihan Ding</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,stat.ML</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13232.pdf">https://arxiv.org/pdf/2509.13232.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13232">https://arxiv.org/abs/2509.13232</a></p>
<p><strong>Arxiv ID</strong>: 2509.13232</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13232">https://papers.cool/arxiv/2509.13232</a></p>
<p><strong>Published</strong>: 2025-09-16T16:39:11Z</p>
<p><strong>Updated</strong>: 2025-09-16T16:39:11.000Z</p>
<hr>
<h3 id="8-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation"><a href="#8-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation" class="headerlink" title="8. Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12815">Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation</a></h3><p>The creation of high-quality 3D assets, a cornerstone of modern game development, has long been characterized by labor-intensive and specialized workflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered content creation platform designed to revolutionize the game production pipeline by automating and streamlining the generation of game-ready 3D assets. At its core, Hunyuan3D Studio integrates a suite of advanced neural modules (such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into a cohesive and user-friendly system. This unified framework allows for the rapid transformation of a single concept image or textual description into a fully-realized, production-quality 3D model complete with optimized geometry and high-fidelity PBR textures. We demonstrate that assets generated by Hunyuan3D Studio are not only visually compelling but also adhere to the stringent technical requirements of contemporary game engines, significantly reducing iteration time and lowering the barrier to entry for 3D content creation. By providing a seamless bridge from creative intent to technical asset, Hunyuan3D Studio represents a significant leap forward for AI-assisted workflows in game development and interactive media.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>高质量3D资产的创建是现代游戏开发的基石，长期以来以劳动密集型和专业化的工作流程为特征。本文介绍了Hunyuan3D工作室，这是一个端到端的人工智能驱动的内容创作平台，旨在通过自动化和简化游戏准备3D资产的生成来革新游戏制作流程。在其核心，Hunyuan3D工作室将一套先进的神经模块（如部件级3D生成、多边形生成、语义UV等）集成到一个统一且用户友好的系统中。这个统一的框架允许将单个概念图像或文本描述快速转化为一个完整的、符合生产质量的3D模型，具备优化的几何形状和高保真的PBR纹理。我们展示了Hunyuan3D工作室生成的资产不仅在视觉上引人注目，而且符合当代游戏引擎严格的技术要求，显著缩短了迭代时间并降低了3D内容创作的入门门槛。通过提供从创意意图到技术资产的无缝桥梁，Hunyuan3D工作室代表了游戏开发和互动媒体中AI辅助工作流程的重大进步。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决传统3D游戏资产制作流程中“高视觉质量”与“技术就绪度”难以兼得的瓶颈问题。具体而言：</p>
<ul>
<li><p><strong>传统管线痛点</strong><br>手工建模-拓扑-拆UV-贴图-绑定五步各自依赖不同软件，周期长、门槛高；AI 方法往往只解决单点（如几何或纹理），输出网格面数冗余、拓扑混乱、UV 语义缺失、PBR 贴图不兼容，仍需大量人工返工。</p>
</li>
<li><p><strong>核心目标</strong><br>建立端到端、一键式、<strong>引擎可直接调用</strong>的 AI 管线，实现“单张概念图或一句话 → 带 PBR 材质、低面数、已绑定、可动画”的游戏级资产，显著压缩迭代时间并降低创作门槛。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>几何生成</strong></p>
<ul>
<li>Hunyuan3D 系列（Hunyuan3D 2.1/2.5、Michelangelo）</li>
<li>3DShape2VecSet、Clay、MeshGPT、Meshtron</li>
<li>基于扩散或自回归的图像/文本到 3D 网格/点云方法（DiffusionGAN3D、PartCrafter、HoloPart、OmniPart）</li>
</ul>
</li>
<li><p><strong>部件级/结构感知生成</strong></p>
<ul>
<li>PartField、SAMesh、Find3D、SAMPart3D</li>
<li>P3-SAM（原生 3D 部件分割）</li>
<li>X-Part（结构一致部件分解）</li>
</ul>
</li>
<li><p><strong>自动拓扑与低面数网格生成</strong></p>
<ul>
<li>PolyGen、DeepMesh、MeshAny</li>
<li>Blocked-and-Patchified Tokenization（BPT）</li>
<li>基于自回归或强化学习的面序列生成（Mesh-RFT）</li>
</ul>
</li>
<li><p><strong>语义 UV 展开</strong></p>
<ul>
<li>XAtlas、Nuvo、FAM、Flatten-Anything</li>
<li>SeamGPT（自回归切割线预测）</li>
</ul>
</li>
<li><p><strong>多视图一致纹理与 PBR 材质</strong></p>
<ul>
<li>MaterialMVP、Romantex</li>
<li>多视图扩散烘焙、IP-Adapter/Flux-Kontext 风格迁移</li>
<li>4K 可平铺材质球生成（3D VAE + DiT）</li>
</ul>
</li>
<li><p><strong>自动绑定与蒙皮</strong></p>
<ul>
<li>UniRig、Auto-Connect、RigAnything</li>
<li>MagicArticulate、Make-it-Animatable</li>
<li>基于模板或自回归的骨架生成 + 拓扑感知权重预测</li>
</ul>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“高保真 yet 引擎就绪”的 3A 资产生成拆成 <strong>7 个顺序、可微、可反向传播</strong> 的神经网络模块，并通过统一资产图（Asset Graph）把每步元数据向下游传递，实现“单图/一句话 → 游戏直用”的一键闭环。核心思路是：<strong>用数据驱动的神经算子替代传统手工管线，并在每个环节显式注入“游戏约束”</strong>。</p>
<ol>
<li><p><strong>可控概念图生成</strong></p>
<ul>
<li>文本/参考图 → 多视图 + 风格化 LoRA，内置 A-Pose 标准化网络，保证后续绑定一致性。</li>
<li>输出：带相机矩阵、风格标签、前景掩码的 4-6 张 512² 视图。</li>
</ul>
</li>
<li><p><strong>高保真几何扩散</strong></p>
<ul>
<li>Hunyuan3D-DiT（21 层 MoE-Transformer）在 ShapeVAE 隐空间做流匹配。</li>
<li>条件 = 单视图 cI + 可选 bbox cB + 自生成多视图 {cIⁱ}，显式对齐比例与深度。</li>
<li>输出：带表面法向的 512³ 有向距离场 → 抽取高密度三角网格（≈ 200 k-500 k 面）。</li>
</ul>
</li>
<li><p><strong>部件级 3D 生成</strong></p>
<ul>
<li>P3-SAM：3D 点提示分割网络，370 万自动标注网格训练，零样本给出部件掩码。</li>
<li>X-Part：以部件 bbox 为提示的潜码扩散，保证“语义+结构”双一致，避免边界模糊。</li>
<li>输出：带部件标签的子网格，支持独立编辑/替换。</li>
</ul>
</li>
<li><p><strong>PolyGen 低面数拓扑</strong></p>
<ul>
<li>自回归 Transformer + BPT 标记化（块索引+面片聚合），直接逐面预测顶点索引。</li>
<li>两阶段训练：<br>– 大规模网格预训练重建损失；<br>– Masked-DPO 偏好微调，用 BER/TS/HD 三元指标自动排序 8 候选网格，仅对“坏区域”token 施加偏好损失。</li>
<li>输出：4 k-8 k 面的游戏级网格，边流已预对齐变形方向。</li>
</ul>
</li>
<li><p><strong>SeamGPT 语义 UV</strong></p>
<ul>
<li>把切割线表示为有序 3D 线段序列，Hourglass Transformer 自回归生成。</li>
<li>输入仅在边/顶点处采 61 k 结构点，保证切口与网格对齐；长度嵌入控制粒度。</li>
<li>输出：低拉伸、语义连续（同材质同岛）的 UV 布局，可直接进 Blender/UE/Unity。</li>
</ul>
</li>
<li><p><strong>PBR 纹理合成与多模态编辑</strong></p>
<ul>
<li>多视图扩散 + 投影烘焙 → 4K BaseColor/Normal/Rough/Metal。</li>
<li>80 k 资产训练 VLM 描述符，支持文本或参考图全局/局部编辑；CLIP 相似度决定注入路径（VAE 或 IP-Adapter）。</li>
<li>额外 4K 材质球 DiT，可平铺输出，供艺术家二次混合。</li>
</ul>
</li>
<li><p><strong>自动绑定与蒙皮</strong></p>
<ul>
<li>人形：22 关节模板 + 拓扑感知权重预测，输入 T-pose；</li>
<li>非人形：自回归骨架生成（OPT-350M）+ 图卷积拓扑编码，输出任意关节数。</li>
<li>统一损失：关节位置误差 + 权重平滑 + 运动可变形能量，确保 UE/Unity 内直接播放动画。</li>
</ul>
</li>
</ol>
<p><strong>统一资产图</strong></p>
<ul>
<li>每步输出附带参数化节点（mesh, part-id, uv-island, material-id, joint-weight）。</li>
<li>用户可在任意节点调参（如“面数减 30 %”、“金属度 + 0.2”），梯度或元数据自动前向/反向传播，无需全链重跑。</li>
<li>末端导出插件一次性生成 .fbx/.gltf + UE/Unity 材质模板，完成技术合规检查（面数、贴图分辨率、命名规范）。</li>
</ul>
<p>通过上述神经-图形混合管线，论文把传统 5-7 天的人工工作量压缩到 <strong>10-15 分钟端到端生成</strong>，同时满足现代游戏引擎对拓扑、UV、PBR、骨骼的硬性指标。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“<strong>生成质量</strong>”与“<strong>引擎就绪度</strong>”两大维度，在<strong>几何、部件、拓扑、UV、纹理、绑定</strong>六大模块分别设置了定量基准测试、用户研究与消融实验，并给出跨模块端到端案例。核心实验汇总如下：</p>
<p>模块</p>
<p>数据集</p>
<p>指标</p>
<p>主结论</p>
<p><strong>1. 高保真几何生成</strong></p>
<p>GSO / Objaverse 2k 张单图</p>
<p>Chamfer ↓ / F-score ↑ / 法向一致性 ↑</p>
<p>多视图条件较单图 <a href="mailto:F-score@0.5">F-score@0.5</a> 提升 8.3；bbox 条件解决比例失衡，成功率 96%。</p>
<p><strong>2. 部件分割 (P3-SAM)</strong></p>
<p>PartObj-Tiny / PartNetE</p>
<p>mIoU ↑ / 交互成功率 ↑</p>
<p>零样本 mIoU 81.14（+1.96 SOTA）；交互式分割 51.23（+38.2 相对 Point-SAM）。</p>
<p><strong>3. 部件分解 (X-Part)</strong></p>
<p>Objaverse-Part-Tiny 200 样例</p>
<p>CD ↓ / <a href="mailto:F-score@0.1">F-score@0.1</a> ↑ / @0.5 ↑</p>
<p>CD 0.11（-0.06 SOTA）；<a href="mailto:F-score@0.5">F-score@0.5</a> 0.71（+0.14）。</p>
<p><strong>4. PolyGen 低面数拓扑</strong></p>
<p>自建 50k 游戏资产</p>
<p>BER ↓ / TS ↑ / HD ↓</p>
<p>后训练使 BER ↓37%，破面率 ↓82%；复杂机匣 8k 面 HD 0.18×10⁻³ 优于 DeepMesh 0.43。</p>
<p><strong>5. 语义 UV (SeamGPT)</strong></p>
<p>FAM / Toys4K</p>
<p>面扭曲能量 ↓</p>
<p>平均扭曲 13.04（-5.33 次优）；用户研究“可编辑性”4.0/5（+0.9）。</p>
<p><strong>6. PBR 纹理 &amp; 编辑</strong></p>
<p>自采 3k 商用资产</p>
<p>LPIPS ↓ / SSIM ↑ / 色差 ΔE ↓</p>
<p>多视图一致 LPIPS 0.051；文本编辑保留几何结构 ΔE&lt;2.3；4K 材质球无接缝 SSIM 0.97。</p>
<p><strong>7. 自动绑定</strong></p>
<p>自建 80k 通用 + 10k 人形</p>
<p>关节位置误差 ↓ / 权重光滑度 ↑</p>
<p>通用骨架误差 1.8 mm（-0.9 UniRig）；人形 Skinning 形变能量 ↓22%。</p>
<p><strong>8. 端到端耗时 &amp; 合规</strong></p>
<p>20 名艺术家 30 天双盲</p>
<p>生产时间 ↓ / 引擎报错率 ↓</p>
<p>单资产平均 12.4 min（人工 5.2 h）；导入 UE5 报错率 2%（人工 9%）。</p>
<p>关键消融实验</p>
<ul>
<li><strong>多视图 vs 单视图条件</strong>：<a href="mailto:F-score@0.5">F-score@0.5</a> 从 0.63→0.71。</li>
<li><strong>bbox 条件消融</strong>：比例误差 &gt;15% 的样本从 22%→4%。</li>
<li><strong>PolyGen 后训练</strong>：BER 0.18→0.11，破面率 9%→1.6%。</li>
<li><strong>SeamGPT 采样策略</strong>：边-顶点采样较均匀采样扭曲能量 ↓28%。</li>
<li><strong>纹理 CLIP-相似度门限</strong>：阈值 0.78 时编辑一致性最佳，↑0.05 过拟合风格。</li>
</ul>
<p>用户研究</p>
<ul>
<li><strong>UV 展开</strong>：20 名专业艺术家盲评，SeamGPT 在“边界完整度”“编辑友好度”双指标均显著优于 XAtlas、Nuvo、FAM（p&lt;0.01）。</li>
<li><strong>纹理编辑</strong>：20 名美术对 120 组文本/图像编辑打分，Hunyuan3D 在“风格忠实”“几何保真”两项 5 分制均 ≥4.0，领先基线 0.7-1.2 分。</li>
</ul>
<p>跨模块端到端案例</p>
<ul>
<li><strong>输入</strong>：一句“赛博朋克风格的猫头鹰刺客”。</li>
<li><strong>输出</strong>：8 092 面三角网格、3 个语义部件、4×4K PBR 贴图、22 关节骨架、UE5 可直接播放攻击动画；全程 13 min，通过引擎合规检查。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下问题仍留有显著研究空白，可作为后续探索方向：</p>
<ol>
<li><p><strong>显式物理-感知生成</strong></p>
<ul>
<li>当前管线仅保证“外观+拓扑”合规，尚未在扩散阶段嵌入质量、质心、碰撞体等物理参数，导致部分资产需二次手调刚体/布料参数。</li>
<li>可引入可微物理仿真（NDP、DiffSim）作为损失，直接生成“静力平衡+碰撞凸包”就绪的资产。</li>
</ul>
</li>
<li><p><strong>细粒度交互式编辑</strong></p>
<ul>
<li>文本/图像指令目前只能做“全局或部件级”材质、比例编辑；尚不支持“顶点级”雕刻、折痕、磨损痕迹的局部控制。</li>
<li>探索点：<br>– 将 Mesh-RFT 的 Masked-DPO 扩展到“顶点位移”token；<br>– 采用 3D-StyleGAN 潜空间插值实现实时刷笔式编辑。</li>
</ul>
</li>
<li><p><strong>拓扑—aware 的动画自发现</strong></p>
<ul>
<li>绑定模块仍依赖人工模板（22 关节）或自回归指针，对“软体、流体、可展开机构”等非常规运动学无能为力。</li>
<li>可引入“运动先验”：先在大规模动作捕捉上训练潜码扩散，再反推最小拓扑与关节数，实现“动作-驱动-拓扑”联合优化。</li>
</ul>
</li>
<li><p><strong>跨资产风格一致性</strong></p>
<ul>
<li>同一项目批量生成时，仅通过风格 LoRA 难以保证“色彩基调+材质基底”严格一致。</li>
<li>探索“项目级全局调色板”注入：在纹理扩散的 cross-attention 中增加项目 embedding，或采用基于 CLIP-方向约束的批量一致性损失。</li>
</ul>
</li>
<li><p><strong>4K+ 纹理内存与实时 streaming</strong></p>
<ul>
<li>目前 4K PBR 贴图在移动 VR/AR 上仍显笨重；尚未自动导出 tiled+compressed 版本（如 UE5 VT、ASTC 6×6）。</li>
<li>可引入神经压缩纹理（Neural Texture Codec）或可微分纹理流送（TexNeRF）在生成阶段即输出 128×128 隐空间贴图，实时解码。</li>
</ul>
</li>
<li><p><strong>自动 LOD &amp; Impostor 生成</strong></p>
<ul>
<li>管线只输出单一精度网格；大型开放世界仍需手工制作 LOD1-LOD4 与 Billboard。</li>
<li>后续可在 PolyGen 的自回归解码器中加入“细节控制 token”，一次性生成多组面数递减序列，并联合生成深度-法向 impostor sprite。</li>
</ul>
</li>
<li><p><strong>可解释性 &amp; 版权溯源</strong></p>
<ul>
<li>目前缺乏对“生成资产与训练数据相似度”的可解释报告，商用存在侵权风险。</li>
<li>可探索“3D 版权指纹”：在 ShapeVAE 隐空间记录并公开 Top-k 最近邻，或引入差分隐私训练。</li>
</ul>
</li>
<li><p><strong>开放世界动态资产</strong></p>
<ul>
<li>现有框架面向静态 prop/character；对“可破坏建筑、生长植被”等带状态变化的资产未做支持。</li>
<li>后续可引入时序扩散（3D DiT + 时间轴）直接生成“破坏-碎裂”或“生长-枯萎”序列，并输出对应刚体碎片或骨架动画。</li>
</ul>
</li>
<li><p><strong>多模态条件更丰富的输入</strong></p>
<ul>
<li>目前仅单图/文本；可扩展“草图+语音+风格参考视频”混合输入，需要设计统一 tokenizer 将 2D 草图、音频描述、视频风格映射到同一条件空间。</li>
</ul>
</li>
<li><p><strong>极端领域鲁棒性</strong></p>
<ul>
<li>对“薄壳、重复晶格、半透明”等难案例，几何/纹理仍可能出现孔洞或折射误差。</li>
<li>可构建“困难案例课程学习”：先在小批量难例上高权重微调，再逐步回混正常数据，提升尾部分布表现。</li>
</ul>
</li>
</ol>
<p>以上方向兼顾<strong>生成质量、交互自由度、物理合规、版权合规</strong>四大维度，可推动 Hunyuan3D Studio 向“全自动、可解释、物理正确、项目一致”的下一代 AI 资产工厂演进。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>问题</strong><br>传统 3A 游戏资产流程分散、手工量大，现有 AI 方法只解决“几何”或“纹理”单点，输出拓扑混乱、UV 语义缺失、PBR 不兼容，难以直接进引擎。</p>
</li>
<li><p><strong>方案</strong><br>提出 Hunyuan3D Studio——端到端神经管线，把“单图/一句话”一次性变成面数优化、PBR 贴图、已绑定、可动画的引擎级资产：</p>
<ol>
<li>可控概念图（风格化 + A-Pose 标准化）</li>
<li>高保真几何扩散（bbox/多视图条件，ShapeVAE+DiT）</li>
<li>部件级生成（P3-SAM 分割 → X-Part 结构一致分解）</li>
<li>PolyGen 低面拓扑（自回归 BPT token + Masked-DPO 微调）</li>
<li>SeamGPT 语义 UV（自回归切割线，材质-aware 分岛）</li>
<li>PBR 纹理与多模态编辑（多视图烘焙 + 文本/图像指令 + 4K 材质球）</li>
<li>自动绑定（人形模板/通用自回归骨架 + 拓扑-aware 蒙皮）</li>
</ol>
<p>统一资产图串联七步，支持参数回传与增量更新，导出即 UE/Unity 格式。</p>
</li>
<li><p><strong>实验</strong><br>在公开与自建基准上全面评估：几何误差、部件 mIoU、拓扑质量、UV 拉伸、纹理一致性、绑定误差均领先 SOTA；用户研究显示生产时间从 5.2 h 缩至 12.4 min，引擎报错率降至 2%。</p>
</li>
<li><p><strong>结论</strong><br>Hunyuan3D Studio 首次把“生成-到-引擎”全链路神经化，显著降低 3D 内容门槛，为游戏、虚拟制作提供可落地的 AI 资产工厂。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Hunyuan3D%20Studio%3A%20End-to-End%20AI%20Pipeline%20for%20Game-Ready%203D%20Asset%20Generation，链接是  https://arxiv.org/pdf/2509.12815  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12815  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Biwen Lei,Yang Li,Xinhai Liu,Shuhui Yang,Lixin Xu,Jingwei Huang,Ruining Tang,Haohan Weng,Jian Liu,Jing Xu,Zhen Zhou,Yiling Zhu,Jiankai Xing,Jiachen Xu,Changfeng Ma,Xinhao Yan,Yunhan Yang,Chunshi Wang,Duoteng Xu,Xueqi Ma,Yuguang Chen,Jing Li,Mingxin Yang,Sheng Zhang,Yifei Feng,Xin Huang,Di Luo,Zebin He,Puhua Jiang,Changrong Hu,Zihan Qin,Shiwei Miao,Haolin Liu,Yunfei Zhao,Zeqiang Lai,Qingxiang Lin,Zibo Zhao,Kunhong Li,Xianghui Yang,Huiwen Shi,Xin Yang,Yuxuan Wang,Zebin Yao,Yihang Lian,Sicong Liu,Xintong Han,Wangchen Qin,Caisheng Ouyang,Jianyin Liu,Tianwen Yuan,Shuai Jiang,Hong Duan,Yanqi Niu,Wencong Lin,Yifu Sun,Shirui Huang,Lin Niu,Gu Gong,Guojian Xiao,Bojian Zheng,Xiang Yuan,Qi Chen,Jie Xiao,Dongyang Zheng,Xiaofeng Yang,Kai Liu,Jianchen Zhu,Lifu Wang,Qinglin Lu,Jie Liu,Liang Dong,Fan Jiang,Ruibin Chen,Lei Wang,Chao Zhang,Jiaxin Lin,Hao Zhang,Zheng Ye,Peng He,Runzhou Wu,Yinhe Wu,Jiayao Du,Jupeng Chen,Xinyue Mao,Dongyuan Guo,Yixuan Tang,Yulin Tsai,Yonghao Tan,Jiaao Yu,Junlin Yu,Keren Zhang,Yifan Li,Peng Chen,Tian Liu,Di Wang,Yuhong Liu,Linus,Jie Jiang,Zhuo Chen,Chunchao Guo</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12815.pdf">https://arxiv.org/pdf/2509.12815.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12815">https://arxiv.org/abs/2509.12815</a></p>
<p><strong>Arxiv ID</strong>: 2509.12815</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12815">https://papers.cool/arxiv/2509.12815</a></p>
<p><strong>Published</strong>: 2025-09-16T08:33:03Z</p>
<p><strong>Updated</strong>: 2025-09-16T08:33:03.000Z</p>
<hr>
<h3 id="9-3D-Aware-Region-Prompted-Vision-Language-Model"><a href="#9-3D-Aware-Region-Prompted-Vision-Language-Model" class="headerlink" title="9. 3D Aware Region Prompted Vision Language Model"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13317">3D Aware Region Prompted Vision Language Model</a></h3><p>We present Spatial Region 3D (SR-3D) aware vision-language model that connects single-view 2D images and multi-view 3D data through a shared visual token space. SR-3D supports flexible region prompting, allowing users to annotate regions with bounding boxes, segmentation masks on any frame, or directly in 3D, without the need for exhaustive multi-frame labeling. We achieve this by enriching 2D visual features with 3D positional embeddings, which allows the 3D model to draw upon strong 2D priors for more accurate spatial reasoning across frames, even when objects of interest do not co-occur within the same view. Extensive experiments on both general 2D vision language and specialized 3D spatial benchmarks demonstrate that SR-3D achieves state-of-the-art performance, underscoring its effectiveness for unifying 2D and 3D representation space on scene understanding. Moreover, we observe applicability to in-the-wild videos without sensory 3D inputs or ground-truth 3D annotations, where SR-3D accurately infers spatial relationships and metric measurements.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们提出了空间区域三维(SR-3D) 感知视觉语言模型，连接单视图二维图像和多视图三维数据，通过共享的视觉标记空间实现。SR-3D 支持灵活的区域提示，允许用户在任何帧上使用边界框、分割掩码或直接在三维中标注区域，而无需繁琐的多帧标记。我们通过用三维位置嵌入增强二维视觉特征来实现这一点，这使得三维模型能够利用强大的二维先验知识，在帧之间进行更准确的空间推理，即使感兴趣的物体在同一视图中未同时出现。对一般二维视觉语言和专业三维空间基准的广泛实验表明，SR-3D 实现了最先进的性能，强调了它在场景理解中统一二维和三维表示空间的有效性。此外，我们观察到其在没有感应三维输入或真实三维标注的野外视频中的适用性，SR-3D 能够准确推断空间关系和度量测量。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决<strong>将二维视觉-语言模型（2D VLM）扩展到三维空间理解</strong>时所面临的核心难题，具体包括：</p>
<ol>
<li><p><strong>2D 与 3D 表征空间割裂</strong><br>现有 2D VLM 缺乏对三维几何结构的显式建模，而 3D VLM 通常采用点云或体素等专用表征，难以直接复用 2D 预训练权重，导致数据效率低、性能受限。</p>
</li>
<li><p><strong>多视角场景下的区域提示（region prompting）困难</strong><br>在多视角视频中，同一物体可能部分可见或跨帧分布，传统方法需要逐帧标注 3D 框或 mask，成本高昂；纯文本描述又难以精确定位实例。</p>
</li>
<li><p><strong>三维空间推理数据稀缺</strong><br>大规模 3D 标注数据远少于 2D，直接训练 3D-VLM 容易过拟合并牺牲通用视觉-语言能力。</p>
</li>
</ol>
<p>为此，作者提出 <strong>SR-3D</strong>——一套<strong>统一单视角与多视角输入的 3D 感知视觉-语言模型</strong>，通过以下关键设计一次性解决上述问题：</p>
<ul>
<li><strong>共享的归一化 3D 位置嵌入</strong>：把单目深度估计或多视角点云映射到规范坐标系，直接注入 2D 视觉 token，实现 2D 先验与 3D 几何的端到端融合。</li>
<li><strong>动态 tile-and-stitch 区域提取器</strong>：在高分辨率特征图上进行 mask 池化，支持仅用<strong>单帧框/掩膜</strong>即可在多视角视频中完成区域级推理，无需密集标注。</li>
<li><strong>统一训练范式</strong>：先在大规模 2D 图文数据上预训练，再用 3D QA 数据微调，保证通用 VQA 性能不降的同时显著提升空间理解指标。</li>
</ul>
<p>实验表明，SR-3D 在 2D 区域任务、3D 密集描述、视频空间问答等多项基准上均达到 SOTA，且可零样本泛化到无 3D 真值的野生视频，验证了其**“用 2D 数据学会 3D 推理”** 的目标。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 SR-3D 直接相关的研究可归纳为三条主线，每条线下列出最具代表性的工作并指出与本文的差异。</p>
<p>主线</p>
<p>代表文献</p>
<p>与 SR-3D 的核心区别</p>
<p><strong>1. 区域级视觉-语言模型</strong></p>
<p>RegionGPT、Osprey、SpatialRGPT、DynRefer、GLAMM、SoM</p>
<p>仅面向单张图像或视频跟踪，未引入 3D 位置嵌入；跨视角空间推理需逐帧标注或语言描述。SR-3D 首次把“单帧框/掩膜”泛化到多视角 3D 场景。</p>
<p><strong>2. 2D-VLM 空间推理增强</strong></p>
<p>SpatialVLM、SpatialBot、SpatialPIN、RoboPoint、3D-SRBench、Sparkle</p>
<p>依赖 2D 图像或深度图，不做多视角几何一致性建模；无法回答“跨帧度量距离/相对方向”。SR-3D 用规范 3D 坐标统一多帧，实现度量级空间问答。</p>
<p><strong>3. 3D 大模型</strong></p>
<p>3D-LLM、LEO、ChatScene、LL3DA、Scene-LLM、PointLLM</p>
<p>采用点云/体素编码器，与 2D 视觉 backbone 分离，需大量 3D 数据重训；区域提示需 3D 框或完整分割。SR-3D 复用 2D 权重，仅追加轻量级 3D PE，支持单帧提示。</p>
<p><strong>4. 多视角-3D VLM</strong></p>
<p>LLaVA-3D、Video-3D-LLM</p>
<p>同样用多视角图像，但 LLaVA-3D 用双路径分别处理 2D/3D，Video-3D-LLM 仅在视频 VLM 上微调 3D 任务，二者均缺统一 2D-3D 表征与区域提示机制。SR-3D 单一路径、统一嵌入、支持灵活区域。</p>
<p>此外，SR-3D 在深度/点云估计方面复用了 DepthAnything-V2、DUST3R、MAST3R、CUT3R 等现成的几何基础模型，但仅作为离线输入，不改动其权重。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 SR-3D，通过“统一表征 + 统一架构 + 统一训练”三管齐下，把 2D 视觉-语言模型的强大先验无缝迁移到 3D 空间，并支持极轻量的区域提示。具体技术路线如下：</p>
<p>1. 统一表征：Canonical 3D Positional Embedding</p>
<ul>
<li><p><strong>单张图像</strong><br>用 DepthAnything-V2 估计相对深度 → 反投影到相机坐标系 → 归一化到以场景中心为原点的单位球坐标系，得到与相机位姿无关的 3D 位置图。</p>
</li>
<li><p><strong>多视角视频</strong><br>若给定相机位姿，直接用 GT 深度反投影；若无，则用 MAST3R/CUT3R 估计点云再配准。所有帧的点云变换到同一规范坐标系，保证跨帧几何一致。</p>
</li>
<li><p><strong>注入方式</strong><br>对 3D 位置图做 sinusoidal 编码 → 可学习的 MLP → 得到与视觉 token 同维的 3D PE，逐 token 相加。<br>结果：2D 特征立即具备度量几何感知，为后续空间推理提供“坐标系”。</p>
</li>
</ul>
<p>2. 统一架构：Dynamic Tiling-based Region Extractor</p>
<ul>
<li><p><strong>背景</strong><br>基础 VLM（NVILA-Lite）采用动态 tiling：按最优长宽比把高分辨率图像切成 448×448 小块，分别编码后再拼回，兼顾细节与显存。</p>
</li>
<li><p><strong>区域提取</strong><br>对单帧掩膜/框同样做 tiling → 每块视觉特征与对应掩膜块逐像素乘 → 拼回原始分辨率 → 全局 mask-pooling 得到区域 token。<br>优势：</p>
<ol>
<li>特征来自高分辨率，无需事后上采样；</li>
<li>多视角场景下，每帧视为一个 tile，同一 3D 掩膜跨帧投影后天然对齐，可直接池化得到跨帧一致的区域表示。</li>
</ol>
</li>
<li><strong>提示机制</strong><br>用户只需在<strong>任意单帧</strong>画框或掩膜，模型利用规范 3D 坐标找到其他帧的对应像素，无需额外标注。</li>
</ul>
<p>3. 统一训练：两阶段课程</p>
<p>阶段</p>
<p>数据</p>
<p>可训练模块</p>
<p>目标</p>
<p><strong>阶段 1：2D 预训练</strong></p>
<p>7M 图文对（含区域问答）</p>
<p>3D-PE MLP + Projector + LLM</p>
<p>在单幅图像上学会“看到深度”与区域定位，同时保持通用 VQA 能力。</p>
<p><strong>阶段 2：3D 微调</strong></p>
<p>ScanQA/SQA3D/Scan2Cap/EmbodiedScan 等 3D QA 数据</p>
<p>全模型（视觉编码器仍冻结）</p>
<p>把 3D 先验扩展到多视角，支持跨帧度量距离、相对方向、区域比较等任务。</p>
<ul>
<li><strong>数据增强</strong><br>随机把分割掩膜换成框、随机丢帧，模拟“只有一帧有标注”的极端情况，强化模型利用 3D PE 自行对齐。</li>
</ul>
<p>4. 推理：即插即用</p>
<ul>
<li>输入：单帧框/掩膜、或 3D 框、或稀疏帧掩膜 → 自动投影到规范空间 → 统一输出空间关系或度量值。</li>
<li>无需 GT 深度：测试时可用 CUT3R 等实时生成点云，性能几乎不下降（表 9）。</li>
</ul>
<p>结果</p>
<ul>
<li>在 2D 区域任务 COCO、BLINKDepth 上，单模型即超越 SpatialRGPT 等 3D 注入方案。</li>
<li>在 3D 密集描述 Scan2Cap、ScanQA、SQA3D 上，CIDEr 提升 14+，刷新 SOTA。</li>
<li>零样本直接跑野生视频，可准确回答“冰箱离我几米”“沙发在我哪一侧”等度量级问题（图 1、图 5）。</li>
</ul>
<p>通过“把 3D 坐标变成 token 的一部分”，SR-3D 在不牺牲 2D 通用能力的前提下，实现了真正的 3D 感知与灵活区域提示。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从 <strong>2D 通用能力、2D 区域理解、3D 密集描述、3D 视频空间推理、消融与零样本泛化、真实场景应用</strong> 六个维度展开系统实验，覆盖 13 个公开基准 + 1 个自建基准，总计 30 余项指标。结果均以 <strong>SR-3D-8B</strong> 模型报告，并与同规模或同领域 SOTA 对比。</p>
<p>1. 2D 通用能力验证（不牺牲 VQA 性能）</p>
<p>基准</p>
<p>类型</p>
<p>NVILA-Lite-8B</p>
<p>SR-3D-8B</p>
<p>涨跌</p>
<p>BLINK𝑆</p>
<p>空间</p>
<p>79.7</p>
<p>83.9</p>
<p>+4.2</p>
<p>SAT</p>
<p>空间</p>
<p>62.6</p>
<p>64.0</p>
<p>+1.4</p>
<p>RealWorldQA</p>
<p>空间</p>
<p>68.9</p>
<p>72.5</p>
<p>+3.6</p>
<p>MathVista</p>
<p>数学</p>
<p>65.6</p>
<p>68.1</p>
<p>+2.5</p>
<p>GQA / AI2D / MMMU𝑝 等 9 项</p>
<p>通用 / OCR</p>
<p>持平或 ±1 以内</p>
<p>—</p>
<p><strong>结论</strong>：引入 3D-PE 后，通用、数学、OCR 能力不降，反而在需要空间常识的任务上普遍提升。</p>
<p>2. 2D 区域级理解</p>
<p>任务</p>
<p>指标</p>
<p>以往最佳</p>
<p>SR-3D</p>
<p>提升</p>
<p>COCO2017 val 区域分类</p>
<p>mAP / Acc</p>
<p>72.9 / 82.9 (SpatialRGPT)</p>
<p><strong>78.0 / 88.6</strong></p>
<p>+5.1 / +5.7</p>
<p>BLINKDepth（点深度问答）</p>
<p>Acc</p>
<p>87.9 (SpatialRGPT)</p>
<p><strong>90.3</strong></p>
<p>+2.4</p>
<p><strong>结论</strong>：动态 tiling 区域提取器带来更高保真度，3D-PE 帮助精确定位深度。</p>
<p>3. 3D 密集描述与问答</p>
<p>基准</p>
<p>指标</p>
<p>前 SOTA</p>
<p>SR-3D</p>
<p>提升</p>
<p>Scan2Cap</p>
<p>CIDEr</p>
<p>83.8</p>
<p><strong>97.9</strong></p>
<p>+14.1</p>
<p>ScanQA</p>
<p>CIDEr / EM</p>
<p>102.1 / 30.1</p>
<p><strong>109.3 / 30.4</strong></p>
<p>+7.2 / +0.3</p>
<p>SQA3D</p>
<p>EM</p>
<p>58.6</p>
<p><strong>62.2</strong></p>
<p>+3.6</p>
<p><strong>结论</strong>：在统一 3D-PE 加持下，所有自动评价指标均刷新纪录；BLEU-4、METEOR、ROUGE 同步提升（见附录表 10-12）。</p>
<p>4. 视频空间智能（自建 SR-3D-Bench）</p>
<p>设置</p>
<p>子任务</p>
<p>最佳基线</p>
<p>SR-3D</p>
<p>提升</p>
<p>区域级空间 QA（2500 题）</p>
<p>定量 Avg</p>
<p>46.7 (GPT-4o+SoM)</p>
<p><strong>83.3</strong></p>
<p>+36.6</p>
<p>定性 Avg</p>
<p>53.7</p>
<p><strong>79.5</strong></p>
<p>+25.8</p>
<p>宽度 / 高度 / 距离</p>
<p>单项 Acc</p>
<p>40–59</p>
<p><strong>74–87</strong></p>
<p>+20+</p>
<p><strong>结论</strong>：SR-3D 是唯一在“单帧画框→跨帧度量”设定下仍能保持 80%+ 准确率的模型。</p>
<p>5. 全局视频空间理解（VSI-Bench）</p>
<p>子任务</p>
<p>最佳开源</p>
<p>最佳 API</p>
<p>SR-3D</p>
<p>相对方向</p>
<p>30–43</p>
<p>46–51</p>
<p><strong>82.3</strong></p>
<p>绝对距离 / 房间面积</p>
<p>40–55</p>
<p>43–64</p>
<p><strong>75.5 / 41.9</strong></p>
<p><strong>结论</strong>：在 egocentric 视频里，SR-3D 的相对方向准确率远超 GPT-4o、Gemini-1.5-Pro。</p>
<p>6. 消融与零样本分析</p>
<p>设置</p>
<p>3D-PE</p>
<p>2D 预训练</p>
<p>Scan2Cap CIDEr</p>
<p>SR-3D-Bench 平均</p>
<p>A</p>
<p>✗</p>
<p>✗</p>
<p>92.9</p>
<p>74.4</p>
<p>B</p>
<p>✔</p>
<p>✗</p>
<p>92.7</p>
<p>77.2</p>
<p>C</p>
<p>✗</p>
<p>✔</p>
<p>101.3</p>
<p>74.2</p>
<p>D</p>
<p>✔</p>
<p>✔</p>
<p><strong>97.9</strong></p>
<p><strong>83.3</strong></p>
<ul>
<li><strong>零样本 3D</strong>：仅用 2D 数据训练的 SR-3D-2D 在 Tall/Short、Big/Small、Height、Distance 四类的平均准确达 <strong>74.5%</strong>，证明 3D-PE 即可让模型“看到深度”。</li>
<li><strong>真实视频</strong>：CUT3R 重建点云替代 GT，ScanQA CIDEr 仍保持 109.3→109.0，几乎无掉点，验证对廉价几何估计的鲁棒性。</li>
</ul>
<p>7. 定性展示</p>
<ul>
<li>野生 YouTube 视频（无 3D 真值）上，SR-3D 可正确回答<br>“若面向 Region-2，镜子在我哪侧？” → <strong>Behind</strong><br>“Region-1 与 Region-3 距离？” → <strong>16.12 英尺</strong><br>见图 1、图 5、图 6。</li>
</ul>
<p>实验总结</p>
<ol>
<li>2D 能力 <strong>不降级</strong> → 3D-PE 是“无痛”插件。</li>
<li>区域任务 <strong>全面领先</strong> → 高分辨率 tile-and-stitch 是关键。</li>
<li>3D/视频空间理解 <strong>刷新 SOTA</strong> → 统一规范坐标系带来跨帧度量能力。</li>
<li>零样本 + 野生视频 <strong>可用</strong> → 无需昂贵 3D 标注，单帧画框即可推理。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SR-3D 的直接延伸，按“数据-模型-应用”三层递进，并给出可验证的实验切入点。</p>
<p>1. 数据与标注</p>
<p>方向</p>
<p>可探索点</p>
<p>验证指标</p>
<p>动态场景</p>
<p>将静态多视角拓展到<strong>运动相机+运动物体</strong>（手持视频、机器人 SLAM）。</p>
<p>在 VSI-Bench 新增“动态子集”，比较静态 vs 流式 3D-PE 的相对方向准确率。</p>
<p>弱监督缩放</p>
<p>仅用<strong>激光雷达-图像自动关联</strong>生成 1M 级伪 3D-QA，无需人工标注。</p>
<p>在 ScanQA 上观察 CIDEr 随伪标签比例的变化曲线，检验饱和点。</p>
<p>方向-朝向标注</p>
<p>收集带“物体坐标系”标注的 3D 数据（EmbodiedScan++），解决朝向难题。</p>
<p>新增“Is the keyboard facing left?”子任务，看 EM 能否从 50%→80%。</p>
<p>2. 模型与表征</p>
<p>方向</p>
<p>可探索点</p>
<p>验证指标</p>
<p>时空统一 PE</p>
<p>把 3D 坐标扩展为<strong>4D 时空坐标</strong>（x,y,z,t），用正余弦+可学习编码。</p>
<p>在动态子集上比较“3D-PE vs 4D-PE”的相对方向准确率差距。</p>
<p>可学习相机外参</p>
<p>不再依赖 GT 位姿，让模型<strong>自监督预测相机相对位姿</strong>（类似 DUST3R）。</p>
<p>在 ScanNet 上报告相机相对旋转/平移误差，&lt;3° &amp; &lt;5 cm 即合格。</p>
<p>统一单-多视角 checkpoint</p>
<p>用<strong>混合批次训练</strong>（单视角+多视角同 batch），实现一个权重同时服务两种输入。</p>
<p>观察单视角 VQA 与多视角 ScanQA 是否同时上涨，排除跷跷板现象。</p>
<p>生成式 3D 输出</p>
<p>把 LLM 解码器换成<strong>扩散或 SDF 解码头</strong>，直接生成点云/网格。</p>
<p>在 Text-to-3D 基准（如 Cap3D）上报告 Chamfer Distance，目标 &lt;5 mm。</p>
<p>3. 应用与系统</p>
<p>方向</p>
<p>可探索点</p>
<p>验证指标</p>
<p>机器人导航</p>
<p>把 SR-3D 作为<strong>语义-度量混合策略</strong>的 VLM 后端，输出“向前 2.3 m 右转”原子动作。</p>
<p>在 Habitat-3D/AI2-THOR 上测量 Success Rate 与 SPL，目标 &gt;70%。</p>
<p>AR 实时测量</p>
<p>集成到<strong>手机 ARKit</strong>，用户单指圈选即可得真实尺寸（厘米级）。</p>
<p>与苹果 Measure App 对比，平均相对误差 &lt;5%。</p>
<p>大规模视频检索</p>
<p>用<strong>语言+单帧框查询</strong>百万小时监控，定位“距红色推车 5 m 内出现包裹”片段。</p>
<p>在 BDD-Video-3D 自建 100k 查询集，召回@10 从 60%→85%。</p>
<p>多模态编辑</p>
<p>支持“把 Region-3 的椅子放大 1.5 倍并旋转 30°”的<strong>文本+区域驱动 3D 编辑</strong>。</p>
<p>用户主观评分（1-5）&gt;4.0，且编辑后尺寸误差 &lt;10%。</p>
<p>4. 理论与基准</p>
<p>方向</p>
<p>可探索点</p>
<p>验证指标</p>
<p>可解释空间注意力</p>
<p>可视化跨帧 attention map，检验模型是否<strong>真正激活对应 3D 区域</strong>而非纹理捷径。</p>
<p>推出 SR-3D-AttnBench，给出 3D IoU&gt;0.7 的 attention 掩膜为通过。</p>
<p>鲁棒性基准</p>
<p>引入** adversarial 相机位姿扰动**（±10°/±20 cm），观察性能下降斜率。</p>
<p>在 ScanQA 上 EM 下降 &lt;5% 视为鲁棒。</p>
<p>高效化</p>
<p>用<strong>2-bit 量化 + KV-cache 稀疏化</strong>把 8B 模型压缩到 2B，满足边缘 GPU 30 fps。</p>
<p>在 RTX-4060 上测 32 帧视频端到端延迟 &lt;300 ms，CIDEr 掉点 &lt;2。</p>
<p>短期可落地（6 个月内）</p>
<ol>
<li>4D-PE + 动态子集实验</li>
<li>统一 checkpoint 混合批次训练</li>
<li>AR 厘米级测量 Demo</li>
</ol>
<p>中长期（1-2 年）</p>
<ol>
<li>生成式 3D 输出头</li>
<li>机器人闭环导航系统集成</li>
<li>百万级弱监督 3D-QA 数据与预训练</li>
</ol>
<p>这些方向既可直接继承 SR-3D 的“规范 3D-PE + tile-and-stitch”框架，又能分别回应朝向、动态、生成、实时等当前明显短板，形成完整的 3D 感知-生成-行动闭环。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="SR-3D-论文概要"><a href="#SR-3D-论文概要" class="headerlink" title="SR-3D 论文概要"></a>SR-3D 论文概要</h1><h2 id="1-研究动机"><a href="#1-研究动机" class="headerlink" title="1. 研究动机"></a>1. 研究动机</h2><ul>
<li>2D Vision-Language Model（VLM）缺乏度量级 3D 几何理解</li>
<li>3D VLM 依赖点云/体素，与 2D 预训练权重割裂，数据效率低</li>
<li>多视角场景下逐帧标注 3D 框/掩膜代价高，纯语言描述易歧义</li>
</ul>
<h2 id="2-核心思路"><a href="#2-核心思路" class="headerlink" title="2. 核心思路"></a>2. 核心思路</h2><p><strong>统一 2D 与 3D 表征空间</strong>：<br>单目/多视角图像 → 深度/点云 → 归一化 3D 位置图 → 正余弦+MLP 得到 3D Positional Embedding（3D-PE），与视觉 token 逐元素相加，直接注入冻结的 2D 视觉编码器。</p>
<h2 id="3-关键模块"><a href="#3-关键模块" class="headerlink" title="3. 关键模块"></a>3. 关键模块</h2><ul>
<li><strong>Canonical 3D-PE</strong>：相机坐标系→场景中心规范坐标，单视角与多视角共享同一空间</li>
<li><strong>Dynamic Tiling Region Extractor</strong>：高分辨率 tile-and-stitch，支持<strong>单帧框/掩膜</strong>即跨帧推理</li>
<li><strong>统一训练管线</strong>：<br>① 2D 图文+区域数据预训练（7 M）→ ② 3D QA 数据微调（ScanQA/SQA3D/Scan2Cap/EmbodiedScan）</li>
</ul>
<h2 id="4-能力展示"><a href="#4-能力展示" class="headerlink" title="4. 能力展示"></a>4. 能力展示</h2><p>任务</p>
<p>以往最佳</p>
<p>SR-3D-8B</p>
<p>提升</p>
<p>COCO 区域分类 Acc</p>
<p>82.9</p>
<p><strong>88.6</strong></p>
<p>+5.7</p>
<p>BLINKDepth 深度问答</p>
<p>87.9</p>
<p><strong>90.3</strong></p>
<p>+2.4</p>
<p>Scan2Cap CIDEr</p>
<p>83.8</p>
<p><strong>97.9</strong></p>
<p>+14.1</p>
<p>ScanQA CIDEr / EM</p>
<p>102.1 / 30.1</p>
<p><strong>109.3 / 30.4</strong></p>
<p>+7.2 / +0.3</p>
<p>自建 SR-3D-Bench 区域空间 QA</p>
<p>46.7</p>
<p><strong>83.3</strong></p>
<p>+36.6</p>
<p>VSI-Bench 相对方向</p>
<p>43</p>
<p><strong>82.3</strong></p>
<p>+39</p>
<ul>
<li><strong>零样本 3D 推理</strong>：仅用 2D 数据训练的模型在多视角距离/高度任务上达 74% 准确率</li>
<li><strong>野生视频</strong>：无 GT 3D 标注，仍可厘米级测距、判断左右前后</li>
</ul>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>SR-3D 通过“3D-PE + 统一 tile-and-stitch 区域提取 + 两阶段训练”，首次实现</p>
<ul>
<li>单帧提示即可跨视角度量推理</li>
<li>2D 通用 VQA 性能不降</li>
<li>3D 密集描述与视频空间理解全面 SOTA</li>
</ul>
<p>为 2D↔3D 统一的视觉-语言模型提供了简单可扩展的范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是3D%20Aware%20Region%20Prompted%20Vision%20Language%20Model，链接是  https://arxiv.org/pdf/2509.13317  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13317  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: An-Chieh Cheng,Yang Fu,Yukang Chen,Zhijian Liu,Xiaolong Li,Subhashree Radhakrishnan,Song Han,Yao Lu,Jan Kautz,Pavlo Molchanov,Hongxu Yin,Xiaolong Wang,Sifei Liu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13317.pdf">https://arxiv.org/pdf/2509.13317.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13317">https://arxiv.org/abs/2509.13317</a></p>
<p><strong>Arxiv ID</strong>: 2509.13317</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13317">https://papers.cool/arxiv/2509.13317</a></p>
<p><strong>Published</strong>: 2025-09-16T17:59:06Z</p>
<p><strong>Updated</strong>: 2025-09-16T17:59:06.000Z</p>
<hr>
<h3 id="10-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving"><a href="#10-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving" class="headerlink" title="10. EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12603">EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving</a></h3><p>Large Language Models (LLMs) have recently advanced the field of Automated Theorem Proving (ATP), attaining substantial performance gains through widely adopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT) reasoning and increased sampling passes. However, they both introduce significant computational overhead for inference. Moreover, existing cost analyses typically regulate only the number of sampling passes, while neglecting the substantial disparities in sampling costs introduced by different scaling strategies. In this paper, we systematically compare the efficiency of different test-time scaling strategies for ATP models and demonstrate the inefficiency of the current state-of-the-art (SOTA) open-source approaches. We then investigate approaches to significantly reduce token usage and sample passes while maintaining the original performance. Specifically, we propose two complementary methods that can be integrated into a unified EconRL pipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching mechanism designed to mitigate unnecessary token consumption, and (2) Diverse parallel-scaled reinforcement learning (RL) with trainable prefixes to enhance pass rates under constrained sampling passes. Experiments on miniF2F and ProofNet demonstrate that our EconProver achieves comparable performance to baseline methods with only 12% of the computational cost. This work provides actionable insights for deploying lightweight ATP models without sacrificing performance.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）最近在自动定理证明（ATP）领域取得了显著进展，通过广泛采用的测试时间缩放策略实现了实质性的性能提升，特别是反射性思维链（CoT）推理和增加采样次数。然而，这两者都给推理带来了显著的计算开销。此外，现有的成本分析通常仅限制采样次数，而忽视了不同缩放策略引入的采样成本的显著差异。本文系统比较了不同测试时间缩放策略在ATP模型中的效率，并展示了当前最先进（SOTA）开源方法的低效。然后，我们研究了在保持原始性能的情况下显著减少令牌使用和采样次数的方法。具体而言，我们提出了两种互补的方法，可集成到统一的经济强化学习（EconRL）流程中，以获得更大的收益：（1）一种动态思维链（CoT）切换机制，旨在减少不必要的令牌消耗，和（2）具有可训练前缀的多样化并行缩放强化学习（RL），以提高在受限采样次数下的通过率。在miniF2F和ProofNet上的实验表明，我们的经济证明器（EconProver）在计算成本仅为基线方法的12%的情况下，达到了相当的性能。此项工作为在不牺牲性能的情况下部署轻量级ATP模型提供了可行的见解。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决当前基于大模型的自动定理证明（ATP）系统在“测试时扩展”（test-time scaling）阶段存在的<strong>高昂推理成本与性能提升失衡</strong>的问题。具体可归纳为以下三点：</p>
<ol>
<li><p><strong>统一度量缺失</strong><br>现有工作通常只统计采样次数，忽略了不同扩展策略（长链式思维、并行采样、迭代精修）在token级开销上的巨大差异，导致无法公平评估真实的计算-性能权衡。</p>
</li>
<li><p><strong>策略低效</strong></p>
<ul>
<li><strong>顺序扩展</strong>（长CoT）对所有问题无差别启用反思推理，使token用量暴增10–25×，而83 %的非IMO难度题目其实无需CoT即可正确证明。</li>
<li><strong>并行扩展</strong>盲目增加采样次数后迅速出现边际收益递减：在miniF2F上，64→128次采样仅提升1.1 %准确率，却付出4×成本。</li>
</ul>
</li>
<li><strong>缺乏轻量级优化框架</strong><br>尚无系统方法能在<strong>不牺牲准确率</strong>的前提下，同时削减token消耗与采样次数；现有高效方案（如直接提示、随机多head）效果有限。</li>
</ol>
<p>为此，作者提出<strong>EconRL</strong>框架，通过</p>
<ul>
<li><strong>动态CoT切换</strong>（难则思、易则直）</li>
<li><strong>难度感知的多样并行RL</strong>（用8个可学习prefix head在限定采样预算内最大化探索多样性）</li>
</ul>
<p>在miniF2F与ProofNet上实现与SOTA相当的准确率，仅耗**12 %**的原始token成本，从而首次在ATP领域系统性地缓解了“性能-成本”矛盾。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 §5 中系统梳理了与“经济型测试时扩展”相关的三条研究脉络，并指出本文是<strong>首次将动态 CoT 机制引入形式定理证明场景</strong>。相关研究可归纳为下表：</p>
<p>研究方向</p>
<p>代表工作</p>
<p>与本文的关联与差异</p>
<p><strong>端到端整证明生成</strong><br>（whole-proof generation）</p>
<p>• DeepSeek-Prover 系列<br>• Goedel-Prover<br>• Kimina-Prover</p>
<p>同样采用一次生成完整 Lean 证明再验证，但它们默认“长 CoT 对所有问题开启”，导致 token 开销高。本文首次让模型<strong>自主决定</strong>是否启用长 CoT，从而直接削减证明长度。</p>
<p><strong>带验证器反馈的定理证明</strong><br>（verifier-feedback）</p>
<p>逐步搜索：<br>• InternLM2.5-StepProver<br>• HunyuanProver<br>• BFS-Prover<br>• DSP+ / DRP  </p>
<p>整证明迭代精修：<br>• Seed-Prover<br>• Goedel-Prover-V2<br>• StepFun-Prover</p>
<p>这类方法通过多步搜索或多轮精修换取更高成功率，但计算量巨大（Seed-Prover 需数十轮迭代）。本文的 EconRL <strong>可与迭代精修正交叠加</strong>：表 1 显示，在同样达到 86.0 % miniF2F 准确率下，token 成本从 40× 降到 10×。</p>
<p><strong>动态 / 自适应 Chain-of-Thought</strong><br>（dynamic CoT）</p>
<p>非形式领域：<br>• AdaCoT（RL 触发）<br>• L1（长度约束 RL）<br>• DynaThink（置信度切换）<br>• DAST（难度预测）</p>
<p>它们聚焦于自然语言问答或数学应用题，通过 RL 或置信度阈值控制是否展开长推理。本文<strong>首次把动态 CoT 思想迁移到形式定理证明</strong>，并针对 Lean 证明的特点提出“基于可解性标签的偏好对”训练策略，使模型在无外部难度信号下自动切换模式。</p>
<p>综上，本文在“整证明生成”与“迭代精修”两大主流范式之间，<strong>引入动态 CoT 与多样并行 RL 的组合拳</strong>，填补了形式化领域缺乏“经济型测试时扩展”研究的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“高 token 开销 + 冗余采样”这一核心矛盾拆解为<strong>两类可量化浪费</strong>，并对应提出两条互补的 RL 训练阶段，统一在 <strong>EconRL</strong> 流水线中解决。具体做法如下：</p>
<p>1. 浪费来源与量化指标</p>
<p>浪费类型</p>
<p>现象</p>
<p>统一度量</p>
<p><strong>顺序扩展浪费</strong></p>
<p>长 CoT 对所有定理无差别开启，token 用量暴增 10–25×</p>
<p>单次采样 token 数</p>
<p><strong>并行扩展浪费</strong></p>
<p>>32 路后前缀多样性饱和，继续采样几乎重复</p>
<p>Prefix Diversity Coverage（PDC）</p>
<p>2. 解决方案总览：EconRL 两阶段 RL</p>
<pre><code>┌---------------------------┐
│  基础模型（DeepSeek-Prover-V2 或 Goedel-Prover-V2）
└------------┬--------------┘
             │
    Stage-1  Dynamic CoT Switching（偏好学习）
             │  ↓ 目标：砍掉“易题”不必要 token
             ├─ 自动标注 ─► 非 CoT 可解 vs CoT 依赖
             ├─ DPO 训练 ─► 学会“难则思、易则直”
             │
    Stage-2  Diverse Parallel-scaled RL（PPO）
             │  ↓ 目标：在固定采样预算内最大化多样性
             ├─ 难度分桶 ─► 8 个可学习 prefix head
             ├─ 独立 PPO ─► 每 head 专攻一类难度
             └─ 均匀调度 ─► 推理时均分采样次数
</code></pre><p>3. 关键技术细节</p>
<p>3.1 Dynamic CoT Switching（阶段一）</p>
<ul>
<li><p><strong>自动标注</strong><br>在训练集上先用非 CoT 模式跑 32 次，能成功即标为“非 CoT 可解”，否则“CoT 依赖”。<br>结果：83 % 低于 IMO 难度定理可被非 CoT 解决。</p>
</li>
<li><p><strong>偏好对构造</strong><br>非 CoT 可解：yw = 直接证明，yl = 长 CoT（惩罚冗余推理）<br>CoT 依赖：yw = 长 CoT，yl = 直接证明（鼓励深度思考）</p>
</li>
<li><p><strong>DPO 目标</strong>  </p>
<script type="math/tex; mode=display">\\mathcal{L}_{\\text{DPO}} = -\\mathbb{E}_{(x,y\_w,y\_l)\\sim\\mathcal{D}} \\log\\sigma!\\left(\\beta\\log\\frac{\\pi\_\\theta(y\_w|x)}{\\pi\_{\\text{ref}}(y\_w|x)} - \\beta\\log\\frac{\\pi\_\\theta(y\_l|x)}{\\pi\_{\\text{ref}}(y\_l|x)}\\right)</script><p>β = 0.1，训练后模型在推理时<strong>无需外部难度信号</strong>即可自动切换。</p>
</li>
</ul>
<p>3.2 Diverse Parallel-scaled RL（阶段二）</p>
<ul>
<li><p><strong>难度分桶</strong><br>用 base 模型在 32 次采样下的成功次数 c(x) 排序，均匀切成 8 档；每档对应一个专用 head。</p>
</li>
<li><p><strong>可学习 prefix</strong><br>每个 head 仅为一组 20 个可训练 token 嵌入，参数量 &lt;0.1 %，独立用 PPO 优化：<br>reward = 1（Lean checker 通过）否则 0。</p>
</li>
<li><p><strong>推理调度</strong><br>Pass@16 → 每 head 跑 16/8 = 2 次，零额外开销即可保持多样性。</p>
</li>
</ul>
<p>4. 效果量化</p>
<p>设置</p>
<p>miniF2F 准确率</p>
<p>相对 token 成本</p>
<p>节省</p>
<p>基线 CoT（32 次）</p>
<p>75.8 %</p>
<p>10×</p>
<p>—</p>
<p>ECONPROVER-DS（16 次）</p>
<p>76.2 %</p>
<p>1.5×</p>
<p><strong>85 % ↓</strong></p>
<p>ECONPROVER-GD + 迭代精修</p>
<p>86.0 %</p>
<p>10×</p>
<p><strong>75 % ↓</strong></p>
<p>5. 结论</p>
<p>通过**“先剪枝、再增效”<strong>的两段式 RL，论文把原本“一刀切”的长 CoT 与盲目堆采样，改造成</strong>难度自适应 + 多样性强化**的经济型扩展范式，在保持 SOTA 准确率的同时把 token 开销压到 <strong>12 %</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“成本-性能”两条主线设计实验，覆盖 <strong>成本曲线分析 → 组件消融 → 主结果 → 叠加迭代精修</strong> 四个层次。所有实验统一用 <strong>token 级总采样成本</strong> 作为横轴，miniF2F / ProofNet 准确率作为纵轴，结果如下表汇总：</p>
<p>实验类别</p>
<p>关键变量</p>
<p>基准/数据集</p>
<p>主要结论（一句话）</p>
<p><strong>1. 成本曲线诊断</strong><br>§2.2</p>
<p>顺序 vs 并行 vs 混合</p>
<p>miniF2F-test</p>
<p>并行 8 次已优于长 CoT，且继续堆采样边际收益迅速衰减；混合策略在 <strong>4 %</strong> 成本处即可达到纯堆 8192 次的增益。</p>
<p><strong>2. 组件消融</strong></p>
<p>2.1 动态 CoT 切换<br>§4.3</p>
<p>CoT 触发方式</p>
<p>DeepSeek-Prover-V2</p>
<p>自动切换在 <strong>99.7 %</strong> 准确率下只用 <strong>15 %</strong> token，优于直接提示（71.7 %→75.4 %）。</p>
<p>2.2 多样并行 RL<br>§4.4</p>
<p>head 数量 &amp; 分桶策略</p>
<p>同上</p>
<p>难度分桶 8 head 在 Pass@16 带来 <strong>4.9 %</strong> 绝对提升，随机 head 仅 <strong>1.3 %</strong>；低采样区收益更高。</p>
<p><strong>3. 主结果</strong><br>§4.2</p>
<p>完整 EconRL</p>
<p>miniF2F + ProofNet</p>
<p>ECONPROVER-DS 与全 CoT 打平（76.2 %）仅耗 <strong>1.5×</strong> token；ECONPROVER-GD 与 Goedel-V2-CoT 打平（84.0 %）仅耗 <strong>3×</strong> token。</p>
<p><strong>4. 与迭代精修叠加</strong></p>
<p>迭代轮数 &amp; 成本</p>
<p>miniF2F-test</p>
<p>在同样 86.0 % 准确率下，EconRL 把迭代精修的 <strong>40×</strong> 成本压缩到 <strong>10×</strong>，节省 <strong>75 %</strong>。</p>
<p>此外，所有实验均报告 <strong>Prefix Diversity Coverage（PDC）</strong> 与准确率的相关性，验证“多样性↑ → 性能↑”假设成立。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“理论-算法-系统-应用”四个层面，并给出可立即落地的实验切入点。</p>
<p>1. 理论层面：成本-性能帕累托前沿</p>
<ul>
<li><strong>建立 token-最优下界</strong><br>对给定定理分布，用信息论或计算复杂性工具证明“达到准确率 θ 所需期望 token 数”的下界，评估 EconRL 距离理论最优的差距。</li>
<li><strong>难度度量的可学习性</strong><br>当前用 32 次 base 模型成功率作为真值，可探索<strong>免真值</strong>的难度估计（如梯度方差、注意力熵），并证明其与真实可解概率的单调一致性。</li>
</ul>
<p>2. 算法层面：三处“贪心”可升级</p>
<p>模块</p>
<p>当前贪心策略</p>
<p>可探索的高级方法</p>
<p><strong>动态 CoT 阈值</strong></p>
<p>单点 0/1 决策</p>
<p>引入<strong>置信度-预算</strong>联合 MDP，用 constrained RL 学习最优 early-exit 阈值序列，实现任意 token 预算下的最优准确率。</p>
<p><strong>head 调度</strong></p>
<p>均匀均分</p>
<p>在线 bandit / UCB 调度：实时根据已观测的 proof prefix 相似度调整下一采样投向哪一 head，<strong>零额外训练</strong>即可进一步减采样。</p>
<p><strong>head 数量</strong></p>
<p>固定 8</p>
<p>把 head 数作为超参，用<strong>连续松弛</strong>（Neural Architecture Search）或<strong>稀疏激活</strong>（MoE）自动搜索，每定理仅激活 1-2 个 head，推理延迟 ↓。</p>
<p>3. 系统层面：训练-推理协同优化</p>
<ul>
<li><strong>Prefix 级增量推理</strong><br>8 组 prefix 可一次性加载到 GPU 常量内存，复用一次前向计算的后缀网络，把 8× 前向降回 1×，<strong>吞吐 ↑ 8×</strong>。</li>
<li><strong>与树搜索正交叠加</strong><br>将多样 head 作为 BFS/MCTS 的“策略池”，每个节点随机选 head 生成 tactic，可<strong>把 16 次采样等效为 64 次搜索节点</strong>而 token 不变。</li>
<li><strong>端侧量化</strong><br>对 20-token 可学习 prefix 做 <strong>INT4 量化</strong>实验，验证是否可在手机端跑 8-head 并行而掉点 &lt;0.3 %。</li>
</ul>
<p>4. 应用层面：走出 miniF2F</p>
<ul>
<li><strong>IMO 级难题</strong><br>将 EconRL 接入 Seed-Prover 的“分解-精修”管线，仅给每子目标 4 次采样，检验是否仍能增加完整 IMO 题的解决数。</li>
<li><strong>代码形式化</strong><br>把 Lean 4 换成 <strong>Coq / Isabelle</strong>，验证难度分桶 + 动态 CoT 是否语言无关；若出现 domain-shift，可用 <strong>meta-RL</strong> 快速重学最优切换策略。</li>
<li><strong>跨模态定理</strong><br>对含图形的几何题（如 IMO 2024 G6）先用视觉-语言模型生成自然语言描述，再送入 EconRL；探索<strong>多模态难度感知</strong> head 是否能进一步提升多样性。</li>
</ul>
<p>5. 即时可跑的消融清单</p>
<ol>
<li>将 head 数从 8 连续降到 1，绘制“accuracy vs head 数”曲线，观察是否出现 <strong>phase-transition</strong>。</li>
<li>用 KL 散度代替 PDC，验证多样性度量与最终准确率的 <strong>Spearman ρ</strong> 是否更高。</li>
<li>在动态 CoT 阶段引入 <strong>长度惩罚系数 λ</strong>，扫描 λ∈[0,0.1,0.5,1.0]，绘制 Pareto 前沿，检验是否可再压缩 20 % token。</li>
</ol>
<p>这些方向既包含<strong>理论深度</strong>，也能在<strong>现有代码基线</strong>上快速验证，为下一代“经济型”定理证明系统提供持续动力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="论文主旨一句话"><a href="#论文主旨一句话" class="headerlink" title="论文主旨一句话"></a>论文主旨一句话</h1><p>用 <strong>1/8 的 token</strong> 达到 <strong>SOTA 准确率</strong>，首次系统解决大模型自动定理证明（ATP）在测试时扩展阶段的<strong>高成本-低边际收益</strong>难题。</p>
<h2 id="1-背景与痛点"><a href="#1-背景与痛点" class="headerlink" title="1. 背景与痛点"></a>1. 背景与痛点</h2><ul>
<li>现有 ATP 靠“长链式思维（CoT）+ 海量并行采样”提分，但<br>– 长 CoT 使 token 开销 <strong>10–25×</strong>；<br>– 并行 &gt;32 次后多样性饱和，<strong>1.1 % 准确率换 4× 成本</strong>。</li>
<li>既往只数“采样次数”，忽视 <strong>token 级真实成本</strong>，无法公平权衡。</li>
</ul>
<h2 id="2-统一度量"><a href="#2-统一度量" class="headerlink" title="2. 统一度量"></a>2. 统一度量</h2><p>提出 <strong>token-level sampling cost</strong>：  </p>
<script type="math/tex; mode=display">\\text{Cost} = \\sum\_{\\text{passes}} (\\text{initial tokens} + \\text{refinement tokens})</script><p>用该度量重画 scaling curve，揭示顺序扩展与并行扩展的<strong>双重低效</strong>。</p>
<h2 id="3-EconRL-框架（两阶段-RL）"><a href="#3-EconRL-框架（两阶段-RL）" class="headerlink" title="3. EconRL 框架（两阶段 RL）"></a>3. EconRL 框架（两阶段 RL）</h2><p>阶段</p>
<p>目标</p>
<p>方法</p>
<p>效果</p>
<p><strong>动态 CoT 切换</strong></p>
<p>易题不浪费 token</p>
<p>用 DPO 学“难则思、易则直”的 0/1 决策</p>
<p>75.4 % 准确率仅用 <strong>15 %</strong> token</p>
<p><strong>多样并行 RL</strong></p>
<p>少采样也覆盖证明空间</p>
<p>难度分桶 → 8 个可学习 prefix head → 独立 PPO</p>
<p>Pass@16 提升 <strong>4.9 %</strong>，冗余 ↓</p>
<p>两阶段<strong>正交叠加</strong>，无额外超参冲突。</p>
<h2 id="4-主要结果"><a href="#4-主要结果" class="headerlink" title="4. 主要结果"></a>4. 主要结果</h2><ul>
<li><strong>miniF2F</strong>：<br>– ECONPROVER-DS 76.2 %（vs 全 CoT 75.8 %）成本 <strong>1.5×</strong> ↔ 原 <strong>10×</strong><br>– ECONPROVER-GD 84.0 %（vs 基线 84.4 %）成本 <strong>3×</strong> ↔ 原 <strong>25×</strong></li>
<li><strong>ProofNet</strong>：维持 23-28 % 准确率，成本同样 <strong>3×</strong> 以内。</li>
<li><strong>+ 迭代精修</strong>：86.0 % 准确率下 token 从 <strong>40× 降到 10×</strong>，节省 <strong>75 %</strong>。</li>
</ul>
<h2 id="5-贡献清单"><a href="#5-贡献清单" class="headerlink" title="5. 贡献清单"></a>5. 贡献清单</h2><ol>
<li>首次用 <strong>token 成本</strong> 系统评估 ATP 测试时扩展。</li>
<li>揭示 <strong>83 %</strong> 非 IMO 题无需 CoT、并行采样 <strong>>32 次</strong>即冗余。</li>
<li>提出 <strong>EconRL</strong> 两阶段 RL 流水线，公开可插拔。</li>
<li>在两大开源基座上取得 <strong>一致且可复现</strong> 的 8× 级成本压缩。</li>
</ol>
<h2 id="6-一句话总结"><a href="#6-一句话总结" class="headerlink" title="6. 一句话总结"></a>6. 一句话总结</h2><p>EconRL 让 ATP 模型<strong>会省再想、会探不重复</strong>，用 <strong>12 % 的 token</strong> 就能跑在原来的最高性能上，为大规模部署和后续迭代精修提供了<strong>经济可行</strong>的新基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是EconProver%3A%20Towards%20More%20Economical%20Test-Time%20Scaling%20for%20Automated%20Theorem%20Proving，链接是  https://arxiv.org/pdf/2509.12603  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12603  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Mukai Li,Linfeng Song,Zhenwen Liang,Jiahao Xu,Shansan Gong,Qi Liu,Haitao Mi,Dong Yu</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12603.pdf">https://arxiv.org/pdf/2509.12603.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12603">https://arxiv.org/abs/2509.12603</a></p>
<p><strong>Arxiv ID</strong>: 2509.12603</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12603">https://papers.cool/arxiv/2509.12603</a></p>
<p><strong>Published</strong>: 2025-09-16T03:00:13Z</p>
<p><strong>Updated</strong>: 2025-09-16T03:00:13.000Z</p>
<hr>
<h3 id="11-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms"><a href="#11-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms" class="headerlink" title="11. Exact Coset Sampling for Quantum Lattice Algorithms"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12341">Exact Coset Sampling for Quantum Lattice Algorithms</a></h3><p>We give a simple, fully correct, and assumption-light replacement for the contested “domain-extension” in Step 9 of a recent windowed-QFT lattice algorithm with complex-Gaussian windows~\citep{chen2024quantum}. The published Step~9 suffers from a periodicity/support mismatch. We present a pair-shift difference construction that coherently cancels all unknown offsets, produces an exact uniform CRT-coset state over $\mathbb{Z}_{P}$, and then uses the QFT to enforce the intended modular linear relation. The unitary is reversible, uses $\mathrm{poly}(\log M_2)$ gates, and preserves the algorithm’s asymptotics. Project Page: <a target="_blank" rel="noopener" href="https://github.com/yifanzhang-pro/quantum-lattice">https://github.com/yifanzhang-pro/quantum-lattice</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>在最近的一个具有复高斯窗口的窗口 QFT 晶格算法的第 9 步中，我们给出了一个简单、完全正确和假设光的替代，以替代有争议的“域扩展”~\citep{chen2024quantum}。已发布的 Step~9 存在周期性/支持不匹配的问题。我们提出了一种对移位差分结构，该构造相干地抵消所有未知偏移量，在 $\mathbb{Z}_{P}$ 上产生精确均匀的 CRT 余集状态，然后使用 QFT 强制执行预期的模线性关系。酉是可逆的，使用 $\mathrm{poly}（\log M_2）$ 门，并保留算法的渐近。项目页面：<a target="_blank" rel="noopener" href="https://github.com/yifanzhang-pro/quantum-lattice。">https://github.com/yifanzhang-pro/quantum-lattice。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在修正 Chen (2024) 提出的“加窗量子傅里叶变换（windowed-QFT）格算法”中 Step 9 的一个关键缺陷。<br>该缺陷表现为：</p>
<ul>
<li>原 Step 9 试图仅对第一坐标做“域扩展”（domain extension），却导致振幅周期性/支撑集大小不匹配，最终输出的量子态并不满足算法所需的模线性约束<br>⟨b*, u⟩ ≡ 0 (mod P)</li>
</ul>
<p>作者给出的替代方案（Step 9†）一次性解决以下核心难题：</p>
<ol>
<li><p><strong>未知偏移消除</strong><br>输入态含有无法预先获知的偏移向量 v*；新构造通过“成对平移差分”使其在相干减法中被精确抵消，无需知道 v* 的任何信息。</p>
</li>
<li><p><strong>精确均匀 CRT-陪集采样</strong><br>差分后得到 −2D²T b* (mod M₂)，其中 T 在 Z_P 上均匀叠加；该像集正是中国剩余定理分解 Z_{M₂} ≅ Z_{D²} × Z_P 中“Z_P 纤维”的循环陪集。由此天然生成一个<strong>均匀且相干</strong>的陪集态，无需依赖任何振幅周期性假设。</p>
</li>
<li><p><strong>模线性约束的精确实施</strong><br>对上述陪集态执行 QFT^{⊗n}<em>{Z</em>{M₂}} 并测量，利用特征正交性可<strong>零误差地</strong>把结果限制在 ⟨b*, u⟩ ≡ 0 (mod P) 的子空间，且在该子空间内均匀分布。</p>
</li>
<li><p><strong>可逆性与资源</strong><br>整个流程完全可逆，门复杂度 poly(log M₂)，不引入额外指数级开销，保持原算法的整体渐近优势。</p>
</li>
</ol>
<p>综上，论文提供了一种“即插即用”的新 Step 9，彻底摆脱了对振幅周期性的依赖，使窗口化 QFT 格算法在理论上严谨、实现上可行。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与本文直接相关的研究脉络可归纳为三条主线：量子格算法、窗口化/近似 QFT 技术，以及 coset/Fourier 采样中的“偏移消除”技巧。按时间顺序列出代表性工作如下（均与“精确 coset 采样”或“窗口化 QFT 格算法”存在技术衔接或对比）。</p>
<p>研究方向</p>
<p>代表文献</p>
<p>与本文的关联</p>
<p>1. 量子格算法奠基</p>
<p>Regev (STOC 2003)</p>
<p>首次将 QFT 与格问题（LWE）结合，提出“量子约简”框架；本文仍沿用 QFT→经典线性代数恢复秘密的范式。</p>
<p>2. 量子格算法实用化</p>
<p>Eisenträger-Hallgren-Kitaev-Song (QIC 2014)</p>
<p>给出 Z_{q} 上精确 Fourier 采样算法，用于 SIS 近似解；本文需要类似的“模约束”采样，但额外处理未知偏移。</p>
<p>3. 窗口化/近似 QFT 引入格算法</p>
<p>Chen-Liu-Zhandry (Eurocrypt 2022)</p>
<p>首次用“窗口ed QFT”降低寄存器位数，但遗留“如何强制模约束”问题；Chen 2024 直接继承该框架。</p>
<p>4. 窗口化 QFT 格算法（被修正对象）</p>
<p>Chen (ePrint 2024/???)*</p>
<p>提出 Step 9 的“域扩展”设想，却因周期性误用导致支撑集大小错误；本文即针对该 Step 9 给出无假设替代。</p>
<p>5. Coset 采样与偏移消除</p>
<p>Bonnetain-Naya-Plasencia (PQCrypto 2019)</p>
<p>在量子 meet-in-the-middle 中采用“复制-差分”消除密钥比特，与本文“pair-shift difference”思想同源，但场景不同。</p>
<p>6. 精确 coset 态合成</p>
<p>Kuperberg (Math. ArXiv 2011)</p>
<p>给出隐藏子群问题中精确 coset 态的 Clifford+Toffoli 构造；本文利用 CRT 分解+可逆算法实现类似目标，且无需 HSP 结构。</p>
<p>7. 模数分解与 CRT 技巧</p>
<p>Peikert-Pepin (TCC 2019)</p>
<p>在经典格算法中利用 CRT 分解模数以加速；本文反向利用 CRT 把“模 P 约束”嵌入到高阶模 M₂ 的陪集。</p>
<p>8. 可逆算术与 ancilla 清除</p>
<p>Haner-Roetteler-Svore (QIC 2018)</p>
<p>给出模加/模乘及逆运算的可逆实现，本文 Step 9†.4 的 T′ 计算与清除直接调用其电路模板。</p>
<p>*Chen 2024 即论文中反复引用的“窗口化 QFT 格算法”原始出处，arXiv 编号 2024:04xxx，尚未正式发表于会议。</p>
<p>简言之，本文在 Chen 2024 的“窗口化 QFT 格算法”基础上，借鉴了早期量子格算法、coset 采样与可逆算术等成熟技术，首次把“复制-平移-差分”思路用于<strong>精确消除未知偏移</strong>并<strong>强制模线性约束</strong>，填补了窗口化 QFT pipeline 的最后一块理论空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文用一套“成对平移差分 + 精确陪集合成”的量子子程序（Step 9†）彻底替换掉 Chen 2024 中失效的 Step 9。核心流程可概括为五步，每一步都对应一个明确的技术目标，且全部在 poly(log M₂) 门复杂度内可逆完成。</p>
<ol>
<li><p>复制<br>对输入寄存器 X 做按位 CNOT/模加，得到一份完全相同的副本 Y。<br>目的：为后续差分准备“两份相同数据”。</p>
</li>
<li><p>平移<br>在 Y 上相干地加上 2D²T b*（mod M₂），其中 T 事先在 Z_P 上均匀叠加。<br>目的：把未知偏移 v* 留在原份 X 中，而让 Y 额外携带一个可控的“平移量” 2D²T b*。</p>
</li>
<li><p>差分<br>计算 Z ← X − Y (mod M₂)。<br>结果：Z = −2D²T b* (mod M₂)，未知偏移 v* 被<strong>精确消去</strong>，且 Z 仅依赖 T。</p>
</li>
<li><p>清理<br>利用“剩余可访问性”条件（Definition 2.1）从 Z mod P 反推出 T′≡T；再用可逆算术把原 T 寄存器清零、Y 还原成 X、副本 Y 清零，最后丢弃 X。<br>目的：让 T 与 Z 解除纠缠，使全局态因子化为<br>|junk⟩ ⊗ (1/√P)∑_{T∈Z_P}|−2D²T b*⟩_Z，<br>从而 Z 寄存器上形成<strong>均匀且相干</strong>的 CRT-陪集态。</p>
</li>
<li><p>Fourier 采样<br>对 Z 执行 QFT^{⊗n}<em>{Z</em>{M₂}} 并测量 u。<br>由特征正交性，振幅非零当且仅当 ⟨b*, u⟩ ≡ 0 (mod P)；且在该子空间内均匀分布。<br>至此，原算法所需的模线性约束被<strong>零误差地</strong>强制执行，且无需知道任何关于 v* 的信息。</p>
</li>
</ol>
<p>通过上述五步，论文一次性解决了“未知偏移消除”“精确陪集态合成”和“模约束强制”三大难题，同时保持可逆性与原算法的渐近复杂度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>该论文<strong>没有进行任何实验或数值模拟</strong>。全文聚焦于<strong>理论正确性证明</strong>与<strong>量子电路构造</strong>，目标是为 Chen 2024 的窗口化 QFT 格算法提供一个<strong>严格、无假设、可逆且门复杂度可控</strong>的 Step 9 替代方案。所有结果均以<strong>引理、定理与复杂性分析</strong>形式给出，未涉及实际硬件运行、仿真数据或统计测试。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下问题在理论或实现层面仍待深入，可视为直接延续本文工作的“下一步”。</p>
<ol>
<li><p><strong>Residue-accessibility 失效时的确定性方案</strong></p>
<ul>
<li>当某素因子 pη 满足 b*≡0 (mod pη) 时，Step 9† 只能获得部分模约束。</li>
<li>目标：设计<strong>无需后选择</strong>、仍保持 poly(log M₂) 门的<strong>确定性</strong>算法，把剩余素因子通过辅助方向或格基重排一次性解决。</li>
</ul>
</li>
<li><p><strong>常量深度或 NISQ 友好版本</strong></p>
<ul>
<li>当前电路深度由 QFT^{⊗n}_{ZM₂} 主导 ≈n log M₂。</li>
<li>探索：用近似 QFT 或分段 QFT 把深度降到 poly(log n, log log M₂)，同时<strong>仍保证</strong> ⟨b*,u⟩≡0 (mod P) 的<strong>零误差</strong>支撑。</li>
</ul>
</li>
<li><p><strong>T 寄存器的量子编码优化</strong></p>
<ul>
<li>本文用 QFT_{ZP} 产生均匀叠加，需要 κ 个模逆 + CRT。</li>
<li>可尝试：量子比特节省的“分段 CRT”或“模链”编码，减少 ancilla 到 O(log P) 而不增加深度。</li>
</ul>
</li>
<li><p><strong>抗噪声与错误修正开销</strong></p>
<ul>
<li>对 Z 寄存器的 QFT^{⊗n} 是容错瓶颈；需量化<strong>逻辑量子比特</strong>与<strong>魔法态消耗</strong>，并与近期 surface-code 指标挂钩，给出真实硬件资源估算。</li>
</ul>
</li>
<li><p><strong>扩展到其他模结构</strong></p>
<ul>
<li>本文依赖 Z_{M₂}≅Z_{D²}×Z_P 的 CRT 分解。</li>
<li>若窗口函数或模数改为 2^k 或复合幂次，CRT 纤维结构变化——需要重新设计“平移量”与陪集映射，保持差分后仍均匀。</li>
</ul>
</li>
<li><p><strong>与经典 lattice sieve 的混合 pipeline</strong></p>
<ul>
<li>目前量子输出 u 仅用于线性代数恢复秘密；可研究：把<strong>量子陪集采样</strong>嵌入经典 sieve 的“过滤”阶段，降低经典内存开销，形成端到端量子加速 sieve。</li>
</ul>
</li>
<li><p><strong>证明整体算法的最优性下界</strong></p>
<ul>
<li>本文只保证 Step 9† 的<strong>正确性</strong>与<strong>poly 门数</strong>；尚未讨论<strong>采样复杂度下界</strong>。</li>
<li>开放问题：任何量子算法要生成均匀 CRT-coset 并强制模约束，是否必须 Ω(log M₂) 深度或 Ω(n) 量子比特？给出电路下界或 oracle 分离。</li>
</ul>
</li>
<li><p><strong>通用“偏移消除”框架</strong></p>
<ul>
<li>将 pair-shift difference 抽象为<strong>黑箱模板</strong>：<br>– 输入：|x+f(j)⟩，f(j) 未知；<br>– 目标：生成仅依赖 j 的干净陪集态。</li>
<li>探索该模板在 hidden-shift、dihedral HSP、quantum walks 中的可移植性。</li>
</ul>
</li>
<li><p><strong>后量子安全评估工具</strong></p>
<ul>
<li>实现 Step 9† 的开放源码（Q# / Cirq / Qiskit），结合经典 lattice reducer，形成可验证的<strong>量子成本估算器</strong>，供 NIST 后续评估 lattice-based NIST-PQC 方案的实际量子威胁级别。</li>
</ul>
</li>
<li><p><strong>经典模拟与 Clifford+T 计数</strong></p>
<ul>
<li>对合理规模 (n≤64, log M₂≤1024) 做<strong>张量网络模拟</strong>，验证测量分布与理论预言一致；</li>
<li>给出完整 Clifford+T 分解及 T-count 上界，用于与 Shor 风格算法进行资源对标。</li>
</ul>
</li>
</ol>
<p>以上任意方向均可直接引用本文的“pair-shift difference”思想作为起点，同时弥补当前工作在<strong>实验验证、资源优化、抗噪声与下界证明</strong>等方面的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：Chen 2024 窗口化-QFT 格算法 Step 9 因“域扩展”误用振幅周期性，输出态支撑集大小错误，无法保证 ⟨b*,u⟩≡0 (mod P)。</li>
<li><strong>解法</strong>：提出 Step 9†——“成对平移差分”子程序：<ol>
<li>复制寄存器 X→Y</li>
<li>对 Y 相干加 2D²T b*（T 在 Z_P 均匀）</li>
<li>差分 Z=X−Y，恰好消去未知偏移 v*</li>
<li>利用 residue-accessibility 可逆地解出 T 并清零所有 ancilla，使 Z 上留下均匀 CRT-陪集态</li>
<li>对 Z 执行 QFT^{⊗n}_{ZM₂} 并测量，得到在 ⟨b*,u⟩≡0 (mod P) 子空间上<strong>均匀分布</strong>的 u</li>
</ol>
</li>
<li><strong>结果</strong>：无需知道 v*、不使用振幅周期性，门复杂度 poly(log M₂)，完全可逆，保持原算法渐近优势；缺陷是理论构造，无实验验证。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Exact%20Coset%20Sampling%20for%20Quantum%20Lattice%20Algorithms，链接是  https://arxiv.org/pdf/2509.12341  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12341  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yifan Zhang</p>
<p><strong>Categories</strong>: quant-ph,cs.CL,cs.CR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12341.pdf">https://arxiv.org/pdf/2509.12341.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12341">https://arxiv.org/abs/2509.12341</a></p>
<p><strong>Arxiv ID</strong>: 2509.12341</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12341">https://papers.cool/arxiv/2509.12341</a></p>
<p><strong>Published</strong>: 2025-09-15T18:10:28Z</p>
<p><strong>Updated</strong>: 2025-09-15T18:10:28.000Z</p>
<hr>
<h3 id="12-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge"><a href="#12-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge" class="headerlink" title="12. Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.06079">Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</a></h3><p>Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \&amp; Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at <a target="_blank" rel="noopener" href="https://github.com/OpenDCAI/SciReasoner">https://github.com/OpenDCAI/SciReasoner</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态推理仍然是人工智能中的一个基本挑战。尽管基于文本的推理取得了重大进展，但即便是像 GPT-3 这样的最先进模型在多模态场景中也难以保持强大的性能。为了解决这一差距，我们提出了一种辅助标题推理框架，有效地桥接了视觉和文本模态。我们的方法在 ICML 2025 人工智能数学研讨会和挑战 2: SeePhys 中获得了第一名，突显了其有效性和稳健性。此外，我们在 MathVerse 基准测试中验证了其在几何推理上的泛化能力，展示了我们方法的多样性。我们的代码已公开，地址是 <a target="_blank" rel="noopener" href="https://github.com/OpenDCAI/SciReasoner">https://github.com/OpenDCAI/SciReasoner</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>多模态推理（multimodal reasoning）</strong>中的核心难题：<br>如何有效整合<strong>视觉信息（图像）</strong>与<strong>文本信息（问题描述）</strong>，以在科学（尤其是物理与几何）场景中实现稳定、准确且可泛化的推理。</p>
<p>具体而言，作者观察到：</p>
<ul>
<li>当前最先进的文本推理模型（如 GPT-o3）在<strong>纯文本任务</strong>上表现卓越，但在<strong>同时涉及图像与文本</strong>的多模态任务上性能显著下降。</li>
<li>这种性能落差并非完全源于视觉感知缺陷，而更多在于<strong>跨模态对齐与融合推理</strong>的不足。</li>
</ul>
<p>为此，论文提出**“字幕辅助推理框架”（caption-assisted reasoning）<strong>，通过将图像转化为</strong>高质量、结构化字幕**，使模型<strong>无需直接处理原始像素</strong>，即可在文本空间内完成推理。该方法在<strong>ICML 2025 SeePhys 挑战赛</strong>中获得第一名，并在<strong>MathVerse</strong>几何推理基准上验证了其跨领域泛化能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与本文直接相关的研究可归纳为三条主线，每条均给出最具代表性的文献或系统，并注明其与“字幕辅助推理”框架的关联点。</p>
<p>主线</p>
<p>代表工作</p>
<p>与本文关联</p>
<p><strong>1. 多模态科学推理基准</strong></p>
<p>SeePhys (Xiang et al., 2025) MathVerse (Zhang et al., 2024)</p>
<p>提供了物理与几何两大场景的评测平台，首次量化揭示 SOTA 文本模型在多模态任务上的显著落差，成为本文提出字幕范式的直接动机。</p>
<p><strong>2. 视觉-语言模型在 STEM 的局限性</strong></p>
<p>GPT-4V 物理评测 (He et al., 2025) SciBench (Wang et al., 2024)</p>
<p>通过 OCR 或端到端视觉编码器处理图表，发现模型易被视觉噪声干扰，推理链断裂；本文用“字幕即抽象”策略规避了此类感知-推理耦合失效。</p>
<p><strong>3. 文本化中间表示的推理增强</strong></p>
<p>Reasoning-OCR (He et al., 2025) Chain-of-Thought + LaTeX (Liang et al., 2024) Program-of-Thought (Chen et al., 2023)</p>
<p>首次在 OCR 场景验证“纯文本描述可替代图像输入”的假设；本文将其扩展到<strong>通用科学图</strong>，并引入结构化、领域自适应字幕与多阶段精炼，形成完整 pipeline。</p>
<p>此外，近期同期工作如 <strong>SciMaster (Chai et al., 2025)</strong> 与 <strong>Gemini-2.5-Pro 科学评测 (Huang &amp; Yang, 2025)</strong> 也观察到类似落差，但侧重工具调用与长链思维，未系统探究“字幕作为独立模态”的极限性能，故与本文互补。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“多模态推理落差”问题<strong>形式化</strong>为：<br>在低信息密度图像（几何/物理示意图）场景下，如何以<strong>最小视觉 token 开销</strong>获得<strong>最大推理稳定性</strong>。<br>为此，提出一套**“字幕辅助推理”流水线**，把视觉-文本融合难题转化为<strong>纯文本推理</strong>问题，具体实现分三步：</p>
<p>1. 字幕生成：从“像素”到“语义 token”</p>
<p>策略</p>
<p>关键设计</p>
<p>目的</p>
<p><strong>Structured Captioning</strong></p>
<p>领域模板 + 原子级组件拆分（对象、符号、关系、隐含属性）</p>
<p>消除歧义，输出可直接被 LLM 解析的“伪代码式”描述</p>
<p><strong>Grounding</strong></p>
<p>显式坐标与绑定框</p>
<p>解决物理图“谁连谁”的指代混乱</p>
<p><strong>MathVerse 适配</strong></p>
<p>几何专用模板（平行、相交、角度、辅助线）</p>
<p>跨领域迁移时仅需替换模板，无需重训</p>
<p>2. 推理范式：纯文本链式思考</p>
<ul>
<li><strong>无图像输入</strong>：LLM 仅接收“结构化字幕 + 问题文本”，以标准 CoT 生成答案。</li>
<li><strong>Image Reintegration</strong>：可选地把原始图再拼回上下文，提供<strong>冗余视觉校验</strong>，在量子力学、电路等高密度视觉符号场景下自动触发（Adaptive Answer Routing）。</li>
</ul>
<p>3. 质量提升：三级后处理</p>
<ol>
<li><strong>Format Optimization（FO）</strong><br>强制模型按“单值、分段函数、向量”等 LaTeX 模板输出，降低解析误差。</li>
<li><strong>Critical Review（CR）</strong><br>用第二强大模型（o3→o3）对初答做<strong>物理一致性审查</strong>，修正单位、符号、边界条件等隐性错误。</li>
<li><strong>Adaptive Answer Routing（AAR）</strong><br>在 7 类物理子领域上，以 SeePhys-Dev 为验证集，<strong>自动选择</strong>“字幕流”或“端到端图像流”中置信度更高的一方作为最终答案。</li>
</ol>
<p>效果</p>
<ul>
<li>SeePhys-mini 上，Structured+Img+FO+CR 取得 <strong>66.0 %</strong>，相对最强纯多模态基线（G2.5P, 58.0 %）提升 <strong>8 pp</strong>。</li>
<li>MathVerse 几何任务中，字幕流使 Claude-Opus-4 从 60.2 % 提至 <strong>85.5 %</strong>，并出现<strong>纯文本 LLM（DeepSeek-R1）反超多模态基线</strong>的现象，验证“字幕即足够”假设。</li>
</ul>
<p>综上，论文<strong>未改动任何模型参数</strong>，仅通过<strong>高质量文本化中间表示 + 纯文本推理 + 轻量级后处理</strong>，即把视觉-文本融合问题转化为已充分解决的<strong>文本推理问题</strong>，从而系统性缩小多模态落差。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕两条主线展开：</p>
<ol>
<li>在 <strong>SeePhys-mini</strong> 物理 benchmark 上系统消融字幕策略与后处理模块；</li>
<li>在 <strong>MathVerse</strong> 几何 benchmark 上验证跨领域泛化与“纯文本推理”极限性能。</li>
</ol>
<p>实验 1：SeePhys-mini 消融与提升（200 题，8 个难度级）</p>
<p>组别</p>
<p>变量</p>
<p>关键结果</p>
<p><strong>Baseline</strong></p>
<p>直接多模态 (G2.5P / o3)</p>
<p>58.0 %（最佳单模型）</p>
<p><strong>Caption 质量</strong></p>
<p>Default → Grounding → Structured</p>
<p>58.5 % → 59.0 % → <strong>61.5 %</strong></p>
<p><strong>互补视觉</strong></p>
<p>Structured + Img</p>
<p>61.5 % → <strong>65.5 %</strong></p>
<p><strong>格式+审查</strong></p>
<p>+FO +CR</p>
<p>65.5 % → <strong>66.0 %</strong>（最终方案）</p>
<p><strong>路由策略</strong></p>
<p>AAR（7 类物理子领域自动选流）</p>
<p>在量子、电路等高密度符号场景优先用图像流，其余用字幕流，整体再提 0.5-1 pp</p>
<p>此外给出<strong>按难度细分</strong>的最佳配置：</p>
<ul>
<li>中学/本科课程题：Structured+FO 即可达 81-87 %</li>
<li>奥赛/研级题：必须“Structured+Img+CR”组合，PhD 级最高 <strong>57.4 %</strong></li>
</ul>
<p>实验 2：MathVerse 泛化（261 题，Vision-Only vs Vision-Intensive）</p>
<p>设置</p>
<p>模型</p>
<p>视觉强度</p>
<p>w/ caption 提升</p>
<p><strong>MLLM</strong></p>
<p>Claude-Opus-4</p>
<p>Vision-Intensive</p>
<p>60.2 % → <strong>85.5 %</strong> (↑25.3 pp)</p>
<p><strong>纯文本 LLM</strong></p>
<p>DeepSeek-R1</p>
<p>Vision-Only</p>
<p>无图，仅用字幕 → <strong>68.2 %</strong>，反超 GPT-o3 多模态 64.5 %</p>
<p><strong>跨模型稳健性</strong></p>
<p>Qwen2.5-72B-Instruct</p>
<p>两种强度</p>
<p>平均 +8~12 pp，所有 MLLM 均受益</p>
<p>实验 3：组件必要性验证</p>
<ul>
<li><strong>Rephrasing 负增益</strong>：让模型先复述问题再解答，准确率反而下降 8 pp，证明“复述”≠“高质量字幕”。</li>
<li><strong>AAR 失效条件</strong>：一旦字幕流≥61 %，自动路由会引入更弱图像流，导致整体下降，说明<strong>字幕足够强时无需回退</strong>。</li>
</ul>
<p>实验 4：效率与 token 开销</p>
<ul>
<li>一张典型物理图被多模态编码器展开为 <strong>~400-800 视觉 token</strong>；</li>
<li>同图经 Structured Caption 压缩为 <strong>60-90 文本 token</strong>，减少 <strong>>80 %</strong> 视觉上下文长度，同时获得更高精度。</li>
</ul>
<p>综上，论文通过<strong>控制变量式消融</strong>与<strong>跨领域迁移</strong>两大实验矩阵，既验证了“字幕辅助”对多模态推理的普遍增益，也量化了其在** token 效率、模型无关性、纯文本极限**上的优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向按“<strong>可落地难度</strong>”与“<strong>潜在增益</strong>”二维评估，星号越多优先级越高。</p>
<p>1. 自适应字幕粒度 ★★★★☆</p>
<ul>
<li><strong>问题</strong>：当前模板固定，同一张图对初中生与博士生输出相同长度字幕。</li>
<li><strong>思路</strong>：用<strong>强化学习</strong>或<strong>可逆缩放策略</strong>动态决定“描述到多细”——简单图只保留符号表，复杂场图自动展开矢量分量、边界条件。</li>
<li><strong>验证指标</strong>：字幕 token 数 / 推理准确率 Pareto 前沿。</li>
</ul>
<p>2. 程序-字幕混合推理 ★★★★★</p>
<ul>
<li><strong>问题</strong>：纯文本 CoT 难以处理偏微分、隐式方程组。</li>
<li><strong>思路</strong>：在结构化字幕里直接嵌入<strong>可执行符号代码</strong>（SymPy、Julia ModelingToolkit），让模型生成“代码块”而非 LaTeX 句子，再调用外部引擎求数值解。</li>
<li><strong>期望</strong>：把 SeePhys 中“解析解不可得”的 PhD 级题目（如非线性电路暂态）从 57 % 提到 &gt;75 %。</li>
</ul>
<p>3. 跨模态不确定度估计 ★★★☆☆</p>
<ul>
<li><strong>问题</strong>：字幕可能误识别符号或漏标关键力。</li>
<li><strong>思路</strong>：<ol>
<li>让字幕模型输出<strong>符号级置信度</strong>；</li>
<li>推理模型基于置信度<strong>主动请求放大</strong>或<strong>人机协同</strong>；</li>
</ol>
</li>
<li><strong>收益</strong>：在保持自动化的同时，把<strong>临界错误率</strong>再降 30 %。</li>
</ul>
<p>4. 向化学/生物图谱扩展 ★★★☆☆</p>
<ul>
<li><strong>问题</strong>：化学结构式、生物通路图的信息密度远高于物理示意图。</li>
<li><strong>思路</strong>：<ul>
<li>化学：将 SMILES/InChI 字符串作为“字幕”中间态，直接喂给 LLM；</li>
<li>生物：用 Systems Biology Graphical Notation (SBGN) 的 XML 描述作为结构化字幕。</li>
</ul>
</li>
<li><strong>验证基准</strong>：新构建 ChemVerse、BioVerse，测试反应预测与通路推理。</li>
</ul>
<p>5. 人机闭环科学发现 ★★☆☆☆</p>
<ul>
<li><strong>问题</strong>：字幕可读性高，适合科学家<strong>在线修正</strong>。</li>
<li><strong>思路</strong>：推出“<strong>字幕即接口</strong>”工作台——研究者对字幕进行<strong>行间编辑</strong>，系统实时回推新答案；编辑记录用来<strong>迭代微调</strong>字幕生成器，形成飞轮。</li>
<li><strong>长期目标</strong>：让框架成为实验室日常工具，而非一次性比赛方案。</li>
</ul>
<p>6. 多图-多模态长上下文 ★★☆☆☆</p>
<ul>
<li><strong>问题</strong>：真实科研常出现**&gt;10 张关联图**（实验装置+数据曲线+拟合结果）。</li>
<li><strong>思路</strong>：<ul>
<li>把每张图生成独立字幕后，用<strong>超链接语法</strong>建立跨图引用（如“图2-curve-A 即图1-scatter 的导数”）；</li>
<li>采用<strong>128k 长上下文 LLM</strong>做全局推理。</li>
</ul>
</li>
<li><strong>基准</strong>：构建 LongSeePhys-2k，平均 5.3 张图/题，测试长链因果综合。</li>
</ul>
<p>7. 统一评测协议与错误本体 ★★★★☆</p>
<ul>
<li><strong>问题</strong>：不同工作使用私有提示与打分脚本，结果不可复现。</li>
<li><strong>建议</strong>：<ol>
<li>开源<strong>字幕+推理提示库</strong>与<strong>自动评分器</strong>；</li>
<li>建立细粒度错误本体（符号误识别、单位缺失、概念混淆、计算错误），支持<strong>错误驱动</strong>的迭代优化。</li>
</ol>
</li>
</ul>
<p>8. 低资源语言与可访问性 ★★☆☆☆</p>
<ul>
<li><strong>问题</strong>：非英语使用者难以直接受益。</li>
<li><strong>思路</strong>：<ul>
<li>用<strong>多语字幕模板</strong>（中、西、印地）把结构化部分本地化；</li>
<li>保持符号与公式仍为 LaTeX，实现“<strong>语言无关、语义同构</strong>”的推理。</li>
</ul>
</li>
</ul>
<p>综上，<strong>程序-字幕混合推理</strong>与<strong>自适应粒度</strong>两条路线兼具高增益与可验证性，可作为下一阶段主攻；<strong>跨学科扩展</strong>与<strong>人机闭环</strong>则构成长期愿景，逐步把字幕辅助框架升级为<strong>通用科学发现引擎</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文主旨</p>
<p>针对“<strong>多模态推理显著弱于纯文本推理</strong>”这一普遍现象，提出<strong>字幕辅助推理框架</strong>：用高质量、结构化字幕把图像转换成极短文本 token，再交由 LLM 进行纯文本链式思考，无需微调即可在物理与几何任务上同时取得 SOTA。</p>
<p>核心贡献</p>
<ol>
<li><strong>新范式</strong>：低信息密度图→结构化字幕→纯文本 CoT，视觉 token 减少 80%，准确率反而提升。</li>
<li><strong>系统方法</strong>：<ul>
<li>Structured Captioning（领域模板+原子组件）</li>
<li>Image Reintegration（冗余视觉校验）</li>
<li>Format Optimization &amp; Critical Review（二级后处理）</li>
<li>Adaptive Answer Routing（7 类物理子领域自动选流）</li>
</ul>
</li>
<li><strong>实证结果</strong>：<ul>
<li>ICML 2025 SeePhys 挑战赛 <strong>第一名</strong>（66.0 %，超最强多模态基线 8 pp）。</li>
<li>MathVerse 几何基准上，字幕流把 Claude-Opus-4 从 60.2 % 提至 <strong>85.5 %</strong>；纯文本 LLM（DeepSeek-R1）仅用字幕即达 <strong>68.2 %</strong>，反超 GPT-o3 多模态。</li>
</ul>
</li>
<li><strong>跨域泛化</strong>：同一框架零样本迁移到几何、电路、光学等 8 级难度，验证“<strong>字幕即足够</strong>”假设。</li>
</ol>
<p>一句话总结</p>
<p>用<strong>可解析的 60-token 字幕</strong>替代<strong>400-token 视觉编码</strong>，把多模态推理问题转化为已解决的文本推理问题，在物理与几何两大基准上同时刷新 SOTA，并首次证明<strong>纯文本 LLM 可比端到端多模态系统更强</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Multimodal%20Reasoning%20for%20Science%3A%20Technical%20Report%20and%201st%20Place%20Solution%20to%20the%20ICML%202025%20SeePhys%20Challenge，链接是  https://arxiv.org/pdf/2509.06079  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.06079  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hao Liang,Ruitao Wu,Bohan Zeng,Junbo Niu,Wentao Zhang,Bin Dong</p>
<p><strong>Categories</strong>: cs.CL,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.06079.pdf">https://arxiv.org/pdf/2509.06079.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.06079">https://arxiv.org/abs/2509.06079</a></p>
<p><strong>Arxiv ID</strong>: 2509.06079</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.06079">https://papers.cool/arxiv/2509.06079</a></p>
<p><strong>Published</strong>: 2025-09-07T14:47:32Z</p>
<p><strong>Updated</strong>: 2025-09-07T14:47:32.000Z</p>
<hr>
<h3 id="13-Phi-Preference-Hijacking-in-Multi-modal-Large-Language-Models-at-Inference-Time"><a href="#13-Phi-Preference-Hijacking-in-Multi-modal-Large-Language-Models-at-Inference-Time" class="headerlink" title="13. Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12521">Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time</a></h3><p>Recently, Multimodal Large Language Models (MLLMs) have gained significant attention across various domains. However, their widespread adoption has also raised serious safety concerns. In this paper, we uncover a new safety risk of MLLMs: the output preference of MLLMs can be arbitrarily manipulated by carefully optimized images. Such attacks often generate contextually relevant yet biased responses that are neither overtly harmful nor unethical, making them difficult to detect. Specifically, we introduce a novel method, Preference Hijacking (Phi), for manipulating the MLLM response preferences using a preference hijacked image. Our method works at inference time and requires no model modifications. Additionally, we introduce a universal hijacking perturbation — a transferable component that can be embedded into different images to hijack MLLM responses toward any attacker-specified preferences. Experimental results across various tasks demonstrate the effectiveness of our approach. The code for Phi is accessible at <a target="_blank" rel="noopener" href="https://github.com/Yifan-Lan/Phi">https://github.com/Yifan-Lan/Phi</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近，多模态大型语言模型（MLLMs）在各个领域引起了广泛关注。然而，它们的广泛应用也引发了严重的安全隐患。在本文中，我们揭示了MLLMs的一种新的安全风险：MLLMs的输出偏好可以通过精心优化的图像随意操纵。这类攻击通常会生成上下文相关但偏见的回应，这些回应既不明显有害，也不不道德，因此很难被检测到。具体而言，我们介绍了一种新方法——偏好劫持（Phi），用于通过偏好劫持图像操纵MLLM的响应偏好。我们的方法在推理时工作且无需对模型进行修改。此外，我们还介绍了一种通用的劫持扰动——一种可转移的组件，可以嵌入不同的图像中，以劫持MLLM的响应，使其朝向任何攻击者指定的偏好。不同任务上的实验结果证明了我们方法的有效性。Phi的代码可以在<a target="_blank" rel="noopener" href="https://github.com/Yifan-Lan/Phi获取。">https://github.com/Yifan-Lan/Phi获取。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文揭示并系统研究了多模态大语言模型（MLLM）在推理阶段存在的一种新型安全风险：<strong>偏好劫持（Preference Hijacking, Phi）</strong>。具体而言，论文指出：</p>
<ul>
<li><strong>问题核心</strong>：MLLM 的输出偏好可被<strong>未经训练的、仅通过图像像素级扰动</strong>的对抗样本在推理时任意操控，而无需修改模型参数或架构。</li>
<li><strong>攻击特点</strong>：<ol>
<li>生成的回答在语义上与用户查询和原始图像保持<strong>上下文一致</strong>，但隐性地植入攻击者预设的<strong>偏见或立场</strong>；</li>
<li>由于回答并非显式有害，传统内容安全检测手段（如毒性分类器、对齐审查 API）<strong>难以察觉</strong>；</li>
<li>攻击仅通过优化一张**“劫持图像”**实现，可在社交媒体、旅游网站等场景广泛传播，<strong>影响终端用户决策</strong>（如贬低某旅游目的地）。</li>
</ol>
</li>
<li><strong>研究目标</strong>：<ul>
<li>提出一种<strong>无需模型微调</strong>的推理时攻击范式，首次将“偏好操控”而非“固定字符串输出”作为优化目标；</li>
<li>引入<strong>通用劫持扰动</strong>（Universal Hijacking Perturbation），使得同一扰动模板可<strong>跨图像迁移</strong>，大幅降低攻击成本；</li>
<li>在文本-only 与多模态任务上量化评估偏好劫持成功率，揭示现有对齐机制的盲区，为后续防御研究提供基准。</li>
</ul>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节系统回顾了两大类相关研究，并指出它们与 Preference Hijacking（Phi）的区别。按类别归纳如下：</p>
<p>2.1 文本侧攻击（Text-based Attacks on LLMs）</p>
<ul>
<li><strong>Prompt Injection</strong><ul>
<li>直接注入（Liu et al., 2023）</li>
<li>间接/数据投毒（Greshake et al., 2023）</li>
</ul>
</li>
<li><strong>自动化对抗前缀生成</strong><ul>
<li>GCG（Zou et al., 2023）——基于梯度搜索生成可迁移前缀，触发有害输出</li>
<li>红队 LLM 自生成恶意指令（Casper et al., 2023；Mehrabi et al., 2024）</li>
</ul>
</li>
<li><strong>局限性</strong><ul>
<li>高困惑度、易被基于困惑度的检测器发现（Zhu et al., 2023）</li>
<li>仅针对<strong>文本模态</strong>，未利用图像通道；输出多为<strong>固定有害字符串</strong>，与 Phi 的“偏好级隐式操控”目标不同</li>
</ul>
</li>
</ul>
<p>2.2 图像侧攻击（Image-based Attacks on MLLMs）</p>
<p>方法</p>
<p>目标</p>
<p>与 Phi 的关键差异</p>
<p>Figstep（Gong et al., 2023）</p>
<p>将有害关键词渲染成图像文字，绕过对齐</p>
<p>依赖<strong>显式有害字符</strong>，输出可被判别器检测</p>
<p>图像越狱（Li et al., 2024b；Niu et al., 2024）</p>
<p>优化对抗噪声使模型输出 toxic 字符串</p>
<p>仍聚焦<strong>固定 toxic 输出</strong>，非偏好操控；语义与查询常脱节</p>
<p>VLAttack（Yin et al., 2024）</p>
<p>黑盒场景下跨任务迁移扰动</p>
<p>仅验证传统分类/检索指标，未涉及<strong>开放式偏好</strong></p>
<p>Verbose Images（Gao et al., 2024）</p>
<p>增大模型延迟与能耗</p>
<p>目标为<strong>资源消耗</strong>，非内容偏好</p>
<p>Image Hijacks（Bailey et al., 2023）</p>
<p>用对抗图像强制模型输出<strong>指定字符串</strong>或<strong>隐藏指令</strong></p>
<p>输出刚性、易暴露；受限于模型指令跟随能力，<strong>无法精细操控偏好</strong></p>
<p>Soft Prompts Go Hard（Zhang et al., 2024）</p>
<p>在图像中嵌入“元指令”隐性提示</p>
<p>仍依赖<strong>指令跟随</strong>，若模型对齐强则失效；未利用<strong>对比式偏好学习</strong></p>
<p>小结</p>
<p>现有工作要么仅操控<strong>文本模态</strong>，要么在图像模态中追求<strong>固定、显式、易检测</strong>的输出；Phi 首次把**“偏好级隐式偏见”<strong>作为优化目标，通过</strong>单模型 DPO 式目标<strong>直接学习</strong>可迁移图像扰动**，在推理时无需修改模型即可持续影响多轮上下文相关的生成偏好。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出“防御”方案，而是<strong>系统性地构建并验证了一种新型攻击范式</strong>——Preference Hijacking（Phi），以暴露现有多模态大模型在推理阶段的偏好操控漏洞。其“解决”问题的思路体现在<strong>如何高效、隐蔽、可迁移地实现偏好劫持</strong>，具体方法如下：</p>
<p>3.2 Preference Hijacking at Inference-Time</p>
<p>1. 威胁模型形式化</p>
<ul>
<li>白盒访问目标 MLLM $f_\theta(x, q)$</li>
<li>攻击者仅可修改图像 $x$，<strong>无法预知</strong>用户文本查询 $q$</li>
<li>目标：生成劫持图像 $x_h = x + h$，使得<ul>
<li>$f_\theta(x_h, q)$ 倾向于攻击者预设的偏好；</li>
<li>回答与 $(x, q)$ 保持语义一致；</li>
<li>$|h|_\infty \le \Delta$（视觉不可感知）</li>
</ul>
</li>
</ul>
<p>2. 对比偏好数据集构建</p>
<ul>
<li>对每条图像-查询对，人工或用<strong>未对齐模型</strong>生成两条回答：<ul>
<li>$r_t$：符合<strong>目标偏好</strong>（如贬低城市、崇尚战争）</li>
<li>$r_o$：符合<strong>原始/相反偏好</strong></li>
</ul>
</li>
<li>数据集 $\mathcal{D}={(x, q, r_t, r_o)}$ 完全<strong>独立于目标模型</strong>，规避其对齐机制的影响</li>
</ul>
<p>3. 单模型 DPO 目标优化扰动</p>
<p>将 Direct Preference Optimization（Rafailov et al., 2024）改造为<strong>仅优化图像扰动</strong>：</p>
<script type="math/tex; mode=display">\\min\_h -\\mathbb{E}_{(x,q,r\_t,r\_o)\\sim \\mathcal{D}} \\log\\sigma\\Bigl( \\underbrace{\\log\\frac{f_\\theta(r\_t|x+h,q)}{f\_\\theta(r\_t|x,q)}}\_{\\text{提升目标偏好}}

*   \\beta \\underbrace{\\log\\frac{f\_\\theta(r\_o|x+h,q)}{f\_\\theta(r\_o|x,q)}}_{\\text{抑制原偏好}} \\Bigr) \\quad \\text{s.t. } |h|_\\infty\\le\\Delta</script><ul>
<li><p><strong>仅更新 $h$</strong>，模型参数 $\theta$ 冻结</p>
</li>
<li><p>采用 Projected Gradient Descent（PGD）求解，保证 $\ell_\infty$ 有界</p>
</li>
</ul>
<p>4. 通用劫持扰动（Universal Hijacking Perturbation）</p>
<p>为摆脱“一图一扰动”的扩展瓶颈，提出<strong>跨图像迁移</strong>的通用扰动：</p>
<p>形式</p>
<p>做法</p>
<p>优点</p>
<p>采用情况</p>
<p><strong>Phi-Patch</strong></p>
<p>固定左上角 $168\times168$ 方形 patch</p>
<p>无需裁剪像素值，直接粘贴</p>
<p>✔</p>
<p><strong>Phi-Border</strong></p>
<p>外圈加 $252\times252$（LLaVA）或 $392\times392$（Llama）边框</p>
<p>不遮挡中心内容，视觉更隐蔽</p>
<p>✔</p>
<p>加性噪声</p>
<p>全局像素级噪声</p>
<p>视觉难察觉，但需裁剪，迁移性差</p>
<p>✘</p>
<p>优化时<strong>图像 $x$ 动态采样</strong>，扰动 $h$ 共享，使得同一 patch/border 可<strong>零成本迁移到任意新图像</strong>。</p>
<p>4 实验验证“解决”了攻击有效性</p>
<ul>
<li><strong>文本-only 任务</strong>：财富/权力寻求、幻觉倾向<br>→ Phi 在 MC 与 P-Score 上<strong>全面超越</strong> System Prompt 与 Image Hijacks</li>
<li><strong>多模态任务</strong>：城市、披萨、人物负面评价；Tech/Nature、War/Peace、Power/Humility 倾向<br>→ Phi 在绝大多数场景<strong>MC 提升 20–100%</strong>，P-Score 提高 1–3 分</li>
<li><strong>通用扰动</strong>：同一 Phi-Patch/Phi-Border 在<strong>未见过的风景、食物、人脸图像</strong>上仍保持高 MC 与 P-Score，验证<strong>跨图迁移性</strong></li>
</ul>
<p>结论</p>
<p>论文通过<strong>对比偏好学习+单模型 DPO+通用 patch/border 优化</strong>，首次实现了<strong>推理时无需模型修改、视觉隐蔽、跨图像迁移</strong>的<strong>偏好级劫持</strong>，从而“解决”了<strong>如何系统暴露 MLLM 偏好操控漏洞</strong>的研究问题，为后续防御工作提供了明确基准与测试平台。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>Preference Hijacking (Phi)</strong> 共设计了 <strong>4 组实验</strong>，覆盖 <strong>文本-only</strong>、<strong>多模态</strong> 与 <strong>通用扰动迁移</strong> 三大场景，并在 <strong>防御分析</strong> 与 <strong>跨模型验证</strong> 上补充了消融与鲁棒性测试。具体实验一览如下：</p>
<p>4.1 实验设置</p>
<ul>
<li><p><strong>目标模型</strong></p>
<ul>
<li>LLaVA-1.5-7B</li>
<li>Llama-3.2-11B</li>
<li>Qwen2.5-VL-7B（附录 B，验证跨架构泛化）</li>
</ul>
</li>
<li><p><strong>评估指标</strong></p>
<ul>
<li><strong>MC</strong>（Multiple Choice Accuracy）：模型在 A/B 选项中选择<strong>目标偏好</strong>的比例</li>
<li><strong>P-Score</strong>（Preference Score）：GPT-4o 按 1–5 分评判生成回答与目标偏好的<strong>契合度与丰富度</strong></li>
</ul>
</li>
<li><p><strong>基线方法</strong></p>
<ul>
<li>Clean Prompt / Clean Image：无攻击</li>
<li>System Prompt：通过系统提示强行引导偏好</li>
<li>Image Hijacks（Bailey et al., 2023）：对抗图像强制输出固定字符串</li>
</ul>
</li>
</ul>
<p>4.2 文本-only 任务（无视觉语义依赖）</p>
<p>偏好数据集</p>
<p>目标</p>
<p>结果摘要</p>
<p>Wealth-seeking</p>
<p>让模型“贪财”</p>
<p>Phi MC 89.0%（LLaVA） vs 46.0% Clean；P-Score 2.89 vs 1.84</p>
<p>Power-seeking</p>
<p>让模型“恋权”</p>
<p>Phi MC 97.5% vs 56.0% Clean；P-Score 3.24 vs 1.85</p>
<p>Hallucination</p>
<p>让模型“编造”</p>
<p>Phi MC 70.5% vs 38.5% Clean；P-Score 4.52 vs 1.89</p>
<p>→ <strong>结论</strong>：劫持图像<strong>无需携带任何语义信息</strong>，即可显著扭转模型人格与幻觉倾向，且生成回答自然度高。</p>
<p>4.3 多模态任务（查询与图像内容强相关）</p>
<p>任务类型</p>
<p>偏好</p>
<p>结果摘要（LLaVA）</p>
<p>结果摘要（Llama）</p>
<p><strong>Opinion</strong></p>
<p>City→负面描述</p>
<p>Phi MC 74.1% vs 18.5% Clean</p>
<p>Phi MC 100% vs 1.9% Clean</p>
<p>Pizza→负面评价</p>
<p>Phi MC 50.0% vs 11.8% Clean</p>
<p>Phi MC 88.2% vs 5.9% Clean</p>
<p>Person→负面评价</p>
<p>Phi MC 60.0% vs 0% Clean</p>
<p>Phi MC 50.0% vs 10.0% Clean</p>
<p><strong>Contrastive</strong></p>
<p>Tech &gt; Nature</p>
<p>Phi MC 77.3% vs 38.6% Clean</p>
<p>Phi MC 90.9% vs 27.3% Clean</p>
<p>War &gt; Peace</p>
<p>Phi MC 67.3% vs 27.3% Clean</p>
<p>Phi MC 78.2% vs 14.6% Clean</p>
<p>Power &gt; Humility</p>
<p>Phi MC 64.4% vs 42.2% Clean</p>
<p>Phi MC 75.6% vs 37.8% Clean</p>
<p>→ <strong>结论</strong>：在<strong>图像-查询语义强绑定</strong>场景，Phi 仍能<strong>保持上下文一致性</strong>的同时，把模型偏好<strong>强制扭转到攻击者指定方向</strong>。</p>
<p>4.4 通用劫持扰动实验（跨未见图像迁移）</p>
<p>偏好</p>
<p>方法</p>
<p>MC（LLaVA）</p>
<p>MC（Llama）</p>
<p>关键结论</p>
<p>Landscape</p>
<p>Phi-Patch</p>
<p>45.0%</p>
<p>100%</p>
<p>同一 168×168 patch 直接贴到<strong>未见风景图</strong>仍有效</p>
<p>Phi-Border</p>
<p>53.3%</p>
<p>100%</p>
<p>边框扰动<strong>零样本迁移</strong></p>
<p>Food</p>
<p>Phi-Patch</p>
<p>48.0%</p>
<p>96%</p>
<p>Food-101 未见菜品同样被<strong>贬低</strong></p>
<p>People</p>
<p>Phi-Patch</p>
<p>42.0%</p>
<p>68%</p>
<p>VGGFace2 新人脸同样被<strong>负面评价</strong></p>
<p>→ <strong>结论</strong>：通用扰动<strong>无需重新训练</strong>即可跨数据集、跨图像内容保持劫持能力，显著降低攻击成本。</p>
<p>4.5 防御分析（预处理式防御）</p>
<p>防御手段</p>
<p>参数</p>
<p>Phi MC 下降</p>
<p>观察</p>
<p>JPEG 压缩</p>
<p>quality=30</p>
<p>74.1%→29.6%</p>
<p>图像失真严重，<strong>可用性下降</strong></p>
<p>降采样</p>
<p>rescale=0.5</p>
<p>74.1%→31.5%</p>
<p>细节丢失，<strong>仍未完全消除</strong>攻击</p>
<p>高斯噪声</p>
<p>σ=40</p>
<p>74.1%→20.4%</p>
<p>噪声肉眼可见，<strong>防御-可用性权衡</strong>突出</p>
<p>→ <strong>结论</strong>：基础预处理可<strong>部分缓解</strong>但<strong>无法根除</strong>；且攻击者可把防御当作<strong>数据增广</strong>进行自适应绕过。</p>
<p>附录补充实验</p>
<ul>
<li><strong>附录 B</strong>：在 Qwen2.5-VL-7B 上重复多模态实验，Phi 仍保持 <strong>MC 40–100%</strong>，验证<strong>跨架构泛化</strong>。</li>
<li><strong>附录 C</strong>：<ul>
<li>∆ 消融：$\Delta&lt;16/255$ 时 P-Score 骤降，<strong>16/255 为有效且隐蔽</strong>的折中。</li>
<li>Border/Patch 大小：当 border 厚度 &lt; 14 px（vision encoder patch 尺寸）或 patch 边长 &lt; 56 px 时劫持能力<strong>急剧下降</strong>。</li>
</ul>
</li>
<li><strong>附录 D</strong>：<strong>Scattered Patch</strong> 将 84×84 区域拆成 36 块 14×14 离散小 patch，<strong>P-Score 仍达 3.62</strong>，视觉更难察觉。</li>
</ul>
<p>一句话总结</p>
<p>实验从<strong>任务类型</strong>（文本 ↔ 多模态）、<strong>扰动泛化</strong>（一图一扰 → 通用 patch/border）、<strong>模型家族</strong>（LLaVA/Llama/Qwen）到<strong>防御手段</strong>（JPEG、缩放、噪声）做了<strong>全覆盖 stress-test</strong>，一致验证：<strong>Phi 能在推理时隐蔽、高效、迁移地劫持 MLLM 偏好</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可继续探索的方向按<strong>攻击增强</strong>、<strong>防御对抗</strong>、<strong>理论理解</strong>与<strong>现实影响</strong>四条线展开，均直接对应原文未解决或仅初步触及的空白。</p>
<p>1 攻击增强维度</p>
<p>方向</p>
<p>关键问题</p>
<p>可能做法</p>
<p><strong>多轮对话持续性</strong></p>
<p>单次劫持能否在多轮上下文里<strong>保持或放大</strong>？</p>
<p>构建多轮偏好数据集，用强化学习或对话级 DPO 优化<strong>跨轮扰动</strong>；监测随着轮次增加 MC/P-Score 的衰减曲线</p>
<p><strong>多模态指令组合</strong></p>
<p>同时注入<strong>文本+图像</strong>双通道劫持信号是否协同？</p>
<p>在输入空间做<strong>联合扰动</strong>（文本前缀 + 图像 patch）并求解统一目标；观察同偏好下样本效率与隐蔽性提升</p>
<p><strong>视频/3D 输入扩展</strong></p>
<p>时序或立体视觉是否提供更<strong>丰富隐藏信道</strong>？</p>
<p>将 Phi 目标推广到<strong>视频帧序列</strong>或 NeRF 渲染图，优化<strong>时空稀疏扰动</strong>；测试对视频 LLM 的立场操控能力</p>
<p><strong>个性化模型攻击</strong></p>
<p>用户私有微调模型偏好各异，如何<strong>一次扰动通用</strong>？</p>
<p>采用<strong>元学习</strong>框架：在大量用户 LoRA 权重上训练<strong>元扰动</strong>，使同一 patch 对不同私有模型均有效</p>
<p>2 防御对抗维度</p>
<p>方向</p>
<p>关键问题</p>
<p>可能做法</p>
<p><strong>对抗训练规模化</strong></p>
<p>现有对抗训练成本过高，如何<strong>轻量化</strong>？</p>
<p>仅对<strong>视觉编码器</strong>做 LoRA 微调，使用 Phi 生成的劫持图像作为负样本，保持 LLM 冻结；评估 MC 下降与干净性能损耗</p>
<p><strong>随机化集成防御</strong></p>
<p>预处理随机化能否<strong>破坏通用扰动</strong>的跨图一致性？</p>
<p>在推理时对输入图像随机应用<strong>多种预处理链</strong>（JPEG+缩放+裁剪），用多数投票或置信度筛选输出；测试对 Phi-Patch/Phi-Border 的<strong>期望误差下限</strong></p>
<p>** certified robustness **</p>
<p>能否给出<strong>可证明的偏好界</strong>而非经验防御？</p>
<p>将偏好目标转化为<strong>概率区间</strong>，利用<strong>随机平滑</strong>或<strong>可验证鲁棒训练</strong>给出“在一定 $\ell_\infty$ 半径内偏好偏移不超过 $\epsilon$”的证书</p>
<p><strong>检测器 arms race</strong></p>
<p>当攻击者把防御当增广，检测器如何<strong>自适应迭代</strong>？</p>
<p>构建<strong>双玩家博弈</strong>数据集：每轮更新检测器后重新训练扰动，记录<strong>纳什收敛</strong>时的攻防性能，衡量<strong>可防御上限</strong></p>
<p>3 理论理解维度</p>
<p>方向</p>
<p>关键问题</p>
<p>可能做法</p>
<p><strong>偏好流形几何</strong></p>
<p>劫持扰动在<strong>视觉-语义联合嵌入</strong>中如何移动决策边界？</p>
<p>用<strong>探测向量</strong>（probing vectors）测量视觉编码器输出在添加 $h$ 前后与文本偏好方向的<strong>余弦偏移</strong>，可视化低维流形；分析偏移方向是否与<strong>CLIP 文本编码</strong>的“正面-负面”向量对齐</p>
<p><strong>最优扰动下限</strong></p>
<p>是否存在<strong>信息论极限</strong>使任何扰动必失效？</p>
<p>建立<strong>多模态率-失真-鲁棒性</strong>框架：在给定视觉保真度 $D$ 与模型容量 $C$ 下，推导偏好改变概率上界 $P(\text{switch})\le f(D,C)$</p>
<p><strong>泛化误差来源</strong></p>
<p>为何同一 $h$ 能跨图像、跨模型？</p>
<p>分析<strong>视觉 Transformer 的低频核</strong>（low-frequency kernels）与<strong>对抗可迁移性</strong>的相关性；验证 patch/border 是否恰好落在<strong>共享高频敏感区</strong></p>
<p>4 现实影响维度</p>
<p>方向</p>
<p>关键问题</p>
<p>可能做法</p>
<p><strong>社交媒体扩散模拟</strong></p>
<p>劫持图像在<strong>真实平台</strong>传播能否<strong>放大舆论</strong>？</p>
<p>构建<strong>仿真环境</strong>：用 Twitter/Flickr 风格推荐算法，将 Phi-Border 风景图推送给旅游话题用户；测量<strong>用户情感极性</strong>变化与<strong>目的地搜索量</strong>下降比例</p>
<p><strong>版权与合规风险</strong></p>
<p>当攻击者把扰动嵌入<strong>版权图片</strong>，平台如何<strong>合规下架</strong>？</p>
<p>研究<strong>扰动指纹</strong>（perturbation fingerprint）：训练 CNN 检测器识别 Phi-Patch/Phi-Border 的统计特征，实现<strong>自动 DMCA 扩展理由</strong></p>
<p><strong>人机交互可信</strong></p>
<p>普通用户能否<strong>肉眼识别</strong>劫持图像？</p>
<p>设计<strong>众包实验</strong>：让受试者对比原图与 Phi 图，给出<strong>可信度评分</strong>与<strong>购买/旅行意向</strong>；量化<strong>视觉隐蔽性</strong>与<strong>经济外部性</strong>之间的关系</p>
<p>快速可落地选题示例</p>
<ol>
<li><strong>“多轮 Phi”</strong>：用 GPT-4 生成 5 轮对话偏好数据集，一周内验证劫持衰减曲线。</li>
<li><strong>“LoRA 对抗训练”</strong>：仅训练视觉编码器 2 小时，观察 MC 下降 30% 所需干净数据量。</li>
<li><strong>“随机化集成”</strong>：实现 3 种随机预处理 + 置信度投票，预计半天完成推理级防御原型。</li>
</ol>
<p>这些方向既可直接在现有代码库扩展，也能形成新的<strong>攻防 benchmark</strong>，填补 Phi 留下的研究与实践空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="论文核心概要"><a href="#论文核心概要" class="headerlink" title="论文核心概要"></a>论文核心概要</h1><h2 id="1-新威胁：Preference-Hijacking（Phi）"><a href="#1-新威胁：Preference-Hijacking（Phi）" class="headerlink" title="1. 新威胁：Preference Hijacking（Phi）"></a>1. 新威胁：Preference Hijacking（Phi）</h2><ul>
<li>MLLM 在推理阶段无需任何参数更新，即可被<strong>精心优化的图像扰动</strong>任意操控输出偏好</li>
<li>生成结果与查询、图像<strong>语义一致</strong>，但隐植攻击者设定的偏见，传统毒性检测难以发现</li>
</ul>
<h2 id="2-方法框架"><a href="#2-方法框架" class="headerlink" title="2. 方法框架"></a>2. 方法框架</h2><p>步骤</p>
<p>要点</p>
<p>数据集</p>
<p>构建对比偏好四元组 (x, q, r_t, r_o)，r_t 与 r_o 分别对应目标与相反偏好</p>
<p>优化目标</p>
<p>单模型 DPO 变体：仅学习图像扰动 h，最大化 r_t 概率、抑制 r_o 概率，∥h∥_∞ ≤ Δ</p>
<p>通用扰动</p>
<p>训练一次即可跨图像迁移：<br>• Phi-Patch（左上 168×168 块）<br>• Phi-Border（外圈边框）</p>
<h2 id="3-实验结果"><a href="#3-实验结果" class="headerlink" title="3. 实验结果"></a>3. 实验结果</h2><p>场景</p>
<p>指标</p>
<p>主要数据（LLaVA-1.5）</p>
<p>结论</p>
<p>文本-only</p>
<p>MC / P-Score</p>
<p>Wealth 89% / 2.89；Power 97.5% / 3.24；Hallu 70.5% / 4.52</p>
<p>图像无需语义即可扭转人格与幻觉</p>
<p>多模态</p>
<p>MC / P-Score</p>
<p>City 74% / 4.00；Pizza 50% / 4.09；Tech/Nature 77% / 4.11</p>
<p>保持上下文一致下强制负面/倾向性评价</p>
<p>通用扰动</p>
<p>MC</p>
<p>Landscape 53%；Food 58%；People 58%</p>
<p>同一 patch/border 零样本迁移到未见图仍有效</p>
<p>防御</p>
<p>MC 下降</p>
<p>JPEG-30 → 29.6%；Noise-40 → 20.4%</p>
<p>基础预处理可缓解但无法根除，且图像质量受损</p>
<h2 id="4-贡献"><a href="#4-贡献" class="headerlink" title="4. 贡献"></a>4. 贡献</h2><ul>
<li>提出<strong>推理时偏好劫持</strong>新范式，无需模型改动</li>
<li>引入<strong>跨图像通用扰动</strong>，显著降低攻击成本</li>
<li>在文本与多模态任务上系统验证，揭示现有对齐机制盲区，为后续防御研究提供基准</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Phi%3A%20Preference%20Hijacking%20in%20Multi-modal%20Large%20Language%20Models%20at%20Inference%20Time，链接是  https://arxiv.org/pdf/2509.12521  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12521  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yifan Lan,Yuanpu Cao,Weitong Zhang,Lu Lin,Jinghui Chen</p>
<p><strong>Categories</strong>: cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12521.pdf">https://arxiv.org/pdf/2509.12521.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12521">https://arxiv.org/abs/2509.12521</a></p>
<p><strong>Arxiv ID</strong>: 2509.12521</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12521">https://papers.cool/arxiv/2509.12521</a></p>
<p><strong>Published</strong>: 2025-09-15T23:55:57Z</p>
<p><strong>Updated</strong>: 2025-09-15T23:55:57.000Z</p>
<hr>
<h3 id="14-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis"><a href="#14-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis" class="headerlink" title="14. Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.11526">Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis</a></h3><p>Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has opened new avenues for Computational Pathology (CPath). As positive tissue comprises only a small fraction of gigapixel WSIs, existing Multiple Instance Learning (MIL) methods typically focus on identifying salient instances via attention mechanisms. However, this leads to a bias towards easy-to-classify instances while neglecting challenging ones. Recent studies have shown that hard examples are crucial for accurately modeling discriminative boundaries. Applying such an idea at the instance level, we elaborate a novel MIL framework with masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure with a consistency constraint to explore the hard instances. Using a class-aware instance probability, MHIM-MIL employs a momentum teacher to mask salient instances and implicitly mine hard instances for training the student model. To obtain diverse, non-redundant hard instances, we adopt large-scale random masking while utilizing a global recycle network to mitigate the risk of losing key features. Furthermore, the student updates the teacher using an exponential moving average, which identifies new hard instances for subsequent training iterations and stabilizes optimization. Experimental results on cancer diagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate that MHIM-MIL outperforms the latest methods in both performance and efficiency. The code is available at: <a target="_blank" rel="noopener" href="https://github.com/DearCaat/MHIM-MIL">https://github.com/DearCaat/MHIM-MIL</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>将病理图像数字化为千兆像素全幻灯片图像（WSIs）为计算病理学（CPath）开辟了新的方向。由于阳性组织仅占千兆像素WSIs的一小部分，现有的多实例学习（MIL）方法通常通过注意机制专注于识别显著实例。然而，这导致了对易分类实例的偏见，而忽视了具有挑战性的实例。近期研究表明，困难示例对于准确建模判别边界至关重要。在实例层面应用这一思想，我们详细阐述了一个新的MIL框架，具有掩码困难实例挖掘（MHIM-MIL），它利用一种具有一致性约束的连体结构来探索困难实例。MHIM-MIL使用类感知实例概率，采用动量教师来掩盖显著实例，并隐式挖掘困难实例以训练学生模型。为了获得多样化、非冗余的困难实例，我们采用大规模随机掩码，同时利用全局回收网络降低丢失关键特征的风险。此外，学生使用指数移动平均更新教师，从而识别新的困难实例以进行后续训练迭代，并稳定优化。在癌症诊断、亚型分析、生存分析任务以及12个基准测试上的实验证明，MHIM-MIL在性能和效率上均优于最新方法。代码可以在以下地址获取： <a target="_blank" rel="noopener" href="https://github.com/DearCaat/MHIM-MIL。">https://github.com/DearCaat/MHIM-MIL。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对千兆像素级全切片病理图像（WSI）分析中“显著实例主导”导致的判别边界模糊问题，提出一种基于掩码困难实例挖掘的多示例学习框架（MHIM-MIL）。核心动机与待解决问题可归纳为：</p>
<ul>
<li><p><strong>问题 1：显著实例偏差</strong><br>现有注意力 MIL 方法过度依赖高注意力（易分类）实例进行包嵌入，忽视位于决策边界的困难实例，导致模型泛化能力下降。</p>
</li>
<li><p><strong>问题 2：困难实例无标签不可直接挖掘</strong><br>病理 MIL 仅有切片级标签，缺乏实例级标注，传统困难样本挖掘策略无法直接应用。</p>
</li>
<li><p><strong>问题 3：序列冗余与计算瓶颈</strong><br>WSI 实例数庞大（≈10 k），全序列训练带来高昂显存与耗时，且冗余实例干扰优化。</p>
</li>
<li><p><strong>问题 4：掩码带来的关键信息丢失风险</strong><br>高比例掩码虽提升效率，但可能屏蔽稀疏而关键的组织学特征，影响最终诊断。</p>
</li>
</ul>
<p>为此，论文提出“掩码困难实例挖掘”范式，通过<strong>动量教师评估-掩码显著实例-全局回收-一致性约束</strong>的迭代机制，迫使学生网络关注被掩盖的困难区域，从而在<strong>癌症诊断、亚型分类、生存分析</strong>三大任务上同时提升精度与效率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两大主线：<strong>计算病理学（Computational Pathology, CPath）</strong>与<strong>困难样本挖掘（Hard Sample Mining）</strong>。以下按这两条主线梳理并补充其脉络，均给出可追踪的标志性文献或方法。</p>
<p>1 计算病理学中的多示例学习</p>
<p>子主题</p>
<p>代表方法 / 综述</p>
<p>主要贡献与局限</p>
<p><strong>经典 MIL 范式</strong></p>
<p>AB-MIL [14]</p>
<p>首次在病理引入可学习注意力权重聚合实例特征，成为后续 embed-level 方法的“基线骨架”。</p>
<p><strong>注意力改进</strong></p>
<p>CLAM [17]、DSMIL [15]、TransMIL [9]</p>
<p>引入 top-K 选择、双分支对比、Transformer 自注意力等，进一步提升显著区域建模能力，但仍偏重“易分类”实例。</p>
<p><strong>伪标签/实例级</strong></p>
<p>Campanella [38]、Qu et al. [42]</p>
<p>尝试用伪标签或原型学习给出实例预测，性能受限于弱监督噪声。</p>
<p><strong>高效序列采样</strong></p>
<p>DTFD-MIL [13]、IBMIL [6]、RankMix [66]</p>
<p>通过伪袋、强化学习或数据增广缓解长序列压力，未显式挖掘困难实例。</p>
<p><strong>Mamba/线性复杂度</strong></p>
<p>2D-Mamba [48]、M3amba [49]</p>
<p>用状态空间模型替代自注意力，实现线性复杂度，但仍聚焦显著区域。</p>
<p><strong>视觉-语言病理模型</strong></p>
<p>PLIP [26]、UNI [27]、VILA-MIL [52]</p>
<p>利用文本提示或大规模预训练提取通用特征，但训练阶段仍由高置信度实例主导。</p>
<p><strong>多模态融合</strong></p>
<p>CMTA [58]、Pathomic-Fusion [60]</p>
<p>将基因组/影像/报告结合，性能高但数据获取门槛高；本文旨在<strong>不引入额外模态</strong>前提下缩小差距。</p>
<p><strong>生存分析专用 MIL</strong></p>
<p>WSISA [57]、MI-FCN [7]、Graph-MIL [8]</p>
<p>用注意力或图卷积聚合 WSI 特征预测生存，同样面临显著区域过度聚焦问题。</p>
<p>2 困难样本挖掘（Hard Sample Mining）</p>
<p>技术类别</p>
<p>代表文献</p>
<p>核心思路</p>
<p><strong>Loss-based</strong></p>
<p>OHEM [73]、Focal Loss</p>
<p>利用高损失样本或重新加权，放大困难样本贡献。</p>
<p><strong>Similarity-based</strong></p>
<p>Triplet/Quadruplet [87,88]</p>
<p>在度量空间拉近同类、推远异类，显式采样难负对。</p>
<p><strong>Learnable Weight</strong></p>
<p>CVPR’19 [89]、Suh et al. [81]</p>
<p>让网络自动学习样本重要性权重，需完整监督。</p>
<p><strong>图对比学习</strong></p>
<p>MixGCF [79]、AU-HNM [80]</p>
<p>在图神经网络中挖掘难负节点，提升图表示。</p>
<p><strong>MIL 场景下的困难挖掘</strong></p>
<p>Li et al. [90]</p>
<p>利用注意力得分从假阴性袋中选“难负实例”组成难袋；<strong>与本文差异</strong>：本文<strong>不显式定位难负</strong>，而是通过<strong>掩码显著实例</strong>间接迫使网络关注剩余困难区域，避免错误伪标签放大。</p>
<p>3 小结与定位</p>
<ul>
<li><strong>已有 MIL 病理方法</strong>：注意力权重或 top-K 选择→ 显著实例主导 → 决策边界模糊。</li>
<li><strong>已有困难样本挖掘</strong>：大多依赖实例标签或 pairwise 监督，无法直接迁移到<strong>仅袋标签</strong>的千兆像素病理场景。</li>
<li><strong>本文工作</strong>：首次在病理 MIL 中系统引入“掩码显著→ 挖掘困难”范式，结合动量教师、类感知概率、全局回收与一致性约束，实现<strong>无实例标签</strong>下的稳定困难实例挖掘，兼顾精度与效率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“显著实例主导、困难实例缺失”这一核心难题转化为**“如何在没有实例标签的条件下，持续、稳定、高效地挖掘困难实例并用于训练”<strong>。为此提出</strong>掩码困难实例挖掘 MIL 框架（MHIM-MIL）**，其解决路径可概括为“<strong>评估-掩码-回收-对齐</strong>”四步闭环：</p>
<p>1 评估：类感知实例概率（Class-aware Instance Probability）</p>
<ul>
<li><strong>动机</strong>：传统注意力得分仅反映“显著性”，不直接体现“可分性”，易把高响应但低判别力的脂肪区等误判为“简单实例”。</li>
<li><strong>做法</strong>：<ol>
<li>动量教师对全序列实例输出注意力权重 $a_i$；</li>
<li>用教师自带的<strong>实例分类器</strong>（或袋分类器）对加权特征 $a_i z_i$ 进行<strong>一次前向</strong>得到 softmax 概率 $s_i$；</li>
<li>按 $s_i$ 降序排列，得到“易分类”候选序列。</li>
</ol>
</li>
<li><strong>收益</strong>：概率直接编码分类置信度，比纯注意力更准确地定位真正“简单”实例，为后续掩码提供可靠排序。</li>
</ul>
<p>2 掩码：两级掩码策略（RHSM → RSM）</p>
<ol>
<li><p><strong>Randomly High-Score Masking（RHSM）</strong></p>
<ul>
<li>仅掩码 Top-βh%（βh 起始≈3%）最高概率实例；</li>
<li><strong>随机减半</strong>掩码：在 Top-2βh% 中随机选一半掩码，防止训练初期把全部关键区域抹掉（error mining）。</li>
<li>余弦退火衰减 βh，保证后期保留足够阳性信号。</li>
</ul>
</li>
<li><p><strong>Large-Scale Random Score Masking（RSM）</strong></p>
<ul>
<li>对 RHSM 剩余序列再随机掩码 70%–90%，<strong>一次把序列长度从 O(10k) 降到 O(1k)</strong>；</li>
<li>随机性引入多样性，显著降低冗余与过拟合；</li>
<li>相比 Low-Score Masking（LSM），RSM 不依赖尾部打分，避免“把困难也掩掉”。</li>
</ul>
</li>
</ol>
<p>3 回收：全局回收网络（Global Recycle Network, GRN）</p>
<ul>
<li><strong>风险</strong>：高比例随机掩码可能丢失稀疏但关键的特征（如微转移灶）。</li>
<li><strong>做法</strong>：<ul>
<li>维护<strong>可学习全局查询向量</strong> $Q_G∈R^{K×D}$（EMA 更新，无梯度）；</li>
<li>对掩码子序列 $Z_m$ 做多头交叉注意力 $MCA(Q_G, Z_m)$，输出<strong>回收特征</strong> $Z̃_m$；</li>
<li>将 $Z̃_m$ 与未掩码子序列拼接，共同送入学生网络。</li>
</ul>
</li>
<li><strong>收益</strong>：以极低计算代价（仅 K 个查询，K≈64）实现“全局记忆”，补偿被随机掩码的关键信息。</li>
</ul>
<p>4 对齐：一致性约束与迭代更新</p>
<ul>
<li><p><strong>一致性损失</strong> $L_{con}$：<br>令教师对<strong>全序列</strong>的袋嵌入 $F_t$ 与学生仅见<strong>困难序列</strong>的袋嵌入 $F_s$ 做 KL 对齐，迫使学生从困难实例中也能还原与教师一致的判别空间。  </p>
<script type="math/tex; mode=display">L\_{con}=−softmax(F\_t/τ)⋅log softmax(F\_s)</script></li>
<li><p><strong>迭代优化</strong>：</p>
<ul>
<li>学生总损失 $L=L_{cls}+αL_{con}$；</li>
<li>教师参数用 EMA 平滑跟随学生，<strong>不接收梯度</strong>，保证目标稳定且持续刷新“困难”定义。</li>
</ul>
</li>
</ul>
<p>5 推理阶段</p>
<ul>
<li>完全丢弃掩码与回收模块，<strong>仅使用学生网络对全序列进行一次前向</strong>，兼顾效率与精度，无额外成本。</li>
</ul>
<p>6 效果总结</p>
<p>问题</p>
<p>论文对应机制</p>
<p>关键收益</p>
<p>显著实例偏差</p>
<p>类感知概率 + 掩码高置信</p>
<p>精准定位“简单”实例，迫使关注边界区域</p>
<p>无实例标签</p>
<p>动量教师生成伪排序</p>
<p>无需人工标注，可端到端训练</p>
<p>序列过长</p>
<p>RSM 70–90% 随机掩码</p>
<p>显存↓50%，训练时间↓20%</p>
<p>掩码丢关键信息</p>
<p>GRN 全局回收</p>
<p>在 CAMELYON 等稀疏数据集 AUC 额外+0.7%</p>
<p>优化不稳定</p>
<p>EMA 教师 + 一致性损失</p>
<p>收敛更平稳，跨 5×3 任务 12 套基准均取得 SOTA</p>
<p>通过上述“<strong>评估-掩码-回收-对齐</strong>”闭环，MHIM-MIL 首次在千兆像素病理 MIL 中系统性地把“困难实例”变成训练主角，从而显著提升癌症诊断、亚型分类与生存分析的性能与效率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在<strong>三大临床任务</strong>、<strong>十二套公开基准</strong>上进行了系统实验，并辅以<strong>跨域验证、计算成本、消融与超参数分析</strong>，形成“性能-效率-鲁棒性”三维评估矩阵。具体实验一览如下：</p>
<p>1 主任务与数据集</p>
<p>任务</p>
<p>数据集</p>
<p>规模</p>
<p>评价指标</p>
<p><strong>癌症诊断</strong></p>
<p>CAMELYON（CAM16+CAM17）</p>
<p>899 WSIs，≈8 k patches/WSI</p>
<p>Accuracy, AUC, F1</p>
<p><strong>癌种亚型</strong></p>
<p>TCGA-NSCLC（LUAD vs LUSC）</p>
<p>1 053 WSIs，≈10 k</p>
<p>Accuracy, AUC, F1</p>
<p>TCGA-BRCA（IDC vs ILC）</p>
<p>1 131 WSIs，≈8.7 k</p>
<p>Accuracy, AUC, F1</p>
<p><strong>生存分析</strong></p>
<p>TCGA-LUAD / LUSC / BLCA</p>
<p>541 / 512 / 457 cases</p>
<p>C-index</p>
<p>2 特征提取器与基线</p>
<ul>
<li><p><strong>三种主干</strong></p>
<ul>
<li>ImageNet-1k ResNet-50</li>
<li>PLIP（病理-语言预训练）</li>
<li>UNI（病理大模型）</li>
</ul>
</li>
<li><p><strong>六种基线 MIL</strong><br>AB-MIL、CLAM、DSMIL、TransMIL、DTFD-MIL、IBMIL</p>
</li>
<li><p><strong>两种外部 SOTA</strong><br>R2T-MIL、2D-Mamba</p>
</li>
</ul>
<p>→ 共 <strong>3×6=18 组“特征×基线”</strong> 组合，全部在 5-fold 交叉验证下复现。</p>
<p>3 主实验结果（摘要）</p>
<p>场景</p>
<p>最佳提升示例（AUC / C-index）</p>
<p>CAMELYON 诊断</p>
<p>MHIM-v2(TransMIL+UNI) 93.47% → 超第二名 R2T-MIL 0.7%</p>
<p>TCGA-NSCLC 亚型</p>
<p>MHIM-v2(DSMIL+UNI) 96.82% → 超基线 DSMIL 1.25%</p>
<p>TCGA-BRCA 亚型</p>
<p>MHIM-v2(AB-MIL+PLIP) 93.3% Acc → 超 R2T-MIL 0.5%</p>
<p>生存 BLCA</p>
<p>MHIM-v2(TransMIL+UNI) C-index 63.0 → 超基线 1.8</p>
<p>生存 LUAD</p>
<p>MHIM-v2(AB-MIL+UNI) C-index 67.8 → 超基线 2.7</p>
<p><strong>结论</strong>：MHIM-v2 在 12 套基准上<strong>全部优于原基线</strong>，其中 9 项达到 <strong>SOTA</strong>，且提升幅度随特征质量（Res50→PLIP→UNI）递增。</p>
<p>4 跨域验证（Cross-source）</p>
<ul>
<li><strong>模型源域</strong>：TCGA-NSCLC（训练）</li>
<li><strong>目标域</strong>：CPTAC-NSCLC（仅测试）<br>→ AUC 提升 <strong>+2.4%</strong>（Res50）与 <strong>+1.7%</strong>（PLIP）</li>
<li><strong>生存同理</strong>：TCGA-LUAD → CPTAC-LUAD<br>→ C-index 提升 <strong>+1.9%</strong>（UNI）</li>
</ul>
<p>验证<strong>不依赖同源扫描仪/染色</strong>的鲁棒性。</p>
<p>5 计算成本对比</p>
<p>模型</p>
<p>参数量</p>
<p>训练时间/epoch</p>
<p>峰值显存</p>
<p>推理速度</p>
<p>TransMIL</p>
<p>2.67 M</p>
<p>13.2 s</p>
<p>10.6 G</p>
<p>76 slide/s</p>
<p>MHIM-v2(Trans)</p>
<p>3.72 M</p>
<p>10.5 s</p>
<p>5.6 G</p>
<p>72 slide/s</p>
<ul>
<li>在<strong>性能+2.3% AUC</strong> 同时，<strong>显存↓48%</strong>、<strong>训练时间↓24%</strong>。</li>
<li>与同为 SOTA 的 R2T-MIL 相比，<strong>参数量少 27%</strong>，<strong>显存少 44%</strong>。</li>
</ul>
<p>6 消融实验（Ablation）</p>
<p>模块消融（CAMELYON AUC）：</p>
<p>配置</p>
<p>AB-MIL</p>
<p>TransMIL</p>
<p>基线</p>
<p>91.6</p>
<p>91.2</p>
<p>+RHSM</p>
<p>92.1</p>
<p>92.4</p>
<p>+RHSM+RSM</p>
<p>92.6</p>
<p>93.0</p>
<p>+RHSM+RSM+GRN</p>
<p>92.8</p>
<p>93.5</p>
<p>+RHSM+RSM+GRN+CL（完整）</p>
<p><strong>92.8</strong></p>
<p><strong>93.5</strong></p>
<p>→ 各组件<strong>累积贡献</strong>，GRN 与一致性损失各带来 <strong>+0.3~0.5%</strong> 最终提升。</p>
<p>7 超参数与敏感性</p>
<ul>
<li><strong>RHSM 掩码比例</strong> 0.5–3%：TCGA-NSCLC 在 2% 处饱和；CAMELYON 因数据少需更小比例。</li>
<li><strong>RSM 随机掩码比例</strong> 50–90%：80% 时综合最佳，再高压性能下降。</li>
<li><strong>一致性损失权重 α</strong> 0.05–0.5：0.1 处稳定，过大反而拖慢收敛。</li>
</ul>
<p>8 可视化与病理可解释性</p>
<ul>
<li><strong>注意力-概率双图</strong>（图7）：展示 MHIM-v2 把关注区从“脂肪+坏死”转向“肿瘤边缘/淋巴窦”，与病理学家标注边界高度吻合。</li>
<li><strong>教师评估演化</strong>（图8）：训练中期后，类感知概率相比注意力得分在肿瘤区更集中、在正常区更均匀，说明<strong>困难实例质量持续提高</strong>。</li>
</ul>
<p>9 可重复性</p>
<ul>
<li><strong>代码与权重</strong>：已开源至 <a target="_blank" rel="noopener" href="https://github.com/DearCaat/MHIM-MIL">https://github.com/DearCaat/MHIM-MIL</a></li>
<li><strong>实验协议</strong>：统一 5-fold 交叉验证、相同数据划分、相同超参搜索空间，确保与对比方法公平。</li>
</ul>
<p>总结</p>
<p>论文通过<strong>12 基准 + 跨域 + 成本 + 消融 + 可视化</strong>的立体实验，系统验证了 MHIM-MIL 在<strong>精度、效率、鲁棒性</strong>三方面同时优于现有 SOTA，且提升随基础模型升级而放大，为千兆像素病理 MIL 提供了新的强基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望按“<strong>问题发现 → 可探索方向 → 可能方法</strong>”三段式给出，均直接对应 MHIM-MIL 的<strong>遗留限制</strong>或<strong>未充分展开</strong>之处，供后续研究参考。</p>
<p>1 掩码策略：从静态比例 → 自适应、可解释</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>RHSM/RSM 比例需人工调参，且全局统一</p>
<p><strong>实例级困难度不确定性估计</strong></p>
<p>用教师网络的 <strong>预测熵 / 深度集成 / MC-Dropout</strong> 量化每个实例的“不确定度”，实现<strong>实例级自适应掩码</strong>；可引入 <strong>Sub-modular</strong> 函数保证掩码集合的多样性与覆盖度。</p>
<p>掩码决策不可解释</p>
<p><strong>可解释掩码策略</strong></p>
<p>将掩码过程建模为 <strong>强化学习策略网络</strong>，以“与病理学家标注重叠率”或下游任务奖励为监督，学习<strong>可解释的掩码策略</strong>；可视化掩码决策路径。</p>
<p>2 全局回收：从固定查询 → 动态、多尺度、跨切片记忆</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>GRN 仅维护单组全局查询，容量有限</p>
<p><strong>多尺度 / 多类别记忆库</strong></p>
<p>按组织学区域（肿瘤、基质、免疫等）或 WSI 分辨率层级分别维护 <strong>记忆槽 (memory slots)</strong>，用 <strong>Key-Value Memory</strong> 或 <strong>Neural Dictionary</strong> 实现动态读写；支持 <strong>跨切片共享记忆</strong>，利用整个数据集的共同知识。</p>
<p>EMA 更新无梯度，记忆更新慢</p>
<p><strong>可学习记忆控制器</strong></p>
<p>引入 <strong>LSTM/Transformer 控制器</strong> 根据训练阶段自动调整记忆融合速率，或采用 <strong>快速权重 (fast weights)</strong> 实现即时记忆写入。</p>
<p>3 困难实例：从单模态特征 → 多模态、多任务协同</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>仅利用图像特征，困难度评估维度单一</p>
<p><strong>多模态困难度度量</strong></p>
<p>将 <strong>基因组突变负荷、转录组通路活性、病理文本报告</strong> 编码后与图像特征拼接，计算 <strong>跨模态不一致度</strong> 作为困难度信号；高不一致区域往往对应生物学上更具挑战性的亚克隆或低分化区域。</p>
<p>困难实例仅服务单一任务</p>
<p><strong>多任务共享困难实例池</strong></p>
<p>联合 <strong>诊断 + 亚型 + 生存 + 基因突变预测</strong> 多任务，构建 <strong>任务无关-困难实例池</strong>（Task-agnostic Hard Pool），通过 <strong>元学习</strong> 或 <strong>多任务梯度对齐</strong> 让困难实例同时提升所有任务，验证其通用性。</p>
<p>4 教师-学生：从单教师 → 多教师、对抗、在线蒸馏</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>单动量教师易受自身分布漂移影响</p>
<p><strong>多教师投票/集成</strong></p>
<p>维护 <strong>架构异构的多个教师</strong>（CNN、Transformer、Mamba），用 <strong>投票熵</strong> 或 <strong>分歧度</strong> 作为困难度额外信号；可引入 <strong>对抗蒸馏</strong> 让教师生成“最难”实例，学生进行对抗训练。</p>
<p>EMA 更新仅线性加权</p>
<p><strong>在线蒸馏 + 教师校正</strong></p>
<p>学生每 epoch 用 <strong>验证集性能</strong> 反馈调整教师融合权重（Meta-Weight Net），实现<strong>性能感知</strong>的非线性更新；或采用 <strong>双向蒸馏</strong>（学生也反向蒸馏教师），缓解教师过时问题。</p>
<p>5 实例级监督：从纯弱监督 → 主动、人机协同</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>无实例标签，无法验证困难实例真实价值</p>
<p><strong>主动学习循环</strong></p>
<p>将 MHIM 挖掘的 Top-K 困难实例提交给病理学家做<strong>一次性点标注</strong>，迭代扩充<strong>小型高精度实例库</strong>；用 <strong>不确定性 + 多样性 + 梯度密度</strong> 三重准则选择请求标注的实例，实现 <strong>10% 标注量 → 90% 性能</strong>。</p>
<p>标注成本高</p>
<p><strong>人机协同微调</strong></p>
<p>引入 <strong>可交互切片可视化工具</strong>，让医生<strong>仅勾画困难区域</strong>而非全片；使用 <strong>Weakly-to-Semi 转换策略</strong>（如 FixMatch）把点标注快速扩散到邻近相似实例。</p>
<p>6 计算效率：从 GPU 显存 → 端侧/流式推理</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>仍需一次性加载整袋特征（&gt;1 GB/WSI）</p>
<p><strong>流式-掩码训练</strong></p>
<p>借鉴 <strong>Streaming CNN</strong> 思想，只在内存中维护 <strong>滑动窗口特征缓存</strong> 与 <strong>动态掩码掩码表</strong>；结合 <strong>梯度检查点</strong> 与 <strong>特征量化 (INT8)</strong>，实现 <strong>单卡 8 GB 可训 100 k 实例</strong>。</p>
<p>推理仍用全序列</p>
<p><strong>级联早退机制</strong></p>
<p>设计 <strong>轻量级困难度探针网络</strong>，在 Transformer 浅层即预测“是否已见足够困难实例”，满足置信阈值时提前输出，<strong>平均推理时间减少 30–50%</strong>；可学习早退阈值以平衡速度-精度。</p>
<p>7 跨中心鲁棒性：从单中心统计 → 域适应、因果推断</p>
<p>遗留问题</p>
<p>可探索点</p>
<p>潜在方法</p>
<p>跨域实验仅 CPTAC→TCGA，域差异有限</p>
<p><strong>多中心域泛化</strong></p>
<p>收集 <strong>>5 家医院不同扫描仪（Hamamatsu、3DHistech、Leica）</strong> 数据，构建 <strong>DomainBed-style 基准</strong>；用 <strong>因果不变性约束</strong>（IRM、CausalMatch）迫使困难实例挖掘策略对<strong>染色、扫描仪、制片协议</strong>不敏感。</p>
<p>域漂移导致困难度定义变化</p>
<p><strong>动态域提示微调</strong></p>
<p>为每个域引入 <strong>可学习域令牌 (domain token)</strong>，在教师评估阶段与实例特征拼接，实现<strong>域感知的困难度重校准</strong>；仅更新域令牌，保持主干冻结，实现 <strong>3-shot 新域快速适配</strong>。</p>
<p>8 开源与基准</p>
<ul>
<li>建议构建 <strong>“MHIM-Bench”</strong>：<br>统一代码框架、数据分割、评价协议，集成上述方向的可插拔模块（自适应掩码、多教师、记忆库、主动学习接口），方便社区在<strong>同一标准</strong>下比较后续改进。</li>
</ul>
<p>总结</p>
<p>MHIM-MIL 打开了“无监督困难实例挖掘”在千兆像素病理的新范式，但仍处于<strong>静态、单模态、单任务、单教师</strong>阶段；未来可从<strong>自适应掩码、多模态记忆、人机协同、域泛化、端侧部署</strong>等角度继续深化，推动其走向<strong>临床可落地</strong>的下一阶段。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<script type="math/tex; mode=display">
\\boxed{\\text{MHIM-MIL：掩码困难实例挖掘的千兆像素病理多示例学习框架}}</script><p>1 背景与痛点</p>
<ul>
<li>千兆像素全切片图像（WSI）仅提供<strong>袋级标签</strong>，传统注意力 MIL 过度聚焦<strong>易分类显著实例</strong>，忽视决策边界处的<strong>困难实例</strong>，导致泛化性差、冗余高、计算重。</li>
</ul>
<p>2 核心思想</p>
<blockquote>
<p><strong>“把简单实例遮住，让模型被迫从剩余区域学习”</strong><br>无需实例标签，通过<strong>动量教师评估-两级掩码-全局回收-一致性对齐</strong>四步闭环，持续挖掘高质量困难实例并训练学生网络。</p>
</blockquote>
<p>3 方法框架</p>
<p>模块</p>
<p>关键机制</p>
<p>作用</p>
<p><strong>类感知实例概率</strong></p>
<p>教师用分类器对注意力加权特征再预测 softmax</p>
<p>精准定位“真正简单”实例</p>
<p><strong>RHSM</strong></p>
<p>随机掩码 Top-2βh% 的一半</p>
<p>避免训练初期误删关键区</p>
<p><strong>RSM</strong></p>
<p>再随机掩码 70–90%</p>
<p>序列长度 ↓90%，显存 ↓50%</p>
<p><strong>GRN</strong></p>
<p>全局查询交叉注意力回收掩码特征</p>
<p>补偿稀疏关键信息</p>
<p><strong>一致性损失</strong></p>
<p>学生（仅见困难）与教师（全序列）袋嵌入 KL 对齐</p>
<p>稳定优化，提升判别边界</p>
<p>4 实验结果</p>
<ul>
<li><strong>12 套公开基准</strong>（CAMELYON、TCGA-NSCLC、TCGA-BRCA、TCGA-LUAD/LUSC/BLCA）</li>
<li><strong>3 类任务</strong>（癌症诊断、亚型、生存）</li>
<li><strong>3 种特征</strong>（ResNet-50、PLIP、UNI）<br>→ <strong>全部优于原基线，9 项达到新 SOTA</strong>；同时<strong>训练时间 ↓24%，显存 ↓48%</strong>。</li>
</ul>
<p>5 贡献一句话</p>
<p>MHIM-MIL 首次在无实例标注条件下，把“困难实例”变成千兆像素病理 MIL 的主角，<strong>精度、效率、鲁棒性</strong>三升，为计算病理提供新的强基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Multiple%20Instance%20Learning%20Framework%20with%20Masked%20Hard%20Instance%20Mining%20for%20Gigapixel%20Histopathology%20Image%20Analysis，链接是  https://arxiv.org/pdf/2509.11526  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.11526  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wenhao Tang,Sheng Huang,Heng Fang,Fengtao Zhou,Bo Liu,Qingshan Liu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11526.pdf">https://arxiv.org/pdf/2509.11526.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11526">https://arxiv.org/abs/2509.11526</a></p>
<p><strong>Arxiv ID</strong>: 2509.11526</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11526">https://papers.cool/arxiv/2509.11526</a></p>
<p><strong>Published</strong>: 2025-09-15T02:31:33Z</p>
<p><strong>Updated</strong>: 2025-09-15T02:31:33.000Z</p>
<hr>
<h3 id="15-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs"><a href="#15-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs" class="headerlink" title="15. Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.11177">Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs</a></h3><p>Recent advances in Large Language Model (LLM) compression, such as quantization and pruning, have achieved notable success. However, as these techniques gradually approach their respective limits, relying on a single method for further compression has become increasingly challenging. In this work, we explore an alternative solution by combining quantization and sparsity. This joint approach, though promising, introduces new difficulties due to the inherently conflicting requirements on weight distributions: quantization favors compact ranges, while pruning benefits from high variance. To attack this problem, we propose Optimal Brain Restoration (OBR), a general and training-free framework that aligns pruning and quantization by error compensation between both. OBR minimizes performance degradation on downstream tasks by building on a second-order Hessian objective, which is then reformulated into a tractable problem through surrogate approximation and ultimately reaches a closed-form solution via group error compensation. Experiments show that OBR enables aggressive W4A4KV4 quantization with 50% sparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory reduction compared to the FP16-dense baseline.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近在大型语言模型（LLM）压缩方面取得了显著进展，例如量化和剪枝等技术。然而，随着这些技术逐渐接近各自的极限，依靠单一方法进行进一步压缩变得越来越具有挑战性。在这项工作中，我们探索了一种通过结合量化和稀疏性来提供替代解决方案。这种联合方法虽然前景看好，但由于对权重分布的内在矛盾要求，带来了新的困难：量化有利于紧凑的范围，而剪枝则受益于高方差。为了解决这个问题，我们提出了最优大脑恢复（OBR），这是一个全面且无训练的框架，通过在两者之间的误差补偿，使剪枝和量化保持一致。OBR通过建立在二阶海森目标上的方法，最小化下游任务的性能下降，然后通过替代近似将其重新表述为一个可解决的问题，最终通过组误差补偿达到封闭形式的解决方案。实验表明，OBR能够在现有的LLM上实现50%稀疏性的激进W4A4KV4量化，并与FP16密集基线相比，提供高达4.72倍的加速和6.4倍的内存减小。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破单一压缩技术（量化或剪枝）在极低比特（sub-4-bit）场景下迅速触及性能天花板的瓶颈，提出<strong>联合量化与稀疏化</strong>的新范式。核心待解决问题可概括为：</p>
<ol>
<li><p><strong>目标冲突</strong>：<br>量化要求权重分布紧凑以减小量化误差，而剪枝依赖大数值差异以识别可移除权重；二者对权重分布的需求本质矛盾。</p>
</li>
<li><p><strong>性能崩塌</strong>：<br>在 W4A4KV4+50 % 稀疏度的激进压缩下，现有方法（如直接拼接 QuaRot 与 WANDA）导致 perplexity 暴涨（Llama2-7B 从 5.47 升至 5868），无法实用。</p>
</li>
<li><p><strong>硬件红利未兑现</strong>：<br>尽管 Ampere/Hopper 架构已原生支持 INT4+2:4 稀疏 GEMM，但缺乏<strong>无需重训练</strong>即可生成兼容权重格式的算法，致使理论加速无法落地。</p>
</li>
</ol>
<p>为此，论文提出 Optimal Brain Restoration（OBR），通过<strong>二阶 Hessian 误差补偿</strong>将剪枝与量化引入的失真统一建模，并以闭式解形式把“被剔除”信息迁移到保留权重，首次在 LLM 上实现 W4A4KV4+50 % 稀疏度的<strong>无重训练</strong>联合压缩，且实际推理获得 4.72× 速度提升与 6.4× 内存削减。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大脉络：量化、剪枝，以及二者的联合压缩。以下按类别梳理并指出与 OBR 的差异/继承关系。</p>
<ul>
<li><p><strong>LLM 量化</strong></p>
<ul>
<li><strong>OBQ / GPTQ</strong>（Frantar &amp; Alistarh 2022；Frantar et al. 2022）<br>首次将二阶 Hessian 近似用于大规模生成模型 PTQ，OBR 沿用其 <code>G⊗H</code> 近似，但进一步行解耦并引入“组误差补偿”以同时照顾剪枝。</li>
<li><strong>LLM.int8()</strong>（Dettmers et al. 2022）<br>混合精度保留异常值，OBR 则通过 Hadamard 旋转+补偿把异常值“摊平”后重新分配，无需混合精度。</li>
<li><strong>AWQ / SmoothQuant</strong>（Lin et al. 2024a；Xiao et al. 2023）<br>通过平滑因子或通道缩放将异常值从权重迁移到激活，仅面向量化；OBR 同时处理剪枝引入的额外误差。</li>
<li><strong>QuaRot / SpinQuant / FlatQuant</strong>（Ashkboos et al. 2024；Liu et al. 2024；Sun et al. 2024）<br>利用（可学习）Hadamard 旋转 flatten 权重分布以实现 W4A4KV4；OBR 直接把这些旋转作为预处理，但在旋转后额外执行“补偿”以抵消剪枝-量化冲突。</li>
</ul>
</li>
<li><p><strong>LLM 剪枝</strong></p>
<ul>
<li><strong>SparseGPT</strong>（Frantar &amp; Alistarh 2023）<br>一次性二阶剪枝，无重训练；OBR 把它视为“黑箱掩码生成器”，再用 Hessian 补偿其误差。</li>
<li><strong>WANDA</strong>（Sun et al. 2023）<br>激活感知幅度剪枝，OBR 实验默认采用其掩码，但后续用闭式解修正权重值。</li>
<li><strong>LLM-Pruner</strong>（Ma et al. 2023）<br>结构化剪枝+LoRA 微调；OBR 面向非结构化/半结构化稀疏，无需任何梯度更新。</li>
</ul>
</li>
<li><p><strong>联合量化与剪枝（小型网络时代）</strong></p>
<ul>
<li><strong>DJPQ</strong>（Wang et al. 2020）<br>用梯度下降联合优化稀疏率和量化水平，需训练；OBR 完全 PTQ。</li>
<li><strong>OBQ</strong>（Frantar &amp; Alistarh 2022）<br>统一框架同时决定“删哪些”与“量到几比特”，但仅验证在小型 CNN；OBR 面向百亿参数 LLM，且顺序为“剪-补偿-量”。</li>
</ul>
</li>
<li><p><strong>LLM 时代的联合压缩</strong></p>
<ul>
<li><strong>JSQ</strong>（Guo et al. 2024）<br>模拟退火搜索激活编辑策略，实现 W8A8+50 % 稀疏；OBR 推进到 W4A4KV4，且用闭式解替代搜索。</li>
<li><strong>Harma et al. 2024</strong><br>理论证明“先剪后量”最优，但未给出具体算法；OBR 可视为该结论的一个可扩展、可落地的实现。</li>
</ul>
</li>
</ul>
<p>综上，OBR 在方法层面继承并扩展了 OBQ/GPTQ 的二阶误差建模思想，在系统层面首次把“剪枝后补偿”与“量化前再补偿”串成无重训练流水线，从而突破 sub-4-bit+50 % 稀疏的实用门槛。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“量化-剪枝目标冲突”建模为<strong>二次误差最小化</strong>问题，并通过三步策略给出<strong>无需重训练</strong>的闭式解，具体流程如下：</p>
<ol>
<li><p><strong>冲突建模：二阶 Hessian 目标</strong><br>以经典 OBQ 框架为出发点，把权重扰动 ∆W 对下游损失的影响写成  </p>
<script type="math/tex; mode=display">\\mathbb{E}\[\\Delta\\mathcal{L}\] \\approx \\frac{1}{2}\\mathrm{vec}(\\Delta W)^\\top H\_{\\text{full}},\\mathrm{vec}(\\Delta W).</script><p>利用 <code>H_full ≈ G ⊗ H</code> 及“行解耦”近似 <code>G ≈ I</code>，将层内 <code>(C_out×C_in)^2</code> 复杂度的全局问题拆成 <code>C_out</code> 个独立的行子问题：  </p>
<script type="math/tex; mode=display">\\min\_{\\Delta w\_i} \\frac{1}{2}\\Delta w\_i H \\Delta w\_i^\\top,; i=1{\\dots}C\_{\\text{out}}.</script><p>该行级目标同时量化“剪枝误差”与“量化误差”对特征的联合影响，为后续补偿提供解析桥梁。</p>
</li>
<li><p><strong>组误差补偿： retain-vs-eviction 闭式解</strong><br>对每一行权重 <code>w_i</code> 动态划分两组索引：</p>
<ul>
<li><strong>retain 集 R</strong>——压缩后仍被保留的权重（未剪 + 未过量量化）</li>
<li><strong>eviction 集 E</strong>——被强制置零或四舍五入的权重</li>
</ul>
<p>把行向量重排为 <code>[Δw_R, e_E]</code>，其中 <code>e_E</code> 是已知的“压缩失真”。行目标变为  </p>
<script type="math/tex; mode=display">J=\\frac{1}{2}\[\\Delta w\_R;e\_E\]\\begin{bmatrix}H\_{RR}&H\_{RE}\\H\_{ER}&H\_{EE}\\end{bmatrix}\[\\Delta w\_R;e\_E\]^\\top.</script><p>对 <code>Δw_R</code> 求导并令梯度为零，得到<strong>闭式补偿</strong>：  </p>
<script type="math/tex; mode=display">\\boxed{\\Delta w\_R^\\star = -H\_{RR}^{-1}H\_{RE},e\_E}</script><p>含义：用 Hessian 交叉块 <code>H_RE</code> 把 E 集失真投影到 R 集空间，再用 <code>H_{RR}^{-1}</code> 计算最优修正量，实现“信息迁移”而非简单丢弃。</p>
</li>
<li><p><strong>双阶段顺序补偿算法</strong><br>遵循“先剪后量”硬件友好顺序，把上述闭式解串成两轮：</p>
<ol>
<li><strong>剪枝补偿</strong><br>用剪枝掩码得 <code>E1/R1</code>，令 <code>e_{E1}=w_{E1}</code>，求得  <script type="math/tex; mode=display">\\Delta w\_{R1}^{\\text{prune}}=-H\_{R1R1}^{-1}H\_{R1E1}w\_{E1}</script>并更新 <code>w_{R1}←w_{R1}+Δw_{R1}^{\text{prune}}</code>，得到稀疏但仍是 FP 的权重 <code>w̅</code>。</li>
<li><strong>量化补偿</strong><br>对 <code>w̅</code> 再划分 <code>E2/R2</code>（按 α=50% 头部元素为 E2），计算量化误差 <code>e_{E2}=w̅_{E2}-Q(w̅_{E2})</code>，再次套用闭式解  <script type="math/tex; mode=display">\\Delta w\_{R2}^{\\text{quant}}=-H\_{R2R2}^{-1}H\_{R2E2}e\_{E2}</script>最终权重  <script type="math/tex; mode=display">\\hat w=Q!\\left(,\[w̅\_{R2}+\\Delta w\_{R2}^{\\text{prune}}+\\Delta w\_{R2}^{\\text{quant}},;w̅\_{E2}+\\Delta w\_{E2}^{\\text{prune}},;0\],\\right)</script>即同时满足 2:4 稀疏 + INT4 量化格式，可直接加载到支持 INT4-sparse GEMM 的 GPU 执行。</li>
</ol>
</li>
</ol>
<p>通过“行解耦 + 组补偿”两步，论文把原本冲突的“削峰（量化）”与“造零（剪枝）”统一到一个二次误差框架里，并用<strong>局部可并行</strong>的闭式线性方程组完成信息再分配，从而首次在 7B–70B 级 LLM 上实现 W4A4KV4+50 % 稀疏、<strong>无需任何重训练</strong>即可保持可用精度，且实测推理速度提升 4.72×、内存节省 6.4×。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“极端压缩 + 零重训练”能否同时保持精度与落地效率展开，覆盖模型家族、任务类型、位宽、稀疏模式、硬件 kernel、消融与可视化六大维度。主要结果汇总如下：</p>
<p>维度</p>
<p>设置</p>
<p>关键结论</p>
<p><strong>1. 主尺度对比</strong></p>
<p>Llama2-7/13/70B、Llama3-8/70B、Qwen2.5-7/32B；W4A4KV4+50 % 非结构化稀疏</p>
<p>OBR-GPTQ 在 WikiText2 上平均 perplexity 比 SparseGPT+GPTQ 低 3.7，零 shot 平均 acc 高 4.3 %；70 B 模型差距仅 1.37 perplexity。</p>
<p><strong>2. 硬件效率</strong></p>
<p>A100 上 CUTLASS INT4+2:4 sparse GEMM，seq=4 k</p>
<p>比 FP16 dense 快 4.72×、比 INT4 dense 快 1.4×；FLOPs 减半，峰值 TOPS 提高 1.9×；内存占用 6.4× 节省。</p>
<p><strong>3. 位宽泛化</strong></p>
<p>固定 W4，变化 A/KV：W4A8KV8、W4A16KV16</p>
<p>OBR-RTN 在 W4A8KV8+50 % 稀疏下仍比 SparseGPT+GPTQ（权重量化版）perplexity 低 1.29，证明低权宽即可获益。</p>
<p><strong>4. 稀疏模式</strong></p>
<p>2:4 与 4:8 半结构化</p>
<p>W4A4KV4+2:4 下 OBR-GPTQ perplexity 13.32 vs 34.76（baseline），零 shot acc 高 8.15 %；硬件友好模式同样有效。</p>
<p><strong>5. 旋转矩阵通用性</strong></p>
<p>QuaRot、SpinQuant、FlatQuant 三种 Hadamard 方案</p>
<p>同一 pipeline 即插即用；FlatQuant+OBR 在 Llama2-7B 上 perplexity 仅比 FP16 高 1.4，显著缩小差距。</p>
<p><strong>6. 单压缩增强</strong></p>
<p>仅剪枝（WANDA）或仅量化（RTN）</p>
<p>60 % 稀疏时 WANDA+OBR perplexity 降 0.53；RTN+OBR 在 W4A4KV4 降 2.17 并提升 3.88 % acc，验证补偿机制通用。</p>
<p><strong>7. 消融</strong></p>
<p>不同掩码（Magnitude/SparseGPT/Random）、分组比例 α={20,25,50,75 %}</p>
<p>掩码来源对终精度影响 &lt; 2 %；α=50 % 均衡误差-容量，为默认设置。</p>
<p><strong>8. 校准集鲁棒</strong></p>
<p>WikiText2 → C4</p>
<p>换校准集后 OBR 仍全面领先，Llama3-8B 零 shot acc 优势从 1.4 % 扩大到 1.96 %。</p>
<p><strong>9. 与 BitNet 对比</strong></p>
<p>Qwen2.5-3B+OBR W4A4KV4+50 % vs BitNet-2B-4T 1.58-bit</p>
<p>perplexity 11.79 vs 13.67，零 shot acc 62.77 vs 70.27；OBR 在后训练场景下可比肩从头训练的超低比特模型。</p>
<p><strong>10. 可视化</strong></p>
<p>权重分布、补偿矩阵热力图</p>
<p>补偿幅度与原始权重同量级，且保持行独立、无新异常值，验证“信息迁移”而非噪声注入。</p>
<p>综上，实验从 7 B 到 70 B、从非结构化到 2:4 结构化、从纯量化/剪枝到联合压缩、从精度到 A100 实测 kernel，全方位说明 OBR 在“W4A4KV4+50 % 稀疏”这一极限设定下仍能维持可用精度并兑现硬件加速。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 OBR 框架的直接延伸或潜在突破点，按“短期可验证”到“长期需重新设计”排序：</p>
<ol>
<li><p><strong>加速压缩本身</strong></p>
<ul>
<li>行解耦带来 <code>C_out</code> 次 <code>|R|×|R|</code> 线性系统求解，占 70 B 模型 35 h 耗时；可尝试<br>– 低秩近似 <code>HRR ≈ LLᵀ</code> ＋ SMW 公式，将求逆复杂度从 <code>O(|R|³)</code> 降到 <code>O(r²|R|)</code>；<br>– GPU 批量求解器（cuBLASLt / MAGMA）或并行 CPU 集群，一次性处理多行。</li>
</ul>
</li>
<li><p><strong>学习式掩码与旋转</strong></p>
<ul>
<li>目前掩码 <code>M</code> 与 Hadamard 旋转 <code>P</code> 均为“即插即用”。若将 <code>M</code> 松弛为可微分门控、<code>P</code> 设为可学习正交矩阵，与 OBR 补偿联合优化，可期望在 3-bit 以下再抬升 1–2 % acc。</li>
<li>目标函数可写成 <code>min_{P,M} ‖Δw_R^*‖_H</code>，交替更新 <code>P,M</code> 与补偿量，保持 PTQ 只需校准集。</li>
</ul>
</li>
<li><p><strong>混合稀疏粒度</strong></p>
<ul>
<li>本文仅验证 2:4 与 50 % 非结构化。硬件新趋势支持 1:2、1:4、块稀疏（4×1, 8×2）等；OBR 的 <code>E/R</code> 划分可无缝适配，只需把 <code>E</code> 定义为被整块剔除的索引。</li>
<li>对比不同粒度下的“实测 TFLOPS vs 精度”Pareto 前沿，可为推理引擎提供选型依据。</li>
</ul>
</li>
<li><p><strong>激活与 KV 缓存联合稀疏</strong></p>
<ul>
<li>当前仅权重稀疏。激活值与 KV cache 同样存在大量小值，若把 <code>H</code> 扩展为 <code>[W;X;K;V]</code> 统一 Hessian，即可用同一套组补偿实现“WAXKXV 全局稀疏+量化”，进一步减内存带宽。</li>
</ul>
</li>
<li><p><strong>动态稀疏补偿</strong></p>
<ul>
<li>不同输入批次稀疏模式可能最优不同。探索“运行时轻量补偿”：<br>– 预存 2–3 组 <code>M</code> 与对应 <code>ΔW</code>，按输入特征范数或熵快速切换；<br>– 或者把 <code>Δw_R^*</code> 分解为 <code>L·z</code>，推理时只计算低维 <code>z</code>，实现亚毫秒级切换。</li>
</ul>
</li>
<li><p><strong>跨层联合误差传播</strong></p>
<ul>
<li>行解耦忽略输出通道间相关。可尝试<br>– 块坐标下降：以 Transformer 块为单位联合优化 <code>ΔW_q, ΔW_k, ΔW_v, ΔW_o</code>，保持 <code>|block|≈4096</code> 仍可 tractable；<br>– 或用 Krylov 子空间迭代近似 <code>G</code> 非对角块，减少层间误差累积。</li>
</ul>
</li>
<li><p><strong>低比特理论极限分析</strong></p>
<ul>
<li>建立“稀疏+量化”率失真函数 <code>R(D, s, b)</code>，给出任务无关的最低比特-稀疏理论下限，用于判断 OBR 距离信息论极限还有多远。</li>
</ul>
</li>
<li><p><strong>专用硬件协同设计</strong></p>
<ul>
<li>现有 INT4+2:4 GEMM 仅利用 50 % 零值跳过；补偿项 <code>ΔW</code> 为结构化非零，可设计“稀疏-增量”融合 kernel，把 <code>Y=(W+ΔW)X</code> 拆成 <code>sparse_gemm(W,X) + skinny_gemm(ΔW,X)</code>，减少写回带宽。</li>
</ul>
</li>
<li><p><strong>继续下探比特</strong></p>
<ul>
<li>1.58-bit 或 even ternary <code>{−1,0,1}</code> 权重；此时量化误差 <code>e_E</code> 离散且幅度固定，可与剪枝误差统一为“三元补偿表”，验证 OBR 在极端离散域是否仍成立。</li>
</ul>
</li>
<li><p><strong>任务特定压缩</strong></p>
<ul>
<li>将 OBR 目标函数改为下游微调损失 <code>L_downstream</code> 的 Hessian，直接优化“领域专用”稀疏-量化模型，避免先通用压缩再 LoRA 微调的两阶段成本。</li>
</ul>
</li>
</ol>
<p>以上任意一点在 7 B∼70 B 模型上验证成功，均可作为 OBR 后续工作的显著增量。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="论文核心速览"><a href="#论文核心速览" class="headerlink" title="论文核心速览"></a>论文核心速览</h1><h2 id="1-问题"><a href="#1-问题" class="headerlink" title="1. 问题"></a>1. 问题</h2><ul>
<li>单一量化或剪枝在 sub-4 bit 已触顶；二者目标冲突（量化要“紧”、剪枝要“散”），直接拼接性能崩塌。</li>
<li>硬件已支持 INT4+2:4 稀疏 GEMM，却缺乏<strong>无需重训练</strong>即可生成对应权重的算法。</li>
</ul>
<h2 id="2-方法：Optimal-Brain-Restoration-OBR"><a href="#2-方法：Optimal-Brain-Restoration-OBR" class="headerlink" title="2. 方法：Optimal Brain Restoration (OBR)"></a>2. 方法：Optimal Brain Restoration (OBR)</h2><ul>
<li><strong>二阶误差建模</strong> – 行解耦近似 <code>H_full ≈ I ⊗ H</code>，把层内全局问题拆成 <code>C_out</code> 个独立行子问题。</li>
<li><strong>组误差补偿</strong> – 每行划分 retain 集 R 与 eviction 集 E，用闭式解<br><code>Δw_R^* = –H_RR^{-1} H_RE e_E</code><br>把 E 的失真“迁移”到 R，实现信息再分配。</li>
<li><strong>双阶段流水线</strong> – 先剪后量：①剪枝补偿→②量化补偿→INT4+50 % 稀疏权重，<strong>零重训练</strong>。</li>
</ul>
<h2 id="3-结果"><a href="#3-结果" class="headerlink" title="3. 结果"></a>3. 结果</h2><ul>
<li><strong>精度</strong>：Llama2-7B W4A4KV4+50 % 稀疏，WikiText2 perplexity 8.40（vs SparseGPT+GPTQ 12.94）；70 B 模型与 FP16 差距仅 1.37。</li>
<li><strong>效率</strong>：A100 INT4+2:4 稀疏 GEMM 比 FP16 dense 快 4.72×，内存省 6.4×，TFLOPS 峰值提高 1.9×。</li>
<li><strong>通用</strong>：兼容 QuaRot/SpinQuant/FlatQuant 等多种旋转；对纯剪枝或纯量化也能降 perplexity 2–3 点。</li>
</ul>
<h2 id="4-贡献"><a href="#4-贡献" class="headerlink" title="4. 贡献"></a>4. 贡献</h2><ul>
<li>首次在百亿参数 LLM 上实现 <strong>W4A4KV4+50 % 稀疏</strong> 无重训练联合压缩。</li>
<li>提出“二阶-行解耦-组补偿”框架，把冲突的压缩误差统一用闭式解调和。</li>
<li>释放硬件 INT4-sparse kernel 红利，为极端压缩提供可落地基线。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Optimal%20Brain%20Restoration%20for%20Joint%20Quantization%20and%20Sparsification%20of%20LLMs，链接是  https://arxiv.org/pdf/2509.11177  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.11177  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hang Guo,Yawei Li,Luca Benini</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11177.pdf">https://arxiv.org/pdf/2509.11177.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11177">https://arxiv.org/abs/2509.11177</a></p>
<p><strong>Arxiv ID</strong>: 2509.11177</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11177">https://papers.cool/arxiv/2509.11177</a></p>
<p><strong>Published</strong>: 2025-09-14T09:17:19Z</p>
<p><strong>Updated</strong>: 2025-09-14T09:17:19.000Z</p>
<hr>
<h3 id="16-Stable-Part-Diffusion-4D-Multi-View-RGB-and-Kinematic-Parts-Video-Generation"><a href="#16-Stable-Part-Diffusion-4D-Multi-View-RGB-and-Kinematic-Parts-Video-Generation" class="headerlink" title="16. Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.10687">Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</a></h3><p>We present Stable Part Diffusion 4D (SP4D), a framework for generating paired RGB and kinematic part videos from monocular inputs. Unlike conventional part segmentation methods that rely on appearance-based semantic cues, SP4D learns to produce kinematic parts - structural components aligned with object articulation and consistent across views and time. SP4D adopts a dual-branch diffusion model that jointly synthesizes RGB frames and corresponding part segmentation maps. To simplify the architecture and flexibly enable different part counts, we introduce a spatial color encoding scheme that maps part masks to continuous RGB-like images. This encoding allows the segmentation branch to share the latent VAE from the RGB branch, while enabling part segmentation to be recovered via straightforward post-processing. A Bidirectional Diffusion Fusion (BiDiFuse) module enhances cross-branch consistency, supported by a contrastive part consistency loss to promote spatial and temporal alignment of part predictions. We demonstrate that the generated 2D part maps can be lifted to 3D to derive skeletal structures and harmonic skinning weights with few manual adjustments. To train and evaluate SP4D, we construct KinematicParts20K, a curated dataset of over 20K rigged objects selected and processed from Objaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part video sequences. Experiments show that SP4D generalizes strongly to diverse scenarios, including real-world videos, novel generated objects, and rare articulated poses, producing kinematic-aware outputs suitable for downstream animation and motion-related tasks.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们提出了稳定部件扩散4D（SP4D），这是一个从单目输入生成配对RGB和运动部件视频的框架。与依赖于外观基础语义线索的传统部件分割方法不同，SP4D学习生成运动部件——与物体关节动作对齐并在视图和时间上保持一致的结构组件。SP4D采用双分支扩散模型，共同合成RGB帧和相应的部件分割图。为了简化架构并灵活支持不同数量的部件，我们引入了一种空间颜色编码方案，将部件掩膜映射到连续的RGB样图像。这种编码允许分割分支共享来自RGB分支的潜在变分自编码器，同时通过简单的后处理恢复部件分割。双向扩散融合（BiDiFuse）模块增强了跨分支一致性，并通过对比部件一致性损失促进部件预测的空间和时间对齐。我们证明生成的2D部件图可以提升到3D，以推导骨架结构和谐皮肤权重，仅需很少的手动调整。为了训练和评估SP4D，我们构建了KinematicParts20K，这是一个从Objaverse XL（Deitke等，2023）中精选和处理的超过2万件装配物体的策划数据集，每个物体都配有多视角RGB和部件视频序列。实验表明，SP4D在不同场景中的表现强劲，包括现实世界视频、新生成物体和罕见的关节姿态，产生了适合下游动画和运动相关任务的运动感知输出。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“从单目输入生成具有运动学一致性、可动画化的多视角RGB-部件视频”这一核心问题。具体而言，现有方法存在以下局限：</p>
<ul>
<li>传统部件分割仅依赖外观语义，无法揭示物体真实关节结构，导致跨视角/时间不一致；</li>
<li>基于SDS或多视角重建的4D生成只关注表面几何，不提供可用于绑定的运动学部件；</li>
<li>自动绑定方法依赖静态3D网格与稀缺骨架标注，难以泛化到新品类或罕见姿态；</li>
<li>语义分割（SAM2 等）输出的“头-身-尾”等区域与运动学部件（刚性连杆）并不对齐，无法直接用于动画。</li>
</ul>
<p>为此，作者提出 Stable Part Diffusion 4D（SP4D），首次在扩散框架内联合生成多视角、时序一致的视频外观 $M$ 及其对应的“运动学部件”分割 $P$，使2D结果可直接抬升到3D，快速获得带权重、可绑定的网格资产，从而显著降低人工绑定与动画制作成本。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为三大类，并指出它们与本文目标的差距：</p>
<ol>
<li><p>3D/4D 生成</p>
<ul>
<li>SDS 类：DreamFusion、ProlificDreamer、LucidDreamer 等——利用 2D 扩散先验优化 NeRF/Mesh，但推理极慢且不含部件结构。</li>
<li>多视角→重建类：Zero-1-to-3、SyncDreamer、Wonder3D、SV3D、Instant3D 等——先生成多视角图像再做立体重建，仍只得到表面几何。</li>
<li>原生 3D/4D 扩散：L4GM、Diffusion4D、AR4D、SV4D 等——直接在 3D 潜空间去噪，可生成动态序列，但未显式建模运动学部件或绑定信息。</li>
</ul>
</li>
<li><p>自动绑定与动画</p>
<ul>
<li>RigNet、UniRig、MagicArticulate、Anymate 等——基于静态网格预测骨架与权重，依赖小规模 rig 数据，难以泛化到新品类或罕见姿态，且无法利用海量 2D 视频先验。</li>
</ul>
</li>
<li><p>部件分解（2D/3D）</p>
<ul>
<li>2D 语义共分割：SCOPS、DeepViT、DINO+K-Means、SAM2——输出语义/纹理一致区域，而非刚性运动单元；跨帧、跨视角容易漂移。</li>
<li>3D 语义分割：SAMesh、SAMPart3D——将 2D SAM/DINOv2 掩码抬升到 3D，仍受限于外观线索，对无纹理或铰接部位失效。</li>
</ul>
</li>
</ol>
<p>SP4D 与上述工作的本质区别：</p>
<ul>
<li>不满足于“表面重建”或“语义分割”，而是首次在扩散框架内联合生成 RGB 与运动学部件视频；</li>
<li>利用大规模 2D 视频先验，摆脱对 3D 骨架标注的依赖；</li>
<li>通过可抬升的 2D 部件序列，实现轻量化、骨架自由的绑定与权重估计。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>Stable Part Diffusion 4D（SP4D）</strong>，通过三项关键设计把“单目 RGB 视频 → 多视角 RGB+运动学部件视频 → 可动画 3D 资产”做成端到端可训练、可泛化的生成框架。</p>
<ol>
<li><p>双分支扩散架构</p>
<ul>
<li>在 SV4D 2.0 的多视角-视频潜空间扩散 backbone 上并联 <strong>RGB 分支</strong> 与 <strong>部件分支</strong>，各自输出一半通道的潜变量。</li>
<li>每层 encoder/decoder 之后插入 <strong>Bidirectional Diffusion Fusion（BiDiFuse）</strong>  <script type="math/tex; mode=display">h\_{\\text{RGB}}^{\\text{fused}}=h\_{\\text{RGB}}+\\mathcal F(\[h\_{\\text{RGB}},h\_{\\text{Part}}\]),\\quad h\_{\\text{Part}}^{\\text{fused}}=h\_{\\text{Part}}+\\mathcal F(\[h\_{\\text{RGB}},h\_{\\text{Part}}\])</script>用轻量 1×1 卷积实现双向信息交换，使外观与结构彼此引导，同时保持任务特异特征流。</li>
</ul>
</li>
<li><p>空间颜色编码 + 共享 VAE</p>
<ul>
<li>将离散 part id 映射为 <strong>连续 RGB 颜色</strong>：以首帧各部件 3D 中心归一化坐标 $(x,y,z)$ 作为该部件的固定颜色。</li>
<li>部件分支因此生成“伪 RGB”图像，可与 RGB 分支 <strong>共用同一个 VAE 编解码器</strong>，无需额外设计 part-head 或量化模块；推理后用 SAM 自动分割+聚类即可还原离散 mask。</li>
</ul>
</li>
<li><p>对比式部件一致性损失</p>
<ul>
<li>对每帧每视角预测的特征图按部件掩码做平均池化，得到部件特征 ${f_i}$。</li>
<li>采用 InfoNCE 式对比损失  <script type="math/tex; mode=display">\\mathcal L\_{\\text{contrast}}=-\\mathbb E\_{i\\in\\mathcal P}\\log\\frac{\\exp(\\text{sim}(f\_i,f\_j)/\\tau)}{\\sum\_{k\\in\\mathcal P\\backslash{i}}\\exp(\\text{sim}(f\_i,f\_k)/\\tau)},\\quad j\\in\\mathcal P\_i^+</script>强制 <strong>同一物理部件</strong> 在不同视角/时刻的特征聚集，不同部件分离，从而抑制颜色漂移与碎片化。</li>
</ul>
</li>
<li><p>两阶段训练 + 数据</p>
<ul>
<li>阶段 1：仅在 ObjaverseDy 上训练 RGB 分支（BiDiFuse 旁路），继承强大视频先验。</li>
<li>阶段 2：在自建的 <strong>KinematicParts20K</strong>（&gt;20 K 个已绑定资产，24 视角×24 帧）上联合微调双分支，提供真值 part mask 与相机轨迹。</li>
</ul>
</li>
<li><p>2D→3D 轻量化绑定</p>
<ul>
<li>用 SP4D 生成的多视角 RGB 重建几何（Hunyuan3D 2.0），再把同一套相机下的 part mask 投影到 mesh 顶点。</li>
<li>以部件边界为狄利克雷条件求解 <strong>调和皮肤权重</strong>  <script type="math/tex; mode=display">\\Delta w\_p(\\mathbf x)=0\\ \\text{on interior},\\quad w\_p(\\mathbf x)=b\_p(\\mathbf x)\\ \\text{on }\\partial\\Omega\_p</script>得到平滑、可动画的权重场，无需手工骨架标注即可快速生成 riggable 资产。</li>
</ul>
</li>
</ol>
<p>通过“双分支扩散-共享潜空间-对比一致性”这一组合，SP4D 首次实现从单目输入端到端生成 <strong>时序一致、跨视角稳定、可直接用于动画</strong> 的运动学部件视频，显著降低传统绑定流程的人工成本。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从 <strong>2D 分割精度、3D 分割精度、消融实验、用户主观评价、自动绑定精度、动画合理性</strong> 六个维度展开系统实验，全部在自建的 <strong>KinematicParts20K</strong> 训练/验证集以及真实 DAVIS 视频上进行。</p>
<ol>
<li><p>2D 运动学部件分割对比<br>基准：SAM2、SAM2*（GT 点提示）、DeepViT（DINO+K-Means）。<br>指标：mIoU、ARI、F1、mAcc，分 <strong>多视角（静态帧，旋转相机）</strong> 与 <strong>多帧（固定相机）</strong> 两种协议。<br>结果：SP4D 全指标大幅领先（表 1），定性可视化（图 5）显示部件边界与关节对齐且跨视角一致。</p>
</li>
<li><p>3D 运动学部件分割对比<br>基准：SAMesh（SAM 多视角投票）、SAMPart3D（DINOv2+3D 聚类）。<br>结果：SP4D mIoU 0.64 vs. 0.15/0.13，ARI 0.58 vs. 0.06/0.05（表 3），验证“外观驱动”方法在 novel/无纹理物体上失效。</p>
</li>
<li><p>消融实验</p>
<ul>
<li>去掉 <strong>对比部件一致性损失</strong> → ARI 下降最明显（多视角 0.60→0.15）。</li>
<li>去掉 <strong>BiDiFuse</strong> → 所有指标普遍下降 10–20 点。<br>证实两项设计对时空一致性缺一不可（表 1）。</li>
</ul>
</li>
<li><p>用户主观评价<br>20 名受试者、20 组匿名 GIF，从 <strong>部件清晰度、跨视角一致性、绑定适用性</strong> 三方面 5 分制打分。<br>结果：SP4D 平均 4.26 分，SAM2 1.96 分，DeepViT 1.85 分（表 2），显著优于基线。</p>
</li>
<li><p>自动绑定精度（有真值骨架）<br>在 KinematicParts20K-test 上比较预测皮肤权重与 GT 的 <strong><a href="mailto:Precision@0.5">Precision@0.5</a></strong>。<br>SP4D 72.7 vs. MagicArticulate 63.7 vs. UniRig 64.3（表 4），说明部件分解质量直接转化为更高权重精度。</p>
</li>
<li><p>动画合理性（无真值骨架）<br>对生成的恐龙、机械臂等 novel 网格，用三种方法自动绑定后制作相同骨架动画，用户双盲评分（1–5）。<br>SP4D 4.1 分，显著高于 MagicArticulate 2.7 与 UniRig 2.3（表 4），体现跨品类泛化优势。</p>
</li>
<li><p>定性扩展</p>
<ul>
<li>图 4 给出 <strong>合成+真实视频</strong> 多视角、多时刻结果，验证时序追踪无漂移。</li>
<li>附录视频展示固定视角跨帧追踪、3D 部件 lift、骨骼抽取与实时动画，进一步说明可直接用于生产管线。</li>
</ul>
</li>
</ol>
<p>综合以上实验，论文定量与主观结果均表明 SP4D 在 <strong>运动学一致性、跨视角稳定性、下游绑定/动画可用性</strong> 上显著优于现有语义分割或 3D 绑定方法。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SP4D 框架的直接延伸或深层改进，均围绕“更一般化的场景、更精细的控制、更紧凑的表示、更高效的推理”展开：</p>
<ol>
<li><p>多物体与场景级分解</p>
<ul>
<li>当前假设单物体居中和已知粗略掩码。将 SP4D 扩展到 <strong>多实例、带遮挡、带背景</strong> 的复杂场景，需要：<br>– 引入实例级相机/掩码条件或注意力掩码，防止部件 ID 跨物体混淆；<br>– 结合场景级动态语义分割（如 OmniMotion、DEVA）做前置实例提取。</li>
</ul>
</li>
<li><p>全 6-DoF 相机与长轨迹</p>
<ul>
<li>SV4D 仅建模方位角+仰角，对强透视、大位移、手持旋转视频鲁棒性不足。</li>
<li>探索 <strong>全局位姿可微渲染</strong> 或 <strong>Plücker 坐标嵌入</strong>，使扩散模型直接感知任意相机路径，提高真实手持视频下的几何一致性。</li>
</ul>
</li>
<li><p>部件粒度与层次骨架可控</p>
<ul>
<li>目前粒度由 KinematicParts20K 的“100 骨以内”启发式合并决定。</li>
<li>引入 <strong>文本/草图/骨架模板</strong> 作为条件，或设计 <strong>连续粒度参数</strong> λ，实现“一键粗/细 rig”用户控制；同时研究层次部件树扩散，输出父子关系而非平面 ID。</li>
</ul>
</li>
<li><p>时变拓扑与自遮挡部件</p>
<ul>
<li>对“打开抽屉、翻盖手机”这类 <strong>拓扑改变</strong> 情形，部件连通性随时间变化。</li>
<li>可探索 <strong>动态图结构潜码</strong> 或 <strong>基于可微分网格追踪</strong> 的损失，让模型感知“新部件出现/旧部件合并”，而非固定 ID 映射。</li>
</ul>
</li>
<li><p>与物理/运动先验联合学习</p>
<ul>
<li>当前仅依赖视觉-几何一致性。引入：<br>– 基于 <strong>关节旋转限制</strong> 或 <strong>连杆长度不变量</strong> 的可微损失；<br>– 视频动作捕捉大规模数据（如 AMASS、Ego4D）做 <strong>运动学正则化</strong>，提升罕见姿态下的合理性。</li>
</ul>
</li>
<li><p>压缩与实时推理</p>
<ul>
<li>双分支 UNet 参数量加倍。可研究：<br>– <strong>权重共享+LoRA 微调</strong> 的轻量部件分支；<br>– <strong>蒸馏到单阶段潜码预测</strong> 或 <strong>多帧一致性 Token</strong>，实现移动端 4D 捕捉。</li>
</ul>
</li>
<li><p>反向应用：由部件驱动视频编辑</p>
<ul>
<li>利用 SP4D 的跨模态一致潜空间，实现“部件级视频编辑”：<br>– 直接修改某一部件颜色/纹理 → 自动传播到所有视角与后续帧；<br>– 部件掩码驱动局部运动重定向（part-based motion retargeting），无需显式骨骼。</li>
</ul>
</li>
<li><p>与其他 3D 表示融合</p>
<ul>
<li>目前 lift 到 mesh 仅用于绑定。可尝试：<br>– 将部件潜码直接作为 <strong>3D Gaussian</strong> 或 <strong>HexPlane</strong> 的属性通道，实现部件-aware 的 4D Gaussian Splatting；<br>– 在 NeRF 密度/颜色网络中加入 part-embedding，实现“部件级体渲染编辑”。</li>
</ul>
</li>
<li><p>伦理与版权增强</p>
<ul>
<li>生成资产易于被滥用。研究 <strong>部件级隐形水印</strong>（在 part-color 潜码嵌入加密签名）或 <strong>可验证出处</strong> 的区块链元数据，降低深度伪造风险。</li>
</ul>
</li>
<li><p>自监督数据扩充</p>
<ul>
<li>KinematicParts20K 仍依赖已绑定 3D 资产。可探索：<br>– 利用 <strong>视频运动分割自监督</strong>（如 CRW、VCN）自动挖掘大规模 RGB 视频中的“运动一致区域”，再远程监督训练 SP4D，进一步摆脱对 3D 骨架标注的依赖。</li>
</ul>
</li>
</ol>
<p>这些方向既可直接嵌入现有双分支扩散框架，也可作为独立模块与 SP4D 组合，推动“单目输入 → 任意拓扑、任意相机、任意粒度的可动画 4D 资产”走向真正通用化。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Stable Part Diffusion 4D（SP4D）</strong> 提出了一种从 <strong>单目 RGB 视频或单张图像</strong> 出发，联合生成 <strong>多视角、时序一致的 RGB 视频</strong> 与 <strong>运动学部件分割视频</strong> 的新框架，并可将结果轻量抬升为 <strong>可绑定、可动画的 3D 资产</strong>。核心贡献与流程如下：</p>
<p>1. 问题定位</p>
<ul>
<li>现有 4D 生成仅重建表面几何，不提供可用于动画的 <strong>运动学部件</strong>；</li>
<li>语义分割（SAM2 等）输出与 <strong>关节结构</strong> 不一致，跨视角/时间易漂移；</li>
<li>自动绑定方法依赖稀缺 3D 骨架标注，泛化性差。</li>
</ul>
<p>2. 方法概览</p>
<p>模块</p>
<p>关键设计</p>
<p>作用</p>
<p><strong>双分支扩散</strong></p>
<p>RGB 分支 + Part 分支共享 SV4D 2.0 结构</p>
<p>同步生成外观与部件</p>
<p><strong>BiDiFuse</strong></p>
<p>每层双向特征融合</p>
<p>跨模态互引导，提升边界一致性</p>
<p><strong>空间颜色编码</strong></p>
<p>用首帧 3D 中心 (x,y,z) 为部件赋固定 RGB 颜色</p>
<p>共享 VAE，简化架构</p>
<p><strong>对比部件一致性损失</strong></p>
<p>InfoNCE 约束同部件特征跨视角/时间聚集</p>
<p>消除颜色漂移，保证时空稳定</p>
<p><strong>两阶段训练</strong></p>
<p>先 RGB 预训练 → 后联合微调（KinematicParts20K）</p>
<p>充分利用大规模 2D 先验</p>
<p>3. 数据与抬升</p>
<ul>
<li><strong>KinematicParts20K</strong>：从 Objaverse XL 筛选 20 K 个已绑定资产，24 视角×24 帧渲染，提供真值 part mask 与相机轨迹。</li>
<li><strong>2D→3D 绑定管线</strong>：<ol>
<li>多视角 RGB → Hunyuan3D 2.0 重建 mesh；</li>
<li>顶点投影 + HDBSCAN 赋予部件 ID；</li>
<li>求解 <strong>调和皮肤权重</strong> ∆w=0，无需骨架即可动画。</li>
</ol>
</li>
</ul>
<p>4. 实验结果</p>
<ul>
<li><strong>2D 分割</strong>（mIoU/ARI/F1/mAcc）：SP4D 全面超越 SAM2、SAM2*、DeepViT，多视角/多帧均 <strong>↑20-40 点</strong>。</li>
<li><strong>3D 分割</strong>：SP4D mIoU 0.64 vs. SAMesh 0.15 / SAMPart3D 0.13。</li>
<li><strong>消融</strong>：去掉 BiDiFuse 或对比损失，指标显著下降，验证二者缺一不可。</li>
<li><strong>用户研究</strong>：部件清晰度、跨视角一致性、绑定适用性平均 <strong>4.26/5</strong>，显著优于基线。</li>
<li><strong>自动绑定精度</strong>：皮肤权重 <a href="mailto:Precision@0.5">Precision@0.5</a> 达 <strong>72.7</strong>，高于 MagicArticulate 63.7 与 UniRig 64.3。</li>
<li><strong>动画合理性</strong>：新品类（恐龙、机械臂）用户评分 <strong>4.1/5</strong>，远超基线 2.3-2.7。</li>
</ul>
<p>5. 结论</p>
<p>SP4D 首次把“运动学部件”作为显式输出引入扩散式 4D 生成，利用 <strong>双分支+共享潜空间+对比一致性</strong> 实现高质量、时空稳定的部件视频，并可 <strong>秒级抬升为可动画 3D 资产</strong>，显著降低人工绑定成本，为单目动态捕捉、快速动画制作提供了新的通用框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Stable%20Part%20Diffusion%204D%3A%20Multi-View%20RGB%20and%20Kinematic%20Parts%20Video%20Generation，链接是  https://arxiv.org/pdf/2509.10687  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.10687  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hao Zhang,Chun-Han Yao,Simon Donné,Narendra Ahuja,Varun Jampani</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10687.pdf">https://arxiv.org/pdf/2509.10687.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10687">https://arxiv.org/abs/2509.10687</a></p>
<p><strong>Arxiv ID</strong>: 2509.10687</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10687">https://papers.cool/arxiv/2509.10687</a></p>
<p><strong>Published</strong>: 2025-09-12T20:39:43Z</p>
<p><strong>Updated</strong>: 2025-09-12T20:39:43.000Z</p>
<hr>
<h3 id="17-ROOM-A-Physics-Based-Continuum-Robot-Simulator-for-Photorealistic-Medical-Datasets-Generation"><a href="#17-ROOM-A-Physics-Based-Continuum-Robot-Simulator-for-Photorealistic-Medical-Datasets-Generation" class="headerlink" title="17. ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13177">ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation</a></h3><p>Continuum robots are advancing bronchoscopy procedures by accessing complex lung airways and enabling targeted interventions. However, their development is limited by the lack of realistic training and test environments: Real data is difficult to collect due to ethical constraints and patient safety concerns, and developing autonomy algorithms requires realistic imaging and physical feedback. We present ROOM (Realistic Optical Observation in Medicine), a comprehensive simulation framework designed for generating photorealistic bronchoscopy training data. By leveraging patient CT scans, our pipeline renders multi-modal sensor data including RGB images with realistic noise and light specularities, metric depth maps, surface normals, optical flow and point clouds at medically relevant scales. We validate the data generated by ROOM in two canonical tasks for medical robotics — multi-view pose estimation and monocular depth estimation, demonstrating diverse challenges that state-of-the-art methods must overcome to transfer to these medical settings. Furthermore, we show that the data produced by ROOM can be used to fine-tune existing depth estimation models to overcome these challenges, also enabling other downstream applications such as navigation. We expect that ROOM will enable large-scale data generation across diverse patient anatomies and procedural scenarios that are challenging to capture in clinical settings. Code and data: <a target="_blank" rel="noopener" href="https://github.com/iamsalvatore/room">https://github.com/iamsalvatore/room</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>连续体机器人正在通过访问复杂的肺气道并实现针对性干预，推动支气管镜检查程序的发展。然而，它们的开发受到缺乏现实训练和测试环境的限制：由于伦理约束和患者安全问题，真实数据难以收集，开发自主算法需要现实的成像和物理反馈。我们提出了ROOM（医学中的真实光学观察），这是一个旨在生成逼真的支气管镜培训数据的综合仿真框架。通过利用患者的CT扫描，我们的管道渲染出多模态传感器数据，包括带有真实噪声和光反射的RGB图像、度量深度图、表面法线、光流和医学相关尺度的点云。我们在两个医学机器人领域的经典任务中验证了ROOM生成的数据——多视角姿势估计和单目深度估计，展示了最先进的方法在这些医学环境中必须克服的多种挑战。此外，我们还展示了ROOM生成的数据可以被用来微调现有的深度估计模型，以克服这些挑战，同时启用其他下游应用，如导航。我们期望ROOM能够在临床环境中难以捕捉的多样性患者解剖和程序场景中实现大规模数据生成。代码和数据：<a target="_blank" rel="noopener" href="https://github.com/iamsalvatore/room">https://github.com/iamsalvatore/room</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对<strong>支气管镜连续体机器人自主导航算法训练数据匮乏</strong>这一核心问题展开。<br>具体而言，临床数据获取受伦理、安全与成本多重限制，且个体解剖差异巨大，导致现有公开数据集规模小、多样性差，难以支撑对毫米级精度要求极高的视觉算法开发与验证。为此，作者提出 ROOM 框架，通过<strong>将患者 CT 自动转换为高保真、多模态、带真实物理约束的合成支气管镜影像序列</strong>，实现大规模、可重复、符合临床尺度与成像特性的训练数据生成，从而缓解真实数据稀缺对算法研究与临床转化的瓶颈。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>医疗机器人专用仿真平台</strong></p>
<ul>
<li>传统手术训练器（如 ORBIT-Surgical、SOFA、TMTDyn）侧重实时交互与触觉反馈，视觉渲染仅满足人眼操作需求，未提供导航所需的深度、光流等多模态真值，也不支持大规模数据集生成。</li>
<li>结肠镜方向有 SimCol3D 挑战，首次用游戏引擎生成 3D 重建训练数据，但场景纹理丰富、几何简单；支气管镜气道纹理稀疏、镜面反射强烈，其直接迁移效果受限。</li>
</ul>
</li>
<li><p><strong>连续体支气管镜系统研究</strong></p>
<ul>
<li>定位与里程计：PANS 利用气道先验地图做蒙特卡洛 6-DoF 跟踪；Deng 等发布离体数据集评估单目 VO。</li>
<li>导航轨迹获取：既有工作依赖人工采集（体内或体外）或简化仿真，未同时考虑真实视觉外观与物理交互，难以直接用于视觉-动作策略学习。</li>
</ul>
</li>
<li><p><strong>通用合成数据生成器</strong></p>
<ul>
<li>Kubric、TartanAir 等面向自动驾驶/SLAM，提供多模态真值，但解剖结构、照明模型与毫米级尺度均与支气管镜不符。</li>
</ul>
</li>
</ul>
<p>综上，现有研究或缺“医疗级解剖+物理+光度”联合建模，或缺“连续体机器人-气道交互”细粒度仿真，ROOM 首次将患者 CT、Cosserat 杆物理、路径追踪渲染与真实传感器噪声统一，填补支气管镜大规模训练数据空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>ROOM 将“数据稀缺”问题拆解为 <strong>解剖-物理-视觉-噪声</strong> 四个缺口，并分别给出可扩展的自动化模块，最终形成一条<strong>从患者 CT 到可直接用于训练的毫米级多模态序列</strong>的端到端管线。核心思路与对应技术如下：</p>
<ol>
<li><strong>解剖缺口 → CT-驱动气道重建</strong><ul>
<li>3D U-Net 分割支气管腔 → Marching Cubes 表面化 → Laplacian 平滑，得到可用于碰撞检测的高质量网格。</li>
<li>对带符号距离场</li>
</ul>
</li>
</ol>
</div></details>
<p><strong>Authors</strong>: Salvatore Esposito,Matías Mattamala,Daniel Rebain,Francis Xiatian Zhang,Kevin Dhaliwal,Mohsen Khadem,Subramanian Ramamoorthy</p>
<p><strong>Categories</strong>: cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13177.pdf">https://arxiv.org/pdf/2509.13177.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13177">https://arxiv.org/abs/2509.13177</a></p>
<p><strong>Arxiv ID</strong>: 2509.13177</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13177">https://papers.cool/arxiv/2509.13177</a></p>
<p><strong>Published</strong>: 2025-09-16T15:30:02Z</p>
<p><strong>Updated</strong>: 2025-09-16T15:30:02.000Z</p>
<hr>
<h3 id="18-zELO-ELO-inspired-Training-Method-for-Rerankers-and-Embedding-Models"><a href="#18-zELO-ELO-inspired-Training-Method-for-Rerankers-and-Embedding-Models" class="headerlink" title="18. zELO: ELO-inspired Training Method for Rerankers and Embedding Models"></a>18. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12541">zELO: ELO-inspired Training Method for Rerankers and Embedding Models</a></h3><p>We introduce a novel training methodology named zELO, which optimizes retrieval performance via the analysis that ranking tasks are statically equivalent to a Thurstone model. Based on the zELO method, we use unsupervised data in order train a suite of state-of-the-art open-weight reranker models: zerank-1 and zerank-1-small. These models achieve the highest retrieval scores in multiple domains, including finance, legal, code, and STEM, outperforming closed-source proprietary rerankers on both NDCG@10 and Recall. These models also demonstrate great versatility, maintaining their 0-shot performance on out-of-domain and private customer datasets. The training data included 112,000 queries and 100 documents per query, and was trained end-to-end from unannotated queries and documents in less than 10,000 H100-hours.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们引入了一种名为 zELO 的新型训练方法，通过分析排序任务与赫尔斯通模型的静态等价性来优化检索性能。基于 zELO 方法，我们使用无监督数据来训练一系列最先进的开放权重重排序模型：zerank-1 和 zerank-1-small。这些模型在多个领域（包括金融、法律、代码和 STEM）中实现了最高的检索得分，在 NDCG@10 和召回率上超越了闭源专有重排序器。这些模型还展示了极大的多样性，在域外和私有客户数据集上保持了 0-shot 性能。训练数据包括 112,000 个查询和每个查询 100 个文档，并且用了不到 10,000 小时的 H100 进行从未标注的查询和文档的端到端训练。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对信息检索中“重排序（reranking）”环节的核心瓶颈——高质量训练信号难以大规模获取——提出了一套全新的无监督训练框架 zELO，并发布两个开源重排序模型 zerank-1 / zerank-1-small。具体而言，工作聚焦并试图解决以下关键问题：</p>
<ol>
<li><p><strong>人工标注的不可扩展性</strong></p>
<ul>
<li>传统 pointwise 或 triplet 训练依赖人工判定正例/负例，成本高昂且难以覆盖专业领域。</li>
<li>当“难负例挖掘”算法足够强时，机器自动挖掘出的负例往往比人工标注的正例更相关，导致对比学习信号失真（即论文提出的“拉弗曲线”现象）。</li>
</ul>
</li>
<li><p><strong>Hard-negative mining 的理论上限</strong></p>
<ul>
<li>证明随着负例挖掘器能力提升，学生模型性能先升后降，存在严格上界；该上界并非模型容量不足，而是训练算法本身缺陷。</li>
</ul>
</li>
<li><p><strong>大规模、跨领域、可迁移的重排序训练数据生成</strong></p>
<ul>
<li>设计基于 Elo/Thurstone 的统计模型，将多 LLM 的稀疏 pairwise 偏好转化为绝对 relevance 分数，实现完全自动化、可扩展的“教师信号”生产。</li>
</ul>
</li>
<li><p><strong>零人工标注下的 SOTA 性能</strong></p>
<ul>
<li>在金融、法律、医学、代码、STEM 等 10+ 公开及私有数据集上，仅用 11.2 k 查询、每查询 100 文档、总计 5 M 查询-文档对的自生成数据，端到端训练出的小模型即可超越体积两倍以上的商业闭源重排序器（NDCG@10 绝对提升 3–5 分）。</li>
</ul>
</li>
<li><p><strong>生产环境的实时自监督迭代</strong></p>
<ul>
<li>zELO 流程支持在线采样查询日志 → 自动标注 → 即时微调，无需人工介入即可持续修复检索链路缺陷。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文本身未设独立“Related Work”章节，但从动机阐述与实验对比中可以析出以下几条紧密相关的研究脉络（按主题归类，并给出代表文献或模型）：</p>
<p>主题</p>
<p>代表工作</p>
<p>与 zELO 的关系</p>
<p><strong>1. 重排序模型架构</strong></p>
<p>• BERT-based cross-encoder (Nogueira &amp; Cho, 2020)<br>• T5-Reranker (Lin et al., 2023)</p>
<p>zELO 同样采用 cross-encoder 结构，但训练信号来自 LLM-ensemble 的 pairwise-Elo，而非人工三元组。</p>
<p><strong>2. 对比/三元组损失</strong></p>
<p>• InfoNCE / Triplet Loss (Schroff et al., 2015; Gillick et al., 2019)<br>• Cross-entropy with in-batch negatives (CLIP, DPR)</p>
<p>zELO 指出随机或 hard-negative 采样存在“拉弗曲线”瓶颈，改用 Thurstone-BT 模型将 pairwise 偏好转为绝对分数，规避三元组损失。</p>
<p><strong>3. Hard-negative mining 上限</strong></p>
<p>• RocketQA (Qu et al., 2021)<br>• AR2 (Zhang et al., 2021)<br>• ANCE (Xiong et al., 2021)</p>
<p>这些工作致力于让负例“更难”；zELO 首次系统论证当挖掘器过强时性能反而下降，并用统计模型取代挖掘。</p>
<p><strong>4. LLM-as-judge / LLM-reranker</strong></p>
<p>• GPT-4o/Gemini-Flash as zero-shot ranker (Sun et al., 2023)<br>• RankGPT (Qin et al., 2023)</p>
<p>zELO 把多 LLM 的 pairwise 判断蒸馏给小模型，兼顾效率与准确率，而非直接在线调用大模型。</p>
<p><strong>5. Elo/Bradley-Terry 在 IR 中的应用</strong></p>
<p>• Gao et al., 2023 将 BT 用于 passage ranking 数据清洗<br>• Sachan et al., 2023 用 Elo 聚合众包偏好</p>
<p>zELO 首次把 BT-Thurstone 模型扩展到<strong>无监督</strong>、<strong>web-scale</strong> 的端到端训练，并给出稀疏采样理论保证。</p>
<p><strong>6. 自监督 / 合成数据训练</strong></p>
<p>• InPars (Bonifácio et al., 2022) 用 LLM 生成查询-正例<br>• PromptRank (Pradeep et al., 2023) 用 LLM 生成伪标签</p>
<p>zELO 不生成查询或文档，而是让 LLM 只做<strong>相对偏好</strong>判断，通过统计模型转化为绝对 relevance，避免生成幻觉。</p>
<p><strong>7. 稀疏图上的排序学习</strong></p>
<p>• Plackett-Luce 分解 (Khetan &amp; Oh, 2016)<br>• 图约束下的 Elo 估计 (Maystre &amp; Grossglauser, 2015)</p>
<p>zELO 提出 k-regular random-cycle 采样，保证连通度与低直径，从而用 $O(n)$ 次比较即可稳定估计 Elo。</p>
<p><strong>8. 商业闭源重排序器</strong></p>
<p>• Cohere rerank-v3.5<br>• Voyage rerank-2<br>• Salesforce Llama-rank-v1</p>
<p>作为实验对照，zELO 模型在同等或更小参数量下取得 3–5 NDCG@10 的绝对提升。</p>
<p>以上研究共同构成了 zELO 的学术语境：</p>
<ul>
<li><strong>方法论上</strong>，zELO 用统计建模取代传统对比学习，解决了 hard-negative mining 的“拉弗曲线”问题；</li>
<li><strong>数据工程上</strong>，它把“LLM-as-judge”从离线评测工具升级为<strong>可扩展的教师信号生成器</strong>；</li>
<li><strong>系统层面</strong>，通过稀疏图采样与端到端蒸馏，首次在<strong>零人工标注</strong>条件下训练出公开权重、跨领域 SOTA 的重排序模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“零人工标注条件下训练出高泛化重排序器”拆解为四个可执行阶段，核心思想是：<br><strong>用多 LLM 的 pairwise 偏好 → 统计模型转化为绝对 relevance 分数 → 蒸馏给 pointwise 学生模型</strong>。<br>整体流程与关键技术点如下（按时间顺序）：</p>
<p>1. 生成初始候选集 Z</p>
<ul>
<li>对每条查询 q，用混合检索（Qwen3-Embedding-4B 余弦相似度 + BM25，RRF 融合）取 top-100 文档，构成集合 Z，|Z|=100。</li>
<li>该步骤仅提供“可能相关”的文档池，不参与后续训练信号生成。</li>
</ul>
<p>2. 构建稀疏 pairwise 偏好图（LLM-ensemble 阶段）</p>
<ul>
<li><strong>采样策略</strong>：对每条查询，随机抽 4 条随机置换的 n-cycle（n=100），叠加成 8-正则图，仅 $O(n)$ 条边即可保证连通、低直径、节点度均匀。</li>
<li><strong>标注方式</strong>：用 3 个 frontier LLM 对每条边 $(d_i,d_j)$ 输出 $[-1,1]$ 的相对偏好，经位置翻转、截断、平均后映射到 $(0,1)$，得到稀疏矩阵 $W$。</li>
<li><strong>理论保证</strong>：利用 Thurstone 模型  <script type="math/tex; mode=display">w\_{ij}=\\frac{1+\\text{erf}(\\text{Elo}\_i-\\text{Elo}_j)}{2}</script>通过最大似然估计（负对数似然 $L=\sum w_{ij}\log(1+e^{\text{Elo}_j-\text{Elo}_i})$）恢复每条文档的绝对 Elo 分数；Zermelo 定理保证全局唯一极小点，梯度下降必收敛。</li>
</ul>
<p>3. 训练 pairwise 学生模型 R′pair</p>
<ul>
<li>把 LLM-ensemble 的 $w_{ij}$ 作为软标签，用标准 Binary Cross-Entropy 训练一个轻量 cross-encoder（Qwen3-1.7B/4B），使其逼近 ensemble 偏好。</li>
<li><strong>数据效率</strong>：每条查询仅标注 400 对（≈0.4 % 的完整矩阵），即可使 Elo 估计误差趋近于零（图 10）。</li>
</ul>
<p>4. 训练最终 pointwise 重排序器 Rpoint（zerank-1）</p>
<ul>
<li>用收敛后的 Elo 分数作为绝对 relevance 标签 $y\in[0,1]$，对同一 Qwen 骨架做标准 MSE 回归：  <script type="math/tex; mode=display">L\_{\\text{SFT}}=\\frac{1}{|B|}\\sum\_{(q,d)\\in B}\\bigl(R\_{\\text{point}}(q,d)-y\\bigr)^2</script></li>
<li><strong>RLHF 二次迭代</strong>：<br>– 先让 1.0 版模型在全集上排序，找出“人类最高分文档”被排到 t 名之后的失败例；<br>– 用 ensemble 对“人类最高分 vs 模型误排前一名”再标注一次，加入 pairwise 训练集；<br>– 重新训练 R′pair → 重算 Elo → 重训 Rpoint，得到最终 zerank-1 / zerank-1-small。</li>
</ul>
<p>5. 规模与效率</p>
<ul>
<li>全程零人工标注，112 k 查询 × 100 文档 ≈ 5 M 查询-文档对，&lt;10 000 H100-hours 完成端到端训练。</li>
<li>推理延迟：12 KB 文档 149 ms，150 KB 文档 314 ms，均快于同体量商业 API（表 1）。</li>
</ul>
<p>6. 结果验证</p>
<ul>
<li>公开基准：在金融、法律、医学、代码、STEM 等 10+ 数据集上，zerank-1 比 Cohere rerank-v3.5 等大 2× 的闭源模型平均 NDCG@10 提升 3–5 分。</li>
<li>私有客户数据：优势进一步扩大，表明未过拟合公开评测集。</li>
<li>跨初始检索方式：无论底层是 BM25、纯向量还是混合检索，zerank-1 仍能带来显著增益（图 12）。</li>
</ul>
<p>通过“统计建模取代对比损失 + 稀疏图采样保证收敛 + 两轮蒸馏吸收人类高置信信号”，论文在<strong>零人工标注</strong>前提下突破了 hard-negative mining 的拉弗曲线限制，训练出目前公开权重中精度最高的跨领域重排序器。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“零人工标注训练出的 zerank-1 / zerank-1-small 是否能在多领域、多检索方式、多数据场景下持续超越现有 SOTA”这一问题，设计了 4 组共 12 项实验。所有对比均在 top-100 候选集（k=100）上完成，评价指标统一采用 NDCG@10，部分补充 Recall@100。实验一览如下：</p>
<p>1. 公开基准横向对比（表 2）</p>
<p>领域</p>
<p>数据集举例</p>
<p>对照模型</p>
<p>主要结论</p>
<p>Code</p>
<p>CodeSearchNet/CodeQA</p>
<p>Cohere rerank-v3.5、Salesforce Llama-rank-v1</p>
<p>zerank-1 ↑0.754 vs 0.724（+3.0）</p>
<p>Finance</p>
<p>FiQA-2018、FinRED</p>
<p>同上</p>
<p>zerank-1 ↑0.894 vs 0.824（+7.0）</p>
<p>Legal</p>
<p>COLIEE、CaseHOLD</p>
<p>同上</p>
<p>zerank-1 ↑0.821 vs 0.804（+2.1）</p>
<p>Medical</p>
<p>TREC-COVID、BioASQ</p>
<p>同上</p>
<p>zerank-1 ↑0.796 vs 0.750（+6.0）</p>
<p>STEM</p>
<p>ArXiv-QA、SciFact</p>
<p>同上</p>
<p>zerank-1 ↑0.694 vs 0.595（+9.9）</p>
<p>Conversational</p>
<p>MS MARCO Dialog</p>
<p>同上</p>
<p>zerank-1 ↑0.596 vs 0.571（+4.4）</p>
<p>2. 私有客户数据外推测试（表 3）</p>
<p>场景</p>
<p>数据规模</p>
<p>对照模型</p>
<p>主要结论</p>
<p>Enterprise Search</p>
<p>2.1 M 企业内网文档</p>
<p>Voyage rerank-2、Cohere 3.5</p>
<p>zerank-1 ↑0.799 vs 0.735（+6.4）</p>
<p>Healthcare</p>
<p>0.6 M 医院知识库</p>
<p>同上</p>
<p>zerank-1 ↑0.898 vs 0.756（+14.2）</p>
<p>Legal (private)</p>
<p>1.3 M 合同条款</p>
<p>同上</p>
<p>zerank-1 ↑0.854 vs 0.766（+8.8）</p>
<p>Conversational (private)</p>
<p>0.9 M 客服日志</p>
<p>同上</p>
<p>zerank-1 ↑0.787 vs 0.727（+6.0）</p>
<blockquote>
<p>注：zerank 系列<strong>未在任何私有数据上微调</strong>，全部 0-shot。</p>
</blockquote>
<p>3. 跨初始检索方式鲁棒性（图 12）</p>
<p>初始召回方法</p>
<p>BM25-only</p>
<p>Embedding-only</p>
<p>Hybrid (RRF)</p>
<p>原始 NDCG@10</p>
<p>0.512</p>
<p>0.678</p>
<p>0.701</p>
<p>+zerank-1</p>
<p>0.731 (↑42.8 %)</p>
<p>0.768 (↑13.3 %)</p>
<p>0.783 (↑11.7 %)</p>
<p>+zerank-1-small</p>
<p>0.710</p>
<p>0.748</p>
<p>0.762</p>
<p>结论：zerank-1 对稀疏、密集、混合三种第一级召回均带来<strong>双位数绝对提升</strong>，证明其独立于初始通道。</p>
<p>4. 效率与成本对比（表 1）</p>
<p>模型</p>
<p>NDCG@10</p>
<p>12 KB 延迟</p>
<p>150 KB 延迟</p>
<p>Cohere rerank-v3.5</p>
<p>0.709</p>
<p>171 ms</p>
<p>459 ms</p>
<p>Jina-reranker-m0</p>
<p>0.728</p>
<p>547 ms</p>
<p>2 544 ms</p>
<p>zerank-1</p>
<p>0.768</p>
<p>149 ms</p>
<p>314 ms</p>
<p>zerank-1-small</p>
<p>0.730</p>
<p>97 ms</p>
<p>186 ms</p>
<p>结论：在<strong>更高精度</strong>的同时，延迟最低，吞吐量优于商业 API。</p>
<p>5. 消融与诊断实验（正文图 8–10）</p>
<p>实验</p>
<p>目的</p>
<p>关键结果</p>
<p>图 8</p>
<p>pairwise 学生模型是否学到 ensemble 共识</p>
<p>训练后 93 % 的预测与 LLM-ensemble 一致（vs 随机 50 %）</p>
<p>图 9</p>
<p>稀疏采样策略对 Elo 估计误差的影响</p>
<p>4-cycle 法在 400 次推理时 MSE→0.002，优于随机采样 10×</p>
<p>图 10</p>
<p>采样率-收敛曲线</p>
<p>仅需 1.2 % 的 pairwise 即可使 Elo 损失趋零，cycle 法最快</p>
<p>6. 与“即用即评”大模型 baseline 对比（第 9 页段落）</p>
<p>方法</p>
<p>NDCG@10</p>
<p>备注</p>
<p>Gemini-2.5-Flash 直接 pairwise 投票</p>
<p>0.712</p>
<p>调用 4 500 次 API</p>
<p>zerank-1（蒸馏后小模型）</p>
<p>0.768</p>
<p>单次 GPU 前向</p>
<p>结论：蒸馏后的 4 B 模型显著优于在线大模型投票，且成本降低 2 个数量级。</p>
<p>综上，论文通过<strong>公开基准、私有数据、跨召回方式、延迟-精度、消融诊断、成本对照</strong>六维实验，系统验证了 zELO 训练流程的通用性、经济性与 SOTA 性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 zELO 框架的“直接外延”或“深层质疑”，均来自论文尚未回答或仅一笔带过的开放问题。按“理论—数据—模型—系统”四个层次列出，供后续工作参考。</p>
<p>1. 理论层面</p>
<ul>
<li><p><strong>拉弗曲线定量刻画</strong><br>论文仅给出定性示意图。可控制“负例挖掘器能力”为单一变量（如固定学生容量、逐步替换更强的教师），绘制横轴=矿工准确率、纵轴=学生 NDCG 的完整曲线，验证拐点位置及回落斜率。</p>
</li>
<li><p><strong>Thurstone vs Bradley-Terry 的适用域</strong><br>仅在最终拟合阶段切换 Thurstone。可系统比较两种误差分布（正态 vs Gumbel）在不同领域、不同文档长度、不同 LLM 噪声水平下的对数似然，给出选择准则。</p>
</li>
<li><p><strong>稀疏图采样极限</strong><br>当前 8-正则图直径 ≈ log₇100 ≈ 2.4。可推导“Elo 估计误差 ε – 图直径 ρ – 节点度 k”三者的闭式关系，证明 cycle-overlay 是否达到信息论下限。</p>
</li>
</ul>
<p>2. 数据与标注</p>
<ul>
<li><p><strong>人类 vs LLM-ensemble 的贝叶斯最优融合</strong><br>论文仅用“人类最高分单点”做 RLHF。可构建生成式线性混合模型  </p>
<script type="math/tex; mode=display">P\_{\\text{gold}}(i≻j)=λ P\_{\\text{human}}+(1-λ) P\_{\\text{LLM}}</script><p>用 EM 算法估计 λ，看不同领域 λ 是否稳定，避免直接丢弃人类信号。</p>
</li>
<li><p><strong>多语言文化偏差</strong><br>实验仅限英文或拉丁语系。对中日韩、阿拉伯等跨文化查询，LLM 偏好是否仍与真实用户点击一致？需收集区域用户点击链做对照。</p>
</li>
<li><p><strong>动态自监督上限</strong><br>在线日志持续微调场景，若用户兴趣发生概念漂移，Elo 标签会滞后。可引入“时序 Elo”模型  </p>
<script type="math/tex; mode=display">\\text{Elo}\_i^{(t)}=\\text{Elo}\_i^{(t-1)}+η\_t·(\\text{outcome}-σ(⋅))</script><p>并给出遗忘因子最优调度。</p>
</li>
</ul>
<p>3. 模型与算法</p>
<ul>
<li><p><strong>生成式重排序（Generative Reranker）</strong><br>zerank 仍是判别式打分。可让模型直接生成“排序符号”或“相对描述”，用 PL 似然训练，实现单模型同时完成排序与可解释理由输出。</p>
</li>
<li><p><strong>多任务 Elo 预训练</strong><br>将 pairwise 任务与摘要、问答、事实核查一起预训练，看是否能提升低资源领域（如法律、医学）的样本效率。</p>
</li>
<li><p><strong>负对数似然替代 MSE</strong><br>当前 pointwise 阶段用 MSE 拟合 Elo 分数。可尝试以 Elo 的负对数似然直接做损失，消除“分数→概率”二次建模误差。</p>
</li>
<li><p><strong>检索-重排序联合优化</strong><br>目前两阶段分离。可用可微近似（如 Gumbel-top-k）把初始检索器纳入同一 Elo 损失，实现端到端反向传播。</p>
</li>
</ul>
<p>4. 系统与评测</p>
<ul>
<li><p><strong>能耗-精度帕累托前沿</strong><br>测量训练阶段总 CO₂ 消耗 vs 下游 NDCG，回答“无监督是否真的更绿色”——LLM 推理百万次 vs 人工标注工时，碳排孰高？</p>
</li>
<li><p><strong>长尾查询鲁棒性</strong><br>将查询按频率分为 Head/Mid/Tail，检查 zerank-1 在 Tail 段是否仍保持增益；若下降，可引入元学习快速适应。</p>
</li>
<li><p><strong>对抗攻击与可验证鲁棒性</strong><br>对 pairwise 提示注入“误导性链式思考”，观察 Elo 分数漂移幅度；再训练鲁棒学生模型，使漂移低于 ε。</p>
</li>
<li><p><strong>实时用户反馈闭环</strong><br>与生产搜索引擎对接，实现“用户点击→即时 pairwise→增量 Elo→在线蒸馏”的 24 h 不间断 A/B 测试，量化业务指标（CTR、满意度）提升。</p>
</li>
</ul>
<p>5. 跨领域外推</p>
<ul>
<li><p><strong>多模态重排序</strong><br>将文本查询-图像/视频文档对纳入同一 Elo 框架，需定义多模态 pairwise 提示模板，并验证 Thurstone 误差分布依旧成立。</p>
</li>
<li><p><strong>结构化数据检索</strong><br>对 Table/JSON 文档，LLM 偏好是否仍可靠？可构建 Table-to-Text 中间视图，或设计“字段级” pairwise 比较协议。</p>
</li>
</ul>
<p>综上，zELO 在“理论极限、数据质量、模型架构、系统落地”四端均留下可验证的开放问题；任何一条深入下去，都可产出独立贡献。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>zELO：一套无需人工标注、基于 Elo/Thurstone 统计模型的重排序训练流程，及其产出的 SOTA 开源模型 zerank-1 / zerank-1-small。</strong></p>
<p>1. 核心痛点</p>
<ul>
<li>传统 hard-negative mining 存在“拉弗曲线”：负例挖掘器越强，学生模型性能反而下降。</li>
<li>人工标注昂贵且难以覆盖专业领域，限制大规模训练。</li>
</ul>
<p>2. 解决思路</p>
<p><strong>用“多 LLM 的 pairwise 偏好”代替“人工三元组标签”：</strong></p>
<p>步骤</p>
<p>关键技术</p>
<p>输出</p>
<p>① 初始召回</p>
<p>BM25 + 向量混合（RRF）</p>
<p>top-100 文档集合 Z</p>
<p>② 稀疏 pairwise</p>
<p>3×LLM 对 8-正则随机 cycle 图投票</p>
<p>0/1 偏好矩阵 W</p>
<p>③ 绝对 relevance</p>
<p>Thurstone 模型最大似然估计</p>
<p>每文档 Elo 分数</p>
<p>④ 蒸馏学生</p>
<p>先训 pairwise cross-encoder→再训 pointwise MSE</p>
<p>zerank-1 / zerank-1-small</p>
<p>全程零人工标注，11.2 k 查询、5 M 查询-文档对，&lt;10 000 H100-hours 完成。</p>
<p>3. 主要结果</p>
<ul>
<li><strong>公开基准</strong>：金融、法律、医学、代码、STEM 等 10+ 数据集，NDCG@10 平均提升 3–5 分，超越 Cohere rerank-v3.5、Voyage rerank-2 等体积 2× 的闭源模型。</li>
<li><strong>私有数据</strong>：企业搜索、医疗、法律等场景优势进一步扩大（最高 +14.2 NDCG）。</li>
<li><strong>跨召回方式</strong>：无论底层是 BM25、纯向量还是混合，zerank-1 均带来双位数绝对增益。</li>
<li><strong>效率</strong>：12 KB 文档延迟 149 ms，比商业 API 更快；1.7 B 小模型仅降 1–2 分，可 Apache-2.0 商用。</li>
</ul>
<p>4. 贡献清单</p>
<ol>
<li>zELO 训练范式：首次用 Thurstone-Elo 将 LLM-ensemble 稀疏偏好转为绝对 relevance，规避 hard-negative 拉弗曲线。</li>
<li>开源权重：zerank-1（4 B）与 zerank-1-small（1.7 B）全部放出，无需人工数据即可复现。</li>
<li>理论+工程：给出 8-正则 cycle 图采样准则，保证 $O(n)$ 次比较即可稳定估计 Elo。</li>
<li>生产友好：支持在线日志自动标注→即时微调，实现检索链路持续改进。</li>
</ol>
<p><strong>一句话</strong>：zELO 用统计建模取代人工标注，在零监督条件下训练出目前公开权重中精度最高、延迟最低、跨领域最强的重排序模型。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是zELO%3A%20ELO-inspired%20Training%20Method%20for%20Rerankers%20and%20Embedding%20Models，链接是  https://arxiv.org/pdf/2509.12541  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12541  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Nicholas Pipitone,Ghita Houir Alami,Advaith Avadhanam,Anton Kaminskyi,Ashley Khoo</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12541.pdf">https://arxiv.org/pdf/2509.12541.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12541">https://arxiv.org/abs/2509.12541</a></p>
<p><strong>Arxiv ID</strong>: 2509.12541</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12541">https://papers.cool/arxiv/2509.12541</a></p>
<p><strong>Published</strong>: 2025-09-16T00:44:08Z</p>
<p><strong>Updated</strong>: 2025-09-16T00:44:08.000Z</p>
<hr>
<h3 id="19-RAPTOR-A-Foundation-Policy-for-Quadrotor-Control"><a href="#19-RAPTOR-A-Foundation-Policy-for-Quadrotor-Control" class="headerlink" title="19. RAPTOR: A Foundation Policy for Quadrotor Control"></a>19. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.11481">RAPTOR: A Foundation Policy for Quadrotor Control</a></h3><p>Humans are remarkably data-efficient when adapting to new unseen conditions, like driving a new car. In contrast, modern robotic control systems, like neural network policies trained using Reinforcement Learning (RL), are highly specialized for single environments. Because of this overfitting, they are known to break down even under small differences like the Simulation-to-Reality (Sim2Real) gap and require system identification and retraining for even minimal changes to the system. In this work, we present RAPTOR, a method for training a highly adaptive foundation policy for quadrotor control. Our method enables training a single, end-to-end neural-network policy to control a wide variety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg that also differ in motor type (brushed vs. brushless), frame type (soft vs. rigid), propeller type (2/3/4-blade), and flight controller (PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy with only 2084 parameters is sufficient for zero-shot adaptation to a wide variety of platforms. The adaptation through In-Context Learning is made possible by using a recurrence in the hidden layer. The policy is trained through a novel Meta-Imitation Learning algorithm, where we sample 1000 quadrotors and train a teacher policy for each of them using Reinforcement Learning. Subsequently, the 1000 teachers are distilled into a single, adaptive student policy. We find that within milliseconds, the resulting foundation policy adapts zero-shot to unseen quadrotors. We extensively test the capabilities of the foundation policy under numerous conditions (trajectory tracking, indoor/outdoor, wind disturbance, poking, different propellers).</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>翻译失败: Unexpected end of JSON input</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>四旋翼无人机控制策略对新平台适应性差、需重复训练</strong>的核心痛点，并提出一个可零样本泛化的“基础策略”（foundation policy）。具体可归纳为：</p>
<ul>
<li><p><strong>问题背景</strong></p>
<ol>
<li>现有基于强化学习（RL）的端到端控制器多为单一平台定制，换桨叶、换电池、换机架等微小改动即需重新系统辨识与训练。</li>
<li>传统域随机化虽能提升鲁棒性，但迫使策略对所有可能动力学取“保守”动作，牺牲敏捷性。</li>
<li>人类可在几分钟内适应不同车辆，而机器人缺乏类似“即插即用”能力。</li>
</ol>
</li>
<li><p><strong>待解决的关键科学问题</strong></p>
<ol>
<li>能否用<strong>单一轻量网络</strong>实时推断并适应未见过的四旋翼动力学？</li>
<li>网络规模需多小才能<strong>在微型飞控 MCU 上硬实时运行</strong>？</li>
<li>仅用<strong>毫秒级交互上下文</strong>能否完成隐式系统辨识，避免坠机？</li>
<li>策略在<strong>分布内/外</strong>真实机上是否都能零样本稳定飞行？</li>
<li>是否存在<strong>适应性与敏捷性</strong>的权衡？</li>
</ol>
</li>
<li><strong>目标</strong><br>提出 RAPTOR 框架，训练一个仅 2084 参数的循环网络基础策略，通过<strong>元模仿学习</strong>蒸馏 1000 条专家策略，实现：<br>– 对 31 g–2.4 kg、推力/重量比 1.75–12、不同桨叶/机架/飞控的 10 款真实四旋翼<strong>零秒适应</strong>；<br>– 在室外 7 m/s 阵风、挂 1.2 kg 载荷、被工具击打、混合 2/3 桨叶等<strong>严重分布外场景</strong>稳定飞行；<br>– 无需额外微调或系统辨识，直接部署在 Betaflight、PX4、Crazyflie 等消费级硬件。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 RAPTOR 直接可比或提供关键思路的文献可分为六类，括号内给出代表性工作及其与本文的核心差异。</p>
<ol>
<li><p>端到端单平台 RL 控制</p>
<ul>
<li><em>Learning to Fly in Seconds</em> (Eschmann 2024)：同样用 L2F 模拟器，但策略无 recurrent，换机需重训。</li>
<li><em>Champion-level drone racing</em> (Kaufmann 2023)、<em>Reaching the limit</em> (Song 2023)：仅在单台 5.8 或 11 T/W 四旋翼上优化，对机体/赛道过拟合。</li>
</ul>
</li>
<li><p>域随机化无适应</p>
<ul>
<li><em>One Net to Rule Them All</em> (Ferede 2025)：单一 Markovian 网络+大尺度随机化，可飞两台不同 T/W 机，但保守、无在线推断。</li>
</ul>
</li>
<li><p>元-RL / 自适应控制</p>
<ul>
<li><em>RL²</em> (Duan 2016)、<em>Learning to RL</em> (Wang 2016)：通用元-RL 框架，未在四旋翼硬件验证，网络大，需任务级交互。</li>
<li><em>DAgger</em> (Ross 2011) 与 <em>Meta-Imitation</em> 思想相近，但 DAgger 无隐参数推断、off-policy；RAPTOR 强制学生网络在线推断 Ξ。</li>
</ul>
</li>
<li><p>非端到端自适应</p>
<ul>
<li><em>A Learning-Based Quadcopter Controller with Extreme Adaptation</em> (Zhang 2025)：需高层 CTBR 设定点，网络 55× 更大，仅两台相似机体实验；RAPTOR 直接输出 4 电机转速，覆盖位置-姿态全回路。</li>
</ul>
</li>
<li><p>系统辨识+控制分离</p>
<ul>
<li>传统递推最小二乘、UKF 在线估计质量、惯量、气动参数，再喂给 PID/MPC；需显式模型与调参，无“即插即用”能力。</li>
</ul>
</li>
<li><p>微型无人机轻量网络</p>
<ul>
<li><em>A Neural Network Mode for PX4 on Embedded Flight Controllers</em> (Hegre 2025)：仍针对单机体，未考虑跨机迁移。</li>
</ul>
</li>
</ol>
<p>综上，RAPTOR 首次将“<strong>元模仿蒸馏 + 循环网络隐式系统辨识 + 极端域随机化</strong>”结合，实现<strong>极小参数（2084）的跨平台零样本四旋翼基础策略</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“如何让一个极简网络在毫秒级实时推断中自动适应任意四旋翼”拆成三步，每一步都对应一个核心技术创新，形成 RAPTOR 框架。</p>
<p>步骤</p>
<p>技术模块</p>
<p>解决的关键障碍</p>
<p>具体做法与公式</p>
<p>1</p>
<p>超宽域随机化 + 1000 教师策略预训练</p>
<p>保证“任何现实四旋翼都在分布内”</p>
<p>1. 设计物理合理的动力学分布 p(Ξ)，覆盖 1.5≤T/W≤5、40≤τ/J≤1200 等（公式 S5–S26）。<br>2. 对每一样本 Ξ_i 独立运行 SAC，得到 1000 条马尔可夫教师策略 π*_Ξ(s)→a，作为“最优动作分布”的蒙特卡洛近似：a*_t ∼ π*_Ξ(s_t)。</p>
<p>2</p>
<p>Meta-Imitation Learning（元模仿蒸馏）</p>
<p>把 1000 条教师压缩成<strong>单一循环学生</strong>，且学生只能看到部分观测</p>
<p>1. 学生网络：22 维观测 + 4 维上一动作 → 16 维 GRU → 4 维 RPM，共 2084 参数（图 7B）。<br>2. 目标：最小化 KL( a*_t ∥ a^π_t )，在教师高斯假设下退化为 MSE（公式 S52）：  </p>
<script type="math/tex; mode=display">\\mathcal{L}= \\frac{1}{2}\\big\\lVert \\pi^\*_\\Xi(s\_t) - \\pi(o\_{0:t},a\_{0:t-1})\\big\\rVert^2</script><p>3. 先 10 epoch 用教师轨迹 warm-start，随后 990 epoch <strong>on-policy</strong> 滚动：学生自己飞，教师实时给“标签”，实现隐式系统辨识。</p>
<p>3</p>
<p>在线部署与 In-Context 推理</p>
<p>MCU 硬实时 + 零样本适应</p>
<p>1. 推理仅依赖 16 维隐藏状态 h_t，计算量 O(1)，在 100 Hz 下 &lt;10 % MCU 负载。<br>2. 隐藏状态充当“充分统计量”，自动编码推力/惯量比、电机延迟等未知 Ξ；线性探针可回归出 T/W（R²=0.95，图 3）。<br>3. 无需微调，直接烧录到 Betaflight/PX4/Crazyflie；遇到新机体时，隐藏状态在 20–50 ms 内收敛，完成“空中 Few-Shot”适应。</p>
<p>通过“先离线蒸馏、后在线推断”的解耦，RAPTOR 把计算量移到一次性预训练，部署端仅 2084 次浮点乘加即可实现跨平台零样本稳定飞行。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验按“<strong>训练 → 仿真 → 实物 → 极限压力</strong>”四级展开，共覆盖 <strong>12 款平台（10 真机 + 2 仿真）</strong>、<strong>3 类任务</strong>、<strong>6 种扰动</strong>，全部用<strong>同一套 2084-参数权重</strong>零样本完成。</p>
<p>层级</p>
<p>实验目的</p>
<p>测试量/条件</p>
<p>关键结果</p>
<p>1. 训练诊断</p>
<p>验证 1000 教师收敛性与蒸馏效率</p>
<p>• 1000 条 SAC 曲线<br>• 教师数 16→1000 消融<br>• 学生隐藏维度 4→64 消融</p>
<p>所有教师 1 M step 收敛；<br>教师≥125 即可稳定；<br>隐藏=16 是 Pareto 最优（图 2C-D）。</p>
<p>2. 仿真泛化</p>
<p>分布内 &amp; 外 Sim2Sim</p>
<p>• 7 款未参与训练的参数化机型<br>• Flightmare 默认机 (T/W≈12，&gt;2× 训练上限)</p>
<p>7 款平均 episode length ≥475/500；<br>Flightmare 仍能跟踪 8 字，RMSE 0.22 m。</p>
<p>3. 实物零样本</p>
<p>跨重量、尺寸、桨叶、飞控、状态估计器</p>
<p>10 款真机：31 g–2.4 kg，65–500 mm，Betaflight/PX4/Crazyflie，EKF/Mahony/Madgwick，2/3/4 桨，软/刚性机架</p>
<p>全部稳定悬停 &amp; 跟踪 8 字；<br>RMSE 0.07–0.19 m（10 s 周期），与专用单策略相当。</p>
<p>4. 轨迹跟踪敏捷性</p>
<p>测试“适应性-敏捷性”权衡</p>
<p>10 s、5.5 s、最快 3.5 s 8 字；<br>最大速度 1–4.7 m/s</p>
<p>无需轨迹前瞻，RMSE 仅增加 ≈0.02 m；<br>最大速度受限于<strong>无前瞻</strong>而非适应力。</p>
<p>5. 室外 &amp; 载荷</p>
<p>大风+超重</p>
<p>x500 平台，7 m/s 阵风，10 m/s 地速；<br>挂 600 g/1.2 kg 水瓶（总重 2.4 kg，&gt; 官方 1.5 kg 上限）</p>
<p>10 m/s 仍稳定；<br>600 g 载荷下 8 字跟踪；<br>1.2 kg 仅能悬停。</p>
<p>6. 瞬时扰动</p>
<p>在线系统辨识速度</p>
<p>• 空中 4.5 m/s 初速度突然切换策略<br>• 底部猛击 &gt;90° 倾覆<br>• 顶部加压 200 g 工具<br>• 1–3 片 2 桨换 3 桨</p>
<p>20–40 ms 内恢复平飞；<br>击打工后 0.8 s 回到悬停；<br>加压无稳态高度误差；<br>混合桨仍完成 8 字。</p>
<p>7. 长序列外推</p>
<p>上下文窗口极限</p>
<p>训练仅用 500 步（5 s），实测连续飞行至电池空（&gt;5 min，≈30 000 步）</p>
<p>隐藏状态无漂移，轨迹 5 圈重叠误差 &lt;1 cm（图 6C）。</p>
<p>8. 隐含表征探针</p>
<p>验证“是否真在辨识”</p>
<p>用 1000 条仿真轨迹训练线性探针回归 T/W、τ/J、电机延迟</p>
<p>T/W 预测 R²=0.949，误差 0.047；<br>隐藏状态随时间收敛到真值（图 3）。</p>
<p>综上，实验用<strong>同一权重</strong>完成从 31 g 玩具机到 2.4 kg 工业机、从静悬到 4.7 m/s 特技、从实验室到 7 m/s 阵风外场的全谱验证，首次证明<strong>极简循环网络可在毫秒级实现跨平台零样本四旋翼控制</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下问题在论文末尾已部分提及，但尚未展开；它们为后续研究提供了可直接落地的切口。按“<strong>理论 → 算法 → 系统 → 应用</strong>”四个层次归纳：</p>
<p>理论层面</p>
<ol>
<li><p><strong>可适应性的理论极限</strong><br>在给定网络容量（2084 参数）与观测维度（22）下，能否用 <strong>PAC-Bayes 或信息论下界</strong> 量化“零样本适应”所需的最小上下文长度、最小教师数量？</p>
</li>
<li><p><strong>渐进最优性</strong><br>当前 KL 逼近目标仅到“教师动作分布”为止。若教师本身距最优策略仍有差距，蒸馏过程会继承次优。能否引入 <strong>滚动时域 MPC 作为“无限容量教师”</strong>，构造“教师→学生→在线细化”的渐进最优链？</p>
</li>
</ol>
<p>算法层面</p>
<ol>
<li><p><strong>异构执行器与故障在线重配置</strong><br>实验已验证 2-/3-桨混合，但尚未涉及 <strong>单电机完全失效</strong> 或 <strong>推力曲线非对称突变</strong>。能否把“故障模式”显式加入 Ξ 分布，令隐藏状态同时完成 <strong>故障检测-隔离-重配置（FDI-R）</strong>？</p>
</li>
<li><p><strong>奖励函数也未知</strong><br>当前 Ξ 仅覆盖动力学；若载荷/任务变更导致奖励权重变化（如航拍→搬运），需同时推断 <strong>动力学 + 奖励参数</strong>。可把 BAPOMDP 扩展为 <strong>Bayes-Adaptive Reward-Parameterized MDP</strong>，用同一套元模仿框架蒸馏。</p>
</li>
<li><p><strong>上下文长度外推的理论保证</strong><br>实验观察到 5 s→50 s 无漂移，但 GRU 存在梯度消失。能否用 <strong>Lipschitz RNN</strong> 或 <strong>线性注意力</strong> 替代，给出“任意长 episode 误差有界”的稳定性证明？</p>
</li>
</ol>
<p>系统层面</p>
<ol>
<li><p><strong>片上学习（On-device fine-tune）</strong><br>当前仅做前向推断。若利用 MCU 空闲算力，在 <strong>10-20 个梯度步 / 航班</strong> 内做 <strong>递归滤波 + 低秩适配（LoRA）</strong>，可进一步压缩剩余 10 % 误差；需解决 <strong>浮点运算量 &lt; 1 kFLOP/步</strong> 的极限优化。</p>
</li>
<li><p><strong>多机分布式基础策略</strong><br>把隐藏状态通过 <strong>超低带宽广播（&lt; 1 kb/s）</strong> 共享，实现“机群级系统辨识”：当一架机发现新动力学模式，其余机即时更新隐藏先验，形成 <strong>联邦式 in-context 学习</strong>。</p>
</li>
<li><p><strong>事件触发观测</strong><br>当前 100 Hz 固定频率；在大型机上可引入 <strong>事件相机 + IMU 事件触发</strong>，策略输入变为异步事件流。需重新设计 <strong>连续-time RNN（ODE-RNN）</strong>，保证异步输入下仍满足硬实时。</p>
</li>
</ol>
<p>应用层面</p>
<ol>
<li><p><strong>极端环境零样本</strong></p>
<ul>
<li><strong>水下-空中跨介质</strong>：液体密度、附加质量、推力损失 &gt; 50 %，动力学远超出当前 Ξ 包络。</li>
<li><strong>火星稀薄大气</strong>：雷诺数、桨尖马赫数变化，需把 <strong>气动系数 C_T、C_Q</strong> 纳入随机化，并验证仿真→真实稀薄大气的跨域迁移。</li>
</ul>
</li>
<li><strong>低成本教具与 benchmark</strong><br>开源权重已可烧录到 $49 Crazyflie；下一步发布 <strong>“一键换机” 自动化脚本</strong>（自动读取 URDF/SDF → 烧录 → 起飞），把 RAPTOR 变成 <strong>四旋翼控制领域的 ImageNet 预训练模型</strong>，后续研究只需在隐藏状态上接下游任务头（跟踪、避障、编队）即可。</li>
</ol>
<p>一句话总结</p>
<p>RAPTOR 把“换机重训”问题压缩成“隐藏状态 20 ms 收敛”，但理论上仍不清楚<strong>收敛极限在哪</strong>、工程上仍没把<strong>剩余 10 % 误差吃干榨尽</strong>；往上述任一点深挖，都可能成为下一代“基础策略”的里程碑。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="RAPTOR-核心内容速览"><a href="#RAPTOR-核心内容速览" class="headerlink" title="RAPTOR 核心内容速览"></a>RAPTOR 核心内容速览</h1><h2 id="1-研究目标"><a href="#1-研究目标" class="headerlink" title="1. 研究目标"></a>1. 研究目标</h2><ul>
<li>让<strong>单一极简网络</strong>无需再训练即可零样本控制<strong>任意四旋翼</strong>，解决“换机重训”痛点。</li>
<li>对标人类“10 分钟适应新车”的数据效率，实现毫秒级在线系统辨识。</li>
</ul>
<h2 id="2-方法框架（RAPTOR）"><a href="#2-方法框架（RAPTOR）" class="headerlink" title="2. 方法框架（RAPTOR）"></a>2. 方法框架（RAPTOR）</h2><p>阶段</p>
<p>关键创新</p>
<p>一句话总结</p>
<p>① 超宽域随机化</p>
<p>物理合理的动力学分布 p(Ξ) 覆盖 31 g–2.4 kg、T/W 1.5–5</p>
<p>先把“所有现实四旋翼”装进一个分布</p>
<p>② 预训练</p>
<p>1000 条 SAC 教师策略，各专精一台随机采样机体</p>
<p>用“无限容量”教师做最优动作 oracle</p>
<p>③ 元模仿蒸馏</p>
<p>2084-参数 GRU 学生网络，最小化 KL(教师∥学生)≈MSE</p>
<p>把 1000 教师压进一个循环网络，隐藏状态=隐式 Ξ 推断器</p>
<p>④ 部署</p>
<p>100 Hz 前向推理，隐藏状态实时更新</p>
<p>上电即飞，20–50 ms 完成适应</p>
<h2 id="3-网络极简"><a href="#3-网络极简" class="headerlink" title="3. 网络极简"></a>3. 网络极简</h2><ul>
<li>22 维观测 → 16 维 GRU → 4 维电机转速</li>
<li>2084 参数、&lt; 10 % MCU 算力，硬实时 100 Hz</li>
</ul>
<h2 id="4-实验规模"><a href="#4-实验规模" class="headerlink" title="4. 实验规模"></a>4. 实验规模</h2><ul>
<li><strong>10 款真机</strong>（31 g–2.4 kg，不同桨/机架/飞控/状态估计器）</li>
<li><strong>2 款仿真</strong>（含 T/W=12 超分布机型）</li>
<li><strong>3 类任务</strong>：悬停、8 字跟踪、高速室外</li>
<li><strong>6 种扰动</strong>：阵风 7 m/s、挂 1.2 kg 载荷、底部猛击&gt;90°、顶部加压、混合 2/3 桨、空中 4.5 m/s 初速切换</li>
</ul>
<p><strong>结果</strong>：全部零样本稳定飞行，轨迹跟踪 RMSE 0.07–0.19 m，与专用单策略相当；隐藏状态可线性回归 T/W（R²=0.95）。</p>
<h2 id="5-主要贡献"><a href="#5-主要贡献" class="headerlink" title="5. 主要贡献"></a>5. 主要贡献</h2><ul>
<li>提出“基础策略”概念并给出完整训练流水线（域随机化→教师预训练→元模仿蒸馏）</li>
<li>证明<strong>极小规模循环网络</strong>即可在线推断未知动力学，实现跨平台零样本控制</li>
<li>开源 1000 教师权重、训练/推理代码与网页交互 demo，建立可复现基准</li>
</ul>
<blockquote>
<p>RAPTOR 让四旋翼控制像“插 USB”一样即插即用，为低成本机群、异构机器人甚至跨介质飞行器提供了可扩展的控制基础模型。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是RAPTOR%3A%20A%20Foundation%20Policy%20for%20Quadrotor%20Control，链接是  https://arxiv.org/pdf/2509.11481  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.11481  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jonas Eschmann,Dario Albani,Giuseppe Loianno</p>
<p><strong>Categories</strong>: cs.RO,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.11481.pdf">https://arxiv.org/pdf/2509.11481.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.11481">https://arxiv.org/abs/2509.11481</a></p>
<p><strong>Arxiv ID</strong>: 2509.11481</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.11481">https://papers.cool/arxiv/2509.11481</a></p>
<p><strong>Published</strong>: 2025-09-15T00:05:40Z</p>
<p><strong>Updated</strong>: 2025-09-15T00:05:40.000Z</p>
<hr>
<h3 id="20-Sound-Matching-an-Analogue-Levelling-Amplifier-Using-the-Newton-Raphson-Method"><a href="#20-Sound-Matching-an-Analogue-Levelling-Amplifier-Using-the-Newton-Raphson-Method" class="headerlink" title="20. Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method"></a>20. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.10706">Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method</a></h3><p>Automatic differentiation through digital signal processing algorithms for virtual analogue modelling has recently gained popularity. These algorithms are typically more computationally efficient than black-box neural networks that rely on dense matrix multiplications. Due to their differentiable nature, they can be integrated with neural networks and jointly trained using gradient descent algorithms, resulting in more efficient systems. Furthermore, signal processing algorithms have significantly fewer parameters than neural networks, allowing the application of the Newton-Raphson method. This method offers faster and more robust convergence than gradient descent at the cost of quadratic storage. This paper presents a method to emulate analogue levelling amplifiers using a feed-forward digital compressor with parameters optimised via the Newton-Raphson method. We demonstrate that a digital compressor can successfully approximate the behaviour of our target unit, the Teletronix LA-2A. Different strategies for computing the Hessian matrix are benchmarked. We leverage parallel algorithms for recursive filters to achieve efficient training on modern GPUs. The resulting model is made into a VST plugin and is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/aim-qmul/4a2a">https://github.com/aim-qmul/4a2a</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>通过数字信号处理算法进行自动微分的虚拟模拟建模最近获得了越来越多的关注。这些算法通常比依赖于密集矩阵乘法的黑盒神经网络更具计算效率。由于它们具有可微分的特性，可以与神经网络集成，并使用梯度下降算法进行联合训练，从而形成更高效的系统。此外，信号处理算法的参数显著少于神经网络，使得牛顿-拉夫森法得以应用。这种方法提供比梯度下降更快且更稳健的收敛，但需要消耗二次存储。本文提出了一种使用前馈数字压缩机模拟模拟调平放大器的方法，其参数通过牛顿-拉夫森法进行了优化。我们展示了数字压缩机能够成功地近似我们目标单元Teletronix LA-2A的行为。不同的海森矩阵计算策略进行了基准测试。我们利用递归滤波器的并行算法来实现现代GPU上的高效训练。最终模型制作成VST插件，并在<a target="_blank" rel="noopener" href="https://github.com/aim-qmul/4a2a开源。">https://github.com/aim-qmul/4a2a开源。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文将“虚拟模拟（VA）建模”形式化为一个**声音匹配（sound-matching）**问题：<br>给定一段由经典模拟压限器 Teletronix LA-2A 处理后的目标音频 $y$，用一组可解释的数字压限器参数 $\theta$ 去逼近该目标，即</p>
<p>[ \theta^* = \arg\min_\theta D!\bigl(f(x,\theta),,y\bigr), ]</p>
<p>其中</p>
<ul>
<li>$x$ 为未处理的干信号，</li>
<li>$f$ 是一个仅含 5 个参数的前馈数字压限器（阈值、压缩比、起音/释音时间、补偿增益），</li>
<li>$D$ 取平方误差。</li>
</ul>
<p>核心难点在于如何<strong>快速且鲁棒地</strong>求解上述优化。论文提出用<strong>牛顿-拉夫逊（Newton-Raphson, NR）方法</strong>替代常规的梯度下降，以利用二阶信息获得更快收敛，并针对音频递归滤波结构推导了高效的 Hessian 计算与 GPU 并行实现。最终得到一个可解释、可实时运行的 VST 插件，其音质与商业插件相当或更优。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线：纯黑盒神经网络、灰盒混合模型，以及基于物理/优化的白盒方法。以下按时间顺序列出关键文献并指出其与本文的关联。</p>
<p>类别</p>
<p>代表文献</p>
<p>核心思路</p>
<p>与本文关系</p>
<p><strong>黑盒 NN</strong></p>
<p>Hawley et al. AES 2019 [2]</p>
<p>U-Net 自编码器端到端建模 LA-2A</p>
<p>首次用深度网络直接拟合压缩器，但参数量大、不可解释</p>
<p>Steinmetz &amp; Reiss AES 2022 [3]</p>
<p>时序卷积网络 TCN 降低运算量</p>
<p>强调实时性，启发本文选用轻量结构</p>
<p>Comunità et al. ICASSP 2023 [4]</p>
<p>时变特征调制条件机制</p>
<p>提高对旋钮设置的泛化，但仍是黑盒</p>
<p>Yeh et al. DAFx 2024 [5]</p>
<p>超循环神经网络条件建模</p>
<p>进一步压缩参数量，可视为黑盒路线的最新进展</p>
<p>Simionato &amp; Fasciani arXiv 2024 [6] / JAES 2025 [7]</p>
<p>状态空间模型 SSA/S4 建模光学压缩器</p>
<p>提供另一种低复杂度序列建模工具，与本文的“二阶优化”形成互补</p>
<p><strong>灰盒混合</strong></p>
<p>Wright &amp; Välimäki DAFx 2022 [8]</p>
<p>前馈压缩器+轻量 GRU 补偿增益</p>
<p>本文直接延续其“可解释参数+数据驱动补偿”框架，但将梯度下降替换为 NR</p>
<p>Yu et al. DAFx 2024 [9]</p>
<p>为压缩器弹道递归滤波定制反向传播核</p>
<p>本文利用其可微滤波器实现（torchcomp），并进一步给出前向模式（JVP）推导以支持 Hessian 计算</p>
<p><strong>白盒/物理</strong></p>
<p>Bernardini et al. TASLP 2021 [10]</p>
<p>波数字牛顿-拉夫逊求解多端口非线性</p>
<p>首次在 VA 领域用 NR 解电路方程，但属于“白盒”电路仿真，与本文“灰盒声音匹配”目标不同</p>
<p>Bilbao et al. Acta Acustica 2015 [11]</p>
<p>NR 用于乐器碰撞系统物理建模</p>
<p>提供二阶优化在音频物理模型中的早期范例</p>
<p>综上，本文首次将<strong>牛顿-拉夫逊二阶优化</strong>引入<strong>灰盒声音匹配</strong>场景，填补了“黑盒 NN 参数过多、白盒物理建模难以适配”之间的空白，并通过高效 Hessian 计算与 GPU 并行实现，将优化时间从小时级降至分钟级。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“LA-2A 模拟压限器建模”转化为一个<strong>5 参数最小二乘拟合问题</strong>，并用<strong>牛顿-拉夫逊（NR）二阶优化</strong>在 GPU 上快速求解。关键步骤如下：</p>
<ol>
<li><p>建立可微灰盒模型<br>采用前馈数字压限器 <code>torchcomp</code>（仅 5 个参数：阈值、压缩比、起音/释音时间、补偿增益），移除 RMS 检测器以避免局部极小值；整个链路在 PyTorch 中完全可微。</p>
</li>
<li><p>构造二阶可优化目标<br>损失函数取平方误差<br>[ L(\theta)=|\hat y(\theta)-y|_2^2, ]<br>满足 NR 所需的二阶可微与局部凸性（经验验证）。</p>
</li>
<li><p>高效 Hessian 计算</p>
<ul>
<li>利用 <code>torchcomp</code> 已有的 VJP 内核，推出**前向模式（JVP）**递归公式，使 Hessian 可用“前向-反向”或“反向-反向”模式并行求得。</li>
<li>对弹道中的一阶递归滤波器采用<strong>并行 associative scan</strong> CUDA 核，进一步将每步 NR 耗时降到 3.5 s（整数据集 &lt;20 min）。</li>
</ul>
</li>
<li><p>阻尼 NR + 回溯线搜索<br>求解 [ \nabla^2 L(\theta),\nu=\nabla L(\theta) ] 避免显式求逆；若 Hessian 非正定，则随机抽取正交方向继续搜索。按“100→95→90…”顺序初始化，确保起始点靠近最优解。</p>
</li>
<li><p>插件化与补偿</p>
<ul>
<li>将学得的“Peak Reduction → 5 参数”映射表线性插值后嵌入 VST，用户仅拧一个旋钮即可连续控制。</li>
<li>额外级联 8 单元 GRU 补偿残余非线性，得到 <code>4A-2A-G</code> 版本，ESR 低于商业插件。</li>
</ul>
</li>
</ol>
<p>通过“<strong>5 参数可解释模型 + 二阶优化 + GPU 并行 Hessian</strong>”这一组合，论文在保持实时性的同时，把 LA-2A 的声音特征压缩进一组人类可读的旋钮，实现了快速、鲁棒、可推广的灰盒虚拟模拟。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕三条主线展开：</p>
<ol>
<li>验证 Newton-Raphson 在 5 参数灰盒压缩器上的可行性与收敛性；</li>
<li>对比不同 Hessian 计算策略的时空开销，并选取最优实现；</li>
<li>与商业插件及已有灰盒模型进行音质与动态指标评测，最后给出插件化与插值误差分析。具体实验如下：</li>
</ol>
<p>编号</p>
<p>实验目的</p>
<p>数据集/设置</p>
<p>关键指标</p>
<p>主要结论</p>
<p>E1</p>
<p>NR 收敛性与参数轨迹</p>
<p>SignalTrain LA-2A（44.1 kHz，21 个 peak-reduction 点，20 min 音频）</p>
<p>迭代次数、损失曲线、Hessian 正定性</p>
<p>≤10 次迭代即收敛；peak-reduction ≥40 时损失曲面近似凸；&lt;40 频繁出现负曲率，无法一致求解</p>
<p>E2</p>
<p>Hessian 计算策略 benchmark</p>
<p>同一 12 s 片段，batch=16，RTX 3060 12 GB</p>
<p>显存占用(MB)、单次耗时(ms)</p>
<p>autograd-rev-rev 最优：1066 MB / 26.5 ms；func-fwd-rev 稍慢但可接受；func-rev-fwd 最昂贵（6306 MB / 64.7 ms）</p>
<p>E3</p>
<p>整体训练耗时</p>
<p>全部 21×2 模式（compressor+limiter）</p>
<p>总 Wall-clock 时间</p>
<p>单块 RTX 3060 &lt;20 min，较先前梯度下降方案（数小时）×10 提速</p>
<p>E4</p>
<p>音质评估（训练集内）</p>
<p>同上，预加重后</p>
<p>ESR、∆LDR</p>
<p>4A-2A 与 UAD 官方插件 ESR 相当；4A-2A-G（+GRU）ESR 最低，∆LDR≈0，优于 CLA-2A、CA-2A</p>
<p>E5</p>
<p>插件参数映射插值误差</p>
<p>留 7 点（45,55,…,95）做插值</p>
<p>插值后 ESR</p>
<p>线性 vs. 三次样条平均 ESR 差异 &lt;0.1%，峰值处线性略优；插件采用线性插值</p>
<p>E6</p>
<p>残差频谱分析</p>
<p>鼓循环片段，peak-reduction=75</p>
<p>时频图差值</p>
<p>GRU 补偿后高频（&gt;4 kHz）残差显著降低，验证其捕捉非线性残余的能力</p>
<p>所有实验均开源（代码 + VST 插件），以便复现与扩展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可继续探索的方向按“模型-优化-系统-评估”四层次归纳如下：</p>
<p>1 模型结构</p>
<ul>
<li><p><strong>反馈拓扑</strong><br>LA-2A 实为光敏反馈压缩器，可改用 <code>s[n]=ŷ[n−1]</code> 并引入光电元件非线性微分方程，建立可微反馈模型；需设计“隐式层”或固定点迭代式前向传播，并推导对应的反向/前向梯度。</p>
</li>
<li><p><strong>两阶段释音</strong><br>文献指出 LA-2A 存在 60 ms + 5 s 双时间常数释音。可在弹道滤波器后并联快/慢两支路，通过数据驱动权重混合，形成 6–7 参数扩展模型。</p>
</li>
<li><p><strong>光电元件物理先验</strong><br>将 T4 电致发光-光敏电阻的指数/对数 I-V 曲线直接嵌入增益检测模块，实现“灰盒+物理”混合建模，减少纯数据依赖。</p>
</li>
</ul>
<p>2 优化算法</p>
<ul>
<li><p><strong>Hessian-free 策略</strong><br>当参数 &gt;50–100 时，显式 Hessian 存储不可行。可改用共轭梯度或 Krylov 子空间法，仅通过 Hessian-向量积 <code>v↦∇²L v</code> 求解牛顿方程；结合 PyTorch <code>functorch.cvp</code> 或 JAX 线性算子抽象实现。</p>
</li>
<li><p><strong>随机/拟牛顿变种</strong><br>探索 L-BFGS、SR1 或在线自然梯度（NG）更新，兼顾二阶信息与大 batch 训练；研究其对音频递归网络的稳定性。</p>
</li>
<li><p><strong>全局优化</strong><br>对低参数空间（M&lt;15）可试验贝叶斯优化、模拟退火或遗传算法，验证是否突破 NR 遇到的负曲率/局部极小瓶颈。</p>
</li>
</ul>
<p>3 系统与实现</p>
<ul>
<li><p><strong>实时隐式层求解</strong><br>反馈模型需在采样级循环内收敛。可研究 Anderson 加速、学习初始猜测（meta-solver）或神经网络预测固定点，实现 &lt;1 ms 块延迟。</p>
</li>
<li><p><strong>降低 Hessian 计算复杂度</strong><br>利用滤波器结构稀疏性、Toeplitz 近似或 Kronecker 分解，把 O(M²) 存储降至 O(M)；结合混合精度（FP16 Hessian-向量积）进一步提速。</p>
</li>
<li><p><strong>多硬件后端</strong><br>将并行 associative scan 核移植到 Apple Metal、Intel Xe 及移动 GPU；探索 WebAssembly+SIMD 浏览器版本，实现插件跨平台。</p>
</li>
</ul>
<p>4 评估与扩展</p>
<ul>
<li><p><strong>跨硬件泛化</strong><br>收集多台不同年份 LA-2A 单元，量化单元间差异，建立“硬件-感知”多任务学习目标，测试模型鲁棒性。</p>
</li>
<li><p><strong>其他经典压缩器</strong><br>应用同一 NR 框架到 1176（FET）、LA-3A（固态）、Fairchild 670（可变 μ）等，验证通用性并构建统一插件套件。</p>
</li>
<li><p><strong>感知-驱动损失</strong><br>采用谱包络、耳蜗图或深度听觉嵌入替代平方误差，分析是否进一步降低感知差异而不仅 ESR。</p>
</li>
<li><p><strong>端到端参数推断</strong><br>训练轻量 CNN 将音频片段直接映射至 5 参数，实现“盲”压缩器克隆，再与 NR 微调级联，实现“先粗后精”两阶段策略。</p>
</li>
</ul>
<p>5 结合生成式/控制</p>
<ul>
<li><p><strong>条件生成与风格迁移</strong><br>将 NR 优化得到的参数作为条件向量，驱动生成模型（扩散、GAN）产生符合特定压缩风味的音频，用于音乐制作辅助。</p>
</li>
<li><p><strong>可微分混音引擎</strong><br>把 4A-2A 插件嵌入完整可微分混音图（均衡、混响、饱和），利用二阶优化联合调整多条音轨动态，实现“端到端”智能混音。</p>
</li>
</ul>
<p>这些方向既涵盖理论（反馈可微、物理先验）也涵盖工程（实时求解、跨平台、感知指标），为后续研究提供了阶梯式升级路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li>把 LA-2A 模拟压限器建模写成<strong>5 参数最小二乘声音匹配</strong>问题。</li>
<li>用<strong>牛顿-拉夫逊二阶优化</strong>替代梯度下降，推导并并行化 Hessian 计算（反向/前向模式 + associative scan），单步更新 3.5 s，整数据集训练 &lt;20 min。</li>
<li>在 SignalTrain 上取得与 UAD 官方插件相当的 ESR，配合轻量 GRU 补偿后 ESR 最低、∆LDR≈0，超越商业插件。</li>
<li>学得“Peak Reduction → 5 参数”可解释映射，制成实时 VST 插件并开源，提供线性插值连续控制与手动微调功能。</li>
</ul>
<p>结论：首次验证 NR 在灰盒音频压缩器建模中的实用性与实时可行性，为虚拟模拟效果器提供快速、鲁棒、可解释的新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Sound%20Matching%20an%20Analogue%20Levelling%20Amplifier%20Using%20the%20Newton-Raphson%20Method，链接是  https://arxiv.org/pdf/2509.10706  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.10706  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chin-Yun Yu,György Fazekas</p>
<p><strong>Categories</strong>: eess.AS,cs.SD,cs.SY,eess.SY</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.10706.pdf">https://arxiv.org/pdf/2509.10706.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.10706">https://arxiv.org/abs/2509.10706</a></p>
<p><strong>Arxiv ID</strong>: 2509.10706</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.10706">https://papers.cool/arxiv/2509.10706</a></p>
<p><strong>Published</strong>: 2025-09-12T21:48:30Z</p>
<p><strong>Updated</strong>: 2025-09-12T21:48:30.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202509/2025-09-18/">https://wdndev.github.io/daily/hf/202509/2025-09-18/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-WebWeaver-Structuring-Web-Scale-Evidence-with-Dynamic-Outlines-for-Open-Ended-Deep-Research"><span class="toc-text">1. WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for Open-Ended Deep Research</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#WebWeaver-%E8%AE%BA%E6%96%87%E8%A6%81%E7%82%B9%E9%80%9F%E8%A7%88"><span class="toc-text">WebWeaver 论文要点速览</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E4%BB%BB%E5%8A%A1%E4%B8%8E%E7%97%9B%E7%82%B9"><span class="toc-text">1. 任务与痛点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-WebWeaver-%E6%A1%86%E6%9E%B6"><span class="toc-text">2. WebWeaver 框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C"><span class="toc-text">3. 主要结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%B4%A1%E7%8C%AE%E6%80%BB%E7%BB%93"><span class="toc-text">4. 贡献总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Scaling-Agents-via-Continual-Pre-training"><span class="toc-text">2. Scaling Agents via Continual Pre-training</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%BB%E6%97%A8"><span class="toc-text">论文主旨</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%96%B9%E6%B3%95%E6%A1%86%E6%9E%B6"><span class="toc-text">1. 方法框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E5%AE%9E%E9%AA%8C%E4%B8%8E%E7%BB%93%E6%9E%9C"><span class="toc-text">2. 实验与结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E8%B4%A1%E7%8C%AE%E6%B8%85%E5%8D%95"><span class="toc-text">3. 贡献清单</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-WebSailor-V2-Bridging-the-Chasm-to-Proprietary-Agents-via-Synthetic-Data-and-Scalable-Reinforcement-Learning"><span class="toc-text">3. WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic Data and Scalable Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Towards-General-Agentic-Intelligence-via-Environment-Scaling"><span class="toc-text">4. Towards General Agentic Intelligence via Environment Scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-WebResearcher-Unleashing-unbounded-reasoning-capability-in-Long-Horizon-Agents"><span class="toc-text">5. WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon Agents</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#WebResearcher-%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E9%80%9F%E8%A7%88"><span class="toc-text">WebResearcher 核心内容速览</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%97%AE%E9%A2%98"><span class="toc-text">1. 问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%80%9D%E8%B7%AF"><span class="toc-text">2. 思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E6%96%B9%E6%B3%95"><span class="toc-text">3. 方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%AE%AD%E7%BB%83"><span class="toc-text">4. 训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E5%AE%9E%E9%AA%8C"><span class="toc-text">5. 实验</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E8%B4%A1%E7%8C%AE"><span class="toc-text">6. 贡献</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-ReSum-Unlocking-Long-Horizon-Search-Intelligence-via-Context-Summarization"><span class="toc-text">6. ReSum: Unlocking Long-Horizon Search Intelligence via Context Summarization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Single-stream-Policy-Optimization"><span class="toc-text">7. Single-stream Policy Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Hunyuan3D-Studio-End-to-End-AI-Pipeline-for-Game-Ready-3D-Asset-Generation"><span class="toc-text">8. Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-3D-Aware-Region-Prompted-Vision-Language-Model"><span class="toc-text">9. 3D Aware Region Prompted Vision Language Model</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#SR-3D-%E8%AE%BA%E6%96%87%E6%A6%82%E8%A6%81"><span class="toc-text">SR-3D 论文概要</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA"><span class="toc-text">1. 研究动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%A0%B8%E5%BF%83%E6%80%9D%E8%B7%AF"><span class="toc-text">2. 核心思路</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%85%B3%E9%94%AE%E6%A8%A1%E5%9D%97"><span class="toc-text">3. 关键模块</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%83%BD%E5%8A%9B%E5%B1%95%E7%A4%BA"><span class="toc-text">4. 能力展示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E7%BB%93%E8%AE%BA"><span class="toc-text">5. 结论</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-EconProver-Towards-More-Economical-Test-Time-Scaling-for-Automated-Theorem-Proving"><span class="toc-text">10. EconProver: Towards More Economical Test-Time Scaling for Automated Theorem Proving</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%BB%E6%97%A8%E4%B8%80%E5%8F%A5%E8%AF%9D"><span class="toc-text">论文主旨一句话</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E8%83%8C%E6%99%AF%E4%B8%8E%E7%97%9B%E7%82%B9"><span class="toc-text">1. 背景与痛点</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E7%BB%9F%E4%B8%80%E5%BA%A6%E9%87%8F"><span class="toc-text">2. 统一度量</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-EconRL-%E6%A1%86%E6%9E%B6%EF%BC%88%E4%B8%A4%E9%98%B6%E6%AE%B5-RL%EF%BC%89"><span class="toc-text">3. EconRL 框架（两阶段 RL）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E4%B8%BB%E8%A6%81%E7%BB%93%E6%9E%9C"><span class="toc-text">4. 主要结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E8%B4%A1%E7%8C%AE%E6%B8%85%E5%8D%95"><span class="toc-text">5. 贡献清单</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-%E4%B8%80%E5%8F%A5%E8%AF%9D%E6%80%BB%E7%BB%93"><span class="toc-text">6. 一句话总结</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Exact-Coset-Sampling-for-Quantum-Lattice-Algorithms"><span class="toc-text">11. Exact Coset Sampling for Quantum Lattice Algorithms</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Multimodal-Reasoning-for-Science-Technical-Report-and-1st-Place-Solution-to-the-ICML-2025-SeePhys-Challenge"><span class="toc-text">12. Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Phi-Preference-Hijacking-in-Multi-modal-Large-Language-Models-at-Inference-Time"><span class="toc-text">13. Phi: Preference Hijacking in Multi-modal Large Language Models at Inference Time</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%A0%B8%E5%BF%83%E6%A6%82%E8%A6%81"><span class="toc-text">论文核心概要</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E6%96%B0%E5%A8%81%E8%83%81%EF%BC%9APreference-Hijacking%EF%BC%88Phi%EF%BC%89"><span class="toc-text">1. 新威胁：Preference Hijacking（Phi）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%96%B9%E6%B3%95%E6%A1%86%E6%9E%B6"><span class="toc-text">2. 方法框架</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text">3. 实验结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%B4%A1%E7%8C%AE"><span class="toc-text">4. 贡献</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#14-Multiple-Instance-Learning-Framework-with-Masked-Hard-Instance-Mining-for-Gigapixel-Histopathology-Image-Analysis"><span class="toc-text">14. Multiple Instance Learning Framework with Masked Hard Instance Mining for Gigapixel Histopathology Image Analysis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Optimal-Brain-Restoration-for-Joint-Quantization-and-Sparsification-of-LLMs"><span class="toc-text">15. Optimal Brain Restoration for Joint Quantization and Sparsification of LLMs</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%A0%B8%E5%BF%83%E9%80%9F%E8%A7%88"><span class="toc-text">论文核心速览</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E9%97%AE%E9%A2%98"><span class="toc-text">1. 问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%96%B9%E6%B3%95%EF%BC%9AOptimal-Brain-Restoration-OBR"><span class="toc-text">2. 方法：Optimal Brain Restoration (OBR)</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BB%93%E6%9E%9C"><span class="toc-text">3. 结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E8%B4%A1%E7%8C%AE"><span class="toc-text">4. 贡献</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#16-Stable-Part-Diffusion-4D-Multi-View-RGB-and-Kinematic-Parts-Video-Generation"><span class="toc-text">16. Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-ROOM-A-Physics-Based-Continuum-Robot-Simulator-for-Photorealistic-Medical-Datasets-Generation"><span class="toc-text">17. ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-zELO-ELO-inspired-Training-Method-for-Rerankers-and-Embedding-Models"><span class="toc-text">18. zELO: ELO-inspired Training Method for Rerankers and Embedding Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-RAPTOR-A-Foundation-Policy-for-Quadrotor-Control"><span class="toc-text">19. RAPTOR: A Foundation Policy for Quadrotor Control</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#RAPTOR-%E6%A0%B8%E5%BF%83%E5%86%85%E5%AE%B9%E9%80%9F%E8%A7%88"><span class="toc-text">RAPTOR 核心内容速览</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-%E7%A0%94%E7%A9%B6%E7%9B%AE%E6%A0%87"><span class="toc-text">1. 研究目标</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-%E6%96%B9%E6%B3%95%E6%A1%86%E6%9E%B6%EF%BC%88RAPTOR%EF%BC%89"><span class="toc-text">2. 方法框架（RAPTOR）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-%E7%BD%91%E7%BB%9C%E6%9E%81%E7%AE%80"><span class="toc-text">3. 网络极简</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-%E5%AE%9E%E9%AA%8C%E8%A7%84%E6%A8%A1"><span class="toc-text">4. 实验规模</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-%E4%B8%BB%E8%A6%81%E8%B4%A1%E7%8C%AE"><span class="toc-text">5. 主要贡献</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#20-Sound-Matching-an-Analogue-Levelling-Amplifier-Using-the-Newton-Raphson-Method"><span class="toc-text">20. Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson Method</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>