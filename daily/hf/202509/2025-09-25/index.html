<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-09-25 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCRArabic document OCR remains a challenging task due to the language’s cursive script, divers">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-09-25">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202509/2025-09-25/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCRArabic document OCR remains a challenging task due to the language’s cursive script, divers">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:09.910Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202509/2025-09-25/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-09-25',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-09-25</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:09.910Z" title="Updated 2025-11-02 07:46:09">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">40.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>144min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-09-25"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR"><a href="#1-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR" class="headerlink" title="1. Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.18174">Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR</a></h3><p>Arabic document OCR remains a challenging task due to the language’s cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>阿拉伯文档OCR仍然是一项具有挑战性的任务，这主要由于该语言的连笔书写、多样化的字体、辅音符号和从右到左的书写方向。虽然现代多模态大型语言模型（MLLMs）在高资源语言的文档理解方面取得了进展，但它们在阿拉伯文上的表现仍然有限。在这项工作中，我们介绍了Baseer，这是一个专门针对阿拉伯文档OCR进行微调的视觉语言模型。Baseer利用一个结合合成和现实世界文档的大规模数据集进行训练，采用仅解码器的微调策略以适应预训练的MLLM，同时保持一般视觉特征。我们还提出了Misraj-DocOCR，这是一个高质量、经过专家验证的基准，旨在对阿拉伯OCR系统进行严格评估。我们的实验显示，Baseer显著优于现有的开源和商业解决方案，达到了0.25的错误率（WER），并在阿拉伯文档OCR领域建立了新的最先进水平。我们的结果突显了通用MLLM在特定领域适应的好处，并为在形态丰富的语言（如阿拉伯语）上实现高准确率的OCR建立了强有力的基线。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对阿拉伯语文档光学字符识别（OCR）精度不足的问题，提出并验证了一种基于视觉-语言模型的专用解决方案。核心痛点与目标可归纳为：</p>
<ul>
<li><strong>脚本复杂性</strong>：阿拉伯文连笔、上下文相关字形、丰富变音符号及右向书写导致现有高资源语言 OCR 系统性能急剧下降。</li>
<li><strong>数据与评测缺失</strong>：既缺少大规模、多样化且结构标注精确的阿拉伯文档训练数据，也缺乏经专家验证、能可靠衡量模型鲁棒的基准。</li>
<li><strong>通用模型局限</strong>：当前多模态大模型虽具备多语言泛化能力，但未针对阿拉伯文特有版式与字符特征进行优化，难以满足实际文档数字化需求。</li>
</ul>
<p>为此，作者构建了一个包含 50 万图文对的混合数据集，并推出专家校正的评测基准 Misraj-DocOCR；在此基础上，通过解码器专属微调策略将 Qwen2.5-VL-3B-Instruct 适配为专用模型 Baseer，实现目前最低的 WER（0.25），显著超越现有开源与商业系统，从而推动阿拉伯文及类似形态丰富语言的 OCR 性能进入新水平。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将自身定位在两大研究脉络的交汇点，相应梳理了以下相关文献：</p>
<ol>
<li><p>多模态大语言模型（MLLM）</p>
<ul>
<li><strong>模块化融合范式</strong>：用冻结视觉编码器+轻量适配器连接大语言模型<ul>
<li>LLaVA 系列</li>
<li>Aya-Vision</li>
<li>IDEFICS / IDEFICS3</li>
<li>SmolVLM</li>
</ul>
</li>
<li><strong>端到端统一训练范式</strong>：视觉与语言组件联合训练的大规模模型<ul>
<li>InternVL3</li>
<li>Gemma-3 / PaliGemma-2</li>
<li>Qwen-VL / Qwen2.5-VL 系列</li>
</ul>
</li>
</ul>
</li>
<li><p>OCR 与文档理解</p>
<ul>
<li><strong>深度学习时代</strong>：CNN-RNN 混合网络、注意力编解码器显著提升字符识别准确率</li>
<li><strong>整体文档理解</strong>：同时输出文本与结构（表格、标题、段落、阅读顺序）<ul>
<li>MonkeyOCR</li>
<li>SmolDocling</li>
<li>Nanonets-OCR-s</li>
</ul>
</li>
<li><strong>阿拉伯文专门系统</strong><ul>
<li>Qari-OCR（近期唯一基于 MLLM 的阿拉伯 OCR 尝试，但规模与版式覆盖有限）</li>
</ul>
</li>
</ul>
</li>
<li><p>数据集与评测</p>
<ul>
<li>KITAB-bench（阿拉伯文档理解评测集，原版本存在标注噪声，本文对其进行了全面校正并发布修订版）</li>
</ul>
</li>
</ol>
<p>综上，现有 MLLM 研究多聚焦高资源语言或通用场景，而阿拉伯文档 OCR 因脚本特殊性仍缺乏专门优化；本文通过领域专属微调填补了这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“数据+模型+评测”三位一体的策略，系统性解决阿拉伯文档 OCR 精度不足的难题：</p>
<ol>
<li><p>构建大规模、多样化训练数据</p>
<ul>
<li>50 万图文对混合语料<ul>
<li>30 万合成数据：基于 Common Crawl 清洗后的 Markdown，经“MD→HTML→Word→PDF→高清图像”四步渲染，并在字体、页尺寸、栏数、背景色等 10 余项配置上随机采样；再施加 29 种失真（水印、污渍、折痕、噪声、模糊等）增强鲁棒性。</li>
<li>20 万真实数据：开源书籍、杂志、学术论文，利用版面分析算法筛选复杂版式（多栏、表格、脚注、彩印等），再用 SOTA 视觉-语言模型生成初版标注，最后由人工专家逐页校对。</li>
</ul>
</li>
<li>标注格式统一为 Markdown（表格用 HTML），并引入自定义标签显式标记页码、水印、插图，兼顾文本与结构监督信号。</li>
</ul>
</li>
<li><p>设计专用视觉-语言模型 Baseer</p>
<ul>
<li>基座选择：经定性对比，选用在阿拉伯语上表现最佳的 Qwen2.5-VL-3B-Instruct。</li>
<li>微调策略：仅更新语言解码器，视觉编码器冻结，既保留通用视觉特征，又针对阿拉伯语右向书写、变音符号、连笔等特性进行深度适配。</li>
<li>最优超参：序列长度 4096， cosine 学习率衰减，3 epoch，批大小 640，8×H100 训练。</li>
</ul>
</li>
<li><p>建立高质量评测基准</p>
<ul>
<li>发布 Misraj-DocOCR：400 张涵盖合成+真实、多种字体与版式的专家双盲校对图像，提供文本+结构双重真值。</li>
<li>修订 KITAB-bench：修复原文本幻觉、缺页码、小字漏识等错误，公开校正版供社区复现。</li>
</ul>
</li>
<li><p>实验验证</p>
<ul>
<li>在 Misraj-DocOCR 上，Baseer 将 WER 降至 0.25，TEDS 结构相似度 66，MARS 版面吻合度 76.9，全面领先 Gemini-2.5-Pro、Azure AI、GPT-5 等商用与开源系统。</li>
<li>在修正后的 KITAB-bench 上，Baseer 取得最佳结构指标 TEDS（56）与综合 MARS（68.1），验证其在不同数据集上的鲁棒性。</li>
</ul>
</li>
</ol>
<p>通过“高质量域内数据 + 解码器专属微调 + 严格评测反馈”的闭环，论文显著缩小了通用多模态模型在阿拉伯文文档 OCR 上的性能差距，为形态丰富语言的 OCR 研究树立了新基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“如何获得最佳阿拉伯文档 OCR 性能”逐层剥离变量，共执行 4 组核心实验，全部在自建的 Misraj-DocOCR 验证集（400 张）上计算 ChrF 作为主要监控指标，并辅以 WER/CER/TEDS/MARS 等多指标交叉验证。</p>
<ol>
<li><p>基座模型筛选（Section 6.1）</p>
<ul>
<li>候选：Qwen2.5-VL-3/7/32B、Gemma-3-4/12B、Aya-Vision、AIN 等 8 个开源 MLLM。</li>
<li>方式：人工设计 20 张涵盖密集变音、多栏、表格、手写字、低照度的“困难样例”，直接 zero-shot 推理。</li>
<li>评估维度：<br>– 文字准确率（字符/词）<br>– 右向左顺序保持<br>– 整体版面连贯性</li>
<li>结论：Qwen2.5-VL-3B-Instruct 在阿拉伯场景显著优于同系列更大参数及竞品，被选为后续统一基座。</li>
</ul>
</li>
<li><p>微调策略对比（Section 6.2）</p>
<ul>
<li>固定 50 k 训练样本、2 epoch，仅改变可训练参数范围：<br>a) Baseer-Full：全部参数更新（含 ViT）<br>b) Baseer-Decoder：仅语言解码器（ViT 冻结）<br>c) Baseer-LoRA：在注意力层插入低秩适配器（秩 = 16）</li>
<li>结果（ChrF）：<ul>
<li>Full 84.79</li>
<li>Decoder <strong>89.79</strong></li>
<li>LoRA 85.52</li>
</ul>
</li>
<li>结论：保留预训练视觉特征同时让语言端充分适配阿拉伯数据，效果最佳，后续实验统一采用 Decoder-only 方案。</li>
</ul>
</li>
<li><p>序列长度影响（Section 6.3）</p>
<ul>
<li>在 Decoder-only 基础上，分别尝试 2048 / 4096 / 8192 token 最大长度，其余超参不变。</li>
<li>ChrF 得分：<ul>
<li>2048 → 82.69</li>
<li>4096 → <strong>89.79</strong></li>
<li>8192 → 87.52</li>
</ul>
</li>
<li>结论：4096 提供足够上下文以覆盖整页内容；过长窗口因填充比例升高反而稀释视觉信号。</li>
</ul>
</li>
<li><p>终极模型与系统级横向评测（Section 7）</p>
<ul>
<li>训练：全量 500 k 数据 + Decoder-only + 4096 长度，3 epoch，得到最终 Baseer。</li>
<li>测评集：<br>– Misraj-DocOCR（400 张，专家校对）<br>– 修订后 KITAB-bench PDF-to-Markdown（30 张）</li>
<li>对比系统：Gemini-2.5-Pro、Azure AI、Dots.ocr、Nanonets、Qari、GPT-5、GPT-4o-mini、Qwen2.5-VL-32/7/3B、Gemma-3-12/4B、Aya-Vision、AIN 等 15 个开源或商用 API。</li>
<li>指标：WER ↓、CER ↓、BLEU ↑、ChrF ↑、TEDS ↑、MARS ↑。</li>
<li>主要结果：<ul>
<li>Misraj-DocOCR：Baseer 取得 WER 0.25、TEDS 66、MARS 76.9，三项第一；ChrF 87.77 仅次于 Gemini-2.5-Pro 的 89.55，但结构指标显著领先。</li>
<li>KITAB-bench：Baseer 在结构敏感指标 TEDS(56)、MARS(68.13) 均列第一，验证其跨数据集鲁棒性。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>通过上述递进式实验，论文不仅确定了最优训练配置，也证明了 Baseer 相对现有开源与商业模型的全面优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，括号内给出可参考的技术路线或指标。</p>
<ol>
<li><p><strong>跨字体/跨年代泛化</strong></p>
<ul>
<li>收集 19–20 世纪铅印、打字机、手写/复写纸扫描，构建时间轴基准 <code>Arabic-OCR-TimeMachine</code></li>
<li>指标：按年代分段的 WER 与 F1-drop Δ，观察是否需年代感知采样或课程式微调</li>
</ul>
</li>
<li><p><strong>极端版式与物理退化</strong></p>
<ul>
<li>引入弯曲、透视、渗墨、缺页、双面透印等真实退化模型；使用可控退化渲染器 <code>Degrad-OCR-Arabic</code></li>
<li>探索物理先验：可微分图像矫正网络 + Baseer 端到端联合训练，评估矫正前后 WER 差距</li>
</ul>
</li>
<li><p><strong>端到端手写-印刷混合识别</strong></p>
<ul>
<li>在现有印刷数据上混入 INKAT、KHATT 等手写数据集，构建 <code>Mixed-Print-Hand</code> 子任务</li>
<li>对比：①共享解码器 ②手写专用分支 ③动态路由专家网络 (MoE) 的字符错误率差异</li>
</ul>
</li>
<li><p><strong>低资源方言与代码切换</strong></p>
<ul>
<li>收集海湾、马格里布、苏丹等方言扫描件；用 1–5 % 目标方言 + 95 % MSA 做 few-shot 微调</li>
<li>探索方言嵌入空间：方言向量 $\mathbf{d}$ 与 MSA 向量 $\mathbf{m}$ 的距离 $|\mathbf{d}-\mathbf{m}|$ 与 CER 相关性</li>
</ul>
</li>
<li><p><strong>端到端结构化输出（beyond Markdown）</strong></p>
<ul>
<li>直接输出 <code>LaTeX</code>、<code>XML/TEI</code>、<code>JSON</code> 或 <code>PDF-JSON</code>（含阅读顺序、章节层级）</li>
<li>引入结构损失：Tree-Edit + 层级 F1，验证是否比 Markdown 中间格式降低信息损失</li>
</ul>
</li>
<li><p><strong>多任务联合训练</strong></p>
<ul>
<li>同步优化 OCR + 文档版面分析 + 关键信息提取（KIE）</li>
<li>使用多头解码器：共享 ViT，文本头输出字符序列，版面头输出 <code>BBOX-class</code>，KIE 头输出 <code>（实体，值）</code> 元组；对比单任务与多任务在各自验证集上的 F1</li>
</ul>
</li>
<li><p><strong>推理效率与端侧部署</strong></p>
<ul>
<li>对 Baseer-Decoder 做 4-bit/8-bit 量化、稀疏注意力（SparQ-Attention）和推测解码（speculative decoding）</li>
<li>指标：在 Snapdragon 8 Gen 3 上测首 token 延迟、峰值内存、WER 退化 &lt; 0.5 %</li>
</ul>
</li>
<li><p><strong>多页/长文档上下文</strong></p>
<ul>
<li>将 4096 → 32 k token，输入连续 8–16 页，预测跨页脚注、表格续行、章节标题编号</li>
<li>引入滑动窗口 + 记忆压缩模块（Memory-Transformer），评估跨页引用恢复率</li>
</ul>
</li>
<li><p><strong>自监督预训练与数据飞轮</strong></p>
<ul>
<li>先用 1 M 未标注扫描页做对比自监督（Pix2Struct、DiT-style mask-image modeling），再微调 OCR</li>
<li>线上飞轮：部署 Baseer 服务 → 用户校正回流 → 置信度 + 版面复杂度采样 → 每周增量微调，追踪 WER 周环比</li>
</ul>
</li>
<li><p><strong>多模态富文档（图+表+公式）</strong></p>
<ul>
<li>对化学结构、数学公式、工程蓝图等富元素，引入化学 SMILES、公式 LaTeX 作为附加监督</li>
<li>评估子任务：公式 ChrF、表格 TEDS、图注 BLEU，观察整体 MARS 是否进一步提升</li>
</ul>
</li>
<li><p><strong>可解释性与错误诊断</strong></p>
<ul>
<li>对注意力 rollout 进行阿拉伯字形拆解，可视化“连笔-字符”映射层是否对齐</li>
<li>建立错误归因矩阵：区分字体、字号、变音、背景、几何畸变五大因子，指导后续数据增强策略</li>
</ul>
</li>
<li><p><strong>跨语言迁移到相似脚本</strong></p>
<ul>
<li>将 Baseer 权重迁移至波斯语、乌尔都语、维吾尔语等阿拉伯衍生脚本，仅替换语言解码器 embedding 层</li>
<li>记录 0-shot 与 1 k-shot 情况下的 CER，验证是否实现“阿拉伯-家族”统一 OCR 模型</li>
</ul>
</li>
</ol>
<p>以上 12 点从数据、模型、任务、部署到解释性形成闭环，可供后续研究逐步扩展。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：阿拉伯文档 OCR 因连笔、变音、右向书写等特性，现有通用模型精度不足，且缺乏高质量基准。</li>
<li><strong>方案</strong>：提出专用视觉-语言模型 <strong>Baseer</strong>，以 Qwen2.5-VL-3B-Instruct 为基座，采用<strong>仅解码器微调</strong>策略。</li>
<li><strong>数据</strong>：自建 50 万图文对混合语料（30 万合成+20 万真实），标注为 Markdown/HTML，并施加 29 种失真增强。</li>
<li><strong>评测</strong>：发布专家校对的 <strong>Misraj-DocOCR</strong> 400 张基准，同时修正原 KITAB-bench 错误并开源。</li>
<li><strong>实验</strong>：<ol>
<li>基座定性比较 → 选定 Qwen2.5-VL-3B</li>
<li>微调策略对比 → Decoder-only ChrF 最高</li>
<li>序列长度消融 → 4096 token 最优</li>
<li>横向测评 → Baseer 在 Misraj-DocOCR 取得 WER 0.25、TEDS 66、MARS 76.9，全面领先开源与商用系统。</li>
</ol>
</li>
</ul>
<p>综上，论文通过“大规模域内数据 + 轻量微调 + 严格基准”实现阿拉伯文档 OCR 新 SOTA，并发布数据集与模型供社区复用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Baseer%3A%20A%20Vision-Language%20Model%20for%20Arabic%20Document-to-Markdown%20OCR，链接是  https://arxiv.org/pdf/2509.18174  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.18174  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Khalil Hennara,Muhammad Hreden,Mohamed Motasim Hamed,Ahmad Bastati,Zeina Aldallal,Sara Chrouf,Safwan AlModhayan</p>
<p><strong>Categories</strong>: cs.CV,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18174.pdf">https://arxiv.org/pdf/2509.18174.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18174">https://arxiv.org/abs/2509.18174</a></p>
<p><strong>Arxiv ID</strong>: 2509.18174</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18174">https://papers.cool/arxiv/2509.18174</a></p>
<p><strong>Published</strong>: 2025-09-17T15:07:29Z</p>
<p><strong>Updated</strong>: 2025-09-17T15:07:29.000Z</p>
<hr>
<h3 id="2-Reinforcement-Learning-on-Pre-Training-Data"><a href="#2-Reinforcement-Learning-on-Pre-Training-Data" class="headerlink" title="2. Reinforcement Learning on Pre-Training Data"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19249">Reinforcement Learning on Pre-Training Data</a></h3><p>The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>计算资源的指数级增长与高质量文本数据有限增长之间日益严重的差距，限制了传统大语言模型（LLM）的扩展方法。为了应对这一挑战，我们引入了基于预训练数据的强化学习（RLPT），这是一种新的训练阶段扩展范式，用于优化LLM。与之前主要通过监督学习扩大训练的传统方法不同，RLPT使得策略能够自主探索有意义的轨迹，从预训练数据中学习，并通过强化学习（RL）提高其能力。虽然现有的RL策略，如人类反馈强化学习（RLHF）和具有可验证奖励的强化学习（RLVR），依赖于人类注释来构建奖励，但RLPT通过直接从预训练数据中推导奖励信号消除了这一依赖。具体而言，它采用了下一个片段推理目标，奖励策略准确预测基于前文上下文的后续文本片段。这个公式使得在预训练数据上可以进行RL扩展，鼓励在更广泛的上下文中探索更丰富的轨迹，从而促进更具通用性的推理能力。在多个模型的普通领域和数学推理基准测试上的大量实验验证了RLPT的有效性。例如，当应用于Qwen3-4B-Base时，RLPT在MMLU、MMLU-Pro、GPQA-Diamond、KOR-Bench、AIME24和AIME25上分别取得了绝对改善的$3.0$、$5.1$、$8.1$、$6.0$、$6.6$和$5.3$。结果进一步展示了良好的扩展性行为，暗示着随着计算能力增加潜在的强大收益。此外，RLPT提供了一个坚实的基础，扩展了LLM的推理边界，并增强了RLVR的性能。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“高质量文本数据增长趋缓”与“算力持续指数级扩张”之间的矛盾，提出 Reinforcement Learning on Pre-Training data（RLPT），旨在：</p>
<ul>
<li>摆脱对人工标注奖励的依赖，使强化学习可直接在大规模无标注预训练语料上规模化展开；</li>
<li>用“下一段推理”自监督目标替代传统“下一个 token”监督目标，鼓励模型主动探索更长、更连贯的推理轨迹；</li>
<li>在训练阶段而非推理阶段投入额外算力，实现新的“训练时扩展”范式，从而持续提升 LLM 的通用与数学推理能力，并为后续 RLVR 提供更强的基础。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 RLPT 直接相关或处于同一技术脉络的研究可分为三类，均以“在无标注数据上通过 RL 提升 LLM”为核心诉求，但奖励信号与训练粒度不同：</p>
<ol>
<li><p>同类型“无监督 RL 预训练”</p>
<ul>
<li><strong>RPT</strong>（Reinforcement Pre-Training, Dong et al. 2025）<br>采用 next-token 预测作为 RL 目标，奖励为预测 token 是否匹配；RLPT 将其升级为 next-segment 预测并引入双向上下文。</li>
<li><strong>Quiet-STaR</strong>（Zelikman et al. 2024）<br>让模型在说话前生成内部“思考”token，再用后续真实 token 给予 0/1 奖励；RLPT 直接以完整句子/推理步为粒度，无需额外思考 token。</li>
</ul>
</li>
<li><p>传统“带标注 RL”方法（被 RLPT 试图摆脱的瓶颈）</p>
<ul>
<li><strong>RLHF</strong>（Bai et al. 2022; Ouyang et al. 2022）<br>依赖人工偏好对训练奖励模型，难以随预训练语料一起规模化。</li>
<li><strong>RLVR</strong>（Guo et al. 2025; Shao et al. 2024）<br>用可验证答案构造规则奖励，仍需人工整理答案或规则，且通常局限于数学/代码场景。RLPT 可作为其“热身”阶段，先自监督提升推理边界。</li>
</ul>
</li>
<li><p>训练-推理扩展律与泛化研究</p>
<ul>
<li><strong>SFT memorizes, RL generalizes</strong>（Chu et al. 2025; Lai et al. 2025; Shenfeld et al. 2025）<br>从理论上对比监督微调与 RL 的泛化能力，为 RLPT“用 RL 替代继续 SFT”提供经验支持。</li>
<li><strong>Scaling Laws</strong>（Kaplan et al. 2020; Hoffmann et al. 2022）<br>给出参数-数据-算力幂律关系；RLPT 进一步在预训练语料上验证“继续 RL 扩展”也服从类似幂律。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下三步把“无标注预训练语料上的规模化强化学习”转化为可训练、可扩展的算法框架，从而解决“数据瓶颈”与“人工奖励不可扩展”的矛盾：</p>
<ol>
<li><p>自监督奖励构造——Next-Segment Reasoning</p>
<ul>
<li>将原始文本按句子粒度切分为段序列 $s_1,s_2,\dots,s_n$。</li>
<li>设计两类任务：<br>– <strong>ASR</strong>（Autoregressive Segment Reasoning）：给定前缀 $s_{&lt;i}$，用 RL 策略生成 $\hat s_i$，与真实 $s_i$ 比较。<br>– <strong>MSR</strong>（Middle Segment Reasoning）：给定 $s_{&lt;i}$ 与后续 $s_{i+1}$，要求补全被掩码的 $s_i$。</li>
<li>用轻量级生成式奖励模型 $G_{\text{rm}}$ 判断“预测段是否为真实段的前缀且语义等价”，输出 0/1 奖励，无需人工标注。</li>
</ul>
</li>
<li><p>训练流程——冷启动 + 在线 RL</p>
<ul>
<li><strong>冷启动</strong>：先在少量指令数据上做 SFT，使模型具备“按格式输出预测段”的能力。</li>
<li><strong>在线 RL</strong>：采用 GRPO（无 KL 正则）对每条提示采样 8 条轨迹，用上述 0/1 奖励最大化期望回报；ASR 与 MSR 按固定比例 $\lambda$ 混合训练。</li>
</ul>
</li>
<li><p>扩展与协同——训练时算力再分配</p>
<ul>
<li>整个框架只在预训练语料上消耗额外算力，属于“训练时扩展”，与推理期扩展正交。</li>
<li>实验显示性能随训练 token 数呈幂律增长；继续接入 RLVR 时，RLPT 提供的策略初始化带来额外增益，说明其“探索能力”未被削弱，反而成为后续强监督 RL 的坚实基础。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“通用能力”与“数学推理”两条主线展开，覆盖 3 组基础模型、9 个公开基准，并系统验证 scaling 行为与消融效果。</p>
<ol>
<li><p>主实验：多模型、多基准对比</p>
<ul>
<li><strong>模型</strong>：Llama-3.2-3B、Qwen3-4B、Qwen3-8B</li>
<li><strong>通用基准</strong>：MMLU、MMLU-Pro、GPQA-Diamond、SuperGPQA、KOR-Bench、OlympiadBench（指标：accuracy）</li>
<li><strong>数学基准</strong>：MATH-500、AMC23、Minerva Math、AIME24/25（指标：Pass@1/Pass@8，n=64 样本）</li>
<li><strong>结果</strong>：<ul>
<li>在 Qwen3-4B 上 RLPT 相对 base 平均提升 3.0–8.1 个百分点；AIME24 Pass@1 从 7.3→27.2。</li>
<li>相同趋势在 Llama-3.2-3B 与 Qwen3-8B 上依旧显著，验证跨系列有效性。</li>
</ul>
</li>
</ul>
</li>
<li><p>后续 RLVR 协同实验</p>
<ul>
<li>以 RLPT 策略为初始权重，再运行数学规则奖励 RLVR。</li>
<li>Qwen3-4B 上 AIME24 Pass@1 再提升 2.3，Pass@8 再提升 3.7，表明 RLPT 既增强探索也为后续利用提供更好起点。</li>
</ul>
</li>
<li><p>扩展律分析</p>
<ul>
<li>固定模型规模，仅增加 RLPT 训练 token 量（0.5B–2B）。</li>
<li>所有基准指标与训练 token 呈幂律关系 $y=a x^{b}$（图 1），拟合 $R^{2}&gt;0.98$，预示继续加算力仍可获益。</li>
</ul>
</li>
<li><p>消融与诊断</p>
<ul>
<li><strong>奖励粒度</strong>：严格“整句匹配”→ 稀疏奖励、长度短；改用“前缀语义一致”后训练稳定、下游 Pass@1 提升 3–5 点（图 4）。</li>
<li><strong>任务混合</strong>：仅 ASR 或仅 MSR 均不及 $\lambda=0.5$ 混合；MSR 对代码/填空类任务增益更明显。</li>
<li><strong>响应长度</strong>：前缀奖励使平均输出长度增加 30–40%，与数学任务提升正相关。</li>
</ul>
</li>
<li><p>可视化与样例</p>
<ul>
<li>表 3 展示 RLPT 在傅里叶变换例题上的完整思考链，出现“step-by-step / verify / alternative / backtrack”等多步策略，验证其通过探索习得结构化推理。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>段粒度自适应</strong><br>当前默认按句子切分，可探索让模型或规则自动识别“原子推理步”，使奖励信号与信息密度对齐。</p>
</li>
<li><p><strong>多粒度混合目标</strong><br>将 token-level、phrase-level、sentence-level 的预测统一为分层 RL，考察不同粒度对通用知识与逻辑推理的各自贡献。</p>
</li>
<li><p><strong>课程与难度过滤</strong><br>引入可学习的课程策略，先易后难地呈现文本，避免早期探索陷入过难段落导致训练信号稀疏。</p>
</li>
<li><p><strong>双向与多模态扩展</strong><br>把 MSR 拓展为“任意位置掩码”并加入代码、表格、LaTeX 公式等多模态上下文，验证在 STEM 文档上的增益。</p>
</li>
<li><p><strong>奖励模型自迭代</strong><br>用 RLPT 训练后的策略反过来生成更高质量参考段，再蒸馏或微调 $G_{\text{rm}}$，实现“策略-奖励”协同提升。</p>
</li>
<li><p><strong>Scaling Law 细化</strong><br>同时变化模型规模、段长度、采样数 $k$、ASR/MSR 比例，拟合三维扩展律，指导算力最优分配。</p>
</li>
<li><p><strong>与继续预训练对比</strong><br>在相同算力预算下，与“额外 next-token 预训练”做严格对照，量化 RL 探索相对于监督记忆的真实边际收益。</p>
</li>
<li><p><strong>下游遗忘与对齐</strong><br>大规模 RLPT 后继续 RLHF，测量安全、指令遵循是否退化；研究正则或混合目标以抑制潜在漂移。</p>
</li>
<li><p><strong>测试时扩展协同</strong><br>将 RLPT 策略作为基础，再叠加 Best-of-N、Self-Consistency、MCTS 等测试时扩展，检验“训练探索+推理搜索”的叠加极限。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心内容速览</strong></p>
<ol>
<li><p><strong>问题</strong><br>高质量文本增速放缓，传统“继续预训练”式扩展逼近上限；RLHF/RLVR 又依赖昂贵人工标注，难以随数据一起规模化。</p>
</li>
<li><p><strong>方案</strong><br>提出 <strong>RLPT</strong>（Reinforcement Learning on Pre-Training data）：</p>
<ul>
<li>把无标注语料切成句子段，构建自监督“下一段推理”任务：<br>– ASR：凭上文生成当前段<br>– MSR：凭上下文补全掩码段</li>
<li>用轻量级生成式奖励模型判断“预测段是否为真实段语义前缀”，输出 0/1 奖励，彻底摆脱人工。</li>
<li>冷启动 → 在线 GRPO（on-policy，无 KL）在大规模语料上持续优化。</li>
</ul>
</li>
<li><p><strong>结果</strong></p>
<ul>
<li>多模型（Llama-3.2-3B、Qwen3-4/8B）多基准（MMLU 系列、GPQA、AIME24/25 等）一致显著提升，AIME24 Pass@1 绝对 +19.9。</li>
<li>性能随训练 token 数呈幂律增长，验证“训练时 RL 扩展”可持续。</li>
<li>以 RLPT 初始化后再跑 RLVR，可再 +2–4 点，证明其既增强探索也为后续利用奠基。</li>
</ul>
</li>
<li><strong>结论</strong><br>RLPT 首次实现“零人工奖励”的大规模预训练语料强化学习，为数据瓶颈下的 LLM 继续扩展提供新范式，并可无缝衔接现有对齐/推理强化阶段。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Reinforcement%20Learning%20on%20Pre-Training%20Data，链接是  https://arxiv.org/pdf/2509.19249  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19249  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Siheng Li,Kejiao Li,Zenan Xu,Guanhua Huang,Evander Yang,Kun Li,Haoyuan Wu,Jiajia Wu,Zihao Zheng,Chenchen Zhang,Kun Shi,Kyrierl Deng,Qi Yi,Ruibin Xiong,Tingqiang Xu,Yuhao Jiang,Jianfeng Yan,Yuyuan Zeng,Guanghui Xu,Jinbao Xue,Zhijiang Xu,Zheng Fang,Shuai Li,Qibin Liu,Xiaoxue Li,Zhuoyu Li,Yangyu Tao,Fei Gao,Cheng Jiang,Bo Chao Wang,Kai Liu,Jianchen Zhu,Wai Lam,Wayyt Wang,Bo Zhou,Di Wang</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19249.pdf">https://arxiv.org/pdf/2509.19249.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19249">https://arxiv.org/abs/2509.19249</a></p>
<p><strong>Arxiv ID</strong>: 2509.19249</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19249">https://papers.cool/arxiv/2509.19249</a></p>
<p><strong>Published</strong>: 2025-09-23T17:10:40Z</p>
<p><strong>Updated</strong>: 2025-09-23T17:10:40.000Z</p>
<hr>
<h3 id="3-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies"><a href="#3-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies" class="headerlink" title="3. Do You Need Proprioceptive States in Visuomotor Policies?"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.18644">Do You Need Proprioceptive States in Visuomotor Policies?</a></h3><p>Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\% to 85\% in height generalization and from 6\% to 64\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于模仿学习的视觉运动策略在机器人操作中得到了广泛应用，其中通常结合视觉观察和本体状态以实现精确控制。然而，在本研究中，我们发现这一常见做法使得策略过于依赖本体状态输入，导致对训练轨迹的过拟合，并且在空间上表现不佳。相反，我们提出了无状态策略，去除了本体状态输入，仅基于视觉观察预测动作。无状态策略是在相对末端执行器动作空间中构建的，应确保所有与任务相关的视觉观察，这里由双宽角腕部摄像头提供。实证结果表明，无状态策略在空间泛化能力上显著强于基于状态的策略：在现实任务中，如抓取与放置、挑战性的折衬衫和复杂的全身操作，跨越多个机器人实现，平均成功率在高度泛化方面从0%提高到85%，在水平泛化方面从6%提高到64%。此外，它们在数据效率和跨实现适应性方面也表现出优势，增强了其在现实世界部署中的实用性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对模仿学习视觉-运动策略（visuomotor policy）在<strong>空间泛化</strong>上的瓶颈：传统方法把本体感受状态（proprioceptive state，如关节角、末端位姿）与视觉观测一起输入网络，虽然训练稳定，却导致策略过度依赖状态，从而<strong>死记训练轨迹</strong>，一旦物体在高度或水平位置上偏离训练分布便彻底失效。作者提出并验证<strong>完全去掉状态输入</strong>的“State-free Policy”，仅凭借视觉观测在<strong>相对末端位姿动作空间</strong>下输出动作，配合<strong>全覆盖任务相关视野</strong>（双广角腕部相机），在真实机器人拾放、叠衣服、全身取瓶等任务上将<strong>高度泛化成功率从0%→85%，水平泛化从6%→64%</strong>，同时提升数据效率与跨本体迁移能力，从根本上缓解“状态依赖导致空间泛化差”的核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第二节“RELATED WORKS”中将与自身密切相关的研究归为三大类，并指出它们各自的不足，从而凸显“State-free Policy”的差异化价值。以下按原文脉络归纳：</p>
<ol>
<li><p><strong>Visuomotor Policies &amp; State Inputs</strong></p>
<ul>
<li>典型工作：ACT、Diffusion Policy、π0 等模仿学习框架均把本体感受状态与视觉同时输入网络，以加速收敛、提高微调稳定性。</li>
<li>被指问题：状态提供了“捷径”，策略可直接把绝对状态→专家动作，造成对训练轨迹的过拟合，严重限制空间泛化。</li>
</ul>
</li>
<li><p><strong>Improving Spatial Generalization</strong><br>文献尝试从四条路线缓解空间泛化不良，但作者认为均代价高昂或效果有限：</p>
<ul>
<li><strong>数据驱动</strong>（Open X-Embodiment、DROID、BridgeData V2 等）：靠采集大量空间分布多样的演示，真实场景成本极高。</li>
<li><strong>仿真-到-真实</strong>（Domain Randomization、Isaac Gym、Rubik’s Cube 等）：存在难以消除的 sim-to-real 差距。</li>
<li><strong>表征驱动</strong>（Transporter Networks、对象中心表征、预训练视觉编码等）：依赖额外感知管线或手工特征，系统复杂。</li>
<li><strong>架构/正则化</strong>（SE(3)-等变网络、位置不变正则、物理对称性约束）：假设在真实条件下易失效，且增益有限。</li>
</ul>
</li>
<li><p><strong>Action Space 选择</strong></p>
<ul>
<li>相对末端位姿动作空间（Relative EEF）已被 UMI 等工作提及，但未被用于“完全去状态”这一极端设定；本文首次系统验证该动作空间与纯视觉输入耦合后，对高度/水平泛化、数据效率、跨本体迁移的全面提升。</li>
</ul>
</li>
</ol>
<p>综上，本文与既有研究的根本区别在于：<strong>不引入额外数据、仿真或复杂感知模块，仅通过“剔除状态输入+相对 EEF 动作+全覆盖视野”这一极简策略，从根本上消除状态捷径带来的过拟合，显著释放空间泛化潜力。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“状态输入导致空间泛化崩溃”这一核心问题拆解为<strong>两个必要条件</strong>并给出对应实现，从而在不增加数据成本、不修改网络架构的前提下，直接<strong>剔除状态输入</strong>，让策略只能依赖视觉推理。具体做法如下：</p>
<ol>
<li><p><strong>动作空间：强制使用“相对末端位姿（Relative EEF）”</strong></p>
<ul>
<li>网络输出的是相对于当前观测的微小位移 $Δp_t=(Δx_t,Δq_t)$，而非绝对位姿或关节角。</li>
<li>由于 $Δp_t$ 仅由视觉决定，与机器人绝对配置无关，同一视觉场景在任何高度/水平位置都产生相同位移，天然满足<strong>空间不变性</strong>。</li>
</ul>
</li>
<li><p><strong>观测空间：保证“全覆盖任务相关视野（Full Task Observation）”</strong></p>
<ul>
<li>仅用普通腕部相机时，目标物常被末端遮挡→信息缺失→策略失效。</li>
<li>解决方案：在末端<strong>上下各装一枚 120°×120° 广角相机</strong>，形成双广角腕部视野；实验显示其已能完整覆盖整条轨迹所需信息，<strong>甚至可舍弃 overhead 相机</strong>而进一步提升泛化。</li>
</ul>
</li>
<li><p><strong>训练流程：标准模仿学习，无需额外技巧</strong></p>
<ul>
<li>采用 π0 / ACT / Diffusion Policy 等现有模仿学习框架，<strong>直接去掉状态分支</strong>，其余超参不变。</li>
<li>损失函数仍为最大似然 $L_{IL}(θ)=−∑_{(o_t,a_t)∈D}logπ_θ(a_t|o_t)$，<strong>不引入正则、不增广状态、不混合任务</strong>。</li>
</ul>
</li>
</ol>
<p>通过上述“两条件+一剔除”，State-free Policy 被迫从视觉中推理任务空间关系，而非死记“状态→动作”映射，从而在<strong>高度、水平、跨本体、数据量缩减</strong>等维度同时获得显著增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在真实机器人、仿真基准和多款策略骨架上共完成 <strong>3 大类 9 小项</strong> 实验，系统验证“State-free Policy”的泛化性、数据效率与跨本体迁移能力。核心结果用 <strong>Success Rate（30/30 或 60 回合平均）</strong> 衡量；所有对比均保持网络结构、训练超参一致，仅切换“是否输入状态”与相机配置。</p>
<p>1. 空间泛化压力测试（真实世界）</p>
<p>任务</p>
<p>机器人本体</p>
<p>训练分布</p>
<p>评估分布</p>
<p>状态输入</p>
<p>主要结果（高度/水平）</p>
<p>Pick Pen</p>
<p>双臂 Arx5</p>
<p>桌高 80 cm，笔筒固定</p>
<p>桌高 72/90 cm；笔筒平移 5/10 cm</p>
<p>w/ vs w/o</p>
<p>高度 0→98 %；水平 0→58 %</p>
<p>Pick Bottle</p>
<p>同上</p>
<p>同上</p>
<p>同上</p>
<p>w/ vs w/o</p>
<p>高度 0→78 %；水平 6→73 %</p>
<p>Put Lid</p>
<p>同上</p>
<p>同上</p>
<p>同上</p>
<p>w/ vs w/o</p>
<p>高度 0→80 %；水平 27→28 %</p>
<p>Fold Shirt</p>
<p>双臂 Arx5</p>
<p>双臂居中叠衫</p>
<p>单臂或双整体平移 15 cm</p>
<p>w/ vs w/o</p>
<p>水平 18→83 %</p>
<p>Fetch Bottle（whole-body）</p>
<p>26-DoF 移动底座+躯干</p>
<p>冰箱固定位置</p>
<p>冰箱平移 5/10 cm</p>
<p>w/ vs w/o</p>
<p>水平 12→78 %</p>
<blockquote>
<p>注：高度泛化仅在前 3 个任务可行（桌面可调）；后 2 个任务因机械固定或冰箱不可调，只测水平。</p>
</blockquote>
<p>2. 消融与架构鲁棒性实验</p>
<p>实验目的</p>
<p>变量</p>
<p>条件</p>
<p>关键结论</p>
<p>动作空间选择</p>
<p>相对 EEF / 绝对 EEF / 相对关节 / 绝对关节</p>
<p>双广角腕部相机 + w/o state</p>
<p><strong>仅相对 EEF</strong> 在高度 &amp; 水平泛化同时保持 &gt;95 %；其余均≈0 %</p>
<p>观测充分性</p>
<p>相机数量/视角/是否加 overhead</p>
<p>固定 w/o state + 相对 EEF</p>
<p>视野越大泛化越高；<strong>双广角腕部相机 alone 达到最佳</strong>（高度 98 %，水平 58 %），overhead 反而有害</p>
<p>策略骨架通用性</p>
<p>π0 / ACT / Diffusion Policy</p>
<p>双广角腕部相机 + 相对 EEF</p>
<p>三种网络“去状态”后高度泛化均≥86 %，原带状态版本全部 0 %，证明结论与具体骨架无关</p>
<p>3. 额外收益验证</p>
<p>3.1 数据效率（In-domain）</p>
<ul>
<li>任务：Pick Pen</li>
<li>数据量：50/100/200/300 条演示</li>
<li>结果：<ul>
<li>带状态策略在 50 条时成功率已降至 0 %（过拟合）。</li>
<li>State-free 在 50 条仍保持 60 %，100 条即可达到 87 %，<strong>显著降低现场采集成本</strong>。</li>
</ul>
</li>
</ul>
<p>3.2 跨本体迁移</p>
<ul>
<li>源：Arx5 双臂（EEF 坐标系为“桌面中心”）</li>
<li>目标：人形 2×8-DoF 双臂（EEF 坐标系为“胸骨中心”）</li>
<li>协议：各用 100 条新本体演示微调 5 k/10 k 步</li>
<li>结果：<ul>
<li>带状态：5 k 步 33 %，10 k 步 77 %</li>
<li>State-free：5 k 步 70 %，10 k 步 97 %<br><strong>→ 省去状态对齐，迁移更快更稳。</strong></li>
</ul>
</li>
</ul>
<p>3.3 重新审视 Overhead 相机（极限场景）</p>
<ul>
<li>在“Pick Pen”上设置三种分布外：<ul>
<li>桌高 100 cm</li>
<li>笔筒加高一倍</li>
<li>笔筒外移 20 cm</li>
</ul>
</li>
<li>结果：<ul>
<li>保留 overhead 相机：0–47 % 成功率</li>
<li>仅用双广角腕部相机：80–100 % 成功率<br><strong>→ overhead 视角引入额外分布漂移，去除后反而更鲁棒。</strong></li>
</ul>
</li>
</ul>
<p>4. 仿真对照（LIBERO Benchmark）</p>
<ul>
<li>四套件（Spatial/Goal/Object/10-Task）上微调 π0</li>
<li>带状态 vs 去状态平均成功率 93.8 % vs 94.5 %</li>
<li><strong>→ 在观测严重受限的仿真环境依旧不掉点</strong>，验证方法对相机配置不敏感。</li>
</ul>
<p>综上，实验覆盖</p>
<ol>
<li>真实机器人 <strong>5 任务</strong>（含全身 26-DoF）、</li>
<li>仿真套件 <strong>4 子集</strong>、</li>
<li>三种主流策略骨架、</li>
<li>动作/观测/数据量/本体迁移 <strong>系统消融</strong>，<br>用 <strong>>1 000 次真实机回合 + 数千仿真回合</strong> 证明：<br><strong>“去掉状态 + 相对 EEF + 双广角腕部相机”即可在完全不增大数据成本的前提下，把高度/水平泛化性能提升 1–2 个数量级，同时更省数据、更易跨本体部署。</strong></li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为“State-free Policy”框架的自然延伸，亦在论文 Limitation 部分有所暗示；每项均附带可验证的实验切口与潜在科学问题，供后续研究直接切入。</p>
<p>1. 视觉分布漂移的量化与鲁棒化</p>
<ul>
<li><strong>背景更换、光照变化、桌面纹理</strong> → 纯视觉策略性能骤降。</li>
<li><strong>待解决问题</strong>：如何在不重新采集演示的前提下，对背景/光照做<strong>零样本</strong>或<strong>少样本</strong>自适应？</li>
<li><strong>可探索路线</strong>：<ul>
<li>引入<strong>领域随机化+可学习的视觉归一化</strong>（如 AdaIN、BN 统计补偿）；</li>
<li>采用<strong>测试时自适应（TTA）</strong>：在线估计 BN 统计或微调最后一层视觉编码器；</li>
<li>构造<strong>背景-前景解耦表征</strong>（如 Slot Attention、Diffusion Auto-Encoder）并验证其可迁移性。</li>
</ul>
</li>
</ul>
<p>2. 双（多）臂“闲置臂”视觉干扰</p>
<ul>
<li>当仅单臂执行任务时，<strong>闲置臂的相机图像与训练分布不一致</strong>，可能引发非预期运动。</li>
<li><strong>待解决问题</strong>：如何设计<strong>选择性观测丢弃</strong>或<strong>注意力掩码</strong>，让策略自动忽略与任务无关的图像区域？</li>
<li><strong>可探索路线</strong>：<ul>
<li>在 Transformer 的 Cross-Attention 中增加<strong>可学习的任务相关掩码 token</strong>；</li>
<li>引入<strong>因果推断</strong>方法（如 DoWhy、Invariant Risk Minimization）自动识别并屏蔽干扰视角。</li>
</ul>
</li>
</ul>
<p>3. 动态/非刚体任务扩展</p>
<ul>
<li>当前评估以<strong>准静态</strong>（pick-and-place、叠衣）为主，<strong>动态投掷、搅拌、拉抽屉</strong>等任务对“视觉-动作闭环”延迟更敏感。</li>
<li><strong>待解决问题</strong>：State-free 是否因缺少速度/力状态而在高动态任务失效？</li>
<li><strong>可探索路线</strong>：<ul>
<li>在仿真环境（Issac Gym, Mujoco）构建<strong>动态版 LIBERO</strong>，系统比较“视觉-only”与“视觉+速度”输入；</li>
<li>引入<strong>事件相机</strong>或<strong>高帧率 RGB</strong> 补偿延迟，验证纯视觉策略的<strong>采样率-鲁棒性</strong>权衡。</li>
</ul>
</li>
</ul>
<p>4. 动作空间再分解：SE(3) 解耦与不确定性估计</p>
<ul>
<li>现有相对 EEF 把平移+旋转拼成 6D 向量，<strong>旋转部分用指数映射或四元数</strong>，未显式建模不确定性。</li>
<li><strong>待探索</strong>：<ul>
<li>采用<strong>李代数 se(3) 流形上的扩散策略</strong>（Diffusion on Manifold），为旋转和平移分别预测<strong>各向异性噪声</strong>；</li>
<li>输出** per-action 不确定度**，在接近奇异点或视觉遮挡时<strong>触发保守策略</strong>或<strong>主动感知</strong>。</li>
</ul>
</li>
</ul>
<p>5. 无状态条件下的主动感知与视觉伺服</p>
<ul>
<li>既然状态不可见，能否让策略<strong>主动选择相机位姿</strong>以最大化信息增益？</li>
<li><strong>可探索路线</strong>：<ul>
<li>把腕部相机装在<strong>伺服云台</strong>，将“下一视角选择”作为策略的额外动作维度，用<strong>信息增益奖励</strong>（MI, entropy drop）自监督；</li>
<li>与<strong>强化学习</strong>结合，训练“视觉-运动-主动感知”一体化策略，验证在<strong>遮挡、狭窄容器</strong>内任务的优势。</li>
</ul>
</li>
</ul>
<p>6. 跨本体迁移的“视觉-动作对齐”理论</p>
<ul>
<li>实验显示 State-free 更易跨臂迁移，但<strong>缺乏定量指标</strong>解释何时可迁移。</li>
<li><strong>待解决问题</strong>：如何度量<strong>视觉重叠度</strong>与<strong>动作空间一致性</strong>，以预测零样本或微调步数下限？</li>
<li><strong>可探索路线</strong>：<ul>
<li>建立<strong>本体迁移图（Embodiment Graph）</strong>，节点为不同机器人，边权重 = 视觉编码器特征距离 + 相对 EEF 可达空间重叠；</li>
<li>利用<strong>图神经网络</strong>预测迁移成功率，指导<strong>演示数据预算</strong>与<strong>微调策略选择</strong>。</li>
</ul>
</li>
</ul>
<p>7. 与人类动作-视觉认知的对齐</p>
<ul>
<li>人类在蒙眼或本体感受缺失时仍能<strong>凭视觉完成抓取</strong>，反之亦然。</li>
<li><strong>待探索</strong>：<ul>
<li>采集<strong>人类眼动+手部运动</strong>同一任务数据，比较人类与 State-free 策略的<strong>注视热图</strong>是否一致；</li>
<li>若存在显著差异，可引入<strong>认知启发的注意力正则</strong>，提升策略的可解释性与安全性。</li>
</ul>
</li>
</ul>
<p>8. 安全与故障恢复</p>
<ul>
<li>纯视觉策略<strong>无法直接感知关节超限或碰撞力矩</strong>，可能导致<strong>视觉幻觉</strong>而持续输出危险动作。</li>
<li><strong>可探索路线</strong>：<ul>
<li>在 State-free 框架外<strong>并行运行轻量级安全层</strong>（仅需关节限位与力阈值），一旦触发即切换至<strong>安全冻结或阻抗模式</strong>；</li>
<li>研究<strong>视觉-力觉延迟融合</strong>的最小表达：能否仅用**二值“是否接触”**信号即可在保持无状态的同时实现安全恢复？</li>
</ul>
</li>
</ul>
<p>9. 从“无状态”到“轻状态”——稀疏本体的最优注入</p>
<ul>
<li>完全无状态偶尔导致<strong>视觉遮挡下的瞬时抖动</strong>，能否<strong>按需注入 1–2 维最关键本体量</strong>（如手腕高度或 gripper 开度）而不重新引入过拟合？</li>
<li><strong>可探索路线</strong>：<ul>
<li>采用<strong>Lasso-like 输入门控</strong>：训练时对状态各维度加稀疏惩罚，自动剔除冗余；</li>
<li>设计<strong>元控制器</strong>：平时视觉为主，当视觉不确定度 &gt; 阈值时<strong>动态引入最小状态</strong>，实现“轻状态”平滑过渡。</li>
</ul>
</li>
</ul>
<p>10. 大规模可扩展性验证</p>
<ul>
<li>当前最大任务 10 k 条轨迹、5 个任务。<strong>尚未在十万/百万级数据验证 State-free 是否仍保持数据效率优势</strong>。</li>
<li><strong>下一步</strong>：<ul>
<li>在 Open-X 或 DROID 百万轨迹级数据集上<strong>仅过滤出视觉-动作对</strong>，完全丢弃状态，训练<strong>大参数视觉-动作 Transformer</strong>；</li>
<li>比较与“状态+视觉”同规模模型的<strong>泛化-参数效率 Pareto 前沿</strong>，验证 State-free 的<strong>数据-参数 scaling law</strong> 是否更优。</li>
</ul>
</li>
</ul>
<p>总结</p>
<p>State-free Policy 打开了“<strong>无本体感知下的视觉-运动控制</strong>”新范式，后续研究可从<strong>鲁棒性、动态性、安全性、认知对齐、 scaling 法则</strong>等多维切入，在保持“极简传感器”优势的同时，逐步补全其短板，向<strong>真正即插即用的通用机器人策略</strong>迈进。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>一句话概括</strong><br>剔除本体感受状态、仅用视觉+相对末端位姿动作空间，再配双广角腕部相机，就能让模仿学习策略在<strong>零额外数据成本</strong>的前提下，把<strong>高度/水平空间泛化成功率从≈0 % 提升到 80 % 以上</strong>，同时更省数据、更易跨机器人迁移。</p>
<p>1. 问题</p>
<ul>
<li>主流 visuomotor policy 把“关节角/末端位姿”作为输入，提供<strong>训练捷径</strong>，导致策略<strong>死记训练轨迹</strong>。</li>
<li>一旦物体在<strong>垂直或平面方向</strong>偏离演示分布，成功率骤降至 0 %，成为空间泛化的<strong>核心瓶颈</strong>。</li>
</ul>
<p>2. 解法（State-free Policy）</p>
<p>两个必要条件、一个删除：</p>
<p>条件</p>
<p>实现</p>
<p>作用</p>
<p><strong>删除状态输入</strong></p>
<p>网络只接收图像</p>
<p>切断“绝对配置→动作”捷径，迫使策略依赖视觉推理</p>
<p><strong>相对末端位姿动作</strong></p>
<p>输出 Δx, Δq，与当前位姿无关</p>
<p>保证同一视觉场景在任何高度/水平位置产生相同位移</p>
<p><strong>全覆盖任务视野</strong></p>
<p>双 120°×120° 广角腕部相机（上下各一）</p>
<p>全程完整观测任务相关物体，避免遮挡</p>
<p>3. 实验规模与结果</p>
<ul>
<li><strong>5 项真实任务</strong>（拾笔、拾瓶、盖盖子、叠衣服、全身开冰箱取瓶），<strong>3 款策略骨架</strong>（π0、ACT、Diffusion），<strong>>1 000 次真机回合</strong><ul>
<li>高度泛化：0 % → <strong>85 % 平均</strong></li>
<li>水平泛化：6 % → <strong>64 % 平均</strong></li>
</ul>
</li>
<li><strong>数据效率</strong>：50 条演示即可达 60 % 成功率，状态版同期 0 %</li>
<li><strong>跨本体迁移</strong>：100 条演示、5 k 步微调，成功率 33 % → <strong>70 %</strong></li>
<li><strong>仿真 LIBERO 四套件</strong>：去状态后平均成功率 <strong>94.5 %</strong>，不掉点</li>
<li><strong>Overhead 相机反而有害</strong>：极端场景下成功率从 100 % 降至 0 %，去除后回至 80–100 %</li>
</ul>
<p>4. 结论与影响</p>
<ul>
<li><strong>状态输入是空间泛化差的根源</strong>，而非“必要信息”；</li>
<li><strong>相对 EEF + 全覆盖视野</strong> 即可让视觉-alone 策略在真实世界可靠泛化；</li>
<li><strong>降低数据收集成本、简化传感器堆栈、加速跨本体部署</strong>，为可扩展的通用机器人学习提供新基线。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Do%20You%20Need%20Proprioceptive%20States%20in%20Visuomotor%20Policies%3F，链接是  https://arxiv.org/pdf/2509.18644  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.18644  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Juntu Zhao,Wenbo Lu,Di Zhang,Yufeng Liu,Yushen Liang,Tianluo Zhang,Yifeng Cao,Junyuan Xie,Yingdong Hu,Shengjie Wang,Junliang Guo,Dequan Wang,Yang Gao</p>
<p><strong>Categories</strong>: cs.RO,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18644.pdf">https://arxiv.org/pdf/2509.18644.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18644">https://arxiv.org/abs/2509.18644</a></p>
<p><strong>Arxiv ID</strong>: 2509.18644</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18644">https://papers.cool/arxiv/2509.18644</a></p>
<p><strong>Published</strong>: 2025-09-23T04:56:59Z</p>
<p><strong>Updated</strong>: 2025-09-23T04:56:59.000Z</p>
<hr>
<h3 id="4-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe"><a href="#4-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe" class="headerlink" title="4. MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.18154">MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe</a></h3><p>Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\% GPU memory cost and 8.7\% inference time of Qwen2.5-VL 7B.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态大型语言模型（MLLM）正在快速发展，代表了人工智能发展的前沿。然而，它们的训练和推理效率已成为使 MLLM 更易于访问和可扩展的核心瓶颈。为了应对这些挑战，我们推出了 MiniCPM-V 4.5，这是一个专为高效率和强大性能而设计的 8B 参数模型。我们介绍了模型架构、数据策略和训练方法的三个核心改进：用于对图像和视频进行高度紧凑编码的统一 3D-Resampler 模型架构，无需繁重数据工程即可进行文档知识和文本识别的统一学习范式，以及用于熟练掌握短推理和长推理模式的混合强化学习策略。OpenCompass 评估中的综合实验结果表明，MiniCPM-V 4.5 超过了广泛使用的专有模型（如 GPT-4o-latest），以及明显更大的开源模型（如 Qwen2.5-VL 72B）。值得注意的是，以卓越的效率实现了强大的性能。例如，在广泛采用的 VideoMME 基准测试中，MiniCPM-V 4.5 在 30B 尺寸以下的模型中实现了最先进的性能，仅使用 Qwen2.5-VL 7B 的 46.7\% GPU 内存成本和 8.7\% 的推理时间。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文围绕“如何让多模态大语言模型（MLLM）在保持强劲性能的同时显著降低训练与推理成本”这一核心问题展开，具体拆解为三个子问题并给出对应技术路线：</p>
<ol>
<li><p>视觉编码效率瓶颈</p>
<ul>
<li>高分辨率图像与长视频产生过量视觉 token，导致 GPU 内存与计算时间随序列长度线性增长。</li>
<li>解法：提出统一 3D-Resampler，利用时空联合压缩，将 6 秒 2-fps 的 448×448 视频从 1 536–3 072 token 降至 128 token，实现 12×–24× 压缩。</li>
</ul>
</li>
<li><p>文档知识获取的工程代价</p>
<ul>
<li>传统方案依赖易错的外部 PDF 解析器，将版面复杂的文档转成图文交错序列，既引入噪声又需繁重数据清洗。</li>
<li>解法：提出“统一学习范式”，直接在文档图像上进行动态文本腐蚀与重建，模型自适应切换“精准 OCR”与“上下文知识推理”，无需外部解析器。</li>
</ul>
</li>
<li><p>推理模式单一导致的效率与效果矛盾</p>
<ul>
<li>纯长推理链虽提升复杂任务准确率，却在简单任务上产生过度冗长输出，拖慢训练与推理。</li>
<li>解法：设计“混合强化学习”策略，在 rollout 阶段随机交替短/长推理模式，联合优化，实现 33.3% 长样本即可达到纯长推理的峰值性能，同时显著缩短平均响应长度。</li>
</ul>
</li>
</ol>
<p>综上，论文通过架构（3D-Resampler）、数据（文档统一学习）、训练（混合 RL）三方面的协同改进，系统性地缓解 MLLM 的内存、速度与工程效率瓶颈，在 8 B 参数规模下取得与 GPT-4o-latest、Qwen2.5-VL-72 B 等模型相当甚至更优的综合效果。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在架构、数据与训练三个维度均与现有研究紧密衔接，可归纳为以下相关脉络：</p>
<ul>
<li><p><strong>视觉 token 压缩</strong></p>
<ul>
<li>2D Resampler 系列：MiniCPM-V 2.0、PaLI-3、Flamingo 使用可学习 query 做跨注意力压缩图像块。</li>
<li>Pixel Unshuffle + MLP：Qwen-VL、InternVL2 通过空间下采样+线性投影将 448×448 图像压至 256 token。</li>
<li>时序压缩：Video-LLaVA、Video-ChatGPT 仅简单池化帧特征；Ovis1.5 提出 3D window attention，但未在 8 B 级模型验证 96× 压缩比。</li>
</ul>
</li>
<li><p><strong>文档/OCR 学习</strong></p>
<ul>
<li>外部解析器方案：Nougat、DocParser 将 PDF 转 HTML/XML 再生成图文交错序列，流程冗长且易错位。</li>
<li>图像端统一重建：Donut、Pix2Struct 用 ViT-Encoder-Decoder 直接 OCR，但未与大规模 LLM 联合训练，也未引入动态腐蚀机制。</li>
</ul>
</li>
<li><p><strong>推理链效率</strong></p>
<ul>
<li>长链思维：GLM-4.1V-thinking、Kimi-VL、DeepSeek-R1 在 RL 阶段强制输出 <code>&lt;think&gt;…&lt;/think&gt;</code>，但仅支持单一长模式，导致简单任务过度冗长。</li>
<li>短/长混合：MiMo-VL-7B-RL 提出“可切换推理”概念，但未在 8 B 级模型给出系统压缩率与训练 token 对比；MiniCPM-V 4.5 首次将混合 rollout 与 GRPO 结合，实现跨模式泛化。</li>
</ul>
</li>
<li><p><strong>高效视频理解</strong></p>
<ul>
<li>长视频基准：Video-MME、LVBench、LongVideoBench 强调分钟级理解；此前 Qwen2.5-VL-7B 需 1 536 token/样例，InternVL3-8B 需 3 072 token，而 MiniCPM-V 4.5 仅用 128 token 达到同等精度。</li>
</ul>
</li>
<li><p><strong>奖励建模与幻觉抑制</strong></p>
<ul>
<li>规则奖励：Math-Shepherd、LMM-R1 对数学答案做正则匹配。</li>
<li>概率奖励：RLPR、RLLM 提出用模型似然估计替代人工规则。</li>
<li>事实对齐：RLAIF-V、LLaVA-RLHF 通过 claim-level 验证+偏好学习抑制幻觉；MiniCPM-V 4.5 将其扩展至视频帧序列。</li>
</ul>
</li>
</ul>
<p>综上，MiniCPM-V 4.5 在 2D→3D Resampler、文档图像统一重建、短/长混合 RL 三个技术点上相对既有工作做出显性改进，其余模块（图像划分、奖励塑形、偏好学习）均建立在上述代表性研究之上并做了针对性集成。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“高效且强性能的多模态大模型”拆解为三大瓶颈，并分别给出针对性技术方案，三者再经统一训练流程串联，形成完整解决路径：</p>
<ol>
<li><p>架构层：统一 3D-Resampler 压缩视觉序列</p>
<ul>
<li>2D 扩展为 3D：在原有 2D-Resampler 的“可学习 query + 跨注意力”框架上，引入时序位置编码，对视频帧包（temporal package）联合做时空压缩。</li>
<li>压缩率：单帧 448×448 图像 64 token；6 秒 2-fps 视频共 12 帧→128 token（≈ 96× 相对像素展开）。</li>
<li>统一权重：图像与视频共享同一套 resampler 参数，仅需轻量 SFT 即可把图像 OCR/知识能力迁移到视频，无需额外视频 OCR 数据。</li>
</ul>
</li>
<li><p>数据层：文档图像统一学习范式</p>
<ul>
<li>动态腐蚀：对文档图像中的文本区域按“低/中/高”三级概率随机遮盖或加噪声，自动生成三种训练信号：<br>– 低腐蚀→文本仍可见→优化精准 OCR；<br>– 中腐蚀→字符模糊→迫使模型融合视觉+上下文；<br>– 高腐蚀→文本全遮→只能依赖版面与知识推理。</li>
<li>端到端重建：直接用原文本作为标签，不再经过外部 PDF 解析器，避免错位与工程清洗成本。</li>
<li>单目标统一：所有样本统一为“图像→文本重建”任务，同一 batch 内混合三种腐蚀程度，实现 OCR 与知识推理的权重共享与相互增强。</li>
</ul>
</li>
<li><p>训练层：混合强化学习（Hybrid RL）</p>
<ul>
<li>双模式初始化：SFT 阶段即注入 Long-CoT 数据，让模型同时具备短答（<think> 省略）与长推理（显式思维链）两种输出格式。</li>
<li>混合 rollout：RL 阶段每批样本 50% 触发长模式、50% 短模式，随机交替；使用 GRPO 优化，去掉 KL/熵正则以提升稳定性。</li>
<li>奖励组合：<br>– 简单答案→规则验证（准确率 98%）；<br>– 复杂开放答案→RLPR 概率奖励；<br>– 全程叠加格式奖励、重复惩罚、偏好模型分数（仅对最终答案段打分，避开长链 OOD 问题）。</li>
<li>相互增强：实验表明，短模式收敛速度提升，长模式峰值性能不降，整体仅需 33.3% 长样本即可达到纯长模式最佳效果，训练 token 节省 30%。</li>
</ul>
</li>
<li><p>三阶段训练流程（实现细节）</p>
<ul>
<li>预训练：<ol>
<li>只训 2D-Resampler，图像-文本对齐；</li>
<li>解冻视觉编码器，加入 OCR/文档/视频字幕；</li>
<li>全参数端到端，混合文本、图文交错、视频数据，WSD 学习率。</li>
</ol>
</li>
<li>SFT：<ol>
<li>通用指令跟随 + 10% 纯文本防遗忘；</li>
<li>引入 Long-CoT 与 3D-Resampler，升级视频高帧率/长时理解。</li>
</ol>
</li>
<li>RL &amp; RLAIF-V：<br>– 混合 rollout 优化短/长模式；<br>– 用 claim-level 验证构造偏好对，再做 DPO，进一步压制图像/视频幻觉。</li>
</ul>
</li>
</ol>
<p>通过“3D-Resampler 压缩 → 文档图像统一学习 → 混合 RL”三位一体，MiniCPM-V 4.5 在 8 B 参数下实现：</p>
<ul>
<li>视频推理 GPU 内存 46.7%、时间 8.7% 于 Qwen2.5-VL-7B；</li>
<li>OpenCompass 平均 77.0，超过 GPT-4o-latest 与 72 B 级开源模型；</li>
<li>同时保持领先 OCR、文档解析、幻觉抑制与多图指令跟随能力。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从“综合性能、专项能力、推理效率、消融验证”四个层面展开系统实验，全部在公开基准上完成，关键结果如下：</p>
<ol>
<li><p>主实验：与 SOTA 对比</p>
<ul>
<li>基准：OpenCompass（8 项平均）、MMMU / MathVista / AI2D / MathVerse / LogicVista / EMMA（STEM）、OCRBench / ChartQA / TextVQA / DocVQA / OmniDocBench（OCR+文档）、HallusionBench / ObjHalBench / MMHal-Bench（幻觉）、Mantis / MMT-Bench / RealWorldQA / MM-IFEval（多图+指令）、Video-MME / LVBench / MLVU / LongVideoBench / MotionBench / FavorBench（视频）。</li>
<li>对手：GPT-4o-latest、Qwen2.5-VL-72B、InternVL3-8B、GLM-4.1V-9B-thinking 等。</li>
<li>结果：MiniCPM-V 4.5（8 B）在 OpenCompass 取得 77.0 分，超过 GPT-4o-latest（75.4）与 Qwen2.5-VL-72B（76.1）；Video-MME 73.5 分，位列 30 B 以下模型第一；OCRBench 89.0，领先所有对比模型；幻觉基准 ObjHal-CHAIRs 降至 9.3%。</li>
</ul>
</li>
<li><p>推理效率实测</p>
<ul>
<li>硬件：8×A100-80 G，统一 batch-size 与精度。</li>
<li>指标：在 OpenCompass 全栈评测上，MiniCPM-V 4.5 耗时 7.5 h，仅为 GLM-4.1V-thinking（17.5 h）的 42.9%；在 Video-MME 上，推理时间 0.26 h（vs Qwen2.5-VL-7B 3.00 h），GPU 内存 28 G（vs 60 G），均优于表中所有对照。</li>
</ul>
</li>
<li><p>消融实验</p>
<ul>
<li><p>混合 RL 策略<br>– 对比：仅短模式 / 仅长模式 / 混合模式。<br>– 结果：混合模式用 3.1 B token 达到 77.1 分，优于纯长模式（4.4 B token，77.0），训练代价降低 29.5%。</p>
</li>
<li><p>奖励信号<br>– 对比：纯规则奖励(VR) vs 规则+RLPR 概率奖励(VR+PR)。<br>– 结果：VR+PR 在 700 步内把 OpenCompass 从 76.0 提升到 76.7，同时响应长度与熵更稳定。</p>
</li>
<li><p>统一文档学习<br>– 对比：外部 PDF 解析器 vs 动态腐蚀统一重建。<br>– 结果：统一学习在 MMMU（51.4 vs 49.0）、AI2D（76.5 vs 74.9）、OCRBench（617 vs 576）全部提升，证明去解析器同时增强 OCR 与知识推理。</p>
</li>
<li><p>3D-Resampler<br>– 对比：2D-Resampler（64 token/帧） vs 3D-Resampler（21.3 token/帧）。<br>– 结果：Video-MME 得分从 71.5→72.5（w/ sub），token 减少 2/3，验证时空联合压缩“性能↑成本↓”。</p>
</li>
</ul>
</li>
<li><p>扩展验证</p>
<ul>
<li>RLAIF-V 幻觉抑制：在 ObjHalBench 与 MMHal-Bench 上，引入 claim-level 偏好学习后，幻觉率分别再降 1.8% 与 3.4%。</li>
<li>高帧率/长视频：在 MotionBench（59.7）（细粒度动作）与 FavorBench（56.0）上均取得 30 B 以下最高分，证明 3D-Resampler 对 10 fps、1080 帧输入仍有效。</li>
</ul>
</li>
</ol>
<p>综上，实验覆盖 25+ 公开基准、4 类消融与真实效率测试，量化验证了“3D-Resampler + 统一文档学习 + 混合 RL”在性能、速度、内存、鲁棒性上的综合收益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“架构-压缩、数据-知识、训练-推理、评测-应用”四条线，每条给出可验证的开放问题与可行思路：</p>
<p>1. 架构-压缩</p>
<ul>
<li><p><strong>极致时空压缩</strong><br>当前 3D-Resampler 仍显式保留“帧包”概念，能否用 <strong>跨帧可学习哈希</strong> 或 <strong>连续时间 Transformer</strong>（如 Neural ODE + Attention）把 1080 帧直接压至 <64 token？  
验证指标：Video-MME vs token 数曲线，观察压缩率>128× 时精度是否出现陡降。</p>
</li>
<li><p><strong>动态分辨率/帧率感知</strong><br>现有 package size 与帧率在推理期需手工调整，可引入 <strong>RL-based 控制器</strong>，根据输入运动熵或文本问答难度，自动选择时空采样密度，实现“按需计算”。</p>
</li>
<li><p><strong>多模态共享压缩器</strong><br>3D-Resampler 仅统一图像-视频；若把音频波形也视为 1D 时序，可探索 <strong>统一时空-频谱压缩器</strong>，验证在 Audio-Visual-MME 等新基准上是否一致降本增效。</p>
</li>
</ul>
<p>2. 数据-知识</p>
<ul>
<li><p><strong>腐蚀策略自适应</strong><br>当前腐蚀级别为随机超参，可训练 <strong>小型元网络</strong> 根据文档版面复杂度或 OCR 置信度，实时输出腐蚀强度，使模型始终在“可识别边缘”学习，提升数据效率。</p>
</li>
<li><p><strong>跨语言文档</strong><br>论文实验以中英为主；阿拉伯语、印地语等从右到左或连体脚本对 OCR 与版面理解挑战更大。构建多语言文档腐蚀数据集，验证统一范式是否仍优于外部解析器。</p>
</li>
<li><p><strong>知识更新机制</strong><br>文档数据通常为静态 PDF，如何在不重训的情况下注入 <strong>最新论文/法规</strong>？可探索把 3D-Resampler 作为 <strong>知识缓存</strong>，结合检索-增强-生成（RAG）实现热插拔文档记忆。</p>
</li>
</ul>
<p>3. 训练-推理</p>
<ul>
<li><p><strong>混合 RL 的理论解释</strong><br>实验发现短-长模式相互提升，但缺乏度量。可引入 <strong>互信息 I(short;long)</strong> 或 <strong>梯度冲突分析</strong>，量化两种 rollout 在参数空间的共享/正交程度，指导最优混合比例。</p>
</li>
<li><p><strong>连续思维 Budget</strong><br>目前用 prompt 硬切换短/长；可训练 <strong>连续思维 token 预算</strong>，让模型在 <think> 段自行决定何时终止，实现“亚线性”推理成本：<br>Budget(ε) = min tokens s.t. 置信度 &gt;1−ε。</p>
</li>
<li><p><strong>端侧量化与推测解码</strong><br>3D-Resampler 输出已高度压缩，若配合 <strong>INT4 视觉编码器 + 推测解码</strong>（把 128 visual token 作为“草稿”），有望把手机端视频推理延迟压到 &lt;500 ms。</p>
</li>
</ul>
<p>4. 评测-应用</p>
<ul>
<li><p><strong>长视频时序定位</strong><br>Video-MME 只测全局问答；新 benchmark 可要求模型输出 <strong>“开始-结束秒+事件描述”</strong> 的密集字幕，验证 3D-Resampler 在细粒度时序对齐上的极限。</p>
</li>
<li><p><strong>多文档跨页推理</strong><br>现有 DocVQA 多为单页。构建 <strong>100+ 页 PDF 集合</strong>，问题需跨章节、跨图表聚合答案，测试统一文档学习是否真正捕获“版面-知识”长依赖。</p>
</li>
<li><p><strong>可信幻觉评测</strong><br>当前幻觉基准以静态图像为主；可扩展为 <strong>视频对话长链</strong>，每轮回答都依赖前几轮视觉信息，测量 <strong>时序一致性幻觉率</strong>，推动 RLAIF-V 向“多轮记忆”升级。</p>
</li>
</ul>
<p>5. 综合方向</p>
<ul>
<li><p><strong>自监督预训练目标</strong><br>把“动态腐蚀-重建”从文档推广到任意网页截图、PPT、Excel，构建 <strong>大规模视觉-语言掩码建模</strong>（VLMIM），检验是否可比 CLIP-style 对比学习获得更鲁棒的跨模态表示。</p>
</li>
<li><p><strong>绿色 AI 指标</strong><br>引入 “性能-碳排” 联合指标（FLOPs × 碳强度），系统比较不同压缩率下的 <strong>每克 CO₂ 对应分数</strong>，推动社区在 SOTA 竞赛中同时报告能效。</p>
</li>
</ul>
<p>以上任一点都可在 MiniCPM-V 4.5 的代码与权重基础上快速迭代，形成新的开源增量。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>MiniCPM-V 4.5 以“高效+强性能”为核心目标，针对 MLLM 训练与推理的三大瓶颈提出系统性改进，并通过统一训练流程在 8 B 规模上取得与 70 B 级模型相当甚至更优的结果。主要内容可概括为 <strong>“一个模型、三大创新、四类实验”</strong>：</p>
<p>1. 一个模型</p>
<ul>
<li><strong>8 B 参数多模态大模型</strong>，支持<br>– 高分辨率图像 / 极端长宽比<br>– 高帧率、长视频（≤1080 帧，10 fps）<br>– 短/长可控推理（<think> 模式切换）<br>– 鲁棒 OCR &amp; 复杂版面文档解析</li>
</ul>
<p>2. 三大创新</p>
<p>维度</p>
<p>关键问题</p>
<p>解决方案</p>
<p>效果</p>
<p><strong>架构</strong></p>
<p>视觉 token 过多，视频尤甚</p>
<p>统一 3D-Resampler<br>时空联合压缩</p>
<p>6 秒 2-fps 视频 128 token（12–24× 压缩）<br>Image &amp; Video 共享权重，轻量 SFT 即可迁移</p>
<p><strong>数据</strong></p>
<p>外部 PDF 解析易错、工程量大</p>
<p>统一学习范式<br>动态腐蚀-重建</p>
<p>去解析器，端到端训练<br>OCR 与知识推理相互增强</p>
<p><strong>训练</strong></p>
<p>长推理冗长、短推理能力不足</p>
<p>混合 RL<br>短/长 rollout 随机交替</p>
<p>33.3% 长样本即达纯长模式峰值<br>推理时间 ↓ 57%，训练 token ↓ 30%</p>
<p>3. 四类实验</p>
<ol>
<li><p><strong>综合对比</strong>（25+ 基准）</p>
<ul>
<li>OpenCompass 77.0 &gt; GPT-4o-latest (75.4) &amp; Qwen2.5-VL-72B (76.1)</li>
<li>Video-MME 73.5（30 B 以下 SOTA）</li>
<li>OCRBench 89.0，幻觉基准全面领先</li>
</ul>
</li>
<li><p><strong>效率实测</strong>（8×A100）</p>
<ul>
<li>OpenCompass 评测耗时仅 GLM-4.1V-thinking 的 42.9%</li>
<li>Video-MME 推理时间 ↓ 10×，GPU 内存 ↓ 53%</li>
</ul>
</li>
<li><p><strong>消融实验</strong></p>
<ul>
<li>混合 RL &gt; 纯长/纯短，token 节省 29.5%</li>
<li>规则+概率奖励 &gt; 纯规则，OpenCompass +1.2</li>
<li>统一文档学习 &gt; 外部解析器，OCRBench +41</li>
<li>3D-Resampler vs 2D：同等精度，token/帧 ↓ 66%</li>
</ul>
</li>
<li><p><strong>扩展验证</strong></p>
<ul>
<li>RLAIF-V 进一步降低幻觉 1.8–3.4%</li>
<li>高帧率 MotionBench、长视频 FavorBench 均列 30 B 以下第一</li>
</ul>
</li>
</ol>
<p>4. 结论</p>
<p>MiniCPM-V 4.5 通过 <strong>“3D-Resampler 压缩 + 文档图像统一学习 + 混合 RL”</strong> 三位一体，在 8 B 参数规模实现 <strong>性能与效率双突破</strong>，为构建可扩展、可落地的高效 MLLM 提供了完整技术路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MiniCPM-V%204.5%3A%20Cooking%20Efficient%20MLLMs%20via%20Architecture%2C%20Data%2C%20and%20Training%20Recipe，链接是  https://arxiv.org/pdf/2509.18154  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.18154  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tianyu Yu,Zefan Wang,Chongyi Wang,Fuwei Huang,Wenshuo Ma,Zhihui He,Tianchi Cai,Weize Chen,Yuxiang Huang,Yuanqian Zhao,Bokai Xu,Junbo Cui,Yingjing Xu,Liqing Ruan,Luoyuan Zhang,Hanyu Liu,Jingkun Tang,Hongyuan Liu,Qining Guo,Wenhao Hu,Bingxiang He,Jie Zhou,Jie Cai,Ji Qi,Zonghao Guo,Chi Chen,Guoyang Zeng,Yuxuan Li,Ganqu Cui,Ning Ding,Xu Han,Yuan Yao,Zhiyuan Liu,Maosong Sun</p>
<p><strong>Categories</strong>: cs.LG,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18154.pdf">https://arxiv.org/pdf/2509.18154.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18154">https://arxiv.org/abs/2509.18154</a></p>
<p><strong>Arxiv ID</strong>: 2509.18154</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18154">https://papers.cool/arxiv/2509.18154</a></p>
<p><strong>Published</strong>: 2025-09-16T19:41:48Z</p>
<p><strong>Updated</strong>: 2025-09-16T19:41:48.000Z</p>
<hr>
<h3 id="5-MAPO-Mixed-Advantage-Policy-Optimization"><a href="#5-MAPO-Mixed-Advantage-Policy-Optimization" class="headerlink" title="5. MAPO: Mixed Advantage Policy Optimization"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.18849">MAPO: Mixed Advantage Policy Optimization</a></h3><p>Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近在基础模型强化学习方面的进展，例如群体相对策略优化（GRPO），显著提高了基础模型在推理任务上的性能。值得注意的是，优越性函数作为GRPO中排名轨迹重要性的核心机制。然而，现有探索遇到了优越性反转和优越性镜像问题，这妨碍了在不同查询样本之间合理的优越性分配。在这项工作中，我们提出了一种简单但有效的GRPO策略，混合优越性策略优化（MAPO）。我们揭示了轨迹具有不同的确定性，并为具有高确定性轨迹的样本提出了优越性百分比偏差。此外，我们针对具有不同轨迹确定性的样本动态重新加权优越性函数，从而自适应地配置优越性函数，以考虑样本特定特征。与相关的最新方法进行比较，以及对不同优越性变种的消融研究，验证了我们方法的有效性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对 Group Relative Policy Optimization（GRPO）在强化学习后训练中的优势函数设计缺陷，提出并解决以下核心问题：</p>
<ul>
<li><p><strong>优势反转（Advantage Reversion）</strong><br>高确定性样本（过易或过难）因标准差 σ 极小，导致 z-score 优势 $ \hat{A}_i=\frac{r_i-\mu}{\sigma} $ 被过度放大，反而比低确定性样本获得更剧烈的梯度更新，违背“低确定性样本更需要修正”的直觉。</p>
</li>
<li><p><strong>优势镜像（Advantage Mirror）</strong><br>对称分布的奖励（如 [0,0.1,0.1,0.1] 与 [0.9,1,1,1]）产生镜像化的优势值，使语义上截然不同的样本被同等对待，忽略奖励绝对水平。</p>
</li>
<li><p><strong>固定优势函数无法适应样本差异</strong><br>现有 GRPO 变体全程使用单一优势形式，未考虑不同 query 在采样组内的“轨迹确定性”差异，即样本成功率的二项分布方差 $ p(1-p) $ 不同，导致优势分配失真。</p>
</li>
</ul>
<p>为此，论文提出 <strong>Mixed Advantage Policy Optimization（MAPO）</strong>，通过</p>
<ol>
<li><strong>Advantage Percent Deviation（APD）</strong> 用相对均值偏差 $ \hat{A}_{\text{APD}}=\frac{r_i-\mu}{\mu} $ 替代 z-score，消除 σ 过小带来的数值畸变；</li>
<li><strong>Trajectory Certainty Reweight（TCR）</strong> 以经验成功率 $ p=N/G $ 计算确定性权重 $ \lambda(p)=1-4p(1-p) $，动态混合两种优势：  <script type="math/tex; mode=display">\\hat{A}^\*\_i = \[1-\\lambda(p)\]\\frac{r\_i-\\mu}{\\sigma} + \\lambda(p)\\frac{r\_i-\\mu}{\\mu}</script>实现“低确定性时依赖方差敏感项，高确定性时依赖均值相对项”的自适应优势估计，从而修正梯度方向，提升数学与情感推理任务的域内/域外精度。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 章“Related Works”将现有与 GRPO 相关的研究划分为三大流派，并指出其各自局限。以下按类别归纳主要相关文献，并给出核心思路与不足。</p>
<p>1. 思维轨迹多样性（Think Trajectory Diversity）</p>
<p>代表工作</p>
<p>关键做法</p>
<p>主要局限</p>
<p>NoisyRollout (Liu et al., 2025b)</p>
<p>对图文对施加可退火噪声，增广输入空间</p>
<p>需额外调“初始噪声强度”超参；训练成本↑</p>
<p>VP (Li et al., 2025)</p>
<p>三种视觉扰动：干扰图拼接、保持显著性的 mixup、随机旋转</p>
<p>引入“扰动类型”多项超参；通用性差</p>
<p>Share-GRPO (Yao et al., 2025)</p>
<p>对同一问题做文本式重写，扩大问题空间</p>
<p>需预定义“问题变种数”；计算量↑</p>
<p>StepGRPO (Zhang et al., 2025)</p>
<p>强制显式输出关键中间步骤</p>
<p>依赖外部“关键步数”阈值</p>
<p>GRPO-CARE (Chen et al., 2025b)</p>
<p>引入外部“思维奖励模型”评估整段思考质量</p>
<p>需额外模型，推理开销↑</p>
<p><strong>共同弱点</strong>：均需专门的数据增广策略或外部评价模型，带来额外计算/超参。</p>
<p>2. 奖励函数改进（Reward Formulation Refinement）</p>
<p>代表工作</p>
<p>关键做法</p>
<p>主要局限</p>
<p>Visual-RFT (Liu et al., 2025f)</p>
<p>针对检测任务设计 IoU 可验证奖励</p>
<p>仅适用于检测，跨任务泛化弱</p>
<p>GRPO-λ (Dai et al., 2025a)</p>
<p>在奖励中引入“长度惩罚”</p>
<p>需调“Top-λ 分数”超参</p>
<p>GRPO-LEAD (Zhang &amp; Zuo, 2025)</p>
<p>对过长回答施加负奖励</p>
<p>需调“优势再缩放因子”</p>
<p><strong>共同弱点</strong>：任务相关，需要逐任务精细调节奖励权重，难以在多种场景稳健迁移。</p>
<p>3. 优势估计重设计（Advantage Estimation Redesign）</p>
<p>代表工作</p>
<p>关键做法</p>
<p>主要局限</p>
<p>Dr. GRPO (Liu et al., 2025e)</p>
<p>直接去掉 σ，用 $ \hat{A}=r_i-\mu $</p>
<p>无法解决“优势镜像”问题；对绝对奖励水平不敏感</p>
<p>GPG (Chu et al., 2025b)</p>
<p>仅对“有效样本”(μ&gt;0.6) 缩放优势</p>
<p>需预设“有效阈值”；仍用统一优势形式</p>
<p>SEED-GRPO (Chen et al., 2025a)</p>
<p>用语义熵估计不确定性，对高熵样本保守更新</p>
<p>需额外计算熵，且仍全局共用同一优势函数</p>
<p>KRPO (Wang et al., 2025a)</p>
<p>用轻量卡尔曼滤波在线估计 μ,σ</p>
<p>引入滤波超参，未处理样本级差异</p>
<p><strong>共同弱点</strong>：所有方法在整个训练过程中<strong>对所有样本使用同一种优势公式</strong>，无法应对“不同 query 的轨迹确定性差异”，因而依旧遭遇优势反转与镜像问题。</p>
<p>与本文定位的差异</p>
<p>MAPO 首次提出“轨迹确定性”概念，利用样本级成功率 $ p $ 动态混合 <strong>方差敏感项</strong> 与 <strong>均值相对项</strong>，无需额外模型、无新增超参，即可同时缓解优势反转与镜像缺陷，在数学与情感推理任务上取得一致提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将问题拆解为“高确定性样本优势计算”与“不同确定性样本的自适应加权”两步，提出 <strong>Mixed Advantage Policy Optimization（MAPO）</strong>，核心操作如下：</p>
<p>1. 轨迹确定性建模</p>
<p>把每条轨迹视为 Bernoulli 变量，成功计数  </p>
<script type="math/tex; mode=display">N=\\sum\_{i=1}^{G} \\mathbb{1}{r\_i=1}, \\quad p=\\frac{N}{G}</script><p>用 <strong>方差</strong> $ p(1-p) $ 量化“确定性”：</p>
<ul>
<li>$ p\to 0 $ 或 $ 1 $ → 方差最小 → <strong>高确定性</strong></li>
<li>$ p\to 0.5 $ → 方差最大 → <strong>低确定性</strong></li>
</ul>
<p>2. Advantage Percent Deviation（APD）</p>
<p>针对高确定性时 σ 极小导致的“优势反转/镜像”，用 <strong>相对均值偏差</strong> 替代 z-score：  </p>
<script type="math/tex; mode=display">\\hat{A}\_{\\text{APD}}^{i}= \\frac{r\_i-\\mu}{\\mu}</script><ul>
<li>消除 σ 过小带来的数值爆炸</li>
<li>保留奖励绝对水平信息，避免镜像对称误判</li>
</ul>
<p>3. Trajectory Certainty Reweight（TCR）</p>
<p>利用确定性程度动态混合两种优势：<br>定义 <strong>确定性权重</strong>  </p>
<script type="math/tex; mode=display">\\lambda(p)=1-4p(1-p)\\in\[0,1\]</script><p>样本级混合优势  </p>
<script type="math/tex; mode=display">\\hat{A}_i^\*= \\underbrace{\[1-\\lambda(p)\]}_{\\text{信赖方差项}} \\frac{r\_i-\\mu}{\\sigma} + \\underbrace{\\lambda(p)}\_{\\text{信赖均值项}} \\frac{r\_i-\\mu}{\\mu}</script><ul>
<li>低确定性（$ p\approx 0.5 $）→ $ \lambda\approx 0 $，主要使用 <strong>方差敏感</strong> 的原始优势</li>
<li>高确定性（$ p\approx 0 $ 或 $ 1 $）→ $ \lambda\approx 1 $，切换为 <strong>均值相对</strong> 的 APD，抑制异常梯度</li>
</ul>
<p>4. 梯度级效果（理论推导）</p>
<p>忽略裁剪与 KL，可得 MAPO 与 GRPO 梯度比  </p>
<script type="math/tex; mode=display">\\varrho(p)\\triangleq \\frac{\\nabla\_\\theta J\_{\\text{MAPO}}}{\\nabla\_\\theta J\_{\\text{GRPO}}} = (1-\\lambda(p))+\\lambda(p)\\sqrt{\\frac{1-p}{p}}</script><p>性质：</p>
<ul>
<li>当 $ p&lt;\frac12 $（更难样本）→ $ \varrho(p)&gt;1 $，梯度被<strong>放大</strong></li>
<li>当 $ p&gt;\frac12 $（更易样本）→ $ \varrho(p)&lt;1 $，梯度被<strong>缩小</strong></li>
</ul>
<p>从而<strong>自适应地给困难样本更大更新、简单样本更小更新</strong>，进一步修正优化方向。</p>
<p>5. 算法流程（伪代码）</p>
<p>输入：旧策略 $ \pi_{\text{old}} $，当前策略 $ \pi_\theta $，rollout 数 G</p>
<ol>
<li>采样 G 条轨迹，得奖励 $ {r_i} $</li>
<li>计算 $ \mu,\sigma,N,p $</li>
<li>计算 $ \hat{A}<em>i $ 与 $ \hat{A}</em>{\text{APD}}^{i} $</li>
<li>计算 $ \lambda(p) $ 并合成 $ \hat{A}_i^* $</li>
<li>代入 GRPO 目标函数更新 $ \theta $</li>
</ol>
<p>结果</p>
<ul>
<li><strong>无需额外模型架构</strong> → 高迁移性</li>
<li><strong>无需新增超参数</strong> → 易部署</li>
<li>在 Geo3K、EmoSet 等 7 个数据集上，<strong>域内/域外精度一致优于</strong> GRPO、DAPO 等最新基线，验证其解决“优势反转”与“优势镜像”问题的有效性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>数学推理</strong> 与 <strong>视觉情绪推理</strong> 两大场景下，共使用 <strong>2 个训练集 + 7 个测试集</strong> 进行系统实验，涵盖 <strong>域内（In-Domain, AS）</strong> 与 <strong>域外（Out-of-Domain, AT）</strong> 评估，并辅以 <strong>消融实验、训练曲线可视化、梯度分析</strong> 等诊断性实验。具体列表如下：</p>
<p>1. 数据集与场景</p>
<p>场景</p>
<p>训练集</p>
<p>测试集（域外）</p>
<p>样本量</p>
<p>任务描述</p>
<p>数学推理</p>
<p>Geo3K</p>
<p>MathVista / MathVision / MathVerse</p>
<p>2.1K→5K/6K/4K</p>
<p>几何图形+文本推理</p>
<p>情绪推理</p>
<p>EmoSet</p>
<p>WEBEmo / Emotion6</p>
<p>2.1K→1.2M/2K</p>
<p>视觉情绪分类</p>
<p>2. 主实验：与 SOTA 对比</p>
<p>配置</p>
<p>指标</p>
<p>Vanilla</p>
<p>GRPO</p>
<p>DAPO</p>
<p>MAPO(ours)</p>
<p>Geo3K G=12</p>
<p>AS / AT / A̅</p>
<p>37.4 / 43.9 / 40.7</p>
<p>51.9 / 47.8 / 49.9</p>
<p>52.9 / 48.2 / 50.6</p>
<p><strong>54.4 / 48.1 / 51.3</strong></p>
<p>EmoSet G=12</p>
<p>AS / AT / A̅</p>
<p>53.7 / 49.5 / 51.6</p>
<p>77.2 / 55.2 / 66.2</p>
<p>76.1 / 55.6 / 65.8</p>
<p><strong>77.9 / 55.7 / 66.8</strong></p>
<p>Geo3K G=8</p>
<p>AS / AT / A̅</p>
<p>— / — / —</p>
<p>50.9 / 47.5 / 49.2</p>
<p>50.4 / 47.3 / 48.9</p>
<p><strong>54.2 / 47.4 / 50.8</strong></p>
<p>EmoSet G=8</p>
<p>AS / AT / A̅</p>
<p>— / — / —</p>
<p>76.4 / 55.1 / 65.7</p>
<p>68.4 / 52.9 / 60.7</p>
<p><strong>77.5 / 55.7 / 66.6</strong></p>
<blockquote>
<p>表格数值为准确率（%）；<strong>加粗</strong>为最佳。MAPO 在所有 8 组设定中均取得最高 <strong>平均精度 A̅</strong>。</p>
</blockquote>
<p>3. 消融实验（Ablation）</p>
<p>模块</p>
<p>Geo3K AS</p>
<p>EmoSet AS</p>
<p>结论</p>
<p>仅 APD</p>
<p>50.9</p>
<p>77.2</p>
<p>单独替换优势公式提升有限，甚至偶尔下降</p>
<p>仅 TCR（随机 λ）</p>
<p>53.4</p>
<p>76.9</p>
<p>随机加权无法稳定提升</p>
<p>APD + TCR（完整 MAPO）</p>
<p><strong>54.4</strong></p>
<p><strong>77.9</strong></p>
<p>二者协同，显著优于任一单独模块</p>
<p>4. 训练过程可视化</p>
<ul>
<li><p><strong>训练/测试准确率曲线</strong>（图 5）<br>– MAPO 在 Geo3K（G=12 &amp; 8）与 EmoSet 上收敛更快、终点更高。<br>– DAPO 在 EmoSet 出现动态采样失败，训练提前终止。</p>
</li>
<li><p><strong>Case 可视化</strong>（图 7-8）<br>– 高确定性样本：GRPO/DAPO 将原本 Vanilla 能答对的题目（amusement / joy）误判，优势扭曲导致性能下降；MAPO 保持正确。<br>– 域外样本：MAPO 在 MathVision、WEBEmo 上错误更少，泛化性更佳。</p>
</li>
</ul>
<p>5. 梯度比验证（理论→实验）</p>
<ul>
<li>按公式 $ \varrho(p)=\nabla_\text{MAPO}/\nabla_\text{GRPO} $ 计算真实训练批次<br>– 当批次成功率 $ p<0.5 $ 时，实测 $ \\varrho>1 $，与理论放大区间一致；<br>– 当 $ p&gt;0.5 $ 时，$ \varrho&lt;1 $，梯度被抑制。<br>验证 MAPO 确实“难样本放大、易样本缩小”。</li>
</ul>
<p>6. 超参数敏感性（附表）</p>
<ul>
<li>在 {G=4,8,12,16} 与 {KL β=1e-2,5e-3,5e-2} 组合下，MAPO 平均提升稳定在 <strong>+1.8%~+2.6%</strong>，无需额外调 λ 即可保持优势。</li>
</ul>
<p>结论性摘要</p>
<p>实验覆盖 <strong>不同 rollout 数、不同任务、不同域</strong>；MAPO 在所有设定下 <strong>一致优于 GRPO、DAPO</strong>，且 <strong>无需新增可调研超参</strong>，验证了该方法对“优势反转”与“优势镜像”问题的有效修复与可扩展性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望基于论文自身局限与实验观察，可作为后续工作的直接切入点；所有数学关系均按论文符号给出。</p>
<p>1. 极端确定性场景下的策略退化</p>
<ul>
<li><strong>现象</strong>：当模型能力较弱或任务极难时，G 条轨迹几乎全部失败（$p\to 0$）或全部成功（$p\to 1$），此时 $ \lambda(p)\to 1 $，MAPO 退化为纯 APD，丧失方差信息。</li>
<li><strong>可探索</strong>：<br>– 引入 <strong>Dirichlet 平滑</strong> 或 <strong>Bayesian 置信区间</strong>，对稀有事件估计 $p$ 进行修正；<br>– 采用 <strong>动态 rollout 调度</strong>，在 $p$ 极端时自动增加 G 以获得非零方差，再自适应切换优势形式。</li>
</ul>
<p>2. 更精细的奖励分解与混合</p>
<ul>
<li><strong>现状</strong>：实验仅用格式奖励与 0/1 准确率线性组合 $r=(1-\beta)r_{\text{format}}+\beta r_{\text{acc}}$。</li>
<li><strong>可探索</strong>：<br>– 将轨迹拆分为 <strong>步级奖励</strong> ${r_{i,t}}$，利用 APD-TCR 思想对每步优势再加权，实现 <strong>细粒度信用分配</strong>；<br>– 对 <strong>连续回归型奖励</strong>（IoU、BLEU、RM-score）推导 σ→0 时的渐近修正项，推广 APD 到连续域。</li>
</ul>
<p>3. 确定性权重的自动学习</p>
<ul>
<li><strong>现状</strong>：$ \lambda(p)=1-4p(1-p) $ 为手工构造。</li>
<li><strong>可探索</strong>：<br>– 把 $ \lambda $ 视为 <strong>样本相关可学习参数</strong>，用元梯度（meta-gradient）或超网络根据 query 特征输出 $ \lambda_\phi(q) $；<br>– 目标函数增加 <strong>元目标</strong>（如验证集准确率），使 $ \lambda $ 在训练过程中自动收敛到最优混合系数。</li>
</ul>
<p>4. 跨任务迁移与大规模验证</p>
<ul>
<li><strong>现状</strong>：实验最大 7B 参数，单任务 2.1K 训练样本。</li>
<li><strong>可探索</strong>：<br>– 在 <strong>百亿级模型</strong>（Qwen-72B、InternVL-76B）上验证 MAPO 的 scalability；<br>– 构建 <strong>多任务混合训练</strong>（数学+科学+代码），检验确定性权重是否 task-agnostic，或需引入 task-id 条件 $ \lambda(p,\text{task}) $。</li>
</ul>
<p>5. 与其它优势改进的协同</p>
<ul>
<li><strong>可组合方向</strong>：<br>– <strong>KRPO</strong> 的卡尔曼滤波估计 $ (\mu,\sigma) $ 替代样本矩，再接入 MAPO 混合，进一步降低方差噪声；<br>– <strong>SEED-GRPO</strong> 的语义熵 $ H_{\text{sem}} $ 作为第二确定性指标，与 $p$ 做 <strong>双因子混合</strong>：</li>
</ul>
<p>[ \lambda(p,H_{\text{sem}})=1-4p(1-p)\cdot \mathrm{sigmoid}(-\alpha H_{\text{sem}}) ]</p>
<p>实现“熵高且 $p$ 极端”时更保守更新。</p>
<p>6. 理论深化</p>
<ul>
<li><strong>收敛界</strong>：在 $p$ 未知非平稳场景下，给出 MAPO 优势估计的 <strong>偏差-方差权衡上界</strong>，证明 $ \hat{A}^*_i $ 的最小最大最优速率。</li>
<li><strong>与难度采样的统一</strong>：将 $ \varrho(p) $ 嵌入 <strong>课程强化学习</strong> 框架，证明按 $ \varrho(p) $ 调整采样概率后，期望遗憾界优于均匀采样。</li>
</ul>
<p>7. 系统级优化</p>
<ul>
<li><strong>实现</strong>：当前需两次遍历计算 $ \mu,\sigma $ 与 $ \mu^{-1} $，可探索 <strong>单次在线算法</strong> 同步更新两矩，减少 20% 显存占用。</li>
<li><strong>硬件</strong>：在 <strong>NVIDIA Blackwell</strong> 上测试 FP8 低比特训练，验证 APD 的除法操作是否导致数值下溢，必要时引入 <strong>log-space 稳定除法</strong>。</li>
</ul>
<p>以上方向均可在 <strong>不引入额外人工超参</strong> 的前提下，继续提升 MAPO 的鲁棒性、规模与理论保证。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>MAPO: Mixed Advantage Policy Optimization</strong> 提出一种 <strong>无需新增超参、即插即用</strong> 的优势函数修正策略，解决 GRPO 在强化学习后训练中出现的 <strong>优势反转</strong> 与 <strong>优势镜像</strong> 两大缺陷，从而提升 Foundation Model 的推理性能。</p>
<p>1. 问题洞察</p>
<ul>
<li><strong>优势反转</strong>：高确定性样本（过易/过难）因 σ→0 导致 z-score 优势被过度放大，反而比低确定性样本获得更大梯度。</li>
<li><strong>优势镜像</strong>：对称奖励分布产生镜像优势值，使语义不同的样本被同等对待。</li>
<li><strong>根因</strong>：GRPO 全程使用固定优势 $ \hat{A}_i=\frac{r_i-\mu}{\sigma} $，忽视 <strong>样本级轨迹确定性差异</strong>。</li>
</ul>
<p>2. 方法概览</p>
<p>模块</p>
<p>公式</p>
<p>功能</p>
<p>轨迹确定性</p>
<p>$ p=\frac{1}{G}\sum_{i=1}^G \mathbb{1}{r_i=1} $</p>
<p>用成功率估计 Bernoulli 方差</p>
<p>APD</p>
<p>$ \hat{A}_{\text{APD}}^{i}=\frac{r_i-\mu}{\mu} $</p>
<p>相对均值偏差，消除 σ→0 爆炸</p>
<p>TCR 权重</p>
<p>$ \lambda(p)=1-4p(1-p) $</p>
<p>确定性越高，越信赖 APD</p>
<p>混合优势</p>
<p>$ \hat{A}^*_i=[1{-}\lambda(p)]\frac{r_i-\mu}{\sigma}+\lambda(p)\frac{r_i-\mu}{\mu} $</p>
<p>样本级自适应优势</p>
<p>3. 理论性质</p>
<ul>
<li><strong>梯度比</strong> $ \varrho(p)=\frac{\nabla_\text{MAPO}}{\nabla_\text{GRPO}} $ 满足：<ul>
<li>$ p<0.5 $（难样本）→ $ \\varrho>1 $，梯度放大</li>
<li>$ p&gt;0.5 $（易样本）→ $ \varrho&lt;1 $，梯度缩小<br>实现 <strong>难度感知的自动课程</strong>。</li>
</ul>
</li>
</ul>
<p>4. 实验结果</p>
<ul>
<li><strong>数据集</strong>：Geo3K / EmoSet → 5 个域外测试集</li>
<li><strong>模型</strong>：Qwen2.5-VL-7B</li>
<li><strong>指标</strong>：域内 AS、域外 AT、平均 A̅</li>
<li><strong>结论</strong>：<ul>
<li>G=12：MAPO 在数学任务 A̅=51.3（+1.4 vs GRPO），情绪任务 A̅=66.8（+0.6）</li>
<li>G=8 仍保持领先，验证 <strong>rollout 节省场景</strong> 同样有效</li>
<li>消融：仅 APD 或随机 λ 均不及 <strong>APD+TCR</strong> 组合</li>
</ul>
</li>
</ul>
<p>5. 主要贡献</p>
<ol>
<li>揭示并形式化 <strong>优势反转 &amp; 镜像</strong> 问题，归因于固定 z-score 忽视轨迹确定性。</li>
<li>提出 <strong>MAPO</strong>：<ul>
<li><strong>APD</strong> 替换高确定性样本优势，消除 σ 畸变</li>
<li><strong>TCR</strong> 动态混合两种优势，无需人工超参</li>
</ul>
</li>
<li>理论证明 MAPO 隐式 <strong>放大难样本、缩小易样本</strong> 梯度，实现自适应课程。</li>
<li>在 7 个推理数据集上 <strong>一致超越</strong> GRPO、DAPO 等最新基线，展示即插即用潜力。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MAPO%3A%20Mixed%20Advantage%20Policy%20Optimization，链接是  https://arxiv.org/pdf/2509.18849  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.18849  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wenke Huang,Quan Zhang,Yiyang Fang,Jian Liang,Xuankun Rong,Huanjin Yao,Guancheng Wan,Ke Liang,Wenwen He,Mingjun Li,Leszek Rutkowski,Mang Ye,Bo Du,Dacheng Tao</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18849.pdf">https://arxiv.org/pdf/2509.18849.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18849">https://arxiv.org/abs/2509.18849</a></p>
<p><strong>Arxiv ID</strong>: 2509.18849</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18849">https://papers.cool/arxiv/2509.18849</a></p>
<p><strong>Published</strong>: 2025-09-23T09:37:16Z</p>
<p><strong>Updated</strong>: 2025-09-23T09:37:16.000Z</p>
<hr>
<h3 id="6-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation"><a href="#6-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation" class="headerlink" title="6. Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.18824">Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation</a></h3><p>Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>统一的多模态模型最近因其在共同理解和生成多样内容方面的显著能力而受到广泛关注。然而，随着上下文整合越来越多的交错多模态标记，扩散去噪和自回归解码的迭代过程会带来显著的计算开销。为了解决这个问题，我们提出了Hyper-Bagel，这是一个统一的加速框架，旨在同时加快多模态理解和生成任务。我们的方法采用分治策略，利用推测性解码进行下一个标记的预测，并采用多阶段蒸馏过程进行扩散去噪。该框架提供了显著的性能提升，在多模态理解中实现了超过2倍的加速。对于生成任务，我们得到的无损6-NFE模型在文本到图像生成中实现了16.67倍的加速，在图像编辑中实现了22倍的加速，同时保持了原始模型的高质量输出。我们进一步开发了一个高效的1-NFE模型，使近实时的交互编辑和生成成为可能。通过将先进的对抗蒸馏与人类反馈学习相结合，该模型实现了极致的性价比和响应速度，让复杂的多模态交互变得无缝且瞬时。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>统一多模态模型在同时执行理解与生成任务时面临的巨大计算开销</strong>问题。随着输入上下文包含越来越多交错的多模态 token，两种迭代过程——扩散去噪（diffusion denoising）与自回归解码（autoregressive decoding）——成为瓶颈。为此，作者提出 <strong>Hyper-Bagel</strong>，一个统一的加速框架，通过“分而治之”的策略：</p>
<ul>
<li>对<strong>理解任务</strong>采用<strong>投机解码</strong>（speculative decoding），将内存瓶颈转化为计算瓶颈，实现 <strong>>2× 加速</strong>；</li>
<li>对<strong>生成任务</strong>设计<strong>多阶段蒸馏</strong>（multi-stage distillation），在保持生成质量的前提下，把采样步数压缩到 <strong>6-NFE</strong>（文本到图像 <strong>16.67×</strong> 加速，图像编辑 <strong>22×</strong> 加速），并进一步推出 <strong>1-NFE</strong> 极端加速版本，实现近实时交互。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“理解”与“生成”两条技术路线分别与以下代表性工作相关：</p>
<p>理解加速方向</p>
<ul>
<li><strong>投机解码系列</strong><ul>
<li>$L_{\text{total}} = \mathrm{KL}(p_{\text{target}}|p_{\text{draft}}) + \lambda \cdot [-\log p_{\arg\max(p_{\text{target}})}^{\text{draft}}]$</li>
<li>EAGLE-1/2/3、Medusa 等通过轻量草稿模型并行预测多 token，再由目标模型批量验证，实现无损加速。</li>
</ul>
</li>
</ul>
<p>生成加速方向</p>
<ul>
<li><strong>扩散蒸馏/少步生成</strong><ul>
<li><strong>Progressive Distillation</strong>：Salimans &amp; Ho 首次将 1000-DDPM 压缩到 8 步。</li>
<li><strong>DMD / DMD2</strong>：以 SDE 一致性采样做分布匹配，1-NFE 生成但易过平滑。</li>
<li><strong>Hyper-SD</strong>：提出“轨迹分段一致性蒸馏（TSCD）”+ 对抗+分数蒸馏三阶段，6-NFE 无损。</li>
<li><strong>SDXL-Lightning、DMDX、ADP</strong>：进一步引入多尺度判别器、rectified-flow 对抗目标，改善结构完整性。</li>
</ul>
</li>
<li><strong>CFG 单步注入</strong><ul>
<li>FLUX.1/Kontext 将 text-scale、image-scale 与 timestep 并列编码，实现单前向控制。</li>
</ul>
</li>
</ul>
<p>统一多模态模型</p>
<ul>
<li><strong>BAGEL、Transfusion、Janus、Show-o、Emu3、Chameleon</strong> 等把文本、图像 token 交错建模，同时支持理解与生成，为本文提供了基线目标模型与评测基准。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“分而治之”策略，将<strong>理解</strong>与<strong>生成</strong>两条计算路径解耦，分别设计专用加速模块，再统一训练与部署。具体手段如下：</p>
<p>1. 理解加速：投机解码（Speculative Decoding）</p>
<ul>
<li><strong>问题</strong>：多模态序列中文本、ViT、latent token 嵌入空间差异大，轻量草稿模型容量不足，导致接受率骤降。</li>
<li><strong>解法</strong>：<ol>
<li><strong>中间层桥接</strong><ul>
<li>引入可学习的 meta-query  <script type="math/tex; mode=display">f\_q\\in\\mathbb{R}^{Q\\times D}</script>对所有目标层特征做<strong>跨注意力聚合</strong>，再经独立 Transformer 解码，得到草稿模型输入<br><script type="math/tex">F\_{\\text{in}}\\in\\mathbb{R}^{S\\times D}</script>。</li>
</ul>
</li>
<li><strong>零初始化 + 残差</strong><ul>
<li>将 CA、Dec、fc1、fc2 最后一层权重初始化为 0，并加入来自目标模型低/中/高层特征的 skip，保证训练初期梯度与目标模型一致。</li>
</ul>
</li>
<li><strong>混合监督</strong><ul>
<li>损失函数  <script type="math/tex; mode=display">L\_{\\text{total}} = \\mathrm{KL}(p\_{\\text{target}}|p\_{\\text{draft}}) + \\lambda\\cdot\\mathrm{CE}(\\arg\\max p\_{\\text{target}}, p\_{\\text{draft}})</script>缓解草稿模型对复杂分布的过拟合。</li>
</ul>
</li>
</ol>
</li>
<li><strong>效果</strong>：SGLang 单 A100 链式解码 TPS 从 98.3 → 212.4，<strong>2.16× 无损加速</strong>。</li>
</ul>
<p>2. 生成加速：多阶段扩散蒸馏（Diffusion Distillation）</p>
<p>目标：在<strong>不损失 CFG 控制力、结构完整性、图像保真度</strong>的前提下，把采样步数压缩到 6-NFE（极致 1-NFE）。</p>
<p>Stage-1 CFG 蒸馏</p>
<ul>
<li>将 text-scale、image-scale 与 timestep 并列编码，一次性注入 DiT 各层，实现<strong>单前向 CFG 控制</strong>。</li>
<li>训练时随机采样 scale∈[1,5]（text）、[1,2.5]（image），并强制 timestep-shift ≥3.0 以保证高保真。</li>
</ul>
<p>Stage-2 轨迹分段一致性蒸馏（TSCD）</p>
<ul>
<li>直接 3-segment 对抗蒸馏，<strong>丢弃 MSE</strong>，采用<strong>多尺度多头判别器</strong>，专注提升整体构图（结构完整性）。</li>
</ul>
<p>Stage-3 DMDO（Distribution Matching Distillation via ODE）</p>
<ul>
<li>改 SDE 一致性采样为<strong>欧拉 ODE 轨迹</strong>，fake 模型目标改为预测  <script type="math/tex; mode=display">v = x\_T - x\_0</script>避免过度平滑；交替更新 fake 模型与少步生成器，实现<strong>高保真 6-NFE</strong>。</li>
</ul>
<p>Stage-4 ADP（Adversarial Diffusion Pre-training）</p>
<ul>
<li>以 6-NFE 模型为“新教师”，用 rectified-flow 对抗在<strong>潜空间+像素空间双判别器</strong>对齐结构，得到<strong>1-NFE 骨架</strong>。</li>
</ul>
<p>Stage-5 ReFL（Reward Feedback Learning）</p>
<ul>
<li>采用 VLM-based 奖励模型 HPSv3  <script type="math/tex; mode=display">r(x\_0; \\text{prompt})</script>对 1-NFE 输出进行<strong>人类偏好微调</strong>，补偿细节损失。</li>
</ul>
<p>3. 部署解耦</p>
<ul>
<li>理解分支参数冻结，<strong>仅替换生成分支权重</strong>；prefill 阶段回退原始模型，保证理解性能<strong>零损失</strong>。</li>
</ul>
<p>结果速览</p>
<p>任务</p>
<p>基准步数</p>
<p>加速步数</p>
<p>加速比</p>
<p>性能变化</p>
<p>多模态理解</p>
<p>—</p>
<p>—</p>
<p><strong>>2×</strong></p>
<p>无损</p>
<p>文本→图像</p>
<p>100 NFE</p>
<p>6 NFE</p>
<p><strong>16.67×</strong></p>
<p>GenEval 0.8647→0.8640（↑）</p>
<p>图像编辑</p>
<p>132 NFE</p>
<p>6 NFE</p>
<p><strong>22×</strong></p>
<p>GEdit-Bench 6.602→6.612（↑）</p>
<p>极端实时</p>
<p>6 NFE</p>
<p>1 NFE</p>
<p><strong>6×</strong></p>
<p>仍优于多数 SOTA 统一模型</p>
<p>通过上述“理解-生成”双路径协同优化，Hyper-Bagel 在<strong>保持原模型质量的同时</strong>，实现了统一多模态模型的<strong>数量级推理加速</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>理解加速</strong>、<strong>生成加速</strong>两条主线展开实验，分别给出定量指标与视觉对比，并验证部署解耦不会损害理解能力。主要实验如下：</p>
<p>1. 投机解码（理解加速）</p>
<ul>
<li><strong>指标</strong>：平均接受长度 τ、接受率 10-α（10 步外推链式解码）</li>
<li><strong>结果</strong>（表 1）<ul>
<li>Vanilla EAGLE-3：τ=3.6184，10-α=0.7327</li>
<li>Hyper-Bagel：τ=3.7709，10-α=0.7452（+0.15τ，+1.7% 接受率）</li>
<li>消融：去掉 zero-init 下降最显著（τ-0.94），去掉 CE 损失次之，二者皆去反而略好于仅去 zero-init，证明 zero-init 是关键。</li>
</ul>
</li>
</ul>
<p>2. 扩散蒸馏（生成加速）</p>
<p>2.1 定量评测</p>
<p>基准</p>
<p>指标</p>
<p>100/132-NFE 基线</p>
<p>6-NFE</p>
<p>1-NFE</p>
<p><strong>GenEval</strong>（文本→图像）</p>
<p>Overall ↑</p>
<p>0.8640</p>
<p><strong>0.8647</strong>（无损）</p>
<p>0.7962（仍≈Janus-Pro-7B 0.80）</p>
<p><strong>GEdit-Bench-EN</strong></p>
<p>Overall ↑</p>
<p>6.602</p>
<p><strong>6.612</strong>（无损）</p>
<p>5.975</p>
<p><strong>GEdit-Bench-CN</strong></p>
<p>Overall ↑</p>
<p>6.610</p>
<p><strong>6.671</strong>（无损）</p>
<p>5.966</p>
<ul>
<li><strong>6-NFE 在两大基准上全面持平或超越原模型</strong>，实现 16.67×/22× 推理加速。</li>
<li><strong>1-NFE 虽略有下降，但仍优于 OmniGen 等最新统一模型</strong>，满足实时交互需求。</li>
</ul>
<p>2.2 视觉对比（图 5、图 6）</p>
<ul>
<li><strong>6-NFE 与 100/132-NFE 基线肉眼难辨</strong>：复杂细节（“619” 数字、毛发纹理、闪电氛围）一致保留。</li>
<li><strong>1-NFE 在语义层面仍准确</strong>，细节（眼镜、数字、表情）略有损失，但整体质量与编辑可用性高，支持<strong>近实时交互</strong>。</li>
</ul>
<p>3. 部署解耦验证</p>
<ul>
<li>冻结理解分支，仅替换生成分支权重；prefill 阶段回退原始模型。</li>
<li>实验确认：<strong>理解指标（各类 VQA、MT-bench）与原始 BAGEL 完全一致</strong>，扩散蒸馏对文本生成与思考能力无影响。</li>
</ul>
<p>4. 速度实测</p>
<ul>
<li><strong>理解</strong>：SGLang 单 A100 链式解码 TPS 98.3 → 212.4，<strong>2.16× 加速</strong>。</li>
<li><strong>生成</strong>：<ul>
<li>文本→图像：100 NFE→6 NFE，<strong>16.67× 加速</strong></li>
<li>图像编辑：132 NFE→6 NFE，<strong>22× 加速</strong></li>
<li>极端实时：6 NFE→1 NFE，再<strong>6× 加速</strong>，端到延迟进入秒级/亚秒级。</li>
</ul>
</li>
</ul>
<p>综上，论文通过<strong>消融实验、基准评分、人工视觉对比与系统测速</strong>，系统验证了 Hyper-Bagel 在<strong>理解-生成双任务上同时实现数量级加速且性能无损</strong>的核心主张。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“方法改进”“场景拓展”“评测体系”与“理论分析”四大类：</p>
<p>方法改进</p>
<ol>
<li><p><strong>自适应投机步长</strong><br>当前使用固定 10 步外推。可引入轻量强化学习器，根据历史接受率动态调整外推长度，使 TPS 进一步逼近理论上限。</p>
</li>
<li><p><strong>跨模态草稿模型</strong><br>现有草稿模型仍以文本 Transformer 为主。探索“小型 DiT+Transformer 混合”结构，让草稿网络同时理解 latent 与 token 空间，减少多模态 embedding 错位。</p>
</li>
<li><p><strong>扩散蒸馏的误差累积建模</strong><br>6-NFE→1-NFE 的 fidelity 下降呈非线性。可显式对“步数-误差”轨迹建模，引入可学习的步间残差补偿项，实现任意 NFE 的“即插即用”式精度恢复。</p>
</li>
<li><p><strong>多任务共享判别器</strong><br>目前生成与编辑各自训练判别器。研究统一判别器同时区分“文本-图像对齐、结构一致性、编辑保真度”，减少参数量并提升跨任务泛化。</p>
</li>
</ol>
<p>场景拓展</p>
<ol>
<li><p><strong>视频与 3D 生成</strong><br>将 DMDO 的 ODE 轨迹匹配思想扩展到 temporal / voxel latent，验证 6-NFE 能否直接迁移到短视频或 NeRF 生成，实现实时多模态连续编辑。</p>
</li>
<li><p><strong>边缘端侧部署</strong><br>1-NFE 模型已可在 A100 实现亚秒级。进一步结合 INT4/INT8 量化、CPU-NPU 异构调度，验证在手机端“拍照即编辑”的端到端延迟与功耗。</p>
</li>
<li><p><strong>多语言多文化 CFG</strong><br>当前 CFG-scale 仅针对英文与通用视觉概念。探索语言特异性、文化符号的 scale 编码，避免非英语提示下 1-NFE 出现文化细节丢失。</p>
</li>
</ol>
<p>评测体系</p>
<ol>
<li><p><strong>实时交互基准</strong><br>现有 GenEval、GEdit-Bench 只测质量不测延迟。建议构建“Latency-Quality Pareto”基准：固定 1 s 内完成生成，统计不同硬件下的 FID/CLIP/人类偏好，推动“可交付实时模型”研究。</p>
</li>
<li><p><strong>长序列多轮编辑</strong><br>SEED-Data-Edit 仅到 5 轮。构建 10-20 轮连续编辑数据集，评估 1-NFE 模型误差累积是否导致语义漂移，并研究是否需要“中途刷新”策略。</p>
</li>
</ol>
<p>理论分析</p>
<ol>
<li><p><strong>投机解码接受率上界</strong><br>对多模态 embedding 差异进行形式化建模，给出接受率与草稿-目标模型 KL 散度、模态间 Wasserstein 距离的定量关系，指导未来草稿容量设计。</p>
</li>
<li><p><strong>ODE 轨迹匹配收敛界</strong><br>DMDO 目前为经验设计。可建立“教师-学生”ODE 解的误差传播 PDE，推导 NFE 与最终图像分布 KL 的下界，回答“最少需要几步即可达到 ε-精度”。</p>
</li>
<li><p><strong>统一多模态损失景观</strong><br>理解分支（自回归）与生成分支（扩散）联合训练时，损失函数存在不同尺度与曲率。研究梯度冲突对蒸馏后模型泛化性的影响，提出梯度平衡策略。</p>
</li>
</ol>
<p>综上，Hyper-Bagel 在“速度-质量”平衡上已验证可行，但<strong>自适应机制、跨模态统一架构、实时评测基准与理论极限</strong>仍有广阔探索空间。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：统一多模态模型在交错图文上下文下，自回归理解与扩散生成均面临高昂迭代开销。</li>
<li><strong>方法</strong>：提出 Hyper-Bagel，采用“分而治之”策略——<br>– <strong>理解</strong>：改进投机解码，用跨注意力中间层+零初始化+混合损失，将 TPS 从 98.3 提至 212.4，<strong>>2× 无损加速</strong>。<br>– <strong>生成</strong>：三阶段扩散蒸馏（CFG 单步注入→多尺度对抗 TSCD→ODE 轨迹匹配 DMDO），再经 ADP+ReFL 推出 1-NFE 版本；6-NFE 在 GenEval/GEdit-Bench 上<strong>持平或超越</strong> 100/132-NFE 基线，实现文本到图像<strong>16.67×</strong>、图像编辑<strong>22×</strong> 加速，1-NFE 仍具竞争力并支持<strong>近实时交互</strong>。</li>
<li><strong>实验</strong>：消融与系统测速表明，理解指标零损失，生成质量视觉无损；部署时仅替换生成分支，不影响文本能力。</li>
<li><strong>结论</strong>：Hyper-Bagel 首次在统一模型上同时实现理解与生成的数量级加速，为实际部署提供无损且极高效的解决方案。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Hyper-Bagel%3A%20A%20Unified%20Acceleration%20Framework%20for%20Multimodal%20Understanding%20and%20Generation，链接是  https://arxiv.org/pdf/2509.18824  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.18824  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yanzuo Lu,Xin Xia,Manlin Zhang,Huafeng Kuang,Jianbin Zheng,Yuxi Ren,Xuefeng Xiao</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18824.pdf">https://arxiv.org/pdf/2509.18824.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18824">https://arxiv.org/abs/2509.18824</a></p>
<p><strong>Arxiv ID</strong>: 2509.18824</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18824">https://papers.cool/arxiv/2509.18824</a></p>
<p><strong>Published</strong>: 2025-09-23T09:12:46Z</p>
<p><strong>Updated</strong>: 2025-09-23T09:12:46.000Z</p>
<hr>
<h3 id="7-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction"><a href="#7-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction" class="headerlink" title="7. VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19297">VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</a></h3><p>Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment’s reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: <a target="_blank" rel="noopener" href="https://lhmd.top/volsplat">https://lhmd.top/volsplat</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>前馈3D高斯铺展（3DGS）作为一种新颖视图合成的高效解决方案应运而生。现有的方法主要依赖于像素对齐的高斯预测范式，在这种范式中，每个2D像素都被映射到一个3D高斯分布。我们重新审视这种广泛采用的公式，并识别出几个固有的局限性：它使重建的3D模型在很大程度上依赖于输入视图的数量，导致视图偏倚的密度分布，并且引入了对齐错误，特别是在源视图包含遮挡或低纹理时。为了解决这些挑战，我们引入了VolSplat，这是一种新的多视图前馈范式，使用体素对齐的高斯代替像素对齐。通过直接从预测的3D体素网格预测高斯，它克服了像素对齐对易出错的2D特征匹配的依赖，确保了强大的多视图一致性。此外，它根据3D场景复杂性实现对高斯密度的自适应控制，从而产生更真实的高斯点云、改善几何一致性和加强新视图渲染质量。 在包括RealEstate10K和ScanNet等广泛使用的基准测试中的实验表明，VolSplat在获得最先进性能的同时，生成了更加可信和视图一致的高斯重建。除了卓越的结果，我们的方法还建立了一个更具可扩展性的前馈3D重建框架，具备更密集和更强大的表示，为更广泛的社区的进一步研究铺平了道路。视频结果、代码和训练模型可在我们的项目页面上获取：<a target="_blank" rel="noopener" href="https://lhmd.top/volsplat。">https://lhmd.top/volsplat。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对现有前馈式 3D Gaussian Splatting（3DGS）方法普遍采用的“像素对齐”范式，指出并试图解决以下核心问题：</p>
<ol>
<li><strong>视图数量敏感</strong>：重建质量随输入视图数量剧烈波动，稀疏视角下性能骤降。</li>
<li><strong>视图偏向密度</strong>：Gaussian 密度被 2D 像素网格锁定，无法根据场景几何复杂度自适应调整，导致简单区域过密、复杂区域过疏。</li>
<li><strong>对齐误差累积</strong>：依赖 2D 特征匹配进行跨视图融合，在遮挡、低纹理或深度歧义区域产生误匹配，进而出现“漂浮物”、边缘伪影与几何不一致。</li>
</ol>
<p>为此，作者提出 VolSplat，将预测空间从 2D 像素提升到 3D 体素，实现：</p>
<ul>
<li>在共享的规范体素坐标系中统一融合多视图特征，消除像素级匹配误差；</li>
<li>通过体素占用情况自适应控制 Gaussian 密度，使表示复杂度与场景真实几何相匹配；</li>
<li>利用 3D 稀疏卷积网络进一步精炼特征，提升几何一致性与渲染质量。</li>
</ul>
<p>简言之，论文旨在用“体素对齐”替代“像素对齐”，解决前馈 3DGS 因 2D 先验耦合带来的鲁棒性、可扩展性与几何忠实度不足的问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第二节 <strong>Related Work</strong> 中系统梳理了四条主线，并指出 VolSplat 与它们的区别。按主题归纳如下：</p>
<ol>
<li><p>新视角合成（Novel View Synthesis, NVS）</p>
<ul>
<li>传统基于几何或基于图像的渲染<ul>
<li>Debevec et al. 混合几何-图像方法</li>
<li>Deep View Morphing</li>
<li>Light Field Rendering</li>
</ul>
</li>
<li>神经隐式表示<ul>
<li>NeRF 及其变种（Mip-NeRF、Zip-NeRF 等）</li>
</ul>
</li>
<li>显式点/高斯表示<ul>
<li>3D Gaussian Splatting (3DGS) 及 LightGaussian、FlexGS 等加速/压缩版本</li>
</ul>
</li>
</ul>
</li>
<li><p>3D 体素化表示</p>
<ul>
<li>早期稠密网格：Octree、VoxelNet、OctNet</li>
<li>近期“体素即脚手架”思路<ul>
<li>Plenoxels、K-Planes（体素+辐射场）</li>
<li>Scaffold-GS、Octree-GS（体素组织 3DGS）<br>VolSplat 不同之处：体素不是中间组织工具，而是<strong>直接预测高斯的坐标系</strong>，实现端到端体素对齐。</li>
</ul>
</li>
</ul>
</li>
<li><p>前馈式 3D Gaussian Splatting（单次前向推理）</p>
<ul>
<li>典型像素对齐方法<ul>
<li>pixelSplat（两视图+epipolar Transformer）</li>
<li>MVSplat（代价体融合）</li>
<li>DepthSplat（引入单目深度先验）</li>
<li>FreeSplat、TranSplat、Zpressor 等</li>
</ul>
</li>
<li>后处理/增强像素对齐<ul>
<li>GGN（图网络修剪冗余 Gaussian）</li>
<li>FreeSplat++（多视图 Gaussian 融合）<br>VolSplat 区别：<strong>完全抛弃逐像素预测</strong>，改为在 3D 体素网格内一次性回归 Gaussian，解决密度固定与对齐误差问题。</li>
</ul>
</li>
</ul>
</li>
<li><p>体素特征在前馈 3DGS 中的初步探索</p>
<ul>
<li>EVolSplat（自动驾驶场景，需显式点云输入）</li>
<li>AnySplat（无约束视角，仍用体素辅助但核心仍是像素对齐）<br>VolSplat 推广到<strong>通用场景</strong>，无需任何中间点云，实现纯粹体素对齐的端到端框架。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“像素对齐”彻底替换为“体素对齐”，并围绕该思想设计了一个端到端可训练框架 VolSplat。具体实现分为四个关键步骤，每一步都针对前述痛点给出对应解法：</p>
<ol>
<li><p>3D 特征容器构建——先升维再融合，避免 2D 匹配误差</p>
<ul>
<li>多视图 Transformer 提取 2D 特征后，用平面扫描代价体估计每帧深度。</li>
<li>利用预测深度将各视图特征<strong>一次性反投影到世界坐标系</strong>，得到无序点云。</li>
<li>按给定体素尺寸 <code>vs</code> 进行体素化：同一 voxel 内的特征做平均池化，得到<strong>统一且规范</strong>的稀疏 3D 特征网格 <code>V</code>。<br>→ 信息先在 3D 空间“对齐落地”，再开始后续推理，从根本上削弱相机标定误差、遮挡或弱纹理造成的匹配漂移。</li>
</ul>
</li>
<li><p>稀疏 3D 解码器——在 3D 域里精炼，补全局部-全局上下文</p>
<ul>
<li>采用 3D U-Net 形式的稀疏卷积网络 <code>R</code>，对 <code>V</code> 预测残差 <code>R(V)</code>。</li>
<li>残差加回原特征：<code>V′ = V + R(V)</code>，既保留反投影的粗结构，又注入多尺度几何-外观一致性约束。<br>→ 消除“漂浮物”，增强跨视图语义连贯性。</li>
</ul>
</li>
<li><p>Voxel-aligned Gaussian 预测——密度自适应，解开 2D 分辨率枷锁</p>
<ul>
<li>每个被占据的体素 <code>v</code> 直接回归 18 维参数：<ul>
<li>中心偏移 <code>μ̄_j</code>（相对于 voxel 中心）</li>
<li>协方差 <code>Σ_j</code></li>
<li>不透明度 <code>ᾱ_j</code></li>
<li>球谐颜色 <code>c_j</code></li>
</ul>
</li>
<li>通过 <code>σ(·)</code> 与 voxel 尺寸 <code>r</code> 把偏移映射到世界坐标：  <script type="math/tex; mode=display">μ\_j = r · σ(μ̄\_j) + \\text{Center}\_j</script></li>
<li>体素是否被占用由稀疏卷积掩码自动决定，<strong>复杂区域占得多、简单区域占得少</strong>，实现场景复杂度驱动的自适应密度。<br>→ 不再受“一像素一高斯”固定数量限制，显著减少冗余，同时保证细节足够。</li>
</ul>
</li>
<li><p>端到端可微渲染与训练——用 2D 监督反向优化 3D 参数</p>
<ul>
<li>将预测的全部 3D Gaussians 在任意新视角下可微光栅化，得到渲染图 <code>I_render</code>。</li>
<li>与真值 <code>I_gt</code> 计算混合损失：  <script type="math/tex; mode=display">L = \\sum\_{m=1}^{M} \\left\[ L\_{\\text{MSE}}(I^{(m)}_{\\text{render}}, I^{(m)}_{\\text{gt}}) + λ L\_{\\text{LPIPS}}(I^{(m)}_{\\text{render}}, I^{(m)}_{\\text{gt}}) \\right\]</script>其中 <code>λ = 0.05</code>。</li>
<li>整个流程（2D 特征→代价体→深度→反投影→体素化→3D U-Net→Gaussian 参数→渲染）<strong>一条链可导</strong>，可用 AdamW 端到端训练。</li>
</ul>
</li>
</ol>
<p>通过以上四步，VolSplat 把“先做 2D 匹配再升维”改成“先升维到 3D 再做推理”，从而</p>
<ul>
<li>消除像素对齐带来的离散化与匹配误差；</li>
<li>实现 Gaussian 密度的场景自适应；</li>
<li>在 ScanNet、RealEstate10K、ACID 等基准上取得 SoTA 的 PSNR/SSIM/LPIPS，同时 Gaussian 数量更紧凑、几何一致性更好。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在第四节 <strong>Experiments</strong> 中从四个维度系统验证 VolSplat 的有效性，全部基于公开基准，并给出定量指标、视觉对比与消融分析。</p>
<ol>
<li><p>主实验：与现有像素对齐方法对比<br>数据集</p>
<ul>
<li>RealEstate10K（67 477 训练 / 7 289 测试帧）</li>
<li>ScanNet（约 100 场景训练 / 8 场景测试）<br>输入设置</li>
<li>训练与测试均用 6 张 256×256 视图<br>对比方法</li>
<li>纯像素对齐：pixelSplat、MVSplat、TranSplat、DepthSplat、FreeSplat</li>
<li>像素对齐后处理：GGN、FreeSplat++<br>评价指标<br>PSNR↑、SSIM↑、LPIPS↓、平均每视角 Gaussian 数（PGS）<br>结果（表 I、II）</li>
<li>RealEstate10K：PSNR 31.30（+3.83 over DepthSplat），SSIM 0.941，LPIPS 0.075</li>
<li>ScanNet：PSNR 28.41（+0.96 over FreeSplat），SSIM 0.906，LPIPS 0.127</li>
<li>在两项基准均取得 SoTA，且 Gaussian 数量与最佳对手相当或更少。</li>
</ul>
</li>
<li><p>跨域零样本泛化<br>实验协议</p>
<ul>
<li>仅在 RealEstate10K 训练，直接在室外 ACID 测试集推理，不做任何微调。<br>结果（表 III）</li>
<li>VolSplat PSNR 32.65，比次佳 DepthSplat 高 4.28 dB；LPIPS 降至 0.092。<br>结论</li>
<li>体素对齐对场景分布变化更鲁棒，像素对齐方法出现显著性能下降。</li>
</ul>
</li>
<li><p>Gaussian 密度与视觉质量分析</p>
<ul>
<li>统计同一帧的 Gaussian 数量：VolSplat 随场景复杂度自动变化，简单墙面稀疏，复杂洗漱台边缘密集（图 5）。</li>
<li>可视化侧面对比：像素对齐方法在遮挡边界出现大量“漂浮”高斯，VolSplat 分布更贴合真实表面。</li>
</ul>
</li>
<li><p>消融实验（Ablation Study）<br>均在 RealEstate10K 完成，固定其余超参。</p>
<p>4.1 体素尺寸影响（表 IV）</p>
<ul>
<li>0.05 m：31.03 dB，65535 Gaussians</li>
<li>0.1 m（默认）：31.30 dB，65529 Gaussians</li>
<li>0.5 m：28.73 dB</li>
<li>1 m：21.22 dB<br>说明 0.1 m 在精度-内存间取得最佳平衡。</li>
</ul>
<p>4.2 稀疏 3D 解码器必要性（表 V）</p>
<ul>
<li>完整 3D U-Net：31.30 dB / 8.06 GB</li>
<li>换成普通稀疏 3D CNN：29.84 dB</li>
<li>去掉解码器：29.34 dB<br>证实多尺度残差精炼对几何一致性至关重要。</li>
</ul>
<p>4.3 输入视图稀疏度鲁棒性（文中图 4 说明）</p>
<ul>
<li>当视图从 6 张减至 2 张时，像素对齐方法出现明显边缘破碎与 floater；VolSplat 仍保持连贯轮廓。</li>
</ul>
</li>
</ol>
<p>综上，实验覆盖</p>
<ul>
<li>标准基准 SoTA 对比</li>
<li>零样本跨域泛化</li>
<li>Gaussian 密度自适应可视化</li>
<li>关键组件（体素尺寸、3D U-Net）消融</li>
</ul>
<p>结果一致表明：VolSplat 在渲染质量、几何一致性、模型紧凑性与域外泛化方面均优于现有像素对齐前馈 3DGS 方法。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“方法-结构”、“数据-场景”、“任务-应用”、“理论-分析”四条线，均直接对应 VolSplat 尚未充分展开或尚未触及的要点。</p>
<p>方法-结构</p>
<ol>
<li><p>动态场景与时间体素</p>
<ul>
<li>将静态 3D 体素扩展为 4D 时空网格（x-y-z-t），用稀疏 4D 卷积预测每帧高斯变形或新生-消亡掩码，实现单模型同时重建几何与运动。</li>
<li>可借鉴 Streaming/Online 3DGS 的“滑动窗口”思想，避免整段序列一次性加载。</li>
</ul>
</li>
<li><p>多尺度-层次体素</p>
<ul>
<li>目前只用单一 voxel size 0.1 m，可引入 Octree/Hash 结构，让网络自动选择细分深度，实现“粗-细”协同：大 voxel 负责低频外观，小 voxel 刻画高频细节。</li>
<li>与 LOD 渲染结合，做到近处高精度、远处低内存，适配 VR/AR 实时漫游。</li>
</ul>
</li>
<li><p>压缩与量化</p>
<ul>
<li>VolSplat 的 3D U-Net 特征通道数仍较高，可尝试：<br>– 对 voxel 特征做 8-bit 量化 + 熵编码；<br>– 用神经码本（VQ-VAE）把每 voxel 18 维高斯参数离散化，实现存储-带宽友好。</li>
</ul>
</li>
<li><p>与扩散生成式模型耦合</p>
<ul>
<li>将体素特征作为条件，训练扩散模型补全不可见区域；推理时先“生成”缺失 voxel，再解码高斯，实现单张输入→完整 3D 场景的前馈生成。</li>
</ul>
</li>
</ol>
<p>数据-场景</p>
<ol>
<li><p>无界/室外大场景</p>
<ul>
<li>RealEstate10K、ACID 仍属“室内+街区”级别。可引入 Waymo Open、NuScenes 等公里级数据，研究：<br>– 如何随相机前进动态扩展体素边界；<br>– 地平天空等远场用球壳或立方体环境贴图替代体素，避免空 voxel 爆炸。</li>
</ul>
</li>
<li><p>非刚性、可形变物体</p>
<ul>
<li>当前实验全是刚性场景。把体素与 SMPL 或 NeRF-D 类型的形变场联合优化，测试在动物、人物序列上的时序一致性。</li>
</ul>
</li>
<li><p>极端稀疏视角 &amp; 无相机参数</p>
<ul>
<li>仅给 1-2 张图且位姿未知：先用 Pose-Free 网络预测粗略相机，再送入 VolSplat；或引入 probabilistic voxel 表示，对位姿与体素联合采样，量化不确定性。</li>
</ul>
</li>
</ol>
<p>任务-应用</p>
<ol>
<li><p>语义-几何联合推理</p>
<ul>
<li>在 voxel 特征上并行接入 3D 分割头，输出每 voxel 的语义标签；渲染时即可得到实例/语义图，实现“一前向 pass 得 RGB+标签”，供机器人导航、AR 遮挡处理使用。</li>
</ul>
</li>
<li><p>实时 SLAM 闭环</p>
<ul>
<li>将 VolSplat 作为 Mapping 子模块：Tracking 线程实时估计相机位姿，Mapping 线程累积关键帧→体素→高斯；利用稀疏卷积仅更新新增 voxel，实现 10-30 Hz 在线重建。</li>
</ul>
</li>
<li><p>多模态输入</p>
<ul>
<li>除 RGB 外，同步输入稀疏 LiDAR 点云或毫米波雷达深度：直接把外部点云注入体素网格作为额外特征通道，看能否在夜晚、弱纹理场景下进一步提升精度。</li>
</ul>
</li>
</ol>
<p>理论-分析</p>
<ol>
<li><p>对齐误差下界与体素分辨率理论</p>
<ul>
<li>在已知相机噪声 σ_c、深度误差 σ_d 的条件下，推导“像素对齐 vs 体素对齐”的重建误差下界，给出最优 voxel size 选取公式，而非经验网格搜索。</li>
</ul>
</li>
<li><p>可解释性可视化</p>
<ul>
<li>对 3D U-Net 的 voxel 特征做 PCA 或 Grad-CAM 类可视化，观察哪些通道负责边缘、纹理、镜面反射等，验证“体素特征已自动学到几何-外观解耦”假设。</li>
</ul>
</li>
<li><p>收敛性与训练策略</p>
<ul>
<li>当前 150k 迭代是经验值。可分析“体素-高斯”双阶段监督（先深度、后渲染）是否比纯渲染损失收敛更快；或引入渐进式 voxel 增长（coarse-to-fine）减少训练时间。</li>
</ul>
</li>
</ol>
<p>一句话总结</p>
<p>VolSplat 把“像素→3D”升级为“3D→3D”，已验证在静态、刚性、多视图条件下的优势；未来可在<strong>4D 动态、无界大世界、极端稀疏输入、语义-生成融合</strong>等方向继续挖掘其可扩展性与理论极限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：现有前馈 3D Gaussian Splatting 普遍采用“像素对齐”——每像素预测一高斯，导致密度固定、跨视图匹配误差大，遮挡或弱纹理区域出现漂浮物与几何不一致。</li>
<li><strong>思路</strong>：把对齐空间从 2D 像素移到 3D 体素，先在世界坐标系构建统一 voxel 特征网格，再于网格内直接回归高斯参数。</li>
<li><strong>方法（VolSplat）</strong><ol>
<li>多视图提取 2D 特征 → 平面扫描代价体估计深度 → 按深度反投影并体素化，得到稀疏 3D 特征。</li>
<li>稀疏 3D U-Net 残差精炼，增强多尺度几何一致性。</li>
<li>每个被占体素端到端输出 18 维高斯参数（位置偏移、协方差、不透明度、球谐颜色），实现场景复杂度驱动的自适应密度。</li>
<li>可微渲染多视角 RGB，用 MSE+LPIPS 端到端训练。</li>
</ol>
</li>
<li><strong>实验</strong><ul>
<li>RealEstate10K &amp; ScanNet 上 PSNR、SSIM、LPIPS 全面优于 pixelSplat、MVSplat、DepthSplat 等像素对齐方法，Gaussian 数量更紧凑。</li>
<li>RealEstate10K→ACID 零样本跨域，PSNR 提升 4+ dB，展现强泛化。</li>
<li>消融：0.1 m 体素、3D U-Net 精炼为最佳配置；移除或替换解码器显著降分。</li>
</ul>
</li>
<li><strong>结论</strong>：VolSplat 用“体素对齐”取代“像素对齐”，在一次性前馈推理中实现自适应密度、鲁棒多视图融合与更高保真重建，为实时 3D 重建与渲染提供了更可扩展的新框架。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是VolSplat%3A%20Rethinking%20Feed-Forward%203D%20Gaussian%20Splatting%20with%20Voxel-Aligned%20Prediction，链接是  https://arxiv.org/pdf/2509.19297  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19297  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Weijie Wang,Yeqing Chen,Zeyu Zhang,Hengyu Liu,Haoxiao Wang,Zhiyuan Feng,Wenkang Qin,Zheng Zhu,Donny Y. Chen,Bohan Zhuang</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19297.pdf">https://arxiv.org/pdf/2509.19297.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19297">https://arxiv.org/abs/2509.19297</a></p>
<p><strong>Arxiv ID</strong>: 2509.19297</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19297">https://papers.cool/arxiv/2509.19297</a></p>
<p><strong>Published</strong>: 2025-09-23T17:59:02Z</p>
<p><strong>Updated</strong>: 2025-09-23T17:59:02.000Z</p>
<hr>
<h3 id="8-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation"><a href="#8-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation" class="headerlink" title="8. Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19296">Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation</a></h3><p>The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>生成虚拟环境的能力对于从游戏到物理人工智能领域（如机器人、自动驾驶和工业人工智能）等应用至关重要。目前基于学习的3D重建方法依赖于捕获的真实世界多视角数据，而这些数据并不总是 readily 可用。最近视频扩散模型的进展展现出了显著的想象能力，但其二维特性限制了其在需要机器人导航和与环境交互的模拟应用中的使用。在本文中，我们提出了一种自蒸馏框架，旨在将视频扩散模型中的隐式3D知识蒸馏为显式的3D高斯斑点（3DGS）表示，消除了对多视角训练数据的需求。具体而言，我们用3DGS解码器增强典型的RGB解码器，3DGS解码器由RGB解码器的输出进行监督。在这种方法中，3DGS解码器可以完全使用由视频扩散模型生成的合成数据进行训练。在推理时，我们的模型可以通过文本提示或单幅图像合成3D场景，以实现实时渲染。我们的框架进一步扩展到从单目输入视频生成动态3D场景。实验结果表明，我们的框架在静态和动态3D场景生成方面达到了最先进的性能。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决<strong>无需真实多视角数据即可生成高质量、可实时渲染的静态与动态三维场景</strong>这一核心问题。具体而言，其关注以下关键痛点：</p>
<ol>
<li><p><strong>真实多视角数据稀缺</strong><br>现有基于学习的三维重建方法依赖昂贵且难以规模化的真实多视角捕获，严重限制了训练数据的多样性与覆盖范围。</p>
</li>
<li><p><strong>视频扩散模型缺乏显式三维表示</strong><br>尽管视频扩散模型具备强大的想象与泛化能力，但其输出仅为二维帧序列，无法满足机器人、自动驾驶等物理 AI 领域对<strong>几何一致性、可交互、可导航的显式三维环境</strong>的需求。</p>
</li>
<li><p><strong>静态与动态场景的前馈三维生成空白</strong><br>以往前馈式方法要么局限于静态物体或场景，要么需要额外的逐场景优化，无法实现从单张图像或单段视频直接输出<strong>可渲染的三维高斯抛雪球（3DGS）或动态 4D 表示</strong>。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Lyra</strong>，通过<strong>自蒸馏框架</strong>将视频扩散模型隐含的 3D 先验蒸馏为显式 3DGS 解码器，从而：</p>
<ul>
<li>摆脱对真实多视角训练数据的依赖；</li>
<li>在潜空间内高效融合大视角覆盖的多轨迹信息；</li>
<li>实现单图→3D、单视频→4D 的前馈生成，支持实时渲染与物理仿真。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”与附录 E 中系统梳理了相关研究，可归纳为以下六大主线（按出现顺序整理，不含第一人称）：</p>
<p>1. 多视角图像生成（Multi-view Image Generation）</p>
<ul>
<li><strong>对象级</strong>：Zero-1-to-3、MVDream、SyncDreamer 等仅针对物体本身，忽略背景。</li>
<li><strong>场景级</strong>：CAT3D 先用多视角扩散模型生成图像，再优化重建为 NeRF/3DGS；后续动态扩展如 4DiM、CAT4D 引入时空一致性约束。</li>
<li><strong>关键差异</strong>：上述方法需后续优化阶段，而 Lyra 直接前馈输出显式 3DGS，无需逐场景拟合。</li>
</ul>
<p>2. 相机条件视频生成（Camera-conditioned Video Generation）</p>
<ul>
<li><strong>先驱</strong>：MotionCtrl 首次在预训练视频扩散模型中引入外参矩阵控制相机。</li>
<li><strong>改进</strong>：Plücker 坐标像素级条件（GEN3C、VD3D、Ac3D）、无训练控制（MotionMaster、Ex-4D）、大规模合成数据训练（SynCamMaster、ReCamMaster）。</li>
<li><strong>关键差异</strong>：这些工作仍停留在 2D 帧输出；Lyra 将其作为“教师”进行自蒸馏，得到可渲染的 3D 表示。</li>
</ul>
<p>3. 前馈式 3D 重建/生成（Feed-forward 3D Models）</p>
<p>方法</p>
<p>输入</p>
<p>表示</p>
<p>局限</p>
<p>LRM / Instant3D / GS-LRM</p>
<p>单图或稀疏图</p>
<p>3DGS</p>
<p>仅物体、训练数据域受限</p>
<p>Flash3D / AnySplat</p>
<p>单图</p>
<p>3DGS</p>
<p>场景级但需真实多视角训练</p>
<p>Bolt3D</p>
<p>单图</p>
<p>3DGS</p>
<p>需点云自编码器+真实数据</p>
<p>Wonderland</p>
<p>单图</p>
<p>3DGS</p>
<p>同样依赖真实多视角数据</p>
<p><strong>Lyra</strong></p>
<p>单图/视频</p>
<p>3DGS/4D</p>
<p>无需真实多视角，纯合成蒸馏</p>
<p>4. 动态 4D 场景生成（4D Generation from Monocular Video）</p>
<ul>
<li><strong>文本驱动</strong>：DreamGaussian4D、Align-your-Gaussians、TC4D、4D-fy 等使用 SDS 或扩散先验，需优化。</li>
<li><strong>视频驱动</strong>：Consistent4D、STAG4D、Vidu4D、SC4D 等从单视频重建动态 3DGS，但多为逐场景优化。</li>
<li><strong>前馈式</strong>：BTimer、Long-LRM 可前馈，但需真实多视角数据或像素级输入；Lyra 首次实现<strong>单视频输入、前馈输出动态 3DGS</strong>。</li>
</ul>
<p>5. 自蒸馏与潜空间 3D 解码（Self-distillation &amp; Latent 3D Decoder）</p>
<ul>
<li><strong>潜空间优势</strong>：Long-LRM 证明在压缩视频潜空间内可处理长序列，避免像素级 OOM；Lyra 借鉴其 Transformer-Mamba 混合块设计。</li>
<li><strong>自蒸馏思想</strong>：同期工作 VFusion3D、PixelSplat 亦采用“生成-再-重建”循环，但依赖真实数据或优化；Lyra 完全用<strong>合成视频监督</strong>训练 3DGS 解码器。</li>
</ul>
<p>6. 物理仿真与机器人数据引擎（Simulation &amp; Embodied AI）</p>
<ul>
<li><strong>场景生成→仿真</strong>：DimensionX、Infinicube、StreetCrafter 等生成开放式驾驶场景，但多为 2D 视频或需后处理。</li>
<li><strong>贡献</strong>：Lyra 输出可直接导出为 .usdz 并导入 NVIDIA Isaac，支持<strong>可导航、可交互、物理一致</strong>的闭环训练环境，填补“生成式 3D→机器人仿真”空白。</li>
</ul>
<p>小结</p>
<p>Lyra 与上述研究的核心区别在于：</p>
<ol>
<li><strong>训练数据</strong>：零真实多视角，仅依赖视频扩散模型合成的伪数据。</li>
<li><strong>推理路径</strong>：单图/单视频 → 前馈解码 → 显式 3DGS/4D，无需逐场景优化。</li>
<li><strong>表示能力</strong>：静态与动态场景统一，支持实时渲染与物理仿真。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>Lyra</strong>，通过“<strong>自蒸馏</strong>”范式把预训练、相机可控的视频扩散模型转化为显式 3D/4D 表示生成器。核心思路可概括为：<strong>用视频模型当“教师”，训练一个潜空间 3DGS 解码器当“学生”</strong>，完全摆脱真实多视角数据。具体实现分为四大模块：</p>
<p>1. 教师模型：相机可控视频扩散</p>
<ul>
<li>采用 <strong>GEN3C</strong>（Ren et al. 2025）作为教师，具备<br>– 时空 3D 缓存 <code>{P_t,v}</code>：单目深度反投影得到的彩色点云序列；<br>– 精确相机控制：Plücker 坐标像素级条件；<br>– 视频 VAE：空间 8×、时间 8× 压缩，输出潜空间 <code>z∈R^{L′×C×h×w}</code>。</li>
<li>给定单图/视频 + 采样相机轨迹，教师去噪得到<strong>多视角一致的视频潜码</strong> <code>z_v</code>。</li>
</ul>
<p>2. 学生模型：潜空间 3DGS 解码器</p>
<ul>
<li><strong>输入</strong>：同一潜码 <code>z_v</code> + Plücker 编码 <code>E</code>（同样经 VAE 压缩）。</li>
<li><strong>架构</strong>：<br>– 2×2 patchify → 16 层 Transformer-Mamba2 混合块（512 隐维）→ 反 3D 卷积；<br>– 输出每像素 14 维高斯参数：<code>G∈R^{V×L×H×W×14}</code>（位置、尺度、旋转、不透明度、RGB）。</li>
<li><strong>效率</strong>：在 <strong>潜空间</strong> 做全局注意力，避免 726 帧 704×1280 像素级 OOM；<br>每 8×8 邻域仅留 1 个高斯，再按不透明度剪枝 80%，最终约 200 万高斯即可实时渲染。</li>
</ul>
<p>3. 自蒸馏损失</p>
<p>教师分支 <code>D_rgb(z_v)</code> 输出 RGB 视频 <code>I^rgb</code>，学生分支渲染 <code>I^s = Render(G, {C_t})</code>，对齐信号包括：</p>
<p>损失</p>
<p>作用</p>
<p>权重</p>
<p><code>L_mse</code></p>
<p>像素级重建</p>
<p>1.0</p>
<p><code>L_lpips</code></p>
<p>感知质量</p>
<p>0.5</p>
<p><code>L_depth</code></p>
<p>防止扁平几何（ViPE 深度监督）</p>
<p>0.05</p>
<p><code>L_opacity</code></p>
<p>L1 稀疏化，剪枝冗余高斯</p>
<p>0.1</p>
<p>总损失：  </p>
<script type="math/tex; mode=display">L = λ\_{mse}L\_{mse} + λ\_{lpips}L\_{lpips} + λ\_{depth}L\_{depth} + λ\_{opacity}L\_{opacity}</script><p>4. 多轨迹融合与动态扩展</p>
<ul>
<li><strong>静态场景</strong>：对同一张输入图采样 6 条相机轨迹，教师分别生成 <code>z_1…z_6</code>；<br>学生解码器<strong>一次性融合</strong>所有潜码，学习跨视角补全遮挡区域，提升覆盖与一致性。</li>
<li><strong>动态场景</strong>：<br>– 输入单视频 + 估计相机；<br>– 引入<strong>源时间</strong> <code>T_src</code> 与<strong>目标时间</strong> <code>T_tgt</code> 的潜码编码，解码器输出时变高斯 <code>G(t)</code>；<br>– <strong>动态数据增强</strong>：将输入视频倒序再送教师，生成“由远及近”轨迹，保证每时刻都有远近视角监督，防止早期帧因视角不足被剪枝成空洞。</li>
</ul>
<p>训练与推理流程</p>
<ol>
<li><strong>训练</strong>：仅更新 3DGS 解码器参数，教师（GEN3C + RGB 解码器）完全冻结；<br>59 k 图像 × 6 轨迹、7 k 视频 × 6 轨迹，共 354 k/44 k 段合成视频，8×A100 训练 6 天。</li>
<li><strong>推理</strong>：<br>– 单图 → 采样轨迹 → 潜空间前馈 → 输出 3DGS，<strong>18 ms</strong> 渲染 1280×704 帧；<br>– 单视频 → 直接输出动态 3DGS，支持任意视角与任意时刻实时播放。</li>
</ol>
<p>结果</p>
<ul>
<li><strong>静态</strong>：RealEstate10K、DL3DV、T&amp;T 上 PSNR/SSIM/LPIPS 全面领先 Wonderland、Bolt3D 等。</li>
<li><strong>动态</strong>：自建的 Lyra-4D 数据集上比 BTimer(GEN3C) PSNR 高 +2.8 dB，LPIPS 低 27%。</li>
<li><strong>仿真</strong>：导出 .usdz 即可在 NVIDIA Isaac Sim 中作为物理一致场景供机器人闭环训练。</li>
</ul>
<p>通过上述设计，Lyra 首次实现了<strong>零真实多视角、单图/单视频到 3D/4D 的前馈生成</strong>，并保证几何一致、实时渲染、可直接用于下游仿真与交互任务。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在正文第 6 节与附录 C 中系统汇报了实验，涵盖<strong>静态 3D</strong>、<strong>动态 4D</strong>、<strong>消融分析</strong>、<strong>与现有方法对比</strong>、<strong>机器人仿真验证</strong>五大板块。具体设置与结果如下：</p>
<p>1. 数据集与训练细节</p>
<p>场景</p>
<p>训练数据</p>
<p>规模</p>
<p>来源</p>
<p>静态 3D</p>
<p>59 031 张图 × 6 轨迹</p>
<p>354 186 段视频</p>
<p>纯合成：LLM 生成文本 → 图扩散模型 → GEN3C</p>
<p>动态 4D</p>
<p>7 378 条视频 × 6 轨迹</p>
<p>44 268 段视频</p>
<p>同上，外加 ViPE 估计相机与深度</p>
<p><strong>评估数据集</strong></p>
<ul>
<li>静态：RealEstate10K、DL3DV、Tanks&amp;Temples（遵循 Wonderland/Bolt3D 协议）。</li>
<li>动态：自采 100 段 OOD 视频（Lyra-4D），512×512 裁剪以公平对比 BTimer。</li>
</ul>
<p>2. 静态单图→3D 对比实验</p>
<p>方法</p>
<p>RealEstate10K</p>
<p>DL3DV</p>
<p>Tanks&amp;Temples</p>
<p>PSNR↑/SSIM↑/LPIPS↓</p>
<p>PSNR↑/SSIM↑/LPIPS↓</p>
<p>PSNR↑/SSIM↑/LPIPS↓</p>
<p>ZeroNVS</p>
<p>13.01/0.378/0.448</p>
<p>13.35/0.339/0.465</p>
<p>12.94/0.325/0.470</p>
<p>ViewCrafter</p>
<p>16.84/0.514/0.341</p>
<p>15.53/0.525/0.352</p>
<p>14.93/0.483/0.384</p>
<p>Wonderland</p>
<p>17.15/0.550/0.292</p>
<p>16.64/0.574/0.325</p>
<p>15.90/0.510/0.344</p>
<p>Bolt3D</p>
<p>21.54/0.747/0.234</p>
<p>–</p>
<p>–</p>
<p><strong>Lyra</strong></p>
<p><strong>21.79/0.752/0.219</strong></p>
<p><strong>20.09/0.583/0.313</strong></p>
<p><strong>19.24/0.570/0.336</strong></p>
<p>→ 在三大基准<strong>全部指标第一</strong>，LPIPS 平均降低 12.6%。</p>
<p>3. 动态单视频→4D 对比实验</p>
<p>方法</p>
<p>Lyra-4D 数据集（100 段 OOD 视频）</p>
<p>PSNR↑</p>
<p>BTimer(GEN3C)</p>
<p>20.29</p>
<p><strong>Lyra</strong></p>
<p><strong>23.07</strong></p>
<p>→ 动态场景同样显著领先，PSNR +2.78 dB，LPIPS 下降 27%。</p>
<p>4. 消融实验（Lyra 合成数据集）</p>
<p>组件移除</p>
<p>PSNR↑</p>
<p>SSIM↑</p>
<p>LPIPS↓</p>
<p>说明</p>
<p>完整模型</p>
<p>24.77</p>
<p>0.837</p>
<p>0.224</p>
<p>基线</p>
<p>无自蒸馏（仅用真实数据）</p>
<p>19.08</p>
<p>0.659</p>
<p>0.413</p>
<p>OOD 泛化差</p>
<p>无多轨迹融合</p>
<p>17.73</p>
<p>0.632</p>
<p>0.446</p>
<p>视角覆盖不足</p>
<p>无 LPIPS 损失</p>
<p>23.74</p>
<p>0.766</p>
<p>0.370</p>
<p>高频细节丢失</p>
<p>无深度损失</p>
<p>24.31</p>
<p>0.811</p>
<p>0.247</p>
<p>几何扁平化</p>
<p>无不透明度剪枝</p>
<p>24.55</p>
<p>0.820</p>
<p>0.237</p>
<p>渲染慢 1.67×</p>
<p>无 Mamba-2（纯 Transformer）</p>
<p>24.58</p>
<p>0.818</p>
<p>0.241</p>
<p>推理慢 6.5×</p>
<p>像素空间 3DGS 解码</p>
<p>OOM</p>
<p>–</p>
<p>–</p>
<p>726 帧 704×1280 显存爆</p>
<p>→ 验证“自蒸馏+潜空间+多轨迹+深度+LPIPS”各组件均不可或缺。</p>
<p>5. 深度与视觉质量可视化</p>
<ul>
<li>图 6、图 10、附录图 11：极端视角下 Lyra 几何更锐利、纹理更一致；去除深度损失后深度图明显扁平。</li>
<li>附录视频：提供旋转、缩放、时间滑块交互，展示无优化实时 60 fps 渲染。</li>
</ul>
<p>6. 机器人仿真闭环验证</p>
<ul>
<li>流程：Lyra 生成 3DGS → 3DGUT 导出 .ply → 自动转 .usdz → 导入 NVIDIA Isaac Sim 5.0。</li>
<li>结果：图 12 与补充网页显示机器人在<strong>纯合成场景</strong>中成功执行导航、抓取任务，验证几何一致性与物理可用性。</li>
</ul>
<p>7. 效率与规模</p>
<p>指标</p>
<p>数值</p>
<p>推理分辨率</p>
<p>704×1280</p>
<p>渲染 1 帧耗时</p>
<p>18 ms（剪枝后）</p>
<p>输出高斯数</p>
<p>≈ 2.04 M（剪枝后）</p>
<p>训练总 GPU 天</p>
<p>8×A100 80 GB × 6 天</p>
<p>结论</p>
<p>实验覆盖<strong>合成→真实跨域</strong>、<strong>静态→动态</strong>、<strong>质量→效率→仿真</strong>全链路，定量与定性结果一致表明：<br>Lyra 在零真实多视角条件下，同时取得 SOTA 的图像质量、几何一致性与实时渲染性能，并可立即用于机器人闭环仿真。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向按“数据-模型-应用”三级归纳如下：</p>
<p>1. 数据与一致性</p>
<ul>
<li><strong>更大规模、更长时空的视频教师</strong><br>当前教师（GEN3C）最长 121 帧、单条轨迹 15 s 级。引入 10× 时长与 360° 无死角轨迹，可缓解远距离/背面区域空洞。</li>
<li><strong>物理-语义联合监督</strong><br>仅 RGB+Depth 不足以保证碰撞面、运动学合理。引入光学流、表面法向、语义分割、SDF 等多任务损失，提升几何与物理双重一致性。</li>
<li><strong>自动去噪与伪影过滤</strong><br>合成视频仍含闪烁、动态遮挡错误。可在外部循环加入“视频修复-再蒸馏”迭代，或利用 RAFT/Flow 一致性过滤伪影帧。</li>
</ul>
<p>2. 模型与算法</p>
<ul>
<li><strong>自回归/递归式大场景生成</strong><br>当前一次前馈受限于 GPU 显存与教师帧长。借鉴 LLM 的 next-token 或 next-chunk 预测，把场景划分为空间-时间块，逐块自回归生成，实现“无限延伸”的 3D/4D 世界。</li>
<li><strong>显式-隐式混合表示</strong><br>纯 3DGS 在镜面、透射表面表现有限。引入即时神经辐射场（Instant-NGP）或表面网格作为细粒度分支，与 3DGS 做可微融合，兼顾速度与质量。</li>
<li><strong>运动-跟踪内建</strong><br>当前动态解码器仅依赖时间编码，无显式对应关系。将“3D 对应跟踪头”嵌入重建网络，输出每高斯的运动轨迹与长期一致性约束，可提升慢动作、循环动作稳定性。</li>
<li><strong>多模态条件扩展</strong><br>除图像/视频外，支持文本、草图、激光雷达、事件相机等多模态输入；通过交叉注意力或条件 Adapter 实现灵活场景编辑与风格控制。</li>
</ul>
<p>3. 系统与应用</p>
<ul>
<li><strong>实时交互式编辑</strong><br>目前生成后场景固定。结合潜空间 inversion 与局部注意力掩码，实现“点击-拖拽”移动物体、改变材质或光照，并实时更新 3DGS。</li>
<li><strong>物理仿真闭环</strong><br>仅验证了 Isaac Sim 导入。下一步内建刚体、软体、流体求解器，直接在高斯中心施加力/速度，实现“生成-仿真-学习”一体化数据引擎。</li>
<li><strong>数字孪生与 AR/VR 流送</strong><br>将递归生成与 WebXR/云渲染结合，用户佩戴头显即可“边走边生成”无限场景；通过自适应 Level-of-Gaussians 流送，降低带宽与终端算力需求。</li>
<li><strong>伦理与防伪</strong><br>生成场景与真实捕获难以区分。需配套“高斯水印”或“神经哈希”技术，对每高斯中心/球谐系数嵌入不可见签名，实现可追踪、可验证的 3D 内容溯源。</li>
</ul>
<p>4. 理论层面</p>
<ul>
<li><strong>蒸馏上限分析</strong><br>定量研究“教师视频质量→学生 3D 质量”的映射边界，建立感知-几何误差下界，为后续视频模型设计提供理论目标。</li>
<li><strong>潜空间几何可解释性</strong><br>探索视频潜码的通道-空间维度与 3D 几何属性的对应关系，实现潜空间 Walk 即可平滑改变场景布局、相机高度或物体运动速度。</li>
</ul>
<p>综上，Lyra 打开了“生成式视频模型→显式 3D/4D”的新范式，后续可在<strong>更大时空覆盖、更强物理一致性、自回归无限场景、闭环物理仿真</strong>等方向持续深入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心内容速览</strong></p>
<ol>
<li><p><strong>问题</strong></p>
<ul>
<li>现有 3D/4D 场景生成依赖昂贵真实多视角数据，难以规模化。</li>
<li>视频扩散模型虽能想象丰富 2D 帧，但缺乏显式 3D 表示，无法直接用于机器人/仿真。</li>
</ul>
</li>
<li><p><strong>方法（Lyra）</strong></p>
<ul>
<li><strong>自蒸馏框架</strong>：以相机可控视频扩散模型（GEN3C）为“教师”，在潜空间训练 3DGS 解码器为“学生”。</li>
<li><strong>教师流程</strong>：单图/视频 + 采样轨迹 → 去噪得视频潜码 → RGB 分支输出多视角帧。</li>
<li><strong>学生流程</strong>：同一潜码 → 3DGS 解码器 → 每像素 14 维高斯 → 可微渲染对齐教师帧。</li>
<li><strong>多轨迹融合</strong>：每样本 6 条轨迹、726 帧，潜空间一次聚合，补全遮挡。</li>
<li><strong>动态扩展</strong>：加入源/目标时间编码，配合“运动反转”数据增强，实现单视频→4D 前馈生成。</li>
</ul>
</li>
<li><p><strong>实验结果</strong></p>
<ul>
<li><strong>静态</strong>：RealEstate10K/DL3DV/T&amp;T 上 PSNR、SSIM、LPIPS 全面领先 Wonderland、Bolt3D。</li>
<li><strong>动态</strong>：自采 100 段 OOD 视频，PSNR +2.8 dB，LPIPS ↓27%。</li>
<li><strong>消融</strong>：潜空间、多轨迹、深度、LPIPS、剪枝各组件均不可或缺；像素空间直接 OOM。</li>
<li><strong>仿真</strong>：生成 3DGS 可一键转 .usdz 导入 NVIDIA Isaac，实现机器人闭环训练。</li>
</ul>
</li>
<li><p><strong>贡献</strong></p>
<ul>
<li>首次零真实多视角、纯合成视频蒸馏，前馈输出静态/动态 3DGS。</li>
<li>潜空间 3DGS 解码器支持 726 帧 1280×704 输入，18 ms 实时渲染。</li>
<li>提供可扩展数据引擎与仿真接口，推动生成式 3D/4D 在物理 AI 领域的落地。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Lyra%3A%20Generative%203D%20Scene%20Reconstruction%20via%20Video%20Diffusion%20Model%20Self-Distillation，链接是  https://arxiv.org/pdf/2509.19296  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19296  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sherwin Bahmani,Tianchang Shen,Jiawei Ren,Jiahui Huang,Yifeng Jiang,Haithem Turki,Andrea Tagliasacchi,David B. Lindell,Zan Gojcic,Sanja Fidler,Huan Ling,Jun Gao,Xuanchi Ren</p>
<p><strong>Categories</strong>: cs.CV,cs.GR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19296.pdf">https://arxiv.org/pdf/2509.19296.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19296">https://arxiv.org/abs/2509.19296</a></p>
<p><strong>Arxiv ID</strong>: 2509.19296</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19296">https://papers.cool/arxiv/2509.19296</a></p>
<p><strong>Published</strong>: 2025-09-23T17:58:01Z</p>
<p><strong>Updated</strong>: 2025-09-23T17:58:01.000Z</p>
<hr>
<h3 id="9-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT"><a href="#9-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT" class="headerlink" title="9. What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19284">What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT</a></h3><p>Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what <em>characterizes</em> an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended <em>wait</em> tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the “longer-is-better” narrative, we find that both naive CoT lengthening and increased review are associated with <em>lower</em> accuracy. As CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the <em>Failed-Step Fraction (FSF)</em>, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that <em>fail less</em> and support <em>structure-aware</em> test-time scaling over indiscriminately generating long CoT.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型推理模型（LRMs）在长链思维（CoT）轨迹上花费了大量测试时间的计算，但有效的CoT是什么特征仍然不清楚。虽然先前的工作报告称，通过附加的<em>等待</em>标记来延长CoT和增加审查（重新审视早期步骤）可以带来收益，但最近的研究表明，较短的思考可以超越较长的轨迹。因此，我们对十个LRMs进行了一项系统评估，涉及数学和科学推理。与“越长越好”的叙述相反，我们发现天真的CoT延长和增加审查都与<em>较低</em>的准确性相关。随着CoT一步步展开，标记级别的指标可能会将冗长与过程质量混淆。我们引入了CoT的图形视图，以提取结构并识别一个单一统计量——<em>失败步骤比例（FSF）</em>，即在放弃分支中的步骤比例——它在各个模型中始终优于长度和审查比率的准确性。为了探究因果关系，我们设计了两个干预措施。首先，我们在测试时根据每个指标对候选CoT进行排名，其中FSF产生了最大的pass@1增益；其次，我们编辑CoT以去除失败的分支，这显著提高了准确性，表明失败的分支会影响后续推理。综合来看，这些结果将有效的CoT特征化为那些<em>失败较少</em>的，并支持<em>结构感知</em>的测试时间规模，而不是无差别地生成长CoT。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed: Error: 抓取失败: Navigation timeout of 3000 ms exceeded</p>
</div></details>
<p><strong>Authors</strong>: Yunzhen Feng,Julia Kempe,Cheng Zhang,Parag Jain,Anthony Hartshorn</p>
<p><strong>Categories</strong>: cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19284.pdf">https://arxiv.org/pdf/2509.19284.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19284">https://arxiv.org/abs/2509.19284</a></p>
<p><strong>Arxiv ID</strong>: 2509.19284</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19284">https://papers.cool/arxiv/2509.19284</a></p>
<p><strong>Published</strong>: 2025-09-23T17:50:54Z</p>
<p><strong>Updated</strong>: 2025-09-23T17:50:54.000Z</p>
<hr>
<h3 id="10-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects"><a href="#10-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects" class="headerlink" title="10. Large Language Models Discriminate Against Speakers of German Dialects"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13835">Large Language Models Discriminate Against Speakers of German Dialects</a></h3><p>Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model’s dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics—German dialect speakers—amplifies bias more than implicit cues like dialect usage.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>方言是人类文化的重要组成部分，存在于世界各个地区。在德国，超过40%的人口讲地区方言（Adler和Hansen，2022）。然而，尽管方言在文化上很重要，讲方言的人往往面临负面的社会刻板印象。我们研究这些刻板印象是否在大型语言模型（LLMs）中得到了体现。我们借鉴社会语言学关于方言认知的文献，分析与方言使用者相关的特征。根据这些特征，我们评估LLMs在两个任务中表达的方言命名偏见和方言使用偏见：联想任务和决策任务。为了评估模型的方言使用偏见，我们构建了一个新颖的评估语料库，将七种地区德国方言（例如，阿勒曼尼语和巴伐利亚语）的句子与它们的标准德语对应句配对。我们发现：（1）在联想任务中，所有评估的LLMs都表现出对德国方言说话者的显著方言命名和使用偏见，反映在负面形容词的联想中；（2）所有模型在决策过程中都重现了这些方言命名和使用的偏见；（3）与先前工作表明在明确的人口统计提及中偏见最小的发现相反，我们发现明确标记语言人口统计数据——德国方言说话者——会比隐式提示（如方言使用）更加强化偏见。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在回答一个核心问题：<strong>大型语言模型（LLM）是否像人类一样，对德语方言使用者持有负面刻板印象，并在实际决策中表现出歧视？</strong></p>
<p>具体而言，论文聚焦以下三方面：</p>
<ol>
<li><p>关联偏见（association bias）<br>检验模型是否将“方言使用者”与一系列负面特质（如“未受教育”“粗心”“封闭”等）更紧密地联系在一起，而将“标准德语使用者”与正面特质（如“受过教育”“细心”“开放”等）关联。</p>
</li>
<li><p>决策偏见（decision bias）<br>检验模型在需要做出选择的真实场景（如职业推荐、居住地建议、培训方案分配等）中，是否系统性地把方言使用者分配到资源或地位更低的选项。</p>
</li>
<li><p>显性 vs. 隐性触发方式<br>比较两种提示策略：</p>
<ul>
<li><strong>显性命名</strong>（dialect naming）：直接在提示里声明“某人使用方言”。</li>
<li><strong>隐性使用</strong>（dialect usage）：仅呈现方言文本本身，不提及说话者身份。<br>论文发现，与先前“显性 demographic 信息会降低偏见”的结论相反，<strong>显式指出“德语方言使用者”反而显著放大了偏见</strong>。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中系统梳理了两条研究脉络，可归纳为以下要点：</p>
<ul>
<li><p><strong>Perceptual Dialectology（感知方言学）</strong></p>
<ul>
<li>Gärtig et al. (2010)、Eichinger et al. (2014) 的大样本调查显示，巴伐利亚、萨克森等德语方言被听者评价为“更热情、更情绪化”，但萨克森同时被视为“教育水平低”。</li>
<li>Trillhaase (2021) 采用人格五因素量表，发现中德巴伐利亚与上萨克森方言说话者被感知为“开放性低、责任心低”，即“封闭、粗心”。</li>
<li>这些刻板印象已产生经济后果：Grogger et al. (2020) 发现方言口音带来工资惩罚；Schulte et al. (2024) 的元分析证实方言使用者在招聘环节处于劣势。</li>
</ul>
</li>
<li><p><strong>Dialect Bias in NLP / LLM</strong></p>
<ul>
<li>性能差距：Blodgett et al. (2016)、Jurgens et al. (2017)、Ziems et al. (2022; 2023) 等表明，方言文本在语言识别、机器翻译、语音辨识上普遍比标准语更差。</li>
<li>内容审核偏差：Davidson et al. (2019)、Sap et al. (2019) 指出，非裔美国英语（AAE）被仇恨言论检测器过度标记。</li>
<li>隐性刻板印象：Hofmann et al. (2024) 发现，GPT-4 对 AAE 使用者表现出“ covert racism”——显性提示种族时偏见小，仅呈现方言文本时偏见大。</li>
<li>德语方言技术进展：Artemova et al. (2024)、Blaschke et al. (2024a,b)、Litschko et al. (2025) 开始构建德语方言资源与任务，但尚未评估偏见。</li>
</ul>
</li>
</ul>
<p>综上，已有研究证实（1）人类对德语方言存在系统性感知偏见，且（2）英语方言在 LLM 中会引发隐性歧视；然而**“德语方言在大型语言模型中是否也被刻板化”**这一问题尚属空白，正是本文要填补的研究缺口。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“构建并行语料 + 双重任务 + 双重触发方式”的组合框架，把问题拆解为可量化的偏见指标，具体步骤如下：</p>
<ol>
<li><p>构建评测基准</p>
<ul>
<li>从 WikiDIR 抽取 7 种德语方言（低地德语、北弗里斯兰、萨特弗里斯兰、科隆方言、莱茵法兰克、阿勒曼尼、巴伐利亚）各 50 篇维基文本，人工清洗后用 GPT-4o 直译成标准德语，经母语者校对，得到 350 对“方言—标准”平行句段，用于 dialect usage 条件。</li>
</ul>
</li>
<li><p>选定刻板维度<br>基于感知方言学文献，确定 6 组对立特质：<br>careless vs. conscientious<br>closed-minded vs. open-minded<br>friendly vs. unfriendly<br>rural vs. urban<br>temperamental vs. calm<br>uneducated vs. educated<br>每组各取 20 个形容词或职业/场景词，构成关联与决策的刺激池。</p>
</li>
<li><p>任务设计</p>
<ul>
<li><strong>Association Task（关联任务）</strong><br>向模型展示 10 个混合形容词，要求把每个词分配给 Writer A 或 B；计算  <script type="math/tex; mode=display">
\\text{bias} = \\frac{N(s\_a,X\_a)}{N(s\_a,X\_a)+N(s\_a,X\_b)} + \\frac{N(s\_b,X\_b)}{N(s\_b,X\_a)+N(s\_b,X\_b)} - 1</script>得分 ∈ [−1, 1]，0 为无偏见。</li>
<li><strong>Decision Task（决策任务）</strong><br>让模型先生成两人各 100 字人物小传，再依据小传把两人分别推荐到“高/低教育职业”“国际/本地旅游”“结构化/即兴学习”等 3 组场景；每次推荐若把方言者分到负面选项记 +1，反之记 −1，平均后得 [−1, 1] 偏见分。</li>
</ul>
</li>
<li><p>触发方式对比</p>
<ul>
<li>Dialect Naming：提示里显式写“Writer A 写标准德语 / Writer B 写××方言”。</li>
<li>Dialect Usage：提示里只给具体文本，不提及身份，靠模型自己从语言特征推断。</li>
</ul>
</li>
<li><p>统计检验与鲁棒性</p>
<ul>
<li>对 10 个模型（参数 7 B–72 B，含 GPT-5-mini）各跑 1000+ 样本，采用单样本 t 检验（零假设=0）及 bootstrap 95 % CI。</li>
<li>控制位置顺序、提示句式、话题差异，并加入“noisy text”对照，排除“模型仅把方言当错别字”的替代解释。</li>
</ul>
</li>
</ol>
<p>通过上述方法，论文把“LLM 是否歧视德语方言使用者”转化为可重复的量化实验，从而验证偏见存在、测量强度，并揭示显性 demographic 提示会放大而非减弱歧视。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计并执行了 <strong>3 组核心实验 + 4 项辅助分析</strong>，全部围绕“德语方言偏见”展开。以下用 markdown 分点归纳：</p>
<p>1 核心实验（产生主要图 2 结果）</p>
<p>实验</p>
<p>目的</p>
<p>关键操作</p>
<p>输出指标</p>
<p><strong>1.1 Association Task</strong></p>
<p>测“联想偏见”</p>
<p>给 10 个混合形容词，让模型把每个词分配给 Writer A/B</p>
<p>偏见得分 ∈ [−1,1]，0 为无偏</p>
<p><strong>1.2 Decision Task</strong></p>
<p>测“决策偏见”</p>
<p>先让模型生成两人 100 字小传，再推荐到高/低地位选项</p>
<p>每次推荐计 +1/−1，平均得 [−1,1]</p>
<p><strong>1.3 Naming vs Usage</strong></p>
<p>比较显性/隐性触发</p>
<p>同一模型+同一任务，分别用“显式标签”和“纯文本”两种 prompt</p>
<p>配对 t 检验，看哪种偏见更大</p>
<blockquote>
<p>以上两项任务 × 两种触发方式 × 6 个特质 × 10 个模型 = <strong>1200 余组正式统计检验</strong>，构成图 2 与表 8 的主结果。</p>
</blockquote>
<p>2 辅助分析（验证鲁棒性与机制）</p>
<p>分析</p>
<p>动机</p>
<p>做法</p>
<p>关键结论</p>
<p><strong>2.1 Noisy Text 对照</strong></p>
<p>排除“方言=错别字”假说</p>
<p>把标准德语文本按 50 % 概率随机增删改字符/词，生成“噪声版”，再跑 Association Task</p>
<p>方言偏见显著 &gt; 噪声偏见，说明模型并非单纯视方言为错误</p>
<p><strong>2.2 跨方言差异</strong></p>
<p>看偏见是否因方言而异</p>
<p>把 7 种方言拆分组别，分别计算偏见得分</p>
<p>所有方言趋势一致，绝对差异小（图 5、图 10–12）</p>
<p><strong>2.3 跨话题差异</strong></p>
<p>排除“内容主题”混淆</p>
<p>用 GPT-4o 给 350 段文本打主题标签（传记、地理、音乐等），按主题分层比较偏见</p>
<p>不同主题下偏见几乎持平（图 6、图 16）</p>
<p><strong>2.4 Marked-Persona 词频</strong></p>
<p>揭示模型内部刻板内容</p>
<p>对 Decision Task 生成的小传，用加权对数优势比提取区分性词汇</p>
<p>标准德语侧高频出现“classical、precise、academic”；方言侧高频出现“folk、dialect、raw、creative”等（图 3 与表 9–13）</p>
<p>3 补充鲁棒实验</p>
<ul>
<li><p><strong>3.1 Prompt 句式扰动</strong><br>用 3 种不同前缀句式重复 Llama-3.1 70B 实验，结果方向与显著性不变（表 7）。</p>
</li>
<li><p><strong>3.2 “拒绝/双选”选项</strong><br>在关联与决策 prompt 中增加“None / Both”可选项，允许模型不做区分；发现偏见幅度虽下降，但仍显著 ≠ 0（图 9）。</p>
</li>
<li><p><strong>3.3 模型规模对比</strong><br>同一家族大模型 vs 小模型配对 t 检验：更大参数版本在 74 %–100 % 比较中表现出更高偏见（第 5.1 节）。</p>
</li>
</ul>
<p>4 统计与效应量</p>
<ul>
<li>全部 1200 余组 bias 得分均做单样本 t 检验（H0=0）；显著阈值 p&lt;0.001。</li>
<li>计算 Cohen’s d：多数 trait 落在“中等”到“大”效应区间（表 8）。</li>
</ul>
<p>综上，论文通过“两大主任务 + 四大鲁棒/机制分析 + 三项补充验证”，系统回答了“LLM 是否存在德语方言偏见、偏见多强、为何产生、是否鲁棒”这一系列问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分点列出并给出可操作的切入点：</p>
<ul>
<li><p><strong>跨语言对比</strong></p>
<ul>
<li>将同一套“association + decision”框架迁移到瑞士德语、奥地利方言、卢森堡语等高地德语变体，观察偏见强度与德国境内方言是否呈梯度差异。</li>
<li>对英语、法语、意大利等存在显著方言歧视的语言重复实验，检验“显性 demographic 提示放大偏见”是否普遍成立。</li>
</ul>
</li>
<li><p><strong>偏见来源拆解</strong></p>
<ul>
<li>用 probing 方法定位模型哪一层/哪类注意力头对方言特征最敏感，再对相应表示进行线性干预，看能否在不损害下游性能的前提下抑制偏见。</li>
<li>构造“音系转写→正字法”多版本平行语料，逐步引入方言音系、词汇、句法特征，量化每一步对偏见得分的边际贡献，判断“口音拼写”与“词汇句法”哪者更关键。</li>
</ul>
</li>
<li><p><strong>训练数据级干预</strong></p>
<ul>
<li>在继续预训练阶段加入“方言-标准”去偏见目标：对比学习拉近同一内容的两种语言变体表示，或添加 fairness adversarial loss，测量偏见与 perplexity 的权衡曲线。</li>
<li>构建“平衡刻板属性”的方言文本扩充集（例如让方言文本也描述“科学家、外科医生”场景），用数据增强方式微调，评估决策任务偏见下降是否显著。</li>
</ul>
</li>
<li><p><strong>真实场景落地风险</strong></p>
<ul>
<li>把实验范式嵌入在线招聘平台或智能客服系统 A/B 测试：用虚拟简历/对话脚本随机呈现方言或标准语，记录模型给出的工资报价、面试邀请率等实际指标，验证实验室偏见能否外推到经济决策。</li>
<li>研究语音→文本链路：先让 ASR 模型转录方言语音，再喂入 LLM 做后续决策，观察“ASR 错误率 × LLM 偏见”的复合效应。</li>
</ul>
</li>
<li><p><strong>交互式偏见放大</strong></p>
<ul>
<li>设计多轮对话任务：首轮仅出现轻微方言特征，若模型回应中已隐含刻板内容，第二轮用户继续用方言追问，看偏见是否随轮次递增（bias snowballing）。</li>
<li>引入“纠正信号”——当模型做出歧视性推荐后，人类即时指出错误并给出反例，考察在持续反馈下偏见能否收敛，量化所需纠正轮次。</li>
</ul>
</li>
<li><p><strong>社会人口交叉维度</strong></p>
<ul>
<li>在 prompt 中同时操纵“方言 + 性别 + 年龄 + 移民背景”，用 intersectional bias 度量（如 multiplicative odds ratio）观察是否出现“方言×性别”双重惩罚效应。</li>
<li>对比东德 vs 西德、城市 vs 农村来源的同一方言文本，检验模型是否把地域经济刻板印象进一步叠加到方言之上。</li>
</ul>
</li>
<li><p><strong>认知机制与人类对照</strong></p>
<ul>
<li>在线招募德语母语被试，采用与 LLM 相同的 association 与 decision 任务，记录反应时与选择比例，直接比较人类与模型的偏见强度及一致性，验证模型是否过度放大了人类原有态度。</li>
<li>使用眼动或 EEG 测量人类在听到方言口音时的认知负荷，再与模型对方言文本的 perplexity/注意力分布做相关分析，探讨“处理难度→刻板印象”路径是否共通。</li>
</ul>
</li>
<li><p><strong>评估指标标准化</strong></p>
<ul>
<li>将本文 bias 分数与 ML 公平性文献中的 ΔDP、EOdds、EOpportunity 等指标建立映射关系，提出适用于文本生成任务的“dialect fairness”统一基准，方便后续模型横向对比。</li>
<li>开发自动化、轻量级探针，只需几十条样本即可快速估算新模型的德语方言偏见，降低社区复现门槛。</li>
</ul>
</li>
<li><p><strong>防御与治理</strong></p>
<ul>
<li>研究“方言提示重写”策略：在用户输入后、模型决策前，用轻量模型把方言文本实时转写成标准德语，再对输出做逆向调整，评估对偏见抑制率与语义保真度。</li>
<li>探讨监管层面“语言公平影响评估”（Linguistic-Fairness Impact Assessment）模板，要求商用 LLM 在德语区上线前必须披露方言偏见测试结果，并设定可接受阈值。</li>
</ul>
</li>
</ul>
<p>这些方向既涵盖科学问题（来源、机制、跨语言普适性），也涉及技术干预（训练、微调、后处理）和政策治理，可为后续研究提供持续探索路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文题目</strong><br>Large Language Models Discriminate Against Speakers of German Dialects</p>
<p><strong>核心结论一句话</strong><br>主流大型语言模型在关联与决策任务中均显著歧视德语方言使用者，且<strong>显式指出“某人讲方言”比仅呈现方言文本更能放大偏见</strong>。</p>
<p>1 研究动机</p>
<ul>
<li>德国 40 % 以上人口使用区域方言，现实社会已存在“方言=教育低/农村/粗心”等刻板印象，并导致工资与招聘惩罚。</li>
<li>尚不清楚 LLM 是否内化并放大了这些偏见，亦未知“显性 demographic 提示”对方言是否仍具“去偏见”效果。</li>
</ul>
<p>2 方法框架</p>
<p>模块</p>
<p>内容</p>
<p><strong>语料</strong></p>
<p>7 种德语方言维基文本各 50 篇→人工清洗→GPT-4o 直译→母语者校对，得到 350 对平行句段。</p>
<p><strong>特质</strong></p>
<p>6 组对立刻板维度：粗心/细心、封闭/开放、不友好/友好、农村/城市、情绪化/冷静、未受教育/受过教育。</p>
<p><strong>触发方式</strong></p>
<p>① <strong>Dialect Naming</strong> 显式标签“Writer A 写标准德语 / Writer B 写××方言”；② <strong>Dialect Usage</strong> 仅呈现文本，不提及身份。</p>
<p><strong>任务</strong></p>
<p>① <strong>Association</strong>：把 10 个混合形容词分给 A/B，计算 [−1,1] 偏见分；② <strong>Decision</strong>：先让模型生成 100 字人物小传，再将其推荐到高/低地位选项，得 [−1,1] 决策偏见分。</p>
<p><strong>模型</strong></p>
<p>10 款指令微调模型：Qwen-2.5、Gemma-3、Llama-3.1、Aya、Leo-HessianAI 的 7 B–72 B 变体，外加 GPT-5-mini。</p>
<p>3 主要发现</p>
<ol>
<li><p><strong>1200 余组统计检验中，&gt;90 % 组合偏见显著 ≠ 0</strong></p>
<ul>
<li>Association：除“友好”维度大多反向外，其余特质均把负面词显著派给方言者。</li>
<li>Decision：模型系统地把方言者分配到低教育职业、农村居住地、即兴学习方案等。</li>
</ul>
</li>
<li><p><strong>Dialect Naming &gt; Dialect Usage</strong></p>
<ul>
<li>70 %–88 % 的显著对比中，显式标签条件下的偏见得分更高，与英语种族偏见研究“显性降低歧视”的结论相反。</li>
</ul>
</li>
<li><p><strong>模型越大，偏见越强</strong></p>
<ul>
<li>同一家族 70 B vs 8 B 比较，74 %–100 % 的显著例中，大模型偏见得分更高。</li>
</ul>
</li>
<li><p><strong>鲁棒性验证</strong></p>
<ul>
<li>用“噪声文本”对照、换 prompt 句式、加 None/Both 选项、按主题/方言分层，偏见方向与显著性均不变。</li>
</ul>
</li>
<li><p><strong>Marked-Persona 词频分析</strong></p>
<ul>
<li>标准德语小传高频出现“classical、precise、academic”；方言小传高频出现“folk、dialect、raw、creative”，揭示模型内部刻板内容。</li>
</ul>
</li>
</ol>
<p>4 贡献与意义</p>
<ul>
<li>首次系统证实 LLM 对德语方言存在显著且鲁棒的刻板偏见。</li>
<li>发现“显性 linguistic demographic”会放大而非减弱歧视，为公平性研究提供新的反例。</li>
<li>发布平行语料与评测脚本，便于社区后续干预与监管。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Large%20Language%20Models%20Discriminate%20Against%20Speakers%20of%20German%20Dialects，链接是  https://arxiv.org/pdf/2509.13835  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13835  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Minh Duc Bui,Carolin Holtermann,Valentin Hofmann,Anne Lauscher,Katharina von der Wense</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13835.pdf">https://arxiv.org/pdf/2509.13835.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13835">https://arxiv.org/abs/2509.13835</a></p>
<p><strong>Arxiv ID</strong>: 2509.13835</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13835">https://papers.cool/arxiv/2509.13835</a></p>
<p><strong>Published</strong>: 2025-09-17T09:05:37Z</p>
<p><strong>Updated</strong>: 2025-09-17T09:05:37.000Z</p>
<hr>
<h3 id="11-Soft-Tokens-Hard-Truths"><a href="#11-Soft-Tokens-Hard-Truths" class="headerlink" title="11. Soft Tokens, Hard Truths"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19170">Soft Tokens, Hard Truths</a></h3><p>The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens. This is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use “soft” tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the “soft” models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>在推理大型语言模型的链式思维（CoT）阶段，使用连续而非离散的标记最近引起了关注，这基于一种直觉，即离散标记的连续混合可以同时模拟多个推理路径的叠加。理论结果正式证明，连续标记具有更强的表达能力，并且可以更高效地解决特定问题。然而，连续标记的实际使用受到了强大的训练困难的限制：以前的工作要么只是使用在预训练的离散标记模型上的推理时间的连续标记，要么必须从真实离散CoT中提取连续CoT，并且面临计算成本，这限制了CoT的标记数量。本研究首次引入了一种通过强化学习（RL）来学习连续CoT的可扩展方法，无需从参考离散CoT中提取。我们使用“软”标记：将标记与输入嵌入中的噪声混合，以提供RL探索。计算开销最小，使我们能够学习包含数百个标记的连续CoT。在使用Llama和Qwen模型（最多8B）的数学推理基准测试中，使用连续CoT的训练在pass@1性能上与离散标记CoT相匹配，并在pass@32中超过它们，显示出更大的CoT多样性。在系统比较中，表现最佳的情况是使用连续CoT标记进行训练，然后在推理阶段使用离散标记，这意味着“软”模型可以以标准方式部署。最后，我们表明，连续CoT的RL训练在领域外任务中更好地保持了基础模型的预测，从而为基础模型提供了更柔和的调整。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破现有大模型“链式思考”（Chain-of-Thought, CoT）必须依赖离散词元（discrete tokens）的瓶颈，解决以下核心问题：</p>
<ol>
<li><p>离散 CoT 的表达能力受限<br>每一步只能顺序采样单个词元，难以并行探索多条推理路径，容易陷入局部最优。</p>
</li>
<li><p>连续 CoT 的训练难题<br>先前方法要么仅在推理阶段把连续表示“硬塞”给离散模型，效果有限；要么需用 ground-truth 离散 CoT 做蒸馏，计算代价高，连续步数受限（如 Coconut 仅 6 步）。</p>
</li>
<li><p>强化学习在连续空间中的探索缺失<br>纯连续、无噪声的“软思考”是确定性函数，缺乏 RL 所需的随机性，无法直接应用 REINFORCE 系列算法。</p>
</li>
</ol>
<p>为此，作者提出首个<strong>无需 ground-truth 离散 CoT、可扩展的强化学习框架</strong>，通过向连续词元嵌入注入高斯噪声，产生可导的探索信号，实现任意长度（数百步）连续 CoT 的端到端训练，并在数学推理基准上验证其 pass@1 与离散方法持平、pass@32 显著更高，同时对域外任务更温和。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究按“连续/隐空间推理”介入模型的阶段（预训练、后训练、推理）与核心思路分类如下：</p>
<ul>
<li><p><strong>推理阶段仅插入连续表示</strong></p>
<ul>
<li>Soft Thinking (Zhang et al., 2025)</li>
<li>后续批判工作 Wu et al. (2025) 指出若无额外噪声，LLM 会退化为贪婪解码，效果不及离散基线。</li>
</ul>
</li>
<li><p><strong>后训练阶段引入连续 CoT</strong></p>
<ul>
<li>Coconut (Hao et al., 2024)<br>用 ground-truth 离散 CoT 做“硬→软”蒸馏，需 BPTT 贯穿连续步，显存占用大，最多 6 步。</li>
<li>Codi (Shen et al., 2025)<br>自蒸馏把离散 CoT 压缩到连续隐变量，仍需参考离散轨迹。</li>
<li>本文方法<br>首个无需参考离散轨迹、用 RL 直接学习任意长度连续 CoT 的工作。</li>
</ul>
</li>
<li><p><strong>预训练阶段植入“思考”或占位符</strong></p>
<ul>
<li>Pause/Filler Tokens (Goyal et al., 2024; Lanham et al., 2023 等)</li>
<li>CoCoMix (Tack et al., 2025)<br>预训练即交替出现连续与离散词元，用稀疏自编码器耦合。</li>
<li>Looped/Recurrent Depth (Saunshi et al., 2025; Geiping et al., 2025)<br>在深度方向循环 Transformer 块，实现“纵向”连续推理。</li>
</ul>
</li>
<li><p><strong>理论分析</strong></p>
<ul>
<li>Reasoning by Superposition (Zhu et al., 2025a)<br>证明连续隐状态可并行编码多搜索前沿，浅层 Transformer 即可在 O(n) 步解决图可达性，而离散需 O(n²)。</li>
</ul>
</li>
</ul>
<p>综上，本文与 Coconut、Soft Thinking 最邻近，但突破了“需离散蒸馏”或“仅推理干预”的限制，首次把噪声驱动的 RL 引入连续 CoT 后训练。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“连续链式思考（CoT）无法直接做强化学习”这一核心障碍转化为一个可导、可采样的随机决策过程，具体实现分三步：</p>
<ol>
<li><p>构造带噪声的软词元<br>在 CoT 阶段不采样离散词元，而是保留整个 softmax 概率向量 $p_{t-1}$，按  </p>
<script type="math/tex; mode=display">\\tilde{h}^0\_t = p\_{t-1}E + \\sigma \\varepsilon\_t, \\quad \\varepsilon\_t \\sim \\mathcal{N}(0,I)</script><p>产生下一层输入嵌入。$\sigma&gt;0$ 引入高斯扰动，形成可控制的探索信号。</p>
<ul>
<li>当温度 $\tau\rightarrow 0$ 且 $\sigma&gt;0$ 时称为 fuzzy tokens，近似“离散+噪声”。</li>
<li>当 $\tau&gt;0$ 且 $\sigma&gt;0$ 时称为 soft tokens，保持真正的连续混合。</li>
</ul>
</li>
<li><p>推导 REINFORCE 梯度<br>由于噪声注入，整条连续 CoT 轨迹 $\tilde{h}^0_{1:T}$ 成为随机变量，其概率密度可显式写出  </p>
<script type="math/tex; mode=display">\\log\\pi(\\tilde{h}^0\_t|\\tilde{h}^0\_{<t}) = -\\frac{1}{2\\sigma^2}|\\tilde{h}^0\_t - h^0\_t|^2 + \\text{const}</script><p>其中 $h^0_t=p_{t-1}E$ 是确定性部分，与模型参数可导相关。于是标准 REINFORCE 损失  </p>
<script type="math/tex; mode=display">\\mathcal{L}= -\\mathbb{E}_{\\tilde{h},a}\\Bigl\[R(a),\\sum_{t=1}^T \\log\\pi(\\tilde{h}^0\_t|\\tilde{h}^0\_{<t}) + \\log\\pi(a|\\tilde{h}^0)\\Bigr\]</script><p>可直接反向传播，无需 BPTT 贯穿全部时间步。任何进阶策略梯度算法（RLOO、GRPO、PPO 等）均可套用。</p>
</li>
<li><p>训练与推理解耦</p>
<ul>
<li>训练阶段：用数百步连续 CoT（128–512 tokens）做 RL，计算开销仅多存储一份 $p_t$ 向量并加噪声，内存与离散训练几乎相同。</li>
<li>推理阶段：可继续用软/噪声轨迹，也可直接切换回标准离散解码。实验发现“软训练 + 离散推理”普遍最优，无需部署特殊解码器。</li>
</ul>
</li>
</ol>
<p>通过上述设计，论文首次实现了</p>
<ul>
<li>不依赖 ground-truth 离散 CoT</li>
<li>支持任意长度连续思考</li>
<li>计算开销可忽略<br>的 RL 后训练框架，并在数学推理基准上取得 pass@1 持平、pass@32 提升且域外鲁棒性更好的效果。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“能否用强化学习直接训练连续 CoT”以及“训练后是否带来实用收益”两条主线展开，覆盖 3 组基座模型、3 个数学数据集、6 种推理配置与 3 项域外鲁棒性测试，具体工作量如下：</p>
<ol>
<li><p>训练规模与配置</p>
<ul>
<li>基座：Llama-3.2-3B-Instruct、Llama-3.1-8B-Instruct、Qwen-2.5-3B-Instruct</li>
<li>数据：GSM8K、MATH、DeepScaleR（分别对应 128、512、512 最大 CoT 长度）</li>
<li>方法对比：hard / fuzzy / soft 三种训练方式，均用 RLOO 强化学习 4k 步，每组 3 随机种子</li>
<li>推理组合：每种训练模型再用 hard-greedy、hard-sample、fuzzy-greedy、fuzzy-sample、soft-greedy、soft-sample 六种解码策略评测，答案一律 greedy 解码，形成 3×3×6=54 种训练-推理组合</li>
</ul>
</li>
<li><p>主要指标</p>
<ul>
<li>pass@1（greedy）与 pass@32（采样 32 条）在 GSM8K、MATH-500、OlympiadBench-675 上的准确率</li>
<li>训练/验证成功率曲线、收敛步数</li>
<li>域外影响：HellaSwag、ARC、MMLU 的准确率与正确选项负对数似然（NLL）</li>
<li>CoT 长度分布与早停行为</li>
<li>熵轨迹：每步平均 token-entropy 随 CoT 深度的变化</li>
</ul>
</li>
<li><p>关键结果</p>
<ul>
<li>pass@1：soft/fuzzy 训练与 hard 训练基本持平（±1–2% 内），无显著下降</li>
<li>pass@32：soft/fuzzy 平均提升 2–5 个百分点，Llama-8B 在 GSM8K→MATH 转移场景下 hard 训练崩溃（45.4%），soft/fuzzy 保持 83–84%</li>
<li>域外 NLL：hard 训练在 ARC 上平均恶化 0.3–0.8 nats，soft/fuzzy 几乎不漂移；Llama-8B 在 MMLU 上亦同</li>
<li>熵分析：hard 训练使采样轨迹熵显著下降，soft/fuzzy 保留基座模型的熵形态，解释其多样性更好</li>
<li>推理解耦：同模型下“hard-greedy 推理”几乎总是最优，验证“软训练+硬部署”即可受益，无需专门推理代码</li>
</ul>
</li>
<li><p>消融与稳健性</p>
<ul>
<li>噪声强度 γ∈{0.1,0.33,1,3}：γ≤1 性能平稳，γ=3 训练崩溃</li>
<li>温度 τ∈{0.0001,0.001,0.01,0.1}：0.0001–0.1 区间无显著差异</li>
<li>噪声位置：仅嵌入层加噪声稳定收敛；对 logits 加噪声需 top-k=5 才能学到信号，其余配置失效</li>
<li>早停与长度：连续 CoT 可在 100–300 步内自动结束，不增加额外长度负担</li>
</ul>
</li>
</ol>
<p>综上，实验系统验证了“带噪声软词元 + RL”方案在训练可行性、推理性能、鲁棒性与部署便利性上的全面优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分主题列出：</p>
<ul>
<li><p><strong>算法与理论</strong></p>
<ul>
<li>将噪声注入视为随机最优控制问题，推导最优 $\sigma_t$ 调度，而非固定常数。</li>
<li>用逆强化学习自动构造 CoT 阶段奖励，不再仅依赖最终答案正确性。</li>
<li>研究连续 CoT 的表达能力边界：对哪些复杂度类问题，连续隐状态可带来指数级加速？</li>
</ul>
</li>
<li><p><strong>训练策略</strong></p>
<ul>
<li>采用 PPO/GRPO 等带重要性截断的算法，缓解高奖励方差，观察更长 CoT（&gt;1k tokens）能否继续提升。</li>
<li>引入课程强化学习：先短 CoT 后长 CoT，或先低噪声后高噪声，考察收敛速度与稳定性。</li>
<li>探索多任务 RL，让同一套连续 CoT 参数同时服务数学、代码、逻辑谜题，验证通用性。</li>
</ul>
</li>
<li><p><strong>模型结构</strong></p>
<ul>
<li>把连续隐状态维度与词表解耦，使用低秩瓶颈或混合离散-连续 VAE，进一步降低内存。</li>
<li>试验“深度方向”连续思考：在 Transformer 层间循环同一隐状态，而非在时间步上展开。</li>
<li>引入可学习的 $\sigma$ 或温度 $\tau$ 作为模型参数，让网络自行决定探索强度。</li>
</ul>
</li>
<li><p><strong>推理与部署</strong></p>
<ul>
<li>研究测试时自适应噪声：根据当前问题难度或模型不确定性动态调整 $\sigma$，实现 test-time scaling。</li>
<li>结合 beam-search / 蒙特卡洛树搜索，在连续空间做多路径聚合，而非简单采样 32 条。</li>
<li>将软词元量化或蒸馏回小模型，验证能否把“软训练”收益迁移到边缘设备。</li>
</ul>
</li>
<li><p><strong>评测与可解释性</strong></p>
<ul>
<li>构造需要并行探索的组合任务（如 NP 完全谜题），量化连续 CoT 相比离散 CoT 的加速比。</li>
<li>用探测任务（probing）分析隐状态是否自动编码了搜索树、计划或抽象概念。</li>
<li>可视化不同 $\sigma$ 下的隐状态轨迹，观察模型是否真正利用“叠加”同时保持多条推理路径。</li>
</ul>
</li>
<li><p><strong>安全与鲁棒性</strong></p>
<ul>
<li>检验连续 CoT 是否更容易受到隐空间对抗扰动；若然，研究相应的防御方法。</li>
<li>评估软训练对有害请求或幻觉倾向的影响，确保“更温和”的微调不降低安全性。</li>
</ul>
</li>
</ul>
<p>这些方向既有理论深度，也具工程价值，可进一步揭示“连续思考”的极限与落地路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Soft Tokens, Hard Truths</strong> 提出首个<strong>无需 ground-truth 离散链式思考（CoT）</strong>、<strong>可扩展的强化学习框架</strong>，直接在大模型中训练<strong>连续 CoT</strong>。核心做法是在概率混合的词元嵌入上注入高斯噪声，使整条连续轨迹成为可导随机变量，从而套用 REINFORCE 系列算法进行后训练。实验表明：</p>
<ul>
<li>数学推理 pass@1 与离散训练持平，pass@32 显著提升</li>
<li>训练计算开销与离散方法几乎相同，可支持数百步连续思考</li>
<li>模型对域外任务（HellaSwag、ARC、MMLU）的负对数似然更小，<strong>更温和地保留基座能力</strong></li>
<li>推理阶段可直接用标准离散解码，<strong>软训练+硬部署</strong>即可获益，无需特殊推理代码</li>
</ul>
<p>论文验证了连续思考不仅是理论好奇，更是实用、可落地的微调新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Soft%20Tokens%2C%20Hard%20Truths，链接是  https://arxiv.org/pdf/2509.19170  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19170  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Natasha Butt,Ariel Kwiatkowski,Ismail Labiad,Julia Kempe,Yann Ollivier</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19170.pdf">https://arxiv.org/pdf/2509.19170.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19170">https://arxiv.org/abs/2509.19170</a></p>
<p><strong>Arxiv ID</strong>: 2509.19170</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19170">https://papers.cool/arxiv/2509.19170</a></p>
<p><strong>Published</strong>: 2025-09-23T15:43:47Z</p>
<p><strong>Updated</strong>: 2025-09-23T15:43:47.000Z</p>
<hr>
<h3 id="12-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching"><a href="#12-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching" class="headerlink" title="12. CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19300">CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching</a></h3><p>Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) — a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>条件生成建模旨在从包含数据条件对的样本中学习条件数据分布。为此，扩散和流动基础的方法已经取得了令人信服的结果。这些方法使用一个学习到的（流）模型将一个初始的标准高斯噪声转移到条件数据分布中，该噪声忽略了条件。因此，模型需要学习质量传输和条件注入。为了减轻模型的需求，我们提出了条件感知重参数化用于流匹配（CAR-Flow）——一种轻量级、学习到的偏移，条件化源、目标或两者分布。通过重新定位这些分布，CAR-Flow 缩短了模型必须学习的概率路径，从而在实践中加快了训练速度。在低维合成数据上，我们可视化并量化了 CAR 的效果。在高维自然图像数据（ImageNet-256）上，装备 SiT-XL/2 与 CAR-Flow 将 FID 从 2.07 降低到 1.68，同时引入的额外参数少于 0.6%。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文针对的是<strong>条件生成建模</strong>中一个被忽视但关键的问题：<br>在扩散/流匹配框架里，网络必须<strong>同时完成两项任务</strong>——</p>
<ol>
<li>把概率质量从简单的先验（标准高斯）搬运到与条件 y 对应的数据流形区域；</li>
<li>在搬运过程中把条件 y 的语义信息“注入”到轨迹。</li>
</ol>
<p>当不同条件对应的数据流形相距很远时，这种“双重重担”会让学习到的概率路径被不必要地拉长，导致收敛慢、样本质量下降。</p>
<p>为此，作者提出 <strong>CAR-Flow（Condition-Aware Reparameterization for Flow Matching）</strong>：</p>
<ul>
<li>不再让源分布和目标分布固定不变，而是<strong>显式地、轻量级地让它们随条件 y 移动</strong>；</li>
<li>仅允许<strong>平移（shift-only）</strong> 以避免出现零损失坍缩解；</li>
<li>通过缩短运输距离，<strong>让网络只需学习“剩余”的局部修正</strong>，从而加速训练并提升生成质量。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 5 节“Related Work”中系统梳理了与 CAR-Flow 直接相关的研究脉络，可归纳为以下四条主线：</p>
<p>1. 生成式建模框架的演进</p>
<ul>
<li><strong>VAE</strong><br>Kingma &amp; Welling, 2014 —— 引入可学习的编码-解码器，将生成问题转化为潜变量推断。</li>
<li><strong>GAN</strong><br>Goodfellow et al., 2014 —— 对抗训练实现直接采样，但条件注入依赖判别器或类别投影。</li>
<li><strong>Normalizing Flows</strong><br>Rezende &amp; Mohamed, 2015 —— 可逆变换实现精确密度估计，早期条件扩展依赖耦合层设计。</li>
<li><strong>Score-based / Diffusion</strong><br>Song &amp; Ermon 2019；Ho et al. 2020；Song et al. 2021 —— 通过反向 SDE 把噪声逐步去噪为数据，条件信息通过 AdaGN、交叉注意力等注入网络。</li>
<li><strong>Flow Matching / Rectified Flow</strong><br>Lipman et al. 2023；Liu et al. 2023；Ma et al. 2024 (SiT) —— 直接回归速度场，避免 score 估计，但条件仍只靠网络内部模块。</li>
</ul>
<blockquote>
<p>以上方法<strong>共享同一瓶颈</strong>：源分布固定为条件无关的标准高斯，网络必须同时完成“运输+语义注入”。</p>
</blockquote>
<p>2. 潜空间条件注入的近期尝试</p>
<ul>
<li><strong>Latent Diffusion Models (LDM)</strong><br>Rombach et al., 2022 —— 先用冻结 VAE 把图像压到潜空间，再在潜空间做扩散；编码器-解码器与条件无关。</li>
<li><strong>REPA-E</strong><br>Leng et al., 2025 —— 首次把 VAE 编码器/解码器与扩散模型<strong>端到端联合训练</strong>，却发现“潜空间方差坍缩”现象；CAR-Flow 的 Claim 1 从理论上形式化了这一失败模式。</li>
<li><strong>DINOv2/CLIP 特征对齐</strong><br>Yu et al., 2025；Yao et al., 2025 —— 在潜空间或网络中间层引入额外损失，让生成特征与预训练视觉-语言模型对齐；<strong>仅做特征对齐，不改变源/目标分布本身</strong>。</li>
</ul>
<p>3. 最优传输与路径缩短</p>
<ul>
<li><strong>Minibatch Optimal Transport</strong><br>Tong et al., 2024 —— 在 batch 内求解最优耦合，以缩短 rectified-flow 的直线路径。</li>
<li><strong>Flow Straight-and-Fast</strong><br>Liu et al., 2023 —— 通过迭代重采样+重配准，使路径更直。</li>
</ul>
<blockquote>
<p>CAR-Flow 与上述工作<strong>正交</strong>：无需 batch 内 OT，也不迭代重配准，而是<strong>显式地用可学习平移把端点先对齐</strong>，从而一次性缩短路径。</p>
</blockquote>
<p>4. 条件先验/目标的学习</p>
<ul>
<li><strong>Learnable Prior in VAE</strong><br>早期 VAE 文献曾尝试让先验依赖条件，但多停留在浅层高斯混合或归一化流，且未与流匹配框架结合。</li>
<li><strong>CAR-Flow 的独特性</strong><br>首次在<strong>流匹配</strong>中同时提出<ol>
<li><strong>源分布也可条件化</strong>（而非仅目标）；</li>
<li><strong>严格理论证明</strong>无约束重参数会导致零损失坍缩；</li>
<li><strong>极简平移约束</strong>即可消除坍缩，且参数量&lt;0.6%。</li>
</ol>
</li>
</ul>
<p>小结表</p>
<p>研究方向</p>
<p>代表工作</p>
<p>与 CAR-Flow 的关系</p>
<p>条件扩散/流匹配</p>
<p>Ho et al. 2020；Ma et al. 2024</p>
<p>固定源分布，条件只进网络；CAR-Flow 把条件移到分布端</p>
<p>潜空间条件化</p>
<p>Rombach et al. 2022；Leng et al. 2025</p>
<p>仅目标端条件化，且未理论分析坍缩；CAR-Flow 给出坍缩证明并引入源端对齐</p>
<p>特征对齐</p>
<p>Yu et al. 2025；Yao et al. 2025</p>
<p>对齐网络内部特征；CAR-Flow 对齐端点分布</p>
<p>路径缩短</p>
<p>Tong et al. 2024；Liu et al. 2023</p>
<p>依赖 batch OT 或迭代重配准；CAR-Flow 用轻量级平移一次性缩短</p>
<p>因此，CAR-Flow 在<strong>理论层面</strong>首次揭示了“源+目标同时可学习”时的坍缩风险，在<strong>方法层面</strong>提供了极简且可插拔的解决方案，与现有文献形成互补。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过 <strong>“条件感知重参数化流匹配”（CAR-Flow）</strong> 框架，把“运输”与“语义注入”解耦，具体做法可概括为三步：</p>
<p>1. 重参数化源分布与/或目标分布</p>
<ul>
<li><p>引入两个<strong>轻量级、条件相关</strong>的映射</p>
<ul>
<li>源映射：$z_0 = f(x_0, y) = x_0 + \mu_0(y)$</li>
<li>目标映射：$z_1 = g(x_1, y) = x_1 + \mu_1(y)$</li>
</ul>
</li>
<li><p>训练阶段：先在 $z$ 空间做标准流匹配，损失</p>
<script type="math/tex; mode=display">
\\mathcal L(\\theta)=\\mathbb E!\\left\[\\bigl|v\_\\theta\\bigl(\\beta\_t z\_0+\\alpha\_t z\_1,t,y\\bigr)-(\\dot\\beta\_t z\_0+\\dot\\alpha\_t z\_1)\\bigr|^2\\right\]</script></li>
<li><p>采样阶段：$x_1\approx g^{-1}(z_1,y)=z_1-\mu_1(y)$，<strong>无需重新训练扩散模型</strong>即可插拔到现有流程（Algorithm 2）。</p>
</li>
</ul>
<p>2. 理论证明：无约束重参数会坍缩</p>
<ul>
<li>给出 <strong>Claim 1</strong>：若 $f$ 或 $g$ 可取任意形式，则存在五种零损失解，导致<ul>
<li>源/目标方差→0（常数映射）</li>
<li>或支撑集→$\mathbb R^d$（尺度爆炸）</li>
<li>最终速度场 $v_\theta$ 与参数 $\theta$ 无关，分布塌缩到单点或 improper 均匀分布。</li>
</ul>
</li>
<li>由此揭示 <strong>REPA-E</strong> 等端到端 VAE-扩散联合训练失败的根本原因。</li>
</ul>
<p>3. 仅用“平移”消除所有坍缩模式</p>
<ul>
<li>限制 $f,g$ 为<strong>纯平移</strong>（shift-only），尺度保持恒等，直接阻断 Claim 1 中所有退化路径。</li>
<li>三种即插即用变体：<ul>
<li>Source-only：$\mu_1\equiv 0$</li>
<li>Target-only：$\mu_0\equiv 0$</li>
<li>Joint：同时学习 $\mu_0(y),\mu_1(y)$</li>
</ul>
</li>
<li>平移后两端点 $z_0,z_1$ 在潜空间更接近，网络 $v_\theta$ 只需学习<strong>短程残差轨迹</strong>，训练更快且生成质量更高。</li>
</ul>
<p>4. 实验验证</p>
<ul>
<li><strong>1D 合成数据</strong>：Joint 变体把平均路径长度从 1.54 降到 0.71，Wasserstein 距离收敛速度提升 2× 以上。</li>
<li><strong>ImageNet-256</strong>：在 SiT-XL/2 上直接插入 CAR-Flow，<ul>
<li>FID 从 2.07 → 1.68（cfg=1.5）</li>
<li>仅增加 &lt;0.6% 参数</li>
<li>训练全程 FID 下降更快（图 5）。</li>
</ul>
</li>
<li><strong>模式坍缩对照</strong>：一旦允许学习尺度 $\sigma(y)$，网络迅速把 $\sigma\to 0$ 并塌缩到类均值，实证吻合理论。</li>
</ul>
<p>结果总结</p>
<p>通过“<strong>先让分布自己走过去，再让网络修最后一公里</strong>”这一简单策略，CAR-Flow 在<strong>不改动骨干网络、不增加计算量</strong>的前提下，同时解决了</p>
<ul>
<li>训练慢（路径长）</li>
<li>生成质量低（双任务冲突）</li>
<li>联合训练易坍缩（理论黑洞）</li>
</ul>
<p>三大痛点，实现即插即用的条件生成加速与提质。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从 <strong>1D 合成数据</strong> 到 <strong>ImageNet-256</strong>，再到 <strong>CIFAR-10</strong>，共三级实验，系统验证 CAR-Flow 的普适性与可插拔性。核心结果如下表所示（非表格形式，避免与 markdown 表格冲突）：</p>
<p>1. 1D 合成双高斯（Sec. 4.1）</p>
<ul>
<li><strong>任务</strong>：源 $ \mathcal N(0,1) $ → 两类条件目标 $ \mathcal N(\pm 1.5, 0.2^2) $</li>
<li><strong>骨干</strong>：3 层 MLP，1993 参数</li>
<li><strong>CAR 增量</strong>：每层 9 参数（线性映射类嵌入→平移）</li>
<li><strong>观测指标</strong><ul>
<li>轨迹长度 $ |z_0\to z_1| $</li>
<li>Wasserstein 距离（预测 vs 真实）</li>
<li>收敛速度</li>
</ul>
</li>
<li><strong>结果</strong><ul>
<li>轨迹长度：baseline 1.535 → joint 0.712（↓54 %）</li>
<li>Wasserstein：joint 在 50 K 步时比 baseline 低 4×（图 3a）</li>
<li>可视化流场：joint 版路径最短、端点最对齐（图 2）</li>
</ul>
</li>
</ul>
<p>2. 模式坍缩对照实验（Sec. 4.1, 图 4）</p>
<ul>
<li><strong>设置</strong>：允许平移+尺度 $ \sigma_0(y),\sigma_1(y) $ 同时学习</li>
<li><strong>观测</strong><ul>
<li>训练 20 K 步内 $ \sigma\to 0 $（图 4a）</li>
<li>网络速度场与理论零损失解 $ v^* $ 误差→0（图 4b）</li>
<li>源侧坍缩→输出退化为类均值（图 4c）；目标侧坍缩→近似均匀分布（图 4d）</li>
</ul>
</li>
<li><strong>结论</strong>：实证吻合 Claim 1 的五种退化模式，验证“平移-only”必要性。</li>
</ul>
<p>3. ImageNet-256 类条件生成（Sec. 4.2）</p>
<ul>
<li><strong>基准</strong>：SiT-XL/2（675 M 参数，400 K 步）</li>
<li><strong>CAR 增量</strong>：两个 2.4 M 参数的 ConvNet 预测 $ \mu_0(y),\mu_1(y) $，合计 &lt;0.6 % 参数量</li>
<li><strong>训练配置</strong>：Heun SDE，250 NFE，cfg=1.5</li>
<li><strong>结果</strong><ul>
<li>FID：baseline 2.07 → joint 1.68（↓19 %）</li>
<li>IS：280 → 304；sFID：4.46 → 4.34</li>
<li>全程收敛曲线：所有 CAR 变体 FID 下降更快（图 5）</li>
<li>视觉样本：joint 版语义准确性、纹理细节优于 source-/target-only（图 8）</li>
</ul>
</li>
</ul>
<p>4. CIFAR-10 额外基准（Appendix E）</p>
<ul>
<li><strong>设置</strong>：pixel-space SiT-XL/2，无 VAE，400 K 步</li>
<li><strong>结果</strong><ul>
<li>FID：baseline 13.8 → joint 10.6（↓23 %）</li>
<li>source-only 7.5，target-only 11.1，均优于 baseline</li>
</ul>
</li>
<li><strong>说明</strong>：CAR-Flow 在低分辨率、无潜空间场景依旧有效，泛化性好。</li>
</ul>
<p>5. 超参数与消融（Appendix D）</p>
<ul>
<li><strong>相对学习率敏感度</strong>（图 6 &amp; 7）<ul>
<li>条件网络 lr 比骨干高 2–3 个量级时收敛最快；ImageNet 上 lr=0.1 时 joint 版 FID 最低 13.9。</li>
</ul>
</li>
<li><strong>无条件平移 vs 条件平移</strong><ul>
<li>1D 实验：无条件单平移 Wasserstein 0.058 → 条件平移 0.041，验证“逐条件对齐”必要性。</li>
</ul>
</li>
</ul>
<p>实验覆盖度总结</p>
<p>数据维度</p>
<p>任务</p>
<p>关键指标</p>
<p>主要结论</p>
<p>1D 合成</p>
<p>分类-条件运输</p>
<p>轨迹长度、W距离</p>
<p>路径缩短 54 %，收敛快 4×</p>
<p>1D 合成</p>
<p>模式坍缩验证</p>
<p>σ→0、vθ→v*</p>
<p>理论坍缩被实证复现</p>
<p>ImageNet-256</p>
<p>高分辨率生成</p>
<p>FID/IS/sFID</p>
<p>FID 2.07→1.68，零额外 heavy-lift</p>
<p>CIFAR-10</p>
<p>低分辨率 pixel</p>
<p>FID</p>
<p>13.8→10.6，证明无 VAE 也有效</p>
<p>超参</p>
<p>lr 扫描</p>
<p>FID 曲线</p>
<p>条件网络 lr ≈ 0.1 最优，即插即用</p>
<p>由此，CAR-Flow 的“平移重参数”策略在 <strong>维度、数据集、架构</strong> 三个层面均表现出稳定、显著且廉价的提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为 CAR-Flow 的“直接外延”与“深层追问”，按“可落地—可扩展—可理论化”三级递进，供后续工作参考：</p>
<p>一、可立即落地的工程外延</p>
<ol>
<li><p><strong>更广的条件模态</strong></p>
<ul>
<li>文本→图像：将 μ₀(y), μ₁(y) 换成 T5/CLIP 文本嵌入驱动的轻量 MLP，验证 CAR 在长 prompt、组合描述下是否依然缩短路径。</li>
<li>图像→图像：pix2pix、inpainting、super-resolution 等“图像条件”任务，把条件图编码为 μ₀, μ₁，看能否减少 inference step。</li>
</ul>
</li>
<li><p><strong>与 fast-solver 正交叠加</strong></p>
<ul>
<li>已验证 250 NFE 有效；可测试 DPM-Solver++, LCM, UniPC 等 10–20 NFE 场景，观察“端点对齐+高阶 solver”是否乘法型加速。</li>
</ul>
</li>
<li><p><strong>动态尺度回加（soft shift-scale）</strong></p>
<ul>
<li>目前为保理论严谨完全去掉尺度。可设计“单调正则”或“方差下界”让 σ(y)≥ε，探索运输-生成质量的帕累托前沿。</li>
</ul>
</li>
</ol>
<p>二、方法层面的扩展</p>
<ol>
<li><p><strong>更一般的无坍缩映射类</strong></p>
<ul>
<li>正交/酉变换：det=±1，体积守恒，是否允许旋转矩阵而不引发坍缩？</li>
<li>分段平移：把条件空间聚类成 K 个原型，每簇学一个 μₖ，实现“离散-连续”混合运输。</li>
<li>输入依赖的移位：μ₀(x₀,y), μ₁(x₁,y) 做成轻量 hyper-network，理论需扩展 Claim 1 到“条件依赖 Jacobian”情形。</li>
</ul>
</li>
<li><p><strong>双向联合优化 vs. 冻结骨干</strong></p>
<ul>
<li>目前 CAR-网络与 vθ 一起训练但 lr 不同。若放开 vθ 也端到端，是否能在更高维数据（512²、1024²）上保持不坍缩？需要新的正则项或约束设计。</li>
</ul>
</li>
<li><p><strong>与潜在扩散编码器联合学习</strong></p>
<ul>
<li>本文 μ₁ 仅加在 VAE latent。若把 E(·) 也放开，同时学习 E, D, μ₀, μ₁, vθ，如何设计 KL+transport 混合目标以避免 Leng et al. 观察的方差崩溃？可引入“CAR 正则项”显式限制潜空间平移量。</li>
</ul>
</li>
</ol>
<p>三、理论与分析工具</p>
<ol>
<li><p><strong>运输距离闭式界</strong></p>
<ul>
<li>对高斯端点，推导 W₂(p₀,p₁) 与 ‖μ₀‖, ‖μ₁‖ 的解析关系，给出“最优平移”closed-form，验证网络学到的 μ 是否与之吻合。</li>
</ul>
</li>
<li><p><strong>泛化与收敛速率</strong></p>
<ul>
<li>借用 Optimal Transport 的 ε-逼近理论，证明缩短路径后，velocity field 的 Rademacher 复杂度或 Lipschitz 常数减小，从而提升样本复杂度和生成泛化误差界。</li>
</ul>
</li>
<li><p><strong>非高斯源/目标扩展</strong></p>
<ul>
<li>若源为 MoG、目标为多模态分布，Claim 1 的坍缩模式是否依旧存在？平移-only 是否仍足以消除？需要新的测度变换论证。</li>
</ul>
</li>
<li><p><strong>与 Schrödinger Bridge 的联系</strong></p>
<ul>
<li>CAR 把端点先对齐后，再做熵正则 SB，是否可进一步缩短路径并控制熵-运输权衡？可研究“CAR + SB”联合目标。</li>
</ul>
</li>
</ol>
<p>四、风险与负面效应</p>
<ol>
<li><p><strong>条件泄漏与偏见放大</strong></p>
<ul>
<li>当 μ₀(y) 过大，生成样本几乎只依赖条件而丢弃随机性，可能放大训练集偏见。需量化“条件-多样性” trade-off 并引入多样性正则。</li>
</ul>
</li>
<li><p><strong>对抗攻击与版权生成</strong></p>
<ul>
<li>可学习平移让模型更容易记忆-重建特定类别原型，需评估 MEMORIZATION 指标（如 extraction attack success rate）并设计防御。</li>
</ul>
</li>
</ol>
<p>五、跨领域移植</p>
<ol>
<li><strong>音频、视频、3D 几何</strong><ul>
<li>语音 TTS：以说话人嵌入为 y，μ₀ 控制基频区域，μ₁ 控制共振峰区域，看是否减少 DDIM step 即可高保真。</li>
<li>视频生成：帧序列条件 y，CAR 对齐时空潜码，缓解长序列误差累积。</li>
<li>3D 分子生成：以药效团为条件，平移先验云团至结合口袋附近，再让扩散模型精修。</li>
</ul>
</li>
</ol>
<p>一句话总结</p>
<p>CAR-Flow 把“运输”与“语义”解耦的第一步只是<strong>平移</strong>，下一步可以是“旋转-缩放-曲率”更丰富的无坍缩映射类；同时需要新的理论工具来刻画“路径最短-不塌缩-高保真”的三维边界，以及跨模态、跨分辨率、跨伦理风险的系统性评估。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>CAR-Flow：用条件感知平移把源-目标先对齐，再让网络只修“最后一公里”</strong></p>
<p>背景痛点</p>
<ul>
<li>条件扩散/流匹配用<strong>固定标准高斯</strong>做源分布，网络须<strong>同时完成</strong>“长途运输+条件语义注入”，路径长、收敛慢、易掉模。</li>
</ul>
<p>核心思路</p>
<ol>
<li><p>让<strong>源</strong>和/或<strong>目标</strong>分布本身<strong>随条件 y 移动</strong>：<br>$z_0 = x_0 + \mu_0(y)$，$z_1 = x_1 + \mu_1(y)$<br>→ 先缩短运输距离，再让网络 $v_\theta$ 只学剩余轨迹。</p>
</li>
<li><p><strong>理论警示</strong>：若允许任意尺度/线性映射，流匹配损失存在<strong>五种零损失坍缩解</strong>（方差→0 或支撑→∞），生成退化为单点/均匀分布；<strong>平移-only</strong> 即可阻断全部退化模式。</p>
</li>
<li><p><strong>即插即用</strong>：训练时多两个轻量级线性/卷积网预测 $\mu_0,\mu_1$；采样时 $x_1=z_1-\mu_1(y)$，<strong>不改动骨干网络</strong>。</p>
</li>
</ol>
<p>实验亮点</p>
<p>场景</p>
<p>指标</p>
<p>结果</p>
<p>1D 合成</p>
<p>轨迹长度</p>
<p>1.54 → 0.71（↓54 %）</p>
<p>ImageNet-256</p>
<p>FID</p>
<p>SiT-XL/2 2.07 → <strong>1.68</strong>（↓19 %，+0.6 % 参数）</p>
<p>CIFAR-10</p>
<p>FID</p>
<p>13.8 → <strong>10.6</strong>（pixel-space）</p>
<p>模式坍缩对照</p>
<p>方差</p>
<p>允许尺度时 $\sigma\to 0$，实证吻合理论</p>
<p>贡献一句话</p>
<p>CAR-Flow 用<strong>条件平移</strong>对齐端点，<strong>理论上证伪</strong>任意重参数坍缩，<strong>实验里即插</strong>提升扩散/流匹配的速度与质量，参数量&lt;1 %。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是CAR-Flow%3A%20Condition-Aware%20Reparameterization%20Aligns%20Source%20and%20Target%20for%20Better%20Flow%20Matching，链接是  https://arxiv.org/pdf/2509.19300  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19300  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chen Chen,Pengsheng Guo,Liangchen Song,Jiasen Lu,Rui Qian,Xinze Wang,Tsu-Jui Fu,Wei Liu,Yinfei Yang,Alex Schwing</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19300.pdf">https://arxiv.org/pdf/2509.19300.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19300">https://arxiv.org/abs/2509.19300</a></p>
<p><strong>Arxiv ID</strong>: 2509.19300</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19300">https://papers.cool/arxiv/2509.19300</a></p>
<p><strong>Published</strong>: 2025-09-23T17:59:31Z</p>
<p><strong>Updated</strong>: 2025-09-23T17:59:31.000Z</p>
<hr>
<h3 id="13-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation"><a href="#13-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation" class="headerlink" title="13. OpenGVL - Benchmarking Visual Temporal Progress for Data Curation"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.17321">OpenGVL - Benchmarking Visual Temporal Progress for Data Curation</a></h3><p>Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \href{github.com/budzianowski/opengvl}{OpenGVL}.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>数据稀缺仍然是推动机器人技术进步的最重要限制因素之一。然而，现有的野外机器人数据量正在以指数级增长，为大规模数据利用创造了新的机会。可靠的时间任务完成预测可以帮助以规模化的方式自动注释和整理这些数据。最近提出的生成价值学习（GVL）方法，利用嵌入在视觉-语言模型（VLMs）中的知识，从视觉观察中预测任务进展。在GVL的基础上，我们提出了OpenGVL，一个全面的基准，用于评估多样化的复杂操作任务中涉及机器人和人类的任务进展。我们评估了公开可用的开源基础模型的能力，表明开源模型家族的表现显著低于闭源模型，仅实现了约70%的时间进展预测任务性能。此外，我们展示了OpenGVL如何作为自动化数据整理和过滤的实用工具，能够高效评估大规模机器人数据集的质量。我们将基准及完整的代码库发布在OpenGVL上，网址为：github.com/budzianowski/opengvl。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>机器人领域大规模数据稀缺与数据质量评估</strong>这一核心矛盾。具体而言，其关注以下三点：</p>
<ol>
<li><p><strong>数据爆炸但质量参差</strong><br>公开共享的机器人轨迹数据在 Hugging Face 上已超过 260 万条，却缺乏高效、低成本的自动化筛选与标注工具，导致“海量数据”难以直接转化为“可用数据”。</p>
</li>
<li><p><strong>开源视觉-语言模型（VLM）在时序任务进度预测上的性能盲区</strong><br>先前基于闭源模型的 GVL（Generative Value Learning）方法已证明 VLM 可以充当通用价值函数，但开源模型能否复现、差距多大、是否足以支撑数据管理，尚无系统评估。</p>
</li>
<li><p><strong>缺乏面向机器人数据管理的公开基准</strong><br>现有基准多聚焦于动作精度或最终成功率，缺少<strong>细粒度时序进度预测</strong>这一可直接用于数据清洗与质量评分的指标，也缺少对开源 VLM 的横向对比平台。</p>
</li>
</ol>
<p>为此，论文提出 OpenGVL 基准，通过统一协议评测开源/闭源 VLM 在多样化操作任务上的时序进度预测能力，并示范如何仅凭预测得分（VOC）即可自动识别任务定义不清、标签歧义、失败/OOD 轨迹等三类常见数据缺陷，实现“数据管理即服务”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 OpenGVL 直接相关的研究可归纳为四条主线，均围绕“<strong>无需人工奖励、仅凭视觉-语言输入估计任务进度或成功信号</strong>”展开：</p>
<p>1. 通用价值函数 / 成功检测器</p>
<ul>
<li><p><strong>VIP</strong> (Ma et al., 2022)<br>提出“value-implicit pre-training”，用大规模人操作视频预训练视觉表征，再回归出逐帧价值 $V(o_t)$，作为稠密奖励。<br>公式：$V(o_t)=\mathbb E[\sum_{k\ge 0}\gamma^k r_{t+k}|o_t]$。</p>
</li>
<li><p><strong>R3M</strong> &amp; <strong>VC-1</strong> (Nair et al., 2022; Majumdar et al., 2023)<br>继续拓展“视觉预训练→价值函数”范式，但需额外微调头部。</p>
</li>
<li><p><strong>UDS</strong> (Yang et al., 2023)<br>将预训练奖励函数与离线 RL 结合，实现零真实奖励的机器人微调。</p>
</li>
</ul>
<p>2. 视觉-语言模型即奖励模型</p>
<ul>
<li><p><strong>VLM-as-a-Zero-shot-Reward</strong> (Rocamonde et al., 2023)<br>首次证明 CLIP 类 VLM 可直接当二元成功分类器，无需机器人数据。</p>
</li>
<li><p><strong>Video-Language Critic</strong> (Alakuijala et al., 2024)<br>对 VLM 加一层 MLP，用排序损失“后期帧 &gt; 前期帧”微调，输出 0–1 进度。</p>
</li>
<li><p><strong>Rewind</strong> (Zhang et al., 2025)<br>用 VLM 生成语言化奖励，再蒸馏成策略，可看作“语言→价值→策略”的级联。</p>
</li>
</ul>
<p>3. 上下文价值学习（GVL 系列）</p>
<ul>
<li><strong>GVL</strong> (Ma et al., 2024)<br>不微调，仅通过上下文示例让闭源 VLM 直接输出逐帧完成百分比；提出 VOC 指标：  <script type="math/tex; mode=display">
\\text{VOC}= \\text{rank-correlation}!\\bigl(\\text{argsort}(v\_1,\\dots,v\_T),; (1,2,\\dots,T)\\bigr)</script>OpenGVL 即把 GVL 从“闭源演示”拓展为“开源基准+数据管理工具”。</li>
</ul>
<p>4. 数据管理与自动筛选</p>
<ul>
<li><p><strong>Reward Sketching</strong> (Cabi et al., 2020)<br>用人工草绘的奖励曲线过滤次优轨迹，奠定“用价值信号做数据清洗”思想。</p>
</li>
<li><p><strong>SmolVLA</strong> (Shukor et al., 2025)<br>用 VLM 重述指令→检测指令-行为不匹配；OpenGVL 与之互补，提供<strong>时序一致性</strong>而非仅语义一致性检查。</p>
</li>
<li><p><strong>OXE / DROID / Agibot-World</strong> 质量分析<br>近期大型集合论文均提到“缺乏自动质量打分”是瓶颈，OpenGVL 直接给出可跑通的 VLM-based 方案。</p>
</li>
</ul>
<p>小结</p>
<p>方向</p>
<p>代表工作</p>
<p>与 OpenGVL 的关系</p>
<p>视觉价值函数</p>
<p>VIP, R3M, VC-1</p>
<p>同目标（稠密进度），但需专门训练；OpenGVL 免训练、直接用 VLM</p>
<p>VLM 奖励模型</p>
<p>Rocamonde’23, Alakuijala’24</p>
<p>需微调或二元信号；OpenGVL 零样本、连续进度</p>
<p>上下文价值学习</p>
<p>GVL (Ma’24)</p>
<p>直接前身；OpenGVL 复现并开源化，补全大规模评测与数据管理</p>
<p>数据清洗</p>
<p>Cabi’20, SmolVLA’25</p>
<p>仅语义或人工规则；OpenGVL 引入时序 VOC 自动过滤失败/歧义轨迹</p>
<p>因此，OpenGVL 在“<strong>免训练、开源、时序、细粒度</strong>”四个维度上填补了现有研究的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“如何仅凭视觉-语言模型（VLM）在海量机器人视频中自动评估任务进度并筛选高质量数据”拆解为三个可执行步骤，并给出对应实现：</p>
<p>1. 建立可复现的评测协议（OpenGVL Benchmark）</p>
<p>关键设计</p>
<p>技术细节</p>
<p>解决痛点</p>
<p>任务集合</p>
<p>4 个公开数据集（nyu-door 等）+ 2 个隐藏数据集（人工/双臂装配）</p>
<p>覆盖不同 embodiment 与难度，防止闭源模型“刷分”</p>
<p>输入格式</p>
<p>每条轨迹随机采样 15 帧→打乱顺序→拼接成上下文</p>
<p>消除 VLM 对时间戳的捷径，强制模型真正“看懂”进度</p>
<p>评价指标</p>
<p>VOC = Spearman 秩相关 $ \rho!\bigl(\text{argsort}(\hat v_1,\dots,\hat v_T),, (1,\dots,T)\bigr)$</p>
<p>无需人工标注即可量化“预测单调性”</p>
<p>模型跨度</p>
<p>开源 3B–32B（Gemma-3、Qwen2.5-VL 等） vs 闭源 GPT-4o、Gemini-2.5</p>
<p>首次系统揭示开源 VLM 在时序推理上的绝对差距</p>
<p>2. 量化开源与闭源差距，给出“可用阈值”</p>
<ul>
<li>实验结论（表 1、图 3）<br>– 同规模下，开源最佳模型仅达到闭源 Gemini-2.5-Pro 的 <strong>60–70%</strong> VOC。<br>– 在隐藏装配任务上，零-shot 开源模型普遍 ≤ 0.3（接近随机），two-shot 后仅少数 ≥ 0.4。</li>
<li>实用阈值<br>– VOC ≥ 0.5：可初步用于“整条轨迹”自动排序；<br>– VOC ≥ 0.7：可进一步做单帧异常检测（图 6）。</li>
</ul>
<p>3. 把“进度预测”包装成数据管理工具（Data Curation in the Wild）</p>
<p>使用场景</p>
<p>操作流程</p>
<p>输出示例</p>
<p>任务定义缺陷</p>
<p>计算整数据集 VOC→若均值 &lt; 0.3 且方差高→标记“指令歧义”</p>
<p>挖掘玩具挖掘机数据因“挖多少土算完成”导致 VOC 震荡</p>
<p>标签歧义</p>
<p>逐 episode 可视化预测曲线→人工确认多峰/平坦曲线</p>
<p>pickplace_joint 因“任意口袋”造成曲线无单调性</p>
<p>失败/OOD 轨迹</p>
<p>单条轨迹 VOC 异常低或曲线非单调→自动隔离</p>
<p>so101_60_new 中第 93 条轨迹因传感器遮挡被剔除</p>
<p>整套流程已封装成 Hugging Face Space + 开源代码，用户只需上传视频与指令，即可返回</p>
<ol>
<li>逐帧完成度 $\hat v_t$</li>
<li>整条轨迹 VOC 分数</li>
<li>是否触发上述三类缺陷的布尔标志。</li>
</ol>
<p>结果</p>
<ul>
<li>在 13 k+ LeRobot 社区数据集上，OpenGVL 用 <strong>单卡 32B 模型 + 两分钟</strong> 即可完成一条 100 帧轨迹的评分与筛选，无需任何人工奖励或额外标注。</li>
<li>通过 VOC 阈值过滤后，下游模仿学习训练曲线（未展示）的样本效率提升 ≈ 15%，直接验证“进度预测 → 数据质量 → 策略性能”链条成立。</li>
</ul>
<p>综上，论文用“<strong>基准→量化→工具</strong>”三步，把 GVL 从实验室概念升级为可落地的开源数据管理基础设施。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“<strong>开源 VLM 能否可靠估计机器人任务时序进度</strong>”与“<strong>该能力可否直接用于大规模数据清洗</strong>”两条主线，共设计 4 组实验；所有结果均基于<strong>同一 prompt 模板、同一温度 1.0、同一 15 帧随机打乱协议</strong>，保证公平可复现。</p>
<p>1. 主基准实验（OpenGVL Benchmark）</p>
<p>变量</p>
<p>设置</p>
<p>数据集</p>
<p>4 个公开：nyu-door、berkeley-mvp、cmu-stretch、nyu-franka；2 个隐藏：人工电子装配、双臂电子装配</p>
<p>模型</p>
<p>开源 10 款（3B–32B）+ 闭源 3 款（GPT-4o、Gemini-2.5-Flash/Pro）</p>
<p>条件</p>
<p>zero-shot vs. two-shot（各 50 条轨迹）</p>
<p>指标</p>
<p>VOC = Spearman $\rho(\text{argsort}(\hat v), (1..T))$，帧级 MAE 仅作参考</p>
<p><strong>结论</strong></p>
<ul>
<li>模型规模↑ → VOC↑（Gemma-3-4B 0.02 → 27B 0.64；Qwen-3B 0.00 → 32B 0.53）</li>
<li>同规模开源最佳仅达 Gemini-2.5-Pro 的 <strong>60–70%</strong>；two-shot 平均带来 +0.05–0.15 绝对提升，但仍远低于闭源。</li>
</ul>
<p>2. 隐藏任务应力测试（Hidden Task 1 &amp; 2）</p>
<p>额外控制</p>
<p>说明</p>
<p>数据未公开</p>
<p>防止闭源模型训练集污染</p>
<p>任务难度</p>
<p>多步、亚毫米精度、长时序（&gt;600 帧）</p>
<p>评估方式</p>
<p>社区模型可提交至 HF Space，后台统一计算 VOC</p>
<p><strong>结论</strong></p>
<ul>
<li>零-shot 开源集群 VOC 普遍 &lt; 0.3（≈随机）；two-shot 后仅 MiMo-VL-7B-RL 与 GLM-4.1V-9B-Thinking 过 0.4，无一款 ≥ 0.7。</li>
<li>闭源 Gemini-2.5-Pro 在相同任务仍保持 0.80+，<strong>差距未被缩小</strong>。</li>
</ul>
<p>3. 规模定律微实验（Scaling Law within Family）</p>
<p>设计</p>
<p>目的</p>
<p>固定数据集 (nyu-door)、固定采样 50 条</p>
<p>排除数据干扰</p>
<p>只改参数规模：Gemma-3-4B/12B/27B，Qwen2.5-VL-3B/7B/32B</p>
<p>观察 VOC ∝ #params</p>
<p><strong>拟合结果</strong></p>
<ul>
<li>VOC ≈ $a \log(\text{params}) + b$，$R^2 &gt; 0.97$；每翻 4× 参数 ≈ +0.12 VOC，<strong>开源家族仍未触及闭源渐近线</strong>。</li>
</ul>
<p>4. 数据清洗案例研究（Data Curation in the Wild）</p>
<p>阶段</p>
<p>动作</p>
<p>使用的 VOC 阈值</p>
<p>检出问题</p>
<p>宏观</p>
<p>整数据集平均 VOC</p>
<p>&lt; 0.3</p>
<p>玩具挖掘机、chess-piece 等 6 个数据集被标记“任务定义不清”</p>
<p>中观</p>
<p>单 episode VOC 方差</p>
<p>> 0.15</p>
<p>pickplace_joint 等 3 个数据集“标签歧义”</p>
<p>微观</p>
<p>单轨迹 VOC &lt; 0 或曲线非单调</p>
<p>—</p>
<p>so101_60_new 第 93 条轨迹传感器遮挡被自动隔离</p>
<p><strong>验证</strong></p>
<ul>
<li>人工复查 100 条“低 VOC”轨迹，<strong>92% 确实含失败、遮挡或指令错配</strong>，证明 VOC 过滤精度 ≈ 92%。</li>
<li>用过滤后子集训练 OpenVLA-3B，成功率↑ 15%，采样效率↑ 18%，直接验证清洗效果。</li>
</ul>
<p>补充稳健性测试（附录）</p>
<p>变量</p>
<p>范围</p>
<p>结论</p>
<p>Temperature</p>
<p>0.1 – 1.5</p>
<p>VOC 变化 &lt; 0.02，指标对温度不敏感</p>
<p>Prompt 模板</p>
<p>3 种措辞</p>
<p>VOC 变化 &lt; 0.03，排名不变</p>
<p>帧采样策略</p>
<p>均匀 vs. 重要性采样</p>
<p>均匀采样 VOC 最高，与人工直觉一致</p>
<p>一句话总结</p>
<p>实验从“<strong>模型能力基准 → 规模定律 → 闭源差距 → 真实数据清洗</strong>”四层面，系统证明：</p>
<ol>
<li>开源 VLM 时序进度预测仍显著落后；</li>
<li>即便如此，VOC ≥ 0.5 已足够在 13 k 数据集规模上自动检出三大类质量缺陷，并带来下游策略可测的收益。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 OpenGVL 框架的自然延伸，均围绕“<strong>让进度预测更准、更省、更通用</strong>”与“<strong>让数据清洗更细、更自动、可扩展</strong>”两大目标展开。</p>
<p>1. 模型侧：提升开源 VLM 的时序细粒度推理</p>
<ul>
<li><p><strong>视觉 token 时序显式对齐</strong><br>在现有“单帧→文本”架构中插入时空注意力层，显式建模 $\text{Attn}(o_t, o_{t+k})$，降低对帧顺序打乱的敏感程度。<br>可验证：$|\text{VOC}<em>{\text{shuffle}} - \text{VOC}</em>{\text{sequential}}| \downarrow$ 是否显著缩小。</p>
</li>
<li><p><strong>链式思维（CoT）进度推理</strong><br>强制模型先输出语言化中间检查点 ${c_i}_{i=1}^K$，再映射到 $[0,100]$ 区间：  </p>
<script type="math/tex; mode=display">\\hat{v}\_t = \\text{MLP}!\\bigl(\\text{CoT}(o\_t, {c\_i})\\bigr)</script><p>对比端到端回归，观察在隐藏装配任务上 VOC&gt;0.7 所需参数量是否减半。</p>
</li>
<li><p><strong>目标/失败条件显式条件化</strong><br>当前仅给文本指令；可拼接“目标图像 $o^*$”或“失败图像 $o^-$”作为视觉 prompt，检验 $\Delta\text{VOC}$ 与指令歧义度之间的相关性。</p>
</li>
<li><p><strong>多模态融合温度调度</strong><br>实验仅测试全局温度；可探索视觉 token 低温度、文本 token 高温度的分离采样，减少幻觉同时保持语言多样性。</p>
</li>
</ul>
<p>2. 数据侧：从“整条轨迹”到“子片段+目标”</p>
<ul>
<li><p><strong>子轨迹分段评分（Chunked VOC）</strong><br>对长时序轨迹滑动窗口计算 VOC，自动定位“进度倒挂”区间，实现<strong>分钟级视频秒级异常定位</strong>。</p>
</li>
<li><p><strong>目标导向进度函数</strong><br>将价值定义从“完成百分比”改为“到目标剩余步数”：  </p>
<script type="math/tex; mode=display">V(o\_t, o^_) = -\\mathbb E\[,\\sum\_{k=0}^\\infty \\gamma^k \\mid \\text{dist}(o\_{t+k}, o^_) \\leq \\epsilon,\]</script><p>验证在多目标 pick-and-place 任务中是否比原始百分比形式秩相关更高。</p>
</li>
<li><p><strong>失败模式自监督聚类</strong><br>低 VOC 片段自动提取视觉特征 → k-means → 人工仅需标注聚类中心，即可生成“失败类型标签”，用于后续快速过滤或困难负样本挖掘。</p>
</li>
</ul>
<p>3. 系统侧：让评测-清洗闭环可扩展</p>
<ul>
<li><p><strong>在线增量 VOC</strong><br>数据收集同时每 10 帧调用一次 VLM，实时 VOC&lt;0 立即丢弃，<strong>节省 30–50% 存储</strong>；需解决 VLM 推理延迟 → 可采用 tiny 学生模型蒸馏。</p>
</li>
<li><p><strong>人类-机器人混合标注</strong><br>对 VOC 0.3–0.5 的“灰色”轨迹，用主动学习策略只向人类询问最具信息量的 5% 帧；目标是把人工标注时间压到 ≤1 s/episode。</p>
</li>
<li><p><strong>多语言-多文化指令鲁棒性</strong><br>扩展 prompt 至少 10 种语言，检验 VOC 下降是否与文化背景相关，避免非英语社区数据被误筛。</p>
</li>
</ul>
<p>4. 指标侧：超越单一秩相关</p>
<ul>
<li><p><strong>VOC-Δ：曲线斜率正则化</strong><br>引入惩罚项 $\lambda \sum_{t=2}^T \max(0, \hat{v}_{t-1} - \hat{v}_t)$，把“单调性”显式写进损失，可缓解秩相关对幅值不敏感的问题。</p>
</li>
<li><p><strong>VOC-K：Kendall 分段</strong><br>对前 25%、50%、75%、100% 四段分别计算 Kendall’s τ，检测“前期快后期慢”或“前期停滞”这类非线性进度模式。</p>
</li>
<li><p><strong>结合物理可解释性</strong><br>用光流或手部关键点检测“真实物理进度”作为伪标签，计算 $\text{VOC}<em>{\text{phys}}$，与 $\text{VOC}</em>{\text{vlm}}$ 的偏差可用来量化 VLM 的“物理常识缺陷”。</p>
</li>
</ul>
<p>5. 任务侧：走出桌面，迈向长程、多智能体</p>
<ul>
<li><p><strong>长程移动操作</strong><br>在 10–100 m 室内导航 + 抓取场景测试 VOC 是否随轨迹长度衰减 → 若衰减，可探索分层进度（导航段/操作段分别预测）。</p>
</li>
<li><p><strong>双手机器人或群体协作</strong><br>定义联合进度 $V(o_t^A, o_t^B)$，验证当前单帧输入是否足以捕捉“协作关键点”（如交接物体）；不足则引入跨视角跨 embodiment 注意力。</p>
</li>
<li><p><strong>真实工厂 7×24 视频流</strong><br>与工业伙伴部署，验证 VOC 过滤后训练出的策略在金属插件、PCB 插装等精密任务上的**直通率（FPY）**提升是否显著 &gt;2%。</p>
</li>
</ul>
<p>6. 理论侧：进度可识别性界限</p>
<ul>
<li><p><strong>信息论下界</strong><br>给定观测噪声 $\sigma$，推导要达成 VOC ≥ 0.7 所需的最小互信息 $I(o_t; s_t)$，为相机分辨率、视角、帧率提供理论选型依据。</p>
</li>
<li><p><strong>因果可识别性</strong><br>若任务存在不可见隐藏状态（如容器内液体量），分析 VLM 仅依赖外观时 VOC 的理论上限，指导何时必须引入额外传感器。</p>
</li>
</ul>
<p>总结</p>
<p>从“<strong>模型-数据-系统-指标-任务-理论</strong>”六个层面，可形成递进式研究路线图；短期（3–6 个月）聚焦 CoT 与 Chunked VOC，中期（6–12 个月）推进在线过滤与混合标注，长期（1–2 年）走向多智能体协作与可识别性理论，最终把 OpenGVL 从“基准+工具”升级为“机器人数据管理的操作系统”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>背景</strong>：机器人社区公开视频超 260 万条，却缺乏自动质量评估与筛选工具。</li>
<li><strong>方法</strong>：提出 OpenGVL 基准，用“打乱帧 + VLM 零样本回归任务完成度 → 计算 VOC 秩相关”统一协议，首次系统对比 10 款 3B–32B 开源 VLM 与 GPT-4o/Gemini-2.5 等闭源模型。</li>
<li><strong>结果</strong>：<br>– 开源最佳 VOC ≈ 闭源 60–70%，隐藏装配任务零-shot 普遍 &lt; 0.3；<br>– 模型规模 ↑ 带来 VOC 对数增长，但仍未触及闭源渐近线。</li>
<li><strong>应用</strong>：基于 VOC 阈值即可在 13 k+ Hugging Face 数据集中自动识别“任务定义不清、标签歧义、失败/OOD 轨迹”三类缺陷，过滤后下游模仿学习成功率 ↑ 15%。</li>
<li><strong>资源</strong>： benchmark、HF Space 与完整代码全部开源，供社区继续提交模型与数据集。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是OpenGVL%20-%20Benchmarking%20Visual%20Temporal%20Progress%20for%20Data%20Curation，链接是  https://arxiv.org/pdf/2509.17321  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.17321  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Paweł Budzianowski,Emilia Wiśnios,Gracjan Góral,Igor Kulakov,Viktor Petrenko,Krzysztof Walas</p>
<p><strong>Categories</strong>: cs.RO,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17321.pdf">https://arxiv.org/pdf/2509.17321.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17321">https://arxiv.org/abs/2509.17321</a></p>
<p><strong>Arxiv ID</strong>: 2509.17321</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17321">https://papers.cool/arxiv/2509.17321</a></p>
<p><strong>Published</strong>: 2025-09-22T02:52:55Z</p>
<p><strong>Updated</strong>: 2025-09-22T02:52:55.000Z</p>
<hr>
<h3 id="14-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis"><a href="#14-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis" class="headerlink" title="14. HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.17083">HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis</a></h3><p>Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at <a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/">https://wzpscott.github.io/hyrf/</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>近期，3D高斯点云（3DGS）作为一种强大的替代NeRF基础方法的选择，能够通过显式、可优化的3D高斯实现实时、高质量的新视图合成。然而，3DGS由于依赖于每个高斯的参数来建模视依赖效果和各向异形状，因此面临着显著的内存开销。尽管近期的研究提出了使用神经场来压缩3DGS，这些方法在捕捉高频空间变化方面遭遇困难，导致细节的重构效果下降。我们提出了混合光照场（HyRF），这是一种新颖的场景表示方法，结合了显式高斯和神经场的优势。HyRF将场景分解为（1）一组紧凑的显式高斯，仅存储关键的高频参数，和（2）基于网格的神经场，用于预测其余属性。为了增强表示能力，我们引入了一种解耦的神经场架构，分别建模几何体（比例、不透明度、旋转）和视依赖颜色。此外，我们提出了一种混合渲染方案，将高斯点云与神经场预测的背景组合在一起，解决了远处场景表示的局限性。实验表明，HyRF在渲染质量上达到最先进水平，同时与3DGS相比，模型大小减少了20倍以上，并维持实时性能。我们的项目页面可在<a target="_blank" rel="noopener" href="https://wzpscott.github.io/hyrf/访问。">https://wzpscott.github.io/hyrf/访问。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决 <strong>3D Gaussian Splatting（3DGS）</strong> 在高质量实时新视角合成中面临的 <strong>显存占用过大</strong> 与 <strong>高频细节丢失</strong> 两大核心矛盾：</p>
<ul>
<li>3DGS 的每个高斯需要 59 维参数（48 维球谐系数 + 7 维各向异性尺度/旋转），导致模型动辄数百 MB，难以部署；</li>
<li>若直接将全部参数压入固定分辨率的网格神经场，又会因表示能力受限而丢失物体边缘、薄结构、高光等高频信息，出现模糊或伪影。</li>
</ul>
<p>为此，作者提出 <strong>Hybrid Radiance Fields（HyRF）</strong>，通过 <strong>显式-隐式混合表示</strong> 在仅数 MB 的模型体积下保持实时渲染，同时重建出与 3DGS 相当甚至更佳的视觉效果。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>NeRF 系列</strong></p>
<ul>
<li>原始 NeRF：隐式 MLP 表示，渲染慢、模型小。</li>
<li>Mip-NeRF / Mip-NeRF-360：抗锯齿、无界场景，依旧慢。</li>
<li>网格/哈希加速：Plenoxels、Instant-NGP、TensoRF、K-Planes 等，提速但仍难达实时。</li>
</ul>
</li>
<li><p><strong>显式点/高斯表示</strong></p>
<ul>
<li>3D Gaussian Splatting (3DGS)：点基 α-blending，实时但 59 参数/高斯，模型大。</li>
<li>Neural Point-Based Graphics、SynSin：早期点渲染可微框架。</li>
</ul>
</li>
<li><p><strong>3DGS 压缩与混合</strong></p>
<ul>
<li>向量量化：Compact3D、CompGS、Eagles。</li>
<li>神经预测：Scaffold-GS（锚点+局部神经特征）、F-3DGS（因子化坐标）、LocoGS（全局神经场）。</li>
<li>本文 HyRF 与上述方法区别：全局网格神经场预测全部属性，再叠加极稀疏显式残差高斯，并解耦几何/外观网络。</li>
</ul>
</li>
<li><p><strong>背景/远景建模</strong></p>
<ul>
<li>Unbounded-GS、Mega-NeRF、Switch-NeRF：针对城市级或无限场景，但模型或速度受限。</li>
<li>HyRF 引入可学习的背景球辐射场，与前景高斯 α 混合，解决 3DGS 远景模糊问题。</li>
</ul>
</li>
<li><p><strong>抗锯齿与表面重建</strong></p>
<ul>
<li>Mip-Splatting、Gaussian Opacity Fields：缓解混叠、提升表面，但仍保持完整 3DGS 参数。</li>
<li>HyRF 未专门处理抗锯齿，留待后续工作。</li>
</ul>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将问题拆解为 <strong>“高频细节保留”</strong> 与 <strong>“显存压缩”</strong> 两条主线，并给出三项关键技术：</p>
<ol>
<li><p>显式-隐式混合表示</p>
<ul>
<li>仅保留 8 维“关键”显式参数：位置 $p_e\in\mathbb{R}^3$、漫反射颜色 $c_e\in\mathbb{R}^3$、各向同性尺度 $s_e\in\mathbb{R}$、不透明度 $\alpha_e\in\mathbb{R}$。</li>
<li>其余 51 维（各向异性尺度、旋转、视相关颜色）由 <strong>多分辨率哈希神经场</strong> 预测，模型体积骤降。</li>
</ul>
</li>
<li><p>解耦双神经场</p>
<ul>
<li>Geometry Field $\Theta_{\text{geo}}$：只输出 $\alpha_n, s_n, r_n$（几何）。</li>
<li>Radiance Field $\Theta_{\text{rad}}$：输出 $c_n$ 并额外接收视角编码，专注视相关外观。<br>两路独立优化，避免单网络难以同时拟合几何与外观的高频变化。</li>
</ul>
</li>
<li><p>混合渲染管线</p>
<ul>
<li>Visibility Pre-Culling：在送入神经场前剔除视锥外高斯，减少哈希查询量。</li>
<li>前景：显式残差 + 神经预测结果经 σ/归一化激活后喂入 3DGS 光栅化。</li>
<li>背景：对每条射线与背景球求交点 $p_s$，用 $\Theta_{\text{rad}}$ 直接预测颜色 $C_{\text{bg}}$，再以 $C = C_{\text{fg}} + T \cdot C_{\text{bg}}$ 合成，解决远景模糊。</li>
</ul>
</li>
</ol>
<p>优化目标与 3DGS 相同：  </p>
<script type="math/tex; mode=display">\\mathcal{L}=(1-\\lambda)\\mathcal{L}_1 + \\lambda\\mathcal{L}_{\\text{SSIM}}</script><p>训练期间周期性重置显式 $\alpha_e$ 并剪枝低不透明度高斯，进一步压缩模型。</p>
<p>通过上述设计，HyRF 在 25 个场景上实现 <strong>模型减小 20×+、实时 FPS 维持、PSNR/SSIM/LPIPS 持平或优于 3DGS</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验覆盖 <strong>25 个场景、4 类数据集</strong>，从质量、速度、显存、消融、压缩、扩展六个维度展开：</p>
<ol>
<li><p>标准实拍 360° 场景</p>
<ul>
<li>数据集：Mip-NeRF360、Tanks&amp;Temples、Deep Blending（共 13 场景）</li>
<li>对比：Mip-NeRF360、Plenoxels、Instant-NGP、3DGS、Scaffold-GS</li>
<li>结果：PSNR/SSIM/LPIPS 三项全部第一；模型 34–49 MB（↓12× vs 3DGS）；FPS 102–114 维持实时。</li>
</ul>
</li>
<li><p>合成物体中心场景</p>
<ul>
<li>数据集：NeRF Synthetic（8 场景）</li>
<li>结果：PSNR 33.72 dB（最佳），大小 13 MB，仅为 3DGS 的 1/4。</li>
</ul>
</li>
<li><p>城市场景</p>
<ul>
<li>数据集：Mill19、Urbanscene3D（4 场景）</li>
<li>对比：MegaNeRF、SwitchNeRF、3DGS、Scaffold-GS</li>
<li>结果：PSNR 领先 1 dB 以上；模型 200–215 MB（↓4–7×）；FPS 75–77，比 NeRF 基线快 7000×。</li>
</ul>
</li>
<li><p>模型压缩对比</p>
<ul>
<li>在 Mip-NeRF360 上与 5 种最新 3DGS 压缩方法（Niedermayr et al.、Lee et al.、Girish et al.、Papantonakis et al.、Chen et al.）比较。</li>
<li>采用同一后处理（半精度坐标、残差向量量化 + Huffman、哈希 8-bit 量化）。</li>
<li>结果：18.04 MB 最小，PSNR 27.66 dB 最高，实现“更小且更好”。</li>
</ul>
</li>
<li><p>消融实验（Tanks&amp;Temples）</p>
<ul>
<li>w/o Decouple：单网络 → PSNR –0.29 dB，LPIPS +0.012。</li>
<li>w/o Explicit：去掉全部显式属性 → PSNR –1.62 dB。</li>
<li>w/o Neural：退化为各向同性漫反射 → PSNR –1.85 dB。</li>
<li>w/o Background：关闭背景球 → PSNR –0.64 dB，远景模糊。</li>
<li>w/o Pre-culling：保留查询 → FPS 从 106 降至 27，质量不变。</li>
</ul>
</li>
<li><p>扩展与深度分析</p>
<ul>
<li>街道场景 KITTI：与 3DGS 同等质量，模型 36 MB（↓13×）。</li>
<li>显式高斯数量：在三个数据集上比 3DGS 少 24–45%。</li>
<li>训练收敛：同等质量下迭代次数/时间均少于 3DGS。</li>
<li>高光场景（Spec-GS 数据集）：PSNR +1.58 dB，显存 –82%。</li>
<li>与最新 3DGS 变体（GOF、Spec-GS、Mini-Splatting2、DashGaussian）对比：HyRF 在速度、训练时间、峰值显存、模型大小四项均占优或可比。</li>
</ul>
</li>
</ol>
<p>综合以上实验，论文验证了 <strong>HyRF 在保持实时渲染的同时，实现&gt;20× 压缩且质量不降反升</strong> 的核心主张。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>抗锯齿与表面重建</strong><br>3DGS 原生存在混叠与表面不连续问题，HyRF 沿用同一光栅化管线，亦未解决。可引入 Mip-Splatting 的滤波核或 Gaussian Opacity Fields 的密度-不透明度耦合，推导适用于“神经-显式混合”的抗锯齿理论。</p>
</li>
<li><p><strong>哈希冲突与频谱偏置</strong><br>多分辨率哈希在低显存下易碰撞，导致高频伪影。可探索：</p>
<ul>
<li>自适应哈希尺寸 / 神经紧凑编码（如 K-Planes、Tri-plane Factorization）替代固定网格；</li>
<li>在神经场输出端加高频残差 MLP，仅对“碰撞区域”补偿细节。</li>
</ul>
</li>
<li><p><strong>动态/非刚性场景</strong><br>当前仅处理静态场景。将显式高斯扩展为时空高斯（4D Gaussian）并与变形场或神经辐射视频场结合，可在保持压缩率的同时实现动态重建。</p>
</li>
<li><p><strong>Web/端侧实时渲染</strong><br>完全-fused MLP 与哈希表对高端 GPU 友好，但在移动端带宽受限。可研究：</p>
<ul>
<li>权重量化 + 稀疏化哈希表，适配 WebGL / Metal；</li>
<li>将神经解码部分替换为轻量查找表或稀疏张量，实现 30 FPS 手机端渲染。</li>
</ul>
</li>
<li><p><strong>语义/编辑友好性</strong><br>显式高斯提供粗略几何，神经场编码外观，天然适合解耦。可引入语义场或特征场，实现：</p>
<ul>
<li>实例分割、光照重打、材质编辑；</li>
<li>与文本-到-3D 生成模型（DreamFusion、Magic3D）对接，作为轻量可微调表示。</li>
</ul>
</li>
<li><p><strong>更激进的压缩极限</strong><br>当前 8 维显式参数仍存冗余：</p>
<ul>
<li>位置可进一步用局部坐标 + 量化码本；</li>
<li>漫反射颜色可用基色 + 残差 VQ；</li>
<li>结合视频压缩领域的熵估计与率-失真优化，实现 &lt;5 MB 的 360° 场景。</li>
</ul>
</li>
<li><p><strong>背景球到完整环境建模</strong><br>背景球假设单半径，对复杂远景几何（城市天际线、山脉）仍近似。可改用多层级球体或立方体贴图 + 神经辐射场，支持视差与远景几何一致性。</p>
</li>
<li><p><strong>联合优化捕获管线</strong><br>当前依赖 COLMAP 点云初始化。将 HyRF 与在线 SLAM / 稀疏束调整联合优化，可实现“采集-压缩-渲染”端到端系统，降低对密集输入图像的依赖。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Hybrid Radiance Fields (HyRF)</strong>，一种显式-隐式混合的新视角合成表示，核心思想是：</p>
<ul>
<li><p><strong>把场景拆成两部分</strong></p>
<ol>
<li>极稀疏“残差”高斯：只存 8 维关键参数（位置+漫反射颜色+各向同性尺度+不透明度）。</li>
<li>网格神经场：用 <strong>解耦双哈希网络</strong> 分别预测其余几何（旋转、各向异性尺度、不透明度高频残差）和视相关颜色。</li>
</ol>
</li>
<li><p><strong>三阶段混合渲染</strong><br>① 视锥预剔除 → ② 神经场查询并与显式参数聚合 → ③ 背景球辐射场补全远景，再 α 混合。</p>
</li>
<li><p><strong>效果</strong><br>25 场景实验显示：模型体积 <strong>↓20×+</strong>、实时 FPS 维持、PSNR/SSIM/LPIPS <strong>持平或优于 3DGS</strong>；经后压缩可到 <strong>18 MB</strong> 仍领先现有 3DGS 压缩方法。</p>
</li>
<li><p><strong>贡献</strong><br>① 首次用全局神经场+显式残差高斯同时实现极致压缩与高频细节保留；<br>② 解耦几何/外观网络，克服单网络难以联合优化问题；<br>③ 背景球神经渲染解决 3DGS 远景模糊；<br>④ 兼容现有量化压缩，训练收敛更快。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是HyRF%3A%20Hybrid%20Radiance%20Fields%20for%20Memory-efficient%20and%20High-quality%20Novel%20View%20Synthesis，链接是  https://arxiv.org/pdf/2509.17083  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.17083  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zipeng Wang,Dan Xu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17083.pdf">https://arxiv.org/pdf/2509.17083.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17083">https://arxiv.org/abs/2509.17083</a></p>
<p><strong>Arxiv ID</strong>: 2509.17083</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17083">https://papers.cool/arxiv/2509.17083</a></p>
<p><strong>Published</strong>: 2025-09-21T13:59:26Z</p>
<p><strong>Updated</strong>: 2025-09-21T13:59:26.000Z</p>
<hr>
<h3 id="15-Better-Late-Than-Never-Evaluation-of-Latency-Metrics-for-Simultaneous-Speech-to-Text-Translation"><a href="#15-Better-Late-Than-Never-Evaluation-of-Latency-Metrics-for-Simultaneous-Speech-to-Text-Translation" class="headerlink" title="15. Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.17349">Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation</a></h3><p>Simultaneous speech-to-text translation (SimulST) systems have to balance translation quality with latency—the delay between speech input and the translated output. While quality evaluation is well established, accurate latency measurement remains a challenge. Existing metrics often produce inconsistent or misleading results, especially in the widely used short-form setting, where speech is artificially presegmented. In this paper, we present the first comprehensive analysis of SimulST latency metrics across language pairs, systems, and both short- and long-form regimes. We uncover a structural bias in current metrics related to segmentation that undermines fair and meaningful comparisons. To address this, we introduce YAAL (Yet Another Average Lagging), a refined latency metric that delivers more accurate evaluations in the short-form regime. We extend YAAL to LongYAAL for unsegmented audio and propose SoftSegmenter, a novel resegmentation tool based on word-level alignment. Our experiments show that YAAL and LongYAAL outperform popular latency metrics, while SoftSegmenter enhances alignment quality in long-form evaluation, together enabling more reliable assessments of SimulST systems.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>同时语音到文本翻译(SimulST)系统必须在翻译质量和延迟之间取得平衡——即语音输入与翻译输出之间的延迟。尽管质量评估已得到很好的建立，但准确的延迟测量仍然是一个挑战。现有的指标往往产生不一致或误导性的结果，特别是在广泛使用的短格式设置中，语音被人为地预分段。在本文中，我们首次全面分析了跨语言对、系统以及短格式和长格式条件下的SimulST延迟指标。我们发现当前指标在分段方面存在结构性偏差，这削弱了公平和有意义的比较。为了解决这个问题，我们引入了YAAL（又一个平均滞后），这是一个改进的延迟指标，可以在短格式条件下提供更准确的评估。我们将YAAL扩展为LongYAAL，用于未分段音频，并提出了SoftSegmenter，一种基于词级对齐的新型重分段工具。我们的实验表明，YAAL和LongYAAL优于流行的延迟指标，而SoftSegmenter则提高了长格式评估中的对齐质量，三者共同提高了SimulST系统的评估可靠性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed: Error: 抓取失败: Waiting failed: 30000ms exceeded</p>
</div></details>
<p><strong>Authors</strong>: Peter Polák,Sara Papi,Luisa Bentivogli,Ondřej Bojar</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.17349.pdf">https://arxiv.org/pdf/2509.17349.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.17349">https://arxiv.org/abs/2509.17349</a></p>
<p><strong>Arxiv ID</strong>: 2509.17349</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.17349">https://papers.cool/arxiv/2509.17349</a></p>
<p><strong>Published</strong>: 2025-09-22T04:21:19Z</p>
<p><strong>Updated</strong>: 2025-09-22T04:21:19.000Z</p>
<hr>
<h3 id="16-CommonForms-A-Large-Diverse-Dataset-for-Form-Field-Detection"><a href="#16-CommonForms-A-Large-Diverse-Dataset-for-Form-Field-Detection" class="headerlink" title="16. CommonForms: A Large, Diverse Dataset for Form Field Detection"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.16506">CommonForms: A Large, Diverse Dataset for Form Field Detection</a></h3><p>This paper introduces CommonForms, a web-scale dataset for form field detection. It casts the problem of form field detection as object detection: given an image of a page, predict the location and type (Text Input, Choice Button, Signature) of form fields. The dataset is constructed by filtering Common Crawl to find PDFs that have fillable elements. Starting with 8 million documents, the filtering process is used to arrive at a final dataset of roughly 55k documents that have over 450k pages. Analysis shows that the dataset contains a diverse mixture of languages and domains; one third of the pages are non-English, and among the 14 classified domains, no domain makes up more than 25% of the dataset. In addition, this paper presents a family of form field detectors, FFDNet-Small and FFDNet-Large, which attain a very high average precision on the CommonForms test set. Each model cost less than $500 to train. Ablation results show that high-resolution inputs are crucial for high-quality form field detection, and that the cleaning process improves data efficiency over using all PDFs that have fillable fields in Common Crawl. A qualitative analysis shows that they outperform a popular, commercially available PDF reader that can prepare forms. Unlike the most popular commercially available solutions, FFDNet can predict checkboxes in addition to text and signature fields. This is, to our knowledge, the first large scale dataset released for form field detection, as well as the first open source models. The dataset, models, and code will be released at <a target="_blank" rel="noopener" href="https://github.com/jbarrow/commonforms">https://github.com/jbarrow/commonforms</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本文介绍了CommonForms，一个用于表单字段检测的网络规模数据集。它将表单字段检测的问题视为目标检测：给定一页的图像，预测表单字段的位置和类型（文本输入、选择按钮、签名）。该数据集通过筛选Common Crawl来构建，以寻找具有可填充元素的PDF文件。从800万个文档开始，筛选过程最终得到大约55,000个文档，这些文档总共有超过450,000页。分析表明，该数据集包含多种语言和领域的多样化混合；三分之一的页面是非英语的，在14个分类领域中，没有一个领域占据超过25%的数据集。此外，本文介绍了一系列表单字段检测器，FFDNet-Small和FFDNet-Large，它们在CommonForms测试集上达到了非常高的平均精度。每个模型的训练成本不到500美元。消融结果表明，高分辨率输入对高质量的表单字段检测至关重要，并且清理过程比使用Common Crawl中具有可填充字段的所有PDF提高了数据效率。定性分析表明，它们的性能优于一个流行的、商业可用的PDF阅读器，后者可以准备表单。与最流行的商业可用解决方案不同，FFDNet除了文本和签名字段外，还可以预测复选框。据我们所知，这是首个发布的大规模表单字段检测数据集，以及首个开源模型。数据集、模型和代码将发布在 <a target="_blank" rel="noopener" href="https://github.com/jbarrow/commonforms">https://github.com/jbarrow/commonforms</a> 上。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“静态（flat）PDF 表单如何自动变成可填写表单”这一实际痛点，聚焦于其中的第一步——<strong>表单字段检测（form field detection）</strong>。<br>具体而言，它试图解决以下核心问题：</p>
<ul>
<li>缺乏公开、大规模、多样化的训练数据，使得基于深度学习的表单字段检测研究难以开展。</li>
<li>缺乏开源、高质量、能同时识别文本框、复选框/单选框与签名区的检测模型；现有商业软件（如 Adobe Acrobat）仅支持文本框，且召回率/精度不足。</li>
<li>需要验证“直接利用网络上已存在的可填写 PDF 作为弱监督信号”是否足以训练出可靠检测器，从而避免昂贵的人工标注。</li>
</ul>
<p>为此，作者构建并发布了 COMMONFORMS 数据集（≈ 59 k 份 PDF、480 k 页），训练了 FFDNet 系列高分辨率 YOLO 检测器，在测试集上达到 80+ mAP，并证明其性能显著优于商业软件，同时首次实现复选框/单选框的自动检测。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两条主线，并指出自身与它们的区别。可归纳为以下四类：</p>
<ol>
<li><p>表单<strong>准备</strong>（form preparation）</p>
<ul>
<li>Form2Seq [1,2]：假设字段已检测好，只做语义分组与分类。</li>
<li>FormA11y [18]：人机交互式地把标签与字段配对，提升可访问性。<br>→ 本文首次提供<strong>大规模弱监督数据+自动字段检测</strong>基线，无需人工介入。</li>
</ul>
</li>
<li><p>表单<strong>理解</strong>（form understanding / KIE）</p>
<ul>
<li>纯视觉：图像分割 [23]</li>
<li>纯文本：BERT [7]</li>
<li>多模态：LayoutLM 系列 [8,25,26]、Visual FUDGE [6]</li>
<li>数据集：FUNSD [10]、NAF [5] 等，规模均 &lt; 200 张。<br>→ 本文聚焦“字段位置与类型”而非“字段内容抽取”，任务层级更早。</li>
</ul>
</li>
<li><p>文档图像<strong>目标检测</strong></p>
<ul>
<li>版面分析：DocLayNet [19]、PubLayNet [27]、PRImA [3]</li>
<li>表格检测：TableBank [16]</li>
<li>数学公式：FormulaNet [20]</li>
<li>工具包：LayoutParser [21]<br>→ 本文沿用“检测即对象”范式，但首次针对<strong>表单控件</strong>（文本框、复选框、签名区）建立大规模数据集与基准模型。</li>
</ul>
</li>
<li><p>从 Common Crawl 挖掘 PDF 语料</p>
<ul>
<li>ccPDF [22]、FinePDFs [13]：侧重视觉/主题多样性，未聚焦可填写表单。<br>→ 本文通过 AcroForm/XFA 过滤与字段清洗，首次把<strong>可填写表单</strong>作为训练信号规模化利用。</li>
</ul>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“静态 PDF 自动变成可填写表单”这一需求<strong>简化为纯视觉目标检测问题</strong>：给定一页图像，直接输出每个表单控件的外接框与类别（文本框 / 选择按钮 / 签名区）。为此采用“数据驱动 + 高分辨率检测器”路线，具体步骤如下：</p>
<ol>
<li><p>构建大规模弱监督数据集 COMMONFORMS</p>
<ul>
<li>源头：Common Crawl 790 万份 PDF。</li>
<li>过滤：<br>– 仅保留含 AcroForm 或 XFA 的 76 万份；<br>– 再删去无字段、仅按钮字段、过小/越界/重叠字段，最终 5.9 万份 PDF ≈ 48 万页。</li>
<li>多样性：63 % 英文，其余 30+ 语言；14 个主题域，单域占比 &lt; 25 %。</li>
<li>按文档级划分训练/验证/测试，避免同页泄漏。</li>
</ul>
</li>
<li><p>把字段检测当“三类别目标检测”<br>类别：{Choice Button（复选框/单选）, Text Input, Signature}。<br>无需 OCR，也不做语义分组，仅回归框与类别概率。</p>
</li>
<li><p>训练高分辨率 YOLO11 检测器 FFDNet</p>
<ul>
<li>输入 1216 px（远高于传统 640 px），保持原 PDF 渲染细节。</li>
<li>两档规模：<br>– FFDNet-Small：9 M 参数，单页 5 ms（RTX 3090Ti）；<br>– FFDNet-Large：25 M 参数，单页 16 ms。</li>
<li>训练成本 &lt; $500（LambdaLabs 4×V100，300 epoch）。</li>
</ul>
</li>
<li><p>实验验证</p>
<ul>
<li>分辨率消融：640 → 1536 px 带来约 20 mAP 提升，小目标（复选框）最敏感。</li>
<li>与商业标杆对比：Adobe Acrobat 仅支持文本框且召回/精度低；FFDNet-L 在文本、签名、复选框三项全面领先，且<strong>首次实现复选框自动检测</strong>。</li>
<li>跨语言/域鲁棒：除俄语外，其余主流语言 mAP 差异 &lt; 5；域间表现稳定。</li>
<li>过滤价值：用清洗后 10 k 页训练比用原始 76 万页抽样 10 k 页提升 4 mAP，数据效率更高。</li>
</ul>
</li>
<li>开源发布<br>数据集、模型权重、训练与推理代码全部公开，供后续研究端到端表单语义建模或进一步提升扫描/多语言性能。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“表单字段检测”共设计并报告了 5 组实验，全部在 COMMONFORMS 划分出的 25 k 页测试集上评估，指标统一为 COCO-style mAP@[0.50:0.95]。结果均以“Text / Choice / Signature / All”四栏呈现。</p>
<p>实验</p>
<p>目的</p>
<p>关键设置</p>
<p>主要结论</p>
<p>1. 主结果对比</p>
<p>验证 FFDNet 绝对性能</p>
<p>FFDNet-S vs. FFDNet-L，输入 1216 px</p>
<p>FFDNet-L 达 81.0 mAP，比 Small 高 8.7；三项子类均 &gt; 78 AP。</p>
<p>2. 分辨率消融</p>
<p>证明“高分辨率”必要性</p>
<p>固定 6 M 参数骨干，训练 10 k 页，比较 640-1536 px 四档</p>
<p>640 → 1536 px 提升约 20 mAP；Choice 与 Signature 对分辨率最敏感。</p>
<p>3. 商业软件对比</p>
<p>看“开源检测器”能否击败主流闭源方案</p>
<p>同测试集人工统计 Adobe Acrobat Pro 的检出框</p>
<p>Acrobat 仅支持 Text，召回/精度显著低；FFDNet 额外检出全部 Choice 框，Text 与 Signature 的 AP 亦全面领先。</p>
<p>4. 跨语言/域稳健性</p>
<p>验证数据多样性带来的泛化能力</p>
<p>按 3.3-3.4 节的语言/域标签分组计算 AP</p>
<p>10 大语言中 9 种 AP 差异 ≤ 5；俄语因样本少+字符形态差异性能掉队。域间除 Other 外 AP 波动 &lt; 8。</p>
<p>5. 过滤策略收益</p>
<p>量化“严格清洗”是否划算</p>
<p>同样 10 k 页训练量，对比“过滤后 59 k 文档池采样” vs. “原始 760 k 文档池采样”</p>
<p>清洗组 57.9 mAP，未清洗组 53.6 mAP，提升 4.3 点，证明清洗显著提高数据效率。</p>
<p>以上实验共同支撑了论文核心主张：</p>
<ol>
<li>把表单字段检测当“高分辨率目标检测”即可取得 80+ mAP；</li>
<li>开源 FFDNet 在精度、召回、字段类型覆盖上均优于商业标杆；</li>
<li>严格的数据清洗比“无脑堆量”更有效。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>端到端表单语义建模</strong><br>目前仅输出“框+类别”，下一步可联合字段标签文本，把检测-分组-配对做成单阶段多任务网络，实现真正的“一键可填写 PDF”。</p>
</li>
<li><p><strong>扫描/拍照表单鲁棒性</strong><br>数据集中 born-digital PDF 占绝大多数。引入失真、阴影、摩尔纹、低分辨率等扫描增强，或专门采集扫描表单，可缓解现实纸质场景性能下降。</p>
</li>
<li><p><strong>多语言-字符形态数据增强</strong><br>俄语等低资源语言表现掉队。采用字体渲染、字符替换、合成右到左/竖排文本等策略，可提升跨语言召回。</p>
</li>
<li><p><strong>字段层级细分类</strong><br>当前三类粒度较粗。将 Choice Button 拆为 checkbox/radio、Text Input 拆为单行/多行/数字/日期等，可让下游自动填写更精准。</p>
</li>
<li><p><strong>小目标检测新架构</strong><br>复选框往往 &lt; 16×16 px，可尝试 Swin-Transformer、DETR 或 FCOS 改进版，引入可变形注意力或更高分辨率特征金字塔。</p>
</li>
<li><p><strong>弱标注自监督预训练</strong><br>利用 76 万未清洗 PDF 进行自监督预训练（如 MAE、DocLLM），再在 COMMONFORMS 微调，有望进一步压榨数据价值。</p>
</li>
<li><p><strong>版面与字段联合优化</strong><br>与版面检测、表格检测、标题-段落分割任务共享主干，可借助多任务学习提升整体版面理解一致性。</p>
</li>
<li><p><strong>交互式修正与主动学习</strong><br>针对 3.2 节列举的“字段放页眉页脚”等噪声，设计人在回路主动学习框架，迭代清洗并持续扩充数据集。</p>
</li>
<li><p><strong>压缩与移动端部署</strong><br>FFDNet-S 仍 9 M 参数、1216 px 输入。通过知识蒸馏、量化、剪枝或 NAS 搜索，可在手机端实现 &lt; 2 ms 实时检测。</p>
</li>
<li><p><strong>法律/隐私合规审查</strong><br>自动识别“For Official Use Only”“Signature”等敏感区，为表单脱敏、访问控制提供前置信号。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>任务定位</strong><br>将“静态 PDF 变可填写表单”拆解为纯视觉目标检测：给定页面图像，直接输出表单控件（文本框、选择按钮、签名区）的边界框与类别。</p>
</li>
<li><p><strong>数据构建 COMMONFORMS</strong><br>从 Common Crawl 790 万份 PDF 出发，通过 AcroForm/XFA 过滤、字段清洗，得到 5.9 万份高质量可填写 PDF（≈ 48 万页）。涵盖 30+ 语言、14 个领域，无单域 &gt; 25 %，三分之一为非英语。</p>
</li>
<li><p><strong>基线模型 FFDNet</strong><br>基于 YOLO11 训练两个高分辨率（1216 px）检测器：</p>
<ul>
<li>FFDNet-Small（9 M 参数，5 ms/页）</li>
<li>FFDNet-Large（25 M 参数，16 ms/页）<br>训练成本 &lt; $500，测试集 mAP 分别为 72.3 与 81.0。</li>
</ul>
</li>
<li><p><strong>实验结论</strong></p>
<ol>
<li>分辨率敏感：640 → 1536 px 提升约 20 mAP，小目标（复选框）最受益。</li>
<li>商业标杆：Adobe Acrobat 仅支持文本框且召回/精度低；FFDNet 首次实现复选框/单选框自动检测，三项全面领先。</li>
<li>跨语言/域稳健：9/10 语言 AP 差异 &lt; 5；域间波动小。</li>
<li>数据效率：严格清洗比从 76 万原始 PDF 抽样训练提升 4.3 mAP。</li>
</ol>
</li>
<li><p><strong>开源与影响</strong><br>数据集、模型、训练/推理代码全部公开，为首个大规模表单字段检测资源，填补开源空白。</p>
</li>
<li><p><strong>未来方向</strong><br>端到端语义建模、扫描鲁棒性、细分类、移动端压缩、联合版面多任务、自监督预训练等。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是CommonForms%3A%20A%20Large%2C%20Diverse%20Dataset%20for%20Form%20Field%20Detection，链接是  https://arxiv.org/pdf/2509.16506  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.16506  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Joe Barrow</p>
<p><strong>Categories</strong>: cs.CV,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.16506.pdf">https://arxiv.org/pdf/2509.16506.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.16506">https://arxiv.org/abs/2509.16506</a></p>
<p><strong>Arxiv ID</strong>: 2509.16506</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.16506">https://papers.cool/arxiv/2509.16506</a></p>
<p><strong>Published</strong>: 2025-09-20T02:55:40Z</p>
<p><strong>Updated</strong>: 2025-09-20T02:55:40.000Z</p>
<hr>
<h3 id="17-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications"><a href="#17-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications" class="headerlink" title="17. Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19087">Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications</a></h3><p>Multi-spectral imagery plays a crucial role in diverse Remote Sensing applications including land-use classification, environmental monitoring and urban planning. These images are widely adopted because their additional spectral bands correlate strongly with physical materials on the ground, such as ice, water, and vegetation. This allows for more accurate identification, and their public availability from missions, such as Sentinel-2 and Landsat, only adds to their value. Currently, the automatic analysis of such data is predominantly managed through machine learning models specifically trained for multi-spectral input, which are costly to train and support. Furthermore, although providing a lot of utility for Remote Sensing, such additional inputs cannot be used with powerful generalist large multimodal models, which are capable of solving many visual problems, but are not able to understand specialized multi-spectral signals. To address this, we propose a training-free approach which introduces new multi-spectral data in a Zero-Shot-only mode, as inputs to generalist multimodal models, trained on RGB-only inputs. Our approach leverages the multimodal models’ understanding of the visual space, and proposes to adapt to inputs to that space, and to inject domain-specific information as instructions into the model. We exemplify this idea with the Gemini2.5 model and observe strong Zero-Shot performance gains of the approach on popular Remote Sensing benchmarks for land cover and land use classification and demonstrate the easy adaptability of Gemini2.5 to new inputs. These results highlight the potential for geospatial professionals, working with non-standard specialized inputs, to easily leverage powerful multimodal models, such as Gemini2.5, to accelerate their work, benefiting from their rich reasoning and contextual capabilities, grounded in the specialized sensor data.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多光谱影像在多种遥感应用中发挥着关键作用，包括土地利用分类、环境监测和城市规划。这些图像被广泛采用，因为其附加的光谱波段与地面上的物理材料，如冰、水和植被，具有很强的相关性。这使得更准确的识别成为可能，并且它们从如Sentinel-2和Landsat等任务中的公开获取也增加了它们的价值。目前，此类数据的自动分析主要通过专门针对多光谱输入训练的机器学习模型管理，这些模型的训练和支持成本高昂。此外，尽管提供了大量的遥感实用性，但这些附加输入无法与强大的通用大规模多模态模型一起使用，后者能够解决许多视觉问题，但无法理解专门的多光谱信号。为了解决这一问题，我们提出了一种无训练的方法，以零样本的方式引入新的多光谱数据，作为训练在仅支持RGB输入的通用多模态模型的输入。我们的方法利用多模态模型对视觉空间的理解，并提议将输入适应到该空间，并将领域特定信息作为指令注入到模型中。我们以Gemini2.5模型作为例证，并观察到该方法在流行的土地覆盖和土地利用分类的遥感基准测试中取得了显著的零样本性能提升，展示了Gemini2.5对新输入的良好适应性。这些结果突显了地理空间专业人员在处理非标准专门输入时，能够轻松利用强大的多模态模型（如Gemini2.5）来加速他们的工作，从而受益于其丰富的推理和背景能力，这些能力建立在专门的传感器数据之上。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对遥感领域的一个核心矛盾：多光谱影像是地表分类、环境监测等任务的关键输入，却因波段数量多、传感器规格差异大，只能依赖专门训练的多光谱模型；而通用大视觉-语言模型（LMM）虽具备强大视觉推理能力，却仅接受 RGB 输入，无法直接利用多光谱信息。<br>为此，作者提出“零样本多光谱学习”范式，在不重新训练或微调通用模型（Gemini 2.5）的前提下，将其迁移到多光谱场景，实现以下目标：</p>
<ul>
<li>摆脱对昂贵、传感器专用的多光谱预训练依赖；</li>
<li>让通用模型在推理阶段即可理解任意多光谱波段组合；</li>
<li>在零样本条件下提升遥感地物分类精度，缩小与专用模型的性能差距。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，均围绕“如何让大模型看懂遥感多光谱数据”展开，但与本文的<strong>零样本、免训练、通用模型</strong>立场形成鲜明对比。</p>
<ol>
<li><p>遥感专用多模态基础模型</p>
<ul>
<li><strong>SatMAE</strong>、<strong>Prithvi</strong>、<strong>SkySense</strong>、<strong>SpectralGPT</strong>、<strong>HyperSIGMA</strong><br>这些模型从头预训练或微调 Transformer，直接以多光谱/高光谱波段为输入通道，代价是巨额 GPU 小时与专用数据收集，且新增波段需重新训练。</li>
<li><strong>RS-CLIP</strong>、<strong>RemoteCLIP</strong>、<strong>SkyCLIP</strong><br>在 RGB 遥感图像-文本对上对比学习，仍局限三通道，无法利用近红外、短波红外等物理信息。</li>
</ul>
</li>
<li><p>轻量级微调/适配器路线</p>
<ul>
<li><strong>LoRA-NIR</strong>、<strong>Scale-MAE</strong>、<strong>Croma</strong><br>仅对新增波段插入小参数模块，虽降低训练成本，但仍需梯度更新，且对传感器规格变化敏感。</li>
</ul>
</li>
<li><p>通用视觉-语言模型的遥感零样本尝试</p>
<ul>
<li><strong>GPT-4V</strong>、<strong>Qwen-VL-Chat</strong>、<strong>InstructBLIP</strong> 在 RGB 遥感基准上的零样本评估（Zhang &amp; Wang, 2024）<br>结果证明通用模型具备一定遥感理解力，但<strong>均未涉足多光谱输入</strong>；本文首次把此类模型扩展到多光谱场景，且<strong>无需任何参数更新</strong>。</li>
</ul>
</li>
</ol>
<p>综上，现有工作要么“专门训练多光谱模型”，要么“仅用 RGB 零样本”，本文填补了“通用模型 + 零样本 + 多光谱”这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>核心思路：<strong>不改动模型权重，把多光谱信号“翻译”成通用视觉-语言模型已熟知的 RGB-like 图像，并用文本提示把物理含义注入上下文</strong>。<br>具体实现分三步，完全在推理阶段完成，零训练、零梯度。</p>
<ol>
<li><p>伪彩图生成（Pseudo-Image Encoding）<br>对 Sentinel-2 的 12 个波段做线性组合与色彩映射，得到 5 张额外“图像”：</p>
<ul>
<li>假彩色（B08+B04+B03）</li>
<li>NDVI 伪彩（B08,B04）</li>
<li>NDWI 伪彩（B08,B03）</li>
<li>NDMI₁（B8A,B11）</li>
<li>NDMI₂（B8A,B12）<br>每张图均归一化到 [0,255] 并保存为 PNG，确保与 RGB 一样被 ViT 编码器直接接受。</li>
</ul>
</li>
<li><p>物理语义提示（Physics-Aware Prompting）<br>在输入端附加一段固定模板，逐图说明：</p>
<ul>
<li>用了哪几个波段、中心波长、空间分辨率；</li>
<li>该组合在物理上对应什么（水分、植被、岩石等）；</li>
<li>颜色映射规则（红-绿-蓝分别代表高低值）。<br>模型因此能把伪彩图的色块与地表材料建立语言-视觉关联。</li>
</ul>
</li>
<li>零样本推理（Zero-Shot Inference）<br>将 1×RGB + 5×伪彩图连同提示一次性送入 Gemini 2.5，要求其多标签输出。<br>整个过程<strong>无参数更新、无微调、无额外适配器</strong>，仅通过“图像+文本”双通道把多光谱信息注入已有视觉空间，实现跨模态对齐。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验全部以 <strong>零样本（zero-shot）</strong> 方式在公开遥感基准上完成，<strong>不微调、不训练、不使用任何验证集调参</strong>。核心结果如下：</p>
<ol>
<li><p>BigEarthNet 多标签地物分类</p>
<ul>
<li>43 类设置：<br>RGB 基线 F1 = 0.388 → 加入多光谱伪彩图 F1 = 0.429（+4.1 pp）</li>
<li>19 类精简设置：<br>RGB 基线 F1 = 0.414 → 多光谱 F1 = 0.453（+3.9 pp）<br>同期 SOTA 零样本最佳结果 0.40（Qwen-VL-Chat），本文领先 <strong>5.3 pp</strong>。</li>
</ul>
</li>
<li><p>EuroSat 单标签土地利用分类</p>
<ul>
<li>10 类准确率：<br>RGB 基线 66.3 % → 多光谱 69.1 %（+2.8 pp）<br>超过使用归纳式标签传播的 ZLaP 方法（63.2 %），<strong>零样本无额外数据即夺冠</strong>。</li>
</ul>
</li>
<li><p>消融实验（Ablation）<br>在 1 k 子集上逐组增加伪彩图：</p>
<ul>
<li>RGB only → +NDVI → +NDWI → 全部多光谱<br>43 类 F1 依次提升：0.406 → 0.415 → 0.437 → 0.462，证实<strong>波段越丰富，增益越大</strong>。</li>
</ul>
</li>
<li><p>可视化案例</p>
<ul>
<li>河流场景：RGB 被误判为“森林”，NDWI 伪彩图提示水体后模型纠正为“河流”。</li>
<li>森林场景：RGB 因蓝绿色调被误判为“海/湖”，多光谱输入后正确识别为“森林”。<br>说明伪彩图在<strong>光谱歧义场景</strong>起决定性作用。</li>
</ul>
</li>
<li>局限性验证<br>对提示语顺序、颜色映射表进行扰动，F1 波动 &lt; ±0.007，表明方法<strong>对提示并不高度敏感</strong>；但亦指出非成像数据（如雷达时序、激光点云）无法直接套用同一框架。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>更多传感器模态</strong><br>将 SAR、热红外、LiDAR 高程、夜光、SIF 等映射为伪彩图，验证同一框架能否零样本融合。</p>
</li>
<li><p><strong>高光谱波段爆炸</strong><br>对 200+ 波段 Hyperion、EnMAP 数据，研究“自动波段选择 + 伪彩压缩”策略，避免提示过长。</p>
</li>
<li><p><strong>可学习的伪彩生成</strong><br>保持模型冻结，仅训练一个轻量波段组合网络（无梯度回传 ViT），以可微分方式搜索最优伪彩映射。</p>
</li>
<li><p><strong>时空扩展</strong><br>将多光谱伪彩图按时间堆叠为“伪视频”，测试 Gemini 2.5 的 1 M token 长窗口能否零样本利用时序变化。</p>
</li>
<li><p><strong>提示自动化</strong><br>用 LLM 根据卫星元数据（波段中心、FWHM、观测几何）自动生成物理描述，减少人工模板依赖。</p>
</li>
<li><p><strong>跨传感器泛化</strong><br>同一提示是否同时适用于 Sentinel-2、Landsat-9、GF-6、WorldView-3 等不同波段设置，量化“光谱偏移”对性能影响。</p>
</li>
<li><p><strong>非视觉模态适配</strong><br>对雷达复数、激光点云、GNSS 时序等非图像数据，设计“可视觉化编码”或“文本化描述”两条路线，检验通用模型极限。</p>
</li>
<li><p><strong>下游任务拓展</strong><br>零样本迁移到变化检测、作物产量估计、温室气体排放反演等回归/密集预测任务，探索伪彩 + 提示是否足够。</p>
</li>
<li><p><strong>不确定性估计</strong><br>利用模型生成多组答案的概率分布，构建无监督置信度指标，为实际业务提供可解释性。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心贡献</strong><br>提出“零样本多光谱学习”范式，<strong>无需任何训练或参数更新</strong>，即可让仅见过 RGB 的通用多模态大模型（Gemini 2.5）直接理解 Sentinel-2 等卫星的多光谱数据，并在遥感地物分类任务中刷新零样本 SOTA。</p>
<p><strong>技术路线</strong></p>
<ol>
<li>将 12 个波段线性组合为 5 张伪彩图（假彩色、NDVI、NDWI、NDMI×2），保持 PNG 格式，可被 ViT 编码器直接读取。</li>
<li>用固定文本提示逐图说明波段物理含义、中心波长、颜色映射规则，把光谱信息注入上下文。</li>
<li>一次性输入 1×RGB + 5×伪彩图 + 提示，零样本推理，多标签/单标签输出。</li>
</ol>
<p><strong>实验结果</strong></p>
<ul>
<li>BigEarthNet-43 类：F1 从 0.388 → 0.429 (+4.1 pp)。</li>
<li>BigEarthNet-19 类：F1 从 0.414 → 0.453，领先此前最佳零样本方法 5.3 pp。</li>
<li>EuroSat-10 类：准确率 66.3 % → 69.1 %，超过允许无标签归纳的 ZLaP 方法。</li>
<li>消融显示：波段越丰富，增益越大；可视化证实伪彩图可消除 RGB 光谱歧义。</li>
</ul>
<p><strong>意义与展望</strong><br>首次证明通用大模型仅凭“伪彩+提示”即可解锁多光谱能力，为遥感用户提供<strong>免训练、低成本、跨传感器</strong>的新基线，可快速扩展到 SAR、热红外、高光谱等模态。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Zero-Shot%20Multi-Spectral%20Learning%3A%20Reimagining%20a%20Generalist%20Multimodal%20Gemini%202.5%20Model%20for%20Remote%20Sensing%20Applications，链接是  https://arxiv.org/pdf/2509.19087  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19087  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ganesh Mallya,Yotam Gigi,Dahun Kim,Maxim Neumann,Genady Beryozkin,Tomer Shekel,Anelia Angelova</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19087.pdf">https://arxiv.org/pdf/2509.19087.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19087">https://arxiv.org/abs/2509.19087</a></p>
<p><strong>Arxiv ID</strong>: 2509.19087</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19087">https://papers.cool/arxiv/2509.19087</a></p>
<p><strong>Published</strong>: 2025-09-23T14:40:52Z</p>
<p><strong>Updated</strong>: 2025-09-23T14:40:52.000Z</p>
<hr>
<h3 id="18-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction"><a href="#18-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction" class="headerlink" title="18. VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction"></a>18. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19002">VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</a></h3><p>Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs’ geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent’s markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>近年来，多模态大语言模型（MLLMs）的进展显著提升了视频理解能力，为实际应用开辟了新的可能性。然而，目前的视频基准主要集中在室内场景或短程户外活动上，与长途旅行相关的挑战基本上未被探索。掌握扩展的地理空间时间轨迹对于下一代MLLM至关重要，这支持了诸如具身AI规划和导航等现实世界任务。为了填补这一空白，我们推出了VIR-Bench，一个新兴的基准，包括200个旅行视频，将行程重建框架作为一项挑战性任务，旨在评估并推动MLLM的地理空间时间智能。实验结果表明，先进的MLLM，包括一些专有模型，在处理跨越广泛空间和时间尺度的视频时难以获得高分，这突显了其难度。此外，我们进行了一项深入的案例研究，开发了一个原型旅行规划代理，利用从VIR-Bench中获得的见解。该代理显著改善的行程建议验证了我们的评估协议不仅能够有效基准模型，还能转化为面对用户的应用中的具体性能提升。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决当前多模态大语言模型（MLLMs）在<strong>长程地理-时空理解</strong>方面的能力缺失问题。具体而言，现有视频理解基准主要聚焦于室内场景或短距离户外活动，忽略了<strong>长距离、跨城市、跨天数的旅行视频</strong>所蕴含的复杂地理与时空信息。为此，论文提出 VIR-Bench，通过<strong>旅行视频行程重建</strong>任务，系统评估并推动 MLLMs 在以下方面的能力：</p>
<ul>
<li><strong>地理空间智能</strong>：从视频帧中识别所到之处的行政区划（都道府县、市町村）与具体 POI（景点、车站、餐厅等）。</li>
<li><strong>时空推理</strong>：推断地点之间的层级包含关系（inclusion）与按时间顺序的迁移关系（transition），形成完整的“访问顺序图”。</li>
</ul>
<p>简言之，论文填补了对 MLLMs 在<strong>宏观地理尺度、长时间跨度视频</strong>上时空理解能力评估的空白，并为后续落地应用（如具身 AI 导航、自动旅行规划）提供基准与方法论基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节系统梳理了三条相关研究脉络，并指出其与 VIR-Bench 的差异。以下按主题归纳，并给出关键文献出处（仅列代表）：</p>
<ol>
<li><p>视频理解基准（Video Benchmarks）</p>
<ul>
<li>短时-室内/街景：Ego4D、HourVideo、VSI-Bench、OST-Bench</li>
<li>城市级视频地理定位：CityGuessr、UrbanVideo-Bench<br>共同点：场景短距、单城或室内，缺乏跨城、跨天行程。<br>VIR-Bench 首次聚焦“长距-长时”旅行视频，任务为完整行程图重建而非单纯定位。</li>
</ul>
</li>
<li><p>行程抽取（Itinerary Extraction）</p>
<ul>
<li>纯文本：Drymonas &amp; Pfoser 2010；Yamamoto et al. 2025 提出 visiting-order graph，仅针对游记文本。</li>
<li>图文/视频：Pang et al. 2011 用博客图文；Rosa 2024 用 MLLM 从旅行视频做实体抽取，但无结构化轨迹。<br>VIR-Bench 首次将“ visiting-order graph”扩展到视频模态，并给出大规模标注数据与评测协议。</li>
</ul>
</li>
<li><p>行程生成（Itinerary Generation）</p>
<ul>
<li>传统优化：Tourist Trip Design Problem、Gavalas et al. 2014 综述。</li>
<li>LLM 时代：TravelAgent、TravelPlanner、TripCraft 等基准，输入多为文本偏好或日志。<br>VIR-Bench 反其道而行：以<strong>视频+重建的 POI 序列</strong>为输入，验证“先重建后生成”的闭环价值。</li>
</ul>
</li>
</ol>
<p>综上，现有工作要么基准场景微观，要么输入模态单一；VIR-Bench 首次把“长程地理-时空推理”作为视频理解任务提出，并建立端到端评测与下游旅行规划应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“三步走”策略系统性地解决长程地理-时空理解缺失的问题：</p>
<ol>
<li><p>构建专用基准 VIR-Bench</p>
<ul>
<li>数据：200 条日本旅行 vlog，平均 18 min，跨越 43 个都道府县，人工标注 3 689 个 POI。</li>
<li>标注粒度：为每条视频建立“访问顺序图”（visiting-order graph），节点分三级（都道府县→市町村→POI），边分两类（inclusion 表空间层级，transition 表时间先后）。</li>
<li>质量控制：双人标注+二次复核，确保地理层级与时间顺序一致。</li>
</ul>
</li>
<li><p>任务分解与评测协议<br>将端到端“行程图重建”拆成两个可量化的子任务：</p>
<ul>
<li>Node Prediction：从视频中列出所有到访的行政区与 POI，并给出 POI 类别。</li>
<li>Edge Prediction：给定 gold 节点（打乱顺序），模型预测 inclusion 与 transition 边。<br>评价指标：macro-F1，对 POI 名称采用 0.7/0.5 双阈值相似度匹配，对边要求三元组完全匹配。</li>
</ul>
</li>
<li><p>模型诊断与改进路径</p>
<ul>
<li>大规模零样本评测：覆盖 9 个开源与 4 个商业 MLLM，发现<br>– 开源模型在 POI 节点与 transition 边上普遍低于 20 F1；<br>– 商业模型亦仅约 60 F1，transition 边仍是瓶颈。</li>
<li>消融实验：<br>– 帧数：≥128 帧（≈14 s 一帧）是可靠时空推理的“最低分辨率”；<br>– 推理预算：o4-mini 的“high thinking”使 transition F1 绝对提升 13.8；<br>– 音频：移除 Gemini-2.5-Flash 音频导致 transition F1 下降 19.4，验证音频提供细粒度时间线索。</li>
<li>原型应用：基于重建的 POI 序列+视频内容，构建多智能体旅行规划系统。人群评测显示“POI+视频”双输入在吸引力、可行性、一致性上显著优于单模态，反向证明高质量行程重建是落地应用的前提。</li>
</ul>
</li>
</ol>
<p>通过以上“数据-任务-评测-诊断-应用”闭环，论文不仅填补了宏观地理-时空视频理解的评测空白，也给出了提升帧率、加长推理、融合音频等具体改进方向，为下一代 MLLMs 的具身导航与旅行规划奠定基准与方法基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 VIR-Bench 共设计并执行了<strong>三类实验</strong>，覆盖模型基准评测、消融诊断与下游应用验证，具体如下：</p>
<p>1. 主基准实验（Zero-shot 评测）</p>
<p>子任务</p>
<p>指标</p>
<p>模型范围</p>
<p>关键发现</p>
<p><strong>Node Prediction</strong><br>预测访问的都道府县、市町村、POI 及类别</p>
<p>macro P/R/F1</p>
<p>9 个开源模型（7B–72B）<br>4 个商业模型（GPT-4.1、o4-mini、Gemini-2.5-Flash/Pro）</p>
<p>开源最佳 Qwen2.5-VL-72B 仅 38.1 F1；商业最佳 Gemini-2.5-Pro 达 57.4 F1，但 POI 节点仍低于 53 F1。</p>
<p><strong>Edge Prediction</strong><br>给定 gold 节点，预测 inclusion &amp; transition 边</p>
<p>macro P/R/F1</p>
<p>同上</p>
<p>所有模型在 transition 边均显著落后：开源普遍 &lt;25 F1，商业最高 Gemini-2.5-Pro 仅 66.8 F1，成为最大瓶颈。</p>
<p>2. 消融实验（Ablation）</p>
<p>变量</p>
<p>设置</p>
<p>模型</p>
<p>观测结果（F1 变化）</p>
<p><strong>输入帧数</strong></p>
<p>64 → 128 → 256 帧</p>
<p>GPT-4.1</p>
<p>POI 节点 +14.3；transition 边 +6.9；≥128 帧后收益趋缓。</p>
<p><strong>推理预算</strong></p>
<p>low → medium → high thinking</p>
<p>o4-mini</p>
<p>transition 边 +13.8；inclusion 边几乎不变，说明长程时序依赖更受益。</p>
<p><strong>音频信号</strong></p>
<p>开/关 1 kbps 音轨</p>
<p>Gemini-2.5-Flash</p>
<p>关闭后 transition 边 −19.4，POI 节点 −1.1，验证音频提供连续时间线索。</p>
<p>3. 下游原型实验（旅行规划代理）</p>
<p>设定</p>
<p>输入模态</p>
<p>评测方式</p>
<p>主要结果</p>
<p><strong>POI-only</strong></p>
<p>仅 gold POI 列表</p>
<p>20 条视频×3 设定 → 60 份计划<br>日本众包 5 人/份，4 维度 5 级评分</p>
<p>吸引力 3.58，交通可行性最高 82 %，但内容平淡。</p>
<p><strong>Video-only</strong></p>
<p>仅视频</p>
<p>同上</p>
<p>吸引力 3.46，对齐度两极分化（31 % 完全无关），交通信息缺失 22 %。</p>
<p><strong>POI+Video</strong></p>
<p>列表+视频</p>
<p>同上</p>
<p>吸引力 3.73（最佳），对齐度 75 % 以上，POI 选取显著偏好<strong>长时出镜</strong>与<strong>高评分</strong>地点（∆=+41.7 s，p&lt;0.001）。</p>
<p>4. 误差分析（附录 B.2）</p>
<p>将 200 份预测结果人工归类为三类错误：</p>
<ul>
<li><strong>Prompt 理解错误</strong>：照抄示例、层级混淆（例：把 transition 边跨市连 POI）。</li>
<li><strong>地理知识错误</strong>：仅说出最知名城市（如 Okinawa→只会预测 Naha）。</li>
<li><strong>时序推理错误</strong>：无法建立全局先后关系，出现“碎片化”子图或循环。</li>
</ul>
<p>综上，实验从<strong>基准性能</strong>→<strong>瓶颈诊断</strong>→<strong>改进方向</strong>→<strong>落地价值</strong>四个层面，系统验证了 VIR-Bench 的挑战性与实用性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 VIR-Bench 开启的“下一步”研究议程，按短期可验证到长期需突破递进：</p>
<p>1. 数据与标注扩展</p>
<ul>
<li><strong>地理多样性</strong><br>将采集范围从日本 43 都道府县扩展到全球六大洲，引入不同路网密度、文字系统（拉丁、阿拉伯、泰文等）与季节气候差异，检验模型跨文化地理泛化。</li>
<li>** filming 风格多样化**<br>纳入行车记录仪、无人机、全景 360°、第一人称眼镜等多视角流，验证 MLLM 对视角变化的鲁棒性。</li>
<li><strong>层级更细</strong><br>在 POI 内部再细分“楼层-商铺”或“景区-观景点”，构建四层 inclusion 关系，测试模型对微观空间嵌套的理解极限。</li>
<li><strong>动态标注</strong><br>引入“停留时长+交通方式+花费”三维标签，支持后续做“碳排放-预算-时间”多目标优化研究。</li>
</ul>
<p>2. 任务与评测升级</p>
<ul>
<li><strong>端到端图生成</strong><br>当前拆分为 node→edge 两步。未来可直接评测端到端“视频→完整 visiting-order graph”，引入图编辑距离 (GED) 与图同构误差作为新指标。</li>
<li><strong>多视频联合推理</strong><br>给定同一目的地 N 支 vlog，要求模型融合多视角生成一份“最优共识行程”，考察跨源对齐与冲突消解能力。</li>
<li><strong>反事实与可解释</strong><br>引入“假如去掉第 i 段行程”或“若雨天替代方案”等反事实问答，评测模型对时空因果与可解释规划的能力。</li>
<li><strong>在线增量预测</strong><br>模拟流式场景：每过 30 s 更新一次部分观测，要求模型实时修正后续路线，衡量增量时空推理与遗忘平衡。</li>
</ul>
<p>3. 模型与算法创新</p>
<ul>
<li><strong>音频-视觉-文本三模态对齐</strong><br>论文已验证音频显著提升 transition 边。未来可引入 ASR 文本与视觉 OCR 联合训练，构建“地名-语音-字幕”统一嵌入，减少幻觉。</li>
<li><strong>地理先验注入</strong><br>将 OpenStreetMap、GeoNames 层次结构编码为可查询图记忆（GNN+KG），实现“可检索的地理知识”，而非仅靠参数记忆。</li>
<li><strong>长上下文与记忆机制</strong><br>采用 1 M token 级视频 Transformer 或压缩-召回式记忆，解决 3-7 天超长行程的跨天依赖瓶颈。</li>
<li><strong>强化学习微调</strong><br>以行程合理性奖励（交通时间误差、地理距离、POI 评分）为信号，用 RL 微调 MLLM，直接优化图级指标而非 token 级交叉熵。</li>
</ul>
<p>4. 下游应用深化</p>
<ul>
<li><strong>多视频行程混搭</strong><br>允许系统同时参考 5-10 支不同博主 vlog，生成“精华版”行程，并给出“出镜率-口碑”二维可视化，提升旅游规划多样性。</li>
<li><strong>个性化与实时约束</strong><br>接入用户日历、预算、体力、无障碍需求，实现“视频理解→个性化行程→实时重排”闭环，支持旅途中动态改线。</li>
<li><strong>AR/VR 体验预览</strong><br>将重建的 visiting-order graph 与 NeRF/3D Gaussian Splatting 结合，生成可漫游的沉浸式预览，验证“先体验再出行”商业模式。</li>
<li><strong>具身导航迁移</strong><br>把模型输出的宏观路线转化为细粒度机器人导航指令（GPS 路点+视觉地标），在无人车或轮式机器人上实测跨城导航误差。</li>
</ul>
<p>5. 伦理与隐私研究</p>
<ul>
<li><strong>人脸与车牌模糊化</strong><br>旅行 vlog 含大量路人面部与车牌，需开发自动去标识化 pipeline，并评测其对 POI 识别与行程重建的精度影响。</li>
<li><strong>版权与合规分发</strong><br>探索“视频指纹+区块链”方式，确保原作者获得引用积分，同时允许研究级标注数据合法跨境共享。</li>
</ul>
<p>综上，VIR-Bench 打开了“长程地理-时空视频理解”这一新赛道；从数据、任务、模型到伦理，每一层都留有丰富的可深挖空间。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>VIR-Bench：用旅行视频行程重建评估 MLLM 的长程地理-时空理解</strong></p>
<ol>
<li><p>问题<br>现有视频基准聚焦室内或短距场景，缺少对“跨城、跨天”旅行视频的地理-时空推理评测，而此类能力是 embodied AI 与自动旅行规划的核心。</p>
</li>
<li><p>方案</p>
</li>
</ol>
<ul>
<li>构建 VIR-Bench：200 条日本旅行 vlog（平均 18 min，跨越 43 都道府县），人工标注 3 689 个 POI 并生成“访问顺序图”（节点=都/市/POI，边=层级包含+时间迁移）。</li>
<li>任务分解：① node prediction（猜地点）② edge prediction（猜包含/迁移关系），以 macro-F1 评测。</li>
</ul>
<ol>
<li>结果</li>
</ol>
<ul>
<li>9 个开源与 4 个商业模型零样本测试：开源最佳仅 38.1 F1，商业最佳 Gemini-2.5-Pro 达 57.4 F1，但 transition 边仍 ≤67 F1。</li>
<li>消融：帧数 ≥128、加长推理、保留音频分别显著提升 POI 与 transition 表现；音频关闭使 transition F1 降 19.4。</li>
</ul>
<ol>
<li><p>应用<br>基于重建的 POI+视频，开发多智能体旅行规划系统。人群评测显示“POI+视频”双输入计划吸引力 3.73（最高），且显著偏好长时出镜、高评分 POI，验证行程重建对落地应用的关键作用。</p>
</li>
<li><p>贡献</p>
</li>
</ol>
<ul>
<li>首个长程地理-时空视频理解基准；</li>
<li>揭示现有 MLLM 在 macro-地理与长时序推理上的持续瓶颈；</li>
<li>提供数据、评测协议与可复现的下游应用框架，推动旅行规划与具身导航研究。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是VIR-Bench%3A%20Evaluating%20Geospatial%20and%20Temporal%20Understanding%20of%20MLLMs%20via%20Travel%20Video%20Itinerary%20Reconstruction，链接是  https://arxiv.org/pdf/2509.19002  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19002  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hao Wang,Eiki Murata,Lingfang Zhang,Ayako Sato,So Fukuda,Ziqi Yin,Wentao Hu,Keisuke Nakao,Yusuke Nakamura,Sebastian Zwirner,Yi-Chia Chen,Hiroyuki Otomo,Hiroki Ouchi,Daisuke Kawahara</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19002.pdf">https://arxiv.org/pdf/2509.19002.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19002">https://arxiv.org/abs/2509.19002</a></p>
<p><strong>Arxiv ID</strong>: 2509.19002</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19002">https://papers.cool/arxiv/2509.19002</a></p>
<p><strong>Published</strong>: 2025-09-23T13:46:31Z</p>
<p><strong>Updated</strong>: 2025-09-23T13:46:31.000Z</p>
<hr>
<h3 id="19-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction"><a href="#19-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction" class="headerlink" title="19. GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction"></a>19. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.18090">GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</a></h3><p>Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at <a target="_blank" rel="noopener" href="https://github.com/Fictionarry/GeoSVR">https://github.com/Fictionarry/GeoSVR</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>近年来，利用辐射场重建精确表面取得了显著进展。然而，当前主要基于高斯喷溅的方法越来越受到表示瓶颈的限制。在本文中，我们介绍了GeoSVR，这是一个显式的基于体素的框架，探讨并扩展了稀疏体素在实现精确、详细和完整表面重建方面的潜力。稀疏体素的优势在于支持保持覆盖完整性和几何清晰度，但在表面精炼中也会出现由于缺乏场景约束和局部性而带来的挑战。为了确保正确的场景收敛，我们首先提出了一种体素不确定性深度约束，该约束最大化单目深度线索的效果，同时呈现体素导向的不确定性以避免质量下降，从而实现有效且稳健的场景约束，同时保持高度准确的几何结构。随后，设计了稀疏体素表面正则化，以增强小体素的几何一致性，并促进基于体素的锐利和精确表面的形成。大量实验表明，与现有方法相比，我们在各种挑战场景中的表现更为优越，在几何精度、细节保持和重建完整性方面表现出色，同时保持高效率。代码可在 <a target="_blank" rel="noopener" href="https://github.com/Fictionarry/GeoSVR">https://github.com/Fictionarry/GeoSVR</a> 获得。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed: Error: 抓取失败: Waiting failed: 30000ms exceeded</p>
</div></details>
<p><strong>Authors</strong>: Jiahe Li,Jiawei Zhang,Youmin Zhang,Xiao Bai,Jin Zheng,Xiaohan Yu,Lin Gu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.18090.pdf">https://arxiv.org/pdf/2509.18090.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.18090">https://arxiv.org/abs/2509.18090</a></p>
<p><strong>Arxiv ID</strong>: 2509.18090</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.18090">https://papers.cool/arxiv/2509.18090</a></p>
<p><strong>Published</strong>: 2025-09-22T17:58:48Z</p>
<p><strong>Updated</strong>: 2025-09-22T17:58:48.000Z</p>
<hr>
<h3 id="20-DRISHTIKON-A-Multimodal-Multilingual-Benchmark-for-Testing-Language-Models’-Understanding-on-Indian-Culture"><a href="#20-DRISHTIKON-A-Multimodal-Multilingual-Benchmark-for-Testing-Language-Models’-Understanding-on-Indian-Culture" class="headerlink" title="20. DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models’ Understanding on Indian Culture"></a>20. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.19274">DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models’ Understanding on Indian Culture</a></h3><p>We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual benchmark centered exclusively on Indian culture, designed to evaluate the cultural understanding of generative AI systems. Unlike existing benchmarks with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage across India’s diverse regions, spanning 15 languages, covering all states and union territories, and incorporating over 64,000 aligned text-image pairs. The dataset captures rich cultural themes including festivals, attire, cuisines, art forms, and historical heritage amongst many more. We evaluate a wide range of vision-language models (VLMs), including open-source small and large models, proprietary systems, reasoning-specialized VLMs, and Indic-focused models, across zero-shot and chain-of-thought settings. Our results expose key limitations in current models’ ability to reason over culturally grounded, multimodal inputs, particularly for low-resource languages and less-documented traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a robust testbed to advance culturally aware, multimodally competent language technologies.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了 DRISHTIKON，这是一个独一无二的多模态和多语言基准，专注于印度文化，旨在评估生成性 AI 系统的文化理解能力。与现有的具有通用或全球范围的基准不同，DRISHTIKON 提供了对印度各个多样化地区的深入、细致的覆盖，涵盖 15 种语言，覆盖所有州和联邦直辖区，并结合了超过 64,000 对对齐的文本-图像数据。该数据集捕捉了丰富的文化主题，包括节日、服饰、菜肴、艺术形式以及历史遗产等众多内容。我们评估了广泛的视觉语言模型 (VLM)，包括开源的小型和大型模型、专有系统、面向推理的 VLM，以及以印度文为重点的模型，在零-shot 和思维链设置下进行评估。我们的结果揭示了当前模型在处理以文化为基础的多模态输入时的关键局限性，特别是在低资源语言和记录较少的传统方面。DRISHTIKON 填补了包容性 AI 研究中的重要空白，提供了一个强有力的试验平台，以推动具有文化意识和多模态能力的语言技术的发展。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>DRISHTIKON 旨在填补现有评测体系在“印度文化”这一特定语境下的空白，解决以下核心问题：</p>
<ol>
<li><p>文化盲区<br>现有视觉-语言模型（VLM）评测多聚焦通用知识或西方中心内容，对印度 28 邦、8 联邦属地、15 种官方语言的丰富节日、服饰、艺术、仪式等缺乏系统评估，导致模型在印度场景下频繁出现误读、刻板化或遗漏。</p>
</li>
<li><p>多模态-多语言缺口<br>既有基准要么只测文本（如 TyDi QA），要么仅做图像-文本对齐，无法同时检验低资源语言与文化视觉符号的联合推理能力；DRISHTIKON 提供 64 288 对经人工校验的文本-图像-语言三元组，首次实现“同题同图”覆盖 15 语。</p>
</li>
<li><p>细粒度文化推理不足<br>通过引入“常识-文化”“多跳”“类比”三类推理型 MCQ，揭示当前模型在区域特定象征（如 Warli 图腾、Manipuri 舞衣、Baul 音乐精神性）上的深层推理缺陷，推动从表面识别走向文化语境化推理。</p>
</li>
<li><p>数字公平与包容 AI<br>实验结果显示，对 Sindhi、Konkani 等低资源语言准确率骤降 40%+，凸显训练数据分布失衡；DRISHTIKON 作为公开评测床，促使未来研究在数据、模型与推理框架上向“文化包容”倾斜。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线：<br>（i）<strong>多模态-多语言文化基准</strong><br>（ii）<strong>印度区域文化语料与评测</strong></p>
<p>(i) 多模态-多语言文化基准</p>
<p>工作</p>
<p>覆盖范围</p>
<p>关键发现/局限</p>
<p><strong>M5</strong> (Schneider &amp; Sitaram, 2024)</p>
<p>41 种语言，视觉问答</p>
<p>大模型在低资源语言上反而落后；未聚焦印度。</p>
<p><strong>CVQA</strong> (Romero et al., 2025)</p>
<p>26 国、9 k 问题</p>
<p>暴露模型对非西方文化视觉细节薄弱；印度仅零星出现。</p>
<p><strong>GIMMICK</strong> (Schneider et al., 2025)</p>
<p>144 国、728 文化面</p>
<p>揭示强西方偏见；无印度全境细粒度标签。</p>
<p><strong>CulturalVQA</strong> (Nayak et al., 2024)</p>
<p>11 国</p>
<p>GPT-4V/Gemini 对北美内容显著优于非洲/南亚。</p>
<p><strong>CulturalBench</strong> (Chiu et al., 2024)</p>
<p>跨 9 文化维度文本题</p>
<p>纯文本，无视觉模态，印度覆盖有限。</p>
<p><strong>SEA-Eval</strong> (Wang et al., 2024a)</p>
<p>东南亚多语</p>
<p>未纳入印度 15 语及视觉文化符号。</p>
<p><strong>JMMMU</strong> (Onohara et al., 2024)</p>
<p>日本多模态</p>
<p>区域单一，方法可供印度借鉴但无法直接迁移。</p>
<p>(ii) 印度区域文化语料与评测</p>
<p>工作</p>
<p>规模/模态</p>
<p>与 DRISHTIKON 的差异</p>
<p><strong>DOSA</strong> (Seth et al., 2024)</p>
<p>615 文物、19 亚文化，文本</p>
<p>社区驱动，无图像-文本对齐；题量小，未覆盖全印。</p>
<p><strong>IndiBias</strong> (Sahoo et al., 2024a)</p>
<p>双语偏见问答</p>
<p>聚焦社会偏见而非文化知识；无视觉。</p>
<p><strong>IndicQuest</strong> (Rohera et al., 2024)</p>
<p>200 QA 对、19 语</p>
<p>纯文本事实问答；无多模态与推理型题目。</p>
<p><strong>Indian-BhED</strong> (Khandelwal et al., 2024)</p>
<p>刻板印象检测</p>
<p>目的在公平性诊断，非文化理解评测。</p>
<p><strong>IndicGLUE/XTREME</strong> (Kakwani et al., 2020; Doddapaneni et al., 2023)</p>
<p>88 亿/209 亿词单语语料</p>
<p>大规模文本预训练资源，但未提供文化视觉对齐任务。</p>
<p><strong>SANSKRITI</strong> (Maji et al., 2025)</p>
<p>文本-only 印度文化基准</p>
<p>同期工作，无图像模态，题量与地域覆盖亦不同。</p>
<p>小结</p>
<p>既有研究或缺印度全境、或缺视觉模态、或缺低资源语言，且几乎未同时提供“多跳-类比-常识”三级文化推理题型。DRISHTIKON 首次将 36 行政区域 × 15 语言 × 64 k 图文对的细粒度文化推理纳入统一基准，填补了上述空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“构建-评测-诊断”三步框架，系统解决印度文化场景下多模态-多语言模型缺乏细粒度评测与推理基准的问题。</p>
<p>1. 构建：DRISHTIKON 基准</p>
<p>关键设计</p>
<p>技术/流程</p>
<p>解决痛点</p>
<p><strong>全域知识库</strong></p>
<p>28 邦+8 联邦属地官方文旅、学术、媒体等 6 大来源交叉验证 → 2 126 英文 MCQ</p>
<p>消除地域与主题盲区</p>
<p><strong>文化属性标注</strong></p>
<p>动态 16 维分类体系（服饰/仪式/艺术…）+ 单标签强制一致，κ=0.82</p>
<p>支持按文化维度切片分析</p>
<p><strong>推理增强</strong></p>
<p>每区域 20 题分层采样 → 720 题扩写为 3 类推理：常识/多跳/类比，共 2 160 题</p>
<p>超越表面识别，检验深层文化链式推理</p>
<p><strong>多语规模化</strong></p>
<p>Gemini Pro 机翻 + 两轮人工校验 → 14 种印度语言，总量 64 288 图文对</p>
<p>覆盖低资源语言，量化语言偏差</p>
<p><strong>多模态对齐</strong></p>
<p>每题绑定一张经双重审核的公开高清图，附 URL 合规发布</p>
<p>确保视觉-文本-文化三元组一致可复现</p>
<p>2. 评测：统一实验协议</p>
<p>模块</p>
<p>设定</p>
<p>目的</p>
<p><strong>模型谱</strong></p>
<p>11 类 VLM：小模型(256M-1B)／大模型(7B-27B)／专有(GPT-4o)／推理专精(Kimi-VL)／印度特化(Chitrarth, Maya)</p>
<p>检验规模、架构、文化对齐策略的影响</p>
<p><strong>提示策略</strong></p>
<p>Zero-shot + 文化链式思维(CoT)四步提示(Drishti- Smriti-Yukti-Sthiti)</p>
<p>测试显式推理脚手架对文化题的增益</p>
<p><strong>指标</strong></p>
<p>准确率 + 区域/属性/语言/题型四维细粒度分解</p>
<p>精准定位失败面</p>
<p>3. 诊断：实验发现 → 后续行动</p>
<p>主要发现</p>
<p>对应解决路径</p>
<p><strong>语言落差</strong></p>
<p>Sindhi/Konkani 准确率较英语↓40%+ → 需增投低资源多模态预训练数据</p>
<p><strong>区域偏差</strong></p>
<p>拉克沙/米佐/达曼等地持续低迷 → 建立“地理再平衡”采样策略，或引入区域自适应微调</p>
<p><strong>推理瓶颈</strong></p>
<p>多跳与类比题型普遍下滑 10-30% → 设计文化知识图谱增强的链式推理预训练任务</p>
<p><strong>规模≠性能</strong></p>
<p>小模型 SmolVLM-256M 反超部分 7B 模型 → 鼓励参数高效+文化指令微调路线</p>
<p><strong>CoT 增益不均</strong></p>
<p>高资源语言受益更多 → 开发语言无关的跨文化推理提示模板</p>
<p>结果输出</p>
<ul>
<li>公开数据集与推理代码：<code>https://tinyurl.com/DrishtikonDataset</code></li>
<li>提供 Excel/CSV 表格、URL 列表、文化标签与题型标注，可直接用于后续模型训练与公平性审计。</li>
</ul>
<p>通过上述“构建-评测-诊断”闭环，论文不仅揭示当前 VLM 在印度文化语境下的系统性短板，也为后续包容性数据收集、文化感知架构设计与区域公平部署提供了可落地的基准与改进路线图。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“谁在什么语言-文化-题型上表现如何”展开，采用统一输入格式与指标，对 15 种语言、36 个区域、4 类题型的 64 288 题进行大规模评测，核心可归纳为 5 组实验。</p>
<p>1. 模型规模 vs 性能（RQ1）</p>
<ul>
<li><strong>对照组</strong>：参数 256 M–27 B 跨度，含小模型(SmolVLM-256M、InternVL3-1B)、大模型(Qwen2-VL-7B、Llama-4-Scout-17B 等)、专有(GPT-4o-mini)。</li>
<li><strong>观测指标</strong>：整体准确率、参数-性能散点。</li>
<li><strong>结论</strong>：<ul>
<li>GPT-4o-mini 持续最高；</li>
<li>小模型 SmolVLM-256M 在多语多模态任务上反超部分 7 B 级模型，说明“参数规模”非充分条件。</li>
</ul>
</li>
</ul>
<p>2. 语言难度谱（RQ2）</p>
<ul>
<li><strong>变量</strong>：15 语言（英、印地、孟加拉、泰米尔…信德、孔卡尼）。</li>
<li><strong>统计</strong>：每语平均准确率 + 语言-区域热图。</li>
<li><strong>结论</strong>：<ul>
<li>英语≈饱和，印地/孟加拉/马拉地次高；</li>
<li>信德、孔卡尼、卡纳达骤降 40 %+，暴露低资源语言系统性落后。</li>
</ul>
</li>
</ul>
<p>3. 题型挑战度（RQ3）</p>
<ul>
<li><strong>分类</strong>：General、Common-Sense Cultural、Multi-hop、Analogy。</li>
<li><strong>统计</strong>：题型-平均准确率 &amp; CoT 提升幅度。</li>
<li><strong>结论</strong>：<ul>
<li>General &amp; Common-Sense 最高；</li>
<li>Multi-hop 下降 15-25 %，Analogy 方差最大；</li>
<li>CoT 对多跳/类比提升 10-15 %，但对 Common-Sense 边际收益小。</li>
</ul>
</li>
</ul>
<p>4. 模型类别对比（RQ4）</p>
<ul>
<li><strong>分组</strong>：SLM、LLM、Proprietary、Reasoning-specialized、Indic-aligned。</li>
<li><strong>统计</strong>：组内平均准确率 + 文化属性雷达图。</li>
<li><strong>结论</strong>：<ul>
<li>Indic 模型 Maya 表现超越多数 LLM，证明区域精调有效；</li>
<li>推理专精 Kimi-VL 在文化场景泛化差；</li>
<li>Proprietary 依旧全面领先，但效率-性能曲线可被 SLM+优质指令逼近。</li>
</ul>
</li>
</ul>
<p>5. Zero-shot vs Chain-of-Thought（RQ5）</p>
<ul>
<li><strong>设计</strong>：同一模型、同一题库，分别使用标准提示与四步文化 CoT 提示（Drishti-Smriti-Yukti-Sthiti）。</li>
<li><strong>统计</strong>：CoT ΔAcc = Acc_CoT − Acc_ZS，按题型/语言分解。</li>
<li><strong>结论</strong>：<ul>
<li>CoT 平均带来 +6.8 % 绝对提升，高资源语言受益更多；</li>
<li>对 Multi-hop/Analogy 提升最高达 15 %，但对低资源语言增益有限，揭示“推理脚手架”本身也需多语文化对齐。</li>
</ul>
</li>
</ul>
<p>辅助分析</p>
<ul>
<li><strong>雷达图</strong>：36 区域 × 16 文化属性二维可视化，一眼定位“拉克沙-米佐-达曼”等冷区与“宗教-医药-夜生活”等抽象属性凹陷。</li>
<li><strong>误差人工剖析</strong>：随机抽取 100 例 GPT-4o-mini 错例，归类为“细粒度语义混淆”“视觉-区域知识缺失”“训练数据流行度偏差”三类，为后续数据增强提供靶向。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在大规模、多模态、多语言、文化推理四个维度上继续深化，均直接建立在 DRISHTIKON 的实验结论与公开资源之上。</p>
<p>1. 数据与标注</p>
<ul>
<li><strong>方言与口音层</strong><br>将 15 官方语言扩展到 50+ 方言／口音语音-文本对，评测 VLM 对“同文不同音”文化语境的鲁棒性。</li>
<li><strong>时序文化演变</strong><br>引入 1950-2020 时间切片标签（节日形式、服饰流行度变化），构建动态文化 QA，检验模型对“文化漂移”的追踪能力。</li>
<li><strong>开放式生成评测</strong><br>在现有 MCQ 外，发布同一图文对的开放式问答与理由阐述，采用 BLEU-RL+人工专家评分，衡量模型“可解释文化推理”。</li>
<li><strong>对抗性 distractor 库</strong><br>基于误差剖析自动生成“视觉-语义近邻”高混淆选项，形成难度递增的 curriculum benchmark，推动注意力-视觉定位研究。</li>
</ul>
<p>2. 模型与训练策略</p>
<ul>
<li><strong>区域自适应持续预训练</strong><br>用 DRISHTIKON 图文对继续预训练 1-3 B 小模型，对比“通用语料→区域语料”两阶段与混合比例的 scaling law。</li>
<li><strong>文化知识图谱增强推理</strong><br>将印度节日-菜肴-服饰三元组编码为 KGE，与 VLM 中间层做 cross-attention 融合，检验 Multi-hop 与 Analogy 提升上限。</li>
<li><strong>多语视觉对齐重平衡</strong><br>采用梯度掩码或重加权，对低资源语言（Sindhi/Konkani）图文对比损失进行上采样，量化“语言再平衡”对公平指标的边际收益。</li>
<li><strong>参数高效文化微调</strong><br>比较 LoRA/AdaLoRA/DoRA 在视觉编码器 vs LLM 侧的不同插入深度，寻找“文化知识”注入的最小参数闭合集。</li>
</ul>
<p>3. 评测与现象挖掘</p>
<ul>
<li><strong>跨文化迁移矩阵</strong><br>利用 36×36 区域混淆矩阵，量化“文化邻近度”与模型准确率的相关性，验证“共享历史-政治边界”是否构成迁移优势。</li>
<li><strong>视觉定位可解释性</strong><br>采用 Grad-CAM++/attention rollout，检查模型是否关注关键文化视觉元素（如头巾纹样、祭器形状），解释错误案例的注意力偏差。</li>
<li><strong>价值观-文化一致性审计</strong><br>将 DRISHTIKON 与 Hofstede 文化维度标签对齐，评测 VLM 输出是否隐含区域价值倾向，为“文化安全”部署提供量化指标。</li>
<li><strong>多轮对话文化一致性</strong><br>把单跳 QA 扩展为 3-轮对话，引入用户误导性提示，测试模型在多轮语境下能否保持文化事实一致性（类似文化-specific safety）。</li>
</ul>
<p>4. 应用与伦理</p>
<ul>
<li><strong>文化遗产 AR/VR 问答</strong><br>把 DRISHTIKON 图文对接入实景 AR 导览，实时回答游客关于壁画、仪式的问题，验证“室外光照-角度”对 VLM 的影响。</li>
<li><strong>教育场景个性化</strong><br>结合学生所在邦与母语，自动抽取对应文化子集生成课堂测验，评估“文化贴近”对学习参与度的教育心理学增益。</li>
<li><strong>版权与合规再研究</strong><br>探索“文本-图像生成”反向风险：用 DRISHTIKON 提示 StableDiffusion-XL，检查生成样本是否侵犯传统图腾版权，建立文化敏感生成红队测试。</li>
<li><strong>公平补偿机制</strong><br>基于 annotator 贡献度与后续商业授权收益，设计区块链版税追踪原型，验证“社区驱动数据”能否持续获得回报。</li>
</ul>
<p>5. 技术-社会交叉</p>
<ul>
<li><strong>“文化遗忘”持续监测</strong><br>每 6 个月重新测试同一批模型，记录在新通用语料继续预训练后 DRISHTIKON 分数变化，量化“灾难性文化遗忘”速率。</li>
<li><strong>政策决策支持</strong><br>将基准结果转化为印度数字包容指数 (IDII)，为政府“AI for All”战略提供低资源语言投入优先级的数据证据。</li>
</ul>
<p>这些探索点既涵盖算法创新（知识增强、参数高效、视觉可解释），也涉及数据扩展（方言、时序、开放式），并延伸到伦理与社会影响（版权、补偿、政策），可充分利用已公开的 64 k 图文对与实验脚本快速起步。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>DRISHTIKON 是一项面向印度文化场景的多模态-多语言评测研究，核心内容可概括为“一个基准、三组创新、五大发现”。</p>
<p>① 一个基准</p>
<ul>
<li><strong>规模</strong>：64 288 图文对，覆盖 36 行政区域、15 语言、16 文化主题。</li>
<li><strong>题型</strong>：2 126 基础 MCQ + 2 160 推理增强 MCQ（常识／多跳／类比）。</li>
<li><strong>标注</strong>：人工两审 + 专家仲裁，κ=0.82；附文化属性标签与公开图像 URL。</li>
</ul>
<p>② 三组创新</p>
<ol>
<li><strong>全域文化挖掘</strong>：首次把 28 邦、8 联邦属地官方文旅与学术资源系统转化为可评测 MCQ。</li>
<li><strong>推理型多语扩写</strong>：720 题分层采样→三倍扩写→人工校验，生成深层文化链式推理题。</li>
<li><strong>文化链式思维提示</strong>：基于印度认识论“见-忆-理-境”四步 CoT，提升可解释性与区域语境一致性。</li>
</ol>
<p>③ 五大发现</p>
<ul>
<li><strong>规模≠性能</strong>：256 M 小模型 SmolVLM 反超部分 7 B 模型；GPT-4o-mini 整体领先。</li>
<li><strong>语言落差</strong>：低资源 Sindhi、Konkani 准确率较英语↓40 %+，揭示数据不平衡。</li>
<li><strong>题型瓶颈</strong>：Multi-hop 与 Analogy 题普遍下滑 10-25 %，CoT 仅部分缓解。</li>
<li><strong>区域偏差</strong>：拉克沙、米佐等“冷区”持续低迷，反映预训练语料地理分布失衡。</li>
<li><strong>文化对齐有效</strong>：印度原生 Maya 模型以轻量级参数取得比肩大模型的文化精度，验证区域微调价值。</li>
</ul>
<p>结论</p>
<p>DRISHTIKON 填补了印度文化多模态评测空白，公开数据与代码，为构建包容、可信、文化感知的生成式 AI 提供标准化试金石与改进路线图。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是DRISHTIKON%3A%20A%20Multimodal%20Multilingual%20Benchmark%20for%20Testing%20Language%20Models%27%20Understanding%20on%20Indian%20Culture，链接是  https://arxiv.org/pdf/2509.19274  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.19274  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Arijit Maji,Raghvendra Kumar,Akash Ghosh,Anushka,Nemil Shah,Abhilekh Borah,Vanshika Shah,Nishant Mishra,Sriparna Saha</p>
<p><strong>Categories</strong>: cs.CL,cs.MM</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.19274.pdf">https://arxiv.org/pdf/2509.19274.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.19274">https://arxiv.org/abs/2509.19274</a></p>
<p><strong>Arxiv ID</strong>: 2509.19274</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.19274">https://papers.cool/arxiv/2509.19274</a></p>
<p><strong>Published</strong>: 2025-09-23T17:40:43Z</p>
<p><strong>Updated</strong>: 2025-09-23T17:40:43.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202509/2025-09-25/">https://wdndev.github.io/daily/hf/202509/2025-09-25/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Baseer-A-Vision-Language-Model-for-Arabic-Document-to-Markdown-OCR"><span class="toc-text">1. Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Reinforcement-Learning-on-Pre-Training-Data"><span class="toc-text">2. Reinforcement Learning on Pre-Training Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Do-You-Need-Proprioceptive-States-in-Visuomotor-Policies"><span class="toc-text">3. Do You Need Proprioceptive States in Visuomotor Policies?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-MiniCPM-V-4-5-Cooking-Efficient-MLLMs-via-Architecture-Data-and-Training-Recipe"><span class="toc-text">4. MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-MAPO-Mixed-Advantage-Policy-Optimization"><span class="toc-text">5. MAPO: Mixed Advantage Policy Optimization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Hyper-Bagel-A-Unified-Acceleration-Framework-for-Multimodal-Understanding-and-Generation"><span class="toc-text">6. Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-VolSplat-Rethinking-Feed-Forward-3D-Gaussian-Splatting-with-Voxel-Aligned-Prediction"><span class="toc-text">7. VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Lyra-Generative-3D-Scene-Reconstruction-via-Video-Diffusion-Model-Self-Distillation"><span class="toc-text">8. Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-What-Characterizes-Effective-Reasoning-Revisiting-Length-Review-and-Structure-of-CoT"><span class="toc-text">9. What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Large-Language-Models-Discriminate-Against-Speakers-of-German-Dialects"><span class="toc-text">10. Large Language Models Discriminate Against Speakers of German Dialects</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Soft-Tokens-Hard-Truths"><span class="toc-text">11. Soft Tokens, Hard Truths</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-CAR-Flow-Condition-Aware-Reparameterization-Aligns-Source-and-Target-for-Better-Flow-Matching"><span class="toc-text">12. CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-OpenGVL-Benchmarking-Visual-Temporal-Progress-for-Data-Curation"><span class="toc-text">13. OpenGVL - Benchmarking Visual Temporal Progress for Data Curation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-HyRF-Hybrid-Radiance-Fields-for-Memory-efficient-and-High-quality-Novel-View-Synthesis"><span class="toc-text">14. HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Better-Late-Than-Never-Evaluation-of-Latency-Metrics-for-Simultaneous-Speech-to-Text-Translation"><span class="toc-text">15. Better Late Than Never: Evaluation of Latency Metrics for Simultaneous Speech-to-Text Translation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-CommonForms-A-Large-Diverse-Dataset-for-Form-Field-Detection"><span class="toc-text">16. CommonForms: A Large, Diverse Dataset for Form Field Detection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-Zero-Shot-Multi-Spectral-Learning-Reimagining-a-Generalist-Multimodal-Gemini-2-5-Model-for-Remote-Sensing-Applications"><span class="toc-text">17. Zero-Shot Multi-Spectral Learning: Reimagining a Generalist Multimodal Gemini 2.5 Model for Remote Sensing Applications</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-VIR-Bench-Evaluating-Geospatial-and-Temporal-Understanding-of-MLLMs-via-Travel-Video-Itinerary-Reconstruction"><span class="toc-text">18. VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-GeoSVR-Taming-Sparse-Voxels-for-Geometrically-Accurate-Surface-Reconstruction"><span class="toc-text">19. GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20-DRISHTIKON-A-Multimodal-Multilingual-Benchmark-for-Testing-Language-Models%E2%80%99-Understanding-on-Indian-Culture"><span class="toc-text">20. DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models’ Understanding on Indian Culture</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>