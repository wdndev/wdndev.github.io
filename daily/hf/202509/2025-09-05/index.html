<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-09-05 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Robix: A Unified Model for Robot Interaction, Reasoning and PlanningWe introduce Robix, a unified model that integrates robot reasoning, task planning, and na">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-09-05">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202509/2025-09-05/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Robix: A Unified Model for Robot Interaction, Reasoning and PlanningWe introduce Robix, a unified model that integrates robot reasoning, task planning, and na">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.815Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202509/2025-09-05/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-09-05',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-09-05</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.815Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">33.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>118min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-09-05"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning"><a href="#1-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning" class="headerlink" title="1. Robix: A Unified Model for Robot Interaction, Reasoning and Planning"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.01106">Robix: A Unified Model for Robot Interaction, Reasoning and Planning</a></h3><p>We introduce Robix, a unified model that integrates robot reasoning, task planning, and natural language interaction within a single vision-language architecture. Acting as the high-level cognitive layer in a hierarchical robot system, Robix dynamically generates atomic commands for the low-level controller and verbal responses for human interaction, enabling robots to follow complex instructions, plan long-horizon tasks, and interact naturally with human within an end-to-end framework. Robix further introduces novel capabilities such as proactive dialogue, real-time interruption handling, and context-aware commonsense reasoning during task execution. At its core, Robix leverages chain-of-thought reasoning and adopts a three-stage training strategy: (1) continued pretraining to enhance foundational embodied reasoning abilities including 3D spatial understanding, visual grounding, and task-centric reasoning; (2) supervised finetuning to model human-robot interaction and task planning as a unified reasoning-action sequence; and (3) reinforcement learning to improve reasoning-action consistency and long-horizon task coherence. Extensive experiments demonstrate that Robix outperforms both open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in interactive task execution, demonstrating strong generalization across diverse instruction types (e.g., open-ended, multi-stage, constrained, invalid, and interrupted) and various user-involved tasks such as table bussing, grocery shopping, and dietary filtering.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍Robix，这是一个统一模型，将机器人推理、任务规划和自然语言交互整合到一个单一的视觉-语言架构中。作为层次化机器人系统中的高级认知层，Robix动态生成原子指令供低级控制器使用，并生成口头响应以便与人类互动，使机器人能够执行复杂指令、规划长远任务，并在端到端框架内与人类自然互动。Robix进一步引入了新的能力，如主动对话、实时中断处理和任务执行中的上下文感知常识推理。Robix的核心利用链式思维推理，并采用三阶段训练策略：(1) 继续预训练以增强基础的具身推理能力，包括三维空间理解、视觉绑定和任务中心的推理；(2) 监督微调以将人机交互和任务规划建模为统一的推理-行动序列；(3) 强化学习以改善推理-行动一致性和长远任务的连贯性。大量实验表明，Robix在交互任务执行中超过了开源和商业基准（例如，GPT-4o和Gemini 2.5 Pro），展示出在多种指令类型（例如，开放式、多阶段、受限、无效和中断）及各种用户参与的任务（如餐桌清理、购物和饮食过滤）中的强泛化能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p><strong>问题陈述</strong></p>
<p>Robix 旨在解决<strong>通用机器人系统在开放、动态环境中执行复杂、长周期任务时，如何同时实现高层次认知推理、任务规划与自然语言交互</strong>这一核心难题。具体而言，现有方法存在以下关键缺陷：</p>
<ol>
<li><strong>模块化系统僵化</strong>：传统分层架构将推理、规划与交互拆分为独立模块，依赖人工设计的工作流，导致系统脆弱、难以适应开放场景。</li>
<li><strong>缺乏统一建模</strong>：现有 VLM 仅聚焦任务分解，忽视实时人机交互与物理常识推理，无法端到端处理“理解-规划-交互”闭环。</li>
<li><strong>具身推理不足</strong>：通用 VLM 在数字领域表现良好，但迁移到机器人时，面临三维空间理解、视觉定位与任务中心推理的显著性能下降。</li>
<li><strong>交互能力缺失</strong>：现有系统无法处理实时中断、模糊指令澄清或主动对话，导致用户体验差、任务失败率高。</li>
</ol>
<p>Robix 通过<strong>单一视觉-语言模型</strong>统一解决上述问题，使机器人能够：</p>
<ul>
<li>理解复杂多阶段指令（如“仅在人用餐完毕后清理盘子”）；</li>
<li>实时响应人类中断或修正（如“留下那只杯子”）；</li>
<li>主动澄清歧义或推断意图（如询问“是否丢弃未吃完的食物”）；</li>
<li>在动态环境中动态重规划，确保长周期任务连贯完成。</li>
</ul>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>相关研究可划分为三大方向，分别对应 Robix 试图整合的<strong>任务规划、人机交互与具身推理</strong>能力。以下按类别梳理代表性工作，并指出其与 Robix 的差异。</p>
<p>1. 机器人任务规划（Robotic Task Planning）</p>
<p>方法</p>
<p>核心思想</p>
<p>局限性</p>
<p><strong>LLM-Planner</strong> [60]、<strong>Inner Monologue</strong> [29]</p>
<p>用 LLM 将高层指令分解为子任务，再调用低层控制器执行</p>
<p>无视觉感知，生成计划常脱离物理可行性</p>
<p><strong>COME-robot</strong> [88]、<strong>VILA</strong> [28]、<strong>REPLAN</strong> [59]</p>
<p>基于 GPT-4V 做闭环重规划，结合视觉反馈修正</p>
<p>仅聚焦任务分解，缺乏人机交互与长期一致性机制</p>
<p><strong>Manipulate-Anything</strong> [17]、<strong>Bumble</strong> [53]</p>
<p>用 VLM 直接生成原子动作，支持开放词汇指令</p>
<p>未整合交互能力，长周期任务易漂移</p>
<blockquote>
<p>Robix 差异：将<strong>规划-交互-推理</strong>统一在单一 VLM 内，避免模块化脆弱性，并通过链式思考与 RL 提升长期一致性。</p>
</blockquote>
<p>2. 人机交互（Human-Robot Interaction）</p>
<p>方法</p>
<p>核心思想</p>
<p>局限性</p>
<p><strong>OLAF</strong> [38]、<strong>YAY Robot</strong> [56]</p>
<p>用 LLM 实时解析人类语言修正，更新策略</p>
<p>仅支持局部修正，无法处理复杂多轮对话</p>
<p><strong>RT-H</strong> [5]</p>
<p>语言指令映射到固定基元动作</p>
<p>交互粒度受限，难以处理开放场景</p>
<p><strong>Hi Robot</strong> [57]</p>
<p>分层 VLA 架构，支持开放指令与实时纠正</p>
<p>交互与规划分离，需人工设计接口</p>
<blockquote>
<p>Robix 差异：原生支持<strong>实时中断、主动澄清、上下文对话</strong>，并将交互历史纳入统一推理链，无需额外模块。</p>
</blockquote>
<p>3. 具身推理（Embodied Reasoning）</p>
<p>方法</p>
<p>核心思想</p>
<p>局限性</p>
<p><strong>Embodied-Reasoner</strong> [85]</p>
<p>在 VLM 中显式建模观察-思考-动作轨迹</p>
<p>仅用于视觉搜索任务，未涉及交互</p>
<p><strong>Gemini Robotics-ER</strong> [64]</p>
<p>将 3D 感知、状态估计嵌入 VLM</p>
<p>侧重感知能力，未整合长周期任务规划</p>
<p><strong>Cosmos-Reason1</strong> [2]、<strong>RoboBrain-2.0</strong> [63]</p>
<p>构建大规模具身推理数据集，强化常识</p>
<p>数据驱动，未解决交互与动作一致性</p>
<blockquote>
<p>Robix 差异：通过<strong>三阶段训练</strong>（预训练→SFT→RL）显式对齐<strong>物理常识、任务状态监控与动作一致性</strong>，并首次在统一模型中整合交互能力。</p>
</blockquote>
<p>小结</p>
<p>现有研究或聚焦单一能力（规划/交互/推理），或依赖模块化拼接；Robix 首次提出<strong>端到端统一框架</strong>，在单一 VLM 内同时实现：</p>
<ul>
<li>长周期任务规划</li>
<li>实时人机交互</li>
<li>具身常识推理</li>
</ul>
<p>并通过大规模数据合成与 RL 解决动作-思维一致性问题，显著优于 GPT-4o、Gemini 2.5 Pro 等商业模型。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>Robix 通过“一个统一架构 + 三阶段训练 + 两类数据增强”的系统化方案，把<strong>机器人推理、任务规划、自然语言交互</strong>整合进单一 vision-language 模型，从根本上解决模块化系统僵化、交互缺失与具身推理不足的问题。</p>
<p>1. 统一架构：端到端 vision-language “大脑”</p>
<ul>
<li><strong>输入</strong>：当前视觉观测 + 用户指令 + 交互历史（短窗口）</li>
<li><strong>输出</strong>：<ul>
<li><code>think</code>：链式思考（场景理解、任务状态、下一步分析）</li>
<li><code>action</code>：原子级动作指令给低层控制器</li>
<li><code>response</code>：自然语言回复给用户</li>
</ul>
</li>
<li><strong>公式化建模</strong>  <script type="math/tex; mode=display">P(t\_n, a\_n, r\_n \\mid o\_{1:n}, u\_{1:n}, t\_{1:n-1}, a\_{1:n-1}, r\_{1:n-1})</script>每一步同时预测思考、动作与回复，实现<strong>单模型闭环</strong>。</li>
</ul>
<p>2. 三阶段训练策略</p>
<p>阶段</p>
<p>目标</p>
<p>关键设计</p>
<p><strong>① Continued Pretraining</strong><br>（200 B tokens）</p>
<p>夯实具身基础能力</p>
<p>- 3D 空间理解（30 M 样本）<br>- 视觉定位（50 M 样本）<br>- 任务中心推理（5 M 样本）<br>- 通用 VQA / OCR / Caption（&gt;100 M 样本）</p>
<p><strong>② Supervised Fine-Tuning</strong></p>
<p>学会“边想边干边说”</p>
<p>- 数据合成：把机器人演示 → 多轮人机对话轨迹<br>- 7 类交互指令：多阶段、约束、随时中断、无效/模糊/聊天等<br>- 链式思考模板：Scene → Status → Long-term → Next-step</p>
<p><strong>③ Reinforcement Learning</strong><br>（GRPO）</p>
<p>消除“想-做”不一致</p>
<p>- 联合训练：机器人交互数据 + 通用视觉推理数据<br>- 奖励设计：<br>  • 动作准确率<br>  • 格式合规<br>  • <strong>思考-动作一致性</strong>（由 Qwen-2.5-32B 作为裁判模型）<br>- 方差过滤：仅保留高梯度样本</p>
<p>3. 数据工程：低成本、高覆盖的合成方案</p>
<ul>
<li><strong>交互数据</strong><ul>
<li>来源：真实遥操作（GR-3、AGIBot）+ 仿真 + AIGC 场景</li>
<li>自动生成：任务分解 → 指令合成 → 中断注入 → 回复生成</li>
</ul>
</li>
<li><strong>推理数据</strong><ul>
<li>用 Seed-1.5-VL-Think 生成 200 token 内的<strong>紧凑思考链</strong></li>
<li>人工 + 模型双重过滤，去除幻觉与逻辑错误</li>
</ul>
</li>
</ul>
<p>4. 运行时机制</p>
<ul>
<li><strong>短窗口记忆</strong>：保留最近 N 帧视觉观测 + 全部思考-动作历史，兼顾效率与一致性</li>
<li><strong>实时重规划</strong>：收到人类中断后立即更新 <code>think</code> 与 <code>action</code>，无需重启流程</li>
<li><strong>主动对话</strong>：当检测到歧义（多个同类物品、未指明约束）时主动提问澄清</li>
</ul>
<p>效果总结</p>
<ul>
<li><strong>离线</strong>：在 31 个基准（空间、定位、任务推理、通用 VQA）全面领先；OOD 任务超 Gemini-2.5-Pro 3.0–11.8 pp</li>
<li><strong>在线</strong>：5 个真实场景（收桌、打包、购物、饮食过滤、餐具整理）平均进度 92.6%，显著优于 GPT-4o、Gemini 等商业模型</li>
<li><strong>系统级</strong>：与 GR-3 VLA 集成后，任务完成率再提升 4.3–28.1 pp，验证端到端优势</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕四个核心问题设计了<strong>三类实验</strong>，覆盖从基础能力到真实机器人系统的全链路验证。实验规模与指标如下：</p>
<p>1. 基础感知与推理能力验证（31 个公开基准）</p>
<p>类别</p>
<p>基准数量</p>
<p>关键示例</p>
<p>指标</p>
<p><strong>3D 空间理解</strong></p>
<p>8</p>
<p>VSIBench, BLINK, CV-Bench, EmbSpatial, SAT, VSR, SpatialBench, DA-2k</p>
<p>准确率</p>
<p><strong>视觉定位</strong></p>
<p>8</p>
<p>LVIS-MG, RefCOCO 系列, VisualWebBench, Pixmo-Point, Where2Place</p>
<p>F1 / 点命中率</p>
<p><strong>任务中心推理</strong></p>
<p>5</p>
<p>Agibot-ER(自建), RoboVQA, OpenEQA-hm3d/ScanNet, EgoTaskQA, ERQA</p>
<p>准确率</p>
<p><strong>通用多模态</strong></p>
<p>10</p>
<p>MME, MMBench, RealWorldQA, SimpleVQA, EgoSchema, VideoMME, NextQA, MathVista, MMMU</p>
<p>准确率</p>
<p><strong>结果摘要</strong></p>
<ul>
<li>Robix-32B 在 <strong>机器人相关任务</strong> 上平均领先 Qwen2.5-VL-32B <strong>7–25 pp</strong>，在 <strong>5/8 空间任务</strong> 超 Gemini-2.5-Pro。</li>
<li>通用任务保持与骨干模型相当，无显著遗忘。</li>
</ul>
<p>2. 离线交互任务评估（自建 3 套基准）</p>
<p>数据集</p>
<p>场景</p>
<p>样本量</p>
<p>指标</p>
<p><strong>AGIBot 评估集</strong></p>
<p>16 个日常任务（未见任务）</p>
<p>142 条轨迹</p>
<p>计划准确率</p>
<p><strong>内部 OOD 基准</strong></p>
<p>16 个脚本化人机交互场景</p>
<p>225 条轨迹</p>
<p>计划准确率</p>
<p><strong>内部 ID 基准</strong></p>
<p>6 类指令：多阶段 / 约束 / 中断 / 开放 / 无效 / 重规划</p>
<p>637 条轨迹</p>
<p>准确率 / F1</p>
<p><strong>结果摘要</strong></p>
<ul>
<li>Robix-32B-RL <strong>全部第一</strong>，OOD 场景领先 Gemini-2.5-Pro <strong>3.3 pp</strong>，ID 场景领先 <strong>12.7 pp</strong>。</li>
<li>消融显示：<ul>
<li>无链式思考（SFT-wo-R）→ OOD 掉 7 pp；</li>
<li>无 RL → 一致性错误率 ↑ 2.4×。</li>
</ul>
</li>
</ul>
<p>3. 在线真实机器人评估（两层实验）</p>
<p>3.1 纯 VLM 在线评估（UMI 人控低层）</p>
<p>任务</p>
<p>场景</p>
<p>重复次数</p>
<p>指标</p>
<p>收桌、结账打包、饮食过滤、杂货购物、餐具整理</p>
<p>厨房 / 会议室 / 超市</p>
<p>每模型-任务 4 次</p>
<p>任务进度（子任务完成率）</p>
<p><strong>结果摘要</strong></p>
<ul>
<li>Robix-32B 平均进度 <strong>92.6%</strong>（最高），领先 Gemini-2.5-Pro <strong>1.6 pp</strong>，领先 Qwen2.5-VL-32B <strong>64.6 pp</strong>。</li>
<li>Gemini-2.5-Pro 延迟常 &gt;30 s，实时性差。</li>
</ul>
<p>3.2 完整 VLM-VLA 系统评估（GR-3 自动低层）</p>
<p>任务</p>
<p>机器人</p>
<p>重复次数</p>
<p>指标</p>
<p>收桌、饮食过滤、结账打包</p>
<p>ByteMini + GR-3</p>
<p>每模型-任务 4 次</p>
<p>任务进度</p>
<p><strong>结果摘要</strong></p>
<ul>
<li>Robix-32B 平均 <strong>92.5%</strong>，领先 Gemini-2.5-Pro <strong>4.3 pp</strong>，领先 GPT-4o <strong>28.1 pp</strong>。</li>
<li>基线模型因 VLM-VLA 语义鸿沟（如 “biscuit box” vs “Oreo”）额外掉分 5–15 pp。</li>
</ul>
<p>实验全景图</p>
<pre><code>graph TD
    A[基础能力] --&gt;|31 公开基准| B[3D/定位/推理/通用 VQA]
    C[离线交互] --&gt;|3 自建基准| D[OOD 任务 / 多类指令]
    E[在线系统] --&gt;|UMI 人控| F[5 真实任务 20 次实验]
    E --&gt;|GR-3 自动| G[3 真实任务 12 次实验]
</code></pre><blockquote>
<p>结论：从原子级空间推理到完整机器人系统，Robix 在所有层级均显著优于开源与商业基线，验证其统一架构与三阶段训练的有效性。</p>
</blockquote>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可被视为 Robix 的直接延伸或长期研究议程，按<strong>短期可落地</strong>到<strong>中长期探索</strong>排序，并给出潜在技术路线。</p>
<p>1. 记忆与上下文工程（短期）</p>
<ul>
<li><strong>问题</strong>：当前仅依赖 32 k token 的短窗口记忆，长周期任务（&gt;1 h）易出现状态遗忘。</li>
<li><strong>思路</strong><ul>
<li>引入<strong>可更新的长期记忆</strong>（向量数据库 + 检索器），每轮仅将最相关的历史片段注入 prompt。</li>
<li>采用<strong>分层记忆</strong>：<ul>
<li>L1：实时感知缓存（秒级）</li>
<li>L2：任务级记忆（分钟级）</li>
<li>L3：场景级记忆（小时级，支持跨会话）</li>
</ul>
</li>
<li>技术验证：以 Ego4D 长视频为测试集，评估跨 30 min 任务的指令保持率。</li>
</ul>
</li>
</ul>
<p>2. 动态场景与快速适应（短期）</p>
<ul>
<li><strong>问题</strong>：高动态场景（多人走动、光照突变）导致幻觉或错误定位。</li>
<li><strong>思路</strong><ul>
<li>在 RL 阶段加入<strong>时序一致性奖励</strong>，鼓励相邻帧输出稳定。</li>
<li>引入<strong>在线快速微调</strong>（LoRA on-the-fly）：当检测到连续 3 帧置信度 &lt; τ 时，用最近 10 s 数据做 1-step 梯度更新。</li>
<li>实验：在 ARKitScenes 动态子集上测试漂移率。</li>
</ul>
</li>
</ul>
<p>3. 多机协同与分布式推理（中期）</p>
<ul>
<li><strong>问题</strong>：单机器人视角受限，复杂家务需多机协作。</li>
<li><strong>思路</strong><ul>
<li>将 Robix 扩展为<strong>Multi-Agent VLM</strong>：<ul>
<li>每台机器人运行轻量级 Robix-7B；</li>
<li>中央协调器（Robix-32B）聚合多视角观测，做全局任务分配。</li>
</ul>
</li>
<li>通信协议：使用 JSON-L 格式的<strong>共享世界模型</strong>（objects + spatial relations）。</li>
<li>场景：厨房-餐厅联动收拾，测量任务完成时间缩短比例。</li>
</ul>
</li>
</ul>
<p>4. 物理常识与因果推理（中期）</p>
<ul>
<li><strong>问题</strong>：对“易碎”“热”“锋利”等隐式属性推理不足。</li>
<li><strong>思路</strong><ul>
<li>构建<strong>物理常识图谱</strong>（Fragile, Hot, Sharp 等节点），以图神经网络插件形式接入 VLM。</li>
<li>训练数据：在现有仿真环境中加入<strong>物理属性标签</strong>（如杯子掉落会碎）。</li>
<li>评估：设计“易碎物品打包”任务，统计破损率。</li>
</ul>
</li>
</ul>
<p>5. 低延迟边缘部署（中期）</p>
<ul>
<li><strong>问题</strong>：32 B 模型在边缘 GPU 上仍难达 1 s 以内响应。</li>
<li><strong>思路</strong><ul>
<li><strong>投机解码</strong>：用小模型（Robix-7B）自回归生成，大模型并行验证。</li>
<li><strong>动作缓存</strong>：对高频动作（pick/place）预编译 CUDA kernel，跳过 VLA 解析。</li>
<li>目标：在 Jetson Orin 上实现 500 ms 端到端延迟。</li>
</ul>
</li>
</ul>
<p>6. 跨模态安全与对齐（长期）</p>
<ul>
<li><strong>问题</strong>：机器人可能被恶意指令诱导危险行为。</li>
<li><strong>思路</strong><ul>
<li>引入<strong>对抗指令红队</strong>：自动生成“看似合理但危险”的 prompt，进行 RLHF 安全对齐。</li>
<li>建立<strong>物理世界安全规则 DSL</strong>（Domain-Specific Language），在推理阶段做硬约束检查。</li>
<li>评估：在模拟厨房中注入 1000 条对抗指令，统计违规率。</li>
</ul>
</li>
</ul>
<p>7. 终身学习与个性化（长期）</p>
<ul>
<li><strong>问题</strong>：用户习惯差异大，模型需持续适应。<ul>
<li>采用<strong>参数高效持续学习</strong>（PEFT + EWC），为每户维护私有 LoRA 权重。</li>
<li>引入<strong>用户画像模块</strong>：记录饮食偏好、收纳习惯，用于个性化指令补全。</li>
<li>隐私保护：本地差分隐私 + 联邦聚合，避免上传原始视频。</li>
</ul>
</li>
</ul>
<p>8. 仿真到现实的零样本迁移（长期）</p>
<ul>
<li><strong>问题</strong>：合成数据与真实场景仍有域差。<ul>
<li>使用<strong>NeRF-RL</strong>：在仿真中训练策略，通过 NeRF 渲染的逼真图像做域随机化。</li>
<li>引入<strong>物理一致性损失</strong>：确保仿真动作在真实动力学下可行。</li>
<li>验证：在未见过的真实厨房完成“整理冰箱”任务，统计零样本成功率。</li>
</ul>
</li>
</ul>
<p>优先级与路线图</p>
<p>时间</p>
<p>重点</p>
<p>预期指标</p>
<p>0–3 个月</p>
<p>长期记忆 + 低延迟优化</p>
<p>长任务遗忘率 &lt;5%，边缘延迟 &lt;1 s</p>
<p>3–6 个月</p>
<p>动态场景适应 + 物理常识</p>
<p>动态场景错误率 ↓30%，易碎物品破损率 ↓50%</p>
<p>6–12 个月</p>
<p>多机协同 + 终身学习</p>
<p>双机协作任务时间 ↓25%，用户个性化满意度 ↑20%</p>
<p>12 个月+</p>
<p>安全对齐 + 零样本迁移</p>
<p>对抗指令违规率 <0.1%，零样本新场景成功率 >70%</p>
<blockquote>
<p>这些方向既可直接基于 Robix 的现有权重与数据管线迭代，也可为下一代通用机器人系统奠定技术与生态基础。</p>
</blockquote>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p><strong>Robix：面向通用机器人的统一推理-规划-交互大模型</strong></p>
<p>1. 问题背景</p>
<ul>
<li><strong>目标</strong>：让机器人在开放、动态环境中完成复杂、长周期任务，并与人类自然交互。</li>
<li><strong>挑战</strong>：现有系统要么只做任务分解，要么只做交互，模块化设计导致脆弱、难扩展。</li>
</ul>
<p>2. 核心贡献</p>
<p>维度</p>
<p>内容</p>
<p><strong>统一架构</strong></p>
<p>单一 Vision-Language 模型同时输出 <code>think</code>（链式思考）、<code>action</code>（原子动作）、<code>response</code>（自然语言回复），端到端闭环。</p>
<p><strong>三阶段训练</strong></p>
<p>① 200 B token 继续预训练 → 强化 3D 空间理解、视觉定位、任务推理；<br>② 监督微调 → 合成多轮人机交互轨迹，覆盖 7 类指令场景；<br>③ 强化学习（GRPO）→ 引入“思考-动作一致性”奖励，减少幻觉与不一致。</p>
<p><strong>数据工程</strong></p>
<p>- 大规模合成：遥操作 + 仿真 + AIGC 场景，自动生成带思考链的对话。<br>- 七类交互指令：多阶段、约束、中断、无效、模糊、开放、聊天。</p>
<p><strong>实验验证</strong></p>
<p>- <strong>31 个公开基准</strong>：机器人相关任务全面领先，通用任务无遗忘。<br>- <strong>3 套离线基准</strong>：OOD 任务领先 Gemini-2.5-Pro 3–12 pp。<br>- <strong>真实机器人</strong>：5 个日常任务平均完成率 92.6%，显著优于 GPT-4o、Gemini 等。</p>
<p>3. 关键结果</p>
<ul>
<li><strong>Robix-32B-RL</strong> 在全部离线/在线评估中<strong>均排名第一</strong>。</li>
<li>链式思考 + RL 带来 <strong>8–12 pp 的 OOD 提升</strong>，并显著降低幻觉与格式错误。</li>
<li>与 GR-3 VLA 集成后，真实场景任务完成率再提升 <strong>4–28 pp</strong>，验证系统级优势。</li>
</ul>
<p>4. 一句话总结</p>
<p>Robix 用一个端到端 vision-language 模型，把“机器人如何想、如何做、如何与人聊”统一起来，在公开基准与真实机器人实验中均刷新最佳表现，向通用具身智能迈出关键一步。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Robix%3A%20A%20Unified%20Model%20for%20Robot%20Interaction%2C%20Reasoning%20and%20Planning，链接是  https://arxiv.org/pdf/2509.01106  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01106  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Huang Fang,Mengxi Zhang,Heng Dong,Wei Li,Zixuan Wang,Qifeng Zhang,Xueyun Tian,Yucheng Hu,Hang Li</p>
<p><strong>Categories</strong>: cs.AI,cs.CV,cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01106.pdf">https://arxiv.org/pdf/2509.01106.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01106">https://arxiv.org/abs/2509.01106</a></p>
<p><strong>Arxiv ID</strong>: 2509.01106</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01106">https://papers.cool/arxiv/2509.01106</a></p>
<p><strong>Published</strong>: 2025-09-01T03:53:47Z</p>
<p><strong>Updated</strong>: 2025-09-01T03:53:47.000Z</p>
<hr>
<h3 id="2-Open-Data-Synthesis-For-Deep-Research"><a href="#2-Open-Data-Synthesis-For-Deep-Research" class="headerlink" title="2. Open Data Synthesis For Deep Research"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.00375">Open Data Synthesis For Deep Research</a></h3><p>Large language models (LLMs) are increasingly expected to go beyond simple factual queries toward Deep Research-tasks that require decomposing questions into sub-problems, coordinating multi-step reasoning, and synthesizing evidence from diverse sources. We formalize Deep Research tasks with verifiable answers as Hierarchical Constraint Satisfaction Problems (HCSPs), which are fundamentally different from single-constraint, multi-hop, or flat CSP formulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA) fail to capture this complexity, while recent synthetic datasets often introduce shortcut reasoning, knowledge leakage, or lack sufficient structural depth. To address this gap, we introduce InfoSeek, a scalable framework for synthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to recursively build a Research Tree from large-scale webpages, blurring intermediate nodes into valid sub-problems, and converting these trees into natural language questions that require traversing the full hierarchy. It also enables rapid scaling, yielding over 50K training examples, a curated test set, and reasoning trajectories generated via reject sampling. Experiments show that models trained on InfoSeek consistently outperform strong baselines. On a challenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass much larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash), while achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro). By preserving meta-information such as intermediate steps and retrieval labels, InfoSeek further supports advanced optimization strategies, including compound reward design and trajectory-level exploration. We provide our codes and datasets in \href{<a target="_blank" rel="noopener" href="https://github.com/VectorSpaceLab/InfoSeek}{this">https://github.com/VectorSpaceLab/InfoSeek}{this</a> repository}.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型 (LLMs) 正越来越多地被期望超越简单的事实查询，向需要将问题分解为子问题、协调多步推理以及从多样化来源合成证据的深度研究任务转变。我们将具有可验证答案的深度研究任务形式化为分层约束满足问题 (HCSPs)，这与单约束、多跳或扁平 CSP 形式在本质上有所不同。然而，现有基准测试（例如，自然问题、HotpotQA）未能捕获这种复杂性，而最近的合成数据集经常引入捷径推理、知识泄露，或缺乏足够的结构深度。为了解决这一空白，我们引入了 InfoSeek，这是一个用于合成复杂深度研究任务的可扩展框架。InfoSeek 采用双代理系统，从大规模网页递归构建研究树，将中间节点模糊为有效的子问题，并将这些树转换为需要遍历完整层次结构的自然语言问题。它还实现了快速扩展，产生超过 50,000 个训练实例，经过策划的测试集，以及通过拒绝采样生成的推理轨迹。实验表明，基于 InfoSeek 训练的模型始终超过强基线。在一个具有挑战性的基准测试 BrowseComp-Plus 上，使用 InfoSeek 优化的 3B LLM 模型超越了更大 32B 模型和轻量级商业 API（例如，Gemini2.5-Flash），并实现了与更强 API（例如，Gemini2.5-Pro）相当的性能。通过保留诸如中间步骤和检索标签等元信息，InfoSeek 进一步支持高级优化策略，包括复合奖励设计和轨迹级探索。我们在这个 <a target="_blank" rel="noopener" href="https://github.com/VectorSpaceLab/InfoSeek">仓库</a> 中提供我们的代码和数据集。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文聚焦的核心问题是：<strong>现有公开数据集无法支撑“Deep Research”任务所需的复杂、可验证、层级化推理训练需求</strong>。具体而言，它试图解决以下三个关键缺陷：</p>
<ol>
<li><p><strong>形式化缺失</strong><br>传统 QA 数据集（NQ、HotpotQA 等）仅覆盖单跳或多跳检索，缺乏对“Deep Research”任务中**层级化约束满足问题（HCSP）**的显式建模，导致无法刻画真实科研场景中的复杂推理结构。</p>
</li>
<li><p><strong>数据合成缺陷</strong><br>近期合成数据集常出现：</p>
<ul>
<li><strong>捷径推理</strong>（shortcut）：问题可通过单条线索直接回答，弱化多步推理；</li>
<li><strong>知识泄露</strong>（leakage）：答案隐含在训练语料中，模型依赖记忆而非检索；</li>
<li><strong>结构深度不足</strong>：问题扁平化，无法体现层级约束的递进关系。</li>
</ul>
</li>
<li><strong>规模与可控性不足</strong><br>现有开源方案（如 WebShaper、SimpleDeepSearcher）仅生成数百至数千样本，缺乏<strong>可扩展、复杂度可控</strong>的自动化框架，难以支撑大规模模型训练。</li>
</ol>
<p>为此，论文提出 <strong>InfoSeek</strong>：</p>
<ul>
<li><strong>理论层面</strong>：将 Deep Research 任务形式化为 HCSP，明确区分于 CSP 与多跳问题；</li>
<li><strong>方法层面</strong>：设计双智能体（Planner + Browser）递归构建“Research Tree”，通过“blurring parent”技术注入约束，确保问题需遍历完整层级才能得唯一可验证答案；</li>
<li><strong>实践层面</strong>：开源生成 50K+ 样本、16.5K 推理轨迹，验证 3B 模型（InfoSeeker-3B）在 BrowseComp-Plus 上超越 32B 模型及商用 API，证明框架有效性。</li>
</ul>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究从三个互补维度与 InfoSeek 形成关联：</p>
<ol>
<li>推理时（inference-time）Agent 框架；2) 面向搜索与检索的模型训练；3) 面向 Deep Research 的自动化数据合成。</li>
</ol>
<p>1. 推理时 Agent 框架（无需训练模型权重）</p>
<p>工作</p>
<p>关键思想</p>
<p>与 InfoSeek 的关系</p>
<p><strong>Agentic Reasoning</strong> (Wu et al., 2025c)</p>
<p>动态将复杂任务分解给搜索、代码、记忆等专用 Agent</p>
<p>同为“多 Agent 协作”范式，但仅推理时使用，未提供训练数据</p>
<p><strong>AgentOrchestra</strong> (Zhang et al., 2025)</p>
<p>层级式多 Agent，中央规划器派发子任务</p>
<p>与 InfoSeek 的 Planner-Browser 双 Agent 结构类似，但无数据合成</p>
<p><strong>ALITA</strong> (Qiu et al., 2025)</p>
<p>运行时自演化，动态生成并复用工具</p>
<p>关注工具自动生成，InfoSeek 关注问题-答案对的自动生成</p>
<p>2. 训练 Agent 进行搜索与检索（需训练模型）</p>
<p>工作</p>
<p>训练方法</p>
<p>与 InfoSeek 的关系</p>
<p><strong>Search-R1</strong> (Jin et al., 2025)</p>
<p>RL 训练 LLM 交错搜索查询与推理步骤</p>
<p>同样用 RL 强化搜索能力，但训练数据来自 NQ/HotpotQA，结构更简单</p>
<p><strong>AutoRefine</strong> (Shi et al., 2025b)</p>
<p>“搜索-精炼”两阶段 RL，检索奖励+答案奖励</p>
<p>与 InfoSeeker 的 Refiner Agent 思路一致，但数据规模小且无 HCSP 结构</p>
<p><strong>ZeroSearch</strong> (Sun et al., 2025a)</p>
<p>无真实搜索引擎，用 LLM 模拟检索做 RL</p>
<p>解决数据稀缺，但模拟检索可能失真；InfoSeek 用真实网页</p>
<p><strong>R1Searcher / R1-Searcher</strong> (Song et al., 2025a,b)</p>
<p>用 RL 激励 LLM 搜索能力</p>
<p>训练目标相似，数据集仍为传统 QA</p>
<p>3. 自动化数据合成（支撑 Deep Research 训练）</p>
<p>工作</p>
<p>数据合成策略</p>
<p>与 InfoSeek 的关系</p>
<p><strong>WebShaper</strong> (Tao et al., 2025)</p>
<p>先定义推理图，再生成问题</p>
<p>同样“先结构后问题”，但仅 500 样本，未开源</p>
<p><strong>DeepResearcher</strong> (Zheng et al., 2025)</p>
<p>RL 在开放 Web 上直接交互，自动生成任务</p>
<p>强调在线 RL 探索，InfoSeek 强调离线大规模合成</p>
<p><strong>WebSailor</strong> (Li et al., 2025a)</p>
<p>合成高不确定性 Web 导航任务</p>
<p>关注导航多样性，InfoSeek 关注层级约束满足</p>
<p><strong>Cognitive Kernel-Pro</strong> (Fang et al., 2025)</p>
<p>提供 Agent 训练生态</p>
<p>提供训练框架，InfoSeek 提供大规模 HCSP 数据</p>
<p><strong>TTD-DR</strong> (Han et al., 2025)</p>
<p>迭代去噪推理链</p>
<p>关注推理链去噪，InfoSeek 关注问题-答案对的构造</p>
<p>小结</p>
<ul>
<li><strong>推理时框架</strong>展示了多 Agent 协作的可行性，但缺乏训练数据；</li>
<li><strong>搜索-训练方法</strong>证明了 RL 可提升检索能力，却受限于简单 QA 数据；</li>
<li><strong>数据合成研究</strong>尝试生成复杂任务，但规模小或未开源。</li>
</ul>
<p>InfoSeek 通过 <strong>“HCSP 形式化 + 双 Agent 数据合成 + 50K 开源样本”</strong> 填补了上述空白，为 Deep Research 提供了可扩展且结构可控的训练基准。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过“理论-数据-训练”三位一体方案系统性地解决了 Deep Research 数据稀缺与结构缺陷问题，具体分为四个层面：</p>
<p>1. 理论层：提出 HCSP 形式化框架</p>
<ul>
<li><p><strong>定义 Hierarchical Constraint Satisfaction Problem（HCSP）</strong><br>将 Deep Research 任务刻画为“根-子问题”层级树，其中</p>
<ul>
<li>每个中间节点是独立子 CSP；</li>
<li>必须自底向上满足全部子约束才能唯一确定根答案；</li>
<li>天然避免 shortcut（单约束即可回答）与 leakage（答案不在表层）。</li>
</ul>
</li>
<li><p><strong>与经典问题区分</strong></p>
<ul>
<li>单约束 CSP：$|\mathcal C|=1$；</li>
<li>多跳 MHP：线性链 $A=S^{(k)}(c)$；</li>
<li>HCSP：树状依赖 $A=H(q_H)=\bigcap_i S(c_i)\cap\bigcap_j H(y_j)$。</li>
</ul>
</li>
</ul>
<p>2. 数据层：InfoSeek 双智能体合成框架</p>
<p>采用 Planner + Browser 双 Agent 递归构建 <strong>Research Tree</strong>，并通过 4 个原子动作保证可控与可扩展：</p>
<p>动作</p>
<p>功能</p>
<p>防缺陷机制</p>
<p><strong>Action 1</strong> 初始化</p>
<p>从维基百科选根实体</p>
<p>过滤无效/琐碎页面</p>
<p><strong>Action 2</strong> 约束模糊</p>
<p>对父节点增加 k 条互斥约束生成子节点</p>
<p>避免 over-determination（单约束即可回答）</p>
<p><strong>Action 3</strong> 深度扩展</p>
<p>通过超链接新增子实体节点</p>
<p>提升树高，强制多跳</p>
<p><strong>Action 4</strong> 终止与提问</p>
<p>复杂度达标后，用 LLM 将树转成自然语言问题</p>
<p>保证唯一可验证答案</p>
<ul>
<li><p><strong>质量保障</strong></p>
<ul>
<li><strong>难度过滤</strong>：Qwen2.5-32B 直接回答仅 2% 正确，剔除低难度样本；</li>
<li><strong>可验证性过滤</strong>：Gemini-2.5-Flash 在提供检索路径下仍答错则丢弃，消除歧义。</li>
</ul>
</li>
<li><p><strong>规模与成本</strong></p>
<ul>
<li>50 138 样本，总成本 $571.8；</li>
<li>4–6 层节点占比最高，Qwen2.5-72B CoT 失败率 92.7%，验证高难度。</li>
</ul>
</li>
</ul>
<p>3. 训练层：两阶段优化流程</p>
<p>阶段</p>
<p>数据</p>
<p>目标</p>
<p>技术细节</p>
<p><strong>SFT</strong></p>
<p>24 K 拒绝采样轨迹（教师模型 Qwen2.5-72B）</p>
<p>习得正确格式与搜索策略</p>
<p>保留“捷径”轨迹以丰富策略空间</p>
<p><strong>RL</strong></p>
<p>InfoSeek-50K + 难度抽样</p>
<p>强化推理与检索</p>
<p>GRPO 算法，二元奖励：格式正确 &amp; 答案正确</p>
<ul>
<li><strong>Round 2 再训练</strong><br>用 InfoSeeker-3B-RL-Round1 再做拒绝采样→3 450 高质量轨迹→二次 RL，最终得到 InfoSeeker-3B。</li>
</ul>
<p>4. 验证层：实验结果</p>
<ul>
<li><strong>通用 QA</strong>（表 3）：InfoSeeker-3B 在 7 个单/多跳基准均优于 RAG 与近期 agentic 方法。</li>
<li><strong>BrowseComp-Plus</strong>（表 4）：3B 模型 16.5% 准确率，超越 Gemini-2.5-Flash、Sonnet-4、GPT-4.1 等商业 API，以及 Qwen3-32B、SearchR1-32B 等开源大模型。</li>
<li><strong>数据有效性</strong>（表 5）：仅用 NQ+HQA 训练仅 3.0%，InfoSeek 训练达 16.5%，证明 HCSP 结构对 Deep Research 的必要性。</li>
</ul>
<p>结论</p>
<p>通过 <strong>HCSP 理论 → InfoSeek 数据合成 → 两阶段训练 → 实证验证</strong> 的闭环，论文首次提供了大规模、结构可控、可验证的 Deep Research 训练基准，并展示了小模型在该基准上的显著优势。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕“通用问答能力”与“深度研究能力”两条主线，共设计并执行了四类实验，覆盖 8 个公开基准与 1 个自建高难度基准，并辅以消融与成本分析。具体实验如下：</p>
<p>1. 通用单跳 / 多跳问答基准</p>
<p><strong>目的</strong>：验证 InfoSeeker 在经典检索-推理任务上的泛化能力。<br><strong>数据集</strong>（7 个）</p>
<ul>
<li>单跳：NQ、TriviaQA、PopQA</li>
<li>多跳：HotpotQA、2WikiMultihopQA、Musique、Bamboogle</li>
</ul>
<p><strong>指标</strong>：Exact Match (EM)<br><strong>对照组</strong>：</p>
<ul>
<li>RAG 系列：Vanilla RAG、IRCoT、RQRAG、Self-RAG</li>
<li>近期 Agent 系列：Search-o1-3B、Search-R1-3B、ZeroSearch-3B、AutoRefine-3B、InForage-3B</li>
</ul>
<p><strong>结果</strong>（表 3）<br>InfoSeeker-3B 在 7 项任务上全部夺魁，平均 EM 43.5%，领先最强基线 InForage-3B（40.6%）2.9 个百分点，证明 InfoSeek 数据可迁移到传统 QA。</p>
<p>2. 深度研究基准 BrowseComp-Plus</p>
<p><strong>目的</strong>：衡量模型在开放域、长链条、可验证 Deep Research 任务中的真实水平。<br><strong>设置</strong>：官方 830 题 + 固定 100 K 网页语料，检索器 BM25，LLM-as-a-judge 评估准确率。</p>
<p><strong>对照组</strong></p>
<ul>
<li>商业 API：Gemini 2.5 Flash / Pro、Sonnet 4、GPT-4.1、GPT-5</li>
<li>开源大模型：Qwen3-32B、SearchR1-32B</li>
</ul>
<p><strong>结果</strong>（表 4）<br>InfoSeeker-3B 取得 <strong>16.5%</strong> 准确率：</p>
<ul>
<li>超越 Gemini 2.5 Flash (15.5%)、Sonnet 4 (14.3%)、GPT-4.1 (14.6%) 等商业模型；</li>
<li>大幅领先同量级开源 32B 模型（Qwen3-32B 3.5%，SearchR1-32B 3.9%）；</li>
<li>仅次于 GPT-5 (55.9%) 与 Gemini 2.5 Pro (19.0%)，但参数量仅为 1/10 量级。</li>
</ul>
<p>3. 训练数据消融实验</p>
<p><strong>目的</strong>：检验 InfoSeek 数据集相较传统 NQ+HotpotQA 的增益。<br><strong>设置</strong>：固定 backbone（Qwen2.5-3B-Inst）与 RL 流程，仅替换训练数据。</p>
<p>训练集</p>
<p>BrowseComp-Plus 准确率</p>
<p>平均搜索调用次数</p>
<p>NQ + HotpotQA</p>
<p>3.0 %</p>
<p>1.39</p>
<p>InfoSeek</p>
<p><strong>16.5 %</strong></p>
<p>8.24</p>
<p>结论：HCSP 结构的 InfoSeek 数据带来 <strong>>5×</strong> 性能提升，且显著增加有效搜索深度。</p>
<p>4. 难度-成本-统计验证</p>
<ul>
<li><strong>难度曲线</strong>：以 Qwen2.5-72B + CoT 为探针，验证 InfoSeek 问题随顶点数增加而变难（表 2）。<ul>
<li>3 顶点：失败率 88.1 %</li>
<li>≥7 顶点：失败率 94.1 %</li>
</ul>
</li>
<li><strong>成本分析</strong>：生成 50 K 样本总花费 $571.8，平均每题 $0.011，验证可扩展性。</li>
<li><strong>轨迹统计</strong>：SFT 阶段共筛得 24 K 正确轨迹（教师模型 21.8 % 成功率），RL 阶段再经两轮拒绝采样→3 450 高质量轨迹（图 4）。</li>
</ul>
<p>实验总结</p>
<ul>
<li><strong>广度</strong>：覆盖单跳、多跳、开放域深度研究三大场景；</li>
<li><strong>深度</strong>：从 3B 小模型到 32B 大模型，从开源到商用 API 全面对标；</li>
<li><strong>消融</strong>：明确 InfoSeek 数据对 Deep Research 的不可替代性；</li>
<li><strong>效率</strong>：低成本（$571.8）即可生成 50 K 高质量 HCSP 样本。</li>
</ul>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可在 InfoSeek 基础上继续推进，按“数据-模型-评测-应用”四维度展开：</p>
<p>1. 数据合成与质量控制</p>
<ul>
<li><strong>跨语言 / 跨领域扩展</strong><br>将双 Agent 框架迁移到非英语网页、学术 PDF、专利库，检验 HCSP 结构在多语言、多模态场景下的通用性。</li>
<li><strong>动态难度调节</strong><br>引入自适应阈值，根据模型实时表现自动增减约束数量或树深度，实现“在线课程学习”式数据生成。</li>
<li><strong>对抗性质量检测</strong><br>构建“红队”LLM 专门寻找捷径或歧义路径，形成自动化的“问题-攻击-修复”闭环，持续净化数据集。</li>
</ul>
<p>2. 模型训练与优化</p>
<ul>
<li><strong>复合奖励设计</strong><br>利用 InfoSeek 保留的中间步骤与检索标签，设计细粒度奖励：<ul>
<li>每步推理正确性</li>
<li>检索文档相关度（BERTScore / RAG-Loss）</li>
<li>信息冗余惩罚（Token 效率）<br>通过多目标 RL 或 RLHF 提升样本效率。</li>
</ul>
</li>
<li><strong>轨迹级探索策略</strong><br>借鉴 Go-Explore、MCTS 思想，在 RL 阶段显式维护“未充分探索”的 HCSP 子树，引导模型主动攻克高难度分支。</li>
<li><strong>小模型蒸馏-压缩</strong><br>将 InfoSeeker-3B 的推理策略蒸馏至 1B 甚至 0.5B 模型，研究在边缘设备部署 Deep Research 的可行性。</li>
</ul>
<p>3. 评测基准与指标</p>
<ul>
<li><strong>开放域 HCSP 基准</strong><br>构建无固定网页池的“Live-HCSP”任务，模型需实时检索全网并给出可验证答案，衡量真实场景下的鲁棒性。</li>
<li><strong>不确定性量化指标</strong><br>引入 ECE、Prediction Rejection Curve，评估模型在信息不足时拒绝回答的能力，防止幻觉。</li>
<li><strong>人机协作效率</strong><br>设计“人-机协同”实验：给定预算（如 3 次人工澄清机会），比较不同模型在成本-准确率 Pareto 前沿的位置。</li>
</ul>
<p>4. 应用与工具集成</p>
<ul>
<li><strong>科学发现流水线</strong><br>将 InfoSeek 嵌入文献综述、实验设计、假设验证环节，形成“自动提出科学问题→检索论文→生成可验证假设”的闭环。</li>
<li><strong>政策分析助手</strong><br>利用 HCSP 树状结构追踪政策条款、历史案例、经济数据，自动生成可审计的政策影响报告。</li>
<li><strong>交互式可视化</strong><br>开发浏览器插件，实时展示 Research Tree 的节点展开过程，让用户可回溯、质疑、修正每一步推理，提升可解释性。</li>
</ul>
<p>5. 理论与安全</p>
<ul>
<li><strong>HCSP 复杂度理论</strong><br>形式化分析 HCSP 的决策复杂度（NP-Complete？PSPACE？），为问题难度可控性提供理论边界。</li>
<li><strong>隐私与版权合规</strong><br>研究合成过程中如何自动检测并剔除受版权保护或含个人信息的网页片段，确保大规模爬取-生成的合法性。</li>
</ul>
<p>通过上述方向，可进一步释放 InfoSeek 框架在数据、算法、评测、落地全链条的潜力，推动 Deep Research 从“可行”走向“可用、可信、可扩展”。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文一句话总结</p>
<p>InfoSeek 首次将“可验证的深度研究”形式化为层级约束满足问题（HCSP），并以开源双智能体框架自动合成 50 K+ 高质量样本，使 3 B 小模型在复杂推理基准上超越 32 B 大模型与多款商用 API。</p>
<p>核心内容速览</p>
<p>维度</p>
<p>关键要点</p>
<p><strong>问题定义</strong></p>
<p>提出 <strong>HCSP</strong>（Hierarchical Constraint Satisfaction Problem）刻画深度研究：需逐层满足子约束，最终得到唯一可验证答案，区别于单跳/多跳 QA。</p>
<p><strong>数据瓶颈</strong></p>
<p>现有公开数据集结构扁平、规模小或存在捷径、知识泄露，无法支撑真正的深度研究训练。</p>
<p><strong>解决方案</strong></p>
<p><strong>InfoSeek 框架</strong>：<br>1. Planner + Browser 双 Agent 递归构建 Research Tree；<br>2. “blurring parent” 注入约束，保证难度与唯一性；<br>3. 自动生成 50 K QA 对 + 16.5 K 推理轨迹，成本 $571.8，全部开源。</p>
<p><strong>质量保障</strong></p>
<p>- 难度过滤：Qwen2.5-32B 直接回答仅 2 % 正确；<br>- 可验证过滤：Gemini-2.5-Flash 在检索路径下仍答错即丢弃。</p>
<p><strong>模型训练</strong></p>
<p>两阶段：<br>1. 拒绝采样 SFT（24 K 轨迹）；<br>2. GRPO 强化学习（无 KL），最终得到 InfoSeeker-3B。</p>
<p><strong>实验结果</strong></p>
<p>- 7 个单/多跳基准全面领先；<br>- BrowseComp-Plus：3 B 模型 16.5 %，超越 Gemini-2.5-Flash、Sonnet-4、GPT-4.1 等，逼近 Gemini-2.5-Pro；<br>- 消融：用传统 NQ+HQA 训练仅 3 %，InfoSeek 提升 &gt;5×。</p>
<p><strong>开源贡献</strong></p>
<p>代码、50 K 数据集、构造框架均 Apache-2.0 协议发布，支持社区继续扩展。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Open%20Data%20Synthesis%20For%20Deep%20Research，链接是  https://arxiv.org/pdf/2509.00375  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.00375  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ziyi Xia,Kun Luo,Hongjin Qian,Zheng Liu</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00375.pdf">https://arxiv.org/pdf/2509.00375.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00375">https://arxiv.org/abs/2509.00375</a></p>
<p><strong>Arxiv ID</strong>: 2509.00375</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00375">https://papers.cool/arxiv/2509.00375</a></p>
<p><strong>Published</strong>: 2025-08-30T06:02:56Z</p>
<p><strong>Updated</strong>: 2025-08-30T06:02:56.000Z</p>
<hr>
<h3 id="3-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations"><a href="#3-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations" class="headerlink" title="3. LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.03405">LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations</a></h3><p>Language models (LMs) increasingly drive real-world applications that require world knowledge. However, the internal processes through which models turn data into representations of knowledge and beliefs about the world, are poorly understood. Insights into these processes could pave the way for developing LMs with knowledge representations that are more consistent, robust, and complete. To facilitate studying these questions, we present LMEnt, a suite for analyzing knowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a knowledge-rich pretraining corpus, fully annotated with entity mentions, based on Wikipedia, (2) an entity-based retrieval method over pretraining data that outperforms previous approaches by as much as 80.4%, and (3) 12 pretrained models with up to 1B parameters and 4K intermediate checkpoints, with comparable performance to popular open-sourced models on knowledge benchmarks. Together, these resources provide a controlled environment for analyzing connections between entity mentions in pretraining and downstream performance, and the effects of causal interventions in pretraining data. We show the utility of LMEnt by studying knowledge acquisition across checkpoints, finding that fact frequency is key, but does not fully explain learning trends. We release LMEnt to support studies of knowledge in LMs, including knowledge representations, plasticity, editing, attribution, and learning dynamics.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>语言模型（LMs）越来越多地推动现实世界应用，需要世界知识。然而，模型将数据转化为知识和关于世界的信念的内部过程尚不完全清楚。对这些过程的深入了解可以为开发具有更一致、稳健和完整的知识表示的语言模型铺平道路。为了方便研究这些问题，我们呈现LMEnt，这是一个分析语言模型在预训练期间知识获取的工具包。LMEnt引入：（1）一个知识丰富的预训练语料库，完全注释了实体提及，基于维基百科；（2）一种基于实体的检索方法，该方法在预训练数据上表现优于以往方法，提升幅度达到80.4%；（3）12个预训练模型，参数最多达10亿，拥有4000个中间检查点，在知识基准测试上表现与流行的开源模型相当。这些资源共同提供了一个受控环境，用于分析预训练中实体提及与下游性能之间的联系，以及在预训练数据中的因果干预效果。我们通过研究各检查点之间的知识获取展示了LMEnt的实用性，发现事实频率是关键，但并不能完全解释学习趋势。我们发布LMEnt以支持对语言模型中知识的研究，包括知识表示、可塑性、编辑、归因和学习动态。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决的核心问题是：<strong>语言模型（LM）在预训练阶段如何形成并塑造其内部的知识表征，以及这一过程如何与训练数据的组成和训练动态相互作用</strong>。具体而言，研究聚焦于以下关键问题：</p>
<ol>
<li><p><strong>知识表征的形成机制</strong><br>现有研究对LM如何从原始文本数据中提取并编码世界知识（如事实、实体关系）缺乏深入理解。论文试图揭示预训练过程中，模型如何将实体提及（entity mentions）转化为可检索、可验证的知识表征。</p>
</li>
<li><p><strong>训练数据与知识获取的可追溯性</strong><br>传统预训练语料（如Common Crawl）缺乏细粒度的实体标注，导致无法精确定位特定知识在训练数据中的来源。论文通过构建<strong>实体级标注的预训练语料</strong>（基于Wikipedia），实现对训练步骤中实体提及的精确追踪。</p>
</li>
<li><p><strong>现有检索方法的局限性</strong><br>现有工具（如WIMBD、Infinigram）依赖字符串匹配检索训练数据，易受别名歧义（如“Buffalo”指球队或城市）和语义变体影响。论文提出<strong>基于实体ID（Wikidata QID）的检索方法</strong>，显著提升检索精度（最高达80.4%的相对改进）。</p>
</li>
<li><p><strong>知识获取的动态分析</strong><br>通过开源12个不同规模（170M-1B参数）的LM及其4K中间检查点，论文首次提供了<strong>可控实验环境</strong>，用于分析：</p>
<ul>
<li>事实频率与知识学习/遗忘的关系</li>
<li>模型规模对知识编码的影响</li>
<li>训练步骤中知识表征的演化规律（如发现高频事实的学习和遗忘率同步上升的现象）。</li>
</ul>
</li>
</ol>
<p>总结：论文通过LMEnt套件（实体标注语料+检索工具+预训练模型）填补了<strong>预训练数据与LM知识表征之间因果关系的实证研究空白</strong>，为提升模型事实性、可解释性和可控性提供了基础工具。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究在问题设定、方法或资源上与 LMEnt 密切相关，可分为四大类：</p>
<p>1. 语言模型中的事实知识获取与评估</p>
<ul>
<li><strong>Petroni et al. (2019)</strong>：首次系统探讨预训练 LM 作为知识库的可能性，提出 LAMA 探针任务。</li>
<li><strong>Roberts et al. (2020)</strong>：展示 T5 在不访问外部知识库的情况下回答开放域问题的能力。</li>
<li><strong>Jiang et al. (2020)</strong>：提出基于提示的方法探测 LM 隐含知识。</li>
<li><strong>Li et al. (2022)</strong>；<strong>Chang et al. (2024)</strong>：分析预训练过程中事实知识的动态变化。</li>
<li><strong>Allen-Zhu &amp; Li (2024)</strong>：从理论角度研究知识在 Transformer 中的存储与提取机制。</li>
</ul>
<p>2. 训练数据检索与归因</p>
<ul>
<li><strong>Elazar et al. (2024)</strong>：WIMBD 工具，通过字符串匹配检索预训练语料中的事实片段。</li>
<li><strong>Liu et al. (2024a)</strong>：Infinigram，基于 n-gram 的万亿级语料检索系统。</li>
<li><strong>Liu et al. (2025)</strong>：OLMoTrace，将模型输出归因到具体训练 token。</li>
<li><strong>Cohen et al. (2024)</strong>：研究知识编辑对训练数据片段的连锁影响。</li>
</ul>
<p>3. 实体链接与知识增强语料</p>
<ul>
<li><strong>Logan et al. (2019)</strong>：Linked WikiText-2，将 Wikipedia 文本与 Wikidata 实体对齐，用于知识增强语言建模。</li>
<li><strong>Ayoola et al. (2022)</strong>：ReFinED 实体链接系统，支持零样本实体消歧。</li>
<li><strong>Martinelli et al. (2024)</strong>：Maverick 指代消解模型，用于捕获隐式实体提及。</li>
</ul>
<p>4. 开源模型与训练框架</p>
<ul>
<li><strong>Biderman et al. (2023)</strong>：Pythia 套件，提供 70 个从 70M 到 12B 参数的模型及完整训练数据。</li>
<li><strong>Groeneveld et al. (2024a,b)</strong>：OLMo 系列，开源 1B-7B 模型及 Dolma 语料。</li>
<li><strong>Allal et al. (2025)</strong>：SmolLM2，小模型高效训练方案。</li>
<li><strong>Liu et al. (2024b)</strong>：LLM360，完全透明的开源 LLM 研究框架。</li>
</ul>
<p>这些研究为 LMEnt 提供了方法论基础（实体链接、检索技术）、评估基准（PopQA、LAMA）及对比基线（Pythia、OLMo）。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过构建并开源 <strong>LMEnt 套件</strong>，从“数据—检索—模型—分析”四个层面系统性地解决了“如何追踪并理解预训练过程中知识表征的形成”这一问题。具体方案如下：</p>
<p>1. 数据层：构建实体级标注的预训练语料</p>
<ul>
<li><strong>语料选择</strong>：以英文 Wikipedia 为基底（3.6 B token），因其实体密度高、结构清晰、时间快照一致。</li>
<li><strong>三层标注流水线</strong>（§2）：<ol>
<li><strong>超链接</strong>：直接利用 Wikipedia 内链→Wikidata QID，置信度 H=1。</li>
<li><strong>实体链接</strong>：ReFinED 零样本模型补全未链接的显式实体提及，输出置信度 EL。</li>
<li><strong>指代消解</strong>：Maverick 模型识别“the team”“his”等隐式提及，通过最长公共子串与已有实体对齐，输出置信度 C/CC。</li>
</ol>
</li>
<li><strong>结果</strong>：7.3 M 实体、400 M 提及（115 M 超链接、203 M 实体链接、310 M 指代），每个提及绑定字符区间与 QID，支持任意分块后仍可追踪实体。</li>
</ul>
<p>2. 检索层：实体驱动的训练数据检索系统</p>
<ul>
<li><strong>Elasticsearch 索引</strong>（§3）：<ul>
<li>10.5 M chunk，每 chunk 记录文本、实体 QID、三源置信度。</li>
<li>支持 <strong>QID + 阈值</strong> 查询，避免字符串歧义。</li>
</ul>
</li>
<li><strong>检索质量</strong>：<ul>
<li>在 1 K 实体测试集上，相比字符串匹配（CS-SS/CI-SS），<strong>Win Rate 66.7 %–80.4 %</strong>；</li>
<li><strong>Precision@100K 仍 ≥97 %</strong>，而字符串方法跌至 27 %（图 7）。</li>
<li>对低频（tail）实体召回提升显著（图 6）。</li>
</ul>
</li>
</ul>
<p>3. 模型层：开源可追踪的预训练模型</p>
<ul>
<li><strong>12 个模型</strong>：170 M / 600 M / 1 B 参数 × 1/2/4/6 epoch，共 4 K 中间检查点（每 1 K 步保存）。</li>
<li><strong>训练细节</strong>：<ul>
<li>采用 OLMo-2 架构，使用 dolma2 tokenizer，Variable Sequence Length Curriculum 避免跨文档分块。</li>
<li>训练数据量仅 0.03 %–4.7 % 于同规模常规语料，却能在 PopQA 上达到与 Pythia-1.4B、OLMo-1B 相当的精度（图 3）。</li>
</ul>
</li>
</ul>
<p>4. 分析层：知识获取动态的可控实验</p>
<ul>
<li><strong>实验设计</strong>（§6）：<ul>
<li>以 PopQA 的 (Subject, Answer) 实体对为“事实”单位；</li>
<li>每 20 K 步评估一次，记录区间内：<ul>
<li><strong>Fact Frequency</strong>（两实体共现 chunk 数）</li>
<li><strong>Learn %</strong>（新掌握的事实比例）</li>
<li><strong>Forget %</strong>（遗忘的事实比例）</li>
</ul>
</li>
</ul>
</li>
<li><strong>关键发现</strong>（图 9）：<ul>
<li>学习率与知识获取不直接相关；</li>
<li><strong>高频事实的学习率与遗忘率同步上升</strong>，提示记忆与遗忘机制并存，需进一步研究。</li>
</ul>
</li>
</ul>
<p>总结</p>
<p>LMEnt 通过 <strong>实体级标注语料 + 高精度检索 + 开源模型 + 动态分析框架</strong>，首次实现了：</p>
<ul>
<li>对任意实体在预训练中的出现位置进行<strong>精确溯源</strong>；</li>
<li>在可控实验条件下<strong>量化事实频率、模型规模与知识获取/遗忘的关系</strong>；</li>
<li>为后续研究（知识编辑、可解释性、数据归因）提供了可直接复用的工具与基准。</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>LMEnt 套件的有效性、检索质量与知识获取动态</strong> 共设计了三组核心实验，并辅以一系列补充分析。具体实验列表如下：</p>
<p>1. 模型知识召回能力评估（§5.1 &amp; §A.2–A.4）</p>
<p><strong>目的</strong>：验证 LMEnt 模型在知识密集型任务上是否具备与同规模开源模型相当的表现。</p>
<ul>
<li><p><strong>基准</strong></p>
<ul>
<li><strong>PopQA</strong>（11 K 实体问答）</li>
<li><strong>PAQ</strong>（从 65 M 中抽取 70 K 与 PopQA 实体对齐的子集）</li>
</ul>
</li>
<li><p><strong>对比模型</strong></p>
<ul>
<li>Pythia（160 M–1.4 B）、OLMo-1B、OLMo-2-1B、SmolLM2（135 M–1.7 B）</li>
</ul>
</li>
<li><p><strong>结果</strong></p>
<ul>
<li><strong>图 3</strong>：在相同 FLOPs 预算下，LMEnt-1B-6E 在“热门实体”上与 Pythia-1.4B、OLMo-1B 精度相当（≈ 66 %），显著优于同计算量基线。</li>
<li><strong>图 4</strong>：模型规模扩大显著提升对高频共现实体（≥100 chunks）的问答准确率，对长尾实体影响有限。</li>
<li><strong>表 3</strong>：在常识推理、阅读理解等 17 个非知识任务上，LMEnt 因语料单一表现弱于基线，侧面验证其“知识专精”特性。</li>
</ul>
</li>
</ul>
<p>2. 实体级检索质量对比（§5.2 &amp; §A.5–A.7）</p>
<p><strong>目的</strong>：证明实体驱动检索优于传统字符串匹配。</p>
<ul>
<li><p><strong>实验设置</strong></p>
<ul>
<li><strong>1 K 实体测试集</strong>（按超链接数分层抽样）。</li>
<li><strong>检索方法</strong><ul>
<li>LMEnt（QID + 阈值 H=1, EL≥0.6, C≥0.6）</li>
<li>字符串基线：CS-SS/CI-SS × Canonical/Expanded 共 4 种。</li>
</ul>
</li>
<li><strong>评估指标</strong><ul>
<li><strong>Win Rate</strong>：两两比较“正确 chunk 数”（Gemini-2.5 作为裁判）。</li>
<li><strong>Precision@k</strong>：k ∈ {1,5,10,100,1 K,…,100 K}。</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>结果</strong></p>
<ul>
<li><strong>图 5</strong>：LMEnt 对 CS-SS Canonical 的 Win Rate 达 <strong>80.4 %</strong>；对 CI-SS Expanded 亦达 66.7 %。</li>
<li><strong>图 7</strong>：当 k=100 K 时，LMEnt Precision 仍 ≥97 %，而 CI-SS Expanded 跌至 27 %。</li>
<li><strong>图 14</strong>：在占实体总数 99.7 % 的 tail/torso 实体上，LMEnt 召回显著高于 Canonical 字符串方法。</li>
</ul>
</li>
</ul>
<p>3. 知识获取动态追踪（§6 &amp; §A.8）</p>
<p><strong>目的</strong>：利用 4 K 中间检查点，量化训练过程中“事实频率—学习/遗忘”关系。</p>
<ul>
<li><p><strong>实验流程</strong></p>
<ol>
<li>以 PopQA 的 (Subject, Answer) 实体对为“事实”单元。</li>
<li>每 20 K 步评估一次模型，记录区间内：<ul>
<li><strong>Fact Frequency</strong>：两实体共现的 chunk 数。</li>
<li><strong>Learn %</strong>：区间内新掌握的事实比例。</li>
<li><strong>Forget %</strong>：区间内遗忘的事实比例。</li>
</ul>
</li>
<li>将事实按频率分桶（1, 10, 100, 1 K+ chunks）。</li>
</ol>
</li>
<li><p><strong>结果</strong></p>
<ul>
<li><strong>图 9</strong>：<ul>
<li>高频事实的学习率与遗忘率<strong>同步上升</strong>（如 1 K+ 区间 Learn≈9 %, Forget≈6 %）。</li>
<li>低频事实几乎无遗忘，但学习率也低。</li>
</ul>
</li>
<li><strong>图 15</strong>：累计净增益（Learn−Forget）随训练持续增加，且高频事实贡献最大。</li>
</ul>
</li>
</ul>
<p>4. 补充实验</p>
<ul>
<li><strong>阈值敏感性分析</strong>（§A.5 &amp; 表 4）：在 60 实体开发集上扫描 EL/C/CC ∈ {0.4,…,0.8}，确认 0.6 为最佳阈值。</li>
<li><strong>非知识任务基准</strong>（表 3）：17 个任务展示 LMEnt 的分布外弱点。</li>
<li><strong>LLM-as-a-Judge 验证</strong>（§B.5）：人工标注 100 样本，证明 Gemini-2.5 与人类一致性 ω=1.0（p&lt;0.001）。</li>
</ul>
<p>总结</p>
<p>实验类别</p>
<p>关键发现</p>
<p>图表/表</p>
<p>模型性能</p>
<p>小语料即可达到同规模模型知识水平</p>
<p>图3、4；表3</p>
<p>检索质量</p>
<p>实体方法全面优于字符串匹配</p>
<p>图5、6、7、14</p>
<p>知识动态</p>
<p>高频事实“高学习+高遗忘”现象</p>
<p>图9、15</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向均可直接利用 LMEnt 提供的实体级标注语料、检索索引与 4 K 中间检查点进行扩展，分为 <strong>数据层面、训练层面、机理层面、应用层面</strong> 四大类：</p>
<p>1. 数据层面：扩展与混合语料</p>
<ul>
<li><strong>跨域知识迁移</strong><br>将 LMEnt 的实体标注流程迁移到非 Wikipedia 语料（新闻、教科书、科学文献、代码、合成故事），研究知识密度与噪声如何影响事实记忆。</li>
<li><strong>多语言实体一致性</strong><br>构建多语言 Wikipedia 实体对齐语料，观察同一实体在不同语言中的学习速率差异，验证“语言桥接”假设。</li>
<li><strong>时序与冲突数据</strong><br>引入带时间戳的 Wikipedia 历史版本，构造“事实更新”场景，量化模型对过时信息的遗忘与纠正能力。</li>
</ul>
<p>2. 训练层面：干预与课程设计</p>
<ul>
<li><strong>实体课程学习（Entity Curriculum）</strong><br>按实体频率、层级（head/torso/tail）或知识图谱关系设计训练顺序，测试能否提升长尾事实记忆。</li>
<li><strong>动态数据编辑</strong><br>在训练中途插入、删除或修改包含特定实体的 chunk，利用检查点回放（replay）精确测量“知识可塑性窗口”。</li>
<li><strong>遗忘机制消融</strong><br>冻结部分参数或调整优化器（如 Adafactor、Sophia），验证高频事实“高遗忘”现象是否与权重震荡或学习率调度直接相关。</li>
</ul>
<p>3. 机理层面：表征与电路解析</p>
<ul>
<li><strong>实体子空间定位</strong><br>使用线性探针或字典学习，在每一层寻找对特定实体激活值贡献最大的方向，建立“实体-神经元”映射。</li>
<li><strong>知识回路追踪</strong><br>结合因果中介分析（causal mediation）与激活修补（activation patching），识别从实体提及 token 到答案输出的最小子图。</li>
<li><strong>频率与记忆容量理论</strong><br>将实体出现频率与 Transformer 键-值记忆槽占用率建立数学模型，解释“高频高遗忘”现象是否源于容量竞争。</li>
</ul>
<p>4. 应用层面：编辑、对齐与评测</p>
<ul>
<li><strong>细粒度知识编辑</strong><br>在检查点级别执行 ROME、MEMIT 等编辑方法，对比一次性编辑与“渐进式编辑”（随训练步骤多次小幅度更新）的鲁棒性。</li>
<li><strong>检索增强预训练</strong><br>用 LMEnt 的实体索引作为“可检索记忆”，在训练时动态检索相关 chunk 并拼接至上下文，探索参数记忆与非参数记忆的互补边界。</li>
<li><strong>面向事实的评测基准</strong><br>基于 LMEnt 语料自动生成对抗性问答（引入同音异义、别名替换、时间冲突），建立更严格的事实一致性测试集。</li>
</ul>
<p>5. 工具与社区扩展</p>
<ul>
<li><strong>在线交互平台</strong><br>提供 Web 界面，允许研究者输入任意实体 QID 或自然语言查询，实时可视化该实体在所有检查点的学习曲线与示例 chunk。</li>
<li><strong>基准挑战赛</strong><br>举办“Knowledge Dynamics Challenge”，任务包括：预测某事实在第 N 步是否被掌握、设计最优课程以提升长尾事实准确率等。</li>
</ul>
<p>上述方向均可直接在 LMEnt 的开源资源（<code>huggingface.co/LMEnt</code> 与 <code>github.com/LMEnt</code>）上开展，无需额外大规模算力即可产出具有学术与实用价值的新发现。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>LMEnt 论文一句话总结</p>
<p>构建并开源了一个<strong>实体级完全可追踪的预训练套件</strong>（实体标注语料 + 高精度检索 + 12 模型/4 K 检查点），首次在可控环境下揭示语言模型在预训练中如何、何时、为何习得事实知识。</p>
<p>核心内容速览</p>
<p>维度</p>
<p>关键信息</p>
<p><strong>问题</strong></p>
<p>预训练 LM 如何将文本中的实体提及转化为内部知识表征？现有工具无法精确定位知识在训练数据中的来源。</p>
<p><strong>方案</strong></p>
<p>1️⃣ <strong>实体标注语料</strong>：3.6 B token 英文 Wikipedia，7.3 M 实体、400 M 提及（超链接 + 实体链接 + 指代消解）。<br>2️⃣ <strong>实体检索引擎</strong>：基于 Wikidata QID，检索精度比字符串匹配高 66–80 %，Precision@100K ≥ 97 %。<br>3️⃣ <strong>开源模型矩阵</strong>：170 M/600 M/1 B × 1/2/4/6 epoch，共 12 模型 + 4 K 中间检查点。</p>
<p><strong>实验</strong></p>
<p>• <strong>知识召回</strong>：在 PopQA/PAQ 上与 Pythia-1.4B、OLMo-1B 精度相当，仅用 0.03–4.7 % 训练量。<br>• <strong>规模效应</strong>：增大模型显著提升高频事实记忆，对长尾实体帮助有限。<br>• <strong>动态追踪</strong>：发现“高频事实同时伴随高学习率与高遗忘率”，提示内部记忆竞争机制。</p>
<p><strong>资源</strong></p>
<p>全量开源：实体标注语料、Elasticsearch 索引、模型与检查点、评估脚本（HuggingFace &amp; GitHub）。</p>
<p><strong>意义</strong></p>
<p>为研究知识表征、可解释性、知识编辑、数据归因等提供了<strong>可直接复用、精确到训练步骤</strong>的实验平台。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是LMEnt%3A%20A%20Suite%20for%20Analyzing%20Knowledge%20in%20Language%20Models%20from%20Pretraining%20Data%20to%20Representations，链接是  https://arxiv.org/pdf/2509.03405  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.03405  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Daniela Gottesman,Alon Gilae-Dotan,Ido Cohen,Yoav Gur-Arieh,Marius Mosbach,Ori Yoran,Mor Geva</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03405.pdf">https://arxiv.org/pdf/2509.03405.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03405">https://arxiv.org/abs/2509.03405</a></p>
<p><strong>Arxiv ID</strong>: 2509.03405</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03405">https://papers.cool/arxiv/2509.03405</a></p>
<p><strong>Published</strong>: 2025-09-03T15:31:18Z</p>
<p><strong>Updated</strong>: 2025-09-03T15:31:18.000Z</p>
<hr>
<h3 id="4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement"><a href="#4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement" class="headerlink" title="4. MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.01977">MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement</a></h3><p>Multi-subject personalized generation presents unique challenges in maintaining identity fidelity and semantic coherence when synthesizing images conditioned on multiple reference subjects. Existing methods often suffer from identity blending and attribute leakage due to inadequate modeling of how different subjects should interact within shared representation spaces. We present MOSAIC, a representation-centric framework that rethinks multi-subject generation through explicit semantic correspondence and orthogonal feature disentanglement. Our key insight is that multi-subject generation requires precise semantic alignment at the representation level - knowing exactly which regions in the generated image should attend to which parts of each reference. To enable this, we introduce SemAlign-MS, a meticulously annotated dataset providing fine-grained semantic correspondences between multiple reference subjects and target images, previously unavailable in this domain. Building on this foundation, we propose the semantic correspondence attention loss to enforce precise point-to-point semantic alignment, ensuring high consistency from each reference to its designated regions. Furthermore, we develop the multi-reference disentanglement loss to push different subjects into orthogonal attention subspaces, preventing feature interference while preserving individual identity characteristics. Extensive experiments demonstrate that MOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably, while existing methods typically degrade beyond 3 subjects, MOSAIC maintains high fidelity with 4+ reference subjects, opening new possibilities for complex multi-subject synthesis applications.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多主题个性化生成在根据多个参考主题合成图像时面临着保持身份忠实度和语义连贯性的独特挑战。现有方法常常由于对不同主题在共享表示空间中如何互动建模不足而遭受身份混合和属性泄漏的问题。我们提出了MOSAIC，一个以表示为中心的框架，通过显式的语义对应和正交特征解缠结来重新思考多主题生成。我们的关键见解是，多主题生成在表示层面需要精确的语义对齐——确切知道生成图像中的哪些区域应该关注每个参考的一部分。为了实现这一点，我们引入了SemAlign-MS，这是一个精心注释的数据集，提供了多个参考主题与目标图像之间的细粒度语义对应，这在该领域是前所未有的。在此基础上，我们提出了语义对应注意力损失，以强制执行精确的逐点语义对齐，确保从每个参考到其指定区域的高度一致性。此外，我们还开发了多参考解缠损失，将不同主题推入正交的注意力子空间，防止特征干扰，同时保留个体身份特征。大量实验表明，MOSAIC在多个基准测试中达到了最先进的性能。值得注意的是，尽管现有方法通常在超过3个主题后性能下降，MOSAIC在4个及以上参考主题的情况下仍能保持高忠实度，为复杂的多主题合成应用开辟了新的可能性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文聚焦 <strong>多主体个性化图像生成</strong> 场景下的两大核心缺陷：</p>
<ol>
<li><p><strong>对应关系缺失</strong><br>现有方法缺乏对“参考图像的哪一区域应当与生成图像的哪一位置精确对齐”的显式建模，导致语义一致性随主体数量增加而迅速下降。</p>
</li>
<li><p><strong>特征纠缠</strong><br>多个主体的表征共享同一潜空间，缺乏显式解耦机制，造成身份混淆与属性泄露；当主体数量超过 3–4 个时，现有方法性能显著退化。</p>
</li>
</ol>
<p>MOSAIC 通过引入细粒度语义对应标注（SemAlign-MS 数据集）与两项损失函数（语义对应注意力损失 + 多参考解耦损失），首次在表征层面同时实现 <strong>精确对齐</strong> 与 <strong>主体间正交分离</strong>，从而支持 4 个以上主体的高质量生成。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>论文在第 2 节系统回顾了两大相关方向的研究，并指出其局限性。</p>
<p>1. 主体驱动图像生成（Subject-Driven Image Generation）</p>
<ul>
<li><p><strong>单主体方法</strong></p>
<ul>
<li>DreamBooth [25]：通过少量微调将新概念绑定到文本 token。</li>
<li>BLIP-Diffusion [16]：用 BLIP 编码参考图像，实现零样本主体驱动生成。</li>
<li>SSR-Encoder [35, 36]：引入“选择性主体表征编码”模块，提升身份一致性。</li>
<li>OminiControl [27]：直接利用预训练 DiT 自身作为参考图像编码器，无需额外网络。</li>
</ul>
</li>
<li><p><strong>多主体扩展</strong></p>
<ul>
<li>MS-Diffusion [29]：在 cross-attention 中注入布局 mask，将不同主体绑定到指定空间区域。</li>
<li>UNO [31]：提出“less-to-more”数据生成管线，逐步增加主体数量以提升泛化。</li>
<li>DreamO [20, 21]：在 DiT 块内引入“路由门控”，显式控制不同主体 token 的更新路径。</li>
<li>XVerse [2]：对文本流进行 token-wise 偏移调制，实现独立的主体控制。</li>
</ul>
</li>
</ul>
<p><strong>共同局限</strong>：均依赖全局特征匹配或隐式对齐，缺乏对“参考区域 ↔ 目标区域”细粒度对应关系的显式监督，导致多主体时身份混淆与属性泄露。</p>
<p>2. 视觉对应关系（Visual Correspondence for Generation）</p>
<ul>
<li><p><strong>传统方法</strong></p>
<ul>
<li>手工特征：SIFT [17]、SURF [1]。</li>
<li>深度监督：Proposal Flow [6]、SFNet [14]、HyperPixel Flow [19] 等依赖稠密标注数据集。</li>
</ul>
</li>
<li><p><strong>基于扩散模型的无监督对应</strong></p>
<ul>
<li>DIFT [28]、SD-DINO [33]、GeoAware-SC [34]：利用预训练 DiT 或 DINO 特征建立跨图像对应。</li>
</ul>
</li>
</ul>
<p><strong>局限</strong>：上述对应研究主要用于匹配或编辑任务，尚未被引入多主体生成流程，无法解决“多参考特征在统一潜空间中的纠缠”问题。</p>
<p>总结</p>
<p>现有研究要么聚焦单主体或全局对齐，要么仅将对应关系用于匹配/编辑；MOSAIC 首次将 <strong>细粒度语义对应</strong> 与 <strong>多主体表征解耦</strong> 结合，填补了这一空白。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文提出 <strong>MOSAIC</strong> 框架，把多主体个性化生成重新表述为“表征层面的对应-解耦联合优化问题”。具体通过以下三步解决前述两大缺陷：</p>
<p>1. 建立显式语义对应基础：SemAlign-MS 数据集</p>
<ul>
<li><strong>五阶段流水线</strong>（图 2）<ol>
<li>GPT-4o 生成含多主体（人、动物、物体）的复杂 prompt；</li>
<li>T2I 模型合成图像并自动过滤低质量样本；</li>
<li>LangSAM 对每幅图做开放词汇检测与分割，精确定位每个主体；</li>
<li>FLUX Kontext 做视角矫正，提升外观多样性；</li>
<li>人工验证后得到 <strong>1.2 M</strong> 对“参考-目标”图像，并为每对提供稠密的 <strong>语义点对应</strong>  <script type="math/tex; mode=display">\\mathcal C^{(i,k)}={(u\_{i,j}^{(k)}, v\_{i,j}^{(k)})}\_{j=1}^{P^{(k)}},</script>其中 $u$ 为参考图像坐标，$v$ 为潜空间 token 位置，且不同参考的 $v$ 集合互不相交（式 2），避免监督冲突。</li>
</ol>
</li>
</ul>
<p>2. 语义对应注意力损失（SCAL）：解决“对应关系缺失”</p>
<ul>
<li><strong>目标</strong>：让参考 token 的注意力只聚焦其对应的目标 token。</li>
<li><strong>实现</strong>：<ol>
<li>计算参考→目标注意力矩阵 $\mathbf A_{\text{ref}\to\text{tgt}}$（式 8）；</li>
<li>对每个对应点对 $(u_{i,j}^{(k)}, v_{i,j}^{(k)})$，最大化  <script type="math/tex; mode=display">\\mathcal L\_{\\text{SCAL}}=-\\frac1K\\sum\_{k=1}^K\\frac1{P^{(k)}}\\sum\_{j=1}^{P^{(k)}}\\log\\mathbf A\_{\\text{ref}\\to\\text{tgt}}\\bigl\[G(u\_{i,j}^{(k)}), v\_{i,j}^{(k)}\\bigr\],</script>其中 $G(\cdot)$ 将局部坐标映射到全局 token 索引（式 9）。</li>
</ol>
</li>
<li><strong>效果</strong>：强制“点对点”精确对齐，显著提升局部结构、纹理保真度。</li>
</ul>
<p>3. 多参考解耦损失（MDL）：解决“特征纠缠”</p>
<ul>
<li><strong>目标</strong>：让不同主体的注意力分布彼此正交，防止跨主体干扰。</li>
<li><strong>实现</strong>：<ol>
<li>对每个参考 $k$，聚合其所有对应点的注意力向量 $\mathbf a^{(k)}\in\mathbb R^{N_{\text{tgt}}}$（式 11–12）；</li>
<li>用对称 KL 散度度量两两分布距离  <script type="math/tex; mode=display">\\text{dist}(\\mathbf a^{(i)},\\mathbf a^{(j)})=\\tfrac12D\_{\\text{KL}}(\\hat{\\mathbf a}^{(i)}|\\hat{\\mathbf a}^{(j)})+\\tfrac12D\_{\\text{KL}}(\\hat{\\mathbf a}^{(j)}|\\hat{\\mathbf a}^{(i)});</script></li>
<li>最大化所有主体对的平均距离  <script type="math/tex; mode=display">\\mathcal L\_{\\text{MD}}=-\\frac1{K(K-1)}\\sum\_{i\\neq j}\\text{dist}(\\mathbf a^{(i)},\\mathbf a^{(j)}).</script></li>
</ol>
</li>
<li><strong>效果</strong>：将不同主体推入近似正交的注意力子空间，显著抑制身份混淆与属性泄露。</li>
</ul>
<p>4. 训练与推理流程</p>
<ul>
<li><strong>架构</strong>：基于 FLUX-1.0-DEV，LoRA rank=128；参考分支与去噪分支共享权重，仅通过 LoRA 微调。</li>
<li><strong>总损失</strong>  <script type="math/tex; mode=display">\\mathcal L=\\mathcal L\_{\\text{flow}}+\\alpha\\mathcal L\_{\\text{SCAL}}+\\beta\\mathcal L\_{\\text{MD}},\\quad \\alpha=0.4,\\ \\beta=0.6.</script></li>
<li><strong>推理</strong>：无需额外 mask 或布局，直接输入多张参考图与文本即可生成 4+ 主体的高质量图像。</li>
</ul>
<p>小结</p>
<p>通过 <strong>显式语义对应监督</strong>（SCAL）与 <strong>正交注意力约束</strong>（MDL），MOSAIC 首次在表征层面同时实现</p>
<ul>
<li>参考→目标的高精度对齐</li>
<li>多主体特征的有效解耦<br>从而突破现有方法在 3–4 主体后的性能瓶颈。</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文在两大公开基准（DreamBench 与 XVerseBench）上，系统评估了单主体与多主体场景，并辅以消融实验与可视化分析。具体实验设置与结果如下。</p>
<p>1. 实验设置</p>
<p>维度</p>
<p>配置</p>
<p>基础模型</p>
<p>FLUX-1.0-DEV</p>
<p>微调方式</p>
<p>LoRA，rank = 128</p>
<p>优化器</p>
<p>AdamW，lr = 1e-4</p>
<p>训练步数</p>
<p>100 K steps</p>
<p>批次大小</p>
<p>1 per GPU</p>
<p>损失权重</p>
<p>α = 0.4 (SCAL)，β = 0.6 (MDL)</p>
<p>2. 定量评估</p>
<p>2.1 DreamBench（表 1）</p>
<ul>
<li><strong>指标</strong>：CLIP-I（图像一致性）、CLIP-T（文本一致性）、DINO（结构相似性）。</li>
<li><strong>结果</strong><ul>
<li>单主体：MOSAIC 84.30 / 31.64 / 77.40，均列第一。</li>
<li>多主体：MOSAIC 76.30 / 32.40 / 56.83，分别领先第二名 ≈ 3.0 / 0.2 / 2.6 分。</li>
</ul>
</li>
</ul>
<p>2.2 XVerseBench（表 2）</p>
<ul>
<li><strong>指标</strong>：DPG（检测-保真）、ID-Sim（身份相似）、IP-Sim（感知相似）、AES（美学）。</li>
<li><strong>结果</strong><ul>
<li>单主体平均：80.05（↑ 3.3 超越 XVerse）。</li>
<li>多主体平均：72.03（↑ 1.95 超越 XVerse）。</li>
<li>综合平均：76.04，刷新 SOTA。</li>
</ul>
</li>
</ul>
<p>3. 定性对比（图 4）</p>
<ul>
<li><strong>场景覆盖</strong>：1–5 个参考主体，含人物、动物、物体。</li>
<li><strong>观察</strong><ul>
<li>3+ 主体时，现有方法出现物体遗漏、重复或形变；MOSAIC 保持完整且一致。</li>
<li>4+ 主体时，仅 MOSAIC 能维持面部与纹理一致性，其他方法显著退化。</li>
</ul>
</li>
</ul>
<p>4. 消融实验</p>
<p>4.1 损失函数贡献（表 3）</p>
<p>配置</p>
<p>CLIP-I ↑</p>
<p>CLIP-T ↑</p>
<p>DINO ↑</p>
<p>基线（无 SCAL &amp; MDL）</p>
<p>73.45</p>
<p>29.90</p>
<p>52.03</p>
<p>+ SCAL</p>
<p>75.89</p>
<p>31.10</p>
<p>55.99</p>
<p>+ SCAL + MDL（完整）</p>
<p>76.30</p>
<p>32.40</p>
<p>56.83</p>
<ul>
<li>SCAL 带来 2.4–3.9 的提升；再加 MDL 继续提升 0.4–1.3，验证解耦必要性。</li>
</ul>
<p>4.2 注意力可视化（图 5 &amp; 6）</p>
<ul>
<li><strong>方法</strong>：将泰迪熊护目镜、小黄人文字等局部区域作为 query，可视化其到生成图的注意力热图。</li>
<li><strong>结论</strong><ul>
<li>基线：注意力分散、跨主体干扰严重。</li>
<li>仅加 SCAL：聚焦度提升但仍存在重叠。</li>
<li>完整 MOSAIC：注意力精准对齐对应区域，且不同主体热图互不重叠，直观展示解耦效果。</li>
</ul>
</li>
</ul>
<p>5. 扩展性验证</p>
<ul>
<li><strong>4+ 主体场景</strong>：在补充实验中，MOSAIC 仍保持 CLIP-I &gt; 75，DINO &gt; 55，而对照组普遍跌破 70 / 50。</li>
<li><strong>计算效率</strong>：LoRA 仅引入 3.7 M 可训练参数，推理延迟与 FLUX 原模型持平，实现“即插即用”。</li>
</ul>
<p>总结</p>
<p>实验从 <strong>定量指标、主观视觉、消融分析、注意力可视化、扩展压力测试</strong> 五个维度一致表明：</p>
<ul>
<li>MOSAIC 在单/多主体任务均刷新 SOTA；</li>
<li>明确验证了 SCAL 与 MDL 的互补贡献；</li>
<li>首次展示了 4+ 主体场景下的高保真生成能力。</li>
</ul>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可在 MOSAIC 的基础上继续深入，分为 <strong>数据、方法、应用、评测</strong> 四大类。</p>
<p>1. 数据层面</p>
<ul>
<li><p><strong>更高阶对应</strong></p>
<ul>
<li>将点级对应扩展到 <strong>区域级 / 部件级 / 语义部件层级</strong>（如“猫的左耳 → 生成猫的左耳”），以支持更细粒度的可控编辑。</li>
<li>引入 <strong>时序对应</strong>（视频帧间），构建多主体视频一致性生成数据集。</li>
</ul>
</li>
<li><p><strong>跨模态对应</strong></p>
<ul>
<li>把文本短语（如 “the logo on his T-shirt”）显式绑定到参考图像区域，实现文本-图像-参考三重对应，减少 prompt-engineering 依赖。</li>
</ul>
</li>
<li><p><strong>自动标注增强</strong></p>
<ul>
<li>利用自监督对应模型（e.g., DIFT + Segment Anything）半自动生成稠密对应，降低人工标注成本，扩大 SemAlign-MS 规模至 10 M+。</li>
</ul>
</li>
</ul>
<p>2. 方法层面</p>
<ul>
<li><p><strong>动态主体数量</strong></p>
<ul>
<li>当前实现固定最大 K=5；设计 <strong>可变长度参考编码器</strong>（如 Set Transformer、Perceiver IO），让网络自适应 1-N 主体，避免黑图填充带来的噪声。</li>
</ul>
</li>
<li><p><strong>层次化解耦</strong></p>
<ul>
<li>仅在 attention 空间做正交约束；可进一步在 <strong>特征通道维度</strong> 引入分组卷积或稀疏掩码，实现“空间-通道”双重解耦，降低 6+ 主体时的残余干扰。</li>
</ul>
</li>
<li><p><strong>生成-对应联合优化</strong></p>
<ul>
<li>将对应估计网络与生成网络端到端训练，用生成质量作为对应置信度的弱监督信号，提升对应精度并减少人工标注需求。</li>
</ul>
</li>
<li><p><strong>扩散架构升级</strong></p>
<ul>
<li>把 RoPE 频率分组策略迁移到 3D RoPE，支持多视角 / 多帧一致性；或引入 <strong>混合专家（MoE）路由</strong>，让不同主体自动选择专属专家子网络。</li>
</ul>
</li>
</ul>
<p>3. 应用层面</p>
<ul>
<li><p><strong>交互式编辑</strong></p>
<ul>
<li>基于已学习的对应场，支持用户拖拽参考图像关键点实时更新生成结果（类似 DragGAN），实现“点-到-点”直观操控。</li>
</ul>
</li>
<li><p><strong>虚拟试穿 / 换装</strong></p>
<ul>
<li>将衣服、配饰作为独立主体，利用解耦表征实现“多人共享同一服饰”或“一人穿多件服饰”的复杂组合。</li>
</ul>
</li>
<li><p><strong>故事板生成</strong></p>
<ul>
<li>给定若干角色与物体参考，自动生成多格漫画或分镜脚本，要求角色外观跨帧一致且物体交互合理，可直接检验长序列一致性。</li>
</ul>
</li>
</ul>
<p>4. 评测与鲁棒性</p>
<ul>
<li><p><strong>新基准</strong></p>
<ul>
<li>构建 <strong>6–8 主体极端场景</strong> 的 benchmark，引入“身份混淆率”、“属性泄露率”等细粒度指标，量化方法在超复杂组合下的极限性能。</li>
</ul>
</li>
<li><p><strong>对抗扰动</strong></p>
<ul>
<li>测试参考图像加入轻微旋转、光照、遮挡扰动时的鲁棒性，验证对应机制对现实拍摄条件的泛化能力。</li>
</ul>
</li>
<li><p><strong>公平性与偏见</strong></p>
<ul>
<li>分析不同人种、性别、物种在对应精度与生成质量上的差异，确保多主体系统不会因训练数据不平衡而放大偏见。</li>
</ul>
</li>
</ul>
<p>小结</p>
<p>从 <strong>更精细的对应粒度</strong> 到 <strong>任意数量主体的动态建模</strong>，再到 <strong>交互式应用与极端场景评测</strong>，MOSAIC 为多主体生成开辟了多条可扩展的研究路径。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>MOSAIC 论文核心内容速览</p>
<p>维度</p>
<p>关键信息</p>
<p><strong>任务</strong></p>
<p>多主体个性化图像生成：给定多张参考图（人/物/动物），按文本描述生成新图，同时保持各主体身份一致且互不干扰。</p>
<p><strong>痛点</strong></p>
<p>1. 缺乏“参考区域 ↔ 目标区域”显式对齐 → 语义漂移；<br>2. 多主体共享潜空间 → 身份混淆、属性泄露；<br>3. 现有方法在 ≥4 主体时性能骤降。</p>
<p><strong>数据集</strong></p>
<p><strong>SemAlign-MS</strong>：首个 120 万对多主体训练集，带稠密语义点对应标注，确保不同参考对应目标 token 无冲突。</p>
<p><strong>方法</strong></p>
<p><strong>MOSAIC 框架</strong>（FLUX-DiT + LoRA）：<br>1. <strong>语义对应注意力损失 (SCAL)</strong>：用点级对应监督 attention，实现精确对齐；<br>2. <strong>多参考解耦损失 (MDL)</strong>：最大化不同主体 attention 分布的 KL 距离，强制正交子空间；<br>3. 总损失：$\mathcal L = \mathcal L_{\text{flow}} + 0.4,\mathcal L_{\text{SCAL}} + 0.6,\mathcal L_{\text{MD}}$。</p>
<p><strong>实验结果</strong></p>
<p>- DreamBench：单/多主体 3 项指标均夺 SOTA，多主体领先约 3 分；<br>- XVerseBench：综合 76.04，刷新最好成绩；<br>- 4+ 主体场景仍保持高保真，其他方法显著退化；<br>- 消融验证 SCAL 与 MDL 的互补贡献。</p>
<p><strong>贡献</strong></p>
<p>1. 提出以“表征对齐 + 解耦”为核心的 MOSAIC 框架；<br>2. 发布 SemAlign-MS 数据集；<br>3. 首次实现 4+ 主体高质量生成，验证可扩展性。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MOSAIC%3A%20Multi-Subject%20Personalized%20Generation%20via%20Correspondence-Aware%20Alignment%20and%20Disentanglement，链接是  https://arxiv.org/pdf/2509.01977  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01977  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Dong She,Siming Fu,Mushui Liu,Qiaoqiao Jin,Hualiang Wang,Mu Liu,Jidong Jiang</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01977.pdf">https://arxiv.org/pdf/2509.01977.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01977">https://arxiv.org/abs/2509.01977</a></p>
<p><strong>Arxiv ID</strong>: 2509.01977</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01977">https://papers.cool/arxiv/2509.01977</a></p>
<p><strong>Published</strong>: 2025-09-02T05:40:07Z</p>
<p><strong>Updated</strong>: 2025-09-02T05:40:07.000Z</p>
<hr>
<h3 id="5-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation"><a href="#5-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation" class="headerlink" title="5. Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.00428">Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation</a></h3><p>Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at <a target="_blank" rel="noopener" href="https://github.com/XavierJiezou/Face-MoGLE">https://github.com/XavierJiezou/Face-MoGLE</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>可控面孔生成在生成建模中面临着关键挑战，因为在语义可控性和照片真实感之间需要复杂的平衡。虽然现有的方法在将语义控制与生成流程分离方面遇到了困难，我们通过专家专业化的视角重新审视扩散变压器（DiTs）的架构潜力。本文介绍了Face-MoGLE，这是一种新的框架，具有以下特点：（1）通过掩膜条件的空间分解，实现语义解耦的潜在建模，从而能够精确地操控属性；（2）结合全局和局部专家的混合模型，捕捉整体结构和区域级语义，实现细粒度的可控性；（3）动态门控网络生成与扩散步骤和空间位置相适应的时间依赖系数。Face-MoGLE为高质量、可控的面孔生成提供了强大且灵活的解决方案，在生成建模和安全应用中具有很强的潜力。大量实验证明了其在多模态和单模态面孔生成情况下的有效性以及强大的零样本泛化能力。项目页面可在 <a target="_blank" rel="noopener" href="https://github.com/XavierJiezou/Face-MoGLE">https://github.com/XavierJiezou/Face-MoGLE</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该论文针对<strong>可控人脸生成</strong>任务中的核心难题——<strong>语义可控性与照片级真实感之间的精细平衡</strong>——提出系统性的解决方案。具体而言，现有方法存在以下关键缺陷：</p>
<ol>
<li><p><strong>语义控制与生成流程高度耦合</strong><br>主流扩散模型（如基于U-Net的LDM）将语义掩码与潜变量直接拼接，导致掩码误差直接传递到纹理合成，限制了局部区域的细粒度控制，并迫使推理阶段依赖像素级精确的掩码，削弱了零样本泛化能力。</p>
</li>
<li><p><strong>全局结构与局部细节难以兼顾</strong><br>U-Net的卷积归纳偏置限制了长程依赖建模，全局人脸结构（如五官布局一致性）与局部细节（如发丝纹理）的特征纠缠，阻碍了属性的精准操控。</p>
</li>
<li><p><strong>复杂多模态条件下的可控性不足</strong><br>文本、掩码等多模态输入的联合引导缺乏灵活机制，现有方法在零样本或组合泛化场景下难以同时满足高保真与语义对齐。</p>
</li>
</ol>
<p><strong>Face-MoGLE的针对性创新</strong>：</p>
<ul>
<li><strong>语义解耦</strong>：通过将语义掩码分解为独立的二值组件（如头发、轮廓、鼻子），实现属性级精确操控。</li>
<li><strong>全局-局部专家混合架构</strong>：全局专家建模整体结构（如头发与面部的空间关系），局部专家专注区域级细节（如眉毛纹理），通过动态门控网络在扩散过程中自适应融合。</li>
<li><strong>扩散感知的时空门控</strong>：门控权重随时间步与空间位置动态演化，解决静态融合导致的语义冲突与纹理不一致问题。</li>
</ul>
<p>综上，论文旨在<strong>突破现有可控人脸生成的架构瓶颈</strong>，通过扩散Transformer与专家混合机制，实现<strong>高保真、细粒度、零样本泛化</strong>的统一框架。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>论文在第2节“Related Work”中系统梳理了与本研究密切相关的三大方向，并指出了现有工作的局限性与Face-MoGLE的差异化定位。以下按主题归纳：</p>
<p>2.1 扩散模型（Diffusion Models）</p>
<ul>
<li><p><strong>奠基工作</strong></p>
<ul>
<li>DDPM（Ho et al., NeurIPS 2020）[13]：提出去噪扩散概率模型框架。</li>
<li>Classifier-Free Guidance（Nichol &amp; Dhariwal, NeurIPS 2021）[14]：无需外部分类器的条件生成方法。</li>
</ul>
</li>
<li><p><strong>架构演进</strong></p>
<ul>
<li>LDM（Rombach et al., CVPR 2022）[44]：基于U-Net的潜空间扩散模型，提升效率。</li>
<li>DiT（Peebles &amp; Xie, ICCV 2023）[38]：用Transformer替代U-Net，增强长程依赖建模。</li>
<li>FLUX（Black Forest Labs, 2024）[26]：最新开源扩散Transformer，作为Face-MoGLE的基础骨干。</li>
</ul>
</li>
</ul>
<p>2.2 人脸生成（Face Generation）</p>
<p>按任务类型分类：</p>
<p>单模态生成</p>
<ul>
<li><p><strong>Mask-to-Face</strong></p>
<ul>
<li>MaskGAN（Lee et al., CVPR 2020）[27]：基于掩码的交互式人脸编辑。</li>
<li>INADE（Tan et al., CVPR 2021）[50]：通过概率分布建模增强多样性。</li>
<li>SemFlow（Wang et al., NeurIPS 2024）[54]：用矫正流统一分割与合成。</li>
</ul>
</li>
<li><p><strong>Text-to-Face</strong></p>
<ul>
<li>clip2latent（Pinkney &amp; Li, BMVC 2022）[39]：结合CLIP与StyleGAN潜空间。</li>
<li>GCDP（Park et al., ICCV 2023）[36]：生成语义布局以提升文本-图像对齐。</li>
<li>E3-FaceNet（Zhang et al., ICML 2024）[60]：引入3D感知与几何正则化。</li>
</ul>
</li>
</ul>
<p>多模态生成</p>
<ul>
<li>TediGAN（Xia et al., CVPR 2021）[56]：文本与掩码联合控制的GAN框架。</li>
<li>PixelFace+（Du et al., ACM MM 2023）[8]：文本+掩码的StyleGAN扩展。</li>
<li>Collaborative Diffusion（Huang et al., CVPR 2023）[17]：扩散模型的即插即用多模态合成。</li>
<li>DDGI（Kim et al., CVPR 2024）[23]：融合GAN反演与扩散特征的多条件生成。</li>
</ul>
<p><strong>局限性总结</strong>：现有方法在<strong>细粒度控制</strong>与<strong>零样本泛化</strong>上难以兼顾，尤其在复杂组合条件下易出现结构不一致或纹理失真。</p>
<p>2.3 专家混合模型（Mixture of Experts, MoE）</p>
<ul>
<li><p><strong>早期理论</strong></p>
<ul>
<li>Adaptive Mixtures of Local Experts（Jacobs et al., 1991）[18]：提出专家网络竞争学习的框架。</li>
</ul>
</li>
<li><p><strong>大规模应用</strong></p>
<ul>
<li>Sparsely-Gated MoE（Shazeer et al., ICLR 2017）[46]：通过稀疏门控实现万亿级参数扩展。</li>
<li>Switch Transformer（Fedus et al., JMLR 2022）[10]：简化路由机制，提升训练稳定性。</li>
<li>V-MoE（Riquelme et al., NeurIPS 2021）[43]：将MoE引入Vision Transformer，实现图像分类任务的高效扩展。</li>
</ul>
</li>
</ul>
<p><strong>Face-MoGLE的创新</strong>：首次将MoE思想引入<strong>扩散Transformer的人脸生成</strong>，通过<strong>全局-局部专家协同</strong>与<strong>时空动态门控</strong>，解决可控性与保真度的矛盾。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过 <strong>Face-MoGLE（Mixture of Global and Local Experts with Diffusion Transformer）</strong> 框架，从 <strong>条件解耦、专家混合、动态门控</strong> 三个层面系统性地解决了可控人脸生成的核心难题。具体解决方案如下：</p>
<p>1. 语义解耦：Mask-Conditioned Space Factorization</p>
<ul>
<li><strong>问题</strong>：传统方法将语义掩码与潜变量直接拼接，导致掩码误差传播、局部控制受限。</li>
<li><strong>解决</strong>：<ul>
<li>将输入的语义掩码 <strong>$M \in \mathbb{R}^{H \times W \times 3}$</strong> 分解为 <strong>$n$ 个独立的二值掩码</strong> ${M^{(i)}}_{i=1}^n$，每个对应一个面部属性（如头发、鼻子、轮廓）。</li>
<li>通过共享的冻结 VAE 编码器 $E_{\text{VAE}}$ 提取各掩码的潜变量：  <script type="math/tex; mode=display">C\_m^{(i)} = E\_{\\text{VAE}}(M^{(i)}) \\in \\mathbb{R}^{L \\times d}, \\quad i=0,1,\\dots,n</script>其中 $M^{(0)}$ 为全局掩码，提供整体结构先验。</li>
<li><strong>效果</strong>：实现属性级精确操控，避免掩码误差耦合，支持零样本泛化。</li>
</ul>
</li>
</ul>
<p>2. 全局-局部专家混合（MoGLE）：结构一致性 + 细节保真</p>
<ul>
<li><strong>问题</strong>：全局结构（如五官布局）与局部细节（如发丝纹理）难以兼顾。</li>
<li><strong>解决</strong>：<ul>
<li><strong>全局专家</strong>（Global Expert）：处理全局掩码 $C_m^{(0)}$，建模整体面部结构（如头发与面部的空间对齐）。</li>
<li><strong>局部专家</strong>（Local Experts）：每个局部专家专注一个二值掩码 $C_m^{(i)}$，细化区域级细节（如眉毛纹理、耳环形状）。</li>
<li>专家输出通过动态门控融合，公式化为：  <script type="math/tex; mode=display">C\_m'^{(i)} = \\text{Expert}\_i(C\_m^{(i)})</script></li>
<li><strong>效果</strong>：全局一致性（无畸形）与局部高保真（如发丝清晰）协同优化。</li>
</ul>
</li>
</ul>
<p>3. 扩散感知的动态门控网络：时空自适应融合</p>
<ul>
<li><strong>问题</strong>：静态权重无法适应扩散过程中不同时间步与空间区域的语义需求。</li>
<li><strong>解决</strong>：<ul>
<li>门控网络 $g_\theta$ 以 <strong>当前噪声潜变量 $Z_t$</strong>、<strong>时间步嵌入 $E_{\text{time}}(t)$</strong> 和 <strong>全局掩码 token $C_m^{(0)}$</strong> 为输入，生成 <strong>时空变化的权重</strong>：  <script type="math/tex; mode=display">\[\\omega\_g^{(t)}, \\omega\_1^{(t)}, \\dots, \\omega\_n^{(t)}\] = g\_\\theta(Z\_t, E\_{\\text{time}}(t), C\_m^{(0)})</script>满足 $\omega_g^{(t)} + \sum_{i=1}^n \omega_i^{(t)} = 1$，且权重为 <strong>像素级热力图</strong>（如图5可视化）。</li>
<li>最终语义嵌入为：  <script type="math/tex; mode=display">C\_m = \\omega\_g^{(t)} \\cdot C\_m'^{(0)} + \\sum\_{i=1}^n \\omega\_i^{(t)} \\cdot C\_m'^{(i)}</script></li>
<li><strong>效果</strong>：早期扩散阶段（高噪声）侧重全局结构，后期（低噪声）强化局部细节，实现 <strong>语义-纹理渐进对齐</strong>。</li>
</ul>
</li>
</ul>
<p>4. 统一扩散Transformer架构：高效训练与推理</p>
<ul>
<li><strong>基础模型</strong>：基于 FLUX.1-dev 的扩散Transformer（DiT），保留冻结主干，仅训练 <strong>LoRA 适配器</strong>（秩=4），降低计算成本。</li>
<li><strong>多模态条件</strong>：支持文本（CLIP+H5编码）与掩码（MoGLE编码）的灵活组合，训练时以0.1概率随机丢弃任一条件，提升鲁棒性。</li>
<li><strong>推理灵活性</strong>：可单独使用文本（Text2Face）、掩码（Mask2Face）或两者联合（Multimodal），无需重新训练。</li>
</ul>
<p>5. 实验验证：全面超越现有方法</p>
<ul>
<li><strong>定量结果</strong>：在 MM-CelebA-HQ 和 MM-FFHQ-Female 上，FID、KID、CLIP分数等指标均优于 PixelFace+、Collaborative Diffusion 等 SOTA 方法（表1、表2）。</li>
<li><strong>零样本泛化</strong>：在未训练的 MM-FFHQ-Female 上，FID 降低 27.3%（相比 UaC），验证架构的通用性。</li>
<li><strong>消融实验</strong>：<ul>
<li>仅全局专家：FID=30.36（结构一致但细节模糊）。</li>
<li>仅局部专家：FID=33.62（细节丰富但结构畸形）。</li>
<li>全局+局部+动态门控：FID=22.24（最优平衡）。</li>
</ul>
</li>
</ul>
<p>总结</p>
<p>Face-MoGLE 通过 <strong>“解耦-分解-融合”</strong> 的三级设计，破解了可控人脸生成中长期存在的 <strong>语义控制与真实感冲突</strong> 难题，实现了 <strong>高保真、细粒度、零样本泛化</strong> 的统一框架。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>可控人脸生成</strong> 的三大任务（多模态、Mask-to-Face、Text-to-Face）及 <strong>零样本泛化、深度伪造检测、消融分析</strong> 共设计了 5 组实验，覆盖定量指标、定性可视化与人类主观评价。具体实验设置与结果如下：</p>
<p>1. 主实验：与 SOTA 方法的全面对比</p>
<p>1.1 数据集</p>
<ul>
<li><strong>MM-CelebA-HQ</strong>（30 k 高分辨率人脸 + 19 类语义掩码 + 10 条文本描述）</li>
<li><strong>MM-FFHQ-Female</strong>（760 张 FFHQ 女性人脸 + 9 条文本描述 + 语义掩码，专用于零样本测试）</li>
</ul>
<p>1.2 评价指标</p>
<p>维度</p>
<p>指标</p>
<p>说明</p>
<p><strong>图像质量</strong></p>
<p>FID ↓ / KID ↓ / CMMD ↓</p>
<p>分布距离、核方法距离、条件分布对齐</p>
<p><strong>条件一致性</strong></p>
<p>Mask ↓（DINO Structure Distance） / Text ↑（CLIP Score）</p>
<p>结构对齐、文本-图像语义对齐</p>
<p><strong>人类偏好</strong></p>
<p>IR ↑（ImageReward）</p>
<p>主观美感与一致性</p>
<p><strong>深度伪造检测</strong></p>
<p>AUC ↑ / EER ↓ / AP ↓</p>
<p>对抗检测器（NPR、Wavelet-CLIP）</p>
<p>1.3 对比方法</p>
<ul>
<li><strong>GAN 系列</strong>：TediGAN、PixelFace+、INADE、E2Style、E3-FaceNet</li>
<li><strong>扩散系列</strong>：UaC、Collaborative Diffusion、DDGI、SemFlow、GCDP</li>
</ul>
<p>1.4 结果摘要</p>
<p>任务</p>
<p>Face-MoGLE 最优指标</p>
<p>相对提升（vs 最强基线）</p>
<p><strong>Multimodal</strong></p>
<p>FID 22.24 / KID 10.87 / IR 0.7014</p>
<p>FID ↓ 9 %（vs Collaborative）</p>
<p><strong>Mask-to-Face</strong></p>
<p>FID 19.63 / KID 8.29 / IR 0.0398</p>
<p>FID ↓ 7 %（vs INADE）</p>
<p><strong>Text-to-Face</strong></p>
<p>FID 34.81 / KID 21.85 / CLIP 26.91</p>
<p>FID ↓ 46 %（vs clip2latent）</p>
<p>2. 零样本泛化实验</p>
<ul>
<li><strong>设置</strong>：仅在 MM-CelebA-HQ 训练，直接在 MM-FFHQ-Female 测试，无任何微调。</li>
<li><strong>结果</strong>：<ul>
<li>FID 62.93（↓ 27.3 % vs UaC）</li>
<li>KID 31.27（↓ 32.9 % vs UaC）</li>
<li>IR 0.1801（唯一正值，其余方法均为负）</li>
</ul>
</li>
</ul>
<p>3. 深度伪造检测评估</p>
<ul>
<li><strong>检测器</strong>：NPR（通用）、Wavelet-CLIP（人脸专用）</li>
<li><strong>结果</strong>：<ul>
<li><strong>NPR</strong>：AUC 0.50（接近随机，与真实人脸混淆）</li>
<li><strong>Wavelet-CLIP</strong>：AUC 0.46（显著优于 Collaborative 的 0.75）</li>
</ul>
</li>
</ul>
<p>4. 消融实验</p>
<p>4.1 全局 vs 局部 vs 联合专家</p>
<p>配置</p>
<p>FID ↓</p>
<p>Mask ↓</p>
<p>Text ↑</p>
<p>结论</p>
<p><strong>Only Global</strong></p>
<p>30.36</p>
<p>2.47</p>
<p>26.30</p>
<p>结构一致，细节模糊</p>
<p><strong>Only Local</strong></p>
<p>33.62</p>
<p>4.87</p>
<p>27.07</p>
<p>细节丰富，结构畸形</p>
<p><strong>Global + Local</strong></p>
<p><strong>22.24</strong></p>
<p><strong>2.44</strong></p>
<p><strong>26.32</strong></p>
<p>最佳平衡</p>
<p>4.2 门控机制对比</p>
<p>门控类型</p>
<p>FID ↓</p>
<p>说明</p>
<p><strong>无扩散感知</strong></p>
<p>25.74</p>
<p>缺乏时间适应性</p>
<p><strong>标量门控</strong></p>
<p>43.48</p>
<p>无法处理空间冲突</p>
<p><strong>矩阵门控（本文）</strong></p>
<p><strong>22.24</strong></p>
<p>时空自适应最优</p>
<p>4.3 专家与门控的协同效应</p>
<p>专家</p>
<p>门控</p>
<p>FID ↓</p>
<p>结论</p>
<p>✗</p>
<p>✗</p>
<p>33.25</p>
<p>基线</p>
<p>✓</p>
<p>✗</p>
<p>26.55</p>
<p>仅专家有效</p>
<p>✗</p>
<p>✓</p>
<p>31.30</p>
<p>仅门控有效</p>
<p>✓</p>
<p>✓</p>
<p><strong>22.24</strong></p>
<p>协同最优</p>
<p>5. 定性可视化</p>
<ul>
<li><strong>图6</strong>：多模态、Mask-to-Face、Text-to-Face 三任务对比，展示更精细的纹理与语义对齐。</li>
<li><strong>图7-9</strong>：分别对应多模态、掩码、文本任务的高分辨率示例。</li>
<li><strong>图10</strong>：零样本场景下，未见过的女性人脸仍保持高保真与文本一致性。</li>
<li><strong>图11</strong>：消融可视化直观呈现各组件对纹理与结构的影响。</li>
</ul>
<p>实验结论</p>
<p>通过 <strong>多维度定量指标 + 零样本测试 + 对抗检测 + 消融分析 + 高分辨率可视化</strong>，论文全面验证了 Face-MoGLE 在 <strong>保真度、可控性、泛化性、鲁棒性</strong> 上的显著优势。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为 Face-MoGLE 的后续研究切入点，按“效率-质量-安全-应用”四维度展开：</p>
<p>1. 架构效率与部署</p>
<ul>
<li><strong>专家剪枝与动态容量</strong><br>当前固定数量的全局/局部专家在推理时全部参与计算。可引入 <strong>Top-k 稀疏路由</strong> 或 <strong>专家重要性估计</strong>，仅激活与当前条件最相关的子网络，降低延迟与显存。</li>
<li><strong>量化-蒸馏联合优化</strong><br>将动态门控网络与专家网络分别进行 <strong>INT8 量化</strong> 与 <strong>知识蒸馏</strong>，在边缘设备（手机、AR 眼镜）上实现实时可控人脸生成。</li>
</ul>
<p>2. 条件粒度与一致性</p>
<ul>
<li><strong>层次化语义掩码</strong><br>将 19 类掩码进一步细化为 <strong>部件子区域</strong>（如“上唇/下唇”、“刘海/发梢”），并引入 <strong>层级门控</strong>（coarse-to-fine），实现更细粒度的局部编辑。</li>
<li><strong>跨帧一致性</strong><br>把单帧模型扩展为 <strong>时序扩散 Transformer</strong>，在视频人脸编辑或虚拟人驱动中保持 <strong>身份-表情-光照</strong> 的跨帧稳定。</li>
</ul>
<p>3. 鲁棒性与安全</p>
<ul>
<li><strong>对抗掩码扰动</strong><br>研究掩码微小扰动（如 1-2 像素偏移）对生成结果的影响，并训练 <strong>掩码鲁棒门控</strong> 网络，提升对自动分割误差的容忍度。</li>
<li><strong>可控水印与溯源</strong><br>在扩散过程中嵌入 <strong>不可感知水印</strong>（频域或噪声调度），实现生成人脸的 <strong>可追踪性</strong>，同时不降低视觉质量。</li>
</ul>
<p>4. 跨模态与下游任务</p>
<ul>
<li><strong>草图-文本-掩码三模态</strong><br>将草图作为第三种条件，设计 <strong>三塔专家</strong>（global-sketch, local-sketch, global-mask, local-mask…）与 <strong>三线性门控</strong>，实现“一句话+一张草图+一张掩码”的复合控制。</li>
<li><strong>少样本身份定制</strong><br>用 3-5 张参考人脸微调 <strong>身份专属专家</strong>，在保持高保真的同时实现 <strong>快速身份适应</strong>，服务于数字替身、影视特效等场景。</li>
<li><strong>生成式数据增强</strong><br>利用 Face-MoGLE 合成的 <strong>高真实度-可控属性</strong> 人脸，扩充深度伪造检测、人脸识别模型的训练集，提升下游任务在 <strong>长尾属性</strong>（罕见发型、特殊配饰）上的鲁棒性。</li>
</ul>
<p>5. 理论分析</p>
<ul>
<li><strong>门控可解释性</strong><br>通过 <strong>Grad-CAM 或注意力 rollout</strong> 可视化不同时空位置的权重分布，验证门控网络是否真正学到“早期全局、后期局部”的物理直觉。</li>
<li><strong>扩散步数-专家容量权衡</strong><br>建立 <strong>理论模型</strong> 刻画“步数 t 与所需专家容量”的关系，指导自适应计算预算分配。</li>
</ul>
<p>以上方向既可直接在 Face-MoGLE 代码库上迭代，也可作为独立子课题深入。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文一句话总结</p>
<p>Face-MoGLE 提出一种基于扩散 Transformer 的“全局-局部专家混合”框架，通过语义掩码解耦、时空动态门控与高效微调，实现高保真、细粒度、零样本可控人脸生成。</p>
<p>核心内容速览</p>
<p>维度</p>
<p>关键设计</p>
<p>作用</p>
<p><strong>问题</strong></p>
<p>现有扩散模型在可控人脸生成中面临“语义控制-照片真实感”两难，且对复杂多模态输入缺乏零样本泛化能力。</p>
<p><strong>架构</strong></p>
<p>• <strong>扩散 Transformer（DiT）</strong> 替代 U-Net，长程建模更优。<br>• <strong>语义掩码解耦</strong>：将整体掩码拆成二值部件掩码，实现属性级控制。<br>• <strong>全局-局部专家混合</strong>：<br> - 全局专家：维护整体结构一致性；<br> - 局部专家：细化区域纹理与细节。<br>• <strong>时空动态门控</strong>：根据扩散时间步与空间位置，自适应融合专家输出。</p>
<p><strong>训练-推理</strong></p>
<p>• 冻结 DiT 主干，仅训练 LoRA（秩=4），12 小时 8×A100 完成。<br>• 支持文本、掩码或二者联合条件，可任意缺失，无需重训。</p>
<p><strong>实验</strong></p>
<p>• <strong>三任务</strong>（多模态 / Mask-to-Face / Text-to-Face）均夺 SOTA：FID 最低 19.63。<br>• <strong>零样本泛化</strong>：在未见过的 MM-FFHQ-Female 上 FID 再降 27.3%。<br>• <strong>深度伪造检测</strong>：AUC≈0.5，成功欺骗主流检测器。<br>• <strong>消融</strong>：全局+局部+矩阵门控缺一不可，联合后 FID 从 33.25→22.24。</p>
<p><strong>贡献</strong></p>
<p>1. 统一 DiT 框架，首次将 MoE 思想引入可控人脸生成。<br>2. 语义解耦 + 动态门控，实现细粒度控制与零样本泛化。<br>3. 开源代码与扩展数据集，推动社区研究。</p>
<p>一句话记忆</p>
<p>“把人脸拆成部件、让专家各司其职、再按时空动态投票”——这就是 Face-MoGLE 的高保真可控秘诀。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Mixture%20of%20Global%20and%20Local%20Experts%20with%20Diffusion%20Transformer%20for%20Controllable%20Face%20Generation，链接是  https://arxiv.org/pdf/2509.00428  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.00428  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xuechao Zou,Shun Zhang,Xing Fu,Yue Li,Kai Li,Yushe Cao,Congyan Lang,Pin Tao,Junliang Xing</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00428.pdf">https://arxiv.org/pdf/2509.00428.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00428">https://arxiv.org/abs/2509.00428</a></p>
<p><strong>Arxiv ID</strong>: 2509.00428</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00428">https://papers.cool/arxiv/2509.00428</a></p>
<p><strong>Published</strong>: 2025-08-30T09:21:07Z</p>
<p><strong>Updated</strong>: 2025-08-30T09:21:07.000Z</p>
<hr>
<h3 id="6-Planning-with-Reasoning-using-Vision-Language-World-Model"><a href="#6-Planning-with-Reasoning-using-Vision-Language-World-Model" class="headerlink" title="6. Planning with Reasoning using Vision Language World Model"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.02722">Planning with Reasoning using Vision Language World Model</a></h3><p>Effective planning requires strong world models, but high-level world models that can understand and reason about actions with semantic and temporal abstraction remain largely underdeveloped. We introduce the Vision Language World Model (VLWM), a foundation model trained for language-based world modeling on natural videos. Given visual observations, the VLWM first infers the overall goal achievements then predicts a trajectory composed of interleaved actions and world state changes. Those targets are extracted by iterative LLM Self-Refine conditioned on compressed future observations represented by Tree of Captions. The VLWM learns both an action policy and a dynamics model, which respectively facilitates reactive system-1 plan decoding and reflective system-2 planning via cost minimization. The cost evaluates the semantic distance between the hypothetical future states given by VLWM roll-outs and the expected goal state, and is measured by a critic model that we trained in a self-supervised manner. The VLWM achieves state-of-the-art Visual Planning for Assistance (VPA) performance on both benchmark evaluations and our proposed PlannerArena human evaluations, where system-2 improves the Elo score by +27% upon system-1. The VLWM models also outperforms strong VLM baselines on RoboVQA and WorldPrediction benchmark.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>有效的规划需要强大的世界模型，但能够理解和推理关于动作的语义和时间抽象的高级世界模型仍然在很大程度上未得到发展。我们引入了视觉语言世界模型（VLWM），这是一个基于自然视频的语言基础世界建模训练的基础模型。给定视觉观察，VLWM首先推断整体目标达成，然后预测由交错动作和世界状态变化组成的轨迹。这些目标通过基于未来压缩观察的标题树所表示的条件，采用迭代的 LLM 自我精炼提取。VLWM同时学习一个动作策略和一个动态模型，分别促进反应式系统-1 计划解码和反思式系统-2 规划，通过成本最小化。成本评估 VLWM 推演给出的假设未来状态与预期目标状态之间的语义距离，并由我们以自监督方式训练的批评模型进行测量。VLWM 在基准评估和我们提出的 PlannerArena 人类评估中，在辅助视觉规划（VPA）性能上达到了最先进的水平，其中系统-2 相较系统-1 提高了 +27% 的 Elo 分数。VLWM 模型在 RoboVQA 和 WorldPrediction 基准上也超越了强大的 VLM 基线。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文聚焦的核心问题是：<br><strong>如何构建一个能够进行高层语义与时间抽象的高阶世界模型，以支持复杂任务中的有效规划。</strong></p>
<p>具体而言，现有方法在以下三方面存在显著缺陷：</p>
<ol>
<li><strong>低层控制局限</strong>：既有世界模型（如扩散模型、JEPA 变体）主要面向连续、低层控制任务（机器人、自动驾驶），难以处理“语义+时间”双重抽象的高层任务规划。</li>
<li><strong>数据与表征瓶颈</strong>：<ul>
<li>基于仿真环境训练难以泛化到真实世界多样性；</li>
<li>基于像素级生成在长时程规划中计算昂贵且易受部分可观测性影响；</li>
<li>纯 LLM/VLM 提示方法缺乏视觉感知接地，且无法显式建模动作-状态转移。</li>
</ul>
</li>
<li><strong>缺乏可解释、可扩展的抽象机制</strong>：现有工作要么停留在像素级重建，要么依赖人工标注的封闭动作词汇，无法从大规模无标注自然视频中自动提炼高层、可解释的“动作-状态”轨迹。</li>
</ol>
<p>因此，论文提出 <strong>Vision Language World Model (VLWM)</strong>，目标是在自然语言空间中学习世界动力学，实现：</p>
<ul>
<li><strong>高层抽象</strong>：用可解释的文本描述动作及其引发的世界状态变化；</li>
<li><strong>长时规划</strong>：支持 System-1（快速反应式）与 System-2（基于代价最小化的反思式）两种规划模式；</li>
<li><strong>大规模训练</strong>：直接利用 180k 小时、无奖励标签的网页教学视频与 egocentric 记录，避免昂贵仿真或人工标注。</li>
</ul>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究按主题归类，与 VLWM 在方法论、任务设定或技术路线层面存在直接关联。</p>
<p>1. 高层任务规划与行为克隆</p>
<ul>
<li><strong>COIN / CrossTask / YouCook2 / HowTo100M</strong><br>大规模教学视频数据集，为 VLWM 提供无标注训练语料。</li>
<li><strong>Procedure Planning in Instructional Videos</strong> (Chang et al., ECCV 2020)<br>早期尝试用监督学习从教学视频中预测下一步动作，受限于封闭动作词汇。</li>
<li><strong>VidAssist</strong> (Islam et al., ECCV 2024)<br>70B LLM + 视觉编码器，在 VPA 基准上取得此前 SOTA；VLWM-8B 在其 12 项指标中 8 项超越。</li>
<li><strong>PlannerArena</strong>（本文提出）<br>受 ChatbotArena 启发，用 Elo 评分直接衡量人类对生成计划的偏好，解决传统 benchmark 标签质量差的问题。</li>
</ul>
<p>2. 世界模型（World Models）</p>
<p>2.1 低层控制与像素级生成</p>
<ul>
<li><strong>Dreamer / Mastering Diverse Domains</strong> (Hafner et al., 2024)<br>基于 RSSM 的 latent-imagination RL，在 Atari、Robotics 取得高样本效率。</li>
<li><strong>GAIA-1</strong> (Hu et al., 2023)<br>自动驾驶生成式世界模型，直接预测未来多视角视频帧。</li>
<li><strong>Genie / Genie 2</strong> (Bruce et al., ICML 2024; Parker-Holder et al., 2024)<br>交互式潜变量生成模型，支持从单张图像生成交互式 3D 环境。</li>
<li><strong>Cosmos / UniSim / Sora</strong> (Agarwal et al., 2025; Brooks et al., 2024; Yang et al., 2024)<br>扩散式视频生成框架，强调高保真模拟，但计算开销巨大且难以长时规划。</li>
</ul>
<p>2.2 JEPA 与表征预测</p>
<ul>
<li><strong>I-JEPA / V-JEPA / DINO-WM</strong> (Assran et al., 2023; Bardes et al., 2024; Zhou et al., 2024)<br>通过 joint-embedding 预测未来表征，避免像素级重建；目前主要应用于低层控制或表征学习。</li>
<li><strong>V-JEPA 2 / NWM</strong> (Assran et al., 2025; Bar et al., 2025)<br>将 JEPA 用于 MPC 规划，但仍局限于连续动作空间。</li>
</ul>
<p>3. 语言作为世界模型接口</p>
<ul>
<li><strong>Reasoning with LM is Planning with WM</strong> (Hao et al., EMNLP 2023)<br>用 LLM 作为文本世界模型，通过提示进行规划；缺乏视觉感知与大规模训练。</li>
<li><strong>Web Agents with World Models</strong> (Chae et al., 2024)<br>在网页导航任务中用语言建模环境转移，但领域狭窄。</li>
<li><strong>Learning to Model the World with Language</strong> (Lin et al., 2023)<br>探索用语言预测环境状态，训练数据局限于文本游戏与仿真环境。</li>
</ul>
<p>4. 自监督代价函数 / 偏好建模</p>
<ul>
<li><strong>Self-Refine</strong> (Madaan et al., NeurIPS 2023)<br>迭代式 LLM 自反馈优化，被 VLWM 用于将 Tree of Captions 提炼为结构化目标-计划。</li>
<li><strong>UltraFeedback / Math-Step-DPO / Orca DPO</strong> (Cui et al., 2023; Lai et al., 2024; Lian et al., 2023)<br>偏好三元组数据，被 VLWM-critic 用作额外自监督信号。</li>
<li><strong>Reward Centering</strong> (Naik et al., 2024)<br>代价中心化正则项，与 VLWM-critic 训练损失形式一致。</li>
</ul>
<p>5. 机器人与具身智能基准</p>
<ul>
<li><strong>RoboVQA</strong> (Sermanet et al., ICRA 2024)<br>多模态长时程机器人问答；VLWM 取得 74.2 BLEU-1，超越 RoboBrain-7B 等专用模型。</li>
<li><strong>WorldPrediction-PP</strong> (Chen et al., 2025)<br>高阶程序规划 benchmark；VLWM-critic 零样本取得 45.4 % 准确率，刷新 SOTA。</li>
</ul>
<p>小结</p>
<p>类别</p>
<p>代表工作</p>
<p>与 VLWM 的关系</p>
<p>教学视频规划</p>
<p>COIN, CrossTask, VidAssist</p>
<p>数据集 &amp; 强基线</p>
<p>像素级世界模型</p>
<p>Dreamer, GAIA-1, Genie, Sora</p>
<p>低层控制，计算昂贵</p>
<p>JEPA 表征预测</p>
<p>V-JEPA, DINO-WM</p>
<p>表征抽象，但局限于低层</p>
<p>语言世界模型</p>
<p>Hao et al., Lin et al.</p>
<p>高层抽象，但缺乏视觉接地</p>
<p>自监督代价学习</p>
<p>Self-Refine, UltraFeedback</p>
<p>VLWM-critic 训练范式</p>
<p>机器人基准</p>
<p>RoboVQA, WorldPrediction-PP</p>
<p>评估高层规划与推理能力</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过 <strong>Vision Language World Model (VLWM)</strong> 框架，从数据构造、模型训练到推理范式三个层面系统性地解决了“高层语义-时间抽象世界模型缺失”的问题。核心思路是：<strong>用自然语言作为世界状态的抽象表征，在大规模无标注视频中直接学习动作-状态动力学，并设计双系统（System-1 / System-2）推理机制实现快速反应与深度规划。</strong></p>
<p>1. 数据构造：从原始视频到结构化〈动作-状态〉轨迹</p>
<p>1.1 Tree of Captions（语义压缩）</p>
<ul>
<li><strong>分层聚类</strong>：对原始视频特征流做层次凝聚聚类，自适应生成多尺度片段，保证每个节点对应“单语义单元”。</li>
<li><strong>高效压缩</strong>：1.1 TB 的 Ego4D 视频压缩至 &lt;900 MB 文本，兼顾长时程与细粒度。</li>
</ul>
<p>1.2 LLM Self-Refine（结构化提炼）</p>
<ul>
<li><strong>输入</strong>：Tree of Captions + 可选 ASR/专家旁白。</li>
<li><strong>输出</strong>：四元组<ul>
<li>Goal Description（一句话目标）</li>
<li>Goal Interpretation（初始 &amp; 期望终态的详细文本）</li>
<li>Action（可执行的高层指令）</li>
<li>World State Change（动作导致的因果状态变化）</li>
</ul>
</li>
<li><strong>迭代优化</strong>：用 Llama-4 Maverick 进行两轮 Self-Refine，显著减少冗余、错位步骤（示例见附录 B）。</li>
</ul>
<p>最终获得 <strong>1.2 M 条轨迹、5.7 M 步</strong> 的高质量训练目标，无需人工标注。</p>
<p>2. 模型训练：统一视觉-语言世界建模</p>
<p>2.1 训练任务</p>
<ul>
<li><p><strong>形式化</strong>：  </p>
<script type="math/tex; mode=display">\[\\text{config}, \\text{context}\] \\xrightarrow{\\text{VLWM}} \[\\text{goal}, \\text{interpretation}, \\langle A\_0,\\Delta S\_0\\rangle,\\dots,\\langle A\_N,\\Delta S\_N\\rangle\]</script><p>同时优化</p>
<ol>
<li>目标推断（Goal Inference）</li>
<li>动作预测（Action Anticipation）</li>
<li>状态转移预测（Dynamics）</li>
</ol>
</li>
<li><strong>初始化</strong>：用 PerceptionLM-8B（VLM）热启，保留强大视觉感知与常识。</li>
</ul>
<p>2.2 训练规模</p>
<ul>
<li>数据：180 k 视频（COIN、CrossTask、YouCook2、HowTo100M、EPIC-KITCHENS、EgoExo4D）+ 1 M 文本推理链。</li>
<li>算力：12×8 H100，5 天完成 8 B 模型训练。</li>
</ul>
<p>3. 推理范式：System-1 与 System-2 双模式</p>
<p>3.1 System-1（反应式）</p>
<ul>
<li><strong>直接文本自回归</strong>：给定视觉上下文 + 目标，一次性生成动作-状态链，适合短程、简单任务。</li>
</ul>
<p>3.2 System-2（反思式）</p>
<ul>
<li><p><strong>三组件协同</strong>：</p>
<ol>
<li><strong>Actor</strong>：VLWM 自身或外部 LLM，提出候选动作序列。</li>
<li><strong>VLWM Roll-out</strong>：对每个候选序列模拟未来状态。</li>
<li><strong>Critic</strong>：自监督训练的语言模型，计算语义距离  <script type="math/tex; mode=display">C=\\text{critic}(\\text{goal}, \\text{trajectory})</script>通过 ranking loss 学习“有效进展 &lt; 基线 &lt; 干扰/乱序”的排序。</li>
</ol>
</li>
<li><strong>搜索策略</strong>：束搜索 / 树搜索，选择代价最小的计划；支持外部约束（安全、格式）注入。</li>
</ul>
<p>实验表明：System-2 在 PlannerArena 人类评测中将 Elo 提升 <strong>+27 %</strong>，验证反思机制的有效性。</p>
<p>4. 关键创新总结</p>
<p>维度</p>
<p>传统方法</p>
<p>VLWM 解决方案</p>
<p>状态表征</p>
<p>像素 / 低维潜变量</p>
<p>自然语言（可解释、易扩展）</p>
<p>数据来源</p>
<p>仿真或人工标注</p>
<p>180 k 小时无标注自然视频</p>
<p>训练目标</p>
<p>重建或单步奖励</p>
<p>联合建模〈动作-状态〉动力学</p>
<p>推理能力</p>
<p>行为克隆（System-1）</p>
<p>System-2 代价最小化搜索</p>
<p>评估体系</p>
<p>固定标签基准</p>
<p>PlannerArena 人类偏好 + 多任务 benchmark</p>
<p>通过上述设计，VLWM 在 VPA、RoboVQA、WorldPrediction-PP 等基准均刷新 SOTA，同时提供可读的“动作-状态”解释轨迹。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文从 <strong>定量基准、人类主观评价、模型内部行为诊断</strong> 三条主线展开实验，覆盖高层视觉规划、机器人问答、世界模型诊断三大任务。所有实验均围绕 VLWM 的两种推理模式（System-1 直接解码 vs System-2 代价最小化搜索）与自监督 Critic 展开。</p>
<p>1. 高层视觉规划：Visual Planning for Assistance (VPA)</p>
<p>1.1 标准基准结果</p>
<ul>
<li><strong>数据集</strong>：COIN（180 任务，11 827 视频）与 CrossTask（18 任务，2 750 视频）。</li>
<li><strong>指标</strong>：Success Rate (SR)、Mean Accuracy (mAcc)、Mean IoU (mIoU)。</li>
<li><strong>对比系统</strong>：DDN、LTA、VLaMP、VidAssist（70B LLM）及两种频率基线。</li>
</ul>
<p>数据集 / 步长</p>
<p>SR↑</p>
<p>mAcc↑</p>
<p>mIoU↑</p>
<p>相对提升</p>
<p>COIN T=3</p>
<p>27.9</p>
<p>50.1</p>
<p>69.3</p>
<p>+6.1 / +5.7 / +4.9</p>
<p>COIN T=4</p>
<p>19.4</p>
<p>46.7</p>
<p>74.0</p>
<p>+5.6 / +8.4 / +7.7</p>
<p>CrossTask T=3</p>
<p>13.5</p>
<p>36.4</p>
<p>48.3</p>
<p>+1.5 / +0.1 / -0.6</p>
<p>CrossTask T=4</p>
<p>7.2</p>
<p>33.6</p>
<p>51.1</p>
<p>-0.2 / +1.7 / -0.5</p>
<p><strong>结论</strong>：VLWM-8B 在 12 项指标中 8 项刷新 SOTA，平均绝对提升 +3.2 % SR、+3.9 % mAcc、+2.9 mIoU，且参数远小于 VidAssist-70B。</p>
<p>2. 人类主观评价：PlannerArena</p>
<p>2.1 实验设计</p>
<ul>
<li><strong>形式</strong>：550 对匿名计划两两对比，Elo 评分体系（K=32）。</li>
<li><strong>参赛者</strong>：<ul>
<li>VLWM System-2（8B 模型 + 1B critic，代价最小化）</li>
<li>VLWM System-1*（仅 8B 模型，无搜索）</li>
<li>Llama-4-Maverick-400B、Qwen2.5VL-72B、PerceptionLM-8B</li>
<li>Ground Truth 标注计划</li>
</ul>
</li>
</ul>
<p>模型</p>
<p>总 Elo</p>
<p>COIN 胜率</p>
<p>CrossTask 胜率</p>
<p>EgoExo4D 胜率</p>
<p><strong>VLWM System-2</strong></p>
<p><strong>1261</strong></p>
<p>87.9 %</p>
<p>70.6 %</p>
<p>87.9 %</p>
<p>Llama-4-Maverick</p>
<p>1099</p>
<p>66.7 %</p>
<p>89.6 %</p>
<p>57.1 %</p>
<p>Ground Truth</p>
<p>952</p>
<p>43.6 %</p>
<p>42.2 %</p>
<p>69.5 %</p>
<p>VLWM System-1*</p>
<p>992</p>
<p>34.3 %</p>
<p>37.0 %</p>
<p>50.0 %</p>
<p><strong>结论</strong>：</p>
<ul>
<li>System-2 搜索模式大幅优于 System-1（+269 Elo）。</li>
<li>Ground Truth 在 COIN/CrossTask 仅略胜最差基线，验证现有数据集标签质量低。</li>
<li>人类 Fleiss’ κ = 0.63，标注一致性“substantial”。</li>
</ul>
<p>3. 机器人问答：RoboVQA</p>
<ul>
<li><strong>任务</strong>：多 embodiment、长时程机器人 VQA。</li>
<li><strong>指标</strong>：BLEU-1（主要报告）及 BLEU-2/3/4。</li>
</ul>
<p>模型</p>
<p>BLEU-1</p>
<p>备注</p>
<p><strong>VLWM-8B</strong></p>
<p><strong>74.2</strong></p>
<p>非机器人专用数据</p>
<p>RoboBrain-7B</p>
<p>72.1</p>
<p>机器人专用</p>
<p>ThinkAct</p>
<p>69.1</p>
<p>强化微调</p>
<p>PhysVLM-3B</p>
<p>65.3</p>
<p>物理可达性感知</p>
<p><strong>结论</strong>：VLWM 在 BLEU-1 夺魁，BLEU-4 55.6 vs RoboBrain 55.1，显示跨域泛化能力。</p>
<p>4. Critic 内部诊断实验</p>
<p>4.1 Goal Achievement Detection</p>
<ul>
<li><strong>任务</strong>：给定目标与轨迹（含正确步骤+干扰步骤），检测何时真正达成目标。</li>
<li><strong>数据集</strong>：<ul>
<li>VLWM-Instruct（同训练分布）</li>
<li>VLWM-Ego（egocentric，OOD）</li>
<li>OGP-Robot/WikiHow（动作-only，完全 OOD）</li>
</ul>
</li>
</ul>
<p>模型</p>
<p>Instruct</p>
<p>Ego</p>
<p>OGP-Robot</p>
<p>OGP-WikiHow</p>
<p><strong>VLWM-critic-1B</strong></p>
<p>98.4 %</p>
<p>92.7 %</p>
<p>72.9 %</p>
<p>50.0 %</p>
<p>最强基线 (Qwen3-Reranker-8B)</p>
<p>67.3 %</p>
<p>65.4 %</p>
<p>65.6 %</p>
<p>49.3 %</p>
<p><strong>结论</strong>：</p>
<ul>
<li>同域几乎完美；egocentric 数据下降但仍显著优于基线。</li>
<li>动作-only 轨迹导致显著噪声，说明状态描述对代价估计至关重要（表 6 ablation 佐证）。</li>
</ul>
<p>4.2 Procedural Planning on WorldPrediction-PP</p>
<ul>
<li><strong>设定</strong>：570 条人工验证样本，四选一选出正确动作顺序（零样本）。</li>
<li><strong>结果</strong>：VLWM-critic 取得 <strong>45.4 %</strong> 准确率，刷新该 benchmark SoTA（图 6）。</li>
</ul>
<p>5. Ablation &amp; 敏感性分析</p>
<ul>
<li><strong>输入表示消融</strong>：去掉 Goal Interpretation 或 World State Description 均导致一致下降，OOD 场景下降更剧烈（表 6）。</li>
<li><strong>代价曲线可视化</strong>：VLWM-critic 在 100 % 计划完成点呈现清晰全局最小，基线模型出现多峰或噪声（图 5）。</li>
</ul>
<p>实验全景一览</p>
<p>实验类别</p>
<p>数据集/平台</p>
<p>关键结果</p>
<p>VPA 基准</p>
<p>COIN, CrossTask</p>
<p>8/12 指标刷新 SOTA，+3.2 % SR 平均提升</p>
<p>人类评测</p>
<p>PlannerArena</p>
<p>System-2 Elo 1261，&gt; Llama-4-400B</p>
<p>机器人问答</p>
<p>RoboVQA</p>
<p>BLEU-1 74.2，领先所有基线</p>
<p>Critic 诊断</p>
<p>VLWM-Instruct/Ego/OGP</p>
<p>目标检测 98.4 %→50 %，验证语义代价有效性</p>
<p>程序规划</p>
<p>WorldPrediction-PP</p>
<p>零样本 45.4 % 准确率，新 SoTA</p>
<p>整体而言，实验体系从 <strong>自动化指标 → 人类主观 → 模型内部行为</strong> 递进，充分验证了 VLWM 在大规模无标注视频上习得高层世界模型并用于可解释规划的可行性与先进性。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为 VLWM 的直接延伸或长期探索，按“数据-模型-推理-应用”四层次归纳：</p>
<p>1. 数据与表征</p>
<ul>
<li><strong>多模态 Tree of Captions</strong><br>当前仅用文本 caption，可引入 3D 场景图、音频转录、触觉信号，构建 <strong>跨模态 Tree</strong>，提升对物理因果的刻画。</li>
<li><strong>事件级时间戳自动校准</strong><br>现有聚类依赖视觉特征，可结合 <strong>自监督时序对齐</strong>（如 V-JEPA 特征 + 对比学习）减少片段边界误差。</li>
<li><strong>开放词汇动作本体</strong><br>用 LLM 自动生成动作同义词层级，解决不同数据集动作粒度不一致问题，支持 <strong>零样本跨任务迁移</strong>。</li>
</ul>
<p>2. 模型架构与训练目标</p>
<ul>
<li><strong>双向动力学</strong><br>当前仅前向预测 St → St+1，可扩展 <strong>逆向模型</strong> St+1 → At，用于反事实解释与安全约束验证。</li>
<li><strong>分层规划 Transformer</strong><br>在 8B 模型内部显式引入 <strong>高层 Planner + 低层 Controller</strong> 两级解码路径，降低长序列生成误差累积。</li>
<li><strong>多智能体世界模型</strong><br>将“人-机-环境”统一建模，支持 <strong>协同任务规划</strong>（如人机共煮、多机器人协作装配）。</li>
</ul>
<p>3. 推理与搜索</p>
<ul>
<li><strong>蒙特卡洛树搜索 (MCTS) 集成</strong><br>把 critic 的代价函数作为 UCB 奖励，实现 <strong>深度 &gt;20 步</strong> 的长时程树搜索，突破当前 20-plan 束搜索限制。</li>
<li><strong>在线自适应 Critic</strong><br>部署阶段允许用户实时反馈（点赞/纠正），用 <strong>强化学习 from Human Feedback (RLHF)</strong> 微调 critic，个性化代价函数。</li>
<li><strong>可解释反事实</strong><br>生成“若未执行动作 A，状态将如何偏离目标”的自然语言解释，用于 <strong>安全审计与教学</strong>。</li>
</ul>
<p>4. 应用与评估</p>
<ul>
<li><strong>真实机器人闭环实验</strong><br>将 VLWM 的 System-2 计划通过 <strong>SayCan / RT-2</strong> 等执行层在真实厨房/工厂场景闭环验证，测量 <strong>任务完成率与人工干预次数</strong>。</li>
<li><strong>可穿戴 AI 助手</strong><br>在 AR 眼镜端部署轻量级 VLWM-LoRA，结合实时 egocentric 视频流，提供 <strong>下一步提示 + 错误恢复</strong>。</li>
<li><strong>跨语言/跨文化泛化</strong><br>测试模型在非英语视频及多语言目标描述下的性能，研究 <strong>文化差异导致的动作语义偏移</strong>。</li>
</ul>
<p>5. 长期挑战</p>
<ul>
<li><strong>部分可观测与因果混淆</strong><br>当关键状态变量被遮挡时，如何利用语言先验进行 <strong>反事实推理与缺失信息补全</strong>。</li>
<li><strong>可证明的安全性</strong><br>将 critic 的语义距离形式化为 <strong>形式化验证</strong> 中的代价函数，给出“计划不会导致危险状态”的概率保证。</li>
<li><strong>持续世界模型更新</strong><br>设计 <strong>在线增量学习</strong> 流程，使 VLWM 在新环境部署后无需重训即可吸收新动力学。</li>
</ul>
<p>小结（路线图）</p>
<p>阶段</p>
<p>研究问题</p>
<p>预期收益</p>
<p>短期</p>
<p>音频/3D 融合 + MCTS 搜索</p>
<p>提升长时程规划成功率</p>
<p>中期</p>
<p>在线 RLHF + 机器人闭环</p>
<p>个性化、可验证的物理 AI</p>
<p>长期</p>
<p>多智能体 + 形式化安全</p>
<p>真正可信赖的通用世界模型</p>
<p>这些方向既可直接基于 VLWM 的开源代码与数据扩展，也能为未来“语言-视觉-行动”统一智能体奠定理论与系统基础。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>Vision Language World Model (VLWM) 核心概要</p>
<p>1. 问题背景</p>
<p>高层任务规划需要既能理解视觉场景、又能进行语义-时间抽象的世界模型；现有方法要么局限于低层像素或潜变量，要么依赖昂贵仿真或封闭动作词汇，难以泛化到真实世界。</p>
<p>2. 解决方案</p>
<p>提出 <strong>Vision Language World Model</strong>——</p>
<ul>
<li><strong>用自然语言作为世界状态的抽象表征</strong></li>
<li><strong>在大规模无标注教学/egocentric 视频上直接训练</strong></li>
<li><strong>支持两种推理模式</strong><ul>
<li>System-1：快速自回归生成计划</li>
<li>System-2：用自监督 Critic 做代价最小化搜索，实现“内部试错”式反思规划</li>
</ul>
</li>
</ul>
<p>3. 技术路线</p>
<p>阶段</p>
<p>关键步骤</p>
<p>输出</p>
<p>数据构造</p>
<p>① 层次聚类生成 Tree of Captions<br>② LLM Self-Refine 提炼目标-动作-状态链</p>
<p>1.2 M 高质量轨迹</p>
<p>模型训练</p>
<p>8 B 视觉-语言 Transformer，统一预测 goal + 〈A, ΔS〉序列</p>
<p>端到端世界模型</p>
<p>推理机制</p>
<p>自监督 Critic 评估语义距离，System-2 搜索最优计划</p>
<p>可解释、可扩展规划</p>
<p>4. 实验结果</p>
<ul>
<li><strong>VPA 基准</strong>：12 项指标中 8 项刷新 SOTA（+3.2 % SR 平均）</li>
<li><strong>PlannerArena 人类评测</strong>：System-2 Elo 1261，领先 400 B Llama-4 162 分</li>
<li><strong>RoboVQA</strong>：BLEU-1 74.2，超越所有机器人专用模型</li>
<li><strong>Critic 诊断</strong>：目标达成检测 96.9 %，程序规划 45.4 %（新 SOTA）</li>
</ul>
<p>5. 贡献与意义</p>
<ul>
<li><strong>首个</strong> 直接在大规模自然视频上训练、以语言为状态空间的高层世界模型</li>
<li><strong>双系统推理</strong>：兼顾实时反应与深度反思</li>
<li><strong>全链路开源</strong>：模型、数据、评估框架全部公开，推动可解释、可落地的视觉-语言-行动智能体研究。</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Planning%20with%20Reasoning%20using%20Vision%20Language%20World%20Model，链接是  https://arxiv.org/pdf/2509.02722  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02722  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Delong Chen,Theo Moutakanni,Willy Chung,Yejin Bang,Ziwei Ji,Allen Bolourchi,Pascale Fung</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02722.pdf">https://arxiv.org/pdf/2509.02722.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02722">https://arxiv.org/abs/2509.02722</a></p>
<p><strong>Arxiv ID</strong>: 2509.02722</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02722">https://papers.cool/arxiv/2509.02722</a></p>
<p><strong>Published</strong>: 2025-09-02T18:18:57Z</p>
<p><strong>Updated</strong>: 2025-09-02T18:18:57.000Z</p>
<hr>
<h3 id="7-Manipulation-as-in-Simulation-Enabling-Accurate-Geometry-Perception-in-Robots"><a href="#7-Manipulation-as-in-Simulation-Enabling-Accurate-Geometry-Perception-in-Robots" class="headerlink" title="7. Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.02530">Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots</a></h3><p>Modern robotic manipulation primarily relies on visual observations in a 2D color space for skill learning but suffers from poor generalization. In contrast, humans, living in a 3D world, depend more on physical properties-such as distance, size, and shape-than on texture when interacting with objects. Since such 3D geometric information can be acquired from widely available depth cameras, it appears feasible to endow robots with similar perceptual capabilities. Our pilot study found that using depth cameras for manipulation is challenging, primarily due to their limited accuracy and susceptibility to various types of noise. In this work, we propose Camera Depth Models (CDMs) as a simple plugin on daily-use depth cameras, which take RGB images and raw depth signals as input and output denoised, accurate metric depth. To achieve this, we develop a neural data engine that generates high-quality paired data from simulation by modeling a depth camera’s noise pattern. Our results show that CDMs achieve nearly simulation-level accuracy in depth prediction, effectively bridging the sim-to-real gap for manipulation tasks. Notably, our experiments demonstrate, for the first time, that a policy trained on raw simulated depth, without the need for adding noise or real-world fine-tuning, generalizes seamlessly to real-world robots on two challenging long-horizon tasks involving articulated, reflective, and slender objects, with little to no performance degradation. We hope our findings will inspire future research in utilizing simulation data and 3D information in general robot policies.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现代机器人操控主要依赖于二维颜色空间中的视觉观察进行技能学习，但在泛化能力上表现较差。相比之下，人类生活在三维世界中，在与物体互动时更多依赖物理属性，如距离、大小和形状，而非纹理。由于可以通过广泛可用的深度摄像头获取这样的三维几何信息，因此似乎可以为机器人赋予类似的感知能力。我们的初步研究发现，使用深度摄像头进行操控具有挑战性，主要是由于其有限的精度和对各种噪声类型的敏感性。在这项工作中，我们提出了相机深度模型（CDMs），作为日常使用的深度摄像头的简单插件，它将RGB图像和原始深度信号作为输入，输出去噪后、准确的度量深度。为了实现这一点，我们开发了一个神经数据引擎，通过对深度摄像头的噪声模式建模，从仿真中生成高质量的成对数据。我们的结果表明，CDMs在深度预测方面实现了几乎与仿真水平相当的精度，有效弥合了操控任务中的仿真与现实之间的差距。值得注意的是，我们的实验首次表明，基于原始模拟深度训练的策略，无需添加噪声或进行现实世界微调，可以无缝地推广到涉及关节、反射和纤细物体的两个具有挑战性的长时间任务的真实机器人上，几乎没有性能降级。我们希望我们的发现能激发未来在机器人政策中利用仿真数据和三维信息的研究。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该工作聚焦的核心问题是：<strong>真实世界中深度相机输出的几何信息严重失真，导致机器人无法像在仿真环境中那样依赖准确的3D几何完成操作任务</strong>。具体而言，论文指出：</p>
<ul>
<li><p><strong>现有深度相机的固有问题</strong>：消费级深度相机（RealSense、ZED、Azure Kinect 等）存在两类典型噪声——</p>
<ol>
<li><strong>值噪声</strong>（value noise）：深度值系统性偏差、模糊、抖动；</li>
<li><strong>空洞噪声</strong>（hole noise）：因算法或材质导致的深度缺失。<br>这些噪声使得机器人难以获得可信的<strong>绝对尺度（metric）深度</strong>，从而限制了基于几何的策略在真实场景中的泛化。</li>
</ol>
</li>
<li><p><strong>现有解决方案的局限</strong>：</p>
<ul>
<li>传统做法在仿真中对深度图加噪声、再训练，但会牺牲几何精度；</li>
<li>或者依赖点云后处理（裁剪、降采样、补洞），流程繁琐且仍受原始噪声影响；</li>
<li>现有深度基础模型只能给出相对深度，无法直接提供操作所需的<strong>绝对尺度</strong>。</li>
</ul>
</li>
</ul>
<p>因此，论文提出 <strong>Camera Depth Models (CDMs)</strong>，目标是在<strong>不改动硬件</strong>的前提下，为每一款具体深度相机提供一个“即插即用”的神经网络插件，将RGB图与原始噪声深度作为输入，直接输出<strong>去噪、带绝对尺度的仿真级深度</strong>，从而：</p>
<ol>
<li><strong>从感知层面弥合 sim-to-real 的几何差距</strong>；</li>
<li><strong>让机器人策略可以像在仿真中一样，直接依赖干净的深度图进行零样本迁移</strong>；</li>
<li><strong>首次验证</strong>“仅用仿真干净深度训练的策略，无需噪声增强或真实微调即可在真实机器人上完成长时程、复杂物体（关节、反光、细长）操作”的可行性。</li>
</ol>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究被论文系统性地关联到三个核心方向，并指出了各自的局限与互补之处：</p>
<p>1. 度量深度估计（Metric Depth Prediction）</p>
<ul>
<li><p><strong>相对深度基础模型</strong></p>
<ul>
<li>Depth Anything V1/V2 [54, 55]：开放世界相对深度估计，但缺乏绝对尺度。</li>
<li>UniDepth [34]、Metric3D [18, 56]：尝试零样本度量估计，仍受单目尺度歧义限制。</li>
</ul>
</li>
<li><p><strong>引入显式尺度线索</strong></p>
<ul>
<li>相机内参注入：Guizilini et al. [12]、Piccinelli et al. [34]</li>
<li>深度提示微调：PromptDA [27]、PriorDA [47] 用低质量深度或稀疏 LiDAR 作为提示，但需手工规则生成提示图，难以泛化到动态场景与多传感器配置。</li>
<li>立体匹配：FoundationStereo [49] 依赖精确标定的双目 RGB，仅限 RGB 输入。</li>
</ul>
</li>
</ul>
<p>2. 基于 3D 表征的机器人操作</p>
<ul>
<li><p><strong>经典规划方法</strong></p>
<ul>
<li>AnyGrasp [8]：基于点云预测抓取姿态，需离线校准感知模块。</li>
</ul>
</li>
<li><p><strong>学习型策略</strong></p>
<ul>
<li>RGB-only：Diffusion Policy [4]、Octo [40]、π0 [1] 等，受纹理与光照变化影响大。</li>
<li>点云输入：GNFactor [57]、3D Diffusion Policy [59]、Perceiver-Actor [38] 通过降采样/裁剪抑制深度噪声，流程繁琐。</li>
<li>仿真专用：3D-VLA [61]、Point Cloud Matters [62] 仅在完美深度仿真环境中验证，未解决真实噪声。</li>
</ul>
</li>
</ul>
<p>3. 视觉 Sim-to-Real 迁移</p>
<ul>
<li><p><strong>RGB 模态</strong></p>
<ul>
<li>高保真渲染：Isaac Sim [31]、SAPIEN [51] 仍需大量课程与增广设计 [42]。</li>
<li>神经逆渲染：Re3Sim [13]、RoboGS [25] 用 Gaussian Splatting 重建真实场景，减少视觉差异，但计算昂贵。</li>
</ul>
</li>
<li><p><strong>几何模态（深度/点云）</strong></p>
<ul>
<li>仿真加噪声：He et al. [16]、Cheng et al. [3]、Liu et al. [28] 在深度图或点云上人工加噪声，牺牲几何精度。</li>
<li>点云后处理：Hua et al. [19] 真实侧需裁剪/降采样，流程复杂。</li>
<li>相机噪声仿真：Tao et al. [39] 用真实相机噪声渲染仿真深度，但仍是“不得已”的折中。</li>
</ul>
</li>
</ul>
<p>小结</p>
<p>方向</p>
<p>代表工作</p>
<p>主要局限</p>
<p>度量深度估计</p>
<p>PromptDA [27], UniDepth [34]</p>
<p>需手工提示或固定相机配置，无法处理真实深度特有噪声</p>
<p>3D 操作表征</p>
<p>3D-DP [59], GNFactor [57]</p>
<p>依赖点云后处理，未解决原始深度噪声</p>
<p>Sim-to-Real</p>
<p>Re3Sim [13], CyberDemo [42]</p>
<p>聚焦 RGB 外观差异，几何噪声仍靠人工增广</p>
<p>论文提出的 <strong>CDMs</strong> 通过“相机专用插件”思路，首次将<strong>真实深度噪声建模</strong>与<strong>仿真级度量深度恢复</strong>结合，直接弥补了上述三条线的缺口。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文提出 <strong>Camera Depth Models (CDMs)</strong> 作为“即插即用”的相机专用插件，将真实深度相机输出的<strong>噪声深度</strong>与<strong>RGB 图像</strong>直接映射为<strong>仿真级绝对尺度深度</strong>。整个解决路径可概括为 <strong>“真实噪声建模 → 仿真数据合成 → 相机专用模型训练 → 零样本 sim-to-real 部署”</strong> 四步闭环。</p>
<p>1. 真实噪声建模：ByteCameraDepth 数据集</p>
<ul>
<li><strong>多相机同步采集</strong>：设计多相机支架，一次拍摄同时获得<ul>
<li>7 款常用深度相机（RealSense 系列、ZED 2i、Azure Kinect）</li>
<li>10 种工作模式（含 ZED 的 4 种离线模式）</li>
<li>7 类真实场景（厨房、客厅、市场等）</li>
<li>总计 &gt;170 k RGB-深度对（5 Hz 采样）。</li>
</ul>
</li>
<li><strong>自动噪声模型学习</strong><ul>
<li><strong>空洞噪声模型</strong> $N_{\text{hole}}$：用 DINOv2 + DPT 预测“有效/空洞”二值掩膜。</li>
<li><strong>值噪声模型</strong> $N_{\text{value}}$：用 Depth Anything V2 预测相对深度，再经仿射变换恢复尺度。</li>
</ul>
</li>
</ul>
<p>2. 仿真数据合成：神经数据引擎</p>
<ul>
<li>在公开仿真数据集（HyperSim、DREDS、HISS、IRS）的<strong>干净深度</strong>上，按公式  <script type="math/tex; mode=display">\\tilde{D} = \\mu!\\left(V(I)\\right) \\cdot \\bigl(H(I)<0.5\\bigr)</script>注入学习到的空洞与值噪声，生成<strong>带真实噪声模式</strong>的成对训练数据。</li>
<li><strong>Guided Filter 尺度校准</strong>：随机核大小的引导滤波在值噪声与真值之间做局部线性映射，解决合成深度与真值的尺度漂移。</li>
<li><strong>高频噪声手工增广</strong>：弥补网络难以捕获的细粒度扰动。</li>
</ul>
<p>3. 相机专用模型训练：CDM 架构</p>
<ul>
<li><strong>双分支 ViT</strong><ul>
<li>RGB 分支：提取语义特征 $X_I$</li>
<li>深度分支：提取带尺度的特征 $X_D$</li>
<li><strong>Token 级交叉注意力融合</strong>：仅对同空间位置 token 做双向融合，保留全局-局部尺度信息。</li>
</ul>
</li>
<li><strong>训练损失</strong><ul>
<li>L1 深度误差 + 梯度误差（边缘保持）</li>
<li>以视差空间为目标，ViT 编码器初始化自 DINOv2，解码器从头训练。</li>
</ul>
</li>
</ul>
<p>4. 零样本 sim-to-real 部署</p>
<ul>
<li><strong>几何对齐而非外观对齐</strong>：<ul>
<li>仿真场景仅需几何相似物体与背景，无需纹理/材质一致。</li>
<li>用可微渲染快速标定相机外参，仿真阶段加入轻微位姿随机化以容忍残余误差。</li>
</ul>
</li>
<li><strong>策略训练与推理</strong><ul>
<li>仅用仿真<strong>干净单视角深度</strong>训练（ResNet + Diffusion Head），无需噪声增广或真实微调。</li>
<li>真实部署时，CDM 作为相机→策略之间的实时插件，输出 6 Hz+ 的干净深度。</li>
</ul>
</li>
</ul>
<p>效果验证</p>
<ul>
<li><strong>深度精度</strong>：在 Hammer 数据集零样本测试，CDM 相对 PromptDA/PriorDA 将 RMSE 降低 30–50 %，且无需空洞填补预处理。</li>
<li><strong>操作成功率</strong>：<ul>
<li>真实短任务（叠碗、挤牙膏）：CDM 将成功率从 0–40 % 提升至 60–73 %，并泛化到未见尺寸/颜色。</li>
<li>长时程 sim-to-real（厨房/食堂任务）：CDM 在 30 次测试中达到 73–87 % 成功率，与仿真表现持平甚至略高；而 PromptDA/PriorDA 或原始深度均接近 0 %。</li>
</ul>
</li>
</ul>
<p>通过上述闭环，论文首次实现了**“仿真深度训练 → 真实相机零样本迁移”**的完整链路，显著缩小了 sim-to-real 的几何差距。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>“CDM 能否提供高质量几何信息、能否直接提升机器人操作、能否实现零样本 sim-to-real”</strong> 三个核心问题，设计并执行了三组互补实验，覆盖 <strong>静态深度基准、真实短任务模仿学习、长时程 sim-to-real 零样本迁移</strong>。所有实验均使用 <strong>纯深度输入</strong>，排除颜色干扰。</p>
<p>1. 静态深度精度评估（零样本）</p>
<ul>
<li><strong>数据集</strong>：Hammer [20] —— 真实场景下由三种传感器（RealSense D435、L515、Lucid Helios）采集的 RGB-深度对，含严重形变与噪声，<strong>未参与训练</strong>。</li>
<li><strong>比较对象</strong>：<ul>
<li>PromptDA [27]、PriorDA [47]（需空洞填补）</li>
<li>PromptDA*（用本文合成数据再微调）</li>
<li>Raw Depth（无模型直接使用）</li>
</ul>
</li>
<li><strong>指标</strong>：L1、RMSE、AbsRel、δ0.5、δ1.0</li>
<li><strong>结果摘要</strong>（表 1）：<ul>
<li><strong>无需预处理</strong>：CDM 在 Holed 设置下仍显著优于填补后的 PromptDA/PriorDA。</li>
<li><strong>跨相机泛化</strong>：CDM-L515 在 D435 数据上甚至略优于 CDM-D435，显示噪声模式共享性。</li>
<li><strong>绝对精度</strong>：CDM-D435 在 D435 分片将 RMSE 从 0.297 → 0.040，相对提升 <strong>>80 %</strong>。</li>
</ul>
</li>
</ul>
<p>2. 真实短任务模仿学习（深度-only）</p>
<ul>
<li><strong>平台</strong>：桌面 UR5 + Robotiq 夹爪，RealSense D435 单视角。</li>
<li><strong>任务</strong>（图 7）：<ol>
<li>Toothpaste-and-Cup：抓取牙膏并放入杯中。</li>
<li>Stack-Bowls：抓取并叠放两个碗（训练用常规尺寸，测试含 4 个未见尺寸）。</li>
</ol>
</li>
<li><strong>数据</strong>：每任务 50 条人工遥操作轨迹。</li>
<li><strong>评估</strong>：5 个位置 × 3 次重复。</li>
<li><strong>结果</strong>（表 2 &amp; 图 5）：<ul>
<li>无 CDM 时成功率 0–40 %；接入 CDM 后提升至 <strong>60–73 %</strong>。</li>
<li><strong>尺寸泛化</strong>：CDM 让策略成功叠放未见大小的碗，无 CDM 则完全失败。</li>
</ul>
</li>
</ul>
<p>3. 长时程零样本 sim-to-real</p>
<ul>
<li><strong>机器人</strong>：UR5 + Robotiq 夹爪。</li>
<li><strong>任务</strong>（图 6）：<ol>
<li>Kitchen：取碗 → 放入微波炉 → 关闭玻璃门（玻璃在原始深度中为空洞）。</li>
<li>Canteen：识别并抓取细长叉子（原始深度中几乎不可见）→ 放入盒中 → 抓盘 → 倒垃圾 → 放盘。</li>
</ol>
</li>
<li><strong>流程</strong>：<ul>
<li>仿真内 680/800 条演示 → 纯深度策略训练 → <strong>无噪声、无真实微调</strong> → 真实部署时插入对应 CDM。</li>
</ul>
</li>
<li><strong>测试</strong>：10 个随机位置 × 3 次 = 30 次/任务，分别在 RealSense D435 与 L515 上测试。</li>
<li><strong>结果</strong>（表 3）：<ul>
<li><strong>CDM-D435</strong>：Kitchen 87 %、Canteen 73 % 总成功率，<strong>与仿真持平或更高</strong>。</li>
<li><strong>PromptDA / PriorDA / Raw</strong>：多数子任务成功率为 0–20 %。</li>
<li><strong>跨相机</strong>：CDM-L515 在 L515 上 Kitchen 60 %、Canteen 73 %，仍显著优于基线。</li>
</ul>
</li>
</ul>
<p>4. 附加实验与诊断</p>
<ul>
<li><strong>延迟测试</strong>（表 4）：RTX 4090 上单帧总延迟 0.151 s（含预处理+推理+后处理），&gt;6 Hz 实时可行。</li>
<li><strong>深度-距离曲线</strong>（图 11）：CDM 在 0.5–2.5 m 范围内绝对/相对误差均显著低于原始深度。</li>
<li><strong>可视化</strong>：<ul>
<li>图 12–15：原始深度 vs CDM 深度及对应点云，直观展示几何修复效果。</li>
<li>图 16：关键帧 rollout 对比，仿真与真实轨迹高度一致。</li>
</ul>
</li>
<li><strong>失败案例</strong>（图 18）：当大面积金属反光导致原始深度整体空洞且 RGB 语义不足时，CDM 仍可能输出错误深度。</li>
</ul>
<p>实验结论</p>
<p>三组实验递进式验证了：</p>
<ol>
<li>CDM 在<strong>静态基准</strong>上实现零样本、填补-无关的 SOTA 度量深度；</li>
<li>高质量深度直接带来<strong>真实短任务</strong>的显著性能提升与尺寸泛化；</li>
<li><strong>零样本 sim-to-real</strong> 首次在复杂长时程任务中达到与仿真一致的成功率，证明几何差距已被有效弥合。</li>
</ol>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向既可直接基于 CDM 框架延伸，也可与更广泛的机器人学习、3D 视觉研究交叉，分为 <strong>短期可行</strong> 与 <strong>中长期探索</strong> 两类：</p>
<p>短期可行（6–12 个月）</p>
<ol>
<li><p><strong>多模态策略融合</strong></p>
<ul>
<li>将 CDM 输出的干净深度与 RGB、语言指令联合输入，训练 <strong>RGB-D-Language</strong> 通用策略，验证几何精度在多模态场景中的互补作用。</li>
<li>参考 π0 [1]、OpenVLA [21] 的架构，仅需把视觉编码器替换为“RGB + CDM 深度”双流编码。</li>
</ul>
</li>
<li><p><strong>实时优化与边缘部署</strong></p>
<ul>
<li><strong>量化 / 蒸馏</strong>：将 CDM 蒸馏为 INT8 或 4-bit 轻量模型，在 Jetson Orin、Intel NUC 等边缘 GPU 上维持 ≥30 Hz 推理。</li>
<li><strong>TensorRT / ONNX 流水线</strong>：固化空洞-值噪声模型与 CDM 的串联图，减少 CPU-GPU 往返。</li>
</ul>
</li>
<li><p><strong>在线自适应校准</strong></p>
<ul>
<li>在机器人运行过程中，用 <strong>在线 SLAM 或 Bundle Adjustment</strong> 估计相机-机器人外参漂移，实时更新 CDM 输入的相机内参，提升长期稳定性。</li>
</ul>
</li>
<li><p><strong>更多相机型号与场景</strong></p>
<ul>
<li>扩展 CDM 到 <strong>iPhone LiDAR、Kinect Azure、OAK-D-S2</strong> 等新设备；</li>
<li>采集 <strong>户外光照变化、透明液体、强反光金属</strong> 场景，验证噪声模型泛化。</li>
</ul>
</li>
</ol>
<p>中长期探索（1–3 年）</p>
<ol>
<li><p><strong>自监督噪声模型更新</strong></p>
<ul>
<li>利用 <strong>机器人交互反馈</strong>（抓取成功/失败、力觉一致性）作为弱监督信号，在线微调空洞/值噪声模型，适应传感器老化或环境变化。</li>
</ul>
</li>
<li><p><strong>4D 时序深度滤波</strong></p>
<ul>
<li>将 CDM 扩展为 <strong>时空网络</strong>，输入连续多帧 RGB-D，联合估计深度 + 场景流，抑制单帧不可恢复的遮挡/反射空洞。</li>
</ul>
</li>
<li><p><strong>生成式数据增强</strong></p>
<ul>
<li>用扩散模型（如 RoboGS [25]）在仿真中 <strong>按 CDM 噪声分布采样</strong>，生成无限量带噪声的深度-真值对，进一步放大训练集规模。</li>
</ul>
</li>
<li><p><strong>跨机器人迁移</strong></p>
<ul>
<li>研究 <strong>“一次训练，多机零样本”</strong>：将 CDM 作为统一接口，使不同臂展、基座的机器人共享同一深度策略，验证几何表征的通用性。</li>
</ul>
</li>
<li><p><strong>物理-几何联合 sim-to-real</strong></p>
<ul>
<li>在仿真中 <strong>随机化摩擦、质量、关节阻尼</strong> 的同时，保持 CDM 提供的几何一致性，研究几何与物理 gap 的耦合效应。</li>
</ul>
</li>
<li><p><strong>大规模 3D 机器人基础模型</strong></p>
<ul>
<li>用 CDM 清洗 <strong>百万级真实 RGB-D-动作</strong> 数据，构建 <strong>几何先验的机器人基础模型</strong>，类似 RT-2 但聚焦 3D 结构，实现“几何-语义-动作”统一表征。</li>
</ul>
</li>
</ol>
<p>一句话总结</p>
<p>CDM 把“真实深度”提升到“仿真深度”的精度，未来可沿着 <strong>实时化、多模态、自适应、大规模</strong> 四条主线持续扩展，最终迈向“几何一致、任务通用”的下一代机器人系统。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文一句话总结</p>
<p>提出 <strong>Camera Depth Models（CDMs）</strong> —— 针对每款深度相机的即插即用神经网络插件，把真实 RGB+噪声深度直接映射为<strong>仿真级绝对尺度深度</strong>，首次实现“<strong>仅用仿真干净深度训练的策略零样本迁移到真实机器人</strong>”，在长时程、复杂物体操作任务中达到与仿真持平的成功率。</p>
<p>核心内容速览</p>
<p>维度</p>
<p>要点</p>
<p><strong>问题</strong></p>
<p>消费级深度相机值噪声+空洞噪声严重，机器人难以获得可信 3D 几何，导致 sim-to-real 失败。</p>
<p><strong>方法</strong></p>
<p>1) 采集 7 相机 10 模式 17 万帧真实数据 → 学习空洞/值噪声模型；<br>2) 在仿真干净深度上注入噪声，生成大规模训练对；<br>3) 设计双分支 ViT 融合 RGB 语义与深度尺度，输出无空洞、带绝对尺度的深度；<br>4) 真实部署时 CDM 作为相机→策略的实时插件。</p>
<p><strong>实验</strong></p>
<p>• Hammer 数据集零样本：RMSE ↓80 %，填补-无关 SOTA；<br>• 真实短任务：叠碗/挤牙膏成功率 0→73 %，并泛化未见尺寸；<br>• 长时程 sim-to-real：厨房/食堂任务成功率 87 %/73 %，与仿真持平，基线≈0。</p>
<p><strong>贡献</strong></p>
<p>① ByteCameraDepth 多相机深度数据集；<br>② CDM 系列模型与训练管线开源；<br>③ 首次验证“仿真干净深度 → 真实零样本”可行，几何精度是 sim-to-real 关键瓶颈。</p>
<p>一句话展望</p>
<p>CDM 把真实深度“仿真化”，为后续多模态、大规模、通用机器人策略奠定了<strong>几何一致</strong>的数据与感知基础。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Manipulation%20as%20in%20Simulation%3A%20Enabling%20Accurate%20Geometry%20Perception%20in%20Robots，链接是  https://arxiv.org/pdf/2509.02530  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02530  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Minghuan Liu,Zhengbang Zhu,Xiaoshen Han,Peng Hu,Haotong Lin,Xinyao Li,Jingxiao Chen,Jiafeng Xu,Yichu Yang,Yunfeng Lin,Xinghang Li,Yong Yu,Weinan Zhang,Tao Kong,Bingyi Kang</p>
<p><strong>Categories</strong>: cs.RO,cs.AI,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.02530.pdf">https://arxiv.org/pdf/2509.02530.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.02530">https://arxiv.org/abs/2509.02530</a></p>
<p><strong>Arxiv ID</strong>: 2509.02530</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.02530">https://papers.cool/arxiv/2509.02530</a></p>
<p><strong>Published</strong>: 2025-09-02T17:29:38Z</p>
<p><strong>Updated</strong>: 2025-09-02T17:29:38.000Z</p>
<hr>
<h3 id="8-SATQuest-A-Verifier-for-Logical-Reasoning-Evaluation-and-Reinforcement-Fine-Tuning-of-LLMs"><a href="#8-SATQuest-A-Verifier-for-Logical-Reasoning-Evaluation-and-Reinforcement-Fine-Tuning-of-LLMs" class="headerlink" title="8. SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.00930">SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs</a></h3><p>Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis and training, or have narrow problem types and formats. To address these limitations, we introduce SATQuest, a systematic verifier designed to evaluate and enhance logical reasoning in LLMs by generating diverse, Satisfiability-based logical reasoning problems directly from Conjunctive Normal Form (CNF) instances. SATQuest structures these problems along three orthogonal dimensions: instance scale, problem type, and question format, employing randomized, SAT-based problem generation and objective answer verification via PySAT. This design mitigates memorization issues, allows for nuanced insights into reasoning performance, and enables effective reinforcement fine-tuning. Our extensive evaluation of various LLMs using SATQuest identified significant limitations in their logical reasoning, particularly in generalizing beyond familiar mathematical formats. Furthermore, we show that reinforcement fine-tuning with SATQuest rewards substantially improves targeted task performance and generalizes to more complex instances, while highlighting remaining challenges in cross-format adaptation. Through these demonstrations, we showcase SATQuest’s potential as a foundational tool and a valuable starting point for advancing LLM logical reasoning.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近在大规模语言模型（LLMs）方面的进展展示了显著的推理能力。然而，由于缺乏可控和可扩展的工具进行细粒度分析，系统性地评估和增强这些推理能力具有挑战性。现有的基准和数据集往往缺乏多维系统分析和训练所需的变量控制，或者问题类型和格式较为狭窄。为了应对这些限制，我们引入了SATQuest，这是一种系统性验证器，旨在通过直接从合取范式（CNF）实例生成多样的可满足性基础逻辑推理问题，来评估和增强LLMs中的逻辑推理。SATQuest在三个正交维度上构建这些问题：实例规模、问题类型和问题格式，采用随机的SAT基础问题生成和通过PySAT进行的客观答案验证。这种设计减轻了记忆问题，使对推理表现的细致洞察成为可能，并使有效的强化微调成为可能。我们使用SATQuest对各种LLMs进行的大规模评估发现它们的逻辑推理存在重大局限性，特别是在超越熟悉的数学格式进行泛化方面。此外，我们还展示了使用SATQuest进行的强化微调奖励显著提升了目标任务的表现，并能够泛化到更复杂的实例，同时突出了在跨格式适应中仍然存在的挑战。通过这些展示，我们展示了SATQuest作为基础工具和推动LLM逻辑推理发展的宝贵起点的潜力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决<strong>如何系统性地评估并增强大语言模型（LLM）的逻辑推理能力</strong>这一核心问题，具体聚焦在以下三方面痛点：</p>
<ol>
<li><p><strong>现有评估工具缺乏可控、可扩展的细粒度分析能力</strong><br>传统基准（MATH、GPQA、MMLU 等）要么题型单一、格式固定，难以做多维度变量控制；要么依赖人工标注难度，无法系统追踪模型在不同推理维度上的进步与局限。</p>
</li>
<li><p><strong>训练数据泄漏与记忆风险</strong><br>公开题库易被预训练语料覆盖，导致评估结果虚高，难以区分真实推理与记忆。</p>
</li>
<li><p><strong>强化微调（RFT）缺乏可验证、可扩展的奖励信号</strong><br>现有逻辑推理强化学习工作（如 Logic-RL）受限于特定题型或格式，难以提供跨维度（规模、题型、格式）的通用奖励。</p>
</li>
</ol>
<p>为此，作者提出 <strong>SATQuest</strong>：</p>
<ul>
<li>以随机生成的 CNF 实例为源头，通过 SAT 求解器自动生成可验证答案的多样化逻辑题；</li>
<li>将题目系统组织在<strong>实例规模、问题类型、问题格式</strong>三个正交维度，实现可控、可扩展的评估与训练；</li>
<li>利用 PySAT 提供客观奖励，支持基于 RLVR（Reinforcement Learning from Verifiable Rewards）的强化微调，从而针对性提升 LLM 的逻辑推理能力并研究其泛化瓶颈。</li>
</ul>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>与 SATQuest 直接相关的研究可划分为三类：逻辑推理基准、可控难度/格式评估框架，以及基于强化学习的推理增强方法。以下列出代表性工作并说明其与 SATQuest 的关联与差异。</p>
<p>1. 逻辑推理基准</p>
<p>文献</p>
<p>核心特点</p>
<p>与 SATQuest 的关系</p>
<p><strong>ZebraLogic</strong> (Lin et al., 2025)</p>
<p>基于 Zebra Puzzle 模板，提供可控难度维度（变量数、约束数）。</p>
<p>仅覆盖“实例规模”这一维度，题型与格式单一；SATQuest 引入五类 SAT 任务与四种格式。</p>
<p><strong>Knights &amp; Knaves</strong> (Xie et al., 2024)</p>
<p>经典逻辑谜题模板化，支持难度旋钮。</p>
<p>同样局限于单一题型与格式；SATQuest 通过 CNF 随机化避免记忆。</p>
<p><strong>MATH</strong> (Hendrycks et al., 2021a)</p>
<p>数学竞赛题，覆盖多领域，人工标注难度。</p>
<p>缺乏结构化关系与可控变量；SATQuest 使用求解器统计量客观量化难度。</p>
<p><strong>LiveBench</strong> (White et al.,</p>
</div></details>
<p><strong>Authors</strong>: Yanxiao Zhao,Yaqian Li,Zihao Bo,Rinyoichi Takezoe,Haojia Hui,Mo Guang,Lei Ren,Xiaolin Qin,Kaiwen Long</p>
<p><strong>Categories</strong>: cs.AI,cs.LG,cs.LO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.00930.pdf">https://arxiv.org/pdf/2509.00930.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.00930">https://arxiv.org/abs/2509.00930</a></p>
<p><strong>Arxiv ID</strong>: 2509.00930</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.00930">https://papers.cool/arxiv/2509.00930</a></p>
<p><strong>Published</strong>: 2025-08-31T16:56:06Z</p>
<p><strong>Updated</strong>: 2025-08-31T16:56:06.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202509/2025-09-05/">https://wdndev.github.io/daily/hf/202509/2025-09-05/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Robix-A-Unified-Model-for-Robot-Interaction-Reasoning-and-Planning"><span class="toc-text">1. Robix: A Unified Model for Robot Interaction, Reasoning and Planning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Open-Data-Synthesis-For-Deep-Research"><span class="toc-text">2. Open Data Synthesis For Deep Research</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-LMEnt-A-Suite-for-Analyzing-Knowledge-in-Language-Models-from-Pretraining-Data-to-Representations"><span class="toc-text">3. LMEnt: A Suite for Analyzing Knowledge in Language Models from Pretraining Data to Representations</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-MOSAIC-Multi-Subject-Personalized-Generation-via-Correspondence-Aware-Alignment-and-Disentanglement"><span class="toc-text">4. MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware Alignment and Disentanglement</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Mixture-of-Global-and-Local-Experts-with-Diffusion-Transformer-for-Controllable-Face-Generation"><span class="toc-text">5. Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Planning-with-Reasoning-using-Vision-Language-World-Model"><span class="toc-text">6. Planning with Reasoning using Vision Language World Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Manipulation-as-in-Simulation-Enabling-Accurate-Geometry-Perception-in-Robots"><span class="toc-text">7. Manipulation as in Simulation: Enabling Accurate Geometry Perception in Robots</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-SATQuest-A-Verifier-for-Logical-Reasoning-Evaluation-and-Reinforcement-Fine-Tuning-of-LLMs"><span class="toc-text">8. SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>