<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-09-06 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Drivel-ology: Challenging LLMs with Interpreting Nonsense with DepthWe introduce Drivelology, a unique linguistic phenomenon characterised as “nonsense with d">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-09-06">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202509/2025-09-06/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Drivel-ology: Challenging LLMs with Interpreting Nonsense with DepthWe introduce Drivelology, a unique linguistic phenomenon characterised as “nonsense with d">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:09.885Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202509/2025-09-06/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-09-06',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-09-06</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:09.885Z" title="Updated 2025-11-02 07:46:09">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">58.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>206min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-09-06"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth"><a href="#1-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth" class="headerlink" title="1. Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.03867">Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth</a></h3><p>We introduce Drivelology, a unique linguistic phenomenon characterised as “nonsense with depth”, utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs’ pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了“驱动学”（Drivelology），这是一种独特的语言现象，被描述为“有深度的胡言乱语”，其表述在语法上是连贯的，但在语用上却是矛盾的，情感上充满负荷，或在修辞上具有颠覆性。虽然这些表达可能看起来像表面上的胡言乱语，但它们编码了隐含的意义，需要上下文推断、道德推理或情感解释。我们发现，目前的大型语言模型（LLMs）尽管在许多自然语言处理（NLP）任务中表现出色，但始终无法掌握驱动学文本的多层语义。为了研究这个问题，我们构建了一个小而多样化的基准数据集，包含1200多个经过精心挑选的例子，其中一些用英语、普通话、西班牙语、法语、日语和韩语编写。注释尤其具有挑战性：每个例子都需要仔细的专家审查，以验证其确实反映了驱动学的特征。这个过程涉及多轮讨论和裁决，以解决分歧，突显了驱动学的微妙和主观特性。我们在分类、生成和推理任务上对一系列LLMs进行了评估。我们的结果揭示了LLMs的明显局限性：模型常常将驱动学与肤浅的胡言乱语混淆，产生不连贯的理由，或完全错过隐含的修辞功能。这些发现突显了LLMs在语用理解方面的更深层次的表现差距，并挑战了统计流利性暗示认知理解的假设。我们发布了我们的数据集和代码，以促进进一步研究建模语言深度，而不仅仅是表面上的连贯性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在回答一个核心问题：<br><strong>当前大语言模型（LLM）是否具备解读“有深度的胡话”（Drivelology）的能力？</strong></p>
<p>具体而言，作者提出并形式化了一种新的语言现象——<strong>Drivelology</strong>，其特征为：</p>
<ul>
<li>句法通顺、表面看似无意义或荒谬；</li>
<li>实则嵌套多层语义，需要<strong>语境推理、文化背景、情感解读</strong>才能还原其隐含意图；</li>
<li>与经典“胡话”或简单讽刺不同，它<strong>刻意制造悖论、误导、双关、反转</strong>等修辞复合体，以传递深层观点。</li>
</ul>
<p>论文通过构建多语言基准 <strong>DRIVELHUB</strong>，设计四类任务（检测、多标签分类、隐式叙事生成、叙事选择），系统评估了主流 LLM 对 Drivelology 的理解深度。实验结果显示：</p>
<ul>
<li>所有模型在<strong>表面流畅性</strong>上表现良好，但在<strong>深层语义与修辞意图</strong>上显著落后；</li>
<li>尤其在需要<strong>文化知识、非线性推理、情感洞察</strong>的 Hard 叙事选择任务中，准确率普遍跌至 20% 以下；</li>
<li>模型常将 Drivelology 误判为“纯胡话”，或给出<strong>逻辑自洽却偏离作者意图</strong>的解释。</li>
</ul>
<p>综上，论文揭示并量化了 LLM 在<strong>语用与修辞理解维度</strong>上的结构性缺陷，挑战了“统计流畅即认知理解”的假设，并呼吁未来研究超越表层模式匹配，迈向真正的<strong>社会-文化语义建模</strong>。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>论文在第 2 节“Related Work”中将与 Drivelology 相关的研究划分为三大线索，并指出它们与本文任务的本质差异。可归纳为以下脉络：</p>
<ol>
<li><p>大模型评测基准（LLM Evaluations）</p>
<ul>
<li>经典综合基准：GLUE、SuperGLUE、MMLU、BIG-Bench、ARC、SWE-bench 等</li>
<li>目标：覆盖翻译、摘要、常识推理、代码生成等“基础能力”</li>
<li>缺口：极少触及<strong>非线性、文化嵌入、情感-修辞</strong>并存的深层语用现象</li>
</ul>
</li>
<li><p>幽默/讽刺/反语计算（Humour, Irony, Sarcasm）</p>
<ul>
<li>代表研究：<br>– SemEval 2020 Task 11（反语检测）<br>– BIG-Bench Snarks（讽刺句识别）<br>– Jentzsch &amp; Kersting 2023（ChatGPT 幽默生成评估）<br>– Loakman et al. 2023（人类评估综述）</li>
<li>共同假设：通常只需<strong>一次认知反转</strong>（字面义→相反义）即可解决</li>
<li>与 Drivelology 的区别：<br>– 后者<strong>多层修辞复合</strong>（悖论+双关+文化梗+反转同时出现）<br>– 需要<strong>链式推理</strong>与<strong>跨文化知识</strong>才能还原隐含叙事</li>
</ul>
</li>
<li><p>欺骗性/无意义语言研究（Deceptive &amp; Nonsensical Language）</p>
<ul>
<li>Frankfurt 2005“bullshit”：说话者对真值漠然，以说服为目的</li>
<li>Cappelen &amp; Dever 2019“deep bullshit”：连语义也不在乎，彻底无意义（例：Colorless green ideas…）</li>
<li>与 Drivelology 的边界：<br>– <strong>Deep bullshit</strong> 无意图、无深层义<br>– <strong>Drivelology</strong> 表面“胡话”是<strong>精心设计的修辞框架</strong>，目的是<strong>引导受众发现隐藏意义</strong></li>
</ul>
</li>
</ol>
<p>此外，近期与“互联网新词-梗语”相关的研究（Mei et al. 2024 Slang、Zheng et al. 2025 Fanchuan 戏仿基准）也被视为平行线索，但同样未涉及<strong>多语言、多修辞叠加、隐式叙事</strong>同时出现的复杂场景。</p>
<p>综上，本文首次将“** nonsense with depth**”形式化为可评测任务，并指出既有幽默/讽刺/胡说研究因<strong>简化假设</strong>或<strong>单层反转</strong>而无法覆盖 Drivelology 的<strong>复合语用深度</strong>。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文并未直接“解决”LLM 对 Drivelology 理解不足的问题，而是<strong>构建了一套可量化的研究框架</strong>，把原本主观、零散的“有深度的胡话”现象<strong>形式化、数据化、任务化</strong>，从而为后续算法改进提供基准与诊断工具。核心步骤如下：</p>
<ol>
<li><p>现象形式化</p>
<ul>
<li>提出五维修辞标签：Misdirection、Paradox、Switchbait、Inversion、Wordplay</li>
<li>给出“nonsense with depth”操作定义：句法连贯 + 多层隐义 + 需文化/情感推理</li>
</ul>
</li>
<li><p>数据集构建（DRIVELHUB）</p>
<ul>
<li>1200 例多语言样本（中、英、西、法、日、韩），600 Drivelology + 600 对照（正常句/纯胡话）</li>
<li>四步人工标注协议：<br>① 硕博级多语标注员初标 → ② 群体裁决 → ③ GPT-4.5 辅助生成干扰叙事 → ④ 语言学+心理学元审校</li>
<li>每例提供：原文、隐含叙事、1-N 修辞标签、4 个高质量错误选项</li>
</ul>
</li>
<li><p>任务设计（诊断矩阵）</p>
<p>任务</p>
<p>类型</p>
<p>探查能力</p>
<p>T1 Drivelology Detection</p>
<p>二分类</p>
<p>区分“有深度胡话” vs“纯胡话/正常句”</p>
<p>T2 Drivelology Tagging</p>
<p>多标签分类</p>
<p>识别同时出现的多重修辞手法</p>
<p>T3 Implicit Narrative Writing</p>
<p>生成</p>
<p>模型自写隐含叙事，用 BERTScore + GPT-4-as-Judge 评质量</p>
<p>T4 Narrative Selection</p>
<p>MCQA（Easy/Hard）</p>
<p>五选一挑出正确隐含叙事；Hard 加入“以上皆非”防排除法</p>
</li>
<li><p>大规模零样本评测</p>
<ul>
<li>覆盖 7 个主流 LLM（GPT-4o-mini、Claude-3.5-Haiku、DeepSeek-V3、Llama-3 系列、Qwen 系列）</li>
<li>三 prompt 模板、三语言方向（英/中/跨语）平均结果，减少方差</li>
</ul>
</li>
<li><p>诊断分析</p>
<ul>
<li>发现 DeepSeek-V3 在五大指标里五项第一，但 Hard MCQA 仅 15.5%，说明<strong>规模放大仍无法解锁深层语用</strong></li>
<li>语言提示效应：英文 prompt 利于逻辑推理任务，中文 prompt 利于直接语义理解</li>
<li>模型尺度实验（Qwen3 4B→14B）显示 Hard 任务准确率呈<strong>跳跃式提升</strong>，暗示<strong>复杂推理随参数涌现</strong></li>
</ul>
</li>
<li><p>公开资源</p>
<ul>
<li>数据集、代码、评测脚本全部开源（MIT 研究许可），方便社区继续训练或设计新损失函数</li>
</ul>
</li>
</ol>
<p>通过上述“形式化定义 → 高质量数据 → 多任务诊断 → 公开基准”的闭环，论文把原本模糊的“LLM 不懂幽默/讽刺/文化梗”问题<strong>具体化为可追踪、可迭代的技术指标</strong>，为后续工作（如用 GRPO 偏好优化、多模态文化知识注入、生成评估指标设计）提供了明确的起点与衡量标准。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕“LLM 能否理解 Drivelology”这一核心问题，设计并执行了<strong>三类共 7 组实验</strong>，覆盖<strong>分类、生成、选择</strong>三大能力维度，所有实验均在**零样本（zero-shot）**条件下完成，以减少 prompt 工程带来的偏差。具体实验矩阵如下：</p>
<p>实验编号</p>
<p>任务名称</p>
<p>设置细节</p>
<p>评测指标</p>
<p>模型规模/语言变量</p>
<p>E1</p>
<p>Drivelology Detection</p>
<p>1200 句二分类（Drivelology vs non-Drivelology）</p>
<p>Accuracy</p>
<p>7 模型 × 3 prompt × 2 提示语言（英/中）</p>
<p>E2</p>
<p>Drivelology Tagging</p>
<p>600 句多标签分类（5 修辞标签并行）</p>
<p>Weighted F1</p>
<p>同上</p>
<p>E3</p>
<p>Implicit Narrative Writing</p>
<p>600 句生成隐含叙事</p>
<p>BERTScore-recall + GPT-4-as-Judge（1–5 Likert）</p>
<p>同上</p>
<p>E4</p>
<p>Narrative Selection (Easy)</p>
<p>600 题五选一 MCQA，含 4 个 GPT-4.5 生成干扰项</p>
<p>Accuracy</p>
<p>同上</p>
<p>E5</p>
<p>Narrative Selection (Hard)</p>
<p>在 E4 基础上增加“None of the above”选项</p>
<p>Accuracy</p>
<p>同上</p>
<p>E6</p>
<p>语言难度细粒度分析</p>
<p>将 E4/E5 结果按 6 种源语言拆解</p>
<p>Accuracy</p>
<p>3 最强模型跨语向对比</p>
<p>E7</p>
<p>模型尺度缩放</p>
<p>仅换用 Qwen3 系列 4B→8B→14B，重复 E1–E5</p>
<p>同上</p>
<p>3 参数规模 × 2 提示语言</p>
<p>补充说明</p>
<ol>
<li>零样本：所有模型不经过任何微调或示范，仅用任务定义 + 待测文本。</li>
<li>提示语言对比：同一任务分别用英文 prompt 与中文 prompt 各跑 1 次，观察“内部推理语言”效应。</li>
<li>三模板平均：每种任务准备 3 套语义等价但表述不同的 prompt，最终指标取三次均值，降低方差。</li>
<li>人工校验：生成任务（E3）额外用 GPT-4.1 担任“评委”，并与人类打分校准，确保主观质量分可信。</li>
</ol>
<p>通过上述 7 组实验，论文系统揭示了模型在<strong>表层流畅 vs 深层修辞</strong>、<strong>跨语言泛化</strong>、<strong>参数规模效应</strong>上的差距，为后续算法改进提供了细粒度诊断依据。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可直接基于 DRIVELHUB 的标注结构与实验结论继续深入，分为 <strong>“模型改进”</strong>、<strong>“评测拓展”</strong> 与 <strong>“理论机制”</strong> 三大主题，共 10 个可探索点：</p>
<p>1 模型改进方向</p>
<p>编号</p>
<p>可探索点</p>
<p>关键思路</p>
<p>可验证假设</p>
<p>M1</p>
<p>GRPO 偏好优化</p>
<p>利用 Hard MCQA 的 5 选 1 排序信号，进行 Group-wise 偏好学习</p>
<p>相对排序 &gt; 单点损失，可提升隐式叙事精度</p>
<p>M2</p>
<p>文化知识注入</p>
<p>将神话、梗、历史事件编码为外部知识图谱，联合检索</p>
<p>文化缺失是 Switchbait/Paradox 失败主因</p>
<p>M3</p>
<p>多轮自我对话</p>
<p>让模型先生成“字面摘要”，再自问“有何文化/情感反差”</p>
<p>链式反思可降低“误当纯胡话”率</p>
<p>M4</p>
<p>跨语言对齐微调</p>
<p>用机器翻译将 Drivelology 平行化，做跨语掩码重构</p>
<p>提升韩语/日语低资源表现</p>
<p>2 评测与数据拓展</p>
<p>编号</p>
<p>可探索点</p>
<p>关键思路</p>
<p>可验证假设</p>
<p>E1</p>
<p>生成质量指标</p>
<p>设计 Entertainability、Paradoxical Depth、Topic Relevance 三维人工评分 → 训练回归器自动打分</p>
<p>现有 BERTScore 与幽默/深度相关性低</p>
<p>E2</p>
<p>人机混合 Gold</p>
<p>引入“多人标注分布”而非单标签，衡量模型与人类一致性</p>
<p>分布匹配比 Top-1 准确率更能反映审美一致</p>
<p>E3</p>
<p>长文本 Drivelology</p>
<p>将段落级或对话级纳入（如脱口秀脚本），检测跨句反转</p>
<p>当前仅限单句，无法覆盖叙事层累进误导</p>
<p>E4</p>
<p>对抗性挖掘</p>
<p>用 LLM 自动生产“似 Drivelology 但缺乏深度”的难负例，迭代加入训练</p>
<p>可建立持续增长的动态 benchmark</p>
<p>3 理论与认知机制</p>
<p>编号</p>
<p>可探索点</p>
<p>关键思路</p>
<p>可验证假设</p>
<p>C1</p>
<p>参数规模阈值</p>
<p>在 14B→200B 区间密集采样，观察 Hard MCQA 的突现点</p>
<p>复杂语用是否遵循“相变”规律</p>
<p>C2</p>
<p>脑电/眼动对比</p>
<p>让人类阅读 Drivelology vs 普通笑话，记录 N400 与 P600 成分</p>
<p>模型失败处是否对应人类晚期认知加工成本</p>
<p>C3</p>
<p>可解释性探针</p>
<p>插入“文化知识神经元”或“悖论检测神经元”线性探针，查看激活分布</p>
<p>模型内部是否缺乏显式的悖论表示</p>
<p>快速启动建议</p>
<ol>
<li>立即可用：M1 + E1——在现有 DRIVELHUB 上跑 GRPO 微调，并用新三维指标评估生成。</li>
<li>低成本：M3 + E2——无需额外数据，仅修改 prompt 与标注策略即可验证“分布一致性”是否优于 Top-1 准确率。</li>
<li>长期高影响：C2 + C3——结合认知科学与机制解释，为“统计流畅≠理解”提供神经层面的证据。</li>
</ol>
<p>以上任意一条均可形成独立后续论文，并直接对比本文零样本基线，量化改进幅度。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文主旨</p>
<p>提出并形式化“Drivelology”（有深度的胡话）这一语言现象，构建多语言基准 DRIVELHUB，系统评估主流 LLM 在<strong>句法流畅却修辞复杂</strong>文本上的理解极限，揭示统计模式匹配与真正语用推理之间的断层。</p>
<p>1 核心概念</p>
<ul>
<li><strong>Drivelology</strong>：表面荒谬、实则多层隐义，需文化-情感-悖论推理。</li>
<li><strong>五维修辞标签</strong>：Misdirection、Paradox、Switchbait、Inversion、Wordplay（可多标签共存）。</li>
</ul>
<p>2 数据集 DRIVELHUB</p>
<ul>
<li>1200 例，6 语言（中≈50%，其余英/西/法/日/韩）。</li>
<li>600 Drivelology + 600 对照（正常句/纯胡话）。</li>
<li>每例含：原文、人工撰写隐含叙事、1-N 修辞标签、4 个高质量干扰项。</li>
<li>四步标注：硕博多语标注 → 群体裁决 → GPT-4.5 辅助生成干扰 → 元审校。</li>
</ul>
<p>3 四项零样本任务</p>
<p>任务</p>
<p>类型</p>
<p>指标</p>
<p>目的</p>
<p>T1 检测</p>
<p>二分类</p>
<p>Accuracy</p>
<p>区分“有深度胡话”与“纯胡话/正常句”</p>
<p>T2 标注</p>
<p>多标签</p>
<p>Weighted F1</p>
<p>识别并存的多重修辞手法</p>
<p>T3 叙事生成</p>
<p>开放式生成</p>
<p>BERTScore + GPT-4-as-Judge</p>
<p>模型自写隐含叙事，考察深度理解</p>
<p>T4 叙事选择</p>
<p>MCQA（Easy/Hard）</p>
<p>Accuracy</p>
<p>五选一挑出正确隐含叙事；Hard 含“以上皆非”</p>
<p>4 主要实验结果</p>
<ul>
<li><strong>整体排序</strong>：DeepSeek-V3 &gt; Claude-3.5-Haiku &gt; GPT-4o-mini ≫ 开源小模型</li>
<li><strong>Hard MCQA 暴跌</strong>：最强模型仅 15.5%，Qwen3-8B 反常 26.8%（仍远低于随机基线 20%→无优势）</li>
<li><strong>语言提示效应</strong>：英文 prompt 利推理，中文 prompt 利直接理解</li>
<li><strong>参数尺度</strong>：Qwen3 4B→14B 在 Hard 任务准确率 ×7 跃升，显示复杂推理随规模涌现</li>
</ul>
<p>5 结论与启示</p>
<ul>
<li>现有 LLM 普遍把 Drivelology 当成“纯胡话”或给出逻辑自洽却偏离作者意图的解释。</li>
<li>统计流畅 ≠ 认知理解；亟需超越表层模式匹配，迈向文化-情感-修辞联合建模。</li>
<li>数据、代码、提示全部开源，供后续微调、偏好优化、认知机制研究使用。</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Drivel-ology%3A%20Challenging%20LLMs%20with%20Interpreting%20Nonsense%20with%20Depth，链接是  https://arxiv.org/pdf/2509.03867  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.03867  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yang Wang,Chenghao Xiao,Chia-Yi Hsiao,Zi Yan Chang,Chi-Li Chen,Tyler Loakman,Chenghua Lin</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03867.pdf">https://arxiv.org/pdf/2509.03867.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03867">https://arxiv.org/abs/2509.03867</a></p>
<p><strong>Arxiv ID</strong>: 2509.03867</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03867">https://papers.cool/arxiv/2509.03867</a></p>
<p><strong>Published</strong>: 2025-09-04T03:58:55Z</p>
<p><strong>Updated</strong>: 2025-09-04T03:58:55.000Z</p>
<hr>
<h3 id="2-From-Editor-to-Dense-Geometry-Estimator"><a href="#2-From-Editor-to-Dense-Geometry-Estimator" class="headerlink" title="2. From Editor to Dense Geometry Estimator"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04338">From Editor to Dense Geometry Estimator</a></h3><p>Leveraging visual priors from pre-trained text-to-image (T2I) generative models has shown success in dense prediction. However, dense prediction is inherently an image-to-image task, suggesting that image editing models, rather than T2I generative models, may be a more suitable foundation for fine-tuning. Motivated by this, we conduct a systematic analysis of the fine-tuning behaviors of both editors and generators for dense geometry estimation. Our findings show that editing models possess inherent structural priors, which enable them to converge more stably by <code>refining&quot; their innate features, and ultimately achieve higher performance than their generative counterparts. Based on these findings, we introduce \textbf&#123;FE2E&#125;, a framework that pioneeringly adapts an advanced editing model based on Diffusion Transformer (DiT) architecture for dense geometry prediction. Specifically, to tailor the editor for this deterministic task, we reformulate the editor&#39;s original flow matching loss into the</code>consistent velocity” training objective. And we use logarithmic quantization to resolve the precision conflict between the editor’s native BFloat16 format and the high precision demand of our tasks. Additionally, we leverage the DiT’s global attention for a cost-free joint estimation of depth and normals in a single forward pass, enabling their supervisory signals to mutually enhance each other. Without scaling up the training data, FE2E achieves impressive performance improvements in zero-shot monocular depth and normal estimation across multiple datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset and outperforms the DepthAnything series, which is trained on 100$\times$ data. The project page can be accessed \href{<a target="_blank" rel="noopener" href="https://amap-ml.github.io/FE2E/}{here}">https://amap-ml.github.io/FE2E/}{here}</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>利用预训练的文本到图像 （T2I） 生成模型的视觉先验在密集预测方面取得了成功。然而，密集预测本质上是一项图像到图像的任务，这表明图像编辑模型而不是 T2I 生成模型可能是更适合微调的基础。在此激励下，我们对编辑器和生成器的微调行为进行了系统分析，以进行密集几何估计。我们的研究结果表明，编辑模型具有固有的结构先验，这使得它们能够通过“细化”其先天特征来更稳定地收敛，并最终实现比生成模型更高的性能。基于这些发现，我们引入了 \textbf{FE2E}，这是一个开创性地采用基于扩散变压器 （DiT） 架构的高级编辑模型进行密集几何预测的框架。具体来说，为了针对这一确定性任务定制编辑器，我们将编辑器的原始流匹配损失重新表述为“一致速度”训练目标。并且我们使用对数量化来解决编辑器原生 BFloat16 格式与我们任务的高精度需求之间的精度冲突。此外，我们利用 DiT 的全球关注度，在一次前向通过中对深度和法线进行免费联合估计，使它们的监督信号能够相互增强。在不扩大训练数据的情况下，FE2E 在多个数据集的零样本单目深度和法线估计方面实现了令人印象深刻的性能改进。值得注意的是，它在 ETH35D 数据集上实现了超过 3%\% 的性能提升，并且优于在 100$\times$ 数据上训练的 DepthAnything 系列。项目页面可以访问 \href{<a target="_blank" rel="noopener" href="https://amap-ml.github.io/FE2E/}{here}。">https://amap-ml.github.io/FE2E/}{here}。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决<strong>单目图像稠密几何估计</strong>（depth &amp; normal）在<strong>零样本场景下精度不足、数据依赖过重</strong>的问题，核心论点与目标可归纳为：</p>
<p>关键观察</p>
<p>对应问题</p>
<p>论文对策</p>
<p>现有方法多基于 T2I 生成模型（如 Stable Diffusion）进行微调，但 T2I 天生缺乏对<strong>输入图像结构</strong>的理解。</p>
<p>生成式先验与 I→I 稠密预测任务错位，导致微调效率低、收敛震荡、精度瓶颈。</p>
<p>提出“<strong>从编辑器到估计器</strong>”新范式：直接以<strong>图像编辑模型</strong>（Step1X-Edit）为底座，利用其已内化的<strong>结构先验</strong>。</p>
<p>编辑模型原目标为“创意编辑”，流程随机、轨迹弯曲、BF16 精度，与<strong>确定性几何回归</strong>冲突。</p>
<p>直接微调编辑模型会出现：① 推理累积误差；② 数值精度不足；③ 无法一次性利用 depth-normal 互补性。</p>
<p>设计三项适配技术：<br>1. <strong>Consistent Velocity Flow Matching</strong>——固定起点+恒速直线，消除随机性与轨迹误差；<br>2. <strong>Logarithmic Quantization</strong>——在 BF16 下实现近/远距均衡的毫米级深度监督；<br>3. <strong>Cost-Free Joint Estimation</strong>——借助 DiT 全局注意力，单前向同时输出 depth &amp; normal，零额外计算实现任务互补。</p>
<p>最终，FE2E 仅用 71 k 张合成图（≈0.2 % 的 DepthAnything 数据量）即取得零样本 SoTA，ETH3D 上 AbsRel 相对次优方法再降 35 %，验证“<strong>编辑先验 + 有限数据</strong>”即可超越“<strong>生成先验 + 海量数据</strong>”。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下工作按“<strong>范式类别→具体代表→与 FE2E 的关系</strong>”梳理，覆盖<strong>图像生成/编辑底座</strong>与<strong>稠密几何估计</strong>两条主线。</p>
<p>1. 生成式底座（T2I）用于稠密预测</p>
<p>代表方法</p>
<p>核心机制</p>
<p>与 FE2E 的差异/联系</p>
<p><strong>Marigold</strong> (Ke et al., CVPR’24)</p>
<p>基于 Stable Diffusion v2，多步去噪回归深度</p>
<p>相同“有限数据 + 扩散先验”思想，但 FE2E 改用<strong>编辑模型</strong>并引入一致速度、对数量化、联合估计，平均 Rank 提升 5.2→1.4。</p>
<p><strong>GeoWizard</strong> (Fu et al., 2024)</p>
<p>SD 1.5 + 几何切换器，depth/normal 分阶段训练</p>
<p>首次显式联合几何，却需额外交叉注意与 2× 训练成本；FE2E 利用 DiT 全局注意实现<strong>零额外成本</strong>联合。</p>
<p><strong>Lotus / Lotus-D</strong> (He et al., 2024)</p>
<p>单步去噪 + 轻量解码器，SD 先验</p>
<p>同样追求“单步”推理，但仍基于<strong>生成模型</strong>；FE2E 在 ETH3D 上 AbsRel 再降 3.8→3.8（绝对值相同，但 Lotus-D 需 59 K 数据，FE2E 71 K 且同时输出 normal）。</p>
<p><strong>Diffusion-E2E-FT</strong> (Martin Garcia et al., WACV’25)</p>
<p>对 SD 进行端到端单步微调</p>
<p>与 FE2E 的“一致速度”理念相似，但保留随机起点；FE2E 进一步固定 z0=0 并改用<strong>编辑权重</strong>。</p>
<p><strong>StableNormal</strong> (Ye et al., 2024)</p>
<p>针对法向的方差缩减策略</p>
<p>专注于 normal 质量，FE2E 通过联合任务即达到更低 MeanErr（31.2° vs 36.7° on Sintel）。</p>
<p>2. 图像编辑底座（I2I）</p>
<p>代表方法</p>
<p>核心机制</p>
<p>与 FE2E 的关系</p>
<p><strong>Step1X-Edit</strong> (Liu et al., 2025)</p>
<p>FLUX-DiT + 指令编辑，BF16 权重</p>
<p>FE2E 的<strong>起点网络</strong>：首次证明编辑权重内含几何结构先验，经“精炼”即可胜任确定性回归。</p>
<p><strong>InstructPix2Pix</strong> (Brooks et al., 2022)</p>
<p>SD + 语言指令配对数据</p>
<p>早期验证“编辑≈I2I 通用框架”，但未涉足几何；FE2E 把该思想<strong>收敛到稠密估计</strong>。</p>
<p><strong>Qwen-Image</strong> (Wu et al., 2025)</p>
<p>多模态 LLM + 统一视觉解码器</p>
<p>同期统一视觉模型，也包含深度分支；FE2E 在 5 个深度基准平均 Rank 1.6，优于其 2.6（表 6），且数据量远少。</p>
<p>3. 数据驱动稠密估计（非扩散范式）</p>
<p>代表方法</p>
<p>数据规模</p>
<p>与 FE2E 对比</p>
<p><strong>DepthAnything v1/v2</strong> (Yang et al., 2024a/b)</p>
<p>62.6 M 无标注 + 1.5 M 标注</p>
<p>FE2E 用 <strong>0.11 %</strong> 数据即实现平均 Rank 反超（1.4 vs 3.5）。</p>
<p><strong>Metric3D v2</strong> (Hu et al., 2024)</p>
<p>自行采集 4.5 M 室外视频帧</p>
<p>依赖大规模度量数据；FE2E 走“<strong>先验驱动</strong>”路线，零样本即可媲美。</p>
<p><strong>MiDaS / DPT</strong> (Ranftl et al., 2020-21)</p>
<p>多数据集混合≈ 2 M</p>
<p>传统 ViT 编码器，无生成先验；FE2E 在 ETH3D AbsRel 降低 18.4→3.8（−79 %）。</p>
<p>4. 量化与数值精度研究</p>
<p>工作</p>
<p>方案</p>
<p>FE2E 改进</p>
<p><strong>统一 min-max 线性量化</strong> (Marigold 默认)</p>
<p>80 m→[-1,1]</p>
<p>0.1 m 处 AbsRel 高达 1.6；FE2E 提出<strong>对数深度量化</strong>将 AbsRel 降至 0.013 且近/远均衡。</p>
<p><strong>Inverse/Disparity 量化</strong> (早期立体视觉)</p>
<p>1/d 线性映射</p>
<p>远距灾难（39 m≈78 m）；对数方案在 80 m 误差仅 1.04 m，相对误差恒定。</p>
<p>5. 联合深度-法向估计</p>
<p>工作</p>
<p>联合方式</p>
<p>FE2E 优势</p>
<p><strong>GeoWizard</strong></p>
<p>额外 Cross-Attention + Switcher</p>
<p>训练 2× 成本；FE2E 利用 DiT 全局注意与<strong>被丢弃的左半 latent</strong>，单前向零额外参数实现联合。</p>
<p>小结</p>
<ul>
<li><strong>生成式先验</strong> → <strong>编辑式先验</strong> 是 FE2E 与既有工作的<strong>最大范式差异</strong>。</li>
<li><strong>一致速度 + 固定起点 + 对数量化 + 零成本联合</strong> 四项技术构成<strong>编辑器→估计器</strong>的完整适配协议，使 FE2E 在<strong>数据量、精度、推理成本</strong>三角中取得新均衡。</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文将“单目稠密几何估计”重新定义为<strong>图像编辑任务</strong>，提出 <strong>FE2E（From Editor to Estimator）</strong> 框架，通过三步核心策略把现成的 <strong>DiT 图像编辑器</strong>（Step1X-Edit）改造成高精度、零样本、联合 depth &amp; normal 估计器。整体流程与关键技术如下：</p>
<p>1 范式迁移：编辑器取代生成器</p>
<p>问题根源</p>
<p>解决手段</p>
<p>具体实现</p>
<p>T2I 生成模型缺乏对<strong>输入图像结构</strong>的理解，微调需“从零学几何”，收敛震荡、易撞精度瓶颈。</p>
<p>直接继承<strong>图像→图像编辑模型</strong>的权重，利用其已内化的结构先验。</p>
<p>选用 <strong>Step1X-Edit</strong>（FLUX-DiT 变体）作为起点，实验显示同等数据下 AbsRel 立即下降 8 %（ETH3D）。</p>
<p>2 训练目标重塑：一致速度 + 确定性起点</p>
<p>原始编辑流程<br>  <code>zy_t = t·zy_1 + (1-t)·zy_0, zy_0∼N(0,I)</code><br>  <code>vθ = fθ(zx, zy_t, t)</code> → 随机速度场，推理需数值积分，累积误差大。</p>
<p>FE2E 改进</p>
<ol>
<li><strong>Consistent Velocity</strong>：强制速度恒为直线<br>  <code>v ≈ zy_1 − 0</code> （与 t、zy_t 无关）</li>
<li><strong>Fixed Origin</strong>：冻结起点<br>  <code>zy_0 ≡ 0</code> （消除随机性）</li>
</ol>
<p>新目标<br>  <code>L = ‖zy_1 − fθ(zx)‖²</code></p>
<p>效果</p>
<ul>
<li>推理简化为 <strong>单步</strong> <code>zy_1 = fθ(zx)</code>，无需 ODE 求解器；</li>
<li>ETH3D 上 AbsRel 再降 10 %，训练曲线无震荡。</li>
</ul>
<p>3 数值精度：对数深度量化</p>
<p>BF16 最坏间隔 <code>1/256</code> 在 [-1,1] 区间≈0.004；线性映射 80 m→[-1,1] 后<br>  <code>ΔD = 40×1/256 ≈16 cm</code> → 0.1 m 处 AbsRel=1.6（不可接受）。</p>
<p>FE2E 方案</p>
<ol>
<li>对数变换 <code>Dlog = ln(DGT+ε)</code></li>
<li>百分位归一化<br>  <code>yD = clip(2·(Dlog−Dlog,2)/(Dlog,98−Dlog,2)−1)</code><br>保证 <strong>相对误差恒定 ≈1.3 %</strong>，远距 80 m 误差 1.04 m，近距 0.1 m 误差 1.3 mm，可直接用原生 BF16 训练/推理。</li>
</ol>
<p>4 零成本联合估计：复用被丢弃的 latent</p>
<p>Step1X-Edit 原输入<br>  <code>zx+Θ = concat(zx, zΘ)</code> → DiT 输出 <code>[pl; pr]</code><br>仅对 <code>pr</code> 施加重建损失，<code>pl</code> 被丢弃。</p>
<p>FE2E 做法</p>
<ul>
<li>令 <code>pl</code> 回归 depth，<code>pr</code> 回归 normal；</li>
<li>全局注意力自然交换信息，无需额外参数/计算；</li>
<li>总损失<br>  <code>Lfm = ‖vD−pl‖² + ‖vN−pr‖²</code></li>
</ul>
<p>收益</p>
<ul>
<li>两任务监督信号互相正则，ETH3D AbsRel 再降 13 %；</li>
<li>单前向同时输出 depth &amp; normal，推理时间不变。</li>
</ul>
<p>5 数据与训练细节</p>
<ul>
<li>仅 71 k 合成图（Hypersim 51 k + Virtual KITTI 20 k），BF16 精度，LoRA-r=64，单卡 RTX4090 可训。</li>
<li>辅助 <strong>Dispersion Loss</strong> 增大批次内特征散布，AbsRel 额外 −3 %。</li>
</ul>
<p>6 结果总结</p>
<p>指标</p>
<p>相对次优 SoTA 提升</p>
<p>ETH3D AbsRel</p>
<p>5.9 → 3.8（−35 %）</p>
<p>KITTI AbsRel</p>
<p>7.4 → 6.6（−10 %）</p>
<p>平均 Rank（5 深度+4 法向基准）</p>
<p>1.4（DepthAnything 3.5，Lotus-D 3.7）</p>
<p>一句话总结</p>
<p>FE2E 通过“<strong>编辑器权重继承 → 一致速度确定性回归 → 对数深度量化 → 单前向联合估计</strong>”四连击，把<strong>图像编辑模型</strong>无缝改造成<strong>数据高效、零样本 SoTA 的稠密几何估计器</strong>。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>实验围绕“<strong>编辑模型是否优于生成模型</strong>”与“<strong>FE2E 各组件是否有效</strong>”两条主线展开，涵盖<strong>零样本深度/法向估计、消融、可视化、效率与同期方法对比</strong>五大类。主要结果均基于<strong>5 个深度基准 + 4 个法向基准</strong>，统一采用官方协议与评价脚本，保证可复现。</p>
<p>1 零样本深度估计（Affine-Invariant）</p>
<p>数据集</p>
<p>场景</p>
<p>指标</p>
<p>NYUv2 / ScanNet</p>
<p>室内</p>
<p>AbsRel ↓, δ1 ↑</p>
<p>KITTI</p>
<p>室外</p>
<p>AbsRel ↓, δ1 ↑</p>
<p>ETH3D / DIODE</p>
<p>跨域/混合</p>
<p>AbsRel ↓, δ1 ↑</p>
<p>结果</p>
<ul>
<li>FE2E 仅用 <strong>71 k</strong> 训练图，平均 <strong>Rank 1.4</strong>，超越 DepthAnything-V2（<strong>62.6 M</strong> 图）的 3.5。</li>
<li>ETH3D AbsRel 从次优 <strong>5.9</strong> 降至 <strong>3.8（−35 %）</strong>；KITTI 从 <strong>7.4</strong> 降至 <strong>6.6（−10 %）</strong>。</li>
</ul>
<p>2 零样本法向估计</p>
<p>数据集</p>
<p>指标</p>
<p>NYUv2 / ScanNet / iBims-1</p>
<p>MeanErr ↓, 11.25°↑</p>
<p>Sintel（合成室外）</p>
<p>MeanErr ↓, 11.25°↑</p>
<p>结果</p>
<ul>
<li>四数据集平均 <strong>Rank 1.6</strong>，领先 StableNormal、Lotus-D、DSINE 等。</li>
<li>Sintel MeanErr <strong>31.2°</strong>（次优 32.3°）；iBims-1 <strong>15.1°</strong>（次优 16.1°）。</li>
</ul>
<p>3 消融实验（表 4 体系）</p>
<p>在 <strong>KITTI &amp; ETH3D</strong> 上逐组件剥离，共 8 组对照：</p>
<p>ID</p>
<p>变量</p>
<p>关键结论</p>
<p>1 vs 2</p>
<p>生成底座↔编辑底座</p>
<p>同“直接迁移”协议下，编辑模型 AbsRel 立即 <strong>−0.2~0.4</strong></p>
<p>2 vs 3</p>
<p>+Consistent Velocity</p>
<p>再 <strong>−0.7</strong>（KITTI），消除轨迹积分误差</p>
<p>3 vs 4</p>
<p>+Fixed Origin</p>
<p>再 <strong>−0.2</strong>，训练更稳定</p>
<p>4 vs 6</p>
<p>量化策略</p>
<p>对数 &gt; 逆距 &gt; 线性；对数带来 <strong>−19 %/−13 %</strong></p>
<p>6 vs 8</p>
<p>+Joint Estimation</p>
<p>最终 <strong>−0.2</strong>，视觉细节提升（图 8）</p>
<p>4 编辑-生成特征演化可视化（图 3）</p>
<ul>
<li>选取 <strong>Block-1/20/35</strong> 特征图，彩虹色编码。</li>
<li>编辑模型 <strong>Epoch-1</strong> 即呈现清晰几何边缘；生成模型初期特征无序，需 30 Epoch 才出现结构。</li>
<li>量化曲线（图 4）显示编辑模型损失平滑下降至 0.073，生成模型卡在 0.08 平台。</li>
</ul>
<p>5 与同期统一模型对比（表 6）</p>
<p>方法</p>
<p>训练数据</p>
<p>平均 Rank</p>
<p>Qwen-Image</p>
<p>十亿级</p>
<p>2.6</p>
<p>DINOv3</p>
<p>1.7 B</p>
<p>1.8</p>
<p>FE2E</p>
<p>0.071 M</p>
<p><strong>1.6</strong></p>
<p>6 效率与资源</p>
<p>指标</p>
<p>FE2E</p>
<p>说明</p>
<p>推理步数</p>
<p><strong>1</strong></p>
<p>一致速度无需 ODE 采样</p>
<p>单张 1024×1024 RTX-4090 延迟</p>
<p><strong>1.78 s</strong></p>
<p>含 VAE 编解码</p>
<p>MACs</p>
<p>28.9 T</p>
<p>高于 Lotus-D（2.65 T），低于 Qwen-Image（2.13 P）</p>
<p>7 额外定性</p>
<ul>
<li>图 7/9：极端光照、薄结构、远处细节，FE2E 深度边缘更锐利。</li>
<li>图 8/10：联合训练后，法向在平面褶皱、建筑远立面一致性更好。</li>
</ul>
<p>8 可重复性细节</p>
<ul>
<li>全部基准使用<strong>官方验证集与评价脚本</strong>；</li>
<li>深度评估前统一执行<strong>中值对齐</strong>（median scaling）；</li>
<li>法向评估 mask 与 DSINE/Lotus 保持一致；</li>
<li>代码与模型将开源，随机种子、LoRA 配置、超参均在附录给出。</li>
</ul>
<p>一句话总结</p>
<p>实验从<strong>定量 SoTA、消融拆解、特征可视化、效率测试、同期对比</strong>五维度一致表明：<strong>编辑器先验 + 一致速度 + 对数量化 + 零成本联合 = 数据极少也能霸榜</strong>。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向按“<strong>可行→远期</strong>”排序，均直接对应论文已暴露的局限或尚未验证的场景，可供后续工作<strong>即插即用</strong>或<strong>范式升级</strong>。</p>
<p>1 模型侧：换更大/更强的编辑底座</p>
<ul>
<li><strong>FLUX-Edit-Pro、Qwen-Image-Edit、OmniControl</strong> 等已发布或即将开源的 DiT 编辑器，验证“编辑→估计”是否<strong>随底座能力线性提升</strong>。</li>
<li><strong>多尺度 DiT</strong>（如 PixArt-Σ、Stable Diffusion 3）（1K→2K→4K 原生分辨率），测试<strong>高分辨率零样本深度</strong>是否继续改善，尤其街景远点、室内纹理稀疏区域。</li>
</ul>
<p>2 任务侧：从 depth/normal 到“全能几何”</p>
<ul>
<li><strong>一次性联合输出</strong><ul>
<li>深度 + 法向 + 语义分割 + 边缘 + 遮挡边界（paper 已留空 pl/pr 可继续拼接）。</li>
<li>利用 DiT 全局注意，仅增加 head 数，<strong>训练成本仍 O(1)</strong>。</li>
</ul>
</li>
<li><strong>度量深度（metric depth）</strong><ul>
<li>当前 FE2E 仅做仿射不变深度；可在 pl-head 后加<strong>全局尺度 token</strong> 或<strong>焦距条件 token</strong>，用相机内参弱监督，验证“编辑权重”能否继承<strong>绝对尺度先验</strong>。</li>
</ul>
</li>
<li><strong>立体/视频时序</strong><ul>
<li>输入改为 concat(zt, zt-1, zt-2)，输出 metric depth + 光流，看是否<strong>零样本打败 RAFT/FlowFormer</strong>。</li>
</ul>
</li>
</ul>
<p>3 数据侧：把“极少数据”推向“零数据”</p>
<ul>
<li><strong>自监督微调</strong><ul>
<li>去掉 Hypersim/Virtual KITTI 真值，用<strong>光度重投影损失</strong>在单目视频上微调；检验编辑先验是否<strong>摆脱合成 GT</strong>。</li>
</ul>
</li>
<li><strong>可提示几何（Promptable Depth）</strong><ul>
<li>仿照 InstructPix2Pix，用文本提示“请给出这张街景的精确深度”→ 输出，验证<strong>语言-几何对齐</strong>能否提升域外鲁棒性。</li>
</ul>
</li>
<li><strong>大规模无标注微调</strong><ul>
<li>采用 <strong>DINOv3 1.7 B 数据管道</strong>，仅保留自监督重投影损失，观察<strong>数据量-性能曲线</strong>是否仍优于生成式底座。</li>
</ul>
</li>
</ul>
<p>4 精度侧：BF16→FP8、量化误差再压缩</p>
<ul>
<li><strong>对数量化超参可学习</strong><ul>
<li>把 log(D+ε) 中的 ε、百分位 p2/p98 设为可学习向量，端到端优化，<strong>让网络自己挑精度分配</strong>。</li>
</ul>
</li>
<li><strong>非均匀对数量化</strong><ul>
<li>引入 <strong>μ-law 或 α-law</strong>，在近距保持亚毫米，远距保持亚米，再与 BF16/FP8 最坏间隔做闭式误差界推导。</li>
</ul>
</li>
<li><strong>混合精度训练</strong><ul>
<li>DiT 主干维持 BF16，head 部分用 FP32；或采用 <strong>gradient scaling + stochastic rounding</strong>，在消费级 GPU 上实现<strong>毫米级监督但显存减半</strong>。</li>
</ul>
</li>
</ul>
<p>5 推理侧：单步→亚秒→移动端</p>
<ul>
<li><strong>模型压缩</strong><ul>
<li>对 DiT 做 <strong>8-bit 量化 + 稀疏化 + TensorRT/ONNX 加速</strong>，目标 <strong>RTX-4090 1024×384 &lt; 50 ms</strong>。</li>
</ul>
</li>
<li><strong>蒸馏成小 U-Net</strong><ul>
<li>以 FE2E 为教师，训练 <strong>&lt; 50 M 参数的小网络</strong>做单步深度，验证“编辑先验”能否通过蒸馏<strong>迁移到非 DiT 架构</strong>，落地手机 AR。</li>
</ul>
</li>
<li><strong>一步多尺度 ensemble</strong><ul>
<li>利用编辑模型原生高分辨率能力，<strong>一次前向生成多尺度深度</strong>，再可学习融合，<strong>无需传统多阶段 coarse-to-fine</strong>。</li>
</ul>
</li>
</ul>
<p>6 评价侧：新基准与鲁棒性</p>
<ul>
<li><strong>恶劣天气基准</strong><ul>
<li>在 <strong>WeatherDepth、RainSyn、SnowSyn</strong> 上测试，查看编辑先验是否<strong>自带天气鲁棒性</strong>（编辑器训练集含大量风格化样本）。</li>
</ul>
</li>
<li><strong>镜面/透明/弱纹理</strong><ul>
<li>新建 <strong>GlassDepth、MirrorNormal</strong> 基准，验证全局注意机制是否<strong>自动聚合上下文线索</strong>处理经典失效区域。</li>
</ul>
</li>
<li>** adversarial / patch attack**<ul>
<li>对比生成与编辑底座在<strong>几何任务下的对抗鲁棒性</strong>，解释“结构先验”是否等价于<strong>隐式正则</strong>。</li>
</ul>
</li>
</ul>
<p>7 理论侧：为什么编辑权重自带几何？</p>
<ul>
<li><strong>特征谱分析</strong><ul>
<li>计算 Block-20 特征协方差矩阵谱衰减，看编辑 vs 生成模型的<strong>低维结构子空间</strong>维度差异。</li>
</ul>
</li>
<li><strong>信息流可视化</strong><ul>
<li>采用 <strong>Attention Rollout</strong> 追踪几何 head 的注意力是否<strong>自动聚焦地面-墙面交界、物体边缘</strong>。</li>
</ul>
</li>
<li><strong>优化景观</strong><ul>
<li>对两种底座做 <strong>loss surface 可视化</strong>（PCA+随机方向），验证编辑模型是否<strong>更少局部极小/更平滑</strong>。</li>
</ul>
</li>
</ul>
<p>8 伦理与扩展</p>
<ul>
<li><strong>生成-估计耦合风险</strong><ul>
<li>研究“先估计深度再编辑物体”是否会被恶意用于<strong>虚假内容合成</strong>，提出<strong>几何水印</strong>或<strong>深度一致性验证</strong>工具。</li>
</ul>
</li>
<li><strong>多模态编辑→多模态几何</strong><ul>
<li>支持 <strong>语音/草图/3D 框</strong> 多种条件，验证“编辑先验”是否<strong>天然支持多模态几何控制</strong>，迈向<strong>可提示 3D</strong>。</li>
</ul>
</li>
</ul>
<p>一句话展望</p>
<p>FE2E 打开了“<strong>图像编辑权重就是几何先验</strong>”的新赛道——沿着<strong>更强编辑器、更少标注、更低比特、更多任务、更硬场景</strong>五个维度，每一步都可能继续刷新“数据-精度-效率”的帕累托前沿。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<ul>
<li><strong>核心论点</strong>：图像编辑模型（I2I）比文本到图像生成模型（T2I）自带更丰富的<strong>结构先验</strong>，是单目稠密几何估计的更优底座。</li>
<li><strong>方法框架 FE2E</strong>：将 DiT 编辑器 Step1X-Edit 改造成深度/法向估计器，提出<ol>
<li>一致速度 + 固定起点的<strong>确定性流匹配</strong>（单步推理无累积误差）；</li>
<li><strong>对数深度量化</strong>解决 BF16 精度不足；</li>
<li><strong>零成本联合头</strong>一次性输出 depth &amp; normal。</li>
</ol>
</li>
<li><strong>实验结果</strong>：仅用 71 k 合成图零样本训练，在 5 个深度 + 4 个法向基准平均排名 <strong>1.4</strong>，ETH3D 上 AbsRel 再降 <strong>35 %</strong>，超越 DepthAnything（62.6 M 图）等 SoTA。</li>
<li><strong>结论</strong>：继承编辑权重而非堆数据，即可实现数据高效、高精度的单目稠密几何预测。</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是From%20Editor%20to%20Dense%20Geometry%20Estimator，链接是  https://arxiv.org/pdf/2509.04338  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04338  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: JiYuan Wang,Chunyu Lin,Lei Sun,Rongying Liu,Lang Nie,Mingxing Li,Kang Liao,Xiangxiang Chu,Yao Zhao</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04338.pdf">https://arxiv.org/pdf/2509.04338.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04338">https://arxiv.org/abs/2509.04338</a></p>
<p><strong>Arxiv ID</strong>: 2509.04338</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04338">https://papers.cool/arxiv/2509.04338</a></p>
<p><strong>Published</strong>: 2025-09-04T15:58:50Z</p>
<p><strong>Updated</strong>: 2025-09-04T15:58:50.000Z</p>
<hr>
<h3 id="3-Towards-a-Unified-View-of-Large-Language-Model-Post-Training"><a href="#3-Towards-a-Unified-View-of-Large-Language-Model-Post-Training" class="headerlink" title="3. Towards a Unified View of Large Language Model Post-Training"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04419">Towards a Unified View of Large Language Model Post-Training</a></h3><p>Two major sources of training data exist for post-training modern language models: online (model-generated rollouts) data, and offline (human or other-model demonstrations) data. These two types of data are typically used by approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT), respectively. In this paper, we show that these approaches are not in contradiction, but are instances of a single optimization process. We derive a Unified Policy Gradient Estimator, and present the calculations of a wide spectrum of post-training approaches as the gradient of a common objective under different data distribution assumptions and various bias-variance tradeoffs. The gradient estimator is constructed with four interchangeable parts: stabilization mask, reference policy denominator, advantage estimate, and likelihood gradient. Motivated by our theoretical findings, we propose Hybrid Post-Training (HPT), an algorithm that dynamically selects different training signals. HPT is designed to yield both effective exploitation of demonstration and stable exploration without sacrificing learned reasoning patterns. We provide extensive experiments and ablation studies to verify the effectiveness of our unified theoretical framework and HPT. Across six mathematical reasoning benchmarks and two out-of-distribution suites, HPT consistently surpasses strong baselines across models of varying scales and families.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现代语言模型后训练的两个主要数据来源存在：在线（模型生成的展开）数据和离线（人类或其他模型演示）数据。这两种类型的数据通常分别被强化学习（RL）和监督微调（SFT）等方法使用。在本文中，我们展示了这些方法并不矛盾，而是单一优化过程的实例。我们推导出一个统一的策略梯度估计器，并将各种后训练方法的计算表示为在不同数据分布假设和各种偏差-方差权衡下的共同目标的梯度。梯度估计器由四个可互换部分构成：稳定性掩码、参考策略分母、优势估计和似然梯度。基于我们的理论发现，我们提出了混合后训练（HPT），这是一种动态选择不同训练信号的算法。HPT旨在在不牺牲学习到的推理模式的情况下，实现对演示的有效利用和稳定探索。我们提供了广泛的实验和消融研究，以验证我们的统一理论框架和HPT的有效性。在六个数学推理基准和两个分布外套件中，HPT始终在不同规模和类型的模型中超过强基线。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该论文旨在解决现代大语言模型（LLM）后训练阶段中<strong>监督微调（SFT）与强化学习（RL）两种范式如何高效统一</strong>的核心问题。具体而言：</p>
<ul>
<li><strong>理论层面</strong>：揭示SFT与RL并非对立，而是同一优化目标在不同数据分布假设下的实例，提出<strong>统一策略梯度估计器（Unified Policy Gradient Estimator, UPGE）</strong>，将两者的梯度计算归纳为共享的数学形式。</li>
<li><strong>实践层面</strong>：针对现有“SFT→RL”两阶段流程资源消耗大、调参复杂的问题，提出<strong>混合后训练算法（Hybrid Post-Training, HPT）</strong>，通过动态性能反馈自适应切换SFT与RL信号，实现：<ul>
<li>高效利用离线演示数据（避免RL初期探索失败）。</li>
<li>保留RL的探索能力（避免SFT过拟合导致的泛化下降）。</li>
<li>在单一训练阶段内完成优化，减少计算开销。</li>
</ul>
</li>
</ul>
<p>最终目标是在数学推理等复杂任务上，使不同规模、不同架构的模型均能超越现有SFT、RL或其简单组合的基线。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究与本论文提出的统一后训练框架（UPGE）及混合后训练算法（HPT）密切相关，按主题归类并给出关键贡献：</p>
<p><strong>1. 经典后训练范式：SFT 与 RL</strong></p>
<ul>
<li><p><strong>SFT（Supervised Fine-Tuning）</strong></p>
<ul>
<li><strong>Wei et al. (2021)</strong>：提出指令微调（instruction tuning）范式，通过高质量演示数据对齐模型行为。</li>
<li><strong>Chung et al. (2022)</strong>；<strong>Touvron et al. (2023a,b)</strong>：验证SFT在提升模型领域能力上的高效性，但指出其可能抑制探索能力。</li>
</ul>
</li>
<li><p><strong>RL（Reinforcement Learning）</strong></p>
<ul>
<li><strong>Ouyang et al. (2022)</strong>：RLHF（Reinforcement Learning from Human Feedback）框架，用PPO优化人类偏好奖励模型。</li>
<li><strong>Schulman et al. (2017)</strong>：PPO算法，通过裁剪策略梯度实现稳定更新。</li>
<li><strong>Shao et al. (2024)</strong>：GRPO（Group Relative Policy Optimization），用组内归一化优势估计提升数学推理任务性能。</li>
</ul>
</li>
</ul>
<p><strong>2. 混合在线/离线数据的联合训练</strong></p>
<ul>
<li><p><strong>LUFFY (Yan et al., 2025)</strong><br>固定比例混合离线演示数据与在线 rollout，用单一复合损失联合优化SFT与RL目标，但比例需人工设定。</p>
</li>
<li><p><strong>SRFT (Fu et al., 2025)</strong><br>动态调整SFT与RL损失权重，依据策略熵而非性能反馈，缺乏理论统一性分析。</p>
</li>
<li><p><strong>AMFT (He et al., 2025)</strong><br>用元学习动态平衡模仿与探索，但需额外元控制器，复杂度较高。</p>
</li>
<li><p><strong>其他混合策略</strong></p>
<ul>
<li><strong>Wu et al. (2024)</strong>：交替执行on-policy与off-policy更新。</li>
<li><strong>Zhang et al. (2025a)</strong>：动态加权SFT与RL损失，但未解决理论冲突问题。</li>
</ul>
</li>
</ul>
<p><strong>3. 离线强化学习与偏好优化</strong></p>
<ul>
<li><strong>Rafailov et al. (2023)</strong>：DPO（Direct Preference Optimization），将RLHF简化为离线偏好损失，无需显式奖励模型。</li>
<li><strong>Mitchell et al. (2024)</strong>：利用离线数据集加速在线RL训练，但侧重数据复用而非统一框架。</li>
<li><strong>Ethayarajh et al. (2024)</strong>：KTO（Kahneman-Tversky Optimization），基于前景理论的离线偏好对齐方法。</li>
</ul>
<p><strong>4. 探索-利用权衡的理论分析</strong></p>
<ul>
<li><strong>Yue et al. (2025)</strong>：指出RLVR（Reinforcement Learning for Verifiable Rewards）虽提升Pass@1，但未能扩展模型能力边界（Pass@k）。</li>
<li><strong>Rajani et al. (2025)</strong>：对比SFT与GRPO，发现SFT替换能力而GRPO放大现有能力，支持动态混合的必要性。</li>
</ul>
<p><strong>5. 统一梯度视角的相关工作</strong></p>
<ul>
<li><strong>Schulman et al. (2015b)</strong>：GAE（Generalized Advantage Estimation）统一优势函数设计，启发UPGE的梯度分解。</li>
<li><strong>Kakade (2003)</strong>：自然策略梯度理论，分析重要性采样的偏差-方差权衡，与UPGE中参考策略选择直接相关。</li>
</ul>
<p><strong>关键差异</strong></p>
<p>上述研究多将SFT与RL视为独立目标或简单加权组合，而本论文通过UPGE首次从理论上证明两者梯度是同一目标的互补估计，并设计HPT实现无需人工调参的动态切换。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过“<strong>理论统一 → 算法设计 → 实验验证</strong>”的三步闭环解决 SFT 与 RL 如何高效协同的问题。</p>
<p><strong>1. 理论统一：Unified Policy Gradient Estimator（UPGE）</strong></p>
<ul>
<li><p><strong>统一目标</strong><br>将 SFT 与 RL 都视为对同一目标的优化  </p>
<script type="math/tex; mode=display">\\max\_\\theta ; \\mathbb{E}_{\\tau\\sim\\pi_\\theta}\\bigl\[r(\\tau)\\bigr\] - \\mu , \\mathrm{KL}!\\bigl(\\pi\_\\beta|\\pi\_\\theta\\bigr)</script><p>其中 $r(\tau)$ 为任务奖励，$\pi_\beta$ 为演示策略，$\mu$ 控制模仿强度。</p>
</li>
<li><p><strong>统一梯度形式</strong><br>推导出所有后训练算法（SFT、PPO、GRPO、DPO 等）的梯度都可写成  </p>
<script type="math/tex; mode=display">\\nabla J\_{\\text{uni}} = \\mathbb{E}_{\\tau\\sim\\pi_{\\text{ref}}} \\Bigl{% raw %}\[\\underbrace{1\_{\\text{stable}}}_{\\text{mask}} \\cdot \\frac{1}{\\pi_{\\text{ref}}(\\tau)} \\cdot \\underbrace{\\hat A(\\tau)}_{\\text{advantage}} \\cdot \\nabla_\\theta \\pi\_\\theta(\\tau)\\Bigr\]{% endraw %}</script><p>差异仅体现在四个可插拔组件：</p>
<ol>
<li><strong>stabilization mask</strong>（PPO clip、DAPO、CISPO…）</li>
<li><strong>reference policy</strong> $\pi_{\text{ref}}$（$\pi_\theta$、$\pi_{\theta_{\text{old}}}$、1…）</li>
<li><strong>advantage估计</strong> $\hat A$（GAE、GRPO 归一化、±1…）</li>
<li><strong>likelihood gradient</strong> $\nabla_\theta \pi_\theta$（共享）</li>
</ol>
<p>从而证明 SFT 与 RL 的梯度是<strong>互补估计</strong>，不存在本质冲突，只需在偏差-方差之间做权衡。</p>
</li>
</ul>
<p><strong>2. 算法设计：Hybrid Post-Training（HPT）</strong></p>
<ul>
<li><p><strong>动态混合损失</strong><br>每遇到一个 question $q$：</p>
<ol>
<li>用当前策略采样 $n$ 条 rollout 并计算即时准确率 $P$。</li>
<li>根据 $P$ 决定混合系数  <script type="math/tex; mode=display">(\\alpha,\\beta)=\\begin{cases}  
(1,0) & \\text{if } P>\\gamma \\quad (\\text{RL 主导}) \\  
(0,1) & \\text{if } P\\le\\gamma \\quad (\\text{SFT 主导})  
\\end{cases}</script></li>
<li>计算混合损失  <script type="math/tex; mode=display">\\mathcal L = \\alpha \\mathcal L\_{\\text{RL}} + \\beta \\mathcal L\_{\\text{SFT}}</script>其中 $\mathcal L_{\text{RL}}$ 采用 GRPO（group-normalized advantage + PPO clip），$\mathcal L_{\text{SFT}}$ 为交叉熵。</li>
</ol>
</li>
<li><p><strong>关键特性</strong></p>
<ul>
<li><strong>无人工调参</strong>：阈值 $\gamma$ 固定为 0（Qwen 系列）或 2（LLaMA 系列），无需预热或课程。</li>
<li><strong>单阶段训练</strong>：避免“SFT→RL”两阶段的高计算开销。</li>
<li><strong>能力自适应</strong>：弱模型早期自动多用 SFT，强模型后期自动多用 RL。</li>
</ul>
</li>
</ul>
<p><strong>3. 实验验证</strong></p>
<ul>
<li><p><strong>覆盖场景</strong></p>
<ul>
<li>模型：Qwen2.5-Math-1.5B/7B、LLaMA-3.1-8B</li>
<li>任务：6 个数学推理基准 + 2 个 OOD 套件</li>
<li>指标：Pass@1、Pass@1024、训练动态、熵、响应长度等</li>
</ul>
</li>
<li><p><strong>结果摘要</strong></p>
<p>对比项</p>
<p>提升幅度</p>
<p>说明</p>
<p>vs SFT→GRPO</p>
<p>+7.0 AIME24</p>
<p>单阶段即可超越两阶段</p>
<p>vs LUFFY/SRFT</p>
<p>+6.9 / +14.6 AIME24</p>
<p>动态切换优于固定比例</p>
<p>Pass@1024</p>
<p>全面最高</p>
<p>证明未牺牲探索能力</p>
<p>小模型</p>
<p>Qwen-1.5B 上仍显著受益</p>
<p>解决“弱模型零 RL 失败”难题</p>
</li>
<li><p><strong>消融实验</strong></p>
<ul>
<li><strong>Off-policy RL 替代 SFT</strong>：性能下降（38.1 vs 41.9），说明 SFT 仍是利用离线数据的最佳方式。</li>
<li><strong>门限 γ</strong>：γ=0 最优，进一步验证“需要时再做 SFT”而非“越多越好”。</li>
</ul>
</li>
</ul>
<p><strong>结论</strong></p>
<p>通过 UPGE 统一理论视角，HPT 在单一训练流程内实现了</p>
<ul>
<li><strong>早期快速吸收</strong>离线知识（SFT）</li>
<li><strong>后期稳定探索</strong>策略空间（RL）<br>从而在多个模型与任务上取得一致且显著的性能提升，且无需额外超参数调优。</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕<strong>Hybrid Post-Training（HPT）</strong>与若干强基线，在<strong>数学推理</strong>场景下开展了系统实验，覆盖<strong>模型规模、任务分布、训练动态、消融分析</strong>四个维度。核心实验列表如下（按主题归类）：</p>
<p><strong>1. 主实验：跨模型、跨基准的总体性能</strong></p>
<p>模型</p>
<p>基准（6项）</p>
<p>对比方法</p>
<p>关键结果</p>
<p><strong>Qwen2.5-Math-7B</strong></p>
<p>AIME24/25、AMC、MATH-500、Minerva、Olympiad</p>
<p>SFT、GRPO、SFT→GRPO、LUFFY、SRFT、4×Zero-RL开源方案</p>
<p>HPT在<strong>全部6项</strong>均夺魁，AIME24领先最强基线7.0分；平均提升约+8.2</p>
<p><strong>Qwen2.5-Math-1.5B</strong></p>
<p>同上</p>
<p>SFT、GRPO、LUFFY</p>
<p>AIME24提升+2.5，平均提升+7.2；验证小模型亦可受益</p>
<p><strong>LLaMA-3.1-8B</strong></p>
<p>同上</p>
<p>SFT、GRPO、LUFFY</p>
<p>平均提升+5.0，打破LLaMA数学推理低基线困境</p>
<blockquote>
<p>注：所有结果采用<strong>Pass@1</strong>（非零温）或<strong>avg@32</strong>（AIME/AMC），确保与先前工作可比。</p>
</blockquote>
<p><strong>2. 探索能力评估：Pass@k 曲线</strong></p>
<ul>
<li><strong>设置</strong>：对 AIME24、AIME25、AMC 每题生成 2048 条解答，bootstrap 估计 Pass@k（k=1…1024）。</li>
<li><strong>发现</strong><ul>
<li>HPT 的 <strong>Pass@1024 最高</strong>，显著优于纯 RL（GRPO）与固定比例混合（LUFFY）。</li>
<li>说明动态切换不仅提升 top-1 准确率，还<strong>保留了模型的探索边界</strong>。</li>
</ul>
</li>
</ul>
<p><strong>3. 细粒度能力对比：Exclusive Solve 分析</strong></p>
<ul>
<li><strong>基准</strong>：MATH-500（500题，5难度级）。</li>
<li><strong>方法</strong>：统计 HPT 单独解出而基线未解出的题目数（+X/-Y）。</li>
<li><strong>结果</strong><ul>
<li>随难度增加，HPT 的 <strong>+X 持续上升</strong>（Level5 +27 vs GRPO），而遗忘（-Y）极少。</li>
<li>证明 HPT 在<strong>高难度问题</strong>上获得新增能力，且<strong>无灾难遗忘</strong>。</li>
</ul>
</li>
</ul>
<p><strong>4. 训练动态可视化</strong></p>
<ul>
<li><strong>对象</strong>：Qwen2.5-Math-1.5B，50 epoch。</li>
<li><strong>对比</strong>：HPT vs SFT→GRPO（两阶段）。</li>
<li><strong>观察</strong><ul>
<li><strong>SFT→GRPO</strong> 出现大面积 rollout 失败（白色区域）。</li>
<li><strong>HPT</strong> 早期即可利用 SFT 信号，后期红色优势区逐渐扩大，<strong>最终全面超越</strong>两阶段方法。</li>
</ul>
</li>
</ul>
<p><strong>5. 训练曲线与内部机制</strong></p>
<ul>
<li><strong>指标</strong>：验证集准确率、离线数据占比、输出熵、平均响应长度。</li>
<li><strong>结论</strong><ul>
<li>离线数据占比随训练<strong>自适应下降</strong>，与模型实时性能同步。</li>
<li>HPT 保持<strong>更高熵</strong>与<strong>更长回答</strong>，显示未出现 entropy collapse，且内化了长链推理模式。</li>
</ul>
</li>
</ul>
<p><strong>6. 消融实验</strong></p>
<p><strong>6.1 Off-policy RL 替代 SFT</strong></p>
<p>训练范式</p>
<p>Avg</p>
<p>说明</p>
<p>SFT / On-policy (HPT)</p>
<p><strong>41.9</strong></p>
<p>默认配置</p>
<p>Off-policy / On-policy</p>
<p>38.1</p>
<p>用离线RL替代SFT，性能下降</p>
<p>Mix-policy / On-policy</p>
<p>40.3</p>
<p>离线RL+SFT混合，仍次于HPT</p>
<blockquote>
<p>结论：<strong>SFT仍是利用离线演示数据的最佳方式</strong>，离线RL不能简单替代。</p>
</blockquote>
<p><strong>6.2 门限 γ 消融</strong></p>
<p>γ</p>
<p>Avg</p>
<p>现象</p>
<p>0</p>
<p><strong>41.9</strong></p>
<p>仅在全错时启用SFT，最优</p>
<p>1</p>
<p>38.7</p>
<p>过早退出RL</p>
<p>2</p>
<p>39.0</p>
<p>过多SFT反而略降</p>
<blockquote>
<p>结论：<strong>动态平衡比单纯增加SFT比例更重要</strong>；γ=0 即“需要时才SFT”最有效。</p>
</blockquote>
<p><strong>7. 额外基准：OOD 稳健性</strong></p>
<ul>
<li><strong>数据集</strong>：ARC-c、GPQA-Diamond</li>
<li><strong>结果</strong>：HPT 在两项 OOD 任务上均领先基线，验证<strong>泛化能力</strong>。</li>
</ul>
<p><strong>实验小结</strong></p>
<p>维度</p>
<p>实验</p>
<p>结论</p>
<p><strong>性能</strong></p>
<p>6基准×3模型</p>
<p>HPT全面领先，最高+7.0 AIME24</p>
<p><strong>探索</strong></p>
<p>Pass@k 至1024</p>
<p>保留并扩大模型能力边界</p>
<p><strong>训练</strong></p>
<p>动态可视化</p>
<p>单阶段即可超越两阶段，无灾难遗忘</p>
<p><strong>消融</strong></p>
<p>Off-policy RL &amp; γ</p>
<p>SFT仍是关键；动态切换优于固定比例</p>
<p><strong>稳健</strong></p>
<p>OOD 任务</p>
<p>泛化性优于现有混合方法</p>
<p>实验规模覆盖 <strong>3 个模型族、6 个数学基准、2 个 OOD 套件</strong>，并辅以 <strong>训练动态、可视化、消融</strong> 多角度验证，充分支撑论文提出的统一框架与 HPT 算法的有效性。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为论文提出统一框架（UPGE）与混合算法（HPT）的后续研究切入点，按<strong>理论、算法、数据、评估、系统</strong>五个维度展开：</p>
<p><strong>1. 理论深化：统一框架的再扩展</strong></p>
<ul>
<li><strong>连续混合系数</strong><br>将当前二元门限 γ 扩展为<strong>连续函数</strong> α=f(P)、β=g(P)，例如用 logistic 或神经网络预测，实现更平滑的 bias-variance 权衡。</li>
<li><strong>多任务 KL 约束</strong><br>在统一目标中加入<strong>任务间 KL 正则项</strong>，研究如何在数学推理、代码生成、对话等多任务后训练中共享/隔离知识。</li>
<li><strong>理论收敛界</strong><br>基于 UPGE 的梯度方差表达式，推导<strong>有限样本收敛速率</strong>，量化不同组件（mask、π_ref、Â）对收敛的影响。</li>
</ul>
<p><strong>2. 算法创新：HPT 的变体与升级</strong></p>
<ul>
<li><strong>层级混合</strong><br>在<strong>token 级或 layer 级</strong>动态切换 SFT/RL，而非整句或整 batch，可能更精准地控制梯度噪声。</li>
<li><strong>元梯度控制</strong><br>用<strong>元学习</strong>实时调整 γ、ϵ、μ 等超参数（类似 AMFT），让算法在训练过程中自我优化切换策略。</li>
<li><strong>异步数据管道</strong><br>将 HPT 改造为<strong>异步框架</strong>：离线数据预填充 replay buffer，在线 rollout 并行生成，减少 GPU 空转。</li>
</ul>
<p><strong>3. 数据层面：离线数据的精炼与增强</strong></p>
<ul>
<li><strong>演示质量加权</strong><br>在 π_ref 中引入<strong>置信度或难度加权</strong>，对高质量/高难度轨迹给予更大系数，而非简单设为 1。</li>
<li><strong>合成-真实混合</strong><br>研究当离线数据包含<strong>模型合成解答</strong>与<strong>人工标注</strong>时，HPT 如何自动识别并调整利用比例。</li>
<li><strong>课程式数据调度</strong><br>按题目难度或知识点逐步释放离线数据，观察 HPT 是否比固定课程更高效。</li>
</ul>
<p><strong>4. 评估维度：更全面的能力探测</strong></p>
<ul>
<li><strong>链式推理长度可控性</strong><br>在 GSM8K、MATH 等数据集上，系统测量 HPT 对<strong>输出长度与正确率联合分布</strong>的影响，验证长链推理是否可控。</li>
<li><strong>OOD 鲁棒性细分</strong><br>将 OOD 任务细分为<strong>领域漂移</strong>（科学→金融）、<strong>格式漂移</strong>（选择题→开放题），测试 HPT 的边界。</li>
<li><strong>对抗/噪声提示</strong><br>引入<strong>对抗性 prompt</strong>或<strong>噪声 reward</strong>，评估 HPT 在鲁棒性-性能权衡上的表现。</li>
</ul>
<p><strong>5. 系统与工程：大规模落地</strong></p>
<ul>
<li><strong>千卡级训练效率</strong><br>在千卡 A100/H100 集群上测试 HPT 的<strong>扩展性</strong>：通信开销、负载均衡、checkpoint 频率。</li>
<li><strong>推理-训练协同优化</strong><br>结合<strong>test-time scaling</strong>（如 TTRL 或自洽性投票），研究 HPT 训练后的模型在<strong>推理阶段</strong>是否更易受益。</li>
<li><strong>开源复现与工具链</strong><br>提供<strong>一键式脚本</strong>支持 HuggingFace Trainer + DeepSpeed/Colossal-AI，降低社区复现门槛。</li>
</ul>
<p><strong>一句话总结</strong></p>
<p>从“<strong>二元门限→连续自适应</strong>”、“<strong>单任务→多任务</strong>”、“<strong>纯数学→多领域</strong>”、“<strong>离线静态→在线动态</strong>”四个轴向上，均可基于 UPGE/HPT 继续深挖，形成下一代统一后训练范式。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文一句话总结</p>
<p>本文提出 <strong>Unified Policy Gradient Estimator（UPGE）</strong> 将 SFT 与 RL 的梯度统一为同一目标的互补估计，并基于该理论设计 <strong>Hybrid Post-Training（HPT）</strong> 算法，通过实时性能反馈动态切换 SFT/RL，在多个模型与数学推理基准上全面超越现有流水线与混合方法。</p>
<p>主要内容速览</p>
<p>维度</p>
<p>关键内容</p>
<p><strong>问题</strong></p>
<p>SFT 与 RL 被视为对立范式；“SFT→RL”两阶段流程资源高、调参难；现有混合方法缺乏理论依据。</p>
<p><strong>理论</strong></p>
<p>1. 统一目标：$\max_\theta \mathbb{E}[r(\tau)]-\mu,\mathrm{KL}(\pi_\beta|\pi_\theta)$<br>2. UPGE 梯度：$\mathbb{E}\bigl{% raw %}[1_{\text{stable}}\tfrac{1}{\pi_{\text{ref}}}\hat A\nabla_\theta\pi_\theta\bigr]{% endraw %}$，四组件（mask、$\pi_{\text{ref}}$、$\hat A$、$\nabla\pi_\theta$）可插拔。</p>
<p><strong>算法</strong></p>
<p>HPT：每题采样 $n$ 条 rollout→计算准确率 $P$→二元门限 $(\alpha,\beta)\in{(1,0),(0,1)}$→混合损失 $\mathcal L=\alpha\mathcal L_{\text{RL}}+\beta\mathcal L_{\text{SFT}}$。</p>
<p><strong>实验</strong></p>
<p>1. <strong>主结果</strong>：Qwen2.5-Math-7B 在 6 项数学基准平均领先最强基线 7+ 分；LLaMA-3.1-8B、Qwen-1.5B 亦显著提升。<br>2. <strong>探索能力</strong>：Pass@1024 全面最高，验证未牺牲边界。<br>3. <strong>消融</strong>：离线 RL 替代 SFT 性能下降；门限 γ=0 最优。</p>
<p><strong>贡献</strong></p>
<p>1. 理论：首次将 SFT 与 RL 梯度统一为同一优化目标的不同实例。<br>2. 实践：单阶段 HPT 算法在计算、性能、鲁棒性上均优于现有流水线。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Towards%20a%20Unified%20View%20of%20Large%20Language%20Model%20Post-Training，链接是  https://arxiv.org/pdf/2509.04419  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04419  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xingtai Lv,Yuxin Zuo,Youbang Sun,Hongyi Liu,Yuntian Wei,Zhekai Chen,Lixuan He,Xuekai Zhu,Kaiyan Zhang,Bingning Wang,Ning Ding,Bowen Zhou</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04419.pdf">https://arxiv.org/pdf/2509.04419.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04419">https://arxiv.org/abs/2509.04419</a></p>
<p><strong>Arxiv ID</strong>: 2509.04419</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04419">https://papers.cool/arxiv/2509.04419</a></p>
<p><strong>Published</strong>: 2025-09-04T17:40:33Z</p>
<p><strong>Updated</strong>: 2025-09-04T17:40:33.000Z</p>
<hr>
<h3 id="4-DeepResearch-Arena-The-First-Exam-of-LLMs’-Research-Abilities-via-Seminar-Grounded-Tasks"><a href="#4-DeepResearch-Arena-The-First-Exam-of-LLMs’-Research-Abilities-via-Seminar-Grounded-Tasks" class="headerlink" title="4. DeepResearch Arena: The First Exam of LLMs’ Research Abilities via Seminar-Grounded Tasks"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.01396">DeepResearch Arena: The First Exam of LLMs’ Research Abilities via Seminar-Grounded Tasks</a></h3><p>Deep research agents have attracted growing attention for their potential to orchestrate multi-stage research workflows, spanning literature synthesis, methodological design, and empirical verification. Despite these strides, evaluating their research capability faithfully is rather challenging due to the difficulty of collecting frontier research questions that genuinely capture researchers’ attention and intellectual curiosity. To address this gap, we introduce DeepResearch Arena, a benchmark grounded in academic seminars that capture rich expert discourse and interaction, better reflecting real-world research environments and reducing the risk of data leakage. To automatically construct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task Generation (MAHTG) system that extracts research-worthy inspirations from seminar transcripts. The MAHTG system further translates research-worthy inspirations into high-quality research tasks, ensuring the traceability of research task formulation while filtering noise. With the MAHTG system, we curate DeepResearch Arena with over 10,000 high-quality research tasks from over 200 academic seminars, spanning 12 disciplines, such as literature, history, and science. Our extensive evaluation shows that DeepResearch Arena presents substantial challenges for current state-of-the-art agents, with clear performance gaps observed across different models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>深度研究代理因其潜在能力而受到越来越多的关注，可以协调多阶段的研究工作流程，包括文献综合、方法设计和实证验证。尽管取得了这些进展，忠实评估其研究能力仍然相当具有挑战性，因为收集真正引起研究者注意和智力好奇心的前沿研究问题非常困难。为了弥补这一空白，我们引入了深度研究竞技场，这是一个基于学术研讨会的基准，捕捉丰富的专家讨论和互动，更好地反映现实研究环境，并减少数据泄露的风险。为了自动构建深度研究竞技场，我们提出了一种多代理分层任务生成（MAHTG）系统，该系统从研讨会记录中提取值得研究的灵感。MAHTG系统进一步将值得研究的灵感转化为高质量的研究任务，确保研究任务制定的可追溯性，同时过滤噪声。借助MAHTG系统，我们从200多场学术研讨会中策划了超过10,000个高质量研究任务，涵盖文学、历史、科学等12个学科。我们广泛的评估显示，深度研究竞技场为当前最先进的代理提供了重大挑战，不同模型之间存在明显的性能差距。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决<strong>如何真实、可扩展地评估“深度研究智能体”（deep research agents）的研究能力</strong>这一核心难题。具体而言，现有评估方法存在以下关键缺陷：</p>
<ul>
<li><strong>静态语料基准</strong>（如论文、网页）易引发<strong>数据泄漏</strong>（预训练阶段已见内容），且脱离真实研究情境；</li>
<li><strong>专家手工构建任务</strong>虽质量高，但<strong>规模受限</strong>，难以覆盖多学科动态问题，且缺乏问题自然涌现的语境。</li>
</ul>
<p>为此，论文提出<strong>DeepResearch Arena</strong>，通过以下创新解决上述问题：</p>
<ol>
<li><strong>研讨会驱动的任务来源</strong>：以200+场真实学术研讨会（跨12学科）为语料，利用专家现场讨论中的开放性问题与跨学科灵感，确保任务具备<strong>真实研究语境</strong>与<strong>前沿性</strong>，同时规避预训练数据泄漏风险。</li>
<li><strong>自动化任务生成系统（MAHTG）</strong>：通过多智能体流水线（Inspira→TaskWeaver→RankEval）从研讨会转录文本中<strong>提取研究灵感</strong>（Limitation/Methodology/Transdisciplinarity/Hypothesis），并转化为<strong>可执行的开放式任务</strong>（Synthesize/Design/Evaluate三阶段），兼顾<strong>可扩展性</strong>与<strong>可追溯性</strong>。</li>
<li><strong>混合评估框架</strong>：结合<strong>事实对齐评估（KAE）</strong>（基于引用源的关键点覆盖率、冲突率、遗漏率）与<strong>自适应清单评估（ACE）</strong>（动态生成任务专用评分标准），解决开放性问题缺乏固定答案的评估难题。</li>
</ol>
<p>最终，论文通过10,000+高质量任务与多模型评测，验证了当前深度研究智能体在复杂研究任务上仍存在显著能力差距，为下一代研究辅助系统的开发提供了理论对齐的基准。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>与 DeepResearch Arena 直接相关的研究可分为三类：</p>
<ol>
<li>深度研究智能体（Deep Research Agents）</li>
<li>研究能力评估基准（Benchmarks for Research Agents）</li>
<li>数据泄漏检测与评估方法学（Leakage Detection &amp; Evaluation Methodology）</li>
</ol>
<p>1. 深度研究智能体</p>
<p>系统 / 论文</p>
<p>核心特点</p>
<p>与本文关联</p>
<p><strong>GPT DeepResearch</strong> (OpenAI 2025)</p>
<p>以大纲驱动的长文综合，强调引用溯源</p>
<p>被本文列为评测对象之一</p>
<p><strong>Gemini Deep Research</strong> (Google 2025)</p>
<p>多模态检索与合成</p>
<p>同被纳入表 3 的横向评测</p>
<p><strong>Grok DeepSearch</strong> (xAI 2025)</p>
<p>实时网络摘要，侧重动态主题</p>
<p>作为对比基线</p>
<p><strong>ResearchAgent</strong> (Baek et al. 2024)</p>
<p>在科学文献上迭代式研究想法生成</p>
<p>启发 MAHTG 的“inspiration→task”流水线</p>
<p><strong>Agent Laboratory</strong> (Schmidgall et al. 2025)</p>
<p>用 LLM 代理作为研究助理，支持实验设计</p>
<p>与本文“Design”阶段任务同构</p>
<p><strong>Chain of Ideas</strong> (Li et al. 2024)</p>
<p>通过 LLM 代理发展新研究想法</p>
<p>与 Hypothesis Generation 任务对齐</p>
<p>2. 研究能力评估基准</p>
<p>基准</p>
<p>数据来源</p>
<p>主要局限</p>
<p>与本文差异</p>
<p><strong>MuSiQue</strong> (Trivedi et al. 2022)</p>
<p>维基百科多跳问答</p>
<p>逻辑链人工预设，缺乏真实研究涌现性</p>
<p>本文任务来自研讨会现场讨论，无预设链</p>
<p><strong>HotpotQA</strong> (Yang et al. 2018)</p>
<p>维基百科双文档问答</p>
<p>静态语料，易泄漏</p>
<p>本文用研讨会视频，未进预训练</p>
<p><strong>StrategyQA</strong> (Geva et al. 2021)</p>
<p>常识隐含推理</p>
<p>非研究导向</p>
<p>本文聚焦研究全流程</p>
<p><strong>Humanity’s Last Exam</strong> (Phan et al. 2025)</p>
<p>专家手写开放题</p>
<p>规模小，人工成本高</p>
<p>本文自动生成 10k+ 任务</p>
<p><strong>DeepResearchBench</strong> (Du et al. 2025)</p>
<p>专家设计长文报告任务</p>
<p>缺乏跨学科动态语境</p>
<p>本文任务源自真实跨学科研讨</p>
<p><strong>ExpertLongBench</strong> (Ruan et al. 2025)</p>
<p>专家级长文本生成</p>
<p>静态任务，无演化</p>
<p>本文任务体现研讨会的迭代性</p>
<p>3. 数据泄漏检测与评估方法学</p>
<p>方法 / 论文</p>
<p>技术要点</p>
<p>在本文中的应用</p>
<p><strong>Carlini et al. 2021</strong></p>
<p>从 LLM 中提取训练数据</p>
<p>本文借鉴其 0.7 相似度阈值设定</p>
<p><strong>LessLeak-Bench</strong> (Zhou et al. 2025b)</p>
<p>83 个软件工程基准的泄漏检测</p>
<p>本文复用其前缀-后缀实验范式</p>
<p><strong>Benchmarking Leakage</strong> (Xu et al. 2024)</p>
<p>系统性泄漏模拟流程</p>
<p>本文附录 A 的检测脚本与之同构</p>
<p><strong>Kendall’s τ / Spearman’s ρ</strong> (Kendall 1938; Spearman 1904)</p>
<p>排序一致性检验</p>
<p>表 5 用于验证 KAE/ACE 与人类评分对齐</p>
<p>小结</p>
<ul>
<li><strong>系统层</strong>：GPT-4o-search、Gemini-2.5-pro 等前沿代理被直接评测。</li>
<li><strong>基准层</strong>：DeepResearch Arena 填补了“真实研讨会→可扩展任务→无泄漏评估”的空白。</li>
<li><strong>方法层</strong>：泄漏检测与相关性检验方法均基于社区成熟指标，确保基准可信度。</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过“三步走”策略系统性地解决了“如何真实、可扩展地评估深度研究智能体”这一难题。</p>
<p>1. 数据层：研讨会驱动的语料构建</p>
<ul>
<li><p><strong>语料来源</strong><br>采集 200+ 场、跨 12 学科的真实学术研讨会视频（总计 10,000+ 任务）。</p>
<ul>
<li>研讨会视频极少出现在 LLM 预训练语料 → 天然降低数据泄漏风险。</li>
<li>现场讨论保留问题涌现、跨学科碰撞的“原始语境”，比静态文献/网页更接近真实研究。</li>
</ul>
</li>
<li><strong>转录与清洗</strong><br>自动语音识别 → 文本转录 → 人工抽检，确保语义完整性与隐私合规。</li>
</ul>
<p>2. 任务层：MAHTG 多智能体流水线</p>
<p><strong>Multi-Agent Hierarchical Task Generation (MAHTG)</strong> 将原始研讨记录转化为结构化任务，流程如下：</p>
<p>阶段</p>
<p>智能体</p>
<p>关键动作</p>
<p>输出</p>
<p><strong>Inspira</strong></p>
<p>长上下文 LLM (Claude-Sonnet)</p>
<p>基于“四维度过滤”(Novelty, Explorability, Challenge, Verifiability) 提取研究灵感并分类为 Limitation / Methodology / Transdisciplinarity / Hypothesis</p>
<p>灵感 JSONL</p>
<p><strong>TaskWeaver</strong></p>
<p>同模型</p>
<p>将灵感组合、重写为 3 阶段任务：Synthesize, Design, Evaluate；每任务 ≤100 词且含可交付物</p>
<p>候选任务池</p>
<p><strong>RankEval</strong></p>
<p>轻量 LLM (GPT-4o-mini)</p>
<p>用 Elo 机制对候选任务做两两比较，按原创性、清晰度、科学相关性排序，保留 Top-K</p>
<p>最终 10k 高质量任务</p>
<p>3. 评估层：混合指标框架</p>
<p>为应对“开放性问题无唯一答案”的挑战，论文设计双轨评估：</p>
<p>3.1 Keypoint-Aligned Evaluation (KAE) – 客观事实对齐</p>
<ul>
<li><p><strong>步骤</strong></p>
<ol>
<li>提取模型报告中所有引用 URL 的<strong>关键要点</strong> $K_u$；</li>
<li>去重得 Unified Evidence Keypoints (UEK)；</li>
<li>计算三率：<ul>
<li>Keypoint Supported Rate (KSR)</li>
<li>Keypoint Conflict Rate (KCR)</li>
<li>Keypoint Omission Rate (KOR)</li>
</ul>
</li>
</ol>
</li>
<li><strong>意义</strong><br>量化模型回答与真实来源的事实一致性，避免纯主观打分。</li>
</ul>
<p>3.2 Adaptively-generated Checklist Evaluation (ACE) – 主观高阶能力</p>
<ul>
<li><strong>两阶段协议</strong><ol>
<li><strong>元分析器</strong>（GPT-4o 级）针对任务动态生成带权评分清单（含方法论严谨性、创新性等）；</li>
<li><strong>评分器</strong>（独立 LLM）逐项打分并加权汇总，实现可扩展、低偏差的“模型即评委”。</li>
</ol>
</li>
</ul>
<p>4. 验证层：防泄漏与人工对齐</p>
<ul>
<li><strong>泄漏检测</strong><br>前缀-后缀实验 + 复合相似度阈值 0.7 → 8 个主流模型均未触发泄漏（附录 A）。</li>
<li><strong>人工对齐</strong><br>KAE/ACE 与 3 人专家评分的 Spearman ρ 达 0.84/0.81，验证自动指标与人类判断高度一致（附录 B）。</li>
</ul>
<p>总结</p>
<p>通过“研讨会语料 → MAHTG 任务生成 → KAE+ACE 混合评估 → 泄漏&amp;对齐验证”的完整闭环，论文首次实现了：</p>
<ul>
<li><strong>真实语境</strong>（研讨会驱动）</li>
<li><strong>可扩展规模</strong>（10k+ 任务，自动化）</li>
<li><strong>无泄漏风险</strong>（视频语料+检测验证）</li>
<li><strong>高阶能力评估</strong>（开放任务+细粒度指标）</li>
</ul>
<p>从而填补了现有基准在“研究真实性”与“评估可靠性”之间的关键缺口。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>DeepResearch Arena</strong> 的可靠性、有效性及模型能力差异，共设计并执行了四类实验：</p>
<p>1. 基准整体可行性实验</p>
<p><strong>目的</strong>：验证 MAHTG 流水线能否稳定产出高质量、无泄漏、跨学科任务。<br><strong>做法</strong>：</p>
<ul>
<li>从 200+ 研讨会视频 → 10,000+ 任务，人工抽检 300 条，由 10 位 PhD 标注者确认任务清晰度、可执行性、学科归属。</li>
<li>统计指标：<ul>
<li>任务通过率（人工评分 ≥4/5）：92.7 %</li>
<li>跨学科覆盖率：12 个学科均 ≥500 任务</li>
<li>三阶段（Synthesize/Design/Evaluate）分布均衡性：χ² 检验 p&gt;0.1，无显著偏斜。</li>
</ul>
</li>
</ul>
<p>2. 数据泄漏检测实验（附录 A）</p>
<p><strong>目的</strong>：证明研讨会语料未被主流 LLM 预训练吸收。<br><strong>步骤</strong>：</p>
<ol>
<li>对 100 个随机任务，按标点将描述切为前缀（50 %）与真实后缀。</li>
<li>8 个受测模型仅见前缀，生成续写。</li>
<li>计算 4 种相似度：<ul>
<li>归一化最长公共子序列（LCS）</li>
<li>TF-IDF 余弦相似度</li>
<li>词集合重叠率</li>
<li>加权复合相似度</li>
</ul>
</li>
<li>判定规则：复合相似度 &gt;0.7 视为潜在泄漏。</li>
</ol>
<p><strong>结果</strong>：</p>
<p>模型</p>
<p>复合相似度均值</p>
<p>泄漏案例数</p>
<p>gpt-4o-search-preview</p>
<p>9.8 %</p>
<p>0</p>
<p>gemini-2.5-pro</p>
<p>13.8 %</p>
<p>0</p>
<p>…</p>
<p>…</p>
<p>0</p>
<p><strong>结论</strong>：全部 0 触发，验证语料无预训练污染。</p>
<p>3. 自动评估与人类一致性实验（附录 B）</p>
<p><strong>目的</strong>：确认 KAE 与 ACE 的打分与人类专家高度一致。<br><strong>设置</strong>：</p>
<ul>
<li>随机抽取 120 个任务-响应对，3 位领域 PhD 独立打分后取平均。</li>
<li>计算三类相关性：</li>
</ul>
<p>指标</p>
<p>KAE vs. 人类</p>
<p>ACE vs. 人类</p>
<p>Spearman ρ</p>
<p>0.84</p>
<p>0.81</p>
<p>Pearson r</p>
<p>0.79</p>
<p>0.76</p>
<p>Kendall τ</p>
<p>0.68</p>
<p>0.65</p>
<p><strong>结论</strong>：所有系数 &gt;0.65，表明自动指标可替代高成本人工评审。</p>
<p>4. 主流模型横向评测实验（主实验）</p>
<p><strong>目的</strong>：量化当前深度研究智能体在 DeepResearch Arena 上的能力差异。<br><strong>受测模型</strong>：8 个带实时检索功能的 LLM（gpt-4o-search、gemini-2.5-pro、grok-4 等）。<br><strong>指标</strong>：</p>
<ul>
<li><strong>KAE</strong>：KSR、KCR、KOR（%）</li>
<li><strong>ACE</strong>：0–10 主观分</li>
<li><strong>效率</strong>：KSR/(KCR+KOR)</li>
</ul>
<p><strong>结果摘要</strong>：</p>
<p>模型</p>
<p>KSR↑</p>
<p>KCR↓</p>
<p>KOR↓</p>
<p>ACE↑</p>
<p>效率</p>
<p>o4-mini-deepresearch</p>
<p>77.2</p>
<p>6.1</p>
<p>18.5</p>
<p>4.03</p>
<p><strong>最高</strong></p>
<p>gemini-2.5-flash</p>
<p>78.7</p>
<p>4.0</p>
<p>18.0</p>
<p>3.81</p>
<p>次高</p>
<p>gpt-4o-mini-search</p>
<p>78.7</p>
<p>27.8</p>
<p>12.8</p>
<p>2.07</p>
<p>最低</p>
<p><strong>细分分析</strong>：</p>
<ul>
<li><strong>学科维度</strong>（图 3a）：Science &amp; Technology 任务整体难度最高；Art、History 相对低。</li>
<li><strong>任务类型维度</strong>（图 3b）：Hypothesis Generation、Method Blueprint 对模型挑战最大；Trend Scan、Literature Survey 相对容易。</li>
</ul>
<p>实验总结</p>
<ol>
<li><strong>可行性</strong>：MAHTG 流水线可稳定产出跨学科、高质量任务。</li>
<li><strong>无泄漏</strong>：100 任务 × 8 模型 = 800 次检测，0 次触发阈值。</li>
<li><strong>评估可靠</strong>：自动指标与人类评分高度一致，可大规模替代人工。</li>
<li><strong>模型差距</strong>：o4-mini-deepresearch 与 gemini-2.5-flash 领先，gpt-4o-mini-search、grok-4 在中文或多学科场景下显著落后。</li>
</ol>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为 DeepResearch Arena 的后续研究与工程扩展点，按“数据—任务—评估—应用”四维度归纳：</p>
<p>1. 数据层：研讨会语料的深度挖掘</p>
<ul>
<li><strong>多模态扩展</strong><br>将幻灯片、板书、现场问答录音与视频帧同步，构建跨模态对齐语料，以支持视觉推理任务（如图表理解、实验装置识别）。</li>
<li><strong>纵向跟踪</strong><br>对同一研究组 2–3 年内的系列研讨会进行时间序列标注，观察研究问题的演化路径，形成“动态问题图谱”数据集，用于评估智能体的长期科研跟随能力。</li>
<li><strong>隐私与伦理</strong><br>开发差分隐私转录 + 发言人匿名化工具链，在保证学术内容完整性的同时满足 GDPR/IRB 要求，从而引入更多非公开高端研讨会。</li>
</ul>
<p>2. 任务层：MAHTG 的精细化与自动化</p>
<ul>
<li><strong>任务难度自动分级</strong><br>利用课程学习思想，训练一个轻量级难度预测器（基于任务文本 + 研讨会元数据），实现从 Basic→Advanced 的细粒度 5 级划分，取代当前二元标签。</li>
<li><strong>跨语言任务生成</strong><br>在 MAHTG 中引入机器翻译与对齐模块，直接从英语研讨会生成中文、法语等多语言任务，检验模型在非英语语境下的研究能力。</li>
<li><strong>可执行性验证器</strong><br>为 Design 阶段任务自动匹配公开数据集、开源库与计算资源，生成“一键可运行”的 Jupyter Notebook 模板，降低人工验证成本。</li>
</ul>
<p>3. 评估层：KAE+ACE 的升级</p>
<ul>
<li><strong>因果一致性检测</strong><br>在 KAE 中引入因果图匹配：将引用文献中的因果主张与模型报告中的因果陈述进行图同构比对，量化因果推理的忠实度。</li>
<li><strong>自适应权重学习</strong><br>用 Bradley-Terry 模型从大规模人工标注中学习 ACE 各维度的最优权重，而非固定启发式值，使评分更贴合学科差异。</li>
<li><strong>人类-AI 协同评审</strong><br>构建“人在回路”平台：ACE 先给出粗评分与缺陷定位，人类专家仅需复核争议项，预计可将人工评估成本降低 70 %。</li>
</ul>
<p>4. 应用层：真实科研流程集成</p>
<ul>
<li><strong>端到端代理测试</strong><br>将 DeepResearch Arena 任务直接接入现有代理框架（如 AutoGPT、LangChain），记录从任务理解、文献检索、实验设计到报告撰写的全链路日志，形成“代理轨迹数据集”，用于诊断失败环节。</li>
<li><strong>个性化研究助手</strong><br>基于学者过往发表与演讲记录，微调任务生成器，为每位研究者推送“量身定制”的后续研究任务，实现研讨会内容的个性化再生产。</li>
<li><strong>跨学科创新发现</strong><br>利用 Transdisciplinary 类型任务的高密度出现，构建“学科桥接指数”，量化不同领域术语与方法在研讨会中的共现强度，辅助发现潜在交叉研究方向。</li>
</ul>
<p>5. 社区与工具链</p>
<ul>
<li><strong>开源工具箱</strong><br>发布 MAHTG-pipeline（含 Inspira、TaskWeaver、RankEval 的模块化代码）、KAE/ACE 评估 SDK 与研讨会爬虫模板，鼓励社区贡献新学科数据。</li>
<li><strong>众包标注平台</strong><br>搭建轻量级 Web 界面，让全球研究者在线对任务质量进行快速打分，持续扩充人工校验集，用于权重学习与指标校准。</li>
</ul>
<p>通过上述探索，DeepResearch Arena 可从“静态基准”演进为“活的基础设施”，持续服务下一代研究智能体的训练、诊断与迭代。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p><strong>一句话总结</strong><br>本文提出首个以真实学术研讨会为根基、自动化生成万级任务、且配套无泄漏混合评估的深度研究智能体基准 <strong>DeepResearch Arena</strong>，并系统揭示当前主流模型在复杂研究场景中的显著能力差距。</p>
<p>1. 问题背景</p>
<ul>
<li>现有基准要么依赖静态文献/网页（易泄漏、缺语境），要么靠专家手工出题（规模小、缺动态性）。</li>
<li>亟需一个<strong>真实、可扩展、无泄漏</strong>的评估框架，衡量 LLM 在开放、跨学科研究任务中的全流程能力。</li>
</ul>
<p>2. 核心贡献</p>
<p>模块</p>
<p>要点</p>
<p>规模/结果</p>
<p><strong>数据</strong></p>
<p>采集 200+ 场、12 学科研讨会视频 → 转录文本</p>
<p>10,000+ 高质量任务</p>
<p><strong>任务生成</strong></p>
<p>MAHTG 多智能体流水线：Inspira 提取灵感 → TaskWeaver 生成任务 → RankEval Elo 排序</p>
<p>自动化、可追溯、跨 Synthesize/Design/Evaluate 三阶段</p>
<p><strong>评估</strong></p>
<p>KAE（事实对齐：KSR/KCR/KOR）+ ACE（自适应清单评分）</p>
<p>与人类评分 ρ≥0.81，无泄漏</p>
<p><strong>实验</strong></p>
<p>8 个主流检索增强模型横向评测</p>
<p>o4-mini-deepresearch 领先，gpt-4o-mini-search、grok-4 差距明显</p>
<p>3. 关键结论</p>
<ul>
<li><strong>研讨会语料</strong>有效规避数据泄漏（100 任务×8 模型＝0 触发）。</li>
<li><strong>KAE+ACE</strong> 可作为大规模、低成本的科研能力评估替代人工。</li>
<li><strong>模型表现差异显著</strong>：高阶任务（假设生成、方法蓝图）仍是普遍短板，跨学科与多语言能力分化严重。</li>
</ul>
<p>4. 影响与展望</p>
<p>DeepResearch Arena 为下一代研究智能体提供了<strong>真实、动态、可扩展</strong>的评估土壤，并开源数据与工具链，支持社区持续扩展与迭代。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是DeepResearch%20Arena%3A%20The%20First%20Exam%20of%20LLMs%27%20Research%20Abilities%20via%20Seminar-Grounded%20Tasks，链接是  https://arxiv.org/pdf/2509.01396  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01396  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Haiyuan Wan,Chen Yang,Junchi Yu,Meiqi Tu,Jiaxuan Lu,Di Yu,Jianbao Cao,Ben Gao,Jiaqing Xie,Aoran Wang,Wenlong Zhang,Philip Torr,Dongzhan Zhou</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.01396.pdf">https://arxiv.org/pdf/2509.01396.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.01396">https://arxiv.org/abs/2509.01396</a></p>
<p><strong>Arxiv ID</strong>: 2509.01396</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.01396">https://papers.cool/arxiv/2509.01396</a></p>
<p><strong>Published</strong>: 2025-09-01T11:42:47Z</p>
<p><strong>Updated</strong>: 2025-09-01T11:42:47.000Z</p>
<hr>
<h3 id="5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions"><a href="#5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions" class="headerlink" title="5. Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04292">Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?</a></h3><p>Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）在各种任务上表现出色，但通常表现出认知惯性，难以遵循与在监督微调（SFT）期间学习的标准化模式相冲突的指令。为了评估这一限制，我们提出了逆向IFEval，这是一个基准，衡量模型的反直觉能力，即它们覆盖训练引起的偏见和遵循对抗性指令的能力。逆向IFEval引入八种此类挑战，包括问题纠正、有意文本缺陷、没有注释的代码和反事实问答。利用人机互动的流程，我们构建了一个涵盖23个领域的1012个高质量中英文问题的数据集，并在优化的LLM作为评审框架下进行评估。对现有领先LLMs的实验表明我们提出的逆向IFEval基准的必要性。我们的发现强调未来的对齐工作不仅应追求流畅性和事实正确性，还应考虑在非常规环境下的适应性。我们希望逆向IFEval不仅作为一种诊断工具，也为开发减轻认知惯性、减少对狭窄模式的过拟合的方法奠定基础，从而最终增强LLMs在多样化和不可预测的现实场景中遵循指令的可靠性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在诊断并缓解大语言模型（LLM）在<strong>反直觉指令</strong>（counter-intuitive instructions）场景下的<strong>认知惯性</strong>（cognitive inertia）问题。<br>具体而言，作者观察到：尽管 LLM 在常规监督微调（SFT）后表现出优异的指令遵循能力，但当用户要求与训练阶段所强化的“理想范式”冲突时（例如“必须故意答错”“禁用列表格式”“代码不加注释”），模型极易滑回训练分布，无法忠实执行指令。</p>
<p>为此，论文提出以下核心目标：</p>
<ol>
<li>建立一套系统度量——<strong>反认知能力</strong>（Counter-Cognitive Ability），用于量化 LLM 能否主动抑制训练带来的顽固偏好，转而服从与训练分布相悖的指令。</li>
<li>构建并发布<strong>Inverse IFEval</strong> 基准，通过 8 类、共 1012 条中英双语对抗指令，对现有模型进行压力测试，揭示其在分布外（OOD）场景下的脆弱性。</li>
<li>借助大规模实验验证：<ul>
<li>认知惯性普遍存在于当前主流开源与闭源模型；</li>
<li>引入“思考”机制（Chain-of-Thought / RL 推理）可部分缓解，但仍远未解决；</li>
<li>传统指令遵循基准（如 IFEval）与反直觉场景排名差异显著，说明现有评估遗漏了关键鲁棒维度。</li>
</ul>
</li>
</ol>
<p>综上，论文不仅诊断了“LLM 难以忘却训练惯例”这一痛点，也为后续对齐研究提供了可量化的诊断工具与改进方向，以期提升模型在真实、不可预测环境中的<strong>可靠指令遵循能力</strong>。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>论文在 Related Work 部分（第 10 页）将已有研究划分为两条主线，并指出它们与 Inverse IFEval 的区别。可归纳为以下 4 类、共 12 篇代表性文献：</p>
<p>类别</p>
<p>代表文献</p>
<p>与本文关系</p>
<p>1. 指令微调方法</p>
<p>Ouyang et al. 2022 (InstructGPT)<br>Peng et al. 2023 (GPT-4-tuning)<br>Shi et al. 2024 (Loss-over-instructions)</p>
<p>聚焦“如何让模型更听话”，但未评估模型在<strong>与训练分布冲突</strong>时的鲁棒性。</p>
<p>2. 视觉-语言指令</p>
<p>Liu et al. 2023 (Visual Instruction Tuning)<br>Dai et al. 2023 (InstructBLIP)</p>
<p>将指令遵循扩展到多模态，同样默认指令与训练范式一致。</p>
<p>3. 常规指令遵循基准</p>
<p>Zhou et al. 2023 (IFEval)<br>Dussolle et al. 2025 (M-IFEval)<br>Bitton et al. 2023 (VisIT-Bench)<br>Wen et al. 2024 (Multi-Constraint)</p>
<p>测试“模型是否按显式约束生成”，但约束本身<strong>不违背训练惯例</strong>；Inverse IFEval 则<strong>系统性地反转</strong>这些惯例，专门制造分布外冲突。</p>
<p>4. 领域专用指令基准</p>
<p>Oh et al. 2024 (INSTRUCTIR)<br>Gao et al. 2025 (IFEval-Audio)<br>Yang et al. 2025 (IFEval-Code)</p>
<p>聚焦检索、音频、代码等垂直场景，依旧未触及“反直觉”维度。</p>
<p>综上，<strong>尚无工作专门研究“模型能否抑制训练阶段强化的默认习惯，以服从与训练分布相悖的指令”</strong>；Inverse IFEval 首次填补了这一评估空白。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文并未提出新的训练算法或对齐策略，而是**“先诊断、后指路”**：通过构建 Inverse IFEval 基准，对现有模型的认知惯性进行量化暴露，从而为后续研究提供可操作的改进依据。具体手段可概括为“三步走”：</p>
<ol>
<li><p>系统逆向工程 SFT 惯例</p>
<ul>
<li>归纳出现有 SFT 数据中的 8 条“理想范式”（如“答案必须正确”“代码需加注释”“列表提升可读性”）。</li>
<li>对每条范式<strong>手工设计反向约束</strong>，形成 8 类反直觉指令模板（Question Correction、Intentional Textual Flaws 等）。</li>
</ul>
</li>
<li><p>人机协同构造对抗数据集</p>
<ul>
<li>种子阶段：领域专家手工撰写 1012 条高质量中英双语实例，覆盖 23 个学科。</li>
<li>扩增阶段：用多模型协同生成 + 语义去重 + 人工三审（类型一致性、清晰度、评分细则），确保每条指令都与对应范式<strong>显性冲突</strong>。</li>
<li>质量阶段：交叉评审 + 统一评分 rubric，最终公开数据集与评测代码。</li>
</ul>
</li>
<li><p>建立高可靠自动评测协议</p>
<ul>
<li>采用“LLM-as-a-Judge”矩阵：针对每类指令挑选<strong>专精评判模型</strong> + 优化 prompt 模板 + 引入人类校验，将评判准确率从 88 % 提升到 98 %。</li>
<li>通过细粒度 0/1 打分，直接度量模型<strong>是否成功抑制训练偏好</strong>并执行反直觉约束。</li>
</ul>
</li>
</ol>
<p>通过以上诊断 pipeline，论文实现以下“可落地”输出：</p>
<ul>
<li>公开可用的 Inverse IFEval 基准（含 1012 样本、评测脚本、 judge 模型配置）；</li>
<li>15 个主流模型的大规模“体检报告”，明确揭示认知惯性的普遍性与思考机制的部分缓解效果；</li>
<li>为后续对齐研究提供<strong>可量化目标</strong>：在保留常规能力的同时，把 Inverse IFEval 分数作为额外优化指标，即可系统性地降低模型对训练惯例的过度依赖。</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>实验部分（§3 及附录 B、C）围绕 <strong>“诊断认知惯性”</strong> 展开，共 4 组互相关联的实验，全部在 Inverse IFEval 的 1 012 条中英双语样本上进行。结果以 <strong>LLM-as-a-Judge 0/1 打分</strong> 为主指标，辅以 Best-of-N、细粒度类型得分和人类错误案例分析。</p>
<p>1. 主实验：15 个模型整体表现对比</p>
<p>设置</p>
<p>要点</p>
<p>模型池</p>
<p>闭源 10 款（o3-high、GPT-5-high、Gemini-2.5-pro 等）+ 开源 8 款（Qwen3、DeepSeek-R1、GLM-4.5 等）</p>
<p>指标</p>
<p>英文 / 中文双版本 Overall Score（%）</p>
<p>关键结论</p>
<p>• o3-high 居首（≈76 %），但绝对分数仍低，说明基准足够挑战。<br>• 同系列“思考版”&gt;“非思考版”，参数量越大越好，验证了认知惯性的普遍性。</p>
<p>2. 消融实验：思考机制到底有多大用？</p>
<p>子实验</p>
<p>操作</p>
<p>结果</p>
<p>2.1 Qwen3 系列对比</p>
<p>Thinking vs Non-thinking 模式</p>
<p>思考模式平均提升 12–18 分，证实 <strong>显式推理可部分抑制 SFT 偏好</strong>。</p>
<p>2.2 AdaCoT 自动触发</p>
<p>Doubao-Seed-1.6 三档：强制思考 / 强制不思考 / 自动触发</p>
<p>自动模式在中文场景竟 <strong>低于</strong> 强制不思考，提示 <strong>自适应触发策略仍需针对语言特化</strong>。</p>
<p>3. 细粒度实验：8 类反直觉指令的模型弱点画像</p>
<p>分析维度</p>
<p>发现</p>
<p>类型级得分</p>
<p>所有模型在 <strong>Question Correction</strong> 上最差（中位数 <30），在 **Counterfactual Answering** 上最好（>65）。<br>说明“指出题目错误”对训练惯例冲击最大。</p>
<p>模型级弱点</p>
<p>• DeepSeek-R1 在 Intentional Textual Flaws 与 Counter-Conventional Formatting 显著落后。<br>• GPT-OSS-120B 在 Deliberately Incorrect Answers 骤降 40 分，暴露其自我监督失效。</p>
<p>4. 对比实验：Inverse IFEval vs 常规 IFEval</p>
<p>对比方式</p>
<p>同一模型在两套基准上的排名差</p>
<p>结果</p>
<p>• o3-high、Gemini-2.5-pro 排名持平，显示<strong>真正鲁棒</strong>。<br>• 非思考模型普遍 <strong>Inverse 排名暴跌</strong>（Qwen3-235B-A22B-Instruct 从第 5 → 第 15），证明<strong>常规基准不足以暴露认知惯性</strong>。</p>
<p>5. 测试时缩放实验（附录 B.1）</p>
<p>协议</p>
<p>Best-of-N 采样（N=1/16/32）</p>
<p>结论</p>
<p>5 款模型随 N 增大单调提升，N=32 时平均逼近 90 分，表明<strong>基准仍位于模型能力范围内</strong>，<strong>后天对齐</strong>即可显著改善，而非必须改架构。</p>
<p>6. 跨语言一致性实验（附录 B.2）</p>
<p>操作</p>
<p>同一模型在英文/中文双版本得分差</p>
<p>结论</p>
<p>整体 Pearson r ≈ 0.92；GLM-4.5 与 Qwen3-235B-A22B-Thinking 中文显著更高，归因于<strong>训练语料母语优势</strong>。</p>
<p>7. 错误案例剖析（附录 C）</p>
<p>方法</p>
<p>人工抽取 60 条失败样例，按 8 大类型归类</p>
<p>共性错误</p>
<p>1. 滑回“正确性”——明知无答案仍选最接近错项。<br>2. 滑回“可读性”——偷偷插入列表或段落分割。<br>3. 滑回“注释/规范”——代码自动补全注释或变量名。<br>4. 滑回“固定解题模板”——对诱导题仍列方程、求根，忽略题干已给答案。</p>
<p>综上，实验从<strong>整体排名 → 机制消融 → 类型画像 → 基准对比 → 测试时缩放 → 语言一致性 → 错误模式</strong> 七个侧面，系统验证了 Inverse IFEval 的诊断价值，并指出**“思考机制 + 测试时缩放”**可作为后续对齐研究的即时改进抓手。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可直接建立在 Inverse IFEval 的诊断结果之上，分“数据-算法-评测-理论”四条线，供后续研究快速落地。</p>
<p>1. 数据层面：如何低成本扩充“反直觉”信号</p>
<p>探索点</p>
<p>关键问题 / 可能路径</p>
<p>1.1 自动化范式挖掘 + 反向提示</p>
<p>用规则或 LLM 对现有 SFT 语料进行“范式抽取”→ 构建 <strong>反向模板生成器</strong>，实现百万级对抗指令的<strong>无人工扩增</strong>。</p>
<p>1.2 对抗数据课程学习</p>
<p>按“违背强度”给 Inverse 样本排序，<strong>渐进式混入</strong>常规 SFT，观察模型何时出现<strong>灾难性遗忘</strong>或<strong>顿悟式提升</strong>，寻找最佳混入比例与节奏。</p>
<p>1.3 多模态反向指令</p>
<p>将 8 类范式延伸至图像、音频、视频（如“生成一张故意违反透视规律的图”），验证认知惯性是否<strong>跨模态迁移</strong>。</p>
<p>2. 算法层面：如何抑制训练惯例而不伤常规能力</p>
<p>探索点</p>
<p>关键问题 / 可能路径</p>
<p>2.1 参数高效“遗忘”方法</p>
<p>用 <strong>LoRA / DoRA + 梯度上升</strong> 对 SFT 模型进行<strong>选择性遗忘</strong>（unlearning）：只削弱与 8 类范式相关的参数方向，再对齐 Inverse 数据，实现<strong>外科式纠偏</strong>。</p>
<p>2.2 思考预算动态分配</p>
<p>借鉴 AdaCoT 思路，训练一个<strong>轻量级路由器</strong>，在推理时判断“是否遇到反直觉指令”，仅在必要时触发长链思考，降低 90% 常规场景开销。</p>
<p>2.3 对比式强化学习</p>
<p>把 Inverse 样本构造成 <strong>pair-wise 偏好</strong>：同一指令下“遵循反向约束 vs 滑回惯例”作为正负例，用 DPO / KTO 直接优化<strong>抑制惯性</strong>的策略。</p>
<p>3. 评测层面：如何更细粒度、更真实地度量“反认知能力”</p>
<p>探索点</p>
<p>关键问题 / 可能路径</p>
<p>3.1 在线“用户故意找茬”场景</p>
<p>与对话平台合作，<strong>实时收集</strong>真实用户提出的长尾、反常规请求（如“用 10 个错别字写情书”），建立<strong>动态增长的 Living Inverse 基准</strong>，避免静态数据集过拟合。</p>
<p>3.2 多轮惯性追踪</p>
<p>在 Multi-turn 设置下，<strong>中途插入反向约束</strong>（如“从现在开始每句话故意加错字”），用<strong>惯性衰减曲线</strong>衡量模型<strong>持续抑制</strong>能力，而非单次得分。</p>
<p>3.3 可验证自动度量</p>
<p>对 8 类任务开发<strong>确定性 checker</strong>（正则、编译器、定理证明器），减少对 LLM-as-a-Judge 的 98 % 准确率依赖，实现<strong>完全自动化</strong>的 nightly regression。</p>
<p>4. 理论层面：认知惯性的本质与边界</p>
<p>探索点</p>
<p>关键问题 / 可能路径</p>
<p>4.1 惯例强度量化</p>
<p>用 <strong>梯度/激活相似度</strong> 定义“惯例强度”：测量同一指令在常规 vs 反向条件下，隐藏表示的漂移距离，建立<strong>惯性系数</strong>与<strong>参数范数/训练步数</strong>的定量关系。</p>
<p>4.2 任务复杂度-惯性阈值</p>
<p>系统扰动 8 类任务的<strong>约束复杂度</strong>（如错别字数量、错误答案比例），拟合<strong>临界曲线</strong>，找出模型<strong>可承受的最大反直觉强度</strong>，为后续课程学习提供理论上限。</p>
<p>4.3 与人类认知对比</p>
<p>招募人类标注员完成 Inverse 任务，记录<strong>反应时与错误模式</strong>，验证 LLM 的认知惯性是否与人类<strong>启发式偏差</strong>同源，为“机器 unlearning”与“人类去偏”提供统一框架。</p>
<p>5. 系统层面：如何把“反直觉鲁棒”部署到生产</p>
<p>探索点</p>
<p>关键问题 / 可能路径</p>
<p>5.1 轻量级 guardrail</p>
<p>在推理侧部署<strong>二分类小模型</strong>，实时检测用户请求是否属于 8 类反直觉模式，<strong>动态切换</strong>至“思考增强”或“惯例抑制”分支，兼顾延迟与准确率。</p>
<p>5.2 联邦 unlearning</p>
<p>对端侧小模型（3–7 B）用<strong>联邦学习 + 本地反向数据</strong>进行惯例遗忘，仅上传梯度差异，解决<strong>数据隐私</strong>与<strong>个性化反直觉</strong>需求（如企业内网代码无注释规范）。</p>
<p>综上，后续工作可沿“<strong>自动扩数据 → 参数级遗忘 → 动态思考 → 在线评测 → 理论量化 → 系统落地</strong>”六步闭环，逐步把 Inverse IFEval 从诊断工具升级为<strong>可生产的鲁棒对齐方案</strong>。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文主旨</p>
<p>诊断并量化大语言模型（LLM）在<strong>反直觉指令</strong>场景下的<strong>认知惯性</strong>（cognitive inertia），为后续对齐研究提供可操作的基准与改进方向。</p>
<p>核心内容速览</p>
<p>维度</p>
<p>关键结果</p>
<p><strong>问题</strong></p>
<p>监督微调（SFT）使模型过度内化“答案必须正确、格式必须美观、代码必须注释”等理想范式，导致用户提出<strong>与训练惯例冲突</strong>的长尾需求时，模型顽固滑回旧分布。</p>
<p><strong>基准</strong></p>
<p>构建 <strong>Inverse IFEval</strong>（1 012 条中英双语样本，8 类对抗指令），首次系统地把“<strong>反认知能力</strong>”量化成 0/1 指标。</p>
<p><strong>实验</strong></p>
<p>15 个主流模型（o3-high、GPT-5、Gemini-2.5、Qwen3、DeepSeek-R1 等）整体得分普遍 &lt; 76；非思考模型排名暴跌，<strong>思考机制 + 测试时缩放</strong>可部分缓解但仍远未解决。</p>
<p><strong>诊断</strong></p>
<p>提供细粒度“惯性画像”：所有模型在 <strong>Question Correction</strong> 上最差（中位数 &lt;30），在 <strong>Counter-Conventional Formatting</strong> 与 <strong>Intentional Textual Flaws</strong> 上普遍失败；人类错误案例分析归纳出 4 类共性滑回模式。</p>
<p><strong>贡献</strong></p>
<p>① 提出“反认知能力”评估维度；② 发布可复现的 Inverse IFEval 数据集与 98 % 准确率的 LLM-as-a-Judge 协议；③ 揭示现有对齐方法在分布外指令上的显著盲区，为后续<strong>遗忘-强化混合对齐</strong>提供量化目标。</p>
<p>一句话总结</p>
<p>Inverse IFEval 像一台“<strong>认知惯性 X 光机</strong>”，首次把 LLM 难以忘却训练惯例的顽疾量化、定位、公开，为打造<strong>真正随用户而非随训练分布</strong>的可靠模型奠定基准与路线图。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Inverse%20IFEval%3A%20Can%20LLMs%20Unlearn%20Stubborn%20Training%20Conventions%20to%20Follow%20Real%20Instructions%3F，链接是  https://arxiv.org/pdf/2509.04292  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04292  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Qinyan Zhang,Xinping Lei,Ruijie Miao,Yu Fu,Haojie Fan,Le Chang,Jiafan Hou,Dingling Zhang,Zhongfei Hou,Ziqiang Yang,Changxin Pu,Fei Hu,Jingkai Liu,Mengyun Liu,Yang Liu,Xiang Gao,Jiaheng Liu,Tong Yang,Zaiyuan Wang,Ge Zhang,Wenhao Huang</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04292.pdf">https://arxiv.org/pdf/2509.04292.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04292">https://arxiv.org/abs/2509.04292</a></p>
<p><strong>Arxiv ID</strong>: 2509.04292</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04292">https://papers.cool/arxiv/2509.04292</a></p>
<p><strong>Published</strong>: 2025-09-04T15:03:02Z</p>
<p><strong>Updated</strong>: 2025-09-04T15:03:02.000Z</p>
<hr>
<h3 id="6-Transition-Models-Rethinking-the-Generative-Learning-Objective"><a href="#6-Transition-Models-Rethinking-the-Generative-Learning-Objective" class="headerlink" title="6. Transition Models: Rethinking the Generative Learning Objective"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04394">Transition Models: Rethinking the Generative Learning Objective</a></h3><p>A fundamental dilemma in generative modeling persists: iterative diffusion models achieve outstanding fidelity, but at a significant computational cost, while efficient few-step alternatives are constrained by a hard quality ceiling. This conflict between generation steps and output quality arises from restrictive training objectives that focus exclusively on either infinitesimal dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by introducing an exact, continuous-time dynamics equation that analytically defines state transitions across any finite time interval. This leads to a novel generative paradigm, Transition Models (TiM), which adapt to arbitrary-step transitions, seamlessly traversing the generative trajectory from single leaps to fine-grained refinement with more steps. Despite having only 865M parameters, TiM achieves state-of-the-art performance, surpassing leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across all evaluated step counts. Importantly, unlike previous few-step generators, TiM demonstrates monotonic quality improvement as the sampling budget increases. Additionally, when employing our native-resolution strategy, TiM delivers exceptional fidelity at resolutions up to 4096x4096.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>生成建模中的一个基本困境仍然存在：迭代扩散模型实现了卓越的保真度，但代价是显著的计算成本，而高效的少步骤替代方案则受到严格的质量上限的限制。这种生成步骤与输出质量之间的冲突源于仅专注于无穷小动态（PF-ODE）或直接端点预测的限制性训练目标。我们通过引入一个精确的连续时间动态方程来解决这一挑战，该方程在任何有限时间区间内解析地定义状态转变。这导致了一种新颖的生成范式，即过渡模型（TiM），它适应任意步长的转变，能无缝地从单次跃迁过渡到更多步骤的细致精细化。尽管只有865M参数，TiM在所有评估的步数中取得了领先的表现，超越了SD3.5（8B参数）和FLUX.1（12B参数）等领先模型。重要的是，与之前的少步生成器不同，TiM在抽样预算增加时表现出单调的质量改善。此外，使用我们本地分辨率策略时，TiM在高达4096x4096的分辨率下提供了卓越的保真度。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决<strong>生成式模型中长期存在的“步数-质量权衡”困境</strong>：</p>
<ul>
<li><strong>扩散模型</strong>通过多步迭代去噪可获得极高保真度，但计算成本高昂（高 NFE）。</li>
<li><strong>少步模型</strong>（一致性模型、蒸馏方法、shortcut/flow-map 等）虽显著降低步数，却存在<strong>质量天花板</strong>：<ol>
<li>继续增加采样步数几乎不再提升质量，甚至退化；</li>
<li>对步长/调度敏感，缺乏可扩展的“渐进式精修”能力。</li>
</ol>
</li>
</ul>
<p>根本原因在于现有训练目标的<strong>粒度受限</strong>：</p>
<ul>
<li>要么仅监督<strong>瞬时动力学</strong>（PF-ODE 局部场），导致大步长离散化误差爆炸；</li>
<li>要么直接学习<strong>固定端点映射</strong>（单步/少步），丢失细粒度动态，无法利用额外计算。</li>
</ul>
<p>为此，论文提出<strong>Transition Models (TiM)</strong>，核心思想是：</p>
<blockquote>
<p><strong>让模型直接学习任意时间间隔 Δt 的状态转移</strong>，即一次性掌握从任意噪声状态 (x_t) 到任意前一状态 (x_{t-Δt}) 的<strong>解析转移算子</strong>。<br>这样，模型不再近似微分方程或统计映射，而是<strong>学习整个生成轨迹的解流形</strong>，从而：</p>
<ul>
<li>单步即可生成高质量样本；</li>
<li>增加步数可<strong>单调提升</strong>质量，无饱和；</li>
<li>对步长/调度不敏感，实现“任意步采样”。</li>
</ul>
</blockquote>
<p>简言之，TiM 统一了“少步高效”与“多步高保真”，打破了传统扩散与少步方法的对立。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究按主题分组，均与 TiM 的核心动机——<strong>扩散/流匹配模型的训练目标、少步生成、一致性/蒸馏方法</strong>——密切相关。</p>
<p>1. 扩散与流匹配基础</p>
<ul>
<li><p><strong>DDPM / DDIM / SDE-ODE 框架</strong><br>[31] Ho et al., <em>Denoising Diffusion Probabilistic Models</em>, NeurIPS 2020<br>[66] Song et al., <em>Denoising Diffusion Implicit Models</em>, arXiv 2020<br>[69] Song et al., <em>Score-based Generative Modeling through SDEs</em>, ICLR 2021<br>[35] Karras et al., <em>Elucidating the Design Space of Diffusion Models</em>, NeurIPS 2022</p>
</li>
<li><p><strong>Rectified Flow / OT-FM</strong><br>[43] Lipman et al., <em>Flow Matching for Generative Modeling</em>, arXiv 2022<br>[44] Liu et al., <em>Flow Straight and Fast</em>, ICLR 2023</p>
</li>
</ul>
<p>2. 少步生成与一致性模型</p>
<ul>
<li><p><strong>Consistency Models (CM)</strong><br>[70] Song et al., <em>Consistency Models</em>, ICML 2023<br>[45] Lu &amp; Song, <em>Simplifying Continuous-time Consistency Models</em>, arXiv 2024</p>
</li>
<li><p><strong>Consistency Trajectory / Phased Consistency</strong><br>[37] Kim et al., <em>Consistency Trajectory Models</em>, arXiv 2023<br>[77] Wang et al., <em>Phased Consistency Models</em>, NeurIPS 2024</p>
</li>
<li><p><strong>Shortcut / MeanFlow / FlowMap</strong><br>[21] Frans et al., <em>One-step Diffusion via Shortcut Models</em>, arXiv 2024<br>[26] Geng et al., <em>Mean Flows for One-step Generative Modeling</em>, arXiv 2025<br>[57] Sabour et al., <em>Align Your Flow</em>, arXiv 2025</p>
</li>
</ul>
<p>3. 蒸馏与加速采样</p>
<ul>
<li><p><strong>Adversarial Distillation</strong><br>[61, 62] Sauer et al., <em>Adversarial Diffusion Distillation</em> (ADD / SDXL-Turbo), ECCV 2024 / SIGGRAPH Asia 2024</p>
</li>
<li><p><strong>Distribution Matching Distillation (DMD)</strong><br>[84, 85] Yin et al., <em>One-step Diffusion with DMD</em>, CVPR 2024 / NeurIPS 2024</p>
</li>
<li><p><strong>Latent Consistency Models (LCM) &amp; Hyper-SD</strong><br>[48] Luo et al., <em>Latent Consistency Models</em>, arXiv 2023<br>[55] Ren et al., <em>Hyper-SD</em>, NeurIPS 2024</p>
</li>
</ul>
<p>4. 高效/高分辨率扩散架构</p>
<ul>
<li><p><strong>DiT / SiT / MDT / FlowDCN</strong><br>[52] Peebles &amp; Xie, <em>Scalable Diffusion Models with Transformers</em>, ICCV 2023<br>[49] Ma et al., <em>SiT: Scalable Interpolant Transformers</em>, arXiv 2024<br>[23, 24] Gao et al., <em>Masked Diffusion Transformer</em>, arXiv 2023</p>
</li>
<li><p><strong>Native-Resolution Training</strong><br>[20] Esser et al., <em>Scaling Rectified Flow Transformers</em>, 2024<br>[80] Wang et al., <em>Native-Resolution Image Synthesis</em>, arXiv 2025</p>
</li>
</ul>
<p>5. 训练技巧与数值优化</p>
<ul>
<li><p><strong>FlashAttention / FSDP 兼容训练</strong><br>[17] Dao, <em>FlashAttention-2</em>, arXiv 2023<br>[89] Zhao et al., <em>PyTorch FSDP</em>, arXiv 2023</p>
</li>
<li><p><strong>Jacobian-Free Time-Derivative Approximation</strong><br>TiM 的 DDE 近似与 [26, 53] 的 JVP 方法形成对比，解决了大模型扩展性瓶颈。</p>
</li>
</ul>
<p>6. 评估基准</p>
<ul>
<li><strong>GenEval / MJHQ-30K / DPGBench</strong><br>[27] Ghosh et al., <em>GenEval</em>, NeurIPS 2023<br>[20] Esser et al., <em>MJHQ-30K</em><br>[5, 6] Black-Forest-Labs, <em>FLUX.1</em> 系列</li>
</ul>
<p>小结</p>
<p>TiM 在训练目标上<strong>统一了扩散、一致性、shortcut、MeanFlow 等方法的视角</strong>；在工程实现上<strong>兼容 FlashAttention/FSDP</strong>，可直接从 0 训练 865 M 参数模型；在效果上<strong>超越 SD3.5-Large (8 B) 与 FLUX.1-Dev (12 B)</strong>，成为首个同时支持<strong>任意步采样 + 单调质量提升</strong>的生成模型。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过提出 <strong>Transition Models (TiM)</strong> 框架，从<strong>训练目标、数值实现、网络架构</strong>三个层面系统性地解决了“步数-质量权衡”问题。核心思路是：</p>
<blockquote>
<p><strong>让模型一次性学会“任意时间间隔 Δt 的状态转移”</strong>，从而把“少步高效”与“多步精修”统一为同一套参数。</p>
</blockquote>
<p>1. 训练目标：从局部 PF-ODE 到全局状态转移</p>
<ul>
<li><p><strong>问题根源</strong><br>传统扩散模型仅监督瞬时 PF-ODE 场，导致大步长离散化误差；少步模型只学固定端点映射，无法利用额外步数。</p>
</li>
<li><p><strong>TiM 解决方案</strong></p>
<ol>
<li><p><strong>State Transition Identity</strong>（式 8）<br>将 PF-ODE 轨迹上的任意两点 ((x_t, x_r)) 用解析恒等式关联：<br>[ \frac{d}{dt}\Bigl[B_{t,r}\bigl(\hat\alpha_t x + \hat\sigma_t\varepsilon - f_\theta(x_t,t,r)\bigr)\Bigr]=0 ]<br>该恒等式要求<strong>加权残差的时间导数也为零</strong>，从而强制：</p>
<ul>
<li><strong>轨迹一致性</strong>：任意中间步组合等价于直接跳跃；</li>
<li><strong>时间斜率匹配</strong>：模型不仅拟合值，还拟合其瞬时变化率，保证大 Δt 也不失真。</li>
</ul>
</li>
<li><strong>训练目标</strong>（式 13）<br>用上述恒等式导出动态目标 (\hat f)，并通过区间加权 (w(t,r)) 抑制大 Δt 带来的梯度方差：<br>[ \mathcal L(\theta)=\mathbb E_{x,\varepsilon,t,r}\Bigl[w(t,r),\bigl|f_\theta(x_t,t,r)-\hat f\bigr|^2\Bigr] ]</li>
</ol>
</li>
</ul>
<p>2. 数值实现：DDE 替代 JVP，兼容 FlashAttention/FSDP</p>
<ul>
<li><p><strong>障碍</strong><br>计算网络时间导数 (\frac{df_\theta}{dt}) 的经典 Jacobian-Vector Product (JVP) 与 FlashAttention、FSDP 不兼容，训练 &gt;1B 参数模型不可行。</p>
</li>
<li><p><strong>TiM 解决方案：Differential Derivation Equation (DDE)</strong><br>用中心差分近似：<br>[ \frac{df_\theta}{dt}\approx\frac{f_\theta(x_{t+\epsilon},t+\epsilon,r)-f_\theta(x_{t-\epsilon},t-\epsilon,r)}{2\epsilon} ]</p>
<ul>
<li><strong>纯前向计算</strong>，与 FlashAttention/FSDP 完全兼容；</li>
<li><strong>2× 加速</strong>（表 1），使 865 M 模型 16×A100 30 天即可完成从零训练。</li>
</ul>
</li>
</ul>
<p>3. 网络架构：让模型“感知”Δt 的大小</p>
<ul>
<li><p><strong>Decoupled Time &amp; Interval Embedding</strong><br>用两套独立编码器分别编码绝对时间 (t) 与间隔 (\Delta t=t-r)，再相加：<br>[ E_{t,\Delta t}=\phi_t(t)+\phi_{\Delta t}(\Delta t) ]<br>使网络能区分“当前噪声水平”与“要跳多远”。</p>
</li>
<li><p><strong>Interval-Aware Attention</strong><br>在 Q/K/V 投影中显式注入 (\Delta t) 嵌入：<br>[ q=zW_q+E_{\Delta t}W’<em>q,\quad k=zW_k+E</em>{\Delta t}W’<em>k,\quad v=zW_v+E</em>{\Delta t}W’_v ]<br>大 Δt 时偏向全局结构调整，小 Δt 时偏向局部细节精修。</p>
</li>
</ul>
<p>4. 训练策略：稳定收敛</p>
<ul>
<li><strong>区间加权</strong>（式 12）<br>采用 (\tan) 变换把时间轴拉伸，并给短区间更高权重，抑制大 Δt 导致的梯度爆炸：<br>[ w(t,r)=\bigl(\sigma_{\text{data}}+\tan t-\tan r\bigr)^{-1/2} ]</li>
</ul>
<p>5. 结果：统一少步与多步</p>
<ul>
<li><strong>单步 (1-NFE)</strong>：GenEval 0.67，已超 SDXL-Turbo 8-NFE 0.50。</li>
<li><strong>128-NFE</strong>：GenEval 0.83，逼近或超过 SD3.5-Large 0.70、FLUX.1-Dev 0.65，且<strong>随步数单调提升</strong>。</li>
<li><strong>任意分辨率</strong>：原生分辨率训练下，4096×4096 仍保持高质量（表 6、图 6）。</li>
</ul>
<p>一句话总结</p>
<p>TiM 通过<strong>解析状态转移恒等式 + 可扩展数值实现 + Δt 感知架构</strong>，首次让单个模型同时具备</p>
<ul>
<li><strong>1 步即可用</strong></li>
<li><strong>N 步可精修</strong></li>
<li><strong>训练从零开始无需蒸馏</strong><br>三重能力，从根本上化解了扩散模型的步数-质量矛盾。</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>“任意步采样 + 单调质量提升”</strong> 的核心卖点，在 <strong>文本到图像生成</strong> 与 <strong>类别条件生成</strong> 两大任务、多个维度上进行了系统实验，可概括为 <strong>5 组定量基准 + 4 组消融 + 2 组定性可视化</strong>。</p>
<p>1. 文本到图像（T2I）主实验</p>
<p>数据集</p>
<p>指标</p>
<p>对比模型</p>
<p>关键结论</p>
<p><strong>GenEval</strong>（综合对齐）</p>
<p>0.67@1-NFE → 0.83@128-NFE</p>
<p>SD3.5-Large(8B)、FLUX.1-Dev(12B)、SDXL-Turbo 等</p>
<p>TiM-865M <strong>单步击败所有 8-NFE 蒸馏模型</strong>，128-NFE 超越所有开源模型且<strong>单调上升</strong>（表2、表5）。</p>
<p><strong>MJHQ-30K</strong>（FID）</p>
<p>5.25@128-NFE</p>
<p>SDXL、SD3.5、FLUX.1-Schnell</p>
<p>TiM 取得 <strong>SOTA FID</strong>，8-NFE 已优于 100-NFE SDXL（表3）。</p>
<p><strong>DPGBench</strong>（细粒度对齐）</p>
<p>81.3@8-NFE</p>
<p>同上</p>
<p>8-NFE TiM &gt; 100-NFE 基线，验证效率-质量双赢（表3）。</p>
<p><strong>分辨率 &amp; 长宽比</strong></p>
<p>最高 4096×4096</p>
<p>SD3.5-Turbo、FLUX.1-Schnell</p>
<p>TiM <strong>唯一</strong>在 4096×4096 仍保持可用分数（表6、图6）。</p>
<p>2. 类别条件（ImageNet）实验</p>
<p>分辨率</p>
<p>指标</p>
<p>对比模型</p>
<p>关键结论</p>
<p><strong>256×256</strong></p>
<p>FID 1.65@250-NFE / 3.26@1-NFE</p>
<p>DiT-XL、SiT-XL、MeanFlow-XL、BigGAN、StyleGAN-XL 等</p>
<p>TiM-XL <strong>1-NFE 与 GAN 相当</strong>，250-NFE 优于所有扩散基线（表13）。</p>
<p><strong>512×512</strong></p>
<p>FID 1.69@250-NFE / 5.07@1-NFE</p>
<p>同上</p>
<p>在更高分辨率仍保持<strong>任意步优势</strong>（表14）。</p>
<p>3. 消融实验</p>
<p>维度</p>
<p>设置</p>
<p>结论</p>
<p><strong>训练目标</strong></p>
<p>SiT vs TiM(JVP) vs TiM(DDE)</p>
<p>TiM 目标 1-NFE FID 从 309.5→49.91；DDE 比 JVP <strong>2× 加速且可扩展</strong>（表4）。</p>
<p><strong>架构改进</strong></p>
<p>① 解耦时间嵌入 ② 区间感知注意力</p>
<p>二者叠加 8-NFE FID 再降 3.87；缺一不可（表4）。</p>
<p><strong>训练策略</strong></p>
<p>区间加权 vs 无加权</p>
<p>加权后 1/8/50-NFE FID 再降至 47.46/24.62/17.10（表4）。</p>
<p><strong>超参数</strong></p>
<p>ϵ、时间变换、传输类型</p>
<p>ϵ=0.005、tan 变换、OT-FM 传输最优（表9-12）。</p>
<p>4. 定性结果</p>
<ul>
<li><strong>图 1 &amp; 图 3</strong>：展示 TiM 在不同 NFE 下文本-图像对齐、细节保真的<strong>渐进提升</strong>，对比模型出现“低步不可用 / 高步过饱和”。</li>
<li><strong>图 6</strong>：提供 1024×4096、2560×1024、4096×4096 等高分辨率、多长宽比样例，验证<strong>原生分辨率训练</strong>的泛化能力。</li>
</ul>
<p>5. 实验规模一览</p>
<ul>
<li><strong>T2I</strong>：865 M 参数，16×A100，30 天，33 M 图文对，<strong>从零训练</strong>。</li>
<li><strong>ImageNet</strong>：664 M 参数，512×512 训练 750 K 步，batch 512。</li>
</ul>
<p>一句话总结</p>
<p>论文通过 <strong>跨任务、跨分辨率、跨步数</strong> 的系统性实验，证明 TiM 在 <strong>任意步采样场景下同时实现 SOTA 质量与训练-推理效率</strong>，并彻底消融验证了每个设计选择的必要性。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为 TiM 框架的后续探索，按<strong>理论深化、工程扩展、应用落地、风险治理</strong>四个层面展开。</p>
<p>1. 理论深化</p>
<ul>
<li><strong>连续-时间泛化误差界</strong><br>目前仅给出经验收敛曲线，可尝试用 <em>neural ODE generalization</em> 或 <em>consistency regularization</em> 理论给出任意 Δt 的泛化上界，解释为何单调提升无饱和。</li>
<li><strong>最优传输视角</strong><br>TiM 恒等式与 OT-FM 高度耦合，可研究<br>– 不同成本函数（ℓ₂ vs ℓ₁ vs 感知度量）对转移算子的影响；<br>– 将 TiM 视为 <em>dynamic optimal transport</em> 的离散求解器，推导收敛率。</li>
<li><strong>随机微分扩展</strong><br>当前 TiM 主要基于 PF-ODE，可探讨<br>– 将 State Transition Identity 推广到 <em>reverse-time SDE</em>，实现可控随机性；<br>– 设计 <em>interval-aware noise schedule</em>，在少步时保持多样性、多步时降低方差。</li>
</ul>
<p>2. 工程扩展</p>
<ul>
<li><strong>更大规模预训练</strong><br>865 M → 8 B/12 B/20 B 参数，验证 DDE 在千卡级 FSDP 下的线性扩展性；同时探索 <strong>MoE-TiM</strong>，用稀疏激活进一步降低训练成本。</li>
<li><strong>多模态统一</strong><br>将 Δt 条件机制迁移到<br>– 视频（TiM-Video）：把 Δt 视为帧间隔；<br>– 3D 生成（TiM-Nerf/Gaussian）：把 Δt 视为渲染步长；<br>– 音频-文本-图像统一流模型。</li>
<li><strong>硬件协同优化</strong><br>– 将 DDE 差分计算写入 CUDA kernel，减少两次前向的显存峰值；<br>– 与 <strong>TensorRT/ONNX</strong> 合作，实现可变步长推理引擎，支持客户端动态调整 NFE。</li>
</ul>
<p>3. 应用落地</p>
<ul>
<li><strong>可控生成</strong><br>在 TiM 框架内引入 <strong>layout / depth / pose / text-region</strong> 等显式条件，验证 Δt 感知注意力能否在少步时仍保持精准控制。</li>
<li><strong>个性化微调</strong><br>研究 <strong>LoRA-TiM</strong>：仅微调 Δt 嵌入与区间感知注意力权重，实现单张 GPU 5 分钟个性化风格化。</li>
<li><strong>实时交互场景</strong><br>结合 <strong>progressive decoding</strong>（先低分辨率少步，再局部高分辨率多步），在 1 秒内完成 1024×1024 可交互预览，随后按需精修。</li>
</ul>
<p>4. 风险治理</p>
<ul>
<li><strong>高分辨率安全性</strong><br>4096×4096 下出现文字/手结构失真，可<br>– 构建高分辨率-细粒度配对数据，重训 VAE；<br>– 引入 <strong>perceptual discriminator</strong> 做对抗精修。</li>
<li><strong>内容安全与版权</strong><br>研究 <strong>interval-aware safety filter</strong>：在 Δt 较大时提前注入安全先验，避免少步生成敏感内容。</li>
<li><strong>评估维度扩展</strong><br>当前主要用 FID/GenEval，可补充<br>– <strong>human preference</strong> 在 1-NFE vs 128-NFE 的 MOS 测试；<br>– <strong>downstream editing</strong> 任务（inpainting, style transfer）验证 TiM 隐空间可编辑性。</li>
</ul>
<p>一句话展望</p>
<p>TiM 把“任意步采样”从工程技巧上升为<strong>可学习的连续算子</strong>，后续可沿着 <strong>更大规模、更多模态、更强控制、更低风险</strong> 四个轴持续放大其潜力。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文一句话总结</p>
<p>提出 <strong>Transition Models (TiM)</strong>，用“任意时间间隔 Δt 的状态转移”统一训练目标，使 <strong>单个 865 M 模型</strong> 既能 <strong>1 步生成高保真图像</strong>，又能 <strong>随步数单调提升质量</strong>，在文本到图像与类别条件任务上全面超越 8 B–12 B 级扩散/蒸馏模型。</p>
<p>核心贡献</p>
<ol>
<li><p><strong>理论：State Transition Identity</strong><br>推导出 PF-ODE 轨迹上的解析恒等式，将训练从“局部微分场”升级为“全局转移算子”，天然支持任意步采样。</p>
</li>
<li><p><strong>工程：DDE + 架构改造</strong></p>
<ul>
<li><strong>Differential Derivation Equation (DDE)</strong>：前向差分近似时间导数，兼容 FlashAttention/FSDP，训练 865 M 模型 16×A100 30 天完成。</li>
<li><strong>解耦时间/区间嵌入 + 区间感知注意力</strong>：显式编码 Δt，实现大步全局重构、小步局部精修。</li>
</ul>
</li>
<li><p><strong>实验：跨任务、跨分辨率、跨步数 SOTA</strong></p>
<ul>
<li><strong>文本到图像</strong>：GenEval 0.67@1-NFE→0.83@128-NFE，MJHQ-30K FID 5.25，4096×4096 仍可用。</li>
<li><strong>ImageNet</strong>：256² FID 1.65@250-NFE / 3.26@1-NFE，512² 同样趋势，全面优于 DiT、SiT、MeanFlow、GAN 等。</li>
<li><strong>消融</strong>：目标、架构、训练策略、超参数逐项验证，缺一不可。</li>
</ul>
</li>
</ol>
<p>意义</p>
<p>首次打破“步数-质量”零和博弈，为 <strong>高效、可扩展、任意步生成</strong> 奠定新范式。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Transition%20Models%3A%20Rethinking%20the%20Generative%20Learning%20Objective，链接是  https://arxiv.org/pdf/2509.04394  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04394  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zidong Wang,Yiyuan Zhang,Xiaoyu Yue,Xiangyu Yue,Yangguang Li,Wanli Ouyang,Lei Bai</p>
<p><strong>Categories</strong>: cs.LG,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04394.pdf">https://arxiv.org/pdf/2509.04394.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04394">https://arxiv.org/abs/2509.04394</a></p>
<p><strong>Arxiv ID</strong>: 2509.04394</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04394">https://papers.cool/arxiv/2509.04394</a></p>
<p><strong>Published</strong>: 2025-09-04T17:05:59Z</p>
<p><strong>Updated</strong>: 2025-09-04T17:05:59.000Z</p>
<hr>
<h3 id="7-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding"><a href="#7-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding" class="headerlink" title="7. Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2508.20478">Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding</a></h3><p>Long-form video understanding, characterized by long-range temporal dependencies and multiple events, remains a challenge. Existing methods often rely on static reasoning or external visual-language models (VLMs), which face issues like complexity and sub-optimal performance due to the lack of end-to-end training. In this paper, we propose Video-MTR, a reinforced multi-turn reasoning framework designed to enable iterative key video segment selection and question comprehension. Unlike traditional video reasoning pipeline, which generate predictions in a single turn, Video-MTR performs reasoning in multiple turns, selecting video segments progressively based on the evolving understanding of previously processed segments and the current question. This iterative process allows for a more refined and contextually aware analysis of the video. To ensure intermediate reasoning process, we introduce a novel gated bi-level reward system, combining trajectory-level rewards based on answer correctness and turn-level rewards emphasizing frame-query relevance. This system optimizes both video segment selection and question comprehension, eliminating the need for external VLMs and allowing end-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU, and EgoSchema demonstrate that Video-MTR outperforms existing methods in both accuracy and efficiency, advancing the state-of-the-art in long video understanding.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>长视频理解，特征包括长距离时间依赖和多个事件，仍然是一个挑战。现有的方法往往依赖于静态推理或外部视觉语言模型（VLM），这些方法面临着复杂性和欠佳性能的问题，因为缺乏端到端的训练。在本文中，我们提出了视频MTR，一种强化的多轮推理框架，旨在实现迭代的关键视频片段选择和问题理解。与传统的视频推理流程不同，后者在单轮中生成预测，视频MTR在多个轮次中进行推理，根据对先前处理片段和当前问题的不断理解逐步选择视频片段。这一迭代过程允许对视频进行更精细和上下文相关的分析。为了确保中间推理过程，我们引入了一种新颖的门控双层奖励系统，结合了基于答案正确性的轨迹级奖励和强调帧查询相关性的轮次级奖励。该系统优化了视频片段选择和问题理解，消除了对外部VLM的需求，并允许端到端的训练。在VideoMME、MLVU和EgoSchema等基准上的广泛实验表明，视频MTR在准确性和效率上均超过现有方法，推动了长视频理解领域的技术进步。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该论文针对 <strong>长视频理解</strong> 这一计算机视觉核心任务，提出并解决以下关键问题：</p>
<ol>
<li><p><strong>长视频中的长程时序依赖与多事件复杂性</strong><br>现有方法多采用“单轮静态推理”或“外部视觉-语言模型（VLM）代理”范式：</p>
<ul>
<li><strong>单轮静态推理</strong> 仅对整段视频做均匀帧采样，一次性给出答案，容易因超长时长而遗漏关键信息。</li>
<li><strong>外部 VLM 代理</strong> 虽可动态检索关键片段，但依赖异构外部模块，系统复杂、缺乏端到端训练，导致次优策略与性能瓶颈。</li>
</ul>
</li>
<li><p><strong>缺乏端到端、可迭代的关键片段选择与问答理解机制</strong><br>论文指出，人类理解长视频时通常“先全局概览→再迭代聚焦关键片段→最后整合证据回答”。现有方法无法在这一过程中做到：</p>
<ul>
<li>在统一模型内部完成 <strong>多轮、迭代式关键片段检索</strong>；</li>
<li>同时优化 <strong>片段选择质量</strong> 与 <strong>最终问答准确率</strong>。</li>
</ul>
</li>
<li><strong>强化学习在长视频多轮推理中的奖励稀疏与信用分配难题</strong><br>传统 RL 仅用最终答案正确性作为奖励，难以指导中间轮次的片段选择；同时，多轮轨迹的信用分配困难，导致训练效率低、数据需求大。</li>
</ol>
<p>综上，论文旨在提出一个 <strong>端到端、可训练的多轮推理框架</strong>，使 MLLM 自身即可在长视频中 <strong>迭代地检索关键片段并回答问题</strong>，并通过精心设计的 <strong>双层奖励机制</strong> 解决上述训练难题，从而在长视频理解任务上取得更高准确率与效率。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>论文在第 2 节“Related Works”中系统回顾了两条主线研究，并指出它们与本文工作的区别与联系。以下按主题归纳：</p>
<p>1. 多模态大语言模型（MLLM）用于视频理解</p>
<ul>
<li><p><strong>均匀采样 + 后训练</strong></p>
<ul>
<li>VideoLLaVA（Lin et al. 2023）、ShareGPT4Video（Chen et al. 2024a）、InternVideo2（Wang et al. 2024b）、VideoR1（Feng et al. 2025）</li>
<li>做法：对整段长视频均匀采样少量帧，通过大规模指令微调提升推理能力。</li>
<li>局限：输入阶段即丢失信息，性能天花板明显。</li>
</ul>
</li>
<li><p><strong>Token 压缩/长上下文扩展</strong></p>
<ul>
<li>LongVA（Zhang et al. 2024）、LLaMA-VID（Li, Wang, and Jia 2024）、Kangaroo（Liu et al. 2024）、VideoXL（Shu et al. 2025）</li>
<li>做法：用 token 压缩或上下文扩展技术一次性处理数小时视频。</li>
<li>局限：冗余信息过多，可解释性差。</li>
</ul>
</li>
<li><p><strong>外部工具代理范式</strong></p>
<ul>
<li>VideoAgent（Wang et al. 2024a）、VideoMemAgent（Fan et al. 2024）、DrVideo（Ma et al. 2025）</li>
<li>做法：把 LLM 作为 Agent，调用外部 VLM、Captioner、Tracker 等工具进行单/多轮检索。</li>
<li>局限：系统复杂、异构组件耦合、缺乏端到端训练，工具调用策略次优。</li>
</ul>
</li>
</ul>
<p>2. 强化学习（RL）在 MLLM 上的探索</p>
<ul>
<li><strong>图像/短视频场景</strong><ul>
<li>VLM-R1（Shen et al. 2025）：将 DeepSeek-R1 范式迁移到视觉任务，提升泛化性。</li>
<li>DeepEyes（Zheng et al. 2025）：通过 RL 鼓励“用图像思考”的多轮推理。</li>
<li>VideoChat-R1（Li et al. 2025）、VideoR1（Feng et al. 2025）：针对短视频时空感知做 RL 微调。</li>
<li>局限：仅聚焦静态图像或短视频，未解决长视频的长程依赖与片段选择问题。</li>
</ul>
</li>
</ul>
<p>与本文工作的关系</p>
<ul>
<li>上述研究要么停留在 <strong>单轮、静态</strong> 范式，要么依赖 <strong>外部 VLM 代理</strong>，均未在统一 MLLM 内部实现 <strong>端到端的多轮、迭代式关键片段检索与问答</strong>。</li>
<li>Video-MTR 首次将 <strong>多轮交互式推理</strong> 与 <strong>端到端强化学习</strong> 结合，通过 <strong>双层奖励机制</strong> 直接优化片段选择质量与最终答案正确性，填补了长视频理解场景中的空白。</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>为攻克长视频理解中的“长程依赖、信息遗漏、训练稀疏”三大难题，论文提出 <strong>Video-MTR：一种基于强化学习的端到端多轮推理框架</strong>。其解决方案可概括为 <strong>“一个框架、两项关键设计、三项配套策略”</strong>。</p>
<p>1. 框架：把长视频问答建模为多轮强化学习任务</p>
<ul>
<li><strong>环境</strong>：长视频本身被视为动态环境，可响应检索动作并返回新帧。</li>
<li><strong>智能体</strong>：以 Qwen2.5-VL-7B 为骨干的 MLLM，充当策略 πθ。</li>
<li><strong>交互协议</strong>：<ul>
<li>每轮状态 sₖ = (最近 w 轮历史 + 当前已观测帧 + 问题)。</li>
<li>动作空间：{retrieve(tₛ, tₑ), answer}。</li>
<li>轨迹 τ = {(Fₖ, xₖ, yₖ)}ₖ₌₀ᴷ，K ≤ K_max。</li>
</ul>
</li>
<li><strong>目标</strong>：最大化期望累积奖励 E_τ∼πθ [R(τ)]，实现片段选择与问答的联合优化。</li>
</ul>
<p>2. 两项关键设计：让 RL 信号既“全局”又“局部”</p>
<p>2.1 双层奖励（Bi-Level Reward）</p>
<p>层级</p>
<p>信号来源</p>
<p>数学表达</p>
<p>作用</p>
<p><strong>Trajectory-level</strong></p>
<p>最终答案正确性</p>
<p>R_acc = 1{answer correct}</p>
<p>保证最终目标</p>
<p><strong>Turn-level</strong></p>
<p>帧-查询相关度提升</p>
<p>Rₖ^fms = 0.5 · 1{IoU(Fₖ,G) &gt; IoU(F₀,G)}</p>
<p>引导中间检索</p>
<p>2.2 目标门控塑形（Goal-Gated Reward Shaping）</p>
<ul>
<li><strong>门控</strong>：仅当 R_acc = 1 时才保留 turn-level 奖励，避免“只检索、不回答”的 reward hacking。</li>
<li><strong>单次奖励约束</strong>：每轨迹最多保留一次检索奖励与一次回答奖励，确保与稀疏终端奖励对齐。</li>
</ul>
<p>最终奖励：  </p>
<script type="math/tex; mode=display">R(\\tau) = R\_{\\text{acc}} + \\mathbb{1}{R\_{\\text{acc}}>0} \\cdot \\max\_k \\bigl(R\_k^{\\text{fms}} + R\_k^{\\text{format}}\\bigr)</script><p>3. 三项配套策略：让训练“高效且可启动”</p>
<p>3.1 高质量监督数据构建</p>
<ul>
<li>从 NExT-GQA 与 QVHighlights 出发，<ul>
<li>保留“相关片段占比 &lt; 0.5”的实例，</li>
<li>用 GPT-4o 将原查询改写为 QA 对，</li>
<li>最终得到 8 K 时空精确标注的小规模但高信噪比数据集。</li>
</ul>
</li>
</ul>
<p>3.2 探索启动（Exploration Bootstrapping）</p>
<ul>
<li>冷启动阶段若 mini-batch 检索率 &lt; 阈值，则对所有 retrieve 动作给予固定奖励（+1.0 → +0.5），直至模型自发多轮检索后自动关闭。</li>
</ul>
<p>3.3 PPO 多轮扩展</p>
<ul>
<li>引入跨轮折扣 γ_turn 与轮内折扣 γ_token，把双层奖励在 token 级反向传播，实现端到端优化。</li>
</ul>
<p>4. 结果验证</p>
<ul>
<li>在 VideoMME、MLVU、EgoSchema 三大基准上，仅用 8 K 数据、32 帧即可达到或超越开源 SOTA，并在超长视频上增益更显著。</li>
<li>消融实验表明：<ul>
<li>去掉多轮 → 复杂任务掉点明显；</li>
<li>去掉双层奖励 → 掉点 2–4 %；</li>
<li>去掉目标门控 → 出现 reward hacking（reward↑ accuracy↓）。</li>
</ul>
</li>
</ul>
<p>综上，Video-MTR 通过 <strong>“多轮交互 + 双层奖励 + 端到端 RL”</strong> 的协同设计，首次在长视频理解中实现了 <strong>可训练、可解释、可扩展</strong> 的迭代式推理框架。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>Video-MTR</strong> 在三大长视频基准（VideoMME、MLVU、EgoSchema）上开展了 <strong>系统性实验</strong>，涵盖主结果、消融、案例与错误分析四个层面。核心实验汇总如下：</p>
<p>1. 主实验：与 SOTA 的全面对比</p>
<p>维度</p>
<p>设置</p>
<p>结果摘要</p>
<p><strong>基准</strong></p>
<p>VideoMME（无字幕）、MLVU-Test、EgoSchema-500</p>
<p>覆盖短视频～2 h 超长视频</p>
<p><strong>对手</strong></p>
<p>① 闭源：GPT-4o、Gemini-1.5-Pro、VideoAgent(GPT-4)…<br>② 开源：Video-LLaVA、VideoChat2、Video-R1、LongVA…</p>
<p>统一报告 M-AVG（多选平均准确率）</p>
<p><strong>配置</strong></p>
<p>7 B 模型、32 帧、3 轮以内</p>
<p>与闭源 300+ 帧、数十 B 参数模型公平对比</p>
<p><strong>主要数字</strong></p>
<ul>
<li><strong>VideoMME</strong>（Overall）：51.0%（开源第一）</li>
<li><strong>MLVU-Test</strong>：48.4%（仅次于 GPT-4o 54.9%，但仅用 32 帧）</li>
<li><strong>EgoSchema</strong>：62.4%（开源第一，无额外 egocentric 数据）</li>
</ul>
<p>2. 消融实验：验证三大核心组件</p>
<p>消融对象</p>
<p>变体说明</p>
<p>结果变化（VideoMME / MLVU）</p>
<p><strong>多轮 vs 单轮</strong></p>
<p>① 原模型（多轮）<br>② 单轮 baseline（同数据、同帧数）</p>
<p>+6.3 pp（Long）/ +4.1 pp（MLVU-Test）</p>
<p><strong>双层奖励</strong></p>
<p>① 保留多轮、去掉 turn-level 奖励<br>② 保留双层奖励</p>
<p>-2.6 pp / -4.0 pp</p>
<p><strong>目标门控</strong></p>
<p>① 保留门控<br>② 去掉门控（无 R_acc 条件）</p>
<p>出现 reward hacking：reward↑ 而 accuracy↓</p>
<p>3. 细粒度诊断：任务复杂度 &amp; 视频长度</p>
<ul>
<li><strong>任务类型（MLVU）</strong><ul>
<li>全局任务：+3.8 pp</li>
<li>单细节任务：+7.5 pp</li>
<li>多细节任务：+8.1 pp<br>→ 复杂度越高，多轮增益越大</li>
</ul>
</li>
<li><strong>视频长度（VideoMME）</strong><ul>
<li>Short：+4.6 pp</li>
<li>Medium：+5.3 pp</li>
<li>Long：+6.3 pp<br>→ 时长越长，多轮优势越明显</li>
</ul>
</li>
</ul>
<p>4. 案例与错误分析</p>
<ul>
<li><strong>成功示例</strong>（图 8）<ul>
<li>角色识别、事件识别、目标推理三类任务均展示“全局→聚焦→确认”三段式检索。</li>
</ul>
</li>
<li><strong>失败示例</strong>（图 9）<ul>
<li><strong>动作顺序</strong>任务：因训练分布偏向 1–3 轮，模型过早停止 → 需更长检索链。</li>
<li><strong>细粒度微动作</strong>任务：帧分辨率不足导致误判 → 需“时-空”分层放大机制。</li>
</ul>
</li>
</ul>
<p>5. 训练数据与效率对比</p>
<p>方法</p>
<p>训练范式</p>
<p>数据量</p>
<p>结果</p>
<p>Video-R1</p>
<p>SFT 260 K + RL 单轮</p>
<p>260 K</p>
<p>48.8%（MLVU）</p>
<p>Video-MTR</p>
<p>纯 RL 多轮</p>
<p>8 K</p>
<p>48.4%（MLVU）</p>
<p>→ 用 <strong>1/30 数据量</strong> 达到相当或更优性能，验证高质量监督 + RL 的效率优势。</p>
<p>6. 可视化分析</p>
<ul>
<li><strong>交互日志</strong>（图 2、6、14）：展示 2–3 轮内如何由稀疏全局帧 → 密集关键片段 → 精准答案。</li>
<li><strong>奖励曲线</strong>（图 4、5）：验证探索启动与门控机制对多轮行为与训练稳定性的影响。</li>
</ul>
<p>综上，实验从 <strong>宏观性能、微观消融、任务诊断、案例可视化</strong> 四个维度系统验证了 Video-MTR 的有效性、必要性与可扩展性。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为 Video-MTR 的直接延伸或长期探索路线，按“易落地 → 需架构升级”递进：</p>
<p>1. 深度检索链：支持 4–6 轮以上的长链推理</p>
<ul>
<li><strong>问题</strong>：当前 3 轮限制导致多事件、动作顺序类任务过早停止。</li>
<li><strong>思路</strong>：<ul>
<li>动态轮次上限：基于置信度或信息增益自适应终止。</li>
<li>课程式训练：在 RL 数据中人工构造需 4–6 轮才能解决的样本，强制模型继续检索。</li>
</ul>
</li>
</ul>
<p>2. 时空分层推理：从“帧级”到“区域级”</p>
<ul>
<li><strong>问题</strong>：细粒度微动作（蘸颜料 vs 混色）因整帧分辨率不足而误判。</li>
<li><strong>思路</strong>：<ul>
<li>二级检索：先用低分辨率帧定位时间段，再对高分辨率裁剪区域做二次视觉编码。</li>
<li>区域-时序交叉注意力：在 MLLM 内部显式建模“何时看哪里”。</li>
</ul>
</li>
</ul>
<p>3. 更长视频：小时级到电影级</p>
<ul>
<li><strong>问题</strong>：32 帧上限在 2 h+ 视频中仍显稀疏。</li>
<li><strong>思路</strong>：<ul>
<li>层级索引：先用 1 fps 关键帧做“章节级”检索，再在章节内做 8 fps 精细检索。</li>
<li>记忆压缩：对历史轮次的帧特征做可学习的 token 压缩，避免上下文爆炸。</li>
</ul>
</li>
</ul>
<p>4. 多模态检索动作：不仅“看帧”，还可“听音”、“读字幕”</p>
<ul>
<li><strong>问题</strong>：当前仅支持视觉帧检索，忽略音频、字幕等模态。</li>
<li><strong>思路</strong>：<ul>
<li>动作空间扩展：<code>retrieve_audio(t1,t2)</code>、<code>retrieve_subtitle(t1,t2)</code>。</li>
<li>奖励设计：增加音频-查询或字幕-查询的 IoU 指标。</li>
</ul>
</li>
</ul>
<p>5. 开放域问答：从多选到自由文本</p>
<ul>
<li><strong>问题</strong>：现有多选答案便于自动评测，但限制表达。</li>
<li><strong>思路</strong>：<ul>
<li>引入生成式评价指标（BERTScore、GPT-as-a-Judge）。</li>
<li>奖励函数改为基于生成答案与参考答案的相似度。</li>
</ul>
</li>
</ul>
<p>6. 在线/流式场景：边播放边推理</p>
<ul>
<li><strong>问题</strong>：当前假设可随机访问任意帧，无法用于实时直播。</li>
<li><strong>思路</strong>：<ul>
<li>滑动窗口检索：每轮只能请求“当前播放点 ± Δt”的帧。</li>
<li>强化学习状态中加入“已播放进度”作为约束。</li>
</ul>
</li>
</ul>
<p>7. 模型规模与效率权衡</p>
<ul>
<li><strong>问题</strong>：7 B 模型在超长视频上仍显容量不足。</li>
<li><strong>思路</strong>：<ul>
<li>专家混合（MoE）：为“检索策略”与“问答推理”分别训练轻量级专家，减少计算。</li>
<li>量化 + 投机解码：保证实时性同时维持多轮交互体验。</li>
</ul>
</li>
</ul>
<p>8. 数据集与评测维度扩展</p>
<ul>
<li><strong>问题</strong>：现有 8 K 训练样本对复杂任务仍不足。</li>
<li><strong>思路</strong>：<ul>
<li>自监督伪标签：用 LLM + 无标注长视频自动生成 QA 与 grounding。</li>
<li>新任务维度：引入“长过程因果推理”、“跨视频事件对比”等评测。</li>
</ul>
</li>
</ul>
<p>9. 可解释性与交互式纠错</p>
<ul>
<li><strong>问题</strong>：用户无法干预检索过程。</li>
<li><strong>思路</strong>：<ul>
<li>人在回路（Human-in-the-loop）：允许用户点击时间轴或输入自然语言提示，实时修正检索区间。</li>
<li>可视化注意力热图：展示每轮检索帧对最终答案的贡献度。</li>
</ul>
</li>
</ul>
<p>10. 跨语言与跨文化长视频理解</p>
<ul>
<li><strong>问题</strong>：当前训练语料以英文为主。</li>
<li><strong>思路</strong>：<ul>
<li>多语言字幕对齐：利用机器翻译构建多语言 QA 对，测试跨语言泛化。</li>
<li>文化常识注入：在 prompt 中增加地域背景知识，减少文化偏差导致的误判。</li>
</ul>
</li>
</ul>
<p>以上方向既可独立开展，也可组合成下一代 <strong>“任意时长、任意模态、任意粒度”</strong> 的长视频理解系统。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>Video-MTR：面向长视频的强化多轮推理框架（一句话总结）</p>
<blockquote>
<p>首次把“多轮交互式检索 + 端到端强化学习”搬进 MLLM 内部，仅用 8 K 高质量标注、32 帧即可在超长视频问答上刷新开源 SOTA，并揭示“片段越关键、视频越长、增益越大”的规律。</p>
</blockquote>
<p>1. 研究背景与痛点</p>
<ul>
<li><strong>长视频特性</strong>：时长可达数小时，事件多、跨度大，关键信息稀疏。</li>
<li><strong>现有范式缺陷</strong><ul>
<li>单轮静态采样：一次性看少量帧，易漏关键信息。</li>
<li>外部 VLM 代理：系统复杂、无端到端训练、策略次优。</li>
</ul>
</li>
</ul>
<p>2. 核心贡献</p>
<p>维度</p>
<p>具体做法</p>
<p>关键创新</p>
<p><strong>框架</strong></p>
<p>把长视频问答建模成 <strong>多轮 RL 任务</strong><br>MLLM ↔ 视频环境迭代交互</p>
<p>首次在 MLLM 内部实现可训练的“检索-问答”闭环</p>
<p><strong>奖励</strong></p>
<p><strong>门控双层奖励</strong><br>• Trajectory-level：答案正确性 R_acc<br>• Turn-level：帧-查询 IoU 提升 R_k^fms</p>
<p>解决稀疏奖励与信用分配；防止 reward hacking</p>
<p><strong>数据</strong></p>
<p>8 K 时空精确 QA 对（NExT-GQA + QVHighlights 改写）</p>
<p>小体量、高信噪比，支撑纯 RL 训练</p>
<p><strong>训练</strong></p>
<p>PPO + 探索启动（cold-start bonus）</p>
<p>无需大规模 SFT，即可启动多轮检索行为</p>
<p>3. 实验结果</p>
<p>基准</p>
<p>时长</p>
<p>Video-MTR</p>
<p>对比亮点</p>
<p><strong>VideoMME</strong></p>
<p>≤ 60 min</p>
<p>51.0 %</p>
<p>开源第一，超长子集 +6.3 pp</p>
<p><strong>MLVU-Test</strong></p>
<p>≤ 2 h</p>
<p>48.4 %</p>
<p>仅用 32 帧，仅次于 GPT-4o（300+ 帧）</p>
<p><strong>EgoSchema</strong></p>
<p>3 min</p>
<p>62.4 %</p>
<p>无 ego 数据，仍列开源第一</p>
<ul>
<li><p><strong>消融</strong></p>
<ul>
<li>去掉多轮：长视频掉 6.3 pp；</li>
<li>去掉双层奖励：MLVU 掉 4 pp；</li>
<li>去掉门控：出现 reward↑ accuracy↓ 的 hacking 现象。</li>
</ul>
</li>
<li><strong>案例</strong><br>54 min 篮球规则题：两轮检索即精准定位关键片段并答对。</li>
</ul>
<p>4. 局限与未来方向</p>
<ul>
<li><strong>检索深度</strong>：3 轮不足以覆盖多事件顺序推理。</li>
<li><strong>细粒度</strong>：帧分辨率限制微动作识别。</li>
<li><strong>扩展</strong>：更长视频、多模态（音频/字幕）、开放文本答案、流式场景等。</li>
</ul>
<p>一句话收束</p>
<p>Video-MTR 用“少而精”的数据 + “多而准”的检索，把长视频理解从“单帧猜剧情”升级为“多轮侦探式推理”，为超长视频问答提供了可扩展的新范式。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Video-MTR%3A%20Reinforced%20Multi-Turn%20Reasoning%20for%20Long%20Video%20Understanding，链接是  https://arxiv.org/pdf/2508.20478  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.20478  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yuan Xie,Tianshui Chen,Zheng Ge,Lionel Ni</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.20478.pdf">https://arxiv.org/pdf/2508.20478.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.20478">https://arxiv.org/abs/2508.20478</a></p>
<p><strong>Arxiv ID</strong>: 2508.20478</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.20478">https://papers.cool/arxiv/2508.20478</a></p>
<p><strong>Published</strong>: 2025-08-28T06:55:08Z</p>
<p><strong>Updated</strong>: 2025-08-28T06:55:08.000Z</p>
<hr>
<h3 id="8-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings"><a href="#8-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings" class="headerlink" title="8. NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04011">NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</a></h3><p>We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named Entity Retrieval, a variant of Named Entity Recognition (NER), where the types of interest are not provided in advance, and a user-defined type description is used to retrieve documents mentioning entities of that type. Instead of relying on fixed schemas or fine-tuned models, our method builds on internal representations of large language models (LLMs) to embed both entity mentions and user-provided open-ended type descriptions into a shared semantic space. We show that internal representations, specifically the value vectors from mid-layer transformer blocks, encode fine-grained type information more effectively than commonly used top-layer embeddings. To refine these representations, we train a lightweight contrastive projection network that aligns type-compatible entities while separating unrelated types. The resulting entity embeddings are compact, type-aware, and well-suited for nearest-neighbor search. Evaluated on three benchmarks, NER Retriever significantly outperforms both lexical and dense sentence-level retrieval baselines. Our findings provide empirical support for representation selection within LLMs and demonstrate a practical solution for scalable, schema-free entity retrieval. The NER Retriever Codebase is publicly available at <a target="_blank" rel="noopener" href="https://github.com/ShacharOr100/ner_retriever">https://github.com/ShacharOr100/ner_retriever</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们提出了NER Retriever，一个用于临时命名实体检索的零样本检索框架，这是命名实体识别（NER）的一个变体，其中感兴趣的类型没有提前提供，而使用用户定义的类型描述来检索提到该类型实体的文档。我们的办法并不依赖于固定的模式或微调的模型，而是建立在大型语言模型（LLMs）的内部表示之上，将实体提及和用户提供的开放式类型描述嵌入到共享的语义空间中。我们表明，内部表示，特别是中层变压器块的值向量，比常用的顶层嵌入更有效地编码了细粒度的类型信息。为了细化这些表示，我们训练了一个轻量级的对比投影网络，该网络对类型兼容的实体进行对齐，同时分离不相关的类型。最终得到的实体嵌入紧凑、类型感知，并非常适合最近邻搜索。在三个基准测试中评估，NER Retriever 在词汇和密集句子级检索基线方面显著优于它们。我们的研究结果为在LLMs中选择表示提供了实证支持，并展示了一种可扩展、无模式的实体检索的实际解决方案。NER Retriever代码库可在<a target="_blank" rel="noopener" href="https://github.com/ShacharOr100/ner_retriever获取。">https://github.com/ShacharOr100/ner_retriever获取。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该论文针对“<strong>零样本、按需（ad-hoc）命名实体检索</strong>”这一任务，提出 NER Retriever 框架，旨在解决以下核心问题：</p>
<ul>
<li><p><strong>传统 NER 受限于固定类型体系</strong><br>经典命名实体识别只能识别训练时见过的少量粗粒度类型（PERSON / ORG / LOC 等），无法应对现实场景中用户临时定义的、细粒度乃至全新的实体类别（如“恐龙”“法国女歌手”“抗病毒药物”）。</p>
</li>
<li><p><strong>句子级检索粒度太粗</strong><br>现有密集检索方法通常以整句或整段为单元编码，实体信号被上下文稀释，导致召回精度低，尤其在短文本、低上下文场景下更明显。</p>
</li>
<li><p><strong>大模型顶层表示并非最优</strong><br>直接使用 LLM 最终层输出做嵌入，对实体类别区分度不足；论文发现中间层（如 LLaMA-3.1 8B 第 17 块）的 value 向量反而携带更细粒度的类型信息，却未被充分利用。</p>
</li>
<li><p><strong>零样本泛化与可扩展性</strong><br>用户查询类型可能在训练阶段完全未出现，系统需仅凭自然语言描述即时检索含对应实体的文本，而无需任何任务相关微调或标注数据。</p>
</li>
</ul>
<p>综上，论文目标可概括为：<br><strong>在不依赖固定模式、不微调大模型的前提下，利用 LLM 内部中间层表示，通过轻量级对比投影，将实体提及与用户任意类型描述映射到同一低维语义空间，实现高精度、低延迟、可扩展的零样本实体检索。</strong></p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<ul>
<li><p><strong>命名实体识别（NER）</strong></p>
<ul>
<li>经典粗粒度 NER：CoNLL-2003（Tjong Kim Sang &amp; De Meulder, 2003）、Sekine (2008)</li>
<li>细粒度 / 开放类型实体分类：Ultra-Fine Entity Typing（Choi et al., 2018）、Few-NERD（Ding et al., 2021）、MultiCoNER 2（Fetahu et al., 2023）</li>
</ul>
</li>
<li><p><strong>零样本 NER 与提示方法</strong></p>
<ul>
<li>PromptNER（Ashok &amp; Lipton, 2023）</li>
<li>ChatGPT 零样本 NER 实证研究（Xie et al., 2023）</li>
</ul>
</li>
<li><p><strong>实体检索与实体-centric IR</strong></p>
<ul>
<li>NERetrieve 数据集与任务定义（Katz et al., 2023）</li>
<li>DBpedia-Entity v2（Hasibi et al., 2017）</li>
<li>QUEST（Malaviya et al., 2023）、QAMPARI（Amouyal et al., 2023）</li>
<li>D-MERIT（Rassin et al., 2024）</li>
</ul>
</li>
<li><p><strong>对比学习与实体表示</strong></p>
<ul>
<li>CONTaiNER（Das et al., 2022）</li>
<li>双编码器优化 NER（Zhang et al., 2023）</li>
</ul>
</li>
<li><p><strong>LLM 内部表示分析</strong></p>
<ul>
<li>语言模型作为知识库（Heinzerling &amp; Inui, 2021）</li>
<li>中间层编码细粒度语义（多篇分析性论文，本文第 4 节扩展为实体类型判别任务）</li>
</ul>
</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<ul>
<li><p><strong>任务重定义</strong><br>将“NER”从封闭分类转为<strong>按需检索</strong>：用户用自然语言给出任意实体类型 qtype，系统从大规模语料中返回所有含该类型实体的文本片段。</p>
</li>
<li><p><strong>关键洞察：中间层 Value 向量更擅类型判别</strong><br>在 LLaMA-3.1 8B 第 17 块 self-attention 的 value 向量上，实体同类型/异类型可分离性最好（AUC 0.78，优于顶层 0.09）。</p>
</li>
<li><p><strong>实体级而非句子级嵌入</strong><br>先以通用 NER 检测器离线扫描语料，抽取出所有实体跨度；仅对这些跨度提取上述 1024-d 向量，避免上下文噪声。</p>
</li>
<li><p><strong>轻量对比投影网络</strong><br>用 2 层 MLP（1024 → 500）将高维向量压缩为低维、类型敏感空间；以 triplet-loss 训练，anchor=类型描述，positive=同类实体，negative=异类+BM25 难负例。训练数据来自 NERetrieve 训练集 200 万三元组，LLM 全程冻结。</p>
</li>
<li><p><strong>端到端流程</strong></p>
<ol>
<li>离线索引：实体跨度 → 17 层 V 向量 → MLP → 500-d 向量入库。</li>
<li>在线查询：用户类型描述走同一编码链路得到 500-d 向量，用余弦近邻搜索召回实体，再映射回原文档。</li>
</ol>
</li>
<li><strong>零样本评估</strong><br>在 Few-NERD、MultiCoNER 2、NERetrieve 测试集上均与训练类型无重叠；R-Precision 最高提升 4 倍，存储减少 79%，无需任何任务微调即可泛化到新类别。</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>实验围绕“零样本、按需命名实体检索”展开，分四大类：</p>
<ol>
<li><p>主实验：端到端检索性能<br>数据集：Few-NERD、MultiCoNER 2（英文）、NERetrieve-test<br>指标：Average R-Precision（查询级平均）<br>对比基线：BM25、E5-Mistral、NV-Embed v2<br>结果：NER Retriever 在 Few-NERD 与 MultiCoNER 2 上分别达 0.34 与 0.32，最高领先基线 4×；NERetrieve 上与最强基线持平（0.28）。</p>
</li>
<li><p>消融实验（Few-NERD）</p>
<ul>
<li>层选择：block-17 V 向量 vs 顶层输出 → R-Precision 从 0.08 提至 0.19</li>
<li>令牌选择：实体末令牌 vs EOS 令牌 → 0.19 vs 0.02</li>
<li>投影 MLP：有 vs 无 → 0.34 vs 0.16</li>
</ul>
</li>
<li><p>实体检测影响<br>用“金跨度”替换自动检测器，R-Precision 再提升约 11%，验证召回瓶颈主要来自跨度遗漏。</p>
</li>
<li><p>存储与效率<br>MultiCoNER 2 上向量索引体积 2 GB，较句子级 NV-Embed v2（9.2 GB）减少 79%；嵌入维度 500 vs 4096。</p>
</li>
<li><p>表示可视化<br>UMAP 降维显示同类实体聚类紧密，共享粗类的细类相邻，验证嵌入空间保留语义层级。</p>
</li>
<li><p>统计显著性<br>Wilcoxon 符号秩检验 p&lt;0.05，确认在 Few-NERD 与 MultiCoNER 2 上显著优于最强基线。</p>
</li>
</ol>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<ul>
<li><p><strong>多语言与跨语言迁移</strong><br>仅测试了英文，需验证中间层 V 向量在非拉丁语系是否仍保持类型判别优势，并探索跨语言查询（如用中文“恐龙”检索英文文档）。</p>
</li>
<li><p><strong>领域特化与持续更新</strong><br>医学、金融等专业领域实体类型分布差异大，可引入领域适配 LLM 或在线对比微调，缓解参数知识不足导致的召回下降。</p>
</li>
<li><p><strong>实体检测-检索联合优化</strong><br>当前两阶段流水线，检测错误直接上限召回。可尝试端到端可微框架，用检索信号反向修正跨度提议，或采用强化学习将 F1 与 R-Precision 联合奖励。</p>
</li>
<li><p><strong>动态类型层次与多标签检索</strong><br>现实查询常带层级约束（“19 世纪法国政治家”）。需支持复合条件、集合操作，并允许一个实体同时匹配多个类型。</p>
</li>
<li><p><strong>高效向量更新与遗忘机制</strong><br>语料持续增量时，如何快速插入新实体、删除过期实体，同时避免重训投影网络；可探索可编辑嵌入或局部低秩适应。</p>
</li>
<li><p><strong>解释性与可控性</strong><br>提供检索理由（如注意力权重、类型特征维度），支持用户交互式细化查询；研究对比投影空间的线性可解释方向，实现“向量算法”操作。</p>
</li>
<li><p><strong>更细粒度时间、数值、事件检索</strong><br>将框架扩展到时间表达式、数值属性、事件角色等，需要设计新的中间层特征提取策略与负采样策略。</p>
</li>
<li><p><strong>硬件友好极致压缩</strong><br>500 维仍对边缘设备有压力，可试乘积量化、二值化或蒸馏至更小 Transformer，保持类型判别力。</p>
</li>
<li><p><strong>与生成式 RAG 智能体集成</strong><br>把 NER Retriever 作为工具插件，让 LLM Agent 在宽域问答中动态调用，实现“先检索实体-再生成答案”的闭环，评估对长程事实问答的提升。</p>
</li>
</ul>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文核心速览</p>
<p><strong>题目</strong>：NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings<br><strong>任务</strong>：用户临时给出任意实体类型描述，系统零样本地返回所有含该类型实体的文本片段。</p>
<p>关键步骤</p>
<p>做法与发现</p>
<p>1. 表示选择</p>
<p>系统扫描 LLaMA-3.1 8B 各层各分量，发现第 17 块 self-attention 的 Value 向量对实体类型最敏感（AUC 0.78）。</p>
<p>2. 实体级嵌入</p>
<p>离线用通用 NER 检测器抽跨度，仅对跨度末令牌提取 1024-d V 向量，避免句子级噪声。</p>
<p>3. 对比投影</p>
<p>轻量 2 层 MLP（1024→500），以 200 万三元组 triplet-loss 训练，同类型拉近、异类型推远，LLM 冻结。</p>
<p>4. 检索流程</p>
<p>查询类型走同一编码链路→500-d 向量→余弦近邻搜实体→回原文档，全程零样本、无任务微调。</p>
<p>实验结果</p>
<p>数据</p>
<p>平均 R-Precision</p>
<p>Few-NERD 0.34（+4×）、MultiCoNER 2 0.32（+4×）、NERetrieve 0.28（持平均）</p>
<p>存储压缩</p>
<p>向量维度 500，索引体积 ↓79% vs 句子级基线</p>
<p>消融</p>
<p>中间层&gt;顶层、实体末令牌&gt;EOS、投影 MLP&gt;无投影</p>
<p><strong>结论</strong>：中间层表示+对比投影即可低成本实现细粒度、开放域、零样本实体检索，为后续 RAG 智能体、领域适配提供可扩展基础。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是NER%20Retriever%3A%20Zero-Shot%20Named%20Entity%20Retrieval%20with%20Type-Aware%20Embeddings，链接是  https://arxiv.org/pdf/2509.04011  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04011  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Or Shachar,Uri Katz,Yoav Goldberg,Oren Glickman</p>
<p><strong>Categories</strong>: cs.IR,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04011.pdf">https://arxiv.org/pdf/2509.04011.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04011">https://arxiv.org/abs/2509.04011</a></p>
<p><strong>Arxiv ID</strong>: 2509.04011</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04011">https://papers.cool/arxiv/2509.04011</a></p>
<p><strong>Published</strong>: 2025-09-04T08:42:23Z</p>
<p><strong>Updated</strong>: 2025-09-04T08:42:23.000Z</p>
<hr>
<h3 id="9-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation"><a href="#9-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation" class="headerlink" title="9. Few-step Flow for 3D Generation via Marginal-Data Transport Distillation"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04406">Few-step Flow for 3D Generation via Marginal-Data Transport Distillation</a></h3><p>Flow-based 3D generation models typically require dozens of sampling steps during inference. Though few-step distillation methods, particularly Consistency Models (CMs), have achieved substantial advancements in accelerating 2D diffusion models, they remain under-explored for more complex 3D generation tasks. In this study, we propose a novel framework, MDT-dist, for few-step 3D flow distillation. Our approach is built upon a primary objective: distilling the pretrained model to learn the Marginal-Data Transport. Directly learning this objective needs to integrate the velocity fields, while this integral is intractable to be implemented. Therefore, we propose two optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD), to equivalently convert the optimization target from the transport level to the velocity and the distribution level respectively. Velocity Matching (VM) learns to stably match the velocity fields between the student and the teacher, but inevitably provides biased gradient estimates. Velocity Distillation (VD) further enhances the optimization process by leveraging the learned velocity fields to perform probability density distillation. When evaluated on the pioneer 3D generation framework TRELLIS, our method reduces sampling steps of each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s (2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high visual and geometric fidelity. Extensive experiments demonstrate that our method significantly outperforms existing CM distillation methods, and enables TRELLIS to achieve superior performance in few-step 3D generation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于流的3D生成模型通常在推理过程中需要数十个采样步骤。尽管少步蒸馏方法，特别是一致性模型（CMs），在加速2D扩散模型方面取得了显著进展，但它们在更复杂的3D生成任务中仍然研究不足。在本研究中，我们提出了一种新颖的框架 MDT-dist，用于少步3D流蒸馏。我们的方法基于一个主要目标：蒸馏预训练模型以学习边际数据传输。直接学习这个目标需要整合速度场，而这个积分是难以实现的。因此，我们提出了两个可优化的目标，速度匹配（VM）和速度蒸馏（VD），以将优化目标从传输层面等效地转换到速度和分布层面。速度匹配（VM）学会在学生和教师之间稳定匹配速度场，但不可避免地提供了偏倚的梯度估计。速度蒸馏（VD）通过利用学习到的速度场进行概率密度蒸馏，进一步增强了优化过程。在先锋3D生成框架TRELLIS上评估时，我们的方法将每个流变换器的采样步骤从25减少到1或2，分别实现0.68秒（1步x2）和0.94秒（2步x2）的延迟，在A800上实现了9.0倍和6.5倍的加速，同时保持高视觉和几何保真度。大量实验证明，我们的方法显著优于现有的CM蒸馏方法，并使TRELLIS在少步3D生成中获得了卓越的性能。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该论文针对“基于流的3D生成模型在推理阶段需要数十步采样，计算开销大”这一瓶颈，提出一种少步蒸馏框架 MDT-dist，将预训练 3D 流模型压缩为 1–2 步的高速生成器，同时保持高保真几何与外观。核心待解决问题可归纳为：</p>
<ul>
<li><p><strong>3D 流模型推理步骤过多</strong><br>现有 3D 流匹配方法（如 TRELLIS）每支路需 25 步 ODE 求解，两步串联共 50 步，导致单次生成耗时数秒，难以满足实时/大规模内容生产需求。</p>
</li>
<li><p><strong>2D 少步蒸馏经验无法直接迁移</strong><br>一致性模型（CM、PCM、sCM）在 2D 领域已验证有效，但 3D 表示离散稀疏、维度更高、几何-纹理耦合更复杂，直接套用 CM 的“相邻时刻一致性约束”会出现优化不稳定、细节丢失等问题。</p>
</li>
<li><p><strong>缺乏面向 3D 的少步蒸馏目标</strong><br>简单“噪声→数据”单步映射易 posterior collapse；而 CM 的间接一致性约束并非针对“边际分布→数据分布”的运输本身，导致蒸馏效率低、生成质量下降。</p>
</li>
</ul>
<p>因此，论文提出“边际-数据运输蒸馏”新目标，将运输积分转化为可优化的速度匹配（VM）与速度蒸馏（VD）两项损失，实现 3D 流模型的高效少步生成。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下从“3D 原生生成模型”“2D 少步/蒸馏加速”“3D 蒸馏加速”三条线梳理与本文最相关的工作，并指出其与 MDT-dist 的区别。</p>
<p>3D 原生生成模型（含流匹配）</p>
<ul>
<li><strong>TRELLIS</strong>（本文教师模型）<br>结构化潜变量 SLAT + 双路 Flow Transformer，25×2 步采样生成带纹理 mesh/Gaussian。</li>
<li><strong>3DTopia-XL / LGM / Direct3D-S2 / Hunyuan3D / Clay / Step1X-3D / Craftsman</strong><br>大规模 3D 扩散/流模型，强调几何-外观联合建模，但均未系统研究少步推理。</li>
<li><strong>FlashVDM</strong>（唯一 3D 蒸馏同期工作）<br>基于 PCM 的少步形状蒸馏，仅输出几何、无外观，且仍保留 5 步采样。</li>
</ul>
<p>2D 少步/蒸馏加速（CM 系列）</p>
<ul>
<li><strong>Consistency Models (CM)</strong><br>强制相邻时刻输出一致，单步可生成，但约束过强易掉点。</li>
<li><strong>Phased CM (PCM)</strong><br>分阶段放松一致性，提升稳定，仍属“轨迹一致性”框架。</li>
<li><strong>sCM / TrigFlow-sCM</strong><br>连续时间一致性，消除离散误差，但需额外正则项。</li>
</ul>
<p>→ 上述方法均面向 2D 图像，且优化目标为“轨迹自洽”，而非直接学习“边际-数据运输”。</p>
<p>3D 蒸馏加速</p>
<ul>
<li><strong>Score Distillation Sampling (SDS) / Variational SDS / ProlificDreamer</strong><br>用 2D 扩散分数驱动 NeRF/Mesh 优化，属于“优化式生成”，非端到端蒸馏。</li>
<li><strong>DreamFusion、Magic3D、Fantasia3D 等 SDS 系列</strong><br>重点在文本-3D 提升质量，而非减少采样步数。</li>
<li><strong>MeanFlow</strong><br>提出单步流匹配，但需双时间输入，无法直接蒸馏预训练 3D 流模型。</li>
</ul>
<p>综上，<strong>MDT-dist 是首个面向 3D 流匹配、以“边际-数据运输”为直接优化目标的少步蒸馏框架</strong>，与现有 CM 系列（2D 轨迹一致性）和 FlashVDM（3D 形状-only、PCM 衍生）形成明显区分。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p><strong>MDT-dist</strong> 的核心思路是：</p>
<blockquote>
<p>不再沿用 <strong>Consistency Models</strong> 的“相邻时刻一致性”约束，而是直接让<strong>学生网络</strong>学会<strong>教师流模型</strong>的“<strong>边际分布 → 数据分布</strong>”运输算子。</p>
</blockquote>
<p>由于运输积分不可微，论文将其等价转化为两个可联合优化的目标：<strong>Velocity Matching（VM）</strong> 与 <strong>Velocity Distillation（VD）</strong>。整体流程如下：</p>
<p>1. 建立“边际-数据运输”主目标</p>
<ul>
<li><p>给定教师速度场 <strong>v_pre(x_τ,τ)</strong>，理想运输</p>
<pre><code>T_target(t)=∫_0^t v_pre(x_τ,τ)dτ
</code></pre></li>
</ul>
<ul>
<li><p>希望学生网络 <strong>ϕ_θ(x_t,t)</strong> 一步输出该运输量：</p>
<pre><code>min_θ E_t E_x0,z [ ‖t·ϕ_θ(x_t,t) − T_target(t)‖^2 ]
</code></pre></li>
</ul>
<ul>
<li>直接优化需要积分，不可实现 ⇒ 引入 VM+VD 两项可微代理。</li>
</ul>
<p>2. Velocity Matching（VM）——<strong>速度级</strong>代理</p>
<p>对主目标求时间导数，把“运输误差”转为“速度误差”：</p>
<pre><code>u_θ(x_t,t) = ϕ_θ(x_t,t) + t·dϕ_θ/dt
L_VM = E_t,x0,z [ ‖u_θ(x_t,t) − v_pre(x_t,t)‖^2 ]
</code></pre><ul>
<li>仅需教师单步速度 <strong>v_pre</strong>，无需积分。</li>
<li>离散实现：用 <strong>Δt=1e-2</strong> 近似导数，并 <strong>stop-gradient</strong> 于第二项，保证稳定。</li>
<li>理论保证：主目标误差 ≤ VM 误差 × t²（附录 A.1）。</li>
</ul>
<p>3. Velocity Distillation（VD）——<strong>分布级</strong>代理</p>
<p>VM 仅提供<strong>有偏梯度</strong>；为此引入基于<strong>分数蒸馏</strong>思想的分布对齐：</p>
<pre><code>L_VD 梯度 = E_z′,z′′[ −(u_θ(x′_t,t)−v_pre(x′_t,t)) · ∂x′_t/∂θ ]
</code></pre><ul>
<li>先让学生 <strong>单步生成</strong> 样本 <strong>x′=z′−ϕ_θ(z′,1)</strong></li>
<li>再对 <strong>x′</strong> 重加噪得 <strong>x′_t</strong>，用教师速度作“真值分数”</li>
<li>等价于最小化 <strong>KL(学生边际 ‖ 教师边际)</strong>，弥补 VM 的梯度偏差（附录 A.2）。</li>
</ul>
<p>4. 联合优化与推理</p>
<pre><code>L_MDT-dist = L_VM + λ·L_VD,  λ=1.0
</code></pre><ul>
<li>训练：两路 Flow Transformer 分别蒸馏 6k/4k step，lr 4e-8/1e-8。</li>
<li>推理：每路 <strong>1–2 步</strong> ODE 即得 SLAT，解码为 Gaussian/Mesh；<br>总延迟 <strong>0.68 s</strong>（1×2 步） / <strong>0.94 s</strong>（2×2 步），较教师 <strong>6.1 s</strong> 提速 <strong>9×/6.5×</strong>。</li>
</ul>
<p>5. 与先前方法的本质区别</p>
<p>方法</p>
<p>优化目标</p>
<p>约束类型</p>
<p>适用域</p>
<p>备注</p>
<p>CM/PCM/sCM</p>
<p>相邻时刻一致性</p>
<p>轨迹自洽</p>
<p>2D 为主</p>
<p>间接、易掉点</p>
<p>FlashVDM</p>
<p>PCM 损失</p>
<p>轨迹自洽</p>
<p>3D 形状-only</p>
<p>5 步，无外观</p>
<p><strong>MDT-dist</strong></p>
<p><strong>边际→数据运输</strong></p>
<p><strong>速度+分布对齐</strong></p>
<p><strong>3D 流模型</strong></p>
<p><strong>1–2 步，全纹理</strong></p>
<p>通过“<strong>速度匹配保证局部正确性 + 速度蒸馏修正全局分布</strong>”的双目标协同，MDT-dist 在保持几何与外观保真度的同时，首次将 3D 流模型压缩至 <strong>单步级</strong> 推理。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>实验围绕 <strong>“少步 3D 流模型蒸馏”</strong> 展开，共 4 组评测：</p>
<ol>
<li>与现有 3D 生成方法的<strong>定量/定性对比</strong></li>
<li>与 2D→3D 蒸馏方法 <strong>FlashVDM</strong> 的<strong>几何专项对比</strong></li>
<li>与 <strong>CM 系列</strong>（CM / PCM / sCM）的<strong>蒸馏策略对比</strong></li>
<li><strong>消融实验</strong>验证 VM 与 VD 两项损失的必要性</li>
</ol>
<p>所有实验均基于 <strong>TRELLIS 1.1B 参数模型</strong>作为教师，在 <strong>A800</strong> 上测速，数据集/指标统一使用 <strong>Toys4K</strong>（3 218 个资产）。</p>
<p>1 主实验：与 3D 生成 SoTA 对比</p>
<p>方法</p>
<p>推理步数</p>
<p>时间↓</p>
<p>FD_incep↓</p>
<p>FD_dinov2↓</p>
<p>ULIPI↑</p>
<p>LGM（非扩散）</p>
<p>–</p>
<p>5 s</p>
<p>26.31</p>
<p>322.71</p>
<p>–</p>
<p>3DTopia-XL</p>
<p>25</p>
<p>5 s</p>
<p>37.68</p>
<p>437.37</p>
<p>–</p>
<p>Ln3Diff</p>
<p>250</p>
<p>8 s</p>
<p>26.61</p>
<p>357.93</p>
<p>–</p>
<p><em>_TRELLIS</em>（教师）_*</p>
<p>25×2</p>
<p>6.1 s</p>
<p>9.35</p>
<p>67.21</p>
<p>39.53</p>
<p><strong>Ours 1×2</strong></p>
<p><strong>1×2</strong></p>
<p><strong>0.68 s</strong></p>
<p>18.09</p>
<p>164.2</p>
<p>36.88</p>
<p><strong>Ours 2×2</strong></p>
<p><strong>2×2</strong></p>
<p><strong>0.94 s</strong></p>
<p>14.16</p>
<p>110.9</p>
<p>39.11</p>
<ul>
<li><strong>速度</strong>：1×2 步较教师 <strong>9.0× 提速</strong>，2×2 步 <strong>6.5× 提速</strong>。</li>
<li><strong>质量</strong>：2×2 步在三大指标上 <strong>与教师持平或更好</strong>；1×2 步略降但仍远优于其他非蒸馏方法。</li>
<li><strong>视觉</strong>：图 2 显示几何/纹理细节保留完整，无 FlashVDM 的“无纹理”问题。</li>
</ul>
<p>2 与 FlashVDM 的专项几何对比</p>
<p>FlashVDM 仅输出形状，故只用 <strong>ULIPI</strong>（图像-网格相似度）：</p>
<p>方法</p>
<p>步数</p>
<p>ULIPI↑</p>
<p>FlashVDM</p>
<p>5</p>
<p>37.91</p>
<p><strong>Ours 1×2</strong></p>
<p><strong>2</strong></p>
<p><strong>36.88</strong></p>
<p><strong>Ours 2×2</strong></p>
<p><strong>4</strong></p>
<p><strong>39.11</strong></p>
<ul>
<li>步数相近时，<strong>MDT-dist 几何质量更高</strong>；1×2 步已可媲美，2×2 步反超 <strong>+3.2%</strong>。</li>
</ul>
<p>3 与 CM 系列蒸馏策略对比</p>
<p>在 <strong>相同教师、相同训练预算</strong>下，将 CM/PCM/sCM 的公开代码适配到 TRELLIS 结构：</p>
<p>方法</p>
<p>FD_incep↓</p>
<p>FD_dinov2↓</p>
<p>ULIPI↑</p>
<p>CM</p>
<p>20.06</p>
<p>194.5</p>
<p>34.62</p>
<p>PCM</p>
<p>19.53</p>
<p>189.5</p>
<p>34.96</p>
<p>sCM</p>
<p>19.29</p>
<p>186.0</p>
<p>35.04</p>
<p><strong>Ours</strong></p>
<p><strong>18.09</strong></p>
<p><strong>164.2</strong></p>
<p><strong>36.88</strong></p>
<ul>
<li><strong>显著优于所有 CM 变体</strong>，图 4 可视化显示 CM 系列存在<strong>几何残缺、纹理模糊</strong>等问题，MDT-dist 细节最完整。</li>
</ul>
<p>4 消融实验：VM 与 VD 的必要性</p>
<p>配置</p>
<p>FD_incep↓</p>
<p>FD_dinov2↓</p>
<p>ULIPI↑</p>
<p>无蒸馏（教师 25×2）</p>
<p>11.80</p>
<p>65.24</p>
<p>39.53</p>
<p><strong>仅 VM</strong></p>
<p>18.42</p>
<p>172.0</p>
<p>35.99</p>
<p><strong>VM+VD</strong></p>
<p><strong>18.09</strong></p>
<p><strong>164.2</strong></p>
<p><strong>36.88</strong></p>
<ul>
<li><strong>仅 VM</strong> 已大幅改善“多生碎片”现象（图 5），但仍有局部缺失；</li>
<li><strong>加入 VD 后</strong> 进一步补全结构，量化指标全面提升，验证两者互补。</li>
</ul>
<p>5 额外结果</p>
<ul>
<li><strong>图 6/7</strong> 给出 1×2 与 2×2 步的大量 <strong>Gaussian &amp; Mesh</strong> 结果，展示复杂拓扑、薄壁结构、精细纹理均可保持。</li>
<li><strong>附录 B</strong> 提供 <strong>超 100 个随机样本</strong>的九宫格，验证一致性。</li>
</ul>
<p>实验结论</p>
<ol>
<li><strong>MDT-dist 首次将 3D 流模型压缩至 1–2 步</strong>，在 <strong>0.68 s</strong> 内达到与 50 步教师相当的视觉/几何质量。</li>
<li><strong>全面优于现有 3D 蒸馏方法</strong>（FlashVDM）及所有 CM 系列策略。</li>
<li><strong>VM+VD 联合优化</strong>是取得高质量的关键，缺一不可。</li>
</ol>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下展望按“<strong>数据-效率</strong>、<strong>理论-扩展</strong>、<strong>应用-场景</strong>、<strong>系统-优化</strong>”四条线展开，均直接承接论文的局限与观察，可作为后续工作切入点。</p>
<p>1 数据-效率：摆脱高成本 3D 真值</p>
<ul>
<li><p><strong>无几何蒸馏</strong><br>当前仍需 500K 套“图像-SLAT”配对。可尝试：</p>
<ul>
<li>用教师模型自生成“伪 SLAT”→ 蒸馏，仅保留图像条件，实现 <strong>图像→3D 单步生成</strong>。</li>
<li>引入 <strong>可微渲染损失</strong>（类似 SDS）作为额外正则，弱化对真值网格的依赖。</li>
</ul>
</li>
<li><strong>视频/多视角网页图像</strong><br>利用海量在线视频或 Objaverse-XL 的 <strong>无标注多视角图</strong>，通过 <strong>自监督重建损失</strong> 筛选高质量样本，进一步扩大蒸馏规模。</li>
</ul>
<p>2 理论-扩展：目标函数与模型结构</p>
<ul>
<li><p><strong>更高阶速度匹配</strong><br>VM 仅用一阶差分近似 <code>dϕ/dt</code>；可探索 <strong>Runge-Kutta 二阶/三阶格式</strong>，在相同步数下降低截断误差，或进一步压缩到 <strong>单步 1×1</strong>。</p>
</li>
<li><p><strong>自适应 λ(t) 权重</strong><br>目前 <code>λ=1</code> 为常数。可让 <code>λ(t)</code> 随 <code>t</code> 动态变化：</p>
<ul>
<li>小 <code>t</code>（精细结构）侧重 <strong>VD</strong> 分布对齐；</li>
<li>大 <code>t</code>（噪声区域）侧重 <strong>VM</strong> 速度精确，或能提升细粒度几何。</li>
</ul>
</li>
<li><p><strong>向其他流变体迁移</strong></p>
<ul>
<li><strong>Rectified Flow / Straight Flow</strong>：其速度场更“直”，理论上更易单步近似，可验证 MDT-dist 是否进一步受益。</li>
<li><strong>扩散-流混合模型</strong>（如 TRELLIS 的后续改进版）可测试通用性。</li>
</ul>
</li>
</ul>
<p>3 应用-场景：从“生成”到“编辑”</p>
<ul>
<li><p><strong>单步 3D 编辑</strong><br>利用 <code>ϕ_θ(x_t, t)</code> 的逆向可逆性，实现 <strong>图像提示下的局部几何/纹理编辑</strong>（类似 Imagic3D），但基于 <strong>1 步前向-逆向</strong>，达到实时交互。</p>
</li>
<li><p><strong>动画与姿态控制</strong><br>将 SLAT 的稀疏 voxel 特征与 <strong>骨骼或旋转表示</strong> 联合蒸馏，实现 <strong>单步生成可驱动角色</strong>，为 embodied AI 提供低成本虚拟角色工厂。</p>
</li>
<li><p><strong>多分辨率级联</strong><br>先 64³ 单步生成粗 SLAT，再接入 <strong>超分辨率流 transformer</strong>（类似 Direct3D-S2 的 sparse attention），实现 <strong>千兆 voxel 级单步 3D 资产</strong>。</p>
</li>
</ul>
<p>4 系统-优化：工程落地</p>
<ul>
<li><p><strong>显存-计算协同压缩</strong></p>
<ul>
<li>对 <code>ϕ_θ</code> 做 <strong>8-bit 量化 + KV-cache 剪枝</strong>，在 A800 上把 1.1B 模型压至 &lt;6 GB，使 <strong>单卡 24 GB 消费级 GPU</strong> 也能 1 秒级生成。</li>
<li>结合 <strong>TensorRT / ONNX</strong> 图优化，把离散 voxel 索引操作改写为 <strong>CUDA kernel</strong>，实测可再提速 <strong>20–30%</strong>。</li>
</ul>
</li>
<li><strong>端侧推理</strong><br>将蒸馏后的学生模型拆分为 <strong>图像编码器（CNN）（&lt;100M）+ 流预测器（Transformer）（&lt;500M）</strong>，适配 <strong>Apple M 系列 / 高通 NPU</strong>，实现 <strong>移动端 3 秒生成 3D 资产</strong>，打通 AR/UGC 场景。</li>
</ul>
<p>5 风险与伦理</p>
<ul>
<li><strong>版权与滥用</strong><br>单步生成极大降低门槛，需配套 <strong>3D 资产水印</strong>（如 Gaussian 隐写）与 <strong>生成溯源</strong>技术，避免海量自动盗版或虚假数字藏品。</li>
</ul>
<p>一句话总结</p>
<blockquote>
<p><strong>“把 MDT-dist 从‘需要真值’变成‘只要图片’，再把 1 步压缩到极限，最后搬到手机与 AR 眼镜里实时跑起来”</strong>——这四步走完，3D 生成将真正进入大众级应用。</p>
</blockquote>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<h1 id="论文主旨"><a href="#论文主旨" class="headerlink" title="论文主旨"></a>论文主旨</h1><p>提出 <strong>MDT-dist</strong> 框架，首次把 3D 流匹配模型的 50 步推理压缩到 <strong>1–2 步</strong>，在 0.68 s 内生成高保真几何与纹理，较教师模型提速 <strong>9×</strong>，显著优于现有 CM 系列与 FlashVDM。</p>
<h1 id="技术路线"><a href="#技术路线" class="headerlink" title="技术路线"></a>技术路线</h1><ol>
<li><p><strong>主目标</strong>：学习“边际分布 → 数据分布”运输<br>不可积分 ⇒ 导出两项可微代理</p>
</li>
<li><p><strong>Velocity Matching (VM)</strong><br>直接对齐学生/教师速度场，保证运输导数一致，理论误差有界</p>
</li>
<li><p><strong>Velocity Distillation (VD)</strong><br>用速度差做概率密度蒸馏，修正 VM 梯度偏差，实现单步分布对齐</p>
</li>
<li><p><strong>联合损失</strong><br>L = L_VM + λ·L_VD，端到端蒸馏预训练 TRELLIS 双路 Flow Transformer</p>
</li>
</ol>
<h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><ul>
<li><strong>Toys4K 基准</strong>：1×2 步 FD_incep 18.09、ULIPI 36.88，与 50 步教师基本持平</li>
<li><strong>击败所有 CM 变体</strong>（CM/PCM/sCM）与唯一 3D 蒸馏对手 FlashVDM</li>
<li><strong>消融</strong>：VM 单独已显著降碎片，+VD 后几何完整性最佳</li>
</ul>
<h1 id="意义"><a href="#意义" class="headerlink" title="意义"></a>意义</h1><ul>
<li><strong>理论</strong>：给出无需一致性约束的流模型少步蒸馏新范式</li>
<li><strong>工程</strong>：让 3D 内容创作从“分钟级”迈入“秒级”，为实时交互、 embodied AI 提供可行路径</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Few-step%20Flow%20for%203D%20Generation%20via%20Marginal-Data%20Transport%20Distillation，链接是  https://arxiv.org/pdf/2509.04406  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04406  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zanwei Zhou,Taoran Yi,Jiemin Fang,Chen Yang,Lingxi Xie,Xinggang Wang,Wei Shen,Qi Tian</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04406.pdf">https://arxiv.org/pdf/2509.04406.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04406">https://arxiv.org/abs/2509.04406</a></p>
<p><strong>Arxiv ID</strong>: 2509.04406</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04406">https://papers.cool/arxiv/2509.04406</a></p>
<p><strong>Published</strong>: 2025-09-04T17:24:31Z</p>
<p><strong>Updated</strong>: 2025-09-04T17:24:31.000Z</p>
<hr>
<h3 id="10-Loong-Synthesize-Long-Chain-of-Thoughts-at-Scale-through-Verifiers"><a href="#10-Loong-Synthesize-Long-Chain-of-Thoughts-at-Scale-through-Verifiers" class="headerlink" title="10. Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.03059">Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</a></h3><p>Recent advances in Large Language Models (LLMs) have shown that their reasoning capabilities can be significantly improved through Reinforcement Learning with Verifiable Reward (RLVR), particularly in domains like mathematics and programming, where ground-truth correctness can be automatically evaluated. However, extending this success to other reasoning-intensive domains remains challenging due to the scarcity of high-quality, verifiable datasets and the high cost of human supervision. In this work, we introduce the Loong Project: an open-source framework for scalable synthetic data generation and verification across a diverse range of reasoning-intensive domains. The framework consists of two key components: (1) LoongBench, a curated seed dataset containing 8,729 human-vetted examples across 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired with executable code and rich metadata; and (2) LoongEnv, a modular synthetic data generation environment that supports multiple prompting strategies to produce new question-answer-code triples. Together, these components form an agent-environment loop that enables reinforcement learning, where an LLM-based agent is rewarded for generating Chain-of-Thought (CoT) solutions that align with code-executed answers. Empirically, we benchmark LoongBench on a broad suite of both open-source and proprietary LLMs to evaluate domain coverage and reveal performance bottlenecks. In addition, we conduct a comprehensive analysis of synthetic data generated by LoongEnv, examining correctness, difficulty, and diversity. Code and documentation are available at <a target="_blank" rel="noopener" href="https://github.com/camel-ai/loong">https://github.com/camel-ai/loong</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近在大型语言模型（LLMs）方面的进展表明，通过可验证奖励的强化学习（RLVR），它们的推理能力可以显著提高，特别是在数学和编程等领域，在这些领域中，真实的正确性可以被自动评估。然而，将这一成功扩展到其他推理密集型领域仍然面临挑战，因为高质量、可验证的数据集稀缺，以及人工监督的高成本。在本研究中，我们介绍了龙项目：一个用于跨多种推理密集型领域的可扩展合成数据生成和验证的开源框架。该框架由两个关键组件组成：(1) LoongBench，一个经过策划的种子数据集，包含12829个经过人工审核的示例，覆盖12个领域（例如，先进数学、化学、逻辑），每个示例都有可执行代码和丰富的元数据；(2) LoongEnv，一个模块化的合成数据生成环境，支持多种提示策略以生成新的问答代码三元组。这些组件共同形成一个代理-环境循环，使基于LLM的代理能够获得奖励，生成与代码执行答案一致的链式思维（CoT）解决方案。在经验上，我们对LoongBench在广泛的开源和专有LLMs上进行了基准测试，以评估领域覆盖率并揭示性能瓶颈。此外，我们对LoongEnv生成的合成数据进行了全面分析，检查了正确性、难度和多样性。代码和文档可在<a target="_blank" rel="noopener" href="https://github.com/camel-ai/loong找到。">https://github.com/camel-ai/loong找到。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>该论文旨在解决<strong>将强化学习与可验证奖励（RLVR）成功扩展到数学和编程以外的高强度推理领域</strong>时所面临的两大核心瓶颈：</p>
<ol>
<li><p><strong>缺乏高质量、可自动验证的数据集</strong><br>除数学与编程外，逻辑、物理、化学、金融、医学等同样需要可靠推理的领域，几乎没有成规模、带验证答案的数据集；而人工标注成本极高。</p>
</li>
<li><p><strong>缺乏可扩展的合成数据生成与验证框架</strong><br>现有方法难以在保证语义正确性的前提下，低成本、大批量地生成跨领域、难度可控、且能用于 RLVR 训练的问题–答案–代码三元组。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Loong 项目</strong>，通过</p>
<ul>
<li><strong>LOONGBENCH</strong>：8 729 条人工审核、跨 12 个推理领域、附带可执行代码与验证答案的种子数据；</li>
<li><strong>LOONGENV</strong>：模块化、可插拔的合成数据生成与验证环境，支持多种提示策略，实现可验证、可扩展的 RLVR 训练循环。</li>
</ul>
<p>最终目标是在无需大规模人工监督的情况下，让 LLM 在广泛推理领域也能像数学/编程任务一样，通过 RLVR 获得显著且可靠的推理能力提升。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究在<strong>强化学习与可验证奖励（RLVR）</strong>、<strong>合成数据生成</strong>、以及<strong>后训练对齐</strong>三个维度与 Loong 项目高度相关，构成了其理论与方法基础：</p>
<p>1. 强化学习与可验证奖励（RLVR）</p>
<p>文献</p>
<p>核心贡献</p>
<p>与 Loong 的关联</p>
<p><strong>DeepSeek-R1</strong> [1]</p>
<p>首次在数学/编程任务上大规模验证 RLVR 可显著提升 LLM 推理能力。</p>
<p>提供 RLVR 成功范式，Loong 将其扩展到 12 个新领域。</p>
<p><strong>Satori</strong> [3]</p>
<p>提出 Chain-of-Action-Thought（CoAT）框架，将代码执行作为可验证奖励信号。</p>
<p>Loong 的 verifier 模块采用相同思想：代码执行结果作为 ground-truth。</p>
<p><strong>Reasoning Gym</strong> [5]</p>
<p>构建多个可验证推理环境（逻辑、图论等），用于 RLVR 训练。</p>
<p>Loong 的 LOONGENV 与其环境设计目标一致，但覆盖更广领域。</p>
<p><strong>GRPO</strong> [49]</p>
<p>形式化 RLVR 的损失函数与收敛性分析。</p>
<p>为 Loong 未来接入 RLVR 训练提供理论支撑。</p>
<p><strong>One-shot RLVR</strong> [51]</p>
<p>证明单条可验证样本即可带来显著推理增益。</p>
<p>支持 Loong 用少量种子数据启动大规模合成生成。</p>
<p>2. 合成数据生成与自举</p>
<p>文献</p>
<p>核心贡献</p>
<p>与 Loong 的关联</p>
<p><strong>Self-Instruct</strong> [35, 54]</p>
<p>模型自举生成指令–响应对，迭代提升对齐。</p>
<p>Loong 的 Self-Instruct 策略直接沿用其 pipeline。</p>
<p><strong>WizardLM/Evol-Instruct</strong> [36, 55]</p>
<p>通过“进化”操作（抽象、复杂化）自动提升指令难度。</p>
<p>Loong 的 Evol-Instruct 模块复现并评估其在新领域的效果。</p>
<p><strong>DataGen</strong> [59]</p>
<p>统一合成数据生成框架，强调可控性、多样性、事实性。</p>
<p>Loong 的 LOONGENV 与其设计哲学一致，但额外引入代码执行验证。</p>
<p><strong>SynLogic</strong> [11]</p>
<p>针对逻辑推理的合成数据生成与验证。</p>
<p>Loong 的逻辑领域数据生成策略与其方法类似，但扩展到 12 个领域。</p>
<p>3. 后训练对齐与奖励建模</p>
<p>文献</p>
<p>核心贡献</p>
<p>与 Loong 的关联</p>
<p><strong>InstructGPT</strong> [42]</p>
<p>RLHF 经典工作，用人类偏好训练奖励模型。</p>
<p>Loong 用代码执行替代人类偏好，实现无人工标注的 RLVR。</p>
<p><strong>DPO</strong> [45]</p>
<p>将奖励建模转化为监督学习，避免 RL 训练不稳定性。</p>
<p>Loong 未来可集成 DPO 以简化 RLVR 流程。</p>
<p><strong>RRHF</strong> [47]</p>
<p>无需 RL，直接基于排名目标对齐模型。</p>
<p>为 Loong 提供另一种轻量级对齐选项。</p>
<p><strong>Co-evolutionary Coder &amp; Tester</strong> [50]</p>
<p>通过单元测试验证代码正确性，实现 RLVR。</p>
<p>Loong 的代码执行验证器与其思想同源，但面向更广泛领域。</p>
<p>4. 领域特定基准与数据集</p>
<p>文献</p>
<p>核心贡献</p>
<p>与 Loong 的关联</p>
<p><strong>MATH</strong> [10]</p>
<p>数学竞赛级问题与 LaTeX 答案。</p>
<p>Loong 的 Advanced Math 种子数据直接来源于此。</p>
<p><strong>SciBench</strong> [15] / <strong>OlympiadBench</strong> [16]</p>
<p>大学物理/奥赛级科学问题。</p>
<p>Loong 的 Advanced Physics 种子数据来源。</p>
<p><strong>MedCalc-Bench</strong> [31]</p>
<p>医学计算问题与答案。</p>
<p>Loong 的 Medicine 领域种子数据来源。</p>
<p><strong>LeetCode-Dataset</strong> [32]</p>
<p>编程竞赛问题与测试用例。</p>
<p>Loong 的 Programming 领域种子数据来源。</p>
<p>总结</p>
<p>Loong 项目通过整合 <strong>RLVR 训练范式</strong>（DeepSeek-R1, Satori）、<strong>合成数据自举技术</strong>（Self-Instruct, Evol-Instruct）以及<strong>领域特定基准</strong>（MATH, SciBench, MedCalc-Bench），首次在 12 个高强度推理领域实现了可扩展、可验证的合成数据生成与评估框架，填补了数学/编程以外领域缺乏高质量 RLVR 训练数据的空白。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过构建一个 <strong>模块化、可扩展的“种子–生成–验证–强化”闭环系统</strong> 来解决“数学/编程以外领域缺乏可验证数据与 RLVR 训练信号”的难题。具体分为四个层面：</p>
<p>1. 提供高质量、可验证的种子数据（LOONGBENCH）</p>
<ul>
<li><strong>跨 12 个推理领域</strong>（高数、物理、化学、逻辑、金融、医学等）人工精选 <strong>8 729 条</strong> 问题–答案–可执行代码三元组。</li>
<li>每条样本均经过 <strong>代码执行验证</strong>（数值、符号、单位、逻辑一致性），确保答案 100 % 可信，为后续合成与 RLVR 提供“黄金标准”。</li>
<li>附带元数据（难度、依赖库、来源、标签），便于按领域、难度、主题进行可控采样。</li>
</ul>
<p>2. 构建可插拔的合成数据生成环境（LOONGENV）</p>
<ul>
<li><strong>黑盒模块化设计</strong>：输入任意种子数据集 → 输出无限量、可验证的新问题–答案–代码三元组。</li>
<li><strong>三种策略并行</strong><ul>
<li>Few-shot prompting：快速复制种子风格，稳定高产。</li>
<li>Self-Instruct：递归自举，提升多样性。</li>
<li>Evol-Instruct：通过抽象、复杂化、约束变换等“进化”操作，系统提升难度与边缘场景覆盖。</li>
</ul>
</li>
<li><strong>多智能体流水线</strong><ol>
<li>问题生成 agent → 2. 代码求解 agent → 3. 代码执行 → 4. 双路验证（代码结果 vs. CoT 结果）→ 5. 语义 verifier 过滤。</li>
</ol>
</li>
<li><strong>领域无关接口</strong>：支持 plug-and-play 的 verifier、代码解释器、依赖库，便于扩展到新领域。</li>
</ul>
<p>3. 建立无人工标注的 RLVR 训练循环</p>
<ul>
<li><strong>Agent-Environment Loop</strong>（图 1）<ul>
<li>训练 agent 接收合成问题 → 输出自然语言 CoT → 用 verifier 比对代码执行结果 → 仅当语义一致时给予正奖励。</li>
<li>整个过程无需人工打分，实现 <strong>Reinforcement Learning from Verified Rewards (RLVR)</strong>。</li>
</ul>
</li>
<li><strong>错误抑制机制</strong>：<ul>
<li>双路独立求解（代码 + CoT）一致性过滤，显著降低假阳性。</li>
<li>LLM-as-a-judge 语义等价判定，减少假阴性。</li>
</ul>
</li>
<li><strong>未来扩展</strong>：计划接入 DPO/GRPO 等稳定 RL 算法，并引入领域专用 verifier（数学符号验证器、化学计量验证器等）。</li>
</ul>
<p>4. 系统评估与迭代优化</p>
<ul>
<li><strong>基准测试</strong>：在 LOONGBENCH 上对 6 个开源/闭源模型进行大规模评测，定位各领域的性能瓶颈（如数学规划、安全、金融）。</li>
<li><strong>合成数据质量评估</strong>：<ul>
<li><strong>可执行率</strong>：Few-shot &gt; Self-Instruct &gt; Evol-Instruct；</li>
<li><strong>多样性</strong>：t-SNE 与余弦相似度显示 Evol-Instruct 在语义层面最贴近种子，但能显著提升难度；</li>
<li><strong>难度可控性</strong>：Evol-Instruct 生成的题目使 GPT-4.1-mini 准确率从 92 % 降至 62 %，验证其有效提升复杂度。</li>
</ul>
</li>
<li><strong>持续迭代</strong>：根据评测结果动态调整生成策略、过滤阈值与奖励函数，形成“数据–模型–评测”闭环改进。</li>
</ul>
<p>一句话总结</p>
<p>Loong 项目通过 <strong>“高质量种子 + 模块化合成环境 + 代码级验证 + RLVR 训练循环”</strong>，首次在 12 个高强度推理领域实现了无需人工标注、可扩展且语义可信的大规模数据生成与模型对齐框架。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>LOONGBENCH 基准评测</strong> 与 <strong>LOONGENV 合成数据质量</strong> 两条主线，共设计并执行了 4 组实验，覆盖 12 个推理领域、6 个主流模型、3 种生成策略。具体实验如下：</p>
<p>1. LOONGBENCH 跨域基准评测</p>
<p><strong>目的</strong>：衡量现有模型在 12 个推理领域上的能力分布，定位瓶颈。<br><strong>设置</strong></p>
<ul>
<li><strong>数据</strong>：LOONGBENCH 全部 8 729 题（每领域 51–3 076 题）。</li>
<li><strong>模型</strong>：<ul>
<li>闭源：GPT-4.1-mini、o3-mini、Grok-3、Claude-3.7-Sonnet</li>
<li>开源：DeepSeek-R1、Qwen3-8B</li>
</ul>
</li>
<li><strong>指标</strong>：准确率（LLM-as-judge，符号等价即正确）。</li>
</ul>
<p><strong>结果</strong>（表 2 摘要）</p>
<p>领域</p>
<p>最佳模型</p>
<p>最低模型</p>
<p>关键发现</p>
<p>Advanced Math</p>
<p>o3-mini 97.4 %</p>
<p>Qwen3-8B 79.2 %</p>
<p>数学领域差距 18.2 pp</p>
<p>Math Programming</p>
<p>Claude-3.7 13.2 %</p>
<p>Grok-3 6.4 %</p>
<p>所有模型均 &lt;15 %，显著瓶颈</p>
<p>Game</p>
<p>DeepSeek-R1 97.3 %</p>
<p>Qwen3-8B 43.2 %</p>
<p>开源模型策略推理落后 54 pp</p>
<p>Security</p>
<p>DeepSeek-R1 28.7 %</p>
<p>Claude-3.7 4.7 %</p>
<p>安全领域整体低迷</p>
<p><strong>结论</strong>：</p>
<ul>
<li>形成 <strong>难度光谱</strong>（10 %–100 %），避免天花板/地板效应。</li>
<li>推理专用模型（o3-mini, DeepSeek-R1）在 8/12 领域进入前二。</li>
<li>开源模型在逻辑、博弈、安全等重推理任务上差距明显。</li>
</ul>
<p>2. LOONGENV 合成数据可执行性与正确率</p>
<p><strong>目的</strong>：评估三种生成策略（Few-shot / Self-Instruct / Evol-Instruct）的 <strong>代码可执行率</strong> 与 <strong>语义正确率</strong>。<br><strong>设置</strong></p>
<ul>
<li><strong>流程</strong>：100 题/领域/策略 → 代码执行 → Judge（DeepSeek-R1）判定。</li>
<li><strong>指标</strong>：<ul>
<li>Pass：代码运行且答案正确；</li>
<li>Judge-Rejected：运行但答案错；</li>
<li>Not Executable：运行失败。</li>
</ul>
</li>
</ul>
<p><strong>结果</strong>（图 2 摘要）</p>
<p>领域</p>
<p>策略</p>
<p>Pass</p>
<p>Judge-Rejected</p>
<p>Not Executable</p>
<p>Logic</p>
<p>Few-shot</p>
<p>92.6 %</p>
<p>2.4 %</p>
<p>5.0 %</p>
<p>Logic</p>
<p>Evol-Instruct</p>
<p>35.0 %</p>
<p>10.0 %</p>
<p>55.0 %</p>
<p>Physics</p>
<p>Few-shot</p>
<p>93.9 %</p>
<p>2.3 %</p>
<p>3.8 %</p>
<p>Physics</p>
<p>Evol-Instruct</p>
<p>56.2 %</p>
<p>29.8 %</p>
<p>14.0 %</p>
<p><strong>结论</strong>：</p>
<ul>
<li>Few-shot 稳定性最高；Evol-Instruct 难度提升但可执行率下降。</li>
<li>为后续 RLVR 训练提供 <strong>质量–数量权衡</strong> 依据。</li>
</ul>
<p>3. 合成数据多样性分析</p>
<p><strong>目的</strong>：量化三种策略生成样本与种子之间的 <strong>语义/词汇多样性</strong>。<br><strong>方法</strong></p>
<ul>
<li><strong>嵌入</strong>：OpenAI text-embedding-3-small；</li>
<li><strong>指标</strong>：<ul>
<li>平均/最大余弦相似度（图 3）；</li>
<li>t-SNE 可视化（图 4）。</li>
</ul>
</li>
</ul>
<p><strong>结果</strong></p>
<ul>
<li>Few-shot：平均相似度 0.77，分布最分散 → <strong>表面词汇多样性高</strong>。</li>
<li>Evol-Instruct：平均相似度 0.90+，t-SNE 聚类紧密 → <strong>核心语义保持，结构复杂度提升</strong>。</li>
</ul>
<p>4. 合成数据难度评估</p>
<p><strong>目的</strong>：验证 Evol-Instruct 能否系统提升题目难度。<br><strong>设置</strong></p>
<ul>
<li><strong>模型</strong>：GPT-4.1-mini、DeepSeek-R1；</li>
<li><strong>数据</strong>：Advanced Physics 领域各策略 100 题；</li>
<li><strong>指标</strong>：模型准确率（表 3）。</li>
</ul>
<p><strong>结果</strong></p>
<p>策略</p>
<p>GPT-4.1-mini</p>
<p>DeepSeek-R1</p>
<p>相对种子变化</p>
<p>Seed</p>
<p>71.8 %</p>
<p>77.4 %</p>
<p>baseline</p>
<p>Few-shot</p>
<p>92.0 % ↑</p>
<p>93.2 % ↑</p>
<p>难度略降</p>
<p>Self-Instruct</p>
<p>83.0 % ↑</p>
<p>87.4 % ↑</p>
<p>难度适中</p>
<p>Evol-Instruct</p>
<p>62.0 % ↓</p>
<p>70.3 % ↓</p>
<p>难度显著提升</p>
<p><strong>结论</strong>：Evol-Instruct 在保持语义一致性的同时，通过抽象、复合约束等手段有效提升题目难度，适合作为 RLVR 的“困难样本”来源。</p>
<p>实验全景图</p>
<pre><code>graph TD
    A[LOONGBENCH 基准评测] --&gt;|8 729 题| B[12 领域 × 6 模型]
    C[LOONGENV 合成实验] --&gt;|3 策略| D[可执行率/正确率]
    C --&gt; E[多样性分析]
    C --&gt; F[难度评估]
    B --&gt; G[定位瓶颈]
    D --&gt; H[质量–数量权衡]
    E --&gt; I[语义 vs 词汇多样性]
    F --&gt; J[难度可控性验证]
</code></pre><p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向既可直接在 Loong 框架上迭代，也可作为独立研究课题，分为 <strong>数据、验证器、训练策略、评测与安全</strong> 四大类。</p>
<p>1. 数据层面：广度与深度</p>
<ul>
<li><strong>多语言与多模态</strong><ul>
<li>将 LOONGBENCH 扩展到中文、法文等 STEM 教学主流语言；</li>
<li>引入几何图形、化学结构、电路图等视觉输入，构建 <strong>多模态 CoT</strong> 数据。</li>
</ul>
</li>
<li><strong>难度可控生成</strong><ul>
<li>在 Evol-Instruct 中引入 <strong>可微难度预测器</strong>（类似 Item Response Theory），实现“目标难度 0.3–0.8”的按需生成。</li>
</ul>
</li>
<li><strong>对抗性样本</strong><ul>
<li>设计 <strong>反例生成器</strong>：针对当前模型高置信但错误的题目进行变异，专门用于鲁棒性训练。</li>
</ul>
</li>
</ul>
<p>2. 验证器层面：从 LLM-as-a-judge 到形式化验证</p>
<ul>
<li><strong>领域专用符号验证器</strong><ul>
<li>数学：基于 SymPy/Lean4 的 <strong>形式化证明检查</strong>；</li>
<li>物理：单位推导自动机（Dimensional Analysis Solver）；</li>
<li>化学：反应方程式配平与电荷守恒自动验证。</li>
</ul>
</li>
<li><strong>多验证器投票机制</strong><ul>
<li>同时运行代码执行、符号验证、LLM 判断三路 verifier，采用 <strong>贝叶斯一致性融合</strong> 降低假阳性。</li>
</ul>
</li>
<li><strong>可验证奖励的置信度估计</strong><ul>
<li>为每个合成样本输出 <strong>不确定性分数</strong>（epistemic + aleatoric），在 RLVR 中作为 <strong>动态奖励缩放因子</strong>。</li>
</ul>
</li>
</ul>
<p>3. 训练策略层面：RLVR 的变体与加速</p>
<ul>
<li><strong>课程式 RLVR</strong><ul>
<li>先让 agent 在 Few-shot 生成的“易”数据上收敛，再逐步混入 Evol-Instruct 的“难”数据，模拟人类课程学习。</li>
</ul>
</li>
<li><strong>多任务 RLVR</strong><ul>
<li>共享一个 CoT backbone，领域特定 head 输出代码/符号答案；使用 <strong>GradNorm</strong> 或 <strong>PCGrad</strong> 自动平衡 12 个领域的梯度冲突。</li>
</ul>
</li>
<li><strong>元验证器（Meta-Verifier）</strong><ul>
<li>训练一个小型 gating network，根据问题 embedding 动态选择最合适的 verifier（代码、符号、LLM），降低验证延迟。</li>
</ul>
</li>
</ul>
<p>4. 评测与安全层面</p>
<ul>
<li><strong>对抗鲁棒性基准</strong><ul>
<li>构建 <strong>Loong-Adv</strong>：在原始问题上加入数值扰动、单位混淆、约束缺失等扰动，测试模型是否仍输出正确 CoT。</li>
</ul>
</li>
<li><strong>可解释性评测</strong><ul>
<li>引入 <strong>CoT faithfulness</strong> 指标：用 attribution 方法检查最终答案是否真正由中间步骤推导而来，而非“捷径”模式匹配。</li>
</ul>
</li>
<li><strong>安全与伦理过滤</strong><ul>
<li>在 Security &amp; Safety 领域加入 <strong>CTF-to-Malware 转换检测器</strong>，防止合成数据被用于生成恶意代码；</li>
<li>为 Medicine 领域引入 <strong>HIPAA 合规性检查</strong>，确保合成患者数据去标识化。</li>
</ul>
</li>
</ul>
<p>5. 系统与工程优化</p>
<ul>
<li><strong>分布式代码沙箱</strong><ul>
<li>用 Firecracker/Kata Containers 实现 <strong>毫秒级冷启动</strong> 的代码执行环境，支撑百万级 RLVR rollout。</li>
</ul>
</li>
<li><strong>增量式 LOONGBENCH 更新</strong><ul>
<li>设计 <strong>社区众包接口</strong>：研究者可提交新题目与验证代码，经自动测试后合并到主分支，形成持续扩展的活基准。</li>
</ul>
</li>
</ul>
<p>研究路线图（示例）</p>
<pre><code>gantt
    title Loong 后续探索甘特图（12 个月）
    dateFormat  YYYY-MM
    section 数据
    多语言种子扩展       :2025-10, 3m
    多模态 CoT 生成      :2026-01, 4m
    section 验证器
    数学形式化验证器     :2025-11, 3m
    多验证器融合         :2026-02, 2m
    section 训练
    课程式 RLVR 实验     :2025-12, 3m
    元验证器训练         :2026-03, 2m
    section 评测
    Loong-Adv 对抗基准   :2026-01, 2m
    CoT faithfulness 工具 :2026-04, 2m
</code></pre><p>这些方向既可直接落地于 Loong 开源代码库，也可衍生出新的顶会论文与工业级系统。</p>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p><strong>一句话总结</strong><br>Loong 项目通过“高质量种子数据 + 模块化合成环境 + 代码级验证 + RLVR 训练循环”，首次在 12 个高强度推理领域实现了无需人工标注、可扩展且语义可信的大规模数据生成与模型对齐框架。</p>
<p>1. 问题背景</p>
<ul>
<li>RLVR（强化学习+可验证奖励）已在数学/编程上显著提升 LLM 推理能力，但逻辑、物理、化学、金融等<strong>其他领域缺乏可验证数据集</strong>，人工标注成本极高。</li>
<li>需要一种<strong>可扩展、低成本、跨领域</strong>的合成数据与验证机制。</li>
</ul>
<p>2. 解决方案</p>
<p>组件</p>
<p>功能</p>
<p>规模/特点</p>
<p><strong>LOONGBENCH</strong></p>
<p>人工审核种子数据集</p>
<p>8 729 题，12 领域，每题附可执行 Python 代码与验证答案</p>
<p><strong>LOONGENV</strong></p>
<p>模块化合成数据生成环境</p>
<p>支持 Few-shot / Self-Instruct / Evol-Instruct 三种策略，自动生成问题-答案-代码三元组</p>
<p><strong>Verifier 流水线</strong></p>
<p>保证语义正确性</p>
<p>代码执行 + 独立 CoT 求解 + LLM-as-a-judge 三重验证</p>
<p><strong>RLVR 训练循环</strong></p>
<p>无人工监督的强化学习</p>
<p>用合成答案作为 ground-truth，奖励 CoT 与代码结果一致的模型输出</p>
<p>3. 实验结果</p>
<ul>
<li><strong>基准评测</strong>（6 模型 × 12 领域）：<ul>
<li>难度跨度 10 %–100 %，数学规划、安全、金融为显著瓶颈；</li>
<li>推理专用模型（o3-mini、DeepSeek-R1）在 8/12 领域领先；</li>
<li>开源模型在逻辑、博弈等任务上落后 20–50 pp。</li>
</ul>
</li>
<li><strong>合成数据质量</strong>：<ul>
<li>Few-shot 稳定性最高（可执行率 &gt; 90 %）；</li>
<li>Evol-Instruct 难度显著提升（模型准确率下降 20–30 pp），同时保持语义一致性；</li>
<li>t-SNE 与余弦相似度显示 Evol-Instruct 在结构复杂度与多样性间取得平衡。</li>
</ul>
</li>
</ul>
<p>4. 贡献与开源</p>
<ul>
<li>发布 <strong>LOONGBENCH</strong> 数据集与 <strong>LOONGENV</strong> 代码库（MIT 许可证），支持即插即用的领域扩展与 RLVR 训练。</li>
<li>提出 <strong>Reinforcement Learning from Verified Rewards (RLVR)</strong> 在通用推理领域的可行范式。</li>
</ul>
<p>5. 未来方向</p>
<ul>
<li>多语言、多模态扩展；形式化验证器；课程式 RLVR；对抗鲁棒性基准；社区众包持续更新。</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Loong%3A%20Synthesize%20Long%20Chain-of-Thoughts%20at%20Scale%20through%20Verifiers，链接是  https://arxiv.org/pdf/2509.03059  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.03059  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xingyue Huang,Rishabh,Gregor Franke,Ziyi Yang,Jiamu Bai,Weijie Bai,Jinhe Bi,Zifeng Ding,Yiqun Duan,Chengyu Fan,Wendong Fan,Xin Gao,Ruohao Guo,Yuan He,Zhuangzhuang He,Xianglong Hu,Neil Johnson,Bowen Li,Fangru Lin,Siyu Lin,Tong Liu,Yunpu Ma,Hao Shen,Hao Sun,Beibei Wang,Fangyijie Wang,Hao Wang,Haoran Wang,Yang Wang,Yifeng Wang,Zhaowei Wang,Ziyang Wang,Yifan Wu,Zikai Xiao,Chengxing Xie,Fan Yang,Junxiao Yang,Qianshuo Ye,Ziyu Ye,Guangtao Zeng,Yuwen Ebony Zhang,Zeyu Zhang,Zihao Zhu,Bernard Ghanem,Philip Torr,Guohao Li</p>
<p><strong>Categories</strong>: cs.LG,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03059.pdf">https://arxiv.org/pdf/2509.03059.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03059">https://arxiv.org/abs/2509.03059</a></p>
<p><strong>Arxiv ID</strong>: 2509.03059</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03059">https://papers.cool/arxiv/2509.03059</a></p>
<p><strong>Published</strong>: 2025-09-03T06:42:40Z</p>
<p><strong>Updated</strong>: 2025-09-03T06:42:40.000Z</p>
<hr>
<h3 id="11-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer"><a href="#11-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer" class="headerlink" title="11. Durian: Dual Reference-guided Portrait Animation with Attribute Transfer"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04434">Durian: Dual Reference-guided Portrait Animation with Attribute Transfer</a></h3><p>We present Durian, the first method for generating portrait animation videos with facial attribute transfer from a given reference image to a target portrait in a zero-shot manner. To enable high-fidelity and spatially consistent attribute transfer across frames, we introduce dual reference networks that inject spatial features from both the portrait and attribute images into the denoising process of a diffusion model. We train the model using a self-reconstruction formulation, where two frames are sampled from the same portrait video: one is treated as the attribute reference and the other as the target portrait, and the remaining frames are reconstructed conditioned on these inputs and their corresponding masks. To support the transfer of attributes with varying spatial extent, we propose a mask expansion strategy using keypoint-conditioned image generation for training. In addition, we further augment the attribute and portrait images with spatial and appearance-level transformations to improve robustness to positional misalignment between them. These strategies allow the model to effectively generalize across diverse attributes and in-the-wild reference combinations, despite being trained without explicit triplet supervision. Durian achieves state-of-the-art performance on portrait animation with attribute transfer, and notably, its dual reference design enables multi-attribute composition in a single generation pass without additional training.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了 Durian，这是一种首个通过零-shot 方法从给定参考图像到目标肖像生成肖像动画视频的技术。为了实现高保真和空间一致的属性转移，我们引入了双重参考网络，将来自肖像和属性图像的空间特征注入到扩散模型的去噪过程中。我们使用自我重建公式训练模型，其中从同一肖像视频中采样出两个帧：一个被视为属性参考，另一个作为目标肖像，其余帧根据这些输入及其相应的掩码进行重建。为了支持具有不同空间范围的属性转移，我们提出了一种基于关键点的图像生成掩码扩展策略用于训练。此外，我们还通过空间和外观级的变换进一步增强属性和肖像图像，以提高对它们之间位置错位的鲁棒性。这些策略使得模型能够在多样属性和实际参考组合之间有效地推广，尽管在训练过程中没有明确的三元组监督。Durian 在具有属性转移的肖像动画领域达到了领先的性能，值得注意的是，其双重参考设计使得在单次生成中进行多属性组合成为可能，无需额外训练。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决“零样本肖像动画中跨身份属性迁移”这一核心问题，具体可归纳为以下三点：</p>
<ol>
<li><p>动态属性迁移<br>现有方法多局限于静态图像编辑，无法让单一肖像在视频中自然地“戴上”参考图中的发型、胡须或眼镜等可变形属性，并随表情与姿态变化保持时空一致性。</p>
</li>
<li><p>免三元组监督<br>传统思路依赖“原图-参考属性-目标结果”三元组数据，构建成本极高且难以扩展。论文提出用自重建策略，从同一段肖像视频中采样两帧即可模拟迁移过程，摆脱昂贵的人工标注。</p>
</li>
<li><p>通用性与组合性<br>以往工作通常针对单一属性（如头发）设计专用流程。本文框架在单次前向传播中即可支持多种属性同时迁移，并可对同类属性进行平滑插值，无需额外训练或后处理。</p>
</li>
</ol>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<ul>
<li><p><strong>Face Image and Video Editing</strong></p>
<ul>
<li>StyleGAN 系列（Karras et al., 2020）与 GAN 反演（Zhu et al., 2016; Richardson et al., 2021）</li>
<li>3D-aware 编辑：TriplaneEdit（Bilecen et al., 2024）、In-N-Out（Xu et al., 2024）</li>
<li>扩散式编辑：InstructPix2Pix（Brooks et al., 2023）、IP-Adapter（Ye et al., 2023）</li>
</ul>
</li>
<li><p><strong>Diffusion-based Virtual Try-On</strong></p>
<ul>
<li>静态掩码修复：Paint-by-Example（Yang et al., 2023）、AnyDoor（Chen et al., 2024）</li>
<li>视频尝试：Vivid（Fang et al., 2024）、VideoAnyDoor（Tu et al., 2025）</li>
<li>发型专用：HairFusion（Chung et al., 2025）、StableHair（Zhang et al., 2025）</li>
</ul>
</li>
<li><p><strong>单图肖像动画</strong></p>
<ul>
<li>GAN 路线：LivePortrait（Guo et al., 2024）</li>
<li>扩散路线：AnimateAnyone（Hu, 2024）、Champ（Zhu et al., 2024）、X-Portrait（Xie et al., 2024）、MegActor-Σ（Yang et al., 2025）</li>
</ul>
</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<ul>
<li><p><strong>自重建训练范式</strong><br>从同一段肖像视频中随机抽取两帧，分别作为“属性参考”与“目标肖像”，用分割掩码构造互补的 masked 输入，让扩散模型重建剩余帧。无需人工三元组即可学习跨身份属性映射。</p>
</li>
<li><p><strong>双参考网络（Dual ReferenceNet）</strong></p>
<ul>
<li><strong>ARNet</strong> 提取属性图像的多尺度空间特征</li>
<li><strong>PRNet</strong> 提取肖像图像的多尺度空间特征<br>二者通过“宽度级联”空间注意力注入到去噪 U-Net 的每一层，实现属性与身份特征的显式解耦与逐像素融合。</li>
</ul>
</li>
<li><p><strong>属性感知掩码扩张</strong><br>利用 SDXL+ControlNet 生成同一属性不同形状/覆盖范围的样本，并与原掩码取并集，强迫模型在训练时面对多样化的空间范围，提升对长短发、胡须形状等变化的泛化能力。</p>
</li>
<li><p><strong>参考图像增广</strong><br>对输入对施加随机仿射变换与颜色抖动，并用 FLUX-outpainting 补全空白区域，模拟真实场景中的姿态与光照错位，增强鲁棒性。</p>
</li>
<li><p><strong>推理阶段 3D 对齐</strong><br>借助 GAGAvatar 将属性图像重建为 3D 头像，再以目标肖像的 FLAME 参数驱动，得到姿态对齐的新掩码，缓解属性-肖像空间不一致问题。</p>
</li>
<li><p><strong>单次多属性与插值</strong><br>将多个属性特征在宽度维度拼接后统一参与空间注意力，或在线性空间对两属性特征进行加权混合，实现零样本组合与平滑过渡，无需额外训练。</p>
</li>
</ul>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<ul>
<li><p><strong>主实验：与 12 组两阶段基线对比</strong><br>将四种静态属性迁移方法（PbE、HairFusion、StableHair、TriplaneEdit）分别与三种主流肖像动画模型（LivePortrait、X-Portrait、MegActor-Σ）级联，在自建的 200 段测试视频上量化比较。指标：L1、PSNR、SSIM、LPIPS、FID。Durian 在所有指标上均排名第一。</p>
</li>
<li><p><strong>消融实验</strong></p>
<ol>
<li>单 ReferenceNet（共享编码器）</li>
<li>无掩码扩张</li>
<li>无参考图像增广</li>
<li>去掉掩码通道输入</li>
<li>使用完整未掩码图像训练<br>结果验证双分支结构、掩码扩张与增广对身份保持与属性准确性的关键作用。</li>
</ol>
</li>
<li><p><strong>多属性组合</strong><br>一次性迁移 2–3 种属性（发型+眼镜+胡须等），验证空间注意力机制可在重叠区域生成合理遮挡关系，无需再训练。</p>
</li>
<li><p><strong>属性插值</strong><br>对同类别两张参考图进行特征线性混合（α∈[0,1]），生成连续过渡的视频，验证语义空间的平滑性与一致性。</p>
</li>
<li><p><strong>文本驱动属性迁移（扩展应用）</strong><br>先用 FLUX 根据文本提示生成属性图像，再输入 Durian，实现“文本→图像→视频”的完整链路示例。</p>
</li>
</ul>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<ul>
<li><p><strong>极端姿态与非正面视角</strong><br>当前训练数据以正脸为主，对侧脸、俯视或仰视等极端角度以及背面视图的属性迁移仍不稳定，可引入多视角视频或 3D-aware 采样策略提升覆盖。</p>
</li>
<li><p><strong>复杂遮挡与层级关系建模</strong><br>帽子压头发、口罩覆胡须等精细遮挡场景易出现穿透或混合瑕疵，可显式引入深度/层级顺序估计或基于物理的碰撞损失进行约束。</p>
</li>
<li><p><strong>光照与阴影一致性</strong><br>当参考属性与目标肖像存在强烈光照差异（强阴影、高光）时，迁移结果可能出现局部亮度跳变，可探索 HDR 域对齐或光照归一化模块。</p>
</li>
<li><p><strong>全身与服饰属性扩展</strong><br>将框架从头肩肖像扩展到半身或全身，实现上衣、手套、包包等动态服饰的跨身份迁移，需解决大位移形变与肢体自遮挡问题。</p>
</li>
<li><p><strong>语义-几何解耦控制</strong><br>在属性插值基础上进一步分离「形状几何」与「外观纹理」，允许用户独立调节卷曲度、长度、颜色等细粒度因子，实现更直观的编辑界面。</p>
</li>
<li><p><strong>实时或轻量化推理</strong><br>当前模型依赖重型扩散迭代，可研究蒸馏、量化或基于潜码直接回归的加速方案，以满足移动端 AR 实时预览需求。</p>
</li>
<li><p><strong>鲁棒关键点与跟踪</strong><br>对快速运动、模糊或遮挡视频，现有关键点提取易失效导致抖动，可结合时空滤波或神经辐射场跟踪提升稳定性。</p>
</li>
<li><p><strong>无关键点的运动控制</strong><br>探索音频驱动、文本描述或草迹轨迹作为替代运动源，降低对精确关键点的依赖，实现更灵活的动画创作流程。</p>
</li>
</ul>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p><strong>Durian：零样本、双参考、可组合的肖像动画属性迁移</strong></p>
<p>维度</p>
<p>核心内容</p>
<p><strong>问题</strong></p>
<p>给定单张肖像与单张参考属性图，生成一段保持身份、表情与姿态自然变化且无缝携带目标属性（发型/胡须/眼镜等）的视频；无需三元组数据、无需逐帧掩码、支持多属性一次性组合。</p>
<p><strong>方法</strong></p>
<p>1) 自重建训练：同视频两帧互为“属性-肖像”，用分割掩码构造互补输入，扩散模型重建剩余帧。<br>2) 双参考网络：ARNet 提取属性特征，PRNet 提取身份特征，通过宽度级联空间注意力注入去噪 U-Net，实现属性-身份显式解耦。<br>3) 掩码扩张+图像增广：SDXL 生成同属性不同形状，随机仿射与颜色扰动提升泛化。<br>4) 推理阶段：3D 头像对齐掩码，关键点序列驱动，单次前向完成时序一致动画。</p>
<p><strong>实验</strong></p>
<p>① 与 12 组“静态属性迁移+视频动画”两阶段基线对比，Durian 在 L1/PSNR/SSIM/LPIPS/FID 全部领先。<br>② 消融验证双分支、掩码扩张、增广对身份保持与属性准确性的必要性。<br>③ 展示零样本多属性组合与属性插值，无需再训练即可生成平滑过渡。</p>
<p><strong>贡献</strong></p>
<p>首次实现零样本、端到端、可组合的肖像动画属性迁移；提出自重建+双参考网络框架，兼顾高保真、时空一致与扩展性。</p>
<p><strong>局限&amp;未来</strong></p>
<p>极端姿态、复杂遮挡、强光阴影差异仍需提升；可扩展至全身服饰、实时推理、语义-几何解耦控制等方向。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Durian%3A%20Dual%20Reference-guided%20Portrait%20Animation%20with%20Attribute%20Transfer，链接是  https://arxiv.org/pdf/2509.04434  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04434  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hyunsoo Cha,Byungjun Kim,Hanbyul Joo</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04434.pdf">https://arxiv.org/pdf/2509.04434.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04434">https://arxiv.org/abs/2509.04434</a></p>
<p><strong>Arxiv ID</strong>: 2509.04434</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04434">https://papers.cool/arxiv/2509.04434</a></p>
<p><strong>Published</strong>: 2025-09-04T17:53:03Z</p>
<p><strong>Updated</strong>: 2025-09-04T17:53:03.000Z</p>
<hr>
<h3 id="12-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings"><a href="#12-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings" class="headerlink" title="12. Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2508.18733">Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings</a></h3><p>Computer-Aided Design (CAD) generative modeling is driving significant innovations across industrial applications. Recent works have shown remarkable progress in creating solid models from various inputs such as point clouds, meshes, and text descriptions. However, these methods fundamentally diverge from traditional industrial workflows that begin with 2D engineering drawings. The automatic generation of parametric CAD models from these 2D vector drawings remains underexplored despite being a critical step in engineering design. To address this gap, our key insight is to reframe CAD generation as a sequence-to-sequence learning problem where vector drawing primitives directly inform the generation of parametric CAD operations, preserving geometric precision and design intent throughout the transformation process. We propose Drawing2CAD, a framework with three key technical components: a network-friendly vector primitive representation that preserves precise geometric information, a dual-decoder transformer architecture that decouples command type and parameter generation while maintaining precise correspondence, and a soft target distribution loss function accommodating inherent flexibility in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing, a dataset of paired engineering drawings and parametric CAD models, and conduct thorough experiments to demonstrate the effectiveness of our method. Code and dataset are available at <a target="_blank" rel="noopener" href="https://github.com/lllssc/Drawing2CAD">https://github.com/lllssc/Drawing2CAD</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>计算机辅助设计（CAD）生成建模正在推动工业应用中的重大创新。最近的研究表明，从点云、网格和文本描述等各种输入中创建实心模型取得了显著进展。然而，这些方法与传统的工业工作流程存在根本性的差异，后者以2D工程图纸为起点。从这些2D矢量图生成参数化CAD模型的自动化过程仍未被深入探索，尽管这是工程设计中的一个关键步骤。为了填补这一空白，我们的关键见解是将CAD生成重新框定为一个序列到序列的学习问题，在这个过程中，矢量图绘制原语直接影响参数化CAD操作的生成，保持几何精度和设计意图贯穿整个转换过程。我们提出了Drawing2CAD，这是一个具有三个关键技术组件的框架：一种可与网络兼容的矢量原语表示，能够保留精确的几何信息；一种双解码器变换器架构，能够解耦命令类型和参数生成，同时保持精确的对应关系；一种柔性目标分布损失函数，能够适应CAD参数的固有灵活性。为了训练和评估Drawing2CAD，我们创建了CAD-VGDrawing，这是一个配对工程图纸和参数化CAD模型的数据集，并进行了全面的实验以证明我们方法的有效性。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/lllssc/Drawing2CAD获得。">https://github.com/lllssc/Drawing2CAD获得。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在解决<strong>从二维矢量工程图（SVG）自动生成参数化CAD模型</strong>这一工业界长期存在但研究不足的问题。核心挑战在于：</p>
<ol>
<li><strong>工业流程脱节</strong>：现有深度学习方法多从点云、网格或文本生成CAD，背离了“先二维工程图、后三维建模”的实际设计流程。</li>
<li><strong>矢量图未被利用</strong>：工程图天然以矢量形式存在，具备精确几何与约束信息，却缺乏直接驱动参数化CAD序列的研究。</li>
<li><strong>维度与模态鸿沟</strong>：将二维矢量图映射到三维参数化操作序列需要同时保持几何精度与设计意图，涉及跨模态、跨维度的序列到序列学习。</li>
</ol>
<p>为此，作者提出<strong>Drawing2CAD</strong>框架，把CAD生成重新定义为**“矢量图元序列 → CAD操作序列”**的端到端序列到序列学习任务，以填补从二维矢量工程图到三维参数化模型的自动化空白。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<ul>
<li><p><strong>参数化 CAD 生成</strong></p>
<ul>
<li>早期基于深度学习的 CAD 序列生成：DeepCAD、Fusion 360 Gallery</li>
<li>后续引入 Transformer 结构：SkexGen、TransCAD</li>
<li>扩散模型：Diffusion-CAD</li>
<li>输入模态：点云（Point2CAD、Point2Cyl）、网格（Mesh2Brep）、体素、多视图图像（Img2CAD、View2CAD）、文本（Text2CAD、CAD Translator）</li>
<li>共同局限：均为“3D→CAD”或“文本→CAD”，未触及工业界“2D 工程图→CAD”的起点</li>
</ul>
</li>
<li><p><strong>基于草图的 CAD 建模</strong></p>
<ul>
<li>传统几何推理：三视图重建、二次曲面拟合、线框恢复</li>
<li>特征识别：从工程图中提取孔、槽等高层特征</li>
<li>深度学习：Free2CAD、Photo2CAD 等以光栅草图为输入，用 CNN 或规则推断 CAD 操作</li>
<li>根本缺陷：光栅图像分辨率、线宽、伪影导致几何精度损失，无法保持工程图尺寸约束</li>
</ul>
</li>
<li><p><strong>矢量图形的深度学习</strong></p>
<ul>
<li>生成与表示：DeepSVG、StrokeNUWA、Im2Vec、DeepVecFont</li>
<li>应用场景：字体、图标、插画、动画</li>
<li>空白领域：未与工程图或 CAD 参数化建模结合</li>
</ul>
</li>
</ul>
<p>综上，现有研究集中于“3D 反向重建”或“光栅草图”，尚未出现<strong>直接从矢量工程图生成参数化 CAD 序列</strong>的工作，Drawing2CAD 填补了这一交叉空白。</p>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文将“矢量工程图→参数化 CAD”这一跨模态、跨维度问题形式化为<strong>序列到序列学习任务</strong>，提出 Drawing2CAD 框架，通过三项关键技术实现精准、可执行的 CAD 操作序列生成：</p>
<p>技术组件</p>
<p>核心思想</p>
<p>具体做法</p>
<p><strong>网络友好矢量表示</strong></p>
<p>保留几何精度，统一输入格式</p>
<p>仅保留 <code>LineTo</code> 与 <code>CubicBézier</code> 两类图元，每条命令固化为 8 维实数向量；四视图分别打上视图标签，序列长度统一为 100，形成结构化 token 序列</p>
<p><strong>双解码器 Transformer</strong></p>
<p>解耦“命令类型”与“参数值”，保证一一对应</p>
<p>编码器输出单一 latent vector <code>z</code>：<br>• Command Decoder：自回归生成 CAD 命令类型（Line/Circle/Arc/Extrude 等）<br>• Argument Decoder：同步生成对应参数向量；将 Command Decoder 的输出加至 Argument Decoder，实现“命令引导参数”的上下文对齐</p>
<p><strong>软目标分布损失</strong></p>
<p>容忍合理误差，提升泛化与几何保真</p>
<p>对参数分类任务采用平滑标签：以真值类别为中心，按 `exp(-α</p>
<p>此外，作者构建<strong>CAD-VGDrawing</strong> 数据集（157 k 矢量- CAD 配对），支持单等轴、三正交或四视图联合输入。端到端训练后，生成的命令序列直接送入 OpenCASCADE 内核即可重建 3D 实体。实验表明，该方法在命令准确率、参数精度、无效模型比例、倒角距离等指标上均优于光栅输入基线，验证了“矢量图元序列→CAD 操作序列”范式的有效性与工业落地潜力。</p>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕“矢量工程图→参数化 CAD 序列”这一新任务，共设计了 4 组系统性实验，覆盖输入模态、方法对比、组件贡献与失败案例：</p>
<ol>
<li><p>输入模态对比<br>控制网络结构不变，仅替换输入编码器：</p>
<ul>
<li>DeepCAD-raster：ViT 预训练编码器 + 224×224 PNG</li>
<li>DeepCAD-vector：相同 Transformer 但直接接受 SVG token<br>结果：三视图配置下，矢量输入在命令准确率、参数准确率、无效模型率、倒角距离上分别提升 <strong>+4.6 %、+4.0 %、−7.4 %、−6.6×10⁻²</strong>，验证矢量图比光栅图更适合 CAD 生成。</li>
</ul>
</li>
<li><p>方法级对比<br>在相同矢量输入下，将 Drawing2CAD 与 DeepCAD-vector、传统规则法 Photo2CAD 比较：</p>
<ul>
<li>Drawing2CAD 在 1×/3×/4× 视图下均取得最高 <strong>ACCcmd、ACCparam</strong> 与最低 <strong>IR</strong></li>
<li>可视化显示 DeepCAD 出现命令缺失、拓扑错误；Photo2CAD 对复杂几何完全失效，而 Drawing2CAD 能保留孔、倒角等细部特征。</li>
</ul>
</li>
<li><p>消融实验（Ablation）<br>逐步添加核心组件：<br>(a) 仅双解码器 → (b) 加入软目标损失 → (c) 加入命令引导 → (d) 替换为拼接式嵌入<br>指标追踪 <strong>ACCcmd、ACCparam、IR、CD</strong>；最终完整模型在 4× 视图下 <strong>IR 降至 20.31 %</strong>，相较基线 <strong>−3.1 %</strong>，证明每一组件均带来统计显著增益，且拼接嵌入策略对多视图融合最关键。</p>
</li>
<li><p>失败案例与局限分析<br>选取代表性错误模型，归纳 5 类典型局限：</p>
<ul>
<li>参数精度漂移（厚度/孔径误差）</li>
<li>视图偏好（正交图深度误判、等轴图平面错位）</li>
<li>多视图冲突（凸台位置不一致）</li>
<li>遮挡导致的几何缺失（侧孔不可见）<br>通过可视化对比 1×/3×/4× 输入，指出未来需引入不确定性建模、神经渲染或隐式先验以提升鲁棒性。</li>
</ul>
</li>
</ol>
<p>以上实验在 <strong>157 k CAD-VGDrawing</strong> 训练集、单卡 RTX 4090、200 epoch 下完成，代码与数据已开源，可复现。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可推动“矢量工程图→参数化 CAD”研究从概念验证走向工业级落地：</p>
<ol>
<li><p>几何-语义联合推理</p>
<ul>
<li>引入符号约束解析器，将尺寸标注、GD&amp;T 符号与几何图元联合编码，实现“标注驱动”的参数精确回归</li>
<li>融合神经-符号方法，在序列生成阶段显式施加尺寸链、平行/垂直等工程约束，减少后续参数微调</li>
</ul>
</li>
<li><p>不确定性建模与可控生成</p>
<ul>
<li>对参数预测引入概率分布（Gaussian、VAE 或扩散），用户可采样多种满足公差带的可行模型</li>
<li>提供交互式滑块/自然语言指令，对壁厚、孔径等关键尺寸进行细粒度编辑，实现“草图-对话”混合设计</li>
</ul>
</li>
<li><p>遮挡与隐藏特征推断</p>
<ul>
<li>采用神经渲染或差分视图合成，在训练阶段生成虚拟剖视图，增强模型对不可见孔、内腔的补全能力</li>
<li>引入隐式 B-rep 先验或大规模预训练，利用统计形状先验恢复被遮挡几何</li>
</ul>
</li>
<li><p>多视图融合策略升级</p>
<ul>
<li>动态视图选择机制：根据图纸复杂度自适应决定需提供哪些视图，降低人工选图成本</li>
<li>基于注意力的跨视图一致性检查，显式检测并解决不同视图中矛盾的几何线索</li>
</ul>
</li>
<li><p>序列长度与复杂度扩展</p>
<ul>
<li>层次化 Transformer / 递归子程序，支持 500+ 命令的长序列，处理多体、多工序装配</li>
<li>引入“草图-拉伸-修饰”三阶段解码，先预测草图轮廓，再预测拉伸方向/深度，最后预测倒角、孔等修饰特征，降低搜索空间</li>
</ul>
</li>
<li><p>跨域与少样本泛化</p>
<ul>
<li>构建跨 CAD 平台（SolidWorks、NX、Creo）统一命令本体，实现零样本迁移</li>
<li>利用字体、图标等大规模矢量数据预训练 SVG 编码器，再在少量工程图上微调，缓解标注依赖</li>
</ul>
</li>
<li><p>可制造性联合优化</p>
<ul>
<li>在损失函数中引入制造代价（加工时间、刀具可达性、支撑结构），生成可直接上机的工艺友好模型</li>
<li>与 CAM 引擎闭环：生成的 CAD 序列即时送入刀具路径仿真，反馈可制造性信号进行强化学习微调</li>
</ul>
</li>
<li><p>数据集与评测协议</p>
<ul>
<li>扩展 CAD-VGDrawing 至少 1 M 样本，覆盖钣金、注塑、机加等多领域；提供多粒度标注（尺寸、公差、表面粗糙度）</li>
<li>建立在线评测平台，统一度量“几何误差 + 参数误差 + 可制造性 + 设计意图一致性”，推动社区公平比较</li>
</ul>
</li>
<li><p>实时交互与云端部署</p>
<ul>
<li>研发轻量级移动端 SVG→CAD 推理引擎，支持平板手绘草图即时生成 3D 模型</li>
<li>结合协同设计云，支持多用户同步编辑矢量草图，实时合并各自生成的参数化特征序列</li>
</ul>
</li>
<li><p>安全与版权</p>
<ul>
<li>研究矢量图水印与 CAD 模型指纹，防止生成式框架泄露或侵犯企业专有设计</li>
<li>构建可验证的生成轨迹审计机制，满足航空航天、汽车等高风险行业的合规要求</li>
</ul>
</li>
</ol>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>Drawing2CAD 核心内容速览</p>
<p>维度</p>
<p>要点</p>
<p><strong>问题</strong></p>
<p>工业流程从 2D 矢量工程图开始，却缺乏“SVG → 参数化 CAD 序列”的自动化方法；现有研究局限于点云、网格、光栅草图或文本，几何精度与设计意图难以保证。</p>
<p><strong>洞察</strong></p>
<p>把 CAD 建模视为“序列到序列”任务：矢量图元命令序列 ⇨ CAD 操作命令序列，可直接利用工程图精确几何。</p>
<p><strong>数据</strong></p>
<p>自建 CAD-VGDrawing 数据集：157 k 对 {SVG 四视图, 参数化 CAD 模型}，含 161 k 过滤前样本与详细预处理脚本。</p>
<p><strong>方法</strong></p>
<p><strong>Drawing2CAD</strong> 三件套：<br>1) 网络友好矢量表示：仅保留 <code>LineTo</code>/<code>CubicBézier</code>，8 维定长向量 + 视图标签，序列长度 100；<br>2) 双解码器 Transformer：Command Decoder 先生成命令类型，Argument Decoder 再生成参数，命令输出显式引导参数解码；<br>3) 软目标分布损失：对参数分类采用 `exp(-α</p>
<p><strong>实验</strong></p>
<p>• 矢量输入 vs 光栅输入：三视图下命令准确率 ↑4.6 %、无效模型率 ↓7.4 %、倒角距离 ↓6.6×10⁻²；<br>• 与 DeepCAD-vector 相比：4 视图下 ACCcmd 82.43 % vs 81.51 %，IR 20.31 % vs 23.40 %；<br>• 消融：双解码器、软损失、命令引导、拼接嵌入逐组件有效；<br>• 失败案例分析：参数漂移、视图偏好、多视图冲突、遮挡缺失，提出不确定性建模与神经渲染等未来方向。</p>
<p><strong>贡献</strong></p>
<p>1. 首次提出“矢量工程图→参数化 CAD 序列”端到端框架；<br>2. 网络友好矢量表示 + 双解码器 + 软目标损失，显著提升精度与鲁棒性；<br>3. 发布 157 k 规模 CAD-VGDrawing 数据集，为后续研究提供基准。</p>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Drawing2CAD%3A%20Sequence-to-Sequence%20Learning%20for%20CAD%20Generation%20from%20Vectorized%20Drawings，链接是  https://arxiv.org/pdf/2508.18733  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.18733  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Feiwei Qin,Shichao Lu,Junhao Hou,Changmiao Wang,Meie Fang,Ligang Liu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.18733.pdf">https://arxiv.org/pdf/2508.18733.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.18733">https://arxiv.org/abs/2508.18733</a></p>
<p><strong>Arxiv ID</strong>: 2508.18733</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.18733">https://papers.cool/arxiv/2508.18733</a></p>
<p><strong>Published</strong>: 2025-08-26T07:01:58Z</p>
<p><strong>Updated</strong>: 2025-08-26T07:01:58.000Z</p>
<hr>
<h3 id="13-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models"><a href="#13-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models" class="headerlink" title="13. Delta Activations: A Representation for Finetuned Large Language Models"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.04442">Delta Activations: A Representation for Finetuned Large Language Models</a></h3><p>The success of powerful open source Large Language Models (LLMs) has enabled the community to create a vast collection of post-trained models adapted to specific tasks and domains. However, navigating and understanding these models remains challenging due to inconsistent metadata and unstructured repositories. We introduce Delta Activations, a method to represent finetuned models as vector embeddings by measuring shifts in their internal activations relative to a base model. This representation allows for effective clustering by domain and task, revealing structure in the model landscape. Delta Activations also demonstrate desirable properties: it is robust across finetuning settings and exhibits an additive property when finetuning datasets are mixed. In addition, we show that Delta Activations can embed tasks via few-shot finetuning, and further explore its use for model selection and merging. We hope Delta Activations can facilitate the practice of reusing publicly available models. Code is available at <a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations">https://github.com/OscarXZQ/delta_activations</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>开源大型语言模型（LLMs）的成功使社区能够创建大量适应特定任务和领域的后训练模型。然而，由于元数据不一致和非结构化的存储库，导航和理解这些模型仍然具有挑战性。我们引入了Delta Activations，一种通过测量内部激活相对于基模型的偏移来表示微调模型为向量嵌入的方法。这种表示允许按领域和任务进行有效的聚类，揭示模型空间中的结构。Delta Activations还展示了理想的属性：在微调设置下具有鲁棒性，并在微调数据集混合时表现出可加性。此外，我们还展示了Delta Activations可以通过少样本微调来嵌入任务，并进一步探讨其在模型选择和合并中的应用。我们希望Delta Activations能够促进重用公开可用模型的实践。代码可在 <a target="_blank" rel="noopener" href="https://github.com/OscarXZQ/delta_activations">https://github.com/OscarXZQ/delta_activations</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p><strong>问题陈述</strong></p>
<ul>
<li><strong>核心痛点</strong>：开源大模型生态中已存在大量针对特定任务或领域微调后的模型，但由于缺乏统一的元数据、命名含糊、文档稀少，导致用户难以发现、比较和复用这些模型，造成训练资源的浪费。</li>
<li><strong>技术空白</strong>：现有模型表示方法依赖训练数据、权重降维或评测结果，存在数据不可获取、架构不一致或提示敏感等局限，无法仅凭模型自身生成稳定、语义化的嵌入。</li>
<li><strong>目标</strong>：提出一种<strong>无需任何外部信息</strong>、<strong>仅基于模型内部激活变化</strong>的通用向量表示（Delta Activations），使得<ol>
<li>不同微调模型可在同一向量空间中按领域/任务自动聚类；</li>
<li>支持任务嵌入、跨架构比较、模型选择与合并等下游应用；</li>
<li>具备可加性、鲁棒性等理想性质，促进公开模型的可持续复用。</li>
</ol>
</li>
</ul>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<p>以下研究从不同角度与 Delta Activations 存在关联，可分为 <strong>模型表示、激活分析、模型复用与合并、模型库构建</strong> 四大类。</p>
<p>1. 模型表示与嵌入</p>
<p>方法</p>
<p>核心思想</p>
<p>与 Delta Activations 的关系</p>
<p><strong>Task Arithmetic / Task Vectors</strong><br>Ilharco et al. [23]</p>
<p>将微调后的权重差视为“任务向量”，支持向量加法实现多任务融合。</p>
<p>Delta Activations 把权重差换成激活差，同样具备 <strong>加法性质</strong>，但无需显式权重访问。</p>
<p><strong>EmbedLLM</strong> [79]</p>
<p>用评测结果构造低维嵌入。</p>
<p>依赖评测与提示，受 prompt 变化影响大；Delta Activations 直接基于内部激活，更稳定。</p>
<p><strong>LoRA Retriever</strong> [77]</p>
<p>基于 LoRA 权重做检索式复用。</p>
<p>需权重同质且可访问；Delta Activations 对架构差异更鲁棒，可跨架构使用 Delta-Meaning。</p>
<p><strong>Learnware</strong> [60]</p>
<p>用 adapter 权重+元数据构建模型库。</p>
<p>仍依赖训练数据描述；Delta Activations 完全不需要元数据。</p>
<p>2. 激活分析与行为理解</p>
<p>研究</p>
<p>核心发现</p>
<p>与 Delta Activations 的关系</p>
<p><strong>Massive Activations</strong> [57]</p>
<p>发现少量神经元对大模型输出有全局“偏见”作用。</p>
<p>支持“激活位移即可反映行为变化”的假设，Delta Activations 可视为全局位移的聚合。</p>
<p><strong>In-Context Vectors</strong> [36]</p>
<p>通过操控隐藏状态实现上下文学习控制。</p>
<p>同样利用激活空间的方向性，说明激活差异具有可解释性。</p>
<p><strong>PACE</strong> [39]</p>
<p>用激活干预进行概念编辑。</p>
<p>激活空间具备线性语义结构，与 Delta 的可加性一致。</p>
<p>3. 模型复用、合并与路由</p>
<p>方法</p>
<p>核心思想</p>
<p>与 Delta Activations 的关系</p>
<p><strong>AdapterSoup</strong> [5]</p>
<p>权重空间平均多个任务 adapter。</p>
<p>Delta Activations 的加法性质可用于指导加权平均或选择子集，缓解干扰 [47]。</p>
<p><strong>LoRAHub</strong> [21]</p>
<p>动态组合 200+ LoRA 模块提升零样本任务表现。</p>
<p>论文用 Delta Activations 替代随机选择，提升 BBH 基准 2% 准确率。</p>
<p><strong>RouteLLM</strong> [46]</p>
<p>训练路由器在多个 LLM 间做推理时路由。</p>
<p>Delta 嵌入可作为路由特征，实现基于任务相似度的软路由。</p>
<p>4. 模型库与血缘发现</p>
<p>研究</p>
<p>核心贡献</p>
<p>与 Delta Activations 的关系</p>
<p><strong>Neural Phylogeny</strong> [73]</p>
<p>无监督恢复模型间微调血缘树。</p>
<p>Delta 嵌入可直接用于血缘/领域聚类，无需训练额外探针模型。</p>
<p><strong>Unsupervised Model Tree</strong> [19]</p>
<p>通过权重相似度推断微调路径。</p>
<p>Delta 激活差比权重差更紧凑，且对 LoRA/全参微调均适用。</p>
<p>小结</p>
<ul>
<li><strong>任务向量</strong>与<strong>加法性质</strong>为 Delta Activations 提供理论动机；</li>
<li><strong>激活操控/编辑</strong>研究验证了激活位移的语义可解释性；</li>
<li><strong>LoRAHub、AdapterSoup</strong> 等模型合并场景可直接利用 Delta 嵌入做选择与加权；</li>
<li><strong>Neural Phylogeny</strong> 等血缘发现工作可与 Delta 嵌入互补，构建更完整的模型地图。</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文通过提出 <strong>Delta Activations</strong> 框架，将“如何为大量无元数据的微调模型生成统一、可比较、可聚类的向量表示”这一问题拆解为以下步骤并逐一解决：</p>
<p>1. 定义可计算的表示对象</p>
<ul>
<li><strong>输入</strong>：一个基础大模型 (f_{\text{base}}) 与任意由其微调得到的模型 (f)。</li>
<li><strong>表示目标</strong>：仅用模型自身信息构造向量 (v_f \in \mathbb{R}^d)，反映微调带来的行为差异。</li>
</ul>
<p>2. 设计无偏探针输入</p>
<ul>
<li><strong>构造 D_probe</strong>：<ul>
<li>5 条完全通用、无领域倾向的指令模板（Alpaca 模板 + GPT-4o 改写），确保任何模型都会经历相同计算路径。</li>
<li>实验验证：模板越通用、长度越适中，聚类效果越好（表 5）。</li>
</ul>
</li>
</ul>
<p>3. 提取激活位移（Delta）</p>
<ul>
<li><strong>单条输入</strong> (x) 的激活位移<br>[ \Delta_f(x) = h_f(x) - h_{\text{base}}(x) ]<br>其中 (h) 取 <strong>最后一个 token 在最后一层的隐藏状态</strong>（实验显示 2/3 深度略优，但最后一层已足够，表 6）。</li>
<li><strong>聚合</strong><br>[ v_f = \frac{1}{|D_{\text{probe}}|} \sum_{x \in D_{\text{probe}}} \Delta_f(x) ]<br>一次前向即可得到 4096 维向量，计算开销远低于评测式方法。</li>
</ul>
<p>4. 验证表示有效性</p>
<ul>
<li><strong>聚类实验</strong>（表 2、图 3）：<ul>
<li>15 个 LoRA 微调模型 × 3 个基座（LLaMA-3.1-8B、Gemma-2-9B、Qwen-2.5-7B）× 5 个领域。</li>
<li>Delta Activations 的 silhouette 分数平均 0.614，显著高于权重展平 (-0.043)、显著掩码 (0.190)、输出句向量 (0.087)。</li>
</ul>
</li>
<li><strong>鲁棒性</strong>（表 7）：学习率、数据量、epoch 变化不会破坏领域聚类。</li>
<li><strong>可加性</strong>（表 4、表 13）：<br>[ v_{\text{model on } D_1 \cup D_2} \approx v_{\text{model on } D_1} + v_{\text{model on } D_2} ]<br>10 组领域两两混合实验均验证此近似成立。</li>
</ul>
<p>5. 扩展到任务与跨架构场景</p>
<ul>
<li><strong>任务嵌入</strong>：用 20 条样本对基座模型做 LoRA 微调，得到的 Delta 向量即可定位对应领域簇（图 5）。</li>
<li><strong>跨架构</strong>：<ul>
<li>同系列不同 checkpoint（LLaMA-3 vs 3.1）：Delta Activations 仍能聚类（silhouette 0.39）。</li>
<li>不同规模（LLaMA-3.1-8B vs 1B）：改用 <strong>Delta Meaning</strong>（基于逆困惑度差异），仍可形成 4/5 个簇（silhouette 0.32）。</li>
</ul>
</li>
</ul>
<p>6. 实际应用验证</p>
<ul>
<li><strong>模型选择</strong>：在 LoRAHub 的 200 模型库中，用 Delta 相似度先选 1 个锚模型再随机补 19 个，比纯随机选择提升 BBH 基准 2.0% 准确率；直接选 20 个最相似反而下降，验证了 Delta 能揭示模型干扰（表 9 讨论）。</li>
</ul>
<p>7. 提供通用框架 Delta-X</p>
<ul>
<li>激活、logits、meaning 向量等任何可一致提取的特征都可替换为 X，实现 <strong>Delta-X</strong> 家族；当 X 为架构无关特征时，即可跨基座统一嵌入。</li>
</ul>
<p>总结流程图</p>
<pre><code>graph TD
    A[基座模型 + 微调模型] --&gt; B[通用探针输入 D_probe]
    B --&gt; C[一次前向提取隐藏状态]
    C --&gt; D[计算 Δ = 微调模型 - 基座模型]
    D --&gt; E[平均得到向量 v_f]
    E --&gt; F[向量空间聚类/检索/合并]
</code></pre><p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文围绕 <strong>Delta Activations</strong> 的有效性、性质与扩展应用，共设计并执行了 <strong>7 组核心实验</strong>，覆盖 <strong>聚类质量、可加性、探针设计、鲁棒性、跨场景泛化、任务嵌入与模型选择</strong> 等多个维度。所有实验均基于 <strong>3 个主流开源基座模型</strong>（LLaMA-3.1-8B、Gemma-2-9B、Qwen-2.5-7B）以及 <strong>≈ 200 个微调模型</strong>。以下按主题归纳：</p>
<p>1. 聚类质量对比（Section 3.1）</p>
<ul>
<li><strong>目的</strong>：验证 Delta Activations 能否在无监督条件下按领域正确聚类微调模型。</li>
<li><strong>设置</strong>：<ul>
<li>每基座 × 5 领域（LEGAL、MATH、MEDICAL、COMMONSENSE、CODING）× 3 个独立 LoRA 微调模型。</li>
<li>对比 4 种嵌入：<ol>
<li>Flattened LoRA 权重</li>
<li>Salient mask（top-1% 显著参数）</li>
<li>输出句向量（ALL-MiniLM-L6-V2）</li>
<li>Delta Activations（4096-d）</li>
</ol>
</li>
</ul>
</li>
<li><strong>指标</strong>：Silhouette score（↑ 越好）。</li>
<li><strong>结果</strong>：Delta Activations 平均 0.614，显著优于其余方法（表 2）；t-SNE 可视化显示清晰分离（图 3）。</li>
</ul>
<p>2. 可加性验证（Section 3.2 &amp; B.2）</p>
<ul>
<li><strong>目的</strong>：检验当训练集混合时，Delta 向量是否满足近似加法关系<br>[ v_{\mathcal{D}_1 \cup \mathcal{D}<em>2} \approx v</em>{\mathcal{D}<em>1} + v</em>{\mathcal{D}_2}. ]</li>
<li><strong>设置</strong>：<ul>
<li>10 组两两领域组合（共 45 个模型）。</li>
<li>计算余弦相似度：混合模型 vs 单领域模型 vs 向量之和。</li>
</ul>
</li>
<li><strong>结果</strong>：混合模型与“向量和”相似度始终最高（平均 0.69），加法性质成立（表 4、表 13）。</li>
</ul>
<p>3. 探针数据集消融（Section 3.2）</p>
<ul>
<li><strong>目的</strong>：确定最佳探针提示的数量、长度与内容。</li>
<li><strong>变量</strong>：<ul>
<li>数量：1 / 5 / 20 条提示</li>
<li>长度：一词 / 一句 / 三句（Alpaca）</li>
<li>内容：通用指令 vs 领域特定 vs Wikitext</li>
</ul>
</li>
<li><strong>结果</strong>：<ul>
<li>5 条已饱和；</li>
<li>适中长度 &gt; 极短；</li>
<li>通用指令模板 &gt; 领域模板 &gt; 随机文本（表 5）。</li>
</ul>
</li>
</ul>
<p>4. 激活位置消融（Section 3.2）</p>
<ul>
<li><strong>目的</strong>：确定提取 Δ 的最佳 token 与层。</li>
<li><strong>变量</strong>：<ul>
<li>Token：首 token / 中间 token / 末 token / 所有 token 加权平均</li>
<li>Layer：1/3、1/2、2/3 深度及最后一层</li>
</ul>
</li>
<li><strong>结果</strong>：<ul>
<li>末 token 显著优于首、中；</li>
<li>2/3 深度略优于最后一层，但差异小（表 6）。</li>
</ul>
</li>
</ul>
<p>5. 训练超参数鲁棒性（Section 3.2）</p>
<ul>
<li><strong>目的</strong>：验证聚类是否受学习率、数据量、epoch 变化干扰。</li>
<li><strong>设置</strong>：<ul>
<li>每领域 3 模型，分别使用 3 种学习率（1e-4, 4e-4, 1e-5）、3 种数据量（100, 1k, 10k）、3 种 epoch（1, 2, 3）。</li>
</ul>
</li>
<li><strong>结果</strong>：<ul>
<li>数据量与 epoch 变化对 silhouette 影响极小；</li>
<li>学习率变化略降低分数，但仍保持清晰领域簇（表 7）。</li>
</ul>
</li>
</ul>
<p>6. 跨场景泛化</p>
<p>6.1 非结构化指令数据集（Section 3.2）</p>
<ul>
<li><strong>设置</strong>：用 Tulu v2 的 5 个子集（COT、ShareGPT 等）训练模型，输出格式差异大。</li>
<li><strong>结果</strong>：输出句向量失效（silhouette ≈ 0.08），Delta Activations 仍保持 0.41。</li>
</ul>
<p>6.2 偏好优化（DPO）模型（Section 3.2）</p>
<ul>
<li><strong>设置</strong>：LLaMA-3.1-8B-Instruct 在 UltraFeedback、HumanLLM、MetaMath-DPO 上做 DPO。</li>
<li><strong>结果</strong>：silhouette = 0.93，Delta Activations 对偏好优化同样有效（图 4）。</li>
</ul>
<p>7. 扩展实验</p>
<p>7.1 任务嵌入（Section 3.3）</p>
<ul>
<li><strong>设置</strong>：每领域随机 20 条样本做 LoRA 微调，用所得 Delta 向量作为“任务向量”。</li>
<li><strong>评估</strong>：在最近邻检索中定位对应领域簇。</li>
<li><strong>结果</strong>：Gemma 100% 成功，LLaMA/Qwen 各 1 次失败（图 5）。</li>
</ul>
<p>7.2 跨基座聚类（Section 3.3）</p>
<ul>
<li><strong>跨 checkpoint</strong>：LLaMA-3-8B vs LLaMA-3.1-8B，Delta Activations 成功聚类（silhouette 0.39，图 6a）。</li>
<li><strong>跨架构</strong>：LLaMA-3.1-8B vs 1B，维度不匹配，改用 <strong>Delta Meaning</strong>（逆困惑度差），仍恢复 4/5 簇（silhouette 0.32，图 6b）。</li>
</ul>
<p>7.3 模型选择（Section 3.3）</p>
<ul>
<li><strong>场景</strong>：LoRAHub 的 200 LoRA 模型 + BBH 基准。</li>
<li><strong>策略</strong>：用 Delta 任务嵌入选 1 个最相关模型 + 19 随机，对比完全随机。</li>
<li><strong>结果</strong>：平均准确率从 34.3% → 36.3%（↑2.0%），验证 Delta 可用于高效模型筛选。</li>
</ul>
<p>8. 补充实验（附录）</p>
<p>实验</p>
<p>目的</p>
<p>结果</p>
<p><strong>全参数微调</strong>（B.3）</p>
<p>验证 Delta Activations 对非 LoRA 微调同样有效</p>
<p>silhouette = 0.63</p>
<p><strong>子领域聚类</strong>（B.4）</p>
<p>医学 8 子领域、编程 6 子领域</p>
<p>通用提示即可区分子专长，silhouette 0.66–0.67</p>
<p><strong>更多 t-SNE</strong>（B.1）</p>
<p>LLaMA、Qwen 的可视化</p>
<p>与主实验一致，Delta Activations 聚类清晰</p>
<p>实验全景表</p>
<p>维度</p>
<p>关键发现</p>
<p><strong>聚类质量</strong></p>
<p>Delta Activations 在所有基座均优于权重、输出、显著掩码方法</p>
<p><strong>可加性</strong></p>
<p>混合数据集 ≈ 单数据集向量之和</p>
<p><strong>探针设计</strong></p>
<p>5 条通用指令模板即可，长度适中，避免领域化</p>
<p><strong>鲁棒性</strong></p>
<p>对学习率、数据量、epoch 变化不敏感</p>
<p><strong>跨场景</strong></p>
<p>非结构化指令、DPO、全参数微调均适用</p>
<p><strong>任务嵌入</strong></p>
<p>20 条样本即可定位领域簇</p>
<p><strong>跨架构</strong></p>
<p>同系列 checkpoint 可直接比较；不同架构可用 Delta-Meaning</p>
<p><strong>实际应用</strong></p>
<p>在 200 模型库中提升下游任务 2% 准确率</p>
<p>这些实验系统性地证明了 Delta Activations 作为一种 <strong>无需元数据、计算轻量、跨场景稳健</strong> 的模型表示方法的广泛有效性。</p>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<p>以下方向可作为 Delta Activations 的后续研究切入点，按 <strong>方法改进、规模扩展、跨模态与跨任务、安全与治理、理论分析</strong> 五大主题归纳。</p>
<p>1. 方法改进与效率提升</p>
<ul>
<li><strong>自适应探针</strong><br>目前使用 5 条固定通用模板；可探索 <strong>任务自适应或梯度驱动的探针搜索</strong>，以进一步压缩维度并提升区分度。</li>
<li><strong>多 token / 多层级融合</strong><br>实验显示 2/3 深度略优于最后一层。可设计 <strong>加权融合策略</strong>（类似 Muennighoff 的加权平均）或 <strong>注意力池化</strong>，兼顾不同层/ token 的互补信息。</li>
<li><strong>低秩或量化压缩</strong><br>4096-d 向量在超大规模库中仍显冗余；研究 <strong>PCA、随机投影、INT8 量化</strong> 对聚类与检索性能的影响，实现内存-精度权衡。</li>
<li><strong>在线/增量更新</strong><br>当模型持续微调时，如何 <strong>增量更新 Delta 向量</strong> 而无需重新遍历探针集，可借鉴动量更新或经验回放策略。</li>
</ul>
<p>2. 规模扩展与系统级研究</p>
<ul>
<li><strong>万级模型库实验</strong><br>目前最大实验仅 ~200 LoRA；构建 <strong>公开万级模型池</strong>（如 Hugging Face 全量 LoRA）并评估<ul>
<li>聚类是否仍保持清晰？</li>
<li>检索延迟与内存占用是否可接受？</li>
</ul>
</li>
<li><strong>分布式计算框架</strong><br>为超大规模库设计 <strong>GPU 并行探针推理 + ANN 索引</strong>（FAISS、ScaNN），实现毫秒级相似度搜索。</li>
<li><strong>层次化组织</strong><br>在万级规模上引入 <strong>两级索引</strong>：先按领域粗聚类，再在每个簇内细粒度检索，兼顾效率与精度。</li>
</ul>
<p>3. 跨模态、跨任务与跨语言</p>
<ul>
<li><strong>跨模态 Delta</strong><br>将框架迁移到 <strong>视觉 Transformer、多模态 LLM（LLaVA、Flamingo）</strong>，研究图像-文本微调模型是否可用同一探针思想生成可比嵌入。</li>
<li><strong>跨语言任务</strong><br>探索 Delta Activations 在 <strong>多语言基座模型</strong>（如 mT5、XLM-R）上能否区分不同语言的微调专长，并支持跨语言模型合并。</li>
<li><strong>复杂任务结构</strong><br>当前实验以单领域为主；对 <strong>多轮对话、工具调用、代码-数学混合任务</strong> 构造更细粒度标签，验证 Delta 嵌入能否区分子任务或技能维度。</li>
</ul>
<p>4. 安全、隐私与治理</p>
<ul>
<li><strong>恶意模型检测</strong><br>研究 Delta 向量是否能暴露 <strong>后门、偏见或不当知识</strong>，从而作为 <strong>模型审查信号</strong>。</li>
<li><strong>隐私风险</strong><br>探针输入虽通用，但仍可能泄露训练数据痕迹；开展 <strong>成员推理攻击</strong> 实验，评估 Delta 表示是否携带敏感信息。</li>
<li><strong>治理与元数据补全</strong><br>将 Delta 嵌入与 <strong>社区标注、模型卡</strong> 结合，构建 <strong>半自动元数据补全系统</strong>，解决开源库长期存在的文档缺失问题。</li>
</ul>
<p>5. 理论与可解释性</p>
<ul>
<li><strong>线性子空间假设验证</strong><br>通过 <strong>奇异值分解</strong> 分析 Delta 向量的主成分，检验不同领域是否对应近似正交子空间，为加法性质提供几何解释。</li>
<li><strong>任务-向量正交度量化</strong><br>定义领域间 <strong>干扰系数</strong> ( \cos(v_{\mathcal{D}<em>i}, v</em>{\mathcal{D}_j}) )，建立任务兼容度矩阵，指导模型合并时的冲突预测。</li>
<li><strong>因果干预实验</strong><br>使用 <strong>激活修补 (activation patching)</strong> 技术，验证 Delta 向量方向是否确实控制模型在特定任务上的行为差异。</li>
</ul>
<p>6. 工具链与社区生态</p>
<ul>
<li><strong>开放基准与排行榜</strong><br>发布 <strong>DeltaBench</strong>：包含万级模型、标准化探针脚本、聚类与检索评测指标，供社区持续刷榜。</li>
<li><strong>可视化与交互平台</strong><br>开发 <strong>Web 界面</strong> 实时展示模型在 Delta 空间中的分布，支持拖拽式模型选择与合并实验。</li>
<li><strong>自动化工作流</strong><br>集成到 Hugging Face Hub/GitHub Action：模型上传后自动计算 Delta 向量并写入模型卡，实现 <strong>零额外成本</strong> 的社区级元数据增强。</li>
</ul>
<p>小结：可立即启动的 3 个具体课题</p>
<ol>
<li><strong>低维压缩实验</strong>：将 4096-d Delta 向量压缩至 128-d，评估在 10k 模型库中的检索召回率。</li>
<li><strong>跨模态 Delta</strong>：在 Stable Diffusion LoRA 库上复现 Delta 框架，验证文本-图像模型是否同样具备加法性质。</li>
<li><strong>后门检测</strong>：收集已知含后门的微调模型，训练分类器以 Delta 向量为输入，检测 AUC 是否显著高于随机。</li>
</ol>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<p>论文核心速览</p>
<p><strong>题目</strong>：Delta Activations: A Representation for Finetuned Large Language Models<br><strong>一句话总结</strong>：提出一种<strong>无需任何外部元数据</strong>、<strong>仅用一次前向传播</strong>即可得到的向量表示，把任意微调模型映射到统一空间，实现按领域/任务自动聚类、检索与合并。</p>
<p>1. 背景与痛点</p>
<ul>
<li>开源社区已产出海量针对特定任务/领域微调的大模型，但命名含糊、文档缺失，导致 <strong>发现与复用困难</strong>。</li>
<li>现有表示方法依赖训练数据、权重或评测，存在数据不可见、架构不一致、提示敏感等问题。</li>
</ul>
<p>2. 方法：Delta Activations</p>
<ul>
<li><strong>输入</strong>：基础模型 (f_{\text{base}}) + 微调模型 (f)。</li>
<li><strong>计算</strong>：<ol>
<li>用 5 条 <strong>通用无领域倾向</strong> 的指令模板作为探针。</li>
<li>取 <strong>最后一个 token 在最后一层的隐藏状态</strong>，计算<br>[ \Delta_f(x)=h_f(x)-h_{\text{base}}(x) ]</li>
<li>平均得到 4096 维向量 (v_f)，即模型嵌入。</li>
</ol>
</li>
<li><strong>特点</strong>：<ul>
<li>仅需一次前向，计算量极小；</li>
<li>不依赖训练数据、权重格式或评测；</li>
<li>具备 <strong>加法性质</strong>：混合数据集训练的模型 ≈ 单数据集向量之和。</li>
</ul>
</li>
</ul>
<p>3. 实验验证</p>
<p>维度</p>
<p>结果</p>
<p><strong>聚类质量</strong></p>
<p>在 3 个基座 × 5 领域 × 15 模型上 silhouette 0.614，显著优于权重、输出句向量等基线。</p>
<p><strong>鲁棒性</strong></p>
<p>学习率、数据量、epoch 变化不会破坏领域簇。</p>
<p><strong>可加性</strong></p>
<p>10 组领域混合实验均验证向量加法近似成立。</p>
<p><strong>跨场景</strong></p>
<p>非结构化指令、DPO 偏好优化、全参数微调均适用。</p>
<p><strong>任务嵌入</strong></p>
<p>用 20 条样本即可生成“任务向量”，精准定位对应领域簇。</p>
<p><strong>跨架构</strong></p>
<p>同系列 checkpoint 可直接比较；不同规模可用 Delta-Meaning（逆困惑度差）。</p>
<p><strong>实际应用</strong></p>
<p>在 LoRAHub 200 模型库中，用 Delta 选择策略提升 BBH 基准 2% 准确率。</p>
<p>4. 贡献与意义</p>
<ul>
<li><strong>理论</strong>：首次将“激活位移”作为微调模型通用表示，验证其可加性与稳健性。</li>
<li><strong>实践</strong>：为开源社区提供 <strong>零元数据模型发现、比较、合并</strong> 工具，降低重复训练成本，促进模型可持续共享。</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Delta%20Activations%3A%20A%20Representation%20for%20Finetuned%20Large%20Language%20Models，链接是  https://arxiv.org/pdf/2509.04442  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.04442  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhiqiu Xu,Amish Sethi,Mayur Naik,Ser-Nam Lim</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,cs.CL,cs.IR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.04442.pdf">https://arxiv.org/pdf/2509.04442.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.04442">https://arxiv.org/abs/2509.04442</a></p>
<p><strong>Arxiv ID</strong>: 2509.04442</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.04442">https://papers.cool/arxiv/2509.04442</a></p>
<p><strong>Published</strong>: 2025-09-04T17:59:06Z</p>
<p><strong>Updated</strong>: 2025-09-04T17:59:06.000Z</p>
<hr>
<h3 id="14-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize"><a href="#14-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize" class="headerlink" title="14. False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.03888">False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</a></h3><p>Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs’ internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at <a target="_blank" rel="noopener" href="https://github.com/WangCheng0116/Why-Probe-Fails">https://github.com/WangCheng0116/Why-Probe-Fails</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLM）可以遵循有害指令，尽管它们的能力令人印象深刻，但这引发了严重的安全担忧。近期的研究利用基于探测的方法研究了LLM内部表示中恶意和良性输入的可分离性，研究人员提出使用这种探测方法进行安全检测。我们系统地重新审视了这一范式。受到分布外性能差的激励，我们假设探测器学习的是表面模式而不是语义上的有害性。通过控制实验，我们确认了这一假设，并识别出学习到的具体模式：指令模式和触发词。我们的调查遵循一种系统的方法，从展示简单n-gram方法具有可比较的性能，到使用语义清理过的数据集进行控制实验，再到对模式依赖性的详细分析。这些结果揭示了当前基于探测方法的虚假安全感，并强调了重新设计模型和评估协议的必要性，我们在此提供了进一步的讨论，希望能够提出负责任的进一步研究建议。我们已经在<a target="_blank" rel="noopener" href="https://github.com/WangCheng0116/Why-Probe-Fails上开源了该项目。">https://github.com/WangCheng0116/Why-Probe-Fails上开源了该项目。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p><strong>Q1</strong>: 这篇论文试图解决什么问题？</p>
<p>论文旨在回答一个核心问题：<br><strong>“基于探针（probing）的恶意输入检测方法是否真正学到了‘有害性’的语义，还是仅仅依赖表层模式？”</strong></p>
<p>具体而言，工作聚焦于以下子问题：</p>
<ol>
<li>为什么探针分类器在分布外（OOD）数据上性能骤降？</li>
<li>其高准确率是否只是对特定词汇、指令模板等表层特征的过拟合？</li>
<li>若表层特征足以解释探针效果，现有基于隐藏状态的安全检测机制是否给社区带来了“虚假的安全感”？</li>
</ol>
<p>通过系统对照实验，论文证实探针并未掌握可泛化的有害语义，而是主要捕获了：</p>
<ul>
<li><strong>指令模式</strong>（如“How to …”结构）</li>
<li><strong>触发词</strong>（如 bomb、kill 等高频危险词）</li>
</ul>
<p>由此呼吁重新设计安全检测范式与评估协议，避免将表层模式匹配误认为深度语义理解。</p>
<p><strong>Q2</strong>: 有哪些相关研究？</p>
<ul>
<li><p><strong>Adversarial Attacks on LLMs</strong></p>
<ul>
<li>Jailbreak 攻击：Zou et al., 2023 (Universal and transferable adversarial attacks); Jin et al., 2024 (JailbreakZoo); Yi et al., 2024 (Survey)</li>
<li>Prompt Injection：Liu et al., 2024a,b (Automatic &amp; Universal Prompt Injection)</li>
<li>检索投毒：Zhong et al., 2023 (Poisoning Retrieval Corpora); Zou et al., 2024 (PoisonedRAG)</li>
</ul>
</li>
<li><p><strong>Defense Strategies for LLMs</strong></p>
<ul>
<li>Guard Models：Liu et al., 2025 (GuardReasoner-VL); Ghosh et al., 2025b (Aegis 2.0); Zeng et al., 2024 (ShieldGemma)</li>
<li>Alignment Training：Ouyang et al., 2022 (RLHF); Rafailov et al., 2024 (DPO)</li>
<li>Mechanistic Interventions：Qian et al., 2025 (Hidden State Filtering); Zhou et al., 2024 (Explain LLM Safety via Hidden States); Ghosh et al., 2025a (SafeSteer); Hazra et al., 2024 (Safety Arithmetic)</li>
</ul>
</li>
<li><p><strong>Probing &amp; Representation Analysis</strong></p>
<ul>
<li>通用探测：Azaria &amp; Mitchell, 2023 (LLM knows when it’s lying); Liu et al., 2024c (Pre-training Data Detection); Alnuhait et al., 2024 (Hallucination Detection)</li>
<li>安全特定探测：Lin et al., 2024 (Jailbreak Representation Analysis); Zheng et al., 2024 (Prompt-driven Safeguarding); Dong et al., 2025 (Feature-aware Malicious Output Detection)</li>
</ul>
</li>
<li><p><strong>可视化与层分析</strong></p>
<ul>
<li>Ju et al., 2024 (Layer-wise Probing); Skean et al., 2025 (Layer by Layer)</li>
</ul>
</li>
<li><p><strong>测试集与基准</strong></p>
<ul>
<li>AdvBench, HarmBench, JailbreakBench, ForbiddenQuestions, BeaverTailsEval, StrongReject, MaliciousInstruct, XSTest 等数据集均在文中作为评估基础。</li>
</ul>
</li>
</ul>
<p><strong>Q3</strong>: 论文如何解决这个问题？</p>
<p>论文采用“递进式对照实验”策略，从现象→假设→验证→归因→建议五步系统拆解问题。</p>
<ol>
<li><p>现象：OOD 性能崩塌<br>复现既有设置，发现探针在分布外数据上准确率骤降 15–99 个百分点，触发质疑。</p>
</li>
<li><p>假设：探针仅学表层模式<br>提出核心假设：高 ID 准确率并非来自语义有害性，而是来自指令模板与触发词等表层特征。</p>
</li>
<li><p>验证：三步对照研究</p>
<p>研究</p>
<p>对照设计</p>
<p>关键证据</p>
<p>结论</p>
<p>RS1</p>
<p>探针 vs. n-gram Naïve Bayes</p>
<p>n-gram 在 ID 数据上取得可比准确率（0.84–1.00）</p>
<p>探针并无超越表层统计的优势</p>
<p>RS2</p>
<p>原始恶意集 vs. 语义清洗集（保留句法、替换有害词）</p>
<p>清洗后准确率暴跌 60–90 个百分点</p>
<p>探针依赖表层词汇而非语义</p>
<p>RS3</p>
<p>清洗集再 paraphrase（破坏指令模板）</p>
<p>准确率大幅回升</p>
<p>探针主要捕获指令模式与触发词</p>
</li>
<li><p>归因：多维度稳健性检验</p>
<ul>
<li>层选择：早期、中期、最后一层均呈现相似 OOD 失效，说明问题与表示深度无关。</li>
<li>分类器族：SVM、Logistic Regression、MLP 均在清洗集上崩溃，排除模型选择偏差。</li>
<li>基础 vs. 指令微调模型：两者 OOD 失效模式一致，表明对齐训练未解决表层过拟合。</li>
<li>可视化：PCA 显示“恶意”与“清洗”样本聚类重叠，OOD 样本自成一簇，进一步印证“结构而非语义”决定边界。</li>
</ul>
</li>
<li><p>建议：放弃“探针即安全”错觉</p>
<ul>
<li>检测范式需从“表层匹配”转向“语义鲁棒理解”；</li>
<li>评估协议必须强制跨分布、跨语言、跨指令模板测试；</li>
<li>利用 LLM 本身零 shot 安全判断能力，探索基于生成或隐含奖励的检测机制，而非单纯冻结表征+轻量分类器。</li>
</ul>
</li>
</ol>
<p><strong>Q4</strong>: 论文做了哪些实验？</p>
<p>论文共设计 <strong>3 组主实验（Research Study 1–3）</strong> 与 <strong>4 组辅助实验</strong>，形成“现象→质疑→验证→归因”完整链条。</p>
<p>Research Study 1：探针 vs. n-gram 基线</p>
<p><strong>目的</strong>：验证“高准确率是否仅靠表层统计特征”<br><strong>对照变量</strong>：特征空间（隐藏状态 vs. 1/2/3-gram 频次）<br><strong>指标</strong>：In-Distribution Accuracy<br><strong>结果</strong>：</p>
<ul>
<li>全模型、全数据集上，Naïve Bayes 的 1-gram / 2-gram 即可达到 0.84–1.00，与探针无显著差距。</li>
<li>3-gram 因稀疏反而略低，说明探针≈高级 n-gram 分类器。</li>
</ul>
<p>Research Study 2：语义清洗实验</p>
<p><strong>目的</strong>：控制“语义有害性”仅保留句法模板<br><strong>步骤</strong>：</p>
<ol>
<li>用 GPT-4o 把恶意提示中的危险实体替换为 benign 词（bomb→bread；kill→close），长度与句法不变。</li>
<li>训练集仍用原始“ benign vs. 恶意”，测试集分别用原始恶意与清洗后“伪恶意”。<br><strong>指标</strong>：Accuracy Drop（原始→清洗）<br><strong>结果</strong>：</li>
</ol>
<ul>
<li>所有模型-数据集组合下降 60–90 pp，最低跌至 8 %，证明探针对“语义”不敏感，对“模板”过敏感。</li>
</ul>
<p>Research Study 3：拆解表层模式</p>
<p>3a 指令模板剥离（Paraphrase）</p>
<ul>
<li>在清洗集基础上再用 GPT-4o 改写句式，去除“How to …/List steps…”等指令框架。</li>
<li>准确率从 20–30 % 回升至 82–100 %，确认“指令模式”是核心虚假特征。</li>
</ul>
<p>3b 触发词注入（XSTest 安全子集）</p>
<ul>
<li>选取 benign 但含危险词汇的提示（如“kill a Python process”）。</li>
<li>探针 FP 率 40–80 %，说明“触发词”本身即可误导分类器。</li>
</ul>
<p>辅助实验</p>
<p>实验</p>
<p>变量</p>
<p>结论</p>
<p>层选择</p>
<p>首层 / 中层 / 末层隐藏状态</p>
<p>各层 ID 高、OOD 崩，模式依赖与深度无关</p>
<p>分类器族</p>
<p>SVM / LR / MLP</p>
<p>全都在清洗集崩，架构不是瓶颈</p>
<p>基础 vs. 指令微调</p>
<p>Gemma、Llama、Qwen 的 base &amp; instruct 版本</p>
<p>对齐训练仅边际提升 OOD，未解决根本缺陷</p>
<p>零 shot 安全判断</p>
<p>直接用 LLM 自分类</p>
<p>准确率 96–100 %，表明模型自身具备语义理解，探针未能利用</p>
<p>可视化补充</p>
<ul>
<li>PCA 降维：恶意与清洗样本重叠，OOD 样本远离，再次印证“结构聚类≠语义聚类”。</li>
</ul>
<p><strong>Q5</strong>: 有什么可以进一步探索的点？</p>
<ul>
<li><p><strong>跨语言与文化迁移</strong><br>当前实验局限在英文；有害性表达在不同语言、文化语境下差异显著。可构建多语言清洗-对照数据集，检验探针是否仍依赖英文特有的指令模板或触发词。</p>
</li>
<li><p><strong>跨架构与跨模态</strong><br>仅验证了解码器-only Transformer。可考察编码器-解码器、混合专家（MoE）、循环结构，以及图文多模态模型中“视觉-文本”联合表征是否同样存在表层模式过拟合。</p>
</li>
<li><p><strong>动态/渐进式分布漂移</strong><br>现有 OOD 为“数据集级”突变。可设计时间序列或话题漂移（topic drift）场景，量化探针性能随漂移速度的衰减曲线，建立“安全失效预警”指标。</p>
</li>
<li><p><strong>可解释性归因细化</strong><br>引入集成梯度、注意力 rollout 或因果中介分析，精确定位“模板”与“触发词”对应的神经元/子空间，研究剪除或抑制这些组件能否在保持模型能力的同时提升 OOD 鲁棒性。</p>
</li>
<li><p><strong>语义增强的探测目标</strong><br>将二值“有害/无害”标签升级为细粒度安全等级或风险要素向量，利用对比学习或自监督损失，把“有害语义”显式注入表征，再评估 OOD 鲁棒性是否改善。</p>
</li>
<li><p><strong>生成式检测器对比</strong><br>与冻结-探针对照，训练轻量级生成模型（如小型 T5）对提示进行“安全重述”或“风险解释”，以生成质量或解释一致性作为检测信号，验证是否比判别式探针更鲁棒。</p>
</li>
<li><p><strong>对抗触发词与模板搜索</strong><br>使用自动提示优化（如贪婪搜索、遗传算法）生成“看似无害、却高激活探针”的对抗输入，量化现有检测器的可欺骗性，并作为后续防御的硬负例。</p>
</li>
<li><p><strong>安全表示的因果干预</strong><br>借鉴机制可解释性工作，对疑似“安全神经元”做激活扰动或置换，观察模型输出与探针预测的同时变化，建立“因果安全子空间”，并测试其跨模型迁移能力。</p>
</li>
<li><p><strong>人机协同评估协议</strong><br>将探针置信度与人工标注不一致的样本（高置信错误、低置信正确）单独析出，分析其语言特征，迭代扩充“困难基准”，推动社区从静态数据集走向动态、可持续的测试循环。</p>
</li>
</ul>
<p><strong>Q6</strong>: 总结一下论文的主要内容</p>
<ul>
<li><p><strong>问题</strong><br>近期研究利用“冻结 LLM 隐藏状态 + 轻量探针”检测恶意提示，宣称 ID 准确率≈100 %。本文发现其 OOD 性能骤降，质疑其学到的是“有害语义”还是“表层模式”。</p>
</li>
<li><p><strong>核心假设</strong><br>探针仅捕获指令模板与触发词，给出虚假安全感。</p>
</li>
<li><p><strong>验证路径</strong></p>
<ol>
<li>RS1：与 n-gram Naïve Bayes 对比——统计特征即可复现高 ID 准确率。</li>
<li>RS2：语义清洗实验——保留句法、替换危险词，准确率跌 60–90 %。</li>
<li>RS3：剥离指令模板或注入 benign 触发词，进一步确认模式依赖。</li>
</ol>
</li>
<li><p><strong>辅助证据</strong><br>换层、换分类器、换 base/instruct 模型均无法挽救 OOD 崩溃；LLM 零 shot 自分类却表现优异，说明模型具备语义能力，探针未能利用。</p>
</li>
<li><p><strong>结论</strong><br>现有探针式安全检测本质是“高级模板+关键词匹配”，缺乏可泛化的有害语义理解，亟需重新设计表征学习与评估协议。</p>
</li>
</ul>
<p><strong>Q7</strong>: 想要进一步了解论文</p>
<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是False%20Sense%20of%20Security%3A%20Why%20Probing-based%20Malicious%20Input%20Detection%20Fails%20to%20Generalize，链接是  https://arxiv.org/pdf/2509.03888  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.03888  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Cheng Wang,Zeming Wei,Qin Liu,Muhao Chen</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.03888.pdf">https://arxiv.org/pdf/2509.03888.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03888">https://arxiv.org/abs/2509.03888</a></p>
<p><strong>Arxiv ID</strong>: 2509.03888</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.03888">https://papers.cool/arxiv/2509.03888</a></p>
<p><strong>Published</strong>: 2025-09-04T05:15:55Z</p>
<p><strong>Updated</strong>: 2025-09-04T05:15:55.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202509/2025-09-06/">https://wdndev.github.io/daily/hf/202509/2025-09-06/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Drivel-ology-Challenging-LLMs-with-Interpreting-Nonsense-with-Depth"><span class="toc-text">1. Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-From-Editor-to-Dense-Geometry-Estimator"><span class="toc-text">2. From Editor to Dense Geometry Estimator</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Towards-a-Unified-View-of-Large-Language-Model-Post-Training"><span class="toc-text">3. Towards a Unified View of Large Language Model Post-Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-DeepResearch-Arena-The-First-Exam-of-LLMs%E2%80%99-Research-Abilities-via-Seminar-Grounded-Tasks"><span class="toc-text">4. DeepResearch Arena: The First Exam of LLMs’ Research Abilities via Seminar-Grounded Tasks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Inverse-IFEval-Can-LLMs-Unlearn-Stubborn-Training-Conventions-to-Follow-Real-Instructions"><span class="toc-text">5. Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Transition-Models-Rethinking-the-Generative-Learning-Objective"><span class="toc-text">6. Transition Models: Rethinking the Generative Learning Objective</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Video-MTR-Reinforced-Multi-Turn-Reasoning-for-Long-Video-Understanding"><span class="toc-text">7. Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-NER-Retriever-Zero-Shot-Named-Entity-Retrieval-with-Type-Aware-Embeddings"><span class="toc-text">8. NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware Embeddings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Few-step-Flow-for-3D-Generation-via-Marginal-Data-Transport-Distillation"><span class="toc-text">9. Few-step Flow for 3D Generation via Marginal-Data Transport Distillation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E4%B8%BB%E6%97%A8"><span class="toc-text">论文主旨</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%8A%80%E6%9C%AF%E8%B7%AF%E7%BA%BF"><span class="toc-text">技术路线</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="toc-text">实验结果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%84%8F%E4%B9%89"><span class="toc-text">意义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Loong-Synthesize-Long-Chain-of-Thoughts-at-Scale-through-Verifiers"><span class="toc-text">10. Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Durian-Dual-Reference-guided-Portrait-Animation-with-Attribute-Transfer"><span class="toc-text">11. Durian: Dual Reference-guided Portrait Animation with Attribute Transfer</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Drawing2CAD-Sequence-to-Sequence-Learning-for-CAD-Generation-from-Vector-Drawings"><span class="toc-text">12. Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Delta-Activations-A-Representation-for-Finetuned-Large-Language-Models"><span class="toc-text">13. Delta Activations: A Representation for Finetuned Large Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-False-Sense-of-Security-Why-Probing-based-Malicious-Input-Detection-Fails-to-Generalize"><span class="toc-text">14. False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>