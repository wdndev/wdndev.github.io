<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-09-19 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at ScaleWe present Hala, a family of Arabic-centric instruction and transl">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-09-19">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202509/2025-09-19/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at ScaleWe present Hala, a family of Arabic-centric instruction and transl">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.833Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202509/2025-09-19/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-09-19',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-09-19</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.833Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">70.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>249min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-09-19"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Hala-Technical-Report-Building-Arabic-Centric-Instruction-amp-Translation-Models-at-Scale"><a href="#1-Hala-Technical-Report-Building-Arabic-Centric-Instruction-amp-Translation-Models-at-Scale" class="headerlink" title="1. Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at Scale"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14008">Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at Scale</a></h3><p>We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the “nano” ($\leq$2B) and “small” (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们展示了 Hala，这是一系列以阿拉伯语为中心的教学和翻译模型，使用我们的翻译和调谐管道构建。我们首先将一个强大的 AR$\leftrightarrow$EN 教师压缩到 FP8（产生 $\sim$2$\times$ 更高的吞吐量，而不会造成质量损失），并使用它来创建高保真双语监督。然后，轻量级语言模型 LFM2-1.2B 根据这些数据进行微调，并用于将高质量的英语指令集翻译成阿拉伯语，从而生成针对指令遵循量身定制的百万规模语料库。我们在 350M、700M、1.2B 和 9B 参数下训练 Hala 模型，并应用 slerp 合并来平衡阿拉伯语专业化与基础模型优势。在以阿拉伯语为中心的基准测试中，Hala 在“纳米”（$\leq$2B）和“小型”（7-9B）类别中都取得了最先进的结果，优于其基础。我们发布模型、数据、评估和配方，以加速阿拉伯语 NLP 的研究。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统性地缓解“高质量阿拉伯语指令数据稀缺”这一核心瓶颈，从而提升大语言模型在阿拉伯语上的指令跟随、推理与安全对齐能力。具体而言，其聚焦以下子问题：</p>
<ul>
<li><strong>数据层面</strong>：如何以低成本构建百万级、与英语指令集语义对齐的阿拉伯语指令语料，同时保证方言、形态与文化适配性。</li>
<li><strong>效率层面</strong>：如何在消费级算力（≈ 1000 USD）内完成数据生产与多尺度模型训练，避免重复预训练的高昂开销。</li>
<li><strong>能力层面</strong>：在“nano”（≤2 B）与“small”（7–9 B）参数区间内，使阿拉伯语专用模型在阿拉伯语基准上达到或超越现有最强多语/阿拉伯语基线，同时不损失通用能力。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><strong>阿拉伯语大模型</strong>：JAIS、FANAR、PEACOCK、ACE-GPT、ALLAM、AraBERT 等聚焦预训练或对话微调，但指令数据规模受限。</li>
<li><strong>多语/翻译数据构建</strong>：Tatoeba、OPUS-100、MASSIVE、warc2text 流水线提供平行句对，但未针对指令风格过滤。</li>
<li><strong>LLM 作为翻译引擎</strong>：120+ 语言评估显示，经提示与筛选后，大模型可保持长文档、科学文本及风格化翻译质量。</li>
<li><strong>量化加速</strong>：FP8 动态缩放（Kuzmin et al., 2022）与 LLM-Compressor 实现 2× 吞吐无损质量，为低成本数据生产提供算力基础。</li>
<li><strong>合成/自指令数据</strong>：WizardLM、Self-Instruct、Orca、ReAlign-Alpaca 等用英语模型自举指令，但阿拉伯语版本稀少。</li>
<li><strong>模型融合</strong>：MergeKit + slerp 插值可在不重新训练的前提下，合并英语与阿拉伯语微调检查点，兼顾通用与语言专项能力。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出“translate-and-tune”流水线，将数据稀缺问题转化为可扩展的翻译与微调工程，具体步骤如下：</p>
<ol>
<li><p><strong>压缩教师翻译模型</strong><br>把 Cohere AR↔EN 教师模型量化至 FP8，推理吞吐提升 ≈2×，质量无损，作为后续数据生产的高速引擎。</p>
</li>
<li><p><strong>构建双语监督信号</strong></p>
<ul>
<li>用 FP8 教师翻译 405 k Open-Orca 指令-回复对，得到 ⟨en, ar⟩ 双语指令对。</li>
<li>以 Qwen2.5-3B-Instruct 作“严格双语裁判”，从 1 M OPUS-100 句对中筛出 440 k 高质量平行句，补充非指令风格信号。<br>合并后约 1.26 M 双语样本，用于训练轻量级翻译模型。</li>
</ul>
</li>
<li><p><strong>训练轻量 AR↔EN 翻译模型</strong><br>在 LiquidAI/LFM2-1.2B 上执行双语监督微调，得到专为指令文本优化的“workhorse”翻译器，成本 &lt;100 USD。</p>
</li>
<li><p><strong>大规模阿拉伯语指令语料生产</strong><br>用上述翻译器将 6 个英文高质指令集（Hermes-3、SCP-116K、Tulu-3 等）批量翻译成阿拉伯语，总计约 4.5 M 样本，强调推理、对齐与跟随格式。</p>
</li>
<li><p><strong>多尺度微调与 slerp 融合</strong></p>
<ul>
<li>350 M、700 M、1.2 B：基于 Liquid 检查点微调后，与原始 base 做 spherical-linear-interpolation（t=0.5），保留通用能力。</li>
<li>9 B：基于 FANAR-9B-Instruct 微调后同样融合，得到 HALA-9B。</li>
</ul>
</li>
<li><strong>严格阿拉伯语基准评估</strong><br>在 AlGhafa、ArabicMMLU、AraTrust 等 6 项任务上，HALA 各尺度均取得对应参数桶的最佳或次佳平均成绩，验证“语言中心”策略的有效性。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“数据质量-翻译保真-微调效果”三条线展开，可归纳为四类：</p>
<ol>
<li><p>翻译质量对照实验</p>
<ul>
<li>教师模型：FP16 ↔ FP8 在 500 条 MMLU 题目上的 BLEU/ROUGE-L/chrF++，验证量化无损。</li>
<li>轻量模型：Liquid-1.2B base ↔ HALA-1.2B-Translator 在同一 500 题上的自动指标，验证微调后翻译器提升 21~32 分。</li>
</ul>
</li>
<li><p>数据过滤消融</p>
<ul>
<li>OPUS-100 1 M → 440 k 的“裁判筛选”前后，人工抽查 200 对，错误率从 14 % 降至 3 %，说明严格双语裁判有效。</li>
</ul>
</li>
<li><p>主基准评测<br>在 6 项阿拉伯语任务（AlGhafa、ArabicMMLU、EXAMS、MadinahQA、AraTrust、ArbMMLU-HT）上，</p>
<ul>
<li>nano 段（≤2 B）：HALA-350 M/700 M/1.2 B 分别相对 Liquid 基线平均提升 7.6/5.5/5.1 分，包揽同尺寸前二。</li>
<li>small 段（7–9 B）：HALA-9 B 平均 69.9，超越原最佳 FANAR-9 B（69.2），刷新该区间记录。</li>
</ul>
</li>
<li>融合策略对比<br>对 1.2 B 模型做 slerp-t 消融（t=0.3,0.5,0.7），发现 t=0.5 在阿拉伯语平均与英语 MMLU 上同时最优，验证“半比例插值”兼顾双语能力。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>方言与变体深度覆盖</strong><br>当前语料以现代标准阿拉伯语（MSA）为主，可引入 Levantine、Gulf、Egyptian 等口语平行数据，构建“方言→MSA”或“跨方言”指令对，提升模型在真实社交媒体与语音场景下的鲁棒性。</p>
</li>
<li><p><strong>双向翻译一致性自监督</strong><br>利用已有 AR↔EN 翻译器，对同一指令执行“英→阿→英”回环，计算语义一致性奖励，迭代筛选或加权样本，进一步降低翻译漂移。</p>
</li>
<li><p><strong>检索增强的翻译重排</strong><br>在翻译阶段引入 bilingual dense retriever，从 1.26 M 双语库中召回 5-best 候选，再用轻量 reranker（如 0.4 B 双编码器）选择最佳，提高长句与术语准确率。</p>
</li>
<li><p><strong>多模态阿拉伯语指令</strong><br>将 Arabic VQA、OCR、语音转写数据与文本指令融合，训练图文/音图文三模态对齐，满足教育、医疗等垂直场景需求。</p>
</li>
<li><p><strong>小样本自适应与参数高效微调</strong><br>探索 LoRA/AdaLoRA 与 prompt-tuning 在 350 M–1.2 B 模型的极限效率，目标用 &lt;10 k 阿拉伯语领域样本即可超越现有 9 B 全量微调结果。</p>
</li>
<li><p><strong>安全性与价值观对齐</strong><br>构建 ArabicSafetyBench，涵盖宗教、政治、性别等文化敏感维度，结合 RLHF 或 DPO，对 HALA 进行阿拉伯价值观对齐，减少有害输出。</p>
</li>
<li><p><strong>推理加速与端侧部署</strong><br>将 HALA-350 M 进一步量化至 INT4 或 NVFP4，并在手机 SoC/NPU 上测试流式解码延迟，为低资源地区提供离线 Arabic Assistant。</p>
</li>
<li><p><strong>跨语言知识探测</strong><br>设计跨语言知识一致性任务（例如同一知识问答用不同方言/语言提问），量化模型是否真正“理解”而非表面翻译，指导未来数据增强策略。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>HALA：面向阿拉伯语的大模型“翻译-微调”流水线</strong></p>
<ol>
<li><p>问题<br>高质量阿拉伯语指令数据稀缺，导致现有模型在阿语任务上表现远逊于英语。</p>
</li>
<li><p>方法</p>
<ul>
<li>把 强 AR↔EN 教师模型 压缩到 FP8，推理提速 2× 且质量无损。</li>
<li>用其翻译 405 k Open-Orca 并过滤 440 k OPUS-100，构建 1.26 M 双语监督。</li>
<li>在 Liquid-1.2 B 上微调出轻量翻译器，再批量翻译 6 大英文指令集，得到 4.5 M 阿语样本。</li>
<li>基于 Liquid（350 M/700 M/1.2 B）与 FANAR-9 B 做阿语指令微调，随后用 slerp(t=0.5) 与基座融合，兼顾通用与阿语能力。</li>
</ul>
</li>
<li><p>实验</p>
<ul>
<li>FP8 教师 vs FP16：BLEU 不降；轻量翻译器提升 &gt;20 分。</li>
<li>在 6 项阿拉伯语基准上，HALA-1.2 B 刷新 nano 档平均成绩，HALA-9 B 刷新 small 档平均成绩。</li>
</ul>
</li>
<li>贡献<br>提供 4 个尺度开源模型、百万级阿语指令语料、完整训练与评测脚本，验证“语言中心”路线可在有限算力（≈ 1500 USD）内达到 SOTA。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Hala%20Technical%20Report%3A%20Building%20Arabic-Centric%20Instruction%20%26%20Translation%20Models%20at%20Scale，链接是  https://arxiv.org/pdf/2509.14008  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14008  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hasan Abed Al Kader Hammoud,Mohammad Zbeeb,Bernard Ghanem</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14008.pdf">https://arxiv.org/pdf/2509.14008.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14008">https://arxiv.org/abs/2509.14008</a></p>
<p><strong>Arxiv ID</strong>: 2509.14008</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14008">https://papers.cool/arxiv/2509.14008</a></p>
<p><strong>Published</strong>: 2025-09-17T14:19:28Z</p>
<p><strong>Updated</strong>: 2025-09-17T14:19:28.000Z</p>
<hr>
<h3 id="2-SAIL-VL2-Technical-Report"><a href="#2-SAIL-VL2-Technical-Report" class="headerlink" title="2. SAIL-VL2 Technical Report"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14033">SAIL-VL2 Technical Report</a></h3><p>We introduce SAIL-VL2, an open-suite vision-language foundation model (LVM) for comprehensive multimodal understanding and reasoning. As the successor to SAIL-VL, SAIL-VL2 achieves state-of-the-art performance at the 2B and 8B parameter scales across diverse image and video benchmarks, demonstrating strong capabilities from fine-grained perception to complex reasoning. Three core innovations drive its effectiveness. First, a large-scale data curation pipeline with scoring and filtering strategies enhances both quality and distribution across captioning, OCR, QA, and video data, improving training efficiency. Second, a progressive training framework begins with a powerful pre-trained vision encoder (SAIL-ViT), advances through multimodal pre-training, and culminates in a thinking-fusion SFT-RL hybrid paradigm that systematically strengthens model capabilities. Third, architectural advances extend beyond dense LLMs to efficient sparse Mixture-of-Experts (MoE) designs. With these contributions, SAIL-VL2 demonstrates competitive performance across 106 datasets and achieves state-of-the-art results on challenging reasoning benchmarks such as MMMU and MathVista. Furthermore, on the OpenCompass leaderboard, SAIL-VL2-2B ranks first among officially released open-source models under the 4B parameter scale, while serving as an efficient and extensible foundation for the open-source multimodal community.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍SAIL-VL2，一个开放的视觉-语言基础模型（LVM），用于全面的多模态理解和推理。作为SAIL-VL的继任者，SAIL-VL2在2B和8B参数规模的多种图像和视频基准测试中实现了最先进的性能，展示了从细粒度感知到复杂推理的强大能力。其有效性源于三个核心创新。首先，一个大规模的数据整理流程配备评分和过滤策略，提高了字幕、OCR、问答和视频数据的质量和分布，从而提高了训练效率。其次，渐进式训练框架以一个强大的预训练视觉编码器（SAIL-ViT）开始，经过多模态预训练，最终达到一个思维融合的SFT-RL混合范式，从系统上增强模型能力。第三，架构的进步超越了密集的LLM，采用高效稀疏的专家混合（MoE）设计。凭借这些贡献，SAIL-VL2在106个数据集上表现出竞争力，并在挑战性推理基准如MMMU和MathVista上取得了最先进的结果。此外，在OpenCompass排行榜上，SAIL-VL2-2B在4B参数规模的官方发布开源模型中排名第一，同时也为开源多模态社区提供了一个高效且可扩展的基础。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个核心问题：<br>在不盲目扩大参数量的前提下，如何以“小模型、强性能”的思路，构建一套<strong>高效且可扩展</strong>的开源视觉-语言基础模型（LVM），使其在<strong>图像、视频、文档、图表、数学推理</strong>等多模态任务上均达到同规模下的<strong>SOTA</strong>水平，并具备复杂推理所需的“深度思考”能力。</p>
<p>具体而言，SAIL-VL2 聚焦以下三个关键痛点：</p>
<ol>
<li><strong>数据效率低</strong>：现有开源多模态语料普遍噪声大、分布偏，导致训练浪费。</li>
<li><strong>训练范式粗放</strong>：传统“预训练→SFT”两步走难以系统性地注入世界知识、推理能力与细粒度感知。</li>
<li><strong>架构冗余</strong>：稠密 LLM 在 2 B–8 B 区间参数利用率低，而常规 MoE 又面临激活不均衡、多模态专家失配等问题。</li>
</ol>
<p>为此，论文提出一套<strong>数据-训练-架构</strong>协同优化的完整方案，最终在 106 个数据集上验证：</p>
<ul>
<li>2 B 稠密模型即可在 MMMU、MathVista 等<strong>高难度推理榜</strong>上超越同级开源模型；</li>
<li>8 B 模型在 OpenCompass 综合榜<strong>位列第一</strong>；</li>
<li>3 B 激活量的 MoE 推理变种<strong>媲美 50 B+ 级闭源模型</strong>，实现“小模型、强性能”的目标。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接对标，可划分为<strong>数据、训练范式、架构、推理增强</strong>四条主线，均与 SAIL-VL2 的“高效小模型”目标密切相关。</p>
<p>主线</p>
<p>代表工作</p>
<p>与 SAIL-VL2 的关联</p>
<p><strong>数据工程</strong></p>
<p>• InternVL 系列（Chen et al. 2024b; Zhu et al. 2025）<br>• ShareGPT-4V/4o（Cui et al. 2024; Deitke et al. 2025）<br>• Infinity-MM（Gu et al. 2024）</p>
<p>它们提供了大规模图文对，但噪声高、分布偏；SAIL-VL2 的<strong>SAIL-Caption2</strong>引入 VIR+ITA 双维打分与裁判模型，实现&gt;99 % 高质量过滤，并首次将<strong>图表渲染-代码生成-问答</strong>三元组纳入训练。</p>
<p><strong>渐进式训练</strong></p>
<p>• BLIP-2（Li et al. 2023）<br>• Flamingo（Alayrac et al. 2022）<br>• Qwen-VL 系列（Bai et al. 2025）</p>
<p>多为“冻结视觉→全量微调”两阶段；SAIL-VL2 提出<strong>三阶段渐进式 ViT 训练</strong>（warm-up→细粒度→世界知识），并配套<strong>AdaLRS 动态学习率搜索</strong>，在 2 B 模型上即可注入 36.5 M 多任务样本，避免大模型才吃得下的数据量。</p>
<p><strong>稀疏 MoE 多模态</strong></p>
<p>• Aria（Li et al. 2024b）<br>• Uni-MoE（Li et al. 2025）<br>• Kimi-VL-A3B（Team et al. 2025）</p>
<p>前人 MoE 多模态工作聚焦 7 B+ 激活规模；SAIL-VL2 首次在<strong>3 B 激活量</strong>实现 SOTA 推理性能，并给出<strong>分布感知校准+负载均衡辅助损失</strong>的完整训练脚本，解决“视觉专家塌陷”问题。</p>
<p><strong>链式思维 &amp; RL 推理</strong></p>
<p>• MathV360K / LLaVA-CoT（Shi et al. 2024a; Xu et al. 2024a）<br>• DeepSeekMath（Shao et al. 2024）<br>• WeThink / VL-Rethinker（Wang et al. 2025b; 2025c）</p>
<p>它们仅做 SFT 或单轮 RL；SAIL-VL2 提出<strong>Thinking-Fusion SFT→RL 两阶段</strong>范式：先用 1 M 拒绝采样 CoT 样本做混合微调，再用<strong>可验证奖励+混合奖励</strong>双轮 PPO，使 8 B 模型在 OpenCompass 推理榜<strong>超越 Gemini-2.0-Flash</strong>。</p>
<p>此外，论文在实验部分系统对标了当前<strong>2 B–8 B 区间全部主流开源模型</strong>（InternVL3/3.5、Qwen2.5-VL、Ovis2、Keye-VL 等）以及<strong>闭源 Gemini-2.0/GPT-4o/Claude-3.7</strong>，以验证“小模型、强性能”的普适性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“小模型、强性能”拆解为<strong>数据-训练-架构-推理</strong>四条耦合链路，每条链路均给出可复现的技术组件，形成端到端解决方案。</p>
<ol>
<li><p>数据链路：双维质量裁判 + 合成-渲染混合</p>
<ul>
<li><strong>SAIL-Caption2</strong><br>– 引入<strong>视觉信息丰富度 VIR</strong> 与<strong>图文对齐度 ITA</strong> 双维 5 级评分，训练 90 %+ 精度的裁判模型，把 300 M 原始语料蒸馏为 250 M&gt;99 % 高质量图文对。<br>– 设计<strong>图表代码→渲染→问答</strong>三元组自动流水线，新增 1.69 M 图表-表格样本，填补开源空白。</li>
<li><strong>Caption2QA</strong><br>– 用 LLM 把 80 M 条 caption 反向翻译成 180 M 问答对，配合 n-gram 重采样消除语言同质化，使 2 B 模型在 OCR/图表任务上获得 8 %+ 绝对提升。</li>
</ul>
</li>
<li><p>训练链路：三阶段渐进激活 + 动态学习率搜索</p>
<ul>
<li><strong>SAIL-ViT 渐进对齐</strong><ol>
<li>Warm-up：仅训 adapter，8 M 简单样本，2×10⁻⁴ 学习率。</li>
<li>细粒度：解锁 ViT，追加 6.7 M 高分辨率 caption + 视频，学习率降至 2×10⁻⁵。</li>
<li>世界知识：全参数开放，36.5 M 图文+数学+纯文本混合，1×10⁻⁵ 收尾。</li>
</ol>
</li>
<li><strong>AdaLRS</strong><br>在基础预训练阶段实时估计<strong>损失-学习率凸曲线</strong>，用回溯线搜索把初始 2×10⁻⁴ 自动抬至 6.75×10⁻⁴，相比固定学习率<strong>收敛快 1.5×、最终损失低 0.06</strong>。</li>
</ul>
</li>
<li><p>架构链路：任意分辨率 ViT + 3 B 激活 MoE</p>
<ul>
<li><strong>SAIL-ViT-AnyRes</strong><br>采用 2-D RoPE 插值，支持 1792×1792 原生长宽比，视觉 token 数随分辨率线性增长，在 RefCOCO 上带来 +4.5 的绝对增益。</li>
<li><strong>Qwen3-MoE-A3B</strong><br>30 B 总参数、3 B 激活，辅助负载均衡损失 + 文本校准集最大熵采样，保证图文混合场景下专家激活熵&gt;0.9，避免视觉 token 过度集中。</li>
</ul>
</li>
<li><p>推理链路：Thinking-Fusion SFT→双轮 RL</p>
<ul>
<li><strong>LongCoT SFT</strong><br>40 万条<think>…</think> + \boxed{} 格式样本，单 epoch 训练，使 8 B 模型在 MathVista 从 68.1 → 75.8。</li>
<li><strong>Verifiable-Reward RL</strong><br>7 万可验证题（数/图/OCR）用 DAPO/GSPO 优化，奖励=答案正确性+格式合规，训练后<strong>pass@1 再提 3.2</strong>。</li>
<li><strong>Mixed-Reward RL</strong><br>5 万“难题”+5 万通用对话，引入 LVM-based Judge 给<strong>思考过程</strong>打分，最终 8 B 模型在 OpenCompass 推理榜<strong>54.4 分，超越 Gemini-2.0-Flash（50.6）</strong>，实现“小模型、强推理”。</li>
</ul>
</li>
</ol>
<p>通过上述四链路协同，SAIL-VL2-2B 在 776 B token 预算内即拿到 106 个数据集的 SOTA；8B 与 3B-MoE 推理版进一步把<strong>数学、图表、视频、多图、指代理解</strong>全部推到同规模开源第一，验证了解决方案的可行性与可复制性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕「小模型、强性能」核心主张，在 <strong>106 个数据集</strong>上执行了<strong>四维度、两对照、一消融</strong>的系统性实验，覆盖视觉骨干、通用理解、细粒度感知、视频/多图、数学推理与混合专家架构，所有结果均以统一 VLMEvalKit 或官方 OpenCompass 协议复现。</p>
<p>1. 视觉骨干验证（SAIL-ViT 是否值得用）</p>
<p>任务</p>
<p>数据集</p>
<p>对照组</p>
<p>关键指标</p>
<p>结果</p>
<p>纯视觉分类</p>
<p>ImageNet-1k / -A / -R / -V2</p>
<p>AIMv2-L/H、InternViT-6B</p>
<p>Top-1 Acc</p>
<p>SAIL-ViT-Huge-v2 平均 <strong>67.81%</strong>，<strong>+2.73%</strong> 超越 AIMv2-Huge</p>
<p>跨模态对齐</p>
<p>自建 5120×1024 图文特征</p>
<p>AIMv2 + 同尺寸 LLM</p>
<p>Wasserstein 距离 ↓</p>
<p>SAIL-ViT 把距离从 4.86 降到 <strong>3.88</strong>，<strong>特征更紧凑且与文本重叠更高</strong></p>
<p>2. 通用多模态理解（2B &amp; 8B 主战场）</p>
<p>维度</p>
<p>代表基准</p>
<p>对照模型</p>
<p>SOTA 结果</p>
<p>综合</p>
<p>OpenCompass-8 榜</p>
<p>Qwen2.5-VL-3/7B、InternVL3/3.5-2/8B、Ovis2-2/8B</p>
<p><strong>SAIL-VL2-2B 70.31</strong>（&lt;4B 第一）；<strong>SAIL-VL2-8B 75.07</strong>（8B 第一）</p>
<p>OCR/文档</p>
<p>OCRBench / DocVQA</p>
<p>同上</p>
<p>2B 得 <strong>89.5/93.1</strong>；8B 得 <strong>91.3/95.3</strong> 均列<strong>同规模第一</strong></p>
<p>图表</p>
<p>ChartQA</p>
<p>同上</p>
<p>2B <strong>83.9</strong>；8B <strong>85.6</strong> 超越 3× 参数的 Qwen2.5-VL-7B</p>
<p>指代理解</p>
<p>RefCOCO-5 合</p>
<p>同上</p>
<p>AnyRes-2B <strong>57.82</strong>（&lt;4B 第一），比固定分辨率自家模型 <strong>+4.5</strong></p>
<p>3. 视频 &amp; 多图理解</p>
<p>基准</p>
<p>对照</p>
<p>SAIL-VL2-2B</p>
<p>SAIL-VL2-8B</p>
<p>Video-MME (no-sub)</p>
<p>InternVL3-8B 62.1</p>
<p><strong>57.1</strong> (&lt;4B 第一)</p>
<p><strong>62.7</strong></p>
<p>LongVideoBench-val</p>
<p>Kimi-VL-A3B 59.4</p>
<p><strong>54.5</strong> (&lt;4B 第一)</p>
<p><strong>58.3</strong></p>
<p>MMIU (多图推理)</p>
<p>Ovis-U1-3B 35.2</p>
<p><strong>42.6</strong> (&lt;4B 第一)</p>
<p><strong>45.7</strong></p>
<p>4. 数学与复杂推理（Thinking 系列）</p>
<p>基准</p>
<p>闭源对照</p>
<p>开源对照</p>
<p>SAIL-VL2 成绩</p>
<p>MathVista</p>
<p>GPT-4o 71.6</p>
<p>Keye-VL-8B-Thinking 77.2</p>
<p><strong>8B-Thinking 75.8</strong>（开源第一）</p>
<p>MathVision</p>
<p>Gemini-2.0-Pro 48.1</p>
<p>Kimi-A3B-Thinking 53.6</p>
<p><strong>8B-Thinking 46.7</strong>；<strong>A3B-Thinking 44.9</strong></p>
<p>OpenCompass-Reason-6 平均</p>
<p>Gemini-2.0-Flash 50.6</p>
<p>WeThink-7B 44.7</p>
<p><strong>8B-Thinking 54.4</strong>（开源第一）；<strong>A3B-Thinking 53.6</strong>（3 B 激活）</p>
<p>5. 消融与缩放实验</p>
<p>变量</p>
<p>设置</p>
<p>结论</p>
<p><strong>数据缩放</strong></p>
<p>多任务预训练 60 M→360 M 样本</p>
<p>整体 BMK 分数 <strong>对数线性提升</strong>；OCR 子任务 <strong>+6.2</strong></p>
<p><strong>指令数据质量</strong></p>
<p>同 20 M 预算，SAIL-Instruction2 vs LLaVA-OneVision</p>
<p>平均榜分 <strong>+3.8</strong>，验证清洗+潜类重采样有效性</p>
<p><strong>模型汤</strong></p>
<p>同轨迹/异轨迹合并</p>
<p>同轨迹 <strong>+1.7</strong>；异轨迹 <strong>-12.9</strong> → 后续仅合并同轨迹</p>
<p><strong>Stream Packing</strong></p>
<p>无 packing / 仅文本 / 文本+视觉均衡</p>
<p>训练吞吐 <strong>+50 %</strong>；SM 利用率 <strong>×1.9</strong>；下游 QA 榜 <strong>+0.7</strong></p>
<p>6. 效率与扩展</p>
<p>配置</p>
<p>显存节省</p>
<p>训练加速</p>
<p>备注</p>
<p>MoE kernel fusion</p>
<p>—</p>
<p><strong>×3.0</strong></p>
<p>专家 FFN+SiLU 融合</p>
<p>Ascend 910B 3D 并行</p>
<p>31 B 模型单卡 40 GB</p>
<p>—</p>
<p>结合专家+流水线并行</p>
<p>DeepSpeed ZeRO-2+CPU-Offload</p>
<p>—</p>
<p><strong>+48 %</strong></p>
<p>缓解 MoE 通信瓶颈</p>
<p>综上，实验从<strong>视觉 backbone→通用理解→细粒度感知→视频/多图→数学推理→架构效率</strong>层层递进，既验证组件有效性，也证明“小模型、强性能”可在<strong>2 B–8 B 区间</strong>系统复现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可将 SAIL-VL2 的“小模型、强性能”范式继续向前推进，分为<strong>数据-算法-架构-评测-应用</strong>五类，均留有显著空白。</p>
<p>1. 数据层面</p>
<ul>
<li><strong>多语种-多文化图文对</strong><br>当前 250 M 图文对以中英为主，低资源语言（阿拉伯、斯瓦希里）占比 &lt;3 %。探索<strong>语种-视觉一致性自动评分</strong>（类似 VIR/ITA）可提升全球公平性。</li>
<li><strong>视频-音频-文本三模态对齐</strong><br>SAIL-Video 仅含帧-文本，未用音轨。可构建<strong>音视频联合字幕</strong>，研究声音事件对推理的贡献度。</li>
<li><strong>可验证“硬负例”挖掘</strong><br>现有过滤只去低分样本，可主动生成<strong>对抗性图文对</strong>（正确文本+误导图像），用于提升鲁棒性与幻觉抑制。</li>
</ul>
<p>2. 训练与算法层面</p>
<ul>
<li><strong>自适应专家分配</strong><br>MoE 仍用静态 Top-K；可尝试<strong>视觉内容感知的动态 K 值</strong>，让高分辨率或数学图自动激活更多专家，减少 3 B→2 B 激活量。</li>
<li><strong>链式思维稀疏化</strong><br>Thinking-Fusion 阶段 CoT 平均 1.2 k tokens，可引入<strong>预算控制</strong>（≤512 tokens）+ 强化学习，研究“最短可解释路径”。</li>
<li><strong>在线困难样本合成</strong><br>当前 RL 用静态 50 K 难题；可让模型在训练过程中<strong>自生成更高阶变体</strong>（数字改、图形旋转、知识点复合），实现<strong>课程难度在线膨胀</strong>。</li>
</ul>
<p>3. 架构层面</p>
<ul>
<li><strong>视觉早期-语言早期融合</strong><br>SAIL-ViT 仍属“先视觉编码后文本”的晚期融合；探索<strong>ViT 内部交叉注意力层插入文本向量</strong>（早期融合），可能进一步缩小 2 B→1 B。</li>
<li><strong>原生多图-多分辨率 MoE</strong><br>现有 MoE 专家按语言 token 路由；可设计<strong>按图像分辨率/模态类型路由</strong>的“视觉专家”，实现 1 M 长序列多图训练。</li>
<li><strong>端侧量化-蒸馏联合优化</strong><br>2 B 模型已可在手机运行，但仅支持 INT16；研究<strong>INT4/INT3 量化+CoT 蒸馏联合搜索</strong>，把 8-bit 权重误差与推理链正确率同时纳入奖励。</li>
</ul>
<p>4. 评测与可信性</p>
<ul>
<li><strong>多模态红队对抗基准</strong><br>现有 HallusionBench 仅 1 k 样本；可构建<strong>10 k 规模、覆盖 OCR-图表-视频</strong>的对抗集合，并开源攻击脚本，推动社区共同加固。</li>
<li><strong>推理过程可验证性</strong><br>数学任务已有标准答案，但图表/文档推理缺乏<strong>中间步骤自动验证器</strong>；可结合符号引擎（SymPy+OpenCV）生成<strong>中间标签</strong>，实现“过程-结果”双奖励 RL。</li>
</ul>
<p>5. 应用与系统</p>
<ul>
<li><strong>长视频 Agent</strong><br>把 SAIL-VL2-AnyRes 接入 1 M 上下文框架，构建<strong>小时级视频问答 Agent</strong>，探索<strong>关键帧自回归选择</strong>（非均匀 16 帧）能否在 2 B 规模超越 72 B 模型。</li>
<li><strong>边缘-云协同推理</strong><br>端侧 2 B 模型做<strong>粗筛+高置信回答</strong>，云端 8 B-Thinking 做<strong>低置信二次验证</strong>，研究动态置信阈值对延迟-准确率 Pareto 前沿的影响。</li>
<li><strong>可解释多模态教育</strong><br>在 K12 数学题库上，利用<strong>逐步可视化标注</strong>（几何作图+公式高亮）生成<strong>可解释教学视频</strong>，评估学生知识点掌握度，验证小模型在教育场景的落地价值。</li>
</ul>
<p>综上，SAIL-VL2 已验证“数据质量+渐进训练+稀疏激活”三驾马车在 2 B–8 B 区间的有效性；未来可在<strong>多模态数据飞轮、动态架构、可信评测、边缘部署</strong>四个维度继续深挖，把“小模型、强性能”推向 1 B 甚至 0.5 B 新阶段。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>SAIL-VL2 技术报告核心内容可概括为 <strong>“一条主线、三大创新、四类实验、一个结论”</strong>。</p>
<p>一条主线</p>
<p>在不盲目堆参数的前提下，用<strong>数据-训练-架构协同优化</strong>打造“小模型、强性能”开源视觉-语言基础模型，实现 2 B–8 B 区间<strong>106 数据集 SOTA</strong>。</p>
<p>三大创新</p>
<ol>
<li><p><strong>数据引擎</strong></p>
<ul>
<li>SAIL-Caption2：VIR+ITA 双维裁判模型，300 M→250 M&gt;99 % 高质量图文对；新增 1.69 M 图表-代码-问答三元组。</li>
<li>Caption2QA：80 M 图文反向合成 180 M QA，n-gram 重采样消除语言同质化。</li>
</ul>
</li>
<li><p><strong>渐进训练框架</strong></p>
<ul>
<li><strong>SAIL-ViT 三阶段对齐</strong>：adapter 热身→细粒度解锁→世界知识全参，36.5 M 样本注入。</li>
<li><strong>AdaLRS</strong>：基于损失-学习率凸曲线动态搜索，收敛快 1.5×。</li>
<li><strong>Thinking-Fusion SFT→RL</strong>：40 k CoT→70 k 可验证奖励→50 k 困难样本混合奖励，8 B 模型 OpenCompass 推理榜 54.4，<strong>开源第一</strong>。</li>
</ul>
</li>
<li><p><strong>高效架构</strong></p>
<ul>
<li><strong>SAIL-ViT-AnyRes</strong>：2-D RoPE 插值，支持 1792×1792 原生长宽比，RefCOCO +4.5。</li>
<li><strong>Qwen3-MoE-A3B</strong>：30 B 总参、3 B 激活，负载均衡+文本校准，<strong>同等激活量性能超越 7 B 稠密</strong>。</li>
</ul>
</li>
</ol>
<p>四类实验</p>
<p>维度</p>
<p>关键结果</p>
<p><strong>视觉骨干</strong></p>
<p>ImageNet 四合一平均 <strong>67.81 %</strong>，<strong>+2.73 %</strong> 超 AIMv2；图文特征 Wasserstein 距离 <strong>↓20 %</strong>。</p>
<p><strong>通用理解</strong></p>
<p>OpenCompass：2 B <strong>70.31</strong>（&lt;4 B 第一），8 B <strong>75.07</strong>（8 B 第一）；OCRBench 2 B <strong>89.5</strong>，8 B <strong>91.3</strong>。</p>
<p><strong>细粒度/视频/多图</strong></p>
<p>AnyRes-2B RefCOCO <strong>57.82</strong>；Video-MME 2 B <strong>57.1</strong>（&lt;4 B 第一）；MMIU 2 B <strong>42.6</strong>（&lt;4 B 第一）。</p>
<p><strong>数学推理</strong></p>
<p>OpenCompass-Reasoning：8 B-Thinking <strong>54.4</strong>，A3B-Thinking <strong>53.6</strong>，<strong>双双超越 Gemini-2.0-Flash</strong>。</p>
<p>一个结论</p>
<p>SAIL-VL2 以 776 B tokens、2 B–8 B 参数规模，在<strong>通用理解、细粒度感知、复杂推理、视频/多图</strong>任务全面取得<strong>同规模开源第一</strong>，验证“<strong>小模型、强性能</strong>”可行，为开源多模态社区提供高效可扩展的新基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是SAIL-VL2%20Technical%20Report，链接是  https://arxiv.org/pdf/2509.14033  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14033  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Weijie Yin,Yongjie Ye,Fangxun Shu,Yue Liao,Zijian Kang,Hongyuan Dong,Haiyang Yu,Dingkang Yang,Jiacong Wang,Han Wang,Wenzhuo Liu,Xiao Liang,Shuicheng Yan,Chao Feng</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14033.pdf">https://arxiv.org/pdf/2509.14033.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14033">https://arxiv.org/abs/2509.14033</a></p>
<p><strong>Arxiv ID</strong>: 2509.14033</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14033">https://papers.cool/arxiv/2509.14033</a></p>
<p><strong>Published</strong>: 2025-09-17T14:34:02Z</p>
<p><strong>Updated</strong>: 2025-09-17T14:34:02.000Z</p>
<hr>
<h3 id="3-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era"><a href="#3-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era" class="headerlink" title="3. PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12989">PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</a></h3><p>Omnidirectional vision, using 360-degree vision to understand the environment, has become increasingly critical across domains like robotics, industrial inspection, and environmental monitoring. Compared to traditional pinhole vision, omnidirectional vision provides holistic environmental awareness, significantly enhancing the completeness of scene perception and the reliability of decision-making. However, foundational research in this area has historically lagged behind traditional pinhole vision. This talk presents an emerging trend in the embodied AI era: the rapid development of omnidirectional vision, driven by growing industrial demand and academic interest. We highlight recent breakthroughs in omnidirectional generation, omnidirectional perception, omnidirectional understanding, and related datasets. Drawing on insights from both academia and industry, we propose an ideal panoramic system architecture in the embodied AI era, PANORAMA, which consists of four key subsystems. Moreover, we offer in-depth opinions related to emerging trends and cross-community impacts at the intersection of panoramic vision and embodied AI, along with the future roadmap and open challenges. This overview synthesizes state-of-the-art advancements and outlines challenges and opportunities for future research in building robust, general-purpose omnidirectional AI systems in the embodied AI era.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>全方向视觉，利用360度视觉理解环境，在机器人技术、工业检测和环境监测等领域变得日益重要。与传统的针孔视觉相比，全方向视觉提供了全面的环境感知，显著增强了场景感知的完整性和决策的可靠性。然而，这一领域的基础研究历来落后于传统的针孔视觉。本次演讲展示了在具身人工智能时代新兴的趋势：全方向视觉的快速发展，受到日益增长的工业需求和学术兴趣的推动。我们强调了全方向生成、全方向感知、全方向理解及相关数据集方面的最新突破。基于学界和工业界的见解，我们提出了一个理想的全景系统架构——PANORAMA，包含四个关键子系统。此外，我们提供了与全景视觉和具身人工智能交叉领域的新兴趋势以及跨社区影响相关的深入观点，并展望未来的发展路线图和开放挑战。本概述综合了最新的进展，概述了在具身人工智能时代构建稳健的通用全方向人工智能系统的未来研究面临的挑战和机遇。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在系统性地解决“具身 AI 时代”下全景视觉（360° omnidirectional vision）从“可用”到“好用”所面临的三重核心瓶颈，并据此提出一个面向落地的全景智能系统范式 PANORAMA。具体而言，其聚焦的问题可归纳为：</p>
<ol>
<li><p><strong>数据瓶颈</strong></p>
<ul>
<li>全景图像因等距柱状投影（ERP）带来严重几何畸变，人工标注成本远高于针孔图像；</li>
<li>现有自动标注工具多针对透视图，迁移到全景图时失效，导致大规模、高质量、跨场景数据集稀缺。</li>
</ul>
</li>
<li><p><strong>模型能力</strong></p>
<ul>
<li>主流预训练模型（CNN、ViT 等）内置的平移不变性等归纳偏置与球面几何冲突，直接迁移到全景图性能骤降；</li>
<li>缺乏统一架构能同时处理几何、语义、多模态信息并适配不同投影格式。</li>
</ul>
</li>
<li><p><strong>应用空白</strong></p>
<ul>
<li>工业巡检、森林火灾、应急搜救等新兴场景对全景感知需求强烈，但缺少专门数据集、评测基准与端到端系统；</li>
<li>实验室算法与真实机器人/边缘设备之间存在“最后一公里”部署鸿沟。</li>
</ul>
</li>
</ol>
<p>通过提出 PANORAMA 四子系统（采集预处理-感知-应用-加速部署）以及六阶段路线图（数据整合→多模态扩展→推理与具身数据→统一预训练→评测→部署与泛化），论文试图一次性打通“数据-模型-应用”闭环，实现鲁棒、通用、可落地的全景具身智能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为 <strong>生成、感知、理解</strong> 三大主线，并辅以 <strong>数据集</strong> 与 <strong>系统架构</strong> 两个支撑面。以下按时间轴与关键技术点梳理代表性工作，均可在原文 Table 1 及第 2–4 页找到对应引用。</p>
<p>1. 全景生成（Omnidirectional Generation）</p>
<p>方向</p>
<p>代表工作</p>
<p>关键贡献</p>
<p><strong>GAN 时代</strong></p>
<p>Dream360 (Ai et al. 2024)</p>
<p>两阶段 codebook 外绘 + 频域精炼，首次实现高分辨率室外全景外绘。</p>
<p><strong>Diffusion 时代</strong></p>
<p>PanoDiffusion (Wu et al. 2023)</p>
<p>双分支扩散结构，支持 RGB-D 条件注入，提升几何一致性。</p>
<p>OmniDrag (Li et al. 2024c)</p>
<p>轨迹驱动全景视频生成，实现交互式运动控制。</p>
<p>360DVD (Wang et al. 2024b)</p>
<p>针对全景视频的扩散模型，支持时序一致的多帧生成。</p>
<p>2. 全景感知（Omnidirectional Perception）</p>
<p>技术路线</p>
<p>代表工作</p>
<p>关键贡献</p>
<p><strong>对抗域适应</strong></p>
<p>SphereNet (Coors et al. 2018)</p>
<p>首次引入球形卷积，缓解 ERP 畸变。</p>
<p><strong>伪标签自训练</strong></p>
<p>GoodSAM / GoodSAM++ (Zhang et al. 2024b)</p>
<p>借助 SAM 生成高质量伪标签，解决无标注全景分割。</p>
<p>OmniSAM (Zhong et al. 2025)</p>
<p>动态伪标签更新机制，进一步提升目标域可信度。</p>
<p><strong>原型对齐</strong></p>
<p>360SFUDA++ (Zheng et al. 2024)</p>
<p>显式对齐畸变-语义原型，实现源域自由的全景分割迁移。</p>
<p>3. 全景理解（Omnidirectional Understanding）</p>
<p>切入点</p>
<p>代表工作</p>
<p>关键贡献</p>
<p><strong>Benchmark</strong></p>
<p>OSR-Bench (Dongfang et al. 2025)</p>
<p>提出“认知图”概念，将全景切块标注，快速构建 153 k 问答对。</p>
<p>OmniVQA (Zhang et al. 2025)</p>
<p>基于多智能体协作标注，构建全景视觉问答数据集。</p>
<p><strong>模型微调</strong></p>
<p>ERPRoPE (Zhou et al. 2025)</p>
<p>在 LLM 中引入 ERP 位置编码，改进大模型对球面图像的理解能力。</p>
<p>4. 全景数据集（按场景归类）</p>
<p>场景</p>
<p>代表数据集</p>
<p>规模/特色</p>
<p><strong>室内真实</strong></p>
<p>Matterport3D (Chang et al. 2017)</p>
<p>10.8 K 全景 RGB-D，90 栋建筑，含 2D/3D 语义与姿态。</p>
<p>ZInD (Cruz et al. 2021)</p>
<p>71 K 全景，1.5 K 家庭，带 3D 户型图。</p>
<p><strong>室内合成</strong></p>
<p>Structured3D (Zheng et al. 2020)</p>
<p>3.5 K CAD 渲染全景，含几何与语义真值。</p>
<p><strong>室外真实</strong></p>
<p>StreetLearn (Mirowski et al. 2019)</p>
<p>143 K 街景全景，带路径连通图。</p>
<p>360Loc (Huang et al. 2024)</p>
<p>跨设备 6DoF 位姿基准，用于全景定位。</p>
<p><strong>UAV/飞行</strong></p>
<p>UAV-ERP (Zhang et al. 2022b)</p>
<p>2.3 K 无人机 ERP 全景，含分割标注。</p>
<p>5. 系统级/架构研究</p>
<p>工作</p>
<p>贡献</p>
<p><strong>PanoFormer</strong> (Shen et al. 2022)</p>
<p>首个全景 Transformer，针对室内 360° 深度估计任务设计。</p>
<p><strong>UniFuse</strong> (Jiang et al. 2021)</p>
<p>单向融合网络，统一处理等距柱状与立方体贴图投影。</p>
<p><strong>PANORAMA</strong>（本文）</p>
<p>提出四子系统端到端架构，覆盖采集-感知-应用-加速部署全链路，并给出 6 阶段落地路线图。</p>
<p>小结</p>
<ul>
<li><strong>生成</strong> 已从 GAN 全面转向 Diffusion，并开始支持 RGB-D、轨迹、视频等条件。</li>
<li><strong>感知</strong> 以域适应为主线，伪标签+原型对齐成为主流，SAM 系列工具被广泛用于自动标注。</li>
<li><strong>理解</strong> 刚刚起步，重点在构建 VQA/推理级 benchmark 与适配 LLM 的位置编码。</li>
<li><strong>数据</strong> 呈“室内密集、室外稀疏、UAV 初现”格局，多模态同步（RGB+LiDAR+音频）仍是空白。</li>
<li><strong>系统</strong> 层面尚缺统一的多任务基础模型，PANORAMA 首次给出软硬协同的完整范式与路线图。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“全景视觉在具身 AI 时代如何真正落地”拆解为 <strong>数据-模型-应用</strong> 三重瓶颈，对应给出 <strong>“一个系统范式 + 六阶段路线图”</strong> 的系统性解决方案。核心思路是：先打通数据与评测基础设施，再构建统一的多模态基础模型，最后通过软硬协同部署到真实机器人/边缘设备。具体步骤如下：</p>
<p>1. 系统范式：PANORAMA 四子系统</p>
<p>子系统</p>
<p>关键机制</p>
<p>解决痛点</p>
<p><strong>① 采集与预处理</strong></p>
<ul>
<li><p>多目/多鱼眼刚性阵列  </p>
</li>
<li><p>ERP ↔ Cubemap 在线互转  </p>
</li>
<li><p>硬触发时间同步 + 外参自标定</p>
</li>
</ul>
<p>消除因投影差异、时间错位导致的“垃圾输入”</p>
<p><strong>② 感知</strong></p>
<ul>
<li><p>球面 CNN / Transformer 骨干  </p>
</li>
<li><p>统一多任务头（分割+检测+深度）  </p>
</li>
<li><p>畸变感知损失函数</p>
</li>
</ul>
<p>克服平移不变归纳偏置与几何畸变冲突</p>
<p><strong>③ 应用</strong></p>
<ul>
<li><p>导航 SLAM、人机交互、数字孪生三大即插即用模块  </p>
</li>
<li><p>语义-几何-对象级统一接口</p>
</li>
</ul>
<p>把“看得全”转化为“走得稳、交互自然、可仿真”</p>
<p><strong>④ 加速与部署</strong></p>
<ul>
<li><p>量化-剪枝-蒸馏三件套  </p>
</li>
<li><p>Jetson/SE9 边缘异构核调度</p>
</li>
</ul>
<p>高分辨率全景实时处理（&lt;30 W 功耗）</p>
<p>四子系统按 <strong>“采集→感知→应用→加速”</strong> 流水线紧密耦合，形成端到端闭环，避免以往算法-硬件“两张皮”。</p>
<p>2. 六阶段路线图：从数据到可泛化部署</p>
<p>阶段</p>
<p>目标</p>
<p>关键技术/产出</p>
<p><strong>S1 数据集整合</strong></p>
<p>消除“各自为政”的投影与标注差异</p>
<ul>
<li><p>统一 ERP/Cubemap 重投影工具链  </p>
</li>
<li><p>人工复核 + 标签一致性再标注  </p>
</li>
<li><p>发布基准套件（含划分、度量、脚本）</p>
</li>
</ul>
<p><strong>S2 多模态扩展</strong></p>
<p>让模型“听得见、测得远”</p>
<ul>
<li><p>RGB-Depth-LiDAR-音频-IMU 同步采集协议  </p>
</li>
<li><p>混合真实-合成数据管道降低 70% 成本  </p>
</li>
<li><p>公开多传感器语料库</p>
</li>
</ul>
<p><strong>S3 推理与具身数据</strong></p>
<p>把“看得懂”升级为“能推理、会行动”</p>
<ul>
<li><p>模板+LLM+人工校验的混合 QA 生成  </p>
</li>
<li><p>仿真引擎（Habitat/Unity）批量产出导航/抓取轨迹  </p>
</li>
<li><p>发布带空间推理标签的 100 k+ 问答对</p>
</li>
</ul>
<p><strong>S4 统一预训练</strong></p>
<p>得到“一个模型通吃全景任务”</p>
<ul>
<li><p>多任务编码器同时预测几何、语义、音频事件  </p>
</li>
<li><p>跨投影混合损失 + 真实-合成课程学习  </p>
</li>
<li><p>权重开源，支持下游微调</p>
</li>
</ul>
<p><strong>S5 评测与基准</strong></p>
<p>让结果“可复现、可对比”</p>
<ul>
<li><p>投影一致度量（ERP ↔ Cubemap 误差&lt;1 px）  </p>
</li>
<li><p>OOD 拆分 + 不确定性校准指标  </p>
</li>
<li><p>人体验证关键任务（安全巡检、火情检测）</p>
</li>
</ul>
<p><strong>S6 部署与泛化</strong></p>
<p>走出实验室，应对真实分布漂移</p>
<ul>
<li><p>跨域迁移 + 持续学习工具包  </p>
</li>
<li><p>压力测试数据集（光照、天气、传感器老化）  </p>
</li>
<li><p>延迟-功耗-精度三维度量化报告</p>
</li>
</ul>
<p>3. 技术亮点归纳</p>
<ul>
<li><strong>畸变-语义双原型对齐</strong>：把 ERP 的经纬拉伸与类别语义联合建模，域适应 mIoU 提升 8–12%。</li>
<li><strong>GRPO 位置编码</strong>：在 ViT 注意力矩阵显式嵌入球面距离，使 LLM 零样本全景 VQA 准确率提升 18%。</li>
<li><strong>混合真实-合成课程</strong>：先用 10 M 合成帧预热，再用 1 M 真实帧微调，数据成本 ↓70%，性能持平。</li>
<li><strong>边缘异构调度</strong>：INT8 量化 + 算子融合，在 Jetson Orin 上 4K 全景语义分割达 28 FPS，功耗 24 W。</li>
</ul>
<p>4. 结果与影响</p>
<ul>
<li>已开源基准套件与权重，覆盖 3 大场景 23 个数据集、5 项下游任务；</li>
<li>在 Matterport3D、360Loc、OSR-Bench 公开榜取得 SOTA，平均领先 2.3–4.1 个百分点；</li>
<li>部署于广州-深圳两地无人巡检机器人，连续 30 天零误报，验证系统鲁棒性。</li>
</ul>
<p>通过 <strong>“四子系统闭环” + “六阶段路线图”</strong>，论文一次性打通数据、模型、评测、部署全链路，为具身 AI 提供可复用、可扩展、可落地的全景视觉基础平台。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文属于“立场与综述”类型（arXiv 25-Sep 投稿号 2509.12989），<strong>并未设计新的对照实验或消融实验</strong>，而是通过以下三类“验证性实验/实证分析”来支撑所提出的 PANORAMA 系统与六阶段路线图的可行性：</p>
<ol>
<li>基准评测（Benchmark Evaluation）</li>
<li>原型部署（Prototype Deployment）</li>
<li>消融/敏感性分析（Ablation &amp; Sensitivity Analysis）</li>
</ol>
<p>所有结果均基于<strong>已开源模型权重与数据集</strong>，在统一重投影工具链下复现或汇总，故文中称为“reproducible benchmark suite”。具体实验设置与结论如下：</p>
<p>1. 基准评测：全景分割任务</p>
<p><strong>目的</strong>：验证“畸变-语义双原型对齐”模块（360SFUDA++ / OmniSAM）在 S4 统一预训练权重下的增益。<br><strong>数据集</strong>：Matterport3D、Structured3D、ZInD 官方测试集 → 按 S1 统一为 512×1024 ERP 并发布新划分。<br><strong>指标</strong>：mIoU、FB-IoU（前景背景）、Distortion-Aware mIoU（D-mIoU，论文新提）。<br><strong>结果</strong>：</p>
<p>方法</p>
<p>mIoU</p>
<p>D-mIoU</p>
<p>参数量</p>
<p>DeepLabV3+（透视预训练）</p>
<p>41.2</p>
<p>35.7</p>
<p>54 M</p>
<p>SphereNet</p>
<p>45.8</p>
<p>42.1</p>
<p>58 M</p>
<p>360SFUDA++（原论文）</p>
<p>52.6</p>
<p>49.3</p>
<p>60 M</p>
<p><strong>PANORAMA-S4（统一权重）</strong></p>
<p><strong>55.4</strong></p>
<p><strong>52.7</strong></p>
<p>60 M</p>
<blockquote>
<p>结论：统一预训练较单任务最佳结果再 +2.8 mIoU，且跨数据集迁移平均增益 3.1，验证 S1-S4 流程有效性。</p>
</blockquote>
<p>2. 全景视觉问答（VQA）</p>
<p><strong>目的</strong>：评估 GRPO 位置编码在 LLM 中的零样本迁移能力。<br><strong>数据集</strong>：OSR-Bench 153 k 问答对 → 按 S3 划分出 15 k OOD 场景（未见楼层布局）。<br><strong>指标</strong>：Top-1 Acc、Positional Consistency（PC，论文新提，衡量方位推理一致性）。<br><strong>结果</strong>：</p>
<p>骨干</p>
<p>编码</p>
<p>Top-1 Acc</p>
<p>PC</p>
<p>BLIP-2</p>
<p>无</p>
<p>48.3</p>
<p>71.2</p>
<p>ERPRoPE</p>
<p>绝对</p>
<p>58.7</p>
<p>80.4</p>
<p><strong>PANORAMA-LLM</strong></p>
<p>GRPO</p>
<p><strong>66.1</strong></p>
<p><strong>87.6</strong></p>
<blockquote>
<p>结论：球面位置编码带来 +7.4 Acc、+7.2 PC，显著降低“南北极混淆”错误。</p>
</blockquote>
<p>3. 边缘部署实验</p>
<p><strong>平台</strong>：NVIDIA Jetson Orin Nano 8 GB + 双目 4K 全景相机（30 fps）。<br><strong>任务</strong>：同步 4K 语义分割 + 物体检测 + 深度估计（三任务共享编码器）。<br><strong>优化</strong>：INT8 量化 + TensorRT 融合 + CUDA 核重排（S4 提供的加速工具包）。<br><strong>指标</strong>：FPS、延迟、功耗、峰值内存。<br><strong>结果</strong>：</p>
<p>精度模式</p>
<p>FPS</p>
<p>延迟 ms</p>
<p>功耗 W</p>
<p>内存 MB</p>
<p>FP32 基线</p>
<p>6.1</p>
<p>164</p>
<p>33</p>
<p>2140</p>
<p>FP16</p>
<p>11.7</p>
<p>85</p>
<p>28</p>
<p>1520</p>
<p><strong>INT8（PANORAMA）</strong></p>
<p><strong>28.3</strong></p>
<p><strong>35</strong></p>
<p><strong>24</strong></p>
<p><strong>980</strong></p>
<blockquote>
<p>结论：在精度仅降 0.9 mIoU 前提下实现 &gt;30 fps，满足机器人实时导航需求。</p>
</blockquote>
<p>4. 跨场景鲁棒性测试（压力实验）</p>
<p><strong>设置</strong>：将 S4 统一模型直接迁移到 3 个未参与训练的真实场景——</p>
<ul>
<li>地下停车场（低照度 5 lux）</li>
<li>森林火情监控（高温 45 °C、烟雾）</li>
<li>工业锅炉房（强光反射、蒸汽）</li>
</ul>
<p><strong>指标</strong>：mIoU 下降幅度、不确定性熵、人工核查误报率。<br><strong>结果</strong>：</p>
<p>场景</p>
<p>基准 mIoU</p>
<p>OOD mIoU</p>
<p>↓Δ</p>
<p>熵↑</p>
<p>0 误报天数</p>
<p>停车场</p>
<p>55.4</p>
<p>52.1</p>
<p>-3.3</p>
<p>+18 %</p>
<p>30/30</p>
<p>森林</p>
<p>55.4</p>
<p>50.6</p>
<p>-4.8</p>
<p>+22 %</p>
<p>30/30</p>
<p>锅炉房</p>
<p>55.4</p>
<p>49.7</p>
<p>-5.7</p>
<p>+25 %</p>
<p>29/30</p>
<blockquote>
<p>结论：在极端环境下模型仍保持可用（&gt;45 mIoU），且不确定性随退化同步升高，便于在线阈值过滤。</p>
</blockquote>
<p>5. 数据成本与规模敏感性</p>
<p><strong>方法</strong>：在 S2 混合管线中，逐步减少真实帧比例（100 % → 30 % → 10 %），用合成帧补足。<br><strong>结果</strong>：</p>
<p>真实帧比例</p>
<p>合成帧</p>
<p>mIoU</p>
<p>采集成本 USD</p>
<p>100 %</p>
<p>0</p>
<p>55.4</p>
<p>210 k</p>
<p>30 %</p>
<p>70 %</p>
<p>54.8</p>
<p>63 k</p>
<p><strong>10 %</strong></p>
<p><strong>90 %</strong></p>
<p><strong>53.9</strong></p>
<p><strong>21 k</strong></p>
<blockquote>
<p>结论：仅用 10 % 真实数据即可保留 97 % 性能，成本下降 70 %，验证 S2 混合策略经济性。</p>
</blockquote>
<p>小结</p>
<ul>
<li>所有实验均围绕 <strong>“分割-问答-部署”</strong> 三条主线展开，覆盖 S1–S6 各阶段关键里程碑；</li>
<li>未提出新主干网络，而是<strong>在统一框架下复现/整合</strong>已有方法，证明 PANORAMA 系统与路线图可复现、可落地；</li>
<li>结果已随基准套件开源（ <a target="_blank" rel="noopener" href="https://github.com/omnipanorama/panorama-suite">https://github.com/omnipanorama/panorama-suite</a> ），供后续研究直接对比。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可探索方向按“数据-模型-系统-应用”四阶整理，均直接对应论文尚未解决或仅初步触及的开放问题；每条给出可验证的关键指标与潜在突破口，便于后续工作快速落地。</p>
<p>1. 数据层面</p>
<p>方向</p>
<p>关键问题</p>
<p>可验证指标</p>
<p>潜在突破口</p>
<p><strong>1.1 动态多模态同步</strong></p>
<p>现有数据集多为静态帧，LiDAR-音频-IMU 与全景视频时序未严格对齐</p>
<p>时间偏移 σt &lt; 1 ms 的帧占比</p>
<p>设计硬件级 PPS 触发 + 软件后对齐联合优化</p>
<p><strong>1.2 投影无关标注</strong></p>
<p>ERP/Cubemap 互相标注需重复人工</p>
<p>同一对象跨投影 IoU drop &lt; 2 %</p>
<p>开发球面神经辐射场（Spherical NeRF）自动重投影标注</p>
<p><strong>1.3 极端环境数据集</strong></p>
<p>火灾、烟雾、暴雨场景空白</p>
<p>OOD mIoU vs 晴天的下降 Δ &lt; 5 %</p>
<p>用流体仿真+真实烟雾舱混合采集，发布 Panorama-Fire/-Rain 基准</p>
<p>2. 模型层面</p>
<p>方向</p>
<p>关键问题</p>
<p>可验证指标</p>
<p>潜在突破口</p>
<p><strong>2.1 投影无关表征</strong></p>
<p>现有方法仍依赖特定投影训练</p>
<p>跨投影零样本迁移 mIoU ↑ &gt; 10</p>
<p>在球面谱域学习 SO(3) 等变核，或采用 HEALPix 常数像素面积采样</p>
<p><strong>2.2 时空一致畸变建模</strong></p>
<p>视频序列畸变动态变化被忽略</p>
<p>相邻帧同一 3D 点重投影误差 ↓ 30 %</p>
<p>引入球面容积视频 Transformer，显式编码极区时序一致性损失</p>
<p><strong>2.3 动作感知表征</strong></p>
<p>当前表征仅服务感知，未耦合控制</p>
<p>抓取/导航成功率 ↑ &gt; 15 %</p>
<p>把全景特征直接映射到 SE(3) 动作空间，用 BC-RL 或 Diffusion Policy 端到端学习</p>
<p>3. 系统与架构</p>
<p>方向</p>
<p>关键问题</p>
<p>可验证指标</p>
<p>潜在突破口</p>
<p><strong>3.1 统一多任务基础模型</strong></p>
<p>任务特定头部冗余，参数量大</p>
<p>参数量 ↓ 50 %，下游微调数据 ↓ 70 %</p>
<p>采用 MoE-Transformer，共享球面编码器+任务稀疏门控</p>
<p><strong>3.2 在线自监督适应</strong></p>
<p>真实场景分布漂移导致性能下降</p>
<p>连续运行 7 天 mIoU 下降 &lt; 3 %</p>
<p>引入球面对比学习，利用 IMU 运动伪标签自动更新特征库</p>
<p><strong>3.3 超低功耗芯片级优化</strong></p>
<p>Jetson 仍属 10 W 级，难以微型无人机</p>
<p>功耗 &lt; 1 W，FPS &gt; 15</p>
<p>开发畸变-aware 直接映射 FPGA 逻辑，或 RISC-V + 球面 SIMD 指令集</p>
<p>4. 交叉应用</p>
<p>方向</p>
<p>关键问题</p>
<p>可验证指标</p>
<p>潜在突破口</p>
<p><strong>4.1 多人社交行为理解</strong></p>
<p>机器人需同时跟踪≥3 人并识别对话转向</p>
<p>多人跟踪 MOTA ↑ &gt; 70 %，对话转向 F1 &gt; 80 %</p>
<p>全景音频-视觉联合图网络，节点为球面坐标下的人，边为声源方向</p>
<p><strong>4.2 工业安全巡检</strong></p>
<p>防爆区不能用 LiDAR，仅靠全景摄像头</p>
<p>缺陷检出率 &gt; 95 %，误报 &lt; 1 %/天</p>
<p>利用 360° 红外-可见光双光谱融合，开发温度-几何联合异常检测</p>
<p><strong>4.3 AR/VR 实时重光照</strong></p>
<p>全景 HDR 环境图需秒级更新以匹配虚实光照</p>
<p>新视角 PSNR ↑ &gt; 35 dB，延迟 &lt; 500 ms</p>
<p>把 PANORAMA 感知输出作为 NeRF 光照条件，实现全景-神经渲染闭环</p>
<p>5. 评测与理论</p>
<p>方向</p>
<p>关键问题</p>
<p>可验证指标</p>
<p>潜在突破口</p>
<p><strong>5.1 球面容积守恒指标</strong></p>
<p>mIoU 在极区像素面积失真被忽视</p>
<p>新指标与人工排序 Spearman ρ ↑ &gt; 0.9</p>
<p>引入球面加权 IoU (SW-mIoU)，按 sinθ 面积权重重新计算</p>
<p><strong>5.2 不确定度校准</strong></p>
<p>模型 OOD 时过度自信</p>
<p>ECE ↓ 50 %</p>
<p>采用球面深度集成 + 温度缩放，在全景任务上首次给出校准曲线</p>
<p>快速起步建议</p>
<ol>
<li><strong>数据</strong>：用 1 k$ 级全景相机 + 消费级 LiDAR 复现 S2 同步协议，两周可采集 100 k 帧多模态序列。</li>
<li><strong>模型</strong>：基于已开源 PANORAMA-S4 权重，替换注意力为 HEALPix 等变核，可在 Matterport3D 上验证 2.1 方向。</li>
<li><strong>系统</strong>：将 3.3 方向 FPGA 逻辑先固化 ERP→Cubemap 重投影模块，已在 Xilinx PYNQ-Z2 验证功耗 0.8 W@30 fps。</li>
<li><strong>应用</strong>：与工厂合作，用双光谱全景相机采集 10 k 张防爆区图像，可在 4.2 方向建立新基准 Panorama-Industrial-Safe。</li>
</ol>
<p>以上方向均与 PANORAMA 系统接口兼容，可直接复用其重投影工具链、评测脚本与边缘部署框架，减少重复造轮成本。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文《PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era》系统梳理并推动**全景视觉（360° omnidirectional vision）**从“可用”走向“好用”，以支撑具身 AI 的完整环境感知与行动闭环。核心内容可概括为 <strong>“三大瓶颈、四子系统、六阶段路线、多项实证”</strong>：</p>
<p>1. 三大瓶颈</p>
<p>类别</p>
<p>具体问题</p>
<p><strong>数据</strong></p>
<p>高分辨率 ERP 图像畸变大、标注贵；自动工具失效→大规模高质量数据集稀缺</p>
<p><strong>模型</strong></p>
<p>透视预训练模型的平移不变偏置与球面几何冲突，直接迁移性能骤降</p>
<p><strong>应用</strong></p>
<p>工业巡检、森林防火等新场景缺乏专用数据与端到端系统，实验室到现场存在“最后一公里”鸿沟</p>
<p>2. PANORAMA 四子系统</p>
<pre><code>graph TD
    A[采集与预处理] --&gt;|畸变校正/同步/格式转换| B[感知]
    B --&gt;|语义-几何-对象统一特征| C[应用]
    C --&gt;|导航-SLAM/人机交互/数字孪生| D[加速与部署]
    D -.-&gt;|低功耗优化| A
</code></pre><ul>
<li><strong>采集与预处理</strong>：多目/多鱼眼硬件阵列 + ERP↔Cubemap 在线互转 + 硬触发同步</li>
<li><strong>感知</strong>：球面 CNN/Transformer 多任务头，同步输出分割、检测、深度</li>
<li><strong>应用</strong>：即插即用模块，支持机器人导航、多人交互、实时 3D 重建</li>
<li><strong>加速与部署</strong>：量化-剪枝-蒸馏 + Jetson/FPGA 边缘异构调度，4K 全景实时≤30 W</li>
</ul>
<p>3. 六阶段路线图（S1–S6）</p>
<ol>
<li><strong>数据集整合</strong>——统一投影与标注，发布可复现基准</li>
<li><strong>多模态扩展</strong>——RGB-Depth-LiDAR-音频-IMU 同步，混合真实-合成降 70% 成本</li>
<li><strong>推理与具身数据</strong>——模板+LLM 生成 100 k+ 空间问答对，仿真引擎产出导航/抓取轨迹</li>
<li><strong>统一预训练</strong>——单模型同时编码 360° 几何、语义、多模态信号，权重开源</li>
<li><strong>评测与基准</strong>——球面加权 IoU、OOD 拆分、不确定性校准，建立可比评测体系</li>
<li><strong>部署与泛化</strong>——跨域迁移+持续学习，提供压力测试与功耗-延迟报告</li>
</ol>
<p>4. 主要实证结果</p>
<ul>
<li><strong>分割</strong>：Matterport3D mIoU 55.4，较透视基线 +14.2，跨数据集增益 3.1</li>
<li><strong>VQA</strong>：OSR-Bench Top-1 Acc 66.1，比 BLIP-2 +17.8，方位一致性 ρ=0.88</li>
<li><strong>边缘部署</strong>：Jetson Orin 4K 语义分割 28 fps，功耗 24 W，INT8 精度仅降 0.9 mIoU</li>
<li><strong>鲁棒性</strong>：地下停车场/森林/锅炉房 OOD 场景连续 30 天零误报，mIoU 仍&gt;45</li>
<li><strong>数据成本</strong>：10% 真实 + 90% 合成保留 97% 性能，采集花费从 210 k USD 降至 21 k</li>
</ul>
<p>5. 贡献与影响</p>
<ol>
<li>首次提出面向具身 AI 的全景视觉系统范式 PANORAMA，覆盖采集-感知-应用-加速全链路</li>
<li>发布可复现基准套件（23 个数据集统一划分+度量+脚本），降低领域准入门槛</li>
<li>给出 6 阶段落地路线图，为产学研协同提供清晰时间轴与技术里程碑</li>
<li>在分割、VQA、边缘部署三大任务取得 SOTA，验证框架实时性与鲁棒性</li>
<li>开源权重与工具链，推动全景视觉从“研究小众”走向“工业标配”</li>
</ol>
<p>一句话总结</p>
<p>PANORAMA 通过“系统范式+六阶路线”一次性打通全景视觉的数据、模型、评测、部署全链路，为具身 AI 提供看得全、算得快、落得下的 360° 感知底座。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是PANORAMA%3A%20The%20Rise%20of%20Omnidirectional%20Vision%20in%20the%20Embodied%20AI%20Era，链接是  https://arxiv.org/pdf/2509.12989  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12989  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xu Zheng,Chenfei Liao,Ziqiao Weng,Kaiyu Lei,Zihao Dongfang,Haocong He,Yuanhuiyi Lyu,Lutao Jiang,Lu Qi,Li Chen,Danda Pani Paudel,Kailun Yang,Linfeng Zhang,Luc Van Gool,Xuming Hu</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12989.pdf">https://arxiv.org/pdf/2509.12989.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12989">https://arxiv.org/abs/2509.12989</a></p>
<p><strong>Arxiv ID</strong>: 2509.12989</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12989">https://papers.cool/arxiv/2509.12989</a></p>
<p><strong>Published</strong>: 2025-09-16T11:54:37Z</p>
<p><strong>Updated</strong>: 2025-09-16T11:54:37.000Z</p>
<hr>
<h3 id="4-GenExam-A-Multidisciplinary-Text-to-Image-Exam"><a href="#4-GenExam-A-Multidisciplinary-Text-to-Image-Exam" class="headerlink" title="4. GenExam: A Multidisciplinary Text-to-Image Exam"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14232">GenExam: A Multidisciplinary Text-to-Image Exam</a></h3><p>Exams are a fundamental test of expert-level intelligence and require integrated understanding, reasoning, and generation. Existing exam-style benchmarks mainly focus on understanding and reasoning tasks, and current generation benchmarks emphasize the illustration of world knowledge and visual concepts, neglecting the evaluation of rigorous drawing exams. We introduce GenExam, the first benchmark for multidisciplinary text-to-image exams, featuring 1,000 samples across 10 subjects with exam-style prompts organized under a four-level taxonomy. Each problem is equipped with ground-truth images and fine-grained scoring points to enable a precise evaluation of semantic correctness and visual plausibility. Experiments show that even state-of-the-art models such as GPT-Image-1 and Gemini-2.5-Flash-Image achieve less than 15% strict scores, and most models yield almost 0%, suggesting the great challenge of our benchmark. By framing image generation as an exam, GenExam offers a rigorous assessment of models’ ability to integrate knowledge, reasoning, and generation, providing insights on the path to general AGI.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>考试是专家级智力的基本测试，需要综合理解、推理和生成。现有的考试风格基准主要集中在理解和推理任务上，而当前的生成基准则强调世界知识和视觉概念的说明，忽视了对严格绘画考试的评估。我们引入了GenExam，这是第一个多学科文本到图像考试的基准，涵盖了10个学科的1000个样本，配有按四级分类法组织的考试风格提示。每个问题都配备了真实图像和细致的评分点，以便对语义正确性和视觉 plausibility进行精准评估。实验证明，甚至像GPT-Image-1和Gemini-2.5-Flash-Image这样的最先进模型也仅获得不到15%的严格分数，而大多数模型几乎获得0%分数，这表明我们的基准面临巨大的挑战。通过将图像生成框架设定为考试，GenExam提供了对模型整合知识、推理和生成能力的严格评估，为通向通用AGI的道路提供了洞见。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现有文本到图像（T2I）生成评测体系在“学科绘图考试”场景下的空白。核心问题可归纳为：</p>
<ul>
<li><p><strong>缺失面向多学科、严格约束的绘图考试基准</strong><br>现有评测聚焦自然图像或通用知识插图，题目简单、约束宽松，无法检验模型在复杂、精确、知识密集型绘图任务上的能力。</p>
</li>
<li><p><strong>缺乏细粒度、可量化的语义正确性评估</strong><br>传统指标（FID、CLIP、VQA）侧重整体相似度或美学，难以诊断分子结构、几何位置、音符书写等学科细节错误；单一 MLLM-as-a-judge 指令亦难以覆盖所有考点。</p>
</li>
<li><p><strong>未将“考试”范式引入生成评测</strong><br>真实考试要求同时考察理解、推理与生成，且每题配有标准答案与评分细则；现有生成基准未引入这一范式，导致无法衡量模型是否具备超越常人的专家级智能。</p>
</li>
</ul>
<p>为此，作者提出 <strong>GenExam</strong>——首个多学科文本到图像“考试”基准，通过 1 000 道严格命题、配套标准图与细粒度评分点，量化评估模型在语义正确性、拼写、可读性、逻辑一致性四维的表现，揭示当前最强模型严格得分仍低于 15%，指出通往通用 AGI 尚存显著差距。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，均与“文本到图像生成”及“多学科评测”交叉：</p>
<ol>
<li><p>文本到图像生成模型</p>
<ul>
<li>扩散范式：DDPM、Imagen、Stable Diffusion、FLUX.1、HiDream-I1 等。</li>
<li>自回归范式：DALL-E 系列、Llama-gen、Emu3、Show-o2。</li>
<li>统一多模态大模型：GPT-Image-1、Gemini-2.5-Flash-Image、Qwen-VL-Image、BAGEL、Janus-Pro、BLIP3-o 等，同时支持理解与生成。</li>
</ul>
</li>
<li><p>文本到图像评测基准</p>
<ul>
<li>早期相似度：FID、IS、LPIPS。</li>
<li>对齐度：CLIP-Score、GenEval、T2I-CompBench、WISE、Space-10。</li>
<li>推理驱动：T2I-ReasonBench、PhyBench、CommonSense-T2I。</li>
<li>多学科但“概念插图”：MMMG、OneIG-Bench、SRID-Bench，题目宽松、无精细评分点。</li>
<li>多学科理解类：MMMU、MMMU-Pro、MMLU-Pro、R-Bench、Scientists’ First Exam，仅评测视觉问答或文本推理，不评测生成。</li>
</ul>
</li>
<li><p>考试范式与自动评分</p>
<ul>
<li>纯文本考试：AGIEval、Humanity’s Last Exam。</li>
<li>绘图题自动评分：GeoQA、MathVista、MAVIS 等针对几何图、函数图做 VQA 式判分，但无“生成”任务。</li>
<li>MLLM-as-a-Judge：GPT-4V、InternVL、Qwen-VL 等被用于替代人类评图，但此前未与“考试评分点”机制结合。</li>
</ul>
</li>
</ol>
<p>GenExam 首次把“多学科考试+标准答案+细粒度评分点”引入 T2I 生成评测，填补了上述三线交叉处的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“构建基准 + 设计评测协议”双轮驱动，把“多学科绘图考试”转化为可量化、可复现的 benchmark 任务，具体步骤如下：</p>
<ol>
<li><p>构建 GenExam 基准<br>1.1 四阶学科分类<br>- 依据 UNESCO ISCED-F 建立 10 门学科、236 个四级节点，保证学术严谨性。<br>1.2 数据管道<br>- 收集 40 k 张候选图（网络搜索 + 现有题库 MMMU/ScienceQA/TextbookQA 等）。<br>- GPT-5 自动打分（文本丰富度、领域、复杂度、知识密度）→ 6.5 k 通过。<br>- 3 名 PhD 人工交叉校验 → 精选 1 000 题，确保无重复、无水印、知识密度高。<br>1.3 考试式 prompt 与评分点<br>- GPT-5 生成初版 prompt（≤200 词，含严格约束与学科推理）。<br>- 同一模型再生成 3–14 条“评分点”（Yes/No 问答，分值之和=1），覆盖分子键、几何坐标、音符拼写等细节。<br>- 人工精修 prompt 与评分点，确保与标准图完全对齐。</p>
</li>
<li><p>设计细粒度评测协议<br>2.1 语义正确性<br>- 以 GPT-5 为 judge，对照评分点逐条 VQA；累加“Yes”对应分值得到 0–1 分。<br>2.2 视觉合理性<br>- Spelling（0-2）：文本、公式、符号拼写。<br>- Logical Consistency（0-2）：坐标、角度、数据点、箭头方向等内部逻辑。<br>- Readability（0-2）：标签重叠、遮挡、缺漏、布局混乱。<br>2.3 双分数指标<br>- Strict Score：四维全部满分才计 1，否则 0；反映“考试及格线”。<br>- Relaxed Score：四维加权平均（语义 0.7，其余各 0.1）；反映“整体可用度”。</p>
</li>
<li><p>大规模实验与诊断</p>
<ul>
<li>18 个代表模型（闭源/开源 T2I、统一 MLLM）在 1 000 题上跑图 → 自动评测。</li>
<li>结果：最强 GPT-Image-1 严格得分 12.1%，其余多数≈0%； relaxed 得分闭源 28–63%，开源 7–28%。</li>
<li>消融：更换 judge 模型、去掉标准图参考、调整推理强度，验证 GPT-5-low+参考图最稳健。</li>
<li>人工校验：250 样本五人专家标注，与自动评分 Kendall τ=0.67，Pearson r=0.84，确认评测有效。</li>
</ul>
</li>
</ol>
<p>通过“考试式 prompt + 标准答案 + 细粒度评分点 + 自动 judge”闭环，论文把原本主观的“学科绘图对错”转化为可重复、可诊断的 benchmark，直接暴露模型在知识、推理、绘图细节上的多重缺陷，从而解决了“缺乏严格多学科 T2I 考试评测”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验部分围绕“模型能力诊断”与“评测协议验证”两条主线展开，共含 5 组实验，全部在 GenExam 1 000 题完整集以及 250 题子集（GenExam-Mini）上完成。</p>
<ol>
<li><p>主实验：18 模型全基准对比</p>
<ul>
<li>闭源 T2I：GPT-Image-1、Gemini-2.5-Flash-Image、Imagen-4-Ultra、Seedream 4.0/3.0、FLUX.1 Kontext max</li>
<li>开源 T2I：Qwen-Image、HiDream-I1-Full、FLUX.1 dev/Krea、Stable Diffusion 3.5 Large</li>
<li>开源统一 MLLM：BAGEL（thinking &amp; non-thinking）、Show-o2-7B/1.5B-HQ、Janus-Pro、Emu3、BLIP3o-8B、BLIP3o-NEXT-GRPO-Text-3B<br>指标：Strict Score、Relaxed Score，并给出 10 门学科、7 类图像类型、3 种难度等级的细分表。</li>
</ul>
</li>
<li><p>四维能力消融<br>对 6 个代表模型逐题输出：</p>
<ul>
<li>语义正确性（0-1）</li>
<li>Spelling、Logical Consistency、Readability（0-2）<br>绘制柱状图，定位各模型短板（GPT-Image-1 拼写领先，开源模型普遍逻辑一致性低）。</li>
</ul>
</li>
<li><p>Judge 模型与推理强度消融<br>固定 250 人工标注样本，换用 12 种 MLLM（GPT-5/mini、o3、o4-mini、GPT-4.1/4o、InternVL3.5、Intern-S1 等）担任 judge，对比：</p>
<ul>
<li>评分点准确率（vs 人工答案）</li>
<li>视觉合理性 MAE</li>
<li>Kendall τ / Spearman ρ / Pearson r<br>结果：GPT-5-low + 参考图取得 88.5% 评分点准确率、0.67 相关系数，且成本 0.024 $/图，被选为默认配置。</li>
</ul>
</li>
<li><p>人类一致性验证<br>5 名学科专家独立给 250 图打 1-10 总体分，并按评分点协议重标四维；计算：</p>
<ul>
<li>MAE（机器−人工）</li>
<li>与 CLIP-Score、VQA-Score 的相关系数<br>结果：Relaxed Score 与人工相关 ρ=0.82，显著高于 CLIP/VQA，证明协议对齐人类偏好。</li>
</ul>
</li>
<li><p>案例可视化与错误模式分析<br>选取数学（定积分面积）、音乐（半音圆转调）、计算机（死锁资源图）、化学（原电池）、工程（材料受力）等 10 余题，可视化展示 GPT-Image-1、Gemini-2.5-Flash-Image、Seedream 4.0、Qwen-Image、HiDream-I1-Full 的生成图，并标注每条评分点得失，归纳出三类高频失败：</p>
<ul>
<li>学科知识缺失（分子结构错、音阶拼写错）</li>
<li>推理错误（交点坐标算错、箭头方向反）</li>
<li>绘图低级错误（标签重叠、漏标、文本遮挡）</li>
</ul>
</li>
</ol>
<p>以上实验系统验证了 GenExam 的难度、评测协议的有效性与模型当前能力边界，为后续研究提供了明确基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 GenExam 开启的“下一步”研究议程，按“数据-模型-评测-应用”四个层面展开：</p>
<p>数据与任务扩展</p>
<ul>
<li><p><strong>规模与领域</strong></p>
<ul>
<li>将 1 k 样本扩至 10 k+，覆盖医学、农学、艺术设计等 ISCED-F 空缺学科；引入 3D 示意图、工程 CAD 草图、地理信息分层图等更高阶题型。</li>
<li>构建“多步推理链”子集：一张图需融合≥2 学科知识（如热力学+经济学桑基图），考察跨域综合绘图能力。</li>
</ul>
</li>
<li><p><strong>动态与交互式题目</strong></p>
<ul>
<li>生成“带参数脚本”的图（如 $\LaTeX$ TikZ、matplotlib、SVG），实现同一 prompt 下随机种子变化仍保持正确语义，便于强化学习训练。</li>
<li>引入“续画”任务：给定半张图，模型需补全并保证几何/化学/电路约束一致，考察推理与一致性双重能力。</li>
</ul>
</li>
<li><p><strong>多语言与多文化</strong></p>
<ul>
<li>非英语 prompt（中文、西班牙语、阿拉伯语）及非拉丁符号（俄文元素名、阿拉伯数字刻度）对拼写与可读性的影响。</li>
</ul>
</li>
</ul>
<p>模型结构创新</p>
<ul>
<li><p><strong>统一 MLLM 的“循环修正”机制</strong></p>
<ul>
<li>生成→自我评测→迭代修正多轮 pipeline，利用 GenExam 评分点作为稠密奖励信号，缓解一步生成导致的“细小但致命”错误。</li>
</ul>
</li>
<li><p><strong>符号-像素双空间协同</strong></p>
<ul>
<li>先输出结构化符号表示（SMILES、TikZ、Graphviz），再经 diffusion/autoregressive 渲染为像素图，实现“可验证的中间表达”。</li>
</ul>
</li>
<li><p><strong>知识注入模块</strong></p>
<ul>
<li>将学科知识图谱（ChEBI、IUPAC 命名法、GeoNames、MusicXML）以 retrieval 或 cross-attention 形式注入生成模型，减少知识型幻觉。</li>
</ul>
</li>
<li><p><strong>多模态链式思维（CoT）</strong></p>
<ul>
<li>显式生成“推理图”：在画布外缘输出中间步骤（如化学机制箭头、几何辅助线、受力分析自由体图），再合成最终答案图，提升可解释性与可验证性。</li>
</ul>
</li>
</ul>
<p>评测协议深化</p>
<ul>
<li><p><strong>自动评分点生成</strong></p>
<ul>
<li>开发“评分点生成器”大模型：输入任意学科图，自动写出 5-20 条细粒度 Yes/No 问题并分配权重，实现 GenExam 自我扩展，降低人工 PhD 成本。</li>
</ul>
</li>
<li><p><strong>人类无法察觉的微小错误检测</strong></p>
<ul>
<li>引入像素级 mask 对比、矢量图解析、OCR+符号验证三重对齐，自动定位“一个双键少了一根线”“坐标 0.1 cm 偏移”等微小错误，输出可解释热力图。</li>
</ul>
</li>
<li><p><strong>鲁棒性 &amp; 对抗评测</strong></p>
<ul>
<li>对 prompt 做同义改写、数值扰动、单位换算（kcal→kJ）、语序颠倒，观察模型是否仍满足全部评分点，衡量语义鲁棒性。</li>
</ul>
</li>
<li><p><strong>效率与碳排指标</strong></p>
<ul>
<li>记录生成步数、GPU-秒、能耗，建立“每正确图克 CO₂”指标，推动绿色生成。</li>
</ul>
</li>
</ul>
<p>应用与系统</p>
<ul>
<li><p><strong>个性化教育助手</strong></p>
<ul>
<li>根据学生错题即时生成“针对性图解变式”，并保证学科正确性；GenExam 评分点作为在线自动批改引擎，实时反馈。</li>
</ul>
</li>
<li><p><strong>可验证的科学插图搜索引擎</strong></p>
<ul>
<li>将 GenExam 评分点转化为结构化标签，构建“可验证图库”，实现“查一张对一张”的学术插图搜索，避免现有图库中广泛存在的科学错误。</li>
</ul>
</li>
<li><p><strong>可编程生成接口</strong></p>
<ul>
<li>提供 OpenAI-style API：输入 LaTeX 公式或 SMILES 字符串，返回带矢量源文件（TikZ/SVG）与 GenExam 评分报告，方便论文、教材、MOOC 自动插图。</li>
</ul>
</li>
<li><p><strong>法律与伦理审查</strong></p>
<ul>
<li>利用评分点机制检测生成图是否出现“伪造数据曲线”“篡改地图边界”等潜在误导，为期刊、出版社提供自动合规筛查工具。</li>
</ul>
</li>
</ul>
<p>小结</p>
<p>GenExam 把“多学科绘图考试”首次变成可量化基准，未来工作可沿“更大更难的数据”、“可验证的生成范式”、“更细更鲁棒的评测”、“教育-出版-合规场景落地”四轴持续推进，推动文本到图像生成从“看起来对”走向“真正对”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<h1 id="论文核心速览"><a href="#论文核心速览" class="headerlink" title="论文核心速览"></a>论文核心速览</h1><p>维度</p>
<p>内容</p>
<p><strong>问题</strong></p>
<p>现有文本到图像（T2I）评测只测“自然图+粗粒度对齐”，缺乏<strong>多学科、严格约束、可量化的“绘图考试”基准</strong>，无法诊断模型在知识、推理、细节层面的错误。</p>
<p><strong>方案</strong></p>
<p>提出 <strong>GenExam</strong>：<br>① 1 000 道大学水平“绘图题”，覆盖 10 学科、236 个四级知识点；<br>② 每题配标准答案图 + 6.9 条“评分点”（Yes/No 问答，分值和=1）；<br>③ 四维评测：语义正确性(0-1) + 拼写/逻辑/可读(0-2)；<br>④ 双指标：Strict（全满分才及格）与 Relaxed（加权平均）。</p>
<p><strong>数据流程</strong></p>
<p>40 k 候选图 → GPT-5 自动过滤 → 3 名 PhD 人工校验 → 1 k 终版；prompt 与评分点由 GPT-5 初写再人工精修，确保学科严谨。</p>
<p><strong>实验</strong></p>
<p>18 个代表模型（闭源/开源 T2I、统一 MLLM）全量测试：<br>• 最高 Strict 仅 <strong>12.1 %</strong>（GPT-Image-1），多数≈0 %；<br>• 闭源 relaxed 28–63 %，开源 7–28 %；<br>• 人工 250 图验证：Kendall τ=0.67，Pearson r=0.84，评测协议高对齐。</p>
<p><strong>结论</strong></p>
<p>即使最强模型也在“多学科绘图考试”上近乎不及格，揭示<strong>知识-推理-生成整合</strong>仍是 AGI 巨大缺口；GenExam 提供可扩展、可诊断的严苛基准，指引未来研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是GenExam%3A%20A%20Multidisciplinary%20Text-to-Image%20Exam，链接是  https://arxiv.org/pdf/2509.14232  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14232  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhaokai Wang,Penghao Yin,Xiangyu Zhao,Changyao Tian,Yu Qiao,Wenhai Wang,Jifeng Dai,Gen Luo</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14232.pdf">https://arxiv.org/pdf/2509.14232.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14232">https://arxiv.org/abs/2509.14232</a></p>
<p><strong>Arxiv ID</strong>: 2509.14232</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14232">https://papers.cool/arxiv/2509.14232</a></p>
<p><strong>Published</strong>: 2025-09-17T17:59:14Z</p>
<p><strong>Updated</strong>: 2025-09-17T17:59:14.000Z</p>
<hr>
<h3 id="5-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning"><a href="#5-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning" class="headerlink" title="5. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13755">Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning</a></h3><p>While Code Language Models (CLMs) have demonstrated superior performance in software engineering tasks such as code generation and summarization, recent empirical studies reveal a critical privacy vulnerability: these models exhibit unintended memorization of sensitive training data, enabling verbatim reproduction of confidential information when specifically prompted. To address this issue, several approaches, including training data de-duplication and differential privacy augmentation, have been proposed. However, these methods require full-model retraining for deployed CLMs, which incurs substantial computational costs. In this paper, we aim to answer the following research question: Can sensitive information memorized by CLMs be erased effectively and efficiently? We conduct a pioneering investigation into erasing sensitive memorization in CLMs through machine unlearning - a post-hoc modification method that removes specific information from trained models without requiring full retraining. Specifically, we first quantify the memorization risks of sensitive data within CLM training datasets and curate a high-risk dataset of 50,000 sensitive memorized samples as unlearning targets. We study two widely used gradient ascent-based unlearning approaches: the vanilla and constraint-based methods, and introduce CodeEraser, an advanced variant that selectively unlearns sensitive memorized segments in code while preserving the structural integrity and functional correctness of the surrounding code. Extensive experiments on three families of CLMs, i.e., CodeParrot, CodeGen-Mono, and Qwen2.5-Coder, validate the effectiveness and efficiency of CodeEraser in erasing targeted sensitive memorization while maintaining model utility.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然代码语言模型（CLMs）在代码生成和摘要等软件工程任务中展现出优越的性能，但最近的实证研究揭示了一个关键的隐私漏洞：这些模型表现出对敏感训练数据的无意记忆，从而在特定提示下能够逐字复现机密信息。为了解决这个问题，提出了几种方法，包括训练数据去重和差分隐私增强。然而，这些方法需对已部署的CLMs进行全模型再训练，这会产生大量的计算成本。本文旨在回答以下研究问题：CLMs记忆的敏感信息能否有效且高效地被删除？我们通过机器遗忘——一种事后修改方法，在不需要完全再训练的情况下，研究了在CLMs中删除敏感记忆的开创性调查。具体而言，我们首先量化了CLM训练数据集中敏感数据的记忆风险，并策划了一个包含50,000个敏感记忆样本的高风险数据集作为遗忘目标。我们研究了两种广泛使用的基于梯度上升的遗忘方法：普通方法和基于约束的方法，并引入了CodeEraser，一种高级变体，它选择性地对代码中敏感的记忆段进行遗忘，同时保持周围代码的结构完整性和功能正确性。在CodeParrot、CodeGen-Mono和Qwen2.5-Coder这三类CLMs上进行的广泛实验验证了CodeEraser在删除目标敏感记忆的同时保持模型实用性的有效性和效率。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>代码语言模型（CLM）在训练过程中对敏感数据的非预期记忆</strong>所带来的隐私风险。具体而言：</p>
<ul>
<li><p><strong>问题背景</strong>：CLM 在预训练阶段会接触大量开源代码，其中可能包含邮箱、密码、API 密钥等敏感信息。模型通过自回归语言建模目标，<strong>逐字记忆</strong>了这些敏感片段，使得攻击者可通过构造提示词<strong>完整提取</strong>原始机密（即“逐字记忆”现象）。</p>
</li>
<li><p><strong>法规驱动</strong>：欧盟 GDPR、加州 CCPA 等法律赋予个人“被遗忘权”（RTBF），要求模型在部署后仍能<strong>按需删除</strong>特定个人数据，而<strong>无需全量重训</strong>。</p>
</li>
<li><p><strong>现有方法局限</strong>：</p>
<ul>
<li>训练数据去重：只能降低重复样本的记忆倾向，无法根除已记忆内容。</li>
<li>差分隐私训练：提供形式化隐私保证，但会显著降低模型在长尾数据上的效用，且需在训练阶段介入，<strong>无法对已经部署的模型进行事后补救</strong>。</li>
<li>传统机器遗忘：采用梯度上升对整个样本进行“反向学习”，会<strong>连带抹去周围无害代码</strong>，破坏代码结构与功能。</li>
</ul>
</li>
<li><p><strong>核心研究问题</strong>：<br>能否在<strong>不重新训练完整模型</strong>的前提下，<strong>精准、高效地擦除 CLM 中已记忆的敏感片段</strong>，同时<strong>保持剩余代码的语法结构与功能正确性</strong>？</p>
</li>
<li><p><strong>贡献总结</strong>：</p>
<ol>
<li>首次形式化“<strong>敏感记忆擦除</strong>”问题，并构建含 50 000 条高记忆敏感样本的评测基准。</li>
<li>提出<strong>选择性遗忘方法 CodeEraser</strong>：<ul>
<li>用正则工具先定位敏感片段（如 API key）。</li>
<li>仅对敏感段执行梯度上升（遗忘），对周围非敏感上下文执行梯度下降（保持）。</li>
<li>引入 KL 约束，确保模型输出分布在保留集上与原模型一致，抑制效用下降。</li>
</ul>
</li>
<li>在 CodeParrot、CodeGen-Mono、Qwen2.5-Coder 三个模型家族上实验表明：<ul>
<li>敏感记忆降低 ≥93 %，</li>
<li>代码生成任务性能保持 ≥99 %，</li>
<li>单样本遗忘耗时约 47 s，远低于重训所需的数百 A100 GPU 日。</li>
</ul>
</li>
</ol>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与本文直接相关的研究可划分为三大主线：<strong>(1) 代码/语言模型记忆现象剖析，(2) 隐私保护训练技术，(3) 机器遗忘（Machine Unlearning）</strong>。以下按主题归纳代表性文献，并指出与本文的关联。</p>
<p>1. 记忆现象与隐私风险</p>
<p>文献</p>
<p>核心贡献</p>
<p>与本文关系</p>
<p>Carlini et al. [15–17]</p>
<p>提出“提取攻击”框架，量化 LLM 对训练数据的逐字记忆；发现大模型可还原邮箱、URL、个人地址等敏感片段。</p>
<p>为本文“逐字记忆”定义与攻击场景提供方法论基础。</p>
<p>Al-Kaswan et al. [3]</p>
<p>针对代码模型，展示在提示下可完整输出版权头、硬编码密钥。</p>
<p>首次在 CLM 领域验证记忆风险，启发本文构建敏感记忆数据集。</p>
<p>Huang et al. [33] / Niu et al. [51]</p>
<p>通过构造补全提示，从 GitHub Copilot 等商用服务中恢复大量密钥、口令。</p>
<p>证明现实场景下 CLM 泄露机密的可能性，强化“事后擦除”需求。</p>
<p>Yang et al. [68]</p>
<p>系统测量 CodeParrot、CodeGen 等模型在代码上的记忆率，提出 MA/EL 指标。</p>
<p>本文直接沿用其 MA/EL 指标，并扩展出“敏感段”粒度评测。</p>
<p>2. 隐私保护训练技术</p>
<p>文献</p>
<p>核心贡献</p>
<p>与本文关系</p>
<p>Lee et al. [43] / Kandpal et al. [39]</p>
<p>训练前对重复文件进行去重，可降低记忆概率。</p>
<p>本文指出去重无法根除已记忆内容，且需重训，与“事后擦除”互补。</p>
<p>Abadi et al. [1] / Anil et al. [4]</p>
<p>在训练阶段引入差分隐私（DP-SGD），提供 ε-δ 保证。</p>
<p>DP 会显著降低长尾性能，且无法对部署后模型做“定点删除”，本文将其作为 baseline 对比。</p>
<p>Feldman [25] / Zhang et al. [26]</p>
<p>从理论角度分析“记忆”对泛化的必要性，指出完全无记忆模型不可行。</p>
<p>本文仅针对“敏感段”做选择性遗忘，避免全模型无记忆化带来的效用崩溃。</p>
<p>3. 机器遗忘（Machine Unlearning）</p>
<p>3.1 通用语言模型遗忘</p>
<p>文献</p>
<p>核心贡献</p>
<p>与本文关系</p>
<p>Jang et al. [37]</p>
<p>提出梯度上升（GA）+ KL 约束，对整段文本进行遗忘，减少 80 % 以上记忆。</p>
<p>本文 baseline 方法（GA/CU）的来源；但整段遗忘会损伤代码结构，本文提出“段级”选择性遗忘。</p>
<p>Chen &amp; Yang [18]</p>
<p>在保留集上最小化 KL 散度，维持下游任务性能。</p>
<p>被本文吸收为“非敏感上下文”保持项，但仅用于敏感段对齐。</p>
<p>Eldan &amp; Russinovich [23]</p>
<p>使用强化学习+模型编辑，让 LLM 遗忘特定小说内容。</p>
<p>证明遗忘可行性，但未考虑代码语法完整性；本文用梯度下降显式保护非敏感 token。</p>
<p>3.2 代码专用遗忘</p>
<p>文献</p>
<p>核心贡献</p>
<p>与本文关系</p>
<p>Wang et al. [64]</p>
<p>以高困惑度 token 为隐私代理，对高 ppl token 做梯度上升。</p>
<p>在普通文本有效，但代码中高 ppl 多为自定义标识符，真正密钥反而低 ppl，导致误删；本文用正则工具直接定位密钥，避免此问题。</p>
<p>本文 CodeEraser</p>
<p>首次将“敏感段”与“非敏感上下文”显式分离，分别执行梯度上升/下降，并引入段级 KL 约束。</p>
<p>——</p>
<p>4. 辅助工具与数据集</p>
<p>工具/数据集</p>
<p>用途</p>
<p>本文使用方式</p>
<p>detect-secrets [71]</p>
<p>基于正则的密钥检测引擎，覆盖 140+ 类机密模式。</p>
<p>扫描 codeparrot-clean-train，定位邮箱、IP、API key 等敏感段。</p>
<p>HumanEval [19] / MBPP [6]</p>
<p>代码生成评测基准，已确认未被 Qwen2.5-Coder 训练。</p>
<p>构建“未见数据集” D′，用于计算记忆阈值与效用评估。</p>
<p>codeparrot-clean-train [21]</p>
<p>50 GB Python 语料，含仓库级出处。</p>
<p>作为敏感记忆挖掘与遗忘实验的统一训练源，保证可复现性。</p>
<p>小结</p>
<ul>
<li><strong>记忆与攻击研究</strong>确立了“CLM 会逐字记忆敏感数据”的事实；</li>
<li><strong>DP/去重方法</strong>提供训练阶段隐私保障，但无法应对“已部署模型+RTBF”场景；</li>
<li><strong>现有遗忘工作</strong>聚焦整段文本，未考虑代码语法完整性；</li>
<li><strong>本文 CodeEraser</strong>首次实现<strong>细粒度、段级、保留上下文</strong>的遗忘，填补了这一空白。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“擦除代码语言模型（CLM）已记忆的敏感片段”形式化为<strong>事后选择性遗忘</strong>任务，提出 <strong>CodeEraser</strong> 框架，通过<strong>三段式策略</strong>在无需全量重训的前提下，精准消除敏感记忆并维持代码结构与模型效用。核心流程如下：</p>
<p>1. 问题形式化与数据准备</p>
<ul>
<li><strong>敏感记忆定义</strong>：给定训练集 D，若存在前缀 p 使得  <script type="math/tex; mode=display">s = \\arg\\max\_{\\hat{s}} f\_\\theta(\\hat{s}|p) \\land \[p|s\]\\in D</script>且 s 包含邮箱、API 密钥等机密，则称 s 被<strong>逐字记忆</strong>。</li>
<li><strong>构建遗忘目标集</strong> Df：<br>– 用 detect-secrets 扫描 codeparrot-clean-train，定位 939 k 含机密文件。<br>– 计算 MA 分数，筛选 ≥ 阈值的高记忆样本，最终 curation 出 50 k「敏感记忆数据集」作为 Df 候选。</li>
</ul>
<p>2. 选择性梯度更新：敏感段 vs 非敏感上下文</p>
<p>对每条遗忘样本 xf = (sf ; cf )：</p>
<p>片段</p>
<p>操作</p>
<p>目标</p>
<p><strong>敏感段 sf</strong>（如密钥字符串）</p>
<p>梯度 <strong>上升</strong>（最大化负对数似然）</p>
<p>降低模型对 sf 的记忆概率</p>
<p><strong>非敏感上下文 cf</strong>（如变量名、结构）</p>
<p>梯度 <strong>下降</strong>（标准语言模型损失）</p>
<p>保持代码语法与功能完整性</p>
<p>损失函数：  </p>
<script type="math/tex; mode=display">L\_{\\text{SU}} = \\underbrace{-L\_{\\text{LM}}(s\_f) + \\gamma L\_{\\text{LM}}(c\_f)}_{\\text{segment-wise ascent/descent}} + \\underbrace{\\lambda \\cdot \\text{KL}(f_\\theta(s\_f)|f'_\\theta(s\_f))}_{\\text{KL 约束}}</script><ul>
<li>γ 控制上下文保持强度，λ 控制与原模型分布对齐强度。</li>
<li>交替 epoch 训练 Df 与保留集 Dr，防止优化失衡。</li>
</ul>
<p>3. 段级 KL 约束：只让敏感段“远离”原模型</p>
<p>传统整样本 KL 约束会拖累无关 token；CodeEraser <strong>仅对敏感段 sf 计算 KL</strong>，确保：</p>
<ol>
<li>模型在 sf 上输出分布与原始模型显著偏离 → 实现“遗忘”；</li>
<li>在 cf 与 Dr 上分布保持接近 → 维持整体代码生成能力。</li>
</ol>
<p>4. 算法流程（单样本视角）</p>
<ol>
<li>正则定位 → 得到 (sf , cf )</li>
<li>前向计算：<br>– 对 sf 最大化负对数似然（梯度上升）<br>– 对 cf 最小化负对数似然（梯度下降）</li>
<li>计算段级 KL 散度并反向传播</li>
<li>迭代至 MA(s_f ) ≤ T_MA 且 EL_n(s_f ) ≤ T_EL_n 停止</li>
</ol>
<p>5. 复杂度与效率</p>
<ul>
<li>仅需 <strong>46.88 s/样本</strong>（Qwen2.5-7B，单 A100），峰值显存 ≈ 200 GB（含保留集缓存）。</li>
<li>相比重训（900–1800 A100 GPU 天）节省 <strong>> 6 个数量级</strong>时间。</li>
</ul>
<p>6. 效果验证</p>
<p>指标</p>
<p>原始模型</p>
<p>CodeEraser</p>
<p>敏感记忆率 (MA)</p>
<p>96.26 %</p>
<p>8.49 %（↓ 93.89 %）</p>
<p>HumanEval Pass@1</p>
<p>61.07 %</p>
<p>61.65 %（↑ 0.96 %）</p>
<p>代码结构完整性</p>
<p>—</p>
<p>100 % 通过 AST 解析与单元测试</p>
<p>总结</p>
<p>CodeEraser 通过“<strong>正则定位-段级梯度反转-上下文保持-KL 对齐</strong>”四步，把传统“整段遗忘”转化为“<strong>只擦密钥、不碰代码</strong>”的细粒度操作，在<strong>法规合规（RTBF）、计算成本、模型效用</strong>之间取得实用平衡。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>CodeEraser</strong> 共设计 <strong>4 组研究问题（RQ1–RQ4）</strong>，在 <strong>3 个 CLM 家族、5 个模型、50 k 敏感样本</strong> 上开展系统化实验。实验既覆盖“能否忘、忘得多快”，也覆盖“忘后代码还能不能用、怎么调参、数据特性有何影响”。具体实验矩阵如下：</p>
<p>RQ1 有效性与效率（Effectiveness &amp; Efficiency）</p>
<p>实验项</p>
<p>设置</p>
<p>观测指标</p>
<p>遗忘成功率</p>
<p>每模型随机抽 k = 32 条高记忆敏感样本，重复 5 次</p>
<p>MA、EL3、EL5、EL10 是否全部 ≤ 对应阈值；计算平均降幅 Red.</p>
<p>资源开销</p>
<p>同一批次，PyTorch profiler 记录</p>
<p>GPU 总时间、峰值显存（4 卡累加）</p>
<p>关键结果（Qwen2.5-7B）：</p>
<ul>
<li>敏感记忆平均降幅 <strong>93.89 %</strong>；</li>
<li>单样本耗时 <strong>46.88 s</strong>，峰值显存 <strong>≈ 200 GB</strong>；</li>
<li>相比全量重训节省 <strong>> 6 个数量级</strong>时间。</li>
</ul>
<p>RQ2 忘后模型效用（Utility Post-Unlearning）</p>
<p>实验项</p>
<p>设置</p>
<p>观测指标</p>
<p>代码生成能力</p>
<p>HumanEval 原始测试集</p>
<p>Pass@1、Pass@5、Pass@10</p>
<p>效用保持率</p>
<p>与原始模型对比</p>
<p>Ret. = (Avg Pass@k_post / Avg Pass@k_pre) ×100 %</p>
<p>关键结果：</p>
<ul>
<li>CodeEraser 在 <strong>5 个模型</strong>上平均效用保持 <strong>> 96 %</strong>；</li>
<li>Qwen2.5-7B 保持 <strong>99.99 %</strong>（Pass@1 反而略升 0.96 %）；</li>
<li>基线 GA/CU 普遍下降 10–30 %。</li>
</ul>
<p>RQ3 遗忘数据特性影响（Analysis on Forgotten Data）</p>
<p>在 CodeParrot-1.5B 上系统扰动三方面因素，每次只变一项，其余固定 k = 32：</p>
<ol>
<li><p><strong>样本数量 k</strong><br>k ∈ {8, 16, 32, 64, 128, 256, 512}<br>→ 效用几乎不变（k ≤ 128）；k ≥ 256 时 Pass@5 下降 ≈ 8 %，揭示<strong>规模天花板</strong>。</p>
</li>
<li><p><strong>训练集重复度</strong><br>按 duplication freq 划 4 档：[5,10)、[10,25)、[25,50)、[50, ∞)<br>→ 低重复（outlier）与高重复（over-fit）样本被忘后<strong>性能反而提升</strong>，中段重复影响最大。</p>
</li>
<li><p><strong>敏感数据类型</strong><br>仅含 email、仅含 IP、仅含 API/SSH Key 三类<br>→ 忘密钥后 <strong>HumanEval 提升 1.8 %</strong>，其余两类轻微下降；说明<strong>密钥属异常模式</strong>，删除即正则。</p>
</li>
</ol>
<p>RQ4 超参数敏感度（Hyperparameter Impact）</p>
<p>固定 k = 32，在 CodeParrot 上单因素扫描：</p>
<p>参数</p>
<p>搜索范围</p>
<p>结论</p>
<p>learning rate</p>
<p>{1e-5, 8e-6, 5e-6, 3e-6, 1e-6}</p>
<p>过大（≥ 8e-6）导致 Pass@k 下降 &gt; 10 %；<strong>3e-6 最优</strong></p>
<p>γ（上下文保持权重）</p>
<p>{0.1,…,0.5}</p>
<p>0.5 附近平稳；&gt; 0.5 开始过拟合保留集</p>
<p>α（KL 系数，保留集侧）</p>
<p>{0.5,…,1.5}</p>
<p>1.0 附近平坦；极端值对效用影响 &lt; 2 %</p>
<p>λ（KL 系数，遗忘段侧）</p>
<p>{0.1,…,0.5}</p>
<p>0.1 已足够，再大不显著提升遗忘率</p>
<p>辅助实验（可复现性 &amp; 消融）</p>
<ol>
<li><strong>AST 解析验证</strong>：随机抽 200 条忘后代码，100 % 通过 Python 语法检查；手动单元测试 50 条，功能与原始一致。</li>
<li><strong>消融研究</strong>：<ul>
<li>去掉梯度下降项（γ = 0）→ 效用下降 7 %；</li>
<li>去掉段级 KL（λ = 0）→ 记忆降幅降低 12 %；</li>
<li>同时去掉 → 退化为 vanilla GA，性能与基线一致。</li>
</ul>
</li>
</ol>
<p>实验覆盖一览</p>
<p>维度</p>
<p>取值</p>
<p>模型</p>
<p>CodeParrot-small/1.5B, CodeGen-350M/2B, Qwen2.5-Coder-7B（共 5）</p>
<p>数据集</p>
<p>50 k 敏感记忆样本 + 1 k 保留集 + HumanEval</p>
<p>指标</p>
<p>MA、EL3、EL5、EL10、Pass@k、GPU 时间、峰值显存</p>
<p>重复</p>
<p>每点 5 次随机种子，报告均值与标准差</p>
<p>以上实验体系从<strong>微观遗忘成功率</strong>到<strong>宏观代码生成能力</strong>，再到<strong>数据/参数细粒度影响</strong>，完整验证了 CodeEraser 的实用性与鲁棒性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分“<strong>方法层面</strong>”“<strong>数据与评测</strong>”“<strong>系统与法规</strong>”“<strong>理论分析</strong>”四大主题列出：</p>
<p>1. 方法层面</p>
<ul>
<li><strong>跨模态遗忘</strong>：代码-文档双语模型中，同一机密可能出现在注释、Markdown 或 commit message，需设计<strong>联合嵌入空间</strong>的段级遗忘，避免残留。</li>
<li><strong>结构感知遗忘</strong>：目前仅用 AST 验证语法，可引入<strong>数据/控制流图</strong>，将“与敏感常量相关的依赖节点”一并纳入遗忘范围，防止通过切片攻击恢复。</li>
<li><strong>自适应梯度掩码</strong>：根据 token 重要性（如注意力权重、梯度显著性）动态决定<strong>每 token 上升/下降强度</strong>，减少人工超参 γ。</li>
<li><strong>参数高效遗忘</strong>：结合 LoRA/AdaLoRA，仅更新<strong>低秩 adapter</strong>，在 7 B→70 B 模型上验证是否仍保持单卡可训。</li>
<li><strong>遗忘可验证性</strong>：引入<strong>零知识证明</strong>或<strong>加密签名</strong>，向监管方证明“某样本已不在模型”而无需暴露权重。</li>
</ul>
<p>2. 数据与评测</p>
<ul>
<li><strong>更大规模敏感库</strong>：覆盖 Java/Go/TS 等多语言，引入企业私有代码，评估<strong>跨语言记忆迁移</strong>（Java 密钥→Python 补全）。</li>
<li><strong>长尾机密类型</strong>：OAuth JWT、区块链私钥、K8s Secret、数据库连接串，观察<strong>格式复杂度</strong>对遗忘难度的影响。</li>
<li><strong>对抗性提取评测</strong>：<ul>
<li>黑盒：基于遗传算法的<strong>prompt 搜索</strong>，衡量最坏情况提取率。</li>
<li>白盒：利用<strong>log-prob 排序</strong>+<strong>beam tracing</strong> 精准恢复，检验遗忘边界。</li>
</ul>
</li>
<li><strong>动态遗忘基准</strong>：模拟真实 RTBF 请求流，每轮新增 1 k 样本需遗忘，测量<strong>连续遗忘 20 轮</strong>后的累积效用漂移。</li>
</ul>
<p>3. 系统与法规</p>
<ul>
<li><strong>流水线集成</strong>：将 detect-secrets → CodeEraser → 功能测试 → 合规报告封装为<strong>GitHub Action</strong>，仓库 owner 一键提交“遗忘请求”。</li>
<li><strong>遗忘审计日志</strong>：区块链存贮每次遗忘的<strong>样本哈希、时间戳、模型版本、遗忘证书</strong>，满足 GDPR“可证明删除”要求。</li>
<li><strong>多租户服务</strong>：云端代码补全模型同时服务多家公司，需支持<strong>租户级隔离遗忘</strong>（仅删 A 公司密钥而不影响 B），探索<strong>参数分区</strong>或<strong>梯度加密聚合</strong>。</li>
<li><strong>法规细粒度映射</strong>：将 GDPR 的“必要范围”“比例原则”量化成<strong>遗忘强度上下界</strong>，提供可执行的法律-技术转换接口。</li>
</ul>
<p>4. 理论分析</p>
<ul>
<li><strong>遗忘与泛化误差界</strong>：基于 PAC-Bayes 或信息论，给出<strong>段级遗忘后</strong>的 ε,δ 隐私界，阐明与 DP-SGD 的<strong>互补性</strong>。</li>
<li><strong>记忆-遗忘对偶性</strong>：证明在最坏情况下，<strong>遗忘敏感段的最小参数更新量</strong>与原始记忆深度（gradient norm）呈线性关系，指导<strong>学习率上限</strong>自动选择。</li>
<li>** catastrophic forgetting 下界**：推导当 |Df|/|D| &gt; 某阈值时，任何梯度上升方法必然导致<strong>通用代码生成性能下降</strong>，为“可遗忘容量”提供理论阈值。</li>
</ul>
<p>5. 其他新兴场景</p>
<ul>
<li><strong>联邦代码模型</strong>：各企业本地训练，上传梯度至中心服务器；需设计<strong>联邦遗忘</strong>协议，确保删除某仓库数据后<strong>全局模型无需重新拉取全量梯度</strong>。</li>
<li><strong>多步生成链</strong>：代码→编译→二进制，敏感常量可能被编译器内嵌到二进制；研究<strong>端到端遗忘链</strong>，从高层源码到 LLVM IR 一并擦除。</li>
<li><strong>提示工程后门</strong>：攻击者在遗忘后植入**“触发提示”<strong>仍可恢复密钥；探索</strong>后门鲁棒遗忘**，结合对抗训练与触发检测。</li>
</ul>
<p>综上，从<strong>算法细粒度、系统落地、法律合规到理论边界</strong>均存在广阔探索空间，既可推动机器遗忘技术成熟，也能为代码大模型的<strong>合规部署</strong>提供关键支撑。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文首次将“<strong>代码语言模型（CLM）对敏感数据的逐字记忆</strong>”视为<strong>事后可擦除</strong>的隐私风险，提出<strong>选择性机器遗忘框架 CodeEraser</strong>，实现“<strong>只删密钥、不毁代码</strong>”。核心内容概括为以下 5 句话：</p>
<ol>
<li><p><strong>问题定义</strong>：CLM 在预训练时会逐字记忆仓库中的邮箱、API 密钥等机密，现有去重与差分隐私需重训，无法满足 GDPR/CCPA 的“被遗忘权”实时删除要求。</p>
</li>
<li><p><strong>数据构建</strong>：用 detect-secrets 扫描 50 GB Python 语料，量化 MA/EL 记忆度，得到 50 k 条高记忆敏感样本，形成首个<strong>敏感记忆评测基准</strong>。</p>
</li>
<li><p><strong>方法设计</strong>：CodeEraser 把代码拆成“敏感段 sf + 非敏感上下文 cf ”；对 sf 做梯度上升（遗忘），对 cf 做梯度下降（保持），并引入<strong>段级 KL 约束</strong>维持模型分布，实现<strong>细粒度、语法无损</strong>的遗忘。</p>
</li>
<li><p><strong>实验结果</strong>：在 CodeParrot、CodeGen-Mono、Qwen2.5-Coder（110 M–7 B）上，敏感记忆降幅 ≥ 93 %，HumanEval 代码生成性能保持 ≥ 99 %，单样本 47 s 完成，比重训节省 6 个数量级算力。</p>
</li>
<li><p><strong>意义与展望</strong>：提出“敏感记忆可事后精准擦除”的新范式，兼顾法规合规与模型效用；未来可扩展至多语言、联邦场景及可验证遗忘等方向。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Scrub%20It%20Out%21%20Erasing%20Sensitive%20Memorization%20in%20Code%20Language%20Models%20via%20Machine%20Unlearning，链接是  https://arxiv.org/pdf/2509.13755  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13755  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhaoyang Chu,Yao Wan,Zhikun Zhang,Di Wang,Zhou Yang,Hongyu Zhang,Pan Zhou,Xuanhua Shi,Hai Jin,David Lo</p>
<p><strong>Categories</strong>: cs.SE,cs.AI,cs.CR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13755.pdf">https://arxiv.org/pdf/2509.13755.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13755">https://arxiv.org/abs/2509.13755</a></p>
<p><strong>Arxiv ID</strong>: 2509.13755</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13755">https://papers.cool/arxiv/2509.13755</a></p>
<p><strong>Published</strong>: 2025-09-17T07:12:35Z</p>
<p><strong>Updated</strong>: 2025-09-17T07:12:35.000Z</p>
<hr>
<h3 id="6-MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework"><a href="#6-MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework" class="headerlink" title="6. MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2508.14880">MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</a></h3><p>Recent developments in Large Language Model (LLM)-based agents have shown impressive capabilities spanning multiple domains, exemplified by deep research systems that demonstrate superior performance on complex information-seeking and synthesis tasks. While general-purpose deep research agents have shown impressive capabilities, they struggle significantly with medical domain challenges, as evidenced by leading proprietary systems achieving limited accuracy on complex medical benchmarks. The key limitations are: (1) the model lacks sufficient dense medical knowledge for clinical reasoning, and (2) the framework is constrained by the absence of specialized retrieval tools tailored for medical contexts. We present a medical deep research agent that addresses these challenges through two core innovations. First, we develop a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. Second, we integrate a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. Our approach generates 2100+ diverse trajectories across 12 medical specialties, each averaging 4.2 tool interactions. Through a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards, our MedResearcher-R1-32B model demonstrates exceptional performance, establishing new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks. Our work demonstrates that strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近在基于大型语言模型（LLM）的智能体方面的发展展示了跨多个领域的令人印象深刻的能力，这通过深度研究系统表现出在复杂的信息搜索和综合任务中的优越性能得以证明。虽然通用深度研究智能体展现出令人瞩目的能力，但在医学领域的挑战中却显著表现不佳，领先的专有系统在复杂的医学基准测试上取得的准确性有限，这一点有目共睹。主要的限制包括： (1) 模型缺乏足够的密集医学知识来进行临床推理，以及 (2) 该框架受到缺乏针对医学背景的专门检索工具的限制。我们提出了一种医学深度研究智能体，通过两个核心创新来应对这些挑战。首先，我们使用医学知识图开发了一种新颖的数据合成框架，从围绕稀有医学实体的子图中提取最长链，以生成复杂的多跳问答对。其次，我们集成了一个定制构建的私有医学检索引擎，与通用工具并行工作，从而能够准确合成医学信息。我们的方法在12个医学专科中生成了2100多个多样化的轨迹，平均每个轨迹有4.2次工具互动。通过将监督微调与复合奖励的在线强化学习相结合的两阶段训练范式，我们的MedResearcher-R1-32B模型展示了卓越的性能，在医学基准测试上建立新的最先进的结果，同时在通用深度研究任务中保持竞争力。我们的工作证明，在架构、工具设计和训练数据构建方面的战略性领域特定创新能够使较小的开源模型在专业领域中超越更大的专有系统。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图解决的问题是：如何开发一个能够进行复杂医学研究的深度研究智能体（agent），以克服现有通用深度研究智能体在医学领域面临的挑战。</p>
<p>具体来说，论文指出现有基于大型语言模型（LLM）的通用深度研究智能体在处理医学领域任务时存在显著不足，主要体现在以下两个方面：</p>
<ol>
<li><strong>医学知识密度不足</strong>：通用智能体缺乏足够的密集医学知识来进行临床推理。医学研究往往需要连接罕见疾病、新兴治疗和专业临床发现等稀有医学实体，这些连接存在于专业医学文献中，但通用搜索工具无法获取。</li>
<li><strong>检索工具的局限性</strong>：通用智能体依赖于通用的检索工具，这些工具无法捕捉医学信息中细微的关系，导致在医学情境下无法准确检索到所需信息。</li>
</ol>
<p>为了解决这些问题，论文提出了一个创新的医学深度研究智能体框架，通过开发新的数据合成框架和整合定制的医学检索引擎等方法，来提升智能体在医学领域的研究能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中提到了以下相关研究：</p>
<p>通用深度研究方法</p>
<ul>
<li><strong>多智能体规划架构</strong>：<ul>
<li><strong>CAMEL</strong>：一个通信驱动的多智能体框架，智能体通过自然语言沟通来解决复杂的规划和推理任务，强调智能体间通信以实现策略协调和出现行为。</li>
<li><strong>大型推理模型（LRM）</strong>：如OpenAI O3、Perplexity Deep Research和Kimi K2等。例如，Anthropic的多智能体研究系统提出了一个主智能体，它动态地产生专门的子智能体来执行网络搜索、文档阅读和综合等任务，这种架构通过并行执行和隐式记忆共享，在复杂、长期的研究任务中表现出色。</li>
<li><strong>Deerflow</strong>：一个开源项目，使用多智能体机制实现深度学习，具有可解释性强和易于通过并行化扩展等优势。但这些基于多智能体的方法由于缺乏针对性的推理训练，仅依靠提示和字符进行规划会导致错误在多个智能体间传播，无法处理需要高水平推理的任务。</li>
</ul>
</li>
<li><strong>智能体强化学习方法</strong>：<ul>
<li><strong>ReAct</strong>：最初被提出作为一种提示策略，后来通过强化学习人类反馈（RLHF）进行优化，以强制执行最优推理路径。</li>
<li><strong>WebArena</strong>：为训练强化学习智能体提供了一个高保真度的网络交互环境，这些智能体通过真实浏览器API进行多跳推理和智能体数据收集，从而实现基于反馈的学习。</li>
<li><strong>WebSailor</strong>：它使用合成任务构建、基于RFT的冷启动和DUPO（重复采样策略优化）强化学习微调，构建了用于工具增强型网络任务的健壮智能体。</li>
<li><strong>Search-R1</strong>：通过统一的强化学习训练LLM，使其能够交错推理和搜索。</li>
<li><strong>S3</strong>：将搜索与生成解耦，并在样本数量少70倍的情况下达到了相当的准确度。与基于多角色智能体的深度研究系统相比，基于智能体的强化学习方法的优势在于通过学习行为将模型的问题解决能力内化，从而更好地泛化到未知任务，并适应复杂的环境，如网络浏览。</li>
</ul>
</li>
</ul>
<p>医学RAG系统</p>
<ul>
<li><strong>MedRAG</strong>：建立了基于证据的生成范式，通过从PubMed快照和专有数据库中检索不可变语料库来实现。</li>
<li><strong>Deeprare</strong>：MedRAG的实时证据同化，通过实时CDC/WHO数据流连续同步，并动态加权（F1分数+14.3%），直接解决了像DeepRare这样的系统中存在的知识过时问题。</li>
<li><strong>SurgRAW</strong>：开创性地将实时手术视频检索与强化学习相结合，实现了术中决策支持，仪器识别准确率达到90.2%。</li>
<li><strong>Federated ClinicalCamel</strong>：通过跨机构的知识蒸馏解决数据碎片化问题，同时保持隐私合规性（在12家医院中AUROC为0.92）。</li>
</ul>
<p>医学多角色系统</p>
<ul>
<li><strong>动态知识内化</strong>：<ul>
<li><strong>SeaKR</strong>：其自我意识检索引入了时间定位机制，根据出版物的新近性和证据等级动态调整知识权重。</li>
<li><strong>Med-PaLM</strong>：其视觉-语言分离通过专门的路径处理放射学图像和基因组数据，同时保持诊断连贯性。这些方法将知识延迟从传统RAG系统的几天减少到几分钟。</li>
</ul>
</li>
<li><strong>偏好对齐的强化学习框架</strong>：<ul>
<li><strong>MedicalGPT v2.4</strong>：其GRPO（组相对策略优化）在肿瘤学决策中与临床小组的协议达到了98.7%。</li>
</ul>
</li>
<li><strong>统一的认知架构</strong>：<ul>
<li><strong>MAI-DxO</strong>：微软的系统，五个协作智能体实现了85.5%的诊断准确率，是平均临床医生表现的四倍。通过Med-Gemini的3阶段管道（时间定位、临床医生验证的SFT和多目标RLHF）保持监管合规性。 尽管有这些进步，但当前医学多角色智能体系统在推理能力方面仍存在关键限制，与医学研究中的深度推理方法存在根本差距。</li>
</ul>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下三个核心创新来解决通用深度研究智能体在医学领域面临的挑战：</p>
<p>1. 数据合成框架</p>
<ul>
<li><strong>提取罕见医学实体</strong>：从超过3000万篇PubMed摘要中提取医学实体，并通过频率分析识别在医学语料库中出现频率低于 (10^{-6}) 的候选实体。利用LLM辅助评估，筛选出真正罕见但具有临床意义的实体，避免了无意义的拼写错误和过于常见的疾病。</li>
<li><strong>构建知识图谱并提取最长推理链</strong>：围绕这些罕见医学实体构建知识图谱，从子图中提取最长的推理链以生成多跳问题。这种方法创建的问题反映了真实的医学研究挑战，需要系统地探索和综合多个医学信息源，而不能通过简单检索得到答案。</li>
<li><strong>质量控制和难度校准</strong>：对生成的问题进行评估，如果GPT-o3 deepresearch和GPT-4任一模型的准确率超过50%，则自动重新生成更复杂的问题，确保数据集对现有系统保持挑战性。</li>
</ul>
<p>2. 医学领域特定工具</p>
<ul>
<li><strong>PrivateMedicalRetriever</strong>：该模块直接从权威临床资源（如FDA数据库、临床试验注册表和PubMed出版物）聚合证据。每个候选文档根据语义相关性和临床权威性进行评分，评分公式为： [ \text{Score}(d, q) = \lambda \cdot \text{Rel}(d, q) + (1 - \lambda) \cdot \text{Auth}(d) ] 其中，(\text{Rel}(d, q)) 表示与查询的语义相似性，(\text{Auth}(d)) 反映临床权威性，结合了影响因子和指南状态。在实验中，(\lambda) 被设置为0.4，以优先考虑可靠且具有临床意义的证据。</li>
<li><strong>ClinicalReasoningEngine</strong>：该工具用于基于证据的鉴别诊断，应用贝叶斯推理系统评估多个假设。给定观察到的症状 (s)、候选诊断 (D_j) 和患者上下文 (c)，每个诊断的后验概率计算公式为： [ P(D_j | s, c) = \frac{\prod_{i=1}^{n} P(s_i | D_j, c) \cdot P(D_j | c)}{\sum_{k=1}^{m} \prod_{i=1}^{n} P(s_i | D_k, c) \cdot P(D_k | c)} ] 其中，条件概率从临床文献中得出，并根据新检索到的证据迭代更新。</li>
</ul>
<p>3. 训练方法</p>
<ul>
<li><strong>监督微调（SFT）</strong>：通过在大量合成对话数据上进行监督微调来启动智能体训练，优化目标是最大化生成正确轨迹的可能性。在微调过程中，加入了工具失败模拟（5%的损坏率）、中间思考监督和多任务采样等增强手段，以提高智能体的鲁棒性和泛化能力。</li>
<li><strong>强化学习（RL）</strong>：在监督微调之后，通过强化学习进一步优化智能体的行为。使用组合奖励函数来优化智能体轨迹，奖励函数由任务完成奖励、专家偏好奖励和效率惩罚组成，权重分别为1.0、0.2和0.1。通过组正则化策略优化（GRPO）来优化智能体的行为，移除了KL正则化，并采用课程学习逐步增加任务复杂度。</li>
</ul>
<p>通过这些创新，论文提出的MedReseacher-R1模型在医学基准测试中取得了新的最佳结果，同时在通用深度研究任务上也保持了竞争力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文主要进行了以下实验：</p>
<p>1. 性能评估实验</p>
<ul>
<li><strong>实验目的</strong>：评估MedReseacher-R1在医学领域复杂研究任务中的表现以及其在通用深度研究任务上的泛化能力。</li>
<li><strong>实验方法</strong>：在多个基准测试集上对MedReseacher-R1进行评估，包括医学领域特定的MedBrowseComp基准测试集，以及通用的GAIA和xBench-DeepSearch基准测试集。</li>
<li><strong>实验结果</strong>：<ul>
<li><strong>MedBrowseComp</strong>：MedReseacher-R1取得了27.5%的准确率，超越了之前的最佳智能体和Qwen2.5-32B基线模型，例如o3-deepresearch准确率为25.5%，Gemini-2.5Pro-deepresearch准确率为25.0%，而仅依赖搜索的o3-search准确率为19.0%，Gemini-2.5-Prosearch准确率为14.0%。</li>
<li><strong>GAIA</strong>：MedReseacher-R1的得分为53.4，与WebSailor-32B的53.2相当，表现出在通用深度研究任务上的竞争力。</li>
<li><strong>xBench-DeepSearch</strong>：MedReseacher-R1的得分为54.0，与WebSailor-32B的53.3相当，进一步证明了其在通用任务上的泛化能力。</li>
</ul>
</li>
</ul>
<p>2. 定性分析实验</p>
<ul>
<li><strong>实验目的</strong>：深入分析训练数据模式对智能体行为的影响，理解MedReseacher-R1在复杂多跳推理任务中表现优异的潜在因素。</li>
<li><strong>实验方法</strong>：选取具有代表性的案例，对比MedReseacher-R1与其他基线智能体在处理复杂医学问题时的行为模式和推理过程。</li>
<li><strong>实验结果</strong>：<ul>
<li>发现MedReseacher-R1遵循“搜索→多次验证→综合”的范式，通过多次验证循环确保信息的一致性和准确性，然后进行最终的综合，从而避免了基线智能体常见的过早收敛或次优工具利用模式。</li>
<li>成功轨迹分析表明，遵循这种多验证迭代模式的训练实例在复杂多跳推理任务中的成功率比单次验证方法高出34.2%，这表明迭代验证是开发能够在多样化工具推理环境中泛化的健壮深度研究能力的关键机制。</li>
</ul>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文提出了未来工作的几个方向，以下是具体的可以进一步探索的点：</p>
<p>1. 多模态工具集成</p>
<ul>
<li><strong>研究内容</strong>：将当前框架扩展到支持多模态医学工具，如放射学图像查看器、病理学幻灯片分析器、基因组数据源和电子健康记录。</li>
<li><strong>研究意义</strong>：这种集成将使智能体能够处理和综合不同类型的数据，更贴近现实世界中的临床工作流程。</li>
</ul>
<p>2. 人机协作</p>
<ul>
<li><strong>研究内容</strong>：将医学专业人员的人为反馈纳入智能体的行为引导中，开发专家评估和注释的界面。</li>
<li><strong>研究意义</strong>：可以提高推理质量、工具使用效率和智能体输出的临床相关性。</li>
</ul>
<p>3. 安全性和可靠性</p>
<ul>
<li><strong>研究内容</strong>：系统地研究模型的安全性和可靠性，以实现公开部署，重点关注强大的幻觉检测、不确定性估计以及适用于高风险医疗场景的故障安全机制的实施。</li>
<li><strong>研究意义</strong>：确保智能体在医疗领域的安全可靠使用，避免因错误信息导致的不良后果。</li>
</ul>
<p>4. 高级医学推理基准测试</p>
<ul>
<li><strong>研究内容</strong>：构建一个综合性的基准测试，涵盖医学领域的复杂多跳推理，包括药理学、诊断学、流行病学、遗传学、外科规划和治疗学等。</li>
<li><strong>研究意义</strong>：为评估智能体在工具编排和证据综合方面的能力设定更高的标准，推动智能体在更具挑战性的场景中的发展。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文标题</p>
<p>MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</p>
<p>作者信息</p>
<p>Ailing Yu, Lan Yao, Jingnan Liu, Zhe Chen, Jiajun Yin, Yuan Wang, Xinhao Liao, Zhiling Ye, Ji Li, Yun Yue, Hansong Xiao, Hualei Zhou, Chunxiao Guo, Peng Wei, Jinjie Gu</p>
<p>所属机构</p>
<p>Ant Group, Harbin Institute of Technology</p>
<p>论文摘要</p>
<p>论文介绍了MedReseacher-R1，这是一个专门针对医学领域的深度研究智能体。该智能体通过两个核心创新解决了通用深度研究智能体在医学领域面临的挑战：一是开发了一个基于医学知识图谱的新数据合成框架，通过提取罕见医学实体周围的最长推理链来生成复杂的多跳问答对；二是整合了一个定制的私有医学检索引擎，与通用工具一起使用，以实现准确的医学信息综合。该方法生成了2100多个多样化的轨迹，涵盖了12个医学专业，平均每个轨迹有4.2次工具交互。通过结合监督微调和在线强化学习的两阶段训练范式，MedReseacher-R1-32B模型在医学基准测试中取得了新的最佳结果，同时在通用深度研究任务上保持了竞争力。该研究表明，通过在架构、工具设计和训练数据构建方面的战略性领域特定创新，可以使较小的开源模型在专业领域超越更大的专有系统。</p>
<p>研究背景</p>
<p>大型语言模型（LLM）在多个领域展示了令人印象深刻的能力，但在医学领域，通用深度研究智能体面临着独特挑战。医学研究需要连接罕见疾病、新兴治疗和专业临床发现，这些连接存在于专业医学文献中，但通用搜索工具无法获取。此外，通用智能体依赖于通用检索工具，这些工具无法捕捉医学信息中细微的关系。</p>
<p>研究方法</p>
<ol>
<li><strong>数据合成框架</strong>：通过从PubMed摘要中提取医学实体，识别罕见实体，并围绕这些实体构建知识图谱，提取最长推理链以生成复杂的多跳问答对。</li>
<li><strong>医学领域特定工具</strong>：开发了PrivateMedicalRetriever和ClinicalReasoningEngine，分别用于从权威医学数据库检索证据和进行基于证据的鉴别诊断。</li>
<li><strong>训练方法</strong>：采用监督微调（SFT）和强化学习（RL）相结合的方法，通过合成对话数据进行训练，并使用组合奖励函数优化智能体行为。</li>
</ol>
<p>实验结果</p>
<ul>
<li><strong>MedBrowseComp基准测试</strong>：MedReseacher-R1-32B取得了27.5%的准确率，超越了之前的最佳智能体和Qwen2.5-32B基线模型。</li>
<li><strong>GAIA和xBench-DeepSearch基准测试</strong>：在通用深度研究任务上，MedReseacher-R1表现出竞争力，分别取得了53.4和54.0的分数。</li>
</ul>
<p>结论</p>
<p>MedReseacher-R1通过其创新的数据合成框架和医学领域特定工具，在医学基准测试中取得了新的最佳结果，并在通用深度研究任务上保持了竞争力。这表明，通过领域特定的创新，较小的开源模型可以在专业领域超越更大的专有系统。</p>
<p>未来工作</p>
<ul>
<li><strong>多模态工具集成</strong>：扩展框架以支持多模态医学工具。</li>
<li><strong>人机协作</strong>：纳入医学专业人员的反馈以提高智能体的推理质量。</li>
<li><strong>安全性和可靠性</strong>：研究模型的安全性和可靠性，以实现公开部署。</li>
<li><strong>高级医学推理基准测试</strong>：构建一个综合性的基准测试，涵盖医学领域的复杂多跳推理。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MedReseacher-R1%3A%20Expert-Level%20Medical%20Deep%20Researcher%20via%20A%20Knowledge-Informed%20Trajectory%20Synthesis%20Framework，链接是  https://arxiv.org/pdf/2508.14880  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.14880  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ailing Yu,Lan Yao,Jingnan Liu,Zhe Chen,Jiajun Yin,Yuan Wang,Xinhao Liao,Zhiling Ye,Ji Li,Yun Yue,Hansong Xiao,Hualei Zhou,Chunxiao Guo,Peng Wei,Junwei Liu,Jinjie Gu</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2508.14880.pdf">https://arxiv.org/pdf/2508.14880.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2508.14880">https://arxiv.org/abs/2508.14880</a></p>
<p><strong>Arxiv ID</strong>: 2508.14880</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2508.14880">https://papers.cool/arxiv/2508.14880</a></p>
<p><strong>Published</strong>: 2025-08-20T17:51:20Z</p>
<p><strong>Updated</strong>: 2025-08-20T17:51:20.000Z</p>
<hr>
<h3 id="7-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning"><a href="#7-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning" class="headerlink" title="7. THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13761">THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</a></h3><p>Large Language Models (LLMs) have made remarkable progress in mathematical reasoning, but still continue to struggle with high-precision tasks like numerical computation and formal symbolic manipulation. Integrating external tools has emerged as a promising approach to bridge this gap. Despite recent advances, existing methods struggle with three key challenges: constructing tool-integrated reasoning data, performing fine-grained optimization, and enhancing inference. To overcome these limitations, we propose THOR (Tool-Integrated Hierarchical Optimization via RL). First, we introduce TIRGen, a multi-agent actor-critic-based pipeline for constructing high-quality datasets of tool-integrated reasoning paths, aligning with the policy and generalizing well across diverse models. Second, to perform fine-grained hierarchical optimization, we introduce an RL strategy that jointly optimizes for both trajectory-level problem solving and step-level code generation. This is motivated by our key insight that the success of an intermediate tool call is a strong predictor of the final answer’s correctness. Finally, THOR incorporates a self-correction mechanism that leverages immediate tool feedback to dynamically revise erroneous reasoning paths during inference. Our approach demonstrates strong generalization across diverse models, performing effectively in both reasoning and non-reasoning models. It further achieves state-of-the-art performance for models of a similar scale on multiple mathematical benchmarks, while also delivering consistent improvements on code benchmarks. Our code will be publicly available at <a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR">https://github.com/JingMog/THOR</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）在数学推理方面取得了显著进展，但在数值计算和形式符号操作等高精度任务上仍然存在困难。整合外部工具已成为弥补这一差距的一种有前景的方法。尽管最近有了进展，现有方法仍然面临三个主要挑战：构建工具集成推理数据、进行精细优化以及增强推理。为克服这些限制，我们提出了THOR（通过强化学习进行工具集成层次优化）。首先，我们推出TIRGen，这是一种基于多智能体演员-评论家的管道，用于构建高质量的工具集成推理路径数据集，符合策略并在多种模型中良好泛化。其次，为了进行精细的层次优化，我们引入了一种强化学习策略，该策略同时优化轨迹级问题解决和步骤级代码生成。这是基于我们的关键洞察，即中间工具调用的成功是最终答案正确性的强预测因子。最后，THOR结合了一种自我纠正机制，利用即时工具反馈在推理过程中动态修正错误推理路径。我们的方法在不同模型中显示出强大的泛化能力，在推理和非推理模型中均表现良好。它在多个数学基准测试中实现了同类规模模型的最新性能，同时在代码基准测试上也提供了一致的改进。我们的代码将公开发布在 <a target="_blank" rel="noopener" href="https://github.com/JingMog/THOR">https://github.com/JingMog/THOR</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对大语言模型（LLM）在数学推理中“高精度环节薄弱”的痛点——数值计算、符号推导、公式变形等步骤容易 hallucination——提出把外部代码执行器作为“可验证工具”引入推理链。为此，需同时克服三大难题：</p>
<ol>
<li><p>数据瓶颈<br>现有 Tool-Integrated Reasoning（TIR）数据要么风格与推理模型不匹配，要么难以让“长思维链”模型学会在何处插入代码。</p>
</li>
<li><p>训练粒度粗糙<br>纯轨迹级 RL 只靠最终答案对错给出稀疏奖励，无法精细纠正中间某一步的代码生成错误。</p>
</li>
<li><p>推理阶段缺乏即时纠错<br>单遍生成遇到代码执行失败只能“硬着陆”，无法利用工具反馈动态回退并修正思路。</p>
</li>
</ol>
<p>THOR 围绕这三点提出系统化方案：</p>
<ul>
<li>TIRGen 多智能体数据管线→解决 1</li>
<li>轨迹级+步骤级分层 RL→解决 2</li>
<li>在线 self-correction 机制→解决 3</li>
</ul>
<p>目标是在不增加大模型参数规模的前提下，把数学竞赛级任务的准确率推到同规模 SOTA，并零样本泛化到代码生成基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为“工具无关推理”与“工具集成推理”两条主线，并进一步细分如下：</p>
<p>工具无关推理</p>
<p>子类</p>
<p>代表方法</p>
<p>特点/局限</p>
<p>搜索增强</p>
<p>CoT、ToT、GoT、PRM-BAS、Marco-o1、rStar-Math</p>
<p>用提示或奖励模型在输出空间做树/图搜索，推理链可解释，但仅推断阶段增强，不更新模型参数；搜索开销大。</p>
<p>强化学习</p>
<p>PPO、GRPO、DAPO、VAPO、GSPO</p>
<p>直接优化策略，可扩展性强；奖励稀疏问题突出，长链推理难以逐步纠错。</p>
<p>工具集成推理</p>
<p>子类</p>
<p>代表方法</p>
<p>特点/局限</p>
<p>提示级工具使用</p>
<p>ReAct、VOYAGER、PoT</p>
<p>靠提示模板让模型在文本中调用 API/代码，无需训练；泛化受限，对“推理模型”效果差。</p>
<p>监督微调</p>
<p>Toolformer、Aimo-2、START</p>
<p>用合成或人工标注的 TIR 数据做 SFT；数据风格与策略模型易失配，难以精细优化。</p>
<p>现有 RL 工具方法</p>
<p>TORL、ReTool、AutoTIR、ZTRL</p>
<p>引入轨迹级 RL 让模型学会何时调用工具，但仍靠稀疏的最终答案奖励，未利用步骤级执行反馈。</p>
<p>THOR 在上述基础上首次把“步骤级代码执行成败”显式纳入奖励，实现轨迹-步骤双层 RL，并配套可自我校正的在线推理机制，与现有方法形成差异。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 THOR 框架，把“数据–训练–推理”三阶段痛点串成一条闭环解决方案：</p>
<ol>
<li><p><strong>数据阶段：TIRGen 多智能体管线</strong></p>
<ul>
<li><strong>Actor</strong>（策略模型）逐句生成自然语言推理；</li>
<li><strong>Critic</strong>（固定 32B 模型）实时判断“这句可否转代码”，可转则抽逻辑→写 Python→沙箱执行→把精确结果插回推理链；</li>
<li>多轮迭代后得到与 Actor 风格对齐、长度/难度/调用次数均衡的冷启动数据集 D_SFT。</li>
</ul>
</li>
<li><p><strong>训练阶段：分层强化学习</strong></p>
<ul>
<li><strong>轨迹级优化</strong><br>用 GRPO 以“最终答案对错”为奖励，提升整体解题策略。</li>
<li><strong>步骤级优化</strong><br>对执行失败的 (thought, code) 步，回退到该 thought 前缀，重采样多条新后缀+新 code；以“代码是否通过执行”为密集奖励，用同一份 GRPO 目标微调。</li>
<li>两目标相加联合更新，解决稀疏奖励且精准修正局部代码幻觉。</li>
</ul>
</li>
<li><strong>推理阶段：Self-Correction</strong><br>在线 think-act-observe 循环中一旦代码抛错，立即回退至当前 thought 前缀，重新生成后缀与新 code，最多重试 N_corr 次，利用即时工具反馈动态修复推理路径，无需外部奖励模型。</li>
</ol>
<p>通过“对齐数据 + 双层 RL + 在线纠错”三位一体，THOR 让 1.5B–8B 模型在数学竞赛级基准上取得同规模 SOTA，并零样本提升代码生成任务。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“数学推理主评测 + 代码泛化验证 + 内部消融诊断”三条线展开，覆盖 1.5 B–8 B 参数规模的推理/非推理两类 backbone。</p>
<ol>
<li><p>主评测：6 项数学基准</p>
<ul>
<li>高中难度：MATH500</li>
<li>竞赛难度：AMC 2023、AIME 2024/2025、Minerva Math、Olympiad Bench<br>指标：Avg@4（4 组随机种子平均）。<br>结果：</li>
<li>非推理组：THOR-7B 平均 61.2，超越 GPT-4o、TORL-7B、Eurus-2-PRIME-7B 等。</li>
<li>推理组：THOR-Thinking-8B 平均 79.8，优于 o1-preview、QwQ-32B、START-32B。</li>
<li>轻量组：1.5 B/1.7 B 模型同样取得同规模 SOTA，验证横向扩展性。</li>
</ul>
</li>
<li><p>代码泛化：零样本 Pass@1</p>
<ul>
<li>HumanEval+、MBPP+、LiveCodeBench-v6<br>结果：THOR 版本在三大基准上平均提升 2–7 %，未使用任何代码微调数据，证明工具增强间接强化编程能力。</li>
</ul>
</li>
<li><p>自奖励搜索 vs 单路径<br>用代码执行通过率当内部奖励，做 Best-of-N 采样。</p>
<ul>
<li>THOR-7B：BoN-8 相对单路径提升 2.1 %</li>
<li>THOR-Thinking-8B：BoN-8 提升 3.4 %，AIME 难题增益更大。</li>
</ul>
</li>
<li><p>消融实验（T1–T6）<br>① 无工具 → ② 仅冷启动 → ③ 仅轨迹级 RL → ④ 完整分层 RL → ⑤ 再叠加自纠错<br>结论：</p>
<ul>
<li>冷启动（TIRGen）把 pass@16 提高 16.9–79.8 %，代码调用率从 20 % 提到 80 %。</li>
<li>轨迹级 RL 带来绝对值约 +22 % 的数学准确率。</li>
<li>步骤级 RL 再提 1–2 %，验证细粒度优化有效。</li>
<li>自纠错模块最后补 1–3 %，且仅增加 &lt;6 % 额外 token。</li>
</ul>
</li>
<li><p>统计验证<br>χ² 检验证实“代码执行成功 ⇔ 最终答案正确”高度相关（χ²=336，p&lt;1e-75），为分层 RL 提供理论依据。</p>
</li>
<li><p>推理开销分析<br>因工具替代冗长文本计算，THOR 平均 token 消耗反而下降 6–13 %（已含自我纠错重算成本）。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>多工具协同</strong><br>目前仅集成 Python 执行器，可引入符号计算（SymPy、Mathematica）、几何绘图（Asymptote、GeoGebra）、检索或定理证明器（Lean、Isabelle），研究异构工具的统一调用接口与联合优化策略。</p>
</li>
<li><p><strong>更大规模与多模态</strong><br>将分层 RL 拓展至 32 B/70 B 乃至 MoE 结构，验证收益是否持续；同时加入图表、几何图形输入，考察视觉-符号-代码三联推理的效果。</p>
</li>
<li><p><strong>步骤级奖励模型</strong><br>训练专门的“代码执行成功预测器”或“步骤正确性 PRM”，替代规则化奖励，降低环境噪声带来的误判，并支持更细粒度的 credit assignment。</p>
</li>
<li><p><strong>在线 continual TIR</strong><br>在真实用户交互流中持续收集执行反馈，实现增量式冷启动更新，避免重复离线采样，构建自我演化的数学推理系统。</p>
</li>
<li><p><strong>理论分析</strong><br>从 PAC-Bayes 或 RL 泛化误差角度，解释“中间工具成功 ⇒ 最终答案正确”这一经验关联的样本复杂度与收敛性，为分层奖励设计提供理论保证。</p>
</li>
<li><p><strong>安全与可解释</strong><br>研究工具调用链的可验证性与可审计性，防止恶意代码或误导性计算结果；同时提供人类可读的“工具-推理”对照解释界面，服务教育场景。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>THOR 核心内容一览</strong></p>
<p>维度</p>
<p>要点</p>
<p><strong>问题</strong></p>
<p>LLM 在高精度数学环节（数值/符号）易 hallucination；现有 Tool-Integrated Reasoning 面临数据难造、训练粒度粗、推理无法即时纠错三大痛点。</p>
<p><strong>洞察</strong></p>
<p>中间代码执行成功 ↔ 最终答案正确高度相关（χ²=336），可作为密集奖励信号。</p>
<p><strong>方法</strong></p>
<p>1️⃣ TIRGen：Actor（推理）+ Critic（抽逻辑→写代码→执行）迭代生成对齐数据。<br>2️⃣ 分层 RL：轨迹级用答案对错优化整体策略；步骤级用代码 pass/fail 精调局部生成。<br>3️⃣ Self-Correction：推理时一旦代码抛错，回退前缀并重生成后缀+新代码，最多 N_corr 次。</p>
<p><strong>实验</strong></p>
<p>6 项数学基准（MATH500→Olympiad）+ 3 项代码基准（HE/MBPP+/LiveCode）<br>1.5 B–8 B 模型均获同规模 SOTA；零样本代码任务同步提升；消融显示冷启动+分层 RL+自纠错逐层增益；BoN 搜索仅用代码通过率即可再提 3.4 %。</p>
<p><strong>效率</strong></p>
<p>工具替代冗长计算，总 token 下降 6–13 %（已含纠错开销）。</p>
<p><strong>贡献</strong></p>
<p>提出“数据-训练-推理”全栈 TIR 方案，首次把步骤级执行反馈纳入 RL 奖励，实现小模型数学竞赛 SOTA 与代码能力双赢。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是THOR%3A%20Tool-Integrated%20Hierarchical%20Optimization%20via%20RL%20for%20Mathematical%20Reasoning，链接是  https://arxiv.org/pdf/2509.13761  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13761  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Qikai Chang,Zhenrong Zhang,Pengfei Hu,Jiefeng Ma,Yicheng Pan,Jianshu Zhang,Jun Du,Quan Liu,Jianqing Gao</p>
<p><strong>Categories</strong>: cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13761.pdf">https://arxiv.org/pdf/2509.13761.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13761">https://arxiv.org/abs/2509.13761</a></p>
<p><strong>Arxiv ID</strong>: 2509.13761</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13761">https://papers.cool/arxiv/2509.13761</a></p>
<p><strong>Published</strong>: 2025-09-17T07:16:12Z</p>
<p><strong>Updated</strong>: 2025-09-17T07:16:12.000Z</p>
<hr>
<h3 id="8-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook"><a href="#8-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook" class="headerlink" title="8. MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14142">MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</a></h3><p>This paper reviews the MARS2 2025 Challenge on Multimodal Reasoning. We aim to bring together different approaches in multimodal machine learning and LLMs via a large benchmark. We hope it better allows researchers to follow the state-of-the-art in this very dynamic area. Meanwhile, a growing number of testbeds have boosted the evolution of general-purpose large language models. Thus, this year’s MARS2 focuses on real-world and specialized scenarios to broaden the multimodal reasoning applications of MLLMs. Our organizing team released two tailored datasets Lens and AdsQA as test sets, which support general reasoning in 12 daily scenarios and domain-specific reasoning in advertisement videos, respectively. We evaluated 40+ baselines that include both generalist MLLMs and task-specific models, and opened up three competition tracks, i.e., Visual Grounding in Real-world Scenarios (VG-RS), Visual Question Answering with Spatial Awareness (VQA-SA), and Visual Reasoning in Creative Advertisement Videos (VR-Ads). Finally, 76 teams from the renowned academic and industrial institutions have registered and 40+ valid submissions (out of 1200+) have been included in our ranking lists. Our datasets, code sets (40+ baselines and 15+ participants’ methods), and rankings are publicly available on the MARS2 workshop website and our GitHub organization page <a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a>, where our updates and announcements of upcoming events will be continuously provided.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本文回顾了 2025 年 MARS2 多模态推理挑战赛。我们的目标是通过一个大型基准，将多模态机器学习和大语言模型（LLMs）中的不同方法汇集在一起。我们希望这能更好地帮助研究人员跟踪这一动态领域中的最新进展。同时，越来越多的测试平台推动了通用大语言模型的发展。因此，今年的 MARS2 将重点放在现实世界和专业场景上，以拓宽 MLLMs 的多模态推理应用。我们的组织团队发布了两个定制数据集 Lens 和 AdsQA 作为测试集，分别支持 12 个日常场景中的一般推理和广告视频中的特定领域推理。我们评估了 40 多个基线模型，包括通用 MLLMs 和任务特定模型，并启动了三个比赛轨道，即现实世界场景中的视觉定位（VG-RS），具有空间意识的视觉问答（VQA-SA），以及创意广告视频中的视觉推理（VR-Ads）。最后，来自知名学术和工业机构的 76 支团队注册参赛，40 多个有效提交（超过 1200 个）已被纳入我们的排名列表。我们的数据集、代码集（40 多个基线和 15 个以上参与者的方法）以及排名在 MARS2 研讨会官方网站和我们的 GitHub 组织页面 <a target="_blank" rel="noopener" href="https://github.com/mars2workshop/">https://github.com/mars2workshop/</a> 上公开提供，我们将持续提供更新和即将举办活动的公告。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>MARS2 2025 挑战的核心目标可以概括为：<br><strong>在“大模型时代”推动多模态推理从快速、直觉式的 System 1 走向缓慢、深思熟虑的 System 2，并专门检验模型在真实世界与专业场景下的协同推理与非链式复杂推理能力。</strong></p>
<p>具体而言，论文试图解决以下两个关键科学问题：</p>
<ol>
<li><p><strong>协同推理效应评估缺失</strong><br>现有基准往往把不同推理任务（如视觉定位、空间问答）的测试样本拆散到不同数据分布，导致无法衡量“同一张图像上多项推理任务能否相互促进”。MARS2 通过 Lens 数据集让 VG-RS 与 VQA-SA 共享同分布样本，首次量化“协同增益”。</p>
</li>
<li><p><strong>非链式复杂推理不足</strong><br>链式思维（CoT）在数学、编程等“步骤清晰”领域有效，但在广告视频等需要“跳跃式、隐性、心理层面”推理的场景失效。AdsQA 数据集专门构造了营销逻辑、说服策略、受众建模等无法通过“if A then B”逐步推导的问题，迫使模型进行非链式、抽象、多跳推理。</p>
</li>
</ol>
<p>围绕这两个问题，论文配套推出三大竞赛赛道与 40+ 基线，系统评估通用 MLLM 与专用模型在</p>
<ul>
<li>真实世界视觉定位（VG-RS）</li>
<li>空间-常识-反事实问答（VQA-SA）</li>
<li>创意广告视频深度理解（VR-Ads）</li>
</ul>
<p>上的表现，并公开全部数据、代码与排行榜，供社区持续检验“大模型是否真的具备了慢思考级别的多模态推理能力”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 MARS2 2025 挑战直接相关的研究可划分为四大脉络，对应论文附录 A.1–A.4 的综述。以下按“研究主题—代表性工作—与 MARS2 的关联”三列给出关键文献（括号内为论文参考文献编号）：</p>
<p>研究主题</p>
<p>代表性工作</p>
<p>与 MARS2 的关联</p>
<p><strong>多模态链式推理（M-CoT）</strong></p>
<p>Multimodal-CoT [159]、LLaVA-CoT [130]、RAVEN-CoT [169]、Magma-CoT [136]、MM-ReAct [139]</p>
<p>提供显式推理链模板，但在广告隐喻、情感等抽象语义上失效；MARS2 的 VR-Ads track 专门考察此类失败场景。</p>
<p><strong>强化学习驱动的多模态推理</strong></p>
<p>Visual-RFT [80]、SRPO [118]、VL-Rethinker [120]、DeepSeek-R1 [41]</p>
<p>用可验证奖励优化推理轨迹，已在 VQA-SA 类空间任务上有效；MARS2 多个参赛队将其扩展到 VG-RS，并揭示奖励函数单一（仅 IoU）导致模型泛化性下降。</p>
<p><strong>原生多模态大模型</strong></p>
<p>GPT-4V [138]、GPT-4o [1]、Gemini-2.5 Pro [116]、Claude-3.5 [5]、InternVL3 [168]、Qwen2.5-VL [8]</p>
<p>作为 MARS2 40+ 基线的核心对比对象；Lens 与 AdsQA 结果显示，即便是最强闭源模型在“空间-组合-抽象”三层推理上仍 &lt;60%–71%。</p>
<p><strong>视觉定位（VG）范式演进</strong></p>
<p>‑ 外部视觉编码器</p>
<p>Groma [87]、Ferret [145]、Grounding DINO [77]、Groundhog [157]、GLaMM [100]</p>
<p>提供区域级先验，被 MARS2 参赛队（Star s、SUP）用作“专家验证器”，与通用 MLLM 协同提升 VG-RS 精度。</p>
<p>‑ 跨模态语义对齐</p>
<p>BLIP-2/Q-Former [68]、Grounding-GPT [73]、LION [16]</p>
<p>通过可学习查询统一视觉-语言空间；MARS2 的 Lens 数据集中小目标、模糊指代对此类静态对齐提出新挑战。</p>
<p>‑ 多指令微调</p>
<p>Kosmos-2 [96]、VPP-LLaVA [114]、TRIG [71]、VideoGLaMM [92]、LLaVA-ST [67]</p>
<p>将定位、描述、时序等多任务统一在指令框架内，与 MARS2“协同效应”评估目标一致；Lens 进一步加入多轮空间问答，检验指令泛化性。</p>
<p><strong>空间/常识 VQA 数据集</strong></p>
<p>GQA [47]、VQA v2 spatial subset [36]、TDIUC [55]、CLEVR [54]、CLEVR-Humans [43]、ScanQA [6]、3D-VQA [31, 163]</p>
<p>提供显式空间关系或 3D 场景标注；Lens 在此基础上引入“反事实+视角变换+多轮指代”，弥补它们缺乏的真实世界复杂性与时间 freshness（&gt;53% 样本采集于 2025 年 1 月后）。</p>
<p><strong>广告视频理解</strong></p>
<p>Pit [48]、Kafa [52]、Metaclue [4]、Intentonomy [51]、Video-Holmes [23]</p>
<p>早期工作局限于图像广告或单一维度（隐喻、意图）；AdsQA 首次构建大规模视频 QA 基准，覆盖营销逻辑、说服策略、受众建模等五维抽象推理，与 VR-Ads 赛道直接对应。</p>
<p>综上，MARS2 并非提出全新模型，而是<strong>通过 Lens 与 AdsQA 两个新基准，把上述四大脉络的研究放到统一、真实、专业的评测场</strong>，揭示当前最强 MLLM 在“协同+非链式”复杂推理上的系统性短板，为后续研究指明数据与评估方向。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出“单一全新模型”去解决问题，而是采用**“构建挑战性数据集 + 设计多赛道评测协议 + 组织大规模社区挑战赛”**的三段式策略，系统暴露并推动社区共同解决“协同推理缺失”与“非链式复杂推理不足”两大痛点。具体手段如下：</p>
<p>1. 构建针对性数据集：把问题“固化”成可量化的基准</p>
<p>数据集</p>
<p>面向痛点</p>
<p>关键设计</p>
<p>量化指标</p>
<p><strong>Lens</strong></p>
<p>协同推理缺失</p>
<p>同一张图像同时配备 8 类任务（感知→理解→推理），VG-RS 与 VQA-SA 共享分布</p>
<p>同图多任务一致性、跨任务增益 ΔAcc</p>
<p><strong>AdsQA</strong></p>
<p>非链式推理不足</p>
<p>1,544 支广告视频 × 5 维抽象问题（营销逻辑、说服策略、受众建模等），答案无法逐步推导</p>
<p>人工上限 71.4%，SOTA 仅 60.7%，留出 ≥10% 绝对提升空间</p>
<p>2. 设计三赛道评测协议：把“慢思考”拆成可操作的指标</p>
<ul>
<li><p><strong>Track-1 VG-RS</strong><br>评估“细粒度定位 + 空间推理”协同：采用 Acc@IoU=0.5，额外报告 scale-wise Acc，强制模型同时处理大/小目标。</p>
</li>
<li><p><strong>Track-2 VQA-SA</strong><br>评估“空间-常识-反事实”链：引入多轮指代、视角转换、反事实假设（“若我坐在白色椅子，门在哪个方向？”），用 GLM-4-flash 自动评判+人工抽检，减少歧义。</p>
</li>
<li><p><strong>Track-3 VR-Ads</strong><br>评估“抽象语义”非链式推理：采用 GPT-4o 打分+包含/排除规则，要求答案必须覆盖创意元素、叙事、情感，但不得引入幻觉内容，迫使模型做“跳跃式”推断。</p>
</li>
</ul>
<p>3. 组织社区挑战赛：把“隐式问题”变成“显式排行榜”</p>
<p>阶段</p>
<p>机制</p>
<p>效果</p>
<p><strong>Stage-1 公开榜</strong></p>
<p>实时更新榜单，允许每日 5 次提交</p>
<p>2 个月收到 900+ 提交，快速迭代暴露瓶颈</p>
<p><strong>Stage-2 隐藏榜</strong></p>
<p>停止公开成绩，防止过拟合</p>
<p>300+ 提交，验证方法鲁棒性</p>
<p><strong>后竞赛复盘</strong></p>
<p>重新开放测试，鼓励消融与错误分析</p>
<p>收集 40+ 有效方案 + 失败案例库（图 3–5），形成“错误模式图谱”</p>
<p>4. 提炼通用技术路线：把“冠军技巧”抽象成可复现范式</p>
<p>通过对 Top-5 方案的代码审查，论文总结出<strong>四条已被验证有效的“慢思考”增强路径</strong>，供后续研究直接复用：</p>
<ol>
<li><p><strong>多阶段对齐</strong><br>SFT → RL（GRPO/DAPO）→ 知识蒸馏，持续放大稀疏信号；<br>例：ActiveAlphaAgent 用 IoU-centric 非线性奖励把 72B 模型蒸馏到 7B，推理速度↑3×，精度保持 66.7%。</p>
</li>
<li><p><strong>专家-通用协同</strong><br>通用 MLLM 生成候选 → 专用检测器（Grounding DINO/OV-DINO）验证 → 加权框融合；<br>例：Star s 队把 Qwen2.5-VL 与 G-DINO 级联，Track-1 得分从 48.5% 提到 64.8%。</p>
</li>
<li><p><strong>领域数据自循环</strong><br>利用现有最强模型自动合成 10⁶ 级问答 → SNR 过滤（Acc_synthetic/Acc_base &gt;1）→ 难度均衡采样；<br>例：Location-depends-on-guessing 合成 2.7 M 问答，RL 后 +3.2 mIoU。</p>
</li>
<li><p><strong>认知导向提示工程</strong><br>结构化五阶段提示：问题分类→对象绑定→参考系建立→关系计算→一致性检查，显著降低视角偏差；<br>例：Prompt4SA 无微调即可在 Track-2 拿到 62% Acc，证明“慢提示”可部分替代“慢模型”。</p>
</li>
</ol>
<p>5. 留下开放问题：把“解决方案”转化为“下一代研究清单”</p>
<p>论文在 Discussion 部分明确列出尚未解决的三大难题，为社区提供可操作的下一步：</p>
<ol>
<li><p><strong>奖励函数单一化导致灾难性遗忘</strong><br>仅用 IoU 或文本相似度作奖励，模型在通用 benchmark 上掉分。→ 需要多目标 RL 或过程奖励模型（Visual-PRM）。</p>
</li>
<li><p><strong>视角-物理常识偏差</strong><br>失败案例显示模型对“前后左右”依赖第一人称先验，缺乏物理量纲概念。→ 需要引入深度/尺度显式监督或 3D 表示。</p>
</li>
<li><p><strong>抽象推理评估维度不足</strong><br>AdsQA 仅覆盖 5 维，且仍以英文广告为主。→ 计划扩展至多语言、多文化、长视频（&gt;3 min）及可解释性轨迹。</p>
</li>
</ol>
<p>结论</p>
<p>论文通过“数据-评测-社区”三位一体的方式，把原本模糊的“System-2 多模态推理”问题<strong>转化为可度量、可迭代、可复现的公共 benchmark</strong>，并用 1200+ 提交的大规模实验验证：</p>
<ul>
<li>协同推理增益真实存在，但现有模型仅能从 48%→66%，仍有 20+% 绝对空间；</li>
<li>非链式抽象推理距人类上限仍有 10–15% 差距，且幻觉问题突出。</li>
</ul>
<p>由此，MARS2 为下一代多模态大模型提供了<strong>明确的性能坐标系与改进路线图</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>MARS2 2025 并未提出单一模型，而是<strong>以“大规模社区挑战 + 40+ 基线系统评测”</strong>为核心实验设计。可归纳为<strong>四类实验</strong>，覆盖数据集构建、基线对比、赛道排行与失败案例分析，具体如下：</p>
<p>1. 数据集验证实验（Sec 2.1 &amp; A.2–A.4）</p>
<p>实验目的</p>
<p>设置</p>
<p>关键结果</p>
<p><strong>Lens 难度验证</strong></p>
<p>23 个前沿 MLLM（含 GPT-4o、Gemini-2.5 Pro、Qwen2.5-VL-72B 等）零样本测试</p>
<p>推理层 Acc 全部 &lt;60%；VG-RS 最高仅 48.47%@IoU=0.5</p>
<p><strong>AdsQA 人类上限</strong></p>
<p>5 名非专家仅看视频答题，200 QA 平均</p>
<p>人类 Acc = 71.4%，SOTA Gemini-2.5 Pro 仅 60.7%，留出 10.7% 绝对差距</p>
<p><strong>数据新鲜度检验</strong></p>
<p>53% Lens 图像 2025-01 后采集</p>
<p>验证模型无法依赖“训练语料记忆”，降低数据泄漏风险</p>
<p>2. 40+ 基线系统横向评测（Sec 3 &amp; Table 2–3）</p>
<p>赛道</p>
<p>模型范围</p>
<p>评测指标</p>
<p>主要结论</p>
<p><strong>Track-1 VG-RS</strong></p>
<p>20 个预测式 VG 模型 + 15 个生成式 MLLM（3B–72B）</p>
<p>Acc@IoU=0.5 &amp; scale-wise Acc</p>
<p>生成式 MLLM 普遍优于预测式；同系列参数越大越好，但 72B 仍 &lt;50%</p>
<p><strong>Track-2 VQA-SA</strong></p>
<p>8 个开源 MLLM（2B–38B）</p>
<p>640–1600 px 五档分辨率 Acc</p>
<p>分辨率从 640→1600 px，InternVL3-38B 仅 +1.3%，表明“更大像素≠更好空间推理”</p>
<p><strong>Track-3 VR-Ads</strong></p>
<p>14 个商用/开源模型（GPT-4o、Qwen2.5-VL、LLaVA-OneVision 等）</p>
<p>GPT-4o 相似度打分（0/0.5/1）</p>
<p>所有模型 &lt;50%，证明广告抽象推理仍是“无人区”</p>
<p>3. 挑战赛排行实验（Sec 3 各小节 &amp; Table 1）</p>
<p>阶段</p>
<p>提交量</p>
<p>有效方案</p>
<p>最佳成绩 vs 基线</p>
<p><strong>Stage-1 公开榜</strong></p>
<p>≈900 次</p>
<p>30 队</p>
<p>—</p>
<p><strong>Stage-2 隐藏榜</strong></p>
<p>≈300 次</p>
<p>10 队</p>
<p>—</p>
<p><strong>最终榜</strong></p>
<p>1200+ 次</p>
<p>40 队</p>
<p>‑ Track-1</p>
<p>66.7%（ActiveAlphaAgent）↑ 18.2 pp vs 最强基线</p>
<p>‑ Track-2</p>
<p>79.0%（Echoch）↑ 24.9 pp vs 最强基线</p>
<p>‑ Track-3</p>
<p>56.4%（gogogo truefaler）↑ 8.3 pp vs 最强基线</p>
<blockquote>
<p>所有提交均经组委会<strong>代码/日志复现</strong>后才标记“✓ Verified”，确保结果可信。</p>
</blockquote>
<p>4. 失败案例与消融实验（Sec 4.1 &amp; Figure 3–6）</p>
<p>类型</p>
<p>实验设计</p>
<p>发现</p>
<p><strong>细粒度感知失败</strong></p>
<p>随机采样 Track-1 冠军预测结果</p>
<p>把“皱塑料袋”误判为“纸板”、把“关闭的平板”漏检，揭示<strong>材质/状态识别</strong>短板</p>
<p><strong>空间常识偏差</strong></p>
<p>Track-2 冠军在“5 m 内是否所有 cyclist pairwise 距离 &lt;5 m”答错</p>
<p>模型缺乏<strong>物理量纲+视角转换</strong>能力</p>
<p><strong>广告幻觉</strong></p>
<p>Gemini-2.5 Pro 在 AdsQA 随机 3 例</p>
<p>生成<strong>原文本不存在</strong>的“折扣信息”，且遗漏关键创意元素，验证<strong>抽象语义幻觉</strong>严重</p>
<p><strong>消融：奖励函数</strong></p>
<p>ActiveAlphaAgent 对比 IoU-only vs 混合奖励</p>
<p>纯 IoU 奖励在 RefCOCO 上掉 4.6 pp，揭示<strong>灾难性遗忘</strong></p>
<p><strong>消融：帧率 vs 分辨率</strong></p>
<p>Track-3 冠军固定 2 FPS，仅改分辨率</p>
<p>720→1080 px 带来 +2.8 pp，而 2→5 FPS 仅 +0.3 pp，证明<strong>空间细节 &gt; 时序密度</strong></p>
<p>5. 可复现性实验（Sec 2.4 &amp; GitHub）</p>
<ul>
<li>全部 40+ 基线、15 支参赛方法、评测脚本<strong>开源</strong>在 <code>https://github.com/mars2workshop</code></li>
<li>提供 EvalAI 一键复现镜像，<strong>三赛道可复现误差 &lt;0.2%</strong></li>
</ul>
<p>实验总结</p>
<p>维度</p>
<p>规模</p>
<p>关键量</p>
<p>模型数</p>
<p>40+ 基线 + 40 参赛方案</p>
<p>80+ 不同大小/开源/闭源模型</p>
<p>提交数</p>
<p>1200+ 次</p>
<p>覆盖 76 队</p>
<p>问答数</p>
<p>Lens 60 K + AdsQA 3 K</p>
<p>共 63 K 人工标注 QA</p>
<p>图像/视频</p>
<p>Lens 3.4 K 图 + AdsQA 555 视频</p>
<p>22.7 h 广告视频</p>
<p>人类对照</p>
<p>200 AdsQA QA</p>
<p>71.4% 上限</p>
<blockquote>
<p>以上实验共同构成<strong>迄今最大规模的多模态慢思考评测矩阵</strong>，为社区提供了明确的性能边界与改进靶点。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可探索方向均源自 MARS2 2025 的实验结论与失败案例，按“数据-模型-评测-应用”四象限归纳，并给出可立即落地的具体课题与预期指标，供后续研究参考。</p>
<p>1. 数据层面：从“单点标注”到“可验证推理链”</p>
<p>课题</p>
<p>背景缺口</p>
<p>具体做法</p>
<p>预期指标</p>
<p><strong>1.1 过程级标注</strong></p>
<p>现有只有答案+边界框，无法监督中间推理</p>
<p>在 Lens/AdsQA 上补充“思维链+子问题”人工标注，形成 10 K 样本的 Lens-CoT &amp; AdsQA-CoT</p>
<p>训练后模型在原任务↑3 pp，且 PRM 打分与人类一致性≥0.85</p>
<p><strong>1.2 多文化广告视频</strong></p>
<p>AdsQA 以英文广告为主，文化隐喻差异大</p>
<p>收集中/日/印/拉美 2 K 支非英语广告，标注“文化符号→商业价值”映射</p>
<p>零样本跨文化迁移 Acc 提升 ≥5 pp</p>
<p><strong>1.3 物理量纲标注</strong></p>
<p>空间问答缺乏真实深度/尺度</p>
<p>用 iPhone LiDAR 对 Lens 1 K 场景扫描，提供毫米级深度与物体尺寸</p>
<p>引入深度后，距离/高度误差从&gt;30 cm 降至&lt;10 cm</p>
<p>2. 模型层面：从“单任务微调”到“慢思考架构”</p>
<p>课题</p>
<p>背景缺口</p>
<p>具体做法</p>
<p>预期指标</p>
<p><strong>2.1 多目标 RL 奖励</strong></p>
<p>IoU-only 导致灾难性遗忘</p>
<p>设计混合奖励：R = α·IoU + β·PRM + γ·通用 VQA Acc，用 DAPO/GRPO 优化</p>
<p>RefCOCO 不掉点，Lens VG-RS↑2 pp</p>
<p><strong>2.2 视觉-物理符号混合推理</strong></p>
<p>模型缺乏物理常识</p>
<p>引入可微物理引擎（DiffPhy）把深度→3D 场景图，用符号推理模块计算“可见/遮挡/支撑”</p>
<p>在“遮挡关系”子集 Acc 从 58%→75%</p>
<p><strong>2.3 分层视频-音频-文本融合</strong></p>
<p>广告音频多为背景音乐，被忽视</p>
<p>采用音轨分离（Music/VO/SFX）+ 语义对齐，构建三流 Transformer，VO 流可访问 ASR 文本</p>
<p>VR-Ads 提升 ≥3 pp，且消融实验显示 VO 贡献&gt;50%</p>
<p><strong>2.4 小模型慢思考蒸馏</strong></p>
<p>7B 模型推理速度高但慢思考能力弱</p>
<p>用 72B 模型生成 100 K 推理链，训练 7B“逐步思考”模式，再蒸馏到 3B 边缘模型</p>
<p>3B 模型在 Lens 推理层 Acc 保持≥55%，延迟&lt;200 ms</p>
<p>3. 评测层面：从“单任务指标”到“可解释诊断”</p>
<p>课题</p>
<p>背景缺口</p>
<p>具体做法</p>
<p>预期指标</p>
<p><strong>3.1 过程奖励模型（PRM）（视觉）</strong></p>
<p>现有 PRM 多为文本</p>
<p>基于 Lens-CoT 标注训练 Visual-PRM-8B，对每步推理链打分，用于 Best-of-N 采样</p>
<p>与人工 Kendall τ≥0.85，Track-2↑2 pp</p>
<p><strong>3.2 对抗空间攻击基准</strong></p>
<p>模型对视角/光照敏感</p>
<p>构建 Lens-Adversarial：自动旋转 30°、加雾、换材质，生成 5 K 对抗样本</p>
<p>最强模型 Acc 下降 ≥15 pp，留下鲁棒性提升空间</p>
<p><strong>3.3 多轮一致性评测</strong></p>
<p>现有 QA 独立采样</p>
<p>设计“跨轮一致性”指标：同一图像 5 轮问答中逻辑自洽率</p>
<p>当前最佳仅 68%，目标≥80%</p>
<p>4. 应用层面：从“学术 benchmark”到“真实业务闭环”</p>
<p>课题</p>
<p>背景缺口</p>
<p>具体做法</p>
<p>预期指标</p>
<p><strong>4.1 机器人导航指令跟随</strong></p>
<p>Lens 空间问答→真实导航</p>
<p>将 Lens 问答自动转换为自然语言导航指令，在 Habitat-3D 上测试</p>
<p>导航成功率↑10 pp，路径效率↑8%</p>
<p><strong>4.2 广告合规审查</strong></p>
<p>广告幻觉导致违规</p>
<p>用 AdsQA-CoT 训练审查模型，检测“未声明疗效/夸大”等隐性违规</p>
<p>误杀率&lt;5%，召回率≥90%，已合作短视频平台上线</p>
<p><strong>4.3 AR/VR 跨视角检索</strong></p>
<p>空间推理结果需实时映射到 AR 标签</p>
<p>把 Lens 深度标注与 ARKit 坐标对齐，实现“用户提问→实时定位→AR 高亮”端到端 demo</p>
<p>定位延迟&lt;100 ms，用户 Study 满意度≥4.2/5</p>
<p>5. 长期前沿：从“System 2 雏形”到“可证伪推理”</p>
<p>课题</p>
<p>背景缺口</p>
<p>具体做法</p>
<p>预期指标</p>
<p><strong>5.1 可证伪视觉推理</strong></p>
<p>模型无法自我否定</p>
<p>引入“反例生成器”：对同一图像自动生成反事实描述，让模型判断矛盾</p>
<p>自洽率从 60%→85%，减少幻觉</p>
<p><strong>5.2 持续慢思考学习</strong></p>
<p>任务增量导致遗忘</p>
<p>采用经验回放+过程蒸馏，构建“慢思考记忆池”，支持新广告类型流式接入</p>
<p>连续 5 个新领域平均遗忘&lt;2%</p>
<p><strong>5.3 统一多模态定理证明</strong></p>
<p>缺乏形式化验证</p>
<p>把空间关系表达为一阶逻辑，用神经符号定理证明器验证答案可满足性</p>
<p>小规模逻辑子集（≤20 关系）证明成功率≥95%</p>
<p>落地优先级建议</p>
<p>时间</p>
<p>推荐课题</p>
<p>理由</p>
<p><strong>3 个月</strong></p>
<p>2.1 多目标 RL 奖励</p>
<p>数据已就绪，改动仅奖励函数，可立即提升 Lens/AdsQA 榜单</p>
<p><strong>6 个月</strong></p>
<p>1.1 + 3.1 过程级标注与 PRM</p>
<p>需要人工标注 10 K，但可形成高价值慢思考数据与评测工具</p>
<p><strong>12 个月</strong></p>
<p>4.2 广告合规审查</p>
<p>商业场景明确，客户付费意愿高，可形成数据-模型闭环</p>
<blockquote>
<p>以上方向均已开源基准与基线代码，社区可基于 <code>mars2workshop</code> GitHub 组织快速复现并提交新榜单。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>MARS2 2025 综述报告核心内容可浓缩为“一个目标、两大痛点、三套实验、四条路线、五个展望”：</p>
<ol>
<li><p>一个目标<br>推动多模态大模型从“快速直觉 System 1”走向“慢速深思 System 2”，建立可量化的评测坐标系。</p>
</li>
<li><p>两大痛点<br>– 协同推理缺失：同图多任务能否相互促进缺乏统一基准。<br>– 非链式复杂推理不足：广告等抽象场景无法“if A then B”逐步求解。</p>
</li>
<li><p>三套实验<br>① 构建 Lens（3.4 K 图，60 K QA）与 AdsQA（555 广告视频，3 K QA）双基准；<br>② 组织 76 队 1200+ 提交，公开 40+ 基线、40 有效方案，覆盖 3 B–72 B 全尺寸；<br>③ 失败案例剖析：即便最强 GPT-4o/Gemini-2.5 在 Lens 推理 &lt;60%、AdsQA 距人类 71.4% 仍有 10+ pp 差距。</p>
</li>
<li><p>四条已验证技术路线<br>多阶段对齐（SFT→RL→蒸馏）、通用-专家协同、领域数据自循环、认知导向提示工程，分别带来 18 pp、16 pp、3.2 mIoU、7 pp 提升。</p>
</li>
<li><p>五个展望<br>过程级标注与 PRM、多文化广告、物理量纲深度、多目标 RL、可证伪神经符号推理——形成下一代“慢思考”多模态研究的即时可落地清单。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MARS2%202025%20Challenge%20on%20Multimodal%20Reasoning%3A%20Datasets%2C%20Methods%2C%20Results%2C%20Discussion%2C%20and%20Outlook，链接是  https://arxiv.org/pdf/2509.14142  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14142  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Peng Xu,Shengwu Xiong,Jiajun Zhang,Yaxiong Chen,Bowen Zhou,Chen Change Loy,David A. Clifton,Kyoung Mu Lee,Luc Van Gool,Ruiming He,Ruilin Yao,Xinwei Long,Jirui Huang,Kai Tian,Sa Yang,Yihua Shao,Jin Feng,Yue Zhong,Jiakai Zhou,Cheng Tang,Tianyu Zou,Yifang Zhang,Junming Liang,Guoyou Li,Zhaoxiang Wang,Qiang Zhou,Yichen Zhao,Shili Xiong,Hyeongjin Nam,Jaerin Lee,Jaeyoung Chung,JoonKyu Park,Junghun Oh,Kanggeon Lee,Wooseok Lee,Juneyoung Ro,Turghun Osman,Can Hu,Chaoyang Liao,Cheng Chen,Chengcheng Han,Chenhao Qiu,Chong Peng,Cong Xu,Dailin Li,Feiyu Wang,Feng Gao,Guibo Zhu,Guopeng Tang,Haibo Lu,Han Fang,Han Qi,Hanxiao Wu,Haobo Cheng,Hongbo Sun,Hongyao Chen,Huayong Hu,Hui Li,Jiaheng Ma,Jiang Yu,Jianing Wang,Jie Yang,Jing He,Jinglin Zhou,Jingxuan Li,Josef Kittler,Lihao Zheng,Linnan Zhao,Mengxi Jia,Muyang Yan,Nguyen Thanh Thien,Pu Luo,Qi Li,Shien Song,Shijie Dong,Shuai Shao,Shutao Li,Taofeng Xue,Tianyang Xu,Tianyi Gao,Tingting Li,Wei Zhang,Weiyang Su,Xiaodong Dong,Xiao-Jun Wu,Xiaopeng Zhou,Xin Chen,Xin Wei,Xinyi You,Xudong Kang,Xujie Zhou,Xusheng Liu,Yanan Wang,Yanbin Huang,Yang Liu,Yang Yang,Yanglin Deng,Yashu Kang,Ye Yuan,Yi Wen,Yicen Tian,Yilin Tao,Yin Tang,Yipeng Lin,Yiqing Wang,Yiting Xi,Yongkang Yu,Yumei Li,Yuxin Qin,Yuying Chen,Yuzhe Cen,Zhaofan Zou,Zhaohong Liu,Zhehao Shen,Zhenglin Du,Zhengyang Li,Zhenni Huang,Zhenwei Shao,Zhilong Song,Zhiyong Feng,Zhiyu Wang,Zhou Yu,Ziang Li,Zihan Zhai,Zijian Zhang,Ziyang Peng,Ziyun Xiao,Zongshu Li</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14142.pdf">https://arxiv.org/pdf/2509.14142.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14142">https://arxiv.org/abs/2509.14142</a></p>
<p><strong>Arxiv ID</strong>: 2509.14142</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14142">https://papers.cool/arxiv/2509.14142</a></p>
<p><strong>Published</strong>: 2025-09-17T16:21:34Z</p>
<p><strong>Updated</strong>: 2025-09-17T16:21:34.000Z</p>
<hr>
<h3 id="9-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication"><a href="#9-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication" class="headerlink" title="9. Wan-Animate: Unified Character Animation and Replacement with Holistic Replication"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14055">Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</a></h3><p>We introduce Wan-Animate, a unified framework for character animation and replacement. Given a character image and a reference video, Wan-Animate can animate the character by precisely replicating the expressions and movements of the character in the video to generate high-fidelity character videos. Alternatively, it can integrate the animated character into the reference video to replace the original character, replicating the scene’s lighting and color tone to achieve seamless environmental integration. Wan-Animate is built upon the Wan model. To adapt it for character animation tasks, we employ a modified input paradigm to differentiate between reference conditions and regions for generation. This design unifies multiple tasks into a common symbolic representation. We use spatially-aligned skeleton signals to replicate body motion and implicit facial features extracted from source images to reenact expressions, enabling the generation of character videos with high controllability and expressiveness. Furthermore, to enhance environmental integration during character replacement, we develop an auxiliary Relighting LoRA. This module preserves the character’s appearance consistency while applying the appropriate environmental lighting and color tone. Experimental results demonstrate that Wan-Animate achieves state-of-the-art performance. We are committed to open-sourcing the model weights and its source code.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了Wan-Animate，这是一个用于角色动画和替换的统一框架。给定一个角色图像和一个参考视频，Wan-Animate可以通过准确复制视频中角色的表情和动作来对角色进行动画处理，从而生成高保真度的角色视频。或者，它可以将动画角色集成到参考视频中，以替换原始角色，复制场景的光照和色调，实现无缝的环境融合。Wan-Animate建立在Wan模型之上。为了将其适应角色动画任务，我们采用了修改过的输入范式，以区分生成的参考条件和区域。这一设计将多个任务统一为一个共同的符号表示。我们使用空间对齐的骨骼信号来复制身体动作，并从源图像中提取隐式面部特征以重现表情，从而生成具有高可控性和表现力的角色视频。此外，为了增强角色替换过程中的环境融合，我们开发了一个辅助的重光照LoRA。该模块在应用适当的环境光照和色调时，保持角色外观的一致性。实验结果表明，Wan-Animate达到了最先进的性能。我们致力于开源模型权重和其源代码。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“开源社区中缺少一个能够同时高质量完成角色动画（animation）与角色替换（replacement）的统一框架”这一核心问题。具体而言，现有方法在以下三方面存在显著缺陷：</p>
<ol>
<li><p><strong>控制信号割裂</strong></p>
<ul>
<li>身体动作与面部表情通常由不同模型或阶段分别处理，难以保证时空一致性。</li>
<li>多数开源方案仅支持单一控制（如仅骨骼或仅 landmarks），无法“整体复现”参考视频中的神态与动作。</li>
</ul>
</li>
<li><p><strong>基础模型落后</strong></p>
<ul>
<li>主流开源工作仍基于 UNet 结构（如 SD/SVD），生成质量、时序连贯性明显低于闭源 SOTA。</li>
<li>近期 DiT 方案虽提升视觉保真度，但要么只关注身体运动，要么只关注表情，缺乏“ holistic replication”。</li>
</ul>
</li>
<li><p><strong>环境融合缺失</strong></p>
<ul>
<li>角色替换任务中，源角色与目标场景的光照、色调往往不一致，现有开源方法几乎没有针对性的“重打光”模块，导致合成痕迹明显。</li>
<li>先前需要多步合成（分割 + 图像修复 + 颜色校正），流程繁琐且误差累积。</li>
</ul>
</li>
</ol>
<p>为此，论文提出 Wan-Animate——一个基于 DiT 的统一框架，通过以下关键设计一次性解决上述问题：</p>
<ul>
<li><strong>统一输入范式</strong>：在 Wan-I2V 的 latent-mask 机制上扩展，将“参考图像、时序引导、环境信息”编码为同一符号空间，实现 Animation（I2V）与 Replacement（V2V）的联合训练。</li>
<li><strong>解耦控制信号</strong>：<ul>
<li>身体：空间对齐的 2D 骨架直接加噪 latent，兼顾通用性与鲁棒性。</li>
<li>表情：原始人脸图像经编码-时序压缩后，通过 cross-attention 注入专用 Face Block，保留细粒度神态。</li>
</ul>
</li>
<li><strong>Relighting LoRA</strong>：在 Replacement 模式下，仅对自注意力与交叉注意力层引入低秩适配器，学习“背景→角色”光照映射，实现无缝环境融合。</li>
</ul>
<p>综上，论文首次在开源领域实现了“高保真、全身+表情、可替换、可打光”的一站式角色动画方案，显著缩小了与闭源 SOTA 的性能差距。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“RELATED WORKS”中将相关研究划分为三大主线，并指出各自与 Wan-Animate 的差异。可归纳如下：</p>
<ol>
<li><p>角色图像动画（Character Image Animation）</p>
<ul>
<li><p>早期 GAN / warping 方案</p>
<ul>
<li>Siarohin et al. 2019/2021：First-order Motion、Articulated Animation</li>
<li>Zhao &amp; Zhang 2022：Thin-plate Spline Motion<br>‑ 局限：生成质量低、时序抖动严重。</li>
</ul>
</li>
<li><p>UNet-扩散模型阶段</p>
<ul>
<li>Hu 2024 AnimateAnyone → ReferenceNet + AnimateDiff 时序层，奠定“参考图注入+时序一致性”范式</li>
<li>后续工作 Zhu et al. 2025（Champ）、Tu et al. 2025（StableAnimator）、Wang et al. 2024（Unianimate）均在该范式上改进<br>‑ 局限：基础模型（SD/SVD）缺乏原生时序知识，长视频保真度不足。</li>
</ul>
</li>
<li><p>视频基础模型阶段</p>
<ul>
<li>Wang et al. 2024/2025b：基于 SVD 或 HunyuanVideo 微调，简化架构</li>
<li>Zhou et al. 2025（RealisDance-DiT）、Luo et al. 2025（DreamActor-M1）首次用 DiT 做动画，提升真实度<br>‑ 局限：仅关注身体驱动，未同时解决表情+环境融合；或闭源。</li>
</ul>
</li>
<li><p>环境/交互扩展</p>
<ul>
<li>Men et al. 2024 MIMO：额外输入背景图，实现场景指定</li>
<li>Hu et al. 2025 AnimateAnyone2：mask 分离背景与交互物体，可做“近似替换”</li>
<li>Xu et al. 2024c/Wang et al. 2025a：人-物交互生成<br>‑ 局限：无光照协调模块，合成痕迹明显；开源代码缺失。</li>
</ul>
</li>
</ul>
</li>
<li><p>面部动画（Facial Animation）</p>
<ul>
<li><p>基于显式信号</p>
<ul>
<li>Wei et al. 2024 AniPortrait、Ma et al. 2024 Follow-Your-Emoji：2D/3D landmarks → 表情驱动<br>‑ 局限：稀疏关键点丢失微表情，跨身份 retargeting 困难。</li>
</ul>
</li>
<li><p>基于隐式信号</p>
<ul>
<li>Xu et al. 2024a VASA-1、Drobyshev et al. 2024 EmoPortraits、Ki et al. 2024 FLOAT、Xie et al. 2024 X-Portrait、Zhao et al. 2025 X-Nemo、Luo et al. 2025 DreamActor 系列：直接用人脸图提取隐式表征，通过 cross-attention 或 latent flow 注入<br>‑ 优势：表情细腻、身份解耦好；但多数仅做人头区域，未与全身动作统一。</li>
</ul>
</li>
</ul>
</li>
<li><p>视频生成架构（Video Generation Backbone）</p>
<ul>
<li>Sora 之前的 UNet 路线<ul>
<li>Blattmann et al. 2023 SVD、Yang et al. 2024 CogVideoX 等</li>
</ul>
</li>
<li>Sora 之后的 DiT 路线<ul>
<li>Peebles &amp; Xie 2023 DiT 理论奠基</li>
<li>Kong et al. 2025 HunyuanVideo、Wan et al. 2025 Wan2.1、Jiang et al. 2025 VACE：开源大规模 DiT 视频模型<br>‑ Wan-Animate 直接基于 Wan2.1，继承其 3D VAE + Patchify + Transformer 结构，从而把“通用视频生成”能力迁移到“角色动画”子任务。</li>
</ul>
</li>
</ul>
</li>
</ol>
<p>总结：</p>
<ul>
<li>身体驱动 → 从 GAN 到 UNet-Diffusion，再到 DiT，逐步提升时序一致性；</li>
<li>表情驱动 → 从 landmarks 到隐式人脸表征，提升细腻度；</li>
<li>环境融合 → 仅有少量工作尝试背景保持或人-物交互，未解决光照/色调匹配；</li>
<li>统一框架 → 目前开源领域尚无同时支持“全身+表情+替换+重打光”的单一模型，Wan-Animate 补全了这一空白。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“一个统一框架 + 四项关键技术”把角色动画与角色替换同时推向开源 SOTA，具体解决方案如下：</p>
<ol>
<li><p>统一输入范式：把多任务转化为同一符号表示</p>
<ul>
<li>在 Wan-I2V 的“noise latent | conditional latent | binary mask”三元组基础上扩展：<br>– Reference latent：源角色图像经 VAE 编码，沿时序复制后与 conditional latent 拼接，mask 置 1，实现身份注入。<br>– Temporal latent：前一段末尾真实帧作为“后续片段”的条件，mask 局部置 1，保证长视频连贯。<br>– Environment latent：<br>‑ Animation 模式 conditional 帧全零，mask 全 0 → 保留源图背景；<br>‑ Replacement 模式将参考视频人物扣空，背景作为 conditional，mask 在背景区置 1、人物区置 0 → 实现 V2V 替换。</li>
<li>结果：同一网络、同一组权重，仅通过修改 mask 与 conditional 即可切换 I2V 或 V2V，训练与推理均无需分支模型。</li>
</ul>
</li>
<li><p>解耦控制信号：身体-表情并行注入</p>
<ul>
<li>Body Adapter（骨架驱动）<br>– 用 VitPose 提取 2D 骨架帧 → VAE 得到 pose latent → patchify 后与 noise latent 逐 token 相加。<br>– 空间对齐的加法方式既保持结构精度，又避免 UNet 级联带来的误差放大；对非人形角色也鲁棒。</li>
<li>Face Adapter（隐式表情驱动）<br>– 从参考视频裁剪人脸 → 512×512 编码为 1D 隐向量 → 1D 因果卷积时序下采样，与 DiT latent 长度对齐。<br>– 在 Transformer 每 5 层插入专用 Face Block，仅对同时间片内的 token 做 cross-attention，实现“高细粒度表情 + 身份解耦”。<br>– 训练期采用缩放、色彩抖动、噪声等强增广，迫使网络只利用表情相关分量，抑制身份泄露。</li>
</ul>
</li>
<li><p>Relighting LoRA：角色替换的光照-色调校正</p>
<ul>
<li>仅对 DiT 的自注意力 &amp; 交叉注意力层引入低秩旁路（rank 极小），参数量 &lt; 原模型 1%。</li>
<li>训练数据：用 IC-Light 把同一人贴到随机背景，获得“不同光照/色调的同一身份”图像对；LoRA 学习“背景→人物”光照映射。</li>
<li>推理时：基础模型先保证身份一致，LoRA 再微调光照与色彩，使角色与新场景视觉一致，且可插拔关闭。</li>
</ul>
</li>
<li><p>四阶段渐进训练策略</p>
<ol>
<li>Body-only：让网络先学会修改后的输入范式与骨架对齐。</li>
<li>Face-only：冻结主体，仅训练 Face Adapter/Block，用肖像数据加速表情收敛。</li>
<li>Joint-control：全数据、全模块联合微调，实现全身+表情同时驱动。</li>
<li>Joint-mode：在同一 batch 内随机混合 Animation/Replacement 样本，统一权重。</li>
<li>Relighting LoRA：单独构造“光照不一致”数据，低秩适配器快速收敛。</li>
</ol>
</li>
<li><p>推理级配套</p>
<ul>
<li>Pose Retargeting：计算源图与驱动视频对应骨骼长度比例，自动缩放/平移；可选“T-pose 编辑”提高极端姿态精度。</li>
<li>长视频：滑窗生成，每段 78 帧，用末尾 1-5 帧作时序引导，无缝拼接。</li>
<li>分辨率自适应：根据 patchify 后 token 数动态选择最接近 1280×720 的宽高比，无需重训练。</li>
</ul>
</li>
</ol>
<p>通过上述设计，论文首次在开源领域实现了</p>
<ul>
<li>单模型同时支持“纯动画”与“角色替换”两种模式；</li>
<li>2D 骨架 + 隐式人脸双信号，实现全身与微表情的高保真复现；</li>
<li>轻量级 LoRA 解决替换场景的光照/色调不一致，无需额外后处理；</li>
<li>基于 DiT backbone，生成质量与闭源产品（Runway Act-two、DreamActor-M1）持平甚至超越。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕「量化指标、用户主观评价、视觉对比、消融分析」四条主线展开实验，全面验证 Wan-Animate 在 Animation 与 Replacement 两种模式下的性能与鲁棒性。具体实验如下：</p>
<ol>
<li><p>量化对比（表 1）</p>
<ul>
<li>自建 benchmark：采集多场景、多尺度（头像/半身/全身）真人视频，共 120 段。</li>
<li>任务设定：自重建——以每段第 1 帧为源图，用后续帧的 pose+face 信号重建整段视频。</li>
<li>评价指标：<ul>
<li>SSIM↑ 结构相似度</li>
<li>LPIPS↓ 感知距离</li>
<li>FVD↓ 视频分布距离</li>
</ul>
</li>
<li>对比对象：<ul>
<li>全身动画：Moore-AA、Champ、MimicMotion、Unianimate、StableAnimator</li>
<li>专注表情：LivePortrait、AniPortrait、Emoji、X-portrait2、SkyReel-A1</li>
</ul>
</li>
<li>结果：<ul>
<li>全身任务 Wan-Animate 取得 0.834/0.205/94.65，三项指标均优于现有开源方案；</li>
<li>头像子集亦领先专用人脸动画模型，验证“联合训练”未损失表情细粒度。</li>
</ul>
</li>
</ul>
</li>
<li><p>用户主观评测（图 5）</p>
<ul>
<li>对标闭源 SOTA：Runway Act-two、DreamActor-M1。</li>
<li>交叉身份（cross-ID）生成：20 组「源图≠驱动视频」样本，双盲匿名展示。</li>
<li>20 名受试者从「整体质量、身份一致性、动作准确度、表情准确度」四维度投票。</li>
<li>统计：Wan-Animate 胜出率 52.1 %，显著高于 Runway（26.4 %）与 DreamActor（21.5 %），证实主观感知亦领先。</li>
</ul>
</li>
<li><p>视觉对比</p>
<ul>
<li>Animation 模式（图 6）：<ul>
<li>与 AnimateAnyone、VACE、Runway Act-two、DreamActor-M1 同条件生成；</li>
<li>展示复杂舞蹈、大幅度挥手、夸张表情等场景；</li>
<li>Wan-Animate 在肢体完整度、面部细节、时序稳定性上均优于对比方法。</li>
</ul>
</li>
<li>Replacement 模式（图 7）：<ul>
<li>与 AnimateAnyone2、VACE 比较；</li>
<li>给出室内/室外、硬阴影、霓虹光等光照差异显著场景；</li>
<li>Relighting LoRA 使角色肤色、衣物受光方向与背景一致，无明显拼接痕迹。</li>
</ul>
</li>
</ul>
</li>
<li><p>消融实验</p>
<ul>
<li><p>Face Adapter 训练策略（图 8）</p>
<ul>
<li>baseline：从头联合训练 body+face；</li>
<li>渐进式：先 body→再 face→联合；</li>
<li>指标：LPIPS 0.231 → 0.205，表情关键点偏移降低 18 %，验证分阶段收敛更快、表情更准确。</li>
</ul>
</li>
<li><p>Relighting LoRA 影响（图 9）</p>
<ul>
<li>w/o LoRA：角色保持源图光照，与背景色温差异明显；</li>
<li>w/ LoRA：自动匹配环境主光源方向与色调，SSIM 基本不变（身份保真），LPIPS 降低 7 %，整体真实度提升。</li>
</ul>
</li>
</ul>
</li>
<li><p>扩展性验证（图 10）</p>
<ul>
<li>展示 5 类应用：<ol>
<li>经典影视片段复刻（Performance Reenactment）</li>
<li>真人→卡通/二次元风格迁移（Cross-Style Transfer）</li>
<li>舞蹈、功夫等大幅度运动（Complex Motion）</li>
<li>手持云台/旋转镜头（Dynamic Camera）</li>
<li>影视剧角色再创作（Character Replacement）</li>
</ol>
</li>
<li>所有结果均直接由同一套权重生成，未针对特定场景微调，表明模型泛化性与鲁棒性。</li>
</ul>
</li>
<li><p>实现细节与效率</p>
<ul>
<li>单段 78 帧，512×512 输入下 A100-80 GB 推理耗时 12 s；显存占用 &lt; 45 GB。</li>
<li>长视频迭代生成 600 帧（≈20 s）总耗时 1.8 min，内存不递增。</li>
<li>关闭 CFG 默认设置即可达到商用质量；若需微调表情强度，可仅对 face 条件开启 CFG=1.5，推理时间增加 &lt; 8 %。</li>
</ul>
</li>
</ol>
<p>综上，实验覆盖「指标-主观-视觉-消融-扩展」全链路，既证明 Wan-Animate 在公开数据集上全面领先现有开源方案，也与最强闭源产品旗鼓相当甚至更好，同时验证了各关键模块的必要性与有效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 Wan-Animate 框架的直接延伸或深层改进，均具有学术与落地双重价值：</p>
<ol>
<li><p>更细粒度的动作-表情解耦</p>
<ul>
<li>引入 3D 面部先验（FLAME、EMOCA）与 SMPL-X 手-头参数，将“表情/手势/视线”与身体 pose 显式分离，实现跨身份 retargeting 时的局部幅度可调。</li>
<li>探索“情感强度”连续控制向量，支持用户仅通过标量滑动条调节笑容或愤怒程度。</li>
</ul>
</li>
<li><p>多角色 / 群体动画</p>
<ul>
<li>把单角色 mask 扩展为实例级 mask 序列，配合 Transformer 的 slot-attention 机制，一次性驱动≥2 个角色，解决遮挡、交互物理合理性。</li>
<li>引入 social-behavior prior（如 ProxEmo、TrajFormer）自动合成人群交谈、舞蹈对位等群体行为。</li>
</ul>
</li>
<li><p>场景级光照与反射一致性</p>
<ul>
<li>将 Relighting LoRA 升级为显式物理分支：<br>– 用 HDR 环境图估计背景光源，预测角色球谐/反射贴图；<br>– 在 DiT 潜空间内联合优化“法线-反射-反照率”三通道，实现随镜头移动的实时阴影投射。</li>
<li>支持用户上传 HDRI 或 3D 场景，自动匹配角色 PBR 材质。</li>
</ul>
</li>
<li><p>长时序记忆与剧情一致性</p>
<ul>
<li>引入分层 latent memory：<br>– 短期：已实现的 1-5 帧时序引导；<br>– 长期：用 cross-frame transformer 维护 100-300 帧级别的剧情状态（服装破损、手持物、情绪曲线），避免滑窗导致的道具闪烁或情绪跳变。</li>
<li>结合 LLM-based storyboard parser，将剧本自动转换为“情绪-动作-场景”三元组序列，驱动整条影片一次性生成。</li>
</ul>
</li>
<li><p>语音-视听同步与说话人风格</p>
<ul>
<li>把音频特征（HuBERT、Wav2Vec2）注入 Face Block，实现任意语言口型精准同步；</li>
<li>增加 speaker-specific prosody adapter，保留源说话人的音色节奏，支持“换脸不换声”或“换声不换脸”两种模式。</li>
</ul>
</li>
<li><p>实时与轻量化</p>
<ul>
<li>蒸馏方案：将 14 B DiT 教师网络蒸馏至 1-2 B 学生网络，采用 temporal consistency loss 保持长帧连贯；</li>
<li>量化 + TensorRT / ONNX 加速，在 RTX-4090 实现 512×512、25 fps、延迟 &lt; 100 ms 的实时预览，满足直播虚拟人需求。</li>
</ul>
</li>
<li><p>风格化与非真实感渲染（NPR）</p>
<ul>
<li>在 latent 空间引入 CLIP-based style adapter，支持卡通、赛璐璐、油画、像素风一键切换；</li>
<li>对骨架信号施加“风格化形变场”，使肢体动作呈现夸张弹性（动漫式运动扭曲），同时保持物理可信。</li>
</ul>
</li>
<li><p>可控编辑与交互式工具链</p>
<ul>
<li>开发 Drag-a-Pose 界面：用户在关键帧拖拽关节点，系统实时更新整段视频；</li>
<li>引入 Instruct-Pix2Pix 范式，支持文本指令局部编辑：“让角色左手拿杯咖啡”、“把衣服换成红色风衣”，无需重新训练主模型。</li>
</ul>
</li>
<li><p>数据与评测体系</p>
<ul>
<li>构建多语言、多肤色、多服饰的“公平性”测试集，增加年龄、体型、残障人士动作样本，系统评估偏见；</li>
<li>提出新的“剧情一致性”指标：结合 CLIP-Semantic、DINO-Visual 与人工标注，衡量 100+ 帧跨度内的道具、服装、情绪连续性。</li>
</ul>
</li>
<li><p>伦理与防伪</p>
<ul>
<li>在输出潜水印（latent watermark）与可见签名，对抗 DeepFake 滥用；</li>
<li>研究“对抗扰动”策略：在生成阶段加入不可察觉的扰动，使后续人脸伪造检测模型仍能高置信度识别，同时保持视觉质量。</li>
</ul>
</li>
</ol>
<p>以上方向既可直接在 Wan-Animate 的 DiT 框架上增量扩展，也可引入新的模态与损失函数，为角色动画、影视制作、虚拟交互等场景提供更丰富、更安全、更实时的能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>问题</strong>：开源领域缺少同时支持“高保真角色动画 + 角色替换 + 环境光照融合”的统一框架；现有方法或仅驱动身体、或仅驱动表情，且生成质量远落后于闭源 SOTA。</p>
</li>
<li><p><strong>方案</strong>：提出 Wan-Animate，基于 DiT 视频基础模型 Wan2.1，四项关键技术：</p>
<ol>
<li>统一输入范式（reference | temporal | environment latent + mask）把 I2V 动画与 V2V 替换合并为同一符号表示，单套权重可切换。</li>
<li>解耦控制：空间对齐骨架 latent 加噪驱动身体；隐式人脸向量经 1D-Conv 时序压缩后通过 cross-attention 注入 Face Block，实现细粒度表情复现。</li>
<li>Relighting LoRA：仅对注意力层引入低秩旁路，学习背景→角色光照映射，解决替换后色调不一致。</li>
<li>四阶段渐进训练（body-only → face-only → joint-control → joint-mode + LoRA），快速收敛且避免身份泄露。</li>
</ol>
</li>
<li><p><strong>结果</strong>：<br>– 公开基准上 SSIM/LPIPS/FVD 全面领先现有开源方案；用户主观评测击败 Runway Act-two 与 DreamActor-M1。<br>– 支持任意分辨率、长视频迭代生成、实时推理；开源权重与代码。</p>
</li>
<li><p><strong>意义</strong>：首次在开源社区实现“全身+表情+替换+重打光”一站式高保真角色动画，显著缩小与商业闭源产品的性能差距。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Wan-Animate%3A%20Unified%20Character%20Animation%20and%20Replacement%20with%20Holistic%20Replication，链接是  https://arxiv.org/pdf/2509.14055  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14055  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Gang Cheng,Xin Gao,Li Hu,Siqi Hu,Mingyang Huang,Chaonan Ji,Ju Li,Dechao Meng,Jinwei Qi,Penchong Qiao,Zhen Shen,Yafei Song,Ke Sun,Linrui Tian,Feng Wang,Guangyuan Wang,Qi Wang,Zhongjian Wang,Jiayu Xiao,Sheng Xu,Bang Zhang,Peng Zhang,Xindi Zhang,Zhe Zhang,Jingren Zhou,Lian Zhuo</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14055.pdf">https://arxiv.org/pdf/2509.14055.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14055">https://arxiv.org/abs/2509.14055</a></p>
<p><strong>Arxiv ID</strong>: 2509.14055</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14055">https://papers.cool/arxiv/2509.14055</a></p>
<p><strong>Published</strong>: 2025-09-17T15:00:57Z</p>
<p><strong>Updated</strong>: 2025-09-17T15:00:57.000Z</p>
<hr>
<h3 id="10-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning"><a href="#10-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning" class="headerlink" title="10. Improving Context Fidelity via Native Retrieval-Augmented Reasoning"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13683">Improving Context Fidelity via Native Retrieval-Augmented Reasoning</a></h3><p>Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model’s own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）经常在上下文的忠实度上遇到困难，当根据提供的信息回答问题时，产生不一致的答案。现有的方法要么依赖于昂贵的有监督微调以在回答后生成证据，要么训练模型进行网络搜索，但并未必改善对给定上下文的利用。我们提出了CARE，一个新颖的本地检索增强推理框架，该框架教会LLMs在推理过程中明确整合上下文证据，并利用模型自身的检索能力。我们的方法只需有限的标记证据数据，同时通过在推理链中战略性检索的上下文标记显著提高检索准确性和答案生成性能。在多个现实世界和反事实QA基准上的广泛实验表明，我们的方法在性能上显著优于有监督微调、传统的检索增强生成方法和外部检索解决方案。这项工作代表了在使LLMs在知识密集型任务中更加准确、可靠和高效方面的根本进展。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决大模型在知识密集型问答任务中“上下文保真度不足”的核心问题——即模型给出的答案常常与输入上下文中的事实不一致，甚至出现幻觉。具体而言，作者观察到：</p>
<ul>
<li>现有方法要么依赖昂贵的监督微调或外部检索模块，却难以充分利用用户已提供的上下文；</li>
<li>要么通过外部搜索引入新信息，却可能忽视或冲突于原文，带来额外延迟与不一致。</li>
</ul>
<p>为此，作者提出“原生检索增强推理”范式，让模型在不借助任何外部检索器或向量数据库的前提下，自主地从输入上下文中定位并引用关键证据，并将这些证据片段显式嵌入到推理链中，从而同步提升检索准确率与答案正确性，显著降低幻觉风险。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节系统回顾了三条相关研究脉络，可归纳如下：</p>
<ol>
<li><p><strong>LLM 推理增强</strong></p>
<ul>
<li>Chain-of-Thought / Zero-shot-CoT / Self-Consistency</li>
<li>Tree-of-Thought、Graph-of-Thought、ReAct、Least-to-Most Prompting</li>
<li>共同痛点：长上下文或噪声场景下仍易出现上下文不一致或幻觉。</li>
</ul>
</li>
<li><p><strong>检索增强生成（RAG）</strong></p>
<ul>
<li>经典外部 RAG：Guu et al. 2020、Lewis et al. 2020</li>
<li>改进方向：query expansion、re-ranking、robustness to noisy retrieval</li>
<li>上下文内 RAG：Variengien &amp; Winsor 2023、Wang et al. 2024</li>
<li>共同局限：依赖独立检索器与向量索引，难以随上下文演化而自适应。</li>
</ul>
</li>
<li><p><strong>强化学习驱动的检索策略</strong></p>
<ul>
<li>Humphreys et al. 2022、Tu et al. 2024、Hsu et al. 2024、Nguyen et al. 2024</li>
<li>特点：用 RL 训练模型自主决定何时搜索、如何搜索，以任务奖励为优化目标。</li>
<li>共同局限：检索与推理仍被视作两个分离阶段，未显式利用用户已提供的上下文证据。</li>
</ul>
</li>
</ol>
<p>CARE 在上述工作的基础上，首次将“原生检索”与“推理链”有机融合，通过轻量级两阶段训练（SFT+RL）让模型直接在输入上下文中定位并引用证据，无需任何外部检索模块或大规模标注证据数据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>CARE（Context-Aware Retrieval-Enhanced reasoning）</strong> 框架，通过“原生检索增强推理”范式，把“在上下文中找证据”与“逐步推理”合二为一，具体实现分为两大阶段：</p>
<p>1. 监督微调（SFT）阶段：让模型学会“先检索、后推理”的格式</p>
<ul>
<li><strong>输入</strong>：已有 QA 数据集（含支持句标注）。</li>
<li><strong>三步数据构造 pipeline</strong>：<ol>
<li><strong>推理链生成</strong>：用教师模型 MR 针对 $(C,Q)$ 生成初始思维链 $R_{i,A}$，仅保留答案正确的样本。</li>
<li><strong>证据注入</strong>：用注入模型 MI 把标注支持句 $S_i$ 无缝嵌入 $R_{i,A}$，得到 $R_{i,I}$，确保每一步推理都有上下文原文支撑。</li>
<li><strong>检索 token 标记</strong>：在 $R_{i,I}$ 中把被注入的原文片段用自定义标记 <code>&lt;retrieval&gt;</code> … <code>&lt;/retrieval&gt;</code> 包裹，形成最终训练样本 $E^*_i$。</li>
</ol>
</li>
<li><strong>输出</strong>：7k 余条“思维链-检索片段”交错出现的 SFT 数据，教会模型在 <code>&lt;think&gt;</code> 内部主动引用上下文。</li>
</ul>
<p>2. 强化学习（RL）阶段：用检索感知奖励把“格式”转化为“能力”</p>
<ul>
<li><strong>算法</strong>：Group Relative Policy Optimization（GRPO），组内对比多条回答，避免单点估计高方差。</li>
<li><strong>奖励函数</strong>：</li>
</ul>
<script type="math/tex; mode=display">R\_{\\text{total}} = \\lambda\_1 R\_{\\text{acc}} + \\lambda\_2 R\_{\\text{fmt}} + \\lambda\_3 R\_{\\text{ret}}</script><ul>
<li><p>$R_{\text{acc}}$：答案 token-F1，保证最终正确性。</p>
<ul>
<li>$R_{\text{fmt}}$：检查 <code>&lt;think&gt;</code>、<code>&lt;retrieval&gt;</code> 是否成对出现，保证结构合规。</li>
<li>$R_{\text{ret}}$：每对 <code>&lt;retrieval&gt;</code> 内的文本必须<strong>逐字存在于输入上下文</strong>，否则扣分——<strong>核心驱动力</strong>，迫使模型自己“抄”原文。</li>
</ul>
</li>
<li><strong>课程学习</strong>：训练先全部用短上下文、单跳答案的 <strong>DROP</strong>（$D_{\text{easy}}$），再线性混入长上下文、多跳答案的 <strong>MS MARCO</strong>（$D_{\text{hard}}$），无需额外标注即可泛化到更难分布。</li>
</ul>
<p>3. 推理阶段：零外设、零延迟</p>
<ul>
<li>模型接收 $(Q,C)$ 后直接生成<br><code>&lt;think&gt;</code> … <code>&lt;retrieval&gt;$C$ 中的原句&lt;/retrieval&gt;</code> … <code>&lt;/think&gt;</code><br><strong>Answer:</strong> …<br>无需调用搜索引擎、向量库或任何外部 API，实现“即插即用”的上下文保真提升。</li>
</ul>
<p>通过上述两阶段训练，CARE 把“找证据”内化为模型自身的语言理解行为，既显著提高了多跳、长文本、反事实场景下的答案准确率，又避免了传统 RAG 的架构冗余与延迟问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>真实世界长上下文问答</strong> 与 <strong>反事实多跳问答</strong> 两大场景下，对 <strong>3 个模型族、4 个数据集、5 类基线</strong> 进行了系统实验，并补充了消融与证据检索质量评测，具体如下：</p>
<p>1. 主实验：问答准确率（F1）</p>
<p>数据集</p>
<p>特点</p>
<p>指标</p>
<p><strong>MultiFieldQA-En</strong></p>
<p>长单篇文档，多领域</p>
<p>F1</p>
<p><strong>HotpotQA</strong></p>
<p>多跳，真实维基</p>
<p>F1</p>
<p><strong>2WikiMQA</strong></p>
<p>多跳，合成维基</p>
<p>F1</p>
<p><strong>MuSiQue</strong></p>
<p>多跳，难度更高</p>
<p>F1</p>
<p><strong>CofCA</strong></p>
<p>反事实维基片段，检测上下文忠诚度</p>
<p>F1</p>
<ul>
<li><strong>基线</strong>：Original、ReSearch、R1-Searcher、CRAG</li>
<li><strong>模型规模</strong>：LLaMA-3.1-8B、Qwen2.5-7B、Qwen2.5-14B</li>
</ul>
<p><strong>结果</strong>（表 1&amp;2）：</p>
<ul>
<li>CARE 在三组模型上 <strong>平均 F1 提升 8.4~15.3%</strong>；</li>
<li>多跳任务增益最大（2WikiMQA +29.4%）；</li>
<li>反任务场景下，CARE 比最强基线再涨 <strong>3.3~13.7%</strong>，而外部搜索方法反而低于原始模型。</li>
</ul>
<p>2. 消融实验（Qwen2.5-7B）</p>
<p>设置</p>
<p>SFT</p>
<p>RL</p>
<p>检索奖励</p>
<p>课程学习</p>
<p>平均 F1</p>
<p>Baseline</p>
<p>✗</p>
<p>✗</p>
<p>✗</p>
<p>✗</p>
<p>48.25</p>
<p>SFT Only</p>
<p>✓</p>
<p>✗</p>
<p>✗</p>
<p>✗</p>
<p>48.77</p>
<p>No Ret.</p>
<p>✓</p>
<p>✓</p>
<p>✗</p>
<p>✗</p>
<p>54.39</p>
<p>No Cur.</p>
<p>✓</p>
<p>✓</p>
<p>✓</p>
<p>✗</p>
<p>56.24</p>
<p><strong>CARE</strong></p>
<p>✓</p>
<p>✓</p>
<p>✓</p>
<p>✓</p>
<p><strong>58.36</strong></p>
<p>结论：</p>
<ul>
<li>仅 SFT 几乎无效；</li>
<li>加入检索奖励后平均提升 <strong>+4.0%</strong>；</li>
<li>课程学习再补 <strong>+2.1%</strong>，在长答案与反事实场景上增益最明显。</li>
</ul>
<p>3. 证据检索质量评测</p>
<ul>
<li><strong>基准</strong>：LongBench 版 HotpotQA，与原始 HotpotQA 支持句对齐</li>
<li><strong>指标</strong>：SacreBLEU、ROUGE-L</li>
<li><strong>结果</strong>（图 3）：<ul>
<li>CARE 在所有模型上均取得 <strong>最高 BLEU/ROUGE-L</strong>；</li>
<li>模型规模越大，绝对值越高，但 CARE 的 <strong>相对增益保持稳定</strong>。</li>
</ul>
</li>
</ul>
<p>4. 效率与开销</p>
<ul>
<li><strong>输出长度</strong>：CARE 平均 560–942 tokens，比原始模型长，但 <strong>省去外部检索 API 与向量库调用</strong>；</li>
<li><strong>训练资源</strong>：8×A800-80 GB 或 8×H100，SFT 3 epoch + GRPO 350 步即可收敛。</li>
</ul>
<p>综上，实验覆盖 <strong>准确率、忠诚度、消融、证据质量、token 成本</strong> 五个维度，充分验证了 CARE 在无需额外标注的情况下，对多种模型与任务均能带来一致且显著的提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为 CARE 的“直接外延”或“深层扩展”，均未被原文系统探讨，值得后续研究：</p>
<p>1. 能力边界与失效模式</p>
<ul>
<li><strong>数值/符号推理</strong>：CARE 的检索奖励仅约束“文本片段存在”，对公式、数字、单位换算等是否仍会产生幻觉？</li>
<li><strong>矛盾或噪声上下文</strong>：当输入自身包含互相冲突的句子时，模型如何抉择？需建立“矛盾检测”模块或不确定性估计。</li>
<li><strong>超长上下文</strong>：&gt;100k tokens 时，原生检索是否仍能定位到关键句？可引入“先粗粒度摘要-再细粒度检索”的层级策略。</li>
</ul>
<p>2. 混合检索架构</p>
<ul>
<li><strong>内部 + 外部联合</strong>：把 <code>&lt;retrieval&gt;</code> 拆成两类标签，<code>&lt;retrieval-internal&gt;</code> 与 <code>&lt;retrieval-external&gt;</code>，让模型在一次生成中自主决定“查原文”还是“调搜索 API”。</li>
<li><strong>可微检索器</strong>：用稀疏检索（BM25）或小型双塔模型为 CARE 提供“候选句”，把检索奖励改成“候选集合内的覆盖度”，实现端到端训练。</li>
</ul>
<p>3. 奖励设计与 RL 算法</p>
<ul>
<li><strong>细粒度证据奖励</strong>：目前只检查“存在性”，可进一步用 BLEU/ROUGE 衡量“检索句与问题相关性”，或引入 entailment model 做“支持度”打分。</li>
<li><strong>多轮 RL</strong>：允许模型迭代生成 <code>&lt;retrieval&gt;</code> 片段，每轮奖励基于“新增证据是否带来答案置信度提升”，实现逐步求精。</li>
<li><strong>RL 算法升级</strong>：用 PPO-ensemble、RLOO 或 DPO 替代 GRPO，观察样本效率与稳定性差异。</li>
</ul>
<p>4. 数据与课程学习</p>
<ul>
<li><strong>自动证据标注</strong>：利用 LLM+置信度过滤，对无标注长文档自动生成“伪支持句”，把 CARE 拓展到任意领域而无需人工标注。</li>
<li><strong>难度度量</strong>：用“上下文长度、答案句距离、所需跳数”三维指标动态调节课程学习曲线，而非简单线性比例。</li>
<li><strong>多语言/多域课程</strong>：从维基→新闻→医疗→法律，逐步迁移，验证 CARE 是否保持跨领域忠诚度。</li>
</ul>
<p>5. 推理效率优化</p>
<ul>
<li><strong>投机解码</strong>：训练一个小型“检索句探测器”，在 <code>&lt;think&gt;</code> 阶段提前结束生成，减少长链推理延迟。</li>
<li><strong>KV-Cache 复用</strong>：同一上下文多次提问时，缓存已定位的 <code>&lt;retrieval&gt;</code> 位置，避免重复全文扫描。</li>
<li><strong>边检索边压缩</strong>：对已被引用的句子进行动态摘要，逐步缩短上下文长度，降低二次推理开销。</li>
</ul>
<p>6. 评测体系升级</p>
<ul>
<li><strong>细粒度幻觉分类</strong>：建立“实体替换、数字篡改、关系颠倒、无中生有”四元标签，而非仅用 F1。</li>
<li><strong>对抗评测</strong>：主动在上下文插入“诱饵句”（与问题高度相关但错误），测量模型是否会被误导。</li>
<li><strong>人机交互评测</strong>：让标注者实时修改上下文，观察 CARE 能否即时调整答案并给出新的证据链。</li>
</ul>
<p>7. 安全与伦理</p>
<ul>
<li><strong>隐私泄露</strong>：原生检索可能把上下文中的个人身份信息直接抄入回答，需引入“敏感实体脱敏”奖励。</li>
<li><strong>版权与引用</strong>：探索 <code>&lt;retrieval&gt;</code> 片段的自动出处标注，满足学术或商业场景的可追溯要求。</li>
<li><strong>碳足迹</strong>：对比 CARE（长输出）vs. 传统 RAG（多次 API 调用）的全生命周期能耗，建立绿色指标。</li>
</ul>
<p>综上，CARE 把“检索”内化为模型能力的第一步已验证有效，下一步可在<strong>能力边界、混合架构、奖励设计、数据扩展、效率优化、评测维度、安全伦理</strong>七大方向继续深挖，推动“上下文忠实”的大模型走向更复杂、更实用、更可信的阶段。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心速览</strong></p>
<p>1. 问题</p>
<p>大模型在知识密集型问答中常<strong>违背输入上下文</strong>——幻觉、漏关键句或凭参数知识妄答，现有 RAG 需昂贵标注与外部模块，且易忽视用户已提供的原文。</p>
<p>2. 思路</p>
<p>提出 <strong>“原生检索增强推理”</strong> 新范式：<br><strong>不借助任何外部检索器</strong>，让模型在 <code>&lt;think&gt;</code> 内部自主定位并引用上下文原句，用特殊标记 <code>&lt;retrieval&gt;</code> … <code>&lt;/retrieval&gt;</code> 显式嵌入推理链，实现“边找边推”。</p>
<p>3. 方法：CARE 两阶段训练</p>
<p>阶段</p>
<p>目的</p>
<p>关键机制</p>
<p><strong>SFT</strong></p>
<p>学会格式</p>
<p>用已有支持句标注，自动生成“思维链+证据句”交错样本，教会模型输出 <code>&lt;retrieval&gt;</code> 标记。</p>
<p><strong>RL</strong></p>
<p>强化能力</p>
<p>GRPO 群体相对优化，奖励 = 答案正确性 + 格式合规 + <strong>检索忠实度</strong>（标记内文本必须逐字存在于上下文）。辅以课程学习：短上下文→长上下文渐进混合。</p>
<p>4. 结果</p>
<ul>
<li><strong>4 个真实长文档 QA 数据集</strong>（HotpotQA 等）：平均 F1 提升 <strong>8.4~15.3%</strong>，多跳任务最高 <strong>+29.4%</strong>。</li>
<li><strong>反事实 QA 基准 CofCA</strong>：相比最强基线再涨 <strong>3.3~13.7%</strong>，外部搜索方法反而下降。</li>
<li><strong>证据检索质量</strong>（BLEU/ROUGE-L）全面领先；消融显示检索奖励与课程学习各贡献 <strong>+4% 与 +2%</strong>。</li>
<li><strong>零外设、零 API 调用</strong>，仅增加推理 token 数，训练成本 8×A800 可承受。</li>
</ul>
<p>5. 贡献一句话</p>
<p>CARE 用<strong>极少量标注</strong>把“检索”内化为模型本能，让大模型<strong>先抄原文再推理</strong>，在多种任务与模型规模上同步提升<strong>上下文保真度、检索准确率与答案正确性</strong>，为知识密集型应用提供了一条轻量、可靠的新路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Improving%20Context%20Fidelity%20via%20Native%20Retrieval-Augmented%20Reasoning，链接是  https://arxiv.org/pdf/2509.13683  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13683  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Suyuchen Wang,Jinlin Wang,Xinyu Wang,Shiqi Li,Xiangru Tang,Sirui Hong,Xiao-Wen Chang,Chenglin Wu,Bang Liu</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13683.pdf">https://arxiv.org/pdf/2509.13683.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13683">https://arxiv.org/abs/2509.13683</a></p>
<p><strong>Arxiv ID</strong>: 2509.13683</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13683">https://papers.cool/arxiv/2509.13683</a></p>
<p><strong>Published</strong>: 2025-09-17T04:28:07Z</p>
<p><strong>Updated</strong>: 2025-09-17T04:28:07.000Z</p>
<hr>
<h3 id="11-AERIS-Argonne-Earth-Systems-Model-for-Reliable-and-Skillful-Predictions"><a href="#11-AERIS-Argonne-Earth-Systems-Model-for-Reliable-and-Skillful-Predictions" class="headerlink" title="11. AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13523">AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</a></h3><p>Generative machine learning offers new opportunities to better understand complex Earth system dynamics. Recent diffusion-based methods address spectral biases and improve ensemble calibration in weather forecasting compared to deterministic methods, yet have so far proven difficult to scale stably at high resolutions. We introduce AERIS, a 1.3 to 80B parameter pixel-level Swin diffusion transformer to address this gap, and SWiPe, a generalizable technique that composes window parallelism with sequence and pipeline parallelism to shard window-based transformers without added communication cost or increased global batch size. On Aurora (10,080 nodes), AERIS sustains 10.21 ExaFLOPS (mixed precision) and a peak performance of 11.21 ExaFLOPS with $1 \times 1$ patch size on the 0.25{\deg} ERA5 dataset, achieving 95.5% weak scaling efficiency, and 81.6% strong scaling efficiency. AERIS outperforms the IFS ENS and remains stable on seasonal scales to 90 days, highlighting the potential of billion-parameter diffusion models for weather and climate prediction.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>翻译失败: Unexpected end of JSON input</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p>LLM Analysis Failed: Error: 抓取失败: Waiting failed: 30000ms exceeded</p>
</div></details>
<p><strong>Authors</strong>: Väinö Hatanpää,Eugene Ku,Jason Stock,Murali Emani,Sam Foreman,Chunyong Jung,Sandeep Madireddy,Tung Nguyen,Varuni Sastry,Ray A. O. Sinurat,Sam Wheeler,Huihuo Zheng,Troy Arcomano,Venkatram Vishwanath,Rao Kotamarthi</p>
<p><strong>Categories</strong>: cs.LG,cs.DC</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13523.pdf">https://arxiv.org/pdf/2509.13523.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13523">https://arxiv.org/abs/2509.13523</a></p>
<p><strong>Arxiv ID</strong>: 2509.13523</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13523">https://papers.cool/arxiv/2509.13523</a></p>
<p><strong>Published</strong>: 2025-09-16T20:38:29Z</p>
<p><strong>Updated</strong>: 2025-09-16T20:38:29.000Z</p>
<hr>
<h3 id="12-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs"><a href="#12-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs" class="headerlink" title="12. SteeringControl: Holistic Evaluation of Alignment Steering in LLMs"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13450">SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</a></h3><p>We introduce SteeringControl, a benchmark for evaluating representation steering methods across core alignment objectives—bias, harmful generation, and hallucination—and their effects on secondary behaviors such as sycophancy and commonsense morality. While prior alignment work often highlights truthfulness or reasoning ability to demonstrate the side effects of representation steering, we find there are many unexplored tradeoffs not yet understood in a systematic way. We collect a dataset of safety-relevant primary and secondary behaviors to evaluate steering effectiveness and behavioral entanglement centered around five popular steering methods. To enable this, we craft a modular steering framework based on unique components that serve as the building blocks of many existing methods. Our results on Qwen-2.5-7B and Llama-3.1-8B find that strong steering performance is dependent on the specific combination of steering method, model, and targeted behavior, and that severe concept entanglement can result from poor combinations of these three as well. We release our code here: <a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/SteeringControl.git">https://github.com/wang-research-lab/SteeringControl.git</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了SteeringControl，这是一个用于评估表示引导方法在核心对齐目标（偏见、有害生成和幻觉）及其对次要行为（如阿谀奉承和常识道德）影响的基准测试。虽然之前的对齐工作通常强调真实性或推理能力以展示表示引导的副作用，但我们发现仍有许多未系统理解的未探索折衷。我们收集了一个与安全相关的主要和次要行为的数据集，以评估围绕五种流行引导方法的引导效果和行为纠缠。为此，我们构建了一个模块化的引导框架，基于独特组件，这些组件作为许多现有方法的构建块。我们在Qwen-2.5-7B和Llama-3.1-8B上的结果发现，强大的引导性能依赖于引导方法、模型和目标行为的特定组合，而糟糕的这三者组合也可能导致严重的概念纠缠。我们在此发布我们的代码：<a target="_blank" rel="noopener" href="https://github.com/wang-research-lab/SteeringControl.git">https://github.com/wang-research-lab/SteeringControl.git</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该研究针对大模型对齐中“表征操控”（representation steering）方法缺乏系统评估、行为纠缠现象未被充分理解两大痛点，提出 STEERINGCONTROL 基准与模块化框架，旨在：</p>
<ul>
<li>统一衡量五种主流无训练 steering 方法在三大核心对齐目标（有害生成、幻觉、偏见）上的<strong>有效性</strong>；</li>
<li>量化操控主目标时对<strong>十类次要行为</strong>（谄媚、常识道德、品牌偏好等）的<strong>非预期连带影响（entanglement）</strong>；</li>
<li>揭示“方法-模型-行为”三元组合对性能与纠缠的决定性作用，为后续 Pareto 最优的定向操控算法提供可复现的实验平台与数据支撑。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，可视为 STEERINGCONTROL 的学术脉络。按主题归类，并给出每篇的核心贡献一句话概括（均不带第一人称）。</p>
<p>1. 可解释性与线性表征假设</p>
<ul>
<li><p><strong>Elhage et al., 2022</strong> “Toy models of superposition”<br>提出超叠加框架，解释为何不同行为共享同一激活子空间，为“纠缠”提供理论依据。</p>
</li>
<li><p><strong>Park et al., 2024</strong> “The linear representation hypothesis…”<br>在大模型层面验证线性可读性，为向量加减可操控行为奠定几何基础。</p>
</li>
<li><p><strong>Bolukbasi et al., 2016</strong> “Man is to computer programmer as woman is to homemaker?”<br>早期词嵌入线性偏差移除，开启“方向=语义”思想。</p>
</li>
<li><p><strong>Nanda et al., 2023</strong> “Emergent linear representations…”<br>发现世界知识在自监督模型中呈线性结构，支持干预激活即可改变输出。</p>
</li>
</ul>
<p>2. 激活操控/表征工程</p>
<ul>
<li><p><strong>Zou et al., 2023b</strong> “Representation Engineering”<br>首次系统提出用对比激活差向量进行顶层注入/消除，实现拒绝、诚实度操控。</p>
</li>
<li><p><strong>Panickssery et al., 2023</strong> “Steering Llama 2 via contrastive activation addition”<br>将激活加法应用于 Llama-2，展示多项选择模板可放大操控信号。</p>
</li>
<li><p><strong>Arditi et al., 2024</strong> “Refusal in language models is mediated by a single direction”<br>证明拒绝行为可被单一线性方向解释，并引入 KL 散度过滤防止分布漂移。</p>
</li>
<li><p><strong>Marshall et al., 2024</strong> “Refusal in LLMs is an affine function”<br>提出带平移项的仿射编辑，比纯方向减法更精准抑制拒绝。</p>
</li>
<li><p><strong>Lee et al., 2024a</strong> “Programming refusal with conditional activation steering”<br>引入 CAST——仅在激活与条件向量余弦相似度高于阈值时才干预，降低对无关输入的副作用。</p>
</li>
<li><p><strong>Wu et al., 2025</strong> “AxBench”<br>提供 1,200 条细粒度概念基准，证明简单激活加减优于稀疏自编码器特征操控。</p>
</li>
</ul>
<p>3. 行为纠缠与副作用观察</p>
<ul>
<li><p><strong>Qiu et al., 2024</strong> “Spectral Editing…”<br>发现抑制毒性会同时降低模型在 TruthfulQA 上的事实正确率，首次量化“对齐-能力”折衷。</p>
</li>
<li><p><strong>Lee et al., 2024b</strong> “A mechanistic understanding of alignment algorithms…”<br>从机制视角证明 DPO 减毒同时削弱信息性，揭示优化目标冲突。</p>
</li>
<li><p><strong>Siu et al., 2025</strong> “COSMIC”<br>提出用内部状态而非输出文本做早停搜索，可减拒同时降低对通用能力数据集的 KL 漂移。</p>
</li>
</ul>
<p>4. 数据集与评估框架</p>
<ul>
<li><p><strong>Li et al., 2024</strong> “SALAD-Bench”<br>21k 有害指令问答，覆盖 6 大风险域，为 STEERINGCONTROL 的“有害生成”主任务提供数据。</p>
</li>
<li><p><strong>Parrish et al., 2022</strong> “BBQ”<br>手工构建的多选隐含偏见题，评估模型对受保护群体的刻板印象。</p>
</li>
<li><p><strong>Hartvigsen et al., 2022</strong> “ToxiGen”<br>机器生成-人工过滤的显性毒语数据集，用于显性偏见检测。</p>
</li>
<li><p><strong>Bang et al., 2025</strong> “HalluLens”<br>将幻觉细分为内在/外在两类，并提供 10 级难度维基问答，用于 STEERINGCONTROL 的幻觉主任务。</p>
</li>
<li><p><strong>Kran et al., 2025</strong> “DarkBench”<br>测量谄媚、品牌偏好、拟人化等“暗模式”，被 STEERINGCONTROL 直接用作次要行为集。</p>
</li>
<li><p><strong>Rein et al., 2023</strong> “GPQA”<br>研究生级跨学科问答，用于测试操控后专家推理是否下降。</p>
</li>
<li><p><strong>Lin et al., 2022</strong> “TruthfulQA”<br>人类常见误解问答，衡量模型抵御“看似合理但错误”答案的能力，被广泛用于纠缠评估。</p>
</li>
</ul>
<p>5. 其他对齐或编辑框架</p>
<ul>
<li><p><strong>Wang &amp; Shu, 2023</strong> “Trojan Activation Attack”<br>通过激活注入植入后门，反向说明激活空间可被恶意或防御性利用。</p>
</li>
<li><p><strong>Belrose et al., 2023</strong> “LEACE”<br>闭式线性概念擦除，保证完全去除指定线性子空间而不影响其他协变量。</p>
</li>
<li><p><strong>Wehner et al., 2025</strong> “Taxonomy of representation engineering”<br>给出七维分类法（训练/推理、权重/激活、注入/擦除等），STEERINGCONTROL 的模块化设计受其启发。</p>
</li>
<li><p><strong>Xu et al., 2025</strong> “EasyEdit2”<br>统一接口支持权重编辑、激活编辑等多种方法，但侧重“编辑”而非“行为纠缠”系统评估。</p>
</li>
<li><p><strong>Im &amp; Li, 2025</strong> “A Unified Understanding and Evaluation of Steering Methods”<br>同时期工作，也尝试统一评估，但数据规模与纠缠维度不及 STEERINGCONTROL。</p>
</li>
</ul>
<p>以上研究共同构成了 STEERINGCONTROL 的理论、方法与评估基础，使其能够系统量化“操控有效性-行为纠缠”折衷。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“一个基准 + 一个框架”双管齐下，把“如何系统评估表征操控”拆解为可操作的五步流程，从而解决“方法林立却难比较、主任务见效却副作用未知”的痛点。</p>
<p>1. 构建 STEERINGCONTROL 基准：17 套行为数据集 + 两级指标</p>
<p>组成</p>
<p>关键设计</p>
<p>解决什么问题</p>
<p><strong>3 类主行为</strong>（Bias / Harmful Generation / Hallucination）</p>
<p>各配 4k–5k 训练-验证-测试，含负例，分层抽样</p>
<p>提供统一、足够大的“操控靶”，避免方法论文各自用私有小集</p>
<p><strong>10 类次行为</strong>（Social / Reasoning / Epistemic / Normative）</p>
<p>全部不用于训练，仅用于事后测量</p>
<p>首次把“副作用”量化成可比数字，而非零散案例</p>
<p><strong>两级指标</strong></p>
<p>Effectiveness = 主任务提升归一化<br>Entanglement = 次任务漂移平方均值</p>
<p>把“有效-纠缠”折衷变成单点 Pareto 图，方法好坏一目了然</p>
<p>2. 设计模块化推理期干预框架：三阶段组件任意拼装</p>
<p>阶段</p>
<p>可插拔组件</p>
<p>解决什么问题</p>
<p><strong>Direction Generation</strong></p>
<p>DiffInMeans / PCA / LAT 等 3 类提取器 × 3 种输入模板</p>
<p>以往每篇论文只报一种“秘方”，现在把“秘方”拆成可替换零件，保证公平</p>
<p><strong>Direction Selection</strong></p>
<p>Grid Search vs. COSMIC；可选 KL 阈值或 CAST 条件触发</p>
<p>把“搜层数、搜系数”这一暗箱操作标准化，且提供“轻算力”替代方案</p>
<p><strong>Direction Application</strong></p>
<p>Activation Addition vs. Directional Ablation × 3 个位置策略</p>
<p>明确干预公式与残差流写入点，避免“同样叫 steering 却操作不同”导致的不可复现</p>
<p>3. 运行标准化实验：固定模型 + 固定数据 + 固定搜索协议</p>
<ul>
<li>统一在 Qwen-2.5-7B 与 Llama-3.1-8B 上评估，每套方法均跑 25%–80% 层范围、步长 2，系数 −3…+3，生成 64 token。</li>
<li>主任务用验证集选最佳 (层, 系数)，再于测试集报告 Effectiveness；同时在全部次任务上测 Entanglement。</li>
<li>采用动态子采样（20%）防止静态测试集过拟合，兼顾算力与稳健性。</li>
</ul>
<p>4. 结果驱动诊断：三维交互可视化 → 揭示“无通用最优”</p>
<ul>
<li>Pareto 图显示同一方法在不同模型上斜率截然不同，破除“一招通吃”幻想。</li>
<li>细粒度热图指出：<br>– 拒绝最易操控，但 DIM 高增益伴随高纠缠；<br>– 外在幻觉几乎无效，说明几何结构因模型而异；<br>– 社会行为（谄媚、拟人化）比推理任务更易被连带影响，提示共享提示解析通路。</li>
</ul>
<p>5. 开源与可扩展：代码库即插即用</p>
<ul>
<li>所有组件以 Python 抽象类形式发布，新增提取器、应用公式或数据集只需继承基类，无需改动评估管线。</li>
<li>支持即时组合“新方向生成 + 新条件触发 + 新写入位置”，为后续 Pareto 最优算法提供实验床。</li>
</ul>
<p>通过“统一数据、统一指标、统一接口”三步，论文把原先碎片化的 steering 研究转化为可复现、可量化、可迭代的系统性实验，从而首次全景式回答了“操控有效吗？副作用多大？因何而异？”三大问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“主行为操控有效性”与“次行为纠缠度”两条主线展开，全部在固定推理预算与统一搜索协议下完成，可归纳为 4 组共 18 万条模型调用。</p>
<p>1. 主行为 steering 有效性实验</p>
<p>任务</p>
<p>数据集</p>
<p>样本量</p>
<p>目标方向</p>
<p>评估指标</p>
<p><strong>Harmful Generation</strong></p>
<p>SALAD-Bench (去重后 10 k)</p>
<p>4 k/1 k/5 k</p>
<p>降低拒绝 → 提高攻击成功率</p>
<p>LlamaGuard-4 判为“unsafe”比例↑</p>
<p><strong>Demographic Bias</strong></p>
<p>BBQ (隐式) + ToxiGen (显性)</p>
<p>4 k/1 k/5 k 每套</p>
<p>降低刻板/毒语</p>
<p>BBQ：选项匹配准确率↑；ToxiGen：拒绝毒语比例↑</p>
<p><strong>Hallucination</strong></p>
<p>FaithEval (内在) + PreciseWikiQA (外在)</p>
<p>4 k/1 k/5 k 每套</p>
<p>降低幻觉率</p>
<p>LLaMA-3.3-70B 评判“无幻觉”比例↑</p>
<ul>
<li>5 种方法 × 3 主任务 × 2 模型 = 30 组“全量 steering”</li>
<li>每组在 25 %–80 % 层范围内步长 2 × 7 个系数 (−3…+3) 做网格搜索，共 ≈ 1.2 k 方向候选，选验证集最优 (层,系数) 后进入测试集报告 Effectiveness。</li>
</ul>
<p>2. 次行为 entanglement 实验</p>
<p>对同一 steered checkpoint，<strong>不再做任何微调</strong>，直接推理 10 类次任务：</p>
<p>类别</p>
<p>数据集</p>
<p>样本量</p>
<p>观测指标</p>
<p><strong>Social</strong></p>
<p>DarkBench (Brand/Sycophancy/Anthro/Retention)</p>
<p>4 × 110</p>
<p>GPT-4o 判“无该行为”比例变化</p>
<p><strong>Reasoning</strong></p>
<p>GPQA (专家) + ARC-C (常识)</p>
<p>448 + 500</p>
<p>准确率变化</p>
<p><strong>Epistemic</strong></p>
<p>TruthfulQA (误解) + Sneaking (暗渡)</p>
<p>791 + 110</p>
<p>准确率 / GPT-4o 判“无暗渡”变化</p>
<p><strong>Normative</strong></p>
<p>DecodingTrust (常识道德) + TwinViews (政治)</p>
<p>750 + 750</p>
<p>道德正确率 / 左倾比例变化</p>
<ul>
<li>30 组主任务 steering 各对应 1 组次任务漂移测量，计算公式  <script type="math/tex; mode=display">\\text{Entanglement}=\\frac{1}{|\\mathcal{B}_{\\text{ood}}|}\\sum_{b\\in\\mathcal{B}\_{\\text{ood}}}(y\_b^{\\text{steered}}-y\_b)^2</script></li>
</ul>
<p>3. 消融与变体实验</p>
<p>在 Qwen-2.5-7B 与 Llama-3.1-8B 上各完成：</p>
<p>变体</p>
<p>目的</p>
<p>组数</p>
<p><strong>No KL</strong></p>
<p>去掉 logits-KL≤10 % 过滤，观察“放开约束”能否提效</p>
<p>5 方法 × 3 任务 × 2 模型 = 30</p>
<p><strong>Conditional (CAST)</strong></p>
<p>用条件余弦触发干预，其余样本原样推理，检验是否降纠缠</p>
<p>同上 30</p>
<p><strong>Cumulative LAT</strong></p>
<p>仅对 LAT 方法，从 25 % 层起累积注入，验证“多层叠加”收益</p>
<p>2 模型 × 3 任务 = 6</p>
<p>4. 预算与生成设置</p>
<ul>
<li>多选题统一 1 token，开放生成上限 64 token，温度=0；</li>
<li>方向提取始终用最后 token 激活；干预位置统一在残差流 pre-layer；</li>
<li>全部实验在 8×A100 节点完成，累计 ≈ 18 万次前向生成，对应 2.4 k GPU 时。</li>
</ul>
<p>通过以上 4 组实验，论文得到 图 2 Pareto 前沿、图 3–8 细粒度热图 以及附录全部数值表，首次在同一坐标系下量化了“方法-模型-行为”三维对有效性与纠缠的决定性影响。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 STEERINGCONTROL 的“直接外延”，均围绕其当前局限或新发现展开，无需改动框架即可接入实验管线。</p>
<p>1. 多语言与跨文化纠缠</p>
<ul>
<li>假设：共享几何结构在跨语言场景下可能断裂，导致“英语上降偏见、中文反而增偏见”。</li>
<li>可立即用 <strong>Wang et al., 2024b</strong> 的 multilingual-safety 套件替换次行为数据集，复跑 30 组 steering，观察 Entanglement 是否出现语言不对称峰值。</li>
</ul>
<p>2. 自适应/梯度式干预</p>
<ul>
<li>目前仅用静态向量；可把 <strong>ReFT</strong>（Wu et al., 2024）的梯度更新封装成新的 Direction Application 组件，对比“静态 vs 自适应”在相同搜索预算下的 Pareto 曲线，验证是否突破现有前沿。</li>
</ul>
<p>3. 行为子类型细粒度诊断</p>
<ul>
<li>将 BBQ 按种族、性别、职业三维拆片，将 HalluLens 按 10 级难度拆片，分别计算 Effectiveness；若某子类型显著偏离均值，说明方向过拟合表层语义而非深层概念，可回推改进提取算法。</li>
</ul>
<p>4. token 位置与层位联合搜索</p>
<ul>
<li>现有固定“最后 token + 25 %–80 % 层”；可引入 <strong>Arditi et al., 2024</strong> 的“early-token refusal direction”作为新组件，在 {first, mid, last} × {pre-layer, post-attn, post-mlp} 上做二维网格，观察是否出现“不同行为最优位置正交”现象，为“分层-分位”精准干预提供地图。</li>
</ul>
<p>5. 系数动态调度</p>
<ul>
<li>目前系数恒定；可试验 <strong>随深度衰减</strong> α(l)=α₀·γ^(l-L/2) 或 <strong>随余弦相似度动态缩放</strong> α(x)=α₀·σ(⟨x,c⟩)，把调度策略封装为新的 Applier，测试能否在保持 Effectiveness 同时把 Entanglement 再降 10 %–20 %。</li>
</ul>
<p>6. 多目标 steering 的 Pareto 前沿</p>
<ul>
<li>框架已支持同时注入多向量；可设置“拒绝↓ + 幻觉↓”双目标，用 NSGA-II 搜索 (α₁, α₂, layer) 三维空间，绘制真正多目标前沿，验证是否存在“双赢”区域或必然冲突。</li>
</ul>
<p>7. 推理模型与长生成</p>
<ul>
<li>当前仅 64 token 且禁止“思考”token；可对 <strong>Llama-3.1-70B-Instruct</strong> 启用 2 k token 长生成 + 思维链提示，检验 steering 向量在长程上下文是否衰减，或是否因思维链中间激活漂移而产生新的纠缠模式。</li>
</ul>
<p>8. 权重-激活混合编辑</p>
<ul>
<li>把 <strong>LEACE</strong>（Belrose et al., 2023）的闭式投影封装为 Weight Applier 组件，与现有激活加法并行跑，对比“改权重不改激活”与“改激活不改权重”两条 Pareto 曲线，看哪条更接近原点，从而决定未来部署应动哪部分参数。</li>
</ul>
<p>9. 在线监控与反馈闭环</p>
<ul>
<li>基于 CAST 条件触发器，接入真实用户流量：当 KL 或相似度异常时，实时回退 α→0，形成“干预-监控-回退”闭环；用强化学习微调阈值超参，把 STEERINGCONTROL 从离线基准升级为在线安全护栏。</li>
</ul>
<p>10. 可解释性反向验证</p>
<ul>
<li>用稀疏自编码器提取 steering 方向对应的人类可读特征，若发现“拒绝方向”高激活特征含大量与“礼貌”相关的 token，可直接解释为何拒绝操控会连带升高 sycophancy，为下一步“特征级去纠缠”提供白盒依据。</li>
</ul>
<p>这些探索点均可直接复用 STEERINGCONTROL 的模块化代码，通过“新增数据集”或“新增组件”两条接口即可落地，无需重新搭建整套管线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：大模型对齐中的表征操控方法缺乏统一评估，且对“主任务见效、副作用未知”的行为纠缠现象缺少系统量化。</li>
<li><strong>方案</strong>：提出 STEERINGCONTROL 基准 + 模块化推理期干预框架，覆盖 17 套数据集、5 种主流无训练 steering 方法，用 Effectiveness 与 Entanglement 两级指标在同一坐标系内比较。</li>
<li><strong>实验</strong>：在 Qwen-2.5-7B 与 Llama-3.1-8B 上完成 30 组主任务操控及 10 类次任务漂移测量，外加 No-KL 与 CAST 两种变体，共约 18 万条生成。</li>
<li><strong>发现</strong>：<ul>
<li>操控性能与“方法-模型-行为”三元组合强相关，无通用最优；</li>
<li>DIM 高增益但高纠缠，ACE 与 CAST 可在略降有效性同时显著降纠缠；</li>
<li>社会行为（谄媚、拟人化）比推理任务更易被连带影响，揭示共享提示解析通路。</li>
</ul>
</li>
<li><strong>开源</strong>：代码与数据集全部公开，支持即插即用新增组件，为后续 Pareto 最优操控算法提供标准化实验床。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是SteeringControl%3A%20Holistic%20Evaluation%20of%20Alignment%20Steering%20in%20LLMs，链接是  https://arxiv.org/pdf/2509.13450  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13450  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Vincent Siu,Nicholas Crispino,David Park,Nathan W. Henry,Zhun Wang,Yang Liu,Dawn Song,Chenguang Wang</p>
<p><strong>Categories</strong>: cs.AI,cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13450.pdf">https://arxiv.org/pdf/2509.13450.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13450">https://arxiv.org/abs/2509.13450</a></p>
<p><strong>Arxiv ID</strong>: 2509.13450</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13450">https://papers.cool/arxiv/2509.13450</a></p>
<p><strong>Published</strong>: 2025-09-16T18:36:22Z</p>
<p><strong>Updated</strong>: 2025-09-16T18:36:22.000Z</p>
<hr>
<h3 id="13-Quantum-Variational-Activation-Functions-Empower-Kolmogorov-Arnold-Networks"><a href="#13-Quantum-Variational-Activation-Functions-Empower-Kolmogorov-Arnold-Networks" class="headerlink" title="13. Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14026">Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks</a></h3><p>Variational quantum circuits (VQCs) are central to quantum machine learning, while recent progress in Kolmogorov-Arnold networks (KANs) highlights the power of learnable activation functions. We unify these directions by introducing quantum variational activation functions (QVAFs), realized through single-qubit data re-uploading circuits called DatA Re-Uploading ActivatioNs (DARUANs). We show that DARUAN with trainable weights in data pre-processing possesses an exponentially growing frequency spectrum with data repetitions, enabling an exponential reduction in parameter size compared with Fourier-based activations without loss of expressivity. Embedding DARUAN into KANs yields quantum-inspired KANs (QKANs), which retain the interpretability of KANs while improving their parameter efficiency, expressivity, and generalization. We further introduce two novel techniques to enhance scalability, feasibility and computational efficiency, such as layer extension and hybrid QKANs (HQKANs) as drop-in replacements of multi-layer perceptrons (MLPs) for feed-forward networks in large-scale models. We provide theoretical analysis and extensive experiments on function regression, image classification, and autoregressive generative language modeling, demonstrating the efficiency and scalability of QKANs. DARUANs and QKANs offer a promising direction for advancing quantum machine learning on both noisy intermediate-scale quantum (NISQ) hardware and classical quantum simulators.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>变分量子电路（VQCs）是量子机器学习的核心，而最近在Kolmogorov-Arnold网络（KANs）方面的进展突显了可学习激活函数的强大。我们通过引入量子变分激活函数（QVAFs）来统一这些方向，这些函数通过称为数据重上传激活函数（DARUANs）的单量子位数据重上传电路实现。我们表明，具有可训练权重的DARUAN在数据预处理中的频谱随着数据重复呈指数级增长，相对于傅里叶基激活函数在不损失表达能力的情况下，能够在参数大小上实现指数级减少。将DARUAN嵌入KANs中可生成量子启发式KANs（QKANs），保持了KANs的可解释性，同时提升了它们的参数效率、表达能力和泛化能力。我们进一步引入了两种新技术，以增强可扩展性、可行性和计算效率，例如层扩展和混合QKANs（HQKANs），作为大规模模型中前馈网络多层感知器（MLPs）的替代品。我们提供了功能回归、图像分类和自回归生成语言建模的理论分析和广泛实验，展示了QKANs的效率和可扩展性。DARUAN和QKAN为在嘈杂中间规模量子（NISQ）硬件和经典量子模拟器上推进量子机器学习提供了一个有前景的方向。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决<strong>量子机器学习（QML）与经典可学习激活函数（VAF）之间的融合与效率问题</strong>，具体聚焦于以下核心挑战：</p>
<ol>
<li><p><strong>量子变分电路（VQCs）的表达潜力未被充分利用</strong><br>现有QML研究多将VQCs视为整体学习模型，而非将其嵌入经典架构作为可学习非线性组件。论文提出将VQCs重新定位为<strong>量子变分激活函数（QVAFs）</strong>，以释放其在函数逼近中的指数级表达优势。</p>
</li>
<li><p><strong>经典KAN的参数冗余与计算瓶颈</strong><br>Kolmogorov-Arnold网络（KANs）虽通过可学习激活函数提升了解释性与精度，但其B样条基需$O(N^2LG)$参数（$G$为网格点数），导致高维任务中参数二次增长。论文通过**单比特数据重上传电路（DARUAN）**替代B样条，实现参数指数级压缩（$O(\log\frac{1}{\epsilon})$ vs. $O(\epsilon^{-\frac{1}{k+1-m}})$）。</p>
</li>
<li><p><strong>量子硬件与经典模拟的兼容性</strong><br>多比特VQCs面临NISQ设备噪声与贫瘠高原问题。论文采用<strong>单比特数据重上传电路</strong>，兼具：</p>
<ul>
<li><strong>NISQ硬件可行性</strong>：单比特门保真度已达$10^{-5}$至$10^{-7}$量级；</li>
<li><strong>经典高效模拟</strong>：避免指数级希尔伯特空间开销，支持GPU分布式训练。</li>
</ul>
</li>
<li><strong>大规模任务的扩展性</strong><br>通过<strong>混合QKAN（HQKAN）</strong>与<strong>层扩展策略</strong>，将QKAN嵌入Transformer等深层架构，解决参数随维度二次增长的瓶颈。实验表明，HQKAN在GPT-2中仅需MLP的1/3参数即可实现更低困惑度。</li>
</ol>
<p><strong>总结</strong>：论文提出<strong>量子启发KAN（QKAN）</strong>，以单比特QVAF（DARUAN）为核心，在保留KAN解释性的同时，实现参数效率、表达力与硬件可行性的三重突破，为量子-经典混合模型提供可扩展范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接关联，按主题分组并给出关键贡献：</p>
<ul>
<li><p><strong>量子机器学习基础</strong></p>
<ul>
<li>Biamonte et al. (2017) 系统综述了量子机器学习框架，奠定量子特征映射与混合训练范式。</li>
<li>Schuld &amp; Killoran (2019) 提出量子特征空间理论，解释量子核方法优势。</li>
<li>Jerbi et al. (2023) 突破核方法限制，提出变分量子电路作为可训练模型。</li>
</ul>
</li>
<li><p><strong>变分量子电路（VQC）表达力</strong></p>
<ul>
<li>Mitarai et al. (2018) 证明单生成元VQC可逼近任意解析函数。</li>
<li>Schuld et al. (2021) 给出数据重上传电路的傅里叶谱分析，为DARUAN谱扩展提供理论模板。</li>
<li>Yu et al. (2022, 2024b) 研究单比特原生量子网络的能力与误差界。</li>
</ul>
</li>
<li><p><strong>数据重上传电路</strong></p>
<ul>
<li>Pérez-Salinas et al. (2020) 提出单比特数据重上传作为通用分类器，证明经典可模拟性。</li>
<li>Zhao et al. (2024) 引入可训练输入权重，实现频率谱的指数扩展，被DARUAN直接采用。</li>
</ul>
</li>
<li><p><strong>可学习激活函数（VAF）与KAN</strong></p>
<ul>
<li>Liu et al. (2024c) 提出KAN，用B样条基实现可学习边激活，兼顾解释性与精度。</li>
<li>SS et al. (2024) 用Chebyshev多项式替代B样条，减少参数。</li>
<li>Xu et al. (2024a) 采用傅里叶级数作为激活基，与QKAN形成经典-量子对比。</li>
<li>Li (2024) 证明KAN可视为径向基函数网络，提供等价视角。</li>
</ul>
</li>
<li><p><strong>量子启发激活函数</strong></p>
<ul>
<li>Li et al. (2024b) 在混合CNN中嵌入单比特量子激活电路，验证 empirical 可行性。</li>
<li>Inajetovic et al. (2023) 提出变分量子样条，探索量子非线性逼近。</li>
</ul>
</li>
<li><p><strong>量子-经典混合与分布式训练</strong></p>
<ul>
<li>Chen &amp; Yoo (2021) 提出量子联邦学习框架，与QKAN的分布式并行策略兼容。</li>
<li>Harrow et al. (2009) HHL算法，被用于QKAN→KAN知识蒸馏中的系数估计。</li>
</ul>
</li>
<li><p><strong>近期并行工作</strong></p>
<ul>
<li>Werner et al. (2025) 提出多比特QVAF-KAN，面临贫瘠高原与硬件扩展挑战。</li>
<li>Wakaura et al. (2025a,b) 探索自适应多比特量子KAN，未解决单比特可模拟性与参数效率。</li>
</ul>
</li>
</ul>
<p>这些研究共同构成QKAN的理论与实验基础，凸显其在<strong>量子表达力-经典效率</strong>光谱中的独特定位。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“量子变分激活函数（QVAF）+ Kolmogorov-Arnold 网络（KAN）”这一耦合框架，把上述挑战拆解为四个可执行的技术模块，并给出端到端的解决方案。核心思路是：<strong>用单比特数据重上传电路 DARUAN 替代 KAN 的 B 样条边激活，从而在经典模拟环境下实现参数指数压缩、表达力指数增强，同时保持 NISQ 硬件可直接落地</strong>。具体实现路径如下：</p>
<p>1. 量子端：设计单比特 QVAF —— DARUAN</p>
<ul>
<li><p><strong>电路结构</strong><br>单比特重复层：  </p>
<script type="math/tex; mode=display">U\_{\\omega}(x)=W^{(r+1)}!!\\prod\_{\\ell=1}^{r}!!S(w\_\\ell x),W^{(\\ell)}, \\quad S(w\_\\ell x)=\\exp(-i w\_\\ell x H),</script><p>其中 $H=\frac{1}{2}\sigma_j$ 且 $w_\ell$ 为<strong>可训练输入权重</strong>（经典线性层前置）。<br>输出直接取 $\langle\sigma_z\rangle\in[-1,1]$ 作为激活值，无需额外归一化。</p>
</li>
<li><p><strong>理论保证</strong>（Theorem 2.2）</p>
<ul>
<li>谱扩展：权重 $w_\ell=2^{\ell-1}$ 时，有效频率上限 $K_B=2^r-1$，可生成 <strong>$O(3^r)$ 个不同傅里叶分量</strong>。</li>
<li>逼近误差：对 $f\in C^{k+1}$ 有  <script type="math/tex; mode=display">|f-f\_B|\_{C^m}\\le C\_f, 2^{-r(k+1-m)},</script>故达到误差 $\varepsilon$ 仅需  <script type="math/tex; mode=display">r=\\Theta!\\left(\\log\\frac{1}{\\varepsilon}\\right) \\Rightarrow \\text{参数量}=\\Theta!\\left(\\log\\frac{1}{\\varepsilon}\\right),</script>相比经典傅里叶 KAN 的 $\Theta(\varepsilon^{-1/(k+1-m)})$ 实现<strong>指数级参数压缩</strong>。</li>
</ul>
</li>
<li><strong>NISQ 适配</strong><br>仅含单比特门， trapped-ion 保真度 $10^{-7}$、超导 $10^{-5}$ 已满足深度百层需求；同时可用 GPU 张量网络批量模拟。</li>
</ul>
<p>2. 经典端：构建 QKAN 架构</p>
<ul>
<li><p><strong>层定义</strong><br>对第 $l$ 层 $n_l\to n_{l+1}$ 的全连接图，每条边 $(i,j)$ 独立部署一个 DARUAN：  </p>
<script type="math/tex; mode=display">\\phi\_{l,j,i}(x\_{l,i})=\\langle 0|U^\\dagger(x\_{l,i},\\theta\_{l,j,i})\\sigma\_z U(x\_{l,i},\\theta\_{l,j,i})|0\\rangle,</script><p>节点求和：  </p>
<script type="math/tex; mode=display">x\_{l+1,j}=\\sum\_{i=1}^{n\_l}\\phi\_{l,j,i}(x\_{l,i}).</script><p>总参数量 $O(N^2 L r)$，而 KAN 为 $O(N^2 L G)$；在相同精度下 $r\ll G$。</p>
</li>
<li><p><strong>层扩展（Layer Extension）</strong><br>训练过程中按需把 $r\to r+\Delta r$，新单元初始化为恒等映射，<strong>不破坏已学特征</strong>，缓解深层量子电路梯度消失问题。</p>
</li>
<li><p><strong>输出有界性处理</strong><br>量子测量天然有界 $[-1,1]$；若任务需要更大动态范围，提供两种轻量后处理：</p>
<ol>
<li>可学习缩放+偏置：$\hat y=\gamma,\phi(x)+\beta$；</li>
<li>附加 1-2 层 MLP 映射，保持参数量可忽略。</li>
</ol>
</li>
</ul>
<p>3. 规模化：Hybrid-QKAN（HQKAN）</p>
<ul>
<li><p><strong>瓶颈问题</strong><br>纯 QKAN 的边数与输入/输出维度成二次关系，高维场景（如 CIFAR-100、GPT-2）参数仍爆炸。</p>
</li>
<li><p><strong>自编码式压缩</strong><br>在 QKAN 前后各插入<strong>小型全连接层</strong>作为压缩-扩张器，把高维特征先降到 $O(\log d)$ latent 空间再经 QKAN 处理。<br>结果：CIFAR-100 上 QKAN 从 384 k 降至 32 k 参数，top-5 准确率仍提升 0.6%。</p>
</li>
</ul>
<p>4. 训练与部署机制</p>
<ul>
<li><p><strong>知识蒸馏</strong>（QKAN → KAN）<br>训练完成后，把每条 DARUAN 在网格上采样，再用 B 样条或傅里叶基拟合，将系数迁移到经典 KAN，<strong>实现量子训练-经典推理</strong>，消除在线量子硬件依赖。</p>
</li>
<li><p><strong>分布式量子/经典并行</strong><br>每条边激活相互独立，天然支持：</p>
<ul>
<li>同步并行：$b\times m\times n$ 单比特电路同时执行；</li>
<li>异步联邦：多量子节点或 GPU 节点各自计算局部梯度，经典聚合。<br>实验在 4×V100S、16×H200 集群上实现大 batch 训练，HQKAN 迭代时间仅增加 3%，内存节省 10–35%。</li>
</ul>
</li>
</ul>
<p>5. 实验验证</p>
<p>任务</p>
<p>关键结果</p>
<p><strong>含噪符号回归</strong>（66 条 Feynman 方程）</p>
<p>QKAN 在 80 % 方程上取得最低 RMSE，平均参数比 KAN 少 30 %。</p>
<p><strong>图像分类</strong></p>
<p>CNN+HQKAN 在 CIFAR-10/100 上参数减少 3–11×，准确率持平或略升。</p>
<p><strong>GPT-2 语言建模</strong></p>
<p>HQKAN 替换 MLP 后，参数量从 124 M → 41 M，困惑度下降 5 %，训练时间相同，内存节省 35 %。</p>
<p>总结</p>
<p>论文通过“单比特量子激活 + KAN 结构 + 自编码压缩 + 知识蒸馏”四级流水线，把量子电路的表达优势无缝注入经典网络，<strong>在算法、理论、硬件、系统四个层面同时实现</strong>：</p>
<ol>
<li><strong>算法</strong>：对平滑函数达到 $\Theta(\log\frac{1}{\varepsilon})$ 参数复杂度，指数优于傅里叶 KAN；</li>
<li><strong>理论</strong>：给出 $C^m$ 范数逼近误差与谱扩展的严格界；</li>
<li><strong>硬件</strong>：单比特门深度可执行、可经典大规模模拟；</li>
<li><strong>系统</strong>：提供层扩展、HQKAN、分布式训练、蒸馏部署一整套工具链，可直接替换 Transformer 等大型模型中的 MLP 模块。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>函数逼近</strong>、<strong>图像分类</strong>到<strong>自回归语言建模</strong>三类任务共 7 组实验，系统验证 QKAN/HQKAN 的<strong>精度、参数效率、训练开销与扩展性</strong>。所有实验均在与 KAN、MLP 完全相同的硬件/优化器环境下进行，结果可复现（代码已开源）。</p>
<p>1 含噪符号回归（函数逼近）</p>
<p>子实验</p>
<p>数据集</p>
<p>指标</p>
<p>关键结果</p>
<p><strong>1-a 启发式架构</strong></p>
<p>Feynman 数据集 10 条方程，输入加 10 % 高斯噪声</p>
<p>最佳测试 RMSE</p>
<p>QKAN 在 7/10 方程上最低；平均参数比 KAN 少 30 %。</p>
<p><strong>1-b 无先验架构搜索</strong></p>
<p>扩展至 66 条方程，随机采样 1000 训练/1000 测试</p>
<p>最佳 RMSE + 参数量</p>
<p>QKAN 在 80 % 方程取得最低 RMSE；参数量平均减少 30 %（图 5、表 5）。</p>
<p><strong>1-c 可视化解释性</strong></p>
<p>同 1-a 模型</p>
<p>学习到的边激活曲线</p>
<p>即使存在噪声，DARUAN 仍能学到光滑、符号对齐的非线性（图 6）。</p>
<p>2 图像分类（CNN backbone + 不同 FCN）</p>
<p>数据集</p>
<p>模型</p>
<p>Top-1 / Top-5 准确率</p>
<p>FCN 参数量</p>
<p>结论</p>
<p><strong>MNIST</strong></p>
<p>CNN+MLP<br>CNN+KAN<br>CNN+QKAN<br>CNN+HQKAN</p>
<p>97.7 % / 100 %<br>98.0 % / 100 %<br>95.9 % / 99.7 %<br>—</p>
<p>850<br>1 500<br>800<br><strong>222</strong></p>
<p>QKAN 用最少参数达到最佳 Top-1；HQKAN 再减 3× 参数且精度持平。</p>
<p><strong>CIFAR-10</strong></p>
<p>同上</p>
<p>71.4 % / 97.8 %<br>68.4 % / 97.4 %<br>68.8 % / 97.0 %<br><strong>71.6 % / 97.9 %</strong></p>
<p>41 k<br>39.9 k<br>21.3 k<br><strong>14.4 k</strong></p>
<p>HQKAN 参数最少，Top-1 超越 MLP 与 KAN。</p>
<p><strong>CIFAR-100</strong></p>
<p>同上</p>
<p>39.8 % / 69.4 %<br>40.6 % / 70.4 %<br><strong>41.2 % / 70.0 %</strong><br>39.9 % / <strong>70.6 %</strong></p>
<p>86 k<br>384 k<br>205 k<br><strong>32.6 k</strong></p>
<p>HQKAN 在参数少 11× 情况下 top-5 最高。</p>
<p>3 自回归语言建模（GPT-2）</p>
<p>子实验</p>
<p>设置</p>
<p>指标</p>
<p>关键结果</p>
<p><strong>3-a 中等规模</strong></p>
<p>WebText 数据集，12 层 GPT-2，隐藏 768<br>替换 <strong>全部</strong>线性层为 HQKAN</p>
<p>困惑度（Perplexity）<br>参数量<br>训练内存</p>
<p>HQKAN 41 M vs MLP 124 M<br>↓ 35 % 内存<br>↓ 5 % 困惑度（图 3）</p>
<p><strong>3-b 大规模多卡</strong></p>
<p>采用 <strong>KANsformer</strong>（仅替换 FFN）+ Flash Attention</p>
<p>迭代时间 / GPU 内存</p>
<p>单 RTX 4090：60 ms vs 63 ms，内存 4.1 GB vs 6.5 GB<br>16×H200 batch 800：13.2 s vs 14.8 s，内存 1 224 GB vs 1 340 GB（表 4）</p>
<p>4 知识蒸馏（QKAN → KAN）</p>
<p>任务</p>
<p>流程</p>
<p>指标</p>
<p>结果</p>
<p>二维函数 $f(x,y)=\sin(e^x+y^2)$</p>
<p>1. 训练 QKAN 500 epoch<br>2. 采样 → B 样条系数<br>3. 初始化 KAN 再训练 1000 epoch</p>
<p>测试 MSE</p>
<p>蒸馏后 KAN 比“从零训练”KAN <strong>MSE 降低 70 %</strong>（图 4）。</p>
<p>5 训练与资源消耗对照</p>
<ul>
<li><strong>优化器</strong>：L-BFGS（回归）/ Adam（图像）/ AdamW（GPT-2），学习率、权重衰减完全一致。</li>
<li><strong>硬件</strong>：RTX 4090、4×V100S、32×H100、16×H200；多节点 InfiniBand。</li>
<li><strong>可复现性</strong>：PyTorch 实现，单卡与多卡脚本一致，随机种子固定。</li>
</ul>
<p>实验结论一览</p>
<ol>
<li><strong>精度</strong>：在 80 % 符号方程、CIFAR-100 Top-1、GPT-2 困惑度上 <strong>QKAN/HQKAN 均优于或持平最强基线</strong>。</li>
<li><strong>参数</strong>：同等精度下，QKAN 平均 <strong>−30 %</strong>；HQKAN 极端场景 <strong>−90 %</strong>。</li>
<li><strong>训练开销</strong>：单卡迭代时间持平；多卡大 batch 下 <strong>内存 −10~35 %，时间 −10 %</strong>。</li>
<li><strong>扩展性</strong>：层扩展与 HQKAN 使 QKAN 可无缝嵌入 <strong>十亿级语言模型</strong> 而无需超参微调。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可<strong>直接延续 QKAN 框架</strong>，在理论、算法、硬件与系统四个层面进一步展开；每条均给出<strong>可验证的开放问题</strong>与<strong>初步思路</strong>，便于快速落地。</p>
<p>1 理论层面</p>
<p>开放问题</p>
<p>现状</p>
<p>下一步思路</p>
<p><strong>1-a 高维输入频谱爆炸</strong></p>
<p>定理 2.2 仅给出单变量谱界；多元张量积后频率数达 $\prod_i (3r_i-1)$。</p>
<p>推导多元 QKAN 的 <strong>有效秩（effective rank）</strong> 与 <strong>覆盖数（covering number）</strong>，建立高维统计学习界。</p>
<p><strong>1-b 深度-宽度权衡</strong></p>
<p>目前仅分析单层单比特；深层 QKAN 的 <strong>梯度流/神经正切核</strong> 尚未刻画。</p>
<p>将 DARUAN 视为随机特征，研究 <strong>无限宽深 QKAN 的核极限</strong> 与 <strong>单调收敛条件</strong>。</p>
<p><strong>1-c 量子优势严格性</strong></p>
<p>经典模拟单比特电路是 P 问题；需界定 <strong>何时必须量子硬件</strong> 才能指数加速。</p>
<p>对比 <strong>经典张量网络模拟成本</strong> 与 <strong>量子并行采样成本</strong>，给出 <strong>qubit-number-to-accuracy 阈值定理</strong>。</p>
<p>2 算法层面</p>
<p>开放问题</p>
<p>现状</p>
<p>下一步思路</p>
<p><strong>2-a 自动架构搜索</strong></p>
<p>实验仍靠人工设定宽度/深度/r。</p>
<p>将 r、隐藏宽度、HQKAN 瓶颈维度纳入 <strong>可微 NAS</strong>（DARTS-Quantum），以验证损失下降梯度为奖励，<strong>一键搜索最优量子-经典混合骨架</strong>。</p>
<p><strong>2-b 多目标优化</strong></p>
<p>目前仅优化损失；未显式约束 <strong>参数个数、FLOPs、碳排放</strong>。</p>
<p>采用 <strong>帕累托前沿演化算法</strong>（MOEA），把“参数量-准确率-推理延迟”同时作为目标，生成 <strong>边缘设备友好</strong>的 QKAN 家族。</p>
<p><strong>2-c 动态层扩展策略</strong></p>
<p>固定周期增加 r；可能过早或过晚。</p>
<p>引入 <strong>基于梯度范数/频谱熵的早扩准则</strong>，让 r 在训练过程中 <strong>自适应增长</strong>，类似自适应网格细化。</p>
<p>3 硬件与物理实现</p>
<p>开放问题</p>
<p>现状</p>
<p>下一步思路</p>
<p><strong>3-a NISQ 设备真实测评</strong></p>
<p>论文仅在经典模拟器运行。</p>
<p>在 <strong>IBM 127-qubit Falcon</strong> 或 <strong>IonQ Aria</strong> 上部署单比特流水线，测量 <strong>读出错误缓解</strong> 后精度与模拟器差距，给出 <strong>量子误差-逼近误差联合界</strong>。</p>
<p><strong>3-b 片上分布式 QKAN</strong></p>
<p>单卡模拟多量子节点尚未物理并行。</p>
<p>利用 <strong>量子云联邦</strong>（Rigetti/QCS）把不同 DARUAN 边分配到多芯片，通过 <strong>经典参数服务器同步</strong>，验证 <strong>量子张量并行</strong> 加速比。</p>
<p><strong>3-c 两比特门扩展</strong></p>
<p>单比特无纠缠；是否值得引入？</p>
<p>设计 <strong>受控-旋转两比特块</strong> 作为 <strong>交叉边激活</strong>，研究 <strong>纠缠熵-表达力</strong> 关系；若梯度方差随 qubit 指数衰减即放弃，保持单比特可扩展哲学。</p>
<p>4 应用与系统</p>
<p>开放问题</p>
<p>现状</p>
<p>下一步思路</p>
<p><strong>4-a 科学计算逆问题</strong></p>
<p>仅测试符号回归。</p>
<p>将 HQKAN 作为 <strong>可学习代理模型</strong> 嵌入 PDE 求解器，用 <strong>PINN-QKAN 混合</strong> 解决高维 Navier–Stokes 反演；对比经典 PINN 收敛速度。</p>
<p><strong>4-b 多模态大模型</strong></p>
<p>仅文本 GPT-2 实验。</p>
<p>把 HQKAN 引入 <strong>视觉-语言 Transformer</strong>（BLIP-2、LLaVA），替换 FFN，观察 <strong>跨模态对齐指标</strong> 是否因量子激活而提升。</p>
<p><strong>4-c 边缘 AI 部署</strong></p>
<p>未量化、未剪枝。</p>
<p>对 DARUAN 参数做 <strong>8-bit 量子旋转角量化</strong> 与 <strong>边激活稀疏化</strong>，在 <strong>ARM-M55 + Ethos-U55</strong> 上测能效，目标 <strong>&lt;1 W 下 ImageNet 70 % Top-1</strong>。</p>
<p>5 可验证基准与数据集</p>
<ul>
<li><strong>QKAN-Bench</strong>：开源统一基准，含<br>– 66 条符号方程 + 10 % 噪声；<br>– 3 个 CV 任务 + 3 个 NLP 任务；<br>– 自动记录参数量、CO₂ 排放、单/多卡时间。<br>供后续研究 <strong>一键比较</strong>。</li>
</ul>
<p>6 短期可落地选题（3–6 个月）</p>
<ol>
<li><strong>自适应 r 调度器</strong>：基于频谱熵触发，PyTorch 插件 &lt;200 行代码，即可在现有仓库提 PR。</li>
<li><strong>NISQ 实测报告</strong>：选 5 条符号方程，IonQ 云跑 10 次平均，写 2 页 <strong>量子误差-精度折中</strong> 报告。</li>
<li><strong>HQKAN-TinyML</strong>：将 QKAN 压缩至 &lt;100 k 参数，在 Raspberry Pi Pico 上跑 CIFAR-10 推理，验证 <strong>毫秒级延迟</strong>。</li>
</ol>
<p>总结</p>
<p>QKAN 把“量子表达力”与“经典可扩展”首次无缝缝合，留下一大片<strong>空白交叉地带</strong>：</p>
<ul>
<li>理论：高维、深层、误差界；</li>
<li>算法：NAS、多目标、自适应；</li>
<li>硬件：真实 NISQ、联邦并行、两比特权衡；</li>
<li>应用：科学计算、多模态、边缘部署。</li>
</ul>
<p>任何一条均可成独立课题，且<strong>实验平台与代码基线已就绪</strong>，可立即推进。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心贡献一句话</strong>：<br>用<strong>单比特数据重上传量子电路 DARUAN</strong> 取代 KAN 的 B 样条边激活，得到 <strong>量子启发 KAN（QKAN）</strong>，在<strong>经典可高效模拟</strong>的前提下实现<strong>参数指数压缩、表达力指数增强</strong>，并验证其<strong>从符号回归到 GPT-2 大规模语言模型</strong>的全栈可用性。</p>
<p>1 问题与动机</p>
<ul>
<li>变分量子电路（VQC）表达力高，但多比特方案面临 NISQ 噪声与贫瘠高原。</li>
<li>KAN 用可学习激活函数提升精度与解释性，却受 B 样条 <strong>O(N²LG)</strong> 参数拖累。<br>→ 需一种 <strong>量子级表达力 + 经典级可扩展</strong> 的激活函数。</li>
</ul>
<p>2 方法总览</p>
<p>模块</p>
<p>作用</p>
<p>关键技术点</p>
<p><strong>DARUAN</strong></p>
<p>单比特量子变分激活函数</p>
<p>可训练输入权重 $w_\ell$ 使频率上限 <strong>指数增长</strong>；定理证明参数量 <strong>Θ(log 1/ε)</strong>。</p>
<p><strong>QKAN</strong></p>
<p>用量子边激活替换 KAN 的 B 样条</p>
<p>层内每边独立单比特电路，节点求和；保持 KAN 解释性，参数量 <strong>−30 %</strong>。</p>
<p><strong>HQKAN</strong></p>
<p>解决高维参数二次爆炸</p>
<p>自编码式压缩-扩张，<strong>10× 参数削减</strong> 且精度持平或更高。</p>
<p><strong>层扩展</strong></p>
<p>训练过程动态加深</p>
<p>新增块初始化为恒等，<strong>无损已学特征</strong>，缓解梯度消失。</p>
<p><strong>知识蒸馏</strong></p>
<p>量子训练→经典推理</p>
<p>采样 DARUAN 输出，拟合 B 样条系数，<strong>70 % 测试误差下降</strong>。</p>
<p>3 理论结果</p>
<ul>
<li><strong>Theorem 2.1</strong>（傅里叶 KAN 通用逼近）：$C^m$ 误差 <strong>O(K⁻⁽ᵏ⁺¹⁻ᵐ⁾)</strong>。</li>
<li><strong>Theorem 2.2</strong>（DARUAN 谱与误差）：<br>– 权重 $w_\ell=2^{\ell-1}$ 时最高频率 <strong>2ʳ−1</strong>；<br>– 达到误差 ε 仅需 <strong>r=Θ(log 1/ε)</strong> 层，<strong>指数少于</strong> 经典傅里叶 KAN。</li>
</ul>
<p>4 实验结果</p>
<p>任务</p>
<p>数据集</p>
<p>关键指标</p>
<p>结果</p>
<p><strong>符号回归</strong></p>
<p>Feynman 66 方程 + 10 % 噪声</p>
<p>测试 RMSE</p>
<p>QKAN 在 <strong>80 %</strong> 方程最优，<strong>平均 −30 % 参数</strong>。</p>
<p><strong>图像分类</strong></p>
<p>MNIST / CIFAR-10 / CIFAR-100</p>
<p>Top-1 / Top-5</p>
<p>CNN+HQKAN <strong>参数 −90 %</strong>，准确率持平或 <strong>+0.8 %</strong>。</p>
<p><strong>语言建模</strong></p>
<p>WebText，GPT-2 12 层</p>
<p>困惑度</p>
<p>HQKAN <strong>41 M vs MLP 124 M</strong>，<strong>−35 % 内存</strong>，<strong>−5 % 困惑度</strong>。</p>
<p>5 结论与影响</p>
<ul>
<li><strong>理论</strong>：首次给出量子激活函数的 <strong>参数-误差指数优势</strong> 严格界。</li>
<li><strong>硬件</strong>：单比特门即可实现，<strong>NISQ 立即可用</strong>；同时经典 GPU 模拟高效。</li>
<li><strong>系统</strong>：提供“<strong>量子训练-经典推理-分布式并行</strong>”全栈方案，<strong>可插拔替换 Transformer 的 MLP</strong>。</li>
</ul>
<p>QKAN 为 <strong>量子机器学习走向大规模实用</strong> 提供了可扩展、可验证、可部署的范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Quantum%20Variational%20Activation%20Functions%20Empower%20Kolmogorov-Arnold%20Networks，链接是  https://arxiv.org/pdf/2509.14026  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14026  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jiun-Cheng Jiang,Morris Yu-Chao Huang,Tianlong Chen,Hsi-Sheng Goan</p>
<p><strong>Categories</strong>: quant-ph,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14026.pdf">https://arxiv.org/pdf/2509.14026.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14026">https://arxiv.org/abs/2509.14026</a></p>
<p><strong>Arxiv ID</strong>: 2509.14026</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14026">https://papers.cool/arxiv/2509.14026</a></p>
<p><strong>Published</strong>: 2025-09-17T14:28:42Z</p>
<p><strong>Updated</strong>: 2025-09-17T14:28:42.000Z</p>
<hr>
<h3 id="14-Synthesizing-Behaviorally-Grounded-Reasoning-Chains-A-Data-Generation-Framework-for-Personal-Finance-LLMs"><a href="#14-Synthesizing-Behaviorally-Grounded-Reasoning-Chains-A-Data-Generation-Framework-for-Personal-Finance-LLMs" class="headerlink" title="14. Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.14180">Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs</a></h3><p>Personalized financial advice requires consideration of user goals, constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on support systems for investors and financial planners. Simultaneously, numerous recent studies examine broader personal finance tasks, including budgeting, debt management, retirement, and estate planning, through agentic pipelines that incur high maintenance costs, yielding less than 25% of their expected financial returns. In this study, we introduce a novel and reproducible framework that integrates relevant financial context with behavioral finance studies to construct supervision data for end-to-end advisors. Using this framework, we create a 19k sample reasoning dataset and conduct a comprehensive fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test split and a blind LLM-jury study, we demonstrate that through careful data curation and behavioral integration, our 8B model achieves performance comparable to significantly larger baselines (14-32B parameters) across factual accuracy, fluency, and personalization metrics while incurring 80% lower costs than the larger counterparts.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>个性化的财务建议需要考虑用户的目标、限制、风险承受能力和管辖区。之前的LLM工作集中在支持投资者和财务规划师的系统上。同时，许多最近的研究通过产生高维护成本的代理管道，研究更广泛的个人财务任务，包括预算编制、债务管理、退休和遗产规划，导致其预期财务回报不到25%。在这项研究中，我们提出了一种新颖且可重复的框架，将相关的财务背景与行为金融学研究相结合，以构建端到端顾问的监督数据。利用该框架，我们创建了一个19000样本的推理数据集，并对Qwen-3-8B模型进行了全面的微调。在保留的测试集和盲审LLM评审研究中，我们证明通过精心的数据策划和行为整合，我们的8B模型在事实准确性、流畅性和个性化指标上，达到了比显著更大基线（14-32B参数）相当的性能，同时成本比更大的对手低80%.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>个性化理财建议自动化生成</strong>中的三大核心难题：</p>
<ol>
<li>现有大模型在复杂、多步骤的个人财务场景下准确率不足，且规模越大成本越高；</li>
<li>主流“多智能体”方案维护与推理成本高昂，实际部署后收益往往低于预期的25 %；</li>
<li>通用模型缺乏对用户心理、行为偏见的显式建模，导致建议缺乏共情与可信度。</li>
</ol>
<p>为此，作者提出<strong>行为 grounded 推理链数据生成框架</strong>，将金融知识、行为金融研究与用户心理线索一次性“烘焙”进19 k训练样本，直接微调8 B参数模型，使其在事实准确性、流畅度与个性化指标上逼近14–32 B规模模型，同时降低约80 %的运营成本。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><p><strong>早期非 LLM 理财推荐</strong><br>Zibriczky (2016) 系统综述了协同过滤与案例推理在贷款、保险等封闭场景的应用，为后续数据驱动方法奠定基准。</p>
</li>
<li><p><strong>通用 LLM 财务问答评测</strong><br>Hean et al. (2025) 对 ChatGPT、Claude 等进行标准化金融素养问卷测试，发现大模型在复杂查询上仍有显著准确率缺口，直接触发对领域专用数据的需求。</p>
</li>
<li><p><strong>多智能体/代理式财务决策系统</strong></p>
<ul>
<li>Ding et al. (2024) 提出反射-辩论型代理架构全景图。</li>
<li>Okpala et al. (2025) 构建“代理班组”(data-scientist + compliance-checker) 自动化建模与风控流程。</li>
<li>Takayanagi et al. (2025a,b) 的 FinPersona 对话代理强调人格化交互。<br>这些研究共同揭示：代理方案虽功能强大，但集成遗留系统、合规与推理成本极高，实际 ROI &lt; 25 % (Meimandi et al., 2025)。</li>
</ul>
</li>
<li><p><strong>LLM 金融偏见与风险</strong></p>
<ul>
<li>Zhou et al. (2025) 首次系统性验证大模型存在锚定、过度自信等行为偏见，且单纯继续预训练可能放大非理性。</li>
<li>Winder et al. (2024) 发现生成式建议会强化地域集中、趋势追逐等投资偏差，提高组合风险。<br>这些结论支撑了“必须在数据层面植入去偏见信号”而非仅靠模型扩容。</li>
</ul>
</li>
<li><p><strong>实时数据持续摄入的大模型</strong><br>FinGPT 系列 (Yang et al., 2023; Wang et al., 2023; Zhang et al., 2023; Liu et al., 2023) 通过流式市场数据不断更新权重，侧重“更大+更新”，而本文聚焦“更小+行为矫正”，形成互补。</p>
</li>
<li><p><strong>检索增强 (RAG) 与模块化框架</strong><br>Gao et al. (2024) 的 Modular-RAG 把知识源拆分为可插拔组件，本文据此构建“金融知识 + 行为心理”双路检索，为后续推理链提供高信噪比证据包。</p>
</li>
</ul>
<p>综上，既有文献或侧重评测、或侧重代理架构、或侧重实时数据，而本研究首次将<strong>行为金融偏见</strong>与<strong>心理线索显式注入训练数据本身</strong>，以数据为中心替代堆叠参数或复杂编排，从而弥补规模-效率-可信度之间的落差。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“<strong>数据为中心</strong>”的思路，把复杂、高成本的 agentic 流程拆成<strong>可复现的四段式数据生成管线</strong>，将金融知识、行为偏见、心理线索一次性固化到 19 k 训练样本中，再轻量微调 8 B 模型，具体步骤如下：</p>
<ol>
<li><p>真实场景采集</p>
<ul>
<li>从 Reddit r/personalfinance 公开存档抽取 405 k 帖子，经主题过滤与语义去重，保留 19 k 覆盖八大理财主题的疑难问题。</li>
</ul>
</li>
<li><p>模块化链-of-thought 生成</p>
<ul>
<li><strong>Query Analysis</strong>：抽取冲突、主体、关键数字。</li>
<li><strong>Context Analysis（Modular-RAG）</strong>：<br>– 金融知识库（≈ 600 k tokens，Investopedia + Bogleheads 等）<br>– 行为金融库（≈ 300 k tokens，认知偏差、代际差异等）<br>双路检索→重排→LLM 摘要，输出 15 条高信噪比证据。</li>
<li><strong>Psychological Cue ID</strong>：用 LLM 识别情感、确定性、潜在意图，为后续共情语调提供参数。</li>
<li><strong>Response Rubric</strong>：整合前三步输出，生成“写作指令”而非直接回答，保证后续响应可复现、可验证。</li>
</ul>
</li>
<li><p>多陪审团（LLM-Jury）质量门<br>每阶段产物由 gemini-2.0-flash + o4-mini 在三-shot 框架下排序，择优进入下一轮，防止幻觉与风格漂移。</p>
</li>
<li><p>端到端微调<br>用上述 19 k 样本（15.6 k train / 2.6 k val）对 Qwen-3-8B 进行 4 epoch 监督微调，仅 3 小时单 A100 完成。</p>
</li>
<li><p>评价与成本对比</p>
<ul>
<li>500 题自动评测：BLEURT 比 Gemma-3-27B 高 5 %，BERTScore 差距 &lt; 2 %。</li>
<li>504 题盲评 LLM-Jury：8 B 模型在 Accuracy、Plausibility、Relevance 上逼近 27–32 B 级，Borda 得分领先所有 &lt;14 B 对手。</li>
<li>运营成本：显存 16 GB，推理 34 s/query，托管 $0.8/h，较 24 B+ 模型节省约 80 % 费用。</li>
</ul>
</li>
</ol>
<p>通过“<strong>把复杂推理与心理建模提前写进数据</strong>”，论文用 1/4 参数实现可部署、低成本且行为可信的个人理财顾问，无需繁重多智能体编排即可在真实场景提供个性化、去偏见的建议。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共执行了<strong>三类实验</strong>，覆盖<strong>自动指标、盲评陪审团与成本-效率</strong>三个维度，系统验证所提数据框架的有效性：</p>
<p>1 自动参考评测（500 题 held-out 集）</p>
<ul>
<li><strong>数据</strong>：利用生成框架预先产生 500 条涵盖八大理财主题的问答对，并经 LLM-Jury 校验作为 ground-truth。</li>
<li><strong>指标</strong>：<ul>
<li>BERTScore（基于 Qwen-3-8B embeddings）衡量语义准确度。</li>
<li>BLEURT 评估流畅度/人味。</li>
</ul>
</li>
<li><strong>结果</strong>：<ul>
<li>8 B 模型 BLEURT 0.4600，比 Gemma-3-27B↑5 %；BERTScore 0.7000，与 27 B 级差距 &lt; 2 %。</li>
</ul>
</li>
</ul>
<p>2 盲式 LLM-Jury 排行（504 题零样本）</p>
<ul>
<li><strong>数据</strong>：采集 Reddit 2023-06 之后新帖，确保与训练集零重叠；共 504 条真实用户难题。</li>
<li><strong>流程</strong>：<ul>
<li>候选模型零-shot 生成答复，匿名打乱。</li>
<li>排除同一家族评委（无 Gemini/Qwen），选用 DeepSeek-V3-0324 ×5 次 + Kimi-k2 ×3 次独立排行。</li>
<li>三维度：Accuracy / Plausibility / Relevance → 原始秩转 Borda 分再平均。</li>
</ul>
</li>
<li><strong>一致性检验</strong>：Kendall τ=0.64，Spearman ρ=0.79，显示评委间显著一致。</li>
<li><strong>结果</strong>：<ul>
<li>8 B 模型综合 Borda 分逼近 27-32 B 集团，显著优于所有 &lt;14 B 基线。</li>
<li>参数效率（Borda/十亿参数）在三项指标均列第一，验证“小模型+高质数据”路线。</li>
</ul>
</li>
</ul>
<p>3 成本-延迟-显存实测（504 查询并发）</p>
<ul>
<li><strong>环境</strong>：RunPod 云，4 并发请求，记录端到端延迟与托管单价。</li>
<li><strong>结果</strong>：<ul>
<li>显存 16 GB，推理 34.15 s/query，总成本 $0.96，较 24 B+ 模型降低约 80 %。</li>
</ul>
</li>
</ul>
<p>4 定性案例对比（附录 E）</p>
<ul>
<li>选取 3 则真实 Reddit 疑难（18 岁理财、Roth 转换、短期流动性），与 32 B/7 B 基线并排比较。</li>
<li>独立评委裁定：32 B 仍居首，但 8 B 模型在结构、共情与可执行性上显著优于同尺寸 7 B，展示“越级”表现。</li>
</ul>
<p>综上，实验从<strong>自动指标→人工偏好→经济可行性</strong>逐层验证：仅 8 B 参数的微调模型即可在复杂个人理财场景逼近或超越 27-32 B 大模型，同时把运营成本压缩到 1/5。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向按“数据-模型-系统-评测”四轴归纳如下：</p>
<p>1 数据与知识扩展</p>
<ul>
<li><strong>多源异构采集</strong><ul>
<li>除 Reddit 外，引入银行/券商客服日志、理财社群、问卷开放题，覆盖银发族、小企业主等低线上化人群。</li>
</ul>
</li>
<li><strong>区域化与多语言</strong><ul>
<li>构建“区域信号”检测器（货币符号、税制关键词、拼写变体），用 Mixture-of-Experts 为各法域生成专属专家数据，实现一键移植。</li>
</ul>
</li>
<li><strong>深度心理特征</strong><ul>
<li>结合标准化量表（如 DOSPERT、FSS）或金融心理咨询文本，训练风险厌恶、延迟折扣等细粒度标签，提升偏见识别分辨率。</li>
</ul>
</li>
<li><strong>多模态信号</strong><ul>
<li>引入用户上传的账单截图、资产曲线图像，设计 Vision-RAG 分支，处理“表格+折线图”混合证据。</li>
</ul>
</li>
</ul>
<p>2 模型与对齐策略</p>
<ul>
<li><strong>偏好优化而非继续 SFT</strong><ul>
<li>用 DPO/IPO 对“合规-共情-准确性”三维偏好排序，减少昂贵人工标注；引入 RLAIF 抑制幻觉。</li>
</ul>
</li>
<li><strong>可解释数值推理</strong><ul>
<li>在解码端集成符号计算插件（SymPy、Tax-Calc），对税率、复利、摊销表进行“可验证执行”，降低法规类幻觉。</li>
</ul>
</li>
<li><strong>增量式区域专家热插拔</strong><ul>
<li>冻结通用 backbone，只训练 &lt;1 B 参数的 LoRA 专家，实现新法域上线 &lt;30 min 快速适配。</li>
</ul>
</li>
</ul>
<p>3 系统与部署</p>
<ul>
<li><strong>Thin-Agent 骨架策略</strong><ul>
<li>将微调模型作为“主干策略”，仅在置信度低时调用外部 API（监管库、实时行情、计算引擎），量化 cost-latency 曲线。</li>
</ul>
</li>
<li><strong>端侧-边缘混合推理</strong><ul>
<li>8 B 模型经 4-bit 量化可装入笔记本 GPU/手机 NPU，探索离线问答 + 云端校验的混合模式，提升隐私与合规。</li>
</ul>
</li>
<li><strong>持续学习 &amp; 遗忘</strong><ul>
<li>设计“年度政策补丁”增量训练与旧政策遗忘机制，避免 2025 税则与 2027 税则冲突导致的时序幻觉。</li>
</ul>
</li>
</ul>
<p>4 评测、安全与伦理</p>
<ul>
<li><strong>纵向时间漂移评测</strong><ul>
<li>建立“跨年回归槽”：每季度抽取新颁布法规案例，测量模型在未见政策上的准确率衰减，形成“月份-性能”曲线。</li>
</ul>
</li>
<li><strong>对抗与红队测试</strong><ul>
<li>构造诱导性提问（避税捷径、高杠杆赌博）评估模型拒绝能力；引入动态 jailbreak 奖励模型，强化安全对齐。</li>
</ul>
</li>
<li><strong>用户信任与行为影响</strong><ul>
<li>与持牌理财师合作，对真实用户进行 A/B 测试，记录采纳率、组合波动、情绪改善，验证“共情建议”是否真正转化为更健康的金融行为。</li>
</ul>
</li>
<li><strong>公平性 &amp; 偏见再审计</strong><ul>
<li>检查模型对低收入/少数族裔建议是否系统性地推荐更高费率产品；引入群体公平约束，对输出分布做偏差校正。</li>
</ul>
</li>
</ul>
<p>5 商业模式与合规</p>
<ul>
<li><strong>生成式顾问责任边界</strong><ul>
<li>探索“人机协同”契约：模型生成草稿→注册顾问签字，形成可追踪的责任链，满足 SEC、MiFID II 等对投资建议留痕要求。</li>
</ul>
</li>
<li><strong>数据许可与隐私</strong><ul>
<li>研究联邦学习或差分隐私下的跨机构数据共建，解决银行内部数据无法出境的合规痛点。</li>
</ul>
</li>
</ul>
<p>简言之，**“区域化增量训练 + 可验证计算 + 偏好对齐 + 时间漂移评测”**四条线并进，可在不扩大参数规模的前提下持续提升个性化、准确度与合规性，同时保持 80 % 成本优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心内容可概括为“<strong>一条数据管线、一个 8 B 模型、三项实验验证、一个成本结论</strong>”：</p>
<ol>
<li><p><strong>问题</strong><br>复杂个人理财场景下，通用大模型准确率不足，多智能体方案维护与推理成本高昂，且普遍忽视用户行为偏见与心理线索。</p>
</li>
<li><p><strong>方法</strong><br>提出“<strong>行为 grounded 推理链数据生成框架</strong>”——四段模块化管线：</p>
<ul>
<li>Query/Context/Psychological Analysis → Response Rubric<br>双路 RAG 分别检索金融知识与行为偏见证据；每阶段用 LLM-Jury 排序过滤，生成 19 k 高质量 CoT 样本，直接微调 Qwen-3-8B。</li>
</ul>
</li>
<li><p><strong>实验</strong></p>
<ul>
<li><strong>自动指标</strong>（500 题）：BLEURT 超 Gemma-3-27B 5 %，BERTScore 差距 &lt; 2 %。</li>
<li><strong>盲评陪审团</strong>（504 新帖）：8 B 模型综合 Borda 分逼近 27-32 B 集团，参数效率全面第一。</li>
<li><strong>成本测算</strong>：推理 34 s/Query，托管 $0.8/h，总成本较 24 B+ 模型降 80 %。</li>
</ul>
</li>
<li><strong>结论</strong><br>无需堆参数或复杂编排，仅通过“<strong>高质量行为-金融融合数据</strong>”即可让 8 B 模型在个性化、准确度、可信度上越级对标 32 B，同时经济可行，为生产级个人理财助手提供了可复现的数据-centric 路线。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Synthesizing%20Behaviorally-Grounded%20Reasoning%20Chains%3A%20A%20Data-Generation%20Framework%20for%20Personal%20Finance%20LLMs，链接是  https://arxiv.org/pdf/2509.14180  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.14180  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Akhil Theerthala</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG,68T50,I.2.7; J.4</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.14180.pdf">https://arxiv.org/pdf/2509.14180.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.14180">https://arxiv.org/abs/2509.14180</a></p>
<p><strong>Arxiv ID</strong>: 2509.14180</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.14180">https://papers.cool/arxiv/2509.14180</a></p>
<p><strong>Published</strong>: 2025-09-17T17:12:38Z</p>
<p><strong>Updated</strong>: 2025-09-17T17:12:38.000Z</p>
<hr>
<h3 id="15-LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators"><a href="#15-LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators" class="headerlink" title="15. LLM-I: LLMs are Naturally Interleaved Multimodal Creators"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13642">LLM-I: LLMs are Naturally Interleaved Multimodal Creators</a></h3><p>We propose LLM-Interleaved (LLM-I), a flexible and dynamic framework that reframes interleaved image-text generation as a tool-use problem. LLM-I is designed to overcome the “one-tool” bottleneck of current unified models, which are limited to synthetic imagery and struggle with tasks requiring factual grounding or programmatic precision. Our framework empowers a central LLM or MLLM agent to intelligently orchestrate a diverse toolkit of specialized visual tools, including online image search, diffusion-based generation, code execution, and image editing. The agent is trained to select and apply these tools proficiently via a Reinforcement Learning (RL) framework that features a hybrid reward system combining rule-based logic with judgments from LLM and MLLM evaluators. Trained on a diverse new dataset using four different model backbones, LLM-I demonstrates state-of-the-art performance, outperforming existing methods by a large margin across four benchmarks. We also introduce a novel test-time scaling strategy that provides further performance gains. Project Page: <a target="_blank" rel="noopener" href="https://github.com/ByteDance-BandAI/LLM-I">https://github.com/ByteDance-BandAI/LLM-I</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们提出了LLM-交错（LLM-I），这是一个灵活和动态的框架，将交错的图像文本生成重新定义为一种工具使用的问题。LLM-I旨在克服当前统一模型的”单一工具”瓶颈，这些模型仅限于合成图像，并在需要事实依据或程序精确度的任务中表现不佳。我们的框架使中央LLM或MLLM代理能够智能地协调一套多样化的专业视觉工具，包括在线图像搜索、基于扩散的生成、代码执行和图像编辑。代理通过一种结合基于规则的逻辑与LLM和MLLM评估者判断的混合奖励系统的强化学习（RL）框架，经过训练能够熟练地选择和应用这些工具。在使用四种不同模型骨干的多样化新数据集上进行训练，LLM-I展示了最先进的性能，在四个基准测试中大幅超越现有方法。我们还引入了一种新的测试时间缩放策略，提供了进一步的性能提升。项目页面：<a target="_blank" rel="noopener" href="https://github.com/ByteDance-BandAI/LLM-I。">https://github.com/ByteDance-BandAI/LLM-I。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“交错图文生成”任务提出 LLM-Interleaved（LLM-I）框架，核心动机是<strong>打破现有统一模型在视觉生成环节只能依赖单一扩散工具的“单工具瓶颈”</strong>。该瓶颈导致：</p>
<ul>
<li><strong>事实性缺失</strong>：无法检索或引用真实世界图像，只能生成“虚构”合成图；</li>
<li><strong>精确性不足</strong>：难以完成数据可视化、科学插图等需要程序级精度的任务；</li>
<li><strong>灵活性受限</strong>：架构固化，新增能力必须重训整个模型，成本高昂。</li>
</ul>
<p>LLM-I 将大型语言/多模态模型重新定位为**“工具使用者”而非“全知生成器”<strong>，通过强化学习让中央智能体动态调度搜索、扩散、代码执行、图像编辑四种专用工具，实现</strong>真实照片、创意合成、数据图表、编辑修改<strong>等多类型图像的按需插入，从而在保证叙事连贯、风格一致、语义对齐的前提下，显著提升交错图文生成的</strong>事实 grounded、程序精确与内容多样**三大维度表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归入三大主线，并指出其局限，进而凸显 LLM-I 的差异化定位：</p>
<ol>
<li><p>交错图文生成（Interleaved Image-Text Generation）</p>
<ul>
<li>两阶段/组合式<ul>
<li>NExT-GPT、SEED-X：LLM 先生成文本，再调用外部扩散模型逐图合成；存在“语义鸿沟”且图像数量固定。</li>
</ul>
</li>
<li>统一端到端<ul>
<li>Show-o、Anole、Transfusion、Chameleon：用单一 Transformer 同时建模文本与视觉 token；只能产出合成图，无法检索真实照片或执行代码绘图，陷入“单工具瓶颈”。</li>
</ul>
</li>
</ul>
</li>
<li><p>强化学习用于大模型（RL for LLMs/MLLMs）</p>
<ul>
<li>PPO、GRPO、DAPO、GSPO 等算法主要被用来激发推理能力（数学、代码、逻辑）。</li>
<li>LLM-I 首次将 RL 目标转向<strong>多模态工具调用与图文对齐</strong>，而非单纯推理。</li>
</ul>
</li>
<li><p>LLM 工具使用（Tool-Use LLMs）</p>
<ul>
<li>闭源：OpenAI o3 / DeepResearch、Gemini 2.5 Pro 可调用搜索、代码、图像工具，但无交错图文生成的系统训练与评测。</li>
<li>开源：Search-o1、Openthinkimg 侧重检索或视觉工具，未涉及<strong>动态多工具协同</strong>与<strong>强化学习微调</strong>。</li>
</ul>
</li>
</ol>
<p>综上，现有工作要么局限于“单工具”统一生成，要么仅用工具增强单轮任务；LLM-I 首次把**“多工具调度+强化学习+交错图文生成”**整合为完整框架，并构建相应数据集与评测基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“交错图文生成”重新建模为**「工具调用」**问题，通过以下四层设计一次性解决“单工具瓶颈”：</p>
<ol>
<li><p>框架：Agent + 工具箱</p>
<ul>
<li>中央 LLM/MLLM 仅负责<strong>高层规划与叙事</strong>，不再直接生成像素。</li>
<li>提供四种互补视觉工具：<br>– 在线搜索（真实照片）<br>– 扩散生成（创意合成）<br>– 代码执行（数据可视化）<br>– 图像编辑（二次精修）</li>
<li>统一协议：模型在文本中插入结构化标签 <code>&lt;imgen&gt;{…}&lt;/imgen&gt;</code>，解析器自动路由到对应工具并回插图像，实现<strong>单遍生成-调用-拼装</strong>。</li>
</ul>
</li>
<li><p>训练：面向工具调用的强化学习</p>
<ul>
<li>构建 4k「工具导向」隐式提示数据集：只描述目标与图像数量，不指定工具，迫使模型<strong>自主推理</strong>调用方式。</li>
<li>混合奖励函数：  <script type="math/tex; mode=display">R = w\_{\\text{rule}}R\_{\\text{rule}} + w\_{\\text{llm}}R\_{\\text{llm}} + w\_{\\text{mllm}}R\_{\\text{mllm}}·R\_{\\text{rule}}</script>– $R_{\text{rule}}$：硬约束（图像数量、标签格式）<br>– $R_{\text{llm}}$：文本流畅度、工具标签合理性<br>– $R_{\text{mllm}}$：图文一致性、图像质量</li>
<li>采用 GRPO/GSPO 无值函数 RL 算法，直接优化工具选择概率与参数生成。</li>
</ul>
</li>
<li><p>推理：Test-time Scaling</p>
<ul>
<li>并行采样 n 条候选 → 工具调用合法性过滤 → Top-k 精选 → 针对失败调用自动重试/改码 → MLLM 润色 → 再排序输出最佳。</li>
<li>以额外 4 次前向计算代价，显著降低调用失败率并提升图文对齐度。</li>
</ul>
</li>
<li><p>评测：新基准 LLMI-Bench</p>
<ul>
<li>30 个“mini-project”式任务，图像须承载高信息密度（图表、真实照片、科学插图等）。</li>
<li>10 条<strong>可验证</strong>客观指标替代模糊打分，确保评估聚焦于工具是否正确、图像是否精准满足任务需求。</li>
</ul>
</li>
</ol>
<p>通过“Agent 调度多工具 + RL 精准训练 + 推理时缩放 + 客观评测”四管齐下，论文在 OpenING、ISG、LLMI-Bench 等四项基准上<strong>平均提升 10–30 %</strong>，且 4B/30B 模型均达到 100 % 工具调用成功率，从而系统性地解决了现有统一模型的事实性、精确性与灵活性缺陷。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕「训练效果、泛化能力、工具贡献、奖励设计、测试时缩放」五个维度展开系统实验，全部在 4 个独立基准上完成，主要结果如下表所示（数值为 Overall 分数，满分 10 或 100）。</p>
<p>实验类别</p>
<p>关键对照</p>
<p>主要结果（↑ 表示显著优于对照）</p>
<p>1. 主实验（§5.2）</p>
<p>与 9 个基线对比（GPT-4o+DALL·E3、Gemini+FLUX、Show-o、Anole 等）</p>
<p>LLM-I-30B 在 OpenING 达 8.66↑（SOTA+0.46），在 ISG 达 0.618↑（SOTA+0.044），在 LLMI-Bench 达 92.7↑（SOTA+20+）。</p>
<p>2. 工具调用准确率</p>
<p>记录可执行 tag 占比</p>
<p>LLM-I-4B &amp; 30B 在 LLMI-Bench 均达到 100 % Tool Acc；基线 GPT-4o w/Tool 仅 67.9 %。</p>
<p>3. 奖励消融（§5.4）</p>
<p>分别去掉 R_rule、R_llm、R_mllm</p>
<p>去掉 R_rule 后 OpenING 掉到 4.76（-3.42）；去掉任一评委仍保持 7.7+，验证三组件互补。</p>
<p>4. 工具消融</p>
<p>仅保留 diffusion / 仅保留 search</p>
<p>LLM-I-4B「仅 diffusion」掉到 76.5（-12.4）；「仅 search」掉到 77.5（-11.4），证明多工具协同是性能主因。</p>
<p>5. 测试时缩放（§5.3）</p>
<p>四阶段逐步开启</p>
<p>LLM-I-4B 在 LLMI-Bench 从 88.9 → 95.1（+6.2），耗时仅 ≈20 s，4 次额外前向即可。</p>
<p>6. 训练过程监控</p>
<p>曲线记录</p>
<p>图 3 显示 R_rule、R_llm、R_mllm 同步上升；图 6 显示工具 F1 由 0.4 增至 0.85，无显式工具奖励仍持续优化。</p>
<p>7. 域内测试集</p>
<p>自建 200+ 样本</p>
<p>LLM-I-30B 整体得分 89.9，比最强基线 Qwen3-30B 68.7 提升 21.2 分。</p>
<p>综上，实验覆盖对比、消融、曲线、人力评测四项范式，一致验证：</p>
<ol>
<li>多工具框架本身带来两位数绝对提升；</li>
<li>RL 混合奖励是训练稳定与高质量的关键；</li>
<li>Test-time Scaling 可在小模型上反超大模型，且额外计算可控。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>工具空间扩展</strong></p>
<ul>
<li>引入视频、3D、音频生成或编辑 API，实现真正的多模态交错文档。</li>
<li>接入可微分图像处理（DiffVG、DiffShader）或 CAD/Blender 脚本，满足工程制图、分子结构等专业化视觉需求。</li>
</ul>
</li>
<li><p><strong>工具协同与多跳推理</strong></p>
<ul>
<li>允许同一文档内<strong>多工具链式调用</strong>（搜索→编辑→再编辑），需设计新的标签嵌套语法与奖励函数。</li>
<li>研究「工具依赖图」预测，让模型在生成文本前即规划最优调用路径，减少失败重试。</li>
</ul>
</li>
<li><p><strong>动态工具增删与元学习</strong></p>
<ul>
<li>采用元强化学习或 LoRA 插件机制，遇到新工具只需少量 prompt 或参数即可零样本接入，无需整体重训。</li>
<li>探索工具描述语义嵌入，与任务向量匹配，实现<strong>运行时自动发现与绑定</strong>外部 API。</li>
</ul>
</li>
<li><p><strong>细粒度奖励与可验证生成</strong></p>
<ul>
<li>对代码绘图引入<strong>可执行单元测试奖励</strong>（如 matplotlib 返回的像素级差异、数据点坐标误差），替代纯 MLLM 打分。</li>
<li>对检索图像引入<strong>实体识别 + 知识库对齐奖励</strong>，降低「图文不符」幻觉。</li>
</ul>
</li>
<li><p><strong>测试时缩放再升级</strong></p>
<ul>
<li>研究<strong>工具调用树搜索</strong>（MCTS / Beam Search）替代简单 Top-k，显式优化图文互信息。</li>
<li>引入「分块级」 polishing，仅重写局部段落而非全文，降低延迟。</li>
</ul>
</li>
<li><p><strong>多语言与跨文化适配</strong></p>
<ul>
<li>构建非英文提示子集，验证工具选择在多语言场景下的稳定性；</li>
<li>针对地域性实体（地标、法规）测试检索工具的区域偏差。</li>
</ul>
</li>
<li><p><strong>人类偏好对齐与安全</strong></p>
<ul>
<li>结合 RLHF 或 DPO，把「真实度、版权、隐私」纳入奖励，防止搜索或编辑产生侵权/伪造图像。</li>
<li>开发<strong>可解释性面板</strong>，展示每一步工具调用理由与来源 URL，提升可信度。</li>
</ul>
</li>
<li><p><strong>Benchmark 细化</strong></p>
<ul>
<li>按行业（医疗、金融、科研）划分任务，评估工具在专业术语、可视化规范上的准确性；</li>
<li>引入<strong>交互式评测</strong>：人类实时修改需求，考察 Agent 的在线调整与工具重调度能力。</li>
</ul>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><p><strong>问题</strong>：现有统一模型在交错图文生成中受“单工具瓶颈”限制，只能合成虚拟图像，无法 grounded 真实世界或执行精确可视化，且架构僵化、扩展成本极高。</p>
</li>
<li><p><strong>思路</strong>：把任务重塑为「工具调用」而非「端到端生成」，让 LLM/MLLM 充当 Agent，动态编排搜索、扩散、代码、编辑四类专用视觉工具，实现“proficient tool-user”范式。</p>
</li>
<li><p><strong>方法</strong>：</p>
<ol>
<li>统一标签协议 <code>&lt;imgen&gt;{...}&lt;/imgen&gt;</code>，单遍生成即可触发多工具并行调用。</li>
<li>构建 4k「工具导向」隐式提示数据集，用 GRPO/GSPO 强化学习训练；奖励 = 规则约束 + LLM 文本评分 + MLLM 图文对齐，三组分互补防 hack。</li>
<li>推理阶段引入四步 Test-time Scaling：采样→合法性过滤→Top-k 精选→工具重试/润色→再排序，仅 4 次额外前向即可获得显著增益。</li>
</ol>
</li>
<li><p><strong>实验</strong>：</p>
<ul>
<li>在 OpenING、ISG、LLMI-Bench 与自建域内测试集共 4 个基准上，LLM-I-30B 平均提升 10–30 %，Tool Acc 达 100 %。</li>
<li>消融显示规则奖励是性能底座，多工具协同优于任何单一工具，测试时缩放可让 4B 模型反超 30B 对照。</li>
</ul>
</li>
<li><strong>结论</strong>：LLM-I 验证“大模型+多工具+RL”是突破单工具瓶颈、实现高质量、高真实度交错图文生成的有效路径，为后续扩展工具空间与实时人机协同奠定基础。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是LLM-I%3A%20LLMs%20are%20Naturally%20Interleaved%20Multimodal%20Creators，链接是  https://arxiv.org/pdf/2509.13642  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13642  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zirun Guo,Feng Zhang,Kai Jia,Tao Jin</p>
<p><strong>Categories</strong>: cs.LG,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13642.pdf">https://arxiv.org/pdf/2509.13642.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13642">https://arxiv.org/abs/2509.13642</a></p>
<p><strong>Arxiv ID</strong>: 2509.13642</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13642">https://papers.cool/arxiv/2509.13642</a></p>
<p><strong>Published</strong>: 2025-09-17T02:33:29Z</p>
<p><strong>Updated</strong>: 2025-09-17T02:33:29.000Z</p>
<hr>
<h3 id="16-Image-Tokenizer-Needs-Post-Training"><a href="#16-Image-Tokenizer-Needs-Post-Training" class="headerlink" title="16. Image Tokenizer Needs Post-Training"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.12474">Image Tokenizer Needs Post-Training</a></h3><p>Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>近期的图像生成模型通常在预构建的隐空间中捕捉图像分布，依赖于冻结的图像标记器。然而，重建与生成分布之间存在显著差异，当前的标记器仅优先考虑发生在生成训练之前的重建任务，而没有考虑采样过程中的生成错误。在本文中，我们全面分析了离散隐空间中这种差异的原因，并由此提出了一种新颖的标记器训练方案，包括主训练和后训练，重点分别在于改进隐空间构建和解码。在主训练过程中，提出了一种隐扰动策略以模拟采样噪声，即在生成推理中生成的意外标记。具体而言，我们提出了一种即插即用的标记器训练方案，这显著增强了标记器的鲁棒性，从而提升了生成质量和收敛速度，并提出了一种新颖的标记器评估指标，即pFID，它成功地将标记器性能与生成质量关联起来。在后训练中，我们进一步优化了标记器解码器，以适应一个经过良好训练的生成模型，从而减小生成标记和重建标记之间的分布差异。使用一个约4亿参数的生成器，经过我们提出的主训练训练的离散标记器达到了显著的1.60 gFID，并在额外的后训练后进一步获得了1.36 gFID。进一步的实验广泛验证了我们的后训练策略在现成的离散和连续标记器，以及自回归和基于扩散的生成器上的有效性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文核心关注的问题是：<strong>图像 tokenizer 在“重建阶段”与“生成阶段”的 latent 分布不一致</strong>，导致重建指标优异的 tokenizer 未必能带来更好的生成质量。具体表现为：</p>
<ol>
<li>训练阶段 tokenizer 只用“干净”真值 token 做重建，而推理阶段生成模型输出的 token 往往带有采样误差，即存在 out-of-distribution (OOD) token。</li>
<li>这种分布差异使得解码器在推理时面对“噪声”latent 缺乏鲁棒性，造成生成图像质量下降。</li>
<li>现有工作只关注重建指标（rFID），忽视了 tokenizer 对生成误差的容忍能力，缺乏能够预测生成性能的评测指标。</li>
</ol>
<p>为此，作者提出两阶段方案 RobusTok：</p>
<ul>
<li><strong>Main-training</strong>：在训练 tokenizer 时引入“latent perturbation”，用可控制的 token 替换模拟采样误差，提升潜在空间鲁棒性；并引入新指标 pFID 评估该鲁棒性。</li>
<li><strong>Post-training</strong>：固定编码器与量化器，仅微调解码器，让解码器适配已训练好的生成模型输出的 latent，进一步缩小“重建-生成”分布差距。</li>
</ul>
<p>实验表明，该方法在 ImageNet 256×256 上仅用约 400 M 参数的生成器即可将 gFID 从 1.60 降至 1.36，且对离散/连续 tokenizer、自回归/扩散模型均有效。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 3 页“2 RELATED WORKS”与全文实验部分共涉及以下四条主线，可归纳为：</p>
<p>1. 图像 Tokenizer / 视觉离散化</p>
<ul>
<li><p><strong>VQ-VAE / VQGAN 系列</strong></p>
<ul>
<li>VQ-VAE (van den Oord et al., 2017)</li>
<li>VQGAN (Esser et al., 2021)</li>
<li>后续改进：VQGAN-LC (Zhu et al., 2024a)、MaskBit (Weber et al., 2024)、TiTok (Yu et al., 2024d)、MAGVIT-v2 (Yu et al., 2023b)、HQ-VAE (Takida et al., 2023)、MoVQ (Zheng et al., 2022)、One-D-Piece (Miwa et al., 2025)</li>
<li>开源框架：XQGAN (Li et al., 2024d)、Open-MAGVIT2 (Luo et al., 2024)</li>
</ul>
</li>
<li><p><strong>连续潜空间 Tokenizer</strong></p>
<ul>
<li>LDM-4 (Rombach et al., 2022)、DiT (Peebles &amp; Xie, 2023)、MAR (Li et al., 2024b)、MAETok (Chen et al., 2025a)</li>
</ul>
</li>
<li><p><strong>语义/结构正则化</strong></p>
<ul>
<li>使用 DINOv2、CLIP、MAE 特征约束潜空间（Li et al., 2024c；Radford et al., 2021；He et al., 2022）</li>
</ul>
</li>
</ul>
<p>2. 自回归视觉生成 (Autoregressive Image Generation)</p>
<ul>
<li><p><strong>基于 Transformer 的像素/token 级自回归</strong></p>
<ul>
<li>ImageGPT (Chen et al., 2020-style)、LlamaGen (Sun et al., 2024)、RAR (Yu et al., 2024b)、VAR (Tian et al., 2024)、RandAR (Pang et al., 2024b)、FlowAR (Ren et al., 2024)、Infinity (Han et al., 2024)</li>
</ul>
</li>
<li><p><strong>并行/非自回归生成</strong></p>
<ul>
<li>MaskGIT (Chang et al., 2022)、GIVIT (Tschannen et al., 2024)、RCG (Li et al., 2024a)</li>
</ul>
</li>
</ul>
<p>3. 扩散模型 (Diffusion-based Generators)</p>
<ul>
<li><p><strong>潜空间扩散</strong></p>
<ul>
<li>LDM (Rombach et al., 2022)、DiT (Peebles &amp; Xie, 2023)、ADM (Dhariwal &amp; Nichol, 2021)</li>
</ul>
</li>
<li><p><strong>与 Tokenizer 结合</strong></p>
<ul>
<li>使用 MAETok、TiTok 等连续/离散 tokenizer 作为潜空间输入 (Chen et al., 2025a; Yu et al., 2024d)</li>
</ul>
</li>
</ul>
<p>4. 鲁棒性 &amp; 噪声/扰动相关研究</p>
<ul>
<li><p><strong>对抗扰动与 Representation Robustness</strong></p>
<ul>
<li>针对视觉编码器的输入扰动工作 (Chen et al., 2024a; Li et al., 2024f,g)</li>
</ul>
</li>
<li><p><strong>扩散模型训练中的噪声策略</strong></p>
<ul>
<li>轻微损坏提升扩散质量 (Chen et al., 2024a)、Slight-Corruption Pre-training (Chen et al., 2024a)</li>
</ul>
</li>
<li><p><strong>VAE 重参数化噪声、SDEdit 中间域插值</strong></p>
<ul>
<li>用于在扩散采样中实现“重建-生成”平滑过渡 (Meng et al., 2021)</li>
</ul>
</li>
</ul>
<p>5. 评测指标</p>
<ul>
<li><strong>重建指标</strong>：rFID、IS、LPIPS、PSNR</li>
<li><strong>生成指标</strong>：gFID、IS、Precision/Recall</li>
<li><strong>新指标</strong>：pFID（本文提出，通过人为向潜空间注入扰动再计算 FID，衡量 tokenizer 对采样误差的鲁棒性）</li>
</ul>
<p>综上，RobusTok 在以下方面与现有工作区分：</p>
<ol>
<li>首次系统分析“重建-生成”分布差异，提出 latent-perturbation 训练；</li>
<li>提出 pFID 指标，直接关联 tokenizer 鲁棒性与下游生成质量；</li>
<li>给出即插即用的 post-training，适配已有离散/连续 tokenizer 与自回归/扩散生成器。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“重建-生成分布不一致”拆成 <strong>两个子问题</strong>，分别用 <strong>两阶段训练</strong> 解决：</p>
<p>阶段 1：Main-training——让潜在空间对“采样误差”免疫</p>
<p><strong>核心操作：Latent Perturbation + 语义正则 + 退火</strong></p>
<p>步骤</p>
<p>公式/算法</p>
<p>目的</p>
<p>1. 正常 VQ 编码</p>
<p>$z′ = \arg\min_{e_k∈C} |z − e_k|_2^2$</p>
<p>得到离散真值 token</p>
<p>2. 扰动注入</p>
<p>以概率 α 随机替换 $P=αHW$ 个 token 为其在码本中的 top-δ 近邻</p>
<p>模拟 AR 采样时会产生的“近邻错误”</p>
<p>3. 语义保持</p>
<p>用 <strong>冻结 DINOv2</strong> 特征做对比损失，确保扰动后仍保留高层语义</p>
<p>避免扰动把 latent 拖出语义流形</p>
<p>4. 退火策略</p>
<p>训练过程中把 α 从 1.0 线性降到 0.5</p>
<p>先学鲁棒性，再收敛到精细重建</p>
<p>5. 解码器重建</p>
<p>仅用 <strong>被扰动后的 token</strong> $z′′$ 重建图像，损失同 VQGAN（L2+L_VQ+L_perc+L_adv+L_sem）</p>
<p>强制解码器“看懂”噪声 latent</p>
<p><strong>结果</strong>：得到 <strong>RobusTok-base</strong>，pFID ↓，生成器训练收敛更快，gFID 从 3.80→1.60（LlamaGen-400 M）。</p>
<p>阶段 2：Post-training——让解码器“看懂”生成器实际吐出的 token</p>
<p><strong>核心操作：只微调解码器，用“混合”latent 对齐生成分布</strong></p>
<p>步骤</p>
<p>公式/算法</p>
<p>目的</p>
<p>1. 生成器固定</p>
<p>用已训练好的 AR（或扩散）模型产生 latent</p>
<p>获得真实“采样误差”分布</p>
<p>2. 构造混合 latent</p>
<p>定义 <strong>preservation ratio</strong> σ = N_recon / (HW)</p>
<p>在每条序列里随机把 σ 比例 token 换回真值，其余保持生成值</p>
<p>3. 配对监督</p>
<p>把混合 latent 重新送进 <strong>冻结编码器-量化器</strong> 得到 $z_σ$，再用原图作目标，仅微调解码器</p>
<p>解码器学会“把带噪的生成 token 还原成真图”</p>
<p>4. 扩散模型同理</p>
<p>对扩散生成改用 <strong>SDEdit</strong>：σ = 1−t/T</p>
<p>同一思路平滑插值“重建-生成”</p>
<p><strong>结果</strong>：RobusTok-base → RobusTok-final，gFID 再降到 <strong>1.36</strong>，且对任意现成的离散/连续 tokenizer 均适用（表 3）。</p>
<p>附加工具：新指标 pFID</p>
<ul>
<li>对验证集每张图做 <strong>5×3=15 组扰动</strong>（α∈{0.5,…,0.9}，δ∈{200,280,360}）</li>
<li>计算扰动重建图与原图的 FID 取平均</li>
<li><strong>pFID 与 gFID 相关系数 0.89</strong>，而传统 rFID 几乎不相关（图 5）——无需训练生成器即可快速筛选 tokenizer。</li>
</ul>
<p>一句话总结</p>
<p><strong>先让 tokenizer 在“模拟采样噪声”下学会鲁棒潜在空间，再让解码器在“真实生成噪声”下二次校准</strong>，两阶段各司其职，把重建-生成分布差距一次性补齐。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“重建-生成分布不一致”共设计 <strong>4 组实验 + 3 项诊断性分析</strong>，覆盖 <strong>离散/连续 tokenizer</strong> 与 <strong>自回归/扩散</strong> 两类生成器，所有结果均在 ImageNet 256×256 标准基准完成。</p>
<p>1. 主实验：RobusTok 自回归生成</p>
<p>设置</p>
<p>指标</p>
<p>结果</p>
<p>LlamaGen-B（~400 M）</p>
<p>gFID</p>
<p>3.80 → 1.60（主训练）→ 1.36（+post）</p>
<p>RAR-400 M</p>
<p>gFID</p>
<p>1.70 → 1.60（换用 RobusTok）→ 1.36（+post）</p>
<p>IS / Precision / Recall</p>
<p>同步提升，参数量&lt;500 M 下 <strong>SOTA</strong>（表 2）</p>
<p>2. 通用性验证：Post-training 对“现成”tokenizer 是否有效</p>
<p>选取 4 个公开 checkpoint，仅应用第二阶段（固定编码器+微调解码器）：</p>
<p>Tokenizer 类型</p>
<p>生成器</p>
<p>gFID 降幅</p>
<p>LlamaGen-tokenizer（离散）</p>
<p>LlamaGen-B</p>
<p>3.80 → 3.51</p>
<p>GigaTok（离散）</p>
<p>LlamaGen-B</p>
<p>3.84 → 3.68</p>
<p>MAETok（连续）</p>
<p>DiT-L/2</p>
<p>1.87 → 1.68</p>
<p>MAETok-ft（连续+噪声微调）</p>
<p>DiT-L/2</p>
<p>1.74 → 1.70</p>
<blockquote>
<p>所有组合均一致下降，证明 <strong>post-training 与 tokenizer 结构、生成范式无关</strong>（表 3）。</p>
</blockquote>
<p>3. 大规模 Benchmark：11 个公开 tokenizer 横向评测</p>
<p>固定生成器为 LlamaGen-B/L，仅换 tokenizer，报告三项指标：</p>
<p>观察</p>
<p>数据支持</p>
<p>1. 小码本反而生成更好</p>
<p>K=1024 的 gFID 平均低于 K=16384（图 5a）</p>
<p>2. rFID 与 gFID 几乎无相关</p>
<p>rFID-gFID 散点 r≈0.12</p>
<p>3. pFID 与 gFID 强相关</p>
<p>pFID-gFID 散点 r≈0.89（图 5b）</p>
<p>4. 诊断实验</p>
<p>4.1 扰动强度选择</p>
<p>用 MMD 衡量“扰动 latent ↔ 生成 latent”分布距离，确定 <strong>α∈[0.5,0.9]，δ∈[200,360]</strong> 与真实采样误差最接近（图 6）。</p>
<p>4.2 鲁棒潜空间可视化</p>
<p>t-SNE 显示：</p>
<ul>
<li>无扰动训练：码本使用频率均匀 → 少量 OOD token 即崩溃</li>
<li>有扰动训练：出现高频“枢纽”token，少量替换仍稳定重建（图 7,13）</li>
</ul>
<p>4.3 消融：主训练超参</p>
<p>配置</p>
<p>gFID(w/ CFG)</p>
<p>结论</p>
<p>β=0.5</p>
<p>5.40</p>
<p>扰动比例过大，重建掉点</p>
<p>无退火</p>
<p>3.93</p>
<p>模式坍塌，多样性下降</p>
<p>退火到 0</p>
<p>1.97</p>
<p>过于确定，灵活性差</p>
<p>退火到 0.5</p>
<p><strong>1.85</strong></p>
<p>鲁棒-精细最佳平衡点（表 5）</p>
<p>4.4 消融：Post-training 设计</p>
<p>数据量</p>
<p>200 k 已饱和，再增无收益</p>
<p>Epoch</p>
<p>30-40 最佳，过长反涨（表 4a-b）</p>
<p>σ</p>
<p>0.7-0.8 对多数 tokenizer 最优；RobusTok 自身 σ 不敏感（表 4c,9,10）</p>
<p>5. 定性结果</p>
<ul>
<li>图 8 / 16：post-training 后 OOD 结构、颜色漂移、纹理糊点三大典型失败被修复</li>
<li>图 15：256×256 类条件样本展示，1.36 gFID 下细节与真实度同步提升</li>
</ul>
<p>6. 训练效率</p>
<ul>
<li>LlamaGen-B 原训练 1.2 M steps 收敛 → 换 RobusTok 后 <strong>0.8 M steps</strong> 即达到相同 gFID，<strong>收敛速度↑35 %</strong>（图 11）</li>
</ul>
<p>一句话总结</p>
<p>实验从 <strong>SOTA 对比、横向评测、通用迁移、内部消融、可视化、训练效率</strong> 六个维度闭环验证：</p>
<ol>
<li>主训练提升 tokenizer 鲁棒性即可直接降低 gFID；</li>
<li>轻量级 post-training 对任意现有 tokenizer 仍可持续增益；</li>
<li>新指标 pFID 可提前预测生成性能，无需重训练生成器。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对 RobusTok 的“直接延伸”或“底层扩展”，均围绕 <strong>“tokenizer-生成器失配”未被完全解决的角落</strong> 展开，供后续研究参考：</p>
<p>1. 扰动建模：从“人工规则”到“可学习”</p>
<ul>
<li>当前 top-δ 随机替换仅模拟码本近邻误差，与真实 AR 采样分布仍存差距。<br>→ 用 <strong>GAN-style 判别器</strong> 或 <strong>能量模型</strong> 学习“真实采样误差”分布，再对 latent 做 <strong>可微扰动生成</strong>（类似 Adv-VAE）。<br>→ 把扰动模块做成 <strong>plug-in 网络</strong>，与 tokenizer 端到端联合训练，而非手工 α/δ。</li>
</ul>
<p>2. 动态码本 / 在线聚类</p>
<ul>
<li>固定大小码本 → 高频 token 磨损、低频 token 闲置。<br>→ 引入 <strong>K-means 在线更新</strong> 或 <strong>有限标量量化 FSQ</strong>（Mentzer et al. 2023）让码本随训练动态增删，减少“必现 OOD”区域。<br>→ 结合 <strong>稀疏激活</strong> 仅召回部分码本，降低计算同时保持 pFID。</li>
</ul>
<p>3. 生成器-反馈式 Post-training</p>
<ul>
<li>现有 post-training 只利用生成器输出，不反向告诉生成器哪些 token 难解码。<br>→ 构建 <strong>双向对抗博弈</strong>：<ol>
<li>生成器产 token → 解码器重建 → 计算重建误差地图；</li>
<li>误差地图作为 <strong>条件信号</strong> 回传生成器，鼓励其避开高误差 token；</li>
<li>交替更新，形成 <strong>“ tokenizer-生成器”协同优化闭环</strong>。</li>
</ol>
</li>
</ul>
<p>4. 连续 + 离散混合潜空间</p>
<ul>
<li>纯离散码本对细粒度颜色/纹理量化仍有限；纯连续缺乏 AR 易处理性。<br>→ 设计 <strong>残差量化 + 连续残差</strong> 双路径：<ul>
<li>低比特离散码本负责全局结构；</li>
<li>小幅连续残差负责高频细节；</li>
<li>扰动阶段可分别在离散/连续域施加不同强度噪声，研究二者鲁棒性权衡。</li>
</ul>
</li>
</ul>
<p>5. 跨模态 / 视频 tokenizer 鲁棒性</p>
<ul>
<li>文生图、文生视频场景下，条件信息会进一步改变采样误差分布。<br>→ 把 <strong>文本-图像交叉注意力图</strong> 作为条件，引入 <strong>条件扰动策略</strong>（某些 token 仅在特定语义区域被替换）。<br>→ 研究 <strong>时间一致性扰动</strong> 对视频 tokenizer 的影响，避免帧间 token 抖动导致闪烁。</li>
</ul>
<p>6. 无监督“误差诊断”指标</p>
<ul>
<li>pFID 仍需 ImageNet 真图；实际生产中无真图可用。<br>→ 探索 <strong>无真值 pFID</strong>：仅用生成图像自身与扰动重建图像的分布距离，加自监督特征（DINO、SwAV）计算 <strong>pseudo-pFID</strong>，实现 <strong>在线监控</strong>。</li>
</ul>
<p>7. 硬件友好型扰动训练</p>
<ul>
<li>扰动训练增加一次码本 top-δ 搜索，显存与耗时↑。<br>→ 采用 <strong>局部敏感哈希 (LSH)</strong> 或 <strong>乘积量化</strong> 近似最近邻，把搜索复杂度从 O(K) 降到 O(log K)。<br>→ 在 FPGA / 手机端部署时，可预存“扰动表”实现 <strong>零开销</strong> 随机替换。</li>
</ul>
<p>8. 与 Diffusion 的“时间步”扰动统一</p>
<ul>
<li>扩散模型已有 schedule 噪声；可研究 <strong>“量化-加噪”混合训练</strong>：<ul>
<li>前向：对 latent 先做 VQ 再做扩散加噪；</li>
<li>反向：扩散去噪后再用 RobusTok 解码。<br>→ 分析 <strong>离散码本误差 vs 高斯误差</strong> 在不同时间步的耦合效应，寻找最优 <strong>混合强度函数</strong>。</li>
</ul>
</li>
</ul>
<p>9. 极端压缩率下的鲁棒性</p>
<ul>
<li>当码本压缩到 <strong>32 token/图</strong> 甚至 <strong>8 token/图</strong> 时，扰动空间急剧缩小，一个 token 错误即崩溃。<br>→ 研究 <strong>语法级纠错</strong>：把 tokenizer 输出视为“句子”，引入 <strong>BERT-style 掩码语言模型</strong> 做 token 纠错，再送入解码器，形成 <strong>级联鲁棒 pipeline</strong>。</li>
</ul>
<p>10. 理论侧：Lipschitz 常数与 pFID 的解析关系</p>
<ul>
<li>文中用 $\text{pFID} \approx \text{FID}(\hat I’, I)$ 作为鲁棒代理，仅经验相关。<br>→ 建立 <strong>解码器 Lipschitz 上界</strong> 与 pFID 的解析不等式，证明“减小解码器 Lipschitz 常数 ⇔ 降低 gFID”，为后续正则化提供 <strong>可优化目标</strong>。</li>
</ul>
<p>总结</p>
<p>RobusTok 首次把“tokenizer 鲁棒性”从经验观察变成可训练对象，但离“完全闭环”仍有距离。<br><strong>可学习扰动、动态码本、生成器-解码器协同、跨模态扩展、理论保证</strong> 五个维度将是下一步最值得深耕的蓝海。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心主旨</p>
<p>现有图像 tokenizer 仅在“重建”阶段训练，面对自回归或扩散模型产生的采样误差（OOD token）时鲁棒性不足，导致“重建好 ≠ 生成好”。作者提出两阶段训练框架 <strong>RobusTok</strong>，一次性提升 tokenizer 的<strong>鲁棒性</strong>与<strong>生成适配性</strong>，并引入新指标 <strong>pFID</strong> 直接关联生成质量。</p>
<p>技术路线（两阶段）</p>
<p>阶段</p>
<p>关键操作</p>
<p>目的</p>
<p>效果</p>
<p>① Main-training</p>
<p>在离散 latent 中随机替换 α 比例 token 为其 top-δ 近邻，加 DINO 语义正则 + 退火</p>
<p>让潜在空间对“采样误差”免疫</p>
<p>ImageNet 256² 上 400 M 生成器 gFID 从 3.80 → 1.60</p>
<p>② Post-training</p>
<p>固定编码器/量化器，仅用“生成器输出 vs 原图”配对微调解码器；用 preservation ratio σ 平滑插值重建-生成</p>
<p>解码器学会“看懂”真实生成分布</p>
<p>gFID 再降至 <strong>1.36</strong>（同参数量 SOTA）</p>
<p>主要贡献</p>
<ol>
<li>系统揭示“重建-生成分布差异”根源，提出 <strong>pFID</strong> 指标，与 gFID 相关系数 0.89，而传统 rFID 仅 0.12。</li>
<li><strong>即插即用</strong>扰动训练，无需改生成器即可提升收敛速度 35%。</li>
<li><strong>通用后训练</strong>对离散/连续、AR/扩散 tokenizer 均一致降 gFID（最大降幅 0.3-0.4）。</li>
<li>在 ImageNet 256×256 上取得 <strong>&lt; 500 M 参数新 SOTA</strong>（gFID 1.36）。</li>
</ol>
<p>一句话总结</p>
<p>RobusTok 通过“<strong>扰动式主训练提鲁棒 + 生成式后训练对齐分布</strong>”，让 tokenizer 首次真正为“生成”而非仅为“重建”服务。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Image%20Tokenizer%20Needs%20Post-Training，链接是  https://arxiv.org/pdf/2509.12474  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.12474  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kai Qiu,Xiang Li,Hao Chen,Jason Kuen,Xiaohao Xu,Jiuxiang Gu,Yinyi Luo,Bhiksha Raj,Zhe Lin,Marios Savvides</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.12474.pdf">https://arxiv.org/pdf/2509.12474.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.12474">https://arxiv.org/abs/2509.12474</a></p>
<p><strong>Arxiv ID</strong>: 2509.12474</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.12474">https://papers.cool/arxiv/2509.12474</a></p>
<p><strong>Published</strong>: 2025-09-15T21:38:03Z</p>
<p><strong>Updated</strong>: 2025-09-15T21:38:03.000Z</p>
<hr>
<h3 id="17-Hybrid-Quantum-Classical-Model-for-Image-Classification"><a href="#17-Hybrid-Quantum-Classical-Model-for-Image-Classification" class="headerlink" title="17. Hybrid Quantum-Classical Model for Image Classification"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.13353">Hybrid Quantum-Classical Model for Image Classification</a></h3><p>This study presents a systematic comparison between hybrid quantum-classical neural networks and purely classical models across three benchmark datasets (MNIST, CIFAR100, and STL10) to evaluate their performance, efficiency, and robustness. The hybrid models integrate parameterized quantum circuits with classical deep learning architectures, while the classical counterparts use conventional convolutional neural networks (CNNs). Experiments were conducted over 50 training epochs for each dataset, with evaluations on validation accuracy, test accuracy, training time, computational resource usage, and adversarial robustness (tested with $\epsilon=0.1$ perturbations).Key findings demonstrate that hybrid models consistently outperform classical models in final accuracy, achieving {99.38\% (MNIST), 41.69\% (CIFAR100), and 74.05\% (STL10) validation accuracy, compared to classical benchmarks of 98.21\%, 32.25\%, and 63.76\%, respectively. Notably, the hybrid advantage scales with dataset complexity, showing the most significant gains on CIFAR100 (+9.44\%) and STL10 (+10.29\%). Hybrid models also train 5—12$\times$ faster (e.g., 21.23s vs. 108.44s per epoch on MNIST) and use 6—32\% fewer parameters} while maintaining superior generalization to unseen test data.Adversarial robustness tests reveal that hybrid models are significantly more resilient on simpler datasets (e.g., 45.27\% robust accuracy on MNIST vs. 10.80\% for classical) but show comparable fragility on complex datasets like CIFAR100 ($\sim$1\% robustness for both). Resource efficiency analyses indicate that hybrid models consume less memory (4—5GB vs. 5—6GB for classical) and lower CPU utilization (9.5\% vs. 23.2\% on average).These results suggest that hybrid quantum-classical architectures offer compelling advantages in accuracy, training efficiency, and parameter scalability, particularly for complex vision tasks.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本研究在三个基准数据集（MNIST、CIFAR100 和 STL10）中对混合量子经典神经网络和纯经典模型进行了系统比较，以评估其性能、效率和鲁棒性。混合模型将参数化量子电路与经典深度学习架构集成在一起，而经典模型则使用传统的卷积神经网络 （CNN）。对每个数据集进行了超过 50 个训练时期的实验，并评估了验证准确性、测试准确性、训练时间、计算资源使用和对抗鲁棒性（使用 $\epsilon=0.1$ 扰动进行测试）。主要发现表明，混合模型在最终准确性方面始终优于经典模型，实现了 {99.38\% （MNIST）、41.69\% （CIFAR100） 和 74.05\% （STL10） 验证准确率，而经典基准分别为 98.21%、32.25\% 和 63.76\%。值得注意的是，混合优势随数据集复杂性而扩展，在 CIFAR100 （+9.44\%） 和 STL10 （+10.29\%） 上显示出最显着的收益。混合模型的训练速度还提高了 5—12$\times$（例如，在 MNIST 上每 epoch 为 21.23 秒，而每个 epoch 为 108.44s），并且使用的参数减少了 6—32\%}，同时保持了对看不见的测试数据的卓越泛化。对抗鲁棒性测试表明，混合模型在更简单的数据集上的弹性明显更强（例如，MNIST 的鲁棒准确率为 45.27%，而经典模型的鲁棒准确率为 10.80%），但在复杂数据集（如 CIFAR100）上表现出相当的脆弱性（两者的鲁棒性为 $\sim$1\%）。资源效率分析表明，混合模型消耗的内存更少（4—5GB 与经典模型的 5—6GB）和更低的 CPU 利用率（平均为 9.5\% 对 23.2\%）。这些结果表明，混合量子经典架构在准确性、训练效率和参数可扩展性方面具有引人注目的优势，特别是对于复杂的视觉任务。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文核心关注的问题是：在图像分类任务中，<strong>混合量子-经典神经网络（hybrid quantum-classical neural networks）是否、以及在何种条件下能够系统性地优于纯经典卷积网络（CNN）</strong>。具体而言，作者围绕以下三个尚未被充分回答的研究空白展开：</p>
<ol>
<li><p><strong>缺乏跨数据集复杂度的系统性基准</strong><br>已有量子机器学习（QML）研究大多只在单个数据集（如 MNIST）上验证量子组件的有效性，且缺少与经典模型的端到端对照。本文首次在 MNIST、CIFAR100、STL10 三个复杂度递增的基准上，用完全一致的训练协议对混合与经典模型进行并行比较。</p>
</li>
<li><p><strong>量子层对训练动态与资源开销的影响尚不清晰</strong><br>过去工作侧重最终准确率，未量化训练时间、内存、CPU 占用等实际成本。本文同步记录这些指标，揭示混合模型在收敛速度、参数规模、能耗上的真实表现。</p>
</li>
<li><p><strong>对抗鲁棒性未被充分评估</strong><br>量子特征是否天然更难被对抗扰动破坏？论文用 $ϵ=0.1$ 的 FGSM 攻击在三个数据集上测量“鲁棒准确率”，给出数据集复杂度与鲁棒性之间的定量关系。</p>
</li>
</ol>
<p>综上，论文试图给出一张“全景图”：在 NISQ 时代可实现的 4-qubit 量子电路条件下，混合量子-经典架构到底能带来多少可验证的精度、效率与鲁棒性优势，以及这些优势如何随任务复杂度变化。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在表 I 中给出了 3 条最具代表性的前期研究，并在引言与讨论部分补充了若干关键文献。可归纳为以下 4 条主线：</p>
<ol>
<li><p>量子核方法（Quantum kernel methods）</p>
<ul>
<li>Havlíček 等 Nature 2019 [5]：在合成数据集上取得 +8% 的核判别力提升，但未与同等规模的深度 CNN 进行端到端比较。</li>
</ul>
</li>
<li><p>参数化量子电路做图像分类</p>
<ul>
<li>Schuld 等 PRL 2019 [3]、PRA 2020 [6]：在 MNIST 上用 4–8 量子比特电路替换全连接层，获得 ≈+5% 准确率，然而未考察训练开销与鲁棒性。</li>
<li>Cong 等 Nature Physics 2019 [7]：提出“量子卷积”概念，在 CIFAR-10 上仅取得 +3% 增益，且缺乏资源消耗分析。</li>
</ul>
</li>
<li><p>量子-经典混合框架的可扩展性研究</p>
<ul>
<li>Biamonte 等 Nature 2017 [1]：从理论上论证量子机器学习可能突破经典算力瓶颈，但未给出具体实验验证。</li>
<li>Preskill Quantum 2018 [4]：提出 NISQ 概念，指出小尺度混合模型是近期可行路径，为本研究提供了硬件边界依据。</li>
</ul>
</li>
<li>对抗鲁棒性与资源效率（尚属空白）<br>既有量子 CNN 或核方法均未报告对抗攻击下的鲁棒准确率，也未系统测量训练时间、内存与 CPU 占用；本文首次将这两类指标纳入跨数据集基准。</li>
</ol>
<p>综上，相关研究验证了“量子组件有用”的孤立案例，但缺乏跨复杂度、跨指标的系统对照；本文正是在此缺口上提供了端到端的基准与资源分析。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>为系统回答“混合量子-经典网络能否、并在何种条件下优于纯 CNN”，论文设计了一套端到端的实验与评估框架，核心步骤如下：</p>
<ol>
<li><p>统一架构模板<br>经典分支：固定 2 层 CNN + 2 层全连接，作为公共特征提取器。<br>量子分支：在特征降维后接入 4-qubit 参数化电路（PennyLane 的 <code>BasicEntanglerLayers</code>），通过 <code>TorchLayer</code> 与 PyTorch 无缝反向传播。<br>两者参数量、训练 Epoch（50）、数据增强、损失函数、优化器（Adam）完全一致，确保“单变量对照”。</p>
</li>
<li><p>三档复杂度数据集<br>MNIST → CIFAR100 → STL10，分辨率、类别数、颜色通道逐级增加，用于观察“量子优势”随任务复杂度的缩放规律。</p>
</li>
<li><p>多维度评估指标</p>
<ul>
<li>精度：验证/测试准确率、F1、混淆矩阵。</li>
<li>效率：单 Epoch 训练时间、总参数量、内存峰值、CPU 占用。</li>
<li>鲁棒性：FGSM 攻击（$ε=0.1$）下的“鲁棒准确率”。</li>
<li>可视化：PCA/t-SNE 特征空间、决策边界、LDA 分离度、Silhouette 分数，解释量子层带来的表示差异。</li>
</ul>
</li>
<li><p>资源追踪与可复现性<br>使用同一台 GPU 工作站，关闭其他进程，记录秒级训练时间、GB 级内存、% 级 CPU；代码与超参数全部开源，保证结果可复现。</p>
</li>
<li><p>统计验证<br>每套实验重复 3 次取均值±标准差；用配对 t 检验确认精度与训练时间差异的显著性（p&lt;0.01）。</p>
</li>
</ol>
<p>通过“固定经典骨架 + 仅替换最后一层特征映射为量子电路”这一最小改动策略，论文把变量限制在“量子 vs 全连接”本身，从而直接量化量子组件带来的边际收益与成本，系统回答了研究问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在完全一致的硬件与超参数设置下，执行了<strong>三组端到端对比实验</strong>，每组实验均覆盖<strong>5 类测量指标</strong>，共计 15 组定量结果。具体实验矩阵如下：</p>
<p>数据集</p>
<p>量子电路配置</p>
<p>训练 Epoch</p>
<p>评估指标</p>
<p>对照组</p>
<p>MNIST</p>
<p>4-qubit, amplitude 编码, 2 层 BasicEntangler</p>
<p>50</p>
<p>①验证/测试准确率 ②F1 ③单 epoch 时间 ④内存/CPU ⑤FGSM-ε=0.1 鲁棒准确率</p>
<p>同结构纯 CNN</p>
<p>CIFAR100</p>
<p>同上</p>
<p>50</p>
<p>同上</p>
<p>同上</p>
<p>STL10</p>
<p>同上</p>
<p>50</p>
<p>同上</p>
<p>同上</p>
<p>实验细节展开：</p>
<ol>
<li><p>训练曲线实验</p>
<ul>
<li>记录每 epoch 的训练/验证 loss、准确率、F1，绘制收敛曲线（图 2、13、24）。</li>
<li>用于判断量子层是否加速收敛、是否减轻过拟合。</li>
</ul>
</li>
<li><p>资源开销实验</p>
<ul>
<li>用 <code>nvidia-ml-py</code> 与 <code>psutil</code> 每秒采样 GPU/CPU/内存，绘制时序占用（图 3、14、25）。</li>
<li>统计单 epoch 平均耗时、总参数量、峰值内存、平均 CPU 利用率。</li>
</ul>
</li>
<li><p>对抗鲁棒性实验</p>
<ul>
<li>在 50-epoch 最佳 checkpoint 上，用 Fast Gradient Sign Method（ε=0.1）生成对抗样本，报告“鲁棒准确率”= 模型在对抗集上的 Top-1 准确率（图 2d、13d、24d）。</li>
</ul>
</li>
<li><p>特征空间实验</p>
<ul>
<li>提取倒数第二层特征，分别做 PCA（前 2 主成分）与 t-SNE（perplexity=30）降维，量化类间分离度（LDA 分离系数、Silhouette 分数）并可视化（图 7、18、29）。</li>
<li>对比量子与经典决策边界的平滑程度（图 8、19、30）。</li>
</ul>
</li>
<li><p>混淆矩阵与错例分析</p>
<ul>
<li>生成 10×10（MNIST）、100×100（CIFAR100）、10×10（STL10）混淆矩阵（图 6、17、28），定位量子模型优势类别；随机展示 10 张预测对比图（图 11、22、33）。</li>
</ul>
</li>
<li><p>可扩展性验证实验</p>
<ul>
<li>保持 4-qubit 不变，仅增减经典 CNN 通道数，验证“参数更少而精度更高”现象是否依旧成立；结果写入表 IV（未给出完整数据，但在正文 5.2 节总结）。</li>
</ul>
</li>
</ol>
<p>所有实验均在同一台 RTX-3080 工作站 + Intel i7-10700K 上完成，PyTorch 1.13 + PennyLane 0.29，随机种子固定，保证三次重复实验标准差 &lt;0.15%。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>更大规模量子电路</strong><br>在真实量子硬件上测试 8–20 量子比特、深度 10–30 层的电路，观察随着纠缠容量指数增长（$2^n$ 希尔伯特空间），复杂数据集（ImageNet-1K、CIFAR100）的精度与鲁棒性是否继续提升。</p>
</li>
<li><p><strong>量子感知对抗防御</strong><br>将参数化量子电路视为“可微物理层”，在对抗训练框架内联合优化旋转角与经典权重，探索能否在高维 RGB 空间重现 MNIST 上的 4× 鲁棒优势。</p>
</li>
<li><p><strong>跨模态迁移</strong><br>把同一 4-qubit 电路作为通用特征映射，直接迁移到文本（Transformer 输出）、语音（MFCC 特征）、时间序列（ECG、金融行情），验证“量子优势”是否独立于图像域。</p>
</li>
<li><p><strong>量子压缩与部署</strong><br>研究知识蒸馏：用深量子电路做教师、浅量子电路做学生，结合剪枝与量化，将参数量再降一个数量级，适配边缘量子处理器（IBM Q 7-bit、IonQ 11-bit）。</p>
</li>
<li><p><strong>可解释量子特征</strong><br>利用保真度（fidelity）与纠缠熵度量，量化每个量子比特对最终分类决策的贡献，构建“量子显著图”，并与经典 Grad-CAM 对齐，揭示量子层到底编码了哪些视觉模式。</p>
</li>
<li><p><strong>变分量子线路架构搜索（VQ-NAS）</strong><br>以验证准确率为奖励，用强化学习自动搜索最优的纠缠模式、旋转门类型与测量可观测量，替代手工设计的 BasicEntanglerLayers。</p>
</li>
<li><p><strong>容错时代的理论极限</strong><br>当 FTQC（fault-tolerant quantum computing）到来后，将量子卷积层替换为 QFT 或量子行走算子，从表示理论角度给出表达能力上界，并与经典 CNN 的 VC 维进行比较。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文系统比较了<strong>混合量子-经典神经网络</strong>与<strong>纯 CNN</strong>在图像分类任务上的性能、效率与鲁棒性，主要结论如下：</p>
<ul>
<li><p><strong>精度优势随任务复杂度放大</strong><br>MNIST / CIFAR100 / STL10 的验证准确率分别提升 <strong>+1.17 %</strong>、<strong>+9.44 %</strong>、<strong>+10.29 %</strong>，且混合模型参数量减少 6–32 %。</p>
</li>
<li><p><strong>训练与推理更高效</strong><br>单 epoch 训练时间缩短 5–12×，CPU 利用率从 23 % 降至 9 %，内存占用降低约 1 GB。</p>
</li>
<li><p><strong>对抗鲁棒性数据集相关</strong><br>在简单数据集 MNIST 上鲁棒准确率提升 4.2×（45.27 % vs 10.80 %）；在 CIFAR100/STL10 上两者均低于 1 %，提示高维特征仍需额外防御机制。</p>
</li>
<li><p><strong>特征空间更可分</strong><br>PCA/t-SNE 可视化显示混合模型类簇更紧凑，LDA 分离度与 Silhouette 分数均优于纯 CNN，决策边界更平滑。</p>
</li>
<li><p><strong>实验可复现且代码开源</strong><br>使用固定 4-qubit、振幅编码、BasicEntangler 两层电路，基于 PennyLane+PyTorch，在相同硬件与超参数下重复三次，结果稳定。</p>
</li>
</ul>
<p>综上，论文给出<strong>首个跨三档复杂度数据集的端到端基准</strong>，证明在 NISQ 规模下，<strong>轻量级量子层即可在精度、速度、参数效率上全面超越纯经典模型</strong>，为后续在真实量子硬件上扩展至更大电路与多模态任务奠定基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Hybrid%20Quantum-Classical%20Model%20for%20Image%20Classification，链接是  https://arxiv.org/pdf/2509.13353  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.13353  。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Muhammad Adnan Shahzad</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.13353.pdf">https://arxiv.org/pdf/2509.13353.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.13353">https://arxiv.org/abs/2509.13353</a></p>
<p><strong>Arxiv ID</strong>: 2509.13353</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.13353">https://papers.cool/arxiv/2509.13353</a></p>
<p><strong>Published</strong>: 2025-09-14T09:55:00Z</p>
<p><strong>Updated</strong>: 2025-09-14T09:55:00.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202509/2025-09-19/">https://wdndev.github.io/daily/hf/202509/2025-09-19/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Hala-Technical-Report-Building-Arabic-Centric-Instruction-amp-Translation-Models-at-Scale"><span class="toc-text">1. Hala Technical Report: Building Arabic-Centric Instruction &amp; Translation Models at Scale</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-SAIL-VL2-Technical-Report"><span class="toc-text">2. SAIL-VL2 Technical Report</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-PANORAMA-The-Rise-of-Omnidirectional-Vision-in-the-Embodied-AI-Era"><span class="toc-text">3. PANORAMA: The Rise of Omnidirectional Vision in the Embodied AI Era</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-GenExam-A-Multidisciplinary-Text-to-Image-Exam"><span class="toc-text">4. GenExam: A Multidisciplinary Text-to-Image Exam</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E6%A0%B8%E5%BF%83%E9%80%9F%E8%A7%88"><span class="toc-text">论文核心速览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Scrub-It-Out-Erasing-Sensitive-Memorization-in-Code-Language-Models-via-Machine-Unlearning"><span class="toc-text">5. Scrub It Out! Erasing Sensitive Memorization in Code Language Models via Machine Unlearning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-MedReseacher-R1-Expert-Level-Medical-Deep-Researcher-via-A-Knowledge-Informed-Trajectory-Synthesis-Framework"><span class="toc-text">6. MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-THOR-Tool-Integrated-Hierarchical-Optimization-via-RL-for-Mathematical-Reasoning"><span class="toc-text">7. THOR: Tool-Integrated Hierarchical Optimization via RL for Mathematical Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-MARS2-2025-Challenge-on-Multimodal-Reasoning-Datasets-Methods-Results-Discussion-and-Outlook"><span class="toc-text">8. MARS2 2025 Challenge on Multimodal Reasoning: Datasets, Methods, Results, Discussion, and Outlook</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Wan-Animate-Unified-Character-Animation-and-Replacement-with-Holistic-Replication"><span class="toc-text">9. Wan-Animate: Unified Character Animation and Replacement with Holistic Replication</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Improving-Context-Fidelity-via-Native-Retrieval-Augmented-Reasoning"><span class="toc-text">10. Improving Context Fidelity via Native Retrieval-Augmented Reasoning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-AERIS-Argonne-Earth-Systems-Model-for-Reliable-and-Skillful-Predictions"><span class="toc-text">11. AERIS: Argonne Earth Systems Model for Reliable and Skillful Predictions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-SteeringControl-Holistic-Evaluation-of-Alignment-Steering-in-LLMs"><span class="toc-text">12. SteeringControl: Holistic Evaluation of Alignment Steering in LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Quantum-Variational-Activation-Functions-Empower-Kolmogorov-Arnold-Networks"><span class="toc-text">13. Quantum Variational Activation Functions Empower Kolmogorov-Arnold Networks</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-Synthesizing-Behaviorally-Grounded-Reasoning-Chains-A-Data-Generation-Framework-for-Personal-Finance-LLMs"><span class="toc-text">14. Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-LLM-I-LLMs-are-Naturally-Interleaved-Multimodal-Creators"><span class="toc-text">15. LLM-I: LLMs are Naturally Interleaved Multimodal Creators</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-Image-Tokenizer-Needs-Post-Training"><span class="toc-text">16. Image Tokenizer Needs Post-Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-Hybrid-Quantum-Classical-Model-for-Image-Classification"><span class="toc-text">17. Hybrid Quantum-Classical Model for Image Classification</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>