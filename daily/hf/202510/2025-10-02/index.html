<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-10-02 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP UseMCP standardizes how LLMs interact with external systems, forming the foundation fo">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-10-02">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202510/2025-10-02/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP UseMCP standardizes how LLMs interact with external systems, forming the foundation fo">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:09.922Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202510/2025-10-02/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-10-02',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:09'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-10-02</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:09.922Z" title="Updated 2025-11-02 07:46:09">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">58.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>211min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-10-02"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use"><a href="#1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use" class="headerlink" title="1. MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.24002">MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use</a></h3><p>MCP standardizes how LLMs interact with external systems, forming the foundation for general agents. However, existing MCP benchmarks remain narrow in scope: they focus on read-heavy tasks or tasks with limited interaction depth, and fail to capture the complexity and realism of real-world workflows. To address this gap, we propose MCPMark, a benchmark designed to evaluate MCP use in a more realistic and comprehensive manner. It consists of $127$ high-quality tasks collaboratively created by domain experts and AI agents. Each task begins with a curated initial state and includes a programmatic script for automatic verification. These tasks demand richer and more diverse interactions with the environment, involving a broad range of create, read, update, and delete (CRUD) operations. We conduct a comprehensive evaluation of cutting-edge LLMs using a minimal agent framework that operates in a tool-calling loop. Empirical results show that the best-performing model, gpt-5-medium, reaches only $52.56$\% pass@1 and $33.86$\% pass^4, while other widely regarded strong models, including claude-sonnet-4 and o3, fall below $30$\% pass@1 and $15$\% pass^4. On average, LLMs require $16.2$ execution turns and $17.4$ tool calls per task, significantly surpassing those in previous MCP benchmarks and highlighting the stress-testing nature of MCPMark.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>MCP 标准化了大型语言模型（LLM）与外部系统的交互方式，构建了通用智能体的基础。然而，现有的 MCP 基准测试仍然范围有限：它们主要关注以读取为主的任务或交互深度有限的任务，无法反映现实工作流程的复杂性和真实性。为填补这一空白，我们提出了 MCPMark，一个旨在以更真实全面的方式评估 MCP 应用的基准测试。它包含由领域专家与 AI 智能体共同创建的 127 个高质量任务。每个任务都从精心设计的初始状态开始，并附有用于自动验证的程序化脚本。这些任务需要与环境进行更丰富多样的交互，涵盖广泛的创建、读取、更新和删除（CRUD）操作。我们使用在工具调用循环中运行的最小智能体框架，对前沿 LLM 进行了全面评估。实验结果显示，性能最好的模型 gpt-5-medium 的 pass@1 仅为 52.56%，pass^4 为 33.86%，而其他广受认可的强模型，包括 claude-sonnet-4 和 o3，其 pass@1 均低于 30%，pass^4 低于 15%。平均来看，LLM 每个任务需要 16.2 次执行回合和 17.4 次工具调用，显著高于以往 MCP 基准测试的水平，凸显了 MCPMark 的压力测试特性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对现有 MCP（Model Context Protocol）评测基准的两大缺陷——任务模式单一（以读操作为主、交互深度浅）与真实度不足——提出 MCPMark，用于在隔离容器环境中对 LLM 进行高保真、多步骤、全 CRUD 覆盖的“压力测试”。具体目标如下：</p>
<ul>
<li><strong>拓宽任务谱</strong>：127 个任务横跨 Notion、GitHub、Filesystem、PostgreSQL、Playwright 五大真实环境，覆盖创建、读取、更新、删除四类操作，平均需 16.2 轮、17.4 次工具调用，远超既有基准。</li>
<li><strong>提升真实度</strong>：每个任务始于经领域专家与 AI 协作设计的“初始状态”，并附带可自动执行的验证脚本，确保评测可复现、结果可量化。</li>
<li><strong>暴露能力边界</strong>：实验显示最强模型 gpt-5-medium 仅获 52.56% pass@1 与 33.86% pass^4，揭示当前模型在长程规划、工具组合调用、跨环境泛化与执行稳定性方面仍存在显著短板。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 MCPMark 直接相关或构成其研究背景的工作可分为三类：LLM Agent 通用框架、工具使用与规划方法、以及 MCP 场景下的评测基准。代表性文献按类别列举如下：</p>
<ol>
<li><strong>LLM Agent 框架与工具调用</strong></li>
</ol>
<ul>
<li>ReAct: Synergizing reasoning and acting in language models – Yao et al., ICLR 2023</li>
<li>Toolformer: Language models can teach themselves to use tools – Schick et al., NeurIPS 2023</li>
<li>ToolLLM: Facilitating LLMs to master 16000+ real-world APIs – Qin et al., arXiv 2023</li>
<li>Gorilla: Large language model connected with massive APIs – Patil et al., NeurIPS 2024</li>
<li>MetaGPT: Meta programming for multi-agent collaboration – Hong et al., ICLR 2024</li>
<li>AutoGen: Enabling next-gen LLM applications via multi-agent conversations – Wu et al., COLM 2024</li>
</ul>
<ol>
<li><strong>规划、反思与长程任务</strong></li>
</ol>
<ul>
<li>Tree of Thoughts: Deliberate problem solving with LLMs – Yao et al., NeurIPS 2023</li>
<li>Reflexion: Language agents with verbal reinforcement learning – Shinn et al., NeurIPS 2023</li>
<li>Executable code actions elicit better LLM agents – Wang et al., ICML 2024</li>
<li>OpenHands: An open platform for AI software developers as generalist agents – Wang et al., arXiv 2024</li>
</ul>
<ol>
<li><strong>MCP / 工具-使用评测基准</strong></li>
</ol>
<ul>
<li>MCPEval: Automatic MCP-based deep evaluation for AI agent models – Liu et al., arXiv 2025</li>
<li>MCP-Universe: Benchmarking LLMs with real-world MCP servers – Luo et al., arXiv 2025</li>
<li>LiveMCPBench: Can agents navigate an ocean of MCP tools? – Mo et al., arXiv 2025</li>
<li>LiveMCP-101: Stress testing and diagnosing MCP-enabled agents – Yin et al., arXiv 2025</li>
<li>MCP-AgentBench: Evaluating real-world language agent performance with MCP-mediated tools – Guo et al., arXiv 2025</li>
<li>MCPWorld: A unified testbed for API, GUI, and hybrid computer-use agents – Yan et al., arXiv 2025</li>
</ul>
<ol>
<li><strong>GUI/网页自动化与容器化评测环境</strong></li>
</ol>
<ul>
<li>WebArena: A realistic web environment for building autonomous agents – Zhou et al., arXiv 2023</li>
<li>Mind2Web: Towards a generalist agent for the web – Deng et al., NeurIPS 2023</li>
<li>OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments – Xie et al., NeurIPS 2024</li>
</ul>
<p>这些工作共同构成了 MCPMark 提出的学术与技术背景：既包括上层 agent 架构与工具学习算法，也涵盖早期 MCP 基准对“读为主、轻量级交互”局限的反思，为本文设计“重交互、全 CRUD、可复现”的高难度评测提供了直接动机与对比基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“构建新基准 + 设计最小评测框架 + 大规模实验”三位一体的方式解决现有 MCP 评测局限，具体手段如下：</p>
<ol>
<li>构建 MCPMark 基准<br>1.1 五域覆盖</li>
</ol>
<ul>
<li>选用 Notion、GitHub、Filesystem、PostgreSQL、Playwright 五类真实 MCP 服务器，共 127 任务、38 个精心初始化的“快照”环境。</li>
<li>每域均提供完整 CRUD 工具集（图 3），确保任务可“写”可“删”，而非只读查询。</li>
</ul>
<p>1.2 人-机协同造题 pipeline（§2.1）</p>
<ul>
<li>Exploration：专家与 Task-Creation-Agent 共探环境，收集深层上下文。</li>
<li>Evolvement：Agent 迭代增加难度（多步、长输入、隐含约束），专家保证真实可行。</li>
<li>Verification：Task-Creation-Agent 自动生成可执行验证脚本；专家与 Task-Execution-Agent 真跑任务，调脚本直至能精准判别 pass/fail。</li>
<li>Iteration：重复 Evolvement &amp; Verification，逐步抬升难度，同时保持可自动验证。</li>
<li>每题 3–5 小时专家工时，并经过交叉评审与社区一个月审查，确保质量。</li>
</ul>
<p>1.3 状态可追踪与隔离</p>
<ul>
<li>所有任务在容器/沙盒内执行，生命周期为“初始快照 → Agent 交互 → 程序验证 → 环境复位”，保证可复现、无副作用。</li>
</ul>
<ol>
<li>设计最小评测框架 —— MCPMark-Agent</li>
</ol>
<ul>
<li>基于 LiteLLM + MCP Python SDK，仅实现“工具调用循环”：模型接收工具列表 → 决定调用 → 观察返回 → 再决策，直至输出终止。</li>
<li>无任务级启发式、无模型专属优化，以裸测方式反映模型内在能力。</li>
<li>统一路由到各厂商官方 API，支持温度、推理力度等默认设置，保证横向可比。</li>
</ul>
<ol>
<li>大规模实验与诊断<br>3.1 指标</li>
</ol>
<ul>
<li>pass@1 / pass@4 / pass^4（四次独立运行全部成功），兼顾“单次能力”与“稳定性”。</li>
</ul>
<p>3.2 结果</p>
<ul>
<li>最强 gpt-5-medium 仅 52.56% pass@1、33.86% pass^4；claude-sonnet-4、o3 等低于 30%/15%。</li>
<li>平均 16.2 轮、17.4 次工具调用，远超既往基准（表 1），验证“压力测试”属性。</li>
<li>本地服务（PostgreSQL、Filesystem）显著高于远程服务（Notion、GitHub），揭示训练数据偏差导致的泛化缺口。</li>
<li>失败 80%+ 为“隐式错误”——任务看似完成但验证不通过，凸显规划与长上下文管理短板。</li>
</ul>
<p>通过上述“高保真任务集 + 可复现验证 + 裸测框架”，论文系统性地暴露了当前 LLM 在长程 CRUD 工作流、跨环境泛化与执行鲁棒性上的瓶颈，为后续算法与系统改进提供了可量化的靶标。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>MCPMark</strong> 进行了系统性的实验评估，涵盖 <strong>模型能力、环境差异、推理策略、失败分析</strong> 等多个维度。具体实验如下：</p>
<p>1. <strong>主实验：127任务全模型评测</strong></p>
<ul>
<li><strong>目标</strong>：衡量各模型在完整MCPMark基准上的通过率与交互成本。</li>
<li><strong>设置</strong>：</li>
<li>模型：覆盖27个主流闭源/开源模型（如gpt-5系列、claude-sonnet-4、o3、grok-4、qwen3-coder-plus等）。</li>
<li>指标：pass@1、pass@4、pass^4（四次独立运行全部成功）。</li>
<li>限制：单任务最多100轮、3600秒超时。</li>
<li><strong>结果</strong>：</li>
<li>最强模型gpt-5-medium仅达52.56% pass@1、33.86% pass^4。</li>
<li>平均需16.2轮、17.4次工具调用，显著高于既往基准。</li>
<li>开源最佳qwen3-coder-plus为24.80% pass@1。</li>
</ul>
<p>2. <strong>分环境实验：五域MCP服务对比</strong></p>
<ul>
<li><strong>目标</strong>：检验模型在不同服务（本地 vs 远程）上的泛化差异。</li>
<li><strong>设置</strong>：将127任务按服务拆分（Filesystem 30、Notion 28、Playwright 25、GitHub 23、PostgreSQL 21）。</li>
<li><strong>结果</strong>：</li>
<li>本地服务（PostgreSQL、Filesystem）显著优于远程（Notion、GitHub）。</li>
<li>例如gpt-5-medium在PostgreSQL达76.19% pass@1，而在Notion仅41.96%。</li>
</ul>
<p>3. <strong>推理力度实验：gpt-5与claude-sonnet-4多档对比</strong></p>
<ul>
<li><strong>目标</strong>：探究“思考token”数量对工具使用表现的影响。</li>
<li><strong>设置</strong>：</li>
<li>gpt-5系列：low / medium / high三档推理力度。</li>
<li>claude-sonnet-4：默认、low、high三档。</li>
<li><strong>结果</strong>：</li>
<li>gpt-5-medium随推理力度提升，pass@1从46.85%→52.56%，但high档略降。</li>
<li>远程服务（GitHub、Notion）受益最大，本地服务几乎饱和。</li>
<li>claude-sonnet-4各档差异&lt;2%，表明推理增益模型相关。</li>
</ul>
<p>4. <strong>交互效率与成本分析</strong></p>
<ul>
<li><strong>目标</strong>：评估“调用了多少轮/工具/钱”与“是否做对”的关系。</li>
<li><strong>设置</strong>：记录每任务输入/输出token、美元成本、轮数、工具调用数。</li>
<li><strong>结果</strong>：</li>
<li>高效模型（gpt-5-medium）用14.7轮、20.2次工具即获最高pass@1。</li>
<li>部分模型（kimi-k2-instruct）&gt;30轮仍成功率低，呈现“过度调用”循环。</li>
<li>成本与准确率无显著正相关：高成本运行常因冗余调用而得分更低。</li>
</ul>
<p>5. <strong>失败归因实验：隐式 vs 显式错误分类</strong></p>
<ul>
<li><strong>目标</strong>：量化失败类型，指导后续改进。</li>
<li><strong>方法</strong>：对未通过任务按统一规则打标签：</li>
<li>隐式失败：任务看似完成但验证不通过（规划/推理/长上下文问题）。</li>
<li>显式失败：上下文溢出、轮数耗尽、提前放弃、畸形调用等。</li>
<li><strong>结果</strong>（图5）：</li>
<li>所有模型隐式失败占比&gt;50%，gpt-5-high、kimi-k2-instruct超80%。</li>
<li>显式失败模型特异：gemini-2.5-flash多“提前放弃+畸形调用”；gpt-5-high多“上下文溢出”；kimi-k2-instruct常“轮数耗尽”。</li>
</ul>
<p>6. <strong>任务-模型细粒度热力图</strong></p>
<ul>
<li><strong>目标</strong>：展示哪些任务对当前模型仍不可解。</li>
<li><strong>输出</strong>：127×27热力图（图6），单元格颜色=四跑成功次数。</li>
<li><strong>观察</strong>：约1/3任务对所有模型0-1次成功，验证基准难度；个别任务仅gpt-5-medium可解，指示能力缺口。</li>
</ul>
<p>7. <strong>案例轨迹可视化</strong></p>
<ul>
<li><strong>目标</strong>：用具体轨迹说明“为何成功/失败”。</li>
<li><strong>内容</strong>：每域选1个代表性任务，给出：</li>
<li>初始状态截图 + 任务描述。</li>
<li>成功与失败模型的工具调用链 + 验证器逐项检查结果。</li>
<li><strong>覆盖</strong>：</li>
<li>Filesystem：联系人信息抽取+CSV生成。</li>
<li>GitHub：ESLint CI工作流搭建。</li>
<li>Notion：多伦多旅行指南标签统一。</li>
<li>Playwright：Cloudflare Turnstile登录。</li>
<li>PostgreSQL：员工项目跟踪表设计。</li>
</ul>
<p>通过上述7组实验，论文从“整体性能-环境差异-推理策略-交互代价-失败模式-个案细节”全链路验证了MCPMark的难度与诊断价值，系统揭示了当前LLM在长程、多工具、真实环境代理任务上的能力边界。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 MCPMark 暴露出的“能力缺口”自然延伸，亦是对基准自身局限的补足，具备短期可验证性与长期研究价值：</p>
<p>1. 模型侧：从“能用”到“稳健”</p>
<ul>
<li><strong>一致性提升</strong></li>
<li>探索<strong>自洽性解码</strong>（self-consistency sampling）、<strong>多数投票</strong>或<strong>迭代修正</strong>（Reflexion 风格）能否把 pass^4 从 30% 级抬升至 70% 级。</li>
<li>研究<strong>不确定性估计</strong>：让模型在置信度低时主动暂停并请求澄清，降低隐式失败。</li>
<li><strong>长上下文与记忆机制</strong></li>
<li>引入<strong>分段摘要 + 记忆缓存</strong>（记忆树/记忆池）缓解 80%+ 上下文溢出错误。</li>
<li>测试<strong>128k→2M 窗口</strong>模型，量化“窗口变大”与“任务变长”之间的边际收益。</li>
<li><strong>工具调用语法鲁棒性</strong></li>
<li>建立<strong>细粒度语法纠错器</strong>（JSON schema checker + 自动修复）实时校正畸形调用，测量额外成本与收益。</li>
<li>研究<strong>多模态工具</strong>（截图→SVG→元素坐标）对浏览器任务的影响。</li>
</ul>
<p>2. 框架侧：从“裸测”到“智能代理”</p>
<ul>
<li><strong>层次化规划</strong></li>
<li>在 MCPMark-Agent 之上引入<strong>高层规划器</strong>（HTN / PDDL）→ 底层工具调用，检验“先规划后执行”能否减少过度调用（&gt;30 轮）现象。</li>
<li><strong>动态工具选择</strong></li>
<li>面对 90+ 工具时，先用<strong>检索式工具选择</strong>（embedding 匹配 / 强化学习）筛 Top-k，再进入调用循环，测量命中率与速度提升。</li>
<li><strong>多代理分工</strong></li>
<li>将 127 任务拆成“前端+后端+DB”角色，运行<strong>多代理并行</strong>（MetaGPT / AutoGen 风格），对比单 agent 与多 agent 的 pass^4 与成本。</li>
</ul>
<p>3. 数据侧：从“人工”到“半自动”</p>
<ul>
<li><strong>难度梯度自动化</strong></li>
<li>利用 LLM+执行轨迹回放，对同一初始状态自动生成<strong>易/中/难</strong>三档子任务，形成<strong>MCPMark-Curriculum</strong>，评估课程学习对中小模型友好度。</li>
<li><strong>远程服务仿真器</strong></li>
<li>针对 Notion/GitHub 训练数据稀缺问题，构建<strong>可复放的高速沙盒 API</strong>（基于录制的 HTTP 流量），支持百万级规模预训练或强化学习。</li>
<li><strong>失败数据再利用</strong></li>
<li>收集各模型在 MCPMark 上的 2 万余条失败轨迹，用<strong>逆强化学习</strong>或<strong>DPO</strong>训练“不再犯同样错”的奖励模型，观察 pass^4 提升上限。</li>
</ul>
<p>4. 任务侧：从“确定”到“模糊”</p>
<ul>
<li><strong>歧义意图理解</strong></li>
<li>引入<strong>用户表述不完整</strong>任务（如“把报表弄好看一点”），允许 agent<strong>主动提问</strong>或<strong>给出多方案供选择</strong>，评测澄清能力与用户满意度。</li>
<li><strong>多语言/跨地域</strong></li>
<li>将同一任务模板本地化到<strong>日文/德文网页</strong>或<strong>多国 Notion 模板</strong>，测量模型在非英语语境下的工具调用准确率，观察文化差异带来的新失败模式。</li>
</ul>
<p>5. 系统侧：从“离线”到“在线”</p>
<ul>
<li><strong>实时副作用管理</strong></li>
<li>在真实 GitHub/Notion 账号上运行<strong>轻量版在线 MCPMark-Live</strong>，引入<strong>事务回滚</strong>与<strong>差分补偿</strong>机制，评估模型在“不可逆操作”场景下的安全性。</li>
<li><strong>成本-鲁棒帕累托前沿</strong></li>
<li>建立<strong>多目标优化</strong>（pass^4 vs 美元成本 vs 延迟）基准，鼓励社区提交<strong>帕累托最优</strong>代理实现，推动“又好又省”的工业级方案。</li>
</ul>
<p>6. 评测侧：从“单基准”到“生态”</p>
<ul>
<li><strong>服务器多样化</strong></li>
<li>快速接入<strong>Slack、Jira、AWS、Azure、Kubernetes</strong>等官方 MCP 服务器，检验工具空间爆炸后的可扩展性，形成<strong>MCPMark-Enterprise</strong>子集。</li>
<li><strong>安全与隐私维度</strong></li>
<li>增设<strong>权限越界</strong>、<strong>敏感数据泄露</strong>两类恶意任务，检测 agent 是否遵守最小权限原则，推动<strong>可信工具调用</strong>研究。</li>
<li><strong>可解释性评分</strong></li>
<li>在验证脚本之外引入<strong>人类可读的解释轨迹</strong>（rationale）评分，衡量“模型能否把每一步操作讲清楚”，为合规审计提供依据。</li>
</ul>
<p>综上，MCPMark 已揭示“长程-多工具-多环境”代理任务的核心瓶颈；下一步可沿<strong>模型算法、代理架构、数据生成、任务复杂度、系统安全与在线部署</strong>六条主线展开，逐步缩小 pass^4 与 100% 之间的差距，并推动 MCP 生态从实验室走向生产。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use</strong><br>Zijian Wu†, Xiangyan Liu†, Xinyuan Zhang†, et al.<br>arXiv 2025</p>
<p>1. 背景与痛点</p>
<ul>
<li><strong>Model Context Protocol (MCP)</strong> 统一了 LLM 与外部系统（API、DB、浏览器、文件系统）交互的接口，是“通用代理”时代的基础设施。</li>
<li>现有 MCP 评测任务<strong>以读操作为主、交互浅、步骤短</strong>，无法反映真实世界多步、写密集、跨系统的复杂工作流，导致模型能力边界不明。</li>
</ul>
<p>2. MCPMark 基准</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>任务规模</td>
<td>127 个高质量任务，横跨 5 大真实环境：Notion、GitHub、Filesystem、PostgreSQL、Playwright。</td>
</tr>
<tr>
<td>初始状态</td>
<td>38 个“快照”式真实模板（仓库、数据库、网页、目录），可一键复位。</td>
</tr>
<tr>
<td>操作覆盖</td>
<td>均衡 CRUD：创建、读取、更新、删除；平均 16.2 轮、17.4 次工具调用。</td>
</tr>
<tr>
<td>验证方式</td>
<td>每任务配套 200+ 行可执行脚本，自动判定 pass/fail，杜绝 LLM-as-judge 模糊性。</td>
</tr>
<tr>
<td>造题流程</td>
<td>人-机协同四步：探索→演化→验证→迭代；单任务 3–5 专家小时，交叉评审+社区一月审查。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 统一评测框架 —— MCPMark-Agent</p>
<ul>
<li>最小代理：仅“工具调用循环”，无任务级启发式，避免模型偏置。</li>
<li>基于 LiteLLM + MCP Python SDK，支持任意 LLM 即插即用。</li>
<li>沙盒执行：状态追踪→验证→复位，保证可复现、无副作用。</li>
</ul>
<p>4. 主实验结果（127 任务 × 27 模型）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>pass@1</th>
<th>pass^4（四次全过）</th>
</tr>
</thead>
<tbody>
<tr>
<td>gpt-5-medium</td>
<td>52.56 %</td>
<td>33.86 %</td>
</tr>
<tr>
<td>claude-sonnet-4</td>
<td>28.15 %</td>
<td>12.60 %</td>
</tr>
<tr>
<td>o3</td>
<td>25.39 %</td>
<td>12.60 %</td>
</tr>
<tr>
<td>开源最佳 qwen3-coder-plus</td>
<td>24.80 %</td>
<td>12.60 %</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>环境差距</strong>：本地服务（PostgreSQL 76 %）≫ 远程（Notion 42 %, GitHub 48 %）。</li>
<li><strong>交互成本</strong>：kimi-k2-instruct 平均 27 轮仍低于 25 % pass@1，呈现“过度调用”循环。</li>
<li><strong>失败分布</strong>：&gt; 80 % 为隐式错误（规划/上下文/推理），显式错误模型特异（溢出、畸形、放弃）。</li>
</ul>
<p>5. 关键发现</p>
<ol>
<li><strong>一致性缺口</strong>：pass^4 仅 pass@1 的 1/2～2/3，凸显多轮工具使用下的鲁棒性危机。</li>
<li><strong>推理力度增益非线性</strong>：gpt-5 中档推理最佳，再高反降；远程服务受益更大。</li>
<li><strong>成本≠性能</strong>：高美元成本运行常因冗余调用得分更低，高效决策比“暴力尝试”更重要。</li>
</ol>
<p>6. 贡献一览</p>
<ul>
<li>提出首个<strong>重写作、长流程、可复现</strong>的 MCP 压力测试基准 MCPMark。</li>
<li>设计<strong>最小通用代理框架</strong>，公平横向对比 27 个前沿模型。</li>
<li>系统揭示当前 LLM 在长程规划、跨环境泛化、执行稳定性上的<strong>能力天花板</strong>。</li>
<li>开源基准与框架（mcpmark.ai），支持社区持续迭代与工业落地。</li>
</ul>
<p>7. 未来方向</p>
<ul>
<li>自洽性/反思/多代理架构提升 pass^4；</li>
<li>半自动难度梯度与远程服务仿真器，解决数据瓶颈；</li>
<li>引入歧义意图、安全越界、在线副作用管理，迈向可信生产级代理。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MCPMark%3A%20A%20Benchmark%20for%20Stress-Testing%20Realistic%20and%20Comprehensive%20MCP%20Use，链接是https://arxiv.org/pdf/2509.24002，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24002。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zijian Wu,Xiangyan Liu,Xinyuan Zhang,Lingjun Chen,Fanqing Meng,Lingxiao Du,Yiran Zhao,Fanshi Zhang,Yaoqi Ye,Jiawei Wang,Zirui Wang,Jinjie Ni,Yufan Yang,Arvin Xu,Michael Qizhe Shieh</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.24002.pdf">https://arxiv.org/pdf/2509.24002.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.24002">https://arxiv.org/abs/2509.24002</a></p>
<p><strong>Arxiv ID</strong>: 2509.24002</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.24002">https://papers.cool/arxiv/2509.24002</a></p>
<p><strong>Published</strong>: 2025-09-28T17:53:27Z</p>
<p><strong>Updated</strong>: 2025-09-28T17:53:27.000Z</p>
<hr>
<h3 id="2-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain"><a href="#2-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain" class="headerlink" title="2. The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26507">The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</a></h3><p>The relationship between computing systems and the brain has served as motivation for pioneering theoreticians since John von Neumann and Alan Turing. Uniform, scale-free biological networks, such as the brain, have powerful properties, including generalizing over time, which is the main barrier for Machine Learning on the path to Universal Reasoning Models. We introduce `Dragon Hatchling’ (BDH), a new Large Language Model architecture based on a scale-free biologically inspired network of $n$ locally-interacting neuron particles. BDH couples strong theoretical foundations and inherent interpretability without sacrificing Transformer-like performance. BDH is a practical, performant state-of-the-art attention-based state space sequence learning architecture. In addition to being a graph model, BDH admits a GPU-friendly formulation. It exhibits Transformer-like scaling laws: empirically BDH rivals GPT2 performance on language and translation tasks, at the same number of parameters (10M to 1B), for the same training data. BDH can be represented as a brain model. The working memory of BDH during inference entirely relies on synaptic plasticity with Hebbian learning using spiking neurons. We confirm empirically that specific, individual synapses strengthen connection whenever BDH hears or reasons about a specific concept while processing language inputs. The neuron interaction network of BDH is a graph of high modularity with heavy-tailed degree distribution. The BDH model is biologically plausible, explaining one possible mechanism which human neurons could use to achieve speech. BDH is designed for interpretability. Activation vectors of BDH are sparse and positive. We demonstrate monosemanticity in BDH on language tasks. Interpretability of state, which goes beyond interpretability of neurons and model parameters, is an inherent feature of the BDH architecture.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>计算系统与大脑之间的关系一直是自约翰·冯·诺依曼和艾伦·图灵以来先驱理论家的动力。均匀、无尺度的生物网络（例如大脑）具有强大的特性，包括随时间推移的泛化，这是机器学习在通往通用推理模型道路上的主要障碍。我们介绍了“Dragon Hatchling”（BDH），这是一种新的大型语言模型架构，基于 $n$ 个局部相互作用的神经元粒子的无尺度生物启发网络。BDH 结合了强大的理论基础和固有的可解释性，同时又不牺牲类似 Transformer 的性能。BDH 是一种实用、高性能的基于注意力的先进状态空间序列学习架构。除了作为图模型之外，BDH 还承认 GPU 友好的公式。它展示了类似 Transformer 的缩放定律：从经验上讲，对于相同的训练数据，在相同数量的参数（10M 到 1B）下，BDH 在语言和翻译任务上的性能可与 GPT2 相媲美。BDH 可以表示为大脑模型。BDH 在推理过程中的工作记忆完全依赖于突触可塑性，使用尖峰神经元进行赫布学习。我们凭经验证实，每当 BDH 在处理语言输入时听到或推理特定概念时，特定的个体突触都会加强联系。BDH的神经元交互网络是一个高度模块化的图，具有重尾度分布。BDH 模型在生物学上是合理的，解释了人类神经元可以用来实现语音的一种可能机制。BDH 专为可解释性而设计。BDH 的激活载体稀疏且呈阳性。我们在语言任务上展示了 BDH 中的单义性。状态的可解释性超越了神经元和模型参数的可解释性，是 BDH 架构的固有特征。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文试图弥合现有大规模语言模型（以 Transformer 为代表）与大脑这类“均匀、无标度”生物网络之间的结构性鸿沟，并由此解决以下核心问题：</p>
<ol>
<li><p><strong>时间维度上的泛化障碍</strong><br>当前语言模型在链式思维（Chain-of-Thought）推理任务中，无法系统地把在训练时见过的较短推理序列泛化到更长的、超出训练长度的推理场景。</p>
</li>
<li><p><strong>结构与机制的可解释性缺失</strong><br>Transformer 的宏观张量运算（注意力、前馈）缺乏微观层面的“粒子-粒子”本地动力学解释，使得模型行为在长时间尺度上难以被预测或给出 PAC 式保证。</p>
</li>
<li><p><strong>生物合理性与“公理化 AI”空白</strong><br>尚无架构能同时在以下三方面达到统一：</p>
</li>
</ol>
<ul>
<li>具备 Transformer 级别的语言与推理性能；</li>
<li>参数与状态规模呈线性关系，且状态完全本地化为神经元-突触可塑性；</li>
<li>可用分布式计算理论严格描述其微观规则，并在极限规模下呈现可预见的“热力学”行为。</li>
</ul>
<p>为此，作者提出 <strong>BDH（Dragon Hatchling）</strong> 这一新架构，通过“局部边重加权”图动力学把推理形式化为可编程粒子系统，并给出 GPU 可扩展版本 <strong>BDH-GPU</strong>。论文证明该架构：</p>
<ul>
<li>在 10M–1B 参数区间与 GPT-2 性能持平；</li>
<li>状态完全由稀疏正激活向量与突触可塑性矩阵构成，具备单语义可解释性；</li>
<li>其宏观注意力机制与微观“神经元对”突触更新规则之间可通过计算复杂性理论中的一系列归约建立等价性，从而成为连接 Transformer 与大脑模型的“缺失环节”。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“分布式图动力学→可解释状态空间模型→Transformer 级性能”这一主线里，与下列研究直接对话或继承其思想：</p>
<ul>
<li><p><strong>注意力与线性注意力</strong><br>– Bahdanau et al., 2015；Vaswani et al., 2017（原始 Transformer）<br>– Katharopoulos et al., 2020；Choromanski et al., 2021（Performer/FAVOR+ 线性注意力）<br>– Sun et al., 2023；Liu et al., 2025（线性注意力状态空间形式化）</p>
</li>
<li><p><strong>状态空间/长上下文模型</strong><br>– Dai et al., 2019（Transformer-XL 分段缓存）<br>– Gu &amp; Dao, 2024（Mamba</p>
</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Adrian Kosowski,Przemysław Uznański,Jan Chorowski,Zuzanna Stamirowska,Michał Bartoszkiewicz</p>
<p><strong>Categories</strong>: cs.NE,cs.AI,cs.LG,stat.ML</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26507.pdf">https://arxiv.org/pdf/2509.26507.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26507">https://arxiv.org/abs/2509.26507</a></p>
<p><strong>Arxiv ID</strong>: 2509.26507</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26507">https://papers.cool/arxiv/2509.26507</a></p>
<p><strong>Published</strong>: 2025-09-30T16:49:01Z</p>
<p><strong>Updated</strong>: 2025-09-30T16:49:01.000Z</p>
<hr>
<h3 id="3-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play"><a href="#3-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play" class="headerlink" title="3. Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.25541">Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play</a></h3><p>Although reinforcement learning (RL) can effectively enhance the reasoning capabilities of vision-language models (VLMs), current methods remain heavily dependent on labor-intensive datasets that require extensive manual construction and verification, leading to extremely high training costs and consequently constraining the practical deployment of VLMs. To address this challenge, we propose Vision-Zero, a domain-agnostic framework enabling VLM self-improvement through competitive visual games generated from arbitrary image pairs. Specifically, Vision-Zero encompasses three main attributes: (1) Strategic Self-Play Framework: Vision-Zero trains VLMs in “Who Is the Spy”-style games, where the models engage in strategic reasoning and actions across multiple roles. Through interactive gameplay, models autonomously generate their training data without human annotation. (2) Gameplay from Arbitrary Images: Unlike existing gamified frameworks, Vision-Zero can generate games from arbitrary images, thereby enhancing the model’s reasoning ability across diverse domains and showing strong generalization to different tasks. We demonstrate this versatility using three distinct types of image datasets: CLEVR-based synthetic scenes, charts, and real-world images. (3) Sustainable Performance Gain: We introduce Iterative Self-Play Policy Optimization (Iterative-SPO), a novel training algorithm that alternates between Self-Play and reinforcement learning with verifiable rewards (RLVR), mitigating the performance plateau often seen in self-play-only training and achieving sustained long-term improvements. Despite using label-free data, Vision-Zero achieves state-of-the-art performance on reasoning, chart question answering, and vision-centric understanding tasks, surpassing other annotation-based methods. Models and code has been released at <a target="_blank" rel="noopener" href="https://github.com/wangqinsi1/Vision-Zero">https://github.com/wangqinsi1/Vision-Zero</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管强化学习（RL）能够有效提升视觉-语言模型（VLMs）的推理能力，但当前的方法仍然高度依赖需要大量人工构建和验证的劳动密集型数据集，导致训练成本极高，从而限制了VLMs的实际部署。为了解决这一挑战，我们提出了Vision-Zero，这是一个与领域无关的框架，通过从任意图像对生成的竞争性视觉游戏，实现VLM的自我提升。具体而言，Vision-Zero具有三个主要特性：(1) 战略自对弈框架：Vision-Zero在“谁是卧底”风格的游戏中训练VLMs，模型在多个角色中进行战略推理和操作。通过互动式游戏，模型无需人为注释即可自主生成训练数据。(2) 任意图像生成游戏：与现有游戏化框架不同，Vision-Zero可以从任意图像生成游戏，从而提升模型在不同领域的推理能力，并展示对各种任务的强泛化性。我们使用三种不同类型的图像数据集验证了这一多功能性：基于CLEVR的合成场景、图表和真实世界图像。(3) 可持续性能提升：我们提出了迭代自对弈策略优化（Iterative-SPO）这一新的训练算法，它在自对弈和具有可验证奖励的强化学习（RLVR）之间交替进行，有效缓解了单纯自对弈训练中常见的性能瓶颈，实现持续的长期改进。尽管使用无标签数据，Vision-Zero在推理、图表问答及以视觉为核心的理解任务上仍达到了最先进的性能，超越了其他基于标注的方法。模型和代码已发布在 <a target="_blank" rel="noopener" href="https://github.com/wangqinsi1/Vision-Zero。">https://github.com/wangqinsi1/Vision-Zero。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>Vision-Zero 旨在解决视觉-语言模型（VLM）后训练阶段对昂贵人工标注数据的高度依赖问题。具体而言，现有强化学习（RL）方法虽然能提升 VLM 推理能力，但仍需大量人工构造与验证的问答对或奖励函数，导致数据成本高昂、规模受限，且模型性能受限于人类知识天花板。论文提出一种零人工干预、可扩展的“博弈化自对弈”框架，使 VLM 通过自动生成的视觉差异图像对进行“谁是卧底”式策略博弈，自主产生训练信号，实现跨领域、持续且经济高效的自我提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文明确引用或作为对比基准，可视为 Vision-Zero 的直接相关文献，按主题分组并给出核心贡献：</p>
<ul>
<li><strong>多智能体自博弈与 VLM 自我改进</strong></li>
<li>Konyushkova et al., 2025 —— 提出视觉对话博弈，通过目标驱动的图像识别交互实现 VLM 自改进。</li>
<li>Dharna et al., 2025 —— 基础模型自博弈，展示无监督竞争交互可催生开放式策略创新。</li>
<li>Liu et al., 2025 (SPIRAL) —— 在线多回合零和博弈 RL，首次证明棋盘/扑克等游戏可提升 LLM 推理并泛化到下游任务。</li>
<li>Zhai et al., 2024 —— 首个用任务特定奖励对 VLM 做 RL 微调并达到 SOTA，无需专家数据。</li>
<li>Wang et al., 2024b (RL-VLM-F) —— 利用 VLM 自身对图像-文本对的反馈自动生成奖励函数。</li>
<li><strong>卧底/社交推理游戏与 AI</strong></li>
<li>Dong et al., 2024 —— 提出“谁是卧底”多视角团队战术框架，让 LLM 学会身份隐藏与欺骗。</li>
<li>Kim et al., 2024 —— 系统分析 GPT-4 在 Mafia 游戏中识别欺骗者的表现，指出四大推理缺陷。</li>
<li>Yoo &amp; Kim, 2024 —— 量化 GPT-4 在社交推理任务中远超人类的准确率（80.65% vs 28.83%）。</li>
<li><strong>游戏化 VLM 训练与评测</strong></li>
<li>Chen et al., 2025 (G1) —— VLM-Gym 通过感知-推理互引导的 RL 缩小“知-行”差距。</li>
<li>Li et al., 2025 (JARVIS-VLA) —— 用键盘鼠标控制玩视觉游戏，Act-from-Vision 后训练提升 40%。</li>
<li>Paglieri et al., 2024 (BALROG) —— 聚合 6 种游戏环境评测 VLM 短/长期</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Qinsi Wang,Bo Liu,Tianyi Zhou,Jing Shi,Yueqian Lin,Yiran Chen,Hai Helen Li,Kun Wan,Wentian Zhao</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25541.pdf">https://arxiv.org/pdf/2509.25541.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25541">https://arxiv.org/abs/2509.25541</a></p>
<p><strong>Arxiv ID</strong>: 2509.25541</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25541">https://papers.cool/arxiv/2509.25541</a></p>
<p><strong>Published</strong>: 2025-09-29T21:55:55Z</p>
<p><strong>Updated</strong>: 2025-09-29T21:55:55.000Z</p>
<hr>
<h3 id="4-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning"><a href="#4-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning" class="headerlink" title="4. Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.23873">Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning</a></h3><p>As supervised fine-tuning (SFT) evolves from a lightweight post-training step into a compute-intensive phase rivaling mid-training in scale, data efficiency has become critical for aligning large language models (LLMs) under tight budgets. Existing data pruning methods suffer from a fragmented design: they operate either at the sample level or the token level in isolation, failing to jointly optimize both dimensions. This disconnect leads to significant inefficiencies—high-value samples may still contain redundant tokens, while token-level pruning often discards crucial instructional or corrective signals embedded in individual examples. To address this bottleneck, we introduce the Error-Uncertainty (EU) Plane, a diagnostic framework that jointly characterizes the heterogeneous utility of training data across samples and tokens. Guided by this insight, we propose Quadrant-based Tuning (Q-Tuning), a unified framework that strategically coordinates sample pruning and token pruning. Q-Tuning employs a two-stage strategy: first, it performs sample-level triage to retain examples rich in informative misconceptions or calibration signals; second, it applies an asymmetric token-pruning policy, using a context-aware scoring mechanism to trim less salient tokens exclusively from misconception samples while preserving calibration samples in their entirety. Our method sets a new state of the art across five diverse benchmarks. Remarkably, on SmolLM2-1.7B, Q-Tuning achieves a +38\% average improvement over the full-data SFT baseline using only 12.5\% of the original training data. As the first dynamic pruning approach to consistently outperform full-data training, Q-Tuning provides a practical and scalable blueprint for maximizing data utilization in budget-constrained LLM SFT.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>随着监督微调（SFT）从一个轻量级的后训练步骤发展成与中期训练规模相当的计算密集阶段，数据效率在有限预算下对大型语言模型（LLM）的对齐变得至关重要。现有的数据剪枝方法存在设计分散的问题：它们要么在样本层面上操作，要么在令牌层面上操作，未能同时优化两个维度。这种脱节导致了显著的低效——高价值样本中仍可能包含冗余令牌，而令牌层面的剪枝往往丢弃了单个示例中嵌入的重要教学或纠错信号。为了解决这一瓶颈，我们提出了错误-不确定性（Error-Uncertainty, EU）平面，这是一种诊断框架，可联合刻画训练数据在样本和令牌维度上的异质效用。在这一洞察的指导下，我们提出了基于象限的微调（Quadrant-based Tuning, Q-Tuning），一个统一的框架，策略性地协调样本剪枝和令牌剪枝。Q-Tuning采用两阶段策略：首先，进行样本级别的筛选，以保留富含信息误解或校准信号的示例；其次，应用非对称令牌剪枝策略，利用上下文感知评分机制，仅从误解样本中修剪不重要的令牌，同时完整保留校准样本。我们的方法在五个多样化基准上创下了新纪录。值得注意的是，在SmolLM2-1.7B上，Q-Tuning在仅使用原训练数据12.5%的情况下，比完整数据SFT基线平均提升了38%。作为首个持续超过完整数据训练的动态剪枝方法，Q-Tuning为在预算受限的LLM SFT中最大化数据利用率提供了实用且可扩展的蓝图。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对<strong>大规模监督微调（SFT）阶段数据利用率低、计算开销大</strong>的痛点，提出<strong>样本级与词元级剪枝各自为政</strong>导致的信息浪费问题，并给出统一解决方案。</p>
<ul>
<li><p><strong>核心矛盾</strong><br>现有剪枝方法要么只筛样本（丢掉整段数据），要么只剪词元（删除局部 token），二者互不知晓，造成：</p>
</li>
<li><p>高价值样本里仍含冗余词元；</p>
</li>
<li>词元级剪枝可能误删对校准模型至关重要的纠错信号。</li>
<li><strong>目标</strong><br>在<strong>给定训练预算</strong>下，联合优化“留哪些样本”与“留哪些词元”，使每颗梯度更新都落在<strong>真实对学习最有用</strong>的 token 上，最终<strong>用更少数据达到或超越全量训练效果</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两条主线：样本级剪枝（Sample Pruning）与词元级剪枝（Token Pruning），并指出二者长期独立发展、缺乏协同。以下按类别归纳代表性工作，均来自论文附录 A 与正文引用。</p>
<p>1. 样本级剪枝（Sample-level Pruning）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>子类</th>
<th>代表方法</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>静态筛选（训练前一次性决定）</td>
<td>LIMA (Zhou et al., 2023a)</td>
<td>仅用 1 k 高质量指令数据即可对齐 LLM，强调“少而精”。</td>
</tr>
<tr>
<td>SVP (Coleman et al., 2020)</td>
<td>用小代理模型预估样本重要性，提前过滤。</td>
</tr>
<tr>
<td>D³ (Jin et al., 2025)</td>
<td>综合“多样性-难度-可靠性”加权选核集。</td>
</tr>
<tr>
<td>Less-is-More (Deng et al., 2025)</td>
<td>在偏好数据中剔除噪声或冗余对，静态子集即可超全量。</td>
</tr>
<tr>
<td>Alpagasus (Chen et al., 2023)</td>
<td>基于“质量打分”清洗 Alpaca 数据，去噪后性能反升。</td>
</tr>
<tr>
<td>Data Whisperer (Wang et al., 2025a)</td>
<td>用 few-shot 上下文学习评估样本任务贡献度，再决定保留。</td>
</tr>
<tr>
<td>动态筛选（训练过程中在线调整）</td>
<td>RHO-LOSS (Mindermann et al., 2022)</td>
<td>用参考模型计算样本可学习性，实时丢弃已学会的简单例。</td>
</tr>
<tr>
<td>InfoBatch (Qin et al., 2024)</td>
<td>按当前损失是否低于动态阈值，无偏地剔除低损失样本并缩放梯度，实现无损加速。</td>
</tr>
<tr>
<td>MATES (Huang et al., 2024)</td>
<td>根据模型参数变化轨迹，在线重加权样本。</td>
</tr>
<tr>
<td>DWM (Li et al., 2025)</td>
<td>双层优化，每步选择对验证集梯度影响最大的样本。</td>
</tr>
<tr>
<td>ActivePrune (Azeemi et al., 2024)</td>
<td>针对</td>
</tr>
</tbody>
</table>
</div>
</div></details>
<p><strong>Authors</strong>: Shaobo Wang,Jiaming Wang,Jiajun Zhang,Cong Wang,Yue Min,Zichen Wen,Fei Huang,Huiqiang Jiang,Junyang Lin,Dayiheng Liu,Linfeng Zhang</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23873.pdf">https://arxiv.org/pdf/2509.23873.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23873">https://arxiv.org/abs/2509.23873</a></p>
<p><strong>Arxiv ID</strong>: 2509.23873</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23873">https://papers.cool/arxiv/2509.23873</a></p>
<p><strong>Published</strong>: 2025-09-28T13:27:38Z</p>
<p><strong>Updated</strong>: 2025-09-28T13:27:38.000Z</p>
<hr>
<h3 id="5-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning"><a href="#5-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning" class="headerlink" title="5. TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.25760">TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning</a></h3><p>While large language models (LLMs) have demonstrated strong performance on factoid question answering, they are still prone to hallucination and untruthful responses, particularly when tasks demand information outside their parametric knowledge. Indeed, truthfulness requires more than accuracy — models must also recognize uncertainty and abstain when unsure to avoid hallucinations. This presents a fundamental challenge for existing methods: approaches that optimize for accuracy often amplify hallucinations, while those that encourage abstention can become overly conservative, sacrificing correct answers. Both extremes ultimately compromise truthfulness. In this work, we present TruthRL, a general reinforcement learning (RL) framework that directly optimizes the truthfulness of LLMs. Specifically, we implement TruthRL using GRPO with a simple yet effective ternary reward that distinguishes correct answers, hallucinations, and abstentions. It incentivizes models to reduce hallucinations not only by providing correct responses, but also by enabling abstention when uncertain, thereby improving truthfulness. Extensive experiments across four knowledge-intensive benchmarks show that, compared to vanilla RL, TruthRL significantly reduces hallucinations by 28.9% and improves truthfulness by 21.1%, with consistent gains across various backbone models (e.g., Qwen, Llama) under both retrieval and non-retrieval setups. In-depth ablation study demonstrates that vanilla accuracy-driven methods, such as supervised fine-tuning or RL with a binary reward, struggle to balance factual correctness and uncertainty. In contrast, our proposed truthfulness-driven TruthRL achieves strong performance in both accuracy and truthfulness, underscoring the importance of learning objective design for developing truthful LLMs.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然大型语言模型（LLM）在事实性问答中表现出色，但它们仍然容易出现虚构和不真实的回答，尤其是在任务需要超出其参数化知识的信息时。实际上，真实性不仅仅要求准确性——模型还必须识别不确定性，并在不确定时避免回答以防止虚构。这对现有方法提出了根本性挑战：优化准确性的方式往往会加剧虚构，而鼓励回避的方式可能过于保守，从而牺牲正确答案。两种极端最终都会损害真实性。在本研究中，我们提出了 TruthRL，这是一个通用的强化学习（RL）框架，能够直接优化 LLM 的真实性。具体来说，我们使用 GRPO 实现 TruthRL，并设计了一个简单而有效的三元奖励机制，用于区分正确回答、虚构回答和回避行为。该机制不仅通过提供正确回答来激励模型减少虚构，还通过在不确定时允许回避来提升真实性。在四个知识密集型基准上的大量实验表明，与普通 RL 相比，TruthRL 显著减少了 28.9% 的虚构回答，并提升了 21.1% 的真实性，在不同的主干模型（如 Qwen、Llama）以及有检索与无检索设置下均获得一致提升。深入的消融研究显示，普通的以准确性为驱动的方法（如监督微调或使用二元奖励的 RL）在平衡事实正确性和不确定性方面存在困难。相比之下，我们提出的以真实性为驱动的 TruthRL 在准确性和真实性方面均表现出色，强调了学习目标设计对开发真实 LLM 的重要性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决大语言模型（LLM）在知识密集型问答中“幻觉”与“过度保守”并存的<strong>可信度危机</strong>：</p>
<ul>
<li>传统仅优化准确率的微调或强化学习方法会鼓励模型在不确定时“硬猜”，导致幻觉率居高不下；</li>
<li>现有让模型学会“说不知道”的方法（如 R-Tuning）又容易走向另一极端——过度弃权，牺牲正确回答的覆盖率。</li>
</ul>
<p>为此，作者提出<strong>TruthRL</strong>，一个直接以“真实度”（truthfulness）为优化目标的强化学习框架，通过三值奖励（正确 / 弃权 / 幻觉）显式惩罚幻觉、中性对待弃权，从而在不损失过多准确率的前提下显著降低幻觉，并提升模型对自身知识边界的识别能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，均围绕“降低幻觉”与“让模型知道何时说不知道”展开：</p>
<ol>
<li>幻觉检测与缓解</li>
</ol>
<ul>
<li>检索增强生成（RAG）：Atlas、REALTime QA、Search-R1 等通过外部知识库降低参数化记忆错误。</li>
<li>解码策略：Self-Consistency、DoLa、Contrastive Decoding 利用输出分布或层间差异抑制幻觉。</li>
<li>事后校准：Self-Check、LM-Know 让模型自行或借助外部 verifier 对答案做事实性检验。</li>
</ul>
<ol>
<li>不确定性表达与弃权学习</li>
</ol>
<ul>
<li>R-Tuning、ConfQA、UAlign 把“我不知道”作为监督标签，让模型在 OOK 样本上学习弃权。</li>
<li>基于置信度的拒绝：Kadavath’22、LM-Know 用概率阈值或内部置信度触发“我不知道”。</li>
</ul>
<ol>
<li>强化学习与奖励设计</li>
</ol>
<ul>
<li>RLHF / RLVR：InstructGPT、DeepSeek-R1 等用二元“正确/错误”奖励提升通用对齐或推理，但会混淆弃权与幻觉。</li>
<li>多目标/不确定性奖励：FLAME、UA-IPO 尝试在奖励中引入不确定性信号，但仍未显式区分“幻觉”与“弃权”。</li>
</ul>
<p>TruthRL 与上述工作的核心区别：首次在在线 RL 中引入<strong>三值奖励</strong>，把“弃权”从错误中解耦，直接优化可配置的 truthfulness 目标，兼顾准确率、弃权率、幻觉率三维度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“可信度”形式化为一个可优化的三元目标，并通过<strong>在线强化学习</strong>把该目标注入模型参数。关键步骤如下：</p>
<ol>
<li>问题重定义<br>将传统“最大化准确率”改为</li>
</ol>
<p>max<em>θ E</em>(xsim D)![,w_1!·!Acc + w_2!·!Unc - w_3!·!Hall,]</p>
<p>显式把“弃权”当作中性项，把“幻觉”当作负项，从而鼓励模型在不确定时主动弃权而非硬猜。</p>
<ol>
<li><p>知识边界探测<br>对训练集每题采样 256 个回答，若全部错误则标记为 OOK（out-of-knowledge），并强制标签为“我不知道”，为后续奖励计算提供先验。</p>
</li>
<li><p>三值奖励设计<br>在 GRPO 框架内定义</p>
</li>
</ol>
<p>r_(ternary)(x,y)= +1 &amp; 正确 +0 &amp; 弃权 -1 &amp; 幻觉</p>
<p>相对优势估计时，弃权（0）自然高于幻觉（−1），策略梯度会把概率质量从幻觉移向弃权；同时正确回答仍获最高奖励，保证准确率不受显著牺牲。</p>
<ol>
<li><p>在线更新<br>采用群体相对策略优化（GRPO）：每题采样 G=16 条回答，用 LLM-as-a-judge 实时标注正确/弃权/幻觉，在线计算优势并更新策略，使模型持续探索并细化知识边界。</p>
</li>
<li><p>奖励消融与增强</p>
</li>
</ol>
<ul>
<li>对比二元奖励（正确 +1/其他 −1）证实其会抑制弃权；</li>
<li>知识增强版在 OOK 问题上把弃权升为 +1，非 OOK 保持 0，效果略逊于纯三值；</li>
<li>推理增强版额外加入推理质量信号，发现简单三值已足够，复杂设计反而难调优。</li>
</ul>
<p>通过上述流程，TruthRL 在不依赖人工标注“我不知道”样本的前提下，让模型学会**“能答则答，不能答则弃权”**，在四个知识密集型基准上平均降低幻觉 28.9%、提升真实度 21.1%。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“能否在多种场景下同时降低幻觉、提升真实度”展开，覆盖<strong>4 个基准 × 2 种检索设置 × 多 backbone</strong>，并辅以细粒度行为分析。</p>
<ol>
<li><p>主实验：全面对比<br>数据集：CRAG、Natural Questions、HotpotQA、MuSiQue<br>设置：with-/without-retrieval<br>骨干：Qwen2.5-7B-Instruct、Llama3.1-8B-Instruct<br>指标：Truthfulness (↑)、Hallucination (↓)、Accuracy (↑)<br>结果：TruthRL 在 16 组配置中 15 组取得最佳 Truthfulness，平均幻觉相对降低 28.9%，真实度绝对提升 21.1%。</p>
</li>
<li><p>行为拆解：知识边界可视化</p>
</li>
</ol>
<ul>
<li>全量测试集：TruthRL 把 39.5% 的幻觉转为 18.5% 弃权，同时保持最高准确率。</li>
<li>困难子集（几乎无人答对）：基线幻觉≈100%，TruthRL 仅 15.5%，其余 84.5% 为“我不知道”，证明其学会识别知识边界。</li>
</ul>
<ol>
<li><p>对抗性测试：幻觉诱导题<br>使用 CRAG 中“比较型”选择题（A 大还是 B 大？）。<br>基线幻觉率 38%–49%，TruthRL 降至 16.5%，Truthfulness 从 3–13 分提升至 52.4 分。</p>
</li>
<li><p>奖励设计消融</p>
</li>
</ol>
<ul>
<li>Binary → 高准确率但零弃权</li>
<li>Binary+Knowledge → 弃权略升、准确率降</li>
<li>Ternary → 最佳平衡点</li>
<li>Ternary+Knowledge/Reasoning → 无明显增益，说明“简单三值”已足够</li>
</ul>
<ol>
<li><p>RL 范式对比<br>Offline DPO → 10.1 Truthfulness<br>Iterative DPO（4 轮）→ 12.6→回落到 −2.0<br>Online TruthRL → 25.6，验证在线交互的必要性</p>
</li>
<li><p>评价鲁棒性<br>用 Llama3.3-70B、Qwen2.5-72B、Gemma3-27B 分别做 judge，TruthRL 的相对提升一致，排除“刷分”特定 judge 的可能。</p>
</li>
<li><p>模型规模扩展<br>3B→8B→32B：TruthRL 在各尺寸上均降低幻觉 18–22 个百分点，小模型增益更大，说明方法对弱模型同样有效。</p>
</li>
<li><p>推理奖励探索<br>在 outcome-only 基础上叠加乘性、加性、条件式推理奖励，结果推理分略升，但 outcome 分下降，表明需更精细设计，留待后续。</p>
</li>
</ol>
<p>综上，实验从<strong>主结果→行为→对抗→消融→范式→评价→规模→奖励</strong>八个维度验证了三值奖励驱动的在线 RL 在提升 LLM 真实度上的有效性与鲁棒性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可延续 TruthRL 框架，进一步拔高“真实度”上限或拓展应用场景：</p>
<ol>
<li>奖励函数再设计</li>
</ol>
<ul>
<li>连续置信度奖励：将离散三值扩展为 $r∈<br>-1,1<br>$ 的连续量，用模型自身 calibrated 概率或 ensemble 差异度直接映射奖励，实现更细粒度优化。</li>
<li>动态权重  w_1,w_2,w_3 ：根据用户场景（医疗 vs 闲聊）在线调整，实现“可配置风险偏好”。</li>
</ul>
<ol>
<li>推理过程奖励</li>
</ol>
<ul>
<li>引入过程级 verifier（PRM），对每一推理步骤给出 0/1 信号，结合 Outcome 奖励做 multi-scale RL，缓解“答案对但理由错”或“理由对却结论错”的失配。</li>
<li>探索结构化的思维链（tree/graph）（如 Parallel-R1），在分支节点上施加真实性约束，减少推理链早期就陷入幻觉的误差传播。</li>
</ul>
<ol>
<li>多轮交互与检索协同</li>
</ol>
<ul>
<li>将 TruthRL 从单轮 QA 扩展到多轮对话，奖励信号同时考虑“是否主动追问澄清”“是否及时引用最新检索结果”，降低上下文复合幻觉。</li>
<li>对检索文档引入可信度权重，用 retriever+reranker 的置信度作为先验，动态调整弃权阈值，实现“检索-生成-弃权”联合优化。</li>
</ul>
<ol>
<li>跨领域与多语言迁移</li>
</ol>
<ul>
<li>在低资源领域（法律、医学）先用通用领域训练 TruthRL，再用少量领域数据做 reward-shaping/adapter-tuning，检验“真实性”是否比“准确率”更容易迁移。</li>
<li>研究多语言场景下，不同语言对“不确定性”表达的文化差异，是否需要语言特定的弃权模板或奖励修正。</li>
</ul>
<ol>
<li>人类偏好对齐与安全性</li>
</ol>
<ul>
<li>将 TruthRL 与 RLHF 融合：在偏好数据集上同时优化“有用-无害-真实”三维，用多目标 RL（如 Pareto-MPO）防止“过度弃权导致有用性下降”。</li>
<li>针对“幻觉诱导攻击”（prompt injection、对比陷阱题）做 adversarial training，把攻击样本在线加入 rollout，提升鲁棒性。</li>
</ul>
<ol>
<li>模型自省与可解释性</li>
</ol>
<ul>
<li>训练一个轻量级“真实性探测器”挂接到任意 backbone，实时输出 abstention probability，供下游系统调用；探测器本身可用 TruthRL 蒸馏得到。</li>
<li>利用 probing 或机制可解释性方法（如因果中介分析）观察三值奖励究竟改变了哪些 attention head 或 FFN 神经元，从而理解“知识边界”在参数中的编码方式。</li>
</ul>
<ol>
<li>计算效率与系统优化</li>
</ol>
<ul>
<li>用 LoRA/DoRA 替代全参数训练，检验梯度更新量减小后三值奖励是否仍有效；结合 rollout 缓存与 off-policy 校正（如 PPO-Off）降低在线采样成本。</li>
<li>探索 verifier 的“课程蒸馏”：先用大模型 judge 提供奖励，再逐步把小模型 verifier 蒸馏到与策略同步更新，减少训练阶段对大 judge 的依赖。</li>
</ul>
<ol>
<li>评价基准与协议</li>
</ol>
<ul>
<li>构建带“弃权黄金标签”的大规模 benchmark，覆盖事实问答、数值计算、时间敏感知识，避免现有数据集只有“正确/错误”而缺乏“不可答”标注的偏差。</li>
<li>设计“幻觉税”指标：衡量每提升 1% 准确率额外增加多少幻觉，将真实度-准确率权衡形式化，便于不同方法公平比较。</li>
</ul>
<p>通过上述方向的深入，可推动 TruthRL 从“单轮问答降幻觉”走向“可信、可控、可解释”的下一代对齐范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>TruthRL：用强化学习直接优化大模型“真实度”</strong></p>
<ol>
<li><p>问题<br>仅优化准确率的 SFT/RL 会鼓励模型“硬猜”，幻觉高；教模型说“我不知道”的方法又易过度弃权。需要一种<strong>同时降低幻觉、保持准确率、允许合理弃权</strong>的训练范式。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li>把“真实度”定义为</li>
</ul>
<p>Truthfulness=w_1·Acc+w__2·Unc-w_3·Hall</p>
<ul>
<li>在线 RL（GRPO）里给每条回答三值奖励：</li>
</ul>
<p>r=+1,正确;;0,弃权;;-1,幻觉</p>
<p>使弃权相对幻觉获得更高优势，从而把概率质量从幻觉移向弃权，同时保留正确回答的最高奖励。</p>
<ul>
<li>用 LLM-as-a-judge 实时标注，无需人工“我不知道”标签。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>4 个知识密集型基准 × 有/无检索 × 多 backbone：幻觉平均↓28.9%，真实度↑21.1%。</li>
<li>困难题几乎无人答对时，基线幻觉≈100%，TruthRL 仅 15.5%，其余主动弃权。</li>
<li>消融：二元奖励→零弃权；三值奖励→最佳平衡；在线 RL 显著优于离线/半离线 RL。</li>
<li>跨 3B-32B 模型、不同 judge、对抗性诱导题均一致有效。</li>
</ul>
<ol>
<li>结论<br>三值奖励的在线 RL 能在参数内植入“知识边界”意识，让模型**“能答则答，不能答则弃权”**，为构建可信 LLM 提供了简单可扩展的新基线。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是TruthRL%3A%20Incentivizing%20Truthful%20LLMs%20via%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2509.25760，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25760。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhepei Wei,Xiao Yang,Kai Sun,Jiaqi Wang,Rulin Shao,Sean Chen,Mohammad Kachuee,Teja Gollapudi,Tony Liao,Nicolas Scheffer,Rakesh Wanga,Anuj Kumar,Yu Meng,Wen-tau Yih,Xin Luna Dong</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25760.pdf">https://arxiv.org/pdf/2509.25760.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25760">https://arxiv.org/abs/2509.25760</a></p>
<p><strong>Arxiv ID</strong>: 2509.25760</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25760">https://papers.cool/arxiv/2509.25760</a></p>
<p><strong>Published</strong>: 2025-09-30T04:25:17Z</p>
<p><strong>Updated</strong>: 2025-09-30T04:25:17.000Z</p>
<hr>
<h3 id="6-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training"><a href="#6-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training" class="headerlink" title="6. Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26625">Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</a></h3><p>Large Language Models (LLMs), despite being trained on text alone, surprisingly develop rich visual priors. These priors allow latent visual capabilities to be unlocked for vision tasks with a relatively small amount of multimodal data, and in some cases, to perform visual tasks without ever having seen an image. Through systematic analysis, we reveal that visual priors-the implicit, emergent knowledge about the visual world acquired during language pre-training-are composed of separable perception and reasoning priors with unique scaling trends and origins. We show that an LLM’s latent visual reasoning ability is predominantly developed by pre-training on reasoning-centric data (e.g., code, math, academia) and scales progressively. This reasoning prior acquired from language pre-training is transferable and universally applicable to visual reasoning. In contrast, a perception prior emerges more diffusely from broad corpora, and perception ability is more sensitive to the vision encoder and visual instruction tuning data. In parallel, text describing the visual world proves crucial, though its performance impact saturates rapidly. Leveraging these insights, we propose a data-centric recipe for pre-training vision-aware LLMs and verify it in 1T token scale pre-training. Our findings are grounded in over 100 controlled experiments consuming 500,000 GPU-hours, spanning the full MLLM construction pipeline-from LLM pre-training to visual alignment and supervised multimodal fine-tuning-across five model scales, a wide range of data categories and mixtures, and multiple adaptation setups. Along with our main findings, we propose and investigate several hypotheses, and introduce the Multi-Level Existence Bench (MLE-Bench). Together, this work provides a new way of deliberately cultivating visual priors from language pre-training, paving the way for the next generation of multimodal LLMs.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）尽管仅通过文本进行训练，却令人惊讶地展现出丰富的视觉先验。这些先验使得潜在的视觉能力能够通过相对少量的多模态数据被激活，从而用于视觉任务，有些情况下甚至可以在从未见过图像的情况下完成视觉任务。通过系统分析，我们揭示了视觉先验——在语言预训练过程中获得的关于视觉世界的隐性、涌现知识——由可区分的感知先验和推理先验组成，并具有独特的规模趋势和起源。我们展示了LLM的潜在视觉推理能力主要通过以推理为中心的数据（例如代码、数学、学术文本）进行预训练而逐步发展。这种从语言预训练中获得的推理先验是可迁移的，并且通用于视觉推理。相比之下，感知先验更为分散地从广泛的语料中出现，而感知能力对视觉编码器和视觉指令微调数据更为敏感。同时，描述视觉世界的文本同样至关重要，尽管其性能影响很快达到饱和。利用这些洞察，我们提出了一种以数据为中心的视觉感知LLM预训练方案，并在1万亿令牌规模的预训练中进行了验证。我们的发现基于100多次受控实验，消耗50万GPU小时，涵盖了从LLM预训练到视觉对齐和受监督的多模态微调的完整MLLM构建流程——跨越五种模型规模、各种数据类别和混合类型，以及多种适应设置。除了主要发现外，我们提出并研究了若干假设，并介绍了多级存在基准（MLE-Bench）。总体而言，这项工作提供了一种通过语言预训练刻意培养视觉先验的新方法，为下一代多模态LLM的发展铺平了道路。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在系统回答一个核心问题：</p>
<blockquote>
<p><strong>仅通过文本预训练的大型语言模型（LLM）为何能“学会看见”？</strong><br>换言之，LLM 在从未接触任何图像的情况下，为何仍能涌现出可被少量视觉数据激活的视觉先验（visual priors），并在下游视觉任务中表现优异？</p>
</blockquote>
<p>为破解这一悖论，论文将宏观现象拆解为三个可验证的子问题：</p>
<ol>
<li><strong>结构问题</strong>：文本预训练所赋予的“视觉先验”是单一能力，还是可分离的复合能力？</li>
<li><strong>来源问题</strong>：若可分离，各能力分别由何种语言数据催生？其 Scaling 规律与饱和点为何？</li>
<li><strong>利用问题</strong>：能否基于上述规律，在纯文本预训练阶段<strong>主动配置数据配比</strong>，从而<strong>预先栽培</strong>出更适配视觉任务的 LLM，减少后续多模态对齐成本？</li>
</ol>
<p>通过 100 余项控制实验（500k GPU-hours），论文首次给出定量答案：</p>
<ul>
<li>视觉先验可分解为 <strong>感知先验（perception prior）</strong> 与 <strong>推理先验（reasoning prior）</strong>，二者来源、Scaling 曲线与对视觉编码器的依赖度均不同。</li>
<li>推理先验主要由代码、数学、学术等“推理密集型”文本驱动，且随比例增加持续受益，直至 75% 才饱和；感知先验则弥散地来自通用语料，对数据多样性敏感，但快速饱和。</li>
<li>利用该规律，作者提出一条<strong>数据配方</strong>（≈60% 推理 + 15% 视觉世界描述），在 1T token 规模预训练中得到验证：同等参数下，新配方模型在多模态评测上显著优于传统语言优先配方，而语言性能不降反升。</li>
</ul>
<p>综上，论文将“LLM 无图自见”的意外现象，转化为<strong>可预测、可设计</strong>的训练策略，为下一代多模态大模型的<strong>文本预训练阶段</strong>提供了明确的数据工程指南。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与本文主题“LLM 在纯文本预训练中如何获得视觉先验”直接相关的研究可按三条主线梳理：</p>
<ol>
<li>从 LLM 到 MLLM 的适配范式</li>
<li>数据配比决定模型能力的系列工作</li>
<li>跨模态对齐与“柏拉图表示假说”实证</li>
</ol>
<p>以下列出关键文献并指出其与本工作的关联点（按时间先后）：</p>
<ul>
<li><p><strong>Flamingo</strong> (Alayrac et al., NeurIPS 2022)<br>首次证明“冻结 LLM + 少量图文对齐”即可在视觉任务上 Few-shot 生效，提示 LLM 内部已存在可激活的视觉概念。</p>
</li>
<li><p><strong>BLIP-2</strong> (Li et al., ICML 2023)<br>用 Q-Former 将视觉 token 压缩为 32 个可学习 query，进一步降低图文对齐数据量，佐证“语言侧已隐含视觉知识”。</p>
</li>
<li><p><strong>LLaVA 系列</strong> (Liu et al., 2023a; 2024a,b)<br>仅 0.6 M～1.2 M 图文指令数据即可把 LLaMA 变成多模态对话模型；本文图 6 的“盲指令调优”实验直接受其启发，用于剥离感知与推理来源。</p>
</li>
<li><p><strong>Cambrian-1</strong> (Tong et al., NeurIPS 2024)<br>系统研究视觉编码器、投影器与指令数据对 MLLM 的影响，提出 CV-Bench；本文沿用其评测套件，但把变量前移到<strong>文本预训练阶段</strong>。</p>
</li>
<li><p><strong>Scaling Data-Constrained LLM</strong> (Muennighoff et al., 2023)<br>首次在 400 B token 规模给出“代码↑ → 推理↑”的定量曲线；本文图 3 将其结论外推到视觉推理任务，并给出 75% 饱和点。</p>
</li>
<li><p><strong>Platonic Representation Hypothesis</strong> (Huh et al., ICML 2024a,b)<br>提出“文本与图像在足够大的模型里收敛到同一潜在结构”；本文第 4.3 节用 mNN 对齐分数给出直接实验支撑，并指出<strong>结构化文本</strong>（代码/数学）是加速收敛的关键。</p>
</li>
<li><p><strong>Data Mixing Laws</strong> (Ye et al., 2024; Shukor et al., 2025a)<br>用可微分混合器预测不同语料比例对语言 perplexity 的影响；本文把相同思路迁移到<strong>多模态下游任务</strong>，并给出“60 % 推理 + 15 % 视觉描述”的最优配比。</p>
</li>
<li><p><strong>Frozen LLM Layers as Visual Encoder</strong> (Pang et al., ICLR 2024; Bai et al., 2025a)<br>将 LLM 中间层直接当作视觉骨干网络，在 ImageNet-1k 上取得 87 % Top-1，证明 LLM 表示天然兼容视觉信号；本文图 5 用三种不同视觉编码器重复实验，显示<strong>推理先验的提升与编码器无关</strong>，进一步验证其“通用性”。</p>
</li>
<li><p><strong>Bring Reason to Vision</strong> (Chen et al., arXiv 2025)<br>通过测试时参数合并把“推理头”与“感知头”物理分离；本文用相关性分析（图 4）在统计层面得到相同结论，但额外指出<strong>感知头主要在后训练阶段由视觉监督塑造</strong>。</p>
</li>
<li><p><strong>3D-GPT / AutoPresent</strong> (Sun et al., 3DV 2025; Ge et al., 2025)<br>展示 LLM 仅凭文本即可生成可渲染的 3D 场景代码，为本文“程序式视觉知识”段落提供实例。</p>
</li>
</ul>
<p>综上，已有研究多聚焦于<strong>如何高效地把现有 LLM 变成 MLLM</strong>，而本文反向追问“为何 LLM 本来就能看”，并用大规模控制实验给出<strong>数据层面的可复现配方</strong>，填补了“文本预训练 → 视觉先验”这一因果链的定量空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“LLM 为何无图自见”这一黑箱问题转化为<strong>可量化的数据科学问题</strong>，通过三步控制实验流水线完成拆解与验证。核心思路是：<strong>把视觉先验当成可测量、可干预、可优化的统计变量</strong>，而非玄学涌现。</p>
<p>1. 定义可观测指标</p>
<ul>
<li><strong>视觉先验的代理指标</strong>：16 个公开 VQA 基准被聚成 4 类——General、Knowledge、OCR&amp;Chart、Vision-Centric；每类对应不同认知负荷（感知 vs 推理）。</li>
<li><strong>语言先验的代理指标</strong>：WikiText + LAMBADA 困惑度、8 项常识推理准确率，用于监控“纯语言”能力是否因干预而崩溃。</li>
<li><strong>跨模态对齐指标</strong>：基于 WIT 图文对的 mutual nearest-neighbor (mNN) 距离，量化文本表示与视觉表示的拓扑相似度。</li>
</ul>
<p>2. 建立因果链：数据 → 先验 → 能力</p>
<p>在 3B 参数、30B token 的“实验室规模”下，采用<strong>单变量网格扫描 + 双盲随机种子</strong>策略，系统扰动以下变量：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>干预维度</th>
<th>变量水平</th>
<th>控制方式</th>
</tr>
</thead>
<tbody>
<tr>
<td>语料来源</td>
<td>16 个单领域语料（代码、数学、艺术…）</td>
<td>固定模型大小与总 token 数</td>
</tr>
<tr>
<td>语料比例</td>
<td>推理组合 0–100 %、视觉世界描述 0–100 %</td>
<td>其余用通用语料线性插值</td>
</tr>
<tr>
<td>规模大小</td>
<td>340M–13B 参数 × 0–100B token</td>
<td>固定配比，观察 scaling law</td>
</tr>
<tr>
<td>视觉编码器</td>
<td>MetaCLIP / DINOv2 / MAE</td>
<td>冻结 LLM，仅换 encoder</td>
</tr>
</tbody>
</table>
</div>
<p>通过 105 组对照模型，得到三条可重复因果结论：</p>
<ol>
<li><strong>推理先验</strong></li>
</ol>
<ul>
<li>来源：代码、数学、学术文本（≥60 % 时边际增益仍为正）。</li>
<li>表现：Knowledge &amp; Vision-Centric VQA 线性提升，且换视觉编码器后趋势不变 → 证明<strong>与编码器无关的通用推理骨架</strong>。</li>
</ul>
<ol>
<li><strong>感知先验</strong></li>
</ol>
<ul>
<li>来源：通用 web-crawl 的多样性，而非某一特定领域；&gt;15 % 视觉描述文本后出现快速饱和。</li>
<li>表现：General &amp; OCR VQA 提升，但换编码器后曲线不一致 → 证明<strong>依赖视觉监督与编码器质量</strong>。</li>
</ul>
<ol>
<li><strong>语言-视觉权衡</strong></li>
</ol>
<ul>
<li>当推理组合 &gt;75 % 时，语言 perplexity 开始恶化；插入 15 % 视觉世界描述可把“语言性能跌落”拉回零界点以上。</li>
</ul>
<p>3. 反向设计数据配方</p>
<p>利用上述定量边界，作者将问题写成一个<strong>带约束的混合优化</strong>：</p>
<blockquote>
<p>给定 300B token 池，求 (推理组合, 视觉组合) 使得<br>max VQA-average<br>s.t. 语言准确率 ≥ 95 % baseline，perplexity ≤ 110 % baseline.</p>
</blockquote>
<p>网格搜索得到 Pareto 最优点：<strong>60 % 推理 + 15 % 视觉世界描述</strong>（图 3、表 1）。随后把该比例映射到 6 大实际语料（web-crawl、百科、学术、文学、数学、代码），通过插值得到可落地的“mix6”配方（表 2）。</p>
<p>4. 规模验证</p>
<ul>
<li>训练两个 7B 模型到 1T token：<br>– mix0（语言优先）<br>– mix6（均衡）</li>
<li>结果（表 3）：<br>– 语言侧：mix6 perplexity 更低（7.49 vs 8.72），准确率更高（65.5 % vs 64.7 %）。<br>– 视觉侧：mix6 在所有 4 类 VQA 上全面领先，总体 +1.3 pp，Vision-Centric +0.3 pp，OCR&amp;Chart +2.1 pp。</li>
</ul>
<p>5. 机制验证</p>
<ul>
<li><strong>盲指令调优</strong>（附录 G）：在无图阶段先学指令格式，再上图；mix6 额外 +0.9 pp，说明推理先确已内嵌。</li>
<li><strong>表示对齐</strong>（图 10）：随着推理文本比例↑，LLM-vision mNN 分数单调提升至 75 %，然后轻微回落，证实<strong>结构化语言数据驱动跨模态拓扑对齐</strong>。</li>
</ul>
<p>总结</p>
<p>论文用<strong>控制实验 + 量化指标 + 优化配方</strong>的三板斧，把“LLM 如何无图自见”从哲学猜想变成可工程化的数据配方问题，并在 1T token 规模上验证其通用性与可扩展性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共消耗 ≈500k GPU-hours，完成 100 余项对照实验，覆盖从 340M 到 13B 参数、0 到 1T token 的全尺度范围。实验按“问题驱动”划分为 5 大组，每组均固定随机种子、评测脚本与解码超参（temperature=0），保证结果可复现。</p>
<p>1. 规模扫描实验（Scaling Sweep）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>水平</th>
<th>观测指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型大小</td>
<td>340M–1B–3B–7B–13B</td>
<td>4 类 VQA 平均、语言 perplexity</td>
</tr>
<tr>
<td>预训练 token 数</td>
<td>0–20B–50B–100B</td>
<td>同上</td>
</tr>
<tr>
<td>语料</td>
<td>固定 web-crawl</td>
<td>同上</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>目的</strong>：验证“视觉先验是否随规模单调涌现”。</li>
<li><strong>结论</strong>：图 1 显示总体 VQA 呈幂律上升，但 OCR&amp;Chart 对模型大小更敏感，Vision-Centric 对大模型 + 大数据才显著增益，提示不同能力 scaling 系数不同。</li>
</ul>
<p>2. 单领域语料实验（Single-Source Ablation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>水平</th>
<th>控制</th>
</tr>
</thead>
<tbody>
<tr>
<td>预训练语料</td>
<td>16 个领域各 30B token</td>
<td>模型 3B，其余语料置空</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>目的</strong>：定位“哪类文本最有效地注入视觉先验”。</li>
<li><strong>结论</strong>：图 2 热图显示 code/arts/food 在 Vision-Centric 领先；web-crawl 在 General/OCR 领先，首次定性区分“推理源”与“感知源”。</li>
</ul>
<p>3. 细粒度配比实验（Mixture Grid）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>水平</th>
<th>其余补全方式</th>
</tr>
</thead>
<tbody>
<tr>
<td>推理组合</td>
<td>0 %–25 %–50 %–75 %–100 %</td>
<td>用通用池线性补齐</td>
</tr>
<tr>
<td>视觉世界组合</td>
<td>同上</td>
<td>同上</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>目的</strong>：找出边际收益饱和点。</li>
<li><strong>结论</strong>：图 3 左→推理收益线性至 75 %；右→视觉描述 15 % 后饱和，奠定后续优化边界。</li>
</ul>
<p>4. 多目标优化实验（Pareto Search）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>步骤</th>
<th>搜索空间</th>
<th>约束</th>
</tr>
</thead>
<tbody>
<tr>
<td>① 理论池</td>
<td>推理 50–85 % × 视觉 5–30 %</td>
<td>30B token，3B 模型</td>
</tr>
<tr>
<td>② 实际池</td>
<td>6 大语料 0–100 % 插值</td>
<td>语言性能 ≥ 95 % baseline</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>产出</strong>：表 1 给出理论最优 60/15；表 2 的 mix6 为可落地配方（52 % 推理 + 15 % 视觉）。</li>
<li><strong>验证</strong>：mix0–mix10 共 11 个模型，各训 50B token，最终 mix6 取得语言-视觉综合排名第一。</li>
</ul>
<p>5. 视觉编码器鲁棒性实验（Encoder Transfer）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>水平</th>
<th>固定</th>
</tr>
</thead>
<tbody>
<tr>
<td>视觉骨干</td>
<td>MetaCLIP-B/16、DINOv2-G、MAE-H</td>
<td>同一组 LLM（推理比例 0–100 %）</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>目的</strong>：检验“推理先验是否编码器无关”。</li>
<li><strong>结论</strong>：图 5 显示三条曲线在 Knowledge/Vision-Centric 上几乎重合，感知任务曲线分散，定量证明推理先验的<strong>跨编码器通用性</strong>。</li>
</ul>
<p>6. 指令调优剥离实验（Instruction-Tuning Ablation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>干预</th>
<th>阶段</th>
<th>数据量</th>
</tr>
</thead>
<tbody>
<tr>
<td>感知数据剔除</td>
<td>SFT</td>
<td>1.8M → 0.9M → 0</td>
</tr>
<tr>
<td>推理数据剔除</td>
<td>SFT</td>
<td>0.6M → 0.3M → 0</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>目的</strong>：量化“下游能力多少来自 LLM 先验 vs 视觉监督”。</li>
<li><strong>结论</strong>：图 6 显示剔除感知数据→OCR&amp;Chart 跌 9 %；剔除推理数据→Knowledge 仅跌 3 %，反向证实<strong>推理主要靠预训练，感知靠后训练</strong>。</li>
</ul>
<p>7. 表示对齐实验（Cross-modal Kernel Alignment）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>度量</th>
<th>数据</th>
</tr>
</thead>
<tbody>
<tr>
<td>推理文本比例</td>
<td>mNN@20</td>
<td>WIT 3×10k 图文对</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>结论</strong>：图 10 曲线在 75 % 处达峰，说明<strong>结构化文本可增强文本-视觉表示拓扑一致性</strong>，为“柏拉图假说”提供定量证据。</li>
</ul>
<p>8. 规模外推实验（1T Token Scale-Up）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>配方</th>
<th>token</th>
<th>下游</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B-L</td>
<td>mix0</td>
<td>1T</td>
<td>语言优先 baseline</td>
</tr>
<tr>
<td>7B-B</td>
<td>mix6</td>
<td>1T</td>
<td>视觉增强</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>结果</strong>：表 3 显示 7B-B 语言 perplexity 更低（7.49 vs 8.72），VQA 总体 +1.3 pp，验证<strong>小尺度最优配方可零改动外推至大尺度</strong>。</li>
</ul>
<p>9. 辅助诊断实验（Appendix）</p>
<ul>
<li><strong>MLE-Bench</strong>：1 861 张含多尺度对象的存在问答，用于细分感知先验的尺度依赖性（图 7）。</li>
<li><strong>盲指令调优</strong>：先无图 SFT 再上图，探查模型是否利用语言捷径（表 6、H 节）。</li>
<li><strong>幻觉分析</strong>：随机去掉图像让主流模型答题，展示盲 VQA 幻觉普遍存在于 GPT-5、Claude-Opus-4 等前沿模型（附录 H）。</li>
</ul>
<p>实验全景图（一句话总结）</p>
<blockquote>
<p>从 340M 到 13B、从 0 到 1T token，从单领域语料到细粒度配比，从编码器切换到指令剥离，论文用<strong>阶梯式控制变量</strong>把“视觉先验从何而来”拆解成可度量、可优化、可外推的数据科学问题，并在 1T 规模闭环验证。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可将“LLM 视觉先验”从当前静态图像-文本设置推向更通用、更安全、更可解释的多模态智能，均基于论文已暴露的边界与矛盾点延伸而来。</p>
<p>1. 架构外推：超越 adapter 范式</p>
<ul>
<li><strong>离散视觉 token 化模型</strong>（Chameleon、Emu3）与<strong>端到端联合训练</strong>（Diao et al. 2025）是否仍满足“推理先验主导、感知后训练主导”的分解？</li>
<li>若视觉编码器可训练，文本侧先验会不会被视觉信号“覆写”？需设计<strong>联合训练时的梯度掩码实验</strong>，量化语言-视觉梯度互斥/协同比例。</li>
</ul>
<p>2. 时间维度：视频与因果推理</p>
<ul>
<li>静态先验能否直接迁移到<strong>视频问答或动作预测</strong>？</li>
<li>假设：叙事性文本（文学、剧本）比代码更能提供“对象-动作-因果”先验。</li>
<li>实验：用不同比例的“故事类” vs“代码类”文本预训练，随后在 Something-Something V2、OVBench 上测 zero-shot 动作识别与因果问答。</li>
</ul>
<p>3. 层次感知：多尺度、多模态存在性</p>
<ul>
<li>MLE-Bench 仅覆盖 0–100 % 像素占比；可扩展：</li>
<li><strong>极小微物体</strong>（&lt;5 % 像素）与<strong>透明/反光物体</strong>存在性，探查语言模型是否具备材质-光照先验。</li>
<li><strong>跨模态存在幻觉</strong>：同一段文本描述同时搭配存在/不存在图像，看 LLM 是否倾向“信文字”还是“信视觉”，从而诊断先验冲突强度。</li>
</ul>
<p>4. 安全与偏见：文本→视觉的隐性 stereotype</p>
<ul>
<li>当前仅测能力，未测伤害。可构建<strong>Visual StereoBench</strong>：</li>
<li>用职业、性别、种族等敏感词生成图像-文本对，测量预训练 LLM 在盲 VQA 场景下的关联强度（如“护士=女性”概率）。</li>
<li>对比不同单领域语料（医学、法律、文学）模型的偏见差异，进而用<strong>偏见先验系数</strong>作为正则项反向优化数据配比。</li>
</ul>
<p>5. 数据动力学：在线混合与课程学习</p>
<ul>
<li>论文用静态配比；可探索<strong>课程式预训练</strong>：</li>
<li>先大比例推理文本→中期加入视觉世界描述→后期回归通用语料，观察是否比一次性混合收敛更快、先验更强。</li>
<li>引入<strong>可微分数据选择器</strong>（Albalak et al. 2023），把“推理-视觉比例”做成连续可学习参数，在线最大化下游视觉验证集增益。</li>
</ul>
<p>6. 跨模态编辑：先验的局部擦除与注入</p>
<ul>
<li>利用最近<strong>模型手术</strong>技术（Chen et al. 2025 的 parameter merging）：</li>
<li>在 LLM 内部定位“深度感知神经元”“颜色感知神经元”，验证删除这些参数是否仅跌落对应 MLE-Bench 子任务，而不影响 Knowledge VQA。</li>
<li>反向操作：把代码预训练模型的“推理模块” surgically transplant 到通用 LLM，看能否在零视觉数据情况下直接提升 Vision-Centric 任务。</li>
</ul>
<p>7. 小样本极端场景：1 % 图文对齐</p>
<ul>
<li>论文仍用 1M～7M 图文对。若将视觉指令数据压缩到 1 %（≈10k 对），mix6 配方是否仍保持优势？</li>
<li>对比基线：随机配方 vs mix6，测量 5-shot 与 full-data 的 gap，可得出<strong>数据配比带来的样本效率倍数</strong>。</li>
</ul>
<p>8. 多语言与跨文化视觉先验</p>
<ul>
<li>非英语语料（中文、阿拉伯语）是否同样满足“推理主导”规律？</li>
<li>假设：表意文字系统自带更多视觉结构信息，可能降低对视觉世界描述文本的饱和阈值。</li>
<li>实验：用同等规模中文科技文献 vs 中文小说预训练，对比英语模型的 MLE-Bench 性能，计算“语言类型 → 视觉先验”增益系数。</li>
</ul>
<p>9. 可解释工具：可视化语言-视觉共享子空间</p>
<ul>
<li>利用稀疏自编码器（Sparse AutoEncoder）在 LLM 中间层提取“多模态神经元”，检查其激活值是否与图像 patch 的 DINOv2 特征做<strong>线性预测</strong>。</li>
<li>若存在双向可预测性，即可用文本激活向量<strong>无图生成“特征图”</strong>，实现真正的“文本渲染视觉”可解释接口。</li>
</ul>
<p>10. 持续学习与灾难性遗忘</p>
<ul>
<li>先按 mix6 预训练→再在纯代码数据上继续预训练，观察视觉 VQA 性能是否灾难性跌落；</li>
<li>引入<strong>正交梯度投影</strong>（Orthogonal Gradient Descent）或<strong>模型合并</strong>（model soups）看能否锁定视觉先验，同时持续提升代码能力。</li>
</ul>
<p>总结一句话</p>
<p>论文把“文本如何长出视觉”变成可测量、可优化的数据配方；下一步应把该配方放入<strong>动态课程、安全审计、视频时序、跨文化、模型手术</strong>等更复杂场景，检验先验的<strong>通用性、鲁棒性与可解释性</strong>，最终让语言模型在“从未看见”的情况下也能**安全、可控、可解释地“看见”**整个世界。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文主旨</p>
<p>仅通过文本预训练的大型语言模型（LLM）竟能“无图自见”——在少量图文对齐甚至零样本条件下完成视觉任务。作者首次把这一玄学现象转化为可测量、可干预、可放大的数据科学问题，系统回答“视觉先验从何而来、如何按需配制”。</p>
<p>1. 核心发现</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>结论</th>
<th>关键数字</th>
</tr>
</thead>
<tbody>
<tr>
<td>视觉先验能否拆分？</td>
<td>可拆为感知先验与推理先验，统计独立</td>
<td>Spearman ρ≈0.37 vs 0.33</td>
</tr>
<tr>
<td>推理先验来源？</td>
<td>代码、数学、学术文本</td>
<td>比例≤75 % 持续增益</td>
</tr>
<tr>
<td>感知先验来源？</td>
<td>通用 web-crawl 多样性</td>
<td>≥15 % 视觉描述即饱和</td>
</tr>
<tr>
<td>最优配方？</td>
<td>60 % 推理 + 15 % 视觉世界</td>
<td>3B-30B → 1T-7B 均最优</td>
</tr>
<tr>
<td>谁主导下游能力？</td>
<td>推理靠 LLM，感知靠视觉监督与编码器</td>
<td>剔除感知 SFT 数据 OCR 跌 9 %</td>
</tr>
</tbody>
</table>
</div>
<p>2. 实验规模</p>
<ul>
<li>100+ 控制实验，500k GPU-hours</li>
<li>5 模型尺度（340M–13B）× 9 数据规模（0–1T tokens）</li>
<li>16 类单领域语料、24 种配比网格、3 类视觉编码器</li>
<li>新基准 MLE-Bench（1 861 图，分小/中/大对象存在问答）</li>
</ul>
<p>3. 方法论</p>
<ol>
<li>先验量化：16 项 VQA → 4 能力簇（General/Knowledge/OCR/Vision-Centric）</li>
<li>因果干预：固定模型大小与总 token，单变量网格扫描</li>
<li>多目标优化：语言性能不降前提下最大化视觉平均分</li>
<li>规模外推：用 3B-30B 实验室最优配方直接放大到 7B-1T，闭环验证</li>
</ol>
<p>4. 结果</p>
<ul>
<li>语言侧：均衡配方（mix6）perplexity 更低（7.49 vs 8.72），准确率略升</li>
<li>视觉侧：全部 4 簇任务一致提升，总体 VQA 平均 +1.3 pp</li>
<li>通用性：换视觉编码器后推理增益趋势不变，证明先验编码器无关</li>
<li>效率：同等视觉性能下，所需图文对齐数据可减少至原来 40 %</li>
</ul>
<p>5. 贡献一句话</p>
<p>论文首次给出<strong>可复现、可放大、可工程化</strong>的“文本→视觉”配方表：</p>
<blockquote>
<p><strong>60 % 推理密集型文本 + 15 % 视觉世界描述 + 25 % 通用语料</strong><br>在纯文本阶段即可<strong>预先栽培</strong>出更强的多模态 LLM，把“偶然涌现”变成“ deliberate design”。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning%20to%20See%20Before%20Seeing%3A%20Demystifying%20LLM%20Visual%20Priors%20from%20Language%20Pre-training，链接是https://arxiv.org/pdf/2509.26625，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.26625。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Junlin Han,Shengbang Tong,David Fan,Yufan Ren,Koustuv Sinha,Philip Torr,Filippos Kokkinos</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,cs.CV,cs.MM</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26625.pdf">https://arxiv.org/pdf/2509.26625.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26625">https://arxiv.org/abs/2509.26625</a></p>
<p><strong>Arxiv ID</strong>: 2509.26625</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26625">https://papers.cool/arxiv/2509.26625</a></p>
<p><strong>Published</strong>: 2025-09-30T17:57:44Z</p>
<p><strong>Updated</strong>: 2025-09-30T17:57:44.000Z</p>
<hr>
<h3 id="7-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents"><a href="#7-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents" class="headerlink" title="7. OceanGym: A Benchmark Environment for Underwater Embodied Agents"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26536">OceanGym: A Benchmark Environment for Underwater Embodied Agents</a></h3><p>We introduce OceanGym, the first comprehensive benchmark for ocean underwater embodied agents, designed to advance AI in one of the most demanding real-world environments. Unlike terrestrial or aerial domains, underwater settings present extreme perceptual and decision-making challenges, including low visibility, dynamic ocean currents, making effective agent deployment exceptionally difficult. OceanGym encompasses eight realistic task domains and a unified agent framework driven by Multi-modal Large Language Models (MLLMs), which integrates perception, memory, and sequential decision-making. Agents are required to comprehend optical and sonar data, autonomously explore complex environments, and accomplish long-horizon objectives under these harsh conditions. Extensive experiments reveal substantial gaps between state-of-the-art MLLM-driven agents and human experts, highlighting the persistent difficulty of perception, planning, and adaptability in ocean underwater environments. By providing a high-fidelity, rigorously designed platform, OceanGym establishes a testbed for developing robust embodied AI and transferring these capabilities to real-world autonomous ocean underwater vehicles, marking a decisive step toward intelligent agents capable of operating in one of Earth’s last unexplored frontiers. The code and data are available at <a target="_blank" rel="noopener" href="https://github.com/OceanGPT/OceanGym">https://github.com/OceanGPT/OceanGym</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们推出了OceanGym，这是首个针对海洋水下具身智能体的综合基准，旨在推进AI在最具挑战性的真实环境之一的应用。与陆地或空中领域不同，水下环境呈现极端的感知和决策挑战，包括低能见度和动态洋流，使得智能体的有效部署异常困难。OceanGym涵盖了八个现实任务领域，并提供由多模态大语言模型（MLLM）驱动的统一智能体框架，整合了感知、记忆和序列决策功能。智能体需要理解光学和声纳数据，自主探索复杂环境，并在这些恶劣条件下完成长期目标。大量实验显示，当前最先进的MLLM驱动智能体与人类专家之间存在显著差距，凸显了海洋水下环境中感知、规划和适应能力的持续挑战。通过提供高保真、严格设计的平台，OceanGym为开发稳健的具身AI并将这些能力转移到现实水下自主海洋车辆建立了测试床，标志着向能够在地球最后未开发前沿之一中操作的智能体迈出了关键的一步。代码和数据可在 <a target="_blank" rel="noopener" href="https://github.com/OceanGPT/OceanGym">https://github.com/OceanGPT/OceanGym</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在填补“水下具身智能体”研究领域的基准空白，提出首个面向真实海洋环境的综合测试平台 OCEANGYM，系统评估多模态大语言模型（MLLM）在极端水下条件下的感知、记忆与连续决策能力，并揭示当前模型与人类专家之间的显著性能鸿沟，为将来在真实自主水下航行器（AUV）上部署鲁棒可迁移的具身智能奠定基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线：</p>
<ol>
<li>具身仿真平台</li>
</ol>
<ul>
<li>地面：Matterport3D、House3D、Habitat</li>
<li>空中：AirSim、OpenUAV、CityNav</li>
<li>海洋：HoloOcean、MarineGym</li>
</ul>
<ol>
<li>MLLM 驱动的具身智能体</li>
</ol>
<ul>
<li>室内/室外导航：Vision-and-Language Navigation、GOAT-Bench、NavBench</li>
<li>操作抓取：VLM-Bench、EmbodiedBench</li>
<li>无人机/无人车：OpenFly、UrbanVideo-Bench、EmbodiedCity</li>
</ul>
<p>上述工作均未针对“低能见度、动态洋流、声呐-光学混合感知”的真实水下环境提供统一基准，OCEANGYM 首次将该场景引入 MLLM 评估体系。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“构建环境-提出框架-系统评估”三步解决水下具身智能研究缺失的问题：</p>
<ol>
<li>构建高保真基准环境</li>
</ol>
<ul>
<li>基于 UE5.3 与 HoloOcean，建立 800 m×800 m 可调节深度海域，包含管道、沉船、风机基础等 8 类真实结构，支持动态光照与随机起始位姿，形成 8 项典型任务（搜索、巡检、对接等）。</li>
</ul>
<ol>
<li>提出统一 MLLM 智能体框架</li>
</ol>
<ul>
<li>将控制-感知回路形式化为带上下文记忆的 POMDP：</li>
<li>状态  s_t=(x_t,y_t,z_t,φ_t,θ_t,psi_t) </li>
<li>观测  O_t=O_t^(R),O_t^(S) （6 向 RGB+声呐）</li>
<li>动作空间  A=f,b,l,r,u,d,rl,rr </li>
<li>记忆  m_t  用滑动窗口保存最近 K 步文本描述与动作，由 MLLM 摘要更新</li>
<li>策略  π_θ(a_t,y_tmid L,O_t,m_t,T,δ)  一次性输出感知答案或控制指令，实现“感知-记忆-决策”端到端推理。</li>
</ul>
<ol>
<li>系统实验与诊断</li>
</ol>
<ul>
<li>在浅水/深水、光照良好</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Yida Xue,Mingjun Mao,Xiangyuan Ru,Yuqi Zhu,Baochang Ren,Shuofei Qiao,Mengru Wang,Shumin Deng,Xinyu An,Ningyu Zhang,Ying Chen,Huajun Chen</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.CV,cs.LG,cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26536.pdf">https://arxiv.org/pdf/2509.26536.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26536">https://arxiv.org/abs/2509.26536</a></p>
<p><strong>Arxiv ID</strong>: 2509.26536</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26536">https://papers.cool/arxiv/2509.26536</a></p>
<p><strong>Published</strong>: 2025-09-30T17:09:32Z</p>
<p><strong>Updated</strong>: 2025-09-30T17:09:32.000Z</p>
<hr>
<h3 id="8-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models"><a href="#8-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models" class="headerlink" title="8. More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.25848">More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models</a></h3><p>Reasoning has emerged as a pivotal capability in Large Language Models (LLMs). Through Reinforcement Learning (RL), typically Group Relative Policy Optimization (GRPO), these models are able to solve complex tasks such as mathematics and code generation. Building on these advances, recent research has sought to extend reasoning to Vision-Language Models (VLMs), yielding promising results across diverse visual tasks. Despite this progress, our study uncovers the dual nature of multimodal reasoning: while it substantially enhances logical inference and facilitates performance on challenging problems, it may gradually impair perceptual grounding, leading to recognition failures on otherwise basic visual questions. Through further analysis, we attribute this phenomenon to visual forgetting, wherein prolonged reasoning causes the model to increasingly disregard visual input. To address this, we propose Vision-Anchored Policy Optimization (VAPO), a simple yet effective method that explicitly steers the reasoning process toward visually grounded trajectories. Our result model, VAPO-Thinker-7B, significantly strengthens the model’s reliance on visual information and achieves new state-of-the-art results on a wide range of established benchmarks. Project page: <a target="_blank" rel="noopener" href="https://xytian1008.github.io/VAPO/">https://xytian1008.github.io/VAPO/</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>推理已成为大型语言模型（LLMs）中的关键能力。通过强化学习（RL），通常使用群体相对策略优化（GRPO），这些模型能够解决诸如数学和代码生成等复杂任务。在这些进展的基础上，近期研究尝试将推理扩展到视觉-语言模型（VLMs），并在各种视觉任务中取得了令人鼓舞的成果。尽管取得了这些进展，我们的研究揭示了多模态推理的双重性质：虽然它显著增强了逻辑推理能力并有助于解决困难问题，但它可能会逐渐削弱感知基础，从而导致对本应简单的视觉问题出现识别失败。通过进一步分析，我们将这一现象归因于视觉遗忘，即长时间的推理会使模型越来越忽视视觉输入。为了解决这一问题，我们提出了视觉锚定策略优化（VAPO），这是一种简单而有效的方法，能够显式引导推理过程朝向视觉基础的路径。我们的最终模型 VAPO-Thinker-7B 大幅增强了模型对视觉信息的依赖，并在广泛的既有基准测试中取得了新的最先进结果。项目页面：<a target="_blank" rel="noopener" href="https://xytian1008.github.io/VAPO/">https://xytian1008.github.io/VAPO/</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文核心揭示并试图解决“多模态推理中的视觉遗忘”问题：</p>
<ul>
<li><strong>现象</strong>：在视觉-语言模型（VLM）中引入链式推理（chain-of-thought）或强化学习推理（如 GRPO）后，逻辑推理能力虽显著提升，却伴随<strong>视觉感知能力逐渐下降</strong>——模型越“深入思考”，越忽略图像本身，导致在原本简单的视觉问题上出错。</li>
<li><p><strong>归因</strong>：作者将这一现象定义为 <strong>visual forgetting</strong>——随着推理步数增加，模型对视觉 token 的关注度单调递减，决策由历史文本主导而非图像证据。</p>
</li>
<li><p><strong>目标</strong>：提出一种训练级方法 <strong>VISION-ANCHORED POLICY OPTIMIZATION (VAPO)</strong>，通过在推理轨迹中插入可验证的视觉锚点（visual anchors）并施加<strong>感知奖励</strong>，显式强化模型在整个推理过程中对视觉信息的依赖，从而缓解视觉遗忘，实现逻辑与感知双赢。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中系统梳理了三条与本文问题直接相关的研究脉络，并指出它们与“视觉遗忘”现象的交集与空白。按主题归纳如下：</p>
<ol>
<li>大模型推理（LLM Reasoning）</li>
</ol>
<ul>
<li>链式思维提示 / 监督微调</li>
<li>Wei et al. 2022；Kojima et al. 2022；Yao et al. 2023a；Tian et al. 2024, 2025a；Zou et al. 2025</li>
<li>强化学习发现推理策略</li>
<li>GRPO（Shao et al. 2024）→ 无需人工轨迹即可自探索“aha”式自我验证、反思行为</li>
<li>Test-Time Scaling（TTS）</li>
<li>Muennighoff et al. 2025；Snell et al. 2025；Qu et al. 2025；Yao et al. 2025<br>→ 默认“越长推理越好”，本文首次指出该假设在<strong>多模态场景失效</strong>。</li>
</ul>
<ol>
<li>视觉-语言模型推理（VLM Reasoning）</li>
</ol>
<ul>
<li>SFT→RL 两阶段</li>
<li>Peng et al. 2025；Tan et al. 2025；Yang et al. 2025b；Feng et al. 2025</li>
<li>纯 RL 无需 SFT</li>
<li>Liu et al. 2025；Wang et al. 2025c；Meng et al. 2025</li>
<li>早期质疑信号</li>
<li>Li et al. 2025：显式推理仅带来边际增益</li>
<li>Xia et al. 2025：RL 训练后期推理步长<strong>反而缩短</strong>（collapse）<br>→ 本文首次<strong>系统验证</strong>“推理损害感知”并给出训练级解决方案。</li>
</ul>
<ol>
<li>视觉遗忘（Forgetting in VLMs）</li>
</ol>
<ul>
<li>文本偏置早期观察</li>
<li>Chen et al. 2024a；Fu et al. 2024；Yao et al. 2023b</li>
<li>测试阶段缓解</li>
<li>对比解码 Leng et al. 2024；Wang et al. 2024b</li>
<li>注意力重分配 Tu et al. 2025；Gong et al. 2024<br>→ 均为<strong>推理后</strong>补救，无法根治行为缺陷。</li>
<li>本文差异：首次提出<strong>训练期</strong>持续锚定视觉证据的 VAPO，从根本上抑制遗忘。</li>
</ul>
<p>综上，现有文献或聚焦纯文本推理，或仅在测试阶段抑制视觉幻觉，尚未在<strong>多模态强化学习训练框架</strong>内针对“推理越长→视觉越弱”这一双重性提出系统性的训练级解决方案，这正是本文填补的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“视觉遗忘”问题形式化为<strong>推理过程中模型对视觉 token 的关注度单调递减</strong>，导致后期步骤被自身文本历史主导。为此提出<strong>训练级</strong>算法 VAPO（VISION-ANCHORED POLICY OPTIMIZATION），核心思路是：在生成轨迹中嵌入一系列可验证的“视觉锚点”，用锚点处的感知准确率构造额外奖励，显式引导策略保持对图像的依赖。具体步骤如下：</p>
<ol>
<li><p>生成视觉断言（Visual Claims）<br>对每张图像，用 GPT-5 生成 20 条简短、<strong>仅依赖像素即可判定</strong>的陈述（10 正 10 负），且与问题文本解耦，确保模型必须“回头看图”才能判断真伪。</p>
</li>
<li><p>插入视觉锚点（Anchors）<br>在每条推理轨迹 o 上随机采样 K=20 个位置 a_k；将对应断言 c_k 拼接到前缀上下文后，让模型输出 yes/no，得到二元正确性 s_k∈{0,1}。</p>
</li>
<li><p>感知奖励（Perception Reward）<br>采用<strong>后期加重</strong>加权平均</p>
</li>
</ol>
<p>R<em>(perc)=∑</em>(k=1)^K w<em>k s_k∑</em>(k=1)^K w_k,quad w_k=exp!l(β·t(a_k) / (T)r), β=1.5</p>
<p>越靠后的锚点权重越大，直接惩罚“推理后期看不清”。</p>
<ol>
<li>总体奖励</li>
</ol>
<p>R<em>i=R</em>(acc)+R<em>(fmt)+γ·1</em>([R<em>acc)=1]· R</em>(perc),quad γ=0.1</p>
<p>只有当答案正确时才追加感知奖励，防止模型通过“缩短推理+乱猜”来刷感知分。</p>
<ol>
<li>策略优化<br>以上述 R_i 作为序列级优势估计 Â_{i,t}，在 GRPO 框架内做 clipped policy gradient 更新；除新增锚点评分外，其余与标准 GRPO 完全一致，实现代价低（单 token 判断，几乎不增加 rollout 长度）。</li>
</ol>
<p>通过全程“看图答题”式监督，VAPO 把视觉锚点转化为稠密奖励信号，<strong>在训练阶段持续强化模型对图像的依赖</strong>，从而缓解甚至逆转“越长推理越健忘”的趋势。实验显示，VAPO-Thinker-7B 在 10 个基准上平均提升 2–4%，且视觉注意力在整个推理链条中保持高位，首次实现了逻辑与感知双重增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“视觉遗忘”假设与 VAPO 有效性，从<strong>现象验证→根因诊断→方法对比→消融与泛化</strong>四个层次展开系统实验。主要结果如下（均基于 10 个公开基准，覆盖数学、逻辑、通用视觉任务）：</p>
<ol>
<li>双重性现象验证</li>
</ol>
<ul>
<li>早期决策（Early-Decision）曲线<br>在 4 个代表模型、4 类基准上逐段截断推理，发现：<br>– 前 30–50 % 推理步带来显著增益；继续延长反而掉点，<strong>MMStar/HallusionBench 峰值→终点下降 &gt;2 %</strong>。</li>
<li>错误归因<br>用 GPT-5 对 Vision-R1 的 55 % 错误分类：55.23 % 为感知错误，其中 32.35 % 可通过“提前停止”纠正，直接证明<strong>后期推理损害视觉感知</strong>。</li>
</ul>
<ol>
<li>视觉遗忘根因诊断</li>
</ol>
<ul>
<li>注意力时序曲线<br>跟踪视觉 token 注意力占比：vanilla 推理从 0.25 单调降至 0.05；插入“视觉回放”或“focus prompt”可立即拉回注意力并提升 1.5 % 平均精度，验证<strong>遗忘是性能瓶颈</strong>。</li>
</ul>
<ol>
<li>VAPO 主实验</li>
</ol>
<ul>
<li>10 基准全面评估（7B 规模）<br>数学类：平均 49.1 % → 51.1 %，<strong>超 Vision-R1 2.0 %</strong><br>通用类：平均 59.9 % → 63.1 %，<strong>绝对 SOTA</strong></li>
<li>与测试阶段补救对比<br>将强基线再外挂“视觉回放/Focus Prompt”，VAPO 仍领先 2.3–2.8 %，说明<strong>训练级锚定优于推理级补救</strong>。</li>
</ul>
<ol>
<li>消融与超参实验</li>
</ol>
<ul>
<li>锚点数量 K<br>K=0 退化为 GRPO，平均 51.5 %；K=20 达 55.9 % 后饱和。</li>
<li>后期权重 β<br>β=1.5 最佳，再增大反降，证实<strong>后期感知监督最关键</strong>。</li>
<li>感知奖励权重 γ<br>γ=0.1 综合最优；视觉密集型任务需更大 γ，数学任务需较小 γ。</li>
<li>数据增强对照<br>仅用相同视觉断言做“增广训练”几乎无效（+0.3 %），排除<strong>增益来自数据量</strong>的可能。</li>
<li>断言类型消融<br>非视觉依赖或问题耦合断言显著掉点，验证<strong>断言必须纯视觉且与问题解耦</strong>。</li>
</ul>
<ol>
<li>深入分析</li>
</ol>
<ul>
<li>注意力曲线对比<br>VAPO 推理全程视觉注意力维持 ≈0.20，显著高于基线，且精度随步数<strong>单调上升</strong>而非先升后降。</li>
<li>训练动态<br>感知奖励与准确率同步攀升，表明模型逐步学会“边看边想”。</li>
<li>错误修正率<br>VAPO 对基线感知错误修正高达 20 %，是整体增益的主要来源。</li>
<li>计算开销<br>相比 GRPO 仅增加 0.5 h（18.8 h → 19.2 h），远低于 DAPO 的 +6.4 h，却带来 +4.37 % 精度提升。</li>
</ul>
<ol>
<li>尺度与组合实验</li>
</ol>
<ul>
<li>3B 模型：VAPO-Thinker-3B 平均 47.6 %，<strong>超同尺最强基线 2.3 %</strong>。</li>
<li>与推理补救叠加：VAPO 再外挂视觉回放/Focus Prompt 仅 ±0.2 %，说明<strong>视觉遗忘已被充分缓解</strong>。</li>
</ul>
<p>综上，论文通过<strong>现象→原因→方法→消融</strong>的完整实验链条，既验证了“推理越长越健忘”的双重性，也证明了 VAPO 在训练阶段持续锚定视觉信息的有效性与通用性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向可从<strong>问题本质、方法局限与场景扩展</strong>三条线展开：</p>
<p>1. 视觉遗忘的机制层面</p>
<ul>
<li><p><strong>因果剖析</strong><br>建立干预-反事实框架，直接操纵“视觉注意力阈值”或“推理深度”，量化遗忘对错误率的因果效应，排除容量或训练数据混杂。</p>
</li>
<li><p><strong>层级与模态耦合分析</strong><br>追踪<strong>跨层</strong>视觉注意力与<strong>文本自注意力</strong>的互信息，定位“视觉→语言”信息流失具体发生在哪些 Transformer 块。</p>
</li>
<li><p><strong>动态容量分配</strong><br>研究推理步数增加时，模型内部逻辑与感知模块的<strong>梯度冲突</strong>（gradient interference），从优化景观角度解释为何感知权重被挤压。</p>
</li>
</ul>
<p>2. VAPO 方法深化</p>
<ul>
<li><p><strong>自适应锚点</strong><br>让策略网络自己预测“何时需要再看图”，实现<strong>稀疏但关键</strong>的锚点插入，减少 20→K≈5 的调用次数，保持性能同时提速。</p>
</li>
<li><p><strong>断言质量上限</strong><br>用更强多模态模型或人工标注生成<strong>细粒度、关系型、计数/位置敏感</strong>断言，检验感知奖励的<strong>天花板效应</strong>。</p>
</li>
<li><p><strong>奖励分解与防 Hack</strong><br>引入<strong>差异隐私</strong>或<strong>困惑度正则</strong>，防止模型记住断言模板而<strong>抬高感知分却不真看图</strong>；探索<strong>对抗断言生成器</strong>与策略的博弈训练。</p>
</li>
<li><p><strong>多任务权衡</strong><br>构建数学-视觉-知识三维帕累托前沿，自动调节 γ、β 的<strong>任务相关函数</strong> γ(task-embedding)，实现单一模型在逻辑/感知光谱上的<strong>动态切换</strong>。</p>
</li>
</ul>
<p>3. 场景与形态扩展</p>
<ul>
<li><p><strong>多图像/视频推理</strong><br>将视觉锚点扩展为<strong>跨帧断言</strong>（“第 t 帧的公交车与第 t+20 帧是否同一辆”），研究时序一致性遗忘及<strong>长上下文视觉锚定</strong>。</p>
</li>
<li><p><strong>具身导航与机器人</strong><br>在部分可观测环境中，锚点变为**“当前视野是否包含目标物体”<strong>，验证 VAPO 能否缓解</strong>长轨迹推理导致的定位漂移**。</p>
</li>
<li><p><strong>端到端自动驾驶</strong><br>引入 VAPO 对鸟瞰图与摄像头流进行<strong>联合锚定</strong>，防止规划模块在复杂推理（多智能体交互）时忽视<strong>关键视觉障碍</strong>。</p>
</li>
<li><p><strong>多模态大模型安全</strong><br>研究视觉遗忘是否加剧<strong>幻觉与隐私泄露</strong>；利用 VAPO 的“视觉忠诚”属性降低<strong>对抗文本提示</strong>对模型判断的误导成功率。</p>
</li>
</ul>
<p>4. 训练-推理协同</p>
<ul>
<li><p><strong>测试时锚定</strong><br>训练后保持锚点网络，推理阶段<strong>按需调用</strong>少量锚点做“视觉校验”，实现<strong>Test-Time Scaling</strong>的新范式：既延长推理又保证视觉接地。</p>
</li>
<li><p><strong>可解释性工具</strong><br>将锚点评分实时可视化，提供**“何时看图、看何处”<strong>的热力图，为教育、医疗等高风险场景提供</strong>可审计的决策轨迹**。</p>
</li>
</ul>
<p>综上，从<strong>机理揭示→方法自适→场景泛化→系统安全</strong>四个维度，视觉遗忘与 VAPO 仍有广阔的深耕空间。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：将链式推理引入视觉-语言模型（VLM）后，出现“越长推理越健忘”的双重性——逻辑增强却伴随视觉感知下降。</li>
<li><strong>根因</strong>：提出“视觉遗忘”概念，证实随推理步数增加，模型对图像 token 的关注度单调衰减，导致后期决策被自身文本历史主导。</li>
<li><strong>方法</strong>：设计训练级算法 VAPO（Vision-Anchored Policy Optimization），在推理轨迹中嵌入可验证的视觉断言作为锚点，用锚点处的感知准确率构造加权奖励，显式强化模型全程依赖视觉证据。</li>
<li><strong>结果</strong>：VAPO-Thinker-7B 在 10 大基准上平均提升 2–4%，实现逻辑与感知双赢，确立新 SOTA；消融与对比实验验证增益源自“视觉锚定”而非数据增广或测试技巧。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是More%20Thought%2C%20Less%20Accuracy%3F%20On%20the%20Dual%20Nature%20of%20Reasoning%20in%20Vision-Language%20Models，链接是https://arxiv.org/pdf/2509.25848，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25848。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xinyu Tian,Shu Zou,Zhaoyuan Yang,Mengqi He,Fabian Waschkowski,Lukas Wesemann,Peter Tu,Jing Zhang</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25848.pdf">https://arxiv.org/pdf/2509.25848.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25848">https://arxiv.org/abs/2509.25848</a></p>
<p><strong>Arxiv ID</strong>: 2509.25848</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25848">https://papers.cool/arxiv/2509.25848</a></p>
<p><strong>Published</strong>: 2025-09-30T06:37:47Z</p>
<p><strong>Updated</strong>: 2025-09-30T06:37:47.000Z</p>
<hr>
<h3 id="9-Thinking-Free-Policy-Initialization-Makes-Distilled-Reasoning-Models-More-Effective-and-Efficient-Reasoners"><a href="#9-Thinking-Free-Policy-Initialization-Makes-Distilled-Reasoning-Models-More-Effective-and-Efficient-Reasoners" class="headerlink" title="9. Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26226">Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners</a></h3><p>Reinforcement Learning with Verifiable Reward (RLVR) effectively solves complex tasks but demands extremely long context lengths during training, leading to substantial computational costs. While multi-stage training can partially mitigate this, starting with overly short contexts often causes irreversible performance degradation, ultimately failing to reduce overall training compute significantly. In this paper, we introduce <strong>T</strong>hinking-<strong>F</strong>ree <strong>P</strong>olicy <strong>I</strong>nitialization (<strong>TFPI</strong>), a simple yet effective adaptation to RLVR that bridges long Chain-of-Thought (CoT) distillation and standard RLVR. TFPI employs a simple <em>ThinkFree</em> operation, explicitly discarding the thinking content via a direct <em>&lt;/think&gt;</em> append, to reduce token usage during inference. Training with <em>ThinkFree</em>-adapted inputs improves performance and lowers token consumption, even in the original slow-thinking mode. Extensive experiments across various benchmarks have shown that TFPI accelerates RL convergence, achieves a higher performance ceiling, and yields more token-efficient reasoning models without specialized rewards or complex training designs. With TFPI only, we train a 4B model to reach 89.0% accuracy on AIME24 and 65.5% on LiveCodeBench using less than 4K H20 hours.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>带可验证奖励的强化学习（Reinforcement Learning with Verifiable Reward，RLVR）能够有效解决复杂任务，但在训练过程中需要极长的上下文长度，从而导致巨大的计算成本。虽然多阶段训练可以在一定程度上缓解这一问题，但如果从过短的上下文开始训练，往往会导致不可逆的性能下降，最终难以显著降低整体训练计算量。在本文中，我们提出了<strong>无思考策略初始化（Thinking-Free Policy Initialization，TFPI）</strong>，这是一种对RLVR的简单而有效的改进方法，它弥合了长链条思维（Chain-of-Thought, CoT）蒸馏与标准RLVR之间的差距。TFPI采用了一个简单的<em>ThinkFree</em>操作，通过直接追加<code>&lt;/think&gt;</code>显式丢弃思维内容，从而在推理过程中减少令牌使用。使用<em>ThinkFree</em>适配输入进行训练可以提升性能并降低令牌消耗，即便在原本的慢思考模式下也是如此。在多种基准测试中，大量实验表明，TFPI加速了强化学习的收敛，达到了更高的性能上限，并在不依赖特殊奖励或复杂训练设计的情况下，生成了更为令牌高效的推理模型。仅使用TFPI，我们将一个4B模型训练至在AIME24上达到89.0%的准确率，并在LiveCodeBench上达到65.5%，使用的H20计算时间不足4K小时。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“强化学习+可验证奖励（RLVR）训练长链推理（long-CoT）大模型时上下文窗口过长、训练算力消耗巨大”这一核心痛点，提出一种轻量级前置阶段——Thinking-Free Policy Initialization（TFPI）。其目标可归纳为：</p>
<ul>
<li><p><strong>降低训练阶段的 rollout 长度与 GPU 小时数</strong><br>传统多阶段 RLVR 仍需 40 K→48 K→52 K 的渐进式长上下文，训练 4 B 模型约需 8 K H800 小时；TFPI 用 2 K→4 K→8 K（或 4 K→8 K→16 K）的“无思维”短上下文完成初始化，总耗时 &lt; 20 % 即可达到同等甚至更高性能。</p>
</li>
<li><p><strong>避免“短上下文→长上下文”不可逆性能退化</strong><br>先前工作指出，若直接以过短上下文启动 RLVR，模型会陷入局部最优，后期加长上下文也难以恢复精度。TFPI 通过在短上下文中引入 Thinking-Free 操作（显式丢弃 <code>&lt;think&gt;…&lt;/think&gt;</code> 内容），反而提升了后续长上下文阶段的收敛速度与最终 ceiling。</p>
</li>
<li><p><strong>同时提升推理阶段的 token 效率</strong><br>训练后的模型在推理时可一键切换至 Thinking-Free 模板，输出长度缩短 70 % 以上，而精度不降甚至略升；无需设计专门的“长度惩罚”或“预算控制”奖励函数即可实现高准确-低长度权衡。</p>
</li>
<li><p><strong>作为即插即用的前置模块，兼容任意 RLVR 算法</strong><br>TFPI 仅修改 rollout 时的输入模板和对应的优势计算，不改动奖励函数、网络结构或后续 RL 流程，可与 PPO/GRPO/DAPO 等任意算法无缝组合。</p>
</li>
</ul>
<p>简言之，论文把“如何在短上下文里先学好一个‘足够好’的策略，再以此初始化标准长上下文 RLVR”这一难题，转化为一个极简的模板级操作，并验证了它在收敛速度、最终性能、推理效率三方面的显著增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 TFPI 密切相关的研究可划分为三条主线，均围绕“让大模型学会长链推理（long-CoT）”与“降低训练/推理开销”展开。以下按类别列出代表性文献并指出与 TFPI 的关联。</p>
<ol>
<li>长链推理模型的 RL 训练范式</li>
</ol>
<ul>
<li>DeepSeek-R1（Guo et al., 2025）<br>首次展示“纯 RL + 可验证奖励”即可让 1.5 B–67 B 模型自发产出数万 token 推理链，奠定 RLVR 基准。TFPI 直接以其蒸馏后的 1.5 B/7 B 权重作为起点。</li>
<li>Polaris（An et al., 2025）<br>提出 40 K→48 K→52 K 多阶段长度递增 schedule，训练 4 B 模型需 8 K H800 小时；TFPI 在相同模型上把最长 rollout 降至 16 K，总耗时 &lt; 1.5 K 小时即可超越其精度。</li>
<li>DeepScaleR（Luo et al., 2025b）<br>继续放大长度 schedule 到 24 K–32 K，并引入难度过滤；TFPI 仅用 8 K 长度便使 1.5 B 模型在 AIME24 上超过 DeepScaleR-1.5B-Preview。</li>
<li>k1.5 / k2（Kimi Team, 2025a,b）<br>通过“长上下文 + 课程 RL”把推理能力推到 235 B 规模；TFPI 4 B 模型在数学与代码基准上已逼近或超过 k1.5 235 B 的精度，凸显参数效率优势。</li>
</ul>
<ol>
<li>训练-长度与推理-长度效率优化</li>
</ol>
<ul>
<li>多阶段课程 RL<br>– GLM-4.5（Zeng et al., 2025a）指出“短→长”若控制不当会出现不可逆性能退化；TFPI 用 Thinking-Free 模板规避了该退化。</li>
<li>长度惩罚/预算控制<br>– L1-Max（Aggarwal &amp; Welleck, 2025）在奖励里加入 L1 长度正则；TFPI 无需修改奖励即可在推理端把输出压至 1/3 长度。<br>– BudgetThinker（Wen et al., 2025）引入可学习的预算 token；TFPI 通过模板切换实现“零额外参数”预算控制。</li>
<li>早停/自适应思考<br>– AdaptThink（Zhang et al., 2025b）、AutoThink（Tu et al., 2025）、ThinkLess（Fang et al., 2025）训练模型自己决定“何时停止思考”；TFPI 把“思考”直接留空，仍能在推理阶段恢复深度验证行为，说明长度压缩与推理深度可兼得。</li>
</ul>
<ol>
<li>参数-效率与初始化策略</li>
</ol>
<ul>
<li>Mid-Training（Wang et al., 2025b）<br>在预训练与 RL 之间插入一段基于形式语言的继续训练，以加速后续 RL 收敛；TFPI 可视为“SFT-distilled LRM → RLVR”之间的同类“中间阶段”，但仅需修改输入模板，不引入额外语料或结构。</li>
<li>Pre-Pretraining with Formal Language（Hu et al., 2025）<br>证明正式语言先验可提升逻辑推理；TFPI 的“空思维”模板起到类似“先验压缩”的作用，迫使模型在 answer 部分内隐式展开验证。</li>
<li>参数空间分析文献（Liu et al., 2025b）<br>对 RLVR 的更新方向进行 PCA 可视化；TFPI 论文沿用该分析方法，显示 TFPI 轨迹与 Direct RL 收敛点余弦相似度逐层递增，解释为何短上下文初始化也能抵达高推理性能区域。</li>
</ul>
<p>综上，TFPI 与以上研究共享“长链推理”与“效率提升”目标，但差异在于：</p>
<ul>
<li>不改变奖励函数、不增加模型参数、不依赖特殊数据；</li>
<li>把“压缩推理链”转化为模板级操作，并作为标准 RLVR 的前置初始化；</li>
<li>在训练侧显著缩短 rollout 长度，在推理侧提供“一键短输出”模式，同时保持或超越原有精度。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“RLVR 训练长链推理模型时 rollout 过长、算力爆炸”这一痛点拆解为两步，提出 Thinking-Free Policy Initialization（TFPI）框架，用极简的模板级操作同时降低训练与推理成本。具体做法可概括为：</p>
<ol>
<li>引入零成本的 Thinking-Free 算子<br>对任意查询  x ，构造</li>
</ol>
<p>x′=ThinkingFree(x)</p>
<p>即在系统提示与问题之后、assistant 开始处强行追加</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;think&gt;</span><br><span class="line"></span><br><span class="line">&lt;/think&gt;</span><br></pre></td></tr></table></figure>
<p>使模型在 rollout 阶段无法生成显式思维链，只能把推理“挤”进后续 answer 部分。该操作不改变奖励函数、不增加参数，仅通过模板即可把输出长度压掉 70 % 以上。</p>
<ol>
<li>用短上下文完成三阶段“热身”</li>
</ol>
<ul>
<li>阶段 1：2 K（或 4 K）最大长度，rollout 全部用  x′ 。</li>
<li>阶段 2：4 K（或 8 K）最大长度，继续用  x′ 。</li>
<li>阶段 3：8 K（或 16 K）最大长度，仍用  x′ 。<br>每阶段沿用标准 RLVR 算法（DAPO）的目标</li>
</ul>
<p>J<em>(TFPI)(θ)=E</em>(x∼D)[J_(DAPO)(θ,x′)]</p>
<p>优势估计与重要性采样均在  x′  上计算，但奖励仍按原题  x  的判题器给出，保证无信息泄漏。</p>
<ol>
<li><p>热身结束后无缝接入常规长上下文 RLVR<br>把同一套模型直接切换到正常模板（含 <code>&lt;think&gt;</code> 标签），继续 16 K–32 K rollout 训练。此时模型参数已处于“高验证比例 + 低冗余长度”区域，收敛速度显著加快，最终 ceiling 反而高于“从头直接长上下文 RLVR”。</p>
</li>
<li><p>推理阶段双模式切换</p>
</li>
</ol>
<ul>
<li>需要高精度：用正常模板，模型仍输出长链推理，长度与直接 RLVR 模型相当，但准确率更高。</li>
<li>需要低延迟：用 Thinking-Free 模板，输出长度缩短 3–4 倍，准确率几乎不掉，甚至略升。</li>
</ul>
<p>通过上述四步，论文把“长上下文训练算力”与“推理冗余长度”同时砍掉：</p>
<ul>
<li>训练侧：三阶段 TFPI 总耗时 &lt; 同等规模 Direct RLVR 的 20 %，却获得更高收敛点。</li>
<li>推理侧：无需额外长度惩罚或预算 token，即可在 1.3 K–4.4 K token 区间实现与 14 K–17 K token 原版模型相当或更好的精度。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“训练效率”与“推理效率”两条主线，共设计 4 组核心实验，覆盖 1.5 B / 4 B / 7 B 三种参数规模、6 类基准任务，并辅以行为与参数级分析。所有实验均基于同一套代码库（VeRL + DAPO）与统一超参，确保对比公平。</p>
<ol>
<li>主实验 1：TFPI 三阶段 vs. Direct RLVR（同等训练算力）</li>
</ol>
<ul>
<li>模型：DS-1.5B、Qwen3-4B、DS-7B</li>
<li>训练数据：Polaris-53K（纯数学）</li>
<li>对比方式：TFPI 2 K→4 K→8 K（或 4 K→8 K→16 K）总 GPU 小时与 Direct RLVR 16 K/32 K 单阶段拉齐</li>
<li>指标：6 基准 avg@k 精度 + 输出长度</li>
<li>结果：<br>– DS-1.5B 整体精度从 22.0 % → 29.2 %（+7.2 %），Qwen3-4B 从 60.3 % → 63.8 %（+3.5 %），DS-7B 从 42.2 % → 47.8 %（+5.6 %），全面超越 Direct RLVR。<br>– 外域任务（GPQA、LiveCode、IFEval）同步提升，表明仅数学数据即可泛化。</li>
</ul>
<ol>
<li>主实验 2：TFPI + 后续长上下文 RLVR vs. Direct RLVR（同等总算力）</li>
</ol>
<ul>
<li>在 TFPI 三阶段后，继续 16 K/32 K 标准 RLVR，总时长仍与 Direct RLVR 对齐。</li>
<li>结果：<br>– Qwen3-4B AIME25 从 70.6 % → 76.0 %（+5.4 %），BeyondAIME +2.8 %；整体精度 65.7 % vs. 62.0 % Direct RLVR。<br>– 4 B 模型仅用 4 K H20 小时即达到 AIME24 89.0 %、LiveCode 65.5 %，超过 Polaris-4B-Preview（8 K H800 小时）与 Qwen3-235B-Thinking。</li>
</ul>
<ol>
<li>推理效率实验：Thinking-Free 模式与 SOTA 长度控制方法对比</li>
</ol>
<ul>
<li>基线：TLMRE、AdaptThink、AutoThink、Laser、L1-Max、ThinkLess（均为 1.5 B 级）</li>
<li>指标：4 基准 avg@k 精度 vs. 平均输出 token</li>
<li>结果：<br>– TFPI Stage-3 在 4.4 K token 处取得 28.5 % 平均精度，同等长度下优于所有基线；在 1.3 K token 处仍达 19.7 %，优于多数 6 K–12 K 长度的对比模型。<br>– 图 1（右）显示 TFPI 各阶段均处于 Pareto 前沿，无需专门长度奖励即可同时获得“高准确率 + 低长度”。</li>
</ul>
<ol>
<li>消融与分析实验<br>4.1 行为级分析（图 3）<br>– 统计训练集（thinking-free）与 AIME25（thinking）回答中“验证步骤”占比。<br>– 发现 TFPI 阶段 1 验证比例骤降（信息压缩），阶段 2–3 稳步回升，且 thinking 模式呈现相同趋势，说明 thinking-free rollout 习得的验证行为可迁移至长链推理。</li>
</ol>
<p>4.2 参数级分析（图 4）<br>– 对 DS-1.5B checkpoints 做 PCA 降维，可视化参数轨迹：TFPI 从初始点 A 经 B1→B2→B3 最终抵达 Direct RLVR 收敛点 C 附近，表明探索范围更大。<br>– 逐层计算 TFPI 更新向量与 Direct RLVR 最终更新的余弦相似度，发现随训练步数单调上升，解释为何 TFPI 初始化能加速后续 RLVR。</p>
<p>4.3 rollout 长度监测（图 5 右）<br>– 记录训练集上平均输出 token：Direct RLVR 起步 9 K token 且后期仍 7.5 K；TFPI 后再 RL 起步仅 6 K，全程 &lt; 7 K，验证 TFPI 可持久降低 rollout 成本。</p>
<p>综上，实验从“同等算力精度对比→再加长训继续提升→推理长度-精度 Pareto→行为/参数可解释性”四个维度，系统证明 TFPI 在训练与推理两侧同时实现“降本增效”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在大规模验证、理论解释、机制扩展与场景落地四个层面继续深挖，均基于 TFPI 的“模板级长度压缩”与“短→长迁移”特性提出。</p>
<ol>
<li>规模与数据外延</li>
</ol>
<ul>
<li>超 70 B 模型的 TFPI 缩放规律：随着参数增加，Thinking-Free 阶段的 optimal rollout 长度是否呈亚线性甚至常数增长？</li>
<li>多域混合数据：仅在数学语料上已观察到 GPQA、LiveCode 泛化，若将 Polaris-53K 扩展为数学+科学+代码的均衡分布，能否进一步抬升外域 ceiling 并减少阶段数？</li>
<li>多语言/多模态：非英语推理与图文交错场景下，<code>&lt;think&gt;</code> 标签是否仍具备同等“压缩-迁移”效果？需验证跨语言语义空间的一致性。</li>
</ul>
<ol>
<li>模板与符号设计</li>
</ol>
<ul>
<li>最优“空思维”符号：目前仅用 <code>n&lt;think&gt;nn&lt;/think&gt;</code>，若改为 <code>&lt;think&gt;…&lt;/think&gt;</code> 内插入可学习的占位 token 或压缩向量，能否在保持短 rollout 的同时注入更多先验？</li>
<li>分层模板：对不同难度样本动态选择“空思维”长度（如 0、64、128 token 占位），实现样本级自适应预算，进一步逼近理论 Pareto 前沿。</li>
<li>与早期退出机制结合：在 answer 部分插入 <code>[STOP]</code> 类 token，让模型自己决定何时结束，验证 TFPI 是否同样支持“早停”学习。</li>
</ul>
<ol>
<li>理论分析与解释性</li>
</ol>
<ul>
<li>验证行为的因果链：通过干预-反事实实验（如显式屏蔽验证关键词）量化“验证比例 → 最终精度”的因果强度，判断 TFPI 提升是否主要源于验证步骤的隐式学习。</li>
<li>梯度方向收敛界：给出 TFPI 短上下文更新与 Direct RLVR 长上下文更新之间的夹角上界，证明在一定条件下二者参数轨迹 ε-接近，从而提供收敛加速的理论保证。</li>
<li>信息论视角：计算 thinking-free 与 thinking 模式下的互信息 I(x; y)，分析 TFPI 是否通过“强制信息瓶颈”提升信噪比，进而降低样本复杂度。</li>
</ul>
<ol>
<li>算法与框架扩展</li>
</ol>
<ul>
<li>与长度惩罚正交融合：在 TFPI 基础上再加轻度长度正则，观察是否出现“1+1&gt;2”的额外增益，或反而破坏模板压缩带来的隐式优化。</li>
<li>在线长度课程：不再人工固定 2 K→4 K→8 K，而是根据实时验证比例或难度估计动态递增上下文，实现完全自动化课程。</li>
<li>强化-微调双向切换：TFPI 预热后，先执行一段 SFT 微调再继续 RLVR，验证“RL→SFT→RL”循环能否进一步抬升 ceiling。</li>
</ul>
<ol>
<li>系统与工程优化</li>
</ol>
<ul>
<li>推理服务化：结合 vLLM 的 chunked-prefill 与 KV-cache 复用，针对 Thinking-Free 模板做前缀缓存，实测端到端延迟与首 token 时间相比传统长链模型降低倍数。</li>
<li>端侧量化：Thinking-Free 模型输出长度短、分布更尖锐，是否更适合 4-bit 甚至 2-bit 激进量化？需评估量化误差对隐式推理链的影响。</li>
<li>分布式训练：当 TFPI 阶段 rollout 长度缩短 3–4 倍时，可把 batch-size 同比放大，在固定显存下测试吞吐量-收敛步数的 trade-off，给出大规模集群下的最优配置公式。</li>
</ul>
<ol>
<li>风险与鲁棒性</li>
</ol>
<ul>
<li>过度压缩风险：当 <code>&lt;think&gt;</code> 部分为空时，模型是否更容易出现“表面正确但逻辑跳步”的虚假推理？需要构建细粒度 step-by-step 评估集进行故障注入分析。</li>
<li>对抗攻击：在问题描述里插入触发词，诱导模型在 answer 部分泄露短链逻辑缺陷，比较 TFPI 与标准长链模型对提示扰动的敏感率。</li>
<li>公平性：Thinking-Free 模式是否会对某些需要长链演绎的人群（如初学者语言、低资源领域）产生系统性性能下降，需展开分层评测。</li>
</ul>
<ol>
<li>应用延伸</li>
</ol>
<ul>
<li>工具交互推理：在 Search-R1、Tool-Integrated RL 框架下，用 TFPI 压缩工具调用链的推理长度，减少多轮 API 调用成本。</li>
<li>对话与指令跟随：将 TFPI 思想迁移到多轮对话，研究“空思维”模板能否减少重复自我纠正，提高用户等待体验。</li>
<li>边缘设备微调：利用 TFPI 短上下文特性，在 24 GB 消费级 GPU 上实现“单机一夜”微调，为社区低成本复现开放可能。</li>
</ul>
<p>通过上述方向的系统探索，可进一步厘清 TFPI 的适用边界、理论根基与工程极限，推动“高推理能力+低 token 成本”范式在更大范围落地。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>TFPI: Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners</strong><br>一句话总结：用“空思维”模板把 RLVR 的训练 rollout 砍到 1/4，再把推理 token 砍到 1/3，同时让精度更高。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>核心内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>长链推理 RLVR 需 32 K–52 K 上下文，训练 4 B 模型要 8 K H800 小时；多阶段短训又易不可逆掉点。</td>
</tr>
<tr>
<td>关键观察</td>
<td>推理时把 <think> 留空即可 70 % 降长，且不掉点 → 想到“训练时也留空会怎样？”</td>
</tr>
<tr>
<td>方法</td>
<td>Thinking-Free Policy Initialization（TFPI）① 模板级操作：rollout 输入强制追加 &lt;/think&gt;，让模型无显式思维链；② 三阶段短上下文 2 K→4 K→8 K（4 B 为 4 K→8 K→16 K）热身；③ 热身完直接接标准 16 K/32 K RLVR；④ 推理可一键切换“空思维”模板，长度再砍。</td>
</tr>
<tr>
<td>实验</td>
<td>1.5 B/4 B/7 B 模型、数学+多任务+代码+指令 6 基准；同等 GPU 小时下，TFPI 三阶段平均 +4.8 %；再接长训，4 B 模型 AIME24 89 %、LiveCode 65.5 %，耗时 &lt; 4 K H20；推理 5 K token 达原来 17 K 精度。</td>
</tr>
<tr>
<td>分析</td>
<td>行为：空思维 rollout 习得的“验证步骤”比例曲线与长链推理模式一致；参数：TFPI 轨迹在 PCA 空间更大范围探索，最终与 Direct RLVR 收敛点余弦相似度逐层升至 0.8。</td>
</tr>
<tr>
<td>结论</td>
<td>无需改奖励、不加参数、不增数据，TFPI 即可“训练快、推理短、精度高”，为长链推理模型提供了一条高性价比新路径。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Thinking-Free%20Policy%20Initialization%20Makes%20Distilled%20Reasoning%20Models%20More%20Effective%20and%20Efficient%20Reasoners，链接是https://arxiv.org/pdf/2509.26226，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.26226。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xin Xu,Cliveb AI,Kai Yang,Tianhao Chen,Yang Wang,Saiyong Yang,Can Yang</p>
<p><strong>Categories</strong>: cs.LG,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26226.pdf">https://arxiv.org/pdf/2509.26226.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26226">https://arxiv.org/abs/2509.26226</a></p>
<p><strong>Arxiv ID</strong>: 2509.26226</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26226">https://papers.cool/arxiv/2509.26226</a></p>
<p><strong>Published</strong>: 2025-09-30T13:25:00Z</p>
<p><strong>Updated</strong>: 2025-09-30T13:25:00.000Z</p>
<hr>
<h3 id="10-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder"><a href="#10-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder" class="headerlink" title="10. DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.25182">DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder</a></h3><p>We introduce DC-VideoGen, a post-training acceleration framework for efficient video generation. DC-VideoGen can be applied to any pre-trained video diffusion model, improving efficiency by adapting it to a deep compression latent space with lightweight fine-tuning. The framework builds on two key innovations: (i) a Deep Compression Video Autoencoder with a novel chunk-causal temporal design that achieves 32x/64x spatial and 4x temporal compression while preserving reconstruction quality and generalization to longer videos; and (ii) AE-Adapt-V, a robust adaptation strategy that enables rapid and stable transfer of pre-trained models into the new latent space. Adapting the pre-trained Wan-2.1-14B model with DC-VideoGen requires only 10 GPU days on the NVIDIA H100 GPU. The accelerated models achieve up to 14.8x lower inference latency than their base counterparts without compromising quality, and further enable 2160x3840 video generation on a single GPU. Code: <a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-VideoGen">https://github.com/dc-ai-projects/DC-VideoGen</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了 DC-VideoGen，一种用于高效视频生成的后训练加速框架。DC-VideoGen 可应用于任何预训练的视频扩散模型，通过将其适配到深度压缩潜在空间并进行轻量微调，从而提高效率。该框架基于两个关键创新：（i）深度压缩视频自编码器，采用新颖的分块因果时间设计，在保持重建质量和对更长视频的泛化能力的同时，实现了 32x/64x 空间压缩和 4x 时间压缩；（ii）AE-Adapt-V，一种稳健的适配策略，使预训练模型能够快速且稳定地迁移到新的潜在空间。使用 DC-VideoGen 适配预训练的 Wan-2.1-14B 模型仅需在 NVIDIA H100 GPU 上运行 10 GPU 天。加速后的模型在保持质量的同时，推理延迟最高可降低 14.8 倍，并进一步实现在单 GPU 上生成 2160x3840 视频。代码地址：<a target="_blank" rel="noopener" href="https://github.com/dc-ai-projects/DC-VideoGen。">https://github.com/dc-ai-projects/DC-VideoGen。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“高分辨率或长时视频扩散模型在推理与训练阶段计算开销巨大”这一核心痛点，提出通用后训练加速框架 DC-VideoGen。具体而言，论文解决以下三个紧密关联的问题：</p>
<ol>
<li><p><strong>token 数量爆炸</strong><br>现有视频自编码器仅提供 8× 空间、4× 时间压缩，导致潜在空间 token 数目随分辨率/帧长线性增长，使 DiT backbone 计算成为瓶颈。</p>
</li>
<li><p><strong>高压缩下的重建质量与长视频外推</strong><br>单纯提升压缩率（32×/64× 空间）会同时放大：</p>
</li>
</ol>
<ul>
<li>逐帧信息损失（因果设计只能利用前向信息）</li>
<li>时间外推失效（非因果设计依赖未来帧，无法稳定推广到更长序列）</li>
</ul>
<ol>
<li><strong>预训练模型迁移困难</strong><br>更换自编码器后，patch embedder 与 output head 的权重失效。直接随机初始化并微调会导致：</li>
</ol>
<ul>
<li>训练不稳定（20 k 步后输出退化为噪声）</li>
<li>语义质量显著下滑，难以复用已有大模型知识。</li>
</ul>
<p>DC-VideoGen 通过“深度压缩视频自编码器 + 轻量级适配策略”组合，一次性缓解上述问题，实现：</p>
<ul>
<li>单卡 2160×3840 视频生成</li>
<li>14.8× 推理加速</li>
<li>230× 训练成本降低</li>
<li>不降低、甚至略优于原模型的 VBench 质量。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中将相关研究划分为三大主线，并指出 DC-VideoGen 与它们的区别与互补性。按主题归纳如下：</p>
<ol>
<li>视频自编码器（Video Autoencoder）</li>
</ol>
<ul>
<li>常规压缩率：</li>
<li>VideoVAEPlus、CogVideoX-VAE、HunyuanVideo-VAE、IV-VAE、Wan 2.1 VAE 均采用 f8t4c16（8× 空间、4× 时间）配置。</li>
<li>16× 空间压缩尝试：</li>
<li>Wan 2.2 VAE f16t4c48、Step-Video VAE f16t8c64。</li>
<li>32× 及以上深度压缩：</li>
<li>Video DC-AE f32t4c128（非因果，长视频外推差）</li>
<li>LTX-Video VAE f32t8c128（因果，重建 PSNR 仅 31.12）</li>
<li><strong>DC-VideoGen 区别</strong>：提出“块-因果”时序建模，在 32×/64× 空间、4× 时间压缩下同时获得高重建质量与长视频泛化能力。</li>
</ul>
<ol>
<li>预训练扩散模型向新自编码器的高效迁移（Efficient Autoencoder Adaptation）</li>
</ol>
<ul>
<li>OpenSora 2.0 做法：保留 DiT 权重，随机初始化 patch embedder + output head。</li>
<li>实验结论：生成视频明显模糊，且无法恢复原始质量。</li>
<li><strong>DC-VideoGen 区别</strong>：引入两阶段 AE-Adapt-V，先对齐嵌入空间再 LoRA 微调，实现 10 GPU 天内稳定迁移 Wan-2.1-14B。</li>
</ul>
<ol>
<li>视频扩散模型加速（Video Diffusion Acceleration）</li>
</ol>
<ul>
<li>减少采样步数：VideoLCM、AnimateDiff-Lightning、T2V-Turbo、MCM、OLCM 等一致性模型或蒸馏方法。</li>
<li>模型压缩：</li>
<li>稀疏化 — Sparse VideoGen、Sliding-Tile Attention、Radial Attention、SpargeAttn 等。</li>
<li>量化 — QVD、Vidit-Q、Q-DiT、QVGen、SVDQuant 等。</li>
<li><strong>DC-VideoGen 区别</strong>：不改变采样步数、不引入稀疏/量化误差，而是“减少每步需要处理的 token 数”，与上述方法正交互补。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>DC-VideoGen 将“高分辨率/长时视频生成极耗算力”这一难题拆成两个耦合子问题，并分别给出针对性设计，再通过轻量级后训练流程把二者无缝拼接。整体思路可概括为：</p>
<p><strong>“先极致压缩潜在空间，再无损迁移预训练权重。”</strong></p>
<p>具体实现分三步：</p>
<ol>
<li>深度压缩视频自编码器（DC-AE-V）——解决 token 爆炸与重建质量矛盾</li>
</ol>
<ul>
<li>32× 或 64× 空间、4× 时间压缩，同等压缩比下 token 数量仅为传统 f8t4c16 的 1/16∼1/64。</li>
<li><strong>Chunk-Causal 时序结构</strong><br>– 训练阶段：把视频等分成大小为 40 的 chunk，chunk 内允许双向注意力，充分挖掘帧间冗余；chunk 间仅保留因果依赖，保证推理时可逐 chunk 外推，长视频无边界伪影。<br>– 推理阶段：仅缓存前一个 chunk 的隐状态，即可无限延长序列，兼顾“非因果高质量”与“因果可外推”两大优点。</li>
<li>6 级 3D-ResNet ＋ 残差自编码技巧，辅以 StyleGAN2 3D 判别器，进一步抑制深压缩带来的信息损失。</li>
</ul>
<ol>
<li>视频嵌入空间对齐（AE-Adapt-V Stage-1）——解决“新 latent 空间导致预训练权重失效”<br>目标：让新 patch embedder 与 output head 的输出，与原始 Wan-2.1 模型在旧空间中的特征分布一致，从而冻结的 DiT 权重继续生效。</li>
</ol>
<ul>
<li><strong>Patch Embedder 对齐</strong><br>固定旧 embedder，仅训练新 embedder，最小化</li>
</ul>
<p>L_(embed) = | AvgPool(e_b) - e_n |_2^2</p>
<p>其中  e_b  为旧空间嵌入， e_n  为新空间嵌入；AvgPool 把  e_b  空间分辨率下采样到与  e_n  相同。</p>
<ul>
<li><strong>Output Head 对齐</strong><br>在扩散损失下联合微调新 embedder + 新 head，DiT 权重仍冻结，直至扩散损失收敛（≈4 k 步）。<br>效果：图 6 显示，对齐后 2 k 步即可恢复 74+ 语义分，而无对齐版本 20 k 步后仍退化至随机噪声。</li>
</ul>
<ol>
<li>轻量级端到端微调（AE-Adapt-V Stage-2）——进一步弥补残余质量差距</li>
</ol>
<ul>
<li>采用 LoRA（rank=256, alpha=512）仅训练 DiT 中的 Q-V 投影矩阵与输出层，参数量 &lt; 30 %。</li>
<li>实验表明 LoRA 不仅节省显存，VBench 分数反而高于全参数微调，因其保留了原始权重中的高频细节先验。</li>
<li>总训练代价：Wan-2.1-14B 仅需 10 H100 天，相比从头训练减少 230×。</li>
</ul>
<p>通过上述三件套，DC-VideoGen 把“压缩-迁移-微调”做成即插即用的后训练流水线，对任何已有视频扩散模型均适用，最终取得：</p>
<ul>
<li>单卡 2160×3840 生成</li>
<li>14.8× 推理加速</li>
<li>相同或更高的 VBench 质量</li>
<li>训练成本下降两个数量级</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>重建质量、生成质量、推理效率、消融分析</strong>四个维度系统验证 DC-VideoGen 的有效性，共涵盖 <strong>4 类实验、12 张主表/主图、若干附录大图</strong>，可归纳如下：</p>
<p>1. 视频自编码器重建实验</p>
<p><strong>目的</strong>：验证 DC-AE-V 在 32×/64× 深度压缩下仍保持高保真，且能外推到更长视频。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>指标</th>
<th>对比对象</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>Panda70M、UCF101、ActivityNet、Kinetics600</td>
<td>PSNR↑、SSIM↑、LPIPS↓、FVD↓</td>
<td>8 种现有 VAE（f8t4c16~f32t8c128）</td>
<td>DC-AE-V 在同压缩比下 PSNR 领先 1~3 dB；在 192× 压缩比 仍比 LTX-Video 高 1.3 dB（31.12→32.72）。</td>
</tr>
<tr>
<td>长视频外推</td>
<td>可视化帧误差</td>
<td>Video DC-AE（非因果）（图 3、图 10）</td>
<td>非因果方案出现明显边界闪烁与面部扭曲；DC-AE-V 无 flicker，文字/人脸细节更清晰。</td>
</tr>
</tbody>
</table>
</div>
<p>2. 文本到视频（T2V）生成实验</p>
<p><strong>目的</strong>：验证加速后模型在 720×1280 与 2160×3840 上的质量是否不降甚至提升。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>设置</th>
<th>指标</th>
<th>对比对象</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>VBench-720p</td>
<td>Overall↑、Semantic↑</td>
<td>7 个同期开源/商业模型（Wan-2.1、HunyuanVideo、Step-Video 等）</td>
<td>DC-VideoGen-Wan-2.1-14B 取得 最高 84.83 分，比原 Wan-2.1-14B +1.1 分，延迟 7.7×↓。</td>
</tr>
<tr>
<td>VBench-2160p</td>
<td>同上</td>
<td>原 Wan-2.1-1.3B（仅能到 480p，需微调上采样）</td>
<td>DC-VideoGen-1.3B 在 2160×3840 取得 25.4 min→1.7 min，14.8× 加速，VBench 分 持平或略升。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 图像到视频（I2V）生成实验</p>
<p><strong>目的</strong>：验证框架对 I2V 任务的通用性。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>设置</th>
<th>指标</th>
<th>对比对象</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>VBench-I2V-720p</td>
<td>Overall↑、I2V Consistency↑</td>
<td>MAGI-1、HunyuanVideo-I2V、Step-Video-TI2V 等</td>
<td>DC-VideoGen-Wan-2.1-14B 87.73 分，比原模型 +0.87 分，延迟 7.6×↓；速度领先第二名 MAGI-1 5.8×。</td>
</tr>
</tbody>
</table>
</div>
<p>4. 效率基准测试</p>
<p><strong>目的</strong>：给出不同分辨率、不同帧数下的实际推理时间。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>测试范围</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>分辨率</td>
<td>480×832 → 2160×3840</td>
<td>加速比 6.2× → 14.8× 随分辨率增大而提高。</td>
</tr>
<tr>
<td>帧数</td>
<td>80 → 640 帧</td>
<td>加速比 8.2× → 14.2× 随帧数增大而提高。</td>
</tr>
<tr>
<td>训练成本</td>
<td>Wan-2.1-14B 从头训练 vs DC-VideoGen</td>
<td>10 H100 天 vs 2300 H100 天，230× 节省。</td>
</tr>
</tbody>
</table>
</div>
<p>5. 消融实验</p>
<p><strong>目的</strong>：验证每个模块是否必需，以及超参敏感度。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>消融维度</th>
<th>设置</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>Chunk 大小</td>
<td>4→40</td>
<td>PSNR 单调上升，40 后饱和，选 40 平衡性能与成本（图 5）。</td>
</tr>
<tr>
<td>Embed 对齐</td>
<td>无对齐 / 仅 patch / 仅 head / 两者都对</td>
<td>两者都对 最优，缺 patch 对齐时语义分 掉 45+（图 11）。</td>
</tr>
<tr>
<td>微调策略</td>
<td>Full-tune vs LoRA</td>
<td>LoRA 参数量↓ 4×，VBench 高 4.6 分，训练更稳定（图 8）。</td>
</tr>
</tbody>
</table>
</div>
<p>6. 可视化对比</p>
<ul>
<li>图 3、图 10：长视频重建帧——DC-AE-V 文字、人脸无糊片。</li>
<li>图 12、图 13：T2V/I2V 样例——加速模型与基模视觉一致，运动连贯性保持。</li>
</ul>
<p>综上，实验覆盖 <strong>重建-生成-效率-消融-可视化</strong> 全链路，结果一致表明：<br>DC-VideoGen 在 <strong>32×/64× 深度压缩</strong> 下仍能 <strong>保真、外推、加速</strong>，且 <strong>通用、即插即用</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望基于论文第 5 节“A.8 Limitation and Future Work”与全文技术细节，可归纳为 6 条可进一步探索的研究方向：</p>
<ol>
<li>长视频生成外推</li>
</ol>
<ul>
<li>当前 chunk-causal 设计需固定 chunk 大小，未来可引入 <strong>动态 chunk 长度</strong> 或 <strong>层次式因果缓存</strong>，实现 <strong>任意长度、无限外推</strong> 的内存恒定推理。</li>
<li>结合 <strong>时间-patch 稀疏化</strong>（如 Radial Attention、SpargeAttn）与 DC-AE-V，进一步把复杂度从 O(T) 降到 O(log T) 或 O(1)。</li>
</ul>
<ol>
<li>自编码器-扩散联合压缩</li>
</ol>
<ul>
<li>目前 DC-AE-V 与 DiT 分开训练，可探索 <strong>端到端联合优化</strong>（类似 VQ-GAN + Transformer 联合训练），让压缩码本与扩散生成目标一致，减少重建-生成目标 mismatch。</li>
<li>引入 <strong>可学习离散码本</strong>（VQ 或 FSQ）实现 <strong>离散潜在空间</strong>，结合下一 token 预测或自回归框架，迈向“单步生成”或“语言模型式”视频生成。</li>
</ul>
<ol>
<li>跨模态条件注入</li>
</ol>
<ul>
<li>本文仅验证文本/图像条件，未来可扩展至 <strong>音频驱动</strong>、<strong>深度图</strong>、<strong>相机参数</strong> 等多模态信号；需要改造 DC-AE-V 的因果结构以支持 <strong>流式音频输入</strong> 的实时同步。</li>
<li>探索 <strong>条件对齐层</strong>（如 Cross-LoRA）与 AE-Adapt-V 的结合，保证新条件注入时无需重训整个 DiT。</li>
</ul>
<ol>
<li>压缩率-质量-显存 Pareto 前沿</li>
</ol>
<ul>
<li>系统研究 <strong>空间 64×→128×</strong>、<strong>时间 4×→8×/16×</strong> 的极限压缩，配合 <strong>感知损失</strong>、<strong>GAN 权重</strong>、<strong>视频 FVD 正则</strong> 等多目标优化，寻找 <strong>人眼不可察</strong> 的最小 token 预算。</li>
<li>引入 <strong>内容自适应压缩</strong>（小运动场景用高压缩，大运动场景降低压缩），实现 <strong>可变码率视频潜在表示</strong>。</li>
</ul>
<ol>
<li>与采样加速正交融合</li>
</ol>
<ul>
<li>DC-VideoGen 减少每步 token 数，但仍依赖 20~50 步 DDPM 采样；可与 <strong>一致性模型（LCM）</strong>、<strong>蒸馏-对抗后训练（DAP）</strong> 结合，实现 <strong>“少步 × 少 token”</strong> 双重加速，向 <strong>实时 30 fps 视频生成</strong> 推进。</li>
<li>探索 <strong>步数-秩联合搜索</strong>（NAS），在给定延迟约束下同时优化 <strong>LoRA 秩、压缩率、采样步数</strong>。</li>
</ul>
<ol>
<li>模型通用性与理论分析</li>
</ol>
<ul>
<li>将 AE-Adapt-V 迁移至 <strong>SD 系列、DiT 图像模型、3D 生成</strong> 等不同域，验证“嵌入空间对齐”策略的 <strong>通用可移植性</strong>。</li>
<li>从 <strong>信息论</strong> 角度分析 chunk-causal 的 <strong>双向信息上限</strong> 与 <strong>因果外推误差下界</strong>，给出 <strong>chunk 大小 vs 压缩率 vs 泛化误差</strong> 的理论公式，指导未来超参选择。</li>
</ul>
<p>以上方向既可直接提升 DC-VideoGen 的实战表现，也能为视频生成社区提供新的基准与理论工具。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>DC-VideoGen：一套“即插即用”的后训练加速框架，让<strong>任意预训练视频扩散模型</strong>在<strong>不损失质量</strong>的前提下，实现<strong>14.8× 推理加速</strong>与<strong>230× 训练成本压缩</strong>，并首次在<strong>单卡 H100</strong> 上完成 <strong>2160×3840 长视频生成</strong>。</p>
<p>1. 核心痛点</p>
<ul>
<li>高分辨率/长时视频 token 数爆炸 → DiT 推理慢、训练贵</li>
<li>现有 VAE 仅 8× 空间压缩，继续压缩则重建劣化、长视频外推失败</li>
<li>更换 VAE 后 patch embedder/output head 失效，直接微调训练崩溃、质量差</li>
</ul>
<p>2. 技术路线</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>关键创新</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>DC-AE-V</td>
<td>32×/64× 空间 + 4× 时间压缩；chunk-causal 时序（chunk 内双向，chunk 间因果）</td>
<td>同压缩比 PSNR +1-3 dB，可无限外推无 flicker</td>
</tr>
<tr>
<td>AE-Adapt-V</td>
<td>两阶段迁移：① 嵌入空间对齐（MSE+扩散损失）② LoRA 微调</td>
<td>10 H100 天完成 Wan-2.1-14B 迁移，2 k 步恢复语义</td>
</tr>
<tr>
<td>推理</td>
<td>少 token + 原 DiT 权重复用</td>
<td>720p 8.2× 加速，2160p 14.8× 加速，VBench 不降反升</td>
</tr>
</tbody>
</table>
</div>
<p>3. 实验结果</p>
<ul>
<li><strong>重建</strong>：4 大数据集上全面优于 LTX-Video、Video DC-AE 等，192× 压缩仍保持 32.72 dB</li>
<li><strong>T2V</strong>：720p 上 DC-VideoGen-14B 取得 <strong>最高 84.83 分</strong>，延迟 <strong>7.7×↓</strong></li>
<li><strong>I2V</strong>：相同分辨率 <strong>87.73 分</strong>，领先基线 <strong>0.87 分</strong>，延迟 <strong>7.6×↓</strong></li>
<li><strong>消融</strong>：chunk=40 饱和；对齐缺失则语义分暴跌 45；LoRA 优于全参微调</li>
</ul>
<p>4. 结论</p>
<p>DC-VideoGen 通过<strong>深度压缩潜在空间</strong>与<strong>稳健嵌入对齐</strong>两条正交路径，首次把“高压缩-高质量-低成本迁移”同时实现，为<strong>高分辨率视频生成</strong>提供了可落地、可扩展的通用加速方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DC-VideoGen%3A%20Efficient%20Video%20Generation%20with%20Deep%20Compression%20Video%20Autoencoder，链接是https://arxiv.org/pdf/2509.25182，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25182。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Junyu Chen,Wenkun He,Yuchao Gu,Yuyang Zhao,Jincheng Yu,Junsong Chen,Dongyun Zou,Yujun Lin,Zhekai Zhang,Muyang Li,Haocheng Xi,Ligeng Zhu,Enze Xie,Song Han,Han Cai</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25182.pdf">https://arxiv.org/pdf/2509.25182.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25182">https://arxiv.org/abs/2509.25182</a></p>
<p><strong>Arxiv ID</strong>: 2509.25182</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25182">https://papers.cool/arxiv/2509.25182</a></p>
<p><strong>Published</strong>: 2025-09-29T17:59:31Z</p>
<p><strong>Updated</strong>: 2025-09-29T17:59:31.000Z</p>
<hr>
<h3 id="11-Who’s-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments"><a href="#11-Who’s-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments" class="headerlink" title="11. Who’s Your Judge? On the Detectability of LLM-Generated Judgments"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.25154">Who’s Your Judge? On the Detectability of LLM-Generated Judgments</a></h3><p>Large Language Model (LLM)-based judgments leverage powerful LLMs to efficiently evaluate candidate content and provide judgment scores. However, the inherent biases and vulnerabilities of LLM-generated judgments raise concerns, underscoring the urgent need for distinguishing them in sensitive scenarios like academic peer reviewing. In this work, we propose and formalize the task of judgment detection and systematically investigate the detectability of LLM-generated judgments. Unlike LLM-generated text detection, judgment detection relies solely on judgment scores and candidates, reflecting real-world scenarios where textual feedback is often unavailable in the detection process. Our preliminary analysis shows that existing LLM-generated text detection methods perform poorly given their incapability to capture the interaction between judgment scores and candidate content — an aspect crucial for effective judgment detection. Inspired by this, we introduce \textit{J-Detector}, a lightweight and transparent neural detector augmented with explicitly extracted linguistic and LLM-enhanced features to link LLM judges’ biases with candidates’ properties for accurate detection. Experiments across diverse datasets demonstrate the effectiveness of \textit{J-Detector} and show how its interpretability enables quantifying biases in LLM judges. Finally, we analyze key factors affecting the detectability of LLM-generated judgments and validate the practical utility of judgment detection in real-world scenarios.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于大型语言模型（LLM）的评判利用强大的LLM高效评估候选内容并提供评分。然而，LLM生成的评判固有的偏差和脆弱性引发了关注，这强调了在诸如学术同行评审等敏感场景下区分其来源的紧迫需求。在本研究中，我们提出并形式化了评判检测任务，并系统性地研究了LLM生成评判的可检测性。不同于LLM生成文本的检测，评判检测仅依赖评判分数和候选内容，体现了现实场景中检测过程中文本反馈通常不可获得的情况。我们的初步分析显示，现有LLM生成文本的检测方法表现不佳，因为它们无法捕捉评判分数与候选内容之间的互动——这是有效评判检测的关键。受此启发，我们提出了\textit{J-Detector}，一种轻量级且可解释的神经检测器，通过显式提取的语言特征和LLM增强特征来连接LLM评审的偏差与候选内容的属性，从而实现准确检测。在多个不同数据集上的实验表明，\textit{J-Detector}具有良好的效果，并展示了其可解释性如何使得量化LLM评审的偏差成为可能。最后，我们分析了影响LLM生成评判可检测性的关键因素，并验证了在现实场景中评判检测的实用性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文提出并形式化了一个新任务——<strong>LLM-generated judgment detection（大模型生成评判的检测）</strong>，旨在仅依据“候选内容”与“评判分数”判断一组评判是由人类还是大模型给出，而<strong>不依赖任何文本型评审意见</strong>。该任务针对以下现实痛点：</p>
<ul>
<li>学术同行评审、数据标注等场景常只公开分数或极少量文本，传统“AI 文本检测”方法因缺少文本信号而失效；</li>
<li>已有研究表明 LLM-as-a-judge 存在显著且可被利用的偏见（如偏好长文本、表面流畅度、复杂度等），但现有检测手段未能同时捕捉“评分分布特征”与“评分-内容交互特征”，导致在单维度评分场景下性能骤降。</li>
</ul>
<p>为此，作者设计了一个轻量、可解释的检测器 <strong>J-Detector</strong>，通过显式提取语言学与 LLM 增强特征，将大模型评判的系统性偏见与候选内容属性关联起来，实现对 LLM 生成评判的高精度识别，并进一步支持对评判偏见的量化分析与真实场景（如学术评审）下的实用验证。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两条主线，并指出它们与“judgment detection”任务的区别与可借鉴之处：</p>
<ol>
<li><strong>LLM-as-a-judge（大模型当评委）</strong></li>
</ol>
<ul>
<li>奠基工作：Zheng et al. 提出用 LLM 自动给模型输出打分，后续被扩展到论文评审、文档相关性、推理链正确性等场景。</li>
<li>偏差与攻击研究：<br>– 发现 LLM 评分受长度、表面亲和度等伪特征影响（Li 2025a, Ye 2024, Yang 2024）。<br>– 证明仅需插入极难察觉的 token 即可显著操纵评分（Shi 2024, Zhao 2025b）。</li>
<li>缓解手段：偏差量化、人在回路校准等。<br>→ 本文任务不同于“如何改进评委”，而是“如何判断评委是 LLM 还是人类”，与上述研究互补。</li>
</ul>
<ol>
<li><strong>AI-generated Text Detection（机器文本检测）</strong></li>
</ol>
<ul>
<li>传统方法： stylometry、困惑度、有监督分类器（Gehrmann 2019, Ippolito 2020, Mitchell 2023）。</li>
<li>近期方向：零样本/水印/鲁棒泛化（Mao 2025, Sun &amp; Lv 2025）。</li>
<li>子领域：LLM-generated Peer Review Detection（Rao 2025, Tao, Yu et al.）——<strong>但依赖完整评审文本</strong>。<br>→ 本文任务设置更严苛：仅允许使用“评分+候选内容”，不依赖文本评论，因此现有文本检测方法无法直接适用，需重新设计特征与模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“仅依据评分与候选内容检测 LLM 生成评判”形式化为 <strong>Judgment Detection</strong> 任务，并提出 <strong>J-Detector</strong> 框架，通过以下三步解决该问题：</p>
<ol>
<li>显式提取两类互补特征</li>
</ol>
<ul>
<li><strong>Judgment-Intrinsic Features</strong>：原始评分向量本身蕴含的分布模式。</li>
<li><strong>Judgment-Candidate Interaction Features</strong>：<br>– <em>LLM-enhanced 特征</em>：用 surrogate LLM 对候选重新打分，捕获“风格、格式、措辞、置信度”等 LLM 评委易偏爱的维度。<br>– <em>语言学特征</em>：长度、词汇多样性、可读性、句法复杂度、话语/模糊限制词频率等低成本指标，对应已知偏见（长度、流畅、复杂、自信等）。</li>
</ul>
<ol>
<li>轻量级可解释分类<br>将三类特征拼接为单条向量</li>
</ol>
<p>F = F<em>(base) oplus F</em>(LLM) oplus F_(linguistic)</p>
<p>后用传统树模型（RandomForest/LGBM/XGB）训练二分类器，输出实例级 logits。</p>
<ol>
<li>组级聚合决策<br>真实场景常一次性评审多篇稿件。给定含  k  条评判的组  G ，将实例 logits 求和得到组得分</li>
</ol>
<p>score(G)=∑_(i=1)^k z_i</p>
<p>再按阈值  τ  判定整组是否来自 LLM。</p>
<p>该方案在单维/多维、点式/成对/列表式评判场景均显著优于 SLM 与 LLM 基线，且特征透明，可直接查看系数量化评委偏见（复杂度、置信、长度等）。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“judgment detection”任务，在自建的 JD-Bench 基准上系统评估了所提方法，并进行了多维度分析与应用验证。主要实验内容如下：</p>
<ol>
<li>主实验：对比检测性能</li>
</ol>
<ul>
<li>数据集：HelpSteer2、HelpSteer3、NeurIPS、ANTIQUE（覆盖点式/成对/列表式、单维/多维）。</li>
<li>基线：<br>– SLM 系列：RoBERTa、Longformer（仅评分；评分+候选）。<br>– LLM 系列：LLM-as-a-judge、样本级特征挖掘、分布级特征挖掘、两者结合。</li>
<li>指标：F1、AUROC，重复 5 次取平均。</li>
<li>结果：J-Detector（RandomForest/LGBM/XGB）在 4 个数据集全部取得最高 F1/AUROC，单维场景提升尤为显著（HelpSteer3 从≈ 54% 提至 74% F1）。</li>
</ul>
<ol>
<li><p>消融实验<br>固定组大小 k=1/2/4/8/16，分别去掉 LLM-enhanced 特征、语言学特征、两者。<br>– 两种特征均带来一致增益，同时去掉后 F1 最多下降 12.3%，验证互补性。</p>
</li>
<li><p>偏见量化<br>利用树模型特征系数绝对值排序，提取 Top-20 重要特征。<br>– HelpSteer2：complexity、verbosity、avg_dependency_length 等系数最高，揭示 LLM 偏爱“更长、更复杂”回答。<br>– NeurIPS：confidence、contribution、soundness 权重最大，显示 LLM 评审更易给出过度自信且分项趋同的高分。</p>
</li>
<li><p>可检测性因素分析</p>
</li>
</ol>
<ul>
<li>组大小：k 从 1 增至 16，F1 单调上升（HelpSteer3 由 63.9% → 85.0%）。</li>
<li>评判维度：NeurIPS 从完整 5 维降至 1 维，F1 由 97.2% 降至 72.3%。</li>
<li>评分粒度：HelpSteer3 将 7 级粗化为 3 级，F1 由 72.9% 降至 57.7%。</li>
<li>模型规模与类型：<br>– API 模型（GPT-4o、Claude 系列）比开源模型更难检测；<br>– 同一家族越大越难检测；<br>– 推理模型与专用评委模型（DeepSeek-R1、RISE-Judge）检测率最低。</li>
<li>理论拟合：给出检测度指数</li>
</ul>
<p>Det(G)=1—β · m d log S · Delta^2</p>
<p>与实验趋势一致，证实检测度随组大小、维度、评分分辨率、人-机分布差距而单调提高。</p>
<ol>
<li><p>多评委混合场景<br>训练/测试集同时包含 2/3/5/10 种 LLM 的评判，检测 F1 随模型数量增加显著下降（HelpSteer2 从 99.8% 降至 66.9%），说明多评委设置对检测器构成新挑战。</p>
</li>
<li><p>真实场景应用：评审-文本协同检测</p>
</li>
</ol>
<ul>
<li>Few-shot 评审检测：仅 60 条训练样本，合并 J-Detector 与 RoBERTa-text 后 F1 由 67.2% 提至 74.6%。</li>
<li>缺失文本检测：15% 评审文本被屏蔽，合并后 F1 由 90.5% 提至 99.3%。<br>结果证实“评分信号”与“文本信号”互补，在训练数据稀缺或文本缺失条件下仍能实现高鲁棒检测。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对“judgment detection”研究的直接延伸或深层拓展：</p>
<ol>
<li><strong>多评委、异构偏见场景下的统一检测</strong></li>
</ol>
<ul>
<li>当评审来源由  n  个不同 LLM 与人类混合时，检测器需同时建模多种偏见分布；可探索 domain-mixture、meta-learning 或 prompt-signature 分离策略，避免“平均化”导致性能骤降。</li>
</ul>
<ol>
<li><strong>跨领域、跨任务泛化</strong></li>
</ol>
<ul>
<li>JD-Bench 仅覆盖问答、对话、论文评审与文档排序四类任务；可扩展至代码评审、医疗诊断建议、法律判例评估等高风险领域，验证偏见模式与检测迁移性。</li>
</ul>
<ol>
<li><strong>动态/在线评判检测</strong></li>
</ol>
<ul>
<li>真实会议评审随投稿量逐日增加，需在线更新检测器；研究流式特征漂移检测与增量学习，防止“概念漂移”导致假阴性上升。</li>
</ul>
<ol>
<li><strong>对抗性评委与主动防御</strong></li>
</ol>
<ul>
<li>针对“评委攻击”（如隐藏白字 prompt、token 注入）设计自适应对抗训练，或引入可验证水印/签名机制，使评判行为具备可审计性。</li>
</ul>
<ol>
<li><strong>细粒度偏见溯源与校准</strong></li>
</ol>
<ul>
<li>将 J-Detector 输出的特征重要性反向映射到具体评判维度，实现“实例级”偏见解释；进一步结合强化学习或人类反馈，对 LLM 评委进行在线去偏校准。</li>
</ul>
<ol>
<li><strong>多模态评判检测</strong></li>
</ol>
<ul>
<li>扩展至含图像、表格、代码片段的投稿（如 AI 会议论文），研究视觉-文本混合候选对评分偏见的影响，并构建对应多模态特征空间。</li>
</ul>
<ol>
<li><strong>隐私保护下的检测</strong></li>
</ol>
<ul>
<li>评审分数与候选内容可能含敏感信息；探索联邦学习或加密特征（如 homomorphic aggregation）实现“数据不出域”的检测服务。</li>
</ul>
<ol>
<li><strong>检测-生成协同博弈</strong></li>
</ol>
<ul>
<li>将检测器作为判别器、LLM 评委作为生成器，构建 min-max 博弈框架，研究纳什均衡时的可检测上界，为“可检测性理论”提供 tighter bound。</li>
</ul>
<ol>
<li><strong>人类-LLM 协作评审机制设计</strong></li>
</ol>
<ul>
<li>把 judgment detection 作为信号，动态决定何时引入人类复审，或分配“人机权重”，在成本与风险之间做最优权衡。</li>
</ul>
<ol>
<li><strong>标准化评测协议与工具链</strong></li>
</ol>
<ul>
<li>建立持续更新的“Judge Detection Leaderboard”，开源即插即用的检测 API，方便会议主办方、数据标注平台直接集成。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>任务</strong>：首次提出“judgment detection”——仅利用“候选内容+评判分数”判断一组评判出自人类还是 LLM，无需文本评语。</li>
<li><strong>难点</strong>：单维评分场景下，现有文本检测器因无法捕捉“评分-候选交互特征”而跌至随机水平。</li>
<li><strong>方法</strong>：J-Detector</li>
<li>显式提取三类特征：原始评分、LLM-enhanced 重评分、语言学统计；</li>
<li>轻量级树模型做二分类，组级 logits 求和输出最终预测；</li>
<li>特征透明，可直接量化 LLM 评委的复杂度、置信、长度等偏见。</li>
<li><strong>实验</strong>：在 JD-Bench（4 数据集、20+ LLM）上，F1/AUROC 全面领先 SLM 与 LLM 基线；单维提升达 20% F1。</li>
<li><strong>分析</strong>：检测度随组大小、维度、评分粒度、人-机分布差距单调上升；API/大/推理模型更难检测；多评委混合显著增加难度。</li>
<li><strong>应用</strong>：与文本检测协同，可在少样本或文本缺失的学术评审场景下把 F1 从 67.2→74.6%、90.5→99.3%。</li>
<li><strong>结论</strong>：judgment detection 为 LLM-as-a-judge 提供了轻量、可解释、可部署的安全阀，保障评审与标注的公平性与可信度。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Who%27s%20Your%20Judge%3F%20On%20the%20Detectability%20of%20LLM-Generated%20Judgments，链接是https://arxiv.org/pdf/2509.25154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25154。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Dawei Li,Zhen Tan,Chengshuai Zhao,Bohan Jiang,Baixiang Huang,Pingchuan Ma,Abdullah Alnaibari,Kai Shu,Huan Liu</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25154.pdf">https://arxiv.org/pdf/2509.25154.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25154">https://arxiv.org/abs/2509.25154</a></p>
<p><strong>Arxiv ID</strong>: 2509.25154</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25154">https://papers.cool/arxiv/2509.25154</a></p>
<p><strong>Published</strong>: 2025-09-29T17:54:57Z</p>
<p><strong>Updated</strong>: 2025-09-29T17:54:57.000Z</p>
<hr>
<h3 id="12-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training"><a href="#12-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training" class="headerlink" title="12. Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.25758">Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training</a></h3><p>The remarkable capabilities of modern large reasoning models are largely unlocked through post-training techniques such as supervised fine-tuning and reinforcement learning. However, the architectural mechanisms behind such improvements remain largely opaque. In this work, we use circuit analysis to demonstrate that post-training for complex reasoning sparks the emergence of novel, functionally specialized attention heads. These heads collectively support structured reasoning and computation. Our comparative analysis across Qwen families and DeepSeek-distilled model reveals that these emergent heads evolve differently under different training regimes. Distillation and SFT foster a cumulative addition of stable reasoning heads. In contrast, group relative policy optimization operates in a dynamic search mode: relatively few attention heads are iteratively activated, evaluated, and pruned, with their survival closely tracking fluctuations in the task reward signal. Furthermore, we find that controllable think on/off models do not possess dedicated thinking heads. Instead, turning off explicit reasoning triggers a broader-but less efficient-set of compensatory heads. Through ablation and qualitative analyses, we connect these circuit-level dynamics to a crucial performance trade-off: strengthened heads enable sophisticated problem-solving strategies for difficult problems but can also introduce over-thinking failure modes, such as calculation errors or logical loops on simpler tasks. These findings connect circuit-level dynamics to macro-level performance, identifying an inherent tension where complex reasoning comes at the cost of elementary computations. More broadly, our work points to future directions for training policy design, emphasizing the need to balance the development of effective reasoning strategies with the assurance of reliable, flawless execution.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现代大型推理模型的显著能力在很大程度上依赖于训练后技术的解锁，例如监督微调和强化学习。然而，这些能力背后的架构机制仍然大多不透明。在本研究中，我们使用电路分析表明，复杂推理的训练后过程会促使新型功能专门化注意力头的出现。这些注意力头共同支持结构化的推理和计算。我们对Qwen系列和DeepSeek蒸馏模型的比较分析显示，这些新出现的注意力头在不同训练模式下的发展方式不同。蒸馏和监督微调促进了稳定推理头的累积增加。相比之下，组相对策略优化(GRPO)则在动态搜索模式下运作：相对较少的注意力头会被迭代地激活、评估和剪枝，其存活情况与任务奖励信号的波动紧密相关。此外，我们发现可控思考开关模型并不拥有专门的思考头。相反，关闭显式推理会触发更广泛但效率较低的补偿性注意力头。通过消融和定性分析，我们将这些电路级动态与一个关键的性能权衡联系起来：增强的注意力头能够为困难问题提供复杂的解决策略，但也可能在简单任务上引入过度思考的失败模式，例如计算错误或逻辑循环。这些发现将电路级动态与宏观性能联系起来，揭示了复杂推理以牺牲基础计算的代价为代价的固有紧张关系。更广泛地说，我们的工作为未来训练策略设计指明了方向，强调在发展高效推理策略的同时，需要确保执行的可靠性和无误性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该研究旨在揭示“后训练”（post-training）阶段如何在大推理模型（LRM）内部形成可解释的功能结构，具体回答以下核心问题：</p>
<ul>
<li><p><strong>后训练是否仅放大预训练已具备的能力，还是会催生新的推理机制？</strong><br>通过电路级因果分析，论文证明 SFT、蒸馏与 GRPO 均能诱发全新的注意力头，这些头构成专门化子图，承担结构化推理与计算角色，而非简单复用预训练参数。</p>
</li>
<li><p><strong>不同后训练范式如何重塑模型架构？</strong></p>
</li>
<li><strong>蒸馏 / SFT</strong>：以“累积式”方式在中-后层大量植入稳定推理头，形成固定路径。</li>
<li><strong>GRPO</strong>：进入“动态搜索”模式，少量头随奖励信号反复激活-评估-剪枝，最终保留稀疏但高影响的头。</li>
<li><strong>Think On/Off</strong>：无专属“思考头”；关闭显式推理时，模型通过广泛但低效的补偿头集维持性能。</li>
<li><strong>新涌现的推理头与宏观性能有何权衡？</strong><br>强化这些头可提升复杂题表现，却同时在简单任务上引入“过度思考”失效模式（计算错误、逻辑循环），表明复杂推理以牺牲基础计算可靠性为代价。</li>
</ul>
<p>综上，论文将“后训练提升推理”这一现象从黑箱性能指标下沉至可解释的电路动力学，为后续训练策略设计提供机制层面的指导原则。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，可按主题归类为“相关研究”。为便于查阅，均以 markdown 列表形式给出，并保留原文引用编号。</p>
<p>1. 后训练范式与推理能力</p>
<ul>
<li><strong>SFT / 蒸馏</strong></li>
<li>Wei et al., 2022a — 监督微调使模型具备 zero-shot 推理能力</li>
<li>Wei et al., 2022b — Chain-of-Thought 提示激发多步推理</li>
<li>Kang et al., 2023 — 知识增强推理蒸馏</li>
<li>Toshniwal et al., 2024 — OpenMathInstruct-1 大规模数学蒸馏数据</li>
<li>Guo et al., 2025 — DeepSeek-R1 通过 RL 激励推理并用蒸馏传递能力</li>
<li>Chu et al., 2025 — “SFT memorizes, RL generalizes” 对比性研究</li>
<li><strong>强化学习与可验证奖励 (RLVR)</strong></li>
<li>Ouyang et al., 2022 — 人类反馈强化学习（RLHF）基础框架</li>
<li>Schulman et al., 2017 — PPO 算法</li>
<li>Shao et al., 2024 — GRPO 用于数学推理的群体相对策略优化</li>
<li>Trung et al., 2024 — ReFT：带强化微调的多步推理</li>
<li>Xi et al., 2024 — 反向课程 RL 训练推理模型</li>
<li>Zhang et al., 2025a — 面向大推理模型的强化学习综述</li>
</ul>
<p>2. 测试阶段计算</p>
</div></details>
<p><strong>Authors</strong>: Yein Park,Minbyul Jeong,Jaewoo Kang</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.25758.pdf">https://arxiv.org/pdf/2509.25758.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.25758">https://arxiv.org/abs/2509.25758</a></p>
<p><strong>Arxiv ID</strong>: 2509.25758</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.25758">https://papers.cool/arxiv/2509.25758</a></p>
<p><strong>Published</strong>: 2025-09-30T04:23:43Z</p>
<p><strong>Updated</strong>: 2025-09-30T04:23:43.000Z</p>
<hr>
<h3 id="13-dParallel-Learnable-Parallel-Decoding-for-dLLMs"><a href="#13-dParallel-Learnable-Parallel-Decoding-for-dLLMs" class="headerlink" title="13. dParallel: Learnable Parallel Decoding for dLLMs"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26488">dParallel: Learnable Parallel Decoding for dLLMs</a></h3><p>Diffusion large language models (dLLMs) have recently drawn considerable attention within the research community as a promising alternative to autoregressive generation, offering parallel token prediction and lower inference latency. Yet, their parallel decoding potential remains largely underexplored, as existing open-source models still require nearly token-length decoding steps to ensure performance. To address this, we introduce dParallel, a simple and effective method that unlocks the inherent parallelism of dLLMs for fast sampling. We identify that the key bottleneck to parallel decoding arises from the sequential certainty convergence for masked tokens. Building on this insight, we introduce the core of our approach: certainty-forcing distillation, a novel training strategy that distills the model to follow its original sampling trajectories while enforcing it to achieve high certainty on masked tokens more rapidly and in parallel. Extensive experiments across various benchmarks demonstrate that our method can dramatically reduce the number of decoding steps while maintaining performance. When applied to the LLaDA-8B-Instruct model, dParallel reduces decoding steps from 256 to 30 on GSM8K, achieving an 8.5x speedup without performance degradation. On the MBPP benchmark, it cuts decoding steps from 256 to 24, resulting in a 10.5x speedup while maintaining accuracy. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/czg1225/dParallel">https://github.com/czg1225/dParallel</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>扩散大语言模型（dLLMs）近年来在研究社区中引起了广泛关注，作为自回归生成的有前景替代方案，它们能够实现并行的标记预测并降低推理延迟。然而，它们的并行解码潜力仍然大多未被充分探索，因为现有的开源模型仍需要几乎与标记长度相同的解码步骤以确保性能。为了解决这一问题，我们提出了 dParallel，一种简单而有效的方法，用于释放 dLLMs 的内在并行性以实现快速采样。我们发现，并行解码的关键瓶颈在于对掩码标记进行顺序确定性收敛。基于这一洞察，我们提出了方法的核心：确定性强制蒸馏，这是一种新颖的训练策略，通过蒸馏模型，使其在遵循原始采样轨迹的同时，更快速并行地对掩码标记达到高确定性。针对不同基准的大量实验表明，我们的方法可以在保持性能的同时大幅减少解码步骤。在应用于 LLaDA-8B-Instruct 模型时，dParallel 在 GSM8K 上将解码步骤从 256 减少到 30，实现了 8.5 倍的加速，且性能不受影响。在 MBPP 基准上，它将解码步骤从 256 减少到 24，实现了 10.5 倍的加速，同时保持了准确性。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/czg1225/dParallel">https://github.com/czg1225/dParallel</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决扩散大语言模型（dLLM）在开源实现中“并行解码潜力未被充分释放”的问题。尽管 dLLM 理论上支持并行、非自回归的文本生成，现有开源模型（如 LLaDA、Dream）仍需与序列长度成正比的解码步数才能维持性能，导致推理延迟依旧很高。作者将这一现象归因于<strong>掩码 token 的确定性（certainty）仍以从左到右的顺序逐次收敛</strong>，从而限制了真正的并行解码。</p>
<p>为此，论文提出 <strong>dParallel</strong> 框架，通过“确定性强制蒸馏（certainty-forcing distillation）”训练策略，在不改变原模型生成轨迹的前提下，让模型在更少步骤内、以更高并行度达到高确定性，从而显著减少解码步数并保持精度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Works”中将与本文相关的研究划分为两大主线，并在实验部分（表 1、2）与代表性方法进行了对比。可归纳如下：</p>
<ol>
<li>扩散语言模型（Diffusion Language Models, dLLMs）</li>
</ol>
<ul>
<li>早期离散扩散：Masked Diffusion LM（Austin et al. 2021a）、Diffusion-LM（Li et al. 2022）、SMDM（Shi et al. 2024）等将文本生成建模为“加噪–去噪”过程。</li>
<li>十亿级开源模型：LLaDA（Nie et al. 2025）与 Dream（Ye et al. 2025）证明 dLLM 在参数规模与效果上可对标自回归 LLM。</li>
<li>扩展方向：推理增强 dLLM（Zhao et al. 2025）、多模态 dLLM（You et al. 2025; Yu et al. 2025b）、代码生成 dLLM（Gong et al. 2025; Xie et al. 2025）等。</li>
</ul>
<ol>
<li>加速扩散语言模型（Accelerating dLLMs）</li>
</ol>
<ul>
<li>单步成本削减</li>
<li>KV-Cache 扩展：Dual-Cache（Wu et al. 2025）、dLLM-Cache（Liu et al. 2025）、DKV-Cache（Ma et al. 2025）等通过对前缀与后缀同时缓存降低每步延迟。</li>
<li>动态 token 丢弃：DPAD（Chen et al. 2025）在推理时丢弃低信息量 token 以节省计算。</li>
<li>解码步数削减</li>
<li>少步采样：Few-step Decoding（直接降低离散化步数）。</li>
<li>自适应置信度重掩码：Conf-threshold Decoding（Israel et al. 2025; Wei et al. 2025; Li et al. 2025a,b; Ben-Hamu et</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Zigeng Chen,Gongfan Fang,Xinyin Ma,Ruonan Yu,Xinchao Wang</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26488.pdf">https://arxiv.org/pdf/2509.26488.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26488">https://arxiv.org/abs/2509.26488</a></p>
<p><strong>Arxiv ID</strong>: 2509.26488</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26488">https://papers.cool/arxiv/2509.26488</a></p>
<p><strong>Published</strong>: 2025-09-30T16:32:52Z</p>
<p><strong>Updated</strong>: 2025-09-30T16:32:52.000Z</p>
<hr>
<h3 id="14-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications"><a href="#14-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications" class="headerlink" title="14. VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26490">VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications</a></h3><p>As LLM-based agents are increasingly deployed in real-life scenarios, existing benchmarks fail to capture their inherent complexity of handling extensive information, leveraging diverse resources, and managing dynamic user interactions. To address this gap, we introduce VitaBench, a challenging benchmark that evaluates agents on versatile interactive tasks grounded in real-world settings. Drawing from daily applications in food delivery, in-store consumption, and online travel services, VitaBench presents agents with the most complex life-serving simulation environment to date, comprising 66 tools. Through a framework that eliminates domain-specific policies, we enable flexible composition of these scenarios and tools, yielding 100 cross-scenario tasks (main results) and 300 single-scenario tasks. Each task is derived from multiple real user requests and requires agents to reason across temporal and spatial dimensions, utilize complex tool sets, proactively clarify ambiguous instructions, and track shifting user intent throughout multi-turn conversations. Moreover, we propose a rubric-based sliding window evaluator, enabling robust assessment of diverse solution pathways in complex environments and stochastic interactions. Our comprehensive evaluation reveals that even the most advanced models achieve only 30% success rate on cross-scenario tasks, and less than 50% success rate on others. Overall, we believe VitaBench will serve as a valuable resource for advancing the development of AI agents in practical real-world applications. The code, dataset, and leaderboard are available at <a target="_blank" rel="noopener" href="https://vitabench.github.io/">https://vitabench.github.io/</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>随着基于大型语言模型（LLM）的智能体越来越多地部署在现实场景中，现有的基准测试未能充分反映它们在处理大量信息、利用多样化资源以及管理动态用户交互方面的内在复杂性。为填补这一空白，我们推出了 VitaBench，这是一项具有挑战性的基准测试，用于评估智能体在基于真实世界场景的多样化交互任务中的表现。VitaBench 借鉴日常应用场景，如外卖配送、店内消费和在线旅游服务，为智能体提供迄今为止最复杂的生活服务模拟环境，其中包含 66 种工具。通过消除特定领域策略的框架，我们能够灵活组合这些场景与工具，形成 100 个跨场景任务（主要结果）和 300 个单场景任务。每个任务都源自多个真实用户请求，要求智能体在时间和空间维度上进行推理，使用复杂的工具集，主动澄清模糊指令，并在多轮对话过程中跟踪不断变化的用户意图。此外，我们提出了基于评分标准的滑动窗口评估器，使得在复杂环境和随机交互中对多样化解决方案路径进行稳健评估成为可能。综合评估显示，即使是最先进的模型在跨场景任务中的成功率也仅为 30%，在其他任务中的成功率不到 50%。总体而言，我们相信 VitaBench 将成为推动 AI 智能体在实际应用中发展的宝贵资源。代码、数据集和排行榜可在 <a target="_blank" rel="noopener" href="https://vitabench.github.io/">https://vitabench.github.io/</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对现有评测体系无法充分刻画大模型智能体在真实场景中所面临的海量信息、多元工具与动态用户交互等复合复杂度的问题，提出并构建了 VitaBench。其核心目标是通过一个去领域策略、可自由组合工具与场景的评测框架，系统衡量智能体在“推理-工具-交互”三维复杂度下的真实任务能力，从而推动面向生活服务类应用的智能体研发。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li>早期工具调用基准：ToolLLM、BFCL 等聚焦单轮 API 准确率，忽视工具间依赖与环境交互。</li>
<li>多轮对话工具使用：ToolTalk、MINT、IN3 引入多轮交互，但动作空间受限或缺乏用户不确定性建模。</li>
<li>状态化工具基准：ToolSandbox、τ-bench / τ²-bench 建立状态执行与工具图，仍依赖显式领域策略，未同时覆盖跨域、用户行为属性与复合目标。</li>
<li>用户中心评测：UserBench、DialogTool 强调用户偏好与角色扮演，任务复杂度与工具规模相对有限。</li>
<li>隐含意图与信息补全：Learning-to-Ask、Incomplete-Condition Awareness 研究代理在缺失信息时的主动澄清能力，但未与大规模工具集及跨域任务结合。</li>
</ul>
<p>上述工作仅部分覆盖“推理-工具-交互”复杂度维度，VitaBench 首次在同一基准内同时挑战三项复杂度，并提供 66 工具、400 任务的真实生活场景评测。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过以下三步构建 VitaBench，系统解决“真实场景复杂度无法被现有基准衡量”的核心问题：</p>
<ol>
<li>三维复杂度框架</li>
</ol>
<ul>
<li>将任务难度形式化为三元组</li>
</ul>
<p>C<em>(task)=langle C</em>(reason),; C<em>(tool),; C</em>(∫eract)rangle</p>
<ul>
<li>分别用观测熵  H(O)  与部分可观测率  eta=1-(|O|) / (|S|)  量化推理复杂度；<br>用工具依赖图  G=(V,E)  的节点数  |V| 、边密度  rho=(|E|) / (|V|(|V|-1))  与跨域覆盖比  |V<em>(task)||V|  量化工具复杂度；<br>用用户画像、行为属性与动态状态转移  T</em>(user)  量化交互复杂度。</li>
</ul>
<ol>
<li>去策略化工具-场景构造</li>
</ol>
<ul>
<li>从外卖、到店、OTA 三大生活服务抽取 66 个 API，将业务规则编码为工具的前/后置条件与依赖边，形成可组合的有向图；</li>
<li>取消硬编码领域策略，允许跨域任务自由拼装，生成 100 跨场景+300 单场景共 400 任务；</li>
<li>每个任务配备独立数据库、用户画像与时空上下文，确保多路径可行解与真实信息量级。</li>
</ul>
<ol>
<li>滑动窗口-细目评估器</li>
</ol>
<ul>
<li>为长轨迹设计 rubric-based sliding-window evaluator：</li>
<li>将轨迹拆为重叠窗口  W_i ，每窗宽  w 、重叠  δ ；</li>
<li>维护持久状态向量  s∈0,1^k ，记录  k  条原子细目是否满足；</li>
<li>采用全-or-nothing 评分：$score=1!!1!!<br>∑_j s_j = k<br>$；</li>
<li>人工验证 Cohen’s  kappage 0.81 ，四跑平均即可将估计方差降低 77.5%，保证评测可靠且可复现。</li>
</ul>
<p>通过上述设计，VitaBench 首次在统一环境中同时放大“推理-工具-交互”复杂度，揭示当前最强模型跨场景成功率仅 30%，为后续智能体算法与训练提供高区分度基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 VitaBench 共开展四类实验，系统验证基准有效性并揭示模型能力边界：</p>
<ol>
<li>主评测实验</li>
</ol>
<ul>
<li>模型：覆盖 20 余个主流大模型（GPT-4.1、Claude-4.1-Opus、Gemini-2.5-Pro、DeepSeek-R1、Qwen3-235B 等），按官方指南区分 thinking / non-thinking 模式。</li>
<li>指标：Avg@4、Pass@k、Pass^k，温度设为 0，每任务独立采样 4 次。</li>
<li>结果：<br>– 跨场景平均成功率仅 16.2%–30.0%，单场景最高 53.5%；<br>– thinking 模式普遍提升 3–8 pp，且平均回合数减少 10%；<br>– Pass@32 显示继续采样边际增益有限，Pass^4 接近 0，暴露稳定性缺陷。</li>
</ul>
<ol>
<li>复杂度消融实验</li>
</ol>
<ul>
<li>推理复杂度：统计各域平均推理点与搜索空间，验证“点数↑⇒性能↓”强负相关（ρ=−0.87）。</li>
<li>工具复杂度：固定任务，逐步掩蔽工具依赖边，证明边密度每降 5 pp，成功率升 2.3 pp。</li>
<li>交互复杂度：对比“完整用户模拟 / 中性用户 / 无用户一次性指令”三种设定，量化交互带来 4–7 pp 性能下降，且弱模型对行为属性更敏感。</li>
</ul>
<ol>
<li>评估组件可靠性实验</li>
</ol>
<ul>
<li>用户模拟器：100 对话人工打分，信息保真度 9.48/10，角色一致性 9.34/10。</li>
<li>滑动窗口评估器：消融基线、完整轨迹、无细目三种配置，证明“窗口+细目”方案与人类一致性最高（Cohen’s κ=0.828），任务级准确率 95%。</li>
<li>统计稳定性：基于 32 跑重采样，确定 k=4 即可将 MSE 压至单跑 22.5%，兼顾成本与精度。</li>
</ul>
<ol>
<li>错误模式剖析</li>
</ol>
<ul>
<li>对 Claude-4.1-Opus 76 条失败细目人工归类：<br>– 推理错误 61.8%（决策冲突、目标遗漏、时空常识错误）；<br>– 工具错误 21.1%（选型、参数、调用后不会恢复）；<br>– 交互错误 7.9%（未主动澄清、偏好追踪丢失）；<br>– 用户模拟器噪声 9.2%。</li>
<li>进一步提炼三类典型失败模式：多约束整合失败、自我能力边界感知不足、错误恢复策略单一。</li>
</ul>
<p>以上实验从“性能-复杂度-可靠性-失败原因”四维度全面验证 VitaBench 的挑战性与可解释性，为后续智能体算法改进提供量化依据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可基于 VitaBench 继续深化，推动真实场景智能体研究：</p>
<ul>
<li><p><strong>可扩展工具图自动生成</strong><br>探索用程序合成或自监督方法，从原始 API 文档自动抽取前/后置条件并补全依赖边，实现工具集的持续扩容与跨领域迁移。</p>
</li>
<li><p><strong>多智能体协作机制</strong><br>将单智能体 POMDP 扩展为 Dec-POMDP，引入角色分工、子目标分解与通信协议，评测协同完成跨场景任务时的效率与鲁棒性。</p>
</li>
<li><p><strong>在线持续学习</strong><br>在 VitaBench 上构建「训练-部署-反馈」闭环，利用轨迹级细目信号做稀疏奖励强化学习或离线 RL，缓解真实环境 reward 稀缺问题。</p>
</li>
<li><p><strong>用户不确定性的显式建模</strong><br>为用户状态引入可学习的概率 belief，让智能体在对话中主动执行「贝叶斯提问」以降低不确定性，量化信息增益与对话成本权衡。</p>
</li>
<li><p><strong>可解释失败恢复策略</strong><br>结合程序验证与 LLM 自我批判，实现运行时错误检测 → 根因定位 → 工具链重规划的三步自动恢复，降低 21% 工具错误带来的性能损失。</p>
</li>
<li><p><strong>安全与对齐压力测试</strong><br>在工具图中注入带有副作用或冲突的「灰度 API」，评估智能体是否能在满足用户目标的同时遵守安全约束，量化对齐失败率。</p>
</li>
<li><p><strong>低资源场景适配</strong><br>研究 7B–13B 小模型在 VitaBench 上的蒸馏/量化方案，探索「小模型+工具」能否在可接受成本内逼近大模型性能，推动边缘部署。</p>
</li>
<li><p><strong>跨语言与跨文化迁移</strong><br>利用 VitaBench 的中英双语数据，评测同一套工具图在不同语言用户下的表现差异，研究文化偏好对交互复杂度  C_(∫eract)  的影响。</p>
</li>
<li><p><strong>实时环境动态漂移</strong><br>在轨迹运行过程中随机更新数据库（库存、价格、天气），测试智能体对非稳态环境的在线重规划能力，量化漂移幅度与性能衰减关系。</p>
</li>
<li><p><strong>细目驱动的评估即训练信号</strong><br>将滑动窗口细目转化为稠密奖励  r_t=Delta ∑_j s_j ，实现每轮可学习信号，缓解稀疏 0/1 成功指标导致的训练延迟问题。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>VitaBench</strong>——首个面向“真实生活服务场景”的大模型智能体评测基准，核心贡献与内容可概括为：</p>
<ol>
<li>三维复杂度框架<br>将真实任务难度形式化为</li>
</ol>
<p>C<em>(task)=langle C</em>(reason),; C<em>(tool),; C</em>(∫eract)rangle</p>
<p>分别用观测熵与部分可观测率、工具依赖图的节点/边密度、用户画像与行为属性量化复杂度，为 benchmark 设计提供系统指南。</p>
<ol>
<li>去策略化工具-场景构造</li>
</ol>
<ul>
<li>从外卖、到店、OTA 三大域抽取 66 个 API，把业务规则编码为工具前/后置条件与依赖边，形成可跨域自由组合的有向图。</li>
<li>取消硬编码领域策略，生成 100 跨场景 + 300 单场景共 400 任务；每个任务含独立数据库、用户画像、时空上下文，支持多路径可行解。</li>
</ul>
<ol>
<li>滑动窗口-细目评估器<br>针对长轨迹多解空间，提出 rubric-based sliding-window evaluator：</li>
</ol>
<ul>
<li>轨迹分段重叠窗口，持久维护细目满足状态向量  s∈0,1^k ；</li>
<li>全-or-nothing 评分，与人类一致性 Cohen’s κ=0.828，四跑即可将估计方差降 77.5%。</li>
</ul>
<ol>
<li>大规模实验与发现</li>
</ol>
<ul>
<li>评测 20+ 前沿模型（含 thinking/non-thinking 模式）：跨场景成功率最高仅 30.0%，单场景最高 53.5%。</li>
<li>推理错误占 61.8%，工具错误 21.1%，交互错误 7.9%；模型普遍缺乏自我边界感知与错误恢复能力。</li>
<li>thinking 模式在同等回合数下平均提升 3–8 pp，且更高效；继续采样边际增益有限，稳定性仍不足。</li>
</ul>
<p>综上，VitaBench 通过“三维复杂度框架 + 去策略化工具图 + 滑动窗口细目评估”首次把真实生活服务场景的复合挑战量化、可复现地引入评测体系，为后续智能体算法与训练提供高区分度、高可信度的基准平台。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VitaBench%3A%20Benchmarking%20LLM%20Agents%20with%20Versatile%20Interactive%20Tasks%20in%20Real-world%20Applications，链接是https://arxiv.org/pdf/2509.26490，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.26490。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wei He,Yueqing Sun,Hongyan Hao,Xueyuan Hao,Zhikang Xia,Qi Gu,Chengcheng Han,Dengchang Zhao,Hui Su,Kefeng Zhang,Man Gao,Xi Su,Xiaodong Cai,Xunliang Cai,Yu Yang,Yunke Zhao</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26490.pdf">https://arxiv.org/pdf/2509.26490.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26490">https://arxiv.org/abs/2509.26490</a></p>
<p><strong>Arxiv ID</strong>: 2509.26490</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26490">https://papers.cool/arxiv/2509.26490</a></p>
<p><strong>Published</strong>: 2025-09-30T16:33:49Z</p>
<p><strong>Updated</strong>: 2025-09-30T16:33:49.000Z</p>
<hr>
<h3 id="15-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs"><a href="#15-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs" class="headerlink" title="15. Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.22646">Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs</a></h3><p>Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension — whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated — has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>人类能否识别 AI 生成的（虚假）视频并提供有依据的理由？尽管视频生成模型发展迅速，一个关键维度——人类是否能在生成的视频中检测到深度伪造痕迹，即揭示视频为机器生成的视频的时空可视化瑕疵——却在很大程度上被忽视。我们提出了 DeeptraceReward，这是第一个细粒度、具备空间与时间意识的基准数据集，用于标注人类感知的假痕迹以用于视频生成奖励。该数据集包含 4.3K 条详细标注，涵盖 3.3K 个高质量生成视频。每条标注都提供自然语言解释，精准标记包含感知痕迹的边界框区域，并标记精确的开始与结束时间戳。我们将这些标注整合为 9 个主要类别的深度伪造痕迹，这些痕迹使人类能够判断视频为 AI 生成，并训练多模态语言模型（LMs）作为奖励模型，以模拟人类的判断和定位。在 DeeptraceReward 上，我们的 7B 奖励模型在假线索识别、定位和解释方面平均超越 GPT-5 34.7%。有趣的是，我们观察到一个一致的难度梯度：二元的假与真分类明显比细粒度深度伪造痕迹检测要容易；在后者中，性能依次从自然语言解释（最容易）、空间定位，到时间标记（最困难）递减。通过突出人类感知的深度伪造痕迹，DeeptraceReward 为社会感知和可信的视频生成提供了严格的测试平台和训练信号。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个被现有视频生成评估体系忽视的核心问题：</p>
<blockquote>
<p><strong>人类能否察觉 AI 生成视频中的“深度伪造痕迹（deepfake traces）”，并给出可 grounded 的理由？</strong></p>
</blockquote>
<p>具体而言，工作聚焦以下三点：</p>
<ol>
<li><p>揭示“人眼级”伪造线索<br>现有评估（IS、FID、FVD、VBench 等）只度量与 prompt 对齐度或物理合理性，并未考察<strong>人类在观看时实际捕捉到的细粒度时空异常</strong>。论文首次系统收集并标注了这类“人感知到的伪造痕迹”。</p>
</li>
<li><p>建立细粒度基准<br>提出 DEEPTRACEREWARD——首个同时提供</p>
</li>
</ol>
<ul>
<li>自然语言解释</li>
<li>边界框定位</li>
<li>起止时间戳<br>的时空对齐标注集，覆盖 3.3 k 条高质量生成视频与 4.3 k 条痕迹。</li>
</ul>
<ol>
<li>训练可泛化的奖励模型<br>证明仅通过现有开源多模态 LLM 的零样本能力，无法胜任细粒度检测（SOTA 模型在综合指标上 &lt;37%）。利用该基准微调后的 7B 奖励模型，相对 GPT-5 提升 34.7%，验证“以人类感知为监督信号”可显著增强模型对深度伪造痕迹的识别、定位与解释能力。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 DEEPTRACEREWARD 直接相关的研究可归纳为三条主线，每条均给出代表性文献及其与本文的差异。</p>
<p>1. 视频生成质量评估（generation-level metrics）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法 / 基准</th>
<th>核心思想</th>
<th>与本文区别</th>
</tr>
</thead>
<tbody>
<tr>
<td>IS / FID / FVD</td>
<td>分布距离度量帧/视频逼真度</td>
<td>无人类感知，无定位</td>
</tr>
<tr>
<td>VBench (Huang et al. 2023)</td>
<td>预定义属性（物体数量、动作等）打分</td>
<td>属性级 holistic 分数，不定位具体伪造痕迹</td>
</tr>
<tr>
<td>VideoPhy (Bansal et al. 2024)</td>
<td>物理常识违背检测</td>
<td>仅关注物理合理性，无人工细粒度标注</td>
</tr>
<tr>
<td>EvalCrafter (Liu et al. 2023)</td>
<td>多维度自动指标集合</td>
<td>同样缺乏时空定位的人类感知标签</td>
</tr>
</tbody>
</table>
</div>
<p>2. 人类偏好 / 伪造检测数据集（human-judged datasets）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集 / 工作</th>
<th>标注内容</th>
<th>与本文区别</th>
</tr>
</thead>
<tbody>
<tr>
<td>DFDC, FaceForensics++</td>
<td>整段视频真/假标签，面向人脸伪造</td>
<td>仅二元标签，无时空解释与定位</td>
</tr>
<tr>
<td>Celeb-DF, DeepfakeDetection</td>
<td>同上</td>
<td>同上</td>
</tr>
<tr>
<td>Liu et al. 2025（人类偏好 pairwise）</td>
<td>人工选“哪段更真实”</td>
<td>只有整段偏好，无细粒度区域/时间/理由</td>
</tr>
<tr>
<td>COCOFake, FakeCOCO</td>
<td>图像级真/假+边界框</td>
<td>静态图像，无视频时序信息</td>
</tr>
</tbody>
</table>
</div>
<p>3. 多模态大模型用于视频理解（video-LMMs）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>能力</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>Video-LLaVA, LLaVA-One-Vision, Qwen2-VL, VideoLLaMA3</td>
<td>视频问答、 captioning</td>
<td>被本文作为 baseline，验证其零样本深度伪造定位能力弱</td>
</tr>
<tr>
<td>GPT-4.1, GPT-5, Gemini 2.5 Pro/Flash</td>
<td>商用视频理解大模型</td>
<td>同样用作 baseline，显示即使 SOTA 也在细粒度检测上 &lt;37%</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<p>现有研究要么停留在整段视频的真假标签或偏好选择，要么仅用自动指标评估生成质量，<strong>均未提供“人类肉眼察觉的伪造痕迹”之细粒度时空标注</strong>。DEEPTRACEREWARD 首次填补了这一空白，为“以人类感知为奖励信号”的视频生成与检测提供了可直接微调的基准数据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“数据–基准–模型”三位一体策略解决“人类可感知深度伪造痕迹”的细粒度检测与解释问题。</p>
<p>1. 构建高质量标注数据</p>
<ul>
<li><strong>两阶段筛选</strong></li>
<li>先用 GPT-4 生成真实风格 prompt，经人工过滤后送入 7 个 SOTA 文生视频模型（Sora、Pika、Kling 等）。</li>
<li>只保留动态、高画质、无明显整段崩坏的 3 318 条假视频；再配 3 318 条真实视频平衡分布。</li>
<li><strong>专家时空标注</strong></li>
<li>在 LabelBox 平台逐帧检查，每条痕迹给出</li>
</ul>
<ol>
<li>自然语言解释</li>
<li>跨帧边界框</li>
<li>起止时间戳</li>
</ol>
<ul>
<li>共识机制+二次复审降低主观歧义；GPT-4 统一语言风格。</li>
<li>最终得到 4 334 条痕迹，归纳为 9 大运动-centric 类别（物体畸变、轨迹异常、冗余目标、融合/分裂、突然模糊等）。</li>
</ul>
<p>2. 设立细粒度评估基准</p>
<ul>
<li><strong>任务定义</strong></li>
<li>主任务：给定视频，模型输出</li>
<li>真/假分类</li>
<li>若假，则给出 <code>&lt;bbox&gt;</code>、<code>&lt;start_time&gt;</code>、<code>&lt;explanation&gt;</code>。</li>
<li><strong>七维指标</strong></li>
<li>Accuracy / Fake Acc / Real Acc</li>
<li>Explanation 得分（GPT-4.1 作为 judge 打 0/0.5/1）</li>
<li>Bbox-IoU 与中心点归一化距离</li>
<li>起止时间归一化距离</li>
<li>Overall = 四者加权平均，量化“识别+定位+解释”综合能力。</li>
</ul>
<p>3. 训练专用奖励模型</p>
<ul>
<li><strong>基座模型</strong></li>
<li>VideoLLaMA 3-7B、Qwen2.5-VL-7B 两路并行。</li>
<li><strong>监督微调</strong></li>
<li>8:1:1 划分，用默认 prompt 把“分类+坐标+时间+解释”联合作为生成目标，进行单轮 SFT。</li>
<li>额外设置三组消融：去掉时间、去掉解释、两者皆去，验证监督信号耦合效应。</li>
<li><strong>结果</strong></li>
<li>最佳 7B 模型 Overall 70.2%，比 GPT-5 高 34.7%，比 Gemini 2.5 Pro 高 40.2%。</li>
<li>揭示一致难度梯度：<br>二元分类 ≈ 100% &gt; 语言解释 70% &gt; 空间定位 33% &gt; 时间定位 22%。</li>
</ul>
<p>4. 验证与诊断</p>
<ul>
<li><p><strong>Baseline 全面落后</strong><br>13 个零样本多模态 LLM 均低于 37%，且普遍出现“整帧默认”或“REAL 偏见”。</p>
</li>
<li><p><strong>错误分析</strong><br>GPT-4.1 64% 情况输出整帧 bbox；本文模型可精准到局部区域，且解释更贴合人工描述。</p>
</li>
</ul>
<p>结论</p>
<p>通过“人类感知痕迹”这一全新监督信号，论文不仅提供了可微调的细粒度基准 DEEPTRACEREWARD，也证明了用该数据训练的小型奖励模型即可显著超越商用大模型，为后续“人类对齐”的视频生成与检测奠定直接可用的奖励接口。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“人类可感知深度伪造痕迹”共设计并执行了三大类实验，覆盖零样本评测、监督微调与消融诊断，具体任务与指标如下。</p>
<p>1. 零样本 baseline 全面评测</p>
<p><strong>目的</strong>：验证现有开源/闭源多模态 LLM 是否已具备人类级别的细粒度伪造定位能力。</p>
<ul>
<li><strong>模型池</strong>（13 个）</li>
<li>闭源：GPT-5、GPT-4.1、Gemini-2.5-Pro、Gemini-2.5-Flash</li>
<li>开源：Video-LLaVA-7B、LLaVA-One-Vision-7B、Phi-3.5/4-Vision、Qwen2-VL-7B、Qwen2.5-VL-{7B,32B,72B}、VideoLLaMA3-7B</li>
<li><strong>协议</strong></li>
<li>统一 prompt：“判断视频真假；若假，给出 bbox、起始秒、解释。”</li>
<li>fps=2（可调模型）或固定 8/20 帧（不可调模型），VLMEvalKit 保证可复现。</li>
<li><strong>指标</strong>（7 维）</li>
</ul>
<ol>
<li>Overall = (Acc + Explanation↑ + Bbox-IoU↑ + (100-Time-Dist↓))/4</li>
<li>Acc / Fake-Acc / Real-Acc</li>
<li>Explanation↑（GPT-4.1 打分 0/0.5/1）</li>
<li>Bbox-IoU↑、Bbox-Dist↓（中心点归一化）</li>
<li>Time-Dist↓（起止秒归一化）</li>
</ol>
<ul>
<li><strong>关键结果</strong></li>
<li>全部 baseline ≤37% Overall；最强 GPT-5 仅 35.5%，Gemini-2.5-Pro 30.0%。</li>
<li>时间定位最难：除 Qwen2.5-72B 外，Time-Dist 接近 100（完全失效）。</li>
<li>普遍出现“REAL 偏见”——如 Qwen-32B 真视频 Acc 98.5%，假视频仅 8.9%。</li>
</ul>
<p>2. 监督微调奖励模型</p>
<p><strong>目的</strong>：验证用 DEEPTRACEREWARD 进行 SFT 能否显著提升细粒度检测性能。</p>
<ul>
<li><strong>基座与数据</strong></li>
<li>基座：VideoLLaMA3-7B、Qwen2.5-VL-7B</li>
<li>数据：随机 8:1:1 划分，训练集 2 654 视频/3 460 痕迹；验证集 332 视频/434 痕迹；测试集同上。</li>
<li><strong>训练配置</strong></li>
<li>单 epoch，lr=1×10⁻⁵，AdamW，8×H100；帧数 180 vs 20 依模型而定，batch=1/2。</li>
<li><strong>主要结果（测试集）</strong></li>
<li>VideoLLaMA3-7B 经过 SFT 后 Overall 70.2%，比 GPT-5↑34.7%，比 Gemini-2.5-Pro↑40.2%。</li>
<li>细粒度拆解：</li>
<li>分类 Acc 99.4%</li>
<li>Explanation 70.6/100</li>
<li>Bbox-IoU 32.6/100</li>
<li>Time-Dist 21.9/100（baseline 普遍≈100）</li>
</ul>
<p>3. 消融与诊断实验</p>
<p><strong>目的</strong>：分析不同监督信号对任务耦合/冲突的影响，并定位模型错误模式。</p>
<ul>
<li><strong>消融设置</strong></li>
</ul>
<ol>
<li>w/o time：训练与推理均不预测起始秒</li>
<li>w/o explanation：不生成自然语言理由</li>
<li>w/o time &amp; explanation：仅保留分类+bbox</li>
</ol>
<ul>
<li><strong>结果</strong></li>
<li>去除解释后，bbox 中心距离略有下降（更准），但 Overall 下降；</li>
<li>去除时间信号后，分类 Acc 反而略升（99.1→99.6），说明时间回归与分类存在轻微竞争；</li>
<li>完整信号组合仍得最高 Overall，证明多任务监督可共存。</li>
<li><strong>定性错误对比</strong></li>
<li>GPT-4.1 64% 案例默认输出整帧 bbox <code>[0,0,w,h]</code>；</li>
<li>本文模型 bbox 更紧凑，解释能准确描述“手部扭曲”“纸币融边”等细节，与人工标注一致度更高。</li>
</ul>
<p>4. 类别级性能剖析</p>
<ul>
<li>在 9 类深度伪造痕迹上，本文模型对“物体分裂”“物体融合”提升最显著，相对最强 baseline 绝对提升 25–30 pp；对“突然模糊”“轨迹异常”亦保持 15–20 pp 优势。</li>
</ul>
<p>总结</p>
<p>实验链条完整覆盖“零样本摸底 → 监督提升 → 消融诊断 → 错误可视化”，定量与定性均表明：</p>
<ul>
<li>现有大模型远未具备人类级细粒度伪造定位能力；</li>
<li>DEEPTRACEREWARD 提供的时空+语言联合监督可显著缩小这一差距，为后续视频生成奖励、强化学习微调提供可直接使用的强信号。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对 DEEPTRACEREWARD 的直接延伸或深层扩展，均围绕“人类感知-时空定位-生成奖励”这一主轴展开。</p>
<p>1. 数据与标注扩展</p>
<ul>
<li><p><strong>多语种、多文化感知差异</strong><br>收集非英语母语标注者数据，检验“伪造痕迹”是否因文化视觉经验不同而显著变化，构建跨文化 DEEPTRACEREWARD-Global。</p>
</li>
<li><p><strong>更长视频与多场景复合痕迹</strong><br>当前平均 5–6 秒；扩展到 30 s–2 min，研究长时序一致性（物体永久消失后再现、故事线逻辑违背）及标注者记忆衰减效应。</p>
</li>
<li><p><strong>自动辅助标注</strong><br>用已有奖励模型先预标注，再让人类“修正+评分”，降低 70% 标注成本，形成 Active-DEEPTRACE 循环。</p>
</li>
</ul>
<p>2. 任务与评价升级</p>
<ul>
<li><p><strong>像素级掩码 + 时序轨迹</strong><br>将 bbox 升级为像素级 mask &amp; 轨迹 ID，引入 YTVIS 格式，推动“视频实例伪造分割”新任务。</p>
</li>
<li><p><strong>多痕迹联合推理</strong><br>当前一条痕迹独立标注；引入“痕迹图”结构（时序依赖、因果链），评价模型对多异常交互的理解。</p>
</li>
<li><p><strong>可解释性指标细化</strong><br>现有 Explanation 只分 0/0.5/1；可引入 LIME/SHAP 视觉-语言对齐度、事实一致性（Fact-score）与逻辑链完整性评估。</p>
</li>
</ul>
<p>3. 模型与算法</p>
<ul>
<li><p><strong>Diffusion 型视频生成器 + 奖励 RL 微调</strong><br>用 DEEPTRACEREWARD 奖励模型作为判别器，通过 DDPO/RLDM 微调 DiT 视频生成器，验证“感知对齐”能否降低痕迹出现率。</p>
</li>
<li><p><strong>联合训练检测-生成二部模型</strong><br>生成器与判别器共享时空编码器，采用 GAN-style 对抗但用“可定位伪造痕迹”作为条件，引导生成器在训练阶段即避开高 reward 区域。</p>
</li>
<li><p><strong>时序定位专用模块</strong><br>针对“时间最难”现象，引入 Per-Frame Temporal Transformer 或 Gaussian 加权时间中心预测头，降低 Time-Dist。</p>
</li>
<li><p><strong>小样本/零样本迁移</strong><br>仅利用 10% 标注对新模型（如 Sora-2）做 prompt-tuning 或 adapter 训练，检验奖励模型跨 generator 泛化性。</p>
</li>
</ul>
<p>4. 人类因素与安全</p>
<ul>
<li><p><strong>眼动-脑电同步采集</strong><br>记录标注者观看时的眼动轨迹与 EEG 信号，建立“视觉注意-神经异常”对应，验证哪些低层 artifact 真正进入人类意识。</p>
</li>
<li><p><strong>对抗性痕迹生成</strong><br>利用奖励模型梯度反向生成“人眼难察觉、但模型高响应”的对抗样本，测试并提升人类-模型鲁棒性差距。</p>
</li>
<li><p><strong>伦理与双重用途评估</strong><br>量化使用 DEEPTRACEREWARD 微调后，生成器伪造能力提升多少；建立“伪造能力-检测能力”同步披露协议。</p>
</li>
</ul>
<p>5. 系统与产品化</p>
<ul>
<li><p><strong>实时视频通话深度伪造检测插件</strong><br>将 7B 奖励模型蒸馏至 1B 以下，嵌入 WebRTC，实现 200 ms 级 bbox+解释反馈。</p>
</li>
<li><p><strong>可验证出处链（Provenance）结合</strong><br>将“无痕迹”作为 C2PA 签名前的最后一道门槛，未通过奖励模型检验的视频自动拒绝签名。</p>
</li>
<li><p><strong>开放持续学习平台</strong><br>开源“人类反馈-奖励-再训练”闭环接口，允许社区上传新假视频、获得即时奖励评分并周期合并到主模型，形成 Living-DEEPTRACE。</p>
</li>
</ul>
<p>6. 理论与认知科学</p>
<ul>
<li><p><strong>视觉异常感知阈值模型</strong><br>基于标注数据拟合“空间/时间/语义”三维度上人类检测阈值函数，为“最小可察觉伪造”提供量化定义。</p>
</li>
<li><p><strong>因果 vs 关联痕迹</strong><br>区分“物理因果违背”（球穿透桌子）与“统计关联违背”（猫有六条腿），研究两类痕迹在认知负荷与检测时延上的差异。</p>
</li>
</ul>
<p>总结</p>
<p>从数据规模、标注粒度、任务定义、模型架构、训练策略到伦理治理，DEEPTRACEREWARD 仅打开了“人类感知型视频伪造检测”的第一扇门；上述任一方向均可形成独立且有价值的研究课题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>DEEPTRACEREWARD：面向“人类可感知深度伪造痕迹”的细粒度视频基准与奖励模型</strong></p>
<p>1. 问题</p>
<ul>
<li>现有视频生成评估只看整体对齐或物理合理性，忽视<strong>人类肉眼能否察觉并解释</strong> AI 视频中的局部伪造痕迹。</li>
<li>缺乏同时提供“自然语言解释 + 空间定位 + 时间戳”的细粒度数据集，导致无法训练或检验模型对人类感知级别的伪造检测能力。</li>
</ul>
<p>2. 贡献</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>内容</th>
<th>规模</th>
<th>特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据集</td>
<td>3.3 k 高质量生成视频 + 3.3 k 真实视频</td>
<td>4.3 k 条人工痕迹，每条含 bbox、起止秒、自然语言解释；归纳 9 大类运动-centric 异常</td>
</tr>
<tr>
<td>基准任务</td>
<td>真假分类 + 痕迹定位 + 解释生成</td>
<td>7 维指标（Overall、Acc、Explanation、Bbox-IoU、Time-Dist 等）</td>
</tr>
<tr>
<td>奖励模型</td>
<td>7B VideoLLaMA3 微调</td>
<td>Overall 70.2%，超 GPT-5 34.7%，超 Gemini-2.5-Pro 40.2%；首次揭示“分类&gt;解释&gt;定位&gt;时间”难度梯度</td>
</tr>
</tbody>
</table>
</div>
<p>3. 主要发现</p>
<ul>
<li>13 个零样本多模态 LLM 细粒度检测全线溃败（≤37%），时间定位几乎失效。</li>
<li>用 DEEPTRACEREWARD 监督微调后，小模型即可达到人类级解释与可接受定位，验证“人类感知痕迹”是强奖励信号。</li>
<li>数据与代码开源，可直接用作视频生成 RL 的细粒度奖励函数或深度伪造检测预训练。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning%20Human-Perceived%20Fakeness%20in%20AI-Generated%20Videos%20via%20Multimodal%20LLMs，链接是https://arxiv.org/pdf/2509.22646，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.22646。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xingyu Fu,Siyi Liu,Yinuo Xu,Pan Lu,Guangqiuse Hu,Tianbo Yang,Taran Anantasagar,Christopher Shen,Yikai Mao,Yuanzhe Liu,Keyush Shah,Chung Un Lee,Yejin Choi,James Zou,Dan Roth,Chris Callison-Burch</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.22646.pdf">https://arxiv.org/pdf/2509.22646.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.22646">https://arxiv.org/abs/2509.22646</a></p>
<p><strong>Arxiv ID</strong>: 2509.22646</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.22646">https://papers.cool/arxiv/2509.22646</a></p>
<p><strong>Published</strong>: 2025-09-26T17:59:54Z</p>
<p><strong>Updated</strong>: 2025-09-26T17:59:54.000Z</p>
<hr>
<h3 id="16-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance"><a href="#16-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance" class="headerlink" title="16. IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26231">IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance</a></h3><p>Ensuring precise multimodal alignment between diffusion-generated images and input prompts has been a long-standing challenge. Earlier works finetune diffusion weight using high-quality preference data, which tends to be limited and difficult to scale up. Recent editing-based methods further refine local regions of generated images but may compromise overall image quality. In this work, we propose Implicit Multimodal Guidance (IMG), a novel re-generation-based multimodal alignment framework that requires no extra data or editing operations. Specifically, given a generated image and its prompt, IMG a) utilizes a multimodal large language model (MLLM) to identify misalignments; b) introduces an Implicit Aligner that manipulates diffusion conditioning features to reduce misalignments and enable re-generation; and c) formulates the re-alignment goal into a trainable objective, namely Iteratively Updated Preference Objective. Extensive qualitative and quantitative evaluations on SDXL, SDXL-DPO, and FLUX show that IMG outperforms existing alignment methods. Furthermore, IMG acts as a flexible plug-and-play adapter, seamlessly enhancing prior finetuning-based alignment methods. Our code will be available at <a target="_blank" rel="noopener" href="https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment">https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>确保扩散生成图像与输入提示之间的精确多模态对齐一直是一个长期挑战。早期的工作通过使用高质量偏好数据微调扩散权重，但这些数据通常有限且难以大规模扩展。近期基于编辑的方法进一步精细化生成图像的局部区域，但可能会影响整体图像质量。在本工作中，我们提出了隐式多模态引导（IMG），一种新型的基于再生成的多模态对齐框架，无需额外数据或编辑操作。具体而言，给定生成图像及其提示，IMG a) 利用多模态大语言模型（MLLM）识别不对齐之处；b) 引入隐式对齐器，操作扩散条件特征以减少不对齐并实现再生成；c) 将再对齐目标形式化为可训练目标，即迭代更新偏好目标。在SDXL、SDXL-DPO和FLUX上的大量定性和定量评估表明，IMG优于现有对齐方法。此外，IMG还可作为灵活的即插即用适配器，无缝增强先前基于微调的对齐方法。我们的代码将发布于 <a target="_blank" rel="noopener" href="https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment。">https://github.com/SHI-Labs/IMG-Multimodal-Diffusion-Alignment。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>文本到图像扩散模型中的多模态对齐（multimodal alignment）问题</strong>，即生成的图像与输入文本提示（prompt）之间存在的语义不一致现象。具体而言，扩散模型在生成图像时可能<strong>忽略或误解提示中的某些概念</strong>，导致图像内容与文本描述不符。例如，提示中要求“一个橡胶球和一辆金属车”，但生成的图像可能缺少橡胶球或金属车的材质特征。</p>
<p>为解决这一问题，论文提出了<strong>隐式多模态引导（Implicit Multimodal Guidance, IMG）框架</strong>，通过**重新生成（re-generation）**而非传统的微调（finetuning）或局部编辑（editing）来提升对齐性能。IMG的核心创新包括：</p>
<ol>
<li><strong>无需额外数据或编辑操作</strong>：利用多模态大语言模型（MLLM）自动检测提示与图像的错位，并通过隐式对齐器（Implicit Aligner）调整扩散模型的条件特征，引导重新生成更对齐的图像。</li>
<li><strong>迭代更新的偏好目标（Iteratively Updated Preference Objective）</strong>：结合直接偏好优化（DPO）和自博弈微调（SPIN），动态更新参考模型以优化对齐效果。</li>
<li><strong>即插即用（plug-and-play）</strong>：作为轻量级适配器，可无缝集成到现有扩散模型（如SDXL、FLUX）及其微调版本（如SDXL-DPO）中，进一步提升其性能。</li>
</ol>
<p>实验表明，IMG在定性和定量评估中均优于现有的微调方法（如Diffusion-DPO）和编辑方法（如SLD），尤其在<strong>概念理解、美学质量、对象添加与修正</strong>等场景下显著减少错位。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第2节“Related Work”中系统梳理了与扩散模型对齐相关的三条主线研究，并指出它们与IMG的区别。以下按主题归纳：</p>
<p>1. 扩散模型对齐（Diffusion Model Alignment）</p>
<ul>
<li><strong>微调范式</strong></li>
<li>直接微调：Emu<br>11<br>、SDXL<br>57<br>等在高质量图文对上进行全模型微调。</li>
<li>强化学习/人类反馈（RLHF）：DDPO<br>5<br>、Diffusion-DPO<br>67<br>、ImageReward<br>73<br>等引入奖励函数或偏好对，用 RL 或 DPO 目标优化扩散权重。<br><strong>共同瓶颈</strong>：依赖昂贵且规模受限的人工标注偏好数据，难以继续扩容。</li>
<li><strong>编辑范式</strong></li>
<li>基于检测器+LLM</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Jiayi Guo,Chuanhao Yan,Xingqian Xu,Yulin Wang,Kai Wang,Gao Huang,Humphrey Shi</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26231.pdf">https://arxiv.org/pdf/2509.26231.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26231">https://arxiv.org/abs/2509.26231</a></p>
<p><strong>Arxiv ID</strong>: 2509.26231</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26231">https://papers.cool/arxiv/2509.26231</a></p>
<p><strong>Published</strong>: 2025-09-30T13:27:03Z</p>
<p><strong>Updated</strong>: 2025-09-30T13:27:03.000Z</p>
<hr>
<h3 id="17-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively"><a href="#17-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively" class="headerlink" title="17. DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26603">DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</a></h3><p>While previous AI Scientist systems can generate novel findings, they often lack the focus to produce scientifically valuable contributions that address pressing human-defined challenges. We introduce DeepScientist, a system designed to overcome this by conducting goal-oriented, fully autonomous scientific discovery over month-long timelines. It formalizes discovery as a Bayesian Optimization problem, operationalized through a hierarchical evaluation process consisting of “hypothesize, verify, and analyze”. Leveraging a cumulative Findings Memory, this loop intelligently balances the exploration of novel hypotheses with exploitation, selectively promoting the most promising findings to higher-fidelity levels of validation. Consuming over 20,000 GPU hours, the system generated about 5,000 unique scientific ideas and experimentally validated approximately 1100 of them, ultimately surpassing human-designed state-of-the-art (SOTA) methods on three frontier AI tasks by 183.7\%, 1.9\%, and 7.9\%. This work provides the first large-scale evidence of an AI achieving discoveries that progressively surpass human SOTA on scientific tasks, producing valuable findings that genuinely push the frontier of scientific discovery. To facilitate further research into this process, we will open-source all experimental logs and system code at <a target="_blank" rel="noopener" href="https://github.com/ResearAI/DeepScientist/">https://github.com/ResearAI/DeepScientist/</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然先前的 AI Scientist 系统能够生成新的发现，但它们往往缺乏产生科学上有价值贡献的专注力，这些贡献旨在解决紧迫的人类定义的挑战。我们推出了 DeepScientist 系统，旨在通过在为期数月的时间线上进行目标导向的全自主科学发现来克服这一问题。该系统将科学发现形式化为贝叶斯优化问题，并通过“假设、验证和分析”的分层评估流程进行操作。借助累积发现记忆，这一循环智能地平衡了对新假设的探索与开发，选择性地将最有前景的发现提升到更高保真度的验证水平。系统耗费超过 20,000 GPU 小时，生成了约 5,000 个独特的科学创意，并实验性地验证了约 1,100 个，最终在三项前沿 AI 任务上分别超过了人类设计的最先进（SOTA）方法 183.7%、1.9% 和 7.9%。该工作首次提供了大规模证据，表明 AI 在科学任务上取得的发现能够逐步超越人类 SOTA，产生真正推动科学发现前沿的有价值结果。为了促进进一步研究这一过程，我们将在 <a target="_blank" rel="noopener" href="https://github.com/ResearAI/DeepScientist/">https://github.com/ResearAI/DeepScientist/</a> 开源所有实验日志和系统代码。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该研究针对“现有 AI Scientist 系统虽能产出新颖结果，却缺乏聚焦人类重大科学挑战的能力”这一核心缺陷，提出并验证了一个可长期自主运行的目标驱动型科学发现框架。具体而言，论文试图解决以下关键问题：</p>
<ol>
<li><p>目标缺失导致的科学价值不足<br>既有系统多停留在“随机重组已有知识”，缺少面向人类定义难题的明确优化目标，致使产出在人类视角下显得幼稚、缺乏真正的前沿推进力。</p>
</li>
<li><p>昂贵黑箱评价下的探索效率低下<br>在真正的前沿任务中，每一次完整验证（实现-实验-分析）都需消耗  10^(16)  级 FLOPs 与月级时间，传统暴力或随机探索不可行，需要一种能在极稀疏奖励下仍保持样本高效的搜索机制。</p>
</li>
<li><p>自主闭环缺失<br>现有助手仅完成“写、评、建议”等局部功能，无法自我迭代：失败经验无法沉淀，成功路径无法被系统性地复用与扩展。</p>
</li>
<li><p>人类 SOTA 持续被突破的可行性存疑<br>学界尚缺大规模证据表明 AI 能在现代前沿任务上<strong>渐进式地、可重复地</strong>超越人类当前最佳方法，而非偶尔撞运气。</p>
</li>
</ol>
<p>为此，作者将完整科学发现形式化为<strong>目标驱动的贝叶斯优化问题</strong>，引入 DeepScientist 系统，通过“假设-验证-分析”三级分层评价与累积 Findings Memory，在月级时间尺度内对 Agent 故障归因、LLM 推理加速、AI 文本检测三项前沿任务分别取得 +183.7%、+1.9%、+7.9% 的 SOTA 提升，首次提供了 AI 可在现代科研前沿<strong>持续</strong>超越人类最佳方法的规模化实证。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 章“Related Work”中将既有研究划分为三大脉络，并指出它们与 DeepScientist 的核心区别。以下按原文框架归纳，并补充关键代表性文献：</p>
<ol>
<li>复制与工程优化（Replication &amp; Optimization）</li>
</ol>
<ul>
<li>目标：在既定科学范式内复现或微调已有方法，提升工程指标。</li>
<li>代表工作<br>– PaperBench、Paper2Agent：复现已发表论文的实验结果。<br>– Agent Laboratory、MLE-Bench：解决早期 ML 工程问题（数据清洗、超参调优等）。<br>– AlphaTensor、AlphaEvolve：用大规模试错+强化学习改进矩阵乘法或演化代码，但搜索空间由人类给定，不质疑基础假设。</li>
<li>与 DeepScientist 区别：仅“优化当前 SOTA”，而非“针对人类 SOTA 的固有缺陷提出全新方法论”。</li>
</ul>
<ol>
<li>半自动化学术助手（Semi-Automated Scientific Assistance）</li>
</ol>
<ul>
<li>目标：为人类科学家提供单点工具，不闭合探索循环。</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Yixuan Weng,Minjun Zhu,Qiujie Xie,Qiyao Sun,Zhen Lin,Sifan Liu,Yue Zhang</p>
<p><strong>Categories</strong>: cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26603.pdf">https://arxiv.org/pdf/2509.26603.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26603">https://arxiv.org/abs/2509.26603</a></p>
<p><strong>Arxiv ID</strong>: 2509.26603</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26603">https://papers.cool/arxiv/2509.26603</a></p>
<p><strong>Published</strong>: 2025-09-30T17:49:32Z</p>
<p><strong>Updated</strong>: 2025-09-30T17:49:32.000Z</p>
<hr>
<h3 id="18-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation"><a href="#18-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation" class="headerlink" title="18. MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation"></a>18. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26391">MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</a></h3><p>Image-to-video generation has made remarkable progress with the advancements in diffusion models, yet generating videos with realistic motion remains highly challenging. This difficulty arises from the complexity of accurately modeling motion, which involves capturing physical constraints, object interactions, and domain-specific dynamics that are not easily generalized across diverse scenarios. To address this, we propose MotionRAG, a retrieval-augmented framework that enhances motion realism by adapting motion priors from relevant reference videos through Context-Aware Motion Adaptation (CAMA). The key technical innovations include: (i) a retrieval-based pipeline extracting high-level motion features using video encoder and specialized resamplers to distill semantic motion representations; (ii) an in-context learning approach for motion adaptation implemented through a causal transformer architecture; (iii) an attention-based motion injection adapter that seamlessly integrates transferred motion features into pretrained video diffusion models. Extensive experiments demonstrate that our method achieves significant improvements across multiple domains and various base models, all with negligible computational overhead during inference. Furthermore, our modular design enables zero-shot generalization to new domains by simply updating the retrieval database without retraining any components. This research enhances the core capability of video generation systems by enabling the effective retrieval and transfer of motion priors, facilitating the synthesis of realistic motion dynamics.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>随着扩散模型的发展，图像到视频的生成取得了显著进展，但生成具有逼真运动的视频仍然具有很大挑战。这一难题源于准确建模运动的复杂性，其中涉及捕捉物理约束、物体交互以及在不同场景中不易泛化的特定领域动态。为了解决这一问题，我们提出了 MotionRAG，一种通过上下文感知运动适应（CAMA）从相关参考视频中获取运动先验来增强运动真实感的检索增强框架。主要技术创新包括：（i）基于检索的流水线，使用视频编码器和专用重采样器提取高级运动特征，以提炼语义运动表示；（ii）基于上下文学习的运动适应方法，通过因果变换器架构实现；（iii）基于注意力的运动注入适配器，将转移的运动特征无缝整合到预训练的视频扩散模型中。大量实验表明，我们的方法在多个领域和不同的基础模型上均显著提升了性能，同时在推理过程中计算开销几乎可以忽略不计。此外，我们的模块化设计使得通过简单更新检索数据库而无需重新训练任何组件，即可实现对新领域的零样本泛化。本研究通过实现运动先验的有效检索和迁移，增强了视频生成系统的核心能力，从而促进了逼真运动动态的合成。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“图生视频”任务中<strong>运动真实性不足</strong>的核心难题。现有扩散模型虽能生成高保真帧，却难以自行推断符合物理规律、物体交互与领域特有动态的运动。作者观察到<strong>运动规律可跨域迁移</strong>（如地球人骑马的动作可迁移给月球上的宇航员），但直接提取并注入这种高层运动先验十分困难。为此，提出 MotionRAG 框架，通过检索-增强范式显式引入真实视频中的运动模式，并设计 Context-Aware Motion Adaptation（CAMA）模块，以因果 Transformer 做上下文学习，将检索到的运动特征适配到目标图像，再经即插即用的 Motion-Adapter 注入预训练视频扩散模型，实现零样本、无额外微调地提升多基模型、多领域的运动真实度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节系统回顾了三大相关研究脉络，并逐条指出与 MotionRAG 的差异。可归纳为以下 9 个方向（按出现顺序整理）：</p>
<ol>
<li>检索-增强生成（RAG）</li>
</ol>
<ul>
<li>文本领域：RAG<br>16,17<br>通过外部知识库提升 LLM 事实性。</li>
<li>图像领域：ImageRAG<br>18<br>检索参考图像以改善文生图质量。</li>
<li>视频领域：search-T2V<br>19<br>用视频库做“文本→视频”搜索式生成，但无法以单张图像为条件，且每段生成需数分钟微调；MotionRAG 首次把 RAG 引入“图生视频”，并以上下文学习实现秒级推理。</li>
</ul>
<ol>
<li>视频运动定制化（Video Motion Customization）</li>
</ol>
<ul>
<li>MotionDirector<br>20<br>、VMC<br>21<br>、DreamVideo<br>22<br>、Customize-A-Video<br>23<br>等均需针对每条参考视频微调 LoRA/adapter，无法零样本跨域。</li>
<li>MotionRAG 无需任何微调，通过检索+上下文学习即可组合多条运动先验。</li>
</ul>
<ol>
<li>图像动画 / 图生视频（Image Animation）</li>
</ol>
<ul>
<li>Stable Video Diffusion<br>1<br>、DynamiCrafter<br>2<br>、I2VGen-XL<br>25<br>、VideoComposer<br>26<br>、MoG<br>28<br>等依赖端到端训练或低层光流<br>27<br>。</li>
<li>MotionRAG 仅注入高层语义运动特征，不依赖光流，提取过程 &lt;1 s，跨域迁移能力更强。</li>
</ul>
<ol>
<li>文本-视频检索（Text-Video Retrieval）</li>
</ol>
<ul>
<li>采用 SentenceBERT<br>29<br>、GTE<br>39<br>等句嵌入+近似最近邻搜索，实现 40 ms 级百万库</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Chenhui Zhu,Yilu Wu,Shuai Wang,Gangshan Wu,Limin Wang</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26391.pdf">https://arxiv.org/pdf/2509.26391.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26391">https://arxiv.org/abs/2509.26391</a></p>
<p><strong>Arxiv ID</strong>: 2509.26391</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26391">https://papers.cool/arxiv/2509.26391</a></p>
<p><strong>Published</strong>: 2025-09-30T15:26:04Z</p>
<p><strong>Updated</strong>: 2025-09-30T15:26:04.000Z</p>
<hr>
<h3 id="19-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention"><a href="#19-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention" class="headerlink" title="19. Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention"></a>19. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.23610">Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention</a></h3><p>Audio-visual speech separation (AVSS) methods leverage visual cues to extract target speech and have demonstrated strong separation quality in noisy acoustic environments. However, these methods usually involve a large number of parameters and require high computational cost, which is unacceptable in many applications where speech separation serves as only a preprocessing step for further speech processing. To address this issue, we propose an efficient AVSS method, named Dolphin. For visual feature extraction, we develop DP-LipCoder, a dual-path lightweight video encoder that transforms lip-motion into discrete audio-aligned semantic tokens. For audio separation, we construct a lightweight encoder-decoder separator, in which each layer incorporates a global-local attention (GLA) block to efficiently capture multi-scale dependencies. Experiments on three benchmark datasets showed that Dolphin not only surpassed the current state-of-the-art (SOTA) model in separation quality but also achieved remarkable improvements in efficiency: over 50% fewer parameters, more than 2.4x reduction in MACs, and over 6x faster GPU inference speed. These results indicate that Dolphin offers a practical and deployable solution for high-performance AVSS in real-world scenarios. Our code and demo page are publicly available at <a target="_blank" rel="noopener" href="http://cslikai.cn/Dolphin/">http://cslikai.cn/Dolphin/</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视听语音分离（AVSS）方法利用视觉线索提取目标语音，并已在嘈杂的声学环境中显示出强大的分离质量。然而，这些方法通常涉及大量参数并需要高计算成本，这在许多应用中是不可接受的，因为语音分离在这些场景中仅作为进一步语音处理的预处理步骤。为了解决这一问题，我们提出了一种高效的AVSS方法，命名为Dolphin。对于视觉特征提取，我们开发了DP-LipCoder，一种双路径轻量级视频编码器，将唇部动作转换为与音频对齐的离散语义标记。对于音频分离，我们构建了一个轻量级编码器-解码器分离器，每一层都加入了全局-局部注意力（GLA）模块，以高效捕捉多尺度依赖关系。在三个基准数据集上的实验表明，Dolphin不仅在分离质量上超过了目前的最先进（SOTA）模型，同时在效率上也取得了显著提升：参数减少超过50%，MACs减少超过2.4倍，GPU推理速度提升超过6倍。这些结果表明，Dolphin为现实场景下高性能AVSS提供了一种实用且可部署的解决方案。我们的代码和演示页面公开可访问：<a target="_blank" rel="noopener" href="http://cslikai.cn/Dolphin/。">http://cslikai.cn/Dolphin/。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>音频-视觉语音分离（AVSS）任务中“性能与效率难以兼得”的核心矛盾</strong>。具体而言，现有方法存在以下两个痛点：</p>
<ol>
<li><p><strong>视觉编码器路径依赖</strong>：<br>为获得与语音语义对齐的视觉特征，主流方案普遍复用大规模唇读预训练骨干（如3D ResNet-18、AV-HuBERT），参数量大、计算量高，严重限制在边缘设备或实时场景中的部署。</p>
</li>
<li><p><strong>音频分离器计算冗余</strong>：<br>高精度模型往往采用多轮迭代式分离（如AV-Mossformer2、RTFSNet），虽然提升了精度，却带来显著的推理延迟；而轻量级单次网络又难以捕获长程与局部多尺度依赖，导致性能骤降。</p>
</li>
</ol>
<p>为此，作者提出 <strong>Dolphin</strong>——一套<strong>高效、单次前馈的AVSS系统</strong>，通过以下关键设计在保持SOTA精度的同时实现&gt;50%参数量削减、&gt;2.4×计算量降低与&gt;6× GPU推理加速：</p>
<ul>
<li><strong>DP-LipCoder</strong>：双路径轻量视频编码器，结合向量量化与知识蒸馏，将唇动连续帧映射为<strong>音频对齐的离散语义令牌</strong>，兼顾语义密度与计算效率。</li>
<li><strong>GLA Separator</strong>：在单轮encoder-decoder架构中，每层并行嵌入<strong>全局-局部注意力块</strong>（Global-Local Attention, GLA），用粗粒度self-attention捕获长程依赖，用热扩散滤波（HDA）在伪频域完成局部去噪与细节保持，避免迭代开销。</li>
</ul>
<p>实验在LRS2、LRS3、VoxCeleb2三个基准上验证，Dolphin在SI-SNRi、SDRi、PESQ指标全面超越现有SOTA，同时显著降低参数、MAC与延迟，为AVSS在资源受限场景提供了可部署的高效方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文第2节“Related Works”将相关研究划分为两条主线，并指出其各自瓶颈；以下按这两条主线归纳代表性文献及核心观点：</p>
<p>2.1 音频-视觉语音分离（AVSS）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>代表方法</th>
<th>关键思路</th>
<th>主要局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>TF-domain 早期方法</td>
<td>Afouras et al. 2018a,b；Gao &amp; Grauman 2021</td>
<td>基于STFT时频掩蔽，视觉线索用于加权TF-bin</td>
<td>相位误差、高延迟，对非稳态噪声鲁棒性差</td>
</tr>
<tr>
<td>端到端时域</td>
<td>AV-ConvTasNet (Wu et al. 2019)</td>
<td>纯卷积编解码，视觉特征作为条件向量</td>
<td>视觉语义弱，长程依赖建模不足</td>
</tr>
<tr>
<td>高容量SOTA</td>
<td>CTCNet (Li et al. 2024a) IIANet (Li et al. 2024b) AV-Mossformer2 (Zhao et al. 2025)</td>
<td>多尺度/跨模态注意力、迭代细化</td>
<td>参数量大、MACs高，推理慢</td>
</tr>
<tr>
<td>轻量迭代</td>
<td>RTFSNet (Pegg et al. 2023) AVLiT-8 (Martel et al. 2023) Swift-Net (Sang et al. 2025)</td>
<td>循环-轻量模块，减少单次计算量</td>
<td>迭代次数带来延迟，全局依赖仍受限</td>
</tr>
</tbody>
</table>
</div>
<p>2.2 预训练视频编码器</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>策略</th>
<th>代表方法</th>
<th>关键思路</th>
<th>主要局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>大规模唇读骨干</td>
<td>3D ResNet-18 (Ma et al. 2021) AV-HuBERT (Shi et al. 2022)</td>
<td>唇读任务预训练，提取语义对齐视觉特征</td>
<td>参数量/计算量巨大，&gt;10×实时开销</td>
</tr>
<tr>
<td>直接压缩</td>
<td>Wu et al. 2023</td>
<td>量化、剪枝、蒸馏</td>
<td>语义表征能力骤降，分离性能掉&gt;1 dB</td>
</tr>
<tr>
<td>轻量自编码</td>
<td>AE (Martel et al. 2023)</td>
<td>单帧或短视频重建，低层像素级特征</td>
<td>缺乏音频对齐，语义密度低</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<ul>
<li><strong>性能侧</strong>：CTCNet、IIANet、AV-Mossformer2 通过复杂注意力或迭代细化取得SOTA精度，但计算昂贵。</li>
<li><strong>效率侧</strong>：RTFSNet、Swift-Net、AVLiT-8 以循环或轻量模块降低单次开销，却仍受迭代延迟困扰；轻量视觉自编码器虽省算力，却牺牲语义。</li>
<li><strong>Dolphin</strong> 针对上述“性能-效率”两难，提出<strong>离散语义令牌+单轮全局-局部注意力</strong>的新路线，在保持精度的同时实现数量级效率提升。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“性能-效率两难”拆成<strong>视觉编码</strong>与<strong>音频分离</strong>两条耦合的瓶颈，分别给出一次性、可协同的轻量化方案，再端到端训练。核心思路可概括为：</p>
<blockquote>
<p><strong>“先离散压缩视觉语义，再单轮全局-局部联合建模，端到端协同优化。”</strong></p>
</blockquote>
<p>具体实现分三步，每一步都对应一个关键模块与损失/约束，确保“精度不降、计算大减”。</p>
<p>1. 视觉侧：DP-LipCoder —— 把“大唇读模型”压成“轻量离散令牌”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>目标</th>
<th>做法</th>
<th>关键公式/机制</th>
<th>收益</th>
</tr>
</thead>
<tbody>
<tr>
<td>保留语义</td>
<td>双路径 VQ-VAE· 重建路径：保留人脸/身份等辅助线索· 语义路径：VQ 离散化 + AV-HuBERT 蒸馏</td>
<td>L_distill=</td>
<td>D(Z_e)-T(V)</td>
<td>2^2 Lcommit=</td>
<td>sg[Z_e]-V_s</td>
<td>_2^2+β</td>
<td>Z_e-sg[V_s]</td>
<td>_2^2$</td>
<td>0.2 dB 内逼近 3D ResNet-18，参数量↓93%</td>
</tr>
<tr>
<td>降低计算</td>
<td>MagVIT 型 3D 残差+空间注意力，仅 0.78 M 参数</td>
<td>2× 空间下采样，通道最大 32</td>
<td>MACs↓70%，CPU 延迟↓30%</td>
</tr>
</tbody>
</table>
</div>
<p><strong>输出</strong>：两组同步令牌</p>
<ul>
<li>V_r∈R^(N_v× T_v)  重建特征（含身份、表情）</li>
<li>V_s∈R^(N_v× T_v)  离散语义令牌（64 维，256 码本，与音素对齐）</li>
</ul>
<p>2. 融合侧：AVF Module —— 只沿时间维度上采，避免频域膨胀</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>机制</th>
<th>公式</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>视频门控融合</td>
<td>F_1=φ(W_1tilde V)odot W_2X</td>
<td>视觉线索直接调制音频幅值</td>
</tr>
<tr>
<td>多空间注意力融合</td>
<td>tilde V_k∈R^(N_a× T_v),;breve V=φ!(softmax!(frac 1 K∑_k tilde V_k)) F_2=breve Vodot W_4X$</td>
<td>跨通道-时间双重加权，抑制噪声</td>
</tr>
<tr>
<td>输出</td>
<td>F=F_1+F_2∈R^(N_a× T_a)</td>
<td>一次加法，零参数增量</td>
</tr>
</tbody>
</table>
</div>
<p>3. 分离侧：Single-Pass GLA Separator —— 用“全局-局部”一次性替代多轮迭代</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>关键设计</th>
<th>复杂度/参数</th>
<th>功能</th>
</tr>
</thead>
<tbody>
<tr>
<td>GA Block</td>
<td>粗粒度 MHSA → 下采样  T_a/2^q  → 上采样</td>
<td>mathcal O(T_a/2^q)^2</td>
<td>长程依赖，全局上下文</td>
</tr>
<tr>
<td>LA Block</td>
<td>DCT→热扩散滤波→IDCT→门控</td>
<td>每通道 1 个可学习  k_c</td>
<td>多尺度局部去噪，边缘保留</td>
</tr>
<tr>
<td>TDA 解码</td>
<td>顶层全局表示  G  逐层引导上采样</td>
<td>与 TDANet 相同</td>
<td>跨尺度信息回流，免掩膜直接回归</td>
</tr>
</tbody>
</table>
</div>
<p><strong>整体流程</strong></p>
<p>FEncoderF_q∑ GGAbar GTDA+GLAhat E1×1Conv+GLU EAudioDechat S</p>
<p>仅 <strong>4 层下采样 + 4 层上采样</strong>，<strong>单轮前馈</strong>，无迭代。</p>
<p>4. 训练与损失 —— 端到端联合优化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>损失</th>
<th>权重</th>
</tr>
</thead>
<tbody>
<tr>
<td>视觉预训练</td>
<td>L=L_commit+L_distill+L_recon</td>
<td>1:1:1</td>
</tr>
<tr>
<td>AVSS 微调</td>
<td>L_AVSS=(1-λ)SI-SNR_t+λSI-SNR_f</td>
<td>时域主导→频域衰减</td>
</tr>
</tbody>
</table>
</div>
<p>5. 结果 —— 把“两难”变“双赢”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>vs IIANet (SOTA)</th>
<th>vs Swift-Net (轻量)</th>
</tr>
</thead>
<tbody>
<tr>
<td>SI-SNRi</td>
<td>+0.8 dB (LRS2)</td>
<td>+2.9 dB</td>
</tr>
<tr>
<td>参数量</td>
<td>–53 %</td>
<td>–45 %</td>
</tr>
<tr>
<td>MACs</td>
<td>–2.4×</td>
<td>–1.9×</td>
</tr>
<tr>
<td>GPU 延迟</td>
<td>–6.3×</td>
<td>–1.2×</td>
</tr>
</tbody>
</table>
</div>
<p><strong>结论</strong>：DP-LipCoder 把视觉侧“大模型”压成“离散小词典”，GLA Separator 用“全局-局部”一次性完成多尺度建模，两者端到端协同，实现<strong>精度↑ 效率↑</strong> 的同时跃迁。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>“性能-效率”</strong> 两条主线，在 <strong>3 个公开基准、4 类消融、2 个扩展场景</strong> 上共开展 <strong>12 组实验</strong>，覆盖分离质量、效率指标、可视化与通用性。具体可归纳为下表：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验类别</th>
<th>数据集</th>
<th>关键变量</th>
<th>报告指标</th>
<th>目的/结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>A. 视觉编码器对比</td>
<td>LRS2</td>
<td>3D-ResNet-18 / AE / LipCoder / DP-LipCoder</td>
<td>SI-SNRi, SDRi, PESQ, Params, MACs, 延迟</td>
<td>验证离散语义+双路径设计：同等精度下参数量↓93%，MACs↓70%</td>
</tr>
<tr>
<td>B. 跨模型通用性</td>
<td>LRS2</td>
<td>将现有AVSS骨干的视频编码器原位替换为DP-LipCoder</td>
<td>同上</td>
<td>掉点≤0.6 dB，Params↓&gt;1000 MB，MACs↓&gt;5×，证明“即插即用”</td>
</tr>
<tr>
<td>C. 主实验（SOTA对比）</td>
<td>LRS2 / LRS3 / VoxCeleb2</td>
<td>Dolphin vs 8 条baselines</td>
<td>SI-SNRi, SDRi, PESQ</td>
<td>3 数据集全部第一，平均领先 IIANet 0.8 dB SI-SNRi</td>
</tr>
<tr>
<td>D. 效率基准</td>
<td>1-s 音频</td>
<td>同上</td>
<td>Params, MACs, CPU/GPU 延迟, 内存</td>
<td>Dolphin 最低 MACs（10.89 G），GPU 延迟仅 33 ms，6× 快于 IIANet</td>
</tr>
<tr>
<td>E. 多说话人扩展</td>
<td>LRS2-3Mix / 4Mix</td>
<td>专用模型逐说话人提取</td>
<td>SI-SNRi, SDRi</td>
<td>4Mix 上领先 IIANet 1.9 dB，验证复杂场景泛化</td>
</tr>
<tr>
<td>F. 消融1：GLA 块</td>
<td>LRS2</td>
<td>去除 GA / LA / 两者</td>
<td>同上</td>
<td>全局+局部互补，缺一项即掉 ≥0.9 dB</td>
</tr>
<tr>
<td>G. 消融2：HDA 替换</td>
<td>LRS2</td>
<td>HDA vs 大卷积</td>
<td>同上</td>
<td>HDA 提升 0.4 dB，参数量↓0.57 MB</td>
</tr>
<tr>
<td>H. 消融3：迭代次数</td>
<td>LRS2</td>
<td>AV-TDANet-1/8/16 次 vs Dolphin-1 次</td>
<td>同上</td>
<td>单轮 Dolphin &gt; 16 轮 TDANet，MACs 仍低 1.7×</td>
</tr>
<tr>
<td>I. 消融4：编解码深度</td>
<td>LRS2</td>
<td>固定 5 个 GLA 模块，不同 E/D 分配</td>
<td>同上</td>
<td>2 层 Encoder + 3 层 Decoder 最优</td>
</tr>
<tr>
<td>J. 消融5：输出形式</td>
<td>LRS2</td>
<td>Direct Mapping vs Mask</td>
<td>同上</td>
<td>直接回归领先 0.5 dB，PESQ+0.09</td>
</tr>
<tr>
<td>K. 消融6：融合位置</td>
<td>LRS2</td>
<td>F0(最早) … F3(最晚)</td>
<td>同上</td>
<td>越早融合越好，F0 领先 F3 达 0.6 dB</td>
</tr>
<tr>
<td>L. 可视化</td>
<td>LRS2 随机 4 段</td>
<td>时频谱图、唇重建帧</td>
<td>人工+量化</td>
<td>Dolphin 谐波完整、残留噪声最少；DP-LipCoder 重建锐度&gt;AE/LipCoder</td>
</tr>
</tbody>
</table>
</div>
<p>结果速览</p>
<ul>
<li><strong>精度</strong>：3 数据集全部 1st，最高 +0.8 dB SI-SNRi。</li>
<li><strong>效率</strong>：Params↓53%，MACs↓2.4×，GPU 延迟↓6×。</li>
<li><strong>通用性</strong>：DP-LipCoder 可“无痛”植入任意现有 AVSS 架构，掉点&lt;0.6 dB，计算暴降。</li>
<li><strong>鲁棒性</strong>：4Mix 极端场景仍领先 1.9 dB；谱图可视化噪声最少。</li>
</ul>
<p>整套实验从<strong>模块→系统→跨模型→扩展场景</strong>逐层验证，证明 Dolphin 在<strong>不牺牲精度前提下实现数量级效率提升</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望按“数据-模型-系统-评测”四个层次整理，均直接源于 Dolphin 的遗留限制或观察到的现象，可作为后续工作切入点。</p>
<p>1. 数据层面</p>
<ul>
<li><p><strong>跨语种唇动-语音对齐</strong><br>DP-LipCoder 蒸馏自英语 AV-HuBERT，非英语场景语义令牌可能失配。可收集多语种唇读数据，训练语言无关的视觉离散码本，或引入语种条件向量  l  使  V_s=f(V,l) 。</p>
</li>
<li><p><strong>极端视角与低分辨率鲁棒性</strong><br>实验仅采用 88×88 正面唇 crop。可引入随机视角掩码、超分联合训练，或采用 3D 人脸先验将唇动归一化到标准视锥，减少分辨率/视角敏感度。</p>
</li>
<li><p><strong>视觉噪声建模</strong><br>当前混合音频仅含声学噪声  n ；真实场景还存在画面模糊、光照突变。可在 DP-LipCoder 预训练阶段加入视觉失真模拟，显式建模  p(hat V|V_(noisy)) ，提升视觉侧鲁棒。</p>
</li>
</ul>
<p>2. 模型层面</p>
<ul>
<li><p><strong>热扩散系数  k_c  的自适应机制</strong><br>HDA 中每个通道仅学一个标量  k<em>c 。可令  k_c  随信噪比或语音/静音段动态变化，例如用超网络  k_c=h</em>(θ)(SNR_(est)) ，实现内容自适应平滑。</p>
</li>
<li><p><strong>全局-局部块的可扩展性</strong><br>GLA 块目前固定 1 GA + 1 LA。可引入 Neural Architecture Search 让网络自动决定每一层 GA/LA 比例，甚至学习最佳下采样因子  2^q ，在更深模型上进一步压榨性能。</p>
</li>
<li><p><strong>因果/实时约束</strong><br>Dolphin 采用非因果 MHSA。将 CSA 改为因果 FlashAttention，并把热扩散滤波限制在缓存帧内，即可得到因果版 Causal-Dolphin，满足通话、耳机等低延迟场景。</p>
</li>
<li><p><strong>统一离散令牌框架</strong><br>目前仅视觉离散。可尝试把音频也离散成  A_s ，在离散空间做跨模态对比或掩码语言-视觉建模，实现“完全离散”的 AVSS，显著降低跨模态融合复杂度。</p>
</li>
</ul>
<p>3. 系统与部署</p>
<ul>
<li><p><strong>端侧量化与编译优化</strong><br>文中仅报告 FP32 延迟。可对 DP-LipCoder 的 VQ 码本做 8-bit 量化，GLA 块做整数量化 + TensorRT/ONNX 融合，考察 INT8 精度-效率 trade-off。</p>
</li>
<li><p><strong>多说话人一次性输出</strong><br>当前多说话人场景需逐目标串行提取。可扩展解码器为  K  分支，同时输出  K  路说话人嵌入，再辅以 permutation invariant loss，实现单次前馈多说话人分离。</p>
</li>
<li><p><strong>与前端阵列结合</strong><br>将麦克风阵列波束形成输出作为 Dolphin 的“音频先验”，视觉令牌用于选择波束方向，形成 beamforming-AVSS 联合网络，进一步提升远场性能。</p>
</li>
</ul>
<p>4. 评测与理论</p>
<ul>
<li><p><strong>人类主观实验</strong><br>仅采用 SI-SNR、PESQ 客观指标。可设计听音实验检验离散语义令牌是否带来更好的可懂度（Intelligibility）与听觉疲劳度，特别是在极低信噪比。</p>
</li>
<li><p><strong>解释性分析</strong><br>可视化 VQ 码本入口，观察不同离散令牌是否对应音素簇；或对热扩散滤波前后特征做频谱统计，验证其是否物理上抑制了高频噪声，从而解释为何 HDA 优于大卷积。</p>
</li>
<li><p><strong>理论复杂度下界</strong><br>Dolphin 将 MHSA 复杂度从  mathcal O(T^2)  减至  mathcal O((T/2^q)^2) 。可进一步探讨在给定精度 ε 下，AVSS 任务对视觉令牌长度、音频下采样率的理论最小复杂度，指导未来轻量模型设计。</p>
</li>
</ul>
<p>总结</p>
<ol>
<li><strong>数据</strong>：多语种、极端视角、视觉噪声。</li>
<li><strong>模型</strong>：自适应 HDA、因果化、NAS 搜结构、完全离散化。</li>
<li><strong>系统</strong>：INT8 量化、多说话人并行、阵列前端融合。</li>
<li><strong>评测</strong>：主观听感、解释性可视化、复杂度下界理论。</li>
</ol>
<p>这些方向既可直接嵌入 Dolphin 的模块化框架，也可作为独立课题深入，推动 AVSS 向“真实场景、实时部署、可解释” 更进一步。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：音频-视觉语音分离（AVSS）精度高者计算重，轻量者性能跌，视觉编码器“大模型依赖”是主要瓶颈。</li>
<li><strong>方法</strong>：提出 <strong>Dolphin</strong>，核心为</li>
</ul>
<ol>
<li><strong>DP-LipCoder</strong> – 双路径 VQ-VAE，把唇动压成音频对齐的离散语义令牌，参数量↓93%。</li>
<li><strong>GLA Separator</strong> – 单轮编解码，每层并行全局-局部注意力（粗粒度 MHSA + 热扩散滤波），一次性捕获长程与局部依赖。</li>
</ol>
<ul>
<li><strong>结果</strong>：LRS2/LRS3/VoxCeleb2 三基准全面 SOTA，SI-SNRi 最高 +0.8 dB；Params↓50%，MACs↓2.4×，GPU 推理↑6×。</li>
<li><strong>消融</strong>：离散令牌、热扩散滤波、最早融合、直接回归等设计均显著贡献。</li>
<li><strong>结论</strong>：Dolphin 在精度与效率间取得数量级平衡，为边缘实时 AVSS 提供了可部署方案。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Efficient%20Audio-Visual%20Speech%20Separation%20with%20Discrete%20Lip%20Semantics%20and%20Multi-Scale%20Global-Local%20Attention，链接是https://arxiv.org/pdf/2509.23610，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.23610。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Kai Li,Kejun Gao,Xiaolin Hu</p>
<p><strong>Categories</strong>: cs.SD,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.23610.pdf">https://arxiv.org/pdf/2509.23610.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.23610">https://arxiv.org/abs/2509.23610</a></p>
<p><strong>Arxiv ID</strong>: 2509.23610</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.23610">https://papers.cool/arxiv/2509.23610</a></p>
<p><strong>Published</strong>: 2025-09-28T03:25:34Z</p>
<p><strong>Updated</strong>: 2025-09-28T03:25:34.000Z</p>
<hr>
<h3 id="20-DA-2-Depth-Anything-in-Any-Direction"><a href="#20-DA-2-Depth-Anything-in-Any-Direction" class="headerlink" title="20. DA^2: Depth Anything in Any Direction"></a>20. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2509.26618">DA^2: Depth Anything in Any Direction</a></h3><p>Panorama has a full FoV (360$^\circ\times$180$^\circ$), offering a more complete visual description than perspective images. Thanks to this characteristic, panoramic depth estimation is gaining increasing traction in 3D vision. However, due to the scarcity of panoramic data, previous methods are often restricted to in-domain settings, leading to poor zero-shot generalization. Furthermore, due to the spherical distortions inherent in panoramas, many approaches rely on perspective splitting (e.g., cubemaps), which leads to suboptimal efficiency. To address these challenges, we propose $\textbf{DA}$$^{\textbf{2}}$: $\textbf{D}$epth $\textbf{A}$nything in $\textbf{A}$ny $\textbf{D}$irection, an accurate, zero-shot generalizable, and fully end-to-end panoramic depth estimator. Specifically, for scaling up panoramic data, we introduce a data curation engine for generating high-quality panoramic depth data from perspective, and create $\sim$543K panoramic RGB-depth pairs, bringing the total to $\sim$607K. To further mitigate the spherical distortions, we present SphereViT, which explicitly leverages spherical coordinates to enforce the spherical geometric consistency in panoramic image features, yielding improved performance. A comprehensive benchmark on multiple datasets clearly demonstrates DA$^{2}$’s SoTA performance, with an average 38% improvement on AbsRel over the strongest zero-shot baseline. Surprisingly, DA$^{2}$ even outperforms prior in-domain methods, highlighting its superior zero-shot generalization. Moreover, as an end-to-end solution, DA$^{2}$ exhibits much higher efficiency over fusion-based approaches. Both the code and the curated panoramic data will be released. Project page: <a target="_blank" rel="noopener" href="https://depth-any-in-any-dir.github.io/">https://depth-any-in-any-dir.github.io/</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>全景具有完整的视场角 （360$^\circ\times$180$^\circ$），提供比透视图像更完整的视觉描述。由于这一特性，全景深度估计在 3D 视觉中越来越受欢迎。然而，由于全景数据的稀缺性，以前的方法往往局限于域内设置，导致零样本泛化能力差。此外，由于全景图固有的球面畸变，许多方法依赖于透视分割（例如立方体贴图），这导致效率不理想。为了应对这些挑战，我们提出了 $\textbf{DA}$$^{\textbf{2}}$： $\textbf{D}$epth $\textbf{A}$nything 中的 $\textbf{A}$ny $\textbf{D}$irection，一个准确、零样本可泛化且完全端到端的全景深度估计器。具体来说，为了放大全景数据，我们引入了一个数据策展引擎，从透视生成高质量的全景深度数据，并创建了 $\sim$543K 的全景 RGB 深度对，使总数达到 $\sim$607K。为了进一步减轻球面畸变，我们提出了 SphereViT，它明确地利用球面坐标来强制执行全景图像特征中的球面几何一致性，从而提高性能。对多个数据集的综合基准测试清楚地展示了 DA$^{2}$ 的 SoTA 性能，与最强的零样本基线相比，AbsRel 的平均提高了 38%。令人惊讶的是，DA$^{2}$ 甚至优于之前的域内方法，凸显了其卓越的零样本泛化。此外，作为端到端解决方案，DA$^{2}$ 比基于融合的方法表现出更高的效率。代码和精选的全景数据都将发布。项目页面：<a target="_blank" rel="noopener" href="https://depth-any-in-any-dir.github.io/。">https://depth-any-in-any-dir.github.io/。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决<strong>全景图像深度估计中的两个核心瓶颈</strong>：</p>
<ol>
<li><p><strong>数据稀缺导致的零样本泛化能力差</strong><br>现有全景深度模型只能在特定域内表现良好，一旦跨域测试（零样本）性能急剧下降，根本原因是高质量全景深度数据极少（≈63 k）。作者提出“全景数据整理引擎”，将大规模透视深度数据（≈543 k）通过 P2E 投影 + 全景外绘转换为全景格式，把训练集扩大至 ≈607 k，从而显著增强模型的零样本泛化能力。</p>
</li>
<li><p><strong>球面畸变带来的几何不一致</strong><br>等距柱状投影（ERP）在极点区域存在严重拉伸，传统 ViT 的 2D 位置编码无法感知球面几何。为此，作者设计 <strong>SphereViT</strong>：</p>
</li>
</ol>
<ul>
<li>用球面坐标 (ϕ,θ) 生成固定、可复用的球面嵌入  E_(sphere) ；</li>
<li>以交叉注意力让图像特征显式“关注”球面嵌入，无需额外分支或立方体图融合，即可端到端地获得畸变感知特征，提升几何保真度。</li>
</ul>
<p>综上，DA2 的目标是在<strong>任意方向</strong>（360°×180°）上，从单张全景图实现<strong>高精度、零样本泛化、端到端</strong>的尺度不变距离估计，并超越现有零样本乃至域内方法的性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线：透视图像深度估计与全景图像深度估计。以下按类别梳理代表性工作，并指出其与 DA2 的差异。</p>
<ul>
<li><strong>透视深度估计（Perspective Depth Estimation）</strong></li>
</ul>
<ol>
<li><strong>度量 / 尺度不变深度</strong></li>
</ol>
<ul>
<li>UniDepth 系列：UniDepth-v1/v2（Piccinelli et al., 2024 &amp; 2025b）提出通用相机模型，实现零样本度量深度。</li>
<li>Metric3D-v1/v2（Yin et al., 2023；Hu et al., 2024）利用大规模多数据集训练，输出绝对尺度深度。</li>
<li>MoGe/MoGe-v2（Wang et al., 2025c,d）通过最优监督解耦几何与尺度，实现开放域度量估计。<br><em>差异</em>：以上方法仅覆盖窄 FoV，无法一次性预测 360° 距离；DA2 直接输出全景尺度不变距离。</li>
</ul>
<ol>
<li><strong>相对深度</strong></li>
</ol>
<ul>
<li>DepthAnything-v1/v2（Yang et al., 2024a,b）凭借 1.5 M 透视图实现强零样本泛化，但仅提供相对深度。</li>
<li>ZoeDepth（Bhat et al., 2023）混合相对与度量头，仍需多视角融合才能覆盖全景。</li>
</ul>
<ol>
<li><strong>生成式微调</strong></li>
</ol>
<ul>
<li>基于 Stable Diffusion/FLUX 的 Depth-CNN（Ke et al., 2024；He et al., 2024b）用少量高质量数据微调扩散模型，取得锐利深度，但 FoV 受限。</li>
<li><strong>全景深度估计（Panoramic Depth Estimation）</strong></li>
</ul>
<ol>
<li><strong>域内方法（In-domain）</strong></li>
</ol>
<ul>
<li>多投影融合：BiFuse/BiFuse++（Wang et al., 2020 &amp; 2022）、UniFuse（Jiang et al., 2021）、HRDFuse（Ai et al., 2023）将 ERP 与立方体图特征融合，缓解畸变但引入额外分支。</li>
<li>球面表示：SphereDepth（Yan et al., 2022）、Elite360D（Ai &amp; Wang, 2024）、HUSH（Lee et al., 2025）采用二十面体网格或球谐函数，需特殊采样算子。</li>
<li>序列建模：SliceNet（Pintore et al., 2021）、HoHoNet（Sun et al., 2021）沿经度用 RNN/LSTM 扫描，推理效率低。<br><em>差异</em>：DA2 无需立方体图或网格，端到端单网络即可感知球面几何。</li>
</ul>
<ol>
<li><strong>零样本方法（Zero-shot）</strong></li>
</ol>
<ul>
<li>360MonoDepth（Rey et al., 2022）在二十面体上融合 MiDaS 透视深度，存在多视角不一致。</li>
<li>PanDA（Cao et al., 2025）利用 Möbius 增广自监督，仅 ≈20 k 标注数据，性能受限。</li>
<li>UniK3D（Piccinelli et al., 2025a）解耦相机射线与距离图，支持任意相机，但训练数据 &lt;30 k。</li>
<li>DepthAnyCamera（Guo et al., 2025）将透视图投影至球面，但输入 FoV 最大 124°，覆盖不完整。<br><em>差异</em>：DA2 通过 543 k curated 全景对 + SphereViT，实现目前最佳零样本泛化，且推理速度比融合法快两个数量级。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“数据侧扩规模”与“模型侧显式编码球面几何”两条路径，系统性地解决全景深度估计的零样本泛化与球面畸变难题。具体方案如下：</p>
<ol>
<li>数据侧：全景数据整理引擎</li>
</ol>
<ul>
<li>透视→等距柱状（P2E）投影<br>给定透视 RGB-D 与 FoV，按公式</li>
</ul>
<p>f<em>x=W</em>(per)2tan(XFoV/2),quad f<em>y=H</em>(per)2tan(YFoV/2)</p>
<p>计算焦距，再将像素反投影至单位球面，得到球面坐标  (φ,θ) ，最后映射到 ERP 坐标</p>
<p>u=(φ) / (2π)W<em>(pano),quad v=(θ) / (π)H</em>(pano).</p>
<ul>
<li>全景外绘（Out-painting）<br>对 RGB 采用 FLUX-I2P（LoRA 微调 DiT）把局部 ERP 补全为 360°×180°；GT 深度仅做 P2E 保留可靠尺度。</li>
<li>规模效应<br>6 个高质量透视数据集共 543 k 样本被转换为全景，加上 63 k 原生全景，训练集扩大至 ≈607 k，实现“Scaling Law”式性能提升（AbsRel 从 8.07% 降至 6.62%）。</li>
</ul>
<ol>
<li>模型侧：SphereViT 端到端畸变感知网络</li>
</ol>
<ul>
<li>球面嵌入  E_(sphere)<br>对 ERP 的每个 patch 计算  (φ,θ) ，经正弦-余弦基函数展开至与图像特征同维，得到固定、可复用的球面位置编码。</li>
<li>交叉注意力注入<br>图像特征  Z  作为 Query， E_(sphere)  作为 Key/Value，执行</li>
</ul>
<p>CrossAttn(Z,E<em>(sphere))=Softmax!(ZW_Q(E</em>(sphere)W<em>K)^top{√D_k})(E</em>(sphere)W_V).</p>
<p>使网络显式“看到”球面拉伸差异，无需额外立方体图或 icosahedron 分支。</p>
<ul>
<li>双任务损失<br>全局距离损失  L<em>(dis) （L1，median 对齐后）+ 局部法向损失  L</em>(nor) （L1，由距离图差分得到），总损失</li>
</ul>
<p>L=λ<em>d L</em>(dis)+λ<em>n L</em>(nor),quad λ_d=1,λ_n=2.</p>
<p>法向监督抑制了极点与边缘的伪影，提升表面锐利度。</p>
<ol>
<li>训练与推理</li>
</ol>
<ul>
<li>backbone 采用 DINOv2-ViT-L，交叉注意力层仅 4 个，轻量高效；1024×512 输入，单张 H20 GPU 0.3 s 完成推理，比融合法快约 90×。</li>
</ul>
<p>通过“数据+模型”协同，DA2 在三个公开全景基准上实现 SoTA 零样本性能，AbsRel 平均比最强基线再降 38%，甚至超过多数域内方法。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从 <strong>定量基准、消融分析、数据规模曲线、定性可视化、运行效率</strong> 五个维度展开实验，系统验证 DA2 的有效性。关键结果均基于 <strong>Stanford2D3D、Matterport3D、PanoSUNCG</strong> 三大全景基准，指标采用 <strong>AbsRel↓、RMSE↓、δ₁↑、δ₂↑</strong>（median 对齐）。主要实验如下：</p>
<ol>
<li>主基准对比（Table 1）</li>
</ol>
<ul>
<li><strong>零样本设置</strong><br>– 涵盖 13 种透视深度方法（Metric3Dv2、UniDepthv2、MoGev2、DepthAnythingv2 等）经多视角拼接-融合后的全景结果；<br>– 3 种原生零样本全景方法（360MonoDepth、UniK3D、PanDA）；<br>– DA2 取得 <strong>平均 AbsRel 7.23 %</strong>，较第二佳零样本方法（UniK3D 11.31 %）相对降低 <strong>38 %</strong>，δ₁ 达 <strong>95.73 %</strong>。</li>
<li><strong>与域内方法比较</strong><br>– 17 个域内全景网络（HUSH、SphereFusion、PanoFormer 等）均在各自训练域测试；<br>– DA2 零样本仍优于全部，平均排名 <strong>1.00</strong>，验证跨域泛化优势。</li>
</ul>
<ol>
<li>数据规模消融（Table 2 &amp; Figure 2）<br>逐步加入 6 组透视→全景数据（HPSPano、VKPano…→DRPano），总样本从 63 k → 607 k；</li>
</ol>
<ul>
<li>AbsRel 单调下降：8.07 % → 6.62 %；</li>
<li>δ₁ 单调上升：92.91 % → 95.73 %；<br>给出清晰的 <strong>Scaling Law</strong> 曲线，证明数据引擎是性能提升主因。</li>
</ul>
<ol>
<li>组件消融（Table 3 &amp; Figure 6）</li>
</ol>
<ul>
<li><strong>w/o 全景外绘</strong>：AbsRel 从 6.62 % → 7.59 %（↑15 %），验证外绘对全局上下文至关重要；</li>
<li><strong>w/o 球面嵌入 Esphere</strong>：AbsRel → 6.84 %，几何出现“弯曲墙体”伪影（图 6a）；</li>
<li><strong>w/o 法向损失 Lnor</strong>：AbsRel → 6.99 %，表面粗糙、极点 artifacts 增多（图 6b）。</li>
</ul>
<ol>
<li>定性对比（Figure 5）<br>与最强零样本端到端方法 UniK3D 和最强融合法 MoGev2 在同一输入全景上可视化：</li>
</ol>
<ul>
<li>DA2 边缘更锐利、结构更平直，无融合法常见的“不规则墙面/碎片化建筑”；</li>
<li>单张 H20 GPU 推理 <strong>≈0.3 s</strong>，较 MoGev2（≈28 s）快约 <strong>90×</strong>。</li>
</ul>
<ol>
<li>下游应用验证（Supp. Figure 8–9）</li>
</ol>
<ul>
<li><strong>Pano3R</strong>：多房间全景点云全局对齐，无需尺度/旋转后处理；</li>
<li><strong>分层家装</strong>：对“空/简/满”三种复杂度全景，重建点云可精确叠加；</li>
<li><strong>机器人仿真</strong>：DA2 点云直接作为碰撞-free 的 3D 仿真环境。</li>
</ul>
<ol>
<li>训练细节与可复现性</li>
</ol>
<ul>
<li>7 个数据集混合概率（Table 4）、损失权重、LoRA 秩 256、外绘 64 GPU×9 天等全部公开；</li>
<li>代码与 543 k curated 全景数据将开源，供后续研究继续放大训练规模。</li>
</ul>
<p>综上，实验不仅证明 DA2 在 <strong>精度、泛化、效率</strong> 三方面的领先，也揭示了 <strong>“数据规模 + 球面几何先验”</strong> 是突破全景深度估计瓶颈的关键路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可延续 DA2 的框架，推动全景几何估计及相关应用再进一步：</p>
<ul>
<li><strong>更高分辨率与真实 4K/8K 全景</strong><br>当前训练固定在 1024×512，极点区域仍显模糊。探索：</li>
</ul>
<ol>
<li>分块-全局混合训练，逐步上采样至 4K；</li>
<li>联合超分网络，在测试时无损超解析深度；</li>
<li>原生 4K 全景数据集采集，减少二次插值误差。</li>
</ol>
<ul>
<li><strong>度量尺度预测</strong><br>DA2 仅估计尺度不变距离。可引入：</li>
</ul>
<ol>
<li>带 IMU/激光测距的弱监督信号，学习场景级绝对尺度；</li>
<li>相机高度先验或地面平面约束，实现“零样本度量”全景深度。</li>
</ol>
<ul>
<li><strong>时序全景深度 / 动态场景</strong><br>利用 360° 视频，设计球面时空注意力，估计：</li>
</ul>
<ol>
<li>逐帧度量深度；</li>
<li>刚体/非刚体运动分解；</li>
<li>联合光流与深度，实现 360° 场景流估计。</li>
</ol>
<ul>
<li><p><strong>语义-几何联合全景模型</strong><br>将语义分割、实例掩码、全景深度纳入统一 Transformer，共享球面嵌入  E_(sphere) ，实现“全景通用视觉大模型”，支持开放词汇查询与编辑。</p>
</li>
<li><p><strong>球面扩散模型用于深度补全与生成</strong><br>以 DA2 深度为条件，训练球面 DiT：</p>
</li>
</ul>
<ol>
<li>缺失极点/天空深度补全；</li>
<li>文本-到-全景深度生成，服务 VR 场景自动搭建。</li>
</ol>
<ul>
<li><strong>多模态全景定位与 SLAM</strong><br>结合激光雷达-全景-IMU，研究：</li>
</ul>
<ol>
<li>球面特征匹配与回环检测；</li>
<li>度量全景深度前端，实现轻量级 360° SLAM；</li>
<li>支持 AR 眼镜等大 FoV 设备实时运行。</li>
</ol>
<ul>
<li><p><strong>球面几何一致性自监督信号</strong><br>利用左右边界循环一致性、极点闭合约束、光度重投影损失，无需 GT 即可持续优化，进一步提升在野外无标注全景上的泛化。</p>
</li>
<li><p><strong>高效部署与模型压缩</strong></p>
</li>
</ul>
<ol>
<li>将 SphereViT 蒸馏为轻量 CNN 或 MobileViT，适配头显/手机；</li>
<li>量化球面嵌入查找表，避免每次实时计算三角函数；</li>
<li>开发球面专用 Shader，在 GPU 上并行化 D2N 与点云生成。</li>
</ol>
<ul>
<li><p><strong>开放世界全景深度评测基准</strong><br>建立覆盖室外街景、无人机、水下、夜间等多样场景的高清全景-激光 GT 数据集，推动零样本度量深度研究的社区共识。</p>
</li>
<li><p><strong>跨任务迁移：全景→透视知识互惠</strong><br>利用 DA2 的 600 k 全景深度预训练权重，反向微调透视模型，验证“球面→平面”知识迁移能否提升透视深度边缘、遮挡区域表现。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>DA2: Depth Anything in Any Direction</strong> 提出一种<strong>端到端、零样本、360°×180° 全景尺度不变距离估计</strong>方案，核心贡献与结果可概括为：</p>
<ol>
<li>数据瓶颈突破</li>
</ol>
<ul>
<li>设计<strong>全景数据整理引擎</strong>：将 543 k 高质量透视 RGB-D 经 P2E 投影 + FLUX-I2P 外绘，生成完整全景，总量扩至 ≈607 k。</li>
<li>首次在全景深度任务上呈现<strong>Scaling Law</strong>：数据量↑ → 零样本误差单调↓，AbsRel 从 8.07 % 降至 6.62 %。</li>
</ul>
<ol>
<li>球面畸变建模</li>
</ol>
<ul>
<li>提出 <strong>SphereViT</strong>：固定球面坐标嵌入  E_(sphere)  + 交叉注意力，无需立方体图等额外分支，即可显式学习畸变感知特征。</li>
<li>联合<strong>距离 + 法向</strong>损失监督，表面更锐利，极点 artifacts 显著减少。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li><strong>零样本设置</strong>：在 Stanford2D3D、Matterport3D、PanoSUNCG 上平均 AbsRel <strong>7.23 %</strong>，较最强基线再降 <strong>38 %</strong>；δ₁ <strong>95.73 %</strong>。</li>
<li><strong>超越域内方法</strong>：平均排名 1.00，证明跨域泛化优势。</li>
<li><strong>推理效率</strong>：单张 H20 GPU 0.3 s，比融合法快约 90×。</li>
</ul>
<ol>
<li>下游应用<br>多房间点云全局对齐、分层家装、机器人仿真等无需额外标定，验证几何一致性。</li>
</ol>
<p>综上，DA2 通过“<strong>数据侧大规模透视→全景转换</strong>”与“<strong>模型侧球面几何嵌入</strong>”双轮驱动，实现当前最佳零样本全景深度估计，为 360° 3D 重建、AR/VR、仿真等应用提供高效可靠的几何基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DA%24%5E2%24%3A%20Depth%20Anything%20in%20Any%20Direction，链接是https://arxiv.org/pdf/2509.26618，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.26618。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Haodong Li,Wangguangdong Zheng,Jing He,Yuhao Liu,Xin Lin,Xin Yang,Ying-Cong Chen,Chunchao Guo</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2509.26618.pdf">https://arxiv.org/pdf/2509.26618.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.26618">https://arxiv.org/abs/2509.26618</a></p>
<p><strong>Arxiv ID</strong>: 2509.26618</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2509.26618">https://papers.cool/arxiv/2509.26618</a></p>
<p><strong>Published</strong>: 2025-09-30T17:55:37Z</p>
<p><strong>Updated</strong>: 2025-09-30T17:55:37.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202510/2025-10-02/">https://wdndev.github.io/daily/hf/202510/2025-10-02/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-MCPMark-A-Benchmark-for-Stress-Testing-Realistic-and-Comprehensive-MCP-Use"><span class="toc-text">1. MCPMark: A Benchmark for Stress-Testing Realistic and Comprehensive MCP Use</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-The-Dragon-Hatchling-The-Missing-Link-between-the-Transformer-and-Models-of-the-Brain"><span class="toc-text">2. The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Vision-Zero-Scalable-VLM-Self-Improvement-via-Strategic-Gamified-Self-Play"><span class="toc-text">3. Vision-Zero: Scalable VLM Self-Improvement via Strategic Gamified Self-Play</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Winning-the-Pruning-Gamble-A-Unified-Approach-to-Joint-Sample-and-Token-Pruning-for-Efficient-Supervised-Fine-Tuning"><span class="toc-text">4. Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-TruthRL-Incentivizing-Truthful-LLMs-via-Reinforcement-Learning"><span class="toc-text">5. TruthRL: Incentivizing Truthful LLMs via Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Learning-to-See-Before-Seeing-Demystifying-LLM-Visual-Priors-from-Language-Pre-training"><span class="toc-text">6. Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-OceanGym-A-Benchmark-Environment-for-Underwater-Embodied-Agents"><span class="toc-text">7. OceanGym: A Benchmark Environment for Underwater Embodied Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-More-Thought-Less-Accuracy-On-the-Dual-Nature-of-Reasoning-in-Vision-Language-Models"><span class="toc-text">8. More Thought, Less Accuracy? On the Dual Nature of Reasoning in Vision-Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Thinking-Free-Policy-Initialization-Makes-Distilled-Reasoning-Models-More-Effective-and-Efficient-Reasoners"><span class="toc-text">9. Thinking-Free Policy Initialization Makes Distilled Reasoning Models More Effective and Efficient Reasoners</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-DC-VideoGen-Efficient-Video-Generation-with-Deep-Compression-Video-Autoencoder"><span class="toc-text">10. DC-VideoGen: Efficient Video Generation with Deep Compression Video Autoencoder</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Who%E2%80%99s-Your-Judge-On-the-Detectability-of-LLM-Generated-Judgments"><span class="toc-text">11. Who’s Your Judge? On the Detectability of LLM-Generated Judgments</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Thinking-Sparks-Emergent-Attention-Heads-in-Reasoning-Models-During-Post-Training"><span class="toc-text">12. Thinking Sparks!: Emergent Attention Heads in Reasoning Models During Post Training</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-dParallel-Learnable-Parallel-Decoding-for-dLLMs"><span class="toc-text">13. dParallel: Learnable Parallel Decoding for dLLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-VitaBench-Benchmarking-LLM-Agents-with-Versatile-Interactive-Tasks-in-Real-world-Applications"><span class="toc-text">14. VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Learning-Human-Perceived-Fakeness-in-AI-Generated-Videos-via-Multimodal-LLMs"><span class="toc-text">15. Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-IMG-Calibrating-Diffusion-Models-via-Implicit-Multimodal-Guidance"><span class="toc-text">16. IMG: Calibrating Diffusion Models via Implicit Multimodal Guidance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-DeepScientist-Advancing-Frontier-Pushing-Scientific-Findings-Progressively"><span class="toc-text">17. DeepScientist: Advancing Frontier-Pushing Scientific Findings Progressively</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-MotionRAG-Motion-Retrieval-Augmented-Image-to-Video-Generation"><span class="toc-text">18. MotionRAG: Motion Retrieval-Augmented Image-to-Video Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-Efficient-Audio-Visual-Speech-Separation-with-Discrete-Lip-Semantics-and-Multi-Scale-Global-Local-Attention"><span class="toc-text">19. Efficient Audio-Visual Speech Separation with Discrete Lip Semantics and Multi-Scale Global-Local Attention</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20-DA-2-Depth-Anything-in-Any-Direction"><span class="toc-text">20. DA^2: Depth Anything in Any Direction</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>