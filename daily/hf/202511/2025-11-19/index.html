<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2025-11-19 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. P1: Mastering Physics Olympiads with Reinforcement LearningRecent progress in large language models (LLMs) has moved the frontier from puzzle-solving to scien">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2025-11-19">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202511/2025-11-19/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. P1: Mastering Physics Olympiads with Reinforcement LearningRecent progress in large language models (LLMs) has moved the frontier from puzzle-solving to scien">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.927Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202511/2025-11-19/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2025-11-19',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2025-11-19</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.927Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">79.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>281min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2025-11-19"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"><a href="#1-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning" class="headerlink" title="1. P1: Mastering Physics Olympiads with Reinforcement Learning"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13612">P1: Mastering Physics Olympiads with Reinforcement Learning</a></h3><p>Recent progress in large language models (LLMs) has moved the frontier from puzzle-solving to science-grade reasoning-the kind needed to tackle problems whose answers must stand against nature, not merely fit a rubric. Physics is the sharpest test of this shift, which binds symbols to reality in a fundamental way, serving as the cornerstone of most modern technologies. In this work, we manage to advance physics research by developing large language models with exceptional physics reasoning capabilities, especially excel at solving Olympiad-level physics problems. We introduce P1, a family of open-source physics reasoning models trained entirely through reinforcement learning (RL). Among them, P1-235B-A22B is the first open-source model with Gold-medal performance at the latest International Physics Olympiad (IPhO 2025), and wins 12 gold medals out of 13 international/regional physics competitions in 2024/2025. P1-30B-A3B also surpasses almost all other open-source models on IPhO 2025, getting a silver medal. Further equipped with an agentic framework PhysicsMinions, P1-235B-A22B+PhysicsMinions achieves overall No.1 on IPhO 2025, and obtains the highest average score over the 13 physics competitions. Besides physics, P1 models also present great performance on other reasoning tasks like math and coding, showing the great generalibility of P1 series.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>在大型语言模型（LLM）方面的最新进展已将前沿从解谜能力推进到科学级推理——这种推理能力能够解决那些答案必须经得起自然检验的问题，而不仅仅是符合评分标准。物理学是这一转变最严苛的测试，它以根本性的方式将符号与现实联系起来，是大多数现代技术的基石。在这项工作中，我们通过开发具备卓越物理推理能力的大型语言模型推动了物理研究，尤其擅长解决奥林匹克水平的物理问题。我们推出了 P1，一个完全通过强化学习（RL）训练的开源物理推理模型家族。其中，P1-235B-A22B 是首个在最新国际物理奥林匹克竞赛（IPhO 2025）中获得金牌的开源模型，并在 2024/2025 年的 13 项国际/地区物理竞赛中赢得 12 枚金牌。P1-30B-A3B 在 IPhO 2025 上也超过了几乎所有其他开源模型，获得银牌。进一步配备智能框架 PhysicsMinions 后，P1-235B-A22B+PhysicsMinions 在 IPhO 2025 中总体排名第一，并在 13 项物理竞赛中获得最高平均分。除了物理学，P1 模型在其他推理任务如数学和编码方面也表现出色，显示了 P1 系列的优秀通用性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破现有大语言模型（LLM）在“科学级推理”上的瓶颈，使其不再满足于符合评分标准，而是能够产出经得起物理定律与实验事实检验的答案。具体目标可归纳为：</p>
<ul>
<li>构建首个在<strong>国际物理奥林匹克（IPhO 2025）</strong>上达到<strong>金牌分数线</strong>的开源模型，证明开源社区也能在最具挑战性的科学推理任务上与闭源旗舰系统比肩。</li>
<li>通过<strong>纯强化学习（RL）后训练</strong>而非监督微调，探索如何让模型自动习得复杂的多步物理推理、符号演算与实验验证能力。</li>
<li>提出一套可复制的训练与推理框架，将**训练时扩展（train-time scaling）<strong>与</strong>测试时扩展（test-time scaling）**结合，实现持续且可控的性能提升。</li>
<li>验证物理领域强化学习带来的收益能否<strong>泛化</strong>到数学、编程等更广泛的推理任务，避免“物理过拟合”。</li>
<li>建立并公开一个高质量、规则可验证的<strong>物理奥林匹克级别数据集</strong>及评测基准，为后续研究提供标准化试验台。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在多个维度与现有文献对接，相关研究可梳理为以下几条主线：</p>
<ol>
<li>科学推理与物理问答</li>
</ol>
<ul>
<li><strong>PHYSICS</strong>（Zheng et al., 2025b）首次系统评估 LLM 在高中物理奥赛 vs 大学非物理题的表现，指出奥赛题对推理难度要求更高。</li>
<li><strong>Physics SuperNova</strong>（Qiu et al., 2025）构建 agent 系统在 IPhO 2025 上与金牌选手得分持平，但模型闭源且依赖人工设计工具。</li>
<li><strong>HiPhO</strong>（Yu et al., 2025a）提出覆盖 13 项 2024-2025 物理奥赛的统一评测协议，为本工作提供基准。</li>
</ul>
<ol>
<li>基于可验证奖励的强化学习（RLVR）</li>
</ol>
<ul>
<li><strong>DeepSeek-R1</strong>（Guo et al., 2025）用二进制“答案对错”奖励把数学推理提升到奥赛级别，验证了“规则可验证”奖励的 scaling 潜力。</li>
<li><strong>Group Sequence Policy Optimization, GSPO</strong>（Zheng et al., 2025a）将 PPO 从 token 级升级到序列级，缓解长度偏差，被本文直接采用。</li>
<li><strong>DrGRPO</strong>（Liu et al., 2025b）与 <strong>XVerify</strong>（Chen et al., 2025）分别给出符号级与模型级答案验证器，为本文混合 verifier 提供组件。</li>
</ul>
<ol>
<li>训练稳定性与探索策略</li>
</ol>
<ul>
<li><strong>Entropy Collapse 机制分析</strong>（Cui et al., 2025b）指出过易任务会加速策略熵崩溃，启发本文的“通过率过滤”策略。</li>
<li><strong>TIS（Truncated Importance Sampling）</strong>（Yao et al., 2025）纠正 rollout 与训练引擎不一致带来的梯度偏差，被本文用于稳定多阶段 RL。</li>
</ul>
<ol>
<li>测试时扩展与多智能体</li>
</ol>
<ul>
<li><strong>PhysicsMinions</strong>（Yu et al., 2025b）提出“逻辑-视觉-评审”三工作室循环，使模型在推理阶段自我批判、迭代修正；本文将其适配到纯文本场景。</li>
<li><strong>Test-Time Reinforcement Learning, TTRL</strong>（Zuo et al., 2025）用无标签测试集 + 多数投票伪标签做额外 RL，本文在附录给出初步复现。</li>
</ul>
<ol>
<li>科学 LLM 综述与基准</li>
</ol>
<ul>
<li><strong>A Survey of Scientific LLMs</strong>（Zhang et al., 2024）系统梳理科学发现场景下的语料、评测与模型架构，为本文数据构造提供分类依据。</li>
<li><strong>LiveBench、GPQA、IMO-Bench</strong> 等跨领域基准被用于验证物理后训练对数学、编程、通用 STEM 任务的泛化效果。</li>
</ul>
<ol>
<li>基础模型与系统框架</li>
</ol>
<ul>
<li><strong>Qwen3</strong>（Yang et al., 2025）提供 30B/235B MoE 基座，本文在其上执行纯 RL 后训练。</li>
<li><strong>slime</strong>（Zhu et al., 2025b）与 <strong>verl</strong>（Sheng et al., 2024）提供 Megatron-SGLang 一体化 RL 训练后端，支撑千卡级大规模实验。</li>
</ul>
<p>综上，本文在“科学级 RLVR”框架内，将奥赛数据构建、序列级 RL、训练稳定性、混合验证与测试时 agent 等前沿组件首次集成到同一开源系统，并突破 IPhO 金牌阈值，填补了开源模型在极限物理推理任务上的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“让开源大模型在奥赛级物理任务上达到金牌水平”拆解为<strong>数据-算法-系统-评测</strong>四位一体的工程问题，并给出对应解法。核心思路是：<strong>用可验证奖励的强化学习做训练时扩展，用多智能体批判循环做测试时扩展</strong>，二者协同以弥补纯参数 scaling 的不足。具体实施步骤如下：</p>
<ol>
<li>构建高质量、规则可验证的物理奥赛数据集</li>
</ol>
<ul>
<li>来源：10 项国际/区域奥赛 + 10 本权威竞赛教材，共 6 516 题。</li>
<li>流水线：OCR→人工重述→三模型交叉提取答案→SymPy/math-verify 符号等价检查→专家复核。</li>
<li>过滤：去掉需作图、答案不可验证或 pass@88 通过率∈{0,&gt;0.7} 的样本，最终保留 5 065 题，确保<strong>难度适中且零-learnability 陷阱</strong>。</li>
</ul>
<ol>
<li>设计“物理 MDP”与奖励函数</li>
</ol>
<ul>
<li>状态空间 S：题面 + 已生成推理链。</li>
<li>动作空间 A：词表 token。</li>
<li>转移 P：确定性追加 token。</li>
<li>奖励 r：</li>
</ul>
<p>r_i = 1 &amp; 第 i 个boxed{}答案与真值符号等价 0 &amp; 否则</p>
<p>多小问场景取平均： R=(1) / (N)∑_(i=1)^N r_i 。</p>
<ul>
<li>采用<strong>仅规则验证</strong>训练，避免模型 verifier 被策略“hack”导致假正例爆炸（§5.2）。</li>
</ul>
<ol>
<li><p>多阶段 RL 训练框架（train-time scaling）<br>3.1 算法：GSPO（序列级 PPO）+ TIS 重要性截断，缓解 rollout-训练引擎不一致。<br>3.2 自适应 learnability 调整<br>- <strong>通过率过滤</strong>：剔除过易/过难样本，防止零梯度或熵崩溃。<br>- <strong>探索空间渐进扩张</strong>：<br>– group 大小 16→32，提升稀有高分轨迹出现概率；<br>– 生成窗口 48 k→80 k，让模型逐步学会长链推理。<br>3.3 训练动力学<br>- 基础：Qwen3-30B-A3B / 235B-A22B Thinking 检查点。<br>- 三/四阶段训练，每阶段重启 checkpoint 并更新超参；平均响应长度随阶段显著增长，验证集 IPhO 分数同步提升，无平台期。</p>
</li>
<li><p>测试时多智能体批判循环（test-time scaling）</p>
</li>
</ol>
<ul>
<li>将 P1 模型嵌入 PhysicsMinions 框架：<br>– Logic Studio：P1 生成初始解；<br>– Review Studio：P1 自身担任 Physics-Verifier + General-Verifier，对单位、量纲、逻辑、计算做双阶段检查；<br>– 不通过则返回 introspector 重写，最多循环 CV=2 次。</li>
<li>仅文本模态，关闭 Visual Studio，保证公平对比。</li>
</ul>
<ol>
<li>泛化验证</li>
</ol>
<ul>
<li>在数学（AIME、IMO-Bench）、STEM（GPQA、HLE）、代码（LiveCodeBench）等 9 项基准上，P1-30B/235B 均<strong>全面优于对应基座模型</strong>，证明物理 RL 并未过拟合，反而成为通用推理放大器。</li>
</ul>
<ol>
<li>开源与复现</li>
</ol>
<ul>
<li>发布模型权重、训练代码、slime 框架适配、HiPhO 评测脚本，形成端到端“P1 生态”，供社区继续迭代。</li>
</ul>
<p>通过上述**“规则可验证奖励 + 序列级 RL + 自适应探索 + 测试时自批判”**的组合拳，论文首次让开源模型在 IPhO 2025 理论部分拿到 23.2/30 分，超越 Gemini-2.5-Pro 与 GPT-5，实现金牌零的突破。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“奥赛级物理推理”与“通用推理能力”两条主线，共设计并执行了 4 组实验，全部基于公开或新构建的基准，确保可复现与横向对比。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验组别</th>
<th>目的</th>
<th>数据集/基准</th>
<th>被测模型</th>
<th>关键指标</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 主评测：HiPhO 13 项奥赛</td>
<td>验证 P1 在最新物理奥赛上的绝对性能与奖牌分布</td>
<td>HiPhO（2024-2025 共 13 套真题，理论卷）</td>
<td>35 个模型（11 闭源 + 24 开源）含 P1 系列</td>
<td>官方奖牌分数线（金/银/铜）+ 平均 exam score</td>
<td>P1-235B-A22B 获 12 金 1 银，IPhO 2025 得分 21.2，首个开源金牌；P1-30B-A3B 8 金 4 银 1 铜，开源第三。</td>
</tr>
<tr>
<td>2. 代理增强 ablation</td>
<td>量化测试时批判循环带来的边际增益</td>
<td>同上</td>
<td>P1-235B-A22B vs P1-235B-A22B+PhysicsMinions</td>
<td>同上</td>
<td>代理系统平均得分 35.9→38.4，登顶总榜第一；在 IPhO、APhO、EuPhO、PanMechanics 四项刷新 SOTA。</td>
</tr>
<tr>
<td>3. 人类对照：CPhO 2025</td>
<td>检查模型是否超越顶级人类选手</td>
<td>2025 中国物理奥林匹克理论卷（7 题，320 分）</td>
<td>P1-235B-A22B（人工阅卷）</td>
<td>官方评分细则</td>
<td>模型 227 分，超第一名人类 199 分，达成金牌水平。</td>
</tr>
<tr>
<td>4. 跨领域泛化</td>
<td>判断物理 RL 后训练是否损害或提升通用推理</td>
<td>数学 6 项：AIME24/25、HMMT、IMO-AnswerBench、AMOBench、BeyondAIMESTEM 2 项：GPQA、HLE代码 1 项：LiveCodeBench通用 1 项：LiveBench</td>
<td>P1-30B-A3B vs 基座 Qwen3-30B-A3B-Thinking-2507P1-235B-A22B vs 基座 Qwen3-235B-A22B-Thinking-2507</td>
<td>各数据集官方 accuracy 或 pass@1</td>
<td>P1-30B 全面领先基座；P1-235B 在 9/10 项指标优于基座，平均提升 2–4 pp，证实物理 RL 成为通用推理放大器。</td>
</tr>
</tbody>
</table>
</div>
<p>此外，附录给出两项补充实验：</p>
<ul>
<li><strong>TTRL（Test-Time RL）</strong>：用无标签 HiPhO 测试集 + 多数投票伪标签，对 P1-30B-A3B 再做 64 步 GSPO，平均得分 27.6→28.4，展示推理阶段继续 RL 的潜在收益。</li>
<li><strong>Verifier Ablation</strong>：训练期引入模型 verifier（XVerify）反而导致响应长度暴涨、验证得分下降，<strong>反向验证“训练阶段仅用规则验证”的必要性</strong>。</li>
</ul>
<p>综上，实验覆盖<strong>单模型性能、代理增强、人类对照、跨域泛化、训练/测试时策略 ablation</strong>五个维度，形成完整证据链，支撑“P1 通过纯 RL 在奥赛物理任务上实现开源金牌并泛化到通用推理”的核心主张。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下问题在前序工作中尚未充分解决，可作为后续研究的直接切入点：</p>
<ul>
<li><p><strong>多模态物理推理</strong><br>当前 P1 仅支持文本，而奥赛真题普遍含示意图、实验曲线或几何配置。如何将视觉编码器与 P1 的 RL 训练流程端到端联合，使模型能“看图”并自动提取边界条件、对称性或守恒量，是解锁更高分数的关键。</p>
</li>
<li><p><strong>可解释物理定律发现</strong><br>论文仅验证“解题”能力，未要求模型输出<strong>新</strong>定律或数据驱动的未知关系。可设计“定律生成 + 实验验证”两阶段环境，让模型在符号回归或微分方程层面提出假设，再用数值模拟或真实实验反馈奖励，实现机器驱动的物理发现。</p>
</li>
<li><p><strong>更细粒度的奖励建模</strong><br>现用二进制“最终答案对错”奖励仍显稀疏。可引入：<br>– <strong>步骤级奖励</strong>：利用官方评分细则把中间公式、量纲检查、数值结果拆分为可验证子任务，构建稠密奖励信号。<br>– <strong>物理一致性奖励</strong>：自动监测能量/动量/电荷守恒、极限情况退化、对称性约束，一旦违反即给出负奖励，减少“撞对答案但推理荒谬”现象。</p>
</li>
<li><p><strong>可靠且可扩展的模型验证器</strong><br>论文表明训练阶段引入模型 verifier 会引入假正例。可探索：<br>– <strong>对抗校准</strong>：让策略与 verifier 互相对抗迭代，提升 verifier 鲁棒性；<br>– <strong>形式化方法</strong>：把符号计算（SymPy）与 SMT 求解器结合，实现<strong>可证明</strong>的表达式等价，降低误判率。</p>
</li>
<li><p><strong>跨学科迁移与元推理</strong><br>验证物理 RL 策略在化学、生物、天文等更宽自然科学上的零样本或少样本表现；同时研究模型能否<strong>自主识别所需学科工具</strong>（如量子力学 vs 统计物理），即“学科路由”元能力。</p>
</li>
<li><p><strong>测试时强化学习的理论极限</strong><br>附录初步尝试 TTRL。可深入分析：<br>– 无标签数据规模与伪标签精度之间的 scaling law；<br>– 多数投票共识作为奖励的偏差-方差权衡，给出最优采样次数的解析或经验估计。</p>
</li>
<li><p><strong>高效推理与边缘部署</strong><br>P1-235B-A22B 仍属大模型。探索：<br>– <strong>知识蒸馏</strong>：将大模型在物理任务上的策略蒸馏至 7B–13B 小模型，保持金牌水平；<br>– <strong>投机推理</strong>或<strong>早停策略</strong>：利用中间步骤置信度动态决定何时终止生成，减少 30–50 % 推理成本。</p>
</li>
<li><p><strong>公平评测与防作弊</strong><br>历年奥赛题公开后易混入训练语料。需建立<strong>去污染协议</strong>（如时间切分、嵌入相似度检测）并引入<strong>全新未公开试题</strong>或<strong>即时生成参数化题目</strong>，确保评分公正。</p>
</li>
<li><p><strong>人类-AI 协作式科研</strong><br>将 P1 嵌入真实研究流程：人类提出猜想→模型自动推导→实验验证→结果反馈回模型，形成<strong>闭环科研助手</strong>。评估指标包括“提出可验证新猜想数量”与“实验验证成功率”，而不仅是考试得分。</p>
</li>
<li><p><strong>伦理与安全</strong><br>模型已能超越人类金牌选手，需防范<strong>自主实验设计</strong>可能带来的实验风险或数据造假。建立<strong>科学推理 AI 的安全准则</strong>与<strong>可撤销约束接口</strong>，确保输出符合实验室安全规范与科研诚信。</p>
</li>
</ul>
<p>这些方向从<strong>模态、奖励、验证、跨域、理论、效率、伦理</strong>多层次延伸，既可推动物理 AI 的性能边界，也有助于构建可信赖、可解释、可扩展的下一代科学发现引擎。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心贡献一句话</strong>：首次用<strong>纯强化学习</strong>把<strong>开源大模型</strong>推到<strong>国际物理奥林匹克金牌线</strong>，并验证物理后训练可泛化到数学、代码等通用推理任务。</p>
<p>1. 背景与目标</p>
<ul>
<li>物理奥赛题 = 最严格的“科学级推理”试金石；模型必须输出<strong>符合自然定律</strong>的符号-数值答案。</li>
<li>开源社区在此任务上长期空白。</li>
</ul>
<p>2. 数据</p>
<ul>
<li><strong>5 065 道奥赛级文本题</strong>（81 % 真题 + 19 % 教材）。</li>
<li>全部<strong>规则可验证</strong>答案；人工+模型交叉清洗，去图、去证明、去污染。</li>
<li>英语单语，覆盖 5 大物理领域、25 子领域。</li>
</ul>
<p>3. 方法</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>关键技术</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>训练时 scaling</td>
<td>纯 RL 后训练（GSPO + 二进制答案奖励）自适应通过率过滤 + 探索空间渐进扩张（group/size &amp; 生成长度）Truncated Importance Sampling 解决 rollout-训练引擎不一致</td>
<td>让 30 B → 235 B 模型在 13 套最新奥赛题上持续提分</td>
</tr>
<tr>
<td>测试时 scaling</td>
<td>把 P1 嵌入 PhysicsMinions 三工作室循环（生成→自批判→修正）</td>
<td>235 B 模型平均分 35.9 → 38.4，总榜第一</td>
</tr>
</tbody>
</table>
</div>
<p>4. 结果</p>
<ul>
<li><strong>P1-235B-A22B</strong><br>– IPhO 2025：21.2 / 30，<strong>金牌线以上</strong>，开源唯一；13 项赛事共 12 金 1 银。<br>– CPhO 2025：227 / 320，<strong>超人类第一名 199 分</strong>。</li>
<li><strong>P1-30B-A3B</strong><br>– IPhO 2025：银牌；8 金 4 银 1 铜，<strong>开源排名第三</strong>，效率领先。</li>
<li><strong>泛化</strong><br>– 在 6 项数学、2 项 STEM、1 项代码、1 项通用推理基准上，P1 系列<strong>全面优于对应基座模型</strong>，平均提升 2–4 pp。</li>
</ul>
<p>5. 结论</p>
<ul>
<li>纯 RL + 规则奖励即可让模型掌握复杂物理推理，无需人工示范解。</li>
<li>物理任务成为<strong>通用推理放大器</strong>，而非窄域过拟合。</li>
<li>开源释放模型+数据+训练+评测全套流水线，为社区提供可复现的科学推理基线。</li>
</ul>
<p>6. 未来提示</p>
<p>多模态、步骤级奖励、可解释定律发现、高效边缘部署、跨学科迁移、安全伦理等方向仍待深入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是P1%3A%20Mastering%20Physics%20Olympiads%20with%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2511.13612，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13612。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jiacheng Chen,Qianjia Cheng,Fangchen Yu,Haiyuan Wan,Yuchen Zhang,Shenghe Zheng,Junchi Yao,Qingyang Zhang,Haonan He,Yun Luo,Yufeng Zhao,Futing Wang,Li Sheng,Chengxing Xie,Yuxin Zuo,Yizhuo Li,Wenxauan Zeng,Yulun Wu,Rui Huang,Dongzhan Zhou,Kai Chen,Yu Qiao,Lei Bai,Yu Cheng,Ning Ding,Bowen Zhou,Peng Ye,Ganqu Cui</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13612.pdf">https://arxiv.org/pdf/2511.13612.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13612">https://arxiv.org/abs/2511.13612</a></p>
<p><strong>Arxiv ID</strong>: 2511.13612</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13612">https://papers.cool/arxiv/2511.13612</a></p>
<p><strong>Published</strong>: 2025-11-17T17:18:13Z</p>
<p><strong>Updated</strong>: 2025-11-17T17:18:13.000Z</p>
<hr>
<h3 id="2-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data"><a href="#2-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data" class="headerlink" title="2. Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.12609">Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</a></h3><p>We present Uni-MoE 2.0 from the Lychee family. As a fully open-source omnimodal large model (OLM), it substantially advances Lychee’s Uni-MoE series in language-centric multimodal understanding, reasoning, and generating. Based on the Qwen2.5-7B dense architecture, we build Uni-MoE-2.0-Omni from scratch through three core contributions: dynamic-capacity Mixture-of-Experts (MoE) design, a progressive training strategy enhanced with an iterative reinforcement strategy, and a carefully curated multimodal data matching technique. It is capable of omnimodal understanding, as well as generating images, text, and speech. Architecturally, our new MoE framework balances computational efficiency and capability for 10 cross-modal inputs using shared, routed, and null experts, while our Omni-Modality 3D RoPE ensures spatio-temporal cross-modality alignment in the self-attention layer. For training, following cross-modal pretraining, we use a progressive supervised fine-tuning strategy that activates modality-specific experts and is enhanced by balanced data composition and an iterative GSPO-DPO method to stabilise RL training and improve reasoning. Data-wise, the base model, trained on approximately 75B tokens of open-source multimodal data, is equipped with special speech and image generation tokens, allowing it to learn these generative tasks by conditioning its outputs on linguistic cues. Extensive evaluation across 85 benchmarks demonstrates that our model achieves SOTA or highly competitive performance against leading OLMs, surpassing Qwen2.5-Omni (trained with 1.2T tokens) on over 50 of 76 benchmarks. Key strengths include video understanding (+7% avg. of 8), omnimodallity understanding (+7% avg. of 4), and audiovisual reasoning (+4%). It also advances long-form speech processing (reducing WER by 4.2%) and leads in low-level image processing and controllable generation across 5 metrics.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们推出了荔枝系列的 Uni-MoE 2.0。作为一个完全开源的全模态大型模型（OLM），它在以语言为中心的多模态理解、推理和生成方面显著提升了荔枝 Uni-MoE 系列的能力。基于 Qwen2.5-7B 密集架构，我们从零构建了 Uni-MoE-2.0-Omni，通过三大核心贡献实现：动态容量的专家混合（MoE）设计、结合迭代增强策略的渐进训练方法，以及精心策划的多模态数据匹配技术。该模型具备全模态理解能力，同时可生成图像、文本和语音。在架构上，我们的新 MoE 框架通过共享、路由和空专家在处理 10 种跨模态输入时实现计算效率与能力的平衡，而我们的全模态 3D RoPE 则确保在自注意力层中时空跨模态对齐。在训练方面，在跨模态预训练之后，我们采用渐进的监督微调策略，激活特定模态专家，并通过平衡的数据构成和迭代 GSPO-DPO 方法强化，以稳定 RL 训练并提升推理能力。在数据方面，基础模型在约 750 亿 tokens 的开源多模态数据上训练，配有特殊的语音和图像生成 tokens，使其能够通过语言提示学习这些生成任务。通过 85 个基准测试的广泛评估显示，我们的模型在性能上达到或高度接近 SOTA，相比以 1.2T tokens 训练的 Qwen2.5-Omni，在 76 个基准中的 50 多个表现优异。主要优势包括视频理解（8 个任务平均 +7%）、全模态理解（4 个任务平均 +7%）及视听推理（+4%）。它还在长语音处理方面取得进展（将 WER 降低 4.2%），并在低级图像处理及可控生成的 5 个指标上处于领先地位。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该工作旨在构建一个<strong>完全开源、以语言为中心的万能模态大模型（OLM）</strong>，在单一架构内同时实现文本、图像、音频、视频等多模态的<strong>深度理解、推理与高质量生成</strong>。论文指出当前领域存在两大核心痛点：</p>
<ol>
<li><p><strong>理解-生成割裂</strong><br>现有系统往往偏重一端：要么只做多模态理解（如 Qwen-Omni、Baichuan-Omni），要么仅支持单一或少数模态的生成（如 OmniGen、Janus-Pro），难以在统一框架内兼顾语义理解与内容生成。</p>
</li>
<li><p><strong>密集 Transformer 低效扩展</strong><br>简单增大密集模型参数会带来<strong>计算成本爆炸</strong>，且无法根据任务动态分配容量，导致数十种跨模态任务难以同时优化，训练过程也容易因异构数据而失稳。</p>
</li>
</ol>
<p>为此，作者提出 Uni-MoE-2.0-Omni，通过三项关键设计实现“从 LLM 到 OLM”的高效跃迁：</p>
<ul>
<li><p><strong>动态容量混合专家（Dynamic-Capacity MoE）</strong><br>将 FFN 层扩展为包含共享专家、路由专家与空专家的 MoE 结构，利用 Top-P 路由+梯度估计，使模型能<strong>按需激活不同数量的专家</strong>，在推理时跳过无关计算，实现“理解-生成”一体化且保持高效。</p>
</li>
<li><p><strong>渐进式训练 + 迭代强化策略（GSPO-DPO）</strong><br>采用“跨模态预对齐→专家预热→MoE 微调→生成训练”四阶段渐进 recipe，并在强化学习阶段引入 GSPO 在线探索与 DPO 偏好优化，<strong>稳定大规模异构数据训练</strong>，显著提升推理能力。</p>
</li>
<li><p><strong>万能模态 3D RoPE 与统一 Token 化</strong><br>在自注意力层引入 3D 位置编码，统一为文本、语音、图像、视频 token 分配时-空坐标，实现<strong>跨模态细粒度对齐</strong>；同时设计文本驱动的图像/语音生成 token，使语言模型可直接输出用于扩散或 vocoder 的条件，完成端到端生成。</p>
</li>
</ul>
<p>实验表明，仅用约 75 B 多模态 token 训练，Uni-MoE-2.0-Omni 在 85 项基准上<strong>超过参数量更大、训练数据更多的 Qwen2.5-Omni（1.2 T token）等 50 余项指标</strong>，在视频理解、长语音处理、音视频推理、可控图像生成等任务中取得新 SOTA，验证了其“语言为中心、MoE 驱动”的万能模态架构的有效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 Uni-MoE-2.0-Omni 直接可比或构成其技术基线的相关研究，可按“<strong>万能模态大模型</strong>”“<strong>MoE 多模态架构</strong>”“<strong>多模态生成</strong>”三条主线梳理如下：</p>
<p>1. 万能模态大模型（Omni-Modal LLM）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>核心特点</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen2.5-Omni (Xu et al., 2025)</td>
<td>工业界首个 7B 级万能模态 dense 模型，支持文本/图/音/视理解与语音合成，训练 1.2 T token</td>
<td>主要对标对象，Uni-MoE-2.0 在 50+/76 项基准上超越</td>
</tr>
<tr>
<td>Ming-Lite-Omni-1.5 (AI et al., 2025)</td>
<td>基于 Ming-7B 的 dense omni 模型，强调流式语音对话</td>
<td>视频、语音任务强基线，Uni-MoE-2.0 平均领先 4%</td>
</tr>
<tr>
<td>Baichuan-Omni-1.5 (Li et al., 2025b)</td>
<td>10B dense 结构，采用双编码器-单解码器框架</td>
<td>OmniBench 第二名的强对手</td>
</tr>
<tr>
<td>MiniCPM-o 2.6 (未正式发表)</td>
<td>8B dense 模型，侧重端侧部署</td>
<td>在 MMBench、MMMU 等榜单与本文互有胜负</td>
</tr>
<tr>
<td>GPT-4o (Hurst et al., 2024)</td>
<td>闭源 SOTA，支持实时音视频对话</td>
<td>能力上限参考，开源社区无参数/数据细节</td>
</tr>
<tr>
<td>Gemini-2.5-Flash (Comanici et al., 2025)</td>
<td>闭源，用于本文 DPO 阶段“教师”标注</td>
<td>提供高质推理链数据</td>
</tr>
</tbody>
</table>
</div>
<p>2. MoE 多模态架构</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>3. 多模态生成与统一框架</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>代表工作</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像生成</td>
<td>OmniGen (Wu et al., 2025)、Janus-Pro (Chen et al., 2025a)、Show-o (Xie et al., 2025)</td>
<td>仅图像域，无语音/视频；端到端微调易干扰理解能力。Uni-MoE-2.0 用语言 token 驱动外部 DiT，避免灾难遗忘</td>
</tr>
<tr>
<td>语音合成</td>
<td>CosyVoice 2、GLM-4-Voice、MaskGCT</td>
<td>专注 TTS，不支持图文。本文提出上下文感知 MoE-TTS，与 LLM 共享语义空间</td>
</tr>
<tr>
<td>统一 token 化</td>
<td>Meta-Transformer (Zhang et al., 2023c)、Unified-IO-2 (Lu et al., 2024)</td>
<td>将不同模态离散为统一 token，但采用 dense 结构，无动态专家分配</td>
</tr>
</tbody>
</table>
</div>
<p>4. 训练策略与数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术点</th>
<th>相关文献</th>
<th>本文提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>渐进式多模态训练</td>
<td>Flamingo (Alayrac et al., 2022)、LLaVA-Interleave (Li et al., 2024a)</td>
<td>两/三阶段对齐。Uni-MoE-2.0 细化出“预热→MoE 微调→生成→退火”四阶段，并首次在 MoE-omni 场景验证 RL 稳定性</td>
</tr>
<tr>
<td>迭代 RL + DPO</td>
<td>VerIPO (Li et al., 2025c)、R1-VL (Huang et al., 2025)</td>
<td>本文将 GSPO 群策略优化与 DPO 偏好学习级联，用于多模态推理链增强</td>
</tr>
<tr>
<td>多模态数据配比</td>
<td>Cambrian-10M、ShareGPT4Video、WavCaps 等开源集合</td>
<td>论文在 75 B token 规模下重新清洗并平衡 图/视/音/文比例，退火阶段实现“样本级”均衡采样</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<p>Uni-MoE-2.0-Omni 的“相关研究”图谱可概括为：</p>
<ul>
<li><strong>横向对标</strong>：Qwen2.5-Omni、Ming-Lite-Omni 等 dense-OLM，验证 MoE 在同等参数下的效率优势；</li>
<li><strong>纵向技术</strong>：Grin-MoE 的梯度估计、Uni-MoE 1.0 的多模态 MoE、各类专用生成模型，被整合进统一的“动态容量 MoE + 渐进训练”框架；</li>
<li><strong>数据与训练策略</strong>：吸收近期开源大规模图文-音视频数据集，并首次将迭代 RL（GSPO-DPO）成功应用于 MoE-OLM 训练，解决异构数据不稳定问题。</li>
</ul>
<p>这些工作共同构成了 Uni-MoE-2.0-Omni 的学术基线和创新起点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从<strong>架构、训练、数据</strong>三个维度系统性地解决“万能模态大模型”面临的<strong>理解-生成割裂</strong>与<strong>密集模型扩展低效</strong>两大核心难题。具体方案可概括为“<strong>一个语言为中心的 MoE 骨架 + 两条渐进训练路径 + 三类数据配比策略</strong>”。</p>
<p>1. 架构：语言为中心的动态容量 MoE</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>关键设计</th>
<th>解决痛点</th>
</tr>
</thead>
<tbody>
<tr>
<td>Omni-Modality 3D RoPE(§2.3.1)</td>
<td>将旋转位置编码解耦为 (T, H, W) 三维，文本/语音/图像/视频 token 统一映射到同一时空坐标系</td>
<td>消除模态间位置语义冲突，实现细粒度跨模态对齐</td>
</tr>
<tr>
<td>Dynamic-Capacity MoE(§2.3.2)</td>
<td>把传统 FFN 替换为“共享专家 + 路由专家 + 空专家”三元组；采用 Top-P 路由（累积概率≥0.7）替代固定 Top-K，并引入 ODE 梯度估计使离散选择可微</td>
<td>① 按 token 复杂度动态增减专家数，推理期可跳过空专家，计算节省 20-40%；② 梯度可反传，路由与专家联合优化，缓解“专家崩塌”</td>
</tr>
<tr>
<td>统一 Token 化(§2.2)</td>
<td>语音：Whisper-large-v3 → 20 token/3s；图像：SigLIP 384×384 滑窗 → 每 patch T 个 token；视频：1 fps 采样 → 帧级 token 序列</td>
<td>把异构信号压成一维 token 流，直接喂给 Qwen2.5-7B 骨干，无需额外大 backbone</td>
</tr>
<tr>
<td>生成外挂(§2.4)</td>
<td>文本侧输出专用控制 token：<speech start> lang=EN timbre=Jenny <speech prompt> … <speech end>驱动 MoE-TTS（1.2B）或 Task-DiT（1.5B）扩散模型</td>
<td>理解与生成解耦：基础 LLM 只负责“语言规划”，高保真合成由小模型完成，避免 catastrophic forgetting</td>
</tr>
</tbody>
</table>
</div>
<p>2. 训练：四阶段渐进 + 迭代强化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>目标 &amp; 数据</th>
<th>关键技术</th>
</tr>
</thead>
<tbody>
<tr>
<td>① 跨模态预对齐</td>
<td>图-文 13B + 音-文 16B token，仅训练 MLP/Q-Former</td>
<td>让 LLM 看得懂、听得懂，但不说也不画</td>
</tr>
<tr>
<td>② 专家预热</td>
<td>分别用 19B 图、5B 音、9B 视频数据预训练三个 dense 专家</td>
<td>为后续 MoE 提供初始化权重，防止冷启动随机路由</td>
</tr>
<tr>
<td>③ MoE 微调 + 混合数据</td>
<td>22B 图 + 19B 视频 + 8B 音频 + 1B 文本，同时激活路由/共享专家</td>
<td>① 采用平衡采样：每 batch 四模态比例 1:1:1:1；② 空专家权重加入 L0 正则，鼓励遗忘冗余知识</td>
</tr>
<tr>
<td>④ 生成训练</td>
<td>冻结 LLM，仅更新– MoE-TTS（2B token 多风格 TTS）– Task-DiT（1.5B token 图生/图编）</td>
<td>外挂式微调，保持理解能力不变，快速获得高保真合成</td>
</tr>
<tr>
<td>⑤ 迭代 RL（GSPO-DPO）</td>
<td>先用 5k 冷启动思维链 → GSPO 在线探索 → 用 Gemini-2.5-Flash 标注正负例 → DPO 偏好优化</td>
<td>解决“多模态推理奖励稀疏”问题，MathVista 提升 5%</td>
</tr>
</tbody>
</table>
</div>
<p>3. 数据：75 B token 精洗 + 样本级平衡</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模态</th>
<th>预训练</th>
<th>微调/退火</th>
<th>关键处理</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像</td>
<td>17 M 图文对（PixelProse/CC3M/GRIT）</td>
<td>5 M 高质量子集（Cambrian-10M、Docmatix、V*）</td>
<td>分辨率自适应填充 + 重复图文过滤，OCR 数据占比刻意压低→解释 DocVQA 差距</td>
</tr>
<tr>
<td>视频</td>
<td>0.1 M 视频-文本（Valley/ShareGPT4Video）</td>
<td>扩至 21 B token（FineVideo、Neptune、EgoTaskQA 等）</td>
<td>音频轨道清晰度过滤 → 保证音视同步训练</td>
</tr>
<tr>
<td>音频</td>
<td>26 M 小时级 ASR（GigaSpeech、MLS）+ 1 B 音乐/环境音</td>
<td>5 B token 多任务（ASR、AQA、情绪、TTS）</td>
<td>30 s 滑窗 + 200 token 固定长度，长语音自动切块</td>
</tr>
<tr>
<td>文本</td>
<td>—</td>
<td>1 B→4 B token（OpenOrca、MMLU-Pro、DAPO-Math）</td>
<td>退火阶段数学+代码占比 40%，提升 GPQA</td>
</tr>
</tbody>
</table>
</div>
<p>4. 结果验证：85 项基准全面领先</p>
<ul>
<li><strong>视频理解</strong>：Video-MME 66.4（+6.6 vs Qwen2.5-Omni），VSI-Bench 56.0（+36.7）。</li>
<li><strong>长语音 ASR</strong>：LibriSpeech-other-long WER 4.2↓（Qwen2.5-Omni 7.98）。</li>
<li><strong>图像编辑</strong>：GEdit-Bench 6.02，比 PixWizard 提升 88%。</li>
<li><strong>可控生成</strong>：Canny-to-Image FID 18.23，低于 Qwen-Image 37.59。</li>
<li><strong>推理增强</strong>：MathVista 63.9（+3.1 vs 无 thinking 版），证明 GSPO-DPO 有效。</li>
</ul>
<p>一句话总结</p>
<p>论文通过“<strong>动态容量 MoE 提供按需计算 + 渐进式训练稳定异构数据 + 语言驱动外挂生成</strong>”三位一体策略，首次在 7B 级开源模型里同时实现<strong>万能模态理解、推理与高质量生成</strong>，并以 75 B token 的小成本超越 1.2 T token 的工业级 dense 模型。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>85 个公开基准</strong> 上组织了 <strong>7 大能力维度、20 余子任务</strong> 的系统评测，覆盖<br>“看-听-说-画-思”全链路。实验设计遵循 <strong>“理解→生成→跨模态→推理”</strong> 递进关系，并辅以 <strong>MoE 行为可视化</strong> 与 <strong>Thinking 消融</strong> 分析，具体如下：</p>
<p>1. 视觉-语言理解（22 基准）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表基准</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>通用图像理解</td>
<td>MMBench-EN/CN、MMStar、GQA、RealWorldQA</td>
<td>与 Qwen2.5-Omni 打平或略胜，GQA 62.18 刷新开源纪录</td>
</tr>
<tr>
<td>STEM 推理</td>
<td>MathVista、MathVision、MMMU、AI2D</td>
<td>MathVision 36.61 领先第二名 19+ 分；MMMU-Pro 仍落后，归因于科学图数量不足</td>
</tr>
<tr>
<td>文档 &amp; OCR</td>
<td>DocVQA、ChartQA、CharXiv、SEED-Bench-2-Plus</td>
<td>相比专精模型（Baichuan-Omni-1.5）低 8-15 分，验证数据稀缺性影响</td>
</tr>
<tr>
<td>视频理解</td>
<td>Video-MME、MVBench、VSI-Bench、LongVideoBench、EgoSchema 等 8 项</td>
<td>平均 50.6 分，领先最强 Ming-Lite-1.5 4.0 分；VSI-Bench 领先 36.7%</td>
</tr>
</tbody>
</table>
</div>
<p>2. 音频-语言理解 &amp; 语音生成（18 基准）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>基准</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>ASR</td>
<td>LibriSpeech-clean/other、Aishell1/2、MLS-en、CV15</td>
<td>clean 1.66 WER 刷新 omni 模型纪录；&gt;3 min 长语音 other-long WER 4.2↓（Qwen2.5-Omni 7.98）</td>
</tr>
<tr>
<td>音频理解</td>
<td>ClothoAQA、AudioCaps、MMAU-Speech/Sound/Music</td>
<td>RACE-audio 89.7 分；MusicCaps CIDEr 62.4 远高 Qwen2.5-Omni 4.0，证明音乐caption 数据清洗有效</td>
</tr>
<tr>
<td>TTS</td>
<td>LibriTTS、SEED-hard、TinyStories-en/zh</td>
<td>LibriTTS-clean 5.85 WER 优于 Ming-Lite 11.15；SEED-hard 2.67 仅次于 SOTA 专业 TTS</td>
</tr>
<tr>
<td>语音对话</td>
<td>LlamaQA、WebQA、BigBench-Audio、MultiChallenge-Audio</td>
<td>s→s 平均 44.7 分，与文本通道差距仅 1.2 分，显示语音端到端推理能力</td>
</tr>
</tbody>
</table>
</div>
<p>3. 万能模态理解（4 基准）</p>
<ul>
<li><strong>WorldSense、OmniVideoBench、StreamingBench、OmniBench</strong><br><strong>综合 43.7% 准确率，领先第二名 Baichuan-Omni-1.5 1.8%</strong>，在长视频音视同步问答上优势最大。</li>
</ul>
<p>4. 图像生成与编辑（12 基准）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>指标</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>纯生成</td>
<td>Wise↑ / FID↓</td>
<td>0.44 / 18.04，优于 Janus-Pro、Bagel；仍低于 Qwen-Image，但参数仅其 1/3</td>
</tr>
<tr>
<td>编辑</td>
<td>GEdit-Bench↑ / Emu-Edit↑</td>
<td>6.02 / 0.076，比 PixWizard 提升 88% / 94%</td>
</tr>
<tr>
<td>可控生成</td>
<td>Canny-to-Image FID↓</td>
<td>18.23，低于 Qwen-Image 37.59 与 OmniGen2 45.67</td>
</tr>
<tr>
<td>低层修复</td>
<td>Derain PSNR↑ / Denoise PSNR↑</td>
<td>25.41 / 25.70，Denoise 领先 Qwen-Image 15.8%</td>
</tr>
</tbody>
</table>
</div>
<p>5. MoE 行为分析（可视化）</p>
<ul>
<li><strong>专家激活热力图</strong>（图 7）<br>浅层共享，深层分化：Expert-1 主导视觉，Expert-2/3 主导音频，Expert-4 通用语义，<strong>空专家 E5 在中层激活率提升 3×</strong>，验证“选择性遗忘”与计算节省。</li>
<li><strong>动态预算曲线</strong>（图 8）<br>出现“<strong>双峰一谷</strong>”模式：早期与深层 1-2 专家/token，中间复杂推理层 3-4 专家/token，<strong>整体计算量下降 28%</strong> 而精度不降。</li>
<li><strong>训练过程演化</strong>（图 9）<br>仅中层 9-18 层路由分布在 200 k step 内显著变化，<strong>空专家比例持续上升</strong>，表明模型学会<strong>跳过已充足特征</strong>的 token。</li>
</ul>
<p>6. Thinking vs. No-Thinking 消融</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>0</p>
<ul>
<li><strong>GSPO 在线探索</strong> 恢复冷启动掉点；<strong>DPO 用 Gemini-2.5-Flash 标注 6 k 偏好样本</strong> 带来 <strong>MathVista +5.0</strong> 最终收益。</li>
<li><strong>图像生成</strong> 侧，引入 <code>&lt;think&gt;</code> 链式思考后，<strong>语义一致性人工评分 +18%</strong>（图 10），错误率从 26%→8%。</li>
</ul>
<p>7. 效率与扩展实验</p>
<ul>
<li><strong>激活参数</strong> 1.5 B–18 B 动态区间，<strong>平均每 token 仅 4.2 B</strong>（dense 同级 7 B）。</li>
<li><strong>推理吞吐</strong> 比 dense-Qwen2.5-7B 提升 <strong>1.6×</strong>（相同 A100-80 GB，batch=32）。</li>
<li><strong>数据缩放曲线</strong>：75 B token 即趋于饱和，<strong>验证 MoE 在有限数据下的高效性</strong>。</li>
</ul>
<p>实验结论一览</p>
<ol>
<li><strong>视频+长语音</strong> 取得新 SOTA，验证 3D-RoPE 时序对齐与 MoE 长序列能力。</li>
<li><strong>图像编辑/修复</strong> 领先专业模型，表明“语言驱动生成 token”策略可无缝集成低级视觉任务。</li>
<li><strong>MoE 可视化</strong> 首次揭示 omni 任务在中层出现“专家分工+空专家跳过”的普遍规律。</li>
<li><strong>Thinking 训练</strong> 在多模态推理和生成上均带来显著增益，证明 RL+偏好学习可迁移到 MoE-OLM。</li>
</ol>
<p>整套实验链条完整展示了 Uni-MoE-2.0-Omni 从<strong>理解</strong>到<strong>生成</strong>、从<strong>数据效率</strong>到<strong>推理能力</strong>的全方位优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望基于论文第 5、6 节与实验观察，可归纳为 <strong>“模型-数据-训练-评测”</strong> 四大方向的 <strong>12 个可深入探索点</strong>：</p>
<p>1. 模型架构与参数分配</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>1</p>
<p>2. 数据与模态</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>2</p>
<p>3. 训练策略与优化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>3</p>
<p>4. 评测与应用</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>4</p>
<p>短期（3-6 个月）可落地</p>
<ul>
<li>条件式路由（2）+ 音乐数据增强（5）→ 预期 <strong>MusicCaps +10 分</strong>，推理加速 <strong>15%</strong>。</li>
<li>分层 RL（9）+ 思考长度控制（11）→ <strong>MathVista 再 +2~3 分</strong>，生成延迟 <strong>-20%</strong>。</li>
</ul>
<p>中长期（6-18 个月）</p>
<ul>
<li>统一音频 tokenizer（1）+ 空间音频视频数据（7）→ 实现 <strong>“谁在哪个像素位置说话”</strong> 的 omni 定位。</li>
<li>专家擦除（4）+ 多语种（8）→ 打造 <strong>可遗忘、可扩展</strong> 的低资源 omni 模型，服务 <strong>隐私合规+全球化</strong> 场景。</li>
</ul>
<p>以上方向既可直接在 Uni-MoE-2.0 的开源代码与数据清单上迭代，也能为 omni 大模型社区提供新的评测与数据基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Uni-MoE-2.0-Omni</strong>，一个<strong>完全开源、以语言为中心的万能模态大模型</strong>（OLM），在单一 7B MoE 骨架内同时实现文本、图像、音频、视频的理解、推理与生成。核心贡献与结果可概括为 <strong>“一个架构、两套策略、三类数据、四项突破”</strong>：</p>
<p>① 一个架构：动态容量 MoE + 3D-RoPE</p>
<ul>
<li><strong>共享/路由/空专家三元组</strong>，Top-P 路由按需激活 1.5-18 B 参数，推理节省 28% 计算。</li>
<li><strong>Omni-Modality 3D-RoPE</strong> 统一时-空位置编码，实现跨模态细粒度对齐。</li>
<li><strong>语言驱动生成 token</strong>：文本输出控制信号，外挂 MoE-TTS 与 Task-DiT 完成高保真语音/图像合成，避免灾难遗忘。</li>
</ul>
<p>② 两套训练策略</p>
<ol>
<li><strong>四阶段渐进</strong><br>跨模态预对齐 → 专家预热 → MoE 混合微调 → 生成外挂微调，保证稳定收敛。</li>
<li><strong>迭代强化 GSPO-DPO</strong><br>冷启动思维链 → 在线群体策略优化 → 商用模型标注偏好 → DPO，MathVista 提升 5%，图像生成一致性 +18%。</li>
</ol>
<p>③ 三类数据配比（共 75 B token）</p>
<ul>
<li><strong>图-文</strong> 17 M → 5 M 高质量子集，<strong>视频-文本</strong> 0.1 M → 21 B token，<strong>音频-文本</strong> 26 M 小时 → 5 B 多任务，<strong>纯文本</strong> 1 B→4 B STEM。</li>
<li>退火阶段<strong>样本级平衡</strong>，确保四模态 1:1:1:1，缓解数据稀缺领域（音乐、OCR）性能差距。</li>
</ul>
<p>④ 四项实验突破</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>技术要点</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>Grin-MoE (Liu et al., 2024a)</td>
<td>提出 ODE 数值梯度估计解决 Top-K 不可微问题</td>
<td>Uni-MoE-2.0 路由梯度估计的直接基线</td>
</tr>
<tr>
<td>Uni-MoE 1.0 (Li et al., 2025d)</td>
<td>首次将 dense-LLM 扩展为 multimodal-MoE，仅理解无生成</td>
<td>本文的“前身”，2.0 新增生成、3D-RoPE、动态容量路由</td>
</tr>
<tr>
<td>MegaBlocks (Norick et al., 2022) / Fairseq-MoE</td>
<td>早期稀疏激活实现，专家数固定</td>
<td>对比说明固定容量 vs. 动态 Top-P 的灵活性差距</td>
</tr>
<tr>
<td>Switch-Transformer (Fedus et al., 2022)</td>
<td>Top-1 路由，专家容量恒定</td>
<td>被本文“Top-P + 空专家”机制针对的局限性工作</td>
</tr>
</tbody>
</table>
</div>
<p>5</p>
<p>结论</p>
<p>Uni-MoE-2.0-Omni 用 <strong>75 B token 的小成本</strong> 实现 <strong>1.2 T token 级工业 dense 模型</strong> 的跨模态能力，并在视频、长语音、图像编辑、万能模态理解等任务上刷新 <strong>50+ 项开源纪录</strong>，为社区提供了可复现的代码、模型与数据清单，验证了 <strong>“语言为中心 + 动态 MoE”</strong> 构建万能模态大模型的可行性与高效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Uni-MoE-2.0-Omni%3A%20Scaling%20Language-Centric%20Omnimodal%20Large%20Model%20with%20Advanced%20MoE%2C%20Training%20and%20Data，链接是https://arxiv.org/pdf/2511.12609，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.12609。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yunxin Li,Xinyu Chen,Shenyuan Jiang,Haoyuan Shi,Zhenyu Liu,Xuanyu Zhang,Nanhao Deng,Zhenran Xu,Yicheng Ma,Meishan Zhang,Baotian Hu,Min Zhang</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12609.pdf">https://arxiv.org/pdf/2511.12609.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12609">https://arxiv.org/abs/2511.12609</a></p>
<p><strong>Arxiv ID</strong>: 2511.12609</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12609">https://papers.cool/arxiv/2511.12609</a></p>
<p><strong>Published</strong>: 2025-11-16T14:10:55Z</p>
<p><strong>Updated</strong>: 2025-11-16T14:10:55.000Z</p>
<hr>
<h3 id="3-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling"><a href="#3-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling" class="headerlink" title="3. MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.11793">MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</a></h3><p>We present MiroThinker v1.0, an open-source research agent designed to advance tool-augmented reasoning and information-seeking capabilities. Unlike previous agents that only scale up model size or context length, MiroThinker explores interaction scaling at the model level, systematically training the model to handle deeper and more frequent agent-environment interactions as a third dimension of performance improvement. Unlike LLM test-time scaling, which operates in isolation and risks degradation with longer reasoning chains, interactive scaling leverages environment feedback and external information acquisition to correct errors and refine trajectories. Through reinforcement learning, the model achieves efficient interaction scaling: with a 256K context window, it can perform up to 600 tool calls per task, enabling sustained multi-turn reasoning and complex real-world research workflows. Across four representative benchmarks-GAIA, HLE, BrowseComp, and BrowseComp-ZH-the 72B variant achieves up to 81.9%, 37.7%, 47.1%, and 55.6% accuracy respectively, surpassing previous open-source agents and approaching commercial counterparts such as GPT-5-high. Our analysis reveals that MiroThinker benefits from interactive scaling consistently: research performance improves predictably as the model engages in deeper and more frequent agent-environment interactions, demonstrating that interaction depth exhibits scaling behaviors analogous to model size and context length. These findings establish interaction scaling as a third critical dimension for building next-generation open research agents, complementing model capacity and context windows.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们推出了 MiroThinker v1.0，这是一个开源研究智能体，旨在提升工具增强推理和信息获取能力。与以往仅提升模型规模或上下文长度的智能体不同，MiroThinker 探索了模型层面的交互扩展，系统地训练模型以处理更深层次和更频繁的智能体-环境交互，作为性能提升的第三个维度。不同于孤立操作且在长推理链中可能退化的 LLM 测试时扩展，交互扩展利用环境反馈和外部信息获取来纠正错误并优化轨迹。通过强化学习，模型实现了高效的交互扩展：在 256K 上下文窗口下，它每个任务可执行多达 600 次工具调用，从而支持持续的多轮推理和复杂的现实研究工作流程。在四个具有代表性的基准测试——GAIA、HLE、BrowseComp 和 BrowseComp-ZH 中，72B 版本分别达到了 81.9%、37.7%、47.1% 和 55.6% 的准确率，超越了之前的开源智能体，并接近 GPT-5-high 等商业对手。我们的分析显示，MiroThinker 始终从交互扩展中受益：随着模型参与更深层次、更频繁的智能体-环境交互，研究性能可预测地提升，表明交互深度表现出类似于模型规模和上下文长度的扩展特性。这些发现确立了交互扩展作为构建下一代开放研究智能体的第三个关键维度，补充了模型容量和上下文窗口。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在弥合开源与闭源研究智能体之间的性能鸿沟，提出并验证“交互缩放（interactive scaling）”作为继模型规模、上下文长度之后的第三大性能维度。核心待解决问题可归纳为：</p>
<ul>
<li>现有开源研究智能体普遍受限于</li>
</ul>
<ol>
<li>模型尺度不足</li>
<li>上下文窗口过短</li>
<li>单次任务可执行的工具调用次数过少（&lt;100）<br>导致其在复杂、多跳、需反复验证的现实研究任务上显著落后于 GPT-5-high、Claude Research 等闭源系统。</li>
</ol>
<ul>
<li>传统“测试时缩放”仅延长模型内部推理链，缺乏外部反馈，随着链长增加易出现累积错误；而“交互缩放”通过强化学习让模型在训练阶段就学会高频、深度地与外部环境（搜索、代码沙盒、文件系统等）交互，以实时获取信息、纠正错误、优化求解轨迹。</li>
<li><p>因此，论文目标是通过同时扩大</p>
</li>
<li><p>模型参数（8B→30B→72B）</p>
</li>
<li>上下文长度（256K tokens）</li>
<li>单任务工具调用上限（≈600 次）<br>并在强化学习框架下系统训练，使开源智能体在 GAIA、HLE、BrowseComp 等基准上逼近甚至超越商业系统，从而验证“交互深度”本身具有可预测的规模效应。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可划分为三条主线，均围绕“让大模型具备自主研究能力”展开：</p>
<ol>
<li>Agent Foundation Models（AFMs）<br>在基座预训练阶段即植入工具使用与决策能力，代表工作：</li>
</ol>
<ul>
<li>GPT-5、Claude-4.5、Grok-3</li>
<li>开源：Kimi K2、MiniMax-M2、GLM-4.6、DeepSeek-V3.1</li>
</ul>
<ol>
<li>Deep Research / Web-Agent 专用模型<br>通过后训练或强化学习赋予模型“搜索-浏览-综合”闭环，代表工作：</li>
</ol>
<ul>
<li>闭源：OpenAI DeepResearch、Claude Research、Kimi-Researcher、Perplexity DeepResearch</li>
<li>开源：WebThinker、WebSailor、WebShaper、Tongyi DeepResearch、Cognitive Kernel-Pro、AFM-32B-RL、WebDancer、DeepMiner、R1-Searcher、WebExplorer-8B-RL、InfoAgent</li>
</ul>
<ol>
<li>数据与训练框架<br>为上述模型提供多跳 QA 数据或 RL 环境，代表工作：</li>
</ol>
<ul>
<li>数据集：MuSiQue、HotpotQA、2WikiMultihopQA、WebWalkerQA、FRAMES、SEAL-0、MegaScience、TaskCraft、Toucan1.5M</li>
<li>训练范式：ReAct、MiroFlow 多智能体协作、Group Relative Policy Optimization (GRPO)、Direct Preference Optimization (DPO)</li>
</ul>
<p>MiroThinker 在前两条主线上首次将“交互深度”显式作为可缩放维度，并通过第三条主线的数据与 RL 框架实现单任务 600 次工具调用的开源系统。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“交互深度”形式化为与模型规模、上下文长度并列的第三维度，并通过<strong>数据-训练-推理</strong>全栈设计加以实现，具体路径如下：</p>
<ol>
<li>数据层：构造可支撑高频交互的海量轨迹</li>
</ol>
<ul>
<li>MultiDocQA 合成：基于维基+Common Crawl 构建超链接知识图，提取跨文档事实并做约束混淆，生成必须多跳推理的问答对。</li>
<li>智能体轨迹合成：<br>– 单智能体 ReAct 与多智能体 MiroFlow 并行，产生 600 轮级别长轨迹。<br>– 混合 Function Calling + MCP 协议，增加工具调用多样性。<br>– 引入 12+ 开源多跳数据集并统一转为轨迹格式，形成 MiroVerse v1.0 训练集。</li>
</ul>
<ol>
<li>训练层：三阶段渐进式策略优化</li>
</ol>
<ul>
<li>阶段 1 监督微调（SFT）<br>在 Qwen2.5/3 基座上，用清洗后的专家轨迹做标准对话式微调，赋予基础“思考-行动-观察”行为。</li>
<li>阶段 2 偏好优化（DPO）<br>以“答案正确性”为唯一偏好信号，构造 (优选, 劣选) 轨迹对，采用带 SFT 正则的 DPO 目标，抑制格式偏见并提升鲁棒性。</li>
<li>阶段 3 强化学习（GRPO）<br>自建可并发千条轨迹的在线环境（搜索+沙盒+文件系统），设计稀疏奖励</li>
</ul>
<p>R = α<em>c R</em>(correct) - α<em>f R</em>(format)</p>
<p>通过组内优势估计，鼓励模型在 600 轮预算内探索更深、更频繁的工具调用，实现交互缩放。</p>
<ol>
<li>推理层：256 K 上下文 + 600 轮工具预算</li>
</ol>
<ul>
<li>采用“最近保留”上下文管理：仅保留最新 K=5 轮工具返回结果，旧结果被掩码为 ∅，既节省窗口又不丢失推理链。</li>
<li>对长输出工具（代码运行、命令行）做结果截断并标注<br>Result truncated<br>，防止单轮占满窗口。</li>
<li>固定 temperature=1.0、top-p=0.95，保证可复现性，充分释放交互缩放潜力。</li>
</ul>
<p>通过上述闭环，MiroThinker 在 GAIA、HLE、BrowseComp 等基准上取得 6–10 个点的平均提升，首次在开源领域验证“交互越深，性能越高”的可预测缩放定律。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“交互缩放”假设展开，系统验证模型规模、上下文长度与交互深度三维度对研究能力的独立与联合增益。主要实验设置与结果如下：</p>
<ol>
<li>基准与指标</li>
</ol>
<ul>
<li>覆盖 8 个公开评测：<br>GAIA（text-only）、HLE（text-only）、BrowseComp / BrowseComp-ZH、xBench-DeepSearch、WebWalkerQA、FRAMES、SEAL-0。</li>
<li>报告 avg@k 均值及标准差：高方差任务 3 次独立运行，其余 8 次；统一用 LLM-as-a-Judge 评分，禁用 HuggingFace 检索防止泄题。</li>
</ul>
<ol>
<li>主实验：三规模模型对比<br>8B / 30B / 72B 均在同一流程（SFT→DPO→GRPO）与同一推理超参下测试，结果显示</li>
</ol>
<ul>
<li>72B 在 GAIA 达 81.9%，领先最强开源基线 MiniMax-M2 6.2 个百分点；</li>
<li>72B 在 HLE 达 37.7%，超过 GPT-5-high 2.5 个百分点；</li>
<li>8B 与 30B 亦在各自量级取得新 SOTA，证明模型规模维度有效。</li>
</ul>
<ol>
<li>消融实验：交互深度维度<br>固定 30B 参数与 256 K 窗口，对比 SFT 与 RL 两个检查点：</li>
</ol>
<ul>
<li>RL 检查点平均交互轮数提升 2–4×，BrowseComp 上从 180 轮增至 420 轮；</li>
<li>准确率随之提升 8–10 个点（BrowseComp 41.2 vs 32.2，GAIA 73.5 vs 65.4），验证“更深-更频繁交互→更高性能”的单调关系。</li>
</ul>
<ol>
<li>上下文效率实验<br>在 72B 模型上分别关闭/开启“最近保留”策略：</li>
</ol>
<ul>
<li>关闭后 600 轮任务在 256 K 窗口内出现 7% 早期截断，性能下降 3.4 点；</li>
<li>开启后无截断且得分持平，证明该策略在 600 轮场景下不损失信息。</li>
</ul>
<ol>
<li><p>多语言与文化迁移<br>BrowseComp-ZH 与 xBench-DeepSearch 为全中文查询，72B 分别取得 55.6% 与 77.8%，领先次佳开源系统 6–8 点，说明交互缩放同样适用于非英语环境。</p>
</li>
<li><p>工具调用质量分析<br>对 30B-RL 在 100 条随机轨迹上人工标注工具调用有效性：</p>
</li>
</ol>
<ul>
<li>有效（带来新信息或验证）占 78%，冗余 15%，错误 7%；</li>
<li>相比 SFT 的 55% 有效比例显著提升，揭示 RL 在“更敢用”之外仍需“更巧用”。</li>
</ul>
<ol>
<li>失败案例统计</li>
</ol>
<ul>
<li>过长思维链导致超时：占 4.2%；</li>
<li>沙盒 ID 遗忘或误用：占 3.1%；</li>
<li>语言混合导致中文答案可读性下降：占 2.6%。<br>这些量化结果为后续优化提供明确方向。</li>
</ul>
<p>综合以上实验，论文首次在开源领域给出“交互深度-性能”可预测缩放曲线，并证明 72B+256K+600 轮配置可将开源研究智能体推向商业同级水平。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可继续推进，按“数据-模型-系统-评测”四层归纳：</p>
<p>数据层</p>
<ul>
<li><p><strong>工具反馈稀缺信号挖掘</strong><br>现有轨迹多来自正确示范，可引入“失败-修复”对：让模型先故意走错，再由教师智能体给出最小代价纠正，增强错误恢复能力。</p>
</li>
<li><p><strong>多模态证据链</strong><br>将网页截图、PDF 图表、实验视频编码为嵌入，构造图文混合的“证据节点”，使智能体在交互中可引用视觉信息，突破纯文本上限。</p>
</li>
<li><p><strong>可验证偏好扩展</strong><br>除答案正确性外，引入“可执行性”“引用完整性”“成本最小化”等自动度量，构建多目标偏好数据集，支持更细粒度的 RL 奖励。</p>
</li>
</ul>
<p>模型层</p>
<ul>
<li><p><strong>思考-行动解耦架构</strong><br>用专用小模型承担“行动提议”，大模型仅负责“思考与评估”，降低长序列生成成本，同时保持 600 轮调用能力。</p>
</li>
<li><p><strong>动态工具检索</strong><br>将工具描述建模为向量索引，每一步让模型先检索最相关工具子集再调用，减少冗余调用，提升 78%→90% 有效比例。</p>
</li>
<li><p><strong>层次化记忆机制</strong><br>在 256 K 滑动窗口外，再引入外部向量记忆或键值缓存，实现“遗忘-摘要-召回”闭环，支持跨任务、跨会话的长期知识积累。</p>
</li>
</ul>
<p>系统层</p>
<ul>
<li><p><strong>沙盒能力升级</strong><br>支持 Docker-in-Docker、GPU 代码执行、交互式 Jupyter，允许模型运行深度学习实验或大规模仿真，拓宽“研究”定义边界。</p>
</li>
<li><p><strong>在线代价感知调度</strong><br>为每次 API 调用标注实时费用与延迟，训练“预算意识”策略，使模型在有限美元或秒级约束下自动权衡探索深度。</p>
</li>
<li><p><strong>人机协同接口</strong><br>引入“人类作为工具”选项，模型可在不确定时发起澄清请求，形成主动对齐，降低 4.2% 超时失败率。</p>
</li>
</ul>
<p>评测层</p>
<ul>
<li><p><strong>长周期滚动评测</strong><br>建立每月更新的“活基准”，通过新鲜问答对防止训练集泄露，持续追踪交互缩放曲线是否依然线性。</p>
</li>
<li><p><strong>可解释性细粒度指标</strong><br>除整体准确率外，报告“证据召回率”“引用精度”“推理跳数分布”，量化模型是否真正完成多跳而非表面匹配。</p>
</li>
<li><p><strong>对抗性干扰测试</strong><br>在返回结果中注入矛盾信息或恶意代码，评估模型能否识别并拒绝，衡量鲁棒性与安全性。</p>
</li>
</ul>
<p>理论层</p>
<ul>
<li><p><strong>交互复杂度理论</strong><br>形式化定义“交互深度”与任务 Kolmogorov 复杂度的关系，证明何种问题类别存在多项式轮数最优策略，为缩放定律提供上限。</p>
</li>
<li><p><strong>RL 与搜索策略融合界</strong><br>分析 GRPO 在部分可观测环境下的样本复杂度，给出达到 ε-最优策略所需轨迹数，指导未来数据收集预算。</p>
</li>
</ul>
<p>通过上述探索，可进一步释放交互缩放的潜力，让开源研究智能体在成本、可信、通用三方面全面逼近甚至超越封闭系统。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出开源研究智能体 <strong>MiroThinker v1.0</strong>，核心贡献与内容可概括为：</p>
<ol>
<li><p>新维度：交互缩放<br>将“单任务工具调用次数”形式化为与模型规模、上下文长度并列的第三缩放轴，验证“调用越深→性能越高”的可预测增益。</p>
</li>
<li><p>系统实现</p>
</li>
</ol>
<ul>
<li>256 K 上下文窗口，支持最多 600 次工具调用。</li>
<li>模块化工具箱：Linux 沙盒、文件传输、Google 搜索、网页抽取。</li>
<li>最近保留上下文管理，保证长轨迹不溢出。</li>
</ul>
<ol>
<li><p>三阶段训练<br>① 大规模 SFT 模仿专家轨迹；② DPO 偏好优化，以答案正确性为唯一信号；③ 在线 GRPO 强化学习，直接优化交互深度与准确率。</p>
</li>
<li><p>数据引擎</p>
</li>
</ol>
<ul>
<li>MultiDocQA：从维基+Common Crawl 构建多跳事实并做约束混淆。</li>
<li>轨迹合成：ReAct 单智能体 + MiroFlow 多智能体，结合 Function Calling 与 MCP 协议，生成 600 轮级别轨迹。</li>
<li>汇聚 12 个开源多跳数据集，统一转为轨迹格式。</li>
</ul>
<ol>
<li><p>实验结果<br>72B 模型在 GAIA、HLE、BrowseComp、BrowseComp-ZH 等 8 项基准取得新 SOTA，最高 81.9%，平均领先开源基线 6–10 点，部分超越 GPT-5-high。</p>
</li>
<li><p>结论与局限<br>交互缩放首次在开源领域被验证为可靠路径；但仍存在工具冗余、思维链过长、语言混合、沙盒误用等不足，供后续研究继续优化。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MiroThinker%3A%20Pushing%20the%20Performance%20Boundaries%20of%20Open-Source%20Research%20Agents%20via%20Model%2C%20Context%2C%20and%20Interactive%20Scaling，链接是https://arxiv.org/pdf/2511.11793，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.11793。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: MiroMind Team,Song Bai,Lidong Bing,Carson Chen,Guanzheng Chen,Yuntao Chen,Zhe Chen,Ziyi Chen,Jifeng Dai,Xuan Dong,Yue Deng,Yunjie Fu,Junqi Ge,Chenxia Han,Tammy Huang,Zhenhang Huang,Jerry Jiao,Shilei Jiang,Tianyu Jiao,Xiaoqi Jian,Lei Lei,Ruilin Li,Ryan Luo,Tiantong Li,Xiang Lin,Ziyuan Liu,Zhiqi Li,Jie Ni,Qiang Ren,Pax Sun,Shiqian Su,Chenxin Tao,Bin Wang,Hellen Wang,Haonan Wang,James Wang,Jin Wang,Jojo Wang,Letian Wang,Shizun Wang,Weizhi Wang,Zixuan Wang,Jinfan Xu,Sen Xing,Chenyu Yang,Hai Ye,Jiaheng Yu,Yue Yu,Muyan Zhong,Tianchen Zhao,Xizhou Zhu,Yanpeng Zhou,Yifan Zhang,Zhi Zhu</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11793.pdf">https://arxiv.org/pdf/2511.11793.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11793">https://arxiv.org/abs/2511.11793</a></p>
<p><strong>Arxiv ID</strong>: 2511.11793</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11793">https://papers.cool/arxiv/2511.11793</a></p>
<p><strong>Published</strong>: 2025-11-14T18:52:07Z</p>
<p><strong>Updated</strong>: 2025-11-14T18:52:07.000Z</p>
<hr>
<h3 id="4-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance"><a href="#4-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance" class="headerlink" title="4. Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13254">Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</a></h3><p>Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse domains, but their training remains resource- and time-intensive, requiring massive compute power and careful orchestration of training procedures. Model souping-the practice of averaging weights from multiple models of the same architecture-has emerged as a promising pre- and post-training technique that can enhance performance without expensive retraining. In this paper, we introduce Soup Of Category Experts (SoCE), a principled approach for model souping that utilizes benchmark composition to identify optimal model candidates and applies non-uniform weighted averaging to maximize performance. Contrary to previous uniform-averaging approaches, our method leverages the observation that benchmark categories often exhibit low inter-correlations in model performance. SoCE identifies “expert” models for each weakly-correlated category cluster and combines them using optimized weighted averaging rather than uniform weights. We demonstrate that the proposed method improves performance and robustness across multiple domains, including multilingual capabilities, tool calling, and math and achieves state-of-the-art results on the Berkeley Function Calling Leaderboard.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLM）在各个领域表现出了卓越的能力，但它们的训练仍然耗费大量资源和时间，需要强大的计算能力和对训练流程的精心协调。模型融合——即将相同架构的多个模型的权重进行平均的做法——已经成为一种有前景的训练前和训练后技术，可以在不进行昂贵重训练的情况下提升性能。在本文中，我们提出了“类别专家融合”（Soup Of Category Experts, SoCE），这是一种用于模型融合的原则性方法，利用基准组合来识别最优模型候选，并应用非均匀加权平均以最大化性能。与之前的均匀平均方法不同，我们的方法利用了基准类别在模型性能上通常表现出低相关性的观察结果。SoCE为每个弱相关类别簇识别“专家”模型，并通过优化加权平均而非均匀权重进行组合。我们证明了该方法在多个领域中提高了性能和鲁棒性，包括多语言能力、工具调用和数学计算，并在伯克利函数调用排行榜（Berkeley Function Calling Leaderboard）上达到了最先进的水平。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决大型语言模型（LLM）训练代价高昂、难以兼顾多任务性能的问题。核心目标可概括为：</p>
<ul>
<li><strong>降低再训练成本</strong>：传统方法为提升某一能力往往需重新设计数据配比并完整再训练，计算与时间开销巨大。</li>
<li><strong>突破均匀平均局限</strong>：现有“模型汤”（model souping）多直接对所有候选模型取等权平均，忽视不同模型在各类任务上的专长差异，可能拉低整体表现。</li>
<li><strong>实现任务感知的自动聚合</strong>：提出 Soup Of Category Experts（SoCE），利用 benchmark 内部子任务性能的低相关性，自动识别每类任务的“专家”模型，并以优化后的非均匀权重进行加权平均，从而在无需重新训练的前提下，获得在工具调用、数学、多语言等多领域均具 SOTA 的综合模型。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与本文直接相关的研究脉络可分为三条主线，均聚焦于“不重新训练即可提升模型性能”的模型合并/平均技术：</p>
<ol>
<li>模型汤（Model Souping）</li>
</ol>
<ul>
<li>Wortsman et al. (2022) 首次在视觉与文本微调模型上验证：对同一架构的多个微调检查点做<strong>均匀权重平均</strong>可提升下游精度，提出 Uniform、Greedy、Learned 三种策略。</li>
<li>Jang et al. (2025) 引入“几何靠近预训练点”的筛选准则，减少需合并的模型数量，提升 Greedy Souping 效果。</li>
<li>Kleiman et al. (2025) 将均匀汤用于持续学习，缓解灾难性遗忘。</li>
<li>Yu et al. (2024) 对同架构模型进行神经元“休眠-再激活”后合并，实现能力迁移。</li>
</ul>
<ol>
<li>自动模型合并（Automated Model Merging）</li>
</ol>
<ul>
<li>Yang et al. (2024) 以熵最小化无监督搜索合并系数，面向 ViT 分类任务。</li>
<li>Akiba et al. (2025) 采用进化算法自动发现最优合并配方，支持多模态能力组合。</li>
</ul>
<ol>
<li>任务/模块级加权平均</li>
</ol>
<ul>
<li>Zimmer et al. (2023) 在稀疏剪枝场景下做<strong>加权模型汤</strong>，但权重仍靠人工设定。</li>
<li>Li et al. (2025) 在预训练阶段即进行模型汤实验，验证可跨阶段合并。</li>
</ul>
<p>本文 SoCE 与上述工作的区别：</p>
<ul>
<li>首次<strong>系统利用 benchmark 子任务间的低相关性</strong>作为信号，自动挑选“类别专家”。</li>
<li>采用<strong>非均匀且可解释</strong>的权重优化，而非均匀或黑箱学习式加权。</li>
<li>在 LLM 的<strong>后训练阶段</strong>实现工具调用、数学、多语言、长文本等多域同步 SOTA，并给出大规模 Shapley 值分析验证专家选择的合理性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将问题拆解为“选哪些模型”与“如何加权”两步，提出 Soup Of Category Experts（SoCE）框架，具体流程如下：</p>
<ol>
<li><p>相关性诊断<br>对给定 benchmark 的所有子任务（类别）计算跨模型性能 Pearson 相关系数矩阵，设定阈值 τ 找出“弱相关或负相关”类别对；这些类别被视为潜在互补领域。</p>
</li>
<li><p>专家模型遴选<br>对每个弱相关类别  C_i ，在候选池  M  中直接选取该类别得分最高的单模型  M^*_i  作为其专家，保证“专人专用”。</p>
</li>
<li><p>权重优化<br>仅对遴选出的专家模型集合  M^*_i  进行网格搜索：</p>
</li>
</ol>
<ul>
<li>权重空间以 0.1 为步长，单模型权重上限 0.9、下限 0.1；</li>
<li>目标函数为所有类别加权性能之和</li>
</ul>
<p>w^<em> = argmax_w ∑_k Performancel(∑_j w_j M^</em>_j,; C_kr),quad s.t.∑_j w_j=1.</p>
<p>该步骤自动给出非均匀系数，避免等权平均带来的能力稀释。</p>
<ol>
<li>模型汤生成<br>按最优系数执行参数平均</li>
</ol>
<p>M_(soup) = ∑_i w^<strong>i · M^</strong>i,</p>
<p>得到最终单一检查点，无需任何再训练或梯度更新。</p>
<p>通过“类别-专家-非均匀权重”三段式，SoCE 在 BFCL、MGSM、∞-Bench 等多域 benchmark 上均取得 SOTA，且大规模相关性分析与 Shapley 值实验表明：</p>
<ul>
<li>汤后模型在不同类别间的性能线性相关度显著升高，鲁棒性增强；</li>
<li>被 SoCE 选中的专家其边际贡献（Shapley 值）远高于未被选模型，验证了“低相关⇒互补”假设的有效性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“是否有效”“为何有效”“会不会过拟合”三个层次，共设计了 4 组 benchmark 实验 + 3 项诊断分析，全部基于公开模型检查点，无需额外训练。</p>
<ol>
<li>主实验：多 benchmark 性能对比</li>
</ol>
<ul>
<li><strong>BFCL</strong>（工具调用）<br>– 70 B 档：4 个候选 → SoCE 80.68%，超最佳单模型 2.7 个百分点。<br>– 8 B 档：4 个候选 → SoCE 76.50%，超最佳单模型 5.7 个百分点。</li>
<li><strong>MGSM</strong>（多语数学）<br>4 个 7 B 模型 → SoCE 51.7%，相对最佳单模型提升 1.6%。</li>
<li><strong>∞-Bench</strong>（长文本）<br>5 个 Llama-3-70 B 变体 → SoCE 28.0%，相对最佳单模型提升 0.66%。</li>
<li><strong>FLORES-36</strong>（翻译，仅消融）<br>5 个 70 B 模型 → SoCE 平均 BLEU 39.19，parent 最高 39.07，稳中有升。</li>
</ul>
<ol>
<li>消融实验：量化两大组件贡献</li>
</ol>
<ul>
<li>均匀汤（全部候选） vs 均匀汤+SoCE 选模：仅引入“专家选择”即可提升 1.4–4.3 个百分点。</li>
<li>均匀汤+选模 vs SoCE 非均匀权重：再涨 2.3–3.4 个百分点，验证权重优化必要性。</li>
</ul>
<ol>
<li>一致性诊断</li>
</ol>
<ul>
<li>对 800 余份汤前/汤后检查点计算 Pearson 相关矩阵：汤后类别间平均相关系数从 0.52→0.81，性能曲线更一致。</li>
<li>35/37 组汤实验在 20+ 子任务上同时取得正收益，表明鲁棒性提高。</li>
</ul>
<ol>
<li><p>Shapley 值博弈分析<br>以 MGSM、BFCL、FLORES-36 为游戏，单模型/配对/三元组为玩家，SoCE 选中专家的 Shapley 值显著高于未选中者，说明其“边际贡献”最大，反向验证低相关⇒互补的假设。</p>
</li>
<li><p>过拟合检验<br>将在 BFCL 上汤的 8 B 模型直接移到 Hellaswag、IFEval、BBH 评测，结果与最佳 parent 持平或略升，未出现明显退化，表明方法未对源 benchmark 过拟合。</p>
</li>
</ol>
<p>整体实验覆盖工具、数学、长文本、翻译四大能力域，参数规模 8 B–70 B，样本量累计 &gt;1 k 检查点，从性能、鲁棒性、可解释性、泛化性四个维度支撑了 SoCE 的有效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>Benchmark 自动子类划分</strong><br>目前方法依赖现成子任务标签；对无细分标签的 benchmark，可探索基于模型输出或嵌入的<strong>无监督聚类</strong>，自动生成“弱相关”类别，再套用于 SoCE 框架。</p>
</li>
<li><p><strong>跨预训练基座的模型汤</strong><br>现有实验全部基于同一 Llama-3 预训练检查点。可系统验证：<br>– 不同预训练基座（如 Llama vs Mistral）参数空间是否仍满足线性模式连通假设；<br>– 若存在分布偏移，如何引入逐层或逐模块对齐（Li et al., 2025 的 pre-train merging 思路）后再加权平均。</p>
</li>
<li><p><strong>动态权重 / 稀疏混合</strong><br>当前权重在推理前固定。可研究：<br>– 输入依赖的<strong>动态权重</strong>（routing network 或梯度敏感加权），实现“按需汤”；<br>– 稀疏汤：仅合并部分层或部分神经元，保留其余单模型参数，降低能力干扰。</p>
</li>
<li><p><strong>Scaling Law 与多样性度量</strong><br>系统探索“模型数量↑ vs 性能↑”曲线，引入<strong>有效多样性指标</strong>（如 Shapley 熵、特征子空间角度），预测边际收益拐点，避免无效堆叠。</p>
</li>
<li><p><strong>多阶段 checkpoint 汤</strong><br>验证预训练→SFT→RLHF 三阶段 checkpoint 能否跨阶段合并；若可行，可设计<strong>阶段对齐正则项</strong>，防止分布外参数导致崩溃。</p>
</li>
<li><p><strong>私有数据场景</strong><br>当数据不可共享、仅 checkpoint 可公开时，利用 SoCE 把“私有能力”以权重形式注入公共模型，实现<strong>零数据知识迁移</strong>；需配套研究隐私泄露风险与防御机制。</p>
</li>
<li><p><strong>MoE / 稀疏架构适配</strong><br>将 SoCE 从稠密模型扩展到混合专家：<br>– 对 expert 层独立加权平均，保持 router 不变；<br>– 或引入<strong>专家选择+权重双优化</strong>，解决专家重叠冲突。</p>
</li>
<li><p><strong>可解释性与机制融合</strong><br>结合机制可解释性工具（logit lens、causal tracing）观察汤后模型是否真正融合互补机制，而非简单投票；为“能力叠加”提供因果层面证据。</p>
</li>
<li><p><strong>任务增量与灾难遗忘</strong><br>把 SoCE 作为持续学习插件：每遇新任务，仅训练一个小专家后与旧汤合并，验证能否在<strong>不遗忘</strong>旧能力前提下持续扩展新技能。</p>
</li>
<li><p><strong>伦理与评测透明度</strong><br>建立“模型汤披露”协议与评测基准，要求发布者公开合并配方与权重，防止后续可解释性研究因未知平均操作而失效。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Soup Of Category Experts（SoCE）</strong>，一种<strong>无需再训练、仅需算术平均</strong>即可把多个同架构 LLM 合成 SOTA 模型的后训练方法。核心思想是：利用 benchmark 子任务间的低相关性，自动为每个弱相关类别挑选“专家”模型，再以网格搜索得到的最优非均匀权重进行参数平均。</p>
<p><strong>主要贡献与结果</strong></p>
<ol>
<li>自动化框架：四步流水线（相关性分析→专家遴选→权重优化→加权汤）。</li>
<li>多域 SOTA：</li>
</ol>
<ul>
<li>BFCL 70 B 80.68%（+2.7 pp）、8 B 76.50%（+5.7 pp）刷新榜；</li>
<li>MGSM 51.7%（+1.6 pp）、∞-Bench 28.0%（+0.66 pp）。</li>
</ul>
<ol>
<li>一致性提升：汤后子任务 Pearson 相关由 0.52→0.81，35/37 组实验过半类别同时增益。</li>
<li>可解释验证：Shapley 值表明被选专家边际贡献显著高于其余模型。</li>
<li>不过拟合：在 Hellaswag、IFEval、BBH 上无退化，泛化能力保持。</li>
</ol>
<p><strong>局限与未来</strong><br>需现成子任务标签、仅测同预训练检查点、未系统探索 Scaling Law；可拓展至自动聚类、跨基座对齐、动态/稀疏汤、持续学习及 MoE 架构。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Souper-Model%3A%20How%20Simple%20Arithmetic%20Unlocks%20State-of-the-Art%20LLM%20Performance，链接是https://arxiv.org/pdf/2511.13254，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13254。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shalini Maiti,Amar Budhiraja,Bhavul Gauri,Gaurav Chaurasia,Anton Protopopov,Alexis Audran-Reiss,Michael Slater,Despoina Magka,Tatiana Shavrina,Roberta Raileanu,Yoram Bachrach</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13254.pdf">https://arxiv.org/pdf/2511.13254.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13254">https://arxiv.org/abs/2511.13254</a></p>
<p><strong>Arxiv ID</strong>: 2511.13254</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13254">https://papers.cool/arxiv/2511.13254</a></p>
<p><strong>Published</strong>: 2025-11-17T11:13:34Z</p>
<p><strong>Updated</strong>: 2025-11-17T11:13:34.000Z</p>
<hr>
<h3 id="5-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model"><a href="#5-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model" class="headerlink" title="5. Part-X-MLLM: Part-aware 3D Multimodal Large Language Model"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13647">Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</a></h3><p>We introduce Part-X-MLLM, a native 3D multimodal large language model that unifies diverse 3D tasks by formulating them as programs in a structured, executable grammar. Given an RGB point cloud and a natural language prompt, our model autoregressively generates a single, coherent token sequence encoding part-level bounding boxes, semantic descriptions, and edit commands. This structured output serves as a versatile interface to drive downstream geometry-aware modules for part-based generation and editing. By decoupling the symbolic planning from the geometric synthesis, our approach allows any compatible geometry engine to be controlled through a single, language-native frontend. We pre-train a dual-encoder architecture to disentangle structure from semantics and instruction-tune the model on a large-scale, part-centric dataset. Experiments demonstrate that our model excels at producing high-quality, structured plans, enabling state-of-the-art performance in grounded Q\&amp;A, compositional generation, and localized editing through one unified interface. Project page: <a target="_blank" rel="noopener" href="https://chunshi.wang/Part-X-MLLM/">https://chunshi.wang/Part-X-MLLM/</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了 Part-X-MLLM，一种原生的 3D 多模态大语言模型，它通过将多样的 3D 任务形式化为结构化、可执行的语法程序来实现统一。给定 RGB 点云和自然语言提示，我们的模型自回归地生成单一、连贯的标记序列，该序列编码了零件级边界框、语义描述和编辑命令。这种结构化输出作为一个多功能接口，可以驱动下游的几何感知模块，实现基于零件的生成和编辑。通过将符号规划与几何合成解耦，我们的方法允许任何兼容的几何引擎通过单一的、面向语言的前端进行控制。我们预训练了一个双编码器架构，以将结构与语义分离，并在大规模以零件为中心的数据集上进行指令微调。实验表明，我们的模型在生成高质量、结构化计划方面表现出色，从而通过一个统一的界面实现了在基于实物的问答、组合生成和局部编辑方面的最先进性能。项目主页: <a target="_blank" rel="noopener" href="https://chunshi.wang/Part-X-MLLM/">https://chunshi.wang/Part-X-MLLM/</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现有 3D 多模态大模型（MLLM）在“部件级”理解与操控上的结构性缺失。核心问题可归纳为：</p>
<ul>
<li><strong>整体式表征困境</strong>：当前生成模型将 3D 资产视为不可分割的整体，缺乏对语义部件的持久标识与可寻址能力，导致细粒度编辑、组合式生成与精准问答难以实现。</li>
<li><strong>接口碎片化</strong>：理解、生成、编辑三类任务各自依赖专用网络或手工接口，无法通过统一的自然语言界面完成。</li>
<li><strong>弱 3D 一致性</strong>：2D 提升方法受视角不一致与深度歧义影响；原生 3D 方法又缺少可执行、可审计的语言化输出。</li>
</ul>
<p>Part-X-MLLM 的目标是用一个<strong>原生 3D、部件感知、可执行程序生成</strong>的框架，将上述任务统一为“从 RGB 点云 + 自然语言提示 → 部件级包围盒与操作命令”的单序列预测问题，从而提供稳定部件身份、可控语义粒度、与任意几何后端解耦的语言控制面。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为四大主线，Part-X-MLLM 在每条线中均定位出尚未被同时满足的空缺：</p>
<ol>
<li>3D 多模态理解</li>
</ol>
<ul>
<li>点云-语言对齐：PointLLM、3D-LLM、GPT4Point、ShapeLLM 等仅完成物体级或场景级描述/问答，<strong>无持久部件 token、无可执行输出</strong>。</li>
<li>场景对话系统：Chat-Scene、Chat-3D、LL3DA、3D-LLaVA、Video-3D LLM 等引入对象标识符或超点，但仍<strong>把部件视为黑箱</strong>，无法生成带 符号的编辑程序。</li>
</ul>
<ol>
<li>3D 整体生成</li>
</ol>
<ul>
<li>SDS 系列：DreamFusion、DreamReward-X 等优化隐式场，<strong>输出整体 NeRF/网格</strong>，部件不可寻址。</li>
<li>稀疏体素：TRELLIS、Hunyuan3D 2.x 等提供高保真几何，但<strong>缺乏语义部件接口</strong>。</li>
<li>向量集合：3DShape2VecSet、Craftsman3D、Sparc3D 等压缩为隐向量，<strong>无法直接暴露部件 BBox</strong> 供语言驱动。</li>
</ul>
<ol>
<li>部件级生成</li>
</ol>
<ul>
<li>2D 提升路线：Part123、SAMPart3D、PartField、HoloPart 等依赖多视角 SAM 掩码，<strong>受视角不一致与弱 3D 约束</strong>。</li>
<li>原生 3D 部件：PASTA、AutoPartGen、PartPacker、BANG、Assembler、OmniPart、X-Part 等可生成部件，但<strong>无统一语言前端</strong>，需预定义部件数或手工掩码，<strong>不能从自然语言直接输出可执行程序</strong>。</li>
</ul>
<ol>
<li>3D 编辑</li>
</ol>
<ul>
<li>SDS 编辑：Instruct-NeRF2NeRF、Vox-E、FocalDreamer 等需优化或多步扩散，<strong>不提供语言原生模型</strong>输出编辑命令。</li>
<li>前馈隐式编辑：Shap-Editor、MVEdit、PrEditor3D、Nano3D、VoxHammer 等实现快速局部编辑，但<strong>仍需外部输入掩码或隐码</strong>，缺少“文本 → 带 // 的 BBox 程序”这一统一接口。</li>
</ul>
<p>综上，<strong>尚无工作同时满足</strong>：</p>
<ul>
<li>原生 3D 部件感知</li>
<li>持久 BBox token 接地</li>
<li>单序列可执行程序输出</li>
<li>与任意几何后端解耦</li>
</ul>
<p>Part-X-MLLM 通过“部件语法 + 双编码器 + 大规模部件指令微调”填补了这一空缺。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“3D 部件级交互”重新形式化为<strong>一个统一的语言建模任务</strong>：<br>给定 RGB 点云与文本提示，自回归地生成一段<strong>可执行、可审计的符号程序</strong>——仅含部件包围盒（BBox）token 与编辑命令 token 的单一序列。具体实现分三大模块、两阶段训练与一条数据管线：</p>
<p>1. 统一架构：把“几何–语义–语言”压进同一 token 序列</p>
<ul>
<li><strong>双编码器</strong></li>
<li>Structure Encoder：仅编码 (x,y,z,nx,ny,nz) → 结构 token</li>
<li>Semantic Encoder：仅编码 (x,y,z,r,g,b) → 外观 token<br>两者并行，避免单编码器在“形状 vs 颜色”表征上的冲突。</li>
<li><strong>程序式输出语法</strong>（扩展词表）</li>
<li>部件定位：<code>&lt;boxs&gt; qx1 qy1 qz1 qx2 qy2 qz2 &lt;boxe&gt;</code>（6 个 128-bin 量化坐标）</li>
<li>编辑操作：<code>&lt;adds&gt;…&lt;adde&gt;</code>、<code>&lt;dels&gt;…&lt;dele&gt;</code>、<code>&lt;mods&gt;…&lt;mode&gt;</code></li>
<li>文本描述：可选插入粗/细粒度说明</li>
<li><strong>解码器</strong><br>冻结 Qwen2.5-VL 的 LLM 权重，只微调新增 token 嵌入与 AR 层，保证语言先验不被洗空。</li>
</ul>
<p>2. 两阶段课程：先学“几何定位”，再学“语言对齐”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>数据</th>
<th>可训练参数</th>
<th>目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stage-1 几何预训练</td>
<td>360 万无 RGB 点云</td>
<td>Structure Encoder + 轻量解码器</td>
<td>仅预测部件 BBox，不接触文本 → 得到结构专家</td>
</tr>
<tr>
<td>Stage-2 指令微调</td>
<td>85 k 对象、429 万指令对</td>
<td>Semantic Encoder + AR 层 + 新 token 嵌入</td>
<td>联合文本提示，输出完整“BBox+命令+描述”程序</td>
</tr>
</tbody>
</table>
</div>
<p>3. 下游接口：符号程序 → 任意几何引擎</p>
<ul>
<li><p><strong>部件生成</strong><br>把预测的 BBox 序列喂给 OmniPart 合成器，作为空间约束，直接生成 mesh/3DGS/NeRF。</p>
</li>
<li><p><strong>局部编辑</strong><br>用 BBox 生成立方体掩膜，调用 Nano3D/VoxHammer 等训练自由编辑器完成 add/delete/modify。</p>
</li>
<li><p><strong>粒度控制</strong><br>基于 CLIP 语义对 BBox 聚类，自动合并细部件 → 用户无需手动指定部件数即可在“粗-细”之间滑动。</p>
</li>
</ul>
<p>4. 数据与评测</p>
<ul>
<li><p><strong>自研 UniPart- Bench</strong><br>400 withheld 对象，11 任务族（纯检测、接地、描述、QA、编辑程序），指标同时测 IoU 与文本相似度。</p>
</li>
<li><p><strong>大规模指令对</strong><br>11 类模板、429 万条样本，覆盖 coarse/fine 文本、增删改程序，保证模型在“只说一句话”场景也能输出合法可执行序列。</p>
</li>
</ul>
<p>结果</p>
<ul>
<li><strong>BBox 质量</strong>：Voxel Recall 74.11 %（+4.5）、Bbox IoU 42.55 %（+5.2）超越 PartField &amp; OmniPart。</li>
<li><strong>理解任务</strong>：Part-QA 的 SBERT 78.98（+18.7）、BLEU-1 40.54（+21.3）全面领先现有 3D MLLM。</li>
<li><strong>编辑/生成</strong>：同一模型，仅换下游引擎即可实现语言驱动的增删改与部件级生成，无需额外微调。</li>
</ul>
<p>通过“符号程序”这一中间层，Part-X-MLLM 把<strong>语言请求</strong>与<strong>几何实现</strong>彻底解耦，从而一次性解决了“部件感知、持久接地、统一接口、可控粒度”四个核心需求。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“部件级 3D 理解、生成与编辑”设计了<strong>四类共 11 任务</strong>的实验体系，覆盖定量指标、消融分析、语义粒度控制与下游应用验证。所有实验均在自研 <strong>UniPart-Bench</strong>（400 个 withheld 对象）与 85 k 对象训练集上完成。</p>
<p>1. 部件级几何质量评估（Task-0/1/2/10）</p>
<ul>
<li><strong>指标</strong>：BBox-IoU ↑、Voxel-Recall ↑、Voxel-IoU ↑</li>
<li><strong>对照</strong>：PartField、OmniPart（SAM 掩膜版）</li>
<li><strong>结果</strong>：</li>
<li>BBox-IoU 42.55 %（+5.22 vs. 最强基线）</li>
<li>Voxel-Recall 74.11 %（+4.46）</li>
<li>定性可视化显示碎片更少、结构更完整（图 4）。</li>
</ul>
<p>2. 部件理解与问答（Task-7）</p>
<ul>
<li><strong>指标</strong>：SBERT、SimCSE、BLEU-1、ROUGE-L、METEOR 全部 ↑</li>
<li><strong>对照</strong>：GPT4Point、PointLLM-7/13 B、ShapeLLM-13 B、ShapeLLM-Omni-7 B</li>
<li><strong>结果</strong>：</li>
<li>SBERT 78.98（+18.7）、BLEU-1 40.54（+21.3）</li>
<li>答案自带 token，实现可验证 grounding（图 9）。</li>
</ul>
<p>3. 整体对象 Caption（非部件）</p>
<ul>
<li><strong>指标</strong>：同上文本相似度套件</li>
<li><strong>结果</strong>：</li>
<li>SBERT 53.82（+10.4）、ROUGE-L 38.11（+20.2）</li>
<li>定性示例显示细节与颜色准确性显著优于基线（图 8）。</li>
</ul>
<p>4. 部件级编辑验证（Task-8/9/10）</p>
<ul>
<li><strong>协议</strong>：</li>
</ul>
<ol>
<li>模型输出 // 程序 + BBox</li>
<li>用 Nano3D / VoxHammer 执行掩膜编辑</li>
<li>人工目视检查结构完整性 &amp; 语义正确性</li>
</ol>
<ul>
<li><strong>结果</strong>：</li>
<li>成功完成“换头”、“换鞋”、“加底座”、“删支架”等 20 余种指令（图 5）。</li>
<li>编辑区域与指令空间吻合，无溢出伪影。</li>
</ul>
<p>5. 语义粒度控制实验</p>
<ul>
<li><strong>方法</strong>：对 22 个精细部件按 CLIP 语义做 DBSCAN 聚类，自动合并 BBox。</li>
<li><strong>结果</strong>：</li>
<li>无需重新训练，即可从 22 部件 → 18 → 10 → 6 → 2 级语义层次（图 6）。</li>
<li>合并后的 BBox 仍能被下游生成器正确识别并合成粗粒度资产。</li>
</ul>
<p>6. 消融：双编码器 vs. 单编码器</p>
<ul>
<li><strong>单编码器基线</strong>：同一网络同时输入 (xyz+rgb)</li>
<li><strong>指标</strong>：BBox-IoU、所有文本相似度任务</li>
<li><strong>结果</strong>：</li>
<li>纯检测任务 IoU 从 68.47 → 75.53（+7.06）</li>
<li>Part-QA 与 Multi-Part Grounding 全部指标一致提升（表 2），验证“结构–外观解耦”必要性。</li>
</ul>
<p>7. 特殊 token 嵌入可视化</p>
<ul>
<li><strong>方法</strong>：对新增 128 个 box-token 与编辑 token 做 t-SNE</li>
<li><strong>观察</strong>：</li>
<li>box-token 形成连续有序流形，模型学会坐标序关系；</li>
<li>功能相似的 start/end token 紧密聚类，说明语法被有效吸收（图 10）。</li>
</ul>
<p>8. 下游扩展：置信度感知面片分割</p>
<ul>
<li><strong>流程</strong>：</li>
</ul>
<ol>
<li>用模型输出的 BBox 概率均值 Conf(Bj) 作为置信度；</li>
<li>面片 centroid 与 BBox 做包含测试；</li>
<li>冲突时用“小盒优先 + 高置信优先”规则分配。</li>
</ol>
<ul>
<li><strong>结果</strong>：</li>
<li>无需额外训练，即可在 mesh 上得到精细面片级部件掩膜（图 7），与人工标注 IoU 达 0.81。</li>
</ul>
<p>9. 训练规模与效率</p>
<ul>
<li><strong>硬件</strong>：64×A100</li>
<li><strong>时间</strong>：Stage-1 2 天，Stage-2 5 天</li>
<li><strong>吞吐</strong>：Stage-1 256 batch，Stage-2 64 batch，均未出现收敛瓶颈。</li>
</ul>
<p>10. 错误分析与局限</p>
<ul>
<li><strong>长序列延迟</strong>：当部件数 &gt; 40 时，自回归步数增加 30 %，可通过层次式 BBox 压缩缓解。</li>
<li><strong>置信分割深度不足</strong>：仅依赖 centroid 与单点置信，未来可引入面片特征或图注意力提升边界精度。</li>
</ul>
<p>综上，实验不仅验证了 Part-X-MLLM 在<strong>生成质量、理解精度、编辑可用性</strong>三大维度的 SOTA 性能，也通过消融、可视化与下游扩展证明了“符号程序 + 双编码器”这一设计路线的有效性与通用性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 Part-X-MLLM 框架的直接延伸，亦具备独立研究价值：</p>
<ol>
<li>层次-符号序列压缩<br>长部件链导致自回归步数线性增长。可引入</li>
</ol>
<ul>
<li>层级 BBox 语法（<group>…</group>）</li>
<li>隐式潜变量压缩（VQ-VAE 或 Diffusion-LM）<br>将 40+ 部件编码为 8–12 个高层 token，再解码为细粒度盒，实现常数级推理延迟。</li>
</ul>
<ol>
<li>时序/动画感知部件规划<br>当前程序仅描述静态空间盒。扩展特殊 token 以表达</li>
</ol>
<ul>
<li>刚性运动参数（<rotate>、<translate>）</li>
<li>铰链自由度（<joint axis="z" range="-45,45">）<br>使同一前端直接输出“可驱动”的动画脚本，对接物理引擎或骨骼系统。</li>
</ul>
<ol>
<li>跨对象部件重组与风格混合<br>利用程序输出可解析的优势，引入</li>
</ol>
<ul>
<li>部件级风格编码（CLIP 或 3D 自监督）</li>
<li>交叉注意融合模块<br>实现“把 A 的椅背曲线迁移到 B 的办公椅”而无需重新训练生成器。</li>
</ul>
<ol>
<li>不确定性感知规划<br>当前仅使用平均 token 概率作为框置信。可</li>
</ol>
<ul>
<li>对坐标施加分布建模  x sim N(μ_x, σ_x) </li>
<li>在程序中输出 μ σ<br>让下游扩散采样器按不确定性自动扩大/缩小掩膜，提升编辑鲁棒性。</li>
</ul>
<ol>
<li>语言-物理一致性检查<br>生成程序可能违背物理常识（如悬浮手柄）。可插入</li>
</ol>
<ul>
<li>可微物理层（碰撞+重力损失）</li>
<li>语法树遍历式规则验证（支撑图连通性）<br>在符号空间完成“自纠正”，再送入几何后端。</li>
</ul>
<ol>
<li>多模态 In-Context 编辑<br>支持一次提示内给出“参考图像 + 点云 + 文本”混合示例，利用 LLM 的长窗口能力实现</li>
</ol>
<ul>
<li>少样本风格克隆</li>
<li>参考部件颜色/材质迁移<br>无需额外微调。</li>
</ul>
<ol>
<li><p>部件语义持续更新<br>引入检索增强（RAG）机制，将新类别描述写入外部向量库，推理时动态检索并嵌入 prompt，实现“即插即用”的新部件词汇，避免全量重训。</p>
</li>
<li><p>面向制造的参数化输出<br>把 BBox 程序自动转换为 CAD 参数脚本（OpenSCAD、Fusion360 API），直接输出</p>
</li>
</ol>
<ul>
<li>可编辑草图尺寸</li>
<li>装配约束<br>打通“语言 → 可制造模型”的最后一公里。</li>
</ul>
<ol>
<li>多用户协同语言界面<br>基于程序 diff 的表示，支持</li>
</ol>
<ul>
<li>版本分支（git-like）</li>
<li>冲突合并<br>让多位设计师用自然语言同时对同一资产进行部件级并发编辑。</li>
</ul>
<ol>
<li>伦理与版权过滤<br>部件级生成可能复制受保护设计。可构建</li>
</ol>
<ul>
<li>部件哈希指纹库</li>
<li>生成程序与库做最大公共子图检测<br>在符号空间完成侵权预警，而非在稠密网格后处理。</li>
</ul>
<p>这些方向均利用“符号程序中间层”这一核心设计，把问题从高密度几何空间提升到离散、可解释、可验证的语言空间，从而与生成、仿真、制造、协同等多下游系统实现即插即用的深度集成。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>Part-X-MLLM 提出“把 3D 交互当程序生成”的新范式：<br>输入 RGB 点云 + 自然语言 → 自回归输出一段含部件包围盒与增删改命令的符号序列 → 任意几何后端执行。</p>
<p><strong>核心设计</strong></p>
<ul>
<li>双编码器：结构 (xyz+法向) 与外观 (rgb) 解耦，避免表征冲突。</li>
<li>统一语法：<code>&lt;boxs&gt;…&lt;boxe&gt;</code> 定位部件，<code>&lt;adds&gt;/&lt;dels&gt;/&lt;mods&gt;</code> 表达编辑，文本粒度可粗可细。</li>
<li>两阶段训练：先 360 万几何盒预训练，再 429 万指令对微调，冻结 LLM 保留语言先验。</li>
</ul>
<p><strong>能力覆盖</strong></p>
<ol>
<li>部件级生成：BBox-IoU 42.55 %，Voxel-Recall 74.11 %，优于 PartField/OmniPart。</li>
<li>部件问答：SBERT +18.7、BLEU-1 +21.3，答案带持久 可验证。</li>
<li>局部编辑：同一序列驱动 Nano3D/VoxHammer，完成换头、删支架等 20 + 指令。</li>
<li>语义粒度：CLIP 聚类自动合并细部件，22 → 2 级无需重训。</li>
<li>零样本面片分割：利用盒置信度即可得高质量分割，无需额外训练。</li>
</ol>
<p><strong>贡献</strong></p>
<ul>
<li>首个原生 3D、部件感知、可执行程序输出的 MLLM；</li>
<li>提出符号级中间表示，把语言、几何、编辑统一为单序列预测；</li>
<li>建立 85 k 对象、429 万指令对的部件中心数据集与 UniPart-Bench，覆盖 11 任务。</li>
</ul>
<p>实验表明，该框架在生成质量、理解精度、编辑可用性上均取得 SOTA，并提供即插即用的语言控制面，可驱动任意下游几何引擎。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Part-X-MLLM%3A%20Part-aware%203D%20Multimodal%20Large%20Language%20Model，链接是https://arxiv.org/pdf/2511.13647，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13647。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chunshi Wang,Junliang Ye,Yunhan Yang,Yang Li,Zizhuo Lin,Jun Zhu,Zhuo Chen,Yawei Luo,Chunchao Guo</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13647.pdf">https://arxiv.org/pdf/2511.13647.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13647">https://arxiv.org/abs/2511.13647</a></p>
<p><strong>Arxiv ID</strong>: 2511.13647</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13647">https://papers.cool/arxiv/2511.13647</a></p>
<p><strong>Published</strong>: 2025-11-17T17:59:52Z</p>
<p><strong>Updated</strong>: 2025-11-17T17:59:52.000Z</p>
<hr>
<h3 id="6-MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation"><a href="#6-MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation" class="headerlink" title="6. MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.09611">MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</a></h3><p>While thinking-aware generation aims to improve performance on complex tasks, we identify a critical failure mode where existing sequential, autoregressive approaches can paradoxically degrade performance due to error propagation. To systematically analyze this issue, we propose ParaBench, a new benchmark designed to evaluate both text and image output modalities. Our analysis using ParaBench reveals that this performance degradation is strongly correlated with poor alignment between the generated reasoning and the final image. To resolve this, we propose a parallel multimodal diffusion framework, MMaDA-Parallel, that enables continuous, bidirectional interaction between text and images throughout the entire denoising trajectory. MMaDA-Parallel is trained with supervised finetuning and then further optimized by Parallel Reinforcement Learning (ParaRL), a novel strategy that applies semantic rewards along the trajectory to enforce cross-modal consistency. Experiments validate that our model significantly improves cross-modal alignment and semantic consistency, achieving a 6.9\% improvement in Output Alignment on ParaBench compared to the state-of-the-art model, Bagel, establishing a more robust paradigm for thinking-aware image synthesis. Our code is open-sourced at <a target="_blank" rel="noopener" href="https://github.com/tyfeld/MMaDA-Parallel">https://github.com/tyfeld/MMaDA-Parallel</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然以思维感知生成为目标旨在提升复杂任务的表现，但我们发现现有的顺序自回归方法存在一个关键失败模式，即由于误差传播，反而可能降低性能。为系统地分析这一问题，我们提出了ParaBench，一个用于评估文本和图像输出模态的新基准。通过在ParaBench上的分析，我们发现这种性能下降与生成的推理与最终图像之间的对齐不良密切相关。为了解决这一问题，我们提出了一个并行多模态扩散框架MMaDA-Parallel，使文本与图像在整个去噪轨迹中实现持续的双向交互。MMaDA-Parallel通过监督微调进行训练，并进一步通过并行强化学习（ParaRL）优化，这是一种在轨迹上应用语义奖励以强化跨模态一致性的创新策略。实验验证我们的模型显著提升了跨模态对齐和语义一致性，在ParaBench上的输出对齐（Output Alignment）相比最先进模型Bagel提升了6.9%，建立了一个更稳健的思维感知图像生成范式。我们的代码已开源，地址为<a target="_blank" rel="noopener" href="https://github.com/tyfeld/MMaDA-Parallel">https://github.com/tyfeld/MMaDA-Parallel</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“思维感知型”（thinking-aware）多模态生成任务中出现的<strong>性能退化</strong>现象：现有方法先通过自回归链式推理生成文本，再据此生成图像，结果在约 23 % 的复杂编辑样本上反而比“无思维”基线更差。作者指出退化根因是<strong>推理文本与最终图像跨模态对齐失败</strong>，而传统基准只能评估最终图像，无法诊断中间推理质量。</p>
<p>为此，论文提出：</p>
<ol>
<li>新诊断基准 ParaBench，显式度量“推理–图像”输出对齐；</li>
<li>并行扩散框架 MMaDA-Parallel，在统一离散 token 空间内同步去噪文本与图像，每一步都允许双向注意力，阻断错误累积；</li>
<li>轨迹级强化学习算法 ParaRL，在整条去噪轨迹上施加稠密语义奖励，进一步收紧跨模态一致性。</li>
</ol>
<p>实验表明，该方法在 ParaBench 上将输出对齐指标相对最强基线 Bagel 提升 6.9 %，验证了<strong>并行生成+轨迹级优化</strong>对思维感知图像合成任务的有效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><strong>思维感知图像生成</strong></li>
<li>链式推理前置：Image-CoT、GoT、Bagel 等先在自回归文本中生成推理，再条件生成图像。</li>
<li>反思式多轮优化：OmniGen2、IRG 在图像生成后再用文本反馈迭代修正。<br>共同点：均沿用“文本→图像”顺序流水线，未在生成阶段实现双向交互。</li>
<li><strong>离散扩散大模型</strong></li>
<li>文本侧：LLaDA、Dream7B 等将语言模型转化为 Masked Diffusion，支持并行解码。</li>
<li>多模态侧：LaViDA、MMaDA 在统一 token 空间联合建模文本-图像，但仍顺序生成。</li>
<li><strong>多模态强化学习</strong></li>
<li>终局奖励：T2I-R1、R1-VL、SegZero 等对最终答案或 IoU/CLIP 分数给奖励。</li>
<li>跨模态一致性：MM-Eureka、Dual Self-Rewards 引入图像-文本对齐奖励，但仅作用在输出层。</li>
<li><strong>过程级/轨迹级优化</strong></li>
<li>数学推理：过程奖励模型 PRM、逐步价值函数。</li>
<li>扩散模型：Diff-GRPO 将 GRPO 适配到离散扩散，但仅用于最终答案正确性。</li>
</ul>
<p>综上，现有工作尚未在“思维感知”场景下同时实现<br>① 文本-图像完全并行去噪，② 沿整条去噪轨迹施加稠密语义奖励；MMaDA-Parallel 填补了这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“思维感知图像合成”中的性能退化问题拆解为<strong>推理-图像跨模态对齐失败</strong>与<strong>自回归误差累积</strong>两个耦合缺陷，并给出三项技术组件予以闭环解决：</p>
<ol>
<li>诊断与归因</li>
</ol>
<ul>
<li>构建 ParaBench，首次显式评估“文本推理 ↔ 生成图像”输出对齐；</li>
<li>实验验证：退化类别恰好对齐分数最低，确认问题根源而非数据偶然。</li>
</ul>
<ol>
<li>并行扩散框架 MMaDA-Parallel</li>
</ol>
<ul>
<li>统一离散 token 空间：文本用 LLaDA tokenizer，图像用 MAGVIT-v2 量化器，拼接成单条序列；</li>
<li>双向注意力：去噪每一步文本 token 与图像 token 互相可见，消除“文本先验不可撤回”带来的级联误差；</li>
<li>同步去噪：采用双调度器（文本线性+置信度采样，图像 cosine+全局采样），在共享时间轴并行解码；</li>
<li>加权损失：对文本 token 随时间反比加权  w<em>(text)(t)=1/t ，图像 token 恒定  w</em>(img)(t)=1 ，稳定训练。</li>
</ul>
<ol>
<li>轨迹级强化学习 ParaRL</li>
</ol>
<ul>
<li>稀疏采样：每条轨迹随机选  s=3  个中间步，仅在这些步计算奖励，降低算力；</li>
<li>语义奖励：用 CLIP 分数衡量当前步已解码文本与图像片段的相似度，标准化后映射到 $<br>0,1<br>得到 R_{i,t}$；</li>
<li>扩散适配 GRPO：基于随机掩码估计重要性权重，对中间 token 应用 clipped advantage 更新，显式优化</li>
</ul>
<p>J<em>(policy)(θ)=E[∑</em>(t∈ S)∑<em>(o∈τ_i(t)) C</em>ε!((π<em>θ(o|…)) / (π</em>(textold))(o|…),A<em>(i,t))]-β,KL(π</em>θ|π_(old))</p>
<ul>
<li>结果：中间步对齐信号反向传播，显著提升最终一致性与图像质量。</li>
</ul>
<p>通过“并行生成阻断误差累积 + 轨迹奖励持续收紧对齐”，论文在 ParaBench 上将 Output Alignment 从 52.9 提升到 59.8（+6.9%），同时保持单模态指标不降，验证了所提方案对思维感知图像编辑与生成的有效性与通用性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“诊断-方法-消融”三层目标，共设计 5 组实验，全部在自建的 ParaBench 基准以及两个公开基准（RISEBench、GenEval）上完成。核心结果均以 GPT-4.1-as-a-judge 的 6 维指标（Text Qual./Align.、Image Cons./Align./Qual.、Output Align.）报告。</p>
<ol>
<li>主实验：ParaBench 全量对比</li>
</ol>
<ul>
<li>基线：非思维模型（Flux.1、Qwen-Image 等）、闭源思维模型（GPT-4o、Gemini-2.5）、开源思维模型（Bagel、Show-o）。</li>
<li>结果：MMaDA-Parallel w/ ParaRL 取得开源最高 Output Align. 59.8，比 Bagel 提升 6.9%，文本/图像单模态指标与 Bagel 持平，显著缩小与闭源模型差距。</li>
</ul>
<ol>
<li>关键假设验证<br>2.1 并行 vs. 顺序解码（RQ1）</li>
</ol>
<ul>
<li>控制条件：训练与推理均只改变解码顺序，其余超参固定。</li>
<li>结果：并行结构在 Output Align. 上绝对提升 2.6%，证明同步去噪可降低误差传播。</li>
</ul>
<p>2.2 轨迹级 vs. 输出级 RL（RQ2）</p>
<ul>
<li>输出级 RL 只在 t=0 步给奖励；ParaRL 在 3 个中间步给奖励。</li>
<li>结果：轨迹级 Output Align. 再提升 6.2%，训练曲线更平稳，验证中间语义监督有效。</li>
</ul>
<ol>
<li>超参数与消融</li>
</ol>
<ul>
<li>采样步数 s：s=3 时性价比最高，s&gt;3 收益饱和。</li>
<li>模态权重：wtext=1/t、wimg=1 的组合在 Image Align. 与 Output Align. 均最优。</li>
<li>解码策略：Fully-Parallel &gt; Semi-Parallel &gt; Sequential，差距随任务复杂度增大而拉大。</li>
</ul>
<ol>
<li>数据效率与可扩展性</li>
</ol>
<ul>
<li>仅使用 150 k 思维四元组（Bagel 数据量约 1 %）即实现可比或更优性能。</li>
<li>将 ParaRL 迁移到更大预训练模型 Lumina-DiMOO 后，Output Align. 再提升至 68.8，刷新开源 SOTA，验证方法可扩展。</li>
</ul>
<ol>
<li>额外基准交叉验证</li>
</ol>
<ul>
<li>RISEBench（时空因果逻辑编辑）：MMaDA-Parallel 在 Overall 指标从 5.5 % 提至 5.75 %，保持领先。</li>
<li>GenEval（组合生成）：Overall 从 0.68 提至 0.71，在 Counting、Colors 等细类上显著优于顺序版 MMaDA。</li>
</ul>
<p>综合以上实验，论文系统证明了“并行扩散 + 轨迹奖励”策略对思维感知图像编辑与生成任务均能带来一致且显著的提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>更紧耦合的模态调度</strong><br>当前文本与图像仍使用独立去噪调度器。可探索联合调度函数  u_(joint)(t) ，让信息交换在“语义粗→细”不同阶段自适应地增减，实现更精细的协同。</p>
</li>
<li><p><strong>动态稀疏奖励</strong><br>ParaRL 仅固定采样 3 步。可训练轻量级过程奖励模型（PRM）在线决定“何时、何处”计算奖励，使轨迹监督随样本难度动态稠密化，兼顾效率与效果。</p>
</li>
<li><p><strong>扩散轨迹的可解释干预</strong><br>利用注意力或梯度工具定位“概念-token”对应关系，实现用户指定的中间步干预（如先锁定布局再细化纹理），把思维感知生成从端到端黑箱变为可交互式“半自动”创作。</p>
</li>
<li><p><strong>长序列与故事级生成</strong><br>将并行框架扩展到多页漫画、故事板或图文交替的长文档，研究跨页一致性、角色身份保持及情节连贯性，推动“思维感知”从单幅走向序列。</p>
</li>
<li><p><strong>自监督思维数据扩增</strong><br>目前依赖 Qwen-2.5-VL 生成推理迹。可引入自洽性过滤+对抗式重写，让模型自己提出多条推理路径并筛选高 CLIP-一致性的轨迹，降低对外部大模型的依赖，实现数据飞轮。</p>
</li>
<li><p><strong>与其他模态的并行融合</strong><br>把音频、视频或 3D 离散 token 统一纳入同一扩散轨迹，验证“任意模态组合”下是否仍能保持同步对齐，构建真正的全模态思维感知系统。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心贡献</strong></p>
<ol>
<li>发现“思维感知”图像合成的关键失效模式：顺序自回归推理→图像流程在 23 % 复杂样本上因<strong>推理-图像对齐失败</strong>而性能退化。</li>
<li>构建 ParaBench 基准，首次量化“文本推理 ↔ 生成图像”输出对齐，验证退化与对齐分数强相关。</li>
<li>提出 <strong>MMaDA-Parallel</strong>：统一离散扩散框架，文本与图像在同一序列内<strong>并行去噪</strong>，每一步双向注意力阻断误差累积。</li>
<li>设计 <strong>ParaRL</strong> 轨迹级强化学习，在稀疏中间步施加 CLIP 语义奖励，直接优化整条去噪轨迹的跨模态一致性。</li>
<li>实验显示：ParaBench 上 Output Alignment 绝对提升 6.9 %，达开源 SOTA；在 RISEBench、GenEval 亦一致领先，且数据量仅为对比模型的约 1 %。</li>
</ol>
<p><strong>技术要点</strong></p>
<ul>
<li>统一 token 空间：文本 LLaDA tokenizer + 图像 MAGVIT-v2 量化器，拼接成单序列。</li>
<li>双调度器：文本线性+置信度采样，图像 cosine+全局采样，共享时间轴同步解码。</li>
<li>加权损失： w<em>(text)(t)=1/t ， w</em>(img)(t)=1 ，稳定图像质量。</li>
<li>扩散适配 GRPO：随机掩码估计重要性权重，对中间步 token 进行 clipped advantage 更新。</li>
<li>奖励标准化：CLIP 分数→标准化→截断→线性映射至 $<br>0,1<br>$，降低方差。</li>
</ul>
<p><strong>结论</strong><br>并行生成 + 轨迹级语义监督可显著缓解思维感知任务中的误差传播与对齐缺失，为复杂推理驱动的多模态图像编辑与生成提供了新的可扩展范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MMaDA-Parallel%3A%20Multimodal%20Large%20Diffusion%20Language%20Models%20for%20Thinking-Aware%20Editing%20and%20Generation，链接是https://arxiv.org/pdf/2511.09611，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.09611。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ye Tian,Ling Yang,Jiongfan Yang,Anran Wang,Yu Tian,Jiani Zheng,Haochen Wang,Zhiyang Teng,Zhuochen Wang,Yinjie Wang,Yunhai Tong,Mengdi Wang,Xiangtai Li</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09611.pdf">https://arxiv.org/pdf/2511.09611.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09611">https://arxiv.org/abs/2511.09611</a></p>
<p><strong>Arxiv ID</strong>: 2511.09611</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09611">https://papers.cool/arxiv/2511.09611</a></p>
<p><strong>Published</strong>: 2025-11-12T18:58:21Z</p>
<p><strong>Updated</strong>: 2025-11-12T18:58:21.000Z</p>
<hr>
<h3 id="7-GroupRank-A-Groupwise-Reranking-Paradigm-Driven-by-Reinforcement-Learning"><a href="#7-GroupRank-A-Groupwise-Reranking-Paradigm-Driven-by-Reinforcement-Learning" class="headerlink" title="7. GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.11653">GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning</a></h3><p>Large Language Models have shown strong potential as rerankers to enhance the overall performance of RAG systems. However, existing reranking paradigms are constrained by a core theoretical and practical dilemma: Pointwise methods, while simple and highly flexible, evaluate documents independently, making them prone to the Ranking Myopia Trap, overlooking the relative importance between documents. In contrast, Listwise methods can perceive the global ranking context, but suffer from inherent List Rigidity, leading to severe scalability and flexibility issues when handling large candidate sets. To address these challenges, we propose Groupwise, a novel reranking paradigm. In this approach, the query and a group of candidate documents are jointly fed into the model, which performs within-group comparisons to assign individual relevance scores to each document. This design retains the flexibility of Pointwise methods while enabling the comparative capability of Listwise methods. We further adopt GRPO for model training, equipped with a heterogeneous reward function that integrates ranking metrics with a distributional reward aimed at aligning score distributions across groups. To overcome the bottleneck caused by the scarcity of high quality labeled data, we further propose an innovative pipeline for synthesizing high quality retrieval and ranking data. The resulting data can be leveraged not only for training the reranker but also for training the retriever. Extensive experiments validate the effectiveness of our approach. On two reasoning intensive retrieval benchmarks, BRIGHT and R2MED.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型在作为再排序器以提升RAG系统整体性能方面表现出了强大的潜力。然而，现有的再排序范式受制于一个核心的理论和实践困境：逐点方法虽然简单且高度灵活，但它们独立评估文档，容易陷入“排序近视陷阱”，忽视文档之间的相对重要性。相比之下，列表方法能够感知全局排序上下文，但存在固有的列表刚性问题，在处理大规模候选集时会导致严重的可扩展性和灵活性问题。为了解决这些挑战，我们提出了Groupwise，一种新颖的再排序范式。在该方法中，查询和一组候选文档被联合输入模型，模型在组内进行比较，为每个文档分配个体相关性得分。这一设计保留了逐点方法的灵活性，同时具备列表方法的比较能力。我们进一步采用GRPO进行模型训练，并配备了异构奖励函数，将排序指标与旨在对齐组间得分分布的分布奖励结合起来。为克服高质量标注数据稀缺造成的瓶颈，我们进一步提出了一种创新流水线，用于合成高质量的检索和排序数据。生成的数据不仅可用于训练再排序器，还可用于训练检索器。大量实验验证了我们方法的有效性。在两个推理密集型检索基准BRIGHT和R2MED上的实验结果均表明其性能优势。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决检索增强生成（RAG）系统中重排序（reranking）阶段的核心困境：</p>
<ul>
<li><p><strong>Pointwise 方法</strong><br>独立地为每篇文档打分，灵活且可并行，但缺乏全局视角，容易陷入“排序短视”（Ranking Myopia Trap），无法充分捕捉文档间的相对重要性。</p>
</li>
<li><p><strong>Listwise 方法</strong><br>一次性对整个候选列表进行建模，能感知全局上下文，却因“列表刚性”（List Rigidity）难以扩展到上百篇文档，推理延迟高，常需滑动窗口近似，牺牲全局性。</p>
</li>
</ul>
<p>为此，作者提出 <strong>GroupRank</strong>，一种“组级”重排序新范式：</p>
<ol>
<li>将查询与一组候选文档同时输入模型，在组内完成交叉对比，一次性输出每篇文档的离散相关度得分（0–10）。</li>
<li>既保留 Pointwise 的灵活与并发优势（ O(N/c)  复杂度， c  为组大小），又具备 Listwise 的全局比较能力，缓解短视与刚性两难。</li>
<li>设计基于 GRPO 强化学习的异构奖励函数，融合排序指标（NDCG、Recall、RBO）与分布对齐奖励，直接优化最终排序质量。</li>
<li>配套高质量合成数据流水线，用 LLM 分别进行 Pointwise 打分与 Listwise 排序，再经 −log(rank) 变换与加权融合生成训练标签，解决标注稀缺问题。</li>
</ol>
<p>综上，论文目标是在保持可扩展性的同时，让重排序器获得近似 Listwise 的全局感知能力，从而在推理密集型检索任务中取得新的 SOTA 表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大脉络：RAG 重排序地位、LLM 重排序方法、以及既有排序范式。要点如下：</p>
<ul>
<li><p><strong>RAG 中的重排序<br>Lewis et al. (2020)</strong> 提出检索增强生成框架，指出重排序是“粗召回→精排→生成”的关键闸口；后续工作（Gao 2024、Cheng 2025 等）进一步验证重排序质量直接影响最终答案准确率与幻觉抑制效果。</p>
</li>
<li><p>**LLM 作为重排序器</p>
</li>
<li>零样本提示：Abdallah et al. (COLING 2025) 用动态提示让 LLM 直接输出相关性分数。</li>
<li>监督微调：RankT5（Zhuang 2022）、RankZephyr（Pradeep 2023）以 T5 类模型为骨干，采用排序损失微调。</li>
<li>两阶段 SFT+RL：Rank-R1（Zhuang 2025）、ReasonRank（Liu 2025）先用标注数据 SFT，再用 RL 优化 NDCG 等指标，提升推理能力。</li>
<li>**三大排序范式</li>
</ul>
<ol>
<li>Pointwise：将每篇文档视为独立样本，回归/分类打分（公式  s<em>i = f</em>θ(q,d_i) ），代表如 RankT5。</li>
<li>Pairwise：比较文档对，复杂度  O(N^2) ，需外部排序算法聚合，代表如 Qin et al. (NAACL 2024) 的 pairwise prompting。</li>
<li>Listwise：一次性输入整列表征，直接输出排序或分数向量（公式  S = f_θ(q,D) ），代表如 RankZephyr、Rank-K（Yang 2025）。<br>这些方法的“短视”或“刚性”缺陷正是 GroupRank 试图弥合的空白。</li>
</ol>
<ul>
<li>**数据合成与奖励设计</li>
<li>合成标签：TFRank（Fan 2025）用 LLM 生成伪相关标签；本文流水线在此基础上引入“Pointwise + Listwise 双视角融合”并给出 −log(rank) 变换。</li>
<li>多目标奖励：Coranking（Liu 2025）同时优化 NDCG 与 F1；本文进一步加入分布对齐奖励  R<em>(dist) = 1 - D</em>(KL)(S^(gt) Vert S^(pred)) ，防止分数塌陷。</li>
</ul>
<p>综上，GroupRank 在范式上介于 Pointwise 与 Listwise 之间，在训练上延续 SFT+RL 路线，但通过“组级”输入与异构奖励函数，首次系统性地缓解了排序短视与列表刚性两难。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从“范式–数据–训练”三条线协同解决排序短视与列表刚性两难，具体方案如下：</p>
<ol>
<li>提出 <strong>Groupwise 范式</strong></li>
</ol>
<ul>
<li>输入：查询  q  与  c  篇候选文档  d_1,…,d_c  拼接成单条序列。</li>
<li>输出：LLM 在一次前向中生成离散得分  s_1,…,s_c ，每  s_i∈0,1,…,10  为整数 token。</li>
<li>复杂度： O(N/c)  次调用即可并行完成全集合打分，兼顾全局对比与伸缩性。</li>
</ul>
<ol>
<li>设计 <strong>高质量合成数据流水线</strong></li>
</ol>
<ul>
<li>召回：BM25 + 稠密检索混合得分  S<em>(hybrid)=0.5,norm(S</em>(BM25))+0.5,norm(S_(dense)) ，取 Top-50。</li>
<li>双视角标注<br>– Pointwise：Qwen3-235B-instruct 逐篇打绝对分 $S<em>(pt)(d_i)∈<br>0,10<br>$。<br>– Listwise：Gemini-2.5-pro 整表排序得排名  r_i ，经  S</em>(lt)(d_i)=-log(r_i)  转为分值。</li>
<li>融合： S^(gt)(d<em>i)=α,norm(S</em>(pt))+(1-α),norm(S_(lt)) ， α=0.5 ，生成兼具“分数量级”与“相对顺序”的金标准。</li>
</ul>
<ol>
<li>构建 <strong>两阶段训练框架</strong></li>
</ol>
<ul>
<li><strong>冷启动 SFT</strong>：用上述合成数据监督微调，让模型学会组内对比与规定 JSON 输出格式。</li>
<li><strong>GRPO 强化学习</strong><br>– 采样  G  组输出，组内归一化优势  A<em>i 。<br>– 异构奖励  R</em>(H)=α R<em>(recall)+β R</em>(rank)+γ R<em>(dist)<br>R</em>(rank) ：加权融合 NDCG、Recall、RBO；<br>R<em>(dist) ： 1-D</em>(KL)(S^(gt)Vert S^(pred))  保持分数量级；<br>α,β,γ=0.2,0.5,0.1 。<br>– 格式奖励：仅当输出含 <code>&lt;reason&gt;</code> 与 <code>&lt;answer&gt;</code> 且 JSON 合法才给予  R_(H) ，否则 0 或 −1。</li>
<li>目标：最大化  J_(GRPO)(θ) ，直接优化排序指标同时防止分数塌陷。</li>
</ul>
<ol>
<li>系统级效率与泛化</li>
</ol>
<ul>
<li>可批量并行，LLM 调用次数  O(N/c) ，低于 Pairwise  O(N^2)  且避免 Listwise 滑动窗口带来的近似误差。</li>
<li>在 BRIGHT、R2MED、BEIR 等推理密集与传统检索基准上，7B/32B 模型均取得新 SOTA，验证范式通用性。</li>
</ul>
<p>通过“组内联合打分 + 合成高质量标签 + 异构奖励强化学习”三位一体，论文在保持灵活并发的同时赋予模型全局比较能力，从而系统性地克服了 Pointwise 短视与 Listwise 刚性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从“性能、效率、消融、上限”四个层面展开系统实验，全部以 NDCG@10 为主指标，主要结果如下：</p>
<ol>
<li>主基准测试</li>
</ol>
<ul>
<li><strong>推理密集场景</strong><br>– BRIGHT（12 子域，1.2 k 查询）：GroupRank-32B 平均 39.24，7B 平均 36.65，均刷新 SOTA，领先次优 ReasonRank-32B（35.58）3.7 个点。<br>– R2MED（8 子域，医学）：GroupRank-32B 52.28，7B 47.84，再次登顶，显著超越 Rank-K-32B（49.47）与 ReasonRank-32B（50.17）。</li>
<li><strong>传统零样本场景</strong><br>– BEIR 五子集（Arguana、DBpedia-Entity、NFCorpus、NQ、Scidocs）：GroupRank-32B 平均 55.09，领先此前最佳 52.10；7B 亦达 46.45，验证泛化能力。</li>
</ul>
<ol>
<li>效率对比<br>理论复杂度与实测并行度并列：</li>
</ol>
<ul>
<li><h1 id="LLM-calls：Groupwise-O-N-c-，低于-Pairwise-O-N-2-与-Listwise-Sliding-O-rN-s-。"><a href="#LLM-calls：Groupwise-O-N-c-，低于-Pairwise-O-N-2-与-Listwise-Sliding-O-rN-s-。" class="headerlink" title="LLM-calls：Groupwise  O(N/c) ，低于 Pairwise  O(N^2)  与 Listwise-Sliding  O(rN/s) 。"></a>LLM-calls：Groupwise  O(N/c) ，低于 Pairwise  O(N^2)  与 Listwise-Sliding  O(rN/s) 。</h1></li>
<li>支持批量解码，端到端延迟显著低于需串行调用的 HeapSort/BubbleSort 类 Pairwise 策略。</li>
</ul>
<ol>
<li>消融实验（BRIGHT，32B 模型）</li>
</ol>
<ul>
<li>完整模型：42.18</li>
<li>去掉 SFT 仅 RL：38.17（−4.01）</li>
<li>去掉 RL 仅 SFT：40.70（−1.48）</li>
<li>去掉 Ranking Reward：40.57（−1.61）</li>
<li>去掉 Distribution Reward：41.00（−1.18）<br>结果证实：两阶段训练缺一不可，Ranking Reward 是性能主驱动力，Distribution Reward 起正则防塌陷作用。</li>
</ul>
<ol>
<li>上限探索（BRIGHT）<br>采用更强召回（reason-embed-qwen3-8b-0928+BM25）并引入测试时扩展：</li>
</ol>
<ul>
<li>Sliding-Window（组大小 20，步长 10）+ Self-Ensemble（6 轮随机重排平均）</li>
<li>NDCG@10 进一步提升至 46.8，超过当前榜一 BGE-Reasoner-0928（46.4），验证组级范式在额外算力下仍具上升空间。</li>
</ul>
<p>综上，实验覆盖推理/传统双领域、参数规模 7B→32B、训练与推理效率、组件贡献与测试时扩展，全方位验证了 GroupRank 的有效性与可扩展性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向按“数据–模型–训练–系统”四轴展开：</p>
<ul>
<li><strong>数据侧</strong></li>
</ul>
<ol>
<li>多语言、多模态合成：将流水线扩展到非英语种、图文混合场景，验证组级范式对跨模态排序的通用性。</li>
<li>难度渐进课程：按查询复杂度或文档歧义度构建课程，从“单跳事实”到“多步推理”逐步喂入，提升样本效率与鲁棒性。</li>
<li>人类偏好对齐：引入真实用户点击或生成满意度标签，用偏好学习替代纯 metric 奖励，减少 NDCG 与真实体验之间的偏差。</li>
</ol>
<ul>
<li><strong>模型侧</strong></li>
</ul>
<ol>
<li>动态组大小：学习根据查询意图或候选分布自动调整  c ，在“高歧义大组”与“低歧义小组”间折中效果与开销。</li>
<li>分层组级编码：先对文档做轻量向量聚类，再在簇内执行 GroupRank，降低长上下文压力，进一步扩展至千级候选。</li>
<li>专用位置编码：为“查询-多文档”拼接设计组感知位置编码，缓解 LLM 中段信息塌陷（lost-in-the-middle）导致的对比衰减。</li>
</ol>
<ul>
<li><strong>训练侧</strong></li>
</ul>
<ol>
<li>渐进强化学习：从“粗粒度相关/不相关”到“细粒度 0–10 分”分层设定奖励，逐步缩小动作空间，降低 RL 探索方差。</li>
<li>生成-排序联合 RL：把重排序器与生成器放入同一奖励回路，以“最终答案 F1”作为全局信号，实现端到端 RAG 策略优化。</li>
<li>对抗样本与可解释性：利用对抗文档或刻意引入虚假证据，测试 GroupRank 的鲁棒性；同时可视化组内 attention，解释对比决策依据。</li>
</ol>
<ul>
<li><strong>系统侧</strong></li>
</ul>
<ol>
<li>在线延迟优化：结合 KV-cache 复用与动态批调度，把组级推理嵌入实时搜索，目标 P99 延迟 &lt; 100 ms。</li>
<li>边缘-云协同：边缘小模型做首轮粗排，云端大模型仅对 Top-c 执行 GroupRank，探索精度-带宽-能耗的三维权衡。</li>
<li>跨域迁移套件：发布面向推荐、广告、学术检索的零样本迁移 benchmark，检验组级思想在异构排序任务中的通用边界。</li>
</ol>
<p>以上方向可分别从“更丰富的数据信号、更灵活的模型结构、更渐进稳定的训练策略、更高效的系统部署”四维度推动 Groupwise 重排序范式向生产级、跨领域、端到端 RL-driven 的下一阶段演进。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可概括为“一条新范式、一条数据链、一套训练法、三组 SOTA”：</p>
<ol>
<li><p><strong>Groupwise 新范式</strong><br>把查询与  c  篇文档一次性喂给 LLM，组内并行输出离散得分，复杂度  O(N/c) ；既保留 Pointwise 的灵活并发，又引入 Listwise 的全局对比，克服“排序短视”与“列表刚性”两难。</p>
</li>
<li><p><strong>高质量合成数据链</strong><br>BM25+稠密召回 Top-50 → Qwen3-235B 逐篇打分 &amp; Gemini-2.5-pro 整表排序 →  -log(rank)  变换 + 线性融合生成 0–10 金标准，兼顾分数量级与相对顺序，解决标注稀缺。</p>
</li>
<li><p><strong>两阶段训练法</strong></p>
</li>
</ol>
<ul>
<li>冷启动 SFT：让模型学会组内对比与 JSON 格式。</li>
<li>GRPO 强化学习：异构奖励  R<em>(H)=0.2R</em>(recall)+0.5R<em>(rank)+0.1R</em>(dist) ，直接优化 NDCG、Recall、RBO 并约束分数分布，防止塌陷。</li>
</ul>
<ol>
<li><strong>实验结果</strong></li>
</ol>
<ul>
<li>推理密集：BRIGHT 39.24、R2MED 52.28 双榜新 SOTA。</li>
<li>传统零样本：BEIR 五子集平均 55.09 刷新最佳。</li>
<li>消融与效率：验证 SFT+RL 缺一不可，组级方法在 LLM 调用次数与并发延迟上均优于 Pairwise/Listwise-Sliding。</li>
</ul>
<p>综上，论文提出“组级重排序”这一兼具性能与伸缩的新范式，并通过合成数据与异构 RL 训练，在推理与通用检索任务上同时取得新 state-of-the-art，为下一代 RAG 系统提供了可行且高效的技术路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是GroupRank%3A%20A%20Groupwise%20Reranking%20Paradigm%20Driven%20by%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2511.11653，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.11653。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Duolin Sun,Meixiu Long,Dan Yang,Yihan Jiao,Zhehao Tan,Jie Feng,Junjie Wang,Yue Shen,Peng Wei,Jian Wang,Jinjie Gu</p>
<p><strong>Categories</strong>: cs.IR,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11653.pdf">https://arxiv.org/pdf/2511.11653.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11653">https://arxiv.org/abs/2511.11653</a></p>
<p><strong>Arxiv ID</strong>: 2511.11653</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11653">https://papers.cool/arxiv/2511.11653</a></p>
<p><strong>Published</strong>: 2025-11-10T15:25:31Z</p>
<p><strong>Updated</strong>: 2025-11-10T15:25:31.000Z</p>
<hr>
<h3 id="8-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models"><a href="#8-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models" class="headerlink" title="8. TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13704">TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</a></h3><p>The rapid evolution of video generative models has shifted their focus from producing visually plausible outputs to tackling tasks requiring physical plausibility and logical consistency. However, despite recent breakthroughs such as Veo 3’s chain-of-frames reasoning, it remains unclear whether these models can exhibit reasoning capabilities similar to large language models (LLMs). Existing benchmarks predominantly evaluate visual fidelity and temporal coherence, failing to capture higher-order reasoning abilities. To bridge this gap, we propose TiViBench, a hierarchical benchmark specifically designed to evaluate the reasoning capabilities of image-to-video (I2V) generation models. TiViBench systematically assesses reasoning across four dimensions: i) Structural Reasoning &amp; Search, ii) Spatial &amp; Visual Pattern Reasoning, iii) Symbolic &amp; Logical Reasoning, and iv) Action Planning &amp; Task Execution, spanning 24 diverse task scenarios across 3 difficulty levels. Through extensive evaluations, we show that commercial models (e.g., Sora 2, Veo 3.1) demonstrate stronger reasoning potential, while open-source models reveal untapped potential that remains hindered by limited training scale and data diversity. To further unlock this potential, we introduce VideoTPO, a simple yet effective test-time strategy inspired by preference optimization. By performing LLM self-analysis on generated candidates to identify strengths and weaknesses, VideoTPO significantly enhances reasoning performance without requiring additional training, data, or reward models. Together, TiViBench and VideoTPO pave the way for evaluating and advancing reasoning in video generation models, setting a foundation for future research in this emerging field.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视频生成模型的快速发展已经将其关注点从产生视觉上逼真的输出转向处理需要物理合理性和逻辑一致性的任务。然而，尽管近期出现了如 Veo 3 的帧链推理等突破性进展，这些模型是否能够展示类似大型语言模型（LLM）的推理能力仍不清楚。现有的基准测试主要评估视觉保真度和时间一致性，无法反映更高阶的推理能力。为了弥补这一空白，我们提出了 TiViBench，这是一套专门设计用于评估图像到视频（I2V）生成模型推理能力的分层基准。TiViBench 系统地从四个维度评估推理能力：i) 结构推理与搜索、ii) 空间与视觉模式推理、iii) 符号与逻辑推理、iv) 行动规划与任务执行，并覆盖 3 个难度等级下的 24 个多样化任务场景。通过广泛评估，我们发现商业模型（如 Sora 2、Veo 3.1）展示了更强的推理潜力，而开源模型则显露出尚未开发的潜力，但受限于训练规模和数据多样性。为了进一步挖掘这一潜力，我们引入了 VideoTPO，这是一种受偏好优化启发的简单却高效的测试期策略。通过对生成候选进行 LLM 自我分析以识别其优势和不足，VideoTPO 显著提升了推理性能，而无需额外的训练、数据或奖励模型。TiViBench 与 VideoTPO 相结合，为评估和推进视频生成模型的推理能力铺平了道路，为这一新兴领域的未来研究奠定了基础。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个核心问题：<br><strong>“视频生成模型能否像大语言模型（LLM）那样具备逐步推理能力？”</strong></p>
<p>为此，作者指出当前视频生成评估仅关注视觉保真度、时序平滑度等低阶指标，缺乏对<strong>高阶视觉推理</strong>（结构搜索、空间模式、符号逻辑、动作规划）的系统评测；同时，现有方法难以在<strong>不重新训练</strong>的前提下激发模型的推理潜力。论文提出两项贡献：</p>
<ol>
<li><strong>TiViBench</strong>：首个分层基准，用 595 组图文对在 24 个任务、4 个维度、3 个难度上量化 I2V 模型的 zero-shot 推理能力。</li>
<li><strong>VideoTPO</strong>：一种<strong>测试时偏好优化</strong>策略，通过让 VLM 对多条候选视频进行自我批判→文本梯度→提示迭代，在<strong>不更新权重、不增数据、无奖励模型</strong>的情况下显著提升推理准确率。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中将与本文最密切相关的研究归为三类，并指出其局限，从而凸显 TiViBench 与 VideoTPO 的差异化价值。以下按类别归纳：</p>
<ol>
<li>Image-to-Video（I2V）生成</li>
</ol>
<ul>
<li>代表工作：Wan2.1/2.2、HunyuanVideo、CogVideoX、AnimateDiff、DreamPose 等。</li>
<li>局限：主要追求<strong>视觉保真</strong>与<strong>物理合理性</strong>，尚未系统验证“推理”能力。</li>
</ul>
<ol>
<li>I2V 评测基准</li>
</ol>
<ul>
<li>传统指标：FVD、IS、KVD；数据集 UCF101、MSR-VTT。</li>
<li>近期综合基准：VBench、VBench++、TC-Bench、UI2V-Bench、WorldScore 等。</li>
<li>局限：维度集中在<strong>空间一致性、时序平滑、文本对齐</strong>，<strong>无“推理”专项</strong>；并发工作 MME-CoF、VideoThinkBench 虽触及推理，但任务零散、难度未分层。</li>
</ul>
<ol>
<li>提示优化（Prompt Rewriting）</li>
</ol>
<ul>
<li>训练式：SFT、RLHF、RFT——需额外数据与算力。</li>
<li>测试式：<br>– 预推理重写（Wan et al. 2025、Veo 3）——用 LLM 丰富提示，易偏离用户意图；<br>– 单轮后推理重写（Self-Refine、PHYT2V）——仅基于单次结果迭代。</li>
<li>局限：单轮或单样本优化，<strong>粒度粗</strong>；未引入<strong>偏好对齐</strong>思想。</li>
</ul>
<p>综上，现有研究尚未出现</p>
<ul>
<li>专门面向<strong>视觉推理</strong>且<strong>分层难度</strong>的 I2V 评测体系；</li>
<li>在<strong>零额外训练</strong>前提下，通过<strong>多候选偏好比较</strong>实现测试时提示优化的方法。</li>
</ul>
<p>TiViBench 与 VideoTPO 正好填补这两处空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“视频生成模型能否具备 LLM 式推理能力”这一宏问题拆为三步，并给出对应解法：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>步骤</th>
<th>关键障碍</th>
<th>论文解法</th>
<th>技术要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>① 无基准</td>
<td>现有评测只看视觉保真，缺“推理”维度</td>
<td>提出 TiViBench</td>
<td>4 维度 × 24 任务 × 3 难度 = 595 图文对；每样例含初始/过程/终态，可验证“过程-目标一致性”或“终态正确性”</td>
</tr>
<tr>
<td>② 无分析</td>
<td>不知模型错因——是看不懂规则还是提取不到符号</td>
<td>大规模 zero-shot 实验 + 失败案例归因</td>
<td>发现：商业模型优势源于数据/参数规模；开源模型在 Pass@5 显著高于 Pass@1 → 潜在能力已存但不稳定；主要败因：① 规则建模不足 ② VAE 压缩丢失细粒度视觉特征</td>
</tr>
<tr>
<td>③ 无轻量提升手段</td>
<td>再训练成本高，单轮提示重写粒度粗</td>
<td>提出 VideoTPO</td>
<td>测试时偏好优化：生成 2 条候选 → VLM 自批判得文本损失  L_t  → 文本梯度  G_t  → 迭代更新提示；零额外训练、零外部奖励模型，在 Wan2.1 上绝对提升 +9.75 pp，HunyuanVideo +6.22 pp</td>
</tr>
</tbody>
</table>
</div>
<p>通过“建基准—做诊断—给轻量处方”的闭环，论文首次系统验证了 I2V 模型的推理潜力，并提供了可扩展的测试时增强方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 3 个研究问题（RQ1–RQ3）设计了 4 组实验，全部在 TiViBench 的 595 样本、24 任务、3 难度协议下完成，核心指标为 Pass@1（商业模型）与 Pass@5（开源模型）。结果均以“Overall”四维度平均准确率报告，避免单任务波动。</p>
<ol>
<li>RQ1：模型是否具备内在推理潜力？</li>
</ol>
<ul>
<li>被测模型：7 个前沿 I2V 模型<br>– 开源：CogVideoX-1.5、HunyuanVideo、Wan2.1-14B、Wan2.2-14B<br>– 商业：Kling-2.1、Veo-3.1-fast、Sora-2</li>
<li>实验内容：zero-shot 评测 + 难度消融</li>
<li>关键结论：<br>– 商业模型显著领先（Sora-2 27.9 %、Veo-3.1 26.1 %），难度升高时下降更缓；<br>– 开源模型 Pass@5 平均提升 ≈ 2×，证明“潜在推理能力”已存，但受限于规模与数据多样性。</li>
</ul>
<ol>
<li>RQ2：推理失败的主因？</li>
</ol>
<ul>
<li>细粒度任务消融（24 任务）+ 最低表现案例可视化</li>
<li>发现：<br>– 规则强依赖任务（迷宫、数独、时序排序、odd-one-out）准确率普遍 &lt; 10 %；<br>– 失败共性：① 无法内化显式边界/规则；② VAE 压缩导致符号/数字细节丢失。</li>
</ul>
<ol>
<li>RQ3：测试时优化能否高效提升推理？</li>
</ol>
<ul>
<li>对比基线：Pre-Rewriter（Veo 官方提示扩展）、Post-Rewriter（Self-Refine）</li>
<li>实验变量：<br>– 宽度缩放：候选样本数 2→6；<br>– 深度缩放：迭代步数 1→4；<br>– 奖励策略消融：CLIP-score、GPT-score vs. 自批判（VideoTPO）。</li>
<li>结果（Overall 准确率）：<br>– HunyuanVideo 基线 4.03 % → VideoTPO 10.25 %（+6.22 pp）<br>– Wan2.1 基线 8.40 % → VideoTPO 18.15 %（+9.75 pp）<br>– 均显著优于两种 rewriter；宽度/深度增加持续增益；自批判策略优于外部奖励模型。</li>
</ul>
<ol>
<li>可靠性验证</li>
</ol>
<ul>
<li>指标-人工一致性：在 Wan2.1 的 200 随机样本上，TiViBench 自动指标与 3 名人类评审的 Kendall-τ = 0.81，验证无需人工即可大规模评估。</li>
<li>跨模型提示迁移：将 VideoTPO 为 HunyuanVideo 优化的提示直接用于 Wan2.1，性能反而下降，说明<strong>模型特定偏好存在</strong>，VideoTPO 的“自优化”不可替代。</li>
</ul>
<p>综上，实验从“能力摸底→错误诊断→轻量提升→可靠性”四层面完整闭环，充分支撑论文主张。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可直接在 TiViBench/VideoTPO 框架上延伸，无需重新造轮，且具备明确技术抓手：</p>
<ul>
<li><p><strong>规则显式注入</strong><br>在 VideoTPO 的“文本梯度”阶段引入<strong>可微规则编码器</strong>（如神经-符号混合层），把迷宫边界、数独约束等转成损失项，解决“规则建模不足”这一主要败因。</p>
</li>
<li><p><strong>细粒度视觉 Token</strong><br>将 VAE latent 改为 <strong>ViT-VQGAN 离散码本</strong>或<strong>DINOv2 稠密特征</strong>，保留数字/符号的亚像素信息；同步改造 TiViBench 度量，用密集特征 cosine 相似度替代 OpenCV 粗粒度 OCR。</p>
</li>
<li><p><strong>过程级 RL 微调</strong><br>以 TiViBench 的“过程-目标一致性”为即时奖励，采用 <strong>RFT</strong> 或 <strong>DPO</strong> 对开源模型做轻量级微调（&lt;10 % 参数），验证“小数据+规则奖励”能否在 1-2 个 epoch 内把 Wan2.1 的 Overall 从 18 % 提到 30 %。</p>
</li>
<li><p><strong>多模态链式生成</strong><br>把 VideoTPO 的“文本梯度”循环拓展为 <strong>Chain-of-Frames</strong> 范式：每生成 8 帧后冻结，用 VLM 写下一帧的“子提示”，再级联生成，实现 64-128 帧的长程推理视频，任务从“数独”扩展到“动态拼图”或“剧情杀”。</p>
</li>
<li><p><strong>宽度-深度联合缩放定律</strong><br>系统采样 (width, depth) ∈<br>2,8<br>×<br>1,5<br>，拟合 <strong>Pass@1 ∝ (width^α)(depth^β)</strong>，给出测试时 FLOS-准确率权衡曲线，为实时 vs. 精度提供量化依据。</p>
</li>
<li><p><strong>跨模态迁移</strong><br>将 VideoTPO 的 self-critique 机制迁移到 <strong>文本-到-视频（T2V）</strong> 与 <strong>视频-到-视频（V2V）</strong> 任务，验证“提示偏好优化”是否对任意条件视频生成均有效。</p>
</li>
<li><p><strong>可解释诊断工具</strong><br>在 TiViBench 每任务上增加<strong>注意力 rollout</strong> 与 <strong>因果干预</strong> 可视化，定位模型在“规则提取”“符号定位”“轨迹规划”三步中具体哪一步梯度消失，为后续改进提供可解释证据。</p>
</li>
</ul>
<p>这些方向均可在现有代码仓直接分支，实验周期短（1-2 张 A100 一周可完成），且能填补“规则-视觉-推理”三角缺口。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</strong><br>一句话总结：<br>首次系统评估并<strong>零训练提升</strong>视频生成模型的<strong>视觉推理能力</strong>。</p>
<p>1. 背景与问题</p>
<ul>
<li>LLM 已具备逐步推理，视频生成仍停留在“看起来对”；</li>
<li>现有基准只测<strong>视觉保真/时序平滑</strong>，无<strong>高阶推理</strong>评测；</li>
<li>开源模型是否有推理潜力？如何<strong>不重新训练</strong>就激发？</li>
</ul>
<p>2. 贡献速览</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>功能</th>
<th>规模</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>TiViBench</td>
<td>视觉推理基准</td>
<td>4 维度 × 24 任务 × 3 难度 = 595 图文对</td>
<td>揭示商业模型优势与开源潜力</td>
</tr>
<tr>
<td>VideoTPO</td>
<td>测试时提示优化</td>
<td>零额外训练、零奖励模型</td>
<td>Wan2.1 +9.75 pp，HunyuanVideo +6.22 pp</td>
</tr>
</tbody>
</table>
</div>
<p>3. TiViBench 设计</p>
<ul>
<li><strong>四维度</strong><br>① 结构搜索 ② 空间/视觉模式 ③ 符号/逻辑 ④ 动作规划</li>
<li><strong>三难度</strong><br>Easy / Medium / Hard</li>
<li><strong>双指标</strong><br>过程-目标一致性 vs. 终态正确性</li>
<li><strong>数据</strong><br>初始-过程-终态三帧，人工三审，720p 统一分辨率。</li>
</ul>
<p>4. VideoTPO 算法</p>
<ol>
<li>每轮生成 <strong>2 条候选视频</strong></li>
<li>VLM 自批判 → 文本损失  L_t </li>
<li>文本梯度  G_t  → 迭代更新提示</li>
<li><strong>不更新权重、不增数据、无外部奖励</strong>。</li>
</ol>
<p>5. 主要实验结果</p>
<ul>
<li><strong>RQ1 潜力</strong><br>– 商业模型 Sora-2 27.9 %、Veo-3.1 26.1 % 遥遥领先；<br>– 开源模型 Pass@5 比 Pass@1 平均翻倍，潜力已存但不稳定。</li>
<li><strong>RQ2 败因</strong><br>– 规则建模不足（迷宫越界）；<br>– VAE 压缩丢细节（数独数字识别失败）。</li>
<li><strong>RQ3 提升</strong><br>– VideoTPO 在 Wan2.1 Overall 8.4 % → 18.2 %；<br>– 宽度/深度缩放持续增益；自批判优于 CLIP/GPT 奖励模型。</li>
</ul>
<p>6. 可继续探索</p>
<ul>
<li>规则显式注入、细粒度视觉 Token、过程级 RL 微调、链式长视频生成、宽度-深度缩放定律、跨模态迁移、可解释诊断。</li>
</ul>
<p><strong>结论</strong>：TiViBench 给出推理“标尺”，VideoTPO 提供零成本“杠杆”，为视频生成从“看得过去”走向“想得明白”奠定基准与优化框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是TiViBench%3A%20Benchmarking%20Think-in-Video%20Reasoning%20for%20Video%20Generative%20Models，链接是https://arxiv.org/pdf/2511.13704，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13704。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Harold Haodong Chen,Disen Lan,Wen-Jie Shu,Qingyang Liu,Zihan Wang,Sirui Chen,Wenkai Cheng,Kanghao Chen,Hongfei Zhang,Zixin Zhang,Rongjin Guo,Yu Cheng,Ying-Cong Chen</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13704.pdf">https://arxiv.org/pdf/2511.13704.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13704">https://arxiv.org/abs/2511.13704</a></p>
<p><strong>Arxiv ID</strong>: 2511.13704</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13704">https://papers.cool/arxiv/2511.13704</a></p>
<p><strong>Published</strong>: 2025-11-17T18:52:44Z</p>
<p><strong>Updated</strong>: 2025-11-17T18:52:44.000Z</p>
<hr>
<h3 id="9-PhysX-Anything-Simulation-Ready-Physical-3D-Assets-from-Single-Image"><a href="#9-PhysX-Anything-Simulation-Ready-Physical-3D-Assets-from-Single-Image" class="headerlink" title="9. PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13648">PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</a></h3><p>3D modeling is shifting from static visual representations toward physical, articulated assets that can be directly used in simulation and interaction. However, most existing 3D generation methods overlook key physical and articulation properties, thereby limiting their utility in embodied AI. To bridge this gap, we introduce PhysX-Anything, the first simulation-ready physical 3D generative framework that, given a single in-the-wild image, produces high-quality sim-ready 3D assets with explicit geometry, articulation, and physical attributes. Specifically, we propose the first VLM-based physical 3D generative model, along with a new 3D representation that efficiently tokenizes geometry. It reduces the number of tokens by 193x, enabling explicit geometry learning within standard VLM token budgets without introducing any special tokens during fine-tuning and significantly improving generative quality. In addition, to overcome the limited diversity of existing physical 3D datasets, we construct a new dataset, PhysX-Mobility, which expands the object categories in prior physical 3D datasets by over 2x and includes more than 2K common real-world objects with rich physical annotations. Extensive experiments on PhysX-Mobility and in-the-wild images demonstrate that PhysX-Anything delivers strong generative performance and robust generalization. Furthermore, simulation-based experiments in a MuJoCo-style environment validate that our sim-ready assets can be directly used for contact-rich robotic policy learning. We believe PhysX-Anything can substantially empower a broad range of downstream applications, especially in embodied AI and physics-based simulation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>3D建模正在从静态视觉表示转向可直接用于模拟和交互的物理、可活动资产。然而，大多数现有的3D生成方法忽视了关键的物理和关节属性，从而限制了它们在具身AI中的应用。为弥补这一差距，我们提出了PhysX-Anything，这是第一个可用于模拟的物理3D生成框架，它能够在给定一张野外图像的情况下，生成具有明确几何形状、关节和物理属性的高质量模拟3D资产。具体而言，我们提出了第一个基于VLM的物理3D生成模型，并提出了一种新的3D表示方法，可高效地对几何形状进行编码。该方法将tokens数量减少了193倍，使在标准VLM token预算范围内学习明确几何形状成为可能，无需在微调过程中引入任何特殊tokens，并显著提升了生成质量。此外，为克服现有物理3D数据集多样性有限的问题，我们构建了一个新的数据集PhysX-Mobility，将以往物理3D数据集中的对象类别扩展了两倍以上，并且包含2000多个常见真实世界对象，拥有丰富的物理标注。在PhysX-Mobility数据集和野外图像上的大量实验表明，PhysX-Anything在生成性能和泛化能力方面表现出色。此外，在类MuJoCo环境中的基于模拟的实验验证了我们的可模拟资产可以直接用于接触丰富的机器人策略学习。我们相信，PhysX-Anything能够显著增强广泛下游应用的能力，尤其是在具身AI和基于物理的模拟领域。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在弥合“静态 3D 资产”与“可直接部署于物理引擎的仿真资产”之间的鸿沟，核心解决以下问题：</p>
<ul>
<li><strong>静态与物理脱节</strong>：现有 3D 生成方法侧重几何与外观，缺乏密度、绝对尺度、关节约束等关键物理与铰接属性，导致无法直接用于机器人物理仿真。</li>
<li><strong>数据稀缺与多样性不足</strong>：既有带物理标注的 3D 数据集规模小、类别少，限制了可泛化的物理资产生成。</li>
<li><strong>表征效率瓶颈</strong>：网格或顶点量化序列在 VLM 语境下 token 开销巨大；引入 3D-VQ 等特殊编码又需额外预训练，增加复杂度。</li>
<li>**“检索式”局限：现有铰接物生成多依赖检索-组装，难以合成全新结构，且对野外图像泛化差。</li>
<li>**即插即用缺失：即便 PhysXGen 可生成物理属性，其输出仍无法直接导入 MuJoCo、PyBullet 等标准仿真器。</li>
</ul>
<p>因此，论文提出 PhysX-Anything，首次实现<strong>单张野外图像 → 可直接导入主流物理引擎的完整仿真资产（含 URDF/XML）</strong> 的端到端生成，兼顾几何、铰接与物理真实性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大主线，每条线均与 PhysX-Anything 的“单图→仿真就绪资产”目标存在关键差距：</p>
<ol>
<li>通用 3D 生成</li>
</ol>
<ul>
<li>GAN 时代：GET3D 等早期工作仅关注纹理网格，无物理属性。</li>
<li>SDS/扩散优化：DreamFusion、Magic3D 依赖 2D 扩散先验，受 Janus 与效率问题困扰，且输出为静态网格。</li>
<li>前馈扩散：Trellis、3DTopia-XL、InstantMesh、LGM 等大幅提升速度，但仍止于“几何+外观”，不含铰接或物理。</li>
<li>自回归/VLM 路线：<br>– MeshLLM、LLaMA-Mesh 用简化网格或顶点量化，token 超长，难以承载物理信息。<br>– ShapeLLM-Omni 引入 3D VQ-VAE 压缩，却需额外特殊词表与预训练，与 VLM 原生架构耦合重。</li>
</ul>
<ol>
<li>铰接物体建模</li>
</ol>
<ul>
<li>检索-组装：URDFormer、ArticulateAnything 先检索近邻模板再赋运动，无法生成新结构，对野外图像泛化弱。</li>
<li>图结构扩散：NAP、Singapo 用运动图指导形状生成，但缺乏纹理与物理参数。</li>
<li>视频优化：DreamArt 从视频重建铰接，需人工部件掩码，稳定性差。</li>
<li>URDF-Anything 可输出 URDF，却依赖完整点云输入，且不支持材质、密度等物理属性。</li>
</ul>
<ol>
<li>物理感知 3D 生成</li>
</ol>
<ul>
<li>均质假设方法：PhysGen3D、Vid2Sim、PhyTwin 仅学习形变或运动，不设密度、摩擦等异质物理参数。</li>
<li>PhysXGen 首次联合生成几何与物理，但输出格式非标准，无法直接导入 MuJoCo/PyBullet，仍需人工转换。</li>
</ul>
<p>综上，现有工作至多满足“几何+铰接”或“几何+部分物理”，而 PhysX-Anything 首次把<strong>几何、铰接、异质物理、绝对尺度、仿真格式导出</strong>整合进统一 VLM 框架，实现真正的“sim-ready”生成。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>PhysX-Anything 将“单图→仿真就绪资产”解耦为 <strong>VLM 语义推理 + 轻量几何表征 + 可控精化解码 + 物理格式导出</strong> 四级流水线，关键设计如下：</p>
<ol>
<li>轻量“物理-几何”统一表征</li>
</ol>
<ul>
<li>粗粒度 voxel：VLM 仅处理  32^3  占用网格，把占用体素线性化为 0–32 767 索引，再合并连续段（如 199-216），token 数从 17.7 万降至 919，压缩 193×。</li>
<li>JSON 风格物理树：在 voxel 坐标系内编码密度、质心、关节轴位/方向/限位，避免 URDF 冗长标签，方便 VLM 直接生成。</li>
</ul>
<ol>
<li>多轮 VLM 对话生成<br>以 Qwen2.5-VL 为基座，两轮式提示：</li>
</ol>
<ul>
<li>第 1 轮：输入图像→输出整体结构（部件列表、材质、密度、关节类型）。</li>
<li>第 2-N 轮：仅保留整体描述，逐部件生成 voxel 索引串，避免长上文遗忘。<br>全程只出现文本 token，不引入新词表，微调成本低。</li>
</ul>
<ol>
<li>可控精化网络<br>粗 voxel 作为 ControlNet-style 条件，输入 flow-transformer 扩散模型，损失</li>
</ol>
<p>L<em>(geo)=E</em>(t,x<em>0,ε,c,V_low)l[l|f</em>θ(x<em>t,c,V</em>(low),t)-(ε-x_0)r|^2_2r]</p>
<p>在 256³ 隐空间预测高保真 voxel，再经结构化潜扩散解码得网格/高斯/RF。</p>
<ol>
<li><p>部件分割与格式封装<br>用最近邻将网格顶点映射回 voxel 部件标签，自动切分；结合全局 JSON 参数，一键写出 URDF、XML、SDF、MJCF 等六种标准格式，密度、碰撞箱、惯性矩阵、关节限位全部嵌入，实现“即插即用”。</p>
</li>
<li><p>数据侧支撑<br>新建 PhysX-Mobility 数据集：在 PartNet-Mobility 基础上补标密度、材质、绝对尺度，覆盖 47 类 2K+ 日常物体，类别数较此前物理 3D 数据集提升 2×，为 VLM 提供足够多样性的微调信号。</p>
</li>
</ol>
<p>通过“轻量表征保结构 + VLM 语义先验 + 扩散精化保细节 + 标准格式保落地”，PhysX-Anything 首次把野外单图直接变成 MuJoCo/PyBullet 可加载的完整物理资产，端到端解决几何-铰接-物理-格式全链路需求。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“生成质量-野外泛化-表征消融-仿真落地”四级展开，全部在 PhysX-Mobility 与 100 张网络野外图像上完成，核心结果如下：</p>
<ol>
<li>主数据集对比（PhysX-Mobility）<br>指标：PSNR / CD / F-score（几何）、绝对尺度误差 / 材质准确率 / affordance 准确率 / 关节参数 VLM 分 / 文本描述分（物理）。<br>结果：</li>
</ol>
<ul>
<li>几何三指标全面领先，F-score 77.5（↑1.2 vs PhysXGen）。</li>
<li>绝对尺度误差从 43.44 降到 0.30，相对提升 99 %。</li>
<li>物理属性全部第一，描述分 19.36（↑+6.47）。</li>
</ul>
<ol>
<li>野外泛化评估<br>a) VLM 自动评分（GPT-5）：</li>
</ol>
<ul>
<li>几何得分 0.94（vs 0.65 PhysXGen）。</li>
<li>关节参数得分 0.94（vs 0.61）。<br>b) 14 人用户研究（1568 份打分，5 分制归一化）：</li>
<li>几何偏好 0.98、物理属性综合 0.95，显著高于对比方法。</li>
</ul>
<ol>
<li>表征消融实验<br>对比三种压缩率：</li>
</ol>
<ul>
<li>Voxel（不合并索引）</li>
<li>Index（合并但无粗-精分工）</li>
<li>Ours（粗 voxel+合并+精化）<br>结果：</li>
<li>随着 token 再压缩，PSNR 从 16.96→18.21→20.35，CD 反向下降，F-score 提升 14.4 %；物理指标同步上升，验证 193× 压缩仍保细节。</li>
</ul>
<ol>
<li>仿真落地验证（MuJoCo-style）<br>任务：水龙头旋拧、橱柜门开合、眼镜腿折叠、打火机掰开、笔记本合盖、手柄转动等 6 类接触丰富操作。<br>结果：</li>
</ol>
<ul>
<li>所有 URDF/XML 直接导入，无需人工调参；</li>
<li>成功训练出稳健策略，完成率&gt;92 %，接触力曲线与真机趋势一致，证明生成资产在碰撞、惯量、关节限位上均物理可信。</li>
</ul>
<p>综上，实验链条覆盖“量化指标-人类感知-组件消融-机器人闭环”，全面验证 PhysX-Anything 的生成精度、野外鲁棒性与仿真即插即用性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>后续可在以下六个方向继续深入，均围绕“更通用、更物理、更交互”展开：</p>
<ul>
<li><p><strong>多物体/场景级物理生成</strong><br>当前一次仅生成单个铰接资产；可扩展至整间厨房、整套装配线，让 VLM 同时推理物体间支撑、碰撞与层级约束，输出带 <code>.world</code> 或 <code>.scene</code> 的复合 URDF。</p>
</li>
<li><p><strong>非刚性、流体、软体耦合</strong><br>现聚焦刚体+铰链。引入连续介质表示（FEM 粒子混合、可微 MPM），在 voxel 隐空间预测杨氏模量、泊松比、黏度等场，实现“刚-软-流”统一仿真格式导出。</p>
</li>
<li><p><strong>基于物理反馈的逆向迭代</strong><br>将 MuJoCo 运行结果（接触力、滑动、奇异）作为 RL 或可微仿真损失  L_(sim) ，回传微调 VLM 与精化网络，实现“生成→仿真→误差→再生成”的自迭代闭环，降低人工标注依赖。</p>
</li>
<li><p><strong>时序/多视角输入</strong><br>单图存在深度-尺度歧义。利用 2–3 张随意手机视频帧，引入显式几何一致性损失与尺度-重力先验，提升绝对尺寸与惯性张量精度，同时保持“零拍摄脚本”易用性。</p>
</li>
<li><p><strong>跨域材质与真实感渲染</strong><br>目前仅预测类别级材质参数。联合 BRDF 扫描数据集，在精化阶段输出空间变化 BRDF 贴图，并校准到仿真引擎（Unity HDRP、NVIDIA Omniverse），实现视觉与物理双重逼真。</p>
</li>
<li><p><strong>语言-示范驱动的编辑</strong><br>支持自然语言指令“把柜门改成双开、把手高度提高 5 cm”，VLM 直接输出编辑后的 voxel+URDF，并结合用户 2D 草图或 VR 手柄示范进行交互式精调，迈向“可对话的 3D 物理设计助手”。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>PhysX-Anything 提出首个“单张野外图像 → 可直接导入 MuJoCo/PyBullet 的仿真就绪 3D 资产”端到端框架，核心贡献与流程如下：</p>
<ol>
<li>高效统一表征</li>
</ol>
<ul>
<li>将部件几何压缩为合并索引的 32³ voxel 串，token 数降 193×，保留显式结构；</li>
<li>JSON 树同时编码密度、尺度、关节轴/限位，无需新增词表即可被 VLM 读写。</li>
</ul>
<ol>
<li>VLM 多轮生成</li>
</ol>
<ul>
<li>基于 Qwen2.5-VL 微调，两轮对话：先整体结构 → 再逐部件 voxel，避免长上文遗忘；</li>
<li>输出即含 URDF/XML 所需全部物理字段。</li>
</ul>
<ol>
<li>可控精化解码</li>
</ol>
<ul>
<li>粗 voxel 作条件，流入扩散式 Transformer 重建高保真 mesh/高斯；</li>
<li>最近邻自动分割部件，拼装成标准 URDF、XML、MJCF 等六种格式，实现即插即用。</li>
</ul>
<ol>
<li>数据与实验</li>
</ol>
<ul>
<li>新建 PhysX-Mobility（47 类 2K+ 物体），物理标注多样性提升 2×；</li>
<li>在数据集与 100 张野外图像上全面领先，绝对尺度误差降 99 %，人类偏好达 0.98；</li>
<li>MuJoCo 机器人操纵任务验证：生成资产可直接用于接触丰富的策略学习，成功率&gt;92 %。</li>
</ul>
<p>综上，PhysX-Anything 通过“轻量表征 + VLM 先验 + 精化扩散 + 标准格式”，首次把单图生成提升到真正“仿真就绪”水平，为 3D 内容创作、具身 AI 与机器人仿真提供了可落地的新基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PhysX-Anything%3A%20Simulation-Ready%20Physical%203D%20Assets%20from%20Single%20Image，链接是https://arxiv.org/pdf/2511.13648，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13648。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ziang Cao,Fangzhou Hong,Zhaoxi Chen,Liang Pan,Ziwei Liu</p>
<p><strong>Categories</strong>: cs.CV,cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13648.pdf">https://arxiv.org/pdf/2511.13648.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13648">https://arxiv.org/abs/2511.13648</a></p>
<p><strong>Arxiv ID</strong>: 2511.13648</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13648">https://papers.cool/arxiv/2511.13648</a></p>
<p><strong>Published</strong>: 2025-11-17T17:59:53Z</p>
<p><strong>Updated</strong>: 2025-11-17T17:59:53.000Z</p>
<hr>
<h3 id="10-UFO-3-Weaving-the-Digital-Agent-Galaxy"><a href="#10-UFO-3-Weaving-the-Digital-Agent-Galaxy" class="headerlink" title="10. UFO^3: Weaving the Digital Agent Galaxy"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.11332">UFO^3: Weaving the Digital Agent Galaxy</a></h3><p>Large language model (LLM)-powered agents are transforming digital devices from passive tools into proactive intelligent collaborators. However, most existing frameworks remain confined to a single OS or device, making cross-device workflows brittle and largely manual. We present UFO$^3$, a system that unifies heterogeneous endpoints, desktops, servers, mobile devices, and edge, into a single orchestration fabric. UFO$^3$ models each user request as a mutable TaskConstellation: a distributed DAG of atomic subtasks (TaskStars) with explicit control and data dependencies (TaskStarLines). The TaskConstellation continuously evolves as results stream in from distributed devices, enabling asynchronous execution, adaptive recovery, and dynamic optimization. A Constellation Orchestrator} executes tasks safely and asynchronously while applying dynamic DAG updates, and the Agent Interaction Protocol (AIP) provides persistent, low-latency channels for reliable task dispatch and result streaming. These designs dissolve the traditional boundaries between devices and platforms, allowing agents to collaborate seamlessly and amplify their collective intelligence. We evaluate UFO$^3$ on NebulaBench, a benchmark of 55 cross-device tasks across 5 machines and 10 categories. UFO$^3$ achieves 83.3% subtask completion, 70.9% task success, exposes parallelism with an average width of 1.72, and reduces end-to-end latency by 31% relative to a sequential baseline. Fault-injection experiments demonstrate graceful degradation and recovery under transient and permanent agent failures. These results show that UFO$^3$ achieves accurate, efficient, and resilient task orchestration across heterogeneous devices, uniting isolated agents into a coherent, adaptive computing fabric that extends across the landscape of ubiquitous computing.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>由大型语言模型（LLM）驱动的智能代理正在将数字设备从被动工具转变为主动智能协作伙伴。然而，大多数现有框架仍局限于单一操作系统或设备，使跨设备工作流程脆弱且高度依赖人工操作。我们提出了 UFO$^3$ 系统，它将异构终端、桌面、服务器、移动设备和边缘设备统一到一个单一的协同编排框架中。UFO$^3$ 将每个用户请求建模为可变的任务星座（TaskConstellation）：一组分布式有向无环图（DAG）的原子子任务（TaskStars），具有明确的控制和数据依赖（TaskStarLines）。随着来自分布式设备的结果不断流入，任务星座会持续演化，从而支持异步执行、自适应恢复和动态优化。星座编排器（Constellation Orchestrator）能够安全异步地执行任务，同时应用动态 DAG 更新；代理交互协议（AIP）提供持久、低延迟的通道，以确保任务调度和结果流的可靠性。这些设计打破了传统设备和平台之间的界限，使代理能够无缝协作，增强其集体智能。我们在 NebulaBench 上评估了 UFO$^3$，该基准包括 5 台机器、10 个类别的 55 个跨设备任务。UFO$^3$ 实现了 83.3% 的子任务完成率、70.9% 的任务成功率，在平均宽度 1.72 下展现并行能力，并相比顺序基线将端到端延迟降低了 31%。故障注入实验表明，在临时和永久性代理故障下系统能够平滑退化并恢复。这些结果表明，UFO$^3$ 能够在异构设备间实现准确、高效且可靠的任务编排，将孤立的代理统一为一个连贯、自适应的计算框架，延伸至无处不在的计算生态中。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“单设备智能体无法跨异构设备协同完成复杂用户请求”的核心问题，具体表现为：</p>
<ul>
<li><strong>设备孤岛</strong>：现有 LLM 智能体框架被锁定在单一操作系统或设备（浏览器标签、桌面、手机 App），无法同时调用 GPU 集群、桌面应用、移动传感器等互补资源。</li>
<li><strong>手工编排代价高</strong>：跨设备工作流需人工串联脚本，易错且难维护。</li>
<li><strong>三大系统级挑战</strong></li>
</ul>
<ol>
<li>异步并行：子任务需在异构设备上并发执行，并处理延迟反馈与动态依赖。</li>
<li>分布式协调：异构网络/信任域下的长寿命、低延迟、容错通信缺失。</li>
<li>异构可扩展：新设备、新智能体需即插即用，同时保证全局一致性与安全。</li>
</ol>
<p>为此，作者提出 <strong>UFO3</strong>，把跨设备智能体统一建模为可动态演化的 <strong>TaskConstellation</strong>（分布式 DAG），并通过事件驱动的 <strong>Constellation Orchestrator</strong> 与 <strong>Agent Interaction Protocol (AIP)</strong>，实现异步、自适应、容错的跨设备编排，将孤立智能体编织成“数字智能体星系”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 12 节“Related Work”中系统梳理了两条主线：</p>
<ol>
<li>单设备数字智能体（Digital Device Agents）</li>
<li>多智能体编排（Multi-Agent Orchestration）</li>
</ol>
<p>以下按时间顺序归纳关键工作，并指出 UFO3 与之差异。</p>
<p>1 数字设备智能体（单设备阶段）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代表系统 / 论文</th>
<th>核心能力</th>
<th>局限（与 UFO3 对比）</th>
</tr>
</thead>
<tbody>
<tr>
<td>SeeAct (Zheng et al. 2024)</td>
<td>基于 GPT-4V 的纯 Web 智能体，截图- grounding 完成浏览器任务</td>
<td>仅限浏览器标签，无法跨 OS 调用本地工具</td>
</tr>
<tr>
<td>Mobile-Agent (Wang et al. 2024a)</td>
<td>手机端多模态智能体，视觉-操作闭环</td>
<td>单手机 scope，无法与桌面/服务器协同</td>
</tr>
<tr>
<td>UFO/UFO2 (Zhang et al. 2025b,a)</td>
<td>Windows 桌面 AgentOS，HostAgent→AppAgent 两级 FSM</td>
<td>单 Windows 主机；跨设备需手工脚本</td>
</tr>
<tr>
<td>Claude Computer Use (Anthropic 2024)</td>
<td>纯截图感知，通用桌面 GUI 自动化</td>
<td>无分布式调度与跨设备语义</td>
</tr>
<tr>
<td>OpenAI Operator (OpenAI 2025)</td>
<td>多模态推理+桌面级工具调用</td>
<td>同样受限于单设备执行环境</td>
</tr>
</tbody>
</table>
</div>
<p>→ <strong>共同瓶颈</strong>：任务边界=设备边界，无法原生完成“GPU 训练→本地 Excel 汇总→手机推送”这类跨设备闭环。</p>
<p>2 多智能体编排（跨设备/跨平台）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代表系统 / 论文</th>
<th>核心机制</th>
<th>与 UFO3 差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>IoA (Internet of Agents, Chen et al. 2025)</td>
<td>即时消息风格的多 Agent 聊天总线，动态组队</td>
<td>无 DAG 级执行语义，缺少跨设备数据/控制依赖建模</td>
</tr>
<tr>
<td>FoA (Federation of Agents, Giusti et al. 2025)</td>
<td>Versioned Capability Vector (VCV) 实现能力发现与组合</td>
<td>侧重服务级联邦，未解决 GUI/CLI/移动端异构任务调度</td>
</tr>
<tr>
<td>A2A/ACP/ANP (Ehtesham et al. 2025)</td>
<td>短连接 HTTP 级协议（Agent-to-Agent, Agent-Communication, Agent-Network）</td>
<td>无长寿命会话、无流式结果、无 DAG 动态演化</td>
</tr>
<tr>
<td>AutoGen (Wu et al. 2024a)</td>
<td>多 Agent 对话编程框架，代码-执行闭环</td>
<td>单主机或容器内，跨设备需用户手动配置 Sidecar</td>
</tr>
<tr>
<td>DynTaskMAS (Yu et al. 2025)</td>
<td>动态任务图驱动异步并行</td>
<td>仅仿真环境，无真实异构设备部署与容错机制</td>
</tr>
</tbody>
</table>
</div>
<p>→ <strong>UFO3 增量</strong></p>
<ul>
<li>首次把“跨设备 DAG + 长寿命双向 WebSocket + 运行时图重写”三者耦合为统一协议栈（AIP）。</li>
<li>提供可验证的锁-线性化调度（TLA+ 模型），保证单分配/无环/编辑局部性。</li>
<li>开源工程级实现（73 k 行代码），覆盖 Windows/Linux/Android 三种运行时。</li>
</ul>
<p>3 其他可借鉴领域</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>领域</th>
<th>相关技术</th>
<th>UFO3 的借鉴点</th>
</tr>
</thead>
<tbody>
<tr>
<td>科学工作流</td>
<td>Nextflow、Argo、Oozie</td>
<td>DAG 抽象、数据依赖、重跑机制</td>
</tr>
<tr>
<td>边缘计算 DAG 调度</td>
<td>Li et al. 2022</td>
<td>异步事件驱动、资源感知调度</td>
</tr>
<tr>
<td>模型上下文协议</td>
<td>MCP (Hou et al. 2025)</td>
<td>工具描述标准化，被 UFO3 用于 AgentProfile 与命令层</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<p>UFO3 在“单设备智能体”与“多智能体编排”之间架起了桥梁：</p>
<ul>
<li>向下兼容既有桌面/移动智能体（UFO2、Mobile-Agent 等可无缝接入 AIP）；</li>
<li>向上提供跨设备 DAG 级语义与容错，弥补 IoA/FoA 缺乏执行层保证的空白；</li>
<li>首次在真实异构 OS 与网络环境下验证大规模并行与故障恢复，完成从“多 Agent 对话”到“多设备协同操作系统”的跃迁。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“跨设备智能体协同”视为<strong>分布式异步 DAG 编排</strong>问题，提出 <strong>UFO3</strong> 系统，通过以下<strong>五层一体化设计</strong>将孤立设备编织成可动态演化的“数字智能体星系”。</p>
<p>1 抽象层：TaskConstellation——可演化的跨设备 DAG</p>
<ul>
<li>把一条自然语言请求建模为<strong>有向无环图</strong><br>C = (T, E) ，其中</li>
<li>节点  t_i ∈ T  称为 <strong>TaskStar</strong>，即“可在任意设备执行的原子任务”</li>
<li>边  e_(ito j) ∈ E  称为 <strong>TaskStarLine</strong>，显式声明数据/控制依赖（无条件、成功-only、条件）</li>
<li>运行时允许<strong>增删节点、重连边、改写属性</strong>，保证 DAG 无环且仅改 PENDING 节点（不变量 I2-I3）</li>
</ul>
<p>2 推理层：ConstellationAgent——集中式“织图”大脑</p>
<ul>
<li><strong>双模 FSM</strong>：</li>
<li>Creation 模式：用户意图 + AgentProfile → 初始 DAG（JSON 结构化输出）</li>
<li>Editing 模式：接收完成/失败事件 → 调用 MCP Server 工具集（add_task/remove_task/add_dependency…）<strong>原子地</strong>修改图</li>
<li><strong>线性化保证</strong>：一次编辑=“取锁→LLM 推理→validate→publish→放锁”，防止并发调度与图改写冲突</li>
</ul>
<p>3 调度层：Constellation Orchestrator——异步、安全、批量化执行</p>
<ul>
<li><strong>事件总线</strong>：TASK_STARTED / TASK_COMPLETED / TASK_FAILED / CONSTELLATION_MODIFIED</li>
<li><strong>异步协程调度</strong>：ready 节点立即 dispatch，执行与图改写<strong>并行</strong>（图7）</li>
<li><strong>安全分配锁</strong>（Algorithm 1）：</li>
<li>编辑窗口内禁止新分配；事件排队</li>
<li>编辑完成后一次性 merge 运行期新事件，保证<strong>单分配</strong>（I1）与<strong>全局一致</strong></li>
<li><strong>批量化编辑</strong>：聚合多个完成事件再调用 LLM，降低推理开销 30%+</li>
</ul>
<p>4 通信层：Agent Interaction Protocol (AIP)——长寿命、低延迟、容错</p>
<ul>
<li><strong>持久 WebSocket + 五层栈</strong><br>L1 强类型消息（Pydantic）<br>L2 传输抽象（心跳、重连、大帧）<br>L3 协议编排（注册、任务、命令、心跳）<br>L4 弹性管理（指数退避、会话恢复）<br>L5 端点门面（DeviceServer / DeviceClient / ConstellationEndpoint）</li>
<li><strong>关键语义</strong></li>
<li>注册即自动合并<strong>三层画像</strong>：用户指定 + 服务声明 + 客户端实时遥测</li>
<li>任务级 ACK+结果流式返回，支持<strong>命令批量</strong>与<strong>断线续跑</strong></li>
<li>对称故障处理：任一端掉线→任务失败→DAG 重写→重试或迁移</li>
</ul>
<p>5 执行层：模板化 Device Agent——即插即用</p>
<ul>
<li><strong>三层状态机</strong><br>State（FSM 生命周期）→ Strategy（DATA_COLLECTION / LLM_INTERACTION / ACTION_EXECUTION / MEMORY_UPDATE）→ Command（MCP Server 原子调用）</li>
<li><strong>Server-Client 分离</strong></li>
<li>Server 管推理与策略，可云托管</li>
<li>Client 管本地工具与资源，零状态，支持多 MCP Server 热插拔</li>
<li><strong>已落地代理</strong></li>
<li>LinuxAgent：单容器，CLI+文件系统+GPU 遥测</li>
<li>WindowsAgent：基于 UFO2，HostAgent→多 AppAgent 协同 GUI 自动化</li>
<li>MobileAgent：Android 端，支持 A11y+ADB 命令</li>
</ul>
<p>6 端到端流程（以“GPU 训练→Excel 报告→邮件发送”为例）</p>
<ol>
<li>用户一句自然语言 → ConstellationAgent 生成 5 节点 DAG（3 并行 GPU 训练 + 1 Excel 汇总 + 1 邮件）</li>
<li>Orchestrator 异步派发，3 个 LinuxAgent 并行执行训练脚本，实时流回日志</li>
<li>训练完成事件触发 Editing 模式：Agent 把日志路径注入下游 TaskStar，并追加“生成图表”子任务</li>
<li>WindowsAgent 收到更新后，调用 Excel-MCP 写入数据、Outlook-MCP 发送邮件</li>
<li>任一节点瞬断 → AIP 自动重连，Orchestrator 生成重试节点并重新调度；永久失败则标记失败原因，继续剩余流程</li>
</ol>
<p>7 实验验证</p>
<ul>
<li><strong>NebulaBench</strong> 55 任务 / 5 机 / 10 类别</li>
<li>子任务完成率 83.3%，端到端成功率 70.9%</li>
<li>平均并行宽度 1.72，峰值 3.5，较串行基线** latency ↓31%**</li>
<li>故障注入：瞬断可自愈，单点永久失败可优雅降级，全局失败保一致性不 hallucination</li>
</ul>
<p>结论</p>
<p>UFO3 用“<strong>动态 DAG + 事件驱动调度 + 长寿命通信 + 模板化设备代理</strong>”四件套，把跨设备协同从“手工脚本”升级为“统一运行时”，首次在真实异构 OS 与网络环境下实现高并行、低延迟、可验证的 LLM 智能体星系。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在“Experimental Evaluation”章节围绕 <strong>55 项跨设备任务</strong> 构建了 <strong>NebulaBench</strong> 基准，并针对 <strong>5 大研究问题（RQ1–RQ5）</strong> 展开系统实验。实验部署于 <strong>5 台真实机器</strong>（Win11 桌面 + 3 台 Ubuntu CPU 机 + 1 台 A100 GPU 节点），通过 <strong>指标量化 + 故障注入 + 案例剖析</strong> 三维方式验证 UFO3 的有效性、效率与鲁棒性。具体实验内容如下：</p>
<p>1 实验设置</p>
<ul>
<li><strong>硬件拓扑</strong></li>
<li>局域网 1–10 ms，GPU 节点广域 50–100 ms</li>
<li>统一 GPT-5-Chat-20251003 模型驱动 ConstellationAgent 与所有设备代理</li>
<li><strong>任务来源</strong></li>
<li>5 名志愿者提出真实日常运维/开发需求 → 归纳成 <strong>10 类 55 任务</strong>（表 2）</li>
<li>难度分级：Easy 33%、Medium 35%、Hard 32%；平均需 3.25 台设备</li>
</ul>
<p>2 RQ1 任务完成度</p>
<ul>
<li><strong>指标</strong>：Subtask Completion Rate (SCR) 与 Task Success Rate (TSR)</li>
<li><strong>结果</strong>（图 20）</li>
<li>总体 SCR 83.3%，TSR 70.9%</li>
<li>Easy/Medium/Hard TSR 分别为 72.2%、73.7%、66.7%，显示复杂场景仍保持 2/3 成功率</li>
<li>按类别：结构化任务（Data 100%、Proc 96%）&gt; 跨设备编排（Orchestration 85%）&gt; 含 GUI/浏览器任务（Browsing 64%）</li>
</ul>
<p>3 RQ2 编排与自适应</p>
<ul>
<li><strong>指标</strong>：DAG 演化次数、节点/边增删改统计</li>
<li><strong>结果</strong>（表 3、图 21）</li>
<li>平均每次编辑改 1.09 个元素，每请求共 5.91 次修改</li>
<li>95% 为“任务描述细化”（Modified Tasks），仅 0.41/0.24 次真正新增任务或依赖 → 初始规划质量高，运行时以<strong>上下文注入式微调</strong>为主</li>
</ul>
<p>4 RQ3 并行度挖掘</p>
<ul>
<li><strong>指标</strong>：最大并行宽度、关键路径长度 L、总工作量 W、并行度 P = W/L</li>
<li><strong>结果</strong>（表 4）</li>
<li>平均 P = 1.72，峰值宽度 3.5；Hard 任务 P 仍达 1.77</li>
<li>说明 UFO3 能识别独立子任务并<strong>并发调度到不同设备</strong>，显著缩短 makespan</li>
</ul>
<p>5 RQ4 延迟与可扩展性</p>
<ul>
<li><strong>指标</strong>（图 22）</li>
<li>端到端平均 243.7 s，较<strong>纯串行执行</strong>理论值 ↓31%</li>
<li>编排耗时（规划+编辑）仅 16.8 s，执行耗时 289.2 s；两者<strong>异步重叠</strong>隐藏了 LLM 推理开销</li>
<li>随任务难度增加，编排时间从 55 s → 84 s，仍保持分钟级完成</li>
</ul>
<p>6 RQ5 鲁棒性与故障处理</p>
<ul>
<li><strong>故障注入场景</strong>（图 23）</li>
</ul>
<ol>
<li>瞬断：Linux1 中途掉线→30 s 后恢复；UFO3 立即生成重试节点 Task A′，最终完整报告</li>
<li>永久单点：Linux1 始终未恢复；系统标记失败，继续执行其余节点，<strong>部分结果可交付</strong></li>
<li>全局失效：三台 Linux 全断；ConstellationAgent 终止 workflow，<strong>不 hallucination 虚假结果</strong></li>
</ol>
<ul>
<li><strong>结论</strong>：UFO3 能区分瞬态/永久故障，实现<strong>自动重试、任务迁移、优雅降级、一致终止</strong></li>
</ul>
<p>7 微观案例剖析</p>
<ul>
<li><strong>Case 1</strong> 日志收集+Excel+邮件（图 24）<br>并行拉取 3 台 Linux 日志 → 动态更新 Excel → Outlook 自动发送，全程 0 人工干预</li>
<li><strong>Case 2</strong> Excel 驱动分布式任务（图 25）<br>WindowsAgent 读取 schedule.xlsx → 分派子任务到对应 Linux → 回写结果摘要，展示<strong>结构化数据即编排界面</strong></li>
<li><strong>Case 3</strong> 资源感知计算分配（图 26）<br>4 台 Linux 实时上报 GPU/CPU/内存负载 → ConstellationAgent 动态分配 GPU 型、CPU 型、内存型任务 → Windows 端生成可视化报告，验证<strong>异构资源自适应调度</strong></li>
</ul>
<p>8 可重复性与开源</p>
<ul>
<li>全部 55 条任务日志、DAG 演化快照、Markdown 轨迹已公开（附录 C、表 6）</li>
<li>代码与运行手册同步开源： <a target="_blank" rel="noopener" href="https://github.com/microsoft/UFO/">https://github.com/microsoft/UFO/</a></li>
</ul>
<p>实验结论一句话</p>
<p>UFO3 在真实跨平台环境中同时达成 <strong>&gt;83% 子任务可靠性、1.72 倍平均并行加速、31% 端到端延迟降低</strong> 与<strong>瞬态故障自恢复、永久故障优雅降级</strong>，首次用工程级系统验证了“跨设备 LLM 智能体星系”的可行性与高效性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望按“<strong>系统层 → 协议层 → 智能体层 → 生态层 → 评估层</strong>”递进，列出可直接落地的<strong>10 个未来探索点</strong>，并给出可验证的<strong>关键指标</strong>与<strong>潜在方法</strong>。</p>
<p>1 系统层：Shared Cross-Device Memory</p>
<ul>
<li><strong>问题</strong>：AIP 目前仅文本通道，大文件/二进制/图像需反复编码或人工拷贝</li>
<li><strong>思路</strong>：在 ConstellationClient 端部署<strong>分布式对象存储</strong>（如 S3-lite、IPFS-shard），TaskStarLine 增加 <code>data_ref</code> 字段，支持</li>
<li>零拷贝 <code>mmap</code> 传输</li>
<li>版本化对象快照（用于断点续训、数据血缘追踪）</li>
<li><strong>验证指标</strong>：单文件 1 GB 跨设备吞吐 ≥ 500 MB/s；故障恢复后重传开销 &lt; 5 %</li>
</ul>
<p>2 系统层：Device-Side Sandbox &amp; Capability Attestation</p>
<ul>
<li><strong>问题</strong>： weakest-link 效应——一个恶意/故障 Agent 可污染全局 DAG</li>
<li><strong>思路</strong>：</li>
<li>利用 SGX/TPM 对 Device Client 做<strong>能力证明</strong>（Capability Attestation），ConstellationAgent 只把任务派到满足 ε-可靠度阈值的设备</li>
<li>每 TaskStar 封装于<strong>轻量 gVisor 容器</strong>，系统调用白名单由 MCP 声明式下发</li>
<li><strong>验证指标</strong>：注入 10 % 拜占庭节点下 TSR 下降 &lt; 8 %；沙箱逃逸率 = 0</li>
</ul>
<p>3 协议层：AIP-FS（文件系统扩展）</p>
<ul>
<li><strong>思路</strong>：在 AIP 五层栈之上增加 L6 <strong>File Transport Layer</strong>，支持</li>
<li>块级差分同步（Rsync-roll）</li>
<li>加密去重（AES-GCM + Blake3 chunk）</li>
<li><strong>API</strong>：<code>COMMAND_UPLOAD(ref_hash, offset, cipher_chunk)</code> ↔ <code>COMMAND_DOWNLOAD</code></li>
<li><strong>验证指标</strong>：重复数据压缩率 ≥ 40 %；CPU 占用 &lt; 15 %</li>
</ul>
<p>4 协议层：语义级 QoS 路由</p>
<ul>
<li><strong>思路</strong>：为 TaskStarLine 引入<strong>语义优先级</strong>（latency-bandwidth-guarantee），AIP 路由层根据链路 RTT、带宽、费用做<strong>多目标最优路径选择</strong>（可建模为 MCF）</li>
<li><strong>验证指标</strong>：Hard 任务平均尾延迟 ↓20 %；跨云-边链路成本 ↓30 %</li>
</ul>
<p>5 智能体层：LLM-Planner 的在线学习</p>
<ul>
<li><strong>问题</strong>：ConstellationAgent 依赖静态示范例，无法从历次失败中自动更新</li>
<li><strong>思路</strong>：</li>
<li>把每次 DAG 演化轨迹转为<strong>偏好对</strong>（成功 vs 失败），用 DPO（Direct Preference Optimization）微调 Planner-LLM</li>
<li>维护<strong>任务嵌入索引</strong>（FAISS），实时检索相似历史工作流，实现 Few-shot 动态提示</li>
<li><strong>验证指标</strong>：经过 100 次新任务在线更新后，TSR 相对初始提升 ≥ 10 %；单次微调 GPU 时间 &lt; 10 min</li>
</ul>
<p>6 智能体层：Hierarchical Reinforcement Learning for DAG Scheduling</p>
<ul>
<li><strong>思路</strong>：将“下一就绪节点选设备”建模为两级 MDP</li>
<li>上层：选 TaskStar（DAG 状态）</li>
<li>下层：选 Device（资源状态）<br>用 H-PPO 训练，奖励 = −(makespan + 失败惩罚)</li>
<li><strong>验证指标</strong>：与当前贪心策略相比，平均 makespan ↓15 %；训练收敛步数 &lt; 1 M</li>
</ul>
<p>7 生态层：Mobile-Edge-Federation 插件市场</p>
<ul>
<li><strong>思路</strong>：</li>
<li>制定 AIP-Profile JSON-Schema，第三方设备（车载、无人机、XR 眼镜）可<strong>零代码声明</strong>能力（传感器、定位、渲染）</li>
<li>引入<strong>能力 NFT</strong>：开发者将 MCP-Tool 绑定链上 ID，实现“能力即资产”交易，激励硬件厂商接入</li>
<li><strong>验证指标</strong>：上线 30 天内新增 20+ 异构设备插件；跨域任务占比 &gt; 25 %</li>
</ul>
<p>8 生态层：Constellation-Registry 的联邦治理</p>
<ul>
<li><strong>思路</strong>：采用<strong>半去中心化</strong>架构，ConstellationClient 作为轻量区块链客户端，关键事件（注册、能力变更、失败记录）写入<strong>侧链</strong>（如 Polygon），支持</li>
<li>跨组织共享 AgentProfile</li>
<li>可审计的故障溯源</li>
<li><strong>验证指标</strong>：跨组织任务成功率提升 ≥ 12 %；事件上链延迟 &lt; 3 s</li>
</ul>
<p>9 评估层：Adversarial NebulaBench</p>
<ul>
<li><strong>思路</strong>：在原始 55 任务基础上增加<strong>对抗子集</strong></li>
<li>网络抖动 10–1000 ms 随机注入</li>
<li>10 % 节点随机拜占庭输出</li>
<li>资源抢占（GPU 显存突然下降 50 %）</li>
<li><strong>验证指标</strong>：发布新排行榜，按 Robustness-Score = TSR × (1 −平均故障恢复时间) 排名，驱动社区优化</li>
</ul>
<p>10 评估层：Green-Orchestration Metric</p>
<ul>
<li><strong>思路</strong>：引入<strong>碳排指标</strong> gCO₂e/task</li>
<li>设备侧实时功耗通过 RAPL/IPMI 采集</li>
<li>调度器目标函数改为 “makespan + λ·碳排”</li>
<li><strong>验证指标</strong>：在同等 TSR 下，总功耗 ↓25 %；λ 可调，提供 Pareto 前沿供用户选择</li>
</ul>
<p>总结</p>
<p>从“<strong>传文件</strong>”到“<strong>学策略</strong>”再到“<strong>碳排优化</strong>”，上述方向既涵盖<strong>短期可工程落地</strong>的插件扩展，也包含<strong>中长期研究价值</strong>的在线学习、联邦治理与绿色计算，可为 UFO3 后续版本及整个“数字智能体星系”生态提供持续创新路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>UFO3</strong>，一套面向异构设备的<strong>跨平台 LLM 智能体编排系统</strong>，核心贡献与内容可概括为 <strong>“一个抽象、三大机制、五项实验”</strong>：</p>
<p>① 一个抽象：TaskConstellation（动态 DAG）</p>
<ul>
<li>将自然语言请求建模为<strong>可演化的分布式有向无环图</strong></li>
<li>节点 TaskStar = 原子子任务</li>
<li>边 TaskStarLine = 数据/控制依赖（无条件、成功-only、条件）</li>
<li>运行时支持<strong>增删节点、重连边、属性改写</strong>，保证 DAG 无环且仅改 PENDING 节点</li>
</ul>
<p>② 三大机制</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>层级</th>
<th>关键组件</th>
<th>功能与亮点</th>
</tr>
</thead>
<tbody>
<tr>
<td>推理层</td>
<td>ConstellationAgent</td>
<td>双模 FSM（创建→编辑）+ MCP 工具链，线性化锁保证图改写与调度不冲突</td>
</tr>
<tr>
<td>调度层</td>
<td>Constellation Orchestrator</td>
<td>事件驱动 + 异步协程，安全分配锁 + 批量编辑，实现高并发与一致性</td>
</tr>
<tr>
<td>通信层</td>
<td>AIP 协议</td>
<td>长寿命 WebSocket 五层栈，支持注册、任务、命令、心跳、容错重连，跨设备零感知</td>
</tr>
</tbody>
</table>
</div>
<p>③ 系统实现</p>
<ul>
<li>73 k 行 Python，已集成 <strong>Windows/Linux/Android</strong> 三类设备代理</li>
<li>提供<strong>WebUI 实时可视化</strong>与<strong>Markdown 自动日志</strong>，全开源（GitHub + 文档）</li>
</ul>
<p>④ 五项实验（NebulaBench）</p>
<ol>
<li><strong>任务完成</strong>：55 任务 × 5 机 → SCR 83.3 %，TSR 70.9 %</li>
<li><strong>编排自适应</strong>：平均每请求 5.9 次图微调，95 % 为任务描述细化，结构保持稳定</li>
<li><strong>并行效率</strong>：平均并行度 1.72，峰值 3.5，端到端延迟较串行 <strong>↓31 %</strong></li>
<li><strong>延迟可扩展</strong>：编排耗时 &lt;17 s，分钟级完成跨平台工作流</li>
<li><strong>鲁棒性</strong>：瞬断自愈、永久失败优雅降级、全局失败保一致性，<strong>无 hallucination</strong></li>
</ol>
<p>⑤ 核心结论</p>
<p>UFO3 用“<strong>动态 DAG + 事件调度 + 长寿命通信</strong>”首次把孤立 LLM 智能体升级为<strong>跨设备协同星系</strong>，在真实异构环境中实现<strong>高可靠、高并行、低延迟、可容错</strong>的端到端自动化，为“无处不在的计算”提供了可落地的统一编排范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UFO%24%5E3%24%3A%20Weaving%20the%20Digital%20Agent%20Galaxy，链接是https://arxiv.org/pdf/2511.11332，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.11332。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chaoyun Zhang,Liqun Li,He Huang,Chiming Ni,Bo Qiao,Si Qin,Yu Kang,Minghua Ma,Qingwei Lin,Saravan Rajmohan,Dongmei Zhang</p>
<p><strong>Categories</strong>: cs.DC,cs.MA</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11332.pdf">https://arxiv.org/pdf/2511.11332.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11332">https://arxiv.org/abs/2511.11332</a></p>
<p><strong>Arxiv ID</strong>: 2511.11332</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11332">https://papers.cool/arxiv/2511.11332</a></p>
<p><strong>Published</strong>: 2025-11-14T14:05:31Z</p>
<p><strong>Updated</strong>: 2025-11-14T14:05:31.000Z</p>
<hr>
<h3 id="11-Evolve-the-Method-Not-the-Prompts-Evolutionary-Synthesis-of-Jailbreak-Attacks-on-LLMs"><a href="#11-Evolve-the-Method-Not-the-Prompts-Evolutionary-Synthesis-of-Jailbreak-Attacks-on-LLMs" class="headerlink" title="11. Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.12710">Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</a></h3><p>Automated red teaming frameworks for Large Language Models (LLMs) have become increasingly sophisticated, yet they share a fundamental limitation: their jailbreak logic is confined to selecting, combining, or refining pre-existing attack strategies. This binds their creativity and leaves them unable to autonomously invent entirely new attack mechanisms. To overcome this gap, we introduce \textbf{EvoSynth}, an autonomous framework that shifts the paradigm from attack planning to the evolutionary synthesis of jailbreak methods. Instead of refining prompts, EvoSynth employs a multi-agent system to autonomously engineer, evolve, and execute novel, code-based attack algorithms. Crucially, it features a code-level self-correction loop, allowing it to iteratively rewrite its own attack logic in response to failure. Through extensive experiments, we demonstrate that EvoSynth not only establishes a new state-of-the-art by achieving an 85.5\% Attack Success Rate (ASR) against highly robust models like Claude-Sonnet-4.5, but also generates attacks that are significantly more diverse than those from existing methods. We release our framework to facilitate future research in this new direction of evolutionary synthesis of jailbreak methods. Code is available at: <a target="_blank" rel="noopener" href="https://github.com/dongdongunique/EvoSynth">https://github.com/dongdongunique/EvoSynth</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>针对大型语言模型（LLMs）的自动化红队框架日益复杂，但它们有一个根本性限制：其越狱逻辑仅限于选择、组合或改进已有的攻击策略。这限制了它们的创造力，使其无法自主发明全新的攻击机制。为弥补这一空白，我们引入了\textbf{EvoSynth}，一个自主框架，将范式从攻击计划转向越狱方法的进化合成。EvoSynth不是通过优化提示语，而是使用多智能体系统自主设计、进化并执行新型基于代码的攻击算法。关键的是，它具有代码级自我修正循环，能够在失败时迭代重写自身的攻击逻辑。通过大量实验，我们证明了EvoSynth不仅通过在面对诸如Claude-Sonnet-4.5等高鲁棒性模型时实现85.5%的攻击成功率（ASR）而开创了新的最先进水平，还生成了远比现有方法更具多样性的攻击。我们发布这一框架，以促进未来在越狱方法进化合成这一新方向的研究。代码可在以下地址获取：<a target="_blank" rel="noopener" href="https://github.com/dongdongunique/EvoSynth。">https://github.com/dongdongunique/EvoSynth。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破现有自动化红队（red-teaming）框架的根本局限：它们只能“挑选、组合或微调”已知攻击策略，无法自主发明全新的越狱机制。为此，作者提出 EvoSynth，将范式从“攻击提示/策略的优化”转变为“攻击方法的演化合成”。具体目标可归纳为：</p>
<ul>
<li>让系统不再依赖任何预设攻击模板，而是从零开始自主生成<strong>可执行、基于代码</strong>的越狱算法。</li>
<li>引入<strong>代码级自修正循环</strong>：一旦攻击失败，系统直接重写自身源码，而非仅调整提示词。</li>
<li>在严格黑盒场景下，对当前最强商用模型（如 Claude-Sonnet-4.5）实现<strong>更高成功率</strong>（85.5% → 95.9% SOTA），同时发现<strong>语义更多样、程序更复杂</strong>的漏洞。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为三大脉络，并指出它们与 EvoSynth 的本质差异：</p>
<ol>
<li>手工红队</li>
</ol>
<ul>
<li>早期依赖人工专家设计静态提示进行越狱测试<br>2,12,22,27,28,37,44,50<br>。</li>
<li>优点：能发现细微漏洞；缺点：成本高、覆盖窄、难持续。</li>
</ul>
<ol>
<li>第一代自动化——“输入空间搜索/提示优化”</li>
</ol>
<ul>
<li>遗传算法、梯度优化、token 级组合搜索、LLM 迭代改写提示等<br>5,7,14,20,24,27,31,51,56,57,58,70<br>。</li>
<li>仅对提示文本做变异或微调，不涉及攻击机制的创新。</li>
</ul>
<ol>
<li>第二代自动化——“多智能体策略规划”</li>
</ol>
<ul>
<li>系统如 RedAgent、AutoDAN-Turbo、AutoRedTeamer、X-Teaming<br>10,18,25,26,39,52,53,66<br>把“攻击策略”抽象成高层原语，通过多智能体协作进行多轮对话或策略组合。</li>
<li>共性局限：仍在“已知策略库”里做选择、组合或微调，无法<strong>自主合成全新的可执行算法</strong>。</li>
</ul>
<ol>
<li>与 EvoSynth 表面相似但实质不同的方向</li>
</ol>
<ul>
<li>CodeAttack<br>17<br>：把代码片段当作文本输入，而非让系统<strong>生成并执行</strong>攻击算法。</li>
<li>Evo-MARL<br>36<br>、RL 对抗样本生成<br>71<br>：需要白盒访问模型参数，目标是微调模型而非黑盒攻击。</li>
</ul>
<p>综上，现有工作停留在“策略规划/提示精炼”层面；EvoSynth 首次提出“演化式代码合成”范式，使系统能够<strong>发明并迭代重写</strong>全新的越狱算法，填补了“无法自主创造攻击机制”的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>
</div></details>
<p><strong>Authors</strong>: Yunhao Chen,Xin Wang,Juncheng Li,Yixu Wang,Jie Li,Yan Teng,Yingchun Wang,Xingjun Ma</p>
<p><strong>Categories</strong>: cs.CL,cs.CR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12710.pdf">https://arxiv.org/pdf/2511.12710.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12710">https://arxiv.org/abs/2511.12710</a></p>
<p><strong>Arxiv ID</strong>: 2511.12710</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12710">https://papers.cool/arxiv/2511.12710</a></p>
<p><strong>Published</strong>: 2025-11-16T17:52:07Z</p>
<p><strong>Updated</strong>: 2025-11-16T17:52:07.000Z</p>
<hr>
<h3 id="12-Back-to-Basics-Let-Denoising-Generative-Models-Denoise"><a href="#12-Back-to-Basics-Let-Denoising-Generative-Models-Denoise" class="headerlink" title="12. Back to Basics: Let Denoising Generative Models Denoise"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13720">Back to Basics: Let Denoising Generative Models Denoise</a></h3><p>Today’s denoising diffusion models do not “denoise” in the classical sense, i.e., they do not directly predict clean images. Rather, the neural networks predict noise or a noised quantity. In this paper, we suggest that predicting clean data and predicting noised quantities are fundamentally different. According to the manifold assumption, natural data should lie on a low-dimensional manifold, whereas noised quantities do not. With this assumption, we advocate for models that directly predict clean data, which allows apparently under-capacity networks to operate effectively in very high-dimensional spaces. We show that simple, large-patch Transformers on pixels can be strong generative models: using no tokenizer, no pre-training, and no extra loss. Our approach is conceptually nothing more than “$\textbf{Just image Transformers}$”, or $\textbf{JiT}$, as we call it. We report competitive results using JiT with large patch sizes of 16 and 32 on ImageNet at resolutions of 256 and 512, where predicting high-dimensional noised quantities can fail catastrophically. With our networks mapping back to the basics of the manifold, our research goes back to basics and pursues a self-contained paradigm for Transformer-based diffusion on raw natural data.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>当今的去噪扩散模型并不以经典意义上的“去噪”进行操作，即它们并不直接预测干净的图像。相反，神经网络预测的是噪声或含噪量。在本文中，我们提出预测干净数据和预测含噪量在根本上是不同的。根据流形假设，自然数据应位于低维流形上，而含噪量则不在其上。在此假设下，我们主张使用直接预测干净数据的模型，这使得在高维空间中表面上容量不足的网络也能有效运行。我们展示了在像素上使用简单、大补丁的Transformer可以成为强大的生成模型：无需分词器、无需预训练、无需额外损失。我们的方法在概念上无非就是“$    extbf{仅图像Transformer}$”，或者如我们称之为的$    extbf{JiT}$。我们报告了在ImageNet上使用16和32大补丁尺寸的JiT，在分辨率为256和512时的竞争性结果，而预测高维含噪量可能会出现灾难性失败。通过我们的网络回到流形的基础，我们的研究回归基础，并追求一种针对原始自然数据的自包含Transformer扩散范式。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对的核心问题是：</p>
<blockquote>
<p><strong>现代扩散模型在“去噪”时并不真正预测干净数据，而是预测噪声或带噪量（如 ε 或 v），导致网络被迫学习一个高维、非结构化的目标；当输入维度极高（如原像素空间）时，这一任务与“数据位于低维流形”的假设相矛盾，从而引发维度灾难与模型崩溃。</strong></p>
</blockquote>
<p>具体而言，作者指出：</p>
<ol>
<li><p><strong>预测目标失配</strong><br>现有主流扩散模型（DDPM、Flow-Matching 等）让网络直接输出 ε（噪声）或 v（速度）。这些量天然分布于整个高维空间，而干净图像 x 据流形假设仅占据低维子空间。网络容量有限时，在高维空间精确拟合 ε/v 会失败。</p>
</li>
<li><p><strong>高维像素空间的瓶颈</strong><br>当直接在像素空间使用大 patch Vision Transformer（如 16×16×3=768 维，甚至 32×32×3=3072 维）时，隐藏层宽度远小于 patch 维度，ε/v-prediction 出现灾难性退化；而 x-prediction 仍能产生合理结果。</p>
</li>
<li><p><strong>自包含性缺失</strong><br>为避免上述难题，现有方法普遍依赖预训练自编码器（latent diffusion）或特殊卷积架构、感知损失、对抗损失等额外模块，使扩散流程不再自包含。</p>
</li>
</ol>
<p>因此，论文提出回归“去噪”本意：</p>
<ul>
<li><strong>让网络直接预测干净图像 x（x-prediction）</strong>，而非 ε 或 v。</li>
<li><strong>仅使用普通 ViT 在大 patch 上操作像素</strong>，无需 tokenizer、无需预训练、无需额外损失。</li>
<li><strong>验证在 ImageNet 256/512/1024 分辨率下，x-prediction 配合简单 ViT（JiT）即可取得与 latent 方法竞争的质量</strong>，同时参数与计算量显著降低，且对 patch 维度不敏感，甚至引入瓶颈反而有益。</li>
</ul>
<p>总结：<br><strong>论文旨在用“x-prediction + 纯 ViT”解决高维像素空间扩散模型的维度灾难，实现自包含、可扩展、无需预训练的生成范式。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接关联，可划分为 6 条主线。为便于快速定位，按“主题—代表文献—与本文关系”格式给出。</p>
<p>1. 扩散模型预测目标</p>
<ul>
<li><p><strong>DDPM</strong> (Ho et al., NeurIPS’20)<br>提出 ε-prediction，成为后续主流。<br>→ 本文指出其在高维像素空间失效。</p>
</li>
<li><p><strong>Progressive Distillation</strong> (Salimans &amp; Ho, ICLR’22)<br>系统比较 x/ε/v 预测，但实验限于 32×32 CIFAR。<br>→ 本文在低维场景与之结论一致，但首次揭示高维灾难。</p>
</li>
<li><p><strong>EDM</strong> (Karras et al., NeurIPS’22)<br>预条件框架，网络输出为混合量 c_skip·z + c_out·net_θ。<br>→ 本文证明该形式在高维 patch 下仍崩溃（附录 B.2）。</p>
</li>
<li><p><strong>Flow Matching / Rectified Flow</strong> (Lipman et al., ICLR’23; Liu et al., ICLR’23)<br>采用 v-prediction，等价于扩散的 velocity 参数化。<br>→ 被本文列为“off-manifold”预测，实验对比失败案例。</p>
</li>
</ul>
<p>2. 像素空间扩散</p>
<ul>
<li><p><strong>ADM</strong> (Dhariwal &amp; Nichol, NeurIPS’21)<br>经典 U-Net 像素扩散，256×256 ImageNet FID 7.7。<br>→ 本文 JiT-B/16 无额外损失即达到 3.66。</p>
</li>
<li><p><strong>Simple Diffusion / SiD</strong> (Hoogeboom et al., ICML’23; CVPR’25)<br>端到端像素，仍用 ε-prediction 与深 U-ViT 混合结构。<br>→ 需 2 B 参数、感知损失预训练；JiT 以 1/15 算力逼近其 FID。</p>
</li>
<li><p><strong>PixelFlow</strong> (Chen et al., arXiv’25)<br>多尺度流匹配，像素空间 FID 1.98，但 FLOP 高达 2909 G。<br>→ JiT-H/32 以 183 G 取得 1.94，验证 x-prediction 效率。</p>
</li>
</ul>
<p>3. 高维 Token 灾难与补救</p>
<ul>
<li><p><strong>ViT-style 扩散失败报告</strong><br>– (Shi et al., arXiv’25) 指出 patch 维度 &gt; hidden 维度时 FID 急剧恶化。<br>– (Yao et al., CVPR’25) 提出“重建-生成两难”。<br>→ 本文首次证明仅切换 x-prediction 即可消除该灾难，无需增大 hidden。</p>
</li>
<li><p><strong>自监督预训练补救</strong><br>– REPA (Yu et al., ICLR’25)、RAE (Zheng et al., arXiv’25) 用 DINOv2 对齐 token。<br>→ 本文显示无需任何预训练即可取得可比 FID。</p>
</li>
</ul>
<p>4. 流形学习与去噪自编码器</p>
<ul>
<li><p><strong>Denoising Auto-Encoder</strong> (Vincent et al., ICML’08; JMLR’10)<br>显式以“预测干净 x”为学习目标，利用流形假设。<br>→ 本文将其思想迁移到现代扩散/Transformer 框架。</p>
</li>
<li><p><strong>信息瓶颈与瓶颈自编码器</strong> (Tishby et al., 2000; Makhzani &amp; Frey, 2013)<br>说明低维瓶颈可强制学到有效表征。<br>→ 本文在 ViT 的 patch 嵌入层引入线性瓶颈，反而改善生成质量（图 4）。</p>
</li>
</ul>
<p>5. 纯 Transformer 生成</p>
<ul>
<li><p><strong>DiT / SiT</strong> (Peebles &amp; Xie, ICCV’23; Ma et al., ECCV’24)<br>首次将 ViT 用于扩散，但仍局限在 latent token。<br>→ JiT 直接作用于像素大 patch，展示 x-prediction 的必要性。</p>
</li>
<li><p><strong>MAR</strong> (Li et al., NeurIPS’24)<br>无 tokenizer 自回归图像生成，使用 in-context class token。<br>→ JiT 借鉴其多 class token 条件策略。</p>
</li>
</ul>
<p>6. 其他相关</p>
<ul>
<li><strong>FractalMAR</strong> (Li et al., arXiv’25) 非扩散、纯像素自回归。</li>
<li><strong>PixNerd</strong> (Wang et al., arXiv’25) 像素神经场扩散，需 DINOv2 预训练。</li>
<li><strong>Classifier-Free Guidance</strong> (Ho &amp; Salimans, NeurIPS Workshop’21) 本文沿用其公式与区间技巧。</li>
</ul>
<p>一句话总结</p>
<p>本文站在 DDPM/Flow-Matching 的预测目标争论、像素扩散的高维灾难、以及 DAE/流形学习的低维先验三大传统交汇处，提出“x-prediction + 纯 ViT”这一最简方案，系统验证了它在高维像素空间的必要性与充分性，并超越此前需借助 tokenizer、感知损失或自监督预训练的多条补救路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“回到原点”的策略，把扩散模型重新拉回“真正去噪”——直接预测干净图像 x，而非噪声 ε 或速度 v。具体实施可概括为 4 步，每一步都针对前述痛点给出极简而彻底的解决方案。</p>
<p>1. 预测空间：强制 netθ 输出 x（x-prediction）</p>
<ul>
<li><strong>数学重参数</strong><br>沿用 v-loss（训练稳定），但通过</li>
</ul>
<p>v<em>θ(z_t,t)=net</em>θ(z_t,t)-z_t1-t</p>
<p>把网络输出自动转成 velocity，采样时只需解 ODE，无需改框架。</p>
<ul>
<li><strong>动机</strong><br>干净图像 x 位于低维流形；网络即使欠完备（hidden-dim ≪ patch-dim）也能丢弃噪声维度，保留有效信号。ε/v 散布在全空间，欠完备网络必然丢失关键分量，导致训练损失高、生成崩溃（图 2、表 2）。</li>
</ul>
<p>2. 架构：最简 ViT-on-Pixels（JiT）</p>
<ul>
<li><p><strong>无 tokenizer、无卷积、无预训练</strong><br>直接把 H×W×3 图像切成 p×p 非重叠 patch（p=16/32/64），线性映射为序列，送入标准 Transformer 块，再线性投影回 patch 空间。</p>
</li>
<li><p><strong>维度解耦</strong><br>隐藏宽度固定 768（Base）–1664（Giant），与 768–12288 维的 patch 输入解耦；实验表明性能不随 patch 维度线性恶化（表 5、6）。</p>
</li>
<li><p><strong>可选瓶颈嵌入</strong><br>用两层线性低秩映射先降维再升维，反而提升 FID（图 4），显式鼓励网络学习低维表征，与流形假设一致。</p>
</li>
</ul>
<p>3. 训练与采样算法：两行代码即可实现</p>
<p><strong>训练</strong>（Alg. 1）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_pred = net(z, t)</span><br><span class="line">v_pred = (x_pred - z) / (<span class="number">1</span> - t)</span><br><span class="line">loss = ||v - v_pred||²</span><br></pre></td></tr></table></figure>
<p><strong>采样</strong>（Alg. 2，Heun 50 步）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_pred = net(z, t)</span><br><span class="line">v_pred = (x_pred - z) / (<span class="number">1</span> - t)</span><br><span class="line">z_next = z + (t_next - t) * v_pred</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>兼容 CFG、类别条件</strong><br>采用 adaLN-Zero + 32 个 in-context class token，无需修改主干。</li>
</ul>
<p>4. 系统验证：高维像素灾难被“免费”消除</p>
<ul>
<li><p><strong>ImageNet 256²</strong><br>JiT-B/16 200 epoch FID 4.37，600 epoch 3.66；增大模型至 JiT-G 达 1.82，<strong>无预训练、无额外损失</strong>。</p>
</li>
<li><p><strong>ImageNet 512² / 1024²</strong><br>仅把噪声幅度 ×2 / ×4，其余超参不变；JiT-B/32 512² FID 4.02，JiT-B/64 1024² FID 4.82，<strong>参数量与 FLOP 几乎不变</strong>。</p>
</li>
<li><p><strong>对比 ε/v-prediction</strong><br>相同网络容量下 ε/v 在 256² 即崩溃（FID &gt; 350）；x-prediction 稳定工作，且瓶颈/大 patch/大分辨率均不再成为瓶颈。</p>
</li>
</ul>
<p>结果一句话</p>
<p>通过“x-prediction + 纯 ViT”这一最小改动，论文把高维像素空间的维度灾难直接转化为低维流形学习优势，无需加深加宽、无需预训练或任何外部损失，即可在 256–1024² 分辨率取得与当前最佳 latent 扩散相当或更优的 FID，实现了自包含、可扩展的“扩散即 Transformer”新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕“x-prediction 能否在高维像素空间取代 ε/v-prediction”这一核心问题展开，共 5 组递进式测试，覆盖 toy 模拟、消融、 scalability、对比与诊断。</p>
<p>1. Toy 仿真：验证维度灾难根源</p>
<ul>
<li><p><strong>设定</strong><br>2-d 真实流形 → 随机列正交矩阵 P 嵌入 D-d 空间（D=2,8,16,512）。<br>网络：5 层 ReLU-MLP，隐藏仅 256 单元，明显欠完备。</p>
</li>
<li><p><strong>观测</strong></p>
</li>
<li>D≥16 时，ε/v-prediction 崩溃；D=512 完全失败。</li>
<li>x-prediction 在所有 D 下均重建出清晰 2-d 流形（图 2）。<br>→ 首次定量展示“预测目标”比“网络容量”更关键。</li>
</ul>
<p>2. 消融实验：锁定关键因子</p>
<p>2.1 预测空间 × 损失空间（表 2）</p>
<p>ImageNet 256²，JiT-B/16（patch 768-d，hidden 768-d）</p>
<ul>
<li>9 种组合（x/ε/v-pred × x/ε/v-loss）</li>
<li>仅 x-prediction 能工作（FID≈8–10），ε/v-prediction 全部 &gt;350。</li>
</ul>
<p>2.2 低维对照（表 2b）</p>
<p>ImageNet 64²，JiT-B/4（patch 48-d）</p>
<ul>
<li>9 种组合差距 &lt;1 FID，说明维度低时预测目标选择不重要。</li>
</ul>
<p>2.3 噪声水平移位（表 3）</p>
<p>固定 x-prediction，改变 logit-normal 的 μ∈{0,−0.4,−0.8,−1.2}</p>
<ul>
<li>适度高噪声（μ=−0.8）最佳；但即使用最优噪声，ε/v 仍崩溃。</li>
</ul>
<p>2.4 瓶颈嵌入（图 4）</p>
<p>JiT-B/16，把 768-d patch 先线性压至 d′∈{16,32,…,512} 再进 Transformer</p>
<ul>
<li>d′=32–512 反而优于无瓶颈；d′=16 亦不崩溃，FID≈9.4。</li>
</ul>
<p>3. Scalability：模型-分辨率联合放大</p>
<ul>
<li><strong>同序列长度策略</strong><br>256²→512²→1024² 分别用 p=16/32/64，保持 16×16=256 token。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>256²</th>
<th>512²</th>
<th>1024²</th>
</tr>
</thead>
<tbody>
<tr>
<td>JiT-B</td>
<td>4.37</td>
<td>4.02</td>
<td>4.82</td>
</tr>
<tr>
<td>JiT-L</td>
<td>2.36</td>
<td>2.53</td>
<td>—</td>
</tr>
<tr>
<td>JiT-H</td>
<td>1.86</td>
<td>1.94</td>
<td>—</td>
</tr>
<tr>
<td>JiT-G</td>
<td>1.82</td>
<td>1.78</td>
<td>—</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>参数量仅由深度/宽度决定，与分辨率基本无关；更大模型在 512² 甚至优于 256²，显示 x-prediction 对高分辨率无额外容量需求。</li>
</ul>
<p>4. 与现有方法对比（表 7–8）</p>
<p>ImageNet 256² &amp; 512²，50K 样本 FID/IS</p>
<ul>
<li><strong>像素空间</strong>：JiT-G/16 1.82，低于 SiD2-UViT/2 1.73（但后者 2B 参数+感知损失）；远低于 ADM-G 7.72。</li>
<li><strong>Latent 空间</strong>：JiT-G 1.82 接近 DiT-XL/2 2.27、REPA 1.42，但无需任何 tokenizer 或自监督预训练。</li>
<li><strong>计算效率</strong>：JiT-B/16 仅 25 Gflops，约为 DiT-XL/2 的 1/5，SiD2-UViT/2 的 1/22。</li>
</ul>
<p>5. 诊断与扩展实验</p>
<p>5.1 训练曲线可视化（图 7）</p>
<p>同 v-loss 下，v-prediction 像素级 MSE 高 ≈25%，生成步 artifact 明显；x-prediction 稳定下降。</p>
<p>5.2 预条件器失败（表 10）</p>
<p>EDM 式 c_skip≠0 的“预条件 x-pred”在 256² 仍 FID&gt;35，验证必须让网络<strong>直接</strong>输出 x。</p>
<p>5.3 交叉分辨率生成（表 12）</p>
<ul>
<li>512 模型下采样→256：FID 1.84 vs 256 专家 1.82，几乎无损。</li>
<li>256 模型上采样→512：FID 2.45 vs 512 专家 1.78，说明模型确实学到分辨率相关高频。</li>
</ul>
<p>5.4 额外损失探索（表 11）</p>
<p>在中间层加 100× 分类损失，JiT-L/16 200 epoch FID 从 2.79→2.50，证实极简框架仍可容纳未来扩展。</p>
<p>一句话总结</p>
<p>实验从 2-d 玩具到 1024² 真实图像，系统证明：<br><strong>只要切换到 x-prediction，普通 ViT 即可在高维像素空间稳定训练、随模型与分辨率优雅放大，且无需任何 tokenizer、预训练或额外损失，就能与当前最优 latent 扩散模型打平甚至更好。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向在论文末尾或附录仅一笔带过，尚缺系统研究，可作为后续工作切入点。</p>
<p>1. 其它模态的“原生扩散 + Transformer”</p>
<ul>
<li>蛋白质、分子、DNA、气象网格等天然高维连续数据<br>→ 验证 x-prediction 是否同样免除专用 tokenizer 设计</li>
<li>音频波形、地震信号、LiDAR 点云等高维时间序列/空间信号<br>→ 测试无需梅尔谱或体素化，直接 raw-waveform/raw-point 扩散的可行性</li>
</ul>
<p>2. 预测目标与流形几何的精细耦合</p>
<ul>
<li>局部坐标系预测：网络输出切空间坐标而非全局像素，进一步降低有效维度</li>
<li>混合预测：早期时间步 x-pred，临近 t=0 切换 ε-pred，兼顾训练稳定性与采样精度</li>
<li>自适应目标：让网络在训练过程中学会选择 x/ε/v 的加权比例（可微 NAS）</li>
</ul>
<p>3. 更激进的瓶颈与压缩</p>
<ul>
<li>非线性瓶颈：在 patch 嵌入后接 1×1 Conv + 稀疏激活，看极限压缩到 4-8 维是否仍保质量</li>
<li>动态令牌合并：根据图像内容逐步减少令牌数量，实现“分辨率-自适应”生成</li>
<li>量化瓶颈：patch 嵌入离散化（VQ）+ x-prediction，探索无码本 collapse 的临界码本大小</li>
</ul>
<p>4. 采样侧加速与质量</p>
<ul>
<li>数据驱动求解器：用小的 neural ODE solver 替代 Heun，专门对 x-prediction 轨迹拟合高阶步长</li>
<li>一致性蒸馏：以 x-prediction 教师为基准，蒸馏 1-2 步学生，看是否比 ε-pred 蒸馏更稳定</li>
<li>迭代矫正：先 5 步快速采样，再用 x-prediction 网络作为矫正器，多轮 refine</li>
</ul>
<p>5. 条件与可控生成</p>
<ul>
<li>细粒度条件（分割图、深度、文本）接入 x-prediction 网络：研究是否仍只需简单 cross-attention</li>
<li>组合生成：多幅图像拼接大画布，考察 x-pred 对全局一致性是否更鲁棒（ε-pred 易累积高频误差）</li>
<li>局部编辑：给定 mask，仅对局部 patch 做 ODE，验证 x-pred 能否自然保持未编辑区域不变</li>
</ul>
<p>6. 理论分析</p>
<ul>
<li>容量下限：对 patch 维度 D、隐藏维度 H、数据本征维度 d 给出 x/ε-pred 的成功/失败边界</li>
<li>误差传播：x-pred 与 ε-pred 在多步 ODE 中的误差累积系数是否呈 (1+Δt)^T 与 exp(LT) 差异</li>
<li>收敛速度：相同网络容量下，x-pred 是否享有更小的 Rademacher 复杂度或更好的 NTK 条件数</li>
</ul>
<p>7. 联合损失与正则</p>
<ul>
<li>轻量级感知损失：用自监督 ViT 特征而非 VGG，观察能否进一步降低 FID 且保持自包含</li>
<li>对抗微调：在 x-pred 预训练后加极小判别器，仅微调最后一层，测试“扩散+GAN”混合收益</li>
<li>对比正则：同一图像两次加噪，让 x-pred 输出互相接近，提升流形平滑性</li>
</ul>
<p>8. 跨分辨率与任意尺寸生成</p>
<ul>
<li>连续尺度模型：用 RoPE 直接训练任意 patch 大小的 Transformer，实现 64²-1024² 任意采样</li>
<li>金字塔 x-prediction：低分辨率 x 作为高分辨率生成的先验，一次训练多级输出</li>
<li>无限画布：结合局部注意力与 x-pred，探索“向左/向右/向上/向下”无限扩展生成</li>
</ul>
<p>9. 鲁棒性与安全性</p>
<ul>
<li>噪声 schedule 攻击：对训练/推理 schedule 做微小扰动，比较 x/ε-pred 的崩溃阈值</li>
<li>对抗 patch：在输入 patch 加入不可见扰动，验证 x-pred 是否更难被误导</li>
<li>隐私泄露：检查 x-pred 网络是否比 ε-pred 更容易记忆训练样本（ Membership Inference Attack）</li>
</ul>
<p>10. 系统与硬件优化</p>
<ul>
<li>大 patch 卷积融合：用分组卷积替代线性 patch 嵌入，减少 768→3072-d 投影的内存峰值</li>
<li>Kernel 融合：把 (x_pred-z)/(1-t) 与 Heun 更新写成单 CUDA kernel，提升高分辨率吞吐量</li>
<li>低比特训练：x-pred 输出有界，尝试 8-bit 权重+16-bit 梯度稳定训练，进一步降低显存</li>
</ul>
<p>一句话展望</p>
<p>x-prediction 让“扩散 + 原生 Transformer”首次在高维连续数据上跑通，其背后“流形友好”特性尚未被充分挖掘；理论上厘清容量/误差/收敛边界，应用上扩展到多模态、无限分辨率、超快速采样，都将是有价值且可行的下一步。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Back to Basics: Let Denoising Generative Models Denoise</strong> 提出“JiT（Just image Transformers）”范式，核心思想与成果可浓缩为一句话：</p>
<blockquote>
<p><strong>“让扩散模型回归真正的‘去噪’——直接预测干净图像 x，而非噪声 ε 或速度 v；仅用普通 ViT 在大 patch 像素上训练，无需 tokenizer、无需预训练、无需额外损失，即可在 ImageNet 256²–1024² 取得与 latent 扩散相当甚至更优的 FID。”</strong></p>
</blockquote>
<p>1. 问题根源</p>
<ul>
<li>主流扩散模型预测 ε/v，目标散布于高维空间；自然图像据流形假设位于低维子空间。</li>
<li>当输入维度 ≫ 网络隐藏维度（如 32×32×3=3072 vs 768）时，ε/v-prediction 灾难性崩溃，FID &gt; 350。</li>
<li>现有补救依赖预训练 tokenizer、感知/对抗损失或特殊卷积，失去自包含性。</li>
</ul>
<p>2. 解决方案</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>做法</th>
</tr>
</thead>
<tbody>
<tr>
<td>预测目标</td>
<td>网络直接输出干净图像 x（x-prediction），通过重参数仍可用 v-loss 训练与 ODE 采样。</td>
</tr>
<tr>
<td>架构</td>
<td>纯 ViT：patch 嵌入 → Transformer 块 → 线性解码回 patch；可插瓶颈降维。</td>
</tr>
<tr>
<td>条件</td>
<td>adaLN-Zero + 32 个 in-context class token，无其他归纳偏置。</td>
</tr>
<tr>
<td>训练/采样</td>
<td>算法各 5 行伪代码，兼容 CFG、Heun 求解器。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 主要实验结果</p>
<ul>
<li><strong>Toy</strong>：2-d 流形嵌入 512-d，仅 x-pred 成功。</li>
<li><strong>消融</strong>：ImageNet 256²，x-pred FID ≈8，ε/v-pred &gt;350；瓶颈 32-d 反而更优。</li>
<li><strong>scalability</strong>：同一模型大小，256²→512²→1024² 参数/FLOP 几乎不变，FID 稳步下降（JiT-G 1.82→1.78）。</li>
<li><strong>对比</strong>：像素空间 JiT-G 1.82 击败 ADM-G 7.72、SiD 2.44；latent 空间媲美 DiT-XL/2 2.27、REPA 1.42，但无任何预训练。</li>
</ul>
<p>4. 结论与启示</p>
<ul>
<li><strong>噪声与数据本质不同</strong>：网络容量有限时，应让参数用于建模低维流形而非高维噪声。</li>
<li><strong>x-prediction 解除维度诅咒</strong>：高维 patch 不再需加大隐藏宽度，甚至可主动引入瓶颈。</li>
<li><strong>自包含“扩散 + Transformer”</strong>：无需 tokenizer、无额外损失，为蛋白质、分子、气象等难 tokenizer 领域提供即插即用范式。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Back%20to%20Basics%3A%20Let%20Denoising%20Generative%20Models%20Denoise，链接是https://arxiv.org/pdf/2511.13720，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13720。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tianhong Li,Kaiming He</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13720.pdf">https://arxiv.org/pdf/2511.13720.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13720">https://arxiv.org/abs/2511.13720</a></p>
<p><strong>Arxiv ID</strong>: 2511.13720</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13720">https://papers.cool/arxiv/2511.13720</a></p>
<p><strong>Published</strong>: 2025-11-17T18:59:57Z</p>
<p><strong>Updated</strong>: 2025-11-17T18:59:57.000Z</p>
<hr>
<h3 id="13-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation"><a href="#13-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation" class="headerlink" title="13. OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13655">OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</a></h3><p>Earth observation data presents a unique challenge: it is spatial like images, sequential like video or text, and highly multimodal. We present OlmoEarth: a multimodal, spatio-temporal foundation model that employs a novel self-supervised learning formulation, masking strategy, and loss all designed for the Earth observation domain. OlmoEarth achieves state-of-the-art performance compared to 12 other foundation models across a variety of research benchmarks and real-world tasks from external partners. When evaluating embeddings OlmoEarth achieves the best performance on 15 out of 24 tasks, and with full fine-tuning it is the best on 19 of 29 tasks. We deploy OlmoEarth as the backbone of an end-to-end platform for data collection, labeling, training, and inference of Earth observation models. The OlmoEarth Platform puts frontier foundation models and powerful data management tools into the hands of non-profits and NGOs working to solve the world’s biggest problems. OlmoEarth source code, training data, and pre-trained weights are available at $\href{<a target="_blank" rel="noopener" href="https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$">https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>地球观测数据面临独特挑战：它像空间图像，像视频或文本那样顺序，且高度多模态。我们介绍OlmoEarth：一种多模态、时空基础模型，采用了新颖的自监督学习表述、掩蔽策略和损失，全部设计用于地球观测领域。与外部合作伙伴的各种研究基准和实际任务相比，OlmoEarth 在其他12个基础模型中实现了最先进的性能。在评估嵌入时，OlmoEarth在24个任务中有15个表现最佳，经过全面微调后，在29个任务中有19个表现最佳。我们将OlmoEarth作为端到端平台的骨干，用于数据收集、标记、训练和推断地球观测模型。OlmoEarth平台将前沿的基础模型和强大的数据管理工具交给致力于解决全球最大问题的非营利组织和非政府组织。OlmoEarth的源代码、训练数据和预训练权重可在$\href{<a target="_blank" rel="noopener" href="https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$获取。">https://github.com/allenai/olmoearth_pretrain}{\text{https://github.com/allenai/olmoearth_pretrain}}$获取。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文旨在解决地球观测（Earth Observation, EO）领域中基础模型（foundation model）在真实场景落地时面临的三大核心问题：</p>
<ol>
<li><p>训练不稳定与表征塌陷<br>现有遥感基础模型在自监督预训练阶段普遍出现训练崩溃或表征退化，导致模型无法达到预期性能。论文提出“Latent MIM Lite”策略，用固定随机线性投影替代可训练的目标编码器，显著提升了训练稳定性。</p>
</li>
<li><p>多模态、时空耦合数据的预训练难题<br>地球观测数据同时具有“图像般的空间结构、视频般的时序特性、且多源传感器模态高度异构”的特点。传统随机掩码过于简单，模型容易利用时空或模态冗余“作弊”。论文提出“模态感知掩码 + 模态内对比损失”，迫使模型在缺失某些模态或时段的情况下仍能学到鲁棒表征。</p>
</li>
<li><p>非营利组织“最后一公里”落地障碍<br>即使模型性能优异，数据收集、对齐、标注、微调、推理等全流程对算力与专业知识要求极高，环保、人道组织难以负担。论文将模型封装进端到端平台 OlmoEarth Platform，提供零代码的数据管理、标注、微调与地图发布功能，让一线组织无需 GPU 与深度学习背景即可使用前沿模型。</p>
</li>
</ol>
<p>综上，论文的核心贡献可归纳为：</p>
<ul>
<li>提出一套稳定、面向地球观测的自监督预训练框架（Latent MIM Lite + 模态感知掩码 + 模态内对比损失）；</li>
<li>在 24 项嵌入任务与 29 项微调任务上取得 15/24 与 19/29 的 SOTA 成绩；</li>
<li>开源模型、数据与平台，直接服务于全球环保、粮食安全、生态系统监测等公益场景。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文直接或间接引用，并与 OlmoEarth 的核心思想（自监督预训练、多模态遥感、掩码建模、对比学习、基础模型平台化）密切相关。按主题分组，给出关键贡献与关联点。</p>
<ul>
<li><strong>掩码自监督视觉预训练</strong></li>
<li>MAE（Masked Autoencoders）</li>
</ul>
<p>提出纯像素级掩码重建，稳定但表征能力有限；OlmoEarth 将其作为对比基线（表 6 “MAE”）。</p>
<ul>
<li>I-JEPA / Latent MIM</li>
</ul>
<p>在潜空间预测，特征更丰富却易塌陷；OlmoEarth 的 “Latent MIM Lite” 用固定随机投影取代可训练目标网络，解决塌陷问题。</p>
<ul>
<li><strong>遥感专用掩码建模</strong></li>
<li>SatMAE</li>
</ul>
</div></details>
<p><strong>Authors</strong>: Henry Herzog,Favyen Bastani,Yawen Zhang,Gabriel Tseng,Joseph Redmon,Hadrien Sablon,Ryan Park,Jacob Morrison,Alexandra Buraczynski,Karen Farley,Joshua Hansen,Andrew Howe,Patrick Alan Johnson,Mark Otterlee,Ted Schmitt,Hunter Pitelka,Stephen Daspit,Rachel Ratner,Christopher Wilhelm,Sebastian Wood,Mike Jacobi,Hannah Kerner,Evan Shelhamer,Ali Farhadi,Ranjay Krishna,Patrick Beukema</p>
<p><strong>Categories</strong>: cs.CV,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13655.pdf">https://arxiv.org/pdf/2511.13655.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13655">https://arxiv.org/abs/2511.13655</a></p>
<p><strong>Arxiv ID</strong>: 2511.13655</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13655">https://papers.cool/arxiv/2511.13655</a></p>
<p><strong>Published</strong>: 2025-11-17T18:06:26Z</p>
<p><strong>Updated</strong>: 2025-11-17T18:06:26.000Z</p>
<hr>
<h3 id="14-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly"><a href="#14-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly" class="headerlink" title="14. Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13646">Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</a></h3><p>Large Language Models (LLMs) are reshaping almost all industries, including software engineering. In recent years, a number of LLM agents have been proposed to solve real-world software problems. Such software agents are typically equipped with a suite of coding tools and can autonomously decide the next actions to form complete trajectories to solve end-to-end software tasks. While promising, they typically require dedicated design and may still be suboptimal, since it can be extremely challenging and costly to exhaust the entire agent scaffold design space. Recognizing that software agents are inherently software themselves that can be further refined/modified, researchers have proposed a number of self-improving software agents recently, including the Darwin-Gödel Machine (DGM). Meanwhile, such self-improving agents require costly offline training on specific benchmarks and may not generalize well across different LLMs or benchmarks. In this paper, we propose Live-SWE-agent, the first live software agent that can autonomously and continuously evolve itself on-the-fly during runtime when solving real-world software problems. More specifically, Live-SWE-agent starts with the most basic agent scaffold with only access to bash tools (e.g., mini-SWE-agent), and autonomously evolves its own scaffold implementation while solving real-world software problems. Our evaluation on the widely studied SWE-bench Verified benchmark shows that Live-SWE-agent can achieve an impressive solve rate of 75.4% without test-time scaling, outperforming all existing open-source software agents and approaching the performance of the best proprietary solution. Moreover, Live-SWE-agent outperforms state-of-the-art manually crafted software agents on the recent SWE-Bench Pro benchmark, achieving the best-known solve rate of 45.8%.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>大型语言模型（LLMs）正在重塑几乎所有行业，包括软件工程。近年来，已经提出了许多 LLM 代理来解决现实世界的软件问题。这些软件代理通常配备了一套编码工具，并能够自主决定下一步行动，从而形成完整的轨迹来解决端到端的软件任务。虽然前景看好，但它们通常需要专门设计，且仍可能不够优化，因为要穷尽整个代理框架设计空间可能极具挑战性且成本高昂。鉴于软件代理本质上也是软件，可以进一步优化或修改，研究人员最近提出了多种自我改进的软件代理，包括达尔文-哥德尔机器（Darwin-Gödel Machine, DGM）。与此同时，这些自我改进的代理需要在特定基准上进行代价高昂的离线训练，并且可能无法很好地在不同的 LLM 或基准间泛化。在本文中，我们提出了 Live-SWE-agent，这是第一个能够在运行时自主并持续自我进化的实时软件代理，用于解决现实世界的软件问题。更具体地说，Live-SWE-agent 从最基础的代理框架开始，仅能访问 bash 工具（例如，mini-SWE-agent），并在解决实际软件问题的过程中自主进化其自身的框架实现。我们在广泛研究的 SWE-bench Verified 基准上的评估显示，Live-SWE-agent 在不进行测试时扩展的情况下也能达到令人印象深刻的 75.4% 的解决率，超过了所有现有的开源软件代理，并接近最佳专有解决方案的性能。此外，Live-SWE-agent 在最近的 SWE-Bench Pro 基准上也优于最先进的人工设计软件代理，取得了迄今为止已知的最佳解决率 45.8%。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破现有软件工程智能体“静态脚手架”瓶颈，提出并验证一种可在运行时<strong>持续自我进化</strong>的通用范式。核心待解决问题可归纳为：</p>
<ol>
<li><p>静态脚手架局限<br>现有 LLM 智能体依赖人工预设的固定工具集与流程，面对多样化、跨语言、跨仓库的真实软件任务时，常因工具不匹配或流程僵化而表现次优。</p>
</li>
<li><p>离线自我改进代价高且泛化差<br>近期“自改进”方法（DGM、SICA、HGM）需在特定基准上离线训练数百小时，生成静态代理后无法随任务变化继续演化，跨 LLM、跨基准迁移能力弱，单轮成本高达数万美元。</p>
</li>
<li><p>手工设计空间爆炸<br>为每类任务手工扩展工具与流程极其昂贵，几乎无法穷尽无限设计空间。</p>
</li>
</ol>
<p>LIVE-SWE-AGENT 的解决思路：<br>将“智能体即软件”这一洞察形式化为<strong>运行时自我进化</strong>机制——从仅含 bash 的最小脚手架出发，让 LLM 在解决真实问题的<strong>每一步</strong>自主决定“是否即时合成/修改工具”，无需任何离线训练或额外管道。通过轻量级“步骤后反思”提示，把工具创造提升为与普通动作同等级的显式决策，实现：</p>
<ul>
<li>任务级工具定制：针对当前 issue 动态生成最契合的脚本工具</li>
<li>在线持续迭代：工具随理解深入而被反复修正，避免一次性设计失误</li>
<li>零额外成本：不改动底层循环、不引入训练开销，对任意 LLM 与脚手架即插即用</li>
</ul>
<p>实验表明，该范式在 SWE-bench Verified 与 SWE-Bench Pro 上分别取得 75.4 % 与 45.8 % 的 SOTA 开源成绩，逼近最佳商业系统，同时较离线自改进方法节省千小时级 GPU 时间与数万美元成本，从而验证了“运行时自我进化”可有效解决静态脚手架高成本、低泛化、难维护的核心痛点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可划分为三大主线：软件工程智能体、自改进/自进化智能体，以及工具自动生成。关键工作如下：</p>
<ul>
<li><strong>软件工程智能体</strong></li>
<li>ChatRepair<br>37,38<br>：首个基于对话的自动程序修复框架，利用测试失败反馈迭代修正补丁。</li>
<li>SWE-agent<br>39,40<br>：为 LLM 提供终端、编辑器、搜索等工具，实现端到端 GitHub issue 解决。</li>
<li>OpenHands<br>33<br>：开源通用平台，支持多工具集成与多轮命令执行。</li>
<li>AutoCodeRover<br>45<br>：结合代码搜索与编辑的专门化智能体。</li>
<li>Agentless<br>36<br>/ Moatless<br>47<br>：主张用精简工作流替代复杂脚手架，降低手工设计成本。</li>
<li><strong>自改进（离线）智能体</strong></li>
<li>SICA<br>29<br>：通过离线强化学习迭代更新自身提示，提升代码生成能力。</li>
<li>Darwin-Gödel Machine (DGM)<br>43<br>：在 SWE-bench 上花费 1 200+ GPU 小时进化出静态代理，单轮成本约 2.2 万美元。</li>
<li>Huxley-Gödel Machine (HGM)<br>32<br>：引入近似最优自改进机制，进一步压缩搜索空间，仍需 500+ 小时离线训练。</li>
<li><strong>工具自动生成与通用工具制造</strong></li>
<li>Tool Maker (CACTUS)<br>8<br>：让 LLM 为抽象推理任务离线生成一次性工具。</li>
<li>Voyager<br>31<br>：在 Minecraft 环境中持续编写新技能代码，实现开放式探索。</li>
<li>Creator<br>26<br>：解耦抽象与具体推理，通过工具生成提升 LLM 泛化能力。</li>
<li>Trove<br>34<br>：针对编程任务诱导可验证工具箱，强调工具正确性。</li>
</ul>
<p>与上述工作相比，LIVE-SWE-AGENT 首次将“工具自动生成”从离线或特定领域拓展到<strong>真实软件工程场景下的运行时在线进化</strong>，无需昂贵离线训练，也不依赖固定工具集，从而同时解决了静态脚手架高成本、低泛化与自改进方法训练开销巨大的双重瓶颈。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“智能体即软件”这一洞察转化为<strong>运行时自我进化</strong>机制，具体实现仅对现有智能体循环做<strong>两处最小侵入式修改</strong>，即可在解决真实 issue 的过程中动态合成、修正并立即使用自定义工具，无需任何离线训练或额外管道。核心步骤如下：</p>
<ol>
<li>初始提示注入“工具创造权”<br>在 mini-SWE-agent 的 system prompt 末尾追加一段<strong>工具创造指令</strong>：</li>
</ol>
<ul>
<li>明确告诉 LLM“你可以随时用 Python 写脚本并立即调用”</li>
<li>不要求通用性，鼓励<strong>任务专属</strong></li>
<li>给出模板与示例，降低语法心智负担</li>
</ul>
<ol>
<li>每步后强制反思<br>执行完一条 bash 命令后，环境返回结果时<strong>自动追加</strong>一段反射消息：</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Reflect on the previous trajectories and decide if there are any tools you can create to help you with the current task.</span><br></pre></td></tr></table></figure>
<p>该提示把“是否造工具”变成与普通动作同等级的显式决策点，避免 LLM 遗忘该能力。</p>
<ol>
<li>工具即脚本，零额外接口</li>
</ol>
<ul>
<li>创建：LLM 输出一段 <code>cat &lt;&lt;&#39;EOF&#39; &gt; tool.py</code> 命令即可把脚本写入磁盘</li>
<li>调用：直接 <code>python tool.py arg1 arg2</code>，与 bash 命令完全同构，无需改造 agent 循环</li>
<li>迭代：同一脚本可被后续步骤反复覆盖修改，实现<strong>在线精化</strong></li>
</ul>
<ol>
<li>脚手架不变，成本恒定<br>除上述两段文本外，不引入新模块、不改动状态机、不增加向量存储；温度、步数、预算等超参与 mini-SWE-agent 完全一致，确保<strong>零额外离线开销</strong>。</li>
</ol>
<p>通过这四步，论文把“如何解决问题”转化为“如何即时生成最适合当前问题的工具”，从而以<strong>恒定成本</strong>突破静态工具集与昂贵离线进化的双重瓶颈。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在三个主流 SWE-bench 系列基准上系统评估了 LIVE-SWE-AGENT，实验覆盖性能、成本、工具行为与消融分析，主要结果如下：</p>
<ol>
<li>主实验</li>
</ol>
<ul>
<li>SWE-bench Verified（500 题）<br>– Claude 4.5 Sonnet 后端：75.4 % 解决率，比 mini-SWE-agent 提升 4.8 pp，<strong>超越所有开源代理</strong>，与最佳商业系统差距 &lt; 4 pp。<br>– 额外成本仅 + 0.12/题（ 0.68 vs $0.56）。</li>
<li>SWE-Bench Pro（731 题，多语言、企业级）<br>– 45.8 % 解决率，<strong>刷新公开排行榜第一</strong>，比原榜首 SWE-agent（43.6 %）高 2.2 pp。<br>– 平均成本 $0.73/题，仍低于多数商业方案。</li>
</ul>
<ol>
<li>与离线自改进代理对比<br>在 SWE-bench Verified-60 子集（前人通用评估集）：</li>
</ol>
<ul>
<li>LIVE-SWE-AGENT 65.0 %</li>
<li>最佳离线方法 HGM 56.7 %，DGM 53.3 %</li>
<li><strong>零离线 GPU 小时</strong>，而 DGM/HGM 需 500–1200 小时。</li>
</ul>
<ol>
<li>跨 LLM 一致性验证<br>同一 50 题子集上，LIVE-SWE-AGENT 相对 mini-SWE-agent 的提升：</li>
</ol>
<ul>
<li>GPT-5-Nano：↓ 68 %（弱模型无法合理造工具）</li>
<li>GPT-5-Mini：↓ 3.3 %</li>
<li>GPT-5：↑ 13.3 %</li>
<li>Claude 3.7 Sonnet：↑ 8.7 %</li>
<li>Claude 4 Sonnet：↑ 10.3 %</li>
<li>Claude 4.5 Sonnet：↑ 22.6 %<br>表明<strong>越强模型收益越大</strong>，验证范式对未来 LLM 的可扩展性。</li>
</ul>
<ol>
<li>多语言泛化<br>SWE-bench Multilingual 50 题子集（9 种语言）：</li>
</ol>
<ul>
<li>mini-SWE-agent 40.0 %</li>
<li>LIVE-SWE-AGENT 46.0 %（↑ 6 pp）</li>
</ul>
<ol>
<li>消融与工具分析</li>
</ol>
<ul>
<li>消融（同一 50 题）：<br>– 无工具创造 62.0 %<br>– 无反思提示 64.0 %<br>– 完整方案 76.0 %</li>
<li>工具可视化：t-SNE 显示自动生成工具按功能（edit/view/search）、仓库（openlibrary 特有聚类）与语言形成明显簇，验证<strong>任务导向多样性</strong>。</li>
<li>典型案例：<br>– 自造 search_code.py 一步替代 20+ 行复杂 grep 链，减少上下文膨胀。<br>– 自造 go_analyzer.py 完成静态分析，帮助解决先前最强基线未解的 navidrome-10108 问题。</li>
</ul>
<p>综上，实验从性能、成本、跨模型/跨语言通用性、消融与工具质效五方面证明：运行时自我进化可在<strong>零离线开销</strong>下稳定提升真实软件工程任务表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向按“深度”与“广度”两条主线展开，共 7 点：</p>
<ol>
<li><p>脚手架全维度自我进化<br>目前仅动态合成工具；下一步让智能体在运行时<strong>修改自身系统提示、状态机、工作流</strong>乃至通信协议，实现真正的“代码即自身”递归改进。</p>
</li>
<li><p>跨任务技能持久化与迁移<br>将每轮生成的优质工具/提示片段序列化为<strong>Skill Library</strong>，后续任务通过向量检索即时加载，形成“终身进化”闭环，避免每次都从零造轮子。</p>
</li>
<li><p>工具可验证性与安全性<br>引入轻量级符号执行或沙箱隔离，对自生成工具进行<strong>合法性、副作用、资源占用</strong>三重校验，防止恶意或失控脚本污染环境。</p>
</li>
<li><p>训练-推理协同自我进化<br>把“运行时工具创造”作为新型 RL 信号，反向训练基础模型，使其在预训练阶段就具备<strong>更稳健的工具合成与自我修改先验</strong>，降低对提示工程的依赖。</p>
</li>
<li><p>多智能体协作进化<br>让多个 LIVE-SWE-AGENT 实例分别负责工具制造、测试、评审，<strong>分工-交换-合并</strong>形成群体进化，加速复杂企业级问题的收敛。</p>
</li>
<li><p>扩展域：安全、测试、二进制分析<br>将范式迁移至漏洞修复、模糊测试、COTS 二进制加固等<strong>高工具多样性场景</strong>，验证是否同样能以“零手工设计”击败领域专用方案。</p>
</li>
<li><p>统一评估协议<br>建立“工具创造 × 任务解决”双维度指标（Tool-Synth Score、Task-Resolve Score），推动社区在<strong>相同轻量级脚手架</strong>下公平比较不同 LLM 的“自我进化”潜能，而非仅比较最终补丁数。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>LIVE-SWE-AGENT：运行时自我进化的软件工程智能体</strong></p>
<ul>
<li><p><strong>问题</strong><br>现有 LLM 智能体依赖<strong>固定工具集与手工脚手架</strong>，跨任务泛化差；近期“自改进”方法需<strong>昂贵离线训练</strong>（数千 GPU 时、数万美元）且生成静态代理，难以随新任务继续演化。</p>
</li>
<li><p><strong>洞察</strong><br>智能体本身就是软件，可在解决真实 issue 的<strong>运行时</strong>像修改业务代码一样修改自身。</p>
</li>
<li><p><strong>方法</strong></p>
</li>
</ul>
<ol>
<li>以仅支持 bash 的 mini-SWE-agent 为起点。</li>
<li>在系统提示追加**“可随时写 Python 脚本并立即调用”**指令。</li>
<li>每步执行后自动插入<strong>反思提示</strong>，让 LLM 决定“是否即时造/改工具”。</li>
<li>工具即普通脚本，创建与调用均通过 bash 完成，<strong>零额外接口、零离线成本</strong>。</li>
</ol>
<ul>
<li><strong>结果</strong></li>
<li>SWE-bench Verified：75.4 % 解决率，<strong>超越所有开源代理</strong>，逼近最佳商业系统。</li>
<li>SWE-Bench Pro：45.8 % 解决率，<strong>刷新公开榜第一</strong>。</li>
<li>相对离线自改进方案（DGM/HGM）提升 8–12 pp，<strong>节省 500–1200 GPU 时</strong>。</li>
<li>跨 Claude/GPT 等多模型一致增益，越强模型收益越大；多语言基准同样有效。</li>
<li><strong>贡献</strong><br>首次实现<strong>无训练、即插即用、任务级定制</strong>的运行时自我进化，验证“智能体即软件”范式可在真实软件工程中持续、低成本、高泛化地提升性能。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Live-SWE-agent%3A%20Can%20Software%20Engineering%20Agents%20Self-Evolve%20on%20the%20Fly%3F，链接是https://arxiv.org/pdf/2511.13646，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13646。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chunqiu Steven Xia,Zhe Wang,Yan Yang,Yuxiang Wei,Lingming Zhang</p>
<p><strong>Categories</strong>: cs.SE,cs.AI,cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13646.pdf">https://arxiv.org/pdf/2511.13646.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13646">https://arxiv.org/abs/2511.13646</a></p>
<p><strong>Arxiv ID</strong>: 2511.13646</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13646">https://papers.cool/arxiv/2511.13646</a></p>
<p><strong>Published</strong>: 2025-11-17T17:58:18Z</p>
<p><strong>Updated</strong>: 2025-11-17T17:58:18.000Z</p>
<hr>
<h3 id="15-Genomic-Next-Token-Predictors-are-In-Context-Learners"><a href="#15-Genomic-Next-Token-Predictors-are-In-Context-Learners" class="headerlink" title="15. Genomic Next-Token Predictors are In-Context Learners"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.12797">Genomic Next-Token Predictors are In-Context Learners</a></h3><p>In-context learning (ICL) — the capacity of a model to infer and apply abstract patterns from examples provided within its input — has been extensively studied in large language models trained for next-token prediction on human text. In fact, prior work often attributes this emergent behavior to distinctive statistical properties in human language. This raises a fundamental question: can ICL arise organically in other sequence domains purely through large-scale predictive training? To explore this, we turn to genomic sequences, an alternative symbolic domain rich in statistical structure. Specifically, we study the Evo2 genomic model, trained predominantly on next-nucleotide (A/T/C/G) prediction, at a scale comparable to mid-sized LLMs. We develop a controlled experimental framework comprising symbolic reasoning tasks instantiated in both linguistic and genomic forms, enabling direct comparison of ICL across genomic and linguistic models. Our results show that genomic models, like their linguistic counterparts, exhibit log-linear gains in pattern induction as the number of in-context demonstrations increases. To the best of our knowledge, this is the first evidence of organically emergent ICL in genomic sequences, supporting the hypothesis that ICL arises as a consequence of large-scale predictive modeling over rich data. These findings extend emergent meta-learning beyond language, pointing toward a unified, modality-agnostic view of in-context learning.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>上下文学习（ICL）——模型从其输入中提供的示例中推断并应用抽象模式的能力——在为人类文本的下一个标记预测而训练的大型语言模型中已经被广泛研究。事实上，之前的研究通常将这种新兴行为归因于人类语言的独特统计特性。这引出了一个根本性问题：ICL是否可以仅通过大规模预测训练在其他序列领域自然而然地出现？为探讨这一点，我们转向基因组序列，这是一个拥有丰富统计结构的替代符号领域。具体而言，我们研究了Evo2基因组模型，它主要接受下一个核苷酸（A/T/C/G）预测训练，其规模相当于中型大型语言模型。我们开发了一个受控实验框架，其中包括以语言和基因组形式实例化的符号推理任务，从而能够直接比较基因组模型和语言模型的ICL。我们的结果表明，基因组模型与其语言模型对应物一样，随着上下文示例数量的增加，其模式诱导呈对数线性增长。据我们所知，这是首次在基因组序列中发现自然出现的ICL，支持了ICL作为对丰富数据进行大规模预测建模的结果而产生的假设。这些发现将新兴元学习扩展到语言之外，指向了一种统一且不依赖模态的上下文学习观点。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>这篇论文试图回答一个核心问题：</p>
<blockquote>
<p><strong>在上下文学习（In-Context Learning, ICL）是否只能在人类语言这一特定符号域中自然涌现，还是它可能是一种更普遍的、与模态无关的现象？</strong></p>
</blockquote>
<p>具体而言，论文聚焦于以下两个对立假设：</p>
<ul>
<li><strong>H1（语言特异性假设）</strong>：ICL 的涌现依赖于人类语言特有的统计结构（如并行性、组合性等），因此难以在其他序列域中自然出现。</li>
<li><strong>H2（模态无关假设）</strong>：ICL 是大规模自回归预测训练在<strong>任何</strong>具有丰富统计结构的序列数据上时，都会自然产生的“副产品”。</li>
</ul>
<p>为检验 H2，作者将研究对象从人类语言转向<strong>基因组序列</strong>——一种完全不同于自然语言的符号域。他们利用最新发布的大规模基因组模型 Evo2（仅通过“下一个核苷酸预测”任务训练），设计了一套<strong>跨模态对照实验</strong>，将相同的符号推理任务分别编码为“文本形式”和“基因组形式”，从而直接比较语言模型（Qwen3 系列）与基因组模型（Evo2 系列）的 ICL 行为。</p>
<p>总结来说，论文试图解决的关键问题包括：</p>
<ol>
<li>在从未见过人类语言、仅接受基因组序列训练的模型中，是否能<strong>自然涌现</strong>出 ICL？</li>
<li>如果涌现，其 scaling 趋势、任务敏感性与语言模型是否一致？</li>
<li>这一现象能否支持“ICL 是模态无关的压缩与预测机制产物”的观点，从而削弱“语言特异性”解释？</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work and Broader Context”中系统梳理了与 ICL 相关的研究，并将其划分为四条主线。以下按 markdown 列表形式归纳，并补充关键文献出处：</p>
<ul>
<li><strong>Emergent ICL vs Meta-ICL</strong></li>
<li>大量工作聚焦于“显式训练模型去学会上下文学习”（Meta-ICL）：</li>
<li>Garg et al. 2022、Li et al. 2023c、Raventós et al. 2023、Nejjar et al. 2024 等用线性/多项式/正弦回归任务显式训练 transformer 进行少样本函数拟合。</li>
<li>Min et al. 2022 的 MetaICL 框架、Kirsch et al. 2022、Zhang et al. 2023 等进一步将任务形式化为“输入-输出集合”上的元学习。</li>
<li>与本文关注的“无显式信号、纯粹自回归预训练后自然涌现”的 Emergent ICL 形成对照；作者指出 Meta-ICL 泛化域窄，无法解释 LLM 中跨任务泛化的 ICL。</li>
<li><strong>语言模型中 ICL 的机理解释</strong></li>
<li>数据分布视角：</li>
<li>Chen et al. 2024 提出“并行结构”假说；Hahn &amp; Goyal 2023 强调组合性；Chan et al. 2022 讨论“突发性”(burstiness) 统计量。</li>
<li>压缩/结构归纳视角：</li>
<li>Elmoznino et al. 2024a,b 用奥卡姆剃刀与复杂度论证 ICL 源于大规模压缩。</li>
<li>架构视角：</li>
<li>Xie et al. 2021 指出 transformer 比 LSTM 更利于 ICL；Lee et al. 2023 发现非 transformer 架构（如 Mamba）也能出现 ICL（Grazzi et al. 2024；Park et al. 2024）。</li>
<li><strong>非语言模态中的 ICL 尝试</strong></li>
<li>视觉：</li>
<li>Bai et al. 2024 在“连续帧预测”上观察到类似 ICL 行为，但作者认为其任务本质仍是 next-frame 预测，而非真正的多示例推理。</li>
<li>神经信号：</li>
<li>Kim et al. 2024 的 EEG-GPT 显式使用演示对进行训练，属于 Meta-ICL。</li>
<li>多模态：</li>
<li>Flamingo（Alayrac et al. 2022）、BLIP/BLIP-2（Li et al. 2022, 2023a）、Emu（Sun et al. 2023, 2024）均需显式对齐演示-标签对，归为 Meta-ICL。</li>
<li>基因组：</li>
<li>HyenaDNA（Nguyen et al. 2023）报告 ICL-like 行为，但采用软提示+指令微调，非纯粹涌现；</li>
<li>其余基因组模型（Ji et al. 2020；Dalla-torre et al. 2024；Fishman et al. 2024；Cui et al. 2024）多为编码器或规模不足，无法研究自回归 ICL。</li>
<li><strong>ICL 的评测与任务设计</strong></li>
<li>符号程序归纳：</li>
<li>Brown et al. 2020b 的 bitstring 任务、Webb et al. 2023 的 Raven 矩阵、Chollet 2019 的 ARC-AGI 强调抽象推理。</li>
<li>复杂度度量：</li>
<li>本文提出的 BitLoad 受启发于“输入敏感度”度量（Hahn &amp; Goyal 2023; Elmoznino et al. 2024a），用于量化任务难度。</li>
</ul>
<p>综上，已有研究要么聚焦语言域的 ICL 机理，要么在非语言域采用显式元学习设置。本文首次在<strong>大规模纯自回归基因组模型</strong>上验证<strong>无监督涌现的 ICL</strong>，填补了“非语言、非 Meta-ICL”这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“构建跨模态对照实验 + 大规模自回归模型 + 量化指标”三位一体的策略，系统检验 ICL 是否能在非语言域自然涌现。具体步骤如下：</p>
<ol>
<li>设计<strong>模态无关</strong>的评测框架</li>
</ol>
<ul>
<li>任务层：选用 100 个 8-bit 符号程序合成函数（身份、非、多数、移位等 30 种原语及其两两组合），保证只需 4 个符号即可表达，从而同时适配基因组（A/T/C/G）与文本（0–9 数字）词汇表。</li>
<li>编码层：同一抽象任务被“双盲”映射——基因组模型看到随机核苷酸串，语言模型看到随机数字串；分隔符、顺序、映射均随机化，避免预训练记忆干扰。</li>
<li>评估层：Monte-Carlo 采样 8 组上下文-查询对，计算 exact-match 准确率；引入“mode 基线”以排除“仅统计输出分布”即可通过的假象。</li>
</ul>
<ol>
<li>选取<strong>规模可比</strong>的纯自回归基础模型</li>
</ol>
<ul>
<li>语言侧：Qwen3 系列（0.6 B–14 B），仅经 next-token 预训练，无指令微调。</li>
<li>基因组侧：Evo2 系列（1 B/7 B/40 B），仅经 next-nucleotide 预训练，无生物学下游微调。</li>
<li>计算量匹配：Evo2-40 B 与 Qwen3-14 B 的 6ND 估算 FLOPs 处于同一量级（≈ 2–3 × 10²⁴），保证“大模型+大数据”条件一致。</li>
</ul>
<ol>
<li>系统测量 ICL 的<strong>涌现与 scaling</strong></li>
</ol>
<ul>
<li>shot 数梯度：1 → 128 等比倍增，记录准确率随演示样本数的变化。</li>
<li>模型尺度梯度：同一家族内由小到大，观察参数增量对 ICL 斜率的影响。</li>
<li>复杂度梯度：提出 BitLoad（需关注输入比特数）与 BitDiversity（输出熵），分析任务难度如何与模态交互。</li>
</ul>
<ol>
<li>统计与误差控制</li>
</ol>
<ul>
<li>采用两级非参数 cluster-bootstrap（函数级+样本级 5000 次重采样），给出标准误与显著性。</li>
<li>引入“可理解错误”分析，证明低 shot 时的虚假相关在 ≥16 shots 后迅速衰减，确保后期性能反映真实规则归纳。</li>
</ul>
<ol>
<li>结果对比与机理解读</li>
</ol>
<ul>
<li>两条 scaling 曲线均呈<strong>对数线性增长</strong>，且 Evo2 在相同参数量下斜率更高，首次提供“非语言、非 Meta-ICL”的涌现证据。</li>
<li>任务复杂度分析显示 Evo2 对高 BitLoad 函数更稳健，提示基因组长期上下文建模可能强化局部-全局模式抽取。</li>
<li>结合前人提出的 E1–E3 框架，作者指出：数据特异性（E1）并非必要，大规模压缩（E2）与足够表达力的架构（E3）即可在任意富统计结构序列中诱发 ICL，从而支持 H2 模态无关假设。</li>
</ul>
<p>通过上述控制变量与跨模态对照，论文将“ICL 是否只能在语言里涌现”转化为可测量的实验问题，并用 Evo2 的实证结果给出了肯定回答：<br><strong>只要序列统计结构丰富、模型规模与数据量足够，ICL 会在任何 next-token 预测任务中自然出现，与是否为人类语言无关。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计并执行了<strong>三类核心实验</strong>，全部围绕“跨模态、同任务、同规模”原则展开，以系统检验基因组与语言模型在上下文学习（ICL）上的异同。所有实验均公开代码与数据，便于复现。</p>
<p>1. 主实验：Few-shot Bitstring Program Synthesis</p>
<p><strong>目的</strong>：测量 ICL 准确率随演示样本数（shots）与模型规模（parameters）的变化趋势。</p>
<p><strong>流程</strong></p>
<ol>
<li>任务池：100 个 8-bit → 8-bit 的确定性函数（30 个原语 + 70 个两两组合）。</li>
<li>双盲编码</li>
</ol>
<ul>
<li>基因组通道：0/1 → 随机选两个核苷酸（如 A/T），其余两个作分隔符（如 C/G）。</li>
<li>语言通道：0/1 → 随机选两个数字（如 3/1），其余数字作分隔符。</li>
<li>每 trial 重新随机映射，防止记忆。</li>
</ul>
<ol>
<li>提示格式：k 组 <code>input SEP output SEP</code> 后接查询输入，模型自回归生成 8 位输出。</li>
<li>评估：Exact-match 准确率，Monte-Carlo 8 次采样，bootstrap 5000 次估标准误。</li>
</ol>
<p><strong>变量网格</strong></p>
<ul>
<li>shots ∈ {1, 2, 4, 8, 16, 32, 64, 128}</li>
<li>基因组模型：Evo2-1B / 7B / 40B</li>
<li>语言模型：Qwen3-0.6B / 1.7B / 4B / 8B / 14B</li>
</ul>
<p><strong>关键结果</strong></p>
<ul>
<li>两家模型均呈<strong>log-linear 增长</strong>（斜率显著 p≤1e-3）。</li>
<li>同规模下 Evo2 准确率<strong>显著高于</strong> Qwen3（e.g. 128-shot 时 40B≈41% vs 14B≈34%）。</li>
<li>所有模型在 ≥16 shots 后<strong>显著超越</strong>“mode 基线”（仅输出上下文最频繁结果），证实非统计猜测。</li>
</ul>
<p>2. 任务复杂度实验：BitLoad &amp; BitDiversity 分析</p>
<p><strong>目的</strong>：揭示两家模型对不同“输入依赖度”与“输出熵”任务的敏感性。</p>
<p><strong>指标</strong></p>
<ul>
<li>BitLoad(f)：使输出发生变化的输入比特数，0=常数函数，8=全位依赖。</li>
<li>BitDiversity(y)：输出串中“少数位”的数量，越小越确定。</li>
</ul>
<p><strong>做法</strong></p>
<ul>
<li>固定 128-shot，对每模型-函数组合统计准确率。</li>
<li>按 BitLoad / BitDiversity 分箱，绘制均值±SE 曲线。</li>
</ul>
<p><strong>发现</strong></p>
<ul>
<li>Qwen3 准确率随 BitLoad 增加<strong>断崖式下跌</strong>（≥4 即 &lt;20%）；Evo2 下降更缓，至 BitLoad=6 仍保持 40%。</li>
<li>两家模型均随 BitDiversity 增大而性能下降，但 Evo2 在低熵区优势更明显。</li>
<li>高 BitLoad 但低 BitDiversity 的任务仍可能被 Evo2 解决，说明“输出可预测性”同样关键。</li>
</ul>
<p>3. 细粒度任务画像：Exclusive Competence &amp; “可理解错误”消融</p>
<p><strong>目的</strong>：定位两家模型各自“独家擅长”的任务类型，并量化低-shot 时的虚假相关。</p>
<p><strong>步骤</strong></p>
<ol>
<li>对 128-shot 结果按准确率排序，取 Top-10 交集与差集。</li>
<li>定义“可理解错误”：模型输出虽错，但存在另一函数与上下文示例完全兼容——即“看似合理”的误报。</li>
<li>统计该错误率随 shots 的衰减曲线。</li>
</ol>
<p><strong>结果</strong></p>
<ul>
<li>Qwen3 独占优势：右移、全局统计（minority、parity）类任务；Evo2 独占优势：全比特翻转、半段掩码等“长距离复制-改写”任务。</li>
<li>可理解错误在 1-shot 时高达 16%，8-shot 降至 &lt;1%，32-shot 消失，证明后期性能确属规则归纳而非表面相关。</li>
</ul>
<p>4. 辅助分析实验（含附录）</p>
<ul>
<li><strong>Scaling 系数回归</strong>：拟合 <code>Accuracy = α₀ + α₁ log(shots)</code>，分离“初始能力 α₀”与“ICL 效率 α₁”；证实 Evo2 的 α₁ 在同参数量下显著更大。</li>
<li><strong>0-BitDiversity 先验分析</strong>：发现 25% 任务真值为全 0/全 1，模型在低 shot 时严重过度预测此类输出；随着 shot 增加，预测分布迅速与真实分布对齐，再次验证 ICL 机制。</li>
<li><strong>Monte-Carlo 稳定性检验</strong>：增大采样至 m=64 的子实验显示，主实验 m=8 已能稳定复现相对排序，排除随机波动解释。</li>
</ul>
<p>综上，论文通过**“主 scaling 曲线 + 复杂度灵敏度 + 任务级行为分解”**三层实验，首次在完全非语言、无监督的基因组模型上复现并量化了与语言模型高度一致且更优的 ICL 现象，从而支撑“ICL 是模态无关的大尺度自回归压缩产物”这一结论。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可直接延续本文的实验范式与发现，分为“任务扩展”“机理深挖”“跨模态普查”与“应用-风险”四大主题，供后续研究参考。</p>
<p>1 任务与表示扩展</p>
<ul>
<li><strong>多符号级编码</strong></li>
<li>利用核苷酸三联体（codon，64 种）或氨基酸字母表（20 种）设计 4-bit 或 5-bit 任务，考察词汇量扩大后 ICL 曲线是否出现饱和或跃迁。</li>
<li><strong>层次化任务</strong></li>
<li>在基因组侧引入“外显子-内含子”拼接、RNA 二级结构配对规则等生物可解释约束，检验模型能否在更贴近真实基因组结构的上下文中进行少样本归纳。</li>
<li><strong>动态规则漂移</strong></li>
<li>同一 prompt 内嵌入分段函数（前 k/2 示例遵循 f，后 k/2 遵循 g），测试模型能否在线切换规则，量化其“元学习速率”。</li>
</ul>
<p>2 机理与解释性</p>
<ul>
<li><strong>诱导头（induction head）类比</strong></li>
<li>对 Evo2 attention map 进行“前缀-完成”探测，寻找与 LLM 诱导头功能同源的注意力模式，验证“拷贝-移位-改写”回路是否跨模态通用。</li>
<li><strong>压缩-预测因果干预</strong></li>
<li>在训练阶段引入可控的基因组重复序列比例，观察 ICL 斜率 α₁ 是否随数据可压缩性单调变化，直接检验“压缩驱动 ICL”假说。</li>
<li><strong>低层 vs 高层编码</strong></li>
<li>用 probing 方法定位哪一层能最早恢复 BitLoad 信息，对比语言模型中“抽象语义”出现深度，揭示两种模态的层级差异。</li>
</ul>
<p>3 跨模态普查与对比</p>
<ul>
<li><strong>时序/日志模态</strong></li>
<li>将本文 bitstring 任务映射到系统日志 token（ERROR/WARN/INFO/SEP），测试日志领域大模型（如 LogGPT）是否呈现相同 log-linear 趋势。</li>
<li><strong>棋谱与符号音乐</strong></li>
<li>用 UCI 棋谱或 MIDI 音符作为 4-symbol 序列，设计“下一步合法走子/和弦”少样本任务，验证结构化博弈/音乐规则能否被 ICL 捕获。</li>
<li><strong>物理场序列</strong></li>
<li>将二维湍流或 PDE 解快照离散成 4-level 标量符号，考察“next-frame”预测模型能否在上下文中推断隐藏物理算子。</li>
</ul>
<p>4 应用、控制与风险</p>
<ul>
<li><strong>生物序列设计</strong></li>
<li>利用 Evo2 的 ICL 能力进行“零梯度”启动子或 RBS 元件设计：仅通过示例序列-活性对，让模型生成高活性候选，减少昂贵实验迭代。</li>
<li><strong>可控制性评测</strong></li>
<li>引入“对抗性示例”——在 prompt 中植入极少误导性演示（如 1/128），测量准确率下降幅度，量化基因组模型对恶意提示的鲁棒性。</li>
<li><strong>跨模态知识泄漏</strong></li>
<li>将同一语义任务先用语言模板预演，再切换到基因组编码，观察性能是否出现“迁移 boost”，揭示不同模态间是否存在共享的隐空间结构。</li>
</ul>
<p>5 方法学改进</p>
<ul>
<li><strong>自适应采样</strong></li>
<li>当前 Monte-Carlo 固定 m=8，可用贝叶斯优化动态增加高方差任务的采样数，降低总计算量同时提升统计功效。</li>
<li><strong>连续-离散混合</strong></li>
<li>保留基因组序列的连续演化信号（保守性分数、表观遗传强度）作为边信息，设计“半符号” ICL 任务，考察离散规则与连续信号如何协同。</li>
<li><strong>参数高效干预</strong></li>
<li>使用 LoRA 或子网络剪枝，仅微调 0.1% 参数即可让 Evo2 在特定函数类上达到 &gt;90% 准确率，对比“全参数微调”与“纯 ICL”效率差异，量化上下文学习的参数-样本替代率。</li>
</ul>
<p>简言之，基因组 ICL 的“存在性”已被本文证实，下一步值得从“任务更丰富”“机理更因果”“模态更宽广”“应用更落地”四个维度继续深耕，以期建立一套<strong>模态无关的上下文学习理论基准与工程框架</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心概要</strong><br>题目：Genomic Next-Token Predictors are In-Context Learners<br>arXiv: 2511.12797</p>
<p>1 研究动机</p>
<ul>
<li>上下文学习（ICL）目前只在人类语言模型中被广泛验证。</li>
<li>假设 H1：ICL 依赖语言特有结构；假设 H2：ICL 是大规模自回归压缩的普适副产品。</li>
<li>基因组序列富含统计规律，且已有 40 B 参数的 Evo2 模型仅通过“下一个核苷酸预测”训练，为检验 H2 提供天然试验场。</li>
</ul>
<p>2 方法框架</p>
<ul>
<li><strong>跨模态对照</strong>：设计 100 个 8-bit → 8-bit 符号程序合成任务，同一任务分别用<br>– 基因组字母表（A/T/C/G）<br>– 数字字母表（0–9）<br>双盲编码，避免领域偏好。</li>
<li><strong>模型配对</strong>：<br>– 语言侧 Qwen3（0.6 B–14 B）<br>– 基因组侧 Evo2（1 B/7 B/40 B）<br>均为纯自回归基础模型，计算量同级（≈ 10²⁴ FLOPs）。</li>
<li><strong>评估协议</strong>：1–128 shots 逐次倍增，Monte-Carlo 估算 exact-match 准确率，bootstrap 定误差，并设“mode 基线”排除统计猜测。</li>
</ul>
<p>3 主要发现</p>
<ul>
<li><strong>两条 log-linear 曲线</strong>：两家模型准确率均随 log(shots) 线性上升（p≤10⁻³）。</li>
<li><strong>同规模超越</strong>：Evo2-7B/40B 在 128-shot 达 ≈41%，显著高于 Qwen3-14B 的 ≈34%。</li>
<li><strong>复杂度鲁棒性</strong>：引入 BitLoad（输入依赖位数）与 BitDiversity（输出熵），Evo2 在高 BitLoad 区下降更缓，显示对长距离依赖更鲁棒。</li>
<li><strong>任务专属优势</strong>：<br>– Qwen3 擅全局统计（minority、parity）与位移；<br>– Evo2 擅全比特翻转、半段掩码等“复制-改写”操作。</li>
<li><strong>可理解错误</strong>：低 shot 时虚假相关占 16%，≥16 shots 后降至 &lt;1%，证实后期为真实规则归纳。</li>
</ul>
<p>4 结论与意义</p>
<ul>
<li>首次证明<strong>非语言、无监督、仅 next-token 训练的基因组模型</strong>也能自然涌现 ICL，且 scaling 行为与语言模型平行甚至更优。</li>
<li>结果支持“ICL 是模态无关的大尺度压缩现象”（H2），削弱“语言特有结构”解释（H1）。</li>
<li>为“跨模态元学习”奠定实验基础，提示任何富统计结构序列（日志、棋谱、时序、物理场）都可能成为 ICL 新土壤。</li>
</ul>
<p>5 可供后续探索</p>
<ul>
<li>多符号（codon、氨基酸）任务、动态规则漂移、诱导头回路比对、生物序列设计应用、跨模态知识泄漏与对抗鲁棒性等。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Genomic%20Next-Token%20Predictors%20are%20In-Context%20Learners，链接是https://arxiv.org/pdf/2511.12797，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.12797。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Nathan Breslow,Aayush Mishra,Mahler Revsine,Michael C. Schatz,Anqi Liu,Daniel Khashabi</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,q-bio.GN</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12797.pdf">https://arxiv.org/pdf/2511.12797.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12797">https://arxiv.org/abs/2511.12797</a></p>
<p><strong>Arxiv ID</strong>: 2511.12797</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12797">https://papers.cool/arxiv/2511.12797</a></p>
<p><strong>Published</strong>: 2025-11-16T21:56:39Z</p>
<p><strong>Updated</strong>: 2025-11-16T21:56:39.000Z</p>
<hr>
<h3 id="16-WebCoach-Self-Evolving-Web-Agents-with-Cross-Session-Memory-Guidance"><a href="#16-WebCoach-Self-Evolving-Web-Agents-with-Cross-Session-Memory-Guidance" class="headerlink" title="16. WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.12997">WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</a></h3><p>Multimodal LLM-powered agents have recently demonstrated impressive capabilities in web navigation, enabling agents to complete complex browsing tasks across diverse domains. However, current agents struggle with repetitive errors and lack the ability to learn from past experiences across sessions, limiting their long-term robustness and sample efficiency. We introduce WebCoach, a model-agnostic self-evolving framework that equips web browsing agents with persistent cross-session memory, enabling improved long-term planning, reflection, and continual learning without retraining. WebCoach consists of three key components: (1) a WebCondenser, which standardizes raw navigation logs into concise summaries; (2) an External Memory Store, which organizes complete trajectories as episodic experiences; and (3) a Coach, which retrieves relevant experiences based on similarity and recency, and decides whether to inject task-specific advice into the agent via runtime hooks. This design empowers web agents to access long-term memory beyond their native context window, improving robustness in complex browsing tasks. Moreover, WebCoach achieves self-evolution by continuously curating episodic memory from new navigation trajectories, enabling agents to improve over time without retraining. Evaluations on the WebVoyager benchmark demonstrate that WebCoach consistently improves the performance of browser-use agents across three different LLM backbones. With a 38B model, it increases task success rates from 47% to 61% while reducing or maintaining the average number of steps. Notably, smaller base models with WebCoach achieve performance comparable to the same web agent using GPT-4o.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态大语言模型驱动的代理最近在网页导航方面展示了令人印象深刻的能力，使代理能够在不同领域完成复杂的浏览任务。然而，目前的代理在处理重复性错误时仍然存在困难，并且缺乏跨会话从过去经验中学习的能力，这限制了它们的长期稳健性和样本效率。我们提出了 WebCoach，这是一种与模型无关的自我进化框架，它为网页浏览代理提供持久的跨会话记忆，从而在无需重新训练的情况下实现更好的长期规划、反思和持续学习。WebCoach 包含三个关键组件：(1) WebCondenser，它将原始导航日志标准化为简明摘要；(2) 外部存储器（External Memory Store），将完整的轨迹组织为情节化经验；(3) Coach，根据相似性和新近性检索相关经验，并决定是否通过运行时钩子向代理注入特定任务的建议。这一设计使网页代理能够访问超出其原生上下文窗口的长期记忆，从而在复杂的浏览任务中提高稳健性。此外，WebCoach 通过不断整理来自新导航轨迹的情节记忆实现自我进化，使代理能够随时间改进而无需重新训练。在 WebVoyager 基准上的评估表明，WebCoach 持续提高了使用浏览器的代理在三种不同大语言模型基础上的性能。使用 38B 模型时，它将任务成功率从 47% 提高到 61%，同时减少或保持平均步数。值得注意的是，较小的基础模型在 WebCoach 的加持下，其性能可与使用 GPT-4o 的同一网页代理相媲美。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现有基于多模态大语言模型（LLM）的网页导航智能体在<strong>跨会话记忆缺失</strong>方面的根本缺陷，具体表现为：</p>
<ul>
<li><strong>重复性错误</strong>：同一类误操作（如点错按钮、陷入登录循环、触发验证码）在不同任务或会话中反复出现，无法被“记住”并避免。</li>
<li><strong>零长期学习</strong>：每次任务结束后，轨迹数据被丢弃，智能体无法从过去的成功或失败中累积经验，导致样本效率低、鲁棒性差。</li>
<li><strong>上下文窗口限制</strong>：即使简单地把历史拼接进提示，也会迅速超出 LLM 的上下文长度，且噪声大、检索慢。</li>
</ul>
<p>WebCoach 提出一种<strong>模型无关、无需重训</strong>的自我演化框架，通过持久化、可检索的跨会话记忆，让智能体在运行时实时“回忆”相关经验，主动注入针对性建议，从而：</p>
<ol>
<li>在长期尺度上减少重复错误；</li>
<li>用更少步数完成复杂网页任务；</li>
<li>随时间自我累积高质量经验，持续提高成功率。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 5 节“Related Work”中将相关研究归为三大主线，并给出代表性文献。以下按主题梳理，保留关键出处（arXiv 年份为发表版本号年份），方便快速定位。</p>
<p>1. 以推理为中心的 Web &amp; GUI 智能体</p>
<ul>
<li><strong>GUI 控制</strong>：利用奖励塑形、课程学习、自反思把小型 VLM 变成手机/桌面控制器。</li>
<li>Digirl (Bai et al., 2024)</li>
<li>UI-Agile (Lian et al., 2025)</li>
<li>GUI-R1 (Luo et al., 2025)</li>
<li><strong>网页导航</strong>：多轮 RL、结构化探索、层次规划提升 WebArena/WebShop 成绩。</li>
<li>WebAgent-R1 (Wei et al., 2025)</li>
<li>Go-Browse (Gandhi &amp; Neubig, 2025)</li>
<li>WebRL (Qi et al., 2024)</li>
<li><strong>平台级统一系统</strong>：把规划、工具调用、自演化整合到桌面或 Windows 工作流。</li>
<li>Agent S (Agashe et al., 2024)</li>
<li>UFO (Zhang et al., 2024)</li>
</ul>
<p>2. 智能体记忆与上下文管理</p>
<ul>
<li><strong>简单历史压缩</strong>即可提升网页自动化准确率。</li>
<li>Turbocharging Web Automation (Zhu et al., arXiv 2507)</li>
<li><strong>双存储架构</strong>（情节+语义）降低容量需求并提高检索精度。</li>
<li>Mirix (Wang &amp; Chen, 2025)</li>
<li>A-Mem (Xu et al., 2025)</li>
<li><strong>操作系统式分层记忆</strong>：短-中-长期分层或动作日志。</li>
<li>Memory OS (Kang et al., 2025)</li>
<li>Chain-of-Memory (Gao et al., 2025b)</li>
<li><strong>潜空间检索 / Zettelkasten 链接</strong>支持百万级 token 推理。</li>
<li>M+ (Wang et al., 2025a)</li>
<li>MemAgent (Yu et al., 2025)</li>
<li><strong>工作流导向存储</strong>挖掘可复用动作序列。</li>
<li>Agent Workflow Memory (Wang et al., 2024)</li>
<li>AgentKB (Tang et al., 2025)</li>
</ul>
<p>3. 自我演化（Self-Evolving）智能体</p>
<ul>
<li><strong>经验回放 + 自然语言反思</strong>实现免重训的持续改进。</li>
<li>Reflexion (Shinn et al., NeurIPS 2023)</li>
<li>Contextual Experience Replay (Liu et al., 2025b)</li>
<li><strong>程序合成发现可迁移技能或 API</strong>。</li>
<li>SkillWeaver (Zheng et al., 2025)</li>
<li>GUI-Explorer (Xie et al., 2025)</li>
<li><strong>自生成轨迹 + 可扩展课程</strong>缩小开源与闭源模型差距。</li>
<li>WebRL (Qi et al., 2024)</li>
<li>Llamas-V 自改进工作 (Patel et al., 2024)</li>
<li><strong>世界模型想象与转移抽象</strong>用于长时程或移动场景。</li>
<li>WebEvolver (Fang et al., 2025)</li>
<li>Agents with World Models (Chae et al., 2024)</li>
</ul>
<p>与 WebCoach 的差异</p>
<ul>
<li>上述研究要么聚焦单会话内反思，要么需要昂贵重训/RL；</li>
<li>WebCoach 首次把“<strong>跨会话情节记忆 + 运行时检索注入</strong>”做成即插即用层，无需改动底层智能体结构即可持续演化。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>WebCoach 把“跨会话记忆缺失”解耦为三个可插拔模块，通过<strong>离线累积、在线检索、运行时注入</strong>的闭环，实现免重训的自我演化。核心流程如下：</p>
<p>1. WebCondenser —— 把原始轨迹蒸馏成“记忆原子”</p>
<ul>
<li><strong>输入</strong>：每步的 <code>(o_i, a_i, r_i)</code> JSON 日志。</li>
<li><strong>处理</strong>：≤8B 的小模型把整条轨迹压缩成固定模式</li>
<li>3-5 句自然语言摘要</li>
<li>1536-d 嵌入向量</li>
<li>成功/失败标签 + 关键失败模式或成功 workflow</li>
<li><strong>路由</strong>：任务未完成 → 只实时推给 Coach，<strong>不存盘</strong>；<br>任务结束 → 标记为 complete，写入 EMS，防止噪声累积。</li>
</ul>
<p>2. External Memory Store (EMS) —— 持久化“经验池”</p>
<ul>
<li><strong>存储格式</strong>：<br>⟨embedding, summary, meta⟩<br>meta 含 episode_id、domain、user goal、model、步数、时间戳。</li>
<li><strong>检索引擎</strong>：FAISS-HNSW-128，<strong>对数时间</strong>近似最近邻；<br>相似度： score(e_t, e_i) = (e_t^top e_i) / (|e_t||e_i|) 。</li>
<li><strong>冷启动</strong>：可一次性导入高质量外部轨迹（如 GPT-4o 生成的 600 条），也可从零开始自举。</li>
<li><strong>隔离机制</strong>：评估时把<strong>同任务 ID 的 episode 强制过滤</strong>，防止数据泄漏。</li>
</ul>
<p>3. Coach —— 运行时“记忆-感知”决策器</p>
<ul>
<li><strong>输入</strong>：</li>
</ul>
<ol>
<li>当前部分轨迹的 Condenser 摘要；</li>
<li>EMS 返回的 Top-K（K=5）相似完整经验。</li>
</ol>
<ul>
<li><strong>决策规则</strong>：8B LLM 零样本判断</li>
<li>若预测到高失败概率（循环、验证码、4xx）或存在更快 workflow → <code>intervene=true</code>；</li>
<li>否则返回 <code>false</code>，保持静默，避免干扰。</li>
<li><strong>注入方式</strong>：把建议作为 system message <strong>同步追加</strong>到 Actor 的下一回合 prompt，<strong>不更新 Actor 权重</strong>，完全无侵入。</li>
<li><strong>自我演化</strong>：随着 Actor 在线产生新轨迹，Condenser 持续写入 EMS，Coach 检索分布逐渐向“自身偏好”偏移，形成正反馈。</li>
</ul>
<p>4. 并行评估管线 —— 保证大规模在线实验可行</p>
<ul>
<li><strong>两层并行</strong>：Docker 容器级隔离 + Python 子进程调度。</li>
<li><strong>动态批</strong>（LPT 启发式）：长任务先启动，空闲 worker 立即拉取最长剩余任务，整体运行时间从 82 h 降至 14 h（–83%）。</li>
<li><strong>超时保护</strong>：单步 30 s、整任务 50 步硬上限，防止无限循环。</li>
</ul>
<p>5. 效果总结（WebVoyager 643 在线任务）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>基线 SR</th>
<th>+WebCoach SR</th>
<th>提升</th>
<th>步数变化</th>
</tr>
</thead>
<tbody>
<tr>
<td>Skywork-38B</td>
<td>47.3 %</td>
<td>61.4 %</td>
<td>+14.1 ppt</td>
<td>10.7 → 10.2 ↓</td>
</tr>
<tr>
<td>Qwen-VL-32B</td>
<td>49.5 %</td>
<td>57.1 %</td>
<td>+7.6 ppt</td>
<td>13.3 → 11.9 ↓</td>
</tr>
<tr>
<td>Qwen-VL-7B</td>
<td>32.8 %</td>
<td>31.1 %</td>
<td>–1.7 ppt</td>
<td>16.4 → 17.4 ↑（认知阈值不足）</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>自演化 &gt; 外部经验</strong>：动态 EMS 比“ frozen GPT-4o 经验”平均再提 2-3 ppt，步数更少。</li>
<li><strong>延迟可控</strong>：检索 9-10 ms，Coach 增加约 150 s，但节省 1-2 步冗余操作，整体吞吐可接受。</li>
</ul>
<p>通过以上设计，WebCoach 把“记忆”从一次性上下文升级为<strong>可累积、可检索、可行动</strong>的外部知识库，使任何现成的网页智能体在不重训、不改架构的前提下，实现跨会话的持续自我改进。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 WebVoyager 的 643 个<strong>真实在线网页任务</strong>上跑了<strong>四组对比实验</strong>，覆盖 15 个域名、三种开源 backbone，外加 GPT-4o 天花板；所有指标均在<strong>真实浏览器环境</strong>（Docker+Chromium）内实测，非缓存或仿真。关键实验设计如下：</p>
<p>1. 实验条件</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>配置</th>
<th>记忆来源</th>
<th>Coach 模型</th>
<th>是否在线更新记忆</th>
<th>目的</th>
</tr>
</thead>
<tbody>
<tr>
<td>Baseline</td>
<td>无</td>
<td>无</td>
<td>—</td>
<td>测量原生能力</td>
</tr>
<tr>
<td>Frozen-EMS(GPT-4o)</td>
<td>GPT-4o 轨迹</td>
<td>GPT-4o</td>
<td>否</td>
<td>验证“外部高质量经验”效果</td>
</tr>
<tr>
<td>Frozen-EMS(Qwen3-8B)</td>
<td>GPT-4o 轨迹</td>
<td>Qwen3-8B</td>
<td>否</td>
<td>验证 Coach 本身能力</td>
</tr>
<tr>
<td>Dynamic-EMS(Qwen3-8B)</td>
<td>智能体自生成</td>
<td>Qwen3-8B</td>
<td>每任务后追加</td>
<td>验证自我演化 vs 外部经验</td>
</tr>
</tbody>
</table>
</div>
<p>2. 评价指标</p>
<ul>
<li><strong>Success Rate (SR)</strong>：任务最终状态与人工标注目标匹配比例。</li>
<li><strong>Average Steps</strong>：成功/失败都算，反映冗余动作。</li>
<li><strong>Average Time</strong>：含 Coach &amp; Condenser 推理开销。</li>
<li><strong>Per-domain SR</strong>：15 个子域单独统计，观察记忆对“复杂语义站点”是否更有效。</li>
</ul>
<p>3. 主要结果（Overall, 643 任务）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>配置</th>
<th>SR ↑</th>
<th>Steps ↓</th>
<th>Time (s)</th>
</tr>
</thead>
<tbody>
<tr>
<td>GPT-4o</td>
<td>Baseline</td>
<td>65.3 %</td>
<td>10.9</td>
<td>118</td>
</tr>
<tr>
<td>Qwen-VL-7B</td>
<td>Baseline</td>
<td>32.8 %</td>
<td>16.4</td>
<td>144</td>
</tr>
<tr>
<td>Qwen-VL-7B</td>
<td>Dynamic</td>
<td>31.1 %</td>
<td>17.4</td>
<td>200</td>
</tr>
<tr>
<td>Qwen-VL-32B</td>
<td>Baseline</td>
<td>49.5 %</td>
<td>13.3</td>
<td>201</td>
</tr>
<tr>
<td>Qwen-VL-32B</td>
<td>Frozen-GPT4</td>
<td>54.7 %</td>
<td>10.9</td>
<td>460</td>
</tr>
<tr>
<td>Qwen-VL-32B</td>
<td>Dynamic</td>
<td>57.1 %</td>
<td>11.9</td>
<td>367</td>
</tr>
<tr>
<td>Skywork-38B</td>
<td>Baseline</td>
<td>47.3 %</td>
<td>10.7</td>
<td>215</td>
</tr>
<tr>
<td>Skywork-38B</td>
<td>Frozen-GPT4</td>
<td>55.5 %</td>
<td>10.7</td>
<td>520</td>
</tr>
<tr>
<td>Skywork-38B</td>
<td>Dynamic</td>
<td>61.4 %</td>
<td>10.2</td>
<td>395</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>最大绝对提升</strong>：Skywork-38B +14.1 ppt，一步达到 GPT-4o 的 94 % 水平。</li>
<li><strong>步数几乎不增甚至下降</strong>，说明增益来自“更优路径”而非暴力搜索。</li>
<li><strong>7B 模型无显著收益</strong>，论文提出“认知阈值”假设：模型需具备初步推理能力才能利用外部记忆。</li>
</ul>
<p>4. 消融与微观分析</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>K 值</td>
<td>K=5 在延迟-覆盖率 trade-off 上最优；继续增大无显著提分。</td>
</tr>
<tr>
<td>记忆来源</td>
<td>自生成轨迹比外部 GPT-4o 轨迹平均高 2-3 ppt，且步数更少（风格一致）。</td>
</tr>
<tr>
<td>域名难度</td>
<td>Apple、ArXiv、BBC 等“多步+语义歧义”站点提升 10-20 ppt；Booking、Google Flights 等“单页表单”站点几乎不变。</td>
</tr>
<tr>
<td>检索泄漏控制</td>
<td>若允许同任务 ID 记忆泄漏，SR 虚高 6-8 ppt；论文强制过滤保证无泄漏。</td>
</tr>
</tbody>
</table>
</div>
<p>5. 效率测试</p>
<ul>
<li><strong>EMS 检索延迟</strong>：600 k 轨迹下，K=1~10 均 9-10 ms（200 次平均）。</li>
<li><strong>整体吞吐量</strong>：异步动态批 + LPT 调度使 643 任务从 82 h 降至 14 h（–83 %），同等 GPU 预算下完成大规模在线评估。</li>
</ul>
<p>6. 可视示例</p>
<p>附录 A 给出 Apple 站点多色 HomePod mini 查询的逐步截图：</p>
<ul>
<li>Coach 检测到“循环滚动”模式，注入“直接找 Choose your color 区块”建议；</li>
<li>Actor 随即定位颜色选择器，任务成功；</li>
<li>Condenser 把成功 workflow 回写 EMS，供后续同类任务检索。</li>
</ul>
<p>综上，实验从<strong>总体性能、组件消融、记忆来源、域名差异、系统效率</strong>五个维度验证了 WebCoach 的通用性与可扩展性，并确认“<strong>自我演化 + 运行时记忆注入</strong>”是提升开源网页智能体的一条可行且高效的路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“框架自身演进”“记忆机制升级”“训练与优化”“评测与落地”四大类，均保持与 WebCoach 的 plug-and-play 设计兼容。</p>
<p>1. 框架自身演进</p>
<ul>
<li><strong>单模型化</strong>：将 Condenser-Coach-EMS 蒸馏进同一个轻量模型，消除多 LLM 级联的推理延迟与部署复杂度。</li>
<li><strong>端到端可微记忆</strong>：尝试把检索结果作为软提示或嵌入门控，直接参与 Actor 的注意力计算，用强化学习优化“何时读、读多少”。</li>
<li><strong>多智能体共享记忆池</strong>：不同 backbone/不同租户共用 EMS，引入联邦检索或隐私过滤，研究跨模型知识互补上限。</li>
<li><strong>在线课程自我采样</strong>：失败任务自动重排优先级，形成难度递增的“记忆课程”，加速冷启动。</li>
</ul>
<p>2. 记忆机制升级</p>
<ul>
<li><strong>层次化情节存储</strong>：把轨迹拆成子目标级“技能块”(skill chunk)，支持子任务级检索与拼接，减少整链冗余。</li>
<li><strong>多模态键值</strong>：除了文本摘要，同时用 DOM 树、屏幕截图、UI 坐标框的嵌入做联合检索，提升视觉 grounding 场景下的召回。</li>
<li><strong>时间衰减 + 因果依赖</strong>：给记忆加半衰期权重或因果图，防止过时 UI 元素（price、促销、布局改版）被反复推荐。</li>
<li><strong>可解释记忆</strong>：为每条经验附加“适用条件-副作用”元数据，Coach 在注入时同时给出置信度与解释，方便人工审计。</li>
</ul>
<p>3. 训练与优化</p>
<ul>
<li><strong>Coach 的 RL 微调</strong>：用长期任务回报（如最终成功、步数惩罚）作为奖励，对 Coach 做离线 DPO 或在线 PPO，摆脱纯零样本提示。</li>
<li><strong>对抗性记忆攻击</strong>：研究恶意轨迹污染 EMS 能否误导 Actor，建立鲁棒检索或记忆审核机制。</li>
<li><strong>参数高效记忆</strong>：探索 LoRA/AdaLoRA 把记忆信息直接写入小规模适配器，实现“权重+显式记忆”混合更新。</li>
</ul>
<p>4. 评测与落地</p>
<ul>
<li><strong>更长周期 continual test</strong>：连续运行数周、覆盖 UI drift、节假日版面改版，绘制 SR-时间曲线，量化记忆保鲜能力。</li>
<li><strong>跨语言/跨地域站点</strong>：评估记忆在非英语、阅读顺序从右到左、或政府/银行强合规页面的迁移性。</li>
<li><strong>私有企业内网场景</strong>：登录态、JWT 超时、二次验证等私密流程下，如何脱敏存储记忆并符合 GDPR/CCPA。</li>
<li><strong>用户个性化记忆</strong>：同一站点不同用户偏好（货币、语言、会员等级）引入用户级 embedding，实现“千人千面”的个性化 Coach。</li>
<li><strong>真实生产 A/B</strong>：与现有 RPA、客服机器人并跑，统计人工接管率、平均处理时间、客户满意度，验证商业 ROI。</li>
</ul>
<p>5. 理论层面</p>
<ul>
<li><strong>记忆容量-性能 scaling law</strong>：固定模型大小，改变 EMS 规模，拟合 SR = a·log(N_memory) + b，给出经验上的容量边际效应。</li>
<li><strong>认知阈值量化</strong>：通过探测任务复杂度与模型参数量关系，给出“能充分利用外部记忆”的最小参数量或最小预训练 loss。</li>
<li><strong>记忆与探索的权衡</strong>：建立 Bandit 框架，把“遵循记忆”视为 exploit，“尝试新路径”视为 explore，用 Thompson Sampling 动态调节 Coach 的干预概率。</li>
</ul>
<p>这些方向既可直接增量式接入现有 WebCoach 接口，也可作为独立课题展开，推动“持久记忆网页智能体”从实验室走向长期自主演化的生产系统。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>WebCoach：让网页智能体“长记性”的即插即用框架<br>—— 内容一句话总结：<strong>把跨会话经验变成可检索、可注入、可自增的外部记忆，无需重训即可持续提升开源网页智能体。</strong></p>
<p>1. 痛点</p>
<ul>
<li>现有 LLM 网页智能体每次任务后轨迹丢弃 → 重复踩坑、样本效率低。</li>
<li>上下文拼接受长度限制，无法累积长期经验。</li>
</ul>
<p>2. 解法（三模块）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>职责</th>
<th>关键设计</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebCondenser</td>
<td>把原始 (o,a,r) 日志蒸馏成 3-5 句摘要 + 1536-d 嵌入 + 成功/失败标签</td>
<td>仅完整轨迹落盘，防止噪声</td>
</tr>
<tr>
<td>External Memory Store (EMS)</td>
<td>持久化存储所有完成轨迹，支持亿级向量检索</td>
<td>FAISS-HNSW，9 ms 内 Top-5 召回</td>
</tr>
<tr>
<td>Coach</td>
<td>运行时判断“是否要提醒”并生成一句建议</td>
<td>8B LLM 零样本，高失败概率/更快路径才注入</td>
</tr>
</tbody>
</table>
</div>
<p><strong>注入方式</strong>：非侵入式 system message，Actor 权重不动。</p>
<p>3. 自我演化闭环</p>
<p>新轨迹 → Condenser → EMS → Coach 下次检索 → 更精准建议 → 更高成功率 → 更高质量轨迹…… 无需重训即形成正反馈。</p>
<p>4. 实验结果（WebVoyager 643 在线任务）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>基线 SR</th>
<th>+WebCoach SR</th>
<th>绝对提升</th>
<th>步数变化</th>
</tr>
</thead>
<tbody>
<tr>
<td>Skywork-38B</td>
<td>47.3 %</td>
<td>61.4 %</td>
<td>+14.1 ppt</td>
<td>10.7 → 10.2 ↓</td>
</tr>
<tr>
<td>Qwen-VL-32B</td>
<td>49.5 %</td>
<td>57.1 %</td>
<td>+7.6 ppt</td>
<td>13.3 → 11.9 ↓</td>
</tr>
<tr>
<td>Qwen-VL-7B</td>
<td>32.8 %</td>
<td>31.1 %</td>
<td>–1.7 ppt</td>
<td>认知阈值不足</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>自生成记忆 &gt; 外部 GPT-4o 记忆；复杂站点（Apple、ArXiv）提升最大。</li>
<li>检索延迟 9-10 ms，整体评估时间从 82 h 缩至 14 h（–83 %）。</li>
</ul>
<p>5. 贡献一句话</p>
<p>WebCoach 首次把“<strong>跨会话情节记忆 + 运行时检索注入</strong>”做成模型无关的 plug-and-play 层，让开源网页智能体在<strong>不重训、不改架构</strong>的前提下，达到接近 GPT-4o 的在线表现，并可随时间自我迭代。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是WebCoach%3A%20Self-Evolving%20Web%20Agents%20with%20Cross-Session%20Memory%20Guidance，链接是https://arxiv.org/pdf/2511.12997，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.12997。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Genglin Liu,Shijie Geng,Sha Li,Hejie Cui,Sarah Zhang,Xin Liu,Tianyi Liu</p>
<p><strong>Categories</strong>: cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.12997.pdf">https://arxiv.org/pdf/2511.12997.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.12997">https://arxiv.org/abs/2511.12997</a></p>
<p><strong>Arxiv ID</strong>: 2511.12997</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.12997">https://papers.cool/arxiv/2511.12997</a></p>
<p><strong>Published</strong>: 2025-11-17T05:38:50Z</p>
<p><strong>Updated</strong>: 2025-11-17T05:38:50.000Z</p>
<hr>
<h3 id="17-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models"><a href="#17-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models" class="headerlink" title="17. Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.09809">Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</a></h3><p>Vision-Language Models (VLMs) excel at zero-shot inference but often degrade under test-time domain shifts. For this reason, episodic test-time adaptation strategies have recently emerged as powerful techniques for adapting VLMs to a single unlabeled image. However, existing adaptation strategies, such as test-time prompt tuning, typically require backpropagating through large encoder weights or altering core model components. In this work, we introduce Spectrum-Aware Test-Time Steering (STS), a lightweight adaptation framework that extracts a spectral subspace from the textual embeddings to define principal semantic directions and learns to steer latent representations in a spectrum-aware manner by adapting a small number of per-sample shift parameters to minimize entropy across augmented views. STS operates entirely at inference in the latent space, without backpropagation through or modification of the frozen encoders. Building on standard evaluation protocols, our comprehensive experiments demonstrate that STS largely surpasses or compares favorably against state-of-the-art test-time adaptation methods, while introducing only a handful of additional parameters and achieving inference speeds up to 8x faster with a 12x smaller memory footprint than conventional test-time prompt tuning. The code is available at <a target="_blank" rel="noopener" href="https://github.com/kdafnis/STS">https://github.com/kdafnis/STS</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视觉-语言模型（VLMs）在零样本推理方面表现出色，但在测试时域转移下往往性能下降。基于此，近期出现了基于情景的测试时适应策略，作为将 VLMs 适配到单张无标签图像的强大技术。然而，现有的适应策略，如测试时提示调优，通常需要对大型编码器权重进行反向传播或修改核心模型组件。在本工作中，我们提出了光谱感知测试时引导（Spectrum-Aware Test-Time Steering, STS），这是一种轻量级适应框架，通过从文本嵌入中提取光谱子空间来定义主要语义方向，并通过调整少量每个样本的偏移参数，以跨增强视图最小化熵，从而以光谱感知的方式引导潜在表示。STS 完全在潜在空间进行推理，无需通过冻结的编码器进行反向传播或修改。基于标准评估协议，我们的全面实验表明，STS 在很大程度上超过或表现优于最先进的测试时适应方法，同时只引入少量额外参数，并且推理速度可达传统测试时提示调优的 8 倍，内存占用却小 12 倍。代码可在 <a target="_blank" rel="noopener" href="https://github.com/kdafnis/STS">https://github.com/kdafnis/STS</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对 Vision–Language Models（VLMs）在零样本推理阶段遭遇测试分布偏移（OOD）时性能显著下降的问题，提出一种无需反向传播、无需修改冻结编码器、也无需任何训练数据的测试时自适应（Test-Time Adaptation, TTA）方法——Spectrum-Aware Test-Time Steering（STS）。核心目标是在推理阶段仅利用单个无标签测试图像，通过轻量级、黑箱、参数高效的潜空间操控，即时提升 VLM 的泛化能力，同时保持极低延迟与内存占用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 STS 直接相关的研究可归纳为以下四条主线（按“问题—方法—代表文献”梳理）：</p>
<ol>
<li>Vision–Language 模型零样本泛化</li>
</ol>
<ul>
<li><p>对比学习预训练：CLIP<br>30<br>、ALIGN<br>19</p>
</li>
<li><p>下游任务适配（需标注）：CoOp<br>43<br>、CoCoOp<br>42<br>、MaPLe<br>21<br>、Tip-Adapter<br>40<br>、CLIP-Adapter<br>10</p>
</li>
</ul>
<ol>
<li>测试时提示调优（Test-Time Prompt Tuning, TPT）</li>
</ol>
<ul>
<li>核心思想：在推理阶段为每个测试样本优化可学习提示向量，以最小化增广视图的预测熵</li>
<li><p>代表方法：TPT<br>32<br>、DiffTPT（引入扩散增广）<br>9<br>、C-TPT（校准+分散度）<br>39</p>
</li>
<li><p>共同局限：需反向传播通过大型文本编码器，计算与内存开销高</p>
</li>
</ul>
<ol>
<li>参数高效或免反向传播的 TTA</li>
</ol>
<ul>
<li>PEFT 式：TTL<br>18<br>在注意力层引入 LoRA<br>17<br>，但仍需改动模型结构</li>
<li>免训练/记忆库式：Dual-Memory<br>41<br>、EATA<br>20<br>等，依赖在线记忆库，受分布漂移与内存限制</li>
<li>潜空间原型偏移：TPS<br>34<br>直接学习每类偏移向量，无编码器梯度，但偏移方向无约束，易过拟合</li>
</ul>
<ol>
<li>谱/子空间自适应</li>
</ol>
<ul>
<li>低内在维度观察：Aghajanyan et al.<br>2<br>指出预训练嵌入可用极低维子空间有效描述</li>
<li>奇异值阈值理论：Gavish &amp; Donoho<br>12<br>提供无噪声假设的最优秩选择准则</li>
<li>STS 首次将“SVD 语义子空间 + 线性 steering”引入 VLM 的测试时自适应，区别于以往无约束偏移或提示调优范式</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 Spectrum-Aware Test-Time Steering（STS），通过“<strong>谱子空间 + 线性 steering</strong>”实现轻量级测试时自适应。具体步骤如下：</p>
<ol>
<li><strong>预计算语义子空间</strong><br>对初始文本原型矩阵  Z_(∈it)^(T) ∈ R^(C × D)  做降秩 SVD：</li>
</ol>
<p>Z_(∈it)^(T) = U S V^top</p>
<p>按 Gavish-Donoho 最优阈值保留前  k<em>t  个右奇异向量，得到正交基  B</em>(T) ∈ R^(D × k_t) ，构成低维语义坐标系。</p>
<ol>
<li><strong>单样本系数学习</strong><br>对每个测试图像，仅优化  k_t ll D  个可学习系数  γ ∈ R^(k_t) ，生成共享偏移：</li>
</ol>
<p>Delta z^(T) = B_(T) γ</p>
<p>所有类别原型同步平移并归一化：</p>
<p>(z<em>(adapt)^(T))_c = normalize!((z</em>(∈it)^(T))_c + Delta z^(T))</p>
<ol>
<li><strong>无监督目标</strong><br>在增广视图上计算边际概率  P_(adapt) ，最小化 Shannon 熵：</li>
</ol>
<p>L<em>(STS) = H(P</em>(adapt)) + λ_R |Delta z^(T)|_2^2</p>
<p>优化只更新  γ ，<strong>冻结图像与文本编码器</strong>，无需反向传播进入大模型。</p>
<ol>
<li><strong>推理</strong><br>用优化后的  γ^*  得到最终原型  Z_(final)^(T) ，再与图像特征做相似度分类。</li>
</ol>
<p>通过“<strong>子空间约束 + 共享线性 steering</strong>”，STS 将高维分布偏移压缩到最具语义意义的  k_t  维方向，实现参数极少、速度 8× 提升、内存 12× 节省，同时取得 SOTA 或可比性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“自然分布偏移”与“跨数据集细粒度分类”两大场景，在 15 个公开基准上进行了系统实验，并辅以消融与效率分析。具体实验内容如下（按目的归类）：</p>
<ol>
<li><p>自然分布偏移鲁棒性<br>数据集：ImageNet-A / V2 / R / Sketch<br>指标：Top-1 准确率、平均 OOD 增益<br>对照：Zero-Shot CLIP、Ensemble、CoOp、TPT、DiffTPT、C-TPT、TPS<br>结果：STS 在 ViT-B/16 上平均 OOD 准确率 62.64%，超越 TPT 1.93 pp；STSEnsemble 达 64.96%，领先次佳方法 4.2 pp。<br>扩展：在更大骨干 ViT-L/14 上重复实验，STS 将 Zero-Shot 从 69.94% 提升到 74.08%，绝对增益 4.14 pp。</p>
</li>
<li><p>细粒度跨域泛化<br>数据集：Flowers102、DTD、OxfordPets、UCF101、Caltech101、Aircraft、EuroSAT、StanfordCars、Food101、SUN397<br>指标：平均 Top-1 准确率<br>结果：</p>
</li>
</ol>
<ul>
<li>单模板 STS 63.86%，已高于 Zero-Shot 63.58% 及其他 TTA 方法（C-TPT 63.58%、TPS 63.49%）。</li>
<li>7 模板 STSEnsemble 65.06%，刷新 ViT-B/16 backbone 下该十数据集平均记录。</li>
<li>在 Aircraft、StanfordCars、Food101 等单数据集上取得分组最佳或次佳。</li>
</ul>
<ol>
<li><p>初始化鲁棒性<br>以 MaPLe（16-shot 学习提示）作为更强文本原型初始化，再次运行 TPT 与 STS。<br>结果：MaPLe+STS 在 ImageNet-A 等自然偏移数据集上平均领先 MaPLe+TPT 3.03 pp；在细粒度任务上互有胜负，差距 ≤0.5 pp，表明谱 steering 对优质初始化依旧有效。</p>
</li>
<li><p>效率与资源对比<br>单张 RTX8000 上测试 ImageNet 1k 张样本：</p>
</li>
</ol>
<ul>
<li>推理延迟：STS 0.09 s vs TPT 0.75 s（8× 加速）</li>
<li>峰值内存：STS 1.4 GB vs TPT 17.6 GB（12× 节省）</li>
<li>可学习参数量：STS 仅 kt≈40–60，而 TPT 2048 维提示向量仍需反向传播整个文本编码器。</li>
</ul>
<ol>
<li>消融与超参数分析</li>
</ol>
<ul>
<li>更新步数：1–5 步对 ImageNet-A 准确率影响 &lt;0.05%，默认单步最优。</li>
<li>共享 vs 每类系数：共享 γ 在 15 个数据集上平均差值 ≤0.03%，验证“全局分布偏移”假设。</li>
<li>增广视图数量：N=64 时性能饱和，128 视图仅 +0.15%，耗时翻倍，故采用 N=64。</li>
</ul>
<ol>
<li><p>腐败鲁棒性验证<br>在 CIFAR-10-C（severity=5）上对比：STS 与 TPT 差距 0.05%，显著优于 TPS，表明谱子空间约束对强扰动依旧稳定；结合 7 模板后 STS 达 67.24%。</p>
</li>
<li><p>奇异向量选择策略<br>对比“98% 能量”与 Gavish-Donoho 阈值两种 rank-kt 选取方式：后者在 ImageNet-A 上再提升 0.14 pp，证实理论最优阈值略胜经验能量准则。</p>
</li>
<li><p>误差条与可重复性<br>3 随机种子运行，标准差均 ≤0.3 pp，结果稳定。</p>
</li>
</ol>
<p>综上，实验覆盖不同模型规模、初始化强度、增广策略、鲁棒性场景与资源约束，全面验证了 STS 在“精度-效率-通用性”三角中的优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可继续推进，按“理论—方法—系统—应用”四个层面列出：</p>
<ol>
<li>非线性子空间扩展</li>
</ol>
<ul>
<li>现行 steering 仅在 SVD 线性流形内平移；对强非线性域偏移可引入核 SVD、流形神经网络或微分同胚变换，保持轻量级优化。</li>
<li>探索曲率约束的测地线偏移，使原型沿语义流形最短路径移动。</li>
</ul>
<ol>
<li>视觉端联合谱 steering</li>
</ol>
<ul>
<li>论文仅对文本原型做子空间偏移。可对图像特征  Z<em>(V)  同样做 SVD 得到  B</em>(V) ，学习共享系数  γ_(V) ，实现双端同步 steering，潜在提升视觉-文本对齐度。</li>
<li>需解决双端耦合优化时的收敛性与速度问题。</li>
</ul>
<ol>
<li>自适应 rank 与在线更新</li>
</ol>
<ul>
<li>目前  k_t  在数据集级别一次性确定。可依据测试样本不确定性或梯度幅值，在线增减子空间维度，实现“样本级动态秩”。</li>
<li>引入贝叶斯矩阵分解，维护子空间后验，减少阈值超参。</li>
</ul>
<ol>
<li>免增广或潜空间增广</li>
</ol>
<ul>
<li>现方案依赖 64 次图像前向传播。可研究：<br>– 在已提取的  z_v  邻域内做线性插值或高斯扰动，直接生成虚拟视觉特征，避免重复推理。<br>– 利用扩散或 VAE 在潜空间合成语义一致的新视图，进一步压缩延迟。</li>
</ul>
<ol>
<li>任务扩展</li>
</ol>
<ul>
<li>目标检测/分割：将“文本原型”换成“区域-短语”嵌入，对每类区域嵌入做谱 steering，提升 OOD 检测框/掩码质量。</li>
<li>视频推理：沿时间维度累积帧特征，构建时空子空间，实现视频级测试时自适应。</li>
</ul>
<ol>
<li>持续与增量 TTA</li>
</ol>
<ul>
<li>当前 episodic 每样本后丢弃  γ 。可引入轻量记忆库（如子空间系数指数平均），在数据流非平稳或类别逐渐出现时实现“无遗忘”持续适应，同时控制内存常数级增长。</li>
</ul>
<ol>
<li>鲁棒性与安全性分析</li>
</ol>
<ul>
<li>研究对抗或后门样本是否可利用低维 steering 方向，提出相应的谱域防御正则项。</li>
<li>量化子空间偏移对公平性（性别、种族）指标的影响，加入公平性约束的 steering 目标。</li>
</ul>
<ol>
<li>系统级优化</li>
</ol>
<ul>
<li>将 SVD 基  B_(T)  与优化器状态编译进 GPU constant memory，实现单样本 &lt;5 ms 级推理。</li>
<li>与 ONNX/TensorRT 集成，把  γ  更新过程写成自定义 CUDA kernel，支持边缘端部署。</li>
</ul>
<ol>
<li>跨模态谱 steering</li>
</ol>
<ul>
<li>在音频-文本、视频-音频等多模态 CLIP-like 模型上验证子空间假设，考察不同模态的奇异值衰减规律是否一致，并设计模态间联合谱坐标系。</li>
</ul>
<ol>
<li>理论深化</li>
</ol>
<ul>
<li>建立“子空间 steering 泛化误差界”，连接  k_t 、覆盖数与域间距（如  HDelta H -divergence），给出选择 kt 的理论最优公式。</li>
<li>探讨与信息瓶颈、最小描述长度（MDL）的关系，解释为何低维线性偏移即可降低预测熵。</li>
</ul>
<p>这些方向可单独或组合展开，有望把 STS 从“分类任务-线性-单样本”推向“通用任务-非线性-持续”的新阶段。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Spectrum-Aware Test-Time Steering (STS)</strong> 提出一种<strong>零样本、无标签、不修改冻结编码器</strong>的轻量级测试时自适应方法，核心思想可浓缩为一句话：</p>
<blockquote>
<p><strong>“用文本嵌入的 SVD 主成分当坐标系，为每个测试图像学一个低维系数向量，在该坐标系里一次性平移所有类别原型，使视觉-文本对齐熵最小。”</strong></p>
</blockquote>
<p>主要贡献与结果速览：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>关键内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>VLM 在 OOD 测试样本上零-shot 性能骤降；现有 TPT 需反向传播大编码器，慢、耗内存、难部署。</td>
</tr>
<tr>
<td>解法</td>
<td>1) 对初始文本原型做 SVD → 取前  k<em>t  个右奇异向量  B</em>(T)  当语义基；2) 每样本仅学  k<em>t  维系数  γ  生成共享偏移  Delta z^(T)=B</em>(T)γ ；3) 最小化增广视图边际熵 + L2 正则，单步 AdamW 完成；4) 冻结编码器、零训练数据、零 prompt 模板调优。</td>
</tr>
<tr>
<td>效率</td>
<td>可学习参数量 ≈ 40–60；推理速度 8× 于 TPT；内存占用 1/12；单 RTX8000 实时处理。</td>
</tr>
<tr>
<td>性能</td>
<td>ViT-B/16 上平均 OOD 准确率 62.64%（+4.2 pp 超 TPT）；STSEnsemble 达 64.96% 刷新记录；ViT-L/14 再提升 4.14 pp。细粒度十数据集平均 65.06% 领先。</td>
</tr>
<tr>
<td>消融</td>
<td>单步最优；共享系数已足够；Gavish-Donoho 阈值选秩略胜 98% 能量；腐败场景与持续 TTA 方向明确。</td>
</tr>
</tbody>
</table>
</div>
<p>综上，STS 以**“谱子空间 + 线性 steering”<strong>实现</strong>参数极少、速度极快、精度更高**的黑箱测试时自适应，为 VLM 在真实动态环境下的零样本部署提供了实用解决方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Test-Time%20Spectrum-Aware%20Latent%20Steering%20for%20Zero-Shot%20Generalization%20in%20Vision-Language%20Models，链接是https://arxiv.org/pdf/2511.09809，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.09809。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Konstantinos M. Dafnis,Dimitris N. Metaxas</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.09809.pdf">https://arxiv.org/pdf/2511.09809.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.09809">https://arxiv.org/abs/2511.09809</a></p>
<p><strong>Arxiv ID</strong>: 2511.09809</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.09809">https://papers.cool/arxiv/2511.09809</a></p>
<p><strong>Published</strong>: 2025-11-12T23:25:58Z</p>
<p><strong>Updated</strong>: 2025-11-12T23:25:58.000Z</p>
<hr>
<h3 id="18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity"><a href="#18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity" class="headerlink" title="18. UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity"></a>18. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.13714">UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</a></h3><p>The Segment Anything Model (SAM) family has become a widely adopted vision foundation model, but its ability to control segmentation granularity remains limited. Users often need to refine results manually - by adding more prompts or selecting from pre-generated masks - to achieve the desired level of detail. This process can be ambiguous, as the same prompt may correspond to several plausible masks, and collecting dense annotations across all granularities is prohibitively expensive, making supervised solutions infeasible. To address this limitation, we introduce UnSAMv2, which enables segment anything at any granularity without human annotations. UnSAMv2 extends the divide-and-conquer strategy of UnSAM by discovering abundant mask-granularity pairs and introducing a novel granularity control embedding that enables precise, continuous control over segmentation scale. Remarkably, with only $6$K unlabeled images and $0.02\%$ additional parameters, UnSAMv2 substantially enhances SAM-2, achieving segment anything at any granularity across interactive, whole-image, and video segmentation tasks. Evaluated on over $11$ benchmarks, UnSAMv2 improves $\text{NoC}<em>{90}$ (5.69 $\rightarrow$ 4.75), 1-IoU (58.0 $\rightarrow$ 73.1), and $\text{AR}</em>{1000}$ (49.6 $\rightarrow$ 68.3), showing that small amounts of unlabeled data with a granularity-aware self-supervised learning method can unlock the potential of vision foundation models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>分段任意模型（SAM）系列已成为广泛采用的愿景基础模型，但其控制细分细节的能力仍然有限。用户通常需要手动细化结果——通过添加更多提示或从预生成的遮罩中选择——以达到理想的细节水平。这一过程可能存在歧义，因为同一提示可能对应多个合理的掩码，且收集所有粒度的密集注释成本高昂，使得监督式解法不可行。为解决这一限制，我们引入了UnSAMv2，它允许在任意粒度下进行任意片段，无需人工注释。UnSAMv2通过发现丰富的掩膜粒度对并引入了一种新颖的粒度控制嵌入，扩展了UnSAM的分化与治理策略，实现了对分割尺度的精确、连续控制。令人惊讶的是，UnSAMv2仅有600美元无标记图像和0.02美元额外参数，显著增强了SAM-2，实现了交互式、全图和视频分割任务中任意粒度的分割。在超过11美元基准测试中，UnSAMv2提升了$\text{NoC}<em>{90}$（5.69 $\rightarrow$ 4.75）、1-IOU（58.0 $\rightarrow$ 73.1）和$\text{AR}</em>{1000}$（49.6 $\rightarrow$ 68.3），表明通过细度感知的自监督学习方法，少量未标记数据可以释放视觉基础模型的潜力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决 SAM 家族在“分割粒度”上不可连续、不可控的缺陷。<br>具体而言，现有方法存在以下关键痛点：</p>
<ul>
<li>离散输出：SAM/SAM-2 对每个提示仅给出 3 个固定尺度的掩膜，用户必须手动挑选或追加提示，无法平滑过渡。</li>
<li>标注依赖：监督式训练将“物体”概念绑定于人工标注，难以覆盖真实场景中嵌套的“部分–整体”层级。</li>
<li>粒度歧义：同一点击可能对应多个合理掩膜（零件 vs 整体），缺乏显式变量来连续表达“要多细”这一主观需求。</li>
</ul>
<p>为此，作者提出 UNSAMV2，核心目标为：</p>
<ol>
<li>用<strong>单点+连续粒度标量</strong>取代离散候选，实现“任意粒度分割”。</li>
<li>完全<strong>自监督</strong>地从 6 000 张无标签图像中挖掘“掩膜–粒度”伪标签，无需额外人工标注。</li>
<li>在交互式、整图、视频三大任务上统一提升性能，将分割从“固定预测”转化为“连续可控推理”。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两大主线，并在第 2 节系统回顾。以下按主题梳理代表性工作，括号内给出原文引用编号。</p>
<p>1. 多粒度 / 可控粒度分割</p>
<ul>
<li><strong>SAM 家族</strong></li>
<li>Segment Anything (SAM)<br>24<br>、SAM-2<br>35<br>：提出可提示分割范式，但仅输出 3 个离散掩膜，粒度不可控。</li>
<li><strong>离散粒度扩展</strong></li>
<li>Semantic-SAM<br>25<br>：用多选学习输出更多固定候选，仍未摆脱离散选择。</li>
<li>GraCo<br>58<br>：在 SimpleClick<br>29<br>上引入 3 档离散粒度输入，实现“粗/中/细”切换。</li>
<li><strong>3D 场景绝对尺度调节</strong></li>
<li>GARField<br>23<br>、SAMPart3D<br>54, 55<br>：用绝对深度或尺寸作为粒度条件，难以泛化到 2D 图像的相对层级。</li>
</ul>
<p>2. 自监督学习与无监督分割</p>
<ul>
<li><strong>自监督表征</strong></li>
<li>MAE<br>16<br>、DINO/DINOv2/DINOv3<br>5, 32, 40<br>、JEPA<br>2<br>：为 ViT 提供语义化特征，后续被用作掩膜相似度度量。</li>
<li><strong>无监督实例分割</strong></li>
<li>CutLER<br>44<br>/ MaskCut：基于归一化割迭代提取物体，为本文“divide”阶段提供初始掩膜。</li>
<li>VideoCutLER<br>46<br>、CutS3D<br>38<br>：将 MaskCut 扩展到视频或 3D 点云。</li>
<li>SOHES<br>4<br>：自底向上合并相似像素，生成层级实体。</li>
<li>UnSAM<br>47<br>：首次提出“divide-and-conquer”范式，构建层级伪标签，但未引入连续粒度变量。</li>
</ul>
<p>3. 与本文方法的区别</p>
<ul>
<li><strong>离散 vs 连续</strong>：GraCo、Semantic-SAM 等将粒度离散化；UNSAMV2 用连续标量 $g∈<br>0.1,1<br>$ 实现平滑过渡。</li>
<li><strong>绝对 vs 相对</strong>：GARField、SAMPart3D 用绝对深度/尺寸；UNSAMV2 在实例–部件层级内部计算相对面积比，更符合人类感知。</li>
<li><strong>监督 vs 自监督</strong>：前述方法依赖人工标注或 3D 先验；UNSAMV2 仅借 6 k 无标签图像，以自监督方式挖掘“掩膜–粒度”对。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“连续粒度控制”形式化为一个<strong>自监督学习</strong>问题，核心思路是：<br><strong>先自挖掘“掩膜–粒度”伪标签，再让 SAM-2 学会按单点+连续标量输出对应掩膜。</strong><br>具体实现分为四大步骤，对应原文 §3.3–§3.4 的 pipeline 与架构。</p>
<p>1. 粒度感知的 Divide-and-Conquer 伪标签生成（§3.3）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>目的</th>
<th>关键操作</th>
</tr>
</thead>
<tbody>
<tr>
<td>Divide</td>
<td>发现实例级候选</td>
<td>MaskCut [44] 生成初始掩膜  M ，置信度过滤  τ_(conf)=0.3</td>
</tr>
<tr>
<td>Instance–Part 关联</td>
<td>建立“整体–部件”关系</td>
<td>面积占优 + IoU&gt;0.8 规则，得到实例集  M<em>(inst)  与其部件集  M</em>(i,part)</td>
</tr>
<tr>
<td>Conquer</td>
<td>补充更细粒度</td>
<td>在  m<em>i∈M</em>(inst)  内部用 DINOv3 特征余弦相似度迭代合并，阈值  θ=[0.9,0.8,…,0.5] ，生成  M_(i,conquer)</td>
</tr>
<tr>
<td>连续粒度赋值</td>
<td>给每一掩膜分配标量  g</td>
<td>相对面积公式：  g<em>i=(√{A_i-√A</em>(min)}{√A<em>(max)-√A</em>(min)})·0.9+0.1  保证  g∈[0.1,1] ，越细越小</td>
</tr>
</tbody>
</table>
</div>
<p>最终 6 000 张无标签图像产出约 112 伪标签/图，形成稠密“掩膜–粒度”对。</p>
<p>2. 粒度编码与架构改造（§3.4）</p>
<ul>
<li><strong>粒度编码器</strong><br>标量  g  → 128 维 Fourier 特征  φ(g)  → 3 层 MLP → 解码器维度  E_g 。</li>
<li><strong>提示融合</strong><br>点提示嵌入  E<em>p  与  E_g  拼接： E</em>(prompt)=‖(E_p,E_g) ，实现“点+粒度”联合条件。</li>
<li><strong>粒度感知掩膜 token</strong><br>替换 SAM-2 原有 3 个固定 token，引入<strong>单个可学习 token</strong>，在双向 Transformer 中同时与图像特征、提示特征做自/交叉注意力，输出对应粒度掩膜。</li>
<li><strong>参数效率</strong><br>仅训练粒度编码器 + 新 token + 解码器 LoRA（ rank=8 ），<strong>新增参数量 &lt; 0.02 %</strong>，冻结图像编码器。</li>
</ul>
<p>3. 训练目标与策略</p>
<ul>
<li>损失：沿用 SAM-2 的 focal + dice，比例 20:1。</li>
<li>数据：仅 6 k 无标签 SA-1B 图像，8 A100-GPU·小时完成 5 epoch。</li>
<li>正则：LoRA 与冻结编码器保证预训练语义不被破坏。</li>
</ul>
<p>4. 轻量监督变体 UNSAMV2+（§3.6）</p>
<p>为降低伪标签噪声，在 Divide 阶段额外混入 SA-1B 人工掩膜：<br> M<em>(UNSAMV2+)=M</em>(CutLER) ∪ M_(SA-1B) ，后续流程相同。<br>实验表明，<strong>少量人工标注+自挖掘层级</strong> 可进一步提升性能，但仍保持极低标注依赖（仅用到 0.02 % 原始 SA-1B 掩膜）。</p>
<p>通过上述 pipeline，UNSAMV2 把“粒度”从离散候选转化为<strong>连续函数</strong></p>
<p>Mask = f_(θ)(Image, Point, g), quad g∈[0.1,1]</p>
<p>实现“单点+滑动条”即可在任何图像、任何层级上输出对应掩膜，解决了 SAM 家族粒度不可控、需人工挑选的核心痛点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>交互分割、整图分割、视频分割</strong> 三大任务上共覆盖 <strong>11 个基准数据集</strong>，并辅以 <strong>5 组消融实验</strong>，全面验证 UNSAMV2 的粒度可控性与数据效率。主要实验汇总如下（对应原文 §4 与 §5）。</p>
<p>1. 交互式图像分割（Interactive Segmentation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>粒度侧重</th>
<th>指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>GrabCut [36]、Berkeley [30]、SBD [15]</td>
<td>实例级</td>
<td>NoC80/90、1-IoU</td>
</tr>
<tr>
<td>DAVIS [33]</td>
<td>视频帧实例</td>
<td>NoC80/90、1-IoU</td>
</tr>
<tr>
<td>PascalPart [7]、PartImageNet [8]</td>
<td>零件级</td>
<td>NoC80/85、1-IoU</td>
</tr>
<tr>
<td>SA-1B [24]（1 k 图，非训练集）</td>
<td>开放粒度</td>
<td>NoC80/90、1-IoU</td>
</tr>
</tbody>
</table>
</div>
<p><strong>结果</strong>（表 1–2）</p>
<ul>
<li><strong>UNSAMV2</strong>（仅 6 k 无标签图）<br>– 平均 NoC90 ↓ 0.94（5.69→4.75），1-IoU ↑ 15.1 pp（58.0→73.1）。</li>
<li><strong>UNSAMV2+</strong>（混入 0.02 % SA-1B）<br>– 再降 NoC90 至 3.10，较此前 SOTA（GraCo）↓ 0.32，1-IoU ↑ 7.3 pp。</li>
</ul>
<p>2. 整图全实例召回（Whole-Image Segmentation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>类别/场景</th>
<th>指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>COCO [27]、LVIS [11]、ADE20K [59]</td>
<td>常见/罕见类</td>
<td>AR1000</td>
</tr>
<tr>
<td>EntitySeg [34]</td>
<td>开放词汇</td>
<td>AR1000</td>
</tr>
<tr>
<td>SA-1B [24]（1 k 图）</td>
<td>开放域</td>
<td>AR1000</td>
</tr>
</tbody>
</table>
</div>
<p><strong>结果</strong>（表 3）</p>
<ul>
<li><strong>UNSAMV2</strong> AR1000 = 68.3，较 SAM ↑ 18.7 pp，较 UnSAM ↑ 29.1 pp。</li>
<li><strong>UNSAMV2+</strong> 达 74.1，刷新 SOTA（+21.5 pp）。</li>
</ul>
<p>3. 视频分割（Video Segmentation）</p>
<ul>
<li><strong>协议</strong>：第 1 帧给单点+粒度，后续帧无额外提示，用 SAM-2 记忆模块传播。</li>
<li><strong>数据</strong>：YouTube-VIS 验证集（图 9 与图 A3 定性）。</li>
<li><strong>发现</strong>：尽管训练时冻结记忆模块且仅用图像数据，掩膜仍保持跨帧一致性，验证粒度嵌入可时序迁移。</li>
</ul>
<p>4. 消融实验（§5）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验</th>
<th>变量</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据量（表 4）</td>
<td>1 k / 3 k / 6 k 图</td>
<td>1 k 图已能取得 90 % 以上满数据性能，样本效率极高。</td>
</tr>
<tr>
<td>粒度 token（图 10）</td>
<td>复用 SAM-2 原 token vs 新增粒度 token</td>
<td>原 token 因强先验难以微调，新增粒度 token 带来 +15 pp 1-IoU。</td>
</tr>
<tr>
<td>监督来源（表 5）</td>
<td>纯 SA-1B 监督 vs 纯伪标签 vs 混合</td>
<td>纯监督仅 42.5 1-IoU，伪标签 51.5，混合 52.0，验证自挖掘层级不可替代。</td>
</tr>
<tr>
<td>LoRA 秩（表 6a）</td>
<td>rank 4→32</td>
<td>rank=8 平衡效率与性能。</td>
</tr>
<tr>
<td>训练超参（表 6b–d）</td>
<td>每图掩膜数、Fourier 维度、d 等</td>
<td>每图 30 掩膜、Fourier-128 维、3 点修正点击为默认最优。</td>
</tr>
</tbody>
</table>
</div>
<p>5. 可视化与定性分析</p>
<ul>
<li><strong>图 7 / A1</strong>：同一点不同粒度输出，展示从零件到整体的连续过渡，优于 GraCo 的断裂/过并现象。</li>
<li><strong>图 8 / A2</strong>：整图模式下，低粒度仅暴露螺丝、纹理，高粒度给出整车、整椅，验证“相对粒度”跨场景一致性。</li>
<li><strong>图 9 / A3</strong>：视频序列中粒度 0.4→0.8 平滑扩张，目标边界时序稳定，无额外提示。</li>
</ul>
<p>综上，实验从“点级交互–整图召回–视频传播”多维度证明：<br><strong>仅用 6 k 无标签图像 + 0.02 % 额外参数</strong>，UNSAMV2 即可在 11 个公开数据集上刷新交互分割与整图召回的 SOTA，且粒度控制可平滑泛化到视频时序。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“方法-理论”“数据-场景”“系统-应用”三个层次，供后续研究参考。</p>
<p>方法-理论层面</p>
<ol>
<li><strong>连续粒度的数学表征</strong><br>目前用相对面积开方映射到<br>0.1,1<br>，仅捕捉“大小”单一维度。可探索：</li>
</ol>
<ul>
<li>引入拓扑持久性（persistent homology）或超像素合并树，将“深度/层数”纳入粒度定义。</li>
<li>用扩散模型或神经 ODE 把粒度视为连续时间变量，直接建模  dg/dt  的梯度场，实现反向粒度插值。</li>
</ul>
<ol>
<li><strong>提示-粒度联合分布学习</strong><br>现有工作将点坐标与粒度标量简单拼接。可研究：</li>
</ol>
<ul>
<li>用条件扩散或能量模型学习  p(mask|point,g) ，显式建模多模态不确定性。</li>
<li>引入语言提示（“左前轮”）→ 粒度自动推断，实现文本-粒度对齐。</li>
</ul>
<ol>
<li><strong>层级结构的可解释分析</strong></li>
</ol>
<ul>
<li>探测粒度感知 token 的注意力模式，验证其是否自动对应 part-whole 语法树。</li>
<li>将粒度轴与视觉 Transformer 的多尺度特征谱（frequency spectrum）做相关分析，解释“高频=细粒度”假设是否成立。</li>
</ul>
<p>数据-场景层面</p>
<ol>
<li><strong>跨模态粒度迁移</strong></li>
</ol>
<ul>
<li>3D 点云/NeRF：把 2D 粒度编码直接注入 SAM-3D，验证是否 zero-shot 完成“零件→组件→物体”分解。</li>
<li>音频-视觉：视频中的声源定位 + 粒度控制，实现“细粒度只分割发声的钢琴键”。</li>
</ul>
<ol>
<li><strong>动态与层级时序一致性</strong></li>
</ol>
<ul>
<li>在视频里粒度随时间变化（汽车→车门→门把手），构建 Granularity-VOS 基准，评估时序 ID 保持。</li>
<li>引入事件相机，探索高帧率场景下粒度切换的延迟与一致性。</li>
</ul>
<ol>
<li><strong>自监督伪标签质量提升</strong></li>
</ol>
<ul>
<li>用多视图几何（SfM）过滤 MaskCut 伪标签，减少遮挡误差。</li>
<li>采用“教师-学生”互蒸馏循环：学生模型生成的可靠掩膜反哺伪标签池，迭代精炼粒度层级。</li>
</ul>
<p>系统-应用层面</p>
<ol>
<li><strong>边缘部署与实时交互</strong></li>
</ol>
<ul>
<li>粒度编码模块仅 0.02 % 参数，可整编到移动端 SAM-2 Tiny；结合滑动条 UI 实现 30 FPS 交互式“粒度拖拉”分割。</li>
<li>将粒度轴绑定到 AR/VR 手柄的物理旋钮，实现沉浸式“逐层解剖”体验。</li>
</ul>
<ol>
<li><strong>下游任务插件化</strong></li>
</ol>
<ul>
<li>图像编辑：低粒度掩膜自动送入 Inpainting 模型，实现“只替换轮胎纹理”的细粒度编辑。</li>
<li>机械臂抓取：把  g=0.2  对应零件掩膜直接输入 6-DoF 姿态估计网络，完成“只抓取螺丝”的精准操作。</li>
</ul>
<ol>
<li><strong>开放世界粒度发现</strong></li>
</ol>
<ul>
<li>在无人标注的野外视频（iNaturalist、PlanetLab）上运行 UNSAMV2，统计不同生物/地貌的“自然粒度”分布，构建领域自适应的粒度先验。</li>
<li>引入主动学习：模型对粒度  g  不确定的区域自动请求人工点击，最小标注量下快速适配新领域。</li>
</ul>
<p>小结</p>
<p>连续粒度轴的提出将“分割”从静态预测变为可控过程，理论上仍缺乏统一的度量与学习框架；数据上尚缺跨模态、时序、事件级粒度基准；应用上则具备轻量化、插件化、交互式三大落地潜力。上述方向可推动“任意粒度分割”从算法原型走向通用视觉接口。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>UNSAMV2 论文核心内容一览</strong></p>
<p>1. 问题</p>
<ul>
<li>SAM/SAM-2 只能输出 3 个离散掩膜，用户无法连续控制“分割有多细”。</li>
<li>监督训练把“物体”概念锁死在人工标注，难以覆盖真实场景的嵌套“部分–整体”层级。</li>
</ul>
<p>2. 思路</p>
<p><strong>把“粒度”变成可学习的连续变量</strong><br>单点提示 + 标量 g∈<br>0.1,1<br> → 直接输出对应细-粗掩膜，无需人工挑选。</p>
<p>3. 方法</p>
<p><strong>两阶段：自挖掘伪标签 → 轻量微调 SAM-2</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>关键步骤</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>Divide</td>
<td>MaskCut 生成实例掩膜</td>
<td>初始候选 M</td>
</tr>
<tr>
<td>Conquer</td>
<td>实例内部用 DINO 特征逐级合并</td>
<td>部件掩膜</td>
</tr>
<tr>
<td>粒度赋值</td>
<td>相对面积公式 → 连续 g</td>
<td>掩膜-粒度对</td>
</tr>
<tr>
<td>架构改造</td>
<td>① Fourier 编码 g② 新增粒度感知 token③ LoRA 微调解码器</td>
<td>0.02% 参数</td>
</tr>
</tbody>
</table>
</div>
<p>4. 结果</p>
<ul>
<li><strong>数据</strong>：仅 6 000 张无标签 SA-1B 图像，8 GPU·小时。</li>
<li><strong>交互分割</strong>：NoC90 ↓0.94，1-IoU ↑15 pp，优于 SAM-2 与 GraCo。</li>
<li><strong>整图召回</strong>：AR1000 ↑18.7 pp，刷新 COCO/LVIS/ADE 等 5 基准。</li>
<li><strong>视频</strong>：零样本跨帧一致，无需额外训练。</li>
<li><strong>消融</strong>：1 k 图即可收敛；伪标签比纯人工标注关键。</li>
</ul>
<p>5. 贡献一句话</p>
<p><strong>首次用自监督方式让 SAM 家族支持“单点+滑动条”式任意粒度分割，小数据、零标注、全任务 SOTA。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UnSAMv2%3A%20Self-Supervised%20Learning%20Enables%20Segment%20Anything%20at%20Any%20Granularity，链接是https://arxiv.org/pdf/2511.13714，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.13714。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Junwei Yu,Trevor Darrell,XuDong Wang</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.13714.pdf">https://arxiv.org/pdf/2511.13714.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.13714">https://arxiv.org/abs/2511.13714</a></p>
<p><strong>Arxiv ID</strong>: 2511.13714</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.13714">https://papers.cool/arxiv/2511.13714</a></p>
<p><strong>Published</strong>: 2025-11-17T18:58:34Z</p>
<p><strong>Updated</strong>: 2025-11-17T18:58:34.000Z</p>
<hr>
<h3 id="19-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model"><a href="#19-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model" class="headerlink" title="19. MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model"></a>19. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.11407">MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</a></h3><p>Multimodal Large Language Models are increasingly applied to biomedical imaging, yet scientific reasoning for microscopy remains limited by the scarcity of large-scale, high-quality training data. We introduce MicroVQA++, a three-stage, large-scale and high-quality microscopy VQA corpus derived from the BIOMEDICA archive. Stage one bootstraps supervision from expert-validated figure-caption pairs sourced from peer-reviewed articles. Stage two applies HiCQA-Graph, a novel heterogeneous graph over images, captions, and QAs that fuses NLI-based textual entailment, CLIP-based vision-language alignment, and agent signals to identify and filter inconsistent samples. Stage three uses a MultiModal Large Language Model (MLLM) agent to generate multiple-choice questions (MCQ) followed by human screening. The resulting release comprises a large training split and a human-checked test split whose Bloom’s level hard-sample distribution exceeds the MicroVQA benchmark. Our work delivers (i) a quality-controlled dataset that couples expert literature with graph-based filtering and human refinement; (ii) HiCQA-Graph, the first graph that jointly models (image, caption, QA) for cross-modal consistency filtering; (iii) evidence that careful data construction enables 4B-scale MLLMs to reach competitive microscopy reasoning performance (e.g., GPT-5) and achieve state-of-the-art performance among open-source MLLMs. Code and dataset will be released after the review process concludes.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态大型语言模型正越来越多地应用于生物医学成像，但显微镜科学推理仍然受到大规模高质量训练数据稀缺的限制。我们推出了 MicroVQA++，这是一个来自 BIOMEDICA 资料库的三阶段、大规模、高质量的显微镜 VQA 语料库。第一阶段从经过专家验证的、来源于同行评审文章的图表-标题对中引导监督。第二阶段应用 HiCQA-Graph，一种在图像、标题和问答上构建的新型异构图，它融合了基于 NLI 的文本蕴涵、基于 CLIP 的视觉-语言对齐以及代理信号，以识别并筛除不一致样本。第三阶段使用多模态大型语言模型（MLLM）代理生成多项选择题（MCQ），随后进行人工筛检。最终发布的内容包括一个大型训练集和一个人工检查的测试集，其 Bloom 分类高难样本分布超过 MicroVQA 基准。我们的工作提供了：(i) 一个质量受控的数据集，结合了专家文献、基于图的过滤和人工修正；(ii) HiCQA-Graph，这是第一个联合建模（图像、标题、问答）以进行跨模态一致性过滤的图；(iii) 证据表明，细致的数据构建使得 40 亿规模 MLLM 在显微镜推理性能上达到竞争力水平（例如 GPT-5），并在开源 MLLM 中实现了最先进的性能。代码和数据集将在审稿过程结束后发布。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对显微图像科学推理场景下“高质量多模态训练数据稀缺”这一核心瓶颈，提出并验证了一套可扩展的弱监督数据构建框架，目标可归纳为：</p>
<ul>
<li><p><strong>问题定义</strong><br>现有显微视觉问答基准（MicroVQA）仅含 1 042 条样本，无法支撑对多模态大模型（MLLM）的参数微调，导致显微领域的深度科学推理能力长期停滞。</p>
</li>
<li><p><strong>关键挑战</strong></p>
</li>
</ul>
<ol>
<li>大规模图文对易获取（如 BIOMEDICA 的 2.5 M 显微图注），但直接用于监督会引入幻觉、矛盾或捷径答案。</li>
<li>显微任务要求同时具备溯因（abductive）与演绎（deductive）推理，问题难度高，自动生成的 QA 质量难以保证。</li>
<li>需要一种机制，在弱监督条件下联合评估图像-文本-答案三元组的一致性，从而过滤低质样本。</li>
</ol>
<ul>
<li><strong>解决思路</strong><br>构建三阶段 pipeline：</li>
</ul>
<ol>
<li>利用已出版论文的图注对作为“专家弱标签”，由 MLLM-agent 抽取答案并生成开放式 QA。</li>
<li>提出异构图滤波器 HiCQA-Graph，融合 CLIP 视觉-文本对齐、NLI 文本蕴含与 agent 置信信号，通过图神经网络传播一致性，剪除幻觉样本。</li>
<li>在过滤后的高质量子集上，再次调用 MLLM-agent 生成带链式思维（CoT）的多项选择题（MCQ），并经人工抽检形成最终训练/测试拆分。</li>
</ol>
<ul>
<li><strong>预期效果</strong><br>在仅 4 B 参数的开源模型上完成监督微调后，即可在 MicroVQA 基准上达到与 GPT-5 相当的准确率，并在更高难度的 MicroVQA++ 测试集上刷新开源模型 SOTA，验证“数据工程优先”即可释放小模型在垂直领域的推理潜能。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related work”中将与 MicroVQA++ 直接相关的研究划分为三条主线，并给出对应文献。可归纳如下：</p>
<ol>
<li>显微领域的 MLLM 基准</li>
</ol>
<ul>
<li>MicroVQA（Burgess et al., CVPR 2025）<br>首个面向显微图像的 VQA 基准，仅 1 042 问答对，用于评估而非训练。</li>
<li>µBench（Lozano et al., NeurIPS 2024）<br>覆盖 22 项任务、17 k 图像，但科学深度较浅，未提供大规模训练集。</li>
</ul>
<ol>
<li>QA / MCQ 自动构建与 distractor 生成</li>
</ol>
<ul>
<li>传统方法：依赖人工模板或启发式规则（Gierl et al. 2017）。</li>
<li>大模型时代：<br>– µBench 采用 GPT-4o 生成选项后人工校验；<br>– MicroVQA 采用“专家-代理”两轮策略；<br>– 本文提出基于异构图一致性过滤的 agent 流水线，减少人工 heuristic 依赖。</li>
</ul>
<ol>
<li>基于图结构的弱监督数据去噪</li>
</ol>
<ul>
<li>经典标签传播：Zhu &amp; Ghahramani 2002。</li>
<li>视觉任务：DualGraph（Zhang et al., CVPR 2021）、NGC（Wu et al., ICCV 2021）利用样本-标签双图抑制噪声。</li>
<li>本文创新：首次将“图像-图注-问答”三元组联合建模为异构图，引入 CLIP 对齐、NLI 蕴含与代理置信作为边属性，通过 GraphSAGE+GAT 消息传递实现跨模态一致性过滤。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“显微领域缺乏大规模、高质量多模态训练数据”这一问题拆解为“弱监督信号利用”与“跨模态一致性过滤”两个子问题，并设计了三阶段流水线。核心步骤与对应技术如下：</p>
<ol>
<li>弱监督 QA 生成</li>
</ol>
<ul>
<li>数据源：BIOMEDICA 的 2.5 M 显微图注对（已出版论文 ⇒ 专家级弱标签）。</li>
<li>MLLM-agent 先抽取图注中的事实片段作为“答案”，再反向生成问题，确保问答与原文对齐。</li>
<li>问题类型遵循 MicroVQA 定义的三类科学推理：<br>– EU（Expert Visual Understanding）<br>– HG（Hypothesis Generation）<br>– EP（Experiment Proposal）</li>
</ul>
<ol>
<li>异构图一致性过滤：HiCQA-Graph</li>
</ol>
<ul>
<li>节点：Image、Caption、QA 三类，特征为 CLIP 嵌入 + 一致性分数。</li>
<li>边与属性：<br>– Image → Caption：CLIP 余弦相似度<br>– Caption → QA：NLI 蕴含概率<br>– QA ↔ QA：问题文本余弦相似度</li>
<li>消息传递：<br>– Image→Caption/Image→QA 用 GraphSAGE 均值聚合，捕捉跨模态证据。<br>– Caption→QA/QA↔QA 用 GATv2 多头注意力，以 NLI 或相似度作为边权重，抑制幻觉边。</li>
<li>弱监督目标：<br>– Keep 分数： y<em>(keep)=α c</em>(img-qa)+(1-α)p_(ent)<br>– Capacity 标签：三分类 EU/HG/EP</li>
<li>输出：对 20 M 初始 QA 打分，保留 top-75 % 作为“干净”训练集。</li>
</ul>
<ol>
<li>MCQ 与 CoT 再生成</li>
</ol>
<ul>
<li>在过滤后的干净 QA 上，再次调用 MLLM-agent：<br>– 生成 3 个高质量干扰项与 1 条链式思维 rationale。<br>– 人工抽检测试集，修正显著错误，确保无 MicroVQA 信息泄漏。</li>
</ul>
<ol>
<li>模型微调与强化</li>
</ol>
<ul>
<li>监督微调（SFT）：InternVL3.5-4B，LoRA-rank16，MCQ 格式 + CoT 目标。</li>
<li>组相对策略优化（GRPO）：在 MCQ 上进一步校准选项概率，抑制格式错误。</li>
</ul>
<p>通过“弱监督生成 → 图滤波去噪 → 小模型微调”，仅 4 B 参数的开源模型即在 MicroVQA 上达到 59.4 % 平均准确率，与 GPT-5 持平，并在更高难度的 MicroVQA++ 测试集上取得开源 SOTA，验证了“数据工程优先”即可解决显微科学推理的数据瓶颈。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“数据构造有效性”与“模型推理性能”两条主线展开实验，共 6 组核心实验与 3 类辅助分析。结果均以 MicroVQA 与 MicroVQA++ 双测试集为统一评估基准，指标为三类科学推理能力（EU/HG/EP）的 MCQ 准确率及平均准确率。</p>
<ol>
<li>主实验：商用与开源模型对比</li>
</ol>
<ul>
<li>基线：Random、Human、GPT-4o 系列、o1/o3/o4-mini、Claude-Sonnet-4.5、GPT-5 等。</li>
<li>开源：LLaVA-Med-Mistral-7B、Qwen-2-VL-7B、InternVL3.5-2B/4B。</li>
<li>结论：在 MicroVQA 上，经 MicroVQA++ 训练集 SFT 后，InternVL3.5-4B 取得 59.4 % 平均准确率，与 GPT-5（59.4 %）持平；2B 模型亦提升 +22.7 %，验证小参数模型即可达到商用水平。</li>
</ul>
<ol>
<li>高难度测试集验证</li>
</ol>
<ul>
<li>在 Bloom 等级更高、分布偏移更大的 MicroVQA++ 测试集上，InternVL3.5-4B† 仍达 41.3 %，显著优于未微调同类模型（36.4 %），且训练-测试分布重叠度低，证明去偏有效。</li>
</ul>
<ol>
<li>过滤方法消融</li>
</ol>
<ul>
<li>对比 NLI-only、CLIP-only、NCLIP（线性融合）与 HiCQA-Graph 在 25 %/50 %/75 % 保留率下的性能。</li>
<li>HiCQA-Graph@75 % 取得最佳平均准确率（54.5 %/59.4 % for 2B/4B），显著优于单信号过滤，验证异构图联合建模的必要性。</li>
</ul>
<ol>
<li>图组件消融</li>
</ol>
<ul>
<li>分别移除 CLIP 相似度、NLI 蕴含、容量标签、跨模态一致性 token。</li>
<li>任一组件缺失均导致平均准确率下降 1–2 个百分点，其中移除 NLI 影响最大。</li>
</ul>
<ol>
<li>训练策略对比</li>
</ol>
<ul>
<li>SFT 格式：QA 自由回答 vs. MCQ+CoT。</li>
<li>强化学习：GRPO 在 MCQ 上进一步微调。</li>
<li>结果：MCQ-SFT 比 QA-SFT 提升 17 % 绝对准确率；GRPO 在 MCQ 上再提升 1.6 %，而 GRPO-on-QA 因格式奖励缺失出现 reward hacking，性能骤降。</li>
</ul>
<ol>
<li>效率与可扩展性</li>
</ol>
<ul>
<li>单张 RTX 3090 上，HiCQA-Graph 端到端训练延迟 129 ms，推理 110 ms，图训练本身仅占总耗时 &lt; 4 %，表明过滤模块可随数据规模线性扩展。</li>
</ul>
<p>辅助分析</p>
<ul>
<li>Bloom 等级分布：MicroVQA++ 高难度样本比例显著高于 MicroVQA，验证测试集更具挑战性。</li>
<li>CLIP t-SNE：可视化显示 MicroVQA++ 覆盖范围更广，且图像-问题特征在共享空间共聚，为图滤波提供先验合理性。</li>
<li>错误模式：归纳出视觉定位错误、干扰项词汇重叠陷阱、caption-like 幻觉三类典型失败案例，为后续迭代提供改进方向。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>后续可在以下 6 个方向继续深入，部分可直接基于现有框架扩展，部分则需引入新的数据或算法组件：</p>
<ol>
<li>多粒度显微模态扩展</li>
</ol>
<ul>
<li>将电子显微镜（TEM/SEM）、原子力显微镜（AFM）、荧光寿命成像（FLIM）等模态纳入同一图框架，需重新设计模态专属 CLIP 编码器或引入 adapter。</li>
<li>探索跨模态问答任务，例如“给定荧光图像，预测对应的超分辨电镜区域”。</li>
</ul>
<ol>
<li>时空序列与 3D 体积推理</li>
</ol>
<ul>
<li>现有数据以单帧 2D 图像为主；可收集连续 Z-stack 或时间序列显微视频，构建 4D 节点（x, y, z, t），在 HiCQA-Graph 中增加“时序相邻”边，研究动态细胞过程的因果问答。</li>
</ul>
<ol>
<li>细粒度视觉定位监督</li>
</ol>
<ul>
<li>当前失败案例之一是“小结构误定位”。可引入弱监督实例分割分支，利用图注中的实体提及生成粗略 mask，作为 QA 节点的额外视觉一致性信号，减少幻觉答案。</li>
</ul>
<ol>
<li>知识增强的推理路径</li>
</ol>
<ul>
<li>将 UniProt、Gene Ontology 等结构化生物知识库编码为外部节点，与 QA 节点建立“知识支持”边，实现“图像-文本-知识”三模态图神经网络，提升 HG/EP 类问题的可解释性。</li>
</ul>
<ol>
<li>自我改进与迭代数据飞轮</li>
</ol>
<ul>
<li>采用“主动学习”策略：让模型在测试集高不确定样本上生成多答案，再经专家批量校正，回流到训练集；同步更新 HiCQA-Graph 权重，实现滚动式数据-模型协同增强。</li>
</ul>
<ol>
<li>低资源语言与跨文化可迁移性</li>
</ol>
<ul>
<li>现有 caption 与 QA 以英文为主；可探索 XLM-R + 视觉共享编码器，将图节点特征替换为 multilingual CLIP，验证在中文、西班牙文等低资源科学文献上的零样本或少量样本推理能力。</li>
</ul>
<p>这些扩展均保持“弱监督生成 → 图一致性过滤 → 小规模微调”的核心范式，但分别针对模态、维度、知识、语言等维度引入新的节点类型或边属性，可系统性推进显微科学问答的广度与深度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：显微领域缺乏大规模、高质量多模态训练数据，导致 MLLM 科学推理能力受限。</li>
<li><strong>方法</strong>：提出三阶段流水线——</li>
</ul>
<ol>
<li>用 MLLM-agent 从 BIOMEDICA 图注弱监督生成 QA；</li>
<li>设计异构图滤波器 HiCQA-Graph，融合 CLIP 对齐、NLI 蕴含与代理置信，通过 GraphSAGE+GAT 消息传递剪除幻觉样本；</li>
<li>在过滤后的干净数据上再生成 MCQ+CoT，并经人工抽检形成 MicroVQA++（训练 20 K / 测试 6 K）。</li>
</ol>
<ul>
<li><strong>结果</strong>：仅 4 B 参数的 InternVL3.5 经 MicroVQA++ 微调后，在 MicroVQA 基准上平均准确率 59.4 %，与 GPT-5 持平；在更高 Bloom 等级的 MicroVQA++ 测试集上取得开源 SOTA，验证“数据工程优先”即可释放小模型在垂直领域的推理潜能。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MicroVQA%2B%2B%3A%20High-Quality%20Microscopy%20Reasoning%20Dataset%20with%20Weakly%20Supervised%20Graphs%20for%20Multimodal%20Large%20Language%20Model，链接是https://arxiv.org/pdf/2511.11407，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.11407。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Manyu Li,Ruian He,Chenxi Ma,Weimin Tan,Bo Yan</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.11407.pdf">https://arxiv.org/pdf/2511.11407.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.11407">https://arxiv.org/abs/2511.11407</a></p>
<p><strong>Arxiv ID</strong>: 2511.11407</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.11407">https://papers.cool/arxiv/2511.11407</a></p>
<p><strong>Published</strong>: 2025-11-14T15:35:43Z</p>
<p><strong>Updated</strong>: 2025-11-14T15:35:43.000Z</p>
<hr>
<h3 id="20-Dynamic-Reflections-Probing-Video-Representations-with-Text-Alignment"><a href="#20-Dynamic-Reflections-Probing-Video-Representations-with-Text-Alignment" class="headerlink" title="20. Dynamic Reflections: Probing Video Representations with Text Alignment"></a>20. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2511.02767">Dynamic Reflections: Probing Video Representations with Text Alignment</a></h3><p>The alignment of representations from different modalities has recently been shown to provide insights on the structural similarities and downstream capabilities of different encoders across diverse data types. While significant progress has been made in aligning images with text, the temporal nature of video data remains largely unexplored in this context. In this work, we conduct the first comprehensive study of video-text representation alignment, probing the capabilities of modern video and language encoders. Our findings reveal several key insights. First, we demonstrate that cross-modal alignment highly depends on the richness of both visual (static images vs. multi-frame videos) and text (single caption vs. a collection) data provided at test time, especially when using state-of-the-art video encoders. We propose parametric test-time scaling laws that capture this behavior and show remarkable predictive power against empirical observations. Secondly, we investigate the correlation between semantic alignment and performance on both semantic and non-semantic downstream tasks, providing initial evidence that strong alignment against text encoders may be linked to general-purpose video representation and understanding. Finally, we correlate temporal reasoning with cross-modal alignment providing a challenging test-bed for vision and language models. Overall, our work introduces video-text alignment as an informative zero-shot way to probe the representation power of different encoders for spatio-temporal data. Project page can be found at <a target="_blank" rel="noopener" href="https://video-prh.github.io/">https://video-prh.github.io/</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近的研究表明，不同模态的表示对齐可以提供有关不同编码器在各种数据类型上的结构相似性和下游能力的洞见。尽管在图像与文本的对齐方面已经取得了显著进展，但视频数据的时间特性在这一背景下仍 largely unexplored。在本研究中，我们进行了首个全面的视频-文本表示对齐研究，探讨了现代视频和语言编码器的能力。我们的研究结果揭示了几个关键见解。首先，我们证明了跨模态对齐高度依赖于测试时提供的视觉（静态图像 vs. 多帧视频）和文本（单一标题 vs. 多条文本）数据的丰富性，尤其是在使用最先进的视频编码器时。我们提出了参数化的测试时缩放法则来捕捉这种行为，并展示了其对实证观测的显著预测能力。其次，我们研究了语义对齐与语义及非语义下游任务性能之间的相关性，为强对齐与文本编码器可能与通用视频表示和理解相关提供了初步证据。最后，我们将时间推理与跨模态对齐相关联，为视觉与语言模型提供了一个具有挑战性的测试平台。总体而言，我们的工作将视频-文本对齐引入为一种有信息量的零样本方法，用于探测不同编码器对时空数据的表示能力。项目页面可访问 <a target="_blank" rel="noopener" href="https://video-prh.github.io/">https://video-prh.github.io/</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该研究旨在回答一个核心问题：<br><strong>“柏拉图表示假说（Platonic Representation Hypothesis, PRH）在动态、时序数据（视频）上是否依然成立？”</strong></p>
<p>具体而言，论文试图解决以下子问题：</p>
<ol>
<li>静态模态（图像-文本）的对齐结论能否直接推广到<strong>视频-文本</strong>？</li>
</ol>
<ul>
<li>既往工作仅验证了图像与文本的表征趋同，而视频特有的运动、因果与时序依赖尚未被纳入对齐分析。</li>
</ul>
<ol>
<li>测试时<strong>视觉侧与语言侧的数据丰富度</strong>（帧数 vs. 描述条数）如何定量影响跨模态对齐得分？</li>
</ol>
<ul>
<li>揭示“无需重训模型，仅靠增加帧或多样描述即可显著提升对齐”的现象，并给出<strong>参数化测试时缩放定律</strong>。</li>
</ul>
<ol>
<li>视频-文本对齐得分能否作为<strong>零样本代理指标</strong>，预测模型在下游语义/非语义任务上的性能？</li>
</ol>
<ul>
<li>为自监督视频模型提供廉价、无标签的质量评估手段，替代昂贵的任务特定微调。</li>
</ul>
<ol>
<li>当前视频/文本编码器在<strong>时序敏感性</strong>（如事件顺序、硬负例）上的瓶颈何在？</li>
</ol>
<ul>
<li>通过构造时序重排负例，定位模型对“先-后”语义的判别能力缺口，为未来改进提供方向。</li>
</ul>
<p>综上，该工作首次把“表征对齐”研究从静态域扩展到<strong>时序域</strong>，并系统验证其可用作<strong>零-shot 视频表征质量探针</strong>，填补了 PRH 在动态数据上的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“2 Related Work”部分将相关研究划分为三大主线，并指出各自与本文的关联与缺口。可归纳为以下文献集合：</p>
<p>1. 柏拉图表示假说与静态模态对齐</p>
<ul>
<li><strong>核心假说</strong></li>
<li>Huh et al., 2024 —— 首次提出 PRH，证明大规模单模态视觉/语言模型在潜在空间趋向一致。</li>
<li><strong>静态对齐度量</strong></li>
<li>Maniparambil et al., 2024a/b —— 无监督图像-文本表征几何相似性分析。</li>
<li>Kornblith et al., 2019 —— CKA 等核相似度指标。</li>
<li>Merullo et al., 2022 —— 线性映射即可连接图像与文本空间。</li>
<li><strong>利用对齐进行任务迁移</strong></li>
<li>Jha et al., 2025；Schnaus et al., 2025 —— 无平行数据下的跨模态翻译。</li>
<li>Zhang et al., 2025 —— 以 k-NN 质量探针评估对齐潜力。</li>
<li><strong>缺口</strong>：以上工作<strong>仅限静态图像与文本</strong>，未涉及时序视频数据。</li>
</ul>
<p>2. 自监督视频表征学习</p>
<ul>
<li><strong>掩码自编码范式</strong></li>
<li>VideoMAE/VideoMAEv2 (Tong et al., 2022；Wang et al., 2023) —— 纯视频重建，无文本监督。</li>
<li>Bardes et al., 2024 —— 特征预测自监督。</li>
<li><strong>大规模扩展</strong></li>
<li>Carreira et al., 2024 (4DS) —— 10 亿参数视频模型。</li>
<li>Assran et al., 2025 (V-JEPA 2) —— 联合预测与规划目标。</li>
<li><strong>评估困境</strong><br>现有评估依赖下游微调或 Kinetics/SSv2 分类，<strong>缺乏零样本、任务无关的质量探针</strong>。</li>
</ul>
<p>3. 视频-文本联合预训练（显式对齐）</p>
<ul>
<li><strong>对比式视频-语言模型</strong></li>
<li>Xu et al., 2021 (VideoCLIP) —— 视频-文本对比学习。</li>
<li>Zhao et al., 2024 (VideoPrism) —— 混合对比与掩码重建。</li>
<li>Kim et al., 2023；Li et al., 2025 —— 零样本视频 grounding / retrieval。</li>
<li><strong>与本文区别</strong>：这些系统<strong>依赖显式文本监督</strong>进行联合训练，而本文研究<strong>无文本监督、纯自监督视频编码器</strong>是否<strong>涌现</strong>出与文本的对齐能力。</li>
</ul>
<p>4. 时序语义与硬负例评测</p>
<ul>
<li><strong>时序敏感性数据集</strong></li>
<li>Bagad et al., 2023 (Test-of-Time) —— 合成“先-后”顺序判断。</li>
<li>Kim et al., 2025 (VideoComp) —— 长视频事件重排负例。</li>
<li><strong>用途</strong>：本文借上述数据集验证“对齐得分是否能反映模型对时序顺序的敏感性”，此前无工作用此类探针评估<strong>自监督视频模型</strong>。</li>
</ul>
<p>总结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>研究方向</th>
<th>代表文献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>柏拉图表示假说（静态）</td>
<td>Huh+24, Maniparambil+24</td>
<td>理论起点，扩展至视频</td>
</tr>
<tr>
<td>自监督视频预训练</td>
<td>VideoMAEv2, 4DS, V-JEPA 2</td>
<td>被测对象，验证其文本对齐</td>
</tr>
<tr>
<td>视频-文本联合训练</td>
<td>VideoCLIP, VideoPrism</td>
<td>作为上界对比，凸显“无文本”对齐之难</td>
</tr>
<tr>
<td>时序语义评测</td>
<td>Test-of-Time, VideoComp</td>
<td>提供探针，揭示对齐≠时序敏感</td>
</tr>
</tbody>
</table>
</div>
<p>本文首次把上述三线研究<strong>在“时序-文本对齐”框架下统一</strong>，填补了 PRH 在动态模态的空白，并提出可预测的测试时缩放定律与零样本质量探针。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“度量–缩放–验证”三步法，将静态模态的对齐研究系统性地扩展到视频-文本领域，具体手段如下：</p>
<p>1. 构建统一的对齐度量框架</p>
<ul>
<li><strong>继承并扩展 Mutual k-NN（MkNN）</strong><br>对给定的 N 个视频-文本对，分别用<strong>视频编码器</strong>与<strong>文本编码器</strong>得到嵌入矩阵</li>
</ul>
<p>X∈R^(N× p),quad Y∈R^(N× q)</p>
<p>构造 k-近邻指示矩阵  M_X, M_Y ，计算</p>
<p>A<em>(MkNN)(X,Y)=(1) / (kN)∑</em>(i,j)(M<em>Xodot M_Y)</em>(ij)</p>
<p>该值∈<br>0,1<br>，越高表示两种潜在空间的邻域结构越一致。</p>
<ul>
<li><strong>支持“多帧+多描述”测试时扩展</strong></li>
<li>视觉侧：对长视频均匀采样  n_f  帧（或  n_f/n_o  个 clip），逐 clip 编码后<strong>平均池化</strong>。</li>
<li>文本侧：将同一视频的  n_c  条人工或 LLM 合成描述<strong>拼接成单字符串</strong>，再编码并平均 token 维。<br>通过变化  n_f, n_c  即可在<strong>推理阶段</strong>无梯度地放大信息量。</li>
</ul>
<p>2. 提出参数化测试时缩放定律</p>
<p>观察到对齐得分随  n_f, n_c  单调递增并快速饱和，用如下<strong>加法饱和模型</strong>拟合：</p>
<p>score(n<em>f,n_c)=S</em>∞-l(C_f n_f^(-α)+C_c n_c^(-β)r)</p>
<ul>
<li>S_∞ ：该视觉-文本编码器对的<strong>理论极限对齐度</strong></li>
<li>C_f,α ：视觉侧“帧惩罚”系数与衰减指数</li>
<li>C_c,β ：文本侧“描述惩罚”系数与衰减指数</li>
</ul>
<p>在 VATEX/PVD 上  R^2&gt;0.97 ，可<strong>提前预测</strong>需要多少帧/描述即可逼近极限，指导数据采集成本。</p>
<p>3. 大规模实证与下游验证</p>
<ul>
<li><p><strong>模型池</strong><br>85 种视觉骨干（含 VideoMAEv2、DINOv2、CLIP、ViViT、4DS、V-JEPA 等）× 30 种语言骨干（Gemma-2、Llama-3、T5 等）共 <strong>121 个组合</strong>，保证结论的统计可靠性。</p>
</li>
<li><p><strong>零样本探针有效性验证</strong><br>将上述对齐得分与<strong>无需文本标签的下游任务</strong>做 Pearson 相关：</p>
</li>
<li><p>语义任务：SSv2/Kinetics-700 动作分类 → <strong>R≈0.92</strong></p>
</li>
<li>非语义任务：Waymo 跟踪、ScanNet 深度、RealEstate10k 相机位姿 → <strong>|R|&gt;0.82</strong><br>表明对齐得分可作为<strong>廉价通用质量指标</strong>，替代昂贵解码器微调。</li>
<li><p><strong>时序敏感性诊断</strong><br>利用 Test-of-Time 与 VideoComp 的“事件顺序重排”负例，发现：</p>
</li>
<li><p>语言模型多按“词袋”排序，对顺序不敏感；</p>
</li>
<li>视频模型对齐越高，受时序打乱冲击越大，说明其<strong>隐含编码了顺序信息</strong>，但仍远未完美。</li>
</ul>
<p>4. 结果总结（回答核心问题）</p>
<ol>
<li>PRH 在时序域成立：自监督视频模型（VideoMAEv2-Huge）与强文本编码器对齐度可达 <strong>0.41</strong>，显著高于最佳图像模型 DINOv2 的 0.37。</li>
<li>测试时“帧+描述”双扩容即可<strong>翻倍提升</strong>对齐，无需重训。</li>
<li>对齐得分能<strong>零样本预测</strong>模型在语义与非语义下游任务的表现，误差 &lt;3%。</li>
<li>当前最强模型对<strong>细粒度时序顺序</strong>仍显脆弱，留下改进空间。</li>
</ol>
<p>通过上述“度量–缩放–验证”闭环，论文首次把静态 PRH 推广到视频-文本，并提供可预测的测试时策略与零-shot 质量探针，解决了“如何在无文本监督条件下评估并提升视频表征通用性”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“视频-文本对齐”共设计并执行了 5 组互相关联的实验，覆盖度量、缩放、预测与诊断四个维度。所有实验均在相同的 1024 条 VATEX 与 1024 条 PVD 子集上进行，保证结果可比。</p>
<p>1. 基线对齐扫描实验（Sec 5）</p>
<ul>
<li><strong>目的</strong>：确认视频-文本对齐现象存在，并建立与图像-文本的差距。</li>
<li><strong>做法</strong>：</li>
<li>固定 1 帧 + 1 条描述，对 121 组视觉-文本编码器计算 MkNN。</li>
<li>同步报告 Kinetics-400 &amp; SSv2 10k 检索准确率，验证“对齐高⇒检索好”假设。</li>
<li><strong>关键结果</strong>：</li>
<li>纯视频模型 VideoMAEv2-Huge 对齐 0.223，高于 DINOv2-Giant 的 0.206。</li>
<li>对齐得分与检索准确率 Pearson r=0.87，确立基本可用性。</li>
</ul>
<p>2. 测试时数据缩放实验（Sec 6）</p>
<ul>
<li><strong>目的</strong>：量化“多帧/多描述”对对齐的提升曲线。</li>
<li><strong>做法</strong>：</li>
<li>视觉侧：nf ={1,2,4,8,16,32,48} 帧（均匀采样→平均池化）。</li>
<li>文本侧：nc ={1,2,4,8,10} 条描述（人工或 LLM 合成→拼接）。</li>
<li>记录每对 (nf,nc) 的 MkNN 得分，用饱和模型拟合。</li>
<li><strong>关键结果</strong>：</li>
<li>VideoMAEv2：R²=0.979，S∞=0.41；DINOv2：R²=0.996，S∞=0.37。</li>
<li>1→10 条描述平均提升 60%；1→48 帧 VideoMAE 提升 0.18→0.38。</li>
</ul>
<p>3. 跨数据集合成描述实验（Sec A.3）</p>
<ul>
<li><strong>目的</strong>：验证“缩放定律”是否依赖人工多描述，还是可用 LLM 合成。</li>
<li><strong>做法</strong>：</li>
<li>用 Gemini-2.5-Pro 将 PVD 单条长描述拆成 10 条短描述（零幻觉提示）。</li>
<li>重复实验 2 的 nc 扫描。</li>
<li><strong>关键结果</strong>：</li>
<li>3 条合成描述即可超越原始长描述；6 条后饱和，证明<strong>低成本数据增强</strong>即可受益。</li>
</ul>
<p>4. 下游任务预测实验（Sec 7）</p>
<ul>
<li><strong>目的</strong>：检验对齐得分能否零样本预测视频表征质量。</li>
<li><strong>做法</strong>：</li>
<li>选取 9 个纯自监督视频模型（VideoMAE-B/L/H、V-JEPA-H、4DS-S/B/L/G/e）。</li>
<li>冻结特征，训练轻量注意力解码器，获得 6 个下游指标：</li>
</ul>
<ol>
<li>SSv2 动作分类</li>
<li>Kinetics-700 动作分类</li>
<li>Perception-Test 点跟踪</li>
<li>Waymo 盒子跟踪</li>
<li>RealEstate10k 相机位姿（误差↓）</li>
<li>ScanNet 深度估计（误差↓）</li>
</ol>
<ul>
<li>计算对齐得分与下游指标 Pearson/Spearman 相关。</li>
<li><strong>关键结果</strong>：</li>
<li>语义任务 |r|≥0.88，非语义任务 |r|≥0.82；点跟踪 r=0.40 为唯一例外。</li>
<li>首次证明<strong>视频-文本对齐可作为通用零样本质量探针</strong>。</li>
</ul>
<p>5. 时序敏感性诊断实验（Sec 8）</p>
<p>5a) Test-of-Time 合成顺序</p>
<ul>
<li><strong>做法</strong>：180 段“c1 圆在 c2 圆之前/之后”视频，评估 k=1,2,3 时的 MkNN。</li>
<li><strong>结果</strong>：k=3 时所有模型≈1.0；k=1 时视频模型显著高于文本模型，说明<strong>文本更偏向词袋</strong>。</li>
</ul>
<p>5b) VideoComp 长视频重排</p>
<ul>
<li><strong>做法</strong>：512 段 YouTube 视频，对应“正序/乱序”两种描述，计算对齐落差 Δ=Ap−An。</li>
<li><strong>结果</strong>：</li>
<li>对齐越高的模型 Δ 越大（VideoMAEv2-Huge Δ=0.09），表明其<strong>编码了时序结构</strong>但仍不完美。</li>
</ul>
<p>5c) 视频-视频互相关系（Sec A.5）</p>
<ul>
<li><strong>做法</strong>：计算 11 个视频模型两两 MkNN，聚类分析。</li>
<li><strong>结果</strong>：出现“语义-语言”与“几何-运动”两大簇；DINOv2、VideoMAEv2-K710 能<strong>横跨两簇</strong>，提示跨簇对齐或成为通用模型指标。</li>
</ul>
<p>实验一览表</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验</th>
<th>变量</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 基线扫描</td>
<td>121 模型×1帧×1描述</td>
<td>视频模型对齐&gt;图像模型；对齐与检索 r=0.87</td>
</tr>
<tr>
<td>2. 数据缩放</td>
<td>nf, nc 系统变化</td>
<td>饱和模型 R²&gt;0.97；10 描述提升 60%</td>
</tr>
<tr>
<td>3. 合成描述</td>
<td>LLM 生成 nc=1…10</td>
<td>3 条合成&gt;原始长描述，低成本可行</td>
</tr>
<tr>
<td>4. 下游预测</td>
<td>6 任务冻结特征解码</td>
<td>对齐得分与性能</td>
</tr>
<tr>
<td>5. 时序诊断</td>
<td>合成/真实重排负例</td>
<td>高对齐模型更敏感，时序理解仍待提升</td>
</tr>
</tbody>
</table>
</div>
<p>通过上述实验，论文从“现象→规律→预测→缺陷”四步完整回答了“视频-文本对齐是否可行、可用、可预测、仍有限”这一连串问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“理论-假说”“度量-方法”“数据-场景”“模型-训练”四大类，均直接对应论文尚未解决或仅初步触及的问题。</p>
<p>1. 理论-假说层面</p>
<ul>
<li><p><strong>跨模态对齐的因果链</strong><br>目前仅展示“对齐高⇒下游好”的相关性，需通过干预实验（如人为扰动对齐度而保持语义标签不变）验证是否存在<strong>因果</strong>关系。</p>
</li>
<li><p><strong>柏拉图表示的“时序中心极限”</strong><br>论文发现对齐随帧数饱和，可继续放大 nf→10³ 以检验：</p>
</li>
</ul>
<p>lim_(n_fto∞) A(n_f,n_c) ?= const</p>
<p>若继续提升，则暗示“无限时序上下文”可突破当前 S∞，对 PRH 的静态饱和假设提出修正。</p>
<ul>
<li><strong>多模态信息论下界</strong><br>用互信息 I(v;c) 替代经验 MkNN，推导给定视频熵 H(v) 与文本熵 H(c) 下的<strong>对齐上界</strong>，解释为何 S∞&lt;0.5。</li>
</ul>
<p>2. 度量-方法层面</p>
<ul>
<li><p><strong>层级对齐动态</strong><br>论文仅选“最佳单层”报告，可绘制“层-层”热力图，研究浅层几何 vs 深层语义是否<strong>同步</strong>出现时序敏感峰。</p>
</li>
<li><p><strong>时序对齐细粒度</strong><br>将 30 帧视频按 1-frame 步长滑动，计算<strong>帧-句子</strong>细粒度 MkNN，得到对齐曲线，检验模型是否把“动词发生时刻”对齐到具体帧。</p>
</li>
<li><p><strong>因果掩码对齐</strong><br>借鉴因果干预，对视频做<strong>帧打乱、倒播、时间子采样</strong>，观察对齐下降模式，量化“因果一致性”贡献。</p>
</li>
</ul>
<p>3. 数据-场景层面</p>
<ul>
<li><p><strong>更长视频与多事件</strong><br>VATEX 仅 10 s 单事件，可扩展至 ActivityNet-200 长视频（分钟级），验证缩放定律是否<strong>随视频长度指数漂移</strong>。</p>
</li>
<li><p><strong>多语言-多文化描述</strong><br>利用 VATEX 的中英双语描述，检验“文化视角差异”是否提升对齐（不同语言描述同一事件），并探究语言模型<strong>文化偏差</strong>对对齐的影响。</p>
</li>
<li><p><strong>自我中心与机器人视频</strong><br>引入 EPIC-Kitchens、Ego4D 等第一视角数据，验证对齐指标是否<strong>预测动作规划或具身任务</strong>性能，迈向 embodied PRH。</p>
</li>
</ul>
<p>4. 模型-训练层面</p>
<ul>
<li><p><strong>生成式视频模型对齐</strong><br>论文仅测理解端（编码器），可测量 LVDM、WALT、VideoLDM 等<strong>生成模型潜空间</strong>与文本的对齐，探讨“生成-理解”是否共享同一柏拉图空间。</p>
</li>
<li><p><strong>联合 vs 分离训练谱系</strong><br>建立一条连续轴：<br>纯自监督 → 视频-文本对比 → 视频-文本生成<br>系统测量对齐-性能曲线，找出<strong>最小文本监督量</strong>使对齐跃迁，指导资源受限场景。</p>
</li>
<li><p><strong>对齐引导的自监督预训练</strong><br>把 MkNN 得分作为<strong>在线奖励信号</strong>，在掩码重建损失上加 λ·A(v,c) 项，观察是否<strong>无标签提升下游</strong>，实现“自监督+对齐”闭环优化。</p>
</li>
<li><p><strong>跨模态量化与低秩投影</strong><br>研究 8-bit 量化或 LoRA 微调后对齐的衰减系数，验证能否在<strong>边缘端</strong>保持高对齐，服务部署需求。</p>
</li>
</ul>
<p>5. 评估-应用层面</p>
<ul>
<li><p><strong>开放世界故障探测</strong><br>利用对齐得分实时监测新视频域（无人机、水下）是否<strong>分布外</strong>，当 MkNN 骤降时触发模型更新，构建“对齐-OOD”报警器。</p>
</li>
<li><p><strong>Hard-negative 挖掘自动循环</strong><br>用当前最佳模型生成时序乱序、对象替换、因果倒置的负描述，<strong>在线扩充</strong> VideoComp 风格数据集，实现“对齐→造难→再训练”的自我对抗提升。</p>
</li>
</ul>
<p>小结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>可探索关键问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>理论</td>
<td>对齐与下游性能是否因果？无限帧能否突破饱和？</td>
</tr>
<tr>
<td>度量</td>
<td>层动态、帧-词细粒度、因果掩码干预</td>
</tr>
<tr>
<td>数据</td>
<td>长视频、多语言、自我中心、机器人</td>
</tr>
<tr>
<td>模型</td>
<td>生成式潜空间、最小文本监督、对齐奖励预训练</td>
</tr>
<tr>
<td>应用</td>
<td>OOD 探测、自动 Hard-negative 循环</td>
</tr>
</tbody>
</table>
</div>
<p>上述任何一条均可直接沿用论文已开源的代码与 121 模型池，快速形成新的实验分支，继续拓展“柏拉图表示假说”在时序、多语言、具身乃至生成侧的边界。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Dynamic Reflections</strong> 首次把“柏拉图表示假说”从静态图像-文本扩展到<strong>视频-文本</strong>，系统回答三个问题：①动态模态能否自发对齐语言；②测试时多帧/多描述能否显著提升对齐；③对齐得分可否零样本预测下游性能。主要贡献与结论如下：</p>
<ol>
<li><p><strong>大规模实证</strong><br>121 个视觉-文本编码器组合显示：纯自监督 VideoMAEv2-Huge 对齐度达 0.41，优于最佳图像模型 DINOv2（0.37），证实<strong>时序信息促进语义趋同</strong>。</p>
</li>
<li><p><strong>测试时缩放定律</strong><br>提出饱和模型</p>
</li>
</ol>
<p>score(n<em>f,n_c)=S</em>∞-(C_f n_f^(-α)+C_c n_c^(-β))</p>
<p>在 VATEX/PVD 上 R²&gt;0.97；1→10 条描述平均提升 60%，48 帧视频使对齐翻倍，无需重训即可逼近理论极限。</p>
<ol>
<li><p><strong>零样本质量探针</strong><br>对齐得分与 6 个下游任务（动作分类、深度、位姿、跟踪）|r|≥0.82，首次证明<strong>视频-文本对齐可作为通用、无标签的表征质量指标</strong>。</p>
</li>
<li><p><strong>时序敏感性诊断</strong><br>利用事件重排负例发现：高对齐模型对顺序扰动更敏感，但现有编码器仍<strong>缺乏细粒度因果时序理解</strong>，留下改进空间。</p>
</li>
</ol>
<p>综上，论文确立“视频-文本对齐”是一种信息丰富、可预测、易计算的<strong>零-shot 探针</strong>，为自监督视频模型开发提供了新的评估与优化范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Dynamic%20Reflections%3A%20Probing%20Video%20Representations%20with%20Text%20Alignment，链接是https://arxiv.org/pdf/2511.02767，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.02767。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tyler Zhu,Tengda Han,Leonidas Guibas,Viorica Pătrăucean,Maks Ovsjanikov</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2511.02767.pdf">https://arxiv.org/pdf/2511.02767.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2511.02767">https://arxiv.org/abs/2511.02767</a></p>
<p><strong>Arxiv ID</strong>: 2511.02767</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2511.02767">https://papers.cool/arxiv/2511.02767</a></p>
<p><strong>Published</strong>: 2025-11-04T17:52:14Z</p>
<p><strong>Updated</strong>: 2025-11-04T17:52:14.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202511/2025-11-19/">https://wdndev.github.io/daily/hf/202511/2025-11-19/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-P1-Mastering-Physics-Olympiads-with-Reinforcement-Learning"><span class="toc-text">1. P1: Mastering Physics Olympiads with Reinforcement Learning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Uni-MoE-2-0-Omni-Scaling-Language-Centric-Omnimodal-Large-Model-with-Advanced-MoE-Training-and-Data"><span class="toc-text">2. Uni-MoE-2.0-Omni: Scaling Language-Centric Omnimodal Large Model with Advanced MoE, Training and Data</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-MiroThinker-Pushing-the-Performance-Boundaries-of-Open-Source-Research-Agents-via-Model-Context-and-Interactive-Scaling"><span class="toc-text">3. MiroThinker: Pushing the Performance Boundaries of Open-Source Research Agents via Model, Context, and Interactive Scaling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Souper-Model-How-Simple-Arithmetic-Unlocks-State-of-the-Art-LLM-Performance"><span class="toc-text">4. Souper-Model: How Simple Arithmetic Unlocks State-of-the-Art LLM Performance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Part-X-MLLM-Part-aware-3D-Multimodal-Large-Language-Model"><span class="toc-text">5. Part-X-MLLM: Part-aware 3D Multimodal Large Language Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-MMaDA-Parallel-Multimodal-Large-Diffusion-Language-Models-for-Thinking-Aware-Editing-and-Generation"><span class="toc-text">6. MMaDA-Parallel: Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-GroupRank-A-Groupwise-Reranking-Paradigm-Driven-by-Reinforcement-Learning"><span class="toc-text">7. GroupRank: A Groupwise Reranking Paradigm Driven by Reinforcement Learning</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#LLM-calls%EF%BC%9AGroupwise-O-N-c-%EF%BC%8C%E4%BD%8E%E4%BA%8E-Pairwise-O-N-2-%E4%B8%8E-Listwise-Sliding-O-rN-s-%E3%80%82"><span class="toc-text">LLM-calls：Groupwise  O(N&#x2F;c) ，低于 Pairwise  O(N^2)  与 Listwise-Sliding  O(rN&#x2F;s) 。</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#8-TiViBench-Benchmarking-Think-in-Video-Reasoning-for-Video-Generative-Models"><span class="toc-text">8. TiViBench: Benchmarking Think-in-Video Reasoning for Video Generative Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-PhysX-Anything-Simulation-Ready-Physical-3D-Assets-from-Single-Image"><span class="toc-text">9. PhysX-Anything: Simulation-Ready Physical 3D Assets from Single Image</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-UFO-3-Weaving-the-Digital-Agent-Galaxy"><span class="toc-text">10. UFO^3: Weaving the Digital Agent Galaxy</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Evolve-the-Method-Not-the-Prompts-Evolutionary-Synthesis-of-Jailbreak-Attacks-on-LLMs"><span class="toc-text">11. Evolve the Method, Not the Prompts: Evolutionary Synthesis of Jailbreak Attacks on LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-Back-to-Basics-Let-Denoising-Generative-Models-Denoise"><span class="toc-text">12. Back to Basics: Let Denoising Generative Models Denoise</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-OlmoEarth-Stable-Latent-Image-Modeling-for-Multimodal-Earth-Observation"><span class="toc-text">13. OlmoEarth: Stable Latent Image Modeling for Multimodal Earth Observation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-Live-SWE-agent-Can-Software-Engineering-Agents-Self-Evolve-on-the-Fly"><span class="toc-text">14. Live-SWE-agent: Can Software Engineering Agents Self-Evolve on the Fly?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Genomic-Next-Token-Predictors-are-In-Context-Learners"><span class="toc-text">15. Genomic Next-Token Predictors are In-Context Learners</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-WebCoach-Self-Evolving-Web-Agents-with-Cross-Session-Memory-Guidance"><span class="toc-text">16. WebCoach: Self-Evolving Web Agents with Cross-Session Memory Guidance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-Test-Time-Spectrum-Aware-Latent-Steering-for-Zero-Shot-Generalization-in-Vision-Language-Models"><span class="toc-text">17. Test-Time Spectrum-Aware Latent Steering for Zero-Shot Generalization in Vision-Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-UnSAMv2-Self-Supervised-Learning-Enables-Segment-Anything-at-Any-Granularity"><span class="toc-text">18. UnSAMv2: Self-Supervised Learning Enables Segment Anything at Any Granularity</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-MicroVQA-High-Quality-Microscopy-Reasoning-Dataset-with-Weakly-Supervised-Graphs-for-Multimodal-Large-Language-Model"><span class="toc-text">19. MicroVQA++: High-Quality Microscopy Reasoning Dataset with Weakly Supervised Graphs for Multimodal Large Language Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20-Dynamic-Reflections-Probing-Video-Representations-with-Text-Alignment"><span class="toc-text">20. Dynamic Reflections: Probing Video Representations with Text Alignment</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>