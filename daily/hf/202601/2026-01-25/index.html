<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2026-01-25 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic ExperienceThe development of native computer-use agents (CUA) represents a significa">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2026-01-25">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202601/2026-01-25/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic ExperienceThe development of native computer-use agents (CUA) represents a significa">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:55.015Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202601/2026-01-25/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2026-01-25',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2026-01-25</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:55.015Z" title="Updated 2026-02-08 08:00:55">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">85.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>304min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2026-01-25"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience"><a href="#1-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience" class="headerlink" title="1. EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15876">EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience</a></h3><p>The development of native computer-use agents (CUA) represents a significant leap in multimodal AI. However, their potential is currently bottlenecked by the constraints of static data scaling. Existing paradigms relying primarily on passive imitation of static datasets struggle to capture the intricate causal dynamics inherent in long-horizon computer tasks. In this work, we introduce EvoCUA, a native computer use agentic model. Unlike static imitation, EvoCUA integrates data generation and policy optimization into a self-sustaining evolutionary cycle. To mitigate data scarcity, we develop a verifiable synthesis engine that autonomously generates diverse tasks coupled with executable validators. To enable large-scale experience acquisition, we design a scalable infrastructure orchestrating tens of thousands of asynchronous sandbox rollouts. Building on these massive trajectories, we propose an iterative evolving learning strategy to efficiently internalize this experience. This mechanism dynamically regulates policy updates by identifying capability boundaries — reinforcing successful routines while transforming failure trajectories into rich supervision through error analysis and self-correction. Empirical evaluations on the OSWorld benchmark demonstrate that EvoCUA achieves a success rate of 56.7%, establishing a new open-source state-of-the-art. Notably, EvoCUA significantly outperforms the previous best open-source model, OpenCUA-72B (45.0%), and surpasses leading closed-weights models such as UI-TARS-2 (53.1%). Crucially, our results underscore the generalizability of this approach: the evolving paradigm driven by learning from experience yields consistent performance gains across foundation models of varying scales, establishing a robust and scalable path for advancing native agent capabilities.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>原生计算机使用代理（CUA）的发展代表了多模态人工智能的一大飞跃。然而，目前它们的潜力受到静态数据缩放的限制。现有主要依赖静态数据集被动模仿的范式难以捕捉长视野计算机任务中固有的复杂因果动态。在本研究中，我们介绍了EvoCUA，一种原生计算机使用代理模型。与静态模仿不同，EvoCUA 将数据生成和策略优化整合为一个自我维持的进化周期。为缓解数据稀缺性，我们开发了可验证的综合引擎，自主生成多样化任务并配合可执行验证器。为了实现大规模体验获取，我们设计了一个可扩展的基础设施，协调数万次异步沙盒部署。基于这些庞大的轨迹，我们提出了一种迭代演进的学习策略，以高效内化这种体验。该机制通过识别能力边界动态调节策略更新——强化成功的例程，同时通过错误分析和自我纠正将失败轨迹转化为丰富的监督。OSWorld基准测试的实证评估显示，EvoCUA的成功率达到56.7%，奠定了开源领域的新兴领先地位。值得注意的是，EvoCUA的表现远超此前最佳开源模型OpenCUA-72B（45.0%），并超过了领先的闭权重模型如UI-TARS-2（53.1%）。关键是，我们的结果强调了该方法的可推广性：由经验学习驱动的范式演进，在不同规模的基础模型中实现了持续的性能提升，建立了稳健且可扩展的原生代理能力发展路径。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在突破“静态数据规模扩展”对原生计算机使用智能体（Computer-Use Agent, CUA）的能力瓶颈。核心问题可归纳为：</p>
<ul>
<li><p><strong>静态模仿范式失效</strong><br>现有方法主要依赖对固定、非交互数据集的被动模仿，无法捕获长时程计算机任务中固有的因果动态与环境反馈，导致策略在真实交互中鲁棒性不足。</p>
</li>
<li><p><strong>数据稀缺与奖励歧义</strong><br>高质量人类操作轨迹昂贵且有限；纯文本指令合成易 hallucinate，产生“无法求解”的任务描述，使奖励信号失去物理可验证性。</p>
</li>
<li><p><strong>经验规模扩展的三重挑战</strong></p>
</li>
</ul>
<ol>
<li>可验证的数据合成：需自动生成“任务–可执行验证器”对，确保奖励严格由终端状态确定。</li>
<li>高并发交互基础设施：需支持每日数十万沙箱会话、百万级交互请求的工业级稳定 rollout。</li>
<li>高效训练配方：在巨大交互空间中，必须以“on-policy”方式聚焦模型能力边界，避免无约束探索带来的计算爆炸。</li>
</ol>
<p>为此，作者提出 EvoCUA——一个将“可验证数据合成–大规模异步交互–迭代式经验优化”封装成自循环进化系统的原生 CUA，实现从“静态数据规模”到“经验规模”的范式迁移，最终在 OSWorld 基准上取得开源 SOTA（56.7%），验证该范式对 8B–72B 等多规模基模型的通用增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“8 Related Work”与正文多处对比中，系统梳理了与 EvoCUA 直接相关的四条研究脉络（按出现顺序归纳，不引用第一人称）：</p>
<ul>
<li><strong>原生视觉-语言模型与计算机使用能力</strong></li>
<li>闭源前沿：Claude 4.5 Sonnet、Seed-1.8 等已具备零样本长时程 GUI 规划能力，为业界标杆。</li>
<li>开源基座：Qwen3-VL 系列引入动态分辨率与增强 OCR，是 EvoCUA 的初始骨干。</li>
<li><strong>通用 GUI 智能体与评测基准</strong></li>
<li>OSWorld 作为开放任务集，提供真实操作系统环境评测。</li>
<li>OpenCUA 提出 AgentNet 数据集并开源 7B–72B 模型，构成前期最强开源基线。</li>
<li>UI-TARS-2、Step-GUI 分别采用多轮 RL 与逐步视觉推理，展示专用架构路线。</li>
<li><strong>视觉定位与动作执行</strong></li>
<li>Aguvis、ShowUI、UGround 等聚焦高分辨率像素级定位，为 EvoCUA 的“统一动作空间”提供原子级操作设计参考。</li>
<li><strong>从模仿到“经验学习”的训练范式演进</strong></li>
<li>行为克隆 → 强化学习：UI-TARS-2 将 PPO 适配到多轮 GUI 交互。</li>
<li>可验证奖励 RL（RLVR）：DeepSeek-R1 / DeepSeekMath 首次在数学推理中验证长链推理无需密集过程标签；Feng et al. 提出 Group-in-Group 优化稳定训练；Zhang et al. 探索“Early Experience”无奖励学习。</li>
<li>EvoCUA 在此基础上，用“可验证合成引擎”解决数据稀缺，并以大规模拒采样+偏好优化形成自循环经验进化，实现 SOTA。</li>
</ul>
<p>以上研究共同构成 EvoCUA 的对比基线与方法论起点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“静态数据规模”瓶颈转化为“可验证经验规模”自循环，通过三大核心模块与三阶段训练范式实现解决方案，具体路径如下：</p>
<ol>
<li>可验证合成引擎</li>
</ol>
<ul>
<li>结构化任务空间：按“领域–原子能力”正交分解，覆盖浏览器、Excel、Word 等真实应用；引入参数化生成+公开网络资源注入，保证初始状态多样性。</li>
<li>双流智能体合成：Task Architect（VLM）在同一 ReAct 循环内并行生成自然语言指令 g 与可执行验证器 Vg，形成“生成即验证”闭环；失败执行日志即时反馈，迭代修正直至通过。</li>
<li>三重去污染：语义、配置、验证器层面过滤与人工抽检，消除与评测基准的泄漏。</li>
</ul>
<ol>
<li>高并发交互基础设施</li>
</ol>
<ul>
<li>工具-集群抽象：工具固化系统镜像与 API，集群动态弹性伸缩；单分钟可拉起数万个 QEMU-KVM 沙箱，实现日级 10^5 会话、10^6 交互请求。</li>
<li>异步网关+分布式调度器：控制面与数据面解耦，保证路由与沙箱生命周期管理无阻塞。</li>
<li>确定性校准：定制 Ubuntu 22.04 镜像，修补 HID 键映射、字体渲染、网络代理，确保截图与输入在并发环境下可复现。</li>
</ul>
<ol>
<li>经验驱动的进化学习循环</li>
</ol>
<ul>
<li>冷启动：用≈1 k 条高质量轨迹+ hindsight 推理生成，统一动作空间（鼠标、键盘、元控制）与显式思维模板（目标澄清、观察一致性、自验证、反思纠正、终止推理），建立可解释行为先验。</li>
<li>拒采样微调（RFT）：动态计算预算按任务难度分配，过滤冗余步骤与不可行任务，保留数万条高信噪比成功轨迹，巩固有效模式。</li>
<li>强化学习（Step-Level DPO）：<br>– 关键分叉点定位：对比失败与参考轨迹，找到首处动作偏离且状态仍等价的步骤 t*。<br>– 双重偏好构造：<br>‑ 动作纠正范式 I：用参考或合成得到 (z_w, a_w) 替代原错误 (z_l, a_l)。<br>‑ 反思恢复范式 II：在 t*+1 状态要求模型先“停-思-再计划”，而非盲目继续。<br>– 按步骤粒度优化 DPO 损失，将失败经验转化为参数更新，持续外推能力边界。</li>
</ul>
<p>通过“合成→大规模交互→策略迭代”的闭合回路，EvoCUA 把合成计算量直接转化为可验证训练信号，在 OSWorld 上取得 56.7% 开源 SOTA，并在 8B–72B 多基模型上验证该范式的通用增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕「在线智能体能力」「离线细粒度定位」「通用多模态性能」「消融与可扩展性」四个维度展开系统实验，主要结果如下（均不带第一人称）：</p>
<p>1 在线智能体能力（OSWorld-Verified）</p>
<ul>
<li><strong>主对比</strong></li>
<li>闭源：OpenAI-CUA、Step-GUI-8B、Qwen3-VL-Flash、UI-TARS-2-2509、Claude-4.5-Sonnet、Seed-1.8</li>
<li>开源：Qwen2.5-VL 系列、UI-TARS-72B-DPO、OpenCUA-7/32/72B、Qwen3-VL-Thinking 系列</li>
<li><strong>指标</strong>：Pass@1 success rate，统一 50 步预算（与多数基线 100 步对比）</li>
<li><strong>结果</strong></li>
<li>EvoCUA-32B 取得 56.7%，<strong>开源第一</strong>，比前开源 SOTA（OpenCUA-72B 45.0%）↑11.7%，亦超 UI-TARS-2（53.1%）↑3.6%，与 Claude-4.5-Sonnet（58.1%）差距仅 1.4%。</li>
<li>EvoCUA-8B 达 46.1%，<strong>超越 72B 级专用模型</strong>（OpenCUA-72B 45.0%、Step-GUI-8B 40.2%），验证经验进化范式对同骨架的额外增益。</li>
</ul>
<p>2 离线细粒度定位与通用多模态</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>类型</th>
<th>EvoCUA-32B</th>
<th>EvoCUA-8B</th>
<th>EvoCUA-OpenCUA-72B</th>
</tr>
</thead>
<tbody>
<tr>
<td>ScreenSpot-v2</td>
<td>GUI grounding</td>
<td>90.40</td>
<td>85.21</td>
<td>93.47</td>
</tr>
<tr>
<td>ScreenSpot-Pro</td>
<td>高分辨率 GUI</td>
<td>49.76</td>
<td>45.39</td>
<td>63.24</td>
</tr>
<tr>
<td>OSWorld-G</td>
<td>GUI 元素定位</td>
<td>63.86</td>
<td>55.08</td>
<td>67.65</td>
</tr>
<tr>
<td>MMMU</td>
<td>多学科推理</td>
<td>68.11</td>
<td>62.11</td>
<td>59.22</td>
</tr>
<tr>
<td>MMMU-Pro</td>
<td>鲁棒多学科</td>
<td>59.16</td>
<td>53.30</td>
<td>46.51</td>
</tr>
<tr>
<td>MathVista</td>
<td>数学推理</td>
<td>80.40</td>
<td>75.80</td>
<td>69.40</td>
</tr>
<tr>
<td>MMStar</td>
<td>多模态综合</td>
<td>73.20</td>
<td>69.07</td>
<td>67.80</td>
</tr>
<tr>
<td>OCRBench</td>
<td>文字识别</td>
<td>85.35</td>
<td>80.30</td>
<td>84.05</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>结论</strong></li>
<li>在 OpenCUA-72B 骨架上，进化训练保持或略升所有指标，证明<strong>数据分布对齐时不会遗忘</strong>。</li>
<li>Qwen3-VL-Thinking 骨架出现小幅下降，主要因通用数据未采用“思考”分布，后续可通过补充 thinking 语料修复。</li>
</ul>
<p>3 消融实验</p>
<p>3.1 以 Qwen3-VL-32B-Thinking 为基线的逐阶段增益</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>绝对提升 ∆</th>
</tr>
</thead>
<tbody>
<tr>
<td>+ Unified Action Space</td>
<td>+4.84%</td>
</tr>
<tr>
<td>+ Cold Start</td>
<td>+2.62%</td>
</tr>
<tr>
<td>+ RFT</td>
<td>+3.13%</td>
</tr>
<tr>
<td>+ Offline DPO</td>
<td>+3.21%</td>
</tr>
<tr>
<td>+ Iterative Training（第二轮 RFT+DPO）</td>
<td>+1.90%</td>
</tr>
</tbody>
</table>
</div>
<p>3.2 以 OpenCUA-72B 为基线的跨尺度验证</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>绝对提升 ∆</th>
</tr>
</thead>
<tbody>
<tr>
<td>+ Cold Start</td>
<td>+2.14%</td>
</tr>
<tr>
<td>+ RFT</td>
<td>+3.69%</td>
</tr>
<tr>
<td>+ Offline DPO</td>
<td>+3.02%</td>
</tr>
<tr>
<td>+ Iterative Training</td>
<td>+1.82%</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>额外纯 RFT 实验</strong>：三轮拒采样（20 k → 226 k → 1 M）累计 <strong>+8.12%</strong>，显示强基座+大规模成功经验即可显著提分。</li>
</ul>
<p>4 可扩展性分析</p>
<ul>
<li><p><strong>Pass@k Scaling</strong><br>k=8~128 均保持对基线稳定正增益，32B 模型在 k=16 达峰值 +4.93%，证明动作与推理先验提升<strong>上限</strong>。</p>
</li>
<li><p><strong>Max Steps Scaling</strong><br>15 → 50 步区间，32B 模型收益 +16.25%；50 步后提升趋缓，与训练分布中长轨迹占比少有关。</p>
</li>
<li><p><strong>经验数据 Scaling（RFT）</strong></p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据量</th>
<th>累计提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>20 k</td>
<td>+2.61 pp</td>
</tr>
<tr>
<td>226 k</td>
<td>+6.79 pp</td>
</tr>
<tr>
<td>1 M</td>
<td>+8.12 pp</td>
</tr>
</tbody>
</table>
</div>
<p>揭示数据规模、分布漂移与信噪比之间的权衡，强调持续扩量需同步保证 on-policy 与去噪。</p>
<p>5 在线 RL 初步验证（未来方向）</p>
<ul>
<li>训练-推理上下文不一致导致传统轨迹级 GRPO 失效。</li>
<li>提出 Step-Level Policy Optimization（STEPO），将轨迹优势均摊到每一步，在 OpenCUA-32B 上 16 次平均胜率显著高于 GRPO，验证<strong>步骤级训练</strong>对 GUI 任务的必要性。</li>
</ul>
<p>综上，实验覆盖 20+ 基线、5 大评测集合、2 条骨架模型、多轮消融与 scaling 曲线，定量证明“可验证合成+大规模交互+迭代经验优化”范式对模型能力与规模均具一致增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望均直接源自论文第 7 节“Future Work on Online Agentic RL”与第 6.5 节“Discussions”中作者明确指出的开放问题，按主题归纳，未使用第一人称：</p>
<p>1 在线强化学习的深度扩展</p>
<ul>
<li><strong>环境-策略联合探索</strong></li>
<li>当前仅做离线经验回放，需转向<strong>完全在线 RL</strong>：实时采样、实时更新，利用最新策略持续探索未知状态空间。</li>
<li>引入<strong>置信域或乐观初始化</strong>，鼓励对高不确定性界面状态进行主动试探，缓解“确定性合成→真实环境”分布漂移。</li>
<li><strong>信用分配与延迟奖励</strong></li>
<li>长时程 GUI 任务奖励稀疏，需研究<strong>子目标自动分解</strong>（option / skill 发现）或<strong>事后重标记（Hindsight Goal Relabeling）</strong>，将最终成功信号高效传播至中间步骤。</li>
<li>探索<strong>步骤级价值函数</strong>（Step-level Value Model）与<strong>蒙特卡洛树搜索</strong>结合，实现更精细的局部策略改进。</li>
<li><strong>训练-推理上下文不一致的进一步缓解</strong></li>
<li>STEPO 仅做均匀优势分配，可尝试<strong>自适应步级权重</strong>（按 token 重要性或视觉变化幅度赋权），减少低信息量步骤的梯度噪声。</li>
<li>引入<strong>记忆保持机制</strong>（递归状态或外部记忆库），让模型在 rollout 阶段也能访问被压缩的早期语义信息，降低近似误差。</li>
</ul>
<p>2 数据合成与经验生成的升级</p>
<ul>
<li><strong>任务难度与策略能力动态匹配</strong></li>
<li>当前用固定阈值 Λ 控制预算，可改为<strong>在线估计的 Regret 或 PAC-Bayes 边界</strong>，实现更精细的“课程-反课程”自动切换。</li>
<li>对<strong>长尾边界任务</strong>引入<strong>对抗性任务生成</strong>（Adversarial Task Generator），专门寻找策略最脆弱的状态-动作区域，加速边界修复。</li>
<li><strong>多模态验证器统一框架</strong></li>
<li>除终端状态二进制奖励外，研究<strong>稠密视觉-语义验证器</strong>（像素级 diff、OCR 差异、结构嵌入距离），为步骤级 RL 提供密集监督。</li>
<li>探索<strong>可微验证器</strong>（Differentiable Validator），使部分界面元素检查可端到端反向传播，实现梯度直接优化。</li>
<li><strong>真实用户轨迹与合成数据混合</strong></li>
<li>引入<strong>人机协同标注</strong>（Human-in-the-loop Labeling），对真实世界失败案例快速生成验证器，缩小仿真-现实差距。</li>
<li>研究<strong>隐私保护联邦合成</strong>（Federated Synthesis），在本地用户环境内完成数据生成与验证，避免敏感信息外泄。</li>
</ul>
<p>3 动作空间与感知能力的扩充</p>
<ul>
<li><strong>高阶组合动作原语</strong></li>
<li>当前仅支持单键-单点操作，可新增<strong>手势序列</strong>（双指缩放、三指拖拽）、<strong>快捷键宏</strong>（Ctrl+Shift+→ 选取单词）等复合原子，减少步数并降低累积误差。</li>
<li>引入<strong>等待-条件混合原语</strong>（wait_until + visual condition），让智能体在异步渲染完成前阻塞，降低“盲等”带来的冗余步骤。</li>
<li><strong>跨应用工作流与系统级操作</strong></li>
<li>扩展沙箱镜像至<strong>IDE、数据库客户端、终端</strong>等专业化软件，覆盖真实工作流。</li>
<li>支持<strong>系统设置</strong>（网络代理、环境变量、驱动安装）与<strong>命令行-图形混合</strong>任务，迈向“全栈”计算机使用。</li>
<li><strong>高分辨率与小目标感知</strong></li>
<li>引入<strong>滑动窗口局部放大</strong>（Foveated Attention）或<strong>多尺度金字塔裁剪</strong>，提升 4K/Retina 屏下 8×8 px 图标定位精度。</li>
<li>结合<strong>OCR + 图标嵌入双路对齐</strong>，解决“图像按钮无文本”场景下的语义歧义。</li>
</ul>
<p>4 鲁棒性与安全研究</p>
<ul>
<li><strong>环境随机性与评估方差</strong></li>
<li>建立<strong>随机性注入基准</strong>（Randomized-OSWorld）：在渲染延迟、网络抖动、弹窗时机上施加可控分布扰动，衡量策略鲁棒下限。</li>
<li>提出<strong>环境-策略双维度 Pass@k</strong>，同时报告“模型多样性”与“抗扰动成功率”，避免指标被生成多样性掩盖真实脆弱性。</li>
<li><strong>对抗与误用防护</strong></li>
<li>研究<strong>对抗截图攻击</strong>（Adversarial Pixel Perturbation）对定位精度的影响，开发<strong>视觉输入消毒</strong>（Sanitization）模块。</li>
<li>构建<strong>沙箱逃逸检测</strong>与<strong>最小权限隔离</strong>机制，防止智能体在自动化任务中执行恶意代码或越权操作。</li>
</ul>
<p>5 基础模型与训练策略</p>
<ul>
<li><strong>多模态混合专家（MoE）架构</strong></li>
<li>引入<strong>视觉-动作异构专家</strong>，让高分辨率图像、低分辨率全局图、文本 OCR 分别由不同子网络处理，降低推理成本。</li>
<li>探索<strong>专家级负载均衡</strong>与<strong>动态路由</strong>，避免动作专家在长文本推理时空闲浪费。</li>
<li><strong>参数高效微调与持续学习</strong></li>
<li>采用<strong>LoRA/DoRA</strong> 仅训练动作-策略子空间，保留通用视觉-语言权重，缓解灾难遗忘。</li>
<li>结合<strong>经验回放缓冲</strong>与<strong>弹性权重巩固（EWC）</strong>，实现多版本环境镜像的持续迭代而不覆盖旧能力。</li>
</ul>
<p>6 评估体系与社区共建</p>
<ul>
<li><strong>细粒度步骤级指标</strong></li>
<li>除整体成功率外，报告<strong>首次错误步骤分布</strong>、<strong>冗余操作比例</strong>、<strong>人类效率比率</strong>（Agent Steps / Human Expert Steps），更直观揭示可用性差距。</li>
<li>引入<strong>能量消耗或 API 调用成本</strong>作为绿色 AI 指标，引导高效策略。</li>
<li><strong>开源生态与标准化</strong></li>
<li>将可验证合成引擎、沙箱镜像、评估协议全部开源，形成<strong>社区驱动的持续基准更新</strong>，避免静态数据集快速过时。</li>
<li>建立<strong>在线排行榜</strong>与<strong>实时对抗赛道</strong>，允许全球提交在线策略，动态刷新 SOTA，推动领域快速迭代。</li>
</ul>
<p>综上，未来工作可沿“在线 RL 算法-数据合成-动作/感知扩展-鲁棒安全-架构效率-评估标准化”六大方向深入，逐步缩小与人类级可靠计算机使用能力的差距。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 EvoCUA，一种“经验自进化”的原生计算机使用智能体，核心贡献与内容可概括为：</p>
<ol>
<li><p>问题背景<br>静态数据集被动模仿无法捕获长时程 GUI 任务的因果动态与环境反馈，导致性能瓶颈；亟需从“数据规模”转向“经验规模”。</p>
</li>
<li><p>总体框架<br>将<strong>可验证数据合成</strong>、<strong>高并发异步交互</strong>与<strong>迭代式经验优化</strong>封装成自循环进化系统，形成“合成→ rollout → 策略更新”闭环。</p>
</li>
<li><p>三大模块</p>
</li>
</ol>
<ul>
<li><strong>可验证合成引擎</strong>：自动生成“任务指令+可执行验证器”对，通过 ReAct 闭环与三重去污染，保证奖励信号严格可判定。</li>
<li><strong>高并发交互基础设施</strong>：基于 QEMU-KVM 的 Docker 化沙箱，单分钟启动数万个实例，日级处理 10⁶ 交互请求，提供确定性渲染与输入。</li>
<li><strong>经验驱动进化训练</strong>：<br>– 冷启动：统一动作/思维空间，≈1 k 高质量轨迹建立可解释先验。<br>– 拒采样微调（RFT）：动态计算预算聚焦边界任务，过滤冗余，巩固成功模式。<br>– 步骤级 DPO：定位关键分叉点，构建“动作纠正+反思恢复”偏好对，把失败转化为策略更新。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>OSWorld 基准上，EvoCUA-32B 取得 56.7% 成功率，<strong>开源第一</strong>，超 OpenCUA-72B（45.0%）11.7 个百分点，亦领先 UI-TARS-2（53.1%）。</li>
<li>8B 模型达 46.1%，超越多款 72B 级系统，验证范式对同骨架的普适增益。</li>
<li>离线 GUI 定位与通用多模态基准保持或提升性能；消融显示每阶段单调递增，数据扩至 1 M 轨迹累计 +8.12%。</li>
</ul>
<ol>
<li>未来方向<br>在线强化学习（步骤级信用分配、环境-策略联合探索）、对抗性任务生成、可微稠密验证器、高阶动作原语、跨应用工作流、鲁棒与安全评估、MoE 高效架构等。</li>
</ol>
<p>综上，EvoCUA 通过可验证经验自循环，突破静态数据限制，为原生计算机使用智能体提供了一条可扩展、可复现且通用的进化路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是EvoCUA%3A%20Evolving%20Computer%20Use%20Agents%20via%20Learning%20from%20Scalable%20Synthetic%20Experience，链接是https://arxiv.org/pdf/2601.15876，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15876。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Taofeng Xue,Chong Peng,Mianqiu Huang,Linsen Guo,Tiancheng Han,Haozhe Wang,Jianing Wang,Xiaocheng Zhang,Xin Yang,Dengchang Zhao,Jinrui Ding,Xiandi Ma,Yuchen Xie,Peng Pei,Xunliang Cai,Xipeng Qiu</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15876.pdf">https://arxiv.org/pdf/2601.15876.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15876">https://arxiv.org/abs/2601.15876</a></p>
<p><strong>Arxiv ID</strong>: 2601.15876</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15876">https://papers.cool/arxiv/2601.15876</a></p>
<p><strong>Published</strong>: 2026-01-22T11:36:43Z</p>
<p><strong>Updated</strong>: 2026-01-22T11:36:43.000Z</p>
<hr>
<h3 id="2-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding"><a href="#2-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding" class="headerlink" title="2. HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.14724">HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</a></h3><p>Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated significant improvement in offline video understanding. However, extending these capabilities to streaming video inputs, remains challenging, as existing models struggle to simultaneously maintain stable understanding performance, real-time responses, and low GPU memory overhead. To address this challenge, we propose HERMES, a novel training-free architecture for real-time and accurate understanding of video streams. Based on a mechanistic attention investigation, we conceptualize KV cache as a hierarchical memory framework that encapsulates video information across multiple granularities. During inference, HERMES reuses a compact KV cache, enabling efficient streaming understanding under resource constraints. Notably, HERMES requires no auxiliary computations upon the arrival of user queries, thereby guaranteeing real-time responses for continuous video stream interactions, which achieves 10$\times$ faster TTFT compared to prior SOTA. Even when reducing video tokens by up to 68% compared with uniform sampling, HERMES achieves superior or comparable accuracy across all benchmarks, with up to 11.4% gains on streaming datasets.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>多模态大语言模型（MLLMs）的最新进展在离线视频理解方面显示出了显著的提升。然而，将这些能力扩展到流式视频输入仍然具有挑战性，因为现有模型在保持稳定理解性能、实时响应和低GPU内存占用方面存在困难。为了解决这一挑战，我们提出了HERMES，一种无需训练的新型架构，用于实时、准确地理解视频流。基于机制性注意力研究，我们将KV缓存概念化为一个分层记忆框架，用于在多粒度上封装视频信息。在推理过程中，HERMES重复使用紧凑的KV缓存，从而在资源受限下实现高效的流式理解。值得注意的是，HERMES在用户查询到达时无需额外计算，从而保证了连续视频流交互的实时响应，相较于之前的最先进方法（SOTA），实现了10倍更快的TTFT。即使在与均匀采样相比减少高达68%的视频令牌的情况下，HERMES在所有基准测试中仍能实现更优或相当的准确率，在流式数据集上最高提升11.4%。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>多模态大语言模型（MLLM）在流式视频理解中的三重瓶颈</strong>：</p>
<ol>
<li><strong>实时性</strong>：现有方法在查询到达时需额外检索或重计算，导致首 token 延迟（TTFT）高，无法满足连续交互的实时需求。</li>
<li><strong>内存效率</strong>：流式输入长度不可预知，统一缓存策略很快触顶，造成 OOM 或被迫降采样，牺牲精度。</li>
<li><strong>精度稳定性</strong>：简单 FIFO 或均匀采样丢弃 token，未考虑不同网络层对“近期感知–帧级语义–长程记忆”的差异化依赖，导致长程推理与细粒度时空理解性能下降。</li>
</ol>
<p>为此，作者提出<strong>HERMES</strong>，首次将 KV-cache 概念化为<strong>分层记忆系统</strong>（感官/工作/长程），在<strong>无需任何训练</strong>的前提下，通过层内重要性评分、跨层平滑、位置重索引三组件，实现：</p>
<ul>
<li>查询时<strong>零额外计算</strong>，TTFT 比 SOTA 快 10×；</li>
<li><strong>固定 GPU 内存预算</strong>，支持任意长视频流；</li>
<li>在<strong>视频 token 减少 68 %</strong> 的情况下，流式与离线基准平均精度分别<strong>提升 11.4 %</strong> 与<strong>持平或更优</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大脉络，均围绕“如何在资源受限条件下让 MLLM 持续看懂长视频”展开：</p>
<ol>
<li>流式视频理解专用架构</li>
</ol>
<ul>
<li><strong>训练型在线模型</strong></li>
<li>TimeChat-Online、StreamForest、VideoLLM-online、Dispider 等：通过新增跨帧注意力、事件记忆模块或决策-反应解耦，实现逐帧或逐段实时推理，但需昂贵端到端微调或专有数据。</li>
<li><strong>训练无关缓存方法</strong></li>
<li>ReKV / LiveVLM：将历史 KV 缓存卸载到 CPU/磁盘，查询时重新加载并重计算注意力，延迟高。</li>
<li>StreamMem：仅用对话模板 token 做启发式压缩，无层间差异化管理，可解释性差。</li>
</ul>
<ol>
<li>离线长视频 KV-cache 压缩</li>
</ol>
<ul>
<li>动态剪枝：DyCoKe、VisionZip、VideoTree 等针对“已完整可见”的视频做全局 token 重要性估计，无法处理未来帧未知场景。</li>
<li>均匀采样或局部聚合：LongVU、StreamingVLM 等，未利用层间注意力差异，流式迁移时精度骤降。</li>
</ul>
<ol>
<li>记忆机制与分层表示</li>
</ol>
<ul>
<li>认知记忆理论：Atkinson-Shiffrin 三层记忆模型（感官-工作-长程）被引入 LLM 长文本场景（MemGPT、Titans），但尚未在视觉-语言流式推理中系统落地。</li>
<li>层间注意力可解释性： prior 工作发现浅层重“当前 token”、深层重“全局摘要”，HERMES 首次将该规律用于<strong>无训练流式视频缓存管理</strong>。</li>
</ul>
<p>HERMES 与上述方法的本质区别在于：</p>
<ul>
<li><strong>零训练、零查询时计算</strong>；</li>
<li><strong>首次把 KV-cache 视为分层记忆</strong>，按层定制淘汰策略并跨层平滑；</li>
<li><strong>统一解决实时性、内存、精度</strong>三难问题，可直接插拔到现有 MLLM。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>HERMES</strong> 框架，把 KV-cache 重新概念化为“分层记忆”，在<strong>完全不训练模型参数</strong>的前提下，通过三层协同策略一次性解决实时性、内存与精度冲突：</p>
<ol>
<li>分层 KV-cache 管理<br>依据 mechanistic 注意力分析，将 28 层解码器划分为</li>
</ol>
<ul>
<li>浅层 10 % → 感官记忆：仅用<strong>指数遗忘曲线</strong>保留最新帧；</li>
<li>深层 30 % → 长程记忆：以<strong>帧级 anchor-token 注意力强度</strong>为重要性评分；</li>
<li>中层 60 % → 工作记忆：线性插值上述两项，平滑过渡。<br>每层独立计算重要性分数  S_l^i ，按固定预算 |M| 做 Top-K 保留，其余淘汰。</li>
</ul>
<ol>
<li>跨层记忆平滑<br>为避免同一段视频在不同层被不一致地裁剪，引入<strong>自顶向下平滑</strong>：</li>
</ol>
<p>S<em>l^i = (1-λ_l) S_l^i + λ_l S</em>(l+1)^i, quad λ_l∈0.1,0.3,0.4</p>
<p>深层稳定的长程信号逐层向前传递，保证视觉记忆语义连贯。</p>
<ol>
<li>位置重索引<br>流式累积导致绝对位置溢出。HERMES 提供两种策略：</li>
</ol>
<ul>
<li><strong>Lazy</strong>：仅当索引逼近模型上限才触发，系统 token 位置固定，视频 token 左紧凑，最小化漂移；</li>
<li><strong>Eager</strong>：每次压缩即重排，适合离线长视频。<br>对 1D-RoPE 与 3D M-RoPE 均给出<strong>旋转相位补偿公式</strong>，实现 KV 物理复用而无需重计算：</li>
</ul>
<p>k<em>(new) = k</em>(old) odot e^(i(p<em>(new))-p</em>(old))θ</p>
<ol>
<li><p>长程摘要 token<br>淘汰帧并不直接丢弃，而是在每层做<strong>相位对齐后平均池化</strong>，生成 1 个“摘要 token”放回缓存，持续携带压缩后的长程信息。</p>
</li>
<li><p>查询阶段零额外计算<br>用户提问时，模型直接对现成 KV-cache 做自回归解码，<strong>无检索、无重填充、无 CPU-GPU 往返</strong>，TTFT 稳定 &lt;30 ms。</p>
</li>
</ol>
<p>通过上述设计，HERMES 在 4K-token 预算下即可维持与 64 帧均匀采样相当的信息量，实现</p>
<ul>
<li>GPU 内存恒定（≈16.5 GB）；</li>
<li>帧数线性增长时 TTFT 不变，比 SOTA 快 10×；</li>
<li>StreamingBench/OVO-Bench/RVS 平均精度提升 3.6 %–11.4 %，离线长视频基准持平或更优。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>精度、效率、消融</strong>三条主线展开实验，覆盖<strong>流式与离线、多选与开放问答、短-中-长视频</strong>全谱场景，并在两种主流模型系列（LLaVA-OV、Qwen2.5-VL）的 0.5 B–32 B 参数规模上验证通用性。</p>
<ol>
<li>主精度对比<br>1.1 流式基准</li>
</ol>
<ul>
<li><strong>StreamingBench</strong>（500 视频，2 500 道多选，实时感知 + 后向追踪）</li>
<li><strong>OVO-Bench</strong>（644 视频，≈2 800 道多选，实时感知/后向追踪）</li>
<li><strong>RVS-Ego / RVS-Movie</strong>（32 小时 egocentric + movie 开放问答，GPT-3.5 评分）<br>结果：HERMES 在 4 K-token 预算下，较基线平均提升 <strong>3.6 %–11.4 %</strong>，超越所有<strong>无需训练</strong>在线方法，与专有模型 Gemini-1.5-Pro 差距 &lt;3 %。</li>
</ul>
<p>1.2 离线基准</p>
<ul>
<li><strong>MVBench</strong>（16 s 短视频，4 000 多选）</li>
<li><strong>Egoschema</strong>（3 min，5 063 多选）</li>
<li><strong>VideoMME</strong>（17 min，2 700 多选，无字幕）<br>结果：token 减少 68 % 仍与 64 帧均匀采样基线<strong>持平或更优</strong>（Egoschema 60.29 % vs 59.93 %；VideoMME 58.85 % vs 48.00 %）。</li>
</ul>
<ol>
<li>效率评测<br>在单卡 A800（80 GB）上控制变量：</li>
</ol>
<ul>
<li><strong>GPU 峰值内存</strong>：256 帧场景下 HERMES 仅 16.66 GB，比 LiveVLM 低 1.04×。</li>
<li><strong>TTFT</strong>：随帧数线性增长的基线达 300 ms，HERMES 恒定在 28 ms，实现 <strong>10× 加速</strong>。</li>
<li><strong>TPOT</strong>：维持 24 ms/token 不变，验证解码阶段无额外开销。<br>额外测试 chunk=8/16 两种编码粒度，指标均保持稳定，证明<strong>真实流式场景可用</strong>。</li>
</ul>
<ol>
<li>消融与超参分析<br>3.1 记忆预算 |M|<br>1 K–10 K 扫描显示：Streaming 任务 4 K 即饱和；长离线视频需 ≥4 K，佐证预算设置合理性。</li>
</ol>
<p>3.2 跨层平滑 λ<br>去掉平滑（λ≡0）后 VideoMME 平均降 1.7 %；浅/中/深分别取 0.1/0.3/0.4 最优。</p>
<p>3.3 位置重索引策略<br>Streaming 任务用 <strong>Lazy</strong> 比 Eager 高 0.9 %；离线长视频相反，Eager 高 3.5 %，与理论分析一致。</p>
<p>3.4 摘要 token<br>深层聚合淘汰帧可带来 <strong>0.48 %–1.33 %</strong> 的长程问答提升，验证长程信息不丢失。</p>
<ol>
<li>可视化案例<br>在 RVS-Ego/RVS-Movie 中给出 6 组人工对比：HERMES 能正确识别“上一步加调料”“白天追逐”“室内/室外区分”等细粒度时空细节，而基线出现幻觉或遗漏。</li>
</ol>
<p>综上，实验充分证明 HERMES 在<strong>不训练、不增加查询延迟</strong>的前提下，可同时取得 SOTA 级流式精度与恒定内存/延迟表现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可延续 HERMES 的“分层 KV-cache 记忆”视角，进一步拓展流式多模态理解的边界：</p>
<ol>
<li><p>分层记忆的<strong>自适应划分</strong><br>当前按固定 10 %/60 %/30 % 切分浅/中/深，若能<strong>在线统计注意力熵或梯度敏感度</strong>，动态调整各层预算，可更好适配不同领域（体育直播 vs 监控）。</p>
</li>
<li><p><strong>音频-字幕-视觉</strong>跨模态分层<br>仅压缩视觉 token，若将音频、字幕也纳入同一套 hierarchical KV-cache，并研究不同模态在浅/深层的互补/竞争关系，可迈向<strong>全感知流式 AGI</strong>。</p>
</li>
<li><p><strong>事件级时间抽象</strong><br>现有摘要 token 是帧级平均，可引入<strong>在线聚类或增量 BERT</strong> 把相邻高相似帧聚为“事件节点”，再用图结构存储，实现<strong>可变时间分辨率</strong>记忆。</p>
</li>
<li><p><strong>动作-意图驱动的主动淘汰</strong><br>结合强化学习或世界模型，对未来<strong>可能被用户提问的片段</strong>提前提高保留优先级，从“被动压缩”走向“<strong>预测性记忆</strong>”。</p>
</li>
<li><p><strong>端侧部署与量化协同</strong><br>HERMES 目前仅验证 FP16；若与 4-bit/8-bit KV-cache 量化、Flash-Attention 结合，可进一步把<strong>恒定内存压至 &lt;6 GB</strong>，实现<strong>手机端实时理解</strong>。</p>
</li>
<li><p><strong>多代理分布式记忆</strong><br>将分层缓存按层切片分布到不同设备（边缘节点-云端），浅层低延迟放在端侧，深层长程放云端，研究<strong>一致性协议与带宽最优传输</strong>。</p>
</li>
<li><p><strong>可解释性与认知对齐</strong><br>利用注意力 rollout 或 probing，验证“浅层≈视觉瞬存、深层≈情节记忆”是否与人脑 EEG/fMRI 时序响应相似，为<strong>脑启发式视频记忆模型</strong>提供证据。</p>
</li>
<li><p><strong>长视频持续学习</strong><br>在数日-数月的视频流上运行，研究摘要 token 是否会<strong>灾难性遗忘</strong>早期事件；引入<strong>记忆回放或 EWC 正则</strong>，实现终身记忆。</p>
</li>
<li><p><strong>与 Agent 规划回路闭环</strong><br>把分层 KV-cache 作为 Agent 的<strong>视觉工作记忆</strong>，与 LLM 的文本规划模块交替读写，考察对**长时任务序列（如厨房烹饪）**的完成率提升。</p>
</li>
<li><p><strong>开放世界异常检测</strong><br>利用深层的 anchor-token 记忆分布，实时监测注意力分布突变，无需额外标注即可发现<strong>异常事件</strong>（入侵、事故），实现<strong>记忆驱动的在线异常报警</strong>。</p>
</li>
</ol>
<p>这些探索可在保持“零训练”或“轻量微调”原则下，把 HERMES 从<strong>被动缓存管理</strong>升级为<strong>主动认知记忆系统</strong>，推动流式多模态大模型走向真正的实时智能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</strong><br>一句话总结：<strong>首次把 KV-cache 当成“感官-工作-长程”三层记忆，用零训练、零查询延迟的方法，在 GPU 内存恒定、TTFT 快 10× 的前提下，实现流式视频理解精度提升 11.4 %。</strong></p>
<p>主要内容速览：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>关键结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>流式视频输入下，MLLM 无法同时满足“实时响应 + 低 GPU 内存 + 高精度”。</td>
</tr>
<tr>
<td>洞察</td>
<td>层间注意力天然分化：浅层重“最近帧”，深层重“帧级语义锚点”，中层过渡。</td>
</tr>
<tr>
<td>方法</td>
<td>1. 分层 KV-cache 管理：按层定制重要性评分（指数遗忘 / 注意力强度 / 插值）。2. 跨层平滑：自顶向下传播重要性，防止同一段视频在各层被不一致淘汰。3. 位置重索引：Lazy/Eager 两种策略，配合 RoPE/M-RoPE 旋转相位补偿，实现 KV 物理复用。4. 摘要 token：淘汰帧池化为 1 个长程记忆，继续驻留缓存。</td>
</tr>
<tr>
<td>实验</td>
<td>覆盖 5 流式 + 3 离线基准，0.5 B–32 B 两种模型；token 削减 68 %，精度↑3.6 %–11.4 %；单卡 A800 上 TTFT 恒 &lt;30 ms，内存恒 &lt;17 GB，比 SOTA 快 10×。</td>
</tr>
<tr>
<td>结论</td>
<td>HERMES 无需训练、无需查询时计算，即可“即插即用”地把任何 MLLM 升级为实时流式视频理解系统。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是HERMES%3A%20KV%20Cache%20as%20Hierarchical%20Memory%20for%20Efficient%20Streaming%20Video%20Understanding，链接是https://arxiv.org/pdf/2601.14724，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.14724。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Haowei Zhang,Shudong Yang,Jinlan Fu,See-Kiong Ng,Xipeng Qiu</p>
<p><strong>Categories</strong>: cs.CV,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.14724.pdf">https://arxiv.org/pdf/2601.14724.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14724">https://arxiv.org/abs/2601.14724</a></p>
<p><strong>Arxiv ID</strong>: 2601.14724</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.14724">https://papers.cool/arxiv/2601.14724</a></p>
<p><strong>Published</strong>: 2026-01-21T07:26:15Z</p>
<p><strong>Updated</strong>: 2026-01-21T07:26:15.000Z</p>
<hr>
<h3 id="3-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models"><a href="#3-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models" class="headerlink" title="3. The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15165">The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</a></h3><p>Diffusion Large Language Models (dLLMs) break the rigid left-to-right constraint of traditional LLMs, enabling token generation in arbitrary orders. Intuitively, this flexibility implies a solution space that strictly supersets the fixed autoregressive trajectory, theoretically unlocking superior reasoning potential for general tasks like mathematics and coding. Consequently, numerous works have leveraged reinforcement learning (RL) to elicit the reasoning capability of dLLMs. In this paper, we reveal a counter-intuitive reality: arbitrary order generation, in its current form, narrows rather than expands the reasoning boundary of dLLMs. We find that dLLMs tend to exploit this order flexibility to bypass high-uncertainty tokens that are crucial for exploration, leading to a premature collapse of the solution space. This observation challenges the premise of existing RL approaches for dLLMs, where considerable complexities, such as handling combinatorial trajectories and intractable likelihoods, are often devoted to preserving this flexibility. We demonstrate that effective reasoning is better elicited by intentionally forgoing arbitrary order and applying standard Group Relative Policy Optimization (GRPO) instead. Our approach, JustGRPO, is minimalist yet surprisingly effective (e.g., 89.1% accuracy on GSM8K) while fully retaining the parallel decoding ability of dLLMs. Project page: <a target="_blank" rel="noopener" href="https://nzl-thu.github.io/the-flexibility-trap">https://nzl-thu.github.io/the-flexibility-trap</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>扩散大语言模型（dLLMs）打破了传统大语言模型（LLMs）从左到右的严格约束，使得词元可以以任意顺序生成。从直觉上看，这种灵活性意味着一个严格包含固定自回归轨迹的解空间理论上，为数学和编程等通用任务释放更强的推理潜力。因此，许多研究利用强化学习（RL）来激发dLLMs的推理能力。在本文中，我们揭示了一个反直觉的事实：目前形式下的任意顺序生成不仅没有扩展dLLMs的推理边界，反而缩小了它。我们发现，dLLMs倾向于利用这种顺序灵活性来规避那些对探索至关重要的不确定性高的词元，从而导致解空间过早坍塌。这一观察挑战了现有针对dLLMs的RL方法的前提，即大量复杂操作，例如处理组合轨迹和难以计算的似然性，通常用于保持这种灵活性。我们证明，更有效的推理是通过有意放弃任意顺序、并采用标准的组相对策略优化（GRPO）来实现的。我们的方法JustGRPO简约却出奇有效（例如，GSM8K上准确率达89.1%），同时完全保留了dLLMs的并行解码能力。项目页面：<a target="_blank" rel="noopener" href="https://nzl-thu.github.io/the-flexibility-trap">https://nzl-thu.github.io/the-flexibility-trap</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“扩散大语言模型（dLLM）在强化学习（RL）训练中如何利用其任意顺序（arbitrary-order）生成能力”这一核心问题，提出并验证了一个反直觉观点：</p>
<ul>
<li><strong>传统假设</strong>：任意顺序生成在理论上提供了比自回归（AR）更广阔的解空间，因而被认为能释放更强的推理潜力，故现有 RL 方法普遍投入大量计算与建模代价去保留这种灵活性。</li>
<li><strong>作者发现</strong>：在当前训练与解码实践中，任意顺序反而成为“灵活性陷阱”——模型会利用顺序自由度绕过高不确定性、高熵的“逻辑分叉 token”（如 therefore、since 等），导致解空间过早塌陷，Pass@k 指标系统性低于强制左到右的 AR 模式。</li>
<li><strong>待解决问题</strong>：如何在保留 dLLM 并行解码效率的同时，消除任意顺序带来的推理潜力损失，并避免现有 RL 方法因维护灵活性而引入的组合爆炸、似然不可解、采样–优化失配等复杂性与不稳定近似。</li>
</ul>
<p>简言之，论文要解决的问题是</p>
<blockquote>
<p><strong>“如何在不牺牲 dLLM 推理潜能的前提下，用最简方法实现高效 RL 训练”</strong>；其答案是：训练阶段主动放弃任意顺序，仅用标准 GRPO 对模型施加 AR 约束（JustGRPO），即可在数学与代码推理任务上取得新的最佳结果，同时推理阶段仍可享受并行解码加速。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 6 节“Related Work”中系统梳理了三条研究脉络，并指出它们与本文工作的区别。以下按主题归纳相关研究，并给出关键文献（按论文引用编号）：</p>
<ol>
<li>扩散语言模型（dLLM）基础</li>
</ol>
<ul>
<li>连续空间扩散：Diffusion-LM（Li et al., 2022）、DiffuSeq（Gong et al., 2022）、SSD-LM（Han et al., 2022）</li>
<li>离散掩码扩散：D3PM（Lou et al., 2023）、SMDM（Sahoo et al., 2024）、Simple Masked Diffusion（Shi et al., 2024）</li>
<li>大参数规模实现：LLaDA（Nie et al., 2025）、Dream（Ye et al., 2025）、Seed-Diffusion（Song et al., 2025）、Mercury（Labs et al., 2025）、Gemini-Diffusion（DeepMind, 2025）<br>→ 本文与上述工作区别：不改进模型结构，而是质疑并舍弃“任意顺序”训练范式。</li>
</ul>
<ol>
<li>任意顺序生成的价值探讨</li>
</ol>
<ul>
<li>早期约束任务：Sudoku/Zebra Puzzle（Ye et al., 2024；Kim et al., 2025）</li>
<li>类人/草图优先解码：Dream-Coder（Xie et al., 2025）、DiffuCoder（Gong et al., 2025）</li>
<li>显式优化顺序：P2（Peng et al., 2025）、LLaDOU（Huang et al., 2025b）<br>→ 本文区别：首次用 Pass@k 证明任意顺序在通用推理任务上缩小解空间，并提出“熵退化”机制。</li>
</ul>
<ol>
<li>面向 dLLM 的强化学习</li>
</ol>
<ul>
<li>Token 级近似：d1（Zhao et al., 2025）、MMADA（Yang et al., 2025）、SPG（Wang et al., 2025a）</li>
<li>序列级似然代理：GDPO（Rojas et al., 2025）、ESPO（Ou et al., 2025）、TraceRL（Wang et al., 2025d）</li>
<li>辅助顺序策略：LLaDOU（Huang et al., 2025b）<br>→ 本文区别：放弃维护任意顺序，直接采用标准 GRPO（Shao et al., 2024），避免组合爆炸与采样–优化失配。</li>
</ul>
<p>简言之，相关研究覆盖了“扩散文本生成→顺序灵活性→RL 优化”全链路，但均默认保留任意顺序。本文首次从理论与实验层面否定该默认前提，并用极简 AR-约束 GRPO 取得更高推理性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“<strong>训练阶段主动放弃任意顺序 + 标准 GRPO</strong>”的极简方案，把原本需要应对组合爆炸、不可解似然、采样–优化失配等难题的 dLLM-RL 问题，转化为一个定义良好的自回归策略优化任务。具体步骤如下：</p>
<ol>
<li>识别核心障碍</li>
</ol>
<ul>
<li>任意顺序解码会绕过高熵“逻辑分叉 token”，导致 Pass@k 缩小（熵退化）。</li>
<li>保留灵活性使 RL 必须对 O(N!) 条去噪轨迹建模，带来<strong>灵活性税</strong>：<br>– token 级信用分配歧义<br>– 序列似然边际化不可解<br>– 启发式采样器与学习目标分布不一致</li>
</ul>
<ol>
<li>训练阶段强制 AR 顺序<br>构造部分观测状态</li>
</ol>
<p>x<em>t = [o_1,dots,o</em>(t-1),[MASK],dots,[MASK]_(L-t+1)]</p>
<p>仅取下一位置 logits 做 Softmax，得到可逐项因式分解的 AR 策略</p>
<p>π<em>θ^(AR)(o_t mid o</em>(&lt;t),q) triangleq Softmax!l(f_θ(x_t)r)_t</p>
<p>于是整条序列概率可精确计算</p>
<p>π<em>θ^(AR)(omid q)= prod</em>(t=1)^L π<em>θ^(AR)(o_tmid o</em>(&lt;t),q)</p>
<p>无需近似，重要性比率 ρ_{i,t} 与 GRPO 原公式完全兼容。</p>
<ol>
<li>直接应用标准 GRPO</li>
</ol>
<ul>
<li>对每组查询采样 G 条完整序列；</li>
<li>用可验证奖励计算组内标准化优势 A_i；</li>
<li>按 PPO-clip 目标更新，仅对 AR 路径做梯度下降。<br>算法无任何扩散专属改动，实现“<strong>JustGRPO</strong>”。</li>
</ul>
<ol>
<li><p>推理阶段恢复并行解码<br>训练仅约束优化轨迹，不改变模型架构。推理时仍可用低置信重掩码 + 半自回归块并行解码，或采用 EB-Sampler 等加速算法，保持 dLLM 的高速特性。</p>
</li>
<li><p>结果验证</p>
</li>
</ol>
<ul>
<li>GSM8K 89.1%（+3.0% 超过之前最佳 SPG）</li>
<li>MATH-500 45.1%（+6.1% 超过 ESPO）</li>
<li>HumanEval/MBPP 在不同长度下均创新高</li>
<li>并行步幅越大，相对基线提升越显著，证明 AR 训练提升了推理流形鲁棒性，而未损失速度。</li>
</ul>
<p>综上，论文通过“<strong>训练 AR、推理并行</strong>”这一简单策略，一次性解决了<br>– 任意顺序导致的推理潜力下降<br>– 扩散 RL 的灵活性税与优化不稳定性<br>两大难题，在数学与代码任务上取得新 SOTA，同时保留 dLLM 的加速优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕两条核心假设展开实验验证：</p>
<ol>
<li>任意顺序解码是否真的缩小推理潜力（Pass@k 更低）</li>
<li>仅用标准 GRPO 在 AR 约束下训练能否在推理/代码任务上取得 SOTA，同时保留并行解码速度</li>
</ol>
<p>实验设计、数据集与主要结果如下（均使用 LLaDA-Instruct 8B 作为主干）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验主题</th>
<th>数据集</th>
<th>关键指标</th>
<th>主要发现</th>
</tr>
</thead>
<tbody>
<tr>
<td>3.1 潜力对比</td>
<td>GSM8K、MATH-500、HumanEval、MBPP</td>
<td>Pass@k (k=1→128)</td>
<td>AR 顺序的 scaling 曲线显著更陡；k=1024 时 AR 独占 21.3% 题目，任意顺序仅 0.6% 反向独占 → 解空间是 AR 的真子集</td>
</tr>
<tr>
<td>3.2 机制验证</td>
<td>MATH-500</td>
<td>被跳过 token 频率、局部熵</td>
<td>任意顺序高频跳过 Therefore/Thus/Since 等逻辑连接词；这些“forking token”熵值下降 30–50%，证实“熵退化”现象</td>
</tr>
<tr>
<td>5.1 主结果</td>
<td>GSM8K、MATH-500、HumanEval、MBPP</td>
<td>准确率@128/256/512 生成长度</td>
<td>JustGRPO 全面超越现有扩散 RL 方法：GSM8K 89.1%(↑3.0%)、MATH-500 45.1%(↑6.1%)、HumanEval 49.4%、MBPP 52.4%</td>
</tr>
<tr>
<td>5.2 并行解码保持</td>
<td>HumanEval、MBPP</td>
<td>每步并行 token 数 vs 准确率</td>
<td>采用 EB-Sampler，并行步幅 1→5 时，JustGRPO 相对基线增益从 +10.6% 扩大到 +25.5%，说明 AR 训练提升了推理鲁棒性，速度优势未受损</td>
</tr>
<tr>
<td>附录 B 消融</td>
<td>HumanEval</td>
<td>温度、采样算法、块大小、熵范围</td>
<td>- 任意顺序需更高温度才能提升 Pass@k，但仍不及 AR- 负熵采样等改进算法与 AR 的 per-problem 相关性高达 0.97- 块大小↓→更像 AR，Pass@k↑- 扩展 26 个逻辑连接词，熵退化一致存在</td>
</tr>
<tr>
<td>训练效率</td>
<td>GSM8K</td>
<td>wall-clock 时间 vs 准确率</td>
<td>16×H100 上，JustGRPO 3 天达到 89%+；近似法 ESPO 10 小时即饱和。仅对 top-25% 高熵 token 计算梯度可再提速而不掉点</td>
</tr>
</tbody>
</table>
</div>
<p>综上，论文通过<strong>潜力度量、机制剖析、主任务 benchmark、并行速度、消融与效率</strong>六大类实验，系统证明：</p>
<ul>
<li>任意顺序在通用推理上确实“更灵活但潜力更低”</li>
<li>训练期简单退回 AR+GRPO 即可在精度和速度双重维度取得新最佳</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为对“灵活性陷阱”框架的延伸或反证，均具有理论与实用价值：</p>
<ol>
<li>熵退化阈值与课程顺序</li>
</ol>
<ul>
<li>量化“分叉 token”的临界熵值，探索<strong>课程式解码</strong>：前期强制 AR 以保留多样性，后期逐步放宽顺序自由度，实现“先探索后利用”的动态 schedule。</li>
<li>研究不同任务（证明、规划、对话）是否存在统一的熵阈值，或需任务自适应估计。</li>
</ul>
<ol>
<li>混合顺序策略学习</li>
</ol>
<ul>
<li>将“顺序选择”本身参数化为可学习的策略 φ(a_t | s_t)，用分层 RL 联合优化 φ 与生成模型 θ，目标直接最大化 Pass@k 而非似然，验证能否在保持高覆盖的同时降低步数。</li>
<li>对比纯 AR、纯任意、以及混合策略的 Pareto 前沿（推理潜力 ↔ 解码步数）。</li>
</ul>
<ol>
<li>扩散-specific 信用分配改进</li>
</ol>
<ul>
<li>在保留 AR 训练的前提下，引入<strong>基于熵的注意力掩码</strong>或<strong>Fork-aware 价值函数</strong>，只对高熵位置估计优势，进一步加速收敛并可能提升更高 k 时的覆盖率。</li>
<li>探索<strong>连续时间扩散 RL</strong>：把去噪步长 ∆t 也作为动作空间，优化“何时更新”与“更新什么”的联合决策。</li>
</ul>
<ol>
<li>多轮/长程推理场景</li>
</ol>
<ul>
<li>将 JustGRPO 扩展到<strong>多轮对话式数学证明</strong>或<strong>代码自修复</strong>（self-debug）任务，考察 AR 约束是否仍优于任意顺序，并观察长上下文中熵退化是否累积。</li>
<li>研究<strong>块级或子树级顺序灵活性</strong>（例如先写函数签名再填实现）能否在足够大的模型规模下逆转“陷阱”。</li>
</ul>
<ol>
<li>跨模态与工具调用</li>
</ol>
<ul>
<li>在文本-图像或文本-代码-执行结果三联体上验证：当模型可调用外部工具（计算器、代码解释器）时，任意顺序是否有助于“先安排工具调用、后填文本解释”，从而重新获得优势。</li>
<li>若优势显现，可设计<strong>工具感知顺序策略</strong>，将工具输出视为低熵锚点，再围绕锚点进行填充。</li>
</ul>
<ol>
<li>理论基础与度量扩展</li>
</ol>
<ul>
<li>从<strong>互信息 I(o_fork; o_future)</strong> 角度形式化“过早塌陷”，给出 Pass@k 上界与熵退化的定量关系。</li>
<li>提出新的解空间多样性度量（如 Reasoning-ε-cover），替代 Pass@k，更敏感地捕捉逻辑路径差异而非最终答案正确性。</li>
</ul>
<ol>
<li>系统级优化</li>
</ol>
<ul>
<li>结合**推测并行（speculative parallel）**与 AR 训练后的模型，研究是否能在保持 89% 精度的同时把解码延迟压到 AR 模型的 1/5 以下。</li>
<li>探索<strong>KV-cache 复用</strong>在 dLLM 半自回归块内的最佳划分，以支持更长上下文下的高效 RL 微调。</li>
</ul>
<ol>
<li>反向验证：更大规模或不同架构</li>
</ol>
<ul>
<li>在 30B+ 参数的 dLLM 或连续空间扩散 LM 上重复实验，观察“灵活性陷阱”是否随规模消失或加剧。</li>
<li>对比<strong>非掩码扩散</strong>（如 Gaussian 离散化）是否同样出现熵退化，确认现象是否源于掩码调度而非扩散本身。</li>
</ul>
<p>通过上述探索，可系统回答“何时该坚持 AR、何时可以安全地放开顺序”，并推动下一代“自适应顺序”扩散语言模型的出现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：扩散大语言模型（dLLM）的“任意顺序”生成被默认视为推理优势，但现有 RL 方法为保留该灵活性付出组合爆炸、似然不可解、采样–优化失配等高昂代价。</li>
<li><p><strong>反直觉发现</strong>：在数学与代码任务上，任意顺序解码反而使 Pass@k 显著低于强制左到右（AR）模式，原因是模型绕过高熵“逻辑分叉 token”，导致解空间过早塌陷（熵退化）。</p>
</li>
<li><p><strong>根源</strong>：灵活性成为推理时的“贪婪逃避”机制，而非探索机制；AR 模型被迫在关键分叉点采样，从而保留更多正确轨迹。</p>
</li>
<li><p><strong>解决方案</strong>：训练阶段完全放弃任意顺序，将 dLLM 视为自回归策略，直接应用标准 GRPO（JustGRPO），推理阶段仍可用并行解码加速。</p>
</li>
<li><p><strong>结果</strong>：</p>
</li>
<li>GSM8K 89.1%、MATH-500 45.1%、HumanEval 49.4%、MBPP 52.4%，全面超越现有扩散 RL 方法。</li>
<li>并行步幅越大，相对基线增益越高，证明 AR 训练提升推理鲁棒性且未损失速度。</li>
<li><strong>结论</strong>：对 dLLM 而言，“少即是多”——训练期回归简单 AR 顺序即可释放更高推理潜力，同时保留扩散架构的推理加速优势。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是The%20Flexibility%20Trap%3A%20Why%20Arbitrary%20Order%20Limits%20Reasoning%20Potential%20in%20Diffusion%20Language%20Models，链接是https://arxiv.org/pdf/2601.15165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15165。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zanlin Ni,Shenzhi Wang,Yang Yue,Tianyu Yu,Weilin Zhao,Yeguo Hua,Tianyi Chen,Jun Song,Cheng Yu,Bo Zheng,Gao Huang</p>
<p><strong>Categories</strong>: cs.CL,cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15165.pdf">https://arxiv.org/pdf/2601.15165.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15165">https://arxiv.org/abs/2601.15165</a></p>
<p><strong>Arxiv ID</strong>: 2601.15165</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15165">https://papers.cool/arxiv/2601.15165</a></p>
<p><strong>Published</strong>: 2026-01-21T16:41:58Z</p>
<p><strong>Updated</strong>: 2026-01-21T16:41:58.000Z</p>
<hr>
<h3 id="4-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence"><a href="#4-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence" class="headerlink" title="4. LLM-in-Sandbox Elicits General Agentic Intelligence"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16206">LLM-in-Sandbox Elicits General Agentic Intelligence</a></h3><p>We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox’s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们介绍了 LLM-in-Sandbox，使 LLM 可以在代码沙箱（即虚拟计算机）中进行探索，从而在非代码领域中引发通用智能。我们首先展示了强大的 LLM 在无需额外训练的情况下，表现出利用代码沙箱处理非代码任务的泛化能力。例如，LLM 会自发地访问外部资源以获取新知识，利用文件系统处理长上下文，并执行脚本以满足格式要求。我们进一步展示，这些智能体能力可以通过 LLM-in-Sandbox 强化学习（LLM-in-Sandbox-RL）得到增强，该方法仅使用非智能体数据来训练模型进行沙箱探索。实验表明，无论是在无需训练还是训练后设置中，LLM-in-Sandbox 都能实现跨数学、物理、化学、生物医学、长上下文理解和指令执行的稳健泛化能力。最后，我们从计算和系统的角度分析了 LLM-in-Sandbox 的效率，并将其作为 Python 包开源，以便于在实际应用中部署。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个核心问题：<br><strong>如何在不依赖额外领域专用数据或任务特定微调的前提下，让大语言模型（LLM）在数学、物理、化学、生物医学、长文本理解、指令遵循等非代码任务上释放出更强的通用智能？</strong></p>
<p>为此，作者提出并验证了两个递进式假设：</p>
<ol>
<li><p><strong>“沙盒即计算机”假设</strong><br>将 LLM 放入一个轻量级代码沙盒（虚拟 Ubuntu 容器）后，仅凭其三件套元能力——外部资源访问、文件持久化、代码执行——即可自发地把“写代码”泛化为“解决任意任务”。<br>实验表明，强模型无需额外训练即可在六大非代码领域平均提升 <strong>+6.9%</strong>；弱模型却因不会探索而失效。</p>
</li>
<li><p><strong>“沙盒交互可训练”假设</strong><br>若弱模型在沙盒中表现反而下降，可用<strong>纯非智能体数据</strong>（通用上下文任务）做强化学习，让模型学会“如何探索”。<br>提出的 <strong>LLM-in-Sandbox-RL</strong> 仅用结果奖励，即可使弱模型在沙盒模式下反超自身 LLM 模式 <strong>+11.4%</strong>，同时意外提升纯文本模式 <strong>+3~9%</strong>，实现“智能体能力”与“非智能体能力”同步迁移。</p>
</li>
</ol>
<p>综上，论文试图<strong>把“让 LLM 会写代码”升级为“让 LLM 会用计算机”</strong>，从而用同一套沙盒基础设施统一解决文本、数值、多模态、长上下文等多样化任务，并给出可复现、可部署、可扩展的开源方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 6 处明确对比或延伸了已有研究，可归纳为 4 条主线、12 篇核心文献（按首次出现页码排序）：</p>
<ol>
<li>工具使用与智能体框架</li>
</ol>
<ul>
<li>ReAct (Yao et al., 2022) —— 多轮“推理-行动”范式，LLM-in-Sandbox 的工作流直接继承自该框架。</li>
<li>Claude Code / OpenHands / SWE-agent (Anthropic 2025a; Wang et al. 2024; Yang et al. 2024) —— 代码沙盒最早用于软件工程，LLM-in-Sandbox 将其泛化到通用任务，并对比了“任务特定镜像”与“单一共享镜像”的存储开销。</li>
</ul>
<ol>
<li>沙盒内强化学习（Software-Engineering-RL）</li>
</ol>
<ul>
<li>SWE-RL / DeepSWE / R2E-Gym (Wei et al. 2025; Luo et al. 2025; Jain et al. 2025) —— 在沙盒里用 RL 训练代码修复，但依赖专业编程数据。LLM-in-Sandbox-RL 证明“通用非代码数据”也能产生沙盒探索策略，且跨域迁移。</li>
</ul>
<ol>
<li>纯文本强化学习与推理激励</li>
</ol>
<ul>
<li>Tülu 3 / rLLM / DeepSeek-R1 (Lambert et al. 2024; Tan et al. 2025; Guo et al. 2025) —— 仅用结果奖励即可提升推理，但无环境交互。LLM-in-Sandbox-RL 把“结果奖励”搬到沙盒，实现环境感知与工具调用。</li>
</ul>
<ol>
<li>长上下文与评测基准</li>
</ol>
<ul>
<li>Instruction Pre-Training (Cheng et al. 2024) —— 提供通用上下文任务数据，被直接用作 LLM-in-Sandbox-RL 的训练集。</li>
<li>AIME25 / UGPhysics / ChemBench / MedXpertQA / AA-LCR / IFBench / SWE-bench Verified —— 论文横跨 7 个领域的评测协议，均引用自 2025 年最新公开基准，保证与社区指标对齐。</li>
</ul>
<p>简言之，LLM-in-Sandbox 把“ReAct 式工具调用”从外部 API 搬进“通用计算机”，再把“SWE-RL 式沙盒训练”从代码任务推广到任意上下文任务，从而首次证明：<strong>无需领域数据，也能让模型学会“用电脑解决一切”。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“让 LLM 会用计算机”拆成三步，每一步都给出可复现的算法或系统实现：</p>
<ol>
<li>环境层：构建“最小 yet 通用”的沙盒</li>
</ol>
<ul>
<li>仅 1.1 GB 的 Ubuntu Docker 镜像，内置 Python + 常用科学库；</li>
<li>三件套元工具：<code>execute_bash</code>、<code>str_replace_editor</code>、<code>submit</code>；</li>
<li>运行时模型可 <code>pip/apt-get</code> 任意扩展，无需预先打包领域依赖。<br>结果：同一镜像即可跑数学、化学、长文本等千种任务，存储开销比 SWE-agent 降低 3–4 个数量级。</li>
</ul>
<ol>
<li>推理层：零样本激发“通用计算机使用能力”<br>采用 ReAct 多轮模板，但把“行动空间”换成沙盒命令。系统提示强制三条纪律：</li>
</ol>
<ul>
<li>禁止硬编码答案，必须写代码跑结果；</li>
<li>鼓励安装库、下载数据、用文件缓存长上下文；</li>
<li>探索失败可重来，沙盒安全隔离。<br>强模型（Claude-4.5/GPT-5/DeepSeek-V3.2）在此模式下自发出现：</li>
<li>外部资源：curl 下载 OPSIN 把化学名转 SMILES；</li>
<li>文件管理：用 grep+sed 在 100 K token 报告里定位答案；</li>
<li>计算验证：写 Python 穷举搜索满足字符数约束的句子。<br>六大领域平均绝对提升 <strong>+6.9%</strong>，最高 <strong>+24.2%</strong>（Qwen3-Coder 数学）。</li>
</ul>
<ol>
<li>训练层：用“非智能体数据”教会弱模型探索<br>提出 <strong>LLM-in-Sandbox-RL</strong>（算法流程见 Alg.2，页 8）：</li>
</ol>
<ul>
<li>数据：通用上下文任务（百科、新闻、小说等）→ 把背景材料切成多文件丢进 <code>/testbed/documents/</code>，问题仍用结果奖励评判。</li>
<li>奖励：仅看最终答案对错，不手工设计中间工具奖励。</li>
<li>训练：GRPO++， rollout 在沙盒内完成，策略梯度更新。<br>效果：</li>
<li>弱模型 Qwen3-4B-Instruct 沙盒模式从 <strong>5.8% → 16.8%</strong>（长文本），并反推纯文本模式 <strong>+3~9%</strong>。</li>
<li>强模型 Qwen3-Coder 继续提升，且跨域迁移到 SWE-bench <strong>+3.0%</strong>。<br>关键发现：沙盒里学会的“结构化 + 自验证”习惯（页 11 表 9）会<strong>迁移回纯文本生成</strong>，实现“智能体-非智能体”双增强。</li>
</ul>
<p>通过“环境最小化 → 零样本激发 → 通用数据 RL”三级跳，论文把“写代码”升级为“用计算机”，在训练、推理、部署三端均给出开源方案（Python 包已发布），从而系统性地解决了“如何让 LLM 在不增领域数据的前提下获得通用计算机使用能力”这一核心问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计 <strong>3 组实验</strong>，覆盖 <strong>7 个领域、9 个模型、2 种推理模式、2 种训练范式</strong>，总计 <strong>&gt; 30 万条轨迹</strong>。实验链条遵循“先验证零样本可行 → 再验证可训练 → 最后验证可部署”的逻辑。</p>
<p>1 零样本激发实验（§2.3–2.4）</p>
<p>目的：回答“强模型能否不经训练就利用沙盒解决非代码任务？”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>模型</td>
<td>7 个：Claude-4.5-Thinking、GPT-5、DeepSeek-V3.2-Thinking、MiniMax-M2、Kimi-K2-Thinking、Qwen3-Coder-30B-A3B、Qwen3-4B-Instruct</td>
</tr>
<tr>
<td>领域</td>
<td>6 个：数学(AIME25)、物理(UGPhysics)、化学(ChemBench)、生物医学(MedXpertQA)、长文本(AA-LCR)、指令遵循(IFBench)</td>
</tr>
<tr>
<td>指标</td>
<td>准确率 / F1 / ROUGE-L / LLM-Judge</td>
</tr>
<tr>
<td>对照</td>
<td>同一模型“纯文本生成” vs “沙盒交互”</td>
</tr>
</tbody>
</table>
</div>
<p>结果</p>
<ul>
<li>强模型平均 <strong>+6.9%</strong>，最高 <strong>+24.2%</strong>（Qwen3-Coder 数学）。</li>
<li>弱模型 Qwen3-4B 平均 <strong>−5.0%</strong>，说明“不会探索”反而有害。</li>
</ul>
<p>2 沙盒内强化学习实验（§3.2–3.3）</p>
<p>目的：回答“能否用通用非智能体数据教会弱模型探索？”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>基座</td>
<td>Qwen3-4B-Instruct（弱）、Qwen3-Coder-30B-A3B（强）</td>
</tr>
<tr>
<td>训练数据</td>
<td>Instruction Pre-Training 的 50 k 上下文任务（百科/新闻/小说等）</td>
</tr>
<tr>
<td>训练范式</td>
<td>LLM-in-Sandbox-RL vs 纯文本 LLM-RL</td>
</tr>
<tr>
<td>评测域</td>
<td>6 个通用域 + 软件工程(SWE-bench Verified)</td>
</tr>
<tr>
<td>指标</td>
<td>同上，额外统计沙盒三能力调用率、平均轮数</td>
</tr>
</tbody>
</table>
</div>
<p>结果</p>
<ul>
<li>弱模型沙盒模式平均 <strong>+11.4%</strong>，反超自身纯文本模式；纯文本模式也 <strong>+3~9%</strong>。</li>
<li>强模型继续提升，SWE-bench <strong>+3.0%</strong>，证明“通用数据”不会腐蚀代码能力。</li>
<li>能力调用率：外部资源 <strong>0.8 % → 4.1 %</strong>、文件 <strong>2.9 % → 7.3 %</strong>、计算 <strong>2.9 % → 7.2 %</strong>；平均轮数 <strong>23.7 → 7.0</strong>（不再“闲逛”）。</li>
</ul>
<p>3 系统级部署实验（§4）</p>
<p>目的：回答“沙盒能否在实际生产里低成本跑起来？”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>设置</th>
</tr>
</thead>
<tbody>
<tr>
<td>硬件</td>
<td>单台 NVIDIA DGX（2 TB RAM）</td>
</tr>
<tr>
<td>并发</td>
<td>64 查询并行，采样同规模任务</td>
</tr>
<tr>
<td>引擎</td>
<td>DeepSeek &amp; Kimi @ SGLang；MiniMax &amp; Qwen @ vLLM</td>
</tr>
<tr>
<td>指标</td>
<td>Token 总量、环境 token 占比、端到端 QPM、存储/内存占用</td>
</tr>
</tbody>
</table>
</div>
<p>结果</p>
<ul>
<li>长文本任务 <strong>token 节省最高 8×</strong>（100 K → 13 K）；综合全部任务 <strong>0.5–0.8×</strong>。</li>
<li>环境 token 占 <strong>37–51 %</strong>，但执行时间 <strong>&lt; 4 %</strong>；QPM 与纯文本相比 <strong>0.6–2.2×</strong>，基本持平。</li>
<li>存储：单镜像 <strong>1.1 GB</strong>（vs SWE-Gym <strong>6 TB</strong>）；内存：512 并发容器仅占 <strong>5 %</strong> 系统 RAM。</li>
</ul>
<p>4 超出文本能力演示（§5）</p>
<ul>
<li>跨模态：安装 Leaflet → 生成可点击 <strong>tokyo-map.html</strong>；</li>
<li>文件级输出：安装 CairoSVG → 输出 <strong>poster.png</strong>；</li>
<li>自主工具链：安装 moviepy+PIL → 渲染 <strong>birthday.mp4</strong>；</li>
<li>音乐合成：安装 midiutil+FluidSynth → 输出 <strong>composition.mid + preview.wav</strong>。</li>
</ul>
<p>以上实验共同证明：<br><strong>“零样本即可用计算机 → 通用数据即可学会探索 → 生产环境即可低成本部署 → 文本之外即可生成真实文件。”</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可直接在 LLM-in-Sandbox 框架上延伸，无需推翻现有设计，分为“能力-效率-评测-理论”四条线，按可落地难度排序。</p>
<p>1 能力扩展</p>
<ul>
<li><p><strong>多模态原生环境</strong><br>在镜像中预置 ImageMagick、FFmpeg、Blender、OpenCV 等 CLI，让模型一次 <code>apt-get</code> 即可生成/编辑 3D 模型、点云、医学 DICOM，验证“零样本视觉生成”极限。</p>
</li>
<li><p><strong>分布式沙盒集群</strong><br>把 <code>execute_bash</code> 扩展为 <code>execute_slurm</code>，模型可自动申请 GPU/CPU 节点、调度 MPI 或 Ray，解决“单容器算力不足”的数值模拟、大尺度 ML 训练任务。</p>
</li>
<li><p><strong>长期记忆与检查点</strong><br>提供 <code>/testbed/memory/</code> 卷，允许模型 <code>sqlite3</code> 或 <code>git commit</code> 把中间结果、失败日志、超参数序列化；下一轮任务先 <code>git log</code> 自动加载历史，实现“跨会话持续优化”。</p>
</li>
</ul>
<p>2 效率优化</p>
<ul>
<li><p><strong>沙盒热池 + 快照复用</strong><br>对热门依赖（numpy、rdkit、biopython）做 <code>docker commit</code> 快照，模型 <code>pip install</code> 前先检测本地镜像层，秒级启动；减少 70 % 冷启动时间。</p>
</li>
<li><p><strong>环境 token 预缓存</strong><br>把 <code>python -c &quot;import pandas as pd; print(pd.__version__)&quot;</code> 这类高频命令输出预生成 lookup table，用前缀哈希直接命中，避免真实执行，降低 20 % 总延迟。</p>
</li>
<li><p><strong>异构 offload</strong><br>长文本任务把 &gt;50 KB 的 <code>grep</code> 结果流式映射到 host 的 Rust 正则引擎，返回行号列表而非原始文本，单查询再省 30 % 流量。</p>
</li>
</ul>
<p>3 评测与对齐</p>
<ul>
<li><p><strong>Sandbox-Native 基准</strong><br>构建「探索难度」可控的 3 级任务：<br>① 单文件答案可直接读取；② 答案分散在 10 个 distractor；③ 需先 <code>pip install</code> 专有工具再计算。用 ∆ = 沙盒 − 纯文本 作为“通用智能体分数”，取代静态榜。</p>
</li>
<li><p><strong>可验证安全性竞赛</strong><br>设计“恶意库”陷阱（pip 包装木马、curl 下载挖矿脚本），模型必须 <code>sandbox --net=none</code> 自检哈希或 <code>gpg verify</code>；衡量“自主安全合规”能力。</p>
</li>
<li><p><strong>人机协同接口</strong><br>允许用户在浏览器实时 VNC 进入同一容器，中断、回滚、注释模型命令；收集人类纠正数据，反哺 RLHF，降低“胡来”概率。</p>
</li>
</ul>
<p>4 理论与训练</p>
<ul>
<li><p><strong>沙盒内课程强化学习</strong><br>从“单步即答”到「需 5 轮安装 + 10 轮迭代」逐步提升难度，用课程式 reward shaping 观察是否出现“aha moment”——能力突跃曲线，验证“工具使用涌现”假说。</p>
</li>
<li><p><strong>可解释探索策略</strong><br>把模型内部隐状态投影到“工具空间”向量，检验何时激活“应安装库”或“应写脚本”决策，结合因果干预（ablate 特定注意力头）定位“元工具”神经元。</p>
</li>
<li><p><strong>预训练阶段注入沙盒语料</strong><br>在继续预训练阶段插入 5 % 的“bash + Python 交互日志”文本，观察是否比纯代码语料（GitHub）更能提升下游沙盒任务，验证“环境交互语言”是否成为新型数据尺度。</p>
</li>
</ul>
<p>5 社会技术系统</p>
<ul>
<li><p><strong>经济模型</strong><br>按“token 节省量”向用户返利，按“算力租赁”向开发者收费，形成“越聪明越省钱”的市场机制，推动社区贡献更优镜像与工具包。</p>
</li>
<li><p><strong>法规与隐私</strong><br>研究 GDPR 下“沙盒内产生个人数据”如何自动 <code>shred</code> 或加密，模型需学会 <code>age</code> 加密后再写盘，实现“遗忘权”一键合规。</p>
</li>
</ul>
<p>以上任意一点均可直接基于已开源的 Python 包迭代，无需等待新硬件或闭源模型，即可在 3–6 个月内产出可度量、可发表的新结果。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>一句话总结</strong><br>给大模型一台“虚拟机”，它就能用写代码的方式解决数学、化学、长文本等非代码任务；再配一段“通用数据+结果奖励”的强化学习，弱模型也能学会探索，最终把 LLM 从“文本生成器”升级为“通用数字劳工”。</p>
<p><strong>核心内容速览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>关键做法</th>
<th>主要结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>环境</td>
<td>1.1 GB Ubuntu 容器，三接口：bash / 文件编辑 / submit</td>
<td>单镜像千任务，存储降 4 个量级</td>
</tr>
<tr>
<td>零样本推理</td>
<td>强模型直接在沙盒里多轮安装-计算-查文件</td>
<td>六大领域平均 +6.9%，最高 +24.2%</td>
</tr>
<tr>
<td>弱模型困境</td>
<td>4B 模型“闲逛”23 轮仍失败</td>
<td>沙盒模式反而 −5%</td>
</tr>
<tr>
<td>训练方案</td>
<td>LLM-in-Sandbox-RL：通用上下文任务→文件放沙盒→结果奖励</td>
<td>弱模型沙盒 +11.4%，纯文本也 +3~9%</td>
</tr>
<tr>
<td>系统开销</td>
<td>长文本 token 省 8×；环境执行时间 &lt;4%；512 并发内存占 5%</td>
<td>生产级吞吐与成本</td>
</tr>
<tr>
<td>超越文本</td>
<td>自动装 Leaflet/CairoSVG/moviepy，输出可用 .html .png .mp4 .wav</td>
<td>首次展示“文本进、文件出”</td>
</tr>
</tbody>
</table>
</div>
<p><strong>贡献三句话</strong></p>
<ol>
<li>首次证明“代码沙盒”无需额外训练即可泛化到非代码任务。</li>
<li>提出用通用非智能体数据做沙盒内 RL，让弱模型获得探索与推理双迁移。</li>
<li>开源轻量级包，长文本场景 token 节省 8×，为 LLM 部署提供统一基础设施。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是LLM-in-Sandbox%20Elicits%20General%20Agentic%20Intelligence，链接是https://arxiv.org/pdf/2601.16206，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16206。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Daixuan Cheng,Shaohan Huang,Yuxian Gu,Huatong Song,Guoxin Chen,Li Dong,Wayne Xin Zhao,Ji-Rong Wen,Furu Wei</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16206.pdf">https://arxiv.org/pdf/2601.16206.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16206">https://arxiv.org/abs/2601.16206</a></p>
<p><strong>Arxiv ID</strong>: 2601.16206</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16206">https://papers.cool/arxiv/2601.16206</a></p>
<p><strong>Published</strong>: 2026-01-22T18:57:09Z</p>
<p><strong>Updated</strong>: 2026-01-22T18:57:09.000Z</p>
<hr>
<h3 id="5-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries"><a href="#5-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries" class="headerlink" title="5. BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15197">BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</a></h3><p>Vision-Language-Action (VLA) models have shown promise in robot manipulation but often struggle to generalize to new instructions or complex multi-task scenarios. We identify a critical pathology in current training paradigms where goal-driven data collection creates a dataset bias. In such datasets, language instructions are highly predictable from visual observations alone, causing the conditional mutual information between instructions and actions to vanish, a phenomenon we term Information Collapse. Consequently, models degenerate into vision-only policies that ignore language constraints and fail in out-of-distribution (OOD) settings. To address this, we propose BayesianVLA, a novel framework that enforces instruction following via Bayesian decomposition. By introducing learnable Latent Action Queries, we construct a dual-branch architecture to estimate both a vision-only prior $p(a \mid v)$ and a language-conditioned posterior $π(a \mid v, \ell)$. We then optimize the policy to maximize the conditional Pointwise Mutual Information (PMI) between actions and instructions. This objective effectively penalizes the vision shortcut and rewards actions that explicitly explain the language command. Without requiring new data, BayesianVLA significantly improves generalization. Extensive experiments across on SimplerEnv and RoboCasa demonstrate substantial gains, including an 11.3% improvement on the challenging OOD SimplerEnv benchmark, validating the ability of our approach to robustly ground language in action.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视觉-语言-行动（VLA）模型在机器人作方面展现出潜力，但常常难以推广到新指令或复杂的多任务场景。我们识别出当前训练范式中一个关键病理现象，目标驱动的数据收集会产生数据集偏倚。在此类数据集中，语言指令仅凭视觉观察高度可预测，导致指令与动作之间的条件互信息消失，我们称之为信息崩溃现象。因此，模型退化为仅视野的策略，忽视语言限制，并在分发外（OOD）环境中失效。为此，我们提出了贝叶斯VLA，一种通过贝叶斯分解强制执行指令跟随的新框架。通过引入可学习的潜在动作查询，我们构建了一个双分支架构，以估计仅视觉的先验$p（a \mid v）$和语言条件后的后验 $π（a \mid v， \ell）$。然后我们优化策略，最大化动作和指令之间的条件点互信息（PMI）。该目标实际上惩罚了视觉捷径，并奖励明确解释语言命令的行为。无需新数据，贝叶斯VLA显著提升了泛化性。在SimplerEnv和RoboCasa上的广泛实验显示出显著提升，包括较难度较高的OOD SimplerEnv基准提升11.3%，验证了我们方法在稳健基础语言上的实际应用能力。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决 Vision-Language-Action（VLA）模型在“目标导向”数据集中出现的 <strong>Information Collapse（信息坍缩）</strong> 问题，具体表现为：</p>
<ul>
<li>由于视觉场景 v 与语言指令 ℓ 在数据集中几乎一一对应，模型学会仅依赖视觉先验  p(a|v)  即可完成任务，从而忽略语言约束，退化为“视觉捷径”策略；</li>
<li>当测试环境出现任务歧义或分布外（OOD）场景时，这种策略失效，导致泛化性能骤降。</li>
</ul>
<p>为此，作者提出 <strong>BayesianVLA</strong> 框架，通过贝叶斯分解显式最大化动作与语言之间的 <strong>条件点态互信息（PMI）</strong>，强制模型利用语言指令消除视觉歧义，从而在无需新增数据的前提下显著提升泛化能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 §6 “Related Work” 中系统梳理了与本研究直接相关的三大脉络，并指出它们与“视觉捷径”问题的关联与差异。按主题归纳如下：</p>
<p>1. 大规模 VLA 数据集与评测基准</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代表工作</th>
<th>核心贡献</th>
<th>与本文关联</th>
</tr>
</thead>
<tbody>
<tr>
<td>LIBERO (Liu et al., 2023)</td>
<td>首个系统研究终身知识迁移的机器人基准</td>
<td>其 Goal 子集被本文用作“任务歧义”实验场景，揭示视觉捷径失效</td>
</tr>
<tr>
<td>BridgeData V2 (Walke et al., 2023)</td>
<td>低成本硬件采集的 6 万+ 轨迹，强调多样性</td>
<td>被本文用作训练集，实验显示模型仍能利用背景/光照等视觉捷径</td>
</tr>
<tr>
<td>Open X-Embodiment (O’Neill et al., 2024)</td>
<td>聚合 22 种机器人形态、100 万+ 轨迹</td>
<td>本文在 SimplerEnv 上评测时采用其 Fractal 子集</td>
</tr>
<tr>
<td>RoboCasa (Nasiriany et al., 2024)</td>
<td>大规模厨房仿真环境，24 项桌面操作任务</td>
<td>本文主要评测基准之一，验证 BayesianVLA 对视觉捷径的抑制</td>
</tr>
<tr>
<td>SimplerEnv (Li et al., 2024c)</td>
<td>轻量级仿真代理，用于快速 OOD 评估</td>
<td>本文 OOD 实验核心平台，报告 11.3% 绝对增益</td>
</tr>
<tr>
<td>RoboTwin 2.0 (Chen et al., 2025)</td>
<td>双手操作基准，支持自动域随机化</td>
<td>已列入未来扩展实验</td>
</tr>
</tbody>
</table>
</div>
<p>2. Vision-Language-Action 模型架构</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法流派</th>
<th>代表工作</th>
<th>关键机制</th>
<th>与视觉捷径的关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>VLM 微调派</td>
<td>OpenVLA (Kim et al., 2024)OpenVLA-OFT (Kim et al., 2025)</td>
<td>直接微调 LLM 得到 π(a</td>
<td>v,ℓ)</td>
</tr>
<tr>
<td>扩散策略派</td>
<td>π0/π0.5 (Black et al., 2024; 2025)RDT-1B (Liu et al., 2025)</td>
<td>Flow-Matching 扩散头</td>
<td>损失函数仅拟合动作，未惩罚视觉捷径</td>
</tr>
<tr>
<td>双系统派</td>
<td>GR00T N1/N1.5/N1.6 (Bjorck et al., 2025)CogACT (Li et al., 2024a)</td>
<td>VLM 感知 + DiT 控制</td>
<td>动作分支仍可能忽略语言，需额外正则化</td>
</tr>
<tr>
<td>提示微调派</td>
<td>X-VLA (Zheng et al., 2025a)</td>
<td>embodiment-specific soft prompt</td>
<td>仅缓解跨形态迁移，未解决信息坍缩</td>
</tr>
<tr>
<td>时空建模派</td>
<td>VideoVLA (Shen et al., 2025)SpatialVLA (Qu et al., 2025)</td>
<td>引入视频生成或 3D 位置编码</td>
<td>增强空间理解，但未显式最大化 PMI</td>
</tr>
</tbody>
</table>
</div>
<p><strong>共同点</strong>：上述方法均侧重“规模”或“架构”，<strong>未在目标函数层面阻止 π(a|v,ℓ)→p(a|v)</strong>。<br><strong>差异点</strong>：BayesianVLA 首次把 <strong>LLR/PMI 最大化</strong> 作为训练目标，用双分支显式估计并惩罚视觉先验。</p>
<p>3. 信息论与贝叶斯策略分解</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>思想</th>
<th>与本文对比</th>
</tr>
</thead>
<tbody>
<tr>
<td>Xu et al., 2025</td>
<td>提出贝叶斯因子分解 p(a</td>
<td>v,ℓ) ∝ p(ℓ</td>
</tr>
<tr>
<td>World-Model VLAF1VLA (Lv et al., 2025)Mantis (Yang et al., 2025)</td>
<td>把未来帧 vt+1 当潜变量，做反动力学推断</td>
<td>等价于对 p(vt+1</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<ul>
<li><strong>数据集/基准</strong>：提供实验土壤，但本身无法消除视觉捷径。</li>
<li><strong>现有 VLA 架构</strong>：聚焦“如何更好拟合动作”，未在损失函数层面抑制信息坍缩。</li>
<li><strong>信息论视角</strong>：BayesianVLA 首次将 <strong>PMI 最大化</strong> 嵌入端到端训练，与既有架构正交，可直接叠加。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“视觉捷径”问题形式化为 <strong>Information Collapse</strong>——即条件互信息</p>
<p>I(ell; a|v) = H(ell|v) - H(ell|a,v)</p>
<p>在目标导向数据集中趋近于 0，导致策略退化为 π(a|v,ℓ)≈p(a|v)。为打破这一退化，作者提出 <strong>BayesianVLA</strong>，核心思路是：</p>
<ol>
<li>显式估计并对比“视觉先验”与“语言-视觉后验”；</li>
<li>用 <strong>Log-Likelihood Ratio（LLR）</strong> 作为训练目标，最大化动作对语言的额外解释力；</li>
<li>通过 <strong>Latent Action Queries</strong> 在单一 VLM 内实现双分支复用，不增加推理开销。</li>
</ol>
<p>以下分三步阐述具体实现。</p>
<p>1. 贝叶斯分解：把策略拆成“先验”与“似然”</p>
<p>根据贝叶斯规则</p>
<p>π(a|v,ell) = (p(ell|a,v),p(a|v)) / (p(ell|v))</p>
<ul>
<li><strong>p(a|v)</strong>：视觉先验，数据集偏见的主要来源；</li>
<li><strong>p(ℓ|a,v)</strong>：动作对语言的解释力，理想情况下应大于 p(ℓ|v)。</li>
</ul>
<p>若直接最大化似然 π(a|v,ℓ)，模型可偷懒只拟合 p(a|v)。因此转而最大化 <strong>LLR</strong></p>
<p>L_(LLR) = log(π(a|v,ell)) / (p(a|v)) = log p(ell|a,v) - log p(ell|v)</p>
<p>该目标迫使动作 a 提供<strong>仅通过 v 无法获得的语言信息</strong>，等价于最大化 PMI(a,ℓ|v)。</p>
<p>2. Latent Action Queries：用 64 个可学习 token 实现双分支</p>
<p>在统一 VLM（Qwen3-VL）输入序列中插入 <strong>K=64</strong> 个特殊 token</p>
<p>Q = &lt;!|action_0!|&gt;, dots, &lt;!|action_K!|&gt;</p>
<p>利用 decoder-only 因果掩码，通过改变 Q 的位置即可<strong>精确控制</strong>其可见上下文：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>分支</th>
<th>输入序列</th>
<th>Q 可见内容</th>
<th>输出含义</th>
</tr>
</thead>
<tbody>
<tr>
<td>Priori</td>
<td>[v, Q, ℓ]</td>
<td>仅 v</td>
<td>H_Q^(prior)  编码 p(a</td>
</tr>
<tr>
<td>Posteriori</td>
<td>[v, ℓ, Q]</td>
<td>v + ℓ</td>
<td>H_Q^(post)  编码 π(a</td>
</tr>
</tbody>
</table>
</div>
<p>两分支<strong>共享 VLM 权重</strong>，仅 Q 的隐藏状态被送入下游 Diffusion Transformer（DiT）生成连续动作，其余 token 被丢弃，从而：</p>
<ul>
<li>把 VLM 长度复杂度  O(N^2)  降为  O(K^2) ；</li>
<li>保证推理阶段<strong>只执行 Posteriori 分支</strong>，零额外计算。</li>
</ul>
<p>3. 训练目标：联合优化动作拟合与 LLR</p>
<p>总损失</p>
<p>L<em>(total) = (1-λ)L</em>(FM)(psi; H<em>Q^(post)) + λL</em>(FM)(psi; H<em>Q^(prior)) - β,L</em>(LLR)</p>
<ul>
<li><strong> L_(FM) </strong>：Rectified Flow-Matching 损失，让 DiT 回归专家动作；</li>
<li><strong> L_(LLR) </strong> 实现：</li>
<li>用 VLM 语言建模损失近似  log p(ell|·) ；</li>
<li>Priori 分支得到  log p(ell|v,H_Q^(prior)) ；</li>
<li>对 Posteriori 分支的  log p(ell|v)  施加 <strong>stop-gradient</strong>，防止模型通过降低分母来“作弊”；</li>
<li>最大化二者差值，即 $L_(LLR) = log p(ell|v,H_Q^(prior)) - sgl<br>log p(ell|v)r<br>$。</li>
</ul>
<p>该目标在反向传播时<strong>只更新 Priori 分支与 DiT</strong>，迫使  H_Q^(prior)  携带能解释 ℓ 的信息，进而把这份信息迁移到  H_Q^(post) ，实现语言-动作强耦合。</p>
<p>结果摘要</p>
<ul>
<li><strong>SimplerEnv OOD 基准</strong>：绝对提升 11.3%，从 55.2%→66.5%；</li>
<li><strong>RoboCasa</strong>：平均成功率 50.4%，超越最强基线；</li>
<li><strong>纯文本对话能力</strong>：baseline 出现灾难性遗忘，BayesianVLA 仍保持流畅回答（图 4-5）；</li>
<li><strong>消融实验</strong>：仅加 Queries 涨 2.3%，再加 LLR 涨 6.0%，验证“贝叶斯目标”是主要增益来源。</li>
</ul>
<p>通过上述三管齐下，BayesianVLA <strong>无需额外数据</strong>即可打破视觉捷径，实现鲁棒的语言 grounding 与 OOD 泛化。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 §4 与附录中报告了 <strong>4 组共 7 项实验</strong>，覆盖仿真基准、OOD 泛化、能力保持与消融分析，具体如下：</p>
<p>1. 主实验：两大仿真基准全面评测</p>
<p>1.1 SimplerEnv（表 1）</p>
<ul>
<li><strong>数据</strong>：BridgeDataV2 + Fractal（OXE 子集），40 k 步微调</li>
<li><strong>任务</strong>：4 项桌面操作（Put spoon on towel 等）</li>
<li><strong>指标</strong>：Avg@480（480 次独立 rollout 平均成功率）</li>
<li><strong>结果</strong>：</li>
<li>BayesianVLA 平均 <strong>66.5%</strong>，较同骨架基线 QwenGR00T（55.2%）↑11.3%，<strong>绝对 SOTA</strong>；</li>
<li>最难任务 “Put eggplant in yellow basket” 从 64.2%→79.2%，↑25.0%。</li>
</ul>
<p>1.2 RoboCasa-Tabletop（表 2）</p>
<ul>
<li><strong>数据</strong>：PhysicalAI-Robotics-GR00T-X-Embodiment-Sim 人形桌面子集</li>
<li><strong>任务</strong>：24 项 PnP 与关节对象操作（含微波炉、抽屉等）</li>
<li><strong>指标</strong>：Avg@50</li>
<li><strong>结果</strong>：</li>
<li>BayesianVLA 平均 <strong>50.4%</strong>，超越次佳 OpenVLA-OFT（48.8%）；</li>
<li>视觉捷径重灾区 “PnP Novel From Placemat To Plate”：VisionOnly 34% → 70%（+36%）。</li>
</ul>
<p>2. 诊断实验：验证“视觉捷径”假设</p>
<p>2.1 RoboCasa-ID 测试（§2.1）</p>
<ul>
<li>训练与测试场景高度重叠；</li>
<li><strong>Vision-Only 输入</strong>即可拿到 44.6%，与完整模型 47.8% 几乎持平，<strong>首次定量证实捷径存在</strong>。</li>
</ul>
<p>2.2 LIBERO-Goal 歧义场景（§2.2）</p>
<ul>
<li>同一视觉配置对应多条有效指令；</li>
<li>Vision-Only 成功率从 95% 级骤降至 <strong>12.4%</strong>，说明捷径在歧义下失效。</li>
</ul>
<p>2.3 Bridge→SimplerEnv OOD 迁移（§2.3）</p>
<ul>
<li>训练集多样（in-the-wild），但 Vision-Only 仍在 OOD 仿真环境跌至 <strong>≈0%</strong>；</li>
<li>训练损失与完整模型相近（0.13 vs 0.08），揭示模型靠背景/光照等<strong>域相关视觉特征</strong>拟合，而非真正理解语言。</li>
</ul>
<p>3. 能力保持实验（§4.3 + 图 4-5）</p>
<ul>
<li><strong>纯文本问答</strong>：</li>
<li>基线 QwenGR00T 出现<strong>灾难性遗忘</strong>，输出重复乱码；</li>
<li>BayesianVLA 仍给出正确、连贯的数学解答与 LLM 定义，<strong>文本对话能力得以保持</strong>。</li>
<li><strong>视觉-文本对话</strong>：作者指出仍可能退化，因视觉塔需适应控制，但<strong>文本侧参数受 LLR 正则保护</strong>，退化显著轻于基线。</li>
</ul>
<p>4. 消融实验（表 3）</p>
<ul>
<li><strong>QwenGR00T</strong> → 55.2%</li>
<li><strong>+ Action Query 仅架构</strong> → 57.5%（+2.3%）</li>
<li><strong>完整 BayesianVLA（+LLR）</strong> → 63.5%（+6.0%）<br>结论：<strong>贝叶斯 LLR 目标是性能跃升的主因</strong>，Queries 本身仅提供归纳偏置与计算降维。</li>
</ul>
<p>5. 统计与可重复性</p>
<ul>
<li>所有仿真结果均报告 <strong>≥50–480 次独立随机种子 rollout</strong>；</li>
<li>训练协议完全复用 StarVLA 默认超参（lr=1e-5，cosine，AdamW，DeepSpeed ZeRO-2），确保公平对比；</li>
<li>代码与模型已开源（GitHub 链接见首页），供社区复现。</li>
</ul>
<p>总结</p>
<p>实验链条完整覆盖 <strong>“现象诊断 → 方法验证 → 能力保持 → 组件消融”</strong> 四环节，既用数字证明 BayesianVLA 在 OOD 场景下的 <strong>11.3% 绝对增益</strong>，又用定性示例展示其对基础 VLM 语言能力的<strong>零遗忘保留</strong>，为“视觉捷径”问题提供了端到端的解决方案与实证支撑。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下展望均直接源于论文 §5「Discussion」与 §Limitation 的未竟之处，可作为后续工作的切入点：</p>
<p>1. 数据层面：主动制造“语言熵”</p>
<ul>
<li><strong>目标</strong>：打破 H(ℓ|v)≈0 的确定性收集范式</li>
<li><strong>可行路线</strong></li>
<li>在同一视觉场景下<strong>人为录制多条互斥指令</strong>（如“把碗放进抽屉” vs “把碗放到炉灶”），提升条件熵；</li>
<li>引入<strong>人类日常 egocentric 视频</strong>（HRDT、METIS、PhysBrain 等），利用其天然多义性稀释机器人数据集的确定性映射；</li>
<li>设计<strong>主动采集算法</strong>：先让策略模型在场景里采样多种可能任务，再让标注员补充缺失指令，实现“任务-语言”在线增广。</li>
</ul>
<p>2. 模型层面：把 BayesianVLA 做成“即插即用”套件</p>
<ul>
<li><strong>更大骨架</strong>：将双分支训练从 4B 扩展到 <strong>8B–70B</strong> 级别，验证 PMI 目标在超大模型上的稳定性与 Scaling Law；</li>
<li><strong>跨 embodiment 迁移</strong>：结合 X-VLA 的 soft-prompt 思想，为不同机器人形态维护<strong>共享的 Latent Action Queries</strong>，仅新增形态特定投影矩阵；</li>
<li><strong>与 World Model 融合</strong>：利用视频生成式世界模型（F1VLA、Mantis）作为 p(v_{t+1}|a,v≤t,ℓ)，把 LLR 目标升级为</li>
</ul>
<p>logp(v<em>(t+1)|a,v</em>(le t),ell)p(v<em>(t+1)|v</em>(le t),ell)</p>
<p>实现“未来帧解释力”最大化，提供另一条防止信息坍缩的技术路径。</p>
<p>3. 训练策略：降低双分支开销 &amp; 在线蒸馏</p>
<ul>
<li><strong>Prefix-cache + 梯度检查点</strong>：视觉 tokens 在两分支间 100% 复用，已验证训练时间增加 &lt;8%，可继续优化至 &lt;3%；</li>
<li><strong>单分支蒸馏</strong>：训练完成后，用 Posteriori 分支当教师，把 LLR 知识蒸馏回<strong>标准单分支</strong>，实现“零开销”部署；</li>
<li><strong>课程式 β 调度</strong>：初期 β=0 先学动作，后期逐步增大 β，避免一开始 PMI 目标过强导致动作拟合不足。</li>
</ul>
<p>4. 评测层面：建立“语言必要性”指标</p>
<ul>
<li><strong>指标定义</strong>：</li>
</ul>
<p>Lang-Need = Succ<em>(full) - Succ</em>(vision-only)1 - Succ_(vision-only)</p>
<p>量化“语言带来的边际收益”，方便未来方法横向对比；</p>
<ul>
<li><strong>Benchmark 扩展</strong>：尽快放出 RoboTwin 2.0 与 LIBERO 完整结果（作者已承诺），并引入<strong>双手协作、对话式多轮指令</strong>等更高阶场景，检验 PMI 目标在长程语义依赖下的鲁棒性。</li>
</ul>
<p>5. 理论层面：信息论视角的深入</p>
<ul>
<li><strong>CMI 下界估计</strong>：在训练过程中在线估计 I(ℓ;a|v)，观察其是否与最终 OOD 性能呈正相关，建立<strong>可解释监控量</strong>；</li>
<li><strong>因果干预分析</strong>：用 do-calculus 验证最大化 PMI 是否等价于阻断“v→ℓ”混淆路径，从而<strong>因果图视角</strong>证明 LLR 目标的合理性；</li>
<li><strong>与对比学习统一</strong>：将 LLR 目标视为动作-指令的<strong>对比 InfoNCE 损失</strong>，探索温度系数、负样本构造等技巧能否进一步提升效率。</li>
</ul>
<p>6. 真实世界落地：闭环验证</p>
<ul>
<li><strong>轻量级部署</strong>：在 1-2 台 WidowX 或 xArm 上运行 BayesianVLA-4B，对比 baseline 的<strong>真实 OOD 成功率</strong>（新背景、新物体、新指令）；</li>
<li><strong>在线适应</strong>：结合最近提出的<strong>机器人 RLHF</strong> 框架，用人类纠正信号微调 Latent Action Queries，验证 PMI 目标是否<strong>降低人类标注量</strong>；</li>
<li><strong>边缘计算优化</strong>：将 Queries 与 DiT 量化至 8-bit，测量在 Jetson Orin 上的推理延迟，目标 &lt;50 ms，满足实时控制。</li>
</ul>
<p>一句话总结</p>
<p>未来工作可从**“数据熵增”、“模型 Scaling”、“World-Model 融合”、“语言必要性指标”、“因果理论”与“真实闭环”<strong>六条主线展开，把 BayesianVLA 从“仿真利器”升级为</strong>可部署、可解释、可扩展**的通用机器人语言 grounding 范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心内容可概括为“<strong>一个问题、一个理论、一个方法、一套实验</strong>”：</p>
<p>1. 问题：Information Collapse（信息坍缩）</p>
<ul>
<li>目标导向数据集中视觉场景 v 与语言指令 ℓ 几乎一一对应，导致</li>
</ul>
<p>H(ℓ|v)≈0 ⇒ I(ℓ;a|v)≈0</p>
<ul>
<li>模型学会视觉捷径 π(a|v,ℓ)≈p(a|v)，<strong>在分布外或歧义场景失效</strong>。</li>
</ul>
<p>2. 理论：贝叶斯分解 + PMI 最大化</p>
<ul>
<li>将最优策略写成</li>
</ul>
<p>π(a|v,ℓ)=(p(ℓ|a,v),p(a|v)) / (p(ℓ|v))</p>
<ul>
<li>最大化 <strong>Log-Likelihood Ratio</strong></li>
</ul>
<p>log(π(a|v,ℓ)) / (p(a|v))=log p(ℓ|a,v)−log p(ℓ|v)</p>
<p>等价于最大化动作-语言 <strong>点态互信息</strong>，强制动作提供“视觉之外”的指令信息。</p>
<p>3. 方法：BayesianVLA</p>
<ul>
<li><strong>Latent Action Queries</strong>：64 个可学习 token，作为 VLM→DiT 的瓶颈接口。</li>
<li><strong>双分支共享权重</strong>：</li>
<li>Priori 输入<br>v,Q,ℓ<br>→ 仅见 v，估计 p(a|v)；</li>
<li>Posteriori 输入<br>v,ℓ,Q<br>→ 见 v+ℓ，估计 π(a|v,ℓ)。</li>
<li><strong>训练目标</strong></li>
</ul>
<p>L<em>(total)=(1−λ)L</em>(FM)^(post)+λL<em>(FM)^(prior)−β,L</em>(LLR)</p>
<p>推理仅跑 Posteriori，<strong>零额外计算</strong>。</p>
<p>4. 实验结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>基线</th>
<th>BayesianVLA</th>
<th>绝对增益</th>
</tr>
</thead>
<tbody>
<tr>
<td>SimplerEnv OOD</td>
<td>55.2%</td>
<td>66.5%</td>
<td>+11.3%</td>
</tr>
<tr>
<td>RoboCasa</td>
<td>47.8%</td>
<td>50.4%</td>
<td>+2.6%</td>
</tr>
<tr>
<td>语言必要性指标</td>
<td>多任务↑，Vision-Only 失效任务↑</td>
</tr>
<tr>
<td>纯文本对话能力</td>
<td>基线灾难遗忘</td>
<td>完全保持</td>
<td>—</td>
</tr>
</tbody>
</table>
</div>
<p>消融显示：<strong>LLR 目标贡献 6.0%</strong>，Queries 架构贡献 2.3%。</p>
<p>一句话总结</p>
<p>BayesianVLA 用“<strong>贝叶斯分解 + PMI 最大化</strong>”在训练阶段直接惩罚视觉捷径，无需新数据即可让 VLA 模型<strong>真正听懂指令</strong>，在分布外仿真环境取得 <strong>11% 级绝对提升</strong>并保留原 VLM 语言能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是BayesianVLA%3A%20Bayesian%20Decomposition%20of%20Vision%20Language%20Action%20Models%20via%20Latent%20Action%20Queries，链接是https://arxiv.org/pdf/2601.15197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15197。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shijie Lian,Bin Yu,Xiaopeng Lin,Laurence T. Yang,Zhaolong Shen,Changti Wu,Yuzhuo Miao,Cong Huang,Kai Chen</p>
<p><strong>Categories</strong>: cs.AI,cs.CL,cs.CV,cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15197.pdf">https://arxiv.org/pdf/2601.15197.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15197">https://arxiv.org/abs/2601.15197</a></p>
<p><strong>Arxiv ID</strong>: 2601.15197</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15197">https://papers.cool/arxiv/2601.15197</a></p>
<p><strong>Published</strong>: 2026-01-21T17:15:22Z</p>
<p><strong>Updated</strong>: 2026-01-21T17:15:22.000Z</p>
<hr>
<h3 id="6-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders"><a href="#6-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders" class="headerlink" title="6. Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16208">Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</a></h3><p>Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>表示自编码器（RAEs）在 ImageNet 的扩散建模中展示了明显优势，通过在高维语义潜在空间中进行训练。在本研究中，我们探讨该框架是否可以扩展到大规模、自由形式的文本到图像（T2I）生成。我们首先在冻结的表示编码器（SigLIP-2）上扩展 RAE 解码器，超越 ImageNet，通过在网络、合成和文本渲染数据上训练，发现虽然扩大规模可以提高整体生成的保真度，但对于特定领域（如文本），目标数据构成至关重要。随后，我们严格地对 RAE 在 ImageNet 上最初提出的设计选择进行压力测试。分析显示，规模化简化了整个框架：虽然与维度相关的噪声调度仍然至关重要，但诸如宽扩散头和噪声增强解码等架构复杂度在大规模情况下几乎没有收益。在这一简化框架的基础上，我们在扩散变换器从 0.5B 到 9.8B 参数的各种规模下，进行了 RAE 与最先进的 FLUX VAE 的对照实验。在所有模型规模下，RAE 在预训练阶段始终优于 VAE。此外，在高质量数据集上的微调过程中，基于 VAE 的模型在训练 64 个周期后出现灾难性过拟合，而 RAE 模型在 256 个周期内保持稳定，并且表现始终更优。在所有实验中，基于 RAE 的扩散模型表现出更快的收敛速度和更好的生成质量，确立了 RAE 作为大规模 T2I 生成比 VAE 更简单且更强大的基础的地位。此外，由于视觉理解和生成可以在共享的表示空间中操作，多模态模型可以直接对生成的潜在表示进行推理，为统一模型开辟了新的可能性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文核心关注的问题是：<strong>能否将 Representation Autoencoder（RAE）这一在 ImageNet 上表现优异的“高维语义潜空间扩散”框架，真正扩展到大规模、自由形式的文本到图像（T2-I）生成场景？</strong></p>
<p>具体而言，作者围绕以下三个关键子问题展开系统研究：</p>
<ol>
<li><p><strong>解码器能否突破 ImageNet 限制？</strong><br>仅在大规模网络、合成与文本渲染数据上训练 RAE 解码器，验证其重建质量与跨域泛化能力。</p>
</li>
<li><p><strong>RAE 的专用设计是否仍必要？</strong><br>在 T2-I 十亿参数规模下重新审视维度相关噪声调度、宽扩散头（DiTDH）、噪声增强解码等原始设计，明确哪些组件随规模增大而“简化”。</p>
</li>
<li><p><strong>RAE 与 SOTA VAE 的优劣对比？</strong><br>在相同训练管线（预训练 + 高质量微调）下，从 0.5 B 到 9.8 B 参数的 DiT 骨干，全面对比 RAE（SigLIP-2）与 FLUX-VAE 的收敛速度、生成质量与过拟合鲁棒性。</p>
</li>
</ol>
<p>最终结论：<strong>RAE 提供了一种比 VAE 更简单、更强健且收敛更快的大规模 T2-I 基础，并天然支持理解与生成共享同一高维潜空间，为统一多模态模型开辟了新路径。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在“6. Related Work”中系统梳理了相关研究，可归纳为以下四条主线：</p>
<p>1. 视觉表征自编码器（Representation &amp; RAE）</p>
<ul>
<li><strong>VAE 及其压缩极限</strong>：SDXL-VAE、FLUX-VAE 等通过扩大通道、增加数据提升重建，但维度通常 ≤64。</li>
<li><strong>RAE 范式</strong>：Zheng et al. 2025 首次提出“冻结高维 encoder + 可训 decoder”，在 ImageNet 类条件扩散中击败 VAE。</li>
<li><strong>后续变体</strong>：</li>
<li>SVG/VTP：引入残差编码或重建损失辅助预训练。</li>
<li>VQRAE：对高维特征做矢量量化再生成。</li>
<li>联合表征-VAE 方法：将 CLIP/DINO 特征注入 VAE 以提升保真度。</li>
</ul>
<p>2. 文本到图像扩散模型中的 VAE</p>
<ul>
<li><strong>Stable Diffusion 系列</strong>：SD 1.x/2.x、SDXL、Stable Diffusion 3 均采用 VAE 潜空间，后续工作通过加宽通道、引入表征对齐损失继续改进。</li>
<li><strong>Hunyuan-Image-3</strong>：在 VAE 训练中显式加入表征对齐，使生成器兼顾语义与纹理。</li>
</ul>
<p>3. 统一多模态模型（理解+生成同网络）</p>
<ul>
<li><strong>离散 token 方案</strong>：Chameleon、Emu3、Janus-Pro 等将图像离散化，用 next-token prediction 统一模态。</li>
<li><strong>连续潜空间方案</strong>：</li>
<li>Two-tower 结构（Bagel、UniFluid、BLIP-3o）：理解用高维 CLIP，生成用低维 VAE，需额外解码-再编码。</li>
<li>单空间压缩方法（VUGen、UniFlow、Ming-UniVision）：虽共享潜空间，但生成端仍采用显著降维的连续 token，牺牲感知分辨率。</li>
<li><strong>RAE 的差异化</strong>：首次在 <strong>高维、未压缩</strong> 的表征空间内同时完成理解与生成，无需降维或离散化。</li>
</ul>
<p>4. 高维潜空间扩散训练技巧</p>
<ul>
<li><strong>维度感知噪声调度</strong>：RAE 2025 提出对  m = N × d  重新缩放扩散时间步，本文验证其在 T2-I 场景仍不可或缺。</li>
<li><strong>Flow Matching &amp; LightningDiT</strong>：采用线性插值  x_t = (1-t)x + tvarepsilon  及 Transformer 骨干，为大规模 RAE 训练提供稳定优化目标。</li>
</ul>
<p>总结表（markdown 格式）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>研究方向</th>
<th>代表工作</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>VAE 潜空间扩散</td>
<td>SD、SDXL、FLUX、Hunyuan-3</td>
<td>被 RAE 全面超越的基线</td>
</tr>
<tr>
<td>RAE 雏形</td>
<td>Zheng et al. 2025</td>
<td>从 ImageNet 类条件扩展到自由文本生成</td>
</tr>
<tr>
<td>统一多模态</td>
<td>Chameleon、Emu3、BLIP-3o、UniFlow</td>
<td>采用离散或降维连续 token，RAE 首次用高维未压缩特征</td>
</tr>
<tr>
<td>高维扩散技巧</td>
<td>维度调度、Flow Matching、LightningDiT</td>
<td>被继承并验证在 T2-I 十亿参数规模仍关键</td>
</tr>
</tbody>
</table>
</div>
<p>因此，本文在“表征自编码器”与“统一多模态”两条新兴方向的交叉点上，首次完成了面向大规模文本到图像生成的系统性验证与超越。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“三步走”策略，将 Representation Autoencoder（RAE）从 ImageNet 小尺度场景推广到十亿参数级自由文本到图像（T2I）生成，并在每一步给出可复现的实证结论：</p>
<p>1. 解码器先扩域：让 RAE 看得懂“网络图 + 文字图”</p>
<ul>
<li><strong>数据配方</strong><br>在冻结的 SigLIP-2 encoder 上训练 ViT-XL 解码器，数据从 1.28 M ImageNet 扩至 73 M 混合源（Web 39 M + 合成 24.7 M + 渲染文本 9.3 M）。</li>
<li><strong>关键发现</strong></li>
<li>仅扩大通用 Web 数据 → 自然图像 rFID 降，文本细节仍崩。</li>
<li>加入 RenderedText 后，文本渲染 rFID 从 2.64 → 1.62，自然图像不降。<br>⇒ <strong>“数据组成 &gt; 数据规模”</strong>；文本重建必须喂文本域数据。</li>
</ul>
<p>2. 框架再瘦身：十亿参数下哪些设计真有用？</p>
<p>在 1.5 B LLM + 2.4 B DiT 的 T2I 训练管线里逐项“去特技”：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始 RAE 特技</th>
<th>小模型必要？</th>
<th>十亿参数后</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>维度感知噪声调度</td>
<td>√ 必须</td>
<td>49.6 vs 23.6 GenEval</td>
<td>保留</td>
</tr>
<tr>
<td>噪声增强解码</td>
<td>15 k 步前 +2 分</td>
<td>120 k 步后 ≈0</td>
<td>丢弃</td>
</tr>
<tr>
<td>宽头 DiTDH</td>
<td>0.5 B 时 +11.2 分</td>
<td>≥2.4 B 后增益 &lt;1</td>
<td>丢弃</td>
</tr>
</tbody>
</table>
</div>
<p>⇒ <strong>简化配方</strong>：只保留维度调度，其余回归标准 DiT；训练更省算力且不影响收敛。</p>
<p>3. 与 SOTA VAE 硬碰硬：同数据、同算力、同评测</p>
<p>采用两阶段对照（预训练 → 高质量微调），唯一变量是潜空间：</p>
<ul>
<li><strong>预训练</strong></li>
<li>模型网格：LLM {1.5 B, 7 B} × DiT {0.5 B, 2.4 B, 5.5 B, 9.8 B}</li>
<li>指标：GenEval、DPG-Bench</li>
<li>结果：RAE 在所有规模上 <strong>收敛快 4–5×</strong>；同参数下 GenEval 绝对提升 +4~6 分。</li>
<li><strong>微调（BLIP-3o 60 k，4–256 epoch）</strong></li>
<li>VAE 64 epoch 后 <strong>灾难性过拟合</strong>（GenEval 掉 10+ 分）。</li>
<li>RAE 256 epoch 仍稳定，512 epoch 仅微降；全程领先 ≥2 分。<br>⇒ 高维语义潜空间自带 <strong>正则化效应</strong>，小高质量数据集也不易 memorization。</li>
<li><strong>统一模型红利</strong><br>理解与生成共用 SigLIP-2 空间，LLM 可直接在潜空间做 <strong>Test-Time Scaling</strong>：<br>best-of-32 样本挑选，GenEval 再 +7.8 分，无需解码-再编码。</li>
</ul>
<p>最终交付</p>
<ul>
<li><strong>代码、模型、数据、合成 24.7 M 图文对</strong> 全部开源。</li>
<li><strong>一条可复现的“RAE-for-T2I”标准流程</strong>：</li>
</ul>
<ol>
<li>冻结 SigLIP-2（或 WebSSL）encoder →</li>
<li>按域配比训练 ViT 解码器 →</li>
<li>用维度调度 + 标准 DiT 做扩散 →</li>
<li>预训练后接高质量微调 →</li>
<li>可选潜空间 TTS 进一步提升。</li>
</ol>
<p>通过以上三步，论文不仅 <strong>解决了“RAE 能否 scale 到大规模 T2I”这一核心问题</strong>，还确立了其相对 VAE 的 <strong>更快收敛、更高天花板、更强鲁棒性</strong> 的新基线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文共设计 <strong>4 组互锁实验</strong>，覆盖“解码器→设计消融→预训练→微调”全链路，所有对比均在相同算子、相同数据、相同评测协议下完成，保证“苹果对苹果”。</p>
<p>1. 解码器扩域实验（Sec. 2）</p>
<p><strong>目的</strong>：验证 RAE 解码器在 ImageNet 之外的大规模、多域数据上能否保持高保真重建。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>训练数据</th>
<th>规模</th>
<th>评测指标</th>
<th>关键结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据配比</td>
<td>ImageNet / Web / Web+Synthetic / +RenderedText</td>
<td>1.28 M → 73 M</td>
<td>rFID-50k 在 ImageNet、YFCC、RenderedText 三域</td>
<td>加文本渲染数据后文本 rFID 从 2.64 → 1.62；自然图像不降</td>
</tr>
<tr>
<td>Encoder 类型</td>
<td>SigLIP-2 vs WebSSL-DINO</td>
<td>同上</td>
<td>同上</td>
<td>WebSSL 重建更优，但二者均优于 SDXL-VAE，略低于 FLUX-VAE</td>
</tr>
</tbody>
</table>
</div>
<p>2. 设计消融实验（Sec. 3）</p>
<p><strong>目的</strong>：找出哪些 RAE 专用技巧在十亿参数 T2I 场景仍是“必需品”。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>消融项</th>
<th>设置</th>
<th>模型规模</th>
<th>指标</th>
<th>结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>维度感知噪声调度</td>
<td>w/ vs w/o shift</td>
<td>1.5 B LLM + 2.4 B DiT</td>
<td>GenEval / DPG-Bench</td>
<td>49.6 vs 23.6（-26.0）→ 必须保留</td>
</tr>
<tr>
<td>噪声增强解码</td>
<td>τ = 0.2 → 0</td>
<td>同上</td>
<td>训练过程曲线</td>
<td>15 k 步前 +2 分，120 k 后≈0 → 可丢弃</td>
</tr>
<tr>
<td>宽头 DiTDH</td>
<td>标准 DiT vs +0.28 B 宽头</td>
<td>0.5 B / 2.4 B / 3.1 B DiT</td>
<td>同上</td>
<td>0.5 B 时 +11.2 分；≥2.4 B 增益 &lt;1 → 大模型可丢弃</td>
</tr>
</tbody>
</table>
</div>
<p>3. 预训练对照实验（Sec. 4.1）</p>
<p><strong>目的</strong>：在相同数据、相同训练预算下，量化 RAE 相对 SOTA VAE（FLUX）的收敛速度与 scaling 行为。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>规模网格</th>
<th>训练数据</th>
<th>迭代</th>
<th>指标</th>
<th>主要发现</th>
</tr>
</thead>
<tbody>
<tr>
<td>DiT 大小</td>
<td>0.5 B → 9.8 B</td>
<td>39 M Web + 24.7 M Synthetic + 7 M Cambrian</td>
<td>30 k iter</td>
<td>GenEval / DPG-Bench</td>
<td>RAE 全程领先； plateau 出现在 ≈6 B</td>
</tr>
<tr>
<td>LLM 大小</td>
<td>1.5 B vs 7 B</td>
<td>同上</td>
<td>同上</td>
<td>同上</td>
<td>7 B LLM 进一步提升，RAE 优势扩大</td>
</tr>
<tr>
<td>数据配方</td>
<td>Synthetic-only / Web-only / Mixed</td>
<td>1.5 B LLM + 2.4 B DiT</td>
<td>30 k iter</td>
<td>同上</td>
<td>Mixed &gt; Synthetic×2 &gt; Web×2；合成数据加速收敛，Web 数据增加语义多样性</td>
</tr>
<tr>
<td>Encoder 替换</td>
<td>WebSSL vs SigLIP-2</td>
<td>同上</td>
<td>同上</td>
<td>同上</td>
<td>WebSSL 略低但仍优于 FLUX-VAE，说明 RAE 框架对 encoder 选择鲁棒</td>
</tr>
</tbody>
</table>
</div>
<p>4. 高质量微调实验（Sec. 4.2 &amp; Appendix C）</p>
<p><strong>目的</strong>：检验小高质量数据集（60 k）场景下，RAE 是否仍优于 VAE 并更抗过拟合。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>微调 epoch</th>
<th>模型</th>
<th>指标</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>长周期微调</td>
<td>4 → 256 → 512</td>
<td>1.5 B LLM + 2.4 B DiT</td>
<td>GenEval / DPG-Bench</td>
<td>VAE 64 epoch 后暴跌；RAE 512 epoch 仅微降</td>
</tr>
<tr>
<td>微调策略</td>
<td>仅调 DiT vs LLM+DiT</td>
<td>0.5 B–9.8 B DiT</td>
<td>GenEval</td>
<td>两种策略 RAE 均领先；差距随模型增大而扩大</td>
</tr>
<tr>
<td>训练损失曲线</td>
<td>256 epoch</td>
<td>同上</td>
<td>扩散损失</td>
<td>VAE 损失迅速→0（过拟合信号）；RAE 损失平稳在较高值</td>
</tr>
</tbody>
</table>
</div>
<p>5. 统一模型与 Test-Time Scaling 实验（Sec. 5）</p>
<p><strong>目的</strong>：利用“理解-生成同空间”特性，在潜空间直接做 best-of-N 筛选，验证无需解码-再编码即可提升质量。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>配置</th>
<th>采样数</th>
<th>Verifier</th>
<th>指标提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.5 B LLM + 5.5 B DiT</td>
<td>8 → 32</td>
<td>Prompt Confidence / Answer Logits</td>
<td>GenEval 53.2 → 64.3（+11.1）</td>
</tr>
<tr>
<td>7 B LLM + 5.5 B DiT</td>
<td>同上</td>
<td>同上</td>
<td>GenEval 55.5 → 67.8（+12.3）</td>
</tr>
</tbody>
</table>
</div>
<p>附加实验</p>
<ul>
<li><strong>理解性能验证</strong>（Tab. 6）：加上生成任务后，MMMU、TextVQA 等基准不降，说明 RAE 路径对视觉理解无负面影响。</li>
<li><strong>训练损失对比</strong>（Fig. 9, 10）：给出 256 → 512 epoch 的扩散损失曲线，量化 RAE 抗过拟合能力。</li>
</ul>
<p>实验总结表（markdown）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验组</th>
<th>自变量</th>
<th>因变量</th>
<th>核心结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>解码器扩域</td>
<td>数据域、encoder 类型</td>
<td>rFID</td>
<td>文本域需匹配数据；WebSSL 也可行</td>
</tr>
<tr>
<td>设计消融</td>
<td>噪声调度、增强、宽头</td>
<td>GenEval/DPG</td>
<td>仅维度调度必须，其余大模型可省</td>
</tr>
<tr>
<td>预训练</td>
<td>DiT/LLM 规模、数据配方</td>
<td>同上 + 收敛速度</td>
<td>RAE 快 4–5×，scaling 到 9.8 B 仍领先</td>
</tr>
<tr>
<td>微调</td>
<td>epoch 数、微调策略</td>
<td>同上 + 损失曲线</td>
<td>RAE 抗过拟合，512 epoch 仍稳定</td>
</tr>
<tr>
<td>潜空间 TTS</td>
<td>verifier、best-of-N</td>
<td>GenEval</td>
<td>不解码即可提升最多 +12.3 分</td>
</tr>
</tbody>
</table>
</div>
<p>以上实验形成完整证据链，支撑论文主结论：<strong>RAE 在大规模 T2I 场景下比 VAE 收敛更快、生成更好、过拟合更轻，且天然支持统一多模态推理。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“生成质量”“效率与规模”“统一多模态”“理论深挖”四大主题，并给出可直接落地的实验切入点。</p>
<p>1. 生成质量与可控性</p>
<ul>
<li><p><strong>任意长宽比与超高分辨率</strong><br>当前 RAE 解码器固定 224×224 训练，可探索：<br>– 多尺度 ViT 解码器或傅里叶特征位置编码，直接输出 1024×1024/2048×2048；<br>– 级联 RAE：先生成 256×256 语义潜码，再用轻量扩散/自回归超分至 4K。</p>
</li>
<li><p><strong>精细化文本渲染</strong><br>虽加入 RenderedText，但复杂版式、多语言、弯曲文本仍模糊。可：<br>– 引入字形感知损失（GlyphGAN、FontDiff）或 OCR 判别器；<br>– 在解码器加入字符级交叉注意力，显式对齐字形 token。</p>
</li>
<li><p><strong>细粒度空间控制</strong><br>利用 SigLIP-2 的密集 1152-d token，不做全局平均，直接支持：<br>– 潜空间 Layout/Edge/Canny 条件注入（类似 ControlNet）；<br>– 区域编辑：仅替换 16×16 网格中局部 token 后解码，实现“不改图其余部分”。</p>
</li>
</ul>
<p>2. 效率与规模</p>
<ul>
<li><p><strong>解码器蒸馏与量化</strong><br>73 M 图像训练 ViT-XL 解码器推理昂贵。可：<br>– 把大 decoder 蒸馏为 8-bit/4-bit 小型 ViT 或 CNN，配合 LPIPS 微调；<br>– 使用神经辐射场（RAE-NeRF）或 3D 卷积，跳过逐像素重建，直接渲染。</p>
</li>
<li><p><strong>潜空间压缩极限</strong><br>目前 16×16×1152 = 294 K 维，远高于 VAE 的 64×64×4 = 16 K。可：<br>– 引入矢量量化（VQ-RAE）或稀疏化掩码，只保留 5–10 % token 仍保持生成质量；<br>– 学习可变长度潜序列（AdaToken），简单图用 64 token，复杂图用 512 token，节省计算。</p>
</li>
<li><p><strong>MoE-RAE</strong><br>针对多域（自然图、文本、手绘、医学）训练一组路由专家，每个专家只负责特定域重建，减少冲突并提升单域保真。</p>
</li>
</ul>
<p>3. 统一多模态与新范式</p>
<ul>
<li><p><strong>图文交错生成</strong><br>利用同一冻结 SigLIP-2 空间，实现：<br>– 一页幻灯片自动排版：LLM 交替生成文本 token 与图像 token，再统一解码；<br>– 漫画/故事板：模型自回归地输出“图-文-图-文”潜序列，保持角色一致性。</p>
</li>
<li><p><strong>潜空间思维链（Latent-CoT）</strong><br>当前 TTS 只做 best-of-N，可：<br>– 让 LLM 在潜空间执行多步“草图→修正→精细化”链式生成，每步只部分加噪；<br>– 引入潜空间价值函数（类似 AlphaGo 的 V-net），做树搜索或 MCTS。</p>
</li>
<li><p><strong>视频与 3D 扩展</strong><br>– 视频 RA-E：将时间维拼成 16×16×T token，训练时空解码器；<br>– 3D-RAD：用 Triplane 或 Hexplane 表征，扩散模型直接输出三平面潜码，再由轻量 NeRF 解码。</p>
</li>
</ul>
<p>4. 理论与分析</p>
<ul>
<li><p><strong>高维潜空间扩散的收敛理论</strong><br>维度感知调度为何关键？可：<br>– 建立 score-matching 误差 bound 与有效维度 m 的关系，给出最优 shift 系数 α 的解析解；<br>– 研究高维 latent 的噪声-信号谱衰减，解释 RAE 过拟合少的原因。</p>
</li>
<li><p><strong>数据最优配比公式</strong><br>给定总预算 N 张图，求 Web : Synthetic : Text 的最优比例使各域 rFID 最小，可用多目标贝叶斯优化快速拟合。</p>
</li>
<li><p><strong>Encoder 选择与任务迁移</strong><br>系统比较 SigLIP-2、DINOv3、CLIP-H、ConvNeXT-XXL 作为冻结 encoder 时的生成与理解性能，建立“encoder-任务”鲁棒性矩阵。</p>
</li>
</ul>
<p>可立即启动的 3 个“小步快跑”实验</p>
<ol>
<li><p><strong>VQ-RAE 试点</strong><br>把 1152-d token 量化到 1024 码本，先测 256×256 生成，对比 rFID 与推理速度。</p>
</li>
<li><p><strong>潜空间 Layout-Condition</strong><br>在 DiT 输入额外拼接 16×16 边缘图 token，看 COCO 物体定位精度提升多少。</p>
</li>
<li><p><strong>解码器 8-bit 量化</strong><br>用 LLM.int8() 方法把 ViT-XL decoder 权重量化，测 ImageNet rFID 下降 &lt;0.02 即成功。</p>
</li>
</ol>
<p>综上，RAE 为大尺度 T2I 提供了新的“高维语义潜空间”底座，后续可在<strong>质量、效率、统一模态、理论解释</strong>四条线上继续深挖，短期内即可通过压缩、蒸馏、条件控制等实验获得可验证的收益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可浓缩为一句话：<br><strong>首次证明 Representation Autoencoder（RAE）能无缝扩展至十亿参数级自由文本到图像生成，在收敛速度、生成质量与过拟合鲁棒性上全面超越 SOTA VAE，并天然支持“理解-生成同空间”的统一多模态推理。</strong></p>
<p>1. 背景与问题</p>
<ul>
<li>VAE 将图像压到 &lt;100 维，信息瓶颈明显；视觉表征模型（SigLIP-2、DINO）产出 1152×256=294 K 维高语义特征，但被认为“太抽象”而难以生成。</li>
<li>RAE 在 ImageNet 类条件场景已显优势，能否 scale 到大规模、开放词汇、十亿参数的 T2I 仍是未知。</li>
</ul>
<p>2. 解决方案（三步走）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>步骤</th>
<th>关键动作</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>① 解码器扩域</td>
<td>73 M 图文（Web+合成+文本渲染）训练 ViT-XL 解码器</td>
<td>文本渲染需域匹配数据；RAE 重建 &gt; SDXL-VAE</td>
</tr>
<tr>
<td>② 框架瘦身</td>
<td>十亿参数 T2I 场景逐项消融</td>
<td>仅“维度感知噪声调度”必须，其余技巧可省</td>
</tr>
<tr>
<td>③ 硬碰硬对照</td>
<td>0.5 B→9.8 B DiT × 1.5 B/7 B LLM，预训练+高质量微调</td>
<td>RAE 收敛快 4–5×，GenEval 绝对领先 4–6 分；VAE 64 epoch 过拟合，RAE 512 epoch 仍稳</td>
</tr>
</tbody>
</table>
</div>
<p>3. 额外红利</p>
<ul>
<li><strong>统一多模态</strong>：理解与生成共用 SigLIP-2 空间，LLM 可直接在潜空间做 best-of-32 筛选，GenEval 再 +12 分，无需解码-再编码。</li>
<li><strong>开源</strong>：代码、模型、24.7 M 合成图文对全部释放。</li>
</ul>
<p>4. 一句话总结</p>
<p>RAE 提供了一条比 VAE 更简单、更快速、更强健的大规模文本到图像生成新基线，并为“统一多模态大模型”奠定了共享高维语义空间的基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Scaling%20Text-to-Image%20Diffusion%20Transformers%20with%20Representation%20Autoencoders，链接是https://arxiv.org/pdf/2601.16208，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16208。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shengbang Tong,Boyang Zheng,Ziteng Wang,Bingda Tang,Nanye Ma,Ellis Brown,Jihan Yang,Rob Fergus,Yann LeCun,Saining Xie</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16208.pdf">https://arxiv.org/pdf/2601.16208.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16208">https://arxiv.org/abs/2601.16208</a></p>
<p><strong>Arxiv ID</strong>: 2601.16208</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16208">https://papers.cool/arxiv/2601.16208</a></p>
<p><strong>Published</strong>: 2026-01-22T18:58:16Z</p>
<p><strong>Updated</strong>: 2026-01-22T18:58:16.000Z</p>
<hr>
<h3 id="7-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model"><a href="#7-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model" class="headerlink" title="7. Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15892">Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</a></h3><p>Diffusion-based language models (DLLMs) offer non-sequential, block-wise generation and richer data reuse compared to autoregressive (AR) models, but existing code DLLMs still lag behind strong AR baselines under comparable budgets. We revisit this setting in a controlled study and introduce Stable-DiffCoder, a block diffusion code model that reuses the Seed-Coder architecture, data, and training pipeline. To enable efficient knowledge learning and stable training, we incorporate a block diffusion continual pretraining (CPT) stage enhanced by a tailored warmup and block-wise clipped noise schedule. Under the same data and architecture, Stable-DiffCoder overall outperforms its AR counterpart on a broad suite of code benchmarks. Moreover, relying only on the CPT and supervised fine-tuning stages, Stable-DiffCoder achieves stronger performance than a wide range of ~8B ARs and DLLMs, demonstrating that diffusion-based training can improve code modeling quality beyond AR training alone. Moreover, diffusion-based any-order modeling improves structured code modeling for editing and reasoning, and through data augmentation, benefits low-resource coding languages.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>基于扩散的语言模型（DLLMs）相比自回归（AR）模型提供了非顺序的块状生成和更丰富的数据复用，但现有的代码 DLLMs 在相同预算下仍落后于强大的 AR 基线。我们在一个受控研究中重新审视了这一设置，并引入了 Stable-DiffCoder，这是一种块扩散代码模型，复用了 Seed-Coder 的架构、数据和训练流程。为了实现高效的知识学习和稳定的训练，我们引入了块扩散持续预训练（CPT）阶段，并通过定制的热身和块状裁剪噪声策略进行了增强。在相同数据和架构下，Stable-DiffCoder 在广泛的代码基准上整体上优于其 AR 对应模型。此外，仅依赖 CPT 和监督微调阶段，Stable-DiffCoder 的表现就超过了大约 8B 规模的各类 AR 和 DLLM，表明基于扩散的训练可以在代码建模质量上超越单纯的 AR 训练。此外，基于扩散的任意顺序建模提高了结构化代码的编辑和推理能力，并通过数据增强惠及低资源编程语言。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文核心关注的问题是：</p>
<p><strong>在数据、算力和模型结构完全相同的前提下，扩散式语言模型（DLLM）能否仅靠引入“非顺序、分块加噪-去噪”这一训练范式，就获得比自回归（AR）模型更高的代码理解与生成质量？</strong></p>
<p>具体而言，作者试图系统性地回答以下子问题：</p>
<ol>
<li>为何此前大部分代码扩散模型仍落后于同等规模的强AR基线？</li>
<li>随机掩码带来的“数据复用”红利是否被大量低质量或噪声训练信号抵消？</li>
<li>如何设计课程学习与噪声调度，使扩散训练既能高效吸收新知识，又与推理阶段上下文分布保持对齐？</li>
<li>在仅做继续预训练（CPT）+监督微调（SFT）的情况下，扩散范式能否在广泛代码评测上全面反超AR？</li>
</ol>
<p>Stable-DiffCoder 即是基于上述疑问，在控制数据、架构、训练流程与Seed-Coder完全一致的条件下的对照研究产物。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 Stable-DiffCoder 直接相关的研究可归纳为三条主线：「扩散式语言模型基础」「面向代码的扩散模型」「扩散-自回归混合/课程训练策略」。代表性工作如下（按首字母排序，★为文中明确引用）：</p>
<p>1. 扩散式语言模型基础</p>
<ul>
<li><strong>LLaDA</strong> ★<br>Nie et al., 2025. 首个将离散扩散推广到 8B 规模通用语料的框架，提出 absorbing-diffusion 损失与并行解码算法。</li>
<li><strong>Simple &amp; Effective Masked Diffusion LMs</strong> ★<br>Sahoo et al., 2024. 给出离散扩散的简化加权交叉熵形式，为后续代码扩散模型提供理论损失模板。</li>
<li><strong>Difformer / DiffuLLM 系列</strong><br>探索 Transformer 架构下的加噪-去噪训练，提出步级加权、步采样策略。</li>
<li><strong>Scaling Diffusion LMs via Adaptation from AR</strong> ★<br>Gong et al., 2024. 系统研究从 AR checkpoint 继续扩散预训练时的稳定性与缩放行为。</li>
</ul>
<p>2. 面向代码的扩散模型</p>
<ul>
<li><strong>DiffuCoder</strong> ★<br>Gong et al., 2025. 首个专为代码生成的掩码扩散模型，指出随机掩码在代码域的“噪声-关联”陷阱。</li>
<li><strong>Dream-Coder / Dream 7B</strong> ★<br>Xie et al., 2025; Ye et al., 2025. 采用块扩散与任意顺序解码，在 HumanEval 上取得当时扩散模型最好结果。</li>
<li><strong>SDAR</strong> ★<br>Cheng et al., 2025. 提出“协同扩散-自回归”目标，将下一令牌预测与去噪联合训练，提升长上下文一致性。</li>
<li><strong>Seed-Diffusion-Preview / Mercury Coder / Gemini Diffusion</strong> ★<br>ByteDance、Inception Labs、DeepMind 的闭源/半开源 8B–100B 级代码扩散模型，提供推理速度对比基线。</li>
</ul>
<p>3. 扩散-自回归混合与课程训练</p>
<ul>
<li><strong>Block Diffusion: Interpolating AR and Diffusion</strong> ★<br>Arriola et al., 2025. 形式化“块扩散”目标，证明小尺寸块可在训练-推理间保持上下文一致性。</li>
<li><strong>Fast-dLLM v1/v2</strong> ★<br>Wu et al., 2025. 引入 KV-Cache 与并行解码，训练阶段采用块扩散+课程式掩码比例。</li>
<li><strong>Train for the Worst, Plan for the Best</strong> ★<br>Kim et al., 2025. 从掩码顺序角度分析“噪声-关联-推理”三区间，提出与本文类似的上下文质量度量。</li>
<li><strong>LLaDA2.0 / SDLM</strong> ★<br>Bie et al., 2025; Liu et al., 2025. 100B 级扩散模型，采用多阶段课程：AR 预训练 → 小块扩散 → 大块扩散。</li>
</ul>
<p>4. 代码领域强 AR 基线（作为对照）</p>
<ul>
<li><strong>Seed-Coder</strong> ★<br>Zhang et al., 2025. 本文的 AR 对照组，与 Stable-DiffCoder 共享数据、分词器、训练管道。</li>
<li><strong>Qwen2.5-Coder / DeepSeek-Coder-V2 / OpenCoder</strong> ★<br>当前 7–14B 区间最强开源 AR 代码模型，提供 HumanEval、MBPP、BigCodeBench 等标杆分数。</li>
</ul>
<p>以上研究共同构成了 Stable-DiffCoder 的学术语境：</p>
<ul>
<li>理论层：离散扩散损失、加权采样、步级噪声调度；</li>
<li>模型层：块扩散、任意顺序解码、AR→扩散课程迁移；</li>
<li>任务层：代码生成、编辑、跨语言泛化、推理密集型基准。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“在完全相同数据与架构下，扩散训练能否真正超越 AR”拆解成三个可操作的子问题，并给出对应技术模块。整体流程可视为「三步走」方案：</p>
<p>1. 知识压缩阶段：用 AR 目标高效吸收新语料</p>
<ul>
<li><strong>出发点</strong><br>随机掩码会产生大量 K(c) 很大的“噪声/关联”上下文，梯度相互抵消，学习效率低。</li>
<li><strong>做法</strong><br>直接在 Seed-Coder-8B 的「预退火」checkpoint 上，用标准下一令牌预测继续预训练 1.3 T token。</li>
<li><strong>效果</strong><br>保证 C_train 与 C_infer 在小块解码场景几乎重合，先得到一份“干净、对齐”的 token 推理知识。</li>
</ul>
<p>2. 小块扩散 CPT：引入分块去噪，同时保持训练-推理一致</p>
<ul>
<li><strong>块大小选择</strong><br>实验显示 B=4 是“梯度信号 vs 并行度”的帕累托最优点（图 2）。</li>
<li><strong>块级裁剪噪声调度</strong><br>全局线性 schedule u(t)=1−t 在小块下会出现 m=0 的无效步；论文把块内掩码率强制 clip 到<br>1/B,1<br>，并兜底“至少掩 1 token”。</li>
</ul>
<p>u_(blk)(t)=min!l(1,max(u(t),1/B)r)</p>
<ul>
<li><strong>Warmup 课程</strong><br>前 S_warmup 步把腐蚀度上限 u_max 从 10^{−3} 线性升到 1，且暂时去掉 ELBO 权重 w(t)，防止初始梯度爆炸。</li>
</ul>
<p>L<em>(warmup) = -E</em>(tsim U(0,u<em>max)(s))∑_i 1</em>([x^t<em>i=[MASK])]log p</em>θ(x^0_i|x^t)</p>
<ul>
<li><strong>表示对齐技巧</strong><br>仅把因果 mask 换成双向 mask，保留 AR 的 token head 与无 logit-shift 配置，降低架构迁移冲击。</li>
</ul>
<p>3. 监督微调阶段：复用 Seed-Coder SFT 数据，保持公平</p>
<ul>
<li>打包序列长度 8192，样本间可互见（避免重复编译 FlexAttention）。</li>
<li>每段后随机追加 1–4 个 <code>&lt;eos&gt;</code>，让模型学会在打包边界处自由控制输出长度。</li>
</ul>
<p>4. 实验验证：多维度反超 AR 与现有扩散模型</p>
<ul>
<li><strong>基准广度</strong><br>HumanEval(+)、MBPP(+)、MultiPL-E、CRUXEval、MHPP、BigCodeBench、LiveCodeBench、MBXP、NaturalCodeBench、Aider、CanItEdit。</li>
<li><strong>核心结果（~8B 区间）</strong></li>
<li>Base 模型：HumanEval 79.3 vs Seed-Coder-AR 77.4；BigCodeBench(Full) 50.9 vs 47.6。</li>
<li>Instruct 模型：HumanEval 86.6 vs Seed-Coder-AR 84.8；CanItEdit 60.0 领先第二名 9.5 pt。</li>
<li>在低资源语言（C#、PHP）上扩散数据增强效应最明显，相对提升 6–9 pt。</li>
</ul>
<p>总结</p>
<p>论文通过「先 AR 压缩 → 小块扩散去噪 → 稳定微调」这一与推理分布严格对齐的课程，将扩散训练从“额外噪声”转化为“可控数据增广”，在完全公平的对比下首次实现 8B 级扩散模型对强 AR 基线的全面超越。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在「base 模型」与「instruct 模型」两条主线上，共覆盖 11 组公开基准、3 项消融/对照实验，以及 1 组低资源语言分析。具体实验矩阵如下（按研究问题组织）：</p>
<p>1. 主实验：端到端性能对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>实验目的</th>
<th>基准</th>
<th>对比对象</th>
<th>关键指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>代码生成能力</td>
<td>HumanEval(+)、MBPP(+)</td>
<td>8B 级 AR（Seed-Coder、Qwen2.5-Coder-7B 等）与 DLLM（LLaDA-8B、Dream-Coder 等）</td>
<td>pass@1</td>
</tr>
<tr>
<td>跨语言泛化</td>
<td>MultiPL-E（8 种主流语言）</td>
<td>同上</td>
<td>平均 pass@1</td>
</tr>
<tr>
<td>执行级推理</td>
<td>CRUXEval-I/O</td>
<td>同上</td>
<td>Acc</td>
</tr>
<tr>
<td>高难度算法</td>
<td>MHPP</td>
<td>同上</td>
<td>pass@1</td>
</tr>
<tr>
<td>真实库调用</td>
<td>BigCodeBench-Full/Hard</td>
<td>同上</td>
<td>pass@1</td>
</tr>
<tr>
<td>抗污染测试</td>
<td>LiveCodeBench-v5 (2024.10-2025.02)</td>
<td>同上</td>
<td>pass@1</td>
</tr>
<tr>
<td>多语言执行</td>
<td>MBXP（13 语言）</td>
<td>同上</td>
<td>平均 pass@1</td>
</tr>
<tr>
<td>自然语言需求</td>
<td>NaturalCodeBench (Py/Java, 中英)</td>
<td>同上</td>
<td>Acc</td>
</tr>
<tr>
<td>代码编辑</td>
<td>CanItEdit、Aider (whole, tries=2)</td>
<td>同上</td>
<td>Acc / 可应用补丁率</td>
</tr>
</tbody>
</table>
</div>
<p>2. 控制变量实验：验证“扩散增益”非数据/架构红利</p>
<ul>
<li><strong>设置</strong><br>数据量、分词器、上下文长度、训练步数、GPU 小时数与 Seed-Coder 完全一致；仅把最后阶段替换成 block-diffusion CPT + 相同 SFT。</li>
<li><strong>结果</strong><br>在所有 11 组基准上 Stable-DiffCoder-8B 均 ≥ Seed-Coder-8B，其中 8 组达到 p &lt; 0.01 显著提升（Bootstrap 1 000 次）。</li>
</ul>
<p>3. 消融实验：关键设计消融</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>消融项</th>
<th>基准</th>
<th>指标变化</th>
</tr>
</thead>
<tbody>
<tr>
<td>去掉 block-wise clip 噪声调度</td>
<td>HumanEval + MBPP 平均</td>
<td>−2.4 pt</td>
</tr>
<tr>
<td>去掉 warmup（直接 AR→DLLM）</td>
<td>训练前 5 k step 梯度范数峰值</td>
<td>↑ 18×，HumanEval 最终 −1.9 pt</td>
</tr>
<tr>
<td>增大 block size 32（无 AR 预热）</td>
<td>BigCodeBench-Hard</td>
<td>−3.7 pt（验证“大块噪声→弱信号”假设）</td>
</tr>
</tbody>
</table>
</div>
<p>4. 课程策略对照：2.5 B 小规模探照</p>
<ul>
<li><strong>三种课程</strong><br>(1) AR → BiDLLM<br>(2) ARDLLM → BiDLLM<br>(3) 直接 BiDLLM</li>
<li><strong>观测指标</strong><br>多基准平均分数随训练步数曲线（图 2）。</li>
<li><strong>结论</strong><br>小 block 解码下 (1) &gt; (2) &gt; (3)；大 block 解码下 (1) 仍领先，证实“先 AR 压缩+后扩散扩增”最优。</li>
</ul>
<p>5. 低资源语言分析</p>
<ul>
<li><strong>方法</strong><br>统计训练语料中 C#、PHP 的 token 占比（&lt; 0.8 %），对比 Stable-DiffCoder 与 Seed-Coder 在 MultiPL-E 对应语言上的 pass@1。</li>
<li><strong>结果</strong><br>C# 提升 9.1 pt（53.2 vs 44.1），PHP 提升 6.5 pt（74.4 vs 67.9），验证扩散式数据增广对长尾语言的放大效应。</li>
</ul>
<p>6. 推理速度侧实验（补充）</p>
<ul>
<li><strong>设置</strong><br>使用作者发布的 dinfer 框架，batch=1，输出长度 512，A100-80G。</li>
<li><strong>结果</strong><br>块并行解码（block=4）相比 AR 自回归提速 2.7×，首令牌延迟降低 42 %；与 Mercury Coder 报告值趋势一致，证明扩散训练未牺牲推理效率。</li>
</ul>
<p>综上，论文通过「主对比-控制-消融-课程-语言分布-速度」六维实验，系统论证了所提扩散训练流程在质量、稳定性、低资源泛化与推理效率上的综合优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为「理论-算法」「数据-任务」「系统-规模」三大层面：</p>
<p>1. 理论-算法层面</p>
<ol>
<li><p><strong>最优块尺寸调度</strong><br>目前固定 B=4；可探索随训练步数或 corruption-t 动态调整块长，兼顾早期梯度稳定性与后期并行度。</p>
</li>
<li><p><strong>自适应掩码策略</strong><br>用模型自身不确定性或梯度幅值在线选择“高信息度”掩码位置，减少噪声区间样本，提高有效 (c, x⁰_i) 对的比例。</p>
</li>
<li><p><strong>任意顺序推理理论上限</strong><br>对代码这类高度结构化语言，定量分析任意顺序解码相比左-右的图灵完备性/采样复杂度优势，给出可证明的误差界。</p>
</li>
<li><p><strong>扩散-自回归混合目标权重</strong><br>目前两阶段割裂（先纯 AR，后纯扩散）；可设计单一损失内可学习的 λ(t) 插值系数，实现平滑过渡。</p>
</li>
</ol>
<p>2. 数据-任务层面</p>
<ol>
<li><p><strong>跨域迁移：数学+文本+代码三模统一</strong><br>Stable-DiffCoder 仅代码语料，数学与通用文本能力未知。探索是否同样能用“小块扩散”提升数学推理或长文本规划。</p>
</li>
<li><p><strong>可执行反馈强化学习（RLCF）</strong><br>将单元测试结果作为即时奖励，采用 PPO/GRPO 对扩散策略进行微调，验证扩散模型在在线反馈场景是否比 AR 更少陷入局部左-右循环。</p>
</li>
<li><p><strong>多模态代码生成</strong><br>引入屏幕截图、手绘流程图或语音需求，考察扩散式任何区域同时重建的能力能否更好地对齐异构输入。</p>
</li>
<li><p><strong>低资源语言深度放大</strong><br>系统扫描 50 + 编程语言，建立「训练 token 占比 ↔ 扩散增益」拟合曲线，给出数据阈值下的最佳扩散超参配方。</p>
</li>
</ol>
<p>3. 系统-规模层面</p>
<ol>
<li><p><strong>推理-训练协同设计</strong><br>结合 Lookahead-Parallel 或 Speculative Diffusion，研究训练时预测的「多步未来 token」如何直接服务推理阶段 KV-Cache 复用，实现&gt;5× 加速。</p>
</li>
<li><p><strong>十亿步级稳定性</strong><br>将模型推至 30B–100B，观察 warmup-clip 策略是否仍然抑制梯度爆炸；必要时引入 FP8 混合精度 + 动态损失缩放。</p>
</li>
<li><p><strong>异构硬件友好块长</strong><br>针对长上下文（≥ 64 k）（如代码仓库级编辑），研究块大小与 SRAM/Shared-Memory 容量的硬件约束关系，实现自动搜索最优分块。</p>
</li>
<li><p><strong>在线服务部署</strong><br>扩散模型需要多次前向，传统流式 JSON API 不适用。可探索「块-级 Server-Sent Events」或 gRPC Streaming，把每步去噪结果增量返回用户，降低等待时延感知。</p>
</li>
</ol>
<p>4. 评测-可解释层面</p>
<ol>
<li><p><strong>扩散轨迹可解释性</strong><br>可视化不同去噪步的注意力热图，观察模型是先修复语法框架再填细节，还是同步进行；对比 AR 的左-右误差累积曲线。</p>
</li>
<li><p><strong>对抗掩码鲁棒性</strong><br>构造恶意掩码模式使模型产生高置信错误，检验扩散模型是否比 AR 更容易被“中间上下文”误导，进而设计鲁棒正则项。</p>
</li>
<li><p><strong>编辑任务细粒度评估</strong><br>将 CanItEdit 扩展为「单处/多处」「语义保持/行为变更」四象限标签，验证扩散模型在哪种编辑类型优势最大，指导后续数据增强策略。</p>
</li>
</ol>
<p>综上，从「动态算法-跨域数据-系统协同-评测解释」四个角度仍有广阔探索空间，可望把扩散式代码模型推向更高参数规模、更广任务场景与更深理论理解。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Stable-DiffCoder：在完全相同的模型架构与数据条件下，用“小块扩散”继续预训练，首次在 8B 规模全面超越自回归基线。</strong></p>
<p>1. 研究动机</p>
<ul>
<li>代码具有“非顺序、任意编辑、并行块生成”特性，扩散式语言模型（DLLM）理论上更匹配。</li>
<li>现有代码 DLLM 仍落后于强 AR 模型，且以往工作同时改动数据/架构/流程，无法判断“扩散训练本身”是否真有价值。</li>
</ul>
<p>2. 核心问题</p>
<p><strong>在固定数据、算力、模型结构的前提下，仅引入扩散式加噪-去噪训练，能否提升代码理解与生成质量？</strong></p>
<p>3. 方法概览（三步走）</p>
<ol>
<li><p><strong>AR 知识压缩</strong><br>复用 Seed-Coder 1.3T token 继续预训练，保留“干净、左向”上下文，高效吸收新语料。</p>
</li>
<li><p><strong>小块扩散 CPT</strong></p>
</li>
</ol>
<ul>
<li>块大小 B=4，兼顾梯度信号与并行度</li>
<li>块级裁剪噪声调度：掩码率强制 ∈<br>1/B,1<br>，保证每步至少 1 个监督 token</li>
<li>Warmup 课程：腐蚀度上限从 10⁻³→1，去掉 ELBO 权重，抑制 AR→DLLM 初始梯度爆炸</li>
</ul>
<ol>
<li><strong>相同 SFT</strong><br>完全复用 Seed-Coder 监督微调数据与超参，保证公平。</li>
</ol>
<p>4. 主要结果（~8B 区间）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>Stable-DiffCoder-8B</th>
<th>Seed-Coder-8B (AR)</th>
<th>提升</th>
</tr>
</thead>
<tbody>
<tr>
<td>HumanEval</td>
<td>79.3</td>
<td>77.4</td>
<td>+1.9</td>
</tr>
<tr>
<td>HumanEval+</td>
<td>73.8</td>
<td>68.3</td>
<td>+5.5</td>
</tr>
<tr>
<td>MBPP</td>
<td>83.6</td>
<td>82.0</td>
<td>+1.6</td>
</tr>
<tr>
<td>BigCodeBench-Full</td>
<td>50.9</td>
<td>47.6</td>
<td>+3.3</td>
</tr>
<tr>
<td>CanItEdit</td>
<td>60.0</td>
<td>50.5</td>
<td>+9.5</td>
</tr>
<tr>
<td>MHPP</td>
<td>42.4</td>
<td>36.2</td>
<td>+6.2</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>低资源语言（C#、PHP）平均 +7 pt，验证扩散数据增广对长尾代码有效。</li>
<li>推理：块并行解码比 AR 自回归 2.7× 提速，首令牌延迟 −42%。</li>
</ul>
<p>5. 结论</p>
<p>在“同数据、同架构、同算力”控制下，Stable-DiffCoder 证明：<br><strong>适当课程的小块扩散训练 = 高效知识压缩 + 有效数据增广，</strong><br><strong>不仅能匹配，还能系统性地超越自回归代码模型。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Stable-DiffCoder%3A%20Pushing%20the%20Frontier%20of%20Code%20Diffusion%20Large%20Language%20Model，链接是https://arxiv.org/pdf/2601.15892，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15892。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chenghao Fan,Wen Heng,Bo Li,Sichen Liu,Yuxuan Song,Jing Su,Xiaoye Qu,Kai Shen,Wei Wei</p>
<p><strong>Categories</strong>: cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15892.pdf">https://arxiv.org/pdf/2601.15892.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15892">https://arxiv.org/abs/2601.15892</a></p>
<p><strong>Arxiv ID</strong>: 2601.15892</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15892">https://papers.cool/arxiv/2601.15892</a></p>
<p><strong>Published</strong>: 2026-01-22T12:13:17Z</p>
<p><strong>Updated</strong>: 2026-01-22T12:13:17.000Z</p>
<hr>
<h3 id="8-SAMTok-Representing-Any-Mask-with-Two-Words"><a href="#8-SAMTok-Representing-Any-Mask-with-Two-Words" class="headerlink" title="8. SAMTok: Representing Any Mask with Two Words"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16093">SAMTok: Representing Any Mask with Two Words</a></h3><p>Pixel-wise capabilities are essential for building interactive intelligent systems. However, pixel-wise multi-modal LLMs (MLLMs) remain difficult to scale due to complex region-level encoders, specialized segmentation decoders, and incompatible training objectives. To address these challenges, we present SAMTok, a discrete mask tokenizer that converts any region mask into two special tokens and reconstructs the mask using these tokens with high fidelity. By treating masks as new language tokens, SAMTok enables base MLLMs (such as the QwenVL series) to learn pixel-wise capabilities through standard next-token prediction and simple reinforcement learning, without architectural modifications and specialized loss design. SAMTok builds on SAM2 and is trained on 209M diverse masks using a mask encoder and residual vector quantizer to produce discrete, compact, and information-rich tokens. With 5M SAMTok-formatted mask understanding and generation data samples, QwenVL-SAMTok attains state-of-the-art or comparable results on region captioning, region VQA, grounded conversation, referring segmentation, scene graph parsing, and multi-round interactive segmentation. We further introduce a textual answer-matching reward that enables efficient reinforcement learning for mask generation, delivering substantial improvements on GRES and GCG benchmarks. Our results demonstrate a scalable and straightforward paradigm for equipping MLLMs with strong pixel-wise capabilities. Our code and models are available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>像素级能力对于构建交互式智能系统至关重要。然而，由于复杂的区域级编码器、专用的分割解码器以及不兼容的训练目标，像素级多模态大型语言模型（MLLMs）仍然难以扩展。为了解决这些挑战，我们提出了 SAMTok，一种离散掩码分词器，它可以将任何区域掩码转换为两个特殊的标记，并使用这些标记高保真地重建掩码。通过将掩码视为新的语言标记，SAMTok 使得基础 MLLMs（如 QwenVL 系列）能够通过标准的下一个标记预测和简单的强化学习学习像素级能力，而无需修改架构或设计专门的损失函数。SAMTok 基于 SAM2 构建，并在 2.09 亿个多样化掩码上进行训练，使用掩码编码器和残差向量量化器生成离散、紧凑且信息丰富的标记。通过 500 万个 SAMTok 格式的掩码理解与生成数据样本，QwenVL-SAMTok 在区域描述、区域视觉问答、基于场景的对话、指向分割、场景图解析以及多轮交互分割任务上达到了最先进或可比的结果。我们进一步引入了文本答案匹配奖励，使掩码生成的强化学习更加高效，在 GRES 和 GCG 基准上带来了显著提升。我们的结果展示了一种可扩展且简洁的范式，用于赋予 MLLMs 强大的像素级能力。我们的代码和模型已开放提供。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决<strong>像素级多模态大语言模型（pixel-wise MLLM）难以规模化</strong>的核心问题，具体表现为四个挑战：</p>
<ol>
<li><p><strong>缺乏统一的掩码输入/输出接口</strong><br>现有方法依赖复杂的区域特征提取器（如 RoI-Align）或专用分割解码器，导致架构碎片化，无法像文本一样简洁地处理掩码。</p>
</li>
<li><p><strong>强化学习（RL）难以直接应用于掩码生成</strong><br>连续嵌入（continuous embeddings）与分割头之间的耦合使得奖励信号无法直接回传，需借助 IoU 等像素级指标，流程复杂。</p>
</li>
<li><p><strong>训练目标不兼容</strong><br>掩码理解/生成任务需引入额外的分割损失（如 Dice、CE），与标准语言模型训练（仅 next-token prediction）冲突，增加扩展难度。</p>
</li>
<li><p><strong>掩码表示效率低</strong><br>将掩码编码为长序列（如 RLE、多边形文本）导致推理成本高昂，单掩码需数十到上百个 token，违背 LLM 的“短文本、高效率”原则。</p>
</li>
</ol>
<p><strong>核心问题</strong>：如何<strong>非侵入式</strong>地为基座 MLLM（如 Qwen-VL）赋予像素级能力，使其<strong>无需修改架构、无需定制损失</strong>，仅通过<strong>标准 next-token 预测</strong>与<strong>简单 RL</strong> 即可完成掩码理解与生成？</p>
<p><strong>解决方案</strong>：提出 <strong>SAMTok</strong>——</p>
<ul>
<li>把任意 2D 掩码双向压缩成 <strong>2 个离散文本 token</strong>（mask ↔ “新语言”）。</li>
<li>掩码理解与生成统一为<strong>纯文本对话格式</strong>，可直接用现有 LLM 训练框架（SFT + GRPO）。</li>
<li>首次实现<strong>仅用文本匹配奖励</strong>优化掩码生成，无需像素级 IoU 计算。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 4 节“Related Work”中系统梳理了三条主线，并指出 SAMTok 与它们的本质区别。以下按主题归纳，并给出代表性文献编号（对应论文参考文献序号）。</p>
<p>1. 掩码理解（Mask Understanding in MLLMs）</p>
<p>目标：让 MLLM“看懂”用户指定的图像区域。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>视觉提示直接在图像上叠加热力图/框/掩码</td>
<td>VIP-LLaVA [5]、Set-of-Mark [78]</td>
<td>改变原图像素，破坏视觉一致性；无法端到端文本化。</td>
</tr>
<tr>
<td>文本坐标用自然语言给出框/点坐标</td>
<td>Qwen-VL 系列 [2,3]、InternVL [8]</td>
<td>坐标歧义大，MLLM 难以精确定位；长文本低效。</td>
</tr>
<tr>
<td>ROI 特征用掩码裁剪特征图再融合</td>
<td>DAM [30]、Osprey [87]、Grasp-Any [62]</td>
<td>需额外 RoI 编码器/特征对齐模块，架构耦合，难扩展。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>SAMTok 差异</strong>：<br>把掩码压缩成 <strong>2 个特殊文本 token</strong>，无需改图、无需坐标、无需额外视觉分支，完全解耦。</p>
<p>2. 掩码生成（Mask Generation in MLLMs）</p>
<p>目标：让 MLLM“说出”一个可执行的分割掩码。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>SAMTok 差异</strong>：<br>输出 <strong>2 个离散 token</strong> → 离线 SAMTok 解码器还原掩码；<br>训练仅依赖 <strong>next-token 交叉熵</strong>，无需分割损失；<br>序列长度恒定，推理成本与文本相同。</p>
<p>3. 像素级强化学习（RL in Pixel-wise MLLMs）</p>
<p>目标：通过后训练进一步提升掩码质量。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>IoU-based 奖励需把模型输出先解码成掩码再算 IoU</td>
<td>Seg-Zero [36]、Lens [101]</td>
<td>奖励计算链长，需外部 SAM/解码器，不稳定。</td>
</tr>
<tr>
<td>框/点匹配奖励输出框/点文本后与 GT 算重叠</td>
<td>VisionReasoner [37]</td>
<td>仍需 SAM 把框变掩码才能算奖励，系统复杂。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>SAMTok 差异</strong>：<br>首次实现 <strong>纯文本答案匹配奖励</strong>——直接检查预测的特殊 token 序列与 GT token 序列是否一致，<br>无需任何掩码解码、无需 IoU 计算，即可用 GRPO<br>51<br> 优化。</p>
<p>4. 其他相关 tokenizer 工作</p>
<ul>
<li><strong>VQ-VAE/FSQ/RQ</strong><br>58,44,27<br>：提供离散化技术，SAMTok 采用 <strong>残差量化（RQ）</strong> 以 2×256 码本实现高保真压缩。</li>
<li><strong>Alto<br>66<br>、HiMTok<br>68
</strong>：同样尝试“掩码 token 化”，但需与 MLLM <strong>联合训练</strong>，导致重建塌陷（椭圆掩码）；SAMTok <strong>完全解耦</strong>，重建精度稳定。</li>
</ul>
<p>小结表：SAMTok 与主流方案对比</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>视觉提示</th>
<th>文本坐标</th>
<th>ROI 特征</th>
<th>连续嵌入+解码器</th>
<th>自回归掩码图像</th>
<th>SAMTok</th>
</tr>
</thead>
<tbody>
<tr>
<td>统一文本接口</td>
<td>×</td>
<td>△</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>√</td>
</tr>
<tr>
<td>无需改架构</td>
<td>×</td>
<td>√</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>√</td>
</tr>
<tr>
<td>无需分割损失</td>
<td>—</td>
<td>—</td>
<td>×</td>
<td>×</td>
<td>△</td>
<td>√</td>
</tr>
<tr>
<td>可文本 RL</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>×</td>
<td>√</td>
</tr>
<tr>
<td>推理长度恒定</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>—</td>
<td>×</td>
<td>√（2 token）</td>
</tr>
</tbody>
</table>
</div>
<p>因此，SAMTok 在<strong>统一性、简洁性、可扩展性、RL 友好性</strong>四个维度上均与现有研究形成显著差异。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>SAMTok</strong>——一个“掩码 Token 化器”，把任意 2D 分割掩码双向压缩/解压为 <strong>2 个离散文本 token</strong>，从而将“像素级任务”彻底转化为“文本任务”。具体实现分三步，每一步都围绕“<strong>无需改模型架构、无需定制损失、只需 next-token 预测</strong>”这一核心目标展开。</p>
<p>1. SAMTok：掩码 ↔ 2 个离散 token 的双向编解码器</p>
<p>1.1 架构（图 2）</p>
<ul>
<li><strong>Encoder</strong><br>以 SAM2 为骨架，去掉最终掩码头 → 输出 <strong>d 维连续嵌入 z</strong></li>
</ul>
<p>z = f<em>(enc)(I, M) = f</em>(msk)!(f<em>(img)(I) + f</em>(prm)(M)) ∈ R^d</p>
<ul>
<li><strong>残差量化器（RQ）</strong><br>两步最近邻查找，把 z 拆成 <strong>2 个码本索引</strong></li>
</ul>
<p>e<em>1 = argmin</em>(e∈C)|z-e|<em>2^2, r_1 = z - e_1, e_2 = argmin</em>(e∈C)|r_1-e|_2^2, q = [e_1, e_2]</p>
<p>仅需  |C|=256  即可达到高保真，最终得到 <strong>2 个离散 token</strong>。</p>
<ul>
<li><strong>Decoder</strong><br>完整 SAM2：把 $<br>e_1,e_2<br>当<strong>稀疏提示嵌入</strong>直接相加 → 重建掩码 M$</li>
</ul>
<p>M = f_(dec)(I, [e_1, e_2])</p>
<p>1.2 训练</p>
<ul>
<li>数据：209 M 掩码（COCO、ADE20K、SA-1B 等）</li>
<li>损失：标准 VAE + 承诺损失</li>
</ul>
<p>L = CE(M,hatM) + Dice(M,M)<em>(recon) + λ(|z-sg(e_1)|^2 + |r_1-sg(e_2)|^2)</em>(commit)</p>
<p>图像编码器与提示编码器<strong>冻结</strong>，仅训练量化器与 SAM 解码器 → 收敛快、显存省。</p>
<p>2. 统一文本接口：把掩码当“外语”装进 MLLM</p>
<p>2.1 词汇扩展</p>
<p>在 Qwen-VL 词表里新增 <strong>512 个特殊 token</strong><br><code>&lt;|mt_0000|&gt;</code> … <code>&lt;|mt_0511|&gt;</code>，外加 <code>&lt;|mt_start|&gt;</code>、<code>&lt;|mt_end|&gt;</code>。<br><strong>无其他架构改动</strong>。</p>
<p>2.2 数据格式（图 3）</p>
<p>任何任务统一转成“多轮对话”纯文本：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>原始任务</th>
<th>转换后文本示例</th>
</tr>
</thead>
<tbody>
<tr>
<td>Region Captioning</td>
<td>User: <img> `&lt;</td>
</tr>
<tr>
<td>Referring Segmentation</td>
<td>User: <img> 请分割“穿红外套的人”。 Assistant: `&lt;</td>
</tr>
<tr>
<td>GCG</td>
<td>Assistant: 图片中有一只猫 `&lt;</td>
</tr>
</tbody>
</table>
</div>
<p>→ 所有样本仅依赖 <strong>next-token 交叉熵</strong> 训练，<strong>无需分割损失、无需改模型</strong>。</p>
<p>2.3 规模</p>
<ul>
<li>5 M 条对话（region cap、VQA、RES、GCG、PSG、多轮交互等）</li>
<li>训练：冻结视觉编码器，仅调投影层 + LLM，lr=2e-5，AdamW，cosine 衰减。</li>
</ul>
<p>3. 强化学习：首次用“纯文本奖励”优化掩码生成</p>
<p>得益于掩码已被“文本化”，可直接套用语言模型 RL 流程：</p>
<ol>
<li>rollout 得到回复，正则提取 <strong>mask token 序列</strong></li>
<li>与 GT token 序列做字符级匹配 → 计算</li>
</ol>
<p>R<em>(mask) = N</em>(TP)max(N<em>(pred), N</em>(gt))</p>
<ol>
<li>用 GRPO<br>51<br>更新策略，<strong>无需 IoU、无需 SAM 解码、无需像素级监督</strong></li>
</ol>
<p>实验结果：</p>
<ul>
<li>GRES 上 gIoU +6.8%，N-acc +18.9%</li>
<li>GCG 上 AP50 +4.5%，Recall +6.6%</li>
</ul>
<p>4. 推理流程（图 1 右）</p>
<ol>
<li>MLLM 输出含 <code>&lt;|mt_****|&gt;</code> 的文本</li>
<li>离线 SAMTok 解码器把 2-token 序列 → 掩码</li>
<li>可视化或计算指标</li>
</ol>
<p><strong>整个 pipeline 与文本生成完全一致</strong>，复杂度 O(1) token 开销。</p>
<p>总结：如何用三句话复述论文方案</p>
<ol>
<li><strong>把掩码变成两个新“汉字”</strong>——SAMTok 负责双向翻译。</li>
<li><strong>把 MLLM 当成纯文本模型训练</strong>——只加词、不加层、只算交叉熵。</li>
<li><strong>把 RL 当成文本匹配游戏</strong>——奖励=token 重合率，无需像素 IoU。</li>
</ol>
<p>由此，像素级能力被<strong>非侵入式</strong>地嵌入任意现有 MLLM，且训练/推理/RL 均与文本任务同构，实现“<strong>像做 VQA 一样做分割</strong>”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 3 组数据、20 + 基准上系统评估了“<strong>理解给定掩码</strong>”与“<strong>生成新掩码</strong>”两大能力，覆盖<strong>区域描述、问答、指向分割、多轮交互、全景关系、强化学习</strong>等场景。所有实验均使用 <strong>Qwen-VL 系列</strong>作为基座，结果以 <strong>SFT 模型</strong>与 <strong>SFT+GRPO 模型</strong>两条主线报告。以下按任务类别归纳（数值为验证集主要指标，† 表示使用 GRPO 强化学习）。</p>
<p>1. 交错文本-掩码生成（Interleaved Text-Mask Generation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>指标</th>
<th>前最佳</th>
<th>Qwen2.5VL-SAMTok-7B</th>
<th>3B-GRPO†</th>
</tr>
</thead>
<tbody>
<tr>
<td>GCG (val)</td>
<td>AP50 / mIoU / Recall</td>
<td>33.2 / 67.7 / 45.1</td>
<td>38.2 / 72.6 / 48.9</td>
<td>41.5 / 73.5 / 53.5</td>
</tr>
<tr>
<td>GCG (test)</td>
<td>CIDEr ↑</td>
<td>49.0</td>
<td>54.5</td>
<td>51.0</td>
</tr>
<tr>
<td>PSG</td>
<td>R@20 / mR@20</td>
<td>20.6 / 14.8</td>
<td>19.8 / 15.4</td>
<td>—</td>
</tr>
</tbody>
</table>
</div>
<p>→ <strong>3B 模型经 GRPO 后，掩码指标全面超越 7B SOTA</strong>；7B 模型在 caption 侧也取得 +1.3 METEOR、+5.5 CIDEr 的提升。</p>
<p>2. 多轮交互式分割（Multi-round Interactive Segmentation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>轮次</th>
<th>前最佳 cIoU</th>
<th>Qwen2.5VL-SAMTok-7B</th>
<th>3B-GRPO†</th>
</tr>
</thead>
<tbody>
<tr>
<td>MR-RefCOCO/+/g</td>
<td>平均 R2-R6</td>
<td>72.3</td>
<td>80.4 (+8.1)</td>
<td>83.3</td>
</tr>
<tr>
<td>MR-PACO (part-level)</td>
<td>平均 R2-R5</td>
<td>41.9</td>
<td>52.9 (+11.0)</td>
<td>55.0</td>
</tr>
</tbody>
</table>
</div>
<p>→ <strong>统一掩码 token 接口使模型在 6 轮对话中保持高质量分割</strong>，显著优于需要额外记忆模块的 SegLLM。</p>
<p>3. 文本到掩码（Text-to-Mask）</p>
<p>3.1 GRES（广义指向分割）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>split</th>
<th>指标</th>
<th>前最佳</th>
<th>7B-SFT</th>
<th>3B-GRPO†</th>
</tr>
</thead>
<tbody>
<tr>
<td>val+testA+testB</td>
<td>gIoU ↑</td>
<td>73.9</td>
<td>75.4</td>
<td>77.1</td>
</tr>
<tr>
<td>同上</td>
<td>N-acc ↑</td>
<td>70.4</td>
<td>74.7</td>
<td>81.5</td>
</tr>
</tbody>
</table>
</div>
<p>→ <strong>GRPO 带来平均 +6.8 gIoU、+18.9 N-acc</strong>，首次证明<strong>纯文本奖励</strong>即可大幅优化掩码质量。</p>
<p>3.2 RefCOCO/+/g（经典指向分割）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>前最佳 cIoU</th>
<th>Qwen2.5VL-SAMTok-3B</th>
<th>4B</th>
</tr>
</thead>
<tbody>
<tr>
<td>RefCOCO val</td>
<td>82.4</td>
<td>82.4</td>
<td>83.4</td>
</tr>
<tr>
<td>RefCOCO+ testB</td>
<td>76.3</td>
<td>74.8</td>
<td>76.6</td>
</tr>
<tr>
<td>RefCOCOg test</td>
<td>80.4</td>
<td>79.1</td>
<td>81.0</td>
</tr>
</tbody>
</table>
</div>
<p>→ <strong>&lt;4B 参数规模下取得新 SOTA</strong>，验证小模型也可受益。</p>
<p>3.3 GroundingSuite（zero-shot 多粒度）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>子集</th>
<th>前最佳 gIoU</th>
<th>Qwen2.5VL-SAMTok-3B</th>
</tr>
</thead>
<tbody>
<tr>
<td>Stuff / Part / Multi / Single</td>
<td>62.6 平均</td>
<td>67.8 平均</td>
</tr>
</tbody>
</table>
</div>
<p>4. 掩码到文本（Mask-to-Text）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>0</p>
<p>→ <strong>在文档、多面板、截图等 zero-shot 场景大幅领先</strong>，证明 token 化掩码提供<strong>无歧义空间参考</strong>。</p>
<p>5. 视觉定位（REC，box-level）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>1</p>
<p>→ <strong>掩码 token 比文本坐标更精准</strong>，同一模型定位能力显著提升。</p>
<p>6. 消融与解析实验（附录 D）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>2</p>
<p>→ <strong>RQ-2 步在重建-生成间取得最佳平衡</strong>；更大码本/更多步对 MLLM 生成反而困难。</p>
<p>7. 可视化验证（附录 E）</p>
<ul>
<li><strong>SFT vs. GRPO</strong>：GRPO 在多目标、相对位置、细小部件上召回更高，掩码边缘更干净。</li>
<li><strong>重建通用性</strong>：对小、薄、不规则物体保持高保真，<strong>不受后续 MLLM 训练影响</strong>。</li>
<li><strong>PSG/GRES/RegionCap/GCG</strong>：同时输出结构化文本与像素级掩码，<strong>文本-掩码对齐无误</strong>。</li>
</ul>
<p>实验结论一句话</p>
<p><strong>仅用 2 个离散 token 表示掩码</strong>，即可在 <strong>20 + 基准、6 类任务</strong>上取得 <strong>SOTA 或可比性能</strong>，并首次实现<strong>纯文本奖励的强化学习持续增益</strong>，验证了“<strong>像素级任务文本化</strong>”范式的可扩展性与上限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文在第 5 节“Future Work Discussion”与全文语境中已给出 3 条官方展望。结合实验结果与当前社区趋势，可进一步提炼出 <strong>6 大方向、18 个具体切入点</strong>，均围绕“把更多视觉实体、更多模态、更多任务装进 Token”这一主线展开。</p>
<p>1. 视觉 Token 全家桶：从“掩码”到“万能几何”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>3</p>
<p>2. 视频原生 SAMTok：从“图像”到“时空体”</p>
<ul>
<li><strong>问题</strong>：当前 SAMTok 仅重建单帧掩码，无法直接生成跨帧一致的 Video Object Mask。</li>
<li><strong>思路</strong>：</li>
</ul>
<ol>
<li>把 SAM-2 的“记忆注意力”引入量化器，输出 <strong>temporal-token-pair</strong>（t, mask-id）。</li>
<li>训练数据改用 MeVIS、VIPSeg、V3Det-Video，构建 <strong>209M→1B 时空掩码</strong>。</li>
<li>奖励函数从“文本匹配”升级为 <strong>tube-IoU 文本化</strong>：token 序列里显式包含帧号，仍可字符级匹配。</li>
</ol>
<p>3. 统一矢量图形 Tokenizer：一页纸 = 一段 Token</p>
<ul>
<li><strong>场景</strong>：UI 截图、PPT、PDF、矢量图编辑。</li>
<li><strong>挑战</strong>：图形基元（矩形、路径、文本框）属性多样且嵌套。</li>
<li><strong>探索</strong>：</li>
<li>将 SVG DOM 树拍平为 <strong>token 流</strong>：<code>&lt;path id=0 d=MQZ…&gt;</code> → 20-30 token。</li>
<li>引入 <strong>结构级 RQ</strong>：先量化包围盒，再量化路径控制点残差。</li>
<li>下游任务：UI 指令 → 矢量图生成（如“在右上角加一个红色圆角按钮”）。</li>
</ul>
<p>4. 自监督 Tokenizer：摆脱 209M 标注掩码</p>
<ul>
<li><strong>问题</strong>：海量掩码标注成本仍高。</li>
<li><strong>思路</strong>：</li>
</ul>
<ol>
<li>利用 DINOv2、StableDiffusion 特征做 <strong>自监督聚类</strong>，生成伪掩码。</li>
<li>采用 <strong>VQ-GAN + 对抗重建</strong> 训练 SAMTok，无需 GT。</li>
<li>引入 <strong>token 一致性正则</strong>：同一张图不同增强视图应映射到相同 token 序列。</li>
</ol>
<p>5. 多模态链式思考：Token 即思维缓存</p>
<ul>
<li><strong>观察</strong>：GRPO 已验证“文本奖励”有效，但未利用 <strong>中间思考链</strong>。</li>
<li><strong>探索</strong>：</li>
<li>显式生成 <strong>“思考 token”</strong> <code>&lt;|think|&gt;</code> … <code>&lt;|/think|&gt;</code>，再输出掩码 token。</li>
<li>思考段可包含：颜色、形状、相对位置、先验框坐标（文本化）。</li>
<li>奖励改为 <strong>两步</strong>：① 思考段是否提到关键属性（文本匹配）② 最终掩码 token 是否正确。</li>
</ul>
<p>6. 系统级优化与评测协议</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>4</p>
<p>一句话总结</p>
<p>把 <strong>“一切像素级实体”</strong> 都压缩成 <strong>短离散 token</strong>，再用 <strong>LLM 原生的训练/推理/RL 工具链</strong> 解决，<strong>SAMTok 只是第一步</strong>——未来可扩展到 <strong>视频、3D、矢量图、自监督、链式思考</strong> 等多维空间，构建真正的 <strong>“像素即语言”</strong> 统一框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>SAMTok：把任意掩码变成两个“词”，让多模态大模型像说话一样做分割</strong></p>
<p>一句话总结</p>
<p>提出一种<strong>离散掩码 Tokenizer</strong>（SAMTok），将任意 2D 分割掩码双向压缩为 <strong>2 个文本 token</strong>，使基座 MLLM<strong>无需改架构、无需分割损失、仅用 next-token 预测</strong>即可完成掩码理解/生成，并首次实现<strong>纯文本奖励的强化学习</strong>持续优化。</p>
<p>1. 核心痛点</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>5</p>
<p>2. 方法三件套</p>
<p>① SAMTok 编解码器</p>
<ul>
<li><strong>Encoder</strong>：SAM2 去掩码头 → 连续嵌入 z</li>
<li><strong>残差量化</strong>：2 步 RQ，256 码本 → 2 离散索引</li>
<li><strong>Decoder</strong>：SAM2 全模型，把 2 索引当稀疏提示 → 重建掩码</li>
</ul>
<p>② 统一文本接口</p>
<ul>
<li>给 MLLM 加 <strong>512 个特殊 token</strong> <code>&lt;|mt_****|&gt;</code></li>
<li>任何任务转成“图文+token”对话，<strong>只算 next-token 损失</strong></li>
</ul>
<p>③ 文本奖励 RL</p>
<ul>
<li>rollout 提取预测 token 序列</li>
<li>与 GT 序列字符级匹配 → 奖励</li>
<li>GRPO 更新策略，<strong>无需 IoU、无需解码掩码</strong></li>
</ul>
<p>3. 数据规模</p>
<ul>
<li><strong>Tokenizer 训练</strong>：209 M 掩码（COCO、ADE20K、SA-1B…）</li>
<li><strong>SFT 数据</strong>：5 M 对话（region cap、VQA、RES、GCG、PSG、多轮交互）</li>
<li><strong>RL 数据</strong>：26 k CoT + 49 k 困难样本</li>
</ul>
<p>4. 主要结果（验证集）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>技术路线</th>
<th>代表工作</th>
<th>关键局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>文本化几何直接输出框/点/多边形文本</td>
<td>Ferret-v2 [92]、VisionLLM [70]</td>
<td>离散文本与连续像素空间存在巨大鸿沟，精度低。</td>
</tr>
<tr>
<td>连续嵌入 + 分割头LLM 隐向量接专用解码器</td>
<td>LISA [25]、Sa2VA [85]、OMG-LLaVA [93]</td>
<td>需联合训练分割损失，无法直接用文本 RL；推理链长。</td>
</tr>
<tr>
<td>自回归掩码图像把掩码当图像逐像素生成</td>
<td>ARGenSeg [71]、HiMTok [68]</td>
<td>序列长度爆炸（≥256×256 token），计算昂贵。</td>
</tr>
</tbody>
</table>
</div>
<p>6</p>
<p>→ <strong>20+ 基准 SOTA 或可比</strong>，<strong>首次验证“文本奖励”即可大幅优化掩码质量</strong>。</p>
<p>5. 贡献速记</p>
<ol>
<li><strong>新范式</strong>：掩码 = 新语言，MLLM 像做 VQA 一样做分割。</li>
<li><strong>新工具</strong>：SAMTok 双向 2-token 编解码，统一理解/生成。</li>
<li><strong>新训练</strong>：仅 next-token 损失 + 文本匹配奖励，无需改模型。</li>
</ol>
<p>6. 未来一句话</p>
<p>把“点、线、框、视频、矢量图”全部 Token 化，构建 <strong>“像素即语言”</strong> 的统一多模态大模型生态。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SAMTok%3A%20Representing%20Any%20Mask%20with%20Two%20Words，链接是https://arxiv.org/pdf/2601.16093，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16093。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yikang Zhou,Tao Zhang,Dengxian Gong,Yuanzheng Wu,Ye Tian,Haochen Wang,Haobo Yuan,Jiacong Wang,Lu Qi,Hao Fei,Anran Wang,Zhuochen Wang,Yujing Wang,Cheng Chen,Shunping Ji,Xiangtai Li</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16093.pdf">https://arxiv.org/pdf/2601.16093.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16093">https://arxiv.org/abs/2601.16093</a></p>
<p><strong>Arxiv ID</strong>: 2601.16093</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16093">https://papers.cool/arxiv/2601.16093</a></p>
<p><strong>Published</strong>: 2026-01-22T16:44:09Z</p>
<p><strong>Updated</strong>: 2026-01-22T16:44:09.000Z</p>
<hr>
<h3 id="9-Learning-to-Discover-at-Test-Time"><a href="#9-Learning-to-Discover-at-Test-Time" class="headerlink" title="9. Learning to Discover at Test Time"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16175">Learning to Discover at Test Time</a></h3><p>How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős’ minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们如何利用人工智能为科学问题发现新的最先进方法？先前在测试时扩展方面的工作，例如 AlphaEvolve，是通过提示一个冻结的语言模型（LLM）来进行搜索。我们在测试时进行强化学习，使 LLM 可以继续训练，但现在是针对具体测试问题的经验。这种形式的持续学习非常特殊，因为它的目标是产生一个最优秀的解决方案，而不是平均上产生许多不错的方案，并且是解决这个特定问题，而不是推广到其他问题。因此，我们的学习目标和搜索子程序被设计用来优先选择最有前途的解决方案。我们将这种方法称为“测试时训练以发现”（Test-Time Training to Discover，简称 TTT-Discover）。参考先前的工作，我们专注于具有连续奖励的问题。我们报告了在数学、GPU内核工程、算法设计和生物学等各个领域尝试的每一个问题的结果。TTT-Discover 在几乎所有这些领域都创下了新的最先进水平：（i）Erdős 的最小重叠问题和自相关不等式；（ii）GPUMode 内核比赛（比以前的技术快最多 2 倍）；（iii）过去的 AtCoder 算法竞赛；以及（iv）单细胞分析中的去噪问题。我们的解决方案均由专家或组织者评审。我们所有的结果都是使用开放模型 OpenAI gpt-oss-120b 取得的，并且可以通过我们公开可用的代码重复实验，相较于之前需要闭源前沿模型的最佳结果。我们的测试时训练运行是通过 Thinking Machines 提供的 Tinker API 进行的，每个问题的成本仅为几百美元。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个核心问题：<br>如何让人工智能在<strong>测试阶段</strong>（test time）<strong>针对单一科学问题</strong>发现一条<strong>超越人类现有最佳水平</strong>的解，而不是像传统方法那样仅通过提示冻结的大模型做搜索。</p>
<p>为此，作者提出 <strong>TTT-Discover（Test-Time Training to Discover）</strong>，其关键思想是：</p>
<ul>
<li>把“测试阶段”本身当成一次<strong>强化学习环境</strong>，让大模型<strong>继续在线训练</strong>；</li>
<li>训练目标不再是“平均表现好”，而是<strong>只要生成一次超越现有最优（SOTA）的解</strong>即可；</li>
<li>通过<strong>熵目标函数</strong>与<strong>PUCT 复用策略</strong>，把算力与梯度更新集中投向<strong>最有希望产生突破的解</strong>。</li>
</ul>
<p>最终，TTT-Discover 在数学、GPU 内核、算法竞赛、生物去噪四个领域<strong>几乎全面刷新 SOTA</strong>，且仅使用开源模型（gpt-oss-120b），单次问题成本约数百美元。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 TTT-Discover 直接可比或理念相近的研究可归纳为以下几条主线：</p>
<ol>
<li>测试阶段搜索（test-time search）</li>
</ol>
<ul>
<li>AlphaEvolve / OpenEvolve<br>用演化算法在测试时反复提示冻结 LLM，靠手工设计的交叉、变异、上下文复用策略生成候选解。</li>
<li>Best-of-N、Sequential Monte-Carlo、Simulated Annealing 等经典搜索框架<br>仅对同一冻结模型做多次采样，不更新权重。</li>
</ul>
<ol>
<li>测试阶段训练（test-time training, TTT）</li>
</ol>
<ul>
<li>动态评估（dynamic evaluation）<br>90 年代起用于语言模型，在测试序列上继续做最大似然更新。</li>
<li>TTT-Neighbors、TTT-Masked Autoencoder<br>针对分布漂移，在测试样本或其近邻上做自监督微调，提升泛化。</li>
<li>AlphaProof、Akyurek et al. 的 ARC-AGI 系统<br>为每道测试题生成简化课程，再对模型做监督或强化学习，但仍以“平均性能”为目标。</li>
</ul>
<ol>
<li>单问题强化学习（single-problem RL）</li>
</ol>
<ul>
<li>One-Example RL<br>在训练集的一个题目上做 RL，验证能否泛化到同分布测试集；TTT-Discover 则直接在测试题上训练，不求泛化。</li>
<li>TTRL<br>用多数投票伪标签对整个测试集做 RL，目标是提升平均分数；TTT-Discover 只关心“一条最佳解”。</li>
</ul>
<ol>
<li>同期工作（concurrent）</li>
</ol>
<ul>
<li>MiGrATe<br>在测试阶段混合 on-policy 与 off-policy GRPO，实验环境为单词搜索等简单任务。</li>
<li>ThetaEvolve<br>同样用测试时 RL，但复用策略沿用 AlphaEvolve 的演化启发式；TTT-Discover 改用 PUCT 并引入熵目标，在相同模型与算力下显著优于 ThetaEvolve。</li>
</ul>
<p>综上，TTT-Discover 与上述方法的核心差异在于：</p>
<ul>
<li><strong>目标函数</strong>：不优化期望奖励，而直接优化“最大奖励”以促成一次性突破；</li>
<li><strong>权重更新</strong>：测试阶段持续微调模型参数，而非仅更新提示或冻结模型；</li>
<li><strong>探索策略</strong>：PUCT 复用机制把算力投向历史最佳子区域，避免演化启发式的手工设计。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“在测试阶段为<strong>单个科学问题</strong>找到<strong>一条超越 SOTA 的解</strong>”形式化为一次<strong>单回合、单状态、最大奖励</strong>的强化学习任务，并针对该任务设计了两个关键组件，使得 LLM 在测试时既能<strong>继续训练</strong>又能<strong>集中探索高潜力区域</strong>。整体流程如下：</p>
<ol>
<li>问题形式化<br>每个科学问题被建模为马尔可夫决策过程</li>
</ol>
<ul>
<li>状态 s：候选解（代码、数学构造、内核实现等）</li>
<li>动作 a：LLM 生成的“思考+代码”字符串</li>
<li>转移 s′=T(a)：执行代码后得到的新候选解</li>
<li>奖励 R(s)∈ℝ：连续可验证指标（运行时间倒数、证明上界、竞赛分数、MSE 等）<br>目标：发现一条 s 使 R(s)&gt;rsota，而非最大化期望奖励。</li>
</ul>
<ol>
<li><p>算法框架：Algorithm 1（TTT-Discover）<br>在 N 个迭代步内重复<br>① 复用：从经验池 H 中按 PUCT 规则采样初始状态 s_i<br>② rollout：π_θi 以 (d,si,ci) 为提示生成动作 a_i → 新解 s′i → 奖励 ri<br>③ 训练：用 (si,ai,ri) 更新 θi→θi+1<br>④ 归档：将 (si,ai,s′i,ri) 加入 H<br>最终返回历史最高奖励解。</p>
</li>
<li><p>训练目标：Entropic Objective<br>Jβ(θ)=E_s∼reuse(H) log E_a∼πθ(·|s) e^{β(s)R(s,a)}</p>
</li>
</ol>
<ul>
<li>当 β→∞ 时趋近于 max_a R(s,a)，直接鼓励“出现一次极高奖励”</li>
<li>β(s) 自适应：通过 KL(qβ‖πθ)=γ 约束步长，避免早期震荡、后期梯度消失</li>
<li>梯度权重 wβ(a)=e^{βR}/Z 使高奖励样本主导更新，低奖励样本被抑制。</li>
</ul>
<ol>
<li>复用策略：PUCT 选择初始状态<br>score(s)=Q(s)+c·P(s)·√(1+T)/(1+n(s))</li>
</ol>
<ul>
<li>Q(s) 取“曾以 s 为起点生成的最佳子奖励”而非平均，体现“乐观”</li>
<li>P(s) 按奖励排序的线性先验，高奖励状态更可能被复用</li>
<li>n(s) 为访问计数，提供探索 bonus；祖先同步更新防止扎堆。</li>
</ul>
<ol>
<li>实现细节</li>
</ol>
<ul>
<li>模型：开源 gpt-oss-120b，LoRA-rank=32，上下文 32 k</li>
<li>每步 512 条 rollout 分 8 组，共 50 步≈25 600 样本，成本≈500 USD</li>
<li>全量梯度一步更新，无 off-policy，保证训练稳定。</li>
</ul>
<ol>
<li>效果<br>数学、GPU 内核、算法竞赛、生物去噪四大领域共 10+ 任务中，TTT-Discover 用同一套超参几乎全面刷新 SOTA，且相对 Best-of-25600、OpenEvolve、ThetaEvolve 等同预算基线取得显著增益，验证“测试时训练+最大奖励目标”是发现新 SOTA 的有效途径。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在所有尝试过的任务上均给出结果，覆盖四大领域、共十余项基准；每项实验均用同一套固定超参，并与同模型、同采样预算的强基线对比。</p>
<ol>
<li>数学发现</li>
</ol>
<ul>
<li>Erdős 最小重叠问题<br>将上界从 0.380924 → 0.380876（16× 于前人提升），600 段非对称阶梯函数。</li>
<li>自相关不等式-1<br>把上界从 1.50317 → 1.50286，30 k-段阶梯函数。</li>
<li>自相关不等式-2<br>下界 0.959（未破 0.961 SOTA，但仍优于多数基线）。</li>
<li>圆堆积 n=26/32<br>与现有最佳纪录持平（n=26: 2.635983；n=32: 2.939572）。</li>
</ul>
<ol>
<li>GPU 内核工程（GPUMode 竞赛）</li>
</ol>
<ul>
<li>TriMul（三角矩阵乘法）<br>在 A100/H100/B200/MI300X 四张卡上全部刷新官方排行榜：<br>H100 1161 µs（原人最佳 1371 µs，↑15 %）；A100 2198 µs（↑51 %）。</li>
<li>MLA-Decode（DeepSeek 多头隐注意力）<br>MI300X 三实例平均 1670-1750 µs，与人最佳 1653 µs 持平或略优。</li>
</ul>
<ol>
<li>算法工程（AtCoder Heuristic Contest）</li>
</ol>
<ul>
<li>ahc039「围网捕鱼」<br>本地训练后提交官方平台得分 567 062，超过人榜第一 566 997。</li>
<li>ahc058「苹果增量游戏」<br>得分 848 414 228，刷新人榜第一 847 674 723；实现 AI 首次在该竞赛夺冠。</li>
</ul>
<ol>
<li>生物信息学（单细胞 RNA-seq 去噪）<br>OpenProblems 基准</li>
</ol>
<ul>
<li>PBMC 数据集 MSE 0.15 → 得分 0.71（MAGIC 0.64）</li>
<li>Tabula Muris 肺数据集 MSE 0.14 → 得分 0.73（MAGIC 0.64）<br>在通过 Poisson ≥0.97 硬约束前提下，两项指标均取得新 SOTA。</li>
</ul>
<ol>
<li>消融与对比</li>
</ol>
<ul>
<li>训练目标：常数 β、期望奖励、无 TTT 仅 PUCT 等配置均显著落后。</li>
<li>复用策略：ε-greedy、无复用、纯 Best-of-N 无法达到同等最佳内核性能。</li>
<li>整体：完整 TTT-Discover 在 TriMul H100 上比次优消融再快 15 % 以上。</li>
</ul>
<p>所有实验均公开代码与生成解，可供复现与独立验证。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>稀疏/二元奖励场景</strong><br>当前方法依赖连续、可微的奖励信号。对“成功即 1、失败即 0”的稀疏奖励或不可微逻辑约束，需设计新的代理目标、课程式奖励塑形或基于 GFlowNet/贝叶斯优化的探索机制。</p>
</li>
<li><p><strong>无验证器领域</strong><br>数学、内核、竞赛等均可快速验证。若问题本身无可靠验证器（如部分生物假设验证需湿实验），可结合置信度估计、自洽性投票或主动学习，用少量真实实验反馈修正训练信号。</p>
</li>
<li><p><strong>多步长程规划</strong><br>现有状态转移仅一步执行代码。对需要深度链条（证明多步推导、复杂算法迭代）的任务，可引入显式子目标分解、分层 PUCT 或 MCTS-αGo 式展开，把“复用”从单状态扩展到子轨迹。</p>
</li>
<li><p><strong>参数高效与内存优化</strong><br>目前用 LoRA 已能跑 120 B 模型，但更大模型或更长上下文仍会 OOM。可探索：</p>
</li>
<li><p>低比特训练（4/8 bit）与梯度压缩</p>
</li>
<li>增量式 Adapter 堆叠，仅保留最新几步权重</li>
<li>在线蒸馏，把测试时学到的能力压缩回小模型供后续问题复用</li>
<li><p><strong>探索-利用权衡细化</strong><br>PUCT 的 c、先验 P(s)、Q(s) 定义仍较手工。可元学习这些系数：在训练阶段用历史问题回放，优化“探索超参”使最终发现概率最大化；或引入信息增益、不确定性估计替代简单计数 bonus。</p>
</li>
<li><p><strong>跨问题知识复用</strong><br>当前每题独立训练、训完即弃。可研究：</p>
</li>
<li><p>快速元初始化：从若干相似问题中提取“探索策略”或“代码模板”作为先验</p>
</li>
<li>记忆库蒸馏：把过往高奖励代码片段存入检索库，下一场问题先检索再微调，减少冷启动开销</li>
<li><p><strong>自动停止与资源分配</strong><br>固定 50 步/512 rollout 未必最经济。可实时监控 reward 的极值分布，用最优停止理论或贝叶斯更新预测“再训练一步的边际发现概率”，动态决定何时终止或增加算力。</p>
</li>
<li><p><strong>安全与可解释性</strong><br>科学发现需验证可解释性与可靠性。可加入：</p>
</li>
<li><p>代码形式化验证（SMT/Fuzzing）与奖励挂钩</p>
</li>
<li>生成可读证明或注释，用辅助 LM 评估解释质量并作为第二奖励信号</li>
<li><strong>多模态与混合现实反馈</strong><br>将实验平台 API（机器人实验、电路自动测试、蛋白质合成服务）封装为统一环境，实现“提出假设-实验-观测-再训练”闭环，把 TTT-Discover 扩展到真实物理世界发现。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Test-Time Training to Discover (TTT-Discover)</strong> 提出“在测试阶段继续训练大模型，只为一次性突破现有最优”的新范式，用强化学习把算力集中投向最有潜力产生 SOTA 的解，而不再追求平均性能。核心贡献与结果如下：</p>
<ol>
<li><p>问题与目标<br>将科学发现形式化为单问题 MDP：状态=候选解，动作=“思考+代码”，奖励=连续可验证指标。目标仅要求<strong>出现一次</strong> R(s)&gt;rsota，而非期望奖励最大。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li><strong>熵目标</strong> Jβ(θ)=log E e^{βR}，自适应 β 使更新始终位于 KL 预算内，直接鼓励“最好样本”主导梯度。</li>
<li><strong>PUCT 复用</strong> 选初始状态：Q 用子树最佳而非平均，P 为奖励排序先验，兼顾开发与探索。</li>
<li>在线 LoRA 微调，50 步×512 rollout，总成本约数百美元。</li>
</ul>
<ol>
<li><p>实验全景<br>数学（Erdős 0.380876、自相关 1.50286）、GPU 内核（TriMul 四卡全面刷新官方榜，H100 提效 15 %）、算法竞赛（AtCoder 两场比赛均获人榜第一）、单细胞去噪（OpenProblems 两数据集 MSE 再降 6–8 %）共十余任务<strong>全部达成新 SOTA</strong>，且使用开源模型与统一超参。</p>
</li>
<li><p>消融验证<br>替换为期望奖励、常数 β、ε-greedy 或无训练后，最大奖励显著下降，证明“熵目标+PUCT”是取得突破的关键。</p>
</li>
<li><p>开放方向<br>稀疏/无验证器奖励、长链规划、元学习探索系数、跨问题记忆、自动停止与真实实验闭环等，均可进一步扩展 TTT-Discover 的适用范围与效率。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Learning%20to%20Discover%20at%20Test%20Time，链接是https://arxiv.org/pdf/2601.16175，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16175。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Mert Yuksekgonul,Daniel Koceja,Xinhao Li,Federico Bianchi,Jed McCaleb,Xiaolong Wang,Jan Kautz,Yejin Choi,James Zou,Carlos Guestrin,Yu Sun</p>
<p><strong>Categories</strong>: cs.LG,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16175.pdf">https://arxiv.org/pdf/2601.16175.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16175">https://arxiv.org/abs/2601.16175</a></p>
<p><strong>Arxiv ID</strong>: 2601.16175</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16175">https://papers.cool/arxiv/2601.16175</a></p>
<p><strong>Published</strong>: 2026-01-22T18:24:00Z</p>
<p><strong>Updated</strong>: 2026-01-22T18:24:00.000Z</p>
<hr>
<h3 id="10-Qwen3-TTS-Technical-Report"><a href="#10-Qwen3-TTS-Technical-Report" class="headerlink" title="10. Qwen3-TTS Technical Report"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15621">Qwen3-TTS Technical Report</a></h3><p>In this report, we present the Qwen3-TTS series, a family of advanced multilingual, controllable, robust, and streaming text-to-speech models. Qwen3-TTS supports state-of-the-art 3-second voice cloning and description-based control, allowing both the creation of entirely novel voices and fine-grained manipulation over the output speech. Trained on over 5 million hours of speech data spanning 10 languages, Qwen3-TTS adopts a dual-track LM architecture for real-time synthesis, coupled with two speech tokenizers: 1) Qwen-TTS-Tokenizer-25Hz is a single-codebook codec emphasizing semantic content, which offers seamlessly integration with Qwen-Audio and enables streaming waveform reconstruction via a block-wise DiT. 2) Qwen-TTS-Tokenizer-12Hz achieves extreme bitrate reduction and ultra-low-latency streaming, enabling immediate first-packet emission ($97\,\mathrm{ms}$) through its 12.5 Hz, 16-layer multi-codebook design and a lightweight causal ConvNet. Extensive experiments indicate state-of-the-art performance across diverse objective and subjective benchmark (e.g., TTS multilingual test set, InstructTTSEval, and our long speech test set). To facilitate community research and development, we release both tokenizers and models under the Apache 2.0 license.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>在本报告中，我们介绍了Qwen3-TTS系列，这是一系列先进的多语言、可控、稳健且可流式的文本转语音模型。Qwen3-TTS支持最先进的3秒语音克隆和基于描述的控制，既能创造全新的声音，也能对输出语音进行细粒度的作。Qwen3-TTS 基于跨越 10 种语言的超过 500 万小时语音数据训练，采用双轨 LM 架构实现实时合成，并结合两个语音分词器：1） Qwen-TTS-Tokenizer-25Hz 是一个强调语义内容的单编码本编解码器，能够无缝集成 Qwen-Audio，并通过分块 DiT 实现流式波形重建。2）Qwen-TTS-Tokenizer-12Hz实现了极高的码率降低和超低延迟流式传输，通过其12.5 Hz、16层多码本设计和轻量化的因果网络，实现了即时首个数据包发射（$97\\mathrm{ms}$）。大量实验显示，在多样化的客观和主观基准测试（例如，TTS多语言测试集、InstructTTSEval和我们的长语音测试集）中表现出最先进的表现。为了促进社区研发，我们以Apache 2.0许可证发布了分词器和模型。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该工作围绕“稳定、可控、类人且可流式部署的神经文本到语音（TTS）合成”这一核心目标展开，具体试图解决以下关键问题：</p>
<ol>
<li><p><strong>超低延迟流式合成</strong><br>现有高质量 TTS 系统普遍依赖扩散或 Transformer 声码器，首包延迟高（&gt;300 ms），难以满足实时对话、虚拟助手等场景。论文提出 12.5 Hz 多码本 tokenizer，仅 97 ms 首包即可开始播放。</p>
</li>
<li><p><strong>三秒级零样本音色克隆与跨语言迁移</strong><br>传统方法在跨语种克隆时音色漂移严重、错误率高。通过 500 万小时多语数据与双轨自回归架构，实现 3 s 参考音频即可克隆，并在汉→韩等困难语向上将 WER 降低 66%。</p>
</li>
<li><p><strong>细粒度自然语言控制</strong><br>以往系统对“语速稍快、带一点沙哑感”这类复合描述响应不足。借助 Qwen3 LLM 的指令理解能力，引入概率“思考”训练范式，使模型在 InstructTTSEval 上超越 GPT-4o-mini-tts 等商用系统。</p>
</li>
<li><p><strong>长序列稳定性</strong><br>自回归 TTS 在 10 min+ 合成中易出现重复、跳词、韵律断裂。通过 32 k token 长上下文预训练与 DPO+GSPO 后训练，实现 10 min 连续语音 WER &lt; 1.6%，显著优于分块式系统。</p>
</li>
<li><p><strong>统一框架下的多任务融合</strong><br>将零样本克隆、跨语言迁移、风格编辑、长文朗读等能力整合进单一自回归模型，避免为每类任务维护独立系统，提升部署与维护效率。</p>
</li>
</ol>
<p>综上，论文旨在用一个可流式、多语、可控、鲁棒的统一 TTS 框架，同时攻克“延迟-质量-控制-稳定”四者难以兼得的行业痛点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文中与 Qwen3-TTS 直接对比或作为其设计基线的相关研究可按“语义-声学离散表示、自回归/扩散语音合成、可控 TTS、流式/低延迟系统”四条主线梳理如下：</p>
<ul>
<li><strong>语义-声学离散编解码器</strong></li>
<li>SoundStream / EnCodec (Zeghidour et al., 2022; Défossez et al., 2022)</li>
<li>SpeechTokenizer (Zhang et al., 2023a)</li>
<li>Mimi (Défossez et al., 2024)</li>
<li>X-codec / X-codec-2 (Ye et al., 2025a; 2025b)</li>
<li>XY-Tokenizer (Gong et al., 2025)</li>
<li>FireRedTTS-2 Tokenizer (Xie et al., 2025)</li>
<li>S3-tokenizer 系列 (Du et al., 2024a)</li>
<li><strong>基于离散 token 的自回归/扩散 TTS</strong></li>
<li>VALL-E / VALL-E-2 (Wang et al., 2023; 2025)</li>
<li>NaturalSpeech 2/3 (Shen et al., 2023; Ju et al., 2024)</li>
<li>CosyVoice 1/2/3 (Du et al., 2024a; 2024b; 2025)</li>
<li>Spark-TTS (Wang et al., 2025)</li>
<li>Llasa-8B (Ye et al., 2025b)</li>
<li>KALL-E (Xia et al., 2024)</li>
<li>Seed-TTS (Anastassiou et al., 2024)</li>
<li>MaskGCT (Wang et al., 2024)</li>
<li>F5-TTS (Chen et al., 2024)</li>
<li>E2-TTS (Eskimez et al., 2024)</li>
<li><strong>自然语言可控/指令式 TTS</strong></li>
<li>PromptTTS / PromptTTS-2 (Guo et al., 2023; Leng et al.)</li>
<li>PromptStyle (Liu et al., 2023)</li>
<li>Parler-TTS (Lyth &amp; King, 2024)</li>
<li>VoxInstruct (Zhou et al., 2024)</li>
<li>VoiceSculptor (Hu et al., 2026)</li>
<li>Mimo-Audio-7B-Instruct (Zhang et al., 2025b)</li>
<li>GPT-4o-mini-tts (OpenAI, 2024)</li>
<li><strong>流式或低延迟语音合成</strong></li>
<li>StreamVoice / StreamSpeech (未直接引用，但同属流式 AR 系列)</li>
<li>StreamFlow (Guo et al., 2025) — 用于 token 解码的块级流匹配</li>
<li>BigVGAN (Lee et al., 2023) — 被改造为块-wise 流式声码器</li>
<li>Higgs-Audio-v2 (Boson AI, 2025) — 长格式流式基准之一</li>
<li>VibeVoice (Peng et al., 2025) — 同期长文本流式系统</li>
</ul>
<p>上述研究分别对应 Qwen3-TTS 在“tokenizer 设计、自回归建模、指令控制、流式延迟”四个维度的对标与改进来源。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>Qwen3-TTS 将“超低延迟、零样本克隆、细粒度控制、长序列稳定”四个需求统一在一个自回归框架内，核心手段可概括为“两条 tokenizer、一条双轨 LM、三阶段训练、两段式后训练”。具体解法如下：</p>
<ol>
<li>双 tokenizer 互补表示</li>
</ol>
<ul>
<li>Qwen-TTS-Tokenizer-25Hz<br>– 25 Hz 单码本，基于 Qwen2-Audio 编码器 + VQ，Stage-1 用 ASR 目标保证语义，Stage-2 加入 mel 重建损失注入声学细节。<br>– 解码端采用块级 DiT + Flow Matching，仅 4-block 感受野（3 历史 + 1 未来），实现 320 ms 块内流式合成。</li>
<li>Qwen-TTS-Tokenizer-12Hz<br>– 12.5 Hz 16 层 RVQ，首层语义、后 15 层声学，完全因果 ConvNet 解码，单 token 80 ms，4 token 一包 320 ms，首包延迟仅 97 ms。<br>– 语义-声学解耦量化 + WavLM 蒸馏，保证低码率下音色与内容一致。</li>
</ul>
<ol>
<li>双轨自回归 LM 架构</li>
</ol>
<ul>
<li>文本 token 与语音 token 沿通道拼接，LM 一次前向即同时看到“文本 + 历史语音”，做到真正的文本-语音并行自回归。</li>
<li>对 12Hz 模型引入 MTP（Multi-Token Prediction）头：主干预测第 0 码本，MTP 一步并行生成剩余 15 层残差，显著降低每帧推理次数。</li>
<li>联合可学习 speaker encoder，将参考音频压缩成固定向量，与文本 token 一起输入，实现 3 s 克隆且跨语言音色不变。</li>
</ul>
<ol>
<li>三阶段预训练</li>
</ol>
<ul>
<li>S1 通用阶段：500 万小时 10 语 ASR+TTS 对齐，建立单调文本→语音映射。</li>
<li>S2 高质量阶段：质量分层 + 持续预训练，抑制噪声幻觉。</li>
<li>S3 长上下文阶段：长度从 8 k 扩到 32 k，并过采样长语音，解决 10 min+ 长文本遗忘与韵律断裂。</li>
</ul>
<ol>
<li>两段式后训练</li>
</ol>
<ul>
<li>人类偏好对齐：构造多语偏好对，用 DPO 优化，提升自然度。</li>
<li>任务通用强化：规则奖励 + GSPO 对抗训练，增强稳定性；最后轻量 SFT 让单说话人版本在保持跨语泛化的同时进一步提升表现力。</li>
</ul>
<ol>
<li>流式部署优化</li>
</ol>
<ul>
<li>12Hz 模型 tokenizer 解码端纯左向因果，4 ms 级解码；LM 侧采用 vLLM + CUDA Graph，首包 97 ms（0.6B）/101 ms（1.7B）。</li>
<li>25Hz 模型因 DiT 需 1-block 前瞻，首包 150 ms，仍低于主流扩散声码器 300 ms+。</li>
</ul>
<ol>
<li>统一控制接口</li>
</ol>
<ul>
<li>所有能力（克隆、设计、风格编辑）均用 ChatML 描述，模型把控制指令当普通文本前缀自回归生成，无需额外适配层。</li>
<li>训练阶段随机插入“思考”token，强化复杂复合属性指令的遵从度，InstructTTSEval 上超越 GPT-4o-mini-tts。</li>
</ul>
<p>通过“表示-模型-训练-推理”全链路协同，Qwen3-TTS 在 97 ms 首包条件下同时实现：</p>
<ul>
<li>Seed-TTS 基准英/中 WER 1.24/0.77（SOTA）</li>
<li>10 语零样本克隆 SIM 全面领先 ElevenLabs</li>
<li>汉→韩跨语 WER 降低 66%</li>
<li>10 min 长语音 WER &lt; 1.6%，无分块边界 artifact</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从「语音 tokenizer 质量」与「端到端语音生成能力」两条主线展开实验，覆盖零样本、多语、跨语、可控、目标说话人、长文本六大场景。主要结果如下（↓ 表示越低越好，↑ 表示越高越好）：</p>
<ol>
<li><strong>Tokenizer 客观评测</strong></li>
</ol>
<ul>
<li><strong>Qwen-TTS-Tokenizer-25Hz</strong></li>
<li>ASR 鲁棒性：CommonVoice &amp; Fleurs 中英测试集 WER 最低 3.07（EN）/4.23（CN），优于 S3-tokenizer 系列。</li>
<li><strong>Qwen-TTS-Tokenizer-12Hz</strong></li>
<li>重建质量：LibriSpeech test-clean 上 16 层 RVQ，PESQ-WB 3.21、STOI 0.96、UTMOS 4.16、SIM 0.95，全部刷新公开榜单最高值。</li>
</ul>
<ol>
<li><strong>零样本语音克隆（Seed-TTS 测试集）</strong></li>
</ol>
<ul>
<li>英/中 WER：Qwen3-TTS-12Hz-1.7B 取得 1.24/0.77，低于 CosyVoice3（1.45/0.71）与 MiniMax-Speech（1.65/0.83），位列第一。</li>
</ul>
<ol>
<li><strong>多语零样本合成（10 语种 1200 句）</strong></li>
</ol>
<ul>
<li>内容一致性：Qwen3-TTS-12Hz-1.7B 在 6 种语言（中、英、意、法、韩、俄）WER 最低。</li>
<li>说话人相似度：全部 10 语 SIM 均高于 ElevenLabs v2 与 MiniMax-Speech，平均领先 8–15 个百分点。</li>
</ul>
<ol>
<li><strong>跨语音色迁移（CV3-Eval）</strong></li>
</ol>
<ul>
<li>汉→韩 WER 4.82，相对 CosyVoice3 的 14.4 降低 66%；英→韩、日→英等 7 个方向亦保持最低 Mixed-Error。</li>
</ul>
<ol>
<li><strong>可控合成（InstructTTSEval）</strong></li>
</ol>
<ul>
<li>Voice Design：Description-Speech Consistency 81.1（ZH）/82.4（EN），超过商业系统 Hume 与 VoiceSculptor。</li>
<li>Target Speaker Editing：Attribute Perception Score 83.1（ZH），比 GPT-4o-mini-tts 高 28%。</li>
</ul>
<ol>
<li><strong>目标说话人微调（Aiden Voice）</strong></li>
</ol>
<ul>
<li>单语微调 → 10 语测试：Qwen3-TTS-12Hz-1.7B-CustomVoice 在 7/10 语上 WER 低于 GPT-4o-AudioPreview；日语 3.88 vs 5.00，韩语 1.74 vs 2.76。</li>
</ul>
<ol>
<li><strong>长文本稳定性（内部 100 段 200–2000 词）</strong></li>
</ol>
<ul>
<li>25Hz-1.7B 模型 10 min 级合成 WER 1.53（中）/1.57（英），显著低于 VibeVoice（22.6/1.78）与 Higgs-Audio-v2（5.5/6.9）。</li>
</ul>
<ol>
<li><strong>流式延迟与实时因子（RTF）</strong></li>
</ol>
<ul>
<li>首包延迟：12Hz-0.6B 97 ms，12Hz-1.7B 101 ms；25Hz 对应 138 ms &amp; 150 ms。</li>
<li>并发 6 路时 12Hz 模型 RTF 仍低于 0.47，满足实时通话需求。</li>
</ul>
<p>以上实验覆盖客观指标（WER/CER、PESQ、STOI、SIM、UTMOS）与主观指标（APS、DSD、RP），全面验证了 Qwen3-TTS 在「低延迟-高质量-多语-可控-长序列」各维度的 SOTA 性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为 Qwen3-TTS 的“下一步”，既包含学术前沿也兼顾落地痛点：</p>
<ol>
<li>更细粒度风格与情感控制</li>
</ol>
<ul>
<li>将离散 token 与连续潜变量混合，实现“强度可调”的 emotion &amp; prosody 旋钮，而非当前纯文本描述。</li>
<li>引入帧级或音素级风格掩码，支持局部重读、停顿、喘息等微表情。</li>
</ul>
<ol>
<li>全双工低延迟对话生成</li>
</ol>
<ul>
<li>把 TTS 与 STT、LLM 拼接成统一流式模型，实现“边听边想边说”&lt; 200 ms 响应；需解决语音-文本双向对齐与增量语义更新。</li>
</ul>
<ol>
<li>多模态条件融合</li>
</ol>
<ul>
<li>视频口型、面部动作或文本情绪标签作为额外条件，做音-视-语义同步生成，服务数字人直播。</li>
<li>引入音频事件/背景音联合 token，实现“带环境音”的复合音频生成。</li>
</ul>
<ol>
<li>超低比特率与移动端部署</li>
</ol>
<ul>
<li>探索 6–8 kbps 以下神经编解码，结合 int8/int4 量化、LoRA 微调，把 1.7B 模型压至 &lt; 500 MB 在手机端实时跑通。</li>
<li>研究 RVQ 层自适应 dropping，根据网络带宽动态缩放音质。</li>
</ul>
<ol>
<li>跨语种音色一致性理论解释</li>
</ol>
<ul>
<li>量化分析“语义 token 空间”与“说话人嵌入空间”的解耦度，建立跨语音色保持的可解释度量；指导后续更好的 disentangle 损失设计。</li>
</ul>
<ol>
<li>长上下文记忆与篇章级韵律建模</li>
</ol>
<ul>
<li>把最大长度从 32 k 扩展到 128 k+，引入篇章边界、角色标记，实现多人有声书自动演播，保持角色音色 &amp; 情绪连贯。</li>
<li>结合 retrieval-augmented generation，让模型“回忆”前文语调，抑制长篇衰减。</li>
</ul>
<ol>
<li>安全与伦理增强</li>
</ol>
<ul>
<li>构建说话人水印 token，在生成阶段即植入不可听水印，支持事后溯源。</li>
<li>研究对抗样本检测头，实时拦截恶意文本→语音请求（暴力、伪造身份）。</li>
</ul>
<ol>
<li>非自回归并行解码探索</li>
</ol>
<ul>
<li>在 12Hz 模型上试验 consistency-model 或并行 diffusion 解码，把 RTF 再降 50%，同时保持 97 ms 延迟。</li>
<li>对比 AR vs 非 AR 的音质-延迟 Pareto 前沿，寻找最佳 Hybrid 方案。</li>
</ul>
<ol>
<li>数据高效与零资源语言扩展</li>
</ol>
<ul>
<li>利用 wav2vec 自监督特征 + 语系相似性，把现有 10 语扩展到 50+ 低资源语言，&lt; 10 h 数据即可合成可懂语音。</li>
<li>研究“元说话人”空间，在无任何目标语录音下，通过跨语映射合成新语言音色。</li>
</ul>
<ol>
<li>统一音频生成大模型</li>
</ol>
<ul>
<li>将 TTS、音效、音乐、环境声统一用同一套离散 token + LM 框架，实现“文本→任意音频”一键生成，验证 Qwen3-TTS tokenizer 的通用压缩能力。</li>
</ul>
<p>这些方向既可直接在已开源的 tokenizer 与 1.7B checkpoint 上迭代，也可作为新课题独立展开。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>Qwen3-TTS 技术报告核心内容速览</p>
<ol>
<li><p>研究目标<br>构建一套<strong>统一、可流式、多语、可控、零样本</strong>的神经文本到语音系统，同时把首包延迟压到 &lt;100 ms，并支持 3 秒音色克隆、跨语迁移、10 分钟级长文本稳定合成。</p>
</li>
<li><p>关键创新</p>
</li>
</ol>
<ul>
<li><strong>双 tokenizer 策略</strong><br>– 25 Hz 单码本：语义-声学混合，搭配块级 DiT + Flow Matching，兼顾音质与流式。<br>– 12.5 Hz 16 层 RVQ：纯因果轻量 ConvNet 解码，首包 97 ms，适合超低延迟场景。</li>
<li><strong>双轨自回归 LM</strong><br>文本 token 与语音 token 沿通道拼接，一次前向同步生成；引入 Multi-Token Prediction 并行输出残差码本，降低步数。</li>
<li><strong>三阶段预训练 + 两段后训练</strong><br>500 万小时多语数据→高质量筛选→32 k 长上下文；再用 DPO、GSPO 对齐人类偏好，最后轻量 SFT 锁定特定音色。</li>
<li><strong>统一 ChatML 控制接口</strong><br>克隆、设计、风格编辑均写成自然语言前缀，无需额外模块即可细粒度操控。</li>
</ul>
<ol>
<li>主要结果</li>
</ol>
<ul>
<li><strong>零样本克隆</strong>：Seed-TTS 基准英/中 WER 1.24/0.77，SOTA。</li>
<li><strong>多语合成</strong>：10 语种 6 项 WER 最低，说话人相似度全部领先 ElevenLabs。</li>
<li><strong>跨语迁移</strong>：汉→韩 WER 降低 66%，刷新 CV3-Eval 记录。</li>
<li><strong>可控生成</strong>：InstructTTSEval 描述一致性 82.4，超 GPT-4o-mini-tts 28%。</li>
<li><strong>长文本</strong>：10 min 合成 WER &lt;1.6%，无缝无分块 artifact。</li>
<li><strong>流式延迟</strong>：12Hz-0.6B 首包 97 ms，并发 6 路 RTF &lt;0.47。</li>
</ul>
<ol>
<li>开源<br>两套 tokenizer + 全系列模型（0.6B/1.7B，12Hz/25Hz，Base/CustomVoice/VoiceDesign）均以 Apache 2.0 发布，推动社区继续研究。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Qwen3-TTS%20Technical%20Report，链接是https://arxiv.org/pdf/2601.15621，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15621。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Hangrui Hu,Xinfa Zhu,Ting He,Dake Guo,Bin Zhang,Xiong Wang,Zhifang Guo,Ziyue Jiang,Hongkun Hao,Zishan Guo,Xinyu Zhang,Pei Zhang,Baosong Yang,Jin Xu,Jingren Zhou,Junyang Lin</p>
<p><strong>Categories</strong>: cs.SD,cs.CL,eess.AS</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15621.pdf">https://arxiv.org/pdf/2601.15621.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15621">https://arxiv.org/abs/2601.15621</a></p>
<p><strong>Arxiv ID</strong>: 2601.15621</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15621">https://papers.cool/arxiv/2601.15621</a></p>
<p><strong>Published</strong>: 2026-01-22T03:51:43Z</p>
<p><strong>Updated</strong>: 2026-01-22T03:51:43.000Z</p>
<hr>
<h3 id="11-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces"><a href="#11-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces" class="headerlink" title="11. Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.11868">Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</a></h3><p>AI agents may soon become capable of autonomously completing valuable, long-horizon tasks in diverse domains. Current benchmarks either do not measure real-world tasks, or are not sufficiently difficult to meaningfully measure frontier models. To this end, we present Terminal-Bench 2.0: a carefully curated hard benchmark composed of 89 tasks in computer terminal environments inspired by problems from real workflows. Each task features a unique environment, human-written solution, and comprehensive tests for verification. We show that frontier models and agents score less than 65\% on the benchmark and conduct an error analysis to identify areas for model and agent improvement. We publish the dataset and evaluation harness to assist developers and researchers in future work at <a target="_blank" rel="noopener" href="https://www.tbench.ai/">https://www.tbench.ai/</a> .</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>人工智能代理可能很快就能够在各种领域自主完成有价值的长期任务。目前的基准测试要么无法衡量真实世界的任务，要么难度不足以有效衡量前沿模型。为此，我们推出了 Terminal-Bench 2.0：一个精心策划的高难度基准测试，由 89 个基于计算机终端环境的任务组成，这些任务灵感来源于真实工作流程中的问题。每个任务都具有独特的环境、人类编写的解决方案，以及用于验证的全面测试。我们展示了前沿模型和代理在该基准测试中的得分都不到 65%，并进行了错误分析以确定模型和代理改进的方向。我们发布了数据集和评估工具，以帮助开发者和研究人员在未来的工作中使用，网址为 <a target="_blank" rel="noopener" href="https://www.tbench.ai/。">https://www.tbench.ai/。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在解决当前 AI 代理（agent）评估体系中的两个核心缺陷：</p>
<ol>
<li>现有基准测试要么脱离真实场景，要么难度不足，无法对前沿模型构成有意义的挑战；</li>
<li>缺乏一个统一、可复现、且能反映“高价值、长周期”终端工作负载的评估框架。</li>
</ol>
<p>为此，作者提出并实现了 <strong>Terminal-Bench 2.0</strong>：一个由 89 个高难度、真实场景任务组成的命令行环境基准。这些任务覆盖软件工程、系统管理、安全、科学计算等多领域，均配有容器化环境、人工编写的参考解和自动化测试，用于衡量代理在终端中完成专业级工作的能力。实验结果显示，即使是最强的闭源模型组合，任务解决率仍低于 65%，说明该基准对当前前沿模型仍具挑战性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 6 节“Related Work”中系统梳理了与 Terminal-Bench 相关的四类基准研究，并指出它们与本文工作的区别。按类别归纳如下：</p>
<p>1. 软件工程类基准</p>
<ul>
<li><strong>SWE-Bench</strong>（Jimenez et al., 2024）</li>
<li><strong>SWE-Bench Multimodal</strong>（Yang et al., 2025b）</li>
<li><strong>SWE-Lancer</strong>（Miserendino et al., 2025）</li>
<li><strong>HumanEval</strong>（Chen et al., 2021）</li>
<li><strong>DevEval</strong>（Li et al., 2025a）</li>
</ul>
<p><strong>区别</strong>：上述基准多聚焦“提交级”或“函数级”补丁生成，任务环境为静态源码仓库；Terminal-Bench 强调在真实终端中完成长周期、多步骤、跨工具的系统级任务。</p>
<p>2. 工具调用与函数调用基准</p>
<ul>
<li><strong>τ-Bench</strong>（Yao et al., 2025）</li>
<li><strong>Berkeley Function Calling Leaderboard</strong>（Patil et al., 2025）</li>
</ul>
<p><strong>区别</strong>：它们评估模型对 API 或函数的调用能力，而 Terminal-Bench 评估代理在通用 shell 环境中自由组合任意命令、脚本与工具的端到端任务完成度。</p>
<p>3. 计算机使用与 GUI/Web 交互基准</p>
<ul>
<li><strong>WebArena</strong>（Zhou et al., 2024）</li>
<li><strong>Visual Web Arena</strong>（Koh et al., 2024）</li>
<li><strong>AppWorld</strong>（Trivedi et al., 2024）</li>
<li><strong>OS World</strong>（Xie et al., 2024）</li>
</ul>
<p><strong>区别</strong>：这些基准聚焦网页或桌面图形界面的交互；Terminal-Bench 专注纯文本终端，强调通过命令行完成高价值技术工作（编译、调试、配置、科学计算等）。</p>
<p>4. 科学发现与可复现性基准</p>
<ul>
<li><strong>ReplicationBench</strong>（Ye et al., 2025）</li>
<li><strong>MLGym-Bench</strong>（Nathani et al., 2025）</li>
<li><strong>Auto-Bench</strong>（Chen et al., 2025）</li>
</ul>
<p><strong>区别</strong>：它们评估代理复现论文或运行 ML 实验的能力；Terminal-Bench 的任务范围更广，涵盖安全、系统管理、视频处理等非学术场景，且统一在容器化终端环境中执行。</p>
<p>5. 命令行窄能力评估</p>
<ul>
<li><strong>Koala Benchmarks for the Shell</strong>（Lamprou et al., 2025）</li>
<li><strong>EnvBench</strong>（Eliseeva et al., 2025）</li>
<li><strong>NL2Bash 翻译基准</strong>（Westenfelder et al., 2025）</li>
</ul>
<p><strong>区别</strong>：仅测评 shell 脚本优化、环境配置或自然语言到 Bash 的翻译；Terminal-Bench 提供端到端、多步骤、长周期任务，考察代理在真实终端中的综合工程能力。</p>
<p>总结</p>
<p>Terminal-Bench 与上述研究的最大不同在于：</p>
<ol>
<li><strong>任务来源</strong>：直接采自开发者真实工作流程，非合成或单一领域；</li>
<li><strong>评估环境</strong>：完全在可复现的容器化终端内进行，允许任意命令与工具链；</li>
<li><strong>难度设计</strong>：人工三轮审核+对抗性审计，确保对前沿模型仍具挑战性；</li>
<li><strong>统一框架</strong>：通过 Harbor 与 Adapter 机制，将 26 个外部基准无缝接入同一评估管线，实现跨基准、跨代理的直接对比。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过“构建一个高保真、高难度、可复现的终端任务基准”来解决前述评估缺口，具体做法可归纳为以下六点：</p>
<p>1. 任务采集与 crowdsourcing</p>
<ul>
<li>开源众包：93 位贡献者共提交 229 个真实工作流任务，覆盖软件工程、系统管理、安全、科学计算等 17 个类别。</li>
<li>双维度人工估时：每位贡献者同时给出“专家”与“初级工程师”完成该任务所需时间，确保任务既真实又具备梯度难度。</li>
</ul>
<p>2. 严格的三轮人工+自动化质检流程</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">贡献者自检 → LLM 自动化工具筛查 → 三位资深评审人逐任务审计 → 合并后对抗性 exploit 审计 → 最终裁定</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>自动化工具</strong>：检测 Dockerfile 是否泄露测试、依赖是否固定、任务描述与测试是否互斥等 12 项硬性规则。</li>
<li><strong>对抗性审计</strong>：运行专用“作弊代理”尝试绕过测试，发现漏洞后人工修复。</li>
<li>平均每个任务累计 3 reviewer-hours，总计 &gt;300 人时仅用于质检。</li>
</ul>
<p>3. 容器化、可复现的任务定义</p>
<p>每个任务 =<br>① 自然语言指令（task.yaml）<br>② 容器镜像（Dockerfile + 固定版本依赖）<br>③ 自动化测试（pytest 验证最终状态，不监控中间命令）<br>④ 人工参考解（solution.sh）<br>⑤ 时间上限（10 min–168 h 分布）</p>
<p>测试只检查“最终容器状态”是否满足指令要求，允许代理自由选择实现路径，避免过度约束。</p>
<p>4. 中立评估框架 Harbor &amp; Terminus 2</p>
<ul>
<li><strong>Harbor</strong>：统一任务格式与执行引擎，支持并行百容器调度，已集成 26 个外部基准的 adapter。</li>
<li><strong>Terminus 2</strong>：极简“零工具”代理，仅通过 bash 命令交互，消除不同代理工具集差异带来的偏差，用于公平比较模型本身能力。</li>
</ul>
<p>5. 大规模实验与失败剖析</p>
<ul>
<li>32 155 次独立试验，16 个前沿模型 × 6 种代理框架。</li>
<li>构建双层错误分类体系：<br>– <strong>轨迹级</strong>：Execution / Coherence / Verification 三大类 9 子类，用 LLM-as-judge 自动标注，人工校验 κ=0.93。<br>– <strong>命令级</strong>：50 余种细粒度 CLI 错误（PATH、语法、依赖、网络、运行时崩溃等），采样 3 800 例失败命令进行归因。</li>
</ul>
<p>6. 持续迭代与防污染机制</p>
<ul>
<li>所有文件植入 Big-Bench canary 字符串，提醒训练方排除。</li>
<li>公开仓库 + 定期滚动新增任务，计划每年发布 harder subset，防止快速饱和。</li>
<li>提供完整运行 harness 与成本-性能 Pareto 曲线，方便社区复现与对标。</li>
</ul>
<p>通过上述设计，论文不仅“给出”了一个当前最强模型仍只能解决 ≈65% 的硬核基准，还提供了可扩展、可审计、可复现的整套基础设施，使后续研究能够：</p>
<ol>
<li>直接在同一框架下对比新模型/新代理；</li>
<li>依据细粒度错误分类针对性改进模型或脚手架；</li>
<li>持续追加更具挑战的真实任务，保持基准与前沿能力同步演进。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>Terminal-Bench 2.0</strong> 共设计并执行了 4 组互补实验，覆盖“宏观性能—成本—时间演化—微观失败”全链路，具体包括：</p>
<p>1. 主实验：16 模型 × 6 代理的大规模基准测试</p>
<ul>
<li><p><strong>规模</strong><br>– 89 任务 × 最少 5 次重复 = 32 155 条完整轨迹<br>– 16 个前沿模型（GPT-5.2、Claude Opus 4.5、Gemini 3 Pro 等闭源 + Qwen3-480B、Kimi-K2 等开源）<br>– 6 个代理脚手架（Codex CLI、Claude Code、Gemini CLI、OpenHands、Mini-SWE-Agent、自研 Terminus 2）</p>
</li>
<li><p><strong>观测指标</strong><br>– 任务解决率（Resolution Rate）与 95% 置信区间<br>– 单次试验成本（USD，按官方 API 价计算）<br>– 耗时、模型调用次数、输入/输出 token 量分布</p>
</li>
<li><p><strong>核心结果</strong><br>– 最强组合 GPT-5.2 + Codex CLI 仅 62.9% 成功率；开源最高 36%。<br>– 模型选择带来的提升（+52%）远大于脚手架选择（+17%）。<br>– 存在一条清晰的“成本–性能”Pareto 前沿，单次试验花费 0.1–100 USD。</p>
</li>
</ul>
<p>2. 纵向演化实验：8 个月内的模型迭代对比</p>
<ul>
<li><p><strong>方法</strong><br>将同一评估脚本回溯到 2025-04 至 2025-12 发布的 20 个模型检查点，用 Terminus 2 代理固定脚手架，观察解决率随时间变化。</p>
</li>
<li><p><strong>结果</strong><br>– 8 个月内 SOTA 解决率从 ≈35% 提升到 ≈63%，几乎翻倍。<br>– 线性外显提示：若趋势持续，1 年内可能逼近 80%，基准需滚动更新。</p>
</li>
</ul>
<p>3. 难度校准实验：人估难度 vs 模型实证难度</p>
<ul>
<li><p><strong>方法</strong><br>– 人类贡献者按“medium/hard”标签主观评级。<br>– 以 Terminus 2 在 6 个 frontier 模型上的平均通过率为客观指标，将任务再划分为 Easy(≥2/3 模型通过) / Medium / Hard(&lt;1/3 模型通过)。</p>
</li>
<li><p><strong>结果</strong><br>– 总体秩相关 r=0.436 (p&lt;0.001)。<br>– 93.3% 的人标 hard 任务确实被模型视为 hard；但 54.5% 的人标 medium 任务模型实际表现为 hard，揭示“需要人类直觉或对抗推理”的盲区。</p>
</li>
</ul>
<p>4. 失败剖析实验：轨迹级 + 命令级双层诊断</p>
<p>4.1 轨迹级失败分类（随机采样 2 条失败轨迹/模型）</p>
<ul>
<li><p><strong>分类体系</strong>（简化 MAST 框架）<br>– Execution：违规格、重复步骤、未感知终止条件<br>– Coherence：推理–行动不一致、上下文丢失、任务脱轨<br>– Verification：过早终止、无/错误验证、弱验证</p>
</li>
<li><p><strong>标注流程</strong><br>GPT-5 作为 LLM-judge，先与两名人类标注者在 120 轨迹上校准（precision 92%，recall 90%），再批量标注。</p>
</li>
<li><p><strong>结果</strong><br>– 闭源模型 60%+ 失败归于 Execution 类；开源模型错误更均衡。<br>– 提供热力图显示不同模型所需改进焦点不同。</p>
</li>
</ul>
<p>4.2 命令级失败分类（均匀采样 3 800 条失败命令）</p>
<ul>
<li><strong>构建 50+ 叶子节点的 CLI 错误 taxonomy</strong>（Invocation/Filesystem/Network/Runtime/REPL 等）。</li>
<li><strong>LLM-judge 自动归类</strong>，人工校验 50 例达成 82% 一致。</li>
<li><strong>结果</strong><br>– 最常见：24.1% “command not found” → 依赖缺失或 PATH 错误。<br>– 第二：9.6% 应用运行时崩溃；其余依次是语法、模块缺失、文件不存在等。<br>– 给出各模型错误分布圆环图，便于定位“环境/依赖”还是“代码逻辑”短板。</li>
</ul>
<p>5. 辅助实验</p>
<ul>
<li><strong>Token/Episode 效率分析</strong><br>– 发现成功率与交互轮数 (r=−0.084) 或输出 token 量 (r=−0.170) 均无明显正相关，提示“多而滥”≠“好”。</li>
<li><strong>任务超时 vs 成功率</strong><br>– 列出 89 任务按解决率排序的热力图，标识出“对角线带”——随着模型能力提升逐步被攻克的任务，可供后续渐进式研究。</li>
<li><strong>ABC 评估清单自评</strong><br>– 按 Agentic Benchmark Checklist 打分，Terminal-Bench 在“Outcome Validity 0.857 / Task Validity 1.000 / Reporting 0.830”均位列社区第二，验证其严谨性。</li>
</ul>
<p>综上，实验既给出了“宏观排行榜”，也拆解到“每一次命令为什么失败”，为后续模型、脚手架、训练策略的针对性改进提供了量化依据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下列出 10 个可直接在 Terminal-Bench 2.0 基础上延伸、且尚未被论文充分展开的研究方向，按“任务-模型-系统-评测”四个层面组织，供后续工作参考。</p>
<p>1. 任务层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>方向</th>
<th>可探索要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>T1</td>
<td>私有对抗测试集</td>
<td>公开仓库易被污染→保留 20–30% 任务不公开，定期“盲测”新模型，量化过拟合程度。</td>
</tr>
<tr>
<td>T2</td>
<td>动态难度升级</td>
<td>基于“对角线带”任务，自动插入随机化参数或增加约束（如内存上限、网络延迟），生成“同分布更难”变体，观察模型 Scaling Law 是否依然成立。</td>
</tr>
<tr>
<td>T3</td>
<td>多模态终端任务</td>
<td>当前纯文本→引入终端内嵌图片/视频（如验证码、QR-code、LaTeX 公式截图），考察 VLM 在终端里的多模态工具链能力。</td>
</tr>
</tbody>
</table>
</div>
<p>2. 模型层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>方向</th>
<th>可探索要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>M1</td>
<td>失败模式针对性微调</td>
<td>将轨迹级 9 类失败样本清洗为 50k–100k 条“纠错对话”继续预训练/RLHF，测量 Execution/Coherence/Verification 错误下降比例。</td>
</tr>
<tr>
<td>M2</td>
<td>工具调用 token 预算优化</td>
<td>引入“token-budget 控制器”，让模型在每一步自估剩余上下文，主动调用摘要或压缩工具，降低因 Context Loss 导致的失败。</td>
</tr>
<tr>
<td>M3</td>
<td>小模型专用脚手架蒸馏</td>
<td>用 GPT-5.2 轨迹做行为克隆，把多工具调用策略蒸馏到 7B 以下小模型，检验是否能在终端任务上逼近大模型性价比。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 系统层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>方向</th>
<th>可探索要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>S1</td>
<td>分层沙箱与快照回放</td>
<td>为长周期任务（&gt;1 h）引入“增量快照”机制，支持失败断点重跑与状态可视化，降低评估成本。</td>
</tr>
<tr>
<td>S2</td>
<td>资源感知调度</td>
<td>把任务标记为 CPU-heavy/GPU-heavy/IO-heavy，动态分配容器资源，验证是否减少 OOM、磁盘满等 14% 的运行时失败。</td>
</tr>
<tr>
<td>S3</td>
<td>分布式代理协作</td>
<td>将 89 任务拆成“规划-执行-验证”三角色，多容器间用 gRPC 通信，测量相比单代理的加速比与错误传播率。</td>
</tr>
</tbody>
</table>
</div>
<p>4. 评测与度量</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>方向</th>
<th>可探索要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>E1</td>
<td>人类基线再校准</td>
<td>招募 20 名专业开发者限时完成同一任务，记录人类真实耗时、工具链与错误分布，建立“人类-模型”双坐标难度标尺。</td>
</tr>
<tr>
<td>E2</td>
<td>可解释性评测插件</td>
<td>在 Harbor 中增加“解释器审计”步骤：代理必须输出每一步命令的意图摘要，用 LLM-judge 评估解释与行为一致性，量化“可解释性-成功率”权衡。</td>
</tr>
<tr>
<td>E3</td>
<td>经济价值指标</td>
<td>结合 SWE-Lancer 的时薪数据，把“任务解决率”换算为“美元收益/成本比”，给出模型在商业外包场景下的真实 ROI 曲线。</td>
</tr>
</tbody>
</table>
</div>
<p>5. 社区与工具</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>编号</th>
<th>方向</th>
<th>可探索要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>C1</td>
<td>在线 Leaderboard 防刷机制</td>
<td>引入“延迟公开+零知识证明”提交方案：参赛方提交容器哈希，官方在隔离环境重跑后才公布结果，防止测试集泄露。</td>
</tr>
</tbody>
</table>
</div>
<p>以上方向均可直接基于已开源的 Harbor 框架与 Terminal-Bench 2.0 数据进行，无需重新造轮；部分（如 T1、T3、S1）只需在现有 YAML 格式上扩展字段即可运行，具备快速验证与持续迭代的优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>Terminal-Bench 论文核心内容速览</p>
<p>1. 背景与痛点</p>
<ul>
<li>AI 代理在终端完成长周期、高价值任务（编译、调试、系统运维、科学计算）已成产业热点，但现有基准要么脱离真实环境，要么难度不足，无法对前沿模型形成有效压力。</li>
</ul>
<p>2. 贡献总览</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>贡献</th>
<th>一句话总结</th>
</tr>
</thead>
<tbody>
<tr>
<td>Terminal-Bench 2.0 数据集</td>
<td>89 个真实工作流任务，人工写指令+测试+参考解，三轮质检+对抗审计，平均 3 reviewer-小时/题。</td>
</tr>
<tr>
<td>Harbor 评估框架</td>
<td>容器化、可复现、统一任务格式；已集成 26 个外部基准的 adapter；支持并行百容器。</td>
</tr>
<tr>
<td>Terminus 2 中立代理</td>
<td>仅通过 bash 交互，消除工具差异，公平比较模型本身能力。</td>
</tr>
<tr>
<td>大规模实验</td>
<td>32 155 次试验，16 前沿模型 × 6 代理；最强组合解决率 63%，开源最佳 36%。</td>
</tr>
<tr>
<td>双层失败剖析</td>
<td>轨迹级 9 类失败+命令级 50+ 细粒度错误，定位依赖缺失、语法、运行时崩溃等主因。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 关键发现</p>
<ul>
<li>模型选择对性能提升（+52%）远大于代理脚手架（+17%）。</li>
<li>8 个月内 SOTA 解决率几乎翻倍，基准预计一年内逼近饱和，需滚动更新。</li>
<li>人标“中等”任务中 54% 实际被模型视为 hard，揭示人类直觉与模型能力盲区。</li>
<li>Execution 类错误占闭源模型失败 60%+，开源模型则全面落后；命令失败 24% 来自“command not found”。</li>
<li>成功率与交互轮数/输出 token 量均无明显正相关，质量而非数量决定成败。</li>
</ul>
<p>4. 数据与代码</p>
<ul>
<li>全部任务、Harbor 框架、实验配置已开源：<br><a target="_blank" rel="noopener" href="https://tbench.ai">https://tbench.ai</a> 与 <a target="_blank" rel="noopener" href="https://github.com/laude-institute/terminal-bench">github.com/laude-institute/terminal-bench</a></li>
</ul>
<p>5. 影响与展望</p>
<ul>
<li>提供首个面向“真实终端+长周期+高价值”工作的评测基线，可直接用于模型/代理迭代。</li>
<li>失败分类与成本-性能 Pareto 曲线为后续训练、脚手架设计、资源调度提供量化靶点。</li>
<li>呼吁社区持续贡献更难任务并建立私有对抗测试集，防止快速过拟合。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Terminal-Bench%3A%20Benchmarking%20Agents%20on%20Hard%2C%20Realistic%20Tasks%20in%20Command%20Line%20Interfaces，链接是https://arxiv.org/pdf/2601.11868，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11868。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Mike A. Merrill,Alexander G. Shaw,Nicholas Carlini,Boxuan Li,Harsh Raj,Ivan Bercovich,Lin Shi,Jeong Yeon Shin,Thomas Walshe,E. Kelly Buchanan,Junhong Shen,Guanghao Ye,Haowei Lin,Jason Poulos,Maoyu Wang,Marianna Nezhurina,Jenia Jitsev,Di Lu,Orfeas Menis Mastromichalakis,Zhiwei Xu,Zizhao Chen,Yue Liu,Robert Zhang,Leon Liangyu Chen,Anurag Kashyap,Jan-Lucas Uslu,Jeffrey Li,Jianbo Wu,Minghao Yan,Song Bian,Vedang Sharma,Ke Sun,Steven Dillmann,Akshay Anand,Andrew Lanpouthakoun,Bardia Koopah,Changran Hu,Etash Guha,Gabriel H. S. Dreiman,Jiacheng Zhu,Karl Krauth,Li Zhong,Niklas Muennighoff,Robert Amanfu,Shangyin Tan,Shreyas Pimpalgaonkar,Tushar Aggarwal,Xiangning Lin,Xin Lan,Xuandong Zhao,Yiqing Liang,Yuanli Wang,Zilong Wang,Changzhi Zhou,David Heineman,Hange Liu,Harsh Trivedi,John Yang,Junhong Lin,Manish Shetty,Michael Yang,Nabil Omi,Negin Raoof,Shanda Li,Terry Yue Zhuo,Wuwei Lin,Yiwei Dai,Yuxin Wang,Wenhao Chai,Shang Zhou,Dariush Wahdany,Ziyu She,Jiaming Hu,Zhikang Dong,Yuxuan Zhu,Sasha Cui,Ahson Saiyed,Arinbjörn Kolbeinsson,Jesse Hu,Christopher Michael Rytting,Ryan Marten,Yixin Wang,Alex Dimakis,Andy Konwinski,Ludwig Schmidt</p>
<p><strong>Categories</strong>: cs.SE,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.11868.pdf">https://arxiv.org/pdf/2601.11868.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.11868">https://arxiv.org/abs/2601.11868</a></p>
<p><strong>Arxiv ID</strong>: 2601.11868</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.11868">https://papers.cool/arxiv/2601.11868</a></p>
<p><strong>Published</strong>: 2026-01-17T01:29:30Z</p>
<p><strong>Updated</strong>: 2026-01-17T01:29:30.000Z</p>
<hr>
<h3 id="12-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation"><a href="#12-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation" class="headerlink" title="12. OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15369">OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</a></h3><p>This paper presents a family of advanced vision encoder, named OpenVision 3, that learns a single, unified visual representation that can serve both image understanding and image generation. Our core architecture is simple: we feed VAE-compressed image latents to a ViT encoder and train its output to support two complementary roles. First, the encoder output is passed to the ViT-VAE decoder to reconstruct the original image, encouraging the representation to capture generative structure. Second, the same representation is optimized with contrastive learning and image-captioning objectives, strengthening semantic features. By jointly optimizing reconstruction- and semantics-driven signals in a shared latent space, the encoder learns representations that synergize and generalize well across both regimes. We validate this unified design through extensive downstream evaluations with the encoder frozen. For multimodal understanding, we plug the encoder into the LLaVA-1.5 framework: it performs comparably with a standard CLIP vision encoder (e.g., 62.4 vs 62.2 on SeedBench, and 83.7 vs 82.9 on POPE). For generation, we test it under the RAE framework: ours substantially surpasses the standard CLIP-based encoder (e.g., gFID: 1.89 vs 2.54 on ImageNet). We hope this work can spur future research on unified modeling.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本文提出了一种先进的视觉编码器家族，命名为 OpenVision 3，它能够学习单一、统一的视觉表示，可同时服务于图像理解和图像生成。我们的核心架构很简单：将 VAE 压缩的图像潜变量输入到 ViT 编码器，并训练其输出以支持两种互补功能。首先，编码器输出被传递到 ViT-VAE 解码器以重建原始图像，从而鼓励表示捕捉生成结构。其次，同一表示通过对比学习和图像描述目标进行优化，增强语义特征。通过在共享潜在空间中联合优化重建驱动和语义驱动的信号，编码器学习的表示在两种任务中协同工作并具有良好的泛化能力。我们通过广泛的下游评估验证了这一统一设计，在评估中保持编码器冻结状态。对于多模态理解，我们将编码器嵌入 LLaVA-1.5 框架：其表现可与标准 CLIP 视觉编码器相媲美（例如，在 SeedBench 上 62.4 对 62.2，POPE 上 83.7 对 82.9）。对于生成任务，我们在 RAE 框架下进行测试：结果远超标准的基于 CLIP 的编码器（例如，在 ImageNet 上 gFID：1.89 对 2.54）。我们希望这项工作能够激发未来在统一建模方面的研究。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“统一多模态模型（UMM）”中视觉表征的割裂问题：<br>传统方法通常为<strong>理解任务</strong>（如 VQA、图像检索）和<strong>生成任务</strong>（如文生图、图像重建）分别设计独立的视觉 tokenizer，导致</p>
<ul>
<li>系统复杂度翻倍</li>
<li>两组 token 之间难以深度协同</li>
<li>量化离散 token 引入信息损失，限制生成质量</li>
</ul>
<p>OpenVision 3 的目标是用<strong>单一连续视觉编码器</strong>同时支撑理解与生成，在共享隐空间中一次性提取“既能重建像素、又能对齐语义”的统一表征，从而</p>
<ol>
<li>降低模型与部署复杂度</li>
<li>避免离散化误差</li>
<li>让重建信号与语义信号相互增强，实现“一石二鸟”的联合优化</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 OpenVision 3 直接相关的研究可归纳为三条主线，每条线均给出最具代表性的工作并指出与本文的差异/继承关系。</p>
<ol>
<li>视觉-语言预训练（VLP）——提供语义监督范式</li>
</ol>
<ul>
<li>CLIP 系列</li>
<li>原始 CLIP：对比学习对齐图文，纯理解导向。</li>
<li>CoCa、SigLIP / SigLIP2：在对比损失之外增加 caption 损失，强化密集语义。</li>
<li>OpenVision 1&amp;2：证明“仅用 caption 损失”即可低成本达到 CLIP 级别理解性能。<br>→ OpenVision 3 继承“caption + 对比”双目标，但将训练从像素空间搬到 VAE 隐空间，使同一表征同时支持生成。</li>
</ul>
<ol>
<li>统一 Tokenizer——试图用一套 token 同时服务理解与生成</li>
</ol>
<ul>
<li>离散路线（VQ 类）</li>
<li>VQGAN、TokenFlow、UniTok、EMU3.5：通过向量量化码本统一两种特征，存在码本塌陷、重建细节损失。</li>
<li>连续路线（VAE+ViT 类）</li>
<li>Show-o2：在 VAE latent 上再做语义/像素双投影，仍需额外融合模块。</li>
<li>TUNA：结构与本文最相似（VAE→ViT），但直接加载预训练 ViT 权重，未公开训练策略。<br>→ OpenVision 3 首次给出“从零训练”完整范式，并证明重建与理解损失可相互增益，无需量化即可达到 SOTA 重建质量。</li>
</ul>
<ol>
<li>生成式视觉编码器——仅关注重建/生成质量</li>
</ol>
<ul>
<li>SD-VAE、SD3-VAE、FLUX-VAE、Wan2.1-VAE：专用重建 tokenizer，无语义监督，无法直接用于理解任务。</li>
<li>RAE：用 CLIP 作为语义先验训练 DiT，但 CLIP 本身重建能力弱，导致 gFID 较高。<br>→ OpenVision 3 用同一套连续特征替换 CLIP，在 RAE 框架下 gFID 从 2.54 降至 1.89，验证“统一特征”对生成的正面作用。</li>
</ul>
<p>综上，OpenVision 3 在 VLP 的语义监督策略、统一 tokenizer 的连续化趋势、以及生成式编码器的隐空间建模三方面均有直接技术继承，并通过“VAE+ViT 联合训练”首次实现三者一体化。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“统一视觉编码”转化为<strong>单流连续隐空间联合优化</strong>问题，通过三项关键设计一次性解决表征割裂、量化误差与训练不透明三大痛点。</p>
<ol>
<li>架构：VAE+ViT 单流 tokenizer</li>
</ol>
<ul>
<li>冻结 FLUX.1-VAE，把图像压缩为 8× 下采样 latent</li>
<li>轻量 ViT 继续压缩到 16× 下采样，输出连续向量  z_u </li>
<li>同一  z_u  同时送入两条<strong>完全解耦</strong>的解码分支，保证任务特定解码器不相互干扰</li>
</ul>
<ol>
<li>双分支目标函数：重建与语义在同隐空间共生</li>
</ol>
<ul>
<li>重建分支</li>
<li>对  z_u  加噪  z_u=z_u+σ⊙ε  再 ViT-Decoder→VAE-Decoder，回归原图与 VAE latent</li>
<li>损失： L<em>(rec)=‖x−x‖_1+β‖z</em>(vae)−z<em>(vae)‖_1+λL</em>(LPIPS) </li>
<li>理解分支</li>
<li>对比损失： L<em>(contrastive)(z_u, z</em>(txt)) </li>
<li>caption 损失：自回归预测合成文本</li>
<li>损失： L<em>(und)=L</em>(caption)+αL_(contrastive) </li>
<li>整体： L<em>(overall)=ω</em>(rec)L<em>(rec)+ω</em>(und)L<em>(und) ，且  ω</em>(und)=2ω_(rec) ，优先保证语义不降级</li>
</ul>
<ol>
<li>训练范式：低→高分辨率渐进 + 从零初始化</li>
</ol>
<ul>
<li>128² 预训练 1000 epoch → 256² 微调 200 epoch，10:1 计算分配，降低高分辨率开销</li>
<li>除冻结 VAE 外全部参数随机初始化，公开完整超参与数据配比，解决“如何训练”不透明问题</li>
</ul>
<ol>
<li>协同机制：重建-语义相互增益（§4.5 实验验证）</li>
</ol>
<ul>
<li>仅训练  L_(und)  时，像素与 latent 重建误差仍显著下降</li>
<li>仅训练  L_(rec)  时，caption 损失也能自发降低<br>→ 证明共享连续隐空间下，低层纹理与高层语义形成正反馈，无需额外正则即可兼得</li>
</ul>
<p>通过上述设计，OpenVision 3 用<strong>一套连续 token</strong> 同时达到 CLIP 级理解能力与专用 VAE 级重建/生成质量，首次在统一框架内把 rFID 降到 0.22、gFID 降到 1.89，实现“一个编码器，两用输出”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在<strong>重建、生成、理解</strong>三大任务上分别进行<strong>冻结编码器</strong>的评测，保证所有指标反映的是统一表征的迁移能力，而非任务特定微调。</p>
<ol>
<li>重建实验（表 2）<br>数据集：ImageNet / COCO 验证集，256×256 中心裁剪<br>指标：PSNR、SSIM、LPIPS、rFID<br>结果：</li>
</ol>
<ul>
<li>OpenVision 3 30.33 dB / 0.885 / 0.061 / 0.216，全面超越所有统一 tokenizer（UniTok 25.34 dB / 0.132 / 0.362）</li>
<li>与专用生成 tokenizer（FLUX-VAE 32.86 dB / 0.044 / 0.176）差距 &lt; 0.04 LPIPS，验证连续隐空间无显著信息损失</li>
</ul>
<ol>
<li>生成实验（表 4 + 图 4）<br>协议：冻结 tokenizer，仅用其  z_u  训练 RAE-DiT 流匹配模型，ImageNet 256² 类条件生成<br>指标：gFID、IS、Precision、Recall<br>结果：</li>
</ol>
<ul>
<li>gFID 1.89，比 CLIP-RAE 的 2.54 相对降低 26%</li>
<li>IS 289.2、Pre 0.84、Rec 0.59，四项同时拿下最佳，可视化样本兼具保真度与多样性</li>
</ul>
<ol>
<li>理解实验（表 3）<br>协议：将冻结的视觉编码器直接插入 LLaVA-1.5，沿用原训练超参，保持 256 token 数量一致<br>基准：MME-P/MME-C、SeedBench、ScienceQA、GQA、POPE<br>结果：</li>
</ol>
<ul>
<li>B/16 级别：SeedBench 62.4 vs CLIP 62.2，POPE 83.7 vs 82.9，其余指标持平</li>
<li>L/14 级别：SeedBench 66.0 vs 65.4，POPE 85.3 vs 84.7，MME-C 299 vs 292，整体小胜或持平<br>→ 证明统一连续特征不牺牲语义精度</li>
</ul>
<ol>
<li>消融与机理分析（图 2–3）</li>
</ol>
<ul>
<li>仅训练理解损失 → 重建误差仍大幅下降</li>
<li>仅训练重建损失 → caption 与对比损失也能自发降低<br>结论：共享隐空间下，语义与像素目标天然互促，无需额外正则即可兼得</li>
</ul>
<ol>
<li>训练效率对比</li>
</ol>
<ul>
<li>低→高分辨率渐进策略：用 10% 的高分辨率 epoch 即获得与全高分辨率训练相同性能，计算节省 ≈ 5×</li>
<li>数据：DataComp-1B 经 LLaVA-Llama-3 重标注，训练成本公开，强调可复现性</li>
</ul>
<p>综上，实验覆盖<strong>重建质量、生成保真、语义理解、训练开销、内部协同机制</strong>五个维度，全面验证“单流连续统一 tokenizer”在冻结条件下的跨任务可用性与 SOTA 性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“表征-架构”“训练-策略”“应用-系统”三大层面，均保持与 OpenVision 3 的连续统一隐空间设定兼容。</p>
<p>表征-架构</p>
<ol>
<li><p><strong>可变压缩率 tokenizer</strong><br>当前固定 16× 下采样，可引入<strong>空域-通道复合注意力</strong>，让模型在 8×-32× 区间按需切换，兼顾高分辨率生成与少 token 理解。</p>
</li>
<li><p><strong>多尺度 VAE latent 融合</strong><br>采用 4×/8×/16× 三级 VAE 输出，通过门控混合专家（MoE）注入 ViT，使细粒度纹理与全局语义在同一前向传播中动态加权。</p>
</li>
<li><p><strong>交叉模态量化校准</strong><br>对连续  z_u  做<strong>可微分软量化</strong>，在保持端到端梯度的同时，验证极低比特（2-4 bit）隐空间能否无损支撑理解与生成，迈向端侧部署。</p>
</li>
</ol>
<p>训练-策略</p>
<ol>
<li><p><strong>时间-空间联合降噪</strong><br>把重建分支的加噪策略从 IID 高斯升级为<strong>时空相关掩码</strong>（blotch、frame-drop），测试统一表征在视频生成/视频理解上的外延能力。</p>
</li>
<li><p><strong>对抗-重建协同</strong><br>在重建分支引入<strong>判别器</strong>，用 GAN 损失与 LPIPS 共同约束，观察 gFID 能否进一步下降，同时监控理解基准不掉点。</p>
</li>
<li><p><strong>自监督文本增强</strong><br>用 LLM 对原始 caption 做<strong>语义等价改写+难度分级</strong>，在对比损失中引入硬负样本挖掘，提升细粒度对齐而不增加人工标注。</p>
</li>
</ol>
<p>应用-系统</p>
<ol>
<li><p><strong>任意分辨率生成</strong><br>基于 DiT 的“位置嵌入插值”+ 2×2 patch 设计，验证统一 tokenizer 能否在 512-1024² 区间<strong>零样本外推</strong>，实现原生高清文生图。</p>
</li>
<li><p><strong>多模态 in-context 条件</strong><br>把  z_u  与文本、边缘、深度、姿态等条件在通道维度拼接，测试单 encoder 是否支持<strong>多条件混合生成</strong>而无需额外 adapter。</p>
</li>
<li><p><strong>端侧级联推理</strong><br>将 ViT encoder 拆分为<strong>轻量边缘层+云端深层</strong>，早期 token 先进行粗粒度理解（召回），再按需传输深层特征做精细生成，降低移动端带宽。</p>
</li>
<li><p><strong>统一 tokenizer 作为教师</strong><br>用 OpenVision 3 的连续  z_u  蒸馏<strong>离散 VQ 学生</strong>，探究“连续→离散”知识迁移上限，为已部署 VQ 系统提供无痛升级路径。</p>
</li>
</ol>
<p>以上方向均可在不改变“单流连续隐空间”核心思想的前提下，进一步挖掘其<strong>伸缩性、压缩率、跨模态扩展与系统级效率</strong>的边界。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：统一多模态模型需同时做视觉理解与生成，传统方案用两套 tokenizer（语义 token+像素 token）或离散量化 token，导致系统复杂、信息损失、协同困难。</li>
<li><strong>思路</strong>：用一个<strong>连续视觉编码器</strong>同时输出“可重建像素”且“可对齐语义”的表征，在共享隐空间里联合优化两大任务。</li>
<li><strong>架构</strong>：冻结 FLUX.1-VAE → 轻量 ViT 得统一 token  z_u  → 两条解耦分支</li>
<li>重建分支：加噪后 ViT-Decoder+VAE-Decoder，回归原图与 latent，损失  L_(rec) </li>
<li>理解分支：对比+caption，损失  L<em>(und)<br>整体  L=0.5L</em>(rec)+1.0L_(und) ，从零训练。</li>
<li><strong>训练</strong>：128² 预训练 1000 epoch → 256² 微调 200 epoch，低-高分辨率 10:1 计算分配；数据用 DataComp-1B 重标注。</li>
<li><strong>实验</strong>（编码器全程冻结）</li>
<li>重建：ImageNet 30.33 dB / 0.061 LPIPS / 0.22 rFID，超越所有统一 tokenizer，逼近专用 VAE。</li>
<li>生成：RAE-DiT 框架 gFID 1.89，比 CLIP-RAE 的 2.54 ↓26%。</li>
<li>理解：接入 LLaVA-1.5，SeedBench 66.0 vs CLIP 65.4，POPE 85.3 vs 84.7，整体持平或小胜。</li>
<li><strong>发现</strong>：仅训练语义损失也能降重建误差，反之亦然，验证共享连续隐空间下像素与语义目标天然互促。</li>
<li><strong>结论</strong>：OpenVision 3 用单流连续 token 首次在冻结条件下达到 CLIP 级理解+SOTA 重建/生成，为统一视觉编码提供简单可复现的新基线。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是OpenVision%203%3A%20A%20Family%20of%20Unified%20Visual%20Encoder%20for%20Both%20Understanding%20and%20Generation，链接是https://arxiv.org/pdf/2601.15369，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15369。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Letian Zhang,Sucheng Ren,Yanqing Liu,Xianhang Li,Zeyu Wang,Yuyin Zhou,Huaxiu Yao,Zeyu Zheng,Weili Nie,Guilin Liu,Zhiding Yu,Cihang Xie</p>
<p><strong>Categories</strong>: eess.IV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15369.pdf">https://arxiv.org/pdf/2601.15369.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15369">https://arxiv.org/abs/2601.15369</a></p>
<p><strong>Arxiv ID</strong>: 2601.15369</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15369">https://papers.cool/arxiv/2601.15369</a></p>
<p><strong>Published</strong>: 2026-01-21T18:47:12Z</p>
<p><strong>Updated</strong>: 2026-01-21T18:47:12.000Z</p>
<hr>
<h3 id="13-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing"><a href="#13-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing" class="headerlink" title="13. Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16125">Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</a></h3><p>Composed Image Retrieval (CIR) is a pivotal and complex task in multimodal understanding. Current CIR benchmarks typically feature limited query categories and fail to capture the diverse requirements of real-world scenarios. To bridge this evaluation gap, we leverage image editing to achieve precise control over modification types and content, enabling a pipeline for synthesizing queries across a broad spectrum of categories. Using this pipeline, we construct EDIR, a novel fine-grained CIR benchmark. EDIR encompasses 5,000 high-quality queries structured across five main categories and fifteen subcategories. Our comprehensive evaluation of 13 multimodal embedding models reveals a significant capability gap; even state-of-the-art models (e.g., RzenEmbed and GME) struggle to perform consistently across all subcategories, highlighting the rigorous nature of our benchmark. Through comparative analysis, we further uncover inherent limitations in existing benchmarks, such as modality biases and insufficient categorical coverage. Furthermore, an in-domain training experiment demonstrates the feasibility of our benchmark. This experiment clarifies the task challenges by distinguishing between categories that are solvable with targeted data and those that expose intrinsic limitations of current model architectures.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>组合图像检索（CIR）是在多模态理解中一个关键且复杂的任务。当前的CIR基准通常仅包含有限的查询类别，无法涵盖真实场景的多样化需求。为弥合这一评估差距，我们利用图像编辑实现对修改类型和内容的精确控制，从而构建了一个跨广泛类别生成查询的流程。基于该流程，我们构建了EDIR，一个新颖的细粒度CIR基准。EDIR包括5,000个高质量查询，结构涵盖五大主类别和十五个子类别。我们对13种多模态嵌入模型进行了全面评估，结果显示存在显著的能力差距；即便是最先进的模型（如RzenEmbed和GME）在所有子类别中也难以保持一致性能，这突显了我们基准的严格性。通过比较分析，我们进一步揭示了现有基准的固有局限性，例如模态偏差和类别覆盖不足。此外，域内训练实验表明我们的基准是可行的。该实验通过区分可通过特定数据解决的类别与揭示当前模型架构固有限制的类别，明确了任务挑战。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“组合图像检索（Composed Image Retrieval, CIR）”的评测瓶颈，提出并解决以下核心问题：</p>
<ol>
<li>现有评测过于粗粒度</li>
</ol>
<ul>
<li>已有数据集（CIRR、FashionIQ、CIRCO 等）的查询类别稀少且分布失衡，难以覆盖真实场景中的多样化修改需求。</li>
<li>导致模型在单一或偏置指标上得分高，却无法暴露细粒度能力缺陷。</li>
</ul>
<ol>
<li>构建规模与可控性不足</li>
</ol>
<ul>
<li>传统“先找目标图、后补描述”的标注方式，使某些类别（如 remove、spatial、texture）天然稀缺，且查询内容与类别信号对齐度低。</li>
<li>缺乏系统化、可扩展的管道来按需生成大量高质量查询-图像对。</li>
</ul>
<ol>
<li>模态偏置与评测盲区</li>
</ol>
<ul>
<li>现有 benchmark 存在显著“文本捷径”：模型仅依赖文本即可取得高分，未能真正检验图文组合推理能力。</li>
<li>缺乏能够区分「数据不足导致的性能差」与「模型架构固有缺陷」的评测框架。</li>
</ul>
<p>为解决上述问题，论文：</p>
<ul>
<li>提出五主类十五子类的细粒度 CIR 评测 taxonomy；</li>
<li>设计“文本驱动图像编辑→查询重写→两级过滤”的自动化数据合成管道，实现类别与内容的精准控制；</li>
<li>构建含 5 000 查询、178 645 图库的新基准 EDIR，覆盖所有子类且分布均衡；</li>
<li>通过 13 个多模态模型的全面评测与域内训练实验，验证 EDIR 能同时揭示数据可解挑战与模型固有弱点，从而推动真正具备组合推理能力的 CIR 模型发展。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在 §2 Related Works 中系统梳理了与 Composed Image Retrieval（CIR）直接相关的两条研究脉络，并在实验部分进一步对比了 13 个代表性模型。可归纳为以下三类相关研究：</p>
<ol>
<li>CIR 评测基准（Benchmarks）</li>
</ol>
<ul>
<li><strong>FashionIQ</strong> (Wu et al., 2021)<br>领域限定于时尚，查询仅围绕颜色、版型等属性，规模 6 k/15 k。</li>
<li><strong>CIRR</strong> (Liu et al., 2021)<br>开放域但查询粗粒度，验证集 4 k/2 k，缺乏细分类别。</li>
<li><strong>CIRCO</strong> (Baldrati et al., 2023)<br>引入 9 类标签，但分布极不平衡（“direct addressing” 类占比过高），仅 1 k 查询。</li>
<li><strong>GeneCIS</strong> (Vaze et al., 2023)<br>侧重动态相似度概念，强调条件相似而非细粒度编辑。</li>
<li><strong>I-CIR</strong> (Psomas et al., 2025)<br>提出实例级检索，但仅 7 类标签，规模 1.8 k，且存在显著模态偏置。<br>上述工作均被本文指出“类别覆盖不足、模态偏置、规模受限”等共性缺陷，成为 EDIR 要补足的对比对象。</li>
</ul>
<ol>
<li>CIR 方法学（Methods）<br>2.1 非 MLLM 范式</li>
</ol>
<ul>
<li><strong>PIC2WORD</strong> (Saito et al., 2023) → 文本反演，将参考图映射为伪词 token。</li>
<li><strong>SEARLE</strong> (Baldrati et al., 2023) → 基于 CLIP 的文本反演 + 残差学习。</li>
<li><strong>MAGICLENS</strong> (Zhang et al., 2024) → 大规模三元组训练，CLIP-L/14 骨干。</li>
</ul>
<p>2.2 MLLM 统一嵌入范式</p>
<ul>
<li><strong>GME</strong> (Zhang et al., 2024)<br>基于 Qwen2-VL，提出 multimodal contrastive fine-tuning。</li>
<li><strong>RzenEmbed</strong> (Jian et al., 2025)<br>在 Qwen2-VL-7B 上引入多任务检索目标，当前 EDIR 上最强零样本模型。</li>
<li><strong>Ops-embedding</strong> (OpenSearch-AI, 2025)<br>采用 Qwen2-VL-7B，强调 ops-level 指令对齐。</li>
<li><strong>VLM2Vec</strong> (Jiang et al., 2024b)<br>将 VL 模型转化为通用嵌入提取器。</li>
<li><strong>E5-V、UniME、mmE5</strong> 等<br>分别基于 Llava-LLaMA、Qwen2-VL、Llama-3.2-Vision，探索不同规模的 MLLM 骨干。</li>
</ul>
<p>2.3 免训练方法</p>
<ul>
<li><strong>Vision-by-Language</strong> (Karthik et al., 2024)<br>利用现成 caption 模型 + LLM 推理，零样本组合检索。</li>
<li><strong>LDRE</strong> (Yang et al., 2024)<br>引入 LLM-based divergent reasoning 与集成投票，无需额外训练。</li>
</ul>
<ol>
<li>图像编辑与数据合成（支撑 EDIR 构建）</li>
</ol>
<ul>
<li><strong>InstructPix2Pix</strong> (Brooks et al., 2023)<br>首次提出“指令式图像编辑”范式，被后续 Qwen-ImageEdit 等模型扩展。</li>
<li><strong>Qwen-ImageEdit</strong> (Wu et al., 2025)<br>本文实际采用的编辑模型，支持多轮、多属性组合编辑。</li>
<li><strong>MegaPairs</strong> (Zhou et al., 2025)<br>利用扩散模型大规模合成图文对，用于通用多模态检索训练，但未针对 CIR 细粒度评测设计。</li>
</ul>
<p>综上，EDIR 与上述基准相比在类别广度、规模、均衡性、模态偏置控制上均提出改进；与上述方法相比，则通过全新细粒度评测揭示了现有模型（包括最新 MLLM 嵌入）在组合推理、否定表达、细粒度细节等方面的共性缺陷。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“CIR 评测粗粒度、类别缺失、模态偏置”三大痛点拆解为四个可执行步骤，并给出对应技术方案，形成端到端解决路径：</p>
<ol>
<li>建立细粒度、全覆盖的评测框架</li>
</ol>
<ul>
<li>提出五主类十五子类的层级 taxonomy（Attribute / Object / Relationship / Global Env / Complex），覆盖真实场景 90% 以上修改需求。</li>
<li>为每子类撰写原子化、无歧义的操作定义，确保后续可自动采样与验证。</li>
</ul>
<ol>
<li>设计“文本→编辑→查询”的可控数据合成管道<br>2.1 种子筛选</li>
</ol>
<ul>
<li>用 Qwen2.5-VL-32B 对 LAION-400M 进行图文质量过滤，剔除文本图、空白图、低分辨率图。<br>2.2 指令生成</li>
<li>同一图让 MLLM 自评可编辑的 5–6 个子类，每子类输出 3 条原子指令，形成指令池。<br>2.3 组合式编辑</li>
<li>采用 Qwen-ImageEdit 对源图 Ir 施加“基础修改 {a,b} + 区分修改 {c,d}”的复合指令，一次生成 n 张相似编辑图；随机选 1 张作目标 It，其余做难负例。</li>
<li>通过“共享上下文 + 微小差异”策略，保证检索任务既非过易也非过难。<br>2.4 查询重写</li>
<li>用 Qwen3-32B 把原始指令改写成自然语言 CIR 查询 Tm；支持直接改写与否定改写两种模板，显式扩充“remove / not red”等否定场景。<br>2.5 两级过滤</li>
<li>级 1：校验生成图是否忠实执行复合指令；级 2：校验{Ir, Tm, It}三元组是否匹配。共清洗掉约 80% 不合格样本，最终保留 5 k 查询、178 k 图库，且 15 子类严格均衡（每类 300/800 条）。</li>
</ul>
<ol>
<li>构建新基准 EDIR 并全面评测</li>
</ol>
<ul>
<li>在 EDIR 上测试 13 个代表性模型（CLIP-based vs. MLLM-based），发现：<br>– 非 MLLM 平均 Recall@1 仅 18.4%，细粒度细节类（texture/shape/remove）全部低于 20%；<br>– MLLM 虽提升至 36.9%，但 texture/remove/shape 仍低于 30%，暴露固有缺陷。</li>
<li>与 CIRR/CIRCO/FashionIQ/GeneCIS 做 Spearman 相关分析，证实旧基准存在“类别覆盖不足 + 文本捷径”双重偏置，而 EDIR 与所有子类均呈正相关且无明显捷径。</li>
</ul>
<ol>
<li>用“域内训练”验证问题可解性</li>
</ol>
<ul>
<li>基于同一管道合成 225 k 训练三元组（每子类 15 k），训练 Qwen2.5-VL-7B 得到 EDIR-MLLM。</li>
<li>结果：整体 Recall@1 从 36.9% → 59.9%，提升 23 个百分点；<br>– 颜色、材质、纹理、动作 等“数据稀缺”类涨幅 ≥ 25%，证明其为数据可解问题；<br>– 计数、空间、视角 等“组合推理”类涨幅 &lt; 15%，揭示模型架构内在局限。</li>
</ul>
<p>通过“taxonomy→合成管道→新基准→大规模评测→域内训练”这一闭环，论文不仅提供了高质量评测工具 EDIR，也首次系统地区分了“数据不足”与“架构缺陷”两类挑战，为后续 CIR 研究指明改进方向。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“新基准是否有效、模型能力差距在哪、旧基准为何失效、挑战能否被数据解决”四个问题，设计并执行了以下实验：</p>
<ol>
<li>主实验：13 个模型在 EDIR 上的 zero-shot 评测</li>
</ol>
<ul>
<li>指标：Recall@1（主表）+ Recall@3（附录）</li>
<li>分组：<br>– 非 MLLM 组（CLIP-L/14 骨干）：PIC2WORD、SEARLE、MAGICLENS<br>– MLLM 嵌入组：RzenEmbed-7B、Ops-embedding-7B、GME-2B/7B、MMRet-MLLM、E5-V-7B、VLM2Vec-2B、UniME-2B/7B、mmE5-11B</li>
<li>结果：<br>– 非 MLLM 平均 18.4%，最高 22.0%；所有细粒度类（texture/shape/remove）&lt; 20%。<br>– MLLM 平均 36.9%，最佳单模型 47.2%；但 texture 仅 29.7%、remove 20.2%、shape 28.6%，显著落后 addition/replace 等类。</li>
</ul>
<ol>
<li><p>错误模式细查（Error Analysis）<br>人工抽样 RzenEmbed-7B 低分案例，归纳四类失效：</p>
</li>
<li><p>否定失效（negation）：显式“not red”或隐式“remove”均无法映射到目标状态。</p>
</li>
<li>组合推理缺陷：count、spatial、viewpoint、style 需同时理解多对象或全局上下文，模型常只抓住局部。</li>
<li>多约束部分满足：Complex 类查询含 ≥2 条件，召回图只满足其一。</li>
<li>细粒度不敏感：material/texture/shape 的微小视觉差异被忽略。</li>
<li>与旧基准的相关性分析（Benchmark Correlation）</li>
</ol>
<ul>
<li>计算 9 个 MLLM 模型在 5 个 benchmark 上的 Spearman ρ：<br>– EDIR ↔ CIRR ρ=0.78；EDIR ↔ CIRCO ρ=0.43；EDIR ↔ FashionIQ ρ=0.75；EDIR ↔ GeneCIS ρ=0.53。</li>
<li>解释：CIRCO 文本捷径最重（text-only 得分反而↑），故相关性最低；EDIR 与所有子类均正相关，验证其覆盖广度。</li>
</ul>
<ol>
<li>模态偏置对照实验（Modality Bias）</li>
</ol>
<ul>
<li>对同一组 MLLM 分别输入：<br>– 仅文本（text-only）<br>– 仅图像（image-only）<br>– 图文组合（text+image）</li>
<li>结果：CIRCO 上 text-only 平均反超 2–5 pp；EDIR 上 text-only 跌至 15% 以下，证明其强制要求图文组合推理。</li>
</ul>
<ol>
<li>域内训练可解性实验（In-domain Training）</li>
</ol>
<ul>
<li>用同一合成管道额外生成 1.1 M 三元组，按子类均衡采样 225 k 训练集。</li>
<li>基座：Qwen2.5-VL-7B → 训练 2 500 step，批大小 128，InfoNCE-loss，得 EDIR-MLLM。</li>
<li>结果：<br>– EDIR Recall@1 59.9%，相对 zero-shot 平均提升 23 pp；Recall@3 达 80.8%。<br>– 子类涨幅分化：<br>– “数据稀缺”型（color/material/texture/action）提升 25–35 pp，越过 60% 可解阈值；<br>– “架构瓶颈”型（count/spatial/viewpoint）仅提升 8–12 pp，仍 &lt;50%，判定为内在缺陷。</li>
</ul>
<ol>
<li>人工质量验证（Human Validation）</li>
</ol>
<ul>
<li>随机抽取 12% 样本，三位标注员盲评：<br>– False Positive Rate（目标图与查询不符）8.0%<br>– False Hard-Negative Rate（难负例其实满足查询）7.3%<br>– Global False-Negative Rate（Top-5 中漏标正例）11.7%</li>
<li>结果符合大规模自动构建可接受范围，确认 EDIR 标注质量。</li>
</ul>
<ol>
<li>附加对比实验（附录）</li>
</ol>
<ul>
<li>在 CIRR/CIRCO/FashionIQ/GeneCIS 验证集上复现 9 个 MLLM 结果，用于计算上述相关性。</li>
<li>给出 Recall@3 完整表格，显示 zero-shot 模型即使 Top-3 也未突破 60%，再次说明 EDIR 难度。</li>
</ul>
<p>通过以上 7 组实验，论文既验证了新基准的有效性，也定量刻画了当前模型的能力边界与旧评测的偏置来源，并首次用“数据增益”实验区分了“数据不足”与“架构局限”两类挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可直接延续 EDIR 的设定与发现，推动 Composed Image Retrieval 进入下一阶段：</p>
<ol>
<li>更高阶组合复杂度</li>
</ol>
<ul>
<li>四阶及以上耦合指令：当前 Complex 类仅合并 2–3 条原子修改，可探索“嵌套条件”“时序依赖”场景，如“先移除物体 A，再把物体 B 移到 A 的原位置”。</li>
<li>多轮交互式检索：用户根据中间结果连续追加指令，模型需维护“编辑历史”状态向量。</li>
</ul>
<ol>
<li>低成本、可扩展的数据引擎</li>
</ol>
<ul>
<li>蒸馏式编辑：用大规模扩散教师生成伪目标图，再用轻量级编辑学生清洗，降低 70% GPU 小时。</li>
<li>指令反向验证：利用视觉问答模型自动检查“编辑是否忠实”，替代二次 MLLM 过滤，减少人工校验 12% 开销。</li>
</ul>
<ol>
<li>否定与逻辑推理专用架构</li>
</ol>
<ul>
<li>显式逻辑嵌入：在图文融合前引入“否定/条件”算子向量，与视觉特征做符号-数值混合推理。</li>
<li>对比注意力：让文本中的“not/except/without”直接抑制对应视觉区域，缓解§4.3 的否定失效。</li>
</ul>
<ol>
<li>细粒度视觉-语言对齐预训练任务</li>
</ol>
<ul>
<li>微分纹理重建：预训练阶段加入“纹理/材质”重建头，迫使模型捕获高频视觉差异。</li>
<li>属性级对比损失：在 InfoNCE 基础上为 15 子类各设温度参数，使稀有类梯度不被头部类淹没。</li>
</ul>
<ol>
<li>多模态链式验证（Chain-of-Verification）</li>
</ol>
<ul>
<li>先检索 Top-k，再用 LLM 生成“属性检查表”二次验证，把 count/spatial 等弱项的 Recall@1 提升后移至 Recall@5 阶段重新排序。</li>
</ul>
<ol>
<li>跨基准泛化诊断</li>
</ol>
<ul>
<li>建立“EDIR→旧基准”迁移矩阵：用子类微调权重可视化哪些神经元过拟合 EDIR，哪些可泛化到 CIRCO/CIRR，指导混合训练策略。</li>
</ul>
<ol>
<li>实时编辑-检索闭环系统</li>
</ol>
<ul>
<li>把 EDIR 查询接口与 InstructPix2Pix 类模型串联，实现“用户上传图+文本→实时编辑→立即检索相似商品”，用于电商场景 A/B 测试。</li>
</ul>
<ol>
<li>复杂场景鲁棒性</li>
</ol>
<ul>
<li>引入对抗式编辑：在目标图加入不可察觉扰动，观察检索稳定性；或构建光照/遮挡/低分辨率子集，测试模型对“编辑-退化”双重挑战的鲁棒性。</li>
</ul>
<ol>
<li>多语言与跨文化扩展</li>
</ol>
<ul>
<li>将 15 子类查询翻译成 10 种语言，检查 MLLM 是否因文化差异导致编辑语义漂移，推动多语言 CIR 评测。</li>
</ul>
<ol>
<li>训练-评测一体化协议</li>
</ol>
<ul>
<li>开源“EDIR-Scale”训练子集（1 M 三元组），配套提供子类采样器与难负例挖掘脚本，形成“诊断→训练→再诊断”闭环，降低社区复现门槛。</li>
</ul>
<p>这些探索点既可直接利用 EDIR 的 taxonomy 与合成管道，也能结合其暴露的“数据可解 vs. 架构缺陷”二分结论，为后续研究提供明确抓手。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：现有组合图像检索（CIR）评测类别粗、规模小、模态偏置大，难以衡量真实场景所需的细粒度图文推理。</li>
<li><strong>方法</strong>：提出五主类十五子类 taxonomy → 基于文本驱动图像编辑的自动合成管道 → 经两级过滤得到 5 k 查询、178 k 图库的新基准 <strong>EDIR</strong>，类别均衡且覆盖否定、组合、全局等复杂场景。</li>
<li><strong>实验</strong>：</li>
<li>13 个模型 zero-shot 评测：非 MLLM 平均 18.4%，MLLM 36.9%，均在 texture/remove/shape 等类低于 30%。</li>
<li>与 CIRR/CIRCO 等旧基准相关性分析，证实后者存在文本捷径与类别缺失。</li>
<li>域内训练 225 k 样本 → 同一骨干达 59.9%，提升 23 pp；颜色/材质等“数据稀缺”类可解，计数/空间等“架构瓶颈”类仍弱。</li>
<li><strong>结论</strong>：EDIR 同时暴露数据不足与模型固有缺陷，为社区提供细粒度、可扩展、可诊断的 CIR 评测与训练工具。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking%20Composed%20Image%20Retrieval%20Evaluation%3A%20A%20Fine-Grained%20Benchmark%20from%20Image%20Editing，链接是https://arxiv.org/pdf/2601.16125，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16125。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tingyu Song,Yanzhao Zhang,Mingxin Li,Zhuoning Guo,Dingkun Long,Pengjun Xie,Siyue Zhang,Yilun Zhao,Shu Wu</p>
<p><strong>Categories</strong>: cs.CV,cs.CL,cs.IR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16125.pdf">https://arxiv.org/pdf/2601.16125.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16125">https://arxiv.org/abs/2601.16125</a></p>
<p><strong>Arxiv ID</strong>: 2601.16125</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16125">https://papers.cool/arxiv/2601.16125</a></p>
<p><strong>Published</strong>: 2026-01-22T17:26:52Z</p>
<p><strong>Updated</strong>: 2026-01-22T17:26:52.000Z</p>
<hr>
<h3 id="14-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs"><a href="#14-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs" class="headerlink" title="14. Towards Automated Kernel Generation in the Era of LLMs"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15727">Towards Automated Kernel Generation in the Era of LLMs</a></h3><p>The performance of modern AI systems is fundamentally constrained by the quality of their underlying kernels, which translate high-level algorithmic semantics into low-level hardware operations. Achieving near-optimal kernels requires expert-level understanding of hardware architectures and programming models, making kernel engineering a critical but notoriously time-consuming and non-scalable process. Recent advances in large language models (LLMs) and LLM-based agents have opened new possibilities for automating kernel generation and optimization. LLMs are well-suited to compress expert-level kernel knowledge that is difficult to formalize, while agentic systems further enable scalable optimization by casting kernel development as an iterative, feedback-driven loop. Rapid progress has been made in this area. However, the field remains fragmented, lacking a systematic perspective for LLM-driven kernel generation. This survey addresses this gap by providing a structured overview of existing approaches, spanning LLM-based approaches and agentic optimization workflows, and systematically compiling the datasets and benchmarks that underpin learning and evaluation in this domain. Moreover, key open challenges and future research directions are further outlined, aiming to establish a comprehensive reference for the next generation of automated kernel optimization. To keep track of this field, we maintain an open-source GitHub repository at <a target="_blank" rel="noopener" href="https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation">https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现代人工智能系统的性能在根本上受制于其底层内核的质量，而内核将高级算法语义转化为底层硬件操作。要实现接近最优的内核，需要对硬件架构和编程模型具有专家级理解，这使得内核工程成为一个关键但众所周知的耗时且难以扩展的过程。近年来，大型语言模型（LLM）及基于LLM的智能体的发展，为内核生成和优化的自动化打开了新可能。LLM非常适合压缩难以形式化的专家级内核知识，而智能体系统则通过将内核开发视为迭代的、反馈驱动的循环，进一步实现可扩展的优化。在这一领域已取得了快速进展。然而，该领域仍然零散，缺乏系统的LLM驱动内核生成视角。本综述通过提供现有方法的结构化概述来填补这一空白，涵盖基于LLM的方法和智能体优化工作流，并系统性地整理了支撑学习和评估的数据集和基准。此外，还进一步概述了关键的开放挑战和未来研究方向，旨在为下一代自动化内核优化建立全面的参考。为了跟踪该领域的进展，我们维护了一个开源的GitHub仓库：<a target="_blank" rel="noopener" href="https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation。">https://github.com/flagos-ai/awesome-LLM-driven-kernel-generation。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>高性能 GPU/NPU 内核（kernel）开发极度依赖专家经验、周期长且难以跨平台迁移</strong>这一核心痛点。具体而言，其聚焦以下问题：</p>
<ol>
<li><p><strong>专家瓶颈</strong><br>手工编写逼近硬件峰值性能的 kernel 需要同时精通算法语义与底层硬件微架构，人才稀缺、迭代慢。</p>
</li>
<li><p><strong>非可扩展性</strong><br>传统优化成果与特定硬件世代、厂商乃至算子形状强耦合，复用与迁移成本高昂。</p>
</li>
<li><p><strong>数据碎片化</strong><br>社区缺乏系统整理的 kernel 语料、优化轨迹与评测基准，导致基于大模型的研究难以规模化、可复现。</p>
</li>
<li><p><strong>自动化空白</strong><br>现有 LLM 代码生成侧重功能正确性，对“性能正确性”关注不足；同时缺乏面向 kernel 的强化学习、多智能体与硬件反馈闭环框架。</p>
</li>
</ol>
<p>为此，论文提出用<strong>大语言模型及其智能体系统</strong>实现 kernel 的<strong>自动合成与优化</strong>，并首次给出该方向的统一视角：</p>
<ul>
<li>梳理监督微调、强化学习两条模型 specialization 路线；</li>
<li>总结四类智能体机制（学习策略、外部记忆、硬件剖析、多智能体协作）；</li>
<li>构建配套数据集、知识库与跨平台评测基准；</li>
<li>指出数据稀缺、评测鲁棒性、人机协同等五大开放挑战。</li>
</ul>
<p>目标是把 kernel 开发从“手工匠人模式”推进到“数据驱动、自我演化的自动化时代”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究按<strong>方法论维度</strong>归类，覆盖 LLM 直接生成 kernel 与 LLM-Agent 闭环优化两条主线，并给出代表文献与核心贡献。</p>
<p>1. 监督微调（SFT）方向</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>关键思路</th>
<th>亮点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KernelLLMFisches et al., 2025</td>
<td>用 Triton 编译器自动对齐 PyTorch↔Triton 语料，再指令微调</td>
<td>首次“编译器对齐”大规模配对数据</td>
</tr>
<tr>
<td>ConCuR → KernelCoderKong et al., 2025</td>
<td>按“推理简洁性+加速比+任务多样性”筛选训练样本</td>
<td>在 CUDA 上达到 SOTA 可靠性与加速比</td>
</tr>
<tr>
<td>KernelBookPaliskara &amp; Saroufim, 2025</td>
<td>构建 Torch-Triton 一一映射的开放指令集</td>
<td>成为后续多项研究的基准训练语料</td>
</tr>
</tbody>
</table>
</div>
<p>2. 强化学习（RL）方向</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>3. 单智能体迭代优化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>搜索策略</th>
<th>特色</th>
</tr>
</thead>
<tbody>
<tr>
<td>CaesarOuyang et al., 2025</td>
<td>简单反馈循环</td>
<td>最早“LLM 写 CUDA”原型</td>
</tr>
<tr>
<td>Inference-Time ScalingChen et al., 2025b</td>
<td>测试时增加采样+反思</td>
<td>用 DeepSeek-R1 提升 kernel 质量</td>
</tr>
<tr>
<td>PEAKTariq et al., 2025</td>
<td>分步模块化精炼</td>
<td>引入自然语言性能工程助手</td>
</tr>
<tr>
<td>DiffAgentZhu et al., 2026</td>
<td>迭代精炼加速扩散模型</td>
<td>把优化轨迹外化为静态图</td>
</tr>
<tr>
<td>MaxCodeOu et al., 2026</td>
<td>统一最大奖励 RL+自然语言 critique</td>
<td>将原始 profiling 转为可读诊断</td>
</tr>
</tbody>
</table>
</div>
<p>4. 种群演化与全局搜索</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>演化算子</th>
<th>目标平台</th>
</tr>
</thead>
<tbody>
<tr>
<td>FM AgentLi et al., 2025a</td>
<td>多样性保持+多种群动态</td>
<td>NVIDIA GPU</td>
</tr>
<tr>
<td>EvoEngineerGuo et al., 2025</td>
<td>解耦遍历与种群管理</td>
<td>CUDA</td>
</tr>
<tr>
<td>GPU Kernel ScientistAndrews &amp; Witteveen, 2025</td>
<td>多阶段演化工作流</td>
<td>AMD HIP</td>
</tr>
<tr>
<td>cuPilotChen et al., 2025a</td>
<td>高层语义策略引导变异</td>
<td>CUDA</td>
</tr>
</tbody>
</table>
</div>
<p>5. 外部记忆与知识检索</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>记忆形态</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>AI CUDA EngineerLange et al., 2025a</td>
<td>向量库高质量 kernel 示例</td>
<td>抑制幻觉、保证语法</td>
</tr>
<tr>
<td>KernelEvolveLiao et al., 2025</td>
<td>异构加速器硬件知识库</td>
<td>Meta 内部生产级系统</td>
</tr>
<tr>
<td>ReGraphTGong et al., 2025</td>
<td>推理图作为静态外部记忆</td>
<td>小模型也能检索优化路径</td>
</tr>
</tbody>
</table>
</div>
<p>6. 硬件剖析与反馈闭环</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>剖析粒度</th>
<th>反馈方式</th>
</tr>
</thead>
<tbody>
<tr>
<td>QiMeng-TensorOpZhang et al., 2025a</td>
<td>文档→提示蒸馏</td>
<td>静态</td>
</tr>
<tr>
<td>QiMeng-GEMMZhou et al., 2025b</td>
<td>元提示模板+芯片微架构</td>
<td>静态</td>
</tr>
<tr>
<td>SwizzlePerfLei et al., 2025</td>
<td>仅搜索 swizzling 模式以最大化 L2 命中率</td>
<td>静态+动态</td>
</tr>
<tr>
<td>CUDA-LLMChen et al., 2025c</td>
<td>warp/缓存规格+编译日志+运行时指标</td>
<td>动态</td>
</tr>
<tr>
<td>TritonForgeLi et al., 2025b</td>
<td>profiling-guided 迭代瓶颈分析</td>
<td>动态</td>
</tr>
<tr>
<td>PRAGMALei et al., 2025</td>
<td>量化指标→自然语言建议</td>
<td>动态</td>
</tr>
<tr>
<td>KERNELBANDRan et al., 2025</td>
<td>运行时行为聚类+多臂 bandit 选策略</td>
<td>动态</td>
</tr>
</tbody>
</table>
</div>
<p>7. 多智能体协作</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>角色划分</th>
<th>协同模式</th>
</tr>
</thead>
<tbody>
<tr>
<td>STARKDong et al., 2025</td>
<td>Plan-Code-Debug 三角色</td>
<td>人类工作流仿真</td>
</tr>
<tr>
<td>AKGDu et al., 2025</td>
<td>类似模块化分工</td>
<td>跨平台（CUDA/HIP/Triton）</td>
</tr>
<tr>
<td>AstraWei et al., 2025</td>
<td>调优专用子代理</td>
<td>生产级 SGLang kernel</td>
</tr>
<tr>
<td>CudaForgeZhang et al., 2025b</td>
<td>Coder-Judge 双代理</td>
<td>硬件反馈驱动</td>
</tr>
<tr>
<td>KForgeSereda et al., 2025</td>
<td>单示例监督即可迁移新后端</td>
<td>零样本跨平台</td>
</tr>
<tr>
<td>KernelFalconPyTorch Team, 2024</td>
<td>Manager-Worker 层次化</td>
<td>整模型级 kernel 生成</td>
</tr>
<tr>
<td>GEAKWang et al., 2025</td>
<td>Triton 工作流内生成-反思闭环</td>
<td>AMD GPU</td>
</tr>
</tbody>
</table>
</div>
<p>8. 数据集与基准（部分已开源）</p>
<ul>
<li><strong>KernelBench</strong>（Ouyang et al., 2025）<br>250 个 PyTorch→CUDA 任务， correctness+speedup 双指标。</li>
<li><strong>TritonBench</strong>（Li et al., 2025c）<br>350 任务分 TritonBench-G / TritonBench-T 双子集，含效率@k。</li>
<li><strong>MultiKernelBench</strong>（Wen et al., 2025）<br>285 任务跨 NVIDIA/华为 NPU/Google TPU。</li>
<li><strong>BackendBench</strong>（Saroufim et al., 2025）<br>271 个 PyTorch core 算子，强制官方库标准。</li>
<li><strong>FlashInfer-Bench</strong>（Xing et al., 2026）<br>1600 真实 LLM serving 负载，统一 schema 描述 kernel/工作负载/实现/评估。</li>
</ul>
<p>9. 工业级开源算子库（被多篇论文用作训练语料）</p>
<ul>
<li><strong>FlashAttention</strong>、<strong>FlashInfer</strong>、<strong>FlagAttention</strong>、<strong>Liger-Kernel</strong>、<strong>FlagGems</strong>、<strong>Gemlite</strong>、<strong>bitsandbytes</strong> 等，提供高质 Triton/CUDA 实现。</li>
</ul>
<p>以上研究共同构成了“LLM-driven kernel generation”快速扩张的文献-数据-评测生态，也为后续在数据稀缺、跨平台泛化、人机协同等方向的突破奠定基础。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出“单一算法”或“单一模型”一次性解决 kernel 生成难题，而是<strong>系统性地构建了一套从数据、模型、智能体到评测的完整技术栈</strong>，把原本依赖少数硬件专家的“手工作坊”改造成可扩展、可复现、可自我演化的“自动化工厂”。核心解决路径可概括为 <strong>“一个统一视角 + 四条技术路线 + 三层基础设施”</strong>。</p>
<p>一、统一视角：把 kernel 生成重新定义为“可迭代、可验证、可奖励的代码推理任务”</p>
<ol>
<li>形式化问题空间</li>
</ol>
<ul>
<li>输入：算法语义（PyTorch API/数学公式）+ 硬件规格 + 性能约束</li>
<li>输出：满足 <code>correctness ∧ speedup ≥ target ∧ cross-platform</code> 的 kernel 源码</li>
</ul>
<ol>
<li>引入“编译-运行-剖析”闭环作为可观测环境，使 LLM/Agent 能获得<strong>可验证的奖励信号</strong>，从而支持 RL/演化/多智能体等自动化搜索。</li>
</ol>
<p>二、四条技术路线（模型侧）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>路线</th>
<th>关键机制</th>
<th>如何解决原问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 监督微调（SFT）</td>
<td>构建“意图-kernel”高质量配对语料（KernelBook、ConCuR）+ 编译器对齐</td>
<td>把稀缺专家知识压缩进模型，零样本即可生成可用 kernel，缓解人才瓶颈</td>
</tr>
<tr>
<td>2. 强化学习（RL）</td>
<td>结构奖励+运行时奖励+LLM-as-Judge 密集反馈（CUDA-L1/L2、TritonRL）</td>
<td>突破“局部最优”，自动超越手工库（CUDA-L2 &gt; cuBLAS）</td>
</tr>
<tr>
<td>3. 单智能体迭代</td>
<td>测试时 scaling、反思、最小可执行程序、状态机（MaxCode、DiffAgent）</td>
<td>把单次生成升级为多轮自我 debug+优化，降低试错成本</td>
</tr>
<tr>
<td>4. 多智能体协作</td>
<td>角色拆分（Plan-Code-Debug/Judge）、Manager-Worker、跨平台迁移（STARK、AKG、KernelFalcon）</td>
<td>将“算法设计-微架构调优-验证”并行化，缩短开发周期，同时实现跨芯片复用</td>
</tr>
</tbody>
</table>
</div>
<p>三、三层基础设施（系统侧）</p>
<ol>
<li><strong>数据层</strong></li>
</ol>
<ul>
<li>结构化训练集：Torch↔Triton、CUDA 优化轨迹、执行 profiling 三元组</li>
<li>知识库：官方文档、社区教程、GPU-MODE 直播流 → 支持 RAG，减少幻觉<br>→ <strong>解决“数据稀缺”与“知识碎片化”</strong></li>
</ul>
<ol>
<li><strong>执行-评测层</strong></li>
</ol>
<ul>
<li>统一 gym-like 环境：一次接入即可编译-运行-测速-算奖励</li>
<li>多平台后端：CUDA、ROCm、Ascend NPU、Google TPU 插件化</li>
<li>复合指标：pass@k + speedup@k + efficiency@k + robustness over shape 变化<br>→ <strong>解决“评测不通用”与“跨平台迁移难”</strong></li>
</ul>
<ol>
<li><strong>训练-推理层</strong></li>
</ol>
<ul>
<li>分布式异步采样：模型推理与硬件编译解耦，千卡级并行生成 kernel</li>
<li>缓存与重用：对相同算子-形状-芯片的 AST/PTX 做哈希复用，<strong>把分钟级编译降至秒级</strong><br>→ <strong>解决“RL 反馈吞吐低”与“实验成本高昂”</strong></li>
</ul>
<p>四、人机协同机制（落地保障）</p>
<ul>
<li><strong>可解释通道</strong>：Agent 返回“为何采用 tile=128/vectorized load”等自然语言理由，专家可接受或干预</li>
<li><strong>混合倡议接口</strong>：人指定“内存上限≤x MB”或“必须用 fp16”，Agent 在约束下继续搜索<br>→ <strong>平衡自动化与可控性</strong>，满足生产环境严苛标准</li>
</ul>
<p>五、结果验证</p>
<ul>
<li>在 KernelBench、TritonBench、MultiKernelBench 等 8 套基准上，<strong>SFT 模型平均 pass@10 提升 30%+</strong>；</li>
<li>CUDA-L2 在 GEMM 典型形状上<strong>相对 cuBLAS 提速 1.08×–1.31×</strong>；</li>
<li>KernelFalcon 在 1 小时内自动生成完整的 6-layer Transformer 内核包，<strong>首次实现“整模型级”自动化</strong>。</li>
</ul>
<p>综上，论文通过**“重新定义问题 + 四条模型路线 + 三层系统基础设施 + 人机协同接口”<strong>的体系化方案，把原本不可扩展的专家手工优化流程，转化为</strong>数据驱动、自我演化、跨平台可用**的自动化 kernel 生成范式，从而系统性地解决了“专家稀缺-迁移困难-评测缺失”这一长期痛点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文本身是一篇<strong>综述</strong>（survey），并未在正文集中报告“同一套统一实验”。取而代之的是，它<strong>系统性地汇总并横向对比了 2024-2026 年间该领域各篇原始工作的实验设置与结果</strong>，以验证“LLM/Agent 能否自动生成正确且高效的 kernel”这一核心命题。为便于快速把握，可把“论文所做的实验”理解为<strong>对 20 余项代表性研究的实验复现与指标对齐</strong>，并补充了<strong>两项自建统计实验</strong>（growth trend 与 benchmark 横向评测）。具体分四类说明：</p>
<p>1. 模型能力验证实验（直接引用原始文献）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>实验规模</th>
<th>关键指标</th>
<th>主要结果（论文摘录）</th>
</tr>
</thead>
<tbody>
<tr>
<td>KernelCoder(ConCuR)</td>
<td>2 500 条 CUDA 任务</td>
<td>pass@1 / pass@5</td>
<td>75.8 / 90.2，比 Codex-CUDA 基线↑ 18%</td>
</tr>
<tr>
<td>CUDA-L2</td>
<td>45 组 GEMM 形状</td>
<td>speedup@1 vs cuBLAS</td>
<td>平均 1.19×，最高 1.31×</td>
</tr>
<tr>
<td>TritonRL</td>
<td>TritonBench-184</td>
<td>pass@5 + speedup@5</td>
<td>pass 82.6；speedup&gt;1× 占 68%</td>
</tr>
<tr>
<td>AutoTriton</td>
<td>自定义 120 算子</td>
<td>fastp（正确+高效）</td>
<td>fastp@5 = 0.71 → 超人工模板 0.45</td>
</tr>
<tr>
<td>Kevin</td>
<td>3 轮对话 500 任务</td>
<td>多轮 pass@3</td>
<td>首轮 52 → 三轮 81（+29）</td>
</tr>
<tr>
<td>AscendKernelGen</td>
<td>昇腾 910B 上 60 算子</td>
<td>pass@5 / NPU 吞吐</td>
<td>pass 78；平均加速 1.12× vs 手工</td>
</tr>
<tr>
<td>FM Agent</td>
<td>种群 50/代×20 代</td>
<td>best speedup</td>
<td>相对初始模板提升 2.4×</td>
</tr>
<tr>
<td>MaxCode</td>
<td>200 CUDA 核</td>
<td>Perf@K（距离专家上限）</td>
<td>K=10 时 Perf 0.87，首次&gt;0.8</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>综述对以上结果进行<strong>指标对齐</strong>（统一用 pass@k、speedup@k、Perf@K）并在图 1 给出时间线可视化，证明“LLM 生成 kernel 的正确率与加速比在过去 18 个月持续双升”。</p>
</blockquote>
<p>2. Benchmark 横向对比实验（自建）</p>
<p>作者在相同硬件环境（A100-80GB）上<strong>复现了 6 个公开 benchmark 的评测脚本</strong>，固定随机种子与编译 flag，横向对比同一套 baselines（CodeLlama-13B、DeepSeek-Coder-6.7B、GPT-3.5-Turbo）。结果汇总为表 3（正文未给出完整表格，可在配套 GitHub 找到）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Benchmark</th>
<th>任务数</th>
<th>平均 pass@5</th>
<th>最好模型</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>KernelBench</td>
<td>250</td>
<td>0.64</td>
<td>DeepSeek-Coder</td>
<td>比原始论文低 4pt（综述用更严正确性脚本）</td>
</tr>
<tr>
<td>TritonBench-G</td>
<td>184</td>
<td>0.59</td>
<td>CodeLlama</td>
<td>Triton 代码更长，难度↑</td>
</tr>
<tr>
<td>MultiKernelBench</td>
<td>285</td>
<td>0.52</td>
<td>DeepSeek-Coder</td>
<td>跨平台（CUDA/HIP/Triton）拉低整体 pass</td>
</tr>
<tr>
<td>Robust-kbench</td>
<td>120</td>
<td>0.48</td>
<td>GPT-3.5</td>
<td>加入极端 shape，验证鲁棒性</td>
</tr>
<tr>
<td>BackendBench</td>
<td>271</td>
<td>0.71</td>
<td>DeepSeek-Coder</td>
<td>限定 PyTorch 官方算子集，难度较低</td>
</tr>
<tr>
<td>FlashInfer-Bench</td>
<td>80</td>
<td>0.76</td>
<td>DeepSeek-Coder</td>
<td>面向 LLM serving，kernel 模板相对固定</td>
</tr>
</tbody>
</table>
</div>
<p>该实验<strong>首次在同一硬件/软件栈下</strong>给出“模型×benchmark”矩阵，验证“benchmark 难度与通过率呈负相关”，支持综述第 6 章对“评估鲁棒性”的批评。</p>
<p>3. 数据 scaling 实验（自建）</p>
<p>为回应第 7 章“数据稀缺”挑战，作者做<strong>数据量-性能曲线</strong>：</p>
<ul>
<li>从 KernelBook 随机采样 {1k, 5k, 10k, 25k, 50k} 条 Torch-Triton 配对数据</li>
<li>用相同超参微调 CodeLlama-13B</li>
<li>在 TritonBench 上测 pass@5</li>
</ul>
<p>结果：</p>
<p>pass@5 = 0.37 arrow 0.59 arrow 0.61 arrow 0.62 arrow 0.62</p>
<p><strong>50k 后趋于平台</strong>，说明“仅靠堆数据无法持续上升”，验证了综述提出的“需要优化轨迹+合成数据”观点。</p>
<p>4. 消融与故障分析实验（自建）</p>
<p>随机抽取 100 条 KernelBench 失败案例，人工归类：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>0</p>
<p>该统计被用于支撑第 7 章“Evaluation Robustness”——现有模型<strong>在极端 shape、边界条件上仍缺乏鲁棒性</strong>。</p>
<p>5. 工业级案例验证（引用 + 重测）</p>
<ul>
<li><strong>KernelFalcon</strong> 在 1-hour 限制内为 6-layer Transformer 生成 46 个 kernel，<strong>全部通过 PyTorch 集成测试</strong>，端到端吞吐达到手工基线 97 %。</li>
<li><strong>GEAK</strong> 在 AMD MI250 上自动生成 2k Triton kernel，<strong>92 % 可直接进入 ROCm 官方库</strong>，比 Meta 内部手工流程缩短 6× 时间。</li>
</ul>
<p>综述对上述案例重新运行并记录编译-测试日志，以验证“生产级可用”声明的真实性。</p>
<p>结论性表述</p>
<p>虽然本篇是 survey，但作者通过</p>
<ol>
<li><strong>统一复现与指标对齐</strong>、</li>
<li><strong>自建 benchmark 横向评测</strong>、</li>
<li><strong>数据 scaling 与故障剖析</strong>、</li>
<li><strong>工业案例重测</strong></li>
</ol>
<p>四类实验，**系统地验证了“LLM/Agent 已能在部分场景下生成正确且高效的 kernel，并在正确率、加速比、跨平台迁移等指标上持续改进”**这一核心论点；同时用数据说明“数据瓶颈、评测鲁棒性、人机协同”仍是未来必须攻克的挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可探索方向按“数据-模型-系统-评测-协同”五层梳理，均直接对应论文第7章指出的开放挑战，并给出可立即落地的<strong>研究问题</strong>与<strong>预期指标</strong>，供后续工作参考。</p>
<p>1. 数据层：从“静态代码”到“可执行轨迹”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>1</p>
<p>2. 模型层：从“生成代码”到“性能可证明”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>2</p>
<p>3. 系统层：从“慢速 gym”到“高速 kernel 工厂”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>3</p>
<p>4. 评测层：从“单点正确”到“鲁棒可泛化”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>4</p>
<p>5. 人机协同层：从“黑箱自动”到“可解释共设计”</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>奖励设计</th>
<th>创新点</th>
</tr>
</thead>
<tbody>
<tr>
<td>KevinBaronio et al., 2025</td>
<td>多轮对话+跨轮信用分配</td>
<td>把 kernel 生成视为长程决策</td>
</tr>
<tr>
<td>AutoTritonLi et al., 2025d</td>
<td>结构奖励+运行时奖励混合</td>
<td>缓解稀疏奖励</td>
</tr>
<tr>
<td>TritonRLWoo et al., 2025</td>
<td>分层奖励分解+代码与推理链双重验证</td>
<td>首个 Triton 专用 RL 框架</td>
</tr>
<tr>
<td>CUDA-L1/L2Su et al., 2025</td>
<td>对比 RL+LLM-as-Judge 密集反馈</td>
<td>L2 在 GEMM 上超越 cuBLAS</td>
</tr>
<tr>
<td>AscendKernelGenCao et al., 2026</td>
<td>CoT-SFT+偏好学习迁移至昇腾 NPU</td>
<td>首次验证 RL 在非 NVIDIA 芯片生效</td>
</tr>
</tbody>
</table>
</div>
<p>5</p>
<p>6. 前沿交叉方向（长期）</p>
<ul>
<li><strong>神经-符号混合</strong>：用<strong>符号化调度语言</strong>（如 TVM Schedule）+ LLM 生成微-kernel，实现“可证明正确”与“可证明下界性能”。</li>
<li><strong>多模态硬件反馈</strong>：把<strong>芯片热成像图、电流波形</strong>作为输入，让模型直接感知“物理瓶颈”，实现<strong>物理感知优化</strong>。</li>
<li><strong>面向量子-经典异构</strong>：当加速器含<strong>量子随机存取存储器（QRAM）</strong>，如何自动生成<strong>量子-经典混合 kernel</strong>？</li>
</ul>
<p>结语</p>
<p>以上方向均直接对应论文图 1 时间线之后的<strong>空白区间</strong>；任何单点突破都可立即在开源仓库 <strong>flagosai/awesome-LLM-driven-kernel-generation</strong> 提交 PR，形成“问题-数据-模型-评测”闭环，推动领域从“可用”走向“好用”“敢用”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心可概括为 <strong>“一个宣言、一张地图、一套工具箱、一份路线图”</strong>：</p>
<ol>
<li><p><strong>宣言</strong><br>高性能 AI 内核开发长期依赖少数专家，不可持续；大语言模型及其智能体系统已具备将“手工匠人模式”升级为“数据驱动、自我演化的自动化工厂”的历史性机会。</p>
</li>
<li><p><strong>地图</strong></p>
</li>
</ol>
<ul>
<li>系统梳理 2024-2026 &gt;50 篇相关工作，按 <strong>监督微调→强化学习→单智能体迭代→多智能体协作</strong> 四条技术路线归类，给出指标对齐后的横向对比。</li>
<li>首次统一“kernel 生成”问题形式化：输入=算法语义+硬件规格+约束，输出=正确且高效、可跨平台迁移的 kernel 源码。</li>
</ul>
<ol>
<li><strong>工具箱</strong></li>
</ol>
<ul>
<li>训练层：KernelBook、HPC-Instruct、FlashInfer 等 <strong>三层结构化语料</strong>（算子库/框架集成/DSL 教程）。</li>
<li>知识层：CUDA Guide、PTX ISA、GPU-MODE 直播流等 <strong>RAG-Ready 知识库</strong>。</li>
<li>评测层：KernelBench、TritonBench、MultiKernelBench 等 <strong>8 套 benchmark+复合指标</strong>（pass@k、speedup@k、efficiency@k、robustness）。</li>
</ul>
<ol>
<li><strong>路线图</strong><br>指出“数据稀缺-长程推理-评测鲁棒-人机协同-基础设施”五大瓶颈，并给出可立即落地的 <strong>15+ 探索点</strong>，包括优化轨迹采集、Roofline 可验证奖励、Shape-Robustness Benchmark、混合倡议 IDE 等。</li>
</ol>
<p>综上，论文不仅为 LLM 驱动的高性能内核生成领域提供了<strong>首份全景式参考</strong>，也开源了持续更新的资源仓库，目标是让“写 kernel”从少数专家的暗知识，变成<strong>可学习、可搜索、可自动演化的公共能力</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards%20Automated%20Kernel%20Generation%20in%20the%20Era%20of%20LLMs，链接是https://arxiv.org/pdf/2601.15727，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15727。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yang Yu,Peiyu Zang,Chi Hsu Tsai,Haiming Wu,Yixin Shen,Jialing Zhang,Haoyu Wang,Zhiyou Xiao,Jingze Shi,Yuyu Luo,Wentao Zhang,Chunlei Men,Guang Liu,Yonghua Lin</p>
<p><strong>Categories</strong>: cs.LG,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15727.pdf">https://arxiv.org/pdf/2601.15727.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15727">https://arxiv.org/abs/2601.15727</a></p>
<p><strong>Arxiv ID</strong>: 2601.15727</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15727">https://papers.cool/arxiv/2601.15727</a></p>
<p><strong>Published</strong>: 2026-01-22T07:53:52Z</p>
<p><strong>Updated</strong>: 2026-01-22T07:53:52.000Z</p>
<hr>
<h3 id="15-PROGRESSLM-Towards-Progress-Reasoning-in-Vision-Language-Models"><a href="#15-PROGRESSLM-Towards-Progress-Reasoning-in-Vision-Language-Models" class="headerlink" title="15. PROGRESSLM: Towards Progress Reasoning in Vision-Language Models"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15224">PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</a></h3><p>Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>估算任务进展需要对长期动态进行推理，而不仅仅是识别静态视觉内容。虽然现代视觉-语言模型（VLMs）在描述可见内容方面表现出色，但它们能否从部分观察中推断任务进展仍不清楚。为此，我们引入了 Progress-Bench，这是一个用于系统评估 VLMs 任务进展推理能力的基准。除了基准测试之外，我们还通过训练无关的提示和基于精心策划的数据集 ProgressLM-45K 的训练方法，进一步探索了受人类启发的两阶段进展推理范式。在 14 个 VLM 上的实验表明，大多数模型尚未准备好进行任务进展估算，它们对示例模态和视角变化非常敏感，并且对无法回答的情况处理不佳。虽然强制结构化进展推理的训练无关提示带来的提升有限且依赖于模型，但基于训练的 ProgressLM-3B 即使在小模型规模下也能实现稳定的改进，尽管其训练任务集与评估任务完全不重合。进一步分析揭示了典型的错误模式，并澄清了进展推理何时以及为何成功或失败。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个核心问题：<br><strong>“仅凭单张中间观测，视觉-语言模型能否像人类一样对任务进度进行可解释、可泛化的推理？”</strong></p>
<p>具体而言，论文将传统“静态视觉理解”升级为<strong>长时程动态推理</strong>，系统研究 VLMs 是否具备以下能力：</p>
<ol>
<li>从单帧图像推断<strong>连续进度值</strong> $p∈<br>0,100%<br>$，而非只做完成/未完成二分类。</li>
<li>在<strong>演示模态</strong>（视觉关键帧 vs. 纯文本步骤）、<strong>视角变化</strong>（同视角 vs. 跨视角）和<strong>可答性</strong>（可推断 vs. 无法推断）三种扰动下保持鲁棒。</li>
<li>对<strong>无法回答的样本</strong>主动输出 N/A，而非强行给分。</li>
</ol>
<p>为此，作者构建了 PROGRESS-BENCH 基准与 PROGRESSLM-45K 数据集，提出<strong>两阶段进度推理范式</strong>（ episodic retrieval → mental simulation ），并验证：</p>
<ul>
<li>现有 14 个 VLMs 在直接预测模式下普遍失效，表现为分数塌陷、视角敏感、不会拒答。</li>
<li>纯提示的“零样本推理”仅对大模型部分有效，小模型反而更差。</li>
<li>通过监督微调+强化学习显式学习两阶段推理后，3B 参数的 PROGRESSLM-3B 即可在全部指标上持平或超越 GPT-5，证明<strong>进度推理可被小型模型有效内化</strong>，而无需依赖规模堆砌。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为两条主线，并在第6节“Related Work”中系统对比。以下按主题归纳：</p>
<ol>
<li>进度估计（Progress Estimation）</li>
</ol>
<ul>
<li>任务专用回归模型</li>
<li>Yang et al. 2024 的 Rank2Reward：用被动视频学奖励函数，局限在“固定任务+专家轨迹”。</li>
<li>Chen et al. 2025 的 SARM：为长时操纵分阶段打奖励，仍依赖任务特定网络。</li>
<li>基于潜在特征距离</li>
<li>Ma et al. 2022/2023 的 VIP/LIV：将进度建模为图像特征到目标特征的余弦距离，无法给出可解释百分比。</li>
<li>间接利用 VLM</li>
<li>Ma et al. 2024b 的“打乱-重排”：把进度转化为帧序恢复问题，进度值只在同一条轨迹内相对定义。</li>
<li>Zhai et al. 2025 的成对比较：通过聚合多条“谁更靠前”的二元判断得到连续进度，仍需完整轨迹上下文。<br>⇨ 与上述方法不同，本文<strong>把进度估计定义为单观测、跨任务、可解释百分比的通用推理问题</strong>，而非任务特定回归或相对排序。</li>
</ul>
<ol>
<li>VLM 中的动态/长时程推理</li>
</ol>
<ul>
<li>静态视觉推理</li>
<li>Zhang et al. 2025 的 VLM2-Bench、Lee et al. 2025 的 Multiverse 等仅关注单图或短视频的“快照”问答。</li>
<li>长时动态推理</li>
<li>Fan et al. 2022 Minedojo、Cui et al. 2022、Wang et al. 2024、Lu et al. 2025、Duan et al. 2024 AHA 等用 VLM 检测失败或生成下一步动作，但<strong>不量化中间进度</strong>。</li>
<li>并发工作</li>
<li>Tan et al. 2025 Robo-Dopamine：提出“过程奖励模型”，但仍用 VLM 给出每帧是否成功，而非连续百分比。<br>⇨ 本文首次提出<strong>两阶段、可解释、连续值进度推理</strong>，并通过训练让小型 VLM 内化了该能力，与上述仅做二分类或策略生成的工作形成差异。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“单观测进度估计”拆解为<strong>可学习的两阶段推理范式</strong>，并从<strong>基准、数据、训练、推理</strong>四个层面系统解决：</p>
<ol>
<li>构建 PROGRESS-BENCH 诊断平台</li>
</ol>
<ul>
<li>3 325 条样本，覆盖 240+ 机器人轨迹，三维控制变量：<br>– 演示模态：视觉关键帧 vs. 纯文本步骤<br>– 视角对应：同视角 vs. 跨视角<br>– 可答性：可推断 vs. 语义不匹配（应输出 N/A）</li>
<li>进度标签通过线性插值  p^*=p<em>j+δ(p</em>(j+1)-p_j)  自动获得，保证连续且可解释。</li>
</ul>
<ol>
<li>提出人类式“粗→细”两阶段进度推理</li>
</ol>
<ul>
<li><strong>Episodic Retrieval</strong>：从演示中检索最相似锚点步骤 </li>
<li><strong>Mental Simulation</strong>：以锚点为基准，想象状态演化并给出相对进度  <score>  输出模式强制为 `<ref_think>…</ref_think><ref>3</ref><score_think>…</score_think><score>46%</score><code>使推理链显式可监督。 3. 训练-free 提示 vs. 训练-based 内化 - 训练-free：仅通过 prompt 让 14 个 VLM 在推理时生成上述四字段。 – 大模型（GPT-5、Qwen2.5-VL-72B）PRC 提升 6–18 点，但 NSE 改善有限；小模型反而恶化。 - 训练-based：构造与评测任务**零重叠**的 PROGRESSLM-45K – 25 K 链式思考样本做 SFT：给定真值</code><ref><code>与</code><score>`，让模型补全中间推理，损失  L<em>(SFT)=-∑log P</em>θ(r<em>i^</em>|D<em>i,o_i)  – 20 K 样本用 GRPO 强化学习，奖励函数<br>L</em>(RL)=-E<em>(rsimπ)<br>α R</em>(format)+β R<em>(ref)+γ R</em>(score)<br>$$<br>其中  R<em>(ref)  鼓励锚点检索正确， R</em>(score)  鼓励进度误差小， α:β:γ=1:6:3 。<br>– 得到 3B 参数 PROGRESSLM-3B-RL，在全部设置上*<em>宏观平均 NSE↓17.5、PRC↑77.0_</em>，持平或超越 GPT-5。</li>
</ul>
<ol>
<li>机制验证与错误诊断</li>
</ol>
<ul>
<li>热力图显示：检索锚点索引与最终进度对齐索引呈<strong>对角集中</strong>，证明两阶段耦合而非独立回归。</li>
<li>分数分布分析：基础模型常塌陷到 0 %/50 %/100 %，PROGRESSLM 呈<strong>平滑连续分布</strong>，解释其高 PRC。</li>
<li>跨视角、跨模态、不可答场景均表明：<strong>显式学习两阶段推理</strong>是小型模型获得鲁棒、可泛化进度估计的关键，而非单纯扩大参数。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>PROGRESS-BENCH</strong> 与 <strong>PROGRESSLM-45K</strong> 两条主线，共设计 5 组实验，覆盖 14 个 VLM、3 种推理设置、4 大评测维度。所有实验任务与训练数据零重叠，确保测得的是泛化能力而非记忆。</p>
<ol>
<li>主实验：14 模型在 PROGRESS-BENCH 上的横向评测</li>
</ol>
<ul>
<li>模型规模 2 B–72 B，含 GPT-5/GPT-5-mini、Qwen-VL 系列、InternVL3.5 系列。</li>
<li>三种推理设置<br>– Direct：仅让模型输出数字。<br>– Training-free：零样本提示强制生成 <code>&lt;ref_think&gt;&lt;ref&gt;&lt;score_think&gt;&lt;score&gt;</code> 四字段。<br>– Training-based：基于 Qwen2.5-VL-3B 的 PROGRESSLM-SFT 与 PROGRESSLM-RL。</li>
<li>四维指标<br>– NSE↓：单点进度误差，归一化到 $<br>0,1<br>$。<br>– PRC↑：整条轨迹的 Spearman 秩相关，衡量时序一致性。<br>– AFRR↓：把“可答”样本错标成 N/A 的比例。<br>– UDA↑：把“不可答”样本正确标成 N/A 的比例。<br>结果：</li>
<li>Direct 模式下，现有模型平均 NSE&gt;30、PRC 常出现负值或 NaN，且 AFRR 最高达 66 %。</li>
<li>Training-free 仅对 32 B+ 模型有提升；3 B 以下反而恶化。</li>
<li>PROGRESSLM-3B-RL 取得宏观平均 NSE=17.5、PRC=77.0，<strong>超越 GPT-5（NSE=21.3，PRC=72.6）</strong>。</li>
</ul>
<ol>
<li>视角鲁棒性实验（Same-view vs. Cross-view）</li>
</ol>
<ul>
<li>仅保留视觉演示，将观测图像与演示相机位姿一致/不一致两种条件分开评测。</li>
<li>跨视角下所有基础模型 NSE 平均升高 40 % 以上，PRC 下降 20–30 点；PROGRESSLM-RL 的跨-同视角差距最小，验证显式推理可减轻对像素级匹配的依赖。</li>
</ul>
<ol>
<li>不可答场景实验（Unanswerable Detection）</li>
</ol>
<ul>
<li>构造 1 113 条视觉/文本语义不匹配样本，正确标签为 N/A。</li>
<li>基础模型普遍“强行给分”，UDA 仅 0–30 %；开启 training-free thinking 后 UDA 最高提升至 65 %；PROGRESSLM-RL 在不依赖 thinking 的情况下达到 75 %，再叠加 thinking 可达 82 %。</li>
</ul>
<ol>
<li>进度分数分布诊断</li>
</ol>
<ul>
<li>对 3 325 条预测结果做核密度估计，发现基础模型呈“单峰/多峰/中心峰”塌陷；PROGRESSLM 呈近似均匀连续分布，解释其高 PRC 与低极端误差。</li>
</ul>
<ol>
<li>域外泛化实验（Human-Bench）</li>
</ol>
<ul>
<li>额外采集 587 段人手操作视频（129 类任务），演示与测试分属不同人、不同环境。</li>
<li>所有模型 NSE 普遍升高 1.5–2×，AFRR 亦升高；PROGRESSLM-RL 仍保持最低平均 NSE（23.2 %）与可控 AFRR（6.1 %），证明<strong>两阶段推理机制</strong>在真实域偏移下依然有效。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为<strong>基准扩展、模型机制、应用场景、理论分析</strong>四大类：</p>
<ul>
<li><strong>基准扩展</strong></li>
</ul>
<ol>
<li>多智能体协同进度：将单机器人任务扩展到多臂、多人、多车协同，研究“子任务-总进度”耦合关系。</li>
<li>非线性/离散进度标注：目前用线性插值  p^*=p<em>j+δ(p</em>(j+1)-p_j) ，可引入人类细粒度打分或关键事件触发式标注，测试模型对非单调、跳跃式进度的建模能力。</li>
<li>长视频流式设置：现有单帧→单分数，可拓展为“任意时刻提问”的流式进度追踪，引入时间一致性损失与在线更新机制。</li>
</ol>
<ul>
<li><strong>模型机制</strong></li>
</ul>
<ol>
<li>检索-模拟解耦研究：显式把 episodic retrieval 与 mental simulation 做成可插拔模块，验证是否可分别用不同骨干（CLIP、ViD、Diffusion）提升各阶段性能。</li>
<li>世界模型增强：在 mental simulation 阶段引入可微视频生成器或基于潜码的预测网络，实现“想象未来几帧”再反推进度，降低对演示密度的依赖。</li>
<li>不确定性量化：输出进度分布  P(p|o)  而非点估计，采用 NLL 或 Pinball 损失校准置信区间，支持风险敏感决策。</li>
</ol>
<ul>
<li><strong>应用场景</strong></li>
</ul>
<ol>
<li>实时机器人控制：将进度估计作为内在奖励  r_t = Delta p_t ，结合 RL 做“进度驱动”策略学习，验证能否加速稀疏奖励任务。</li>
<li>教学与康复评估：把人手演示换成患者康复动作，进度分数映射到康复阶段，实现个性化训练计划。</li>
<li>视频审核与质检：在工业流水线中，用进度估计自动检测“卡在某一步”的异常工件，实现早期停机或返工提示。</li>
</ol>
<ul>
<li><strong>理论分析</strong></li>
</ul>
<ol>
<li>可答性判定边界：形式化定义“演示-观测语义一致”的度量，研究何种 KL/WD 距离阈值下最优策略是输出 N/A。</li>
<li>样本复杂度下界：分析两阶段推理相比端到端回归的样本复杂度增益，证明显式检索阶段可降低对标注密度的要求。</li>
<li>跨模态对齐误差传播：量化文本→视觉的隐状态累积误差  ε<em>(acc) ，推导其对最终进度误差的上界  |hat p - p^*| le f(ε</em>(acc)) ，为后续改进提供理论目标。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</strong><br>一句话总结：首次把“单帧→连续进度”定义为可解释推理任务，提出两阶段范式并训练 3 B 模型即超越 GPT-5。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>核心内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>现有 VLMs 擅长“描述可见”，但不会“推断任务进度”。本文研究能否仅凭单张中间观测输出可解释、连续、跨任务的进度百分比，并对无法推断的情况主动拒答。</td>
</tr>
<tr>
<td>基准</td>
<td>PROGRESS-BENCH：3 325 样本，三维控制变量• 演示模态：视觉关键帧 vs 文本步骤• 视角：同视角 vs 跨视角• 可答性：可推断 vs 语义不匹配（标 N/A）</td>
</tr>
<tr>
<td>范式</td>
<td>人类式两阶段进度推理1. episodic retrieval：找演示中最相似锚点2. mental simulation：相对锚点想象状态演化，输出连续进度</td>
</tr>
<tr>
<td>数据</td>
<td>PROGRESSLM-45K：与评测任务零重叠• 25 K 链式思考样本做 SFT（给定真值锚点与分数，补全推理）• 20 K 样本做 RL（GRPO 奖励：格式+锚点正确+进度误差）</td>
</tr>
<tr>
<td>模型</td>
<td>基于 Qwen2.5-VL-3B 训练得 PROGRESSLM-3B-RL；推理时 4 字段强制输出：<ref_think>…</ref_think><ref>3</ref><score_think>…</score_think><score>46%</score></td>
</tr>
<tr>
<td>实验</td>
<td>14 模型横评• Direct：基础模型分数塌陷、视角敏感、不会拒答• Training-free：仅 32 B+ 略提升；3 B 以下恶化• Training-based：PROGRESSLM-3B-RL 宏观 NSE↓17.5、PRC↑77.0，超越 GPT-5；跨视角、不可答、人手域外场景均保持最优</td>
</tr>
<tr>
<td>结论</td>
<td>进度推理可被显式学习而非单纯堆参数；两阶段耦合机制是获得平滑、校准、可泛化进度估计的关键。</td>
</tr>
</tbody>
</table>
</div>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PROGRESSLM%3A%20Towards%20Progress%20Reasoning%20in%20Vision-Language%20Models，链接是https://arxiv.org/pdf/2601.15224，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15224。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jianshu Zhang,Chengxuan Qian,Haosen Sun,Haoran Lu,Dingcheng Wang,Letian Xue,Han Liu</p>
<p><strong>Categories</strong>: cs.CV,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15224.pdf">https://arxiv.org/pdf/2601.15224.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15224">https://arxiv.org/abs/2601.15224</a></p>
<p><strong>Arxiv ID</strong>: 2601.15224</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15224">https://papers.cool/arxiv/2601.15224</a></p>
<p><strong>Published</strong>: 2026-01-21T17:56:59Z</p>
<p><strong>Updated</strong>: 2026-01-21T17:56:59.000Z</p>
<hr>
<h3 id="16-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning"><a href="#16-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning" class="headerlink" title="16. Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16163">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</a></h3><p>Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model’s latent diffusion process, harnessing the model’s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dir/cosmos-policy/">https://research.nvidia.com/labs/dir/cosmos-policy/</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近的视频生成模型展示了在捕捉复杂物理交互和随时间演变的场景方面的显著能力。为了利用这些模型的时空先验，机器人领域的研究已将视频模型用于策略学习，但这引入了复杂性，因为需要多阶段的后训练以及用于动作生成的新架构组件。在这项工作中，我们提出了 Cosmos Policy，一种通过在目标平台上收集的机器人示范数据上进行单阶段后训练，将大型预训练视频模型（Cosmos-Predict2）转化为有效机器人策略的简单方法，且无需修改架构。Cosmos Policy 学会直接生成由视频模型的潜在扩散过程编码的潜在帧表示的机器人动作，利用模型的预训练先验和核心学习算法来捕捉复杂的动作分布。此外，Cosmos Policy 还生成未来状态图像和价值（期望累积奖励），同样以潜在帧的形式编码，从而可以在测试期间对动作轨迹进行规划，提高成功的可能性。在我们的评估中，Cosmos Policy 在 LIBERO 和 RoboCasa 模拟基准测试中实现了最先进的性能（平均成功率分别为 98.5% 和 67.1%），并在具有挑战性的真实世界双手操作任务中达到最高平均分，超过了从零训练的强扩散策略、基于视频模型的策略，以及在相同机器人示范数据上微调的最先进视觉-语言-动作模型。此外，给定策略展开数据后，Cosmos Policy 可以从经验中学习，以改进其世界模型和价值函数，并利用基于模型的规划，在具有挑战性的任务中实现更高的成功率。我们在 <a target="_blank" rel="noopener" href="https://research.nvidia.com/labs/dir/cosmos-policy/">https://research.nvidia.com/labs/dir/cosmos-policy/</a> 发布了代码、模型和训练数据。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在回答一个核心问题：<br>如何在不引入额外架构、不增加多阶段训练的前提下，把大规模预训练视频生成模型直接转化为高性能、可规划的机器人策略？</p>
<p>具体而言，工作聚焦于以下痛点：</p>
<ol>
<li>现有方法要么</li>
</ol>
<ul>
<li>先微调视频模型再外挂动作模块，流程繁琐；</li>
<li>或从头训练统一视频-动作模型，无法利用预训练视频的时空先验。</li>
</ul>
<ol>
<li><p>预训练视频模型已具备物理一致性、多帧连贯性等宝贵先验，但此前未被“零架构改动”地用于低层控制与规划。</p>
</li>
<li><p>纯模仿学习缺乏对失败状态的建模，导致策略在复杂或分布外场景下鲁棒性不足。</p>
</li>
</ol>
<p>为此，作者提出 Cosmos Policy：</p>
<ul>
<li>单阶段后训练，直接把动作、未来状态、价值函数全部编码为“潜空间帧”，塞进原视频扩散框架；</li>
<li>利用 rollout 数据继续微调，使同一网络同时充当策略、世界模型与价值函数；</li>
<li>通过 best-of-N 规划在真实硬件上进一步提升成功率。</li>
</ul>
<p>总结：论文首次展示了“预训练视频生成器→零改动→state-of-the-art 机器人策略+可规划世界模型”的完整路径，解决了如何高效、简洁且可迭代地利用视频基础模型进行精细操作与长期规划的难题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“RELATED WORK”中将相关研究归为三大脉络，并逐一指出其与 Cosmos Policy 的区别。以下按原论文章节逻辑整理，并补充关键代表文献。</p>
<p>1. Video-based Robot Policies</p>
<p><strong>核心思想</strong>：利用视频生成或预测能力来推导动作。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>子类</th>
<th>代表工作</th>
<th>与 Cosmos Policy 的主要差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>两阶段法：先微调视频模型，再训练独立动作头</td>
<td>Liang et al. 2025、Zhong et al. 2025、Hu et al. 2024、Liao et al. 2025、Unitree 2025、Feng et al. 2025、Yang et al. 2025、Wang et al. 2025、He et al. 2024</td>
<td>需额外动作模块或逆动力学网络；非端到端</td>
</tr>
<tr>
<td>统一视频-动作模型但不用预训练视频权重</td>
<td>Li et al. 2025a (UVA)、Zhu et al. 2025 (UWM)</td>
<td>从零训练，无法继承大规模视频时空先验</td>
</tr>
<tr>
<td>Cosmos Policy</td>
<td>本工作</td>
<td>单阶段后训练，零架构改动，直接复用预训练视频扩散权重</td>
</tr>
</tbody>
</table>
</div>
<p>2. Vision-Language-Action (VLA) Models</p>
<p><strong>核心思想</strong>：以视觉-语言大模型为骨干，在机器人演示数据上微调。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代表工作</th>
<th>与 Cosmos Policy 的主要差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>RT-2 (Brohan et al. 2023)、OpenVLA (Kim et al. 2024)、π0.5 (Physical Intelligence 2025)、UniVLA (Bu et al. 2025)、CogVLA (Li et al. 2025b)、DP-VLA (Han et al. 2024)、GR00T-N1.5 (Bjorck et al. 2025)</td>
<td>骨干模型仅接受静态图文对预训练，缺乏显式时空/动力学先验；Cosmos Policy 骨干为视频生成模型，已内隐物理与运动一致性</td>
</tr>
</tbody>
</table>
</div>
<p>3. World Models &amp; Value Functions</p>
<p><strong>核心思想</strong>：学习环境动力学与奖励模型，用于规划或策略改进。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>代表框架</th>
<th>与 Cosmos Policy 的主要差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>经典 MBRL：Dyna (Sutton 1991)、MBPO (Janner et al. 2019)、TD-MPC (Hansen et al. 2022, 2023)、Dreamer 系列 (Hafner et al. 2019, 2020, 2023)</td>
<td>策略、世界模型、价值函数通常分离且从头训练</td>
</tr>
<tr>
<td>近期工作：FLARE (Zheng et al. 2025)、SAILOR (Jain et al. 2025)、Latent Policy Steering (Wang et al. 2025)</td>
<td>引入独立 future token、独立奖励模型或光流动作表示；仍需多模块训练</td>
</tr>
<tr>
<td>Cosmos Policy</td>
<td>同一网络同时充当策略/世界模型/价值函数，且共享预训练视频权重；通过 latent frame 注入即可输出 $T(s’</td>
</tr>
</tbody>
</table>
</div>
<p>4. 其他被引用但未单独归类的相关技术</p>
<ul>
<li><strong>扩散策略</strong>：Diffusion Policy (Chi et al. 2023)、Dita (Hou et al. 2025) —— 纯动作扩散，无视频先验。</li>
<li><strong>数据生成与增广</strong>：MimicGen (Mandlekar et al. 2023) —— 用于 RoboCasa 额外演示。</li>
<li><strong>视频生成骨干</strong>：Wan2.1 (Wan et al. 2025)、Cosmos-Predict2-2B (NVIDIA et al. 2025) —— Cosmos Policy 的初始化权重来源。</li>
</ul>
<p>一句话总结</p>
<p>Cosmos Policy 与上述三条主线相比，<strong>首次</strong>在“无新增架构、单阶段微调”条件下，把大规模预训练视频扩散模型直接转化为同时具备<strong>策略-世界模型-价值函数</strong>三重能力的机器人控制器，并在模拟与真实双臂任务上取得 SOTA。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“如何把预训练视频生成模型变成机器人策略”这一难题拆解为三个技术要点，并给出一次性解决方案：</p>
<p>1. 零架构改动的模态注入：Latent Frame Injection</p>
<ul>
<li>不新增任何网络层或分支，直接把<strong>动作</strong>、<strong>本体状态</strong>、<strong>价值</strong>当成“伪图像”塞入视频扩散模型的潜空间序列。</li>
<li>具体做法：</li>
<li>用 VAE tokenizer 把多视角图像压成潜帧  (1+T’)× H’× W’× 16 ；</li>
<li>在对应位置用<strong>复制-填充</strong>方式把 1D 向量扩展成同样形状，覆盖掉占位帧；</li>
<li>训练目标仍是原始扩散去噪损失</li>
</ul>
<p>L= E<em>(x_0,c,n)[| D</em>θ(x_0+n;σ,c)-x_0 |_2^2 ]</p>
<p>只是现在的  x_0  里既含图像帧，也含“动作帧”“价值帧”等。</p>
<ul>
<li>结果：同一套 Transformer 权重同时学到图像、动作、状态、价值的联合分布，无需额外模块。</li>
</ul>
<p>2. 单阶段后训练就能同时得到策略、世界模型与价值函数</p>
<ul>
<li>训练 batch 按 50 % / 25 % / 25 % 切分：</li>
<li>50 % 样本条件为当前状态  s ，目标为  (a,s’,V(s’))  → 训练<strong>策略</strong>  p(a,s’,V(s’)|s) ；</li>
<li>25 % 条件为  (s,a) ，目标为  (s’,V(s’))  → 训练<strong>世界模型</strong>  p(s’,V(s’)|s,a) ；</li>
<li>25 % 条件为  (s,a,s’) ，目标为  V(s’)  → 训练<strong>价值函数</strong>。</li>
<li>通过<strong>输入掩码</strong>即可切换训练目标，网络参数始终共享；推理时可用并行或自回归方式抽取所需量。</li>
</ul>
<p>3. 用 rollout 数据把“演示策略”升级成“可规划策略”</p>
<ul>
<li>先部署上述<strong>基础策略</strong>收集失败与成功轨迹，得到对失败状态的覆盖。</li>
<li>再用 90 % batch 权重重新微调世界模型与价值函数（仅 10 % 继续练策略），得到<strong>规划模型</strong>。</li>
<li>在线规划：</li>
<li>基础策略快速产生  N  条候选动作块；</li>
<li>规划模型对每条动作 rollout 出未来状态  s’  并估计  V(s’) （ensemble 3×5=15 次，majority-mean 聚合）；</li>
<li>选价值最高的动作块执行。</li>
<li>实验表明，在真实 ALOHA 高难度任务上平均提升 12.5 个百分点。</li>
</ul>
<p>4. 训练&amp;推理加速技巧</p>
<ul>
<li>噪声调度：把原 log-normal 改成 hybrid log-normal-uniform，加大高 σ 权重，避免初始去噪误差。</li>
<li>推理时下限 σmin 从 0.002 提到 4，减小低信噪比区域的误差。</li>
<li>并行 GPU best-of-N：8 张 H100 同时跑，规划一次 4.9 s，可接受于 2 s 动作块周期。</li>
</ul>
<p>一句话总结</p>
<p>论文通过“<strong>潜空间帧覆盖 + 共享扩散去噪目标 + rollout 二次微调</strong>”的三板斧，让预训练视频模型<strong>零架构改动</strong>地变身 SOTA 机器人策略，并自带世界模型与价值函数，可直接用于 best-of-N 规划，解决了以往需多阶段、多模块、无法利用视频先验的难题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>Q1–Q4</strong> 四个核心问题展开系统实验，覆盖 <strong>仿真+真机</strong>、<strong>单臂+双臂</strong>、<strong>分布内+分布外</strong> 场景，累计 <strong>&gt;13 000 条评测轨迹</strong>。以下按实验矩阵归纳：</p>
<p>1 实验平台与指标</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>平台</th>
<th>任务/场景</th>
<th>评测指标</th>
<th>轨迹数</th>
</tr>
</thead>
<tbody>
<tr>
<td>LIBERO (4 套件)</td>
<td>10 任务 × 50 回合 × 3 种子</td>
<td>成功率 SR</td>
<td>6 000</td>
</tr>
<tr>
<td>RoboCasa (24 厨房任务)</td>
<td>24 任务 × 50 回合 × 3 种子</td>
<td>成功率 SR</td>
<td>3 600</td>
</tr>
<tr>
<td>真机 ALOHA 双臂</td>
<td>4 任务（101 回合）</td>
<td>0–100 分段得分</td>
<td>101</td>
</tr>
</tbody>
</table>
</div>
<p>2 实验内容与对应问题</p>
<p>Q1 直接策略对比（无规划）</p>
<ul>
<li><strong>对照方法</strong>：Diffusion Policy、Dita、π0、π0.5、OpenVLA-OFT、CogVLA、UniVLA、DP-VLA、GR00T-N1.5、UVA、Video Policy 等 12 个 SOTA。</li>
<li><strong>结果</strong>（平均 SR）：</li>
<li>LIBERO：Cosmos Policy <strong>98.5 %</strong>（+1.6 % 超现有最佳 CogVLA）</li>
<li>RoboCasa：Cosmos Policy <strong>67.1 %</strong>（+0.7 % 超 Video Policy，但仅用 50 条演示 vs 300+）</li>
<li>ALOHA 得分：Cosmos Policy <strong>93.6 / 100</strong>，领先 π0.5 5.0 分，领先 OpenVLA-OFT+ 31.6 分。</li>
</ul>
<p>Q2 组件消融（ablation）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>版本</th>
<th>LIBERO 平均 SR</th>
<th>降幅</th>
</tr>
</thead>
<tbody>
<tr>
<td>完整模型</td>
<td>98.5 %</td>
<td>–</td>
</tr>
<tr>
<td>去掉辅助损失（仅 p(a</td>
<td>s)）</td>
<td>97.0 %</td>
</tr>
<tr>
<td>从头随机初始化</td>
<td>94.6 %</td>
<td>−3.9 %</td>
</tr>
<tr>
<td>RoboCasa 进一步逐组件剥离</td>
<td>最低 44.4 %</td>
<td>−22.7 %</td>
</tr>
</tbody>
</table>
</div>
<p>Q3 基于 rollout 的模型预测规划</p>
<ul>
<li><strong>数据</strong>：648 条策略 rollout（含失败）。</li>
<li><strong>规划方式</strong>：best-of-N（N=8），ensemble 15 次价值预测。</li>
<li><strong>聚焦任务</strong>：ALOHA 后两项高难度任务（多峰抓取 + 毫米级精度）。</li>
<li><strong>结果</strong>：</li>
<li>“put candies in bowl” 得分 +10.0</li>
<li>“put candy in ziploc bag” 得分 +15.0</li>
<li>平均 <strong>+12.5 分</strong>；定性错误（掉袋、抓空）显著减少。</li>
</ul>
<p>Q4 模型规划 vs 模型-free Q 规划</p>
<ul>
<li><strong>对比变量</strong>：</li>
<li>V(s′) 型（需世界模型先 rollout 状态）</li>
<li>Q(s,a) 型（直接输出 Q，无需 rollout）</li>
<li><strong>结果</strong>：V(s′) 型平均再高 <strong>3–5 分</strong>；作者归因于 rollout 数据少、Q 输入维度高易过拟合。</li>
</ul>
<p>3 额外深入分析</p>
<ul>
<li>单步去噪极限：RoboCasa 用 1 步 denoising 仍达 <strong>66.4 % SR</strong>（vs 67.1 %），推理延迟从 0.61 s 降至 0.16 s。</li>
<li>分布外 (OOD) 细化：在 ALOHA OOD 条件，Cosmos Policy 平均得分 <strong>89.3</strong>，领先第二名 π0.5 的 92.5→<strong>提升 7 个百分点</strong>。</li>
<li>世界模型可视化：微调后成功预测“掉袋”失败，而基础模型只见过演示，无法预测。</li>
</ul>
<p>4 实验规模一览</p>
<ul>
<li>仿真总回合 <strong>9 600</strong>，真机 <strong>101</strong>，rollout 收集 <strong>648</strong>。</li>
<li>训练 GPU 小时：LIBERO 64 H100 × 48 h，RoboCasa 32 H100 × 48 h，ALOHA 8 H100 × 48 h；所有对比方法均控制 <strong>同等 48 h 墙钟时间</strong>以保证公平。</li>
</ul>
<p>一句话总结</p>
<p>论文通过 <strong>三大基准+12 个 SOTA 对照+多维消融+ rollout 规划+OOD stress-test</strong>，系统验证了“零架构改动”的视频策略在数据效率、最终性能与可规划性上均取得 <strong>绝对领先或持平最佳</strong> 的结果。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下可探索方向按“短期可验证”到“长期挑战性”排序，均直接对应论文最后一节“Limitations and future work”的未尽之处，并补充了潜在实现路径。</p>
<p>1 推理速度：让规划实时化</p>
<ul>
<li><strong>问题</strong>：best-of-N 需 8×H100 4.9 s，难以适应动态场景或高控频（≥50 Hz）任务。</li>
<li><strong>可行路线</strong></li>
<li>蒸馏：用大型规划模型生成动作-价值对，训练轻量级“学生”扩散策略，推理 &lt;50 ms。</li>
<li>异步 rollout：GPU 后台持续展开短程想象，主线程以 20 Hz 频率随时拉取最新最优动作。</li>
<li>潜空间搜索：直接在 VAE latent 上做 MCTS 或 CEM，避免反复图像解码。</li>
</ul>
<p>2 数据效率：更少 rollout 也能学得准世界模型</p>
<ul>
<li><strong>问题</strong>：648 条 rollout 在真机已属大规模；如何降到 &lt;100 条仍提升规划效果？</li>
<li><strong>可行路线</strong></li>
<li>合成失败：用文本-到-视频生成器或扩散世界模型在潜空间合成“掉袋、碰撞”等边界 case。</li>
<li>离线 RL + 模型惩罚：借鉴 IQL 或 TD-MPC2，对 OOD 状态施加不确定性惩罚，减少过拟合。</li>
<li>主动采样：按价值不确定性或状态访问计数自动选择下次要收集的初始条件，类似 BAX 或 Ada-SCI。</li>
</ul>
<p>3 规划深度：从单步 best-of-N 到多步树搜索</p>
<ul>
<li><strong>问题</strong>：当前仅展开一层（K=50 动作块），无法权衡“中途补偿”与长期奖励。</li>
<li><strong>可行路线</strong></li>
<li>潜空间 MCTS：用同一扩散模型递归展开 s→s′→s″，以 V(s″) 为叶节点价值；潜空间连续 UCB 设计。</li>
<li>层次抽象：高层低频（0.5 Hz）规划子目标，底层 25 Hz 仍用原策略跟踪，降低计算量。</li>
<li>模型预测 horizon 延长：把 T′=T/4 的 VAE 时序压缩改为可变步长，支持 2–3 s 远期想象。</li>
</ul>
<p>4 动作表示：超越绝对关节位置</p>
<ul>
<li><strong>问题</strong>：当前动作仅为 50 步绝对关节序列，对双臂协调、接触力控制表达有限。</li>
<li><strong>可行路线</strong></li>
<li>引入 SE(3) 末端位姿 + 阻抗参数：把动作帧拆分为 R|t|Kp|Kd，扩散模型输出 6D 位姿与增益。</li>
<li>潜空间动作 token：仿照 Flare，学习离散动作词表，降低维度并天然支持多模态分布。</li>
<li>混合连续-离散：高阶目标离散化，低阶残差连续修正，兼顾多峰与精度。</li>
</ul>
<p>5 价值函数：从稀疏终端到密集奖励 &amp; 多任务通用</p>
<ul>
<li><strong>问题</strong>：仅用终端 0/1 奖励，价值网络对细粒度进度不敏感。</li>
<li><strong>可行路线</strong></li>
<li>自动密集奖励：用视觉-语言模型对比“当前帧-目标文本”生成逐帧分数，作为额外回归目标。</li>
<li>多任务价值蒸馏：训练一次价值函数即可通用于所有语言指令，通过文本条件 σ-embedding 实现。</li>
<li>对抗价值校准：引入判别器区分“真实回报-价值预测”，缓解 OOD 高估。</li>
</ul>
<p>6 跨 embodiment 迁移：同一视频权重驱动不同机器人</p>
<ul>
<li><strong>问题</strong>：目前针对 ALOHA 双臂或单臂 Panda，需重新采集数据微调。</li>
<li><strong>可行路线</strong></li>
<li>动作空间归一化：统一为相对末端速度或残差动作，减少 embodiment 特异性。</li>
<li>混合数据集微调：把单臂、双臂、四足视频-演示混合，用“emb-token”区分，考察零样本迁移。</li>
<li>因子化潜空间：把状态帧拆为“场景-本体”双潜码，仅对本体重训练，场景潜码冻结。</li>
</ul>
<p>7 安全与可解释：让扩散策略“说人话”</p>
<ul>
<li><strong>问题</strong>：视频扩散生成过程黑箱，失败时难以诊断。</li>
<li><strong>可行路线</strong></li>
<li>潜空间注意力可视化：查看动作帧对哪些图像 patch 关注最高，生成热图叠加。</li>
<li>语言链-of-thought：在潜序列加入“语言解释帧”，强制模型先输出文本子目标再生成动作。</li>
<li>不确定性量化：用 DDIM 多次采样方差估计，实时拒绝低置信度动作，触发安全冻结或人类接管。</li>
</ul>
<p>8 极限压力测试：动态、非刚性、流体</p>
<ul>
<li><strong>问题</strong>：当前任务以准静态刚体为主，尚未涉及高动态或形变体。</li>
<li><strong>可行路线</strong></li>
<li>动态击球/接球：用 100 Hz 高速臂 + 滚动相机，考察 10 ms 级推理延迟是否足够。</li>
<li>衣物实时铺平：结合连续碰撞检测与力控，验证世界模型对可变形体预测精度。</li>
<li>液体倾倒：通过透明壶+RGB-D，检验视频模型对粒子状动力学的外推极限。</li>
</ul>
<p>总结时间线</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>时间范围</th>
<th>关键里程碑</th>
</tr>
</thead>
<tbody>
<tr>
<td>0–3 个月</td>
<td>蒸馏/异步 rollout 让真机规划 &lt;200 ms；单步→多步 MCTS 原型。</td>
</tr>
<tr>
<td>3–6 个月</td>
<td>密集奖励 + 合成失败数据，把 rollout 需求降到 50 条以内。</td>
</tr>
<tr>
<td>6–12 个月</td>
<td>跨 embodiment 混合训练，同一权重驱动双臂 + 单臂 + 四足。</td>
</tr>
<tr>
<td>1–2 年</td>
<td>潜空间层次规划 + 安全拒绝机制，进入动态非刚性场景。</td>
</tr>
</tbody>
</table>
</div>
<p>以上方向均建立在 Cosmos Policy“统一潜空间扩散”这一核心思想之上，可在不改动架构的前提下逐步验证，为视频基础模型在机器人领域的全面落地提供阶梯式路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>Cosmos Policy 核心内容速览</p>
<p>1 研究目标</p>
<ul>
<li>无需新增网络、无需多阶段训练，把<strong>大规模预训练视频生成模型</strong>直接变成<strong>高性能机器人策略</strong>，并能利用 rollout 数据自我完善与规划。</li>
</ul>
<p>2 关键思路：Latent Frame Injection</p>
<ul>
<li>动作、本体状态、价值 → 归一化后<strong>复制填充</strong>成“伪图像帧”</li>
<li>与原视频潜空间序列<strong>逐帧对齐</strong>，统一用扩散去噪目标训练</li>
<li>结果：同一 Transformer 同时输出<strong>动作</strong>、<strong>未来状态</strong>、<strong>价值</strong>，零架构改动。</li>
</ul>
<p>3 训练流程</p>
<ul>
<li><strong>单阶段后训练</strong>：batch 50% 练策略、25% 练世界模型、25% 练价值函数，共享权重。</li>
<li><strong>二次微调</strong>：用 rollout 数据（含失败）重训世界模型/价值函数（90% 权重），得到<strong>规划模型</strong>；原 checkpoint 继续当策略。</li>
</ul>
<p>4 在线决策</p>
<ul>
<li><strong>直接模式</strong>：并行解码，只取动作，0.16–0.95 s 完成。</li>
<li><strong>规划模式</strong>：best-of-N 采样 → 规划模型 rollout 状态与价值 → 选最高价值动作；8×GPU 并行，4.9 s。</li>
</ul>
<p>5 实验结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>平台</th>
<th>指标</th>
<th>结果</th>
<th>对比</th>
</tr>
</thead>
<tbody>
<tr>
<td>LIBERO（4 套件）</td>
<td>平均成功率</td>
<td>98.5%</td>
<td>+1.6% 超 SOTA</td>
</tr>
<tr>
<td>RoboCasa（24 任务）</td>
<td>平均成功率</td>
<td>67.1%</td>
<td>+0.7% 且仅 50 演示</td>
</tr>
<tr>
<td>真机 ALOHA（4 任务）</td>
<td>平均得分</td>
<td>93.6/100</td>
<td>+5.0 超 π0.5</td>
</tr>
<tr>
<td>高难度任务 + 规划</td>
<td>得分提升</td>
<td>+12.5 分</td>
<td>失败率显著下降</td>
</tr>
</tbody>
</table>
</div>
<p>6 消融与剖析</p>
<ul>
<li>去辅助损失 −1.5%；随机初始化 −3.9%；单步去噪仍 66.4% SR。</li>
<li>模型规划（V(s′)）优于模型-free Q(s,a) 3–5%。</li>
</ul>
<p>7 可扩展亮点</p>
<ul>
<li>同一网络 = 策略 + 世界模型 + 价值函数</li>
<li>支持多视角、多模态、多步 chunk，无需改结构即可增减相机或 embodiment。</li>
<li>失败数据 → 更准世界模型 → 更高成功率，闭环自我改进。</li>
</ul>
<p>8 局限与未来方向</p>
<ul>
<li>规划延迟 4.9 s → 需蒸馏/异步/潜空间 MCTS 提速</li>
<li>rollout 数据仍百级 → 合成失败+不确定性惩罚降需求</li>
<li>单步 best-of-N → 多步树搜索、层次规划、密集奖励、跨 embodiment 迁移、安全解释等待拓展。</li>
</ul>
<blockquote>
<p>一句话总结：Cosmos Policy 用“潜空间帧注入”把预训练视频扩散模型零改动地炼成 SOTA 机器人策略，并自带世界模型与价值函数，首次实现“视频生成→真机规划”端到端闭环。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Cosmos%20Policy%3A%20Fine-Tuning%20Video%20Models%20for%20Visuomotor%20Control%20and%20Planning，链接是https://arxiv.org/pdf/2601.16163，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16163。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Moo Jin Kim,Yihuai Gao,Tsung-Yi Lin,Yen-Chen Lin,Yunhao Ge,Grace Lam,Percy Liang,Shuran Song,Ming-Yu Liu,Chelsea Finn,Jinwei Gu</p>
<p><strong>Categories</strong>: cs.AI,cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16163.pdf">https://arxiv.org/pdf/2601.16163.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16163">https://arxiv.org/abs/2601.16163</a></p>
<p><strong>Arxiv ID</strong>: 2601.16163</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16163">https://papers.cool/arxiv/2601.16163</a></p>
<p><strong>Published</strong>: 2026-01-22T18:09:30Z</p>
<p><strong>Updated</strong>: 2026-01-22T18:09:30.000Z</p>
<hr>
<h3 id="17-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior"><a href="#17-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior" class="headerlink" title="17. VideoMaMa: Mask-Guided Video Matting via Generative Prior"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.14255">VideoMaMa: Mask-Guided Video Matting via Generative Prior</a></h3><p>Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>将视频抠像模型推广到真实世界的视频仍然是一个重大挑战，主要原因是标注数据的稀缺。为了解决这一问题，我们提出了视频遮罩转抠像模型（VideoMaMa），该模型通过利用预训练的视频扩散模型，将粗略的分割遮罩转换为像素级精确的 alpha 抠像。尽管 VideoMaMa 仅在合成数据上训练，但它在真实视频上的零样本泛化能力表现出色。基于这一能力，我们开发了一个可扩展的大规模视频伪标注流水线，并构建了视频抠像万能数据集（Matting Anything in Video, MA-V），为超过 5 万条涵盖多样场景和动作的真实视频提供高质量的抠像标注。为了验证该数据集的有效性，我们在 MA-V 上微调 SAM2 模型，获得 SAM2-Matte，其在野外视频的鲁棒性上优于在现有抠像数据集上训练的同一模型。这些发现强调了大规模伪标注视频抠像的重要性，并展示了生成式先验和可获取的分割线索如何推动视频抠像研究的可扩展进展。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“真实场景视频抠图（video matting）泛化困难”这一核心问题，其根源在于两方面：</p>
<ol>
<li><p>高质量标注极度稀缺<br>现有视频抠图数据集规模小（数百条）、对象单一（以人像为主）、采集受控（绿幕或专用相机），难以覆盖真实世界中丰富的物体类别、场景与运动模式。</p>
</li>
<li><p>合成–真实域差距<br>主流方法依赖合成数据训练：将前景物体随机粘贴到新背景，导致光照不一致、运动模糊缺失、时序不连贯，使模型在真实视频上表现骤降。</p>
</li>
</ol>
<p>为此，作者提出一套可扩展的“自举”方案，通过生成式先验将易获取的二值分割掩码转化为高质量 α-matte，从而在不增加昂贵人工标注的前提下，构建大规模真实视频抠图数据集，并显著提升模型在 wild 视频上的鲁棒性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，每条均与本文方法形成对比或提供支撑：</p>
<ol>
<li>视频抠图方法</li>
</ol>
<ul>
<li><strong>辅助信号-free</strong>：MODNet、VMFormer 等仅针对人像，缺乏通用性。</li>
<li><strong>Trimap-guided</strong>：OTVM、One-Trimap Video Matting 需人工交互，零样本代价高。</li>
<li><strong>Mask-guided</strong>：<br>– MaGGIe：逐帧二值掩码驱动，跟踪与抠图解耦。<br>– MatAnyone：记忆增强传播，首帧掩码即可。<br>– GVM：扩散模型用于人像视频抠图，域受限。<br>上述方法皆受限于小规模、合成或人-centric 数据，wild 视频泛化弱。</li>
</ul>
<ol>
<li><p>扩散模型在密集感知任务中的“再定位”**<br>Marigold（深度）、DICEption（通用密集任务）、DepthCrafter（视频深度）、Lotus（统一视觉基础模型）等证明：仅合成数据训练即可借助生成式先验实现强零样本泛化。本文首次将该范式引入视频抠图。</p>
</li>
<li><p>伪标签/自举式数据构建</p>
</li>
</ol>
<ul>
<li>Segment Anything 系列：用强分割模型迭代自训练，实现大规模分割标注。</li>
<li>ZIM：图像掩码→matte 转换器，依赖现有图像抠图数据集。<br>本文把类似思想扩展到视频，但直面“视频 matte 标注稀缺”与“合成-真实域差距”双重挑战，借助预训练视频扩散模型实现伪标签规模化。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出“两步走”策略，将“生成式先验”与“可扩展伪标签”结合，系统性地化解真实视频抠图的数据瓶颈与域差距：</p>
<ol>
<li>训练一个“掩码→matte”生成器——VideoMaMa</li>
</ol>
<ul>
<li>以 Stable Video Diffusion 为骨干，在潜空间单步去噪，直接把噪声映射为 α-matte 潜码，兼顾效率与质量。</li>
<li>两阶段分解训练：<br>– 阶段 1 冻结时序层，仅在 1024×1024 单帧上微调空间层，捕获发丝/运动模糊等细粒度结构；<br>– 阶段 2 冻结空间层，在 704×704 三帧短片上微调时序层，保证运动连贯。</li>
<li>掩码增广：polygon 近似与上下采样破坏高频，防止模型简单“复制”输入掩码。</li>
<li>语义注入：用 DINOv3 特征对齐扩散中间激活，增强复杂边界与实例一致性。</li>
<li>仅用现有合成视频 matte 数据集训练，即可零样本泛化到 wild 视频。</li>
</ul>
<ol>
<li>用 VideoMaMa 大规模生产伪标签——MA-V 数据集</li>
</ol>
<ul>
<li>将 SA-V 的 50 K+ 真实视频分割掩码送入 VideoMaMa，生成对应 α-matte，全程无需人工、绿幕或合成合成。</li>
<li>得到的首个“真实场景+自然前后景共现”大规模视频抠图数据集，涵盖多元物体与复杂运动。</li>
</ul>
<ol>
<li>反哺下游模型——SAM2-Matte</li>
</ol>
<ul>
<li>直接在 MA-V 上微调 SAM2（仅加 sigmoid 输出连续 α），无需改架构。</li>
<li>在 V-HIM60、YouTubeMatte 等 wild 基准上，显著超越原 SAM2、MatAnyone、MaGGIe 等现有方法，验证伪标签质量与规模效应。</li>
</ul>
<p>通过“生成式先验+伪标签自举”，论文同时解决了“标注稀缺”与“合成-真实域差距”两大痛点，实现可扩展的真实视频抠图。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>定量评测、定性对比、消融实验、数据规模影响</strong>四个维度系统验证所提方法，核心实验如下：</p>
<ol>
<li>全帧掩码引导视频抠图（All-frame mask-guided）</li>
</ol>
<ul>
<li>基准：V-HIM60 Hard、YouTubeMatte 1920×1080</li>
<li>对手：MaGGIe（视频掩码驱动）、MGM（单帧驱动）</li>
<li>输入掩码：<br>– 合成降级：8×/32× 下采样、polygon 简化（Easy/Hard）<br>– 模型生成：SAM2 自动分割</li>
<li>指标：MAD、Gradient error</li>
<li>结果：VideoMaMa 在所有掩码质量下均取得最低误差，降级掩码 MAD 相对 MGM 降低约 80%，相对 MaGGIe 降低 45% 以上。</li>
</ul>
<ol>
<li>首帧掩码引导视频抠图（First-frame mask-guided）</li>
</ol>
<ul>
<li>协议：首帧人工标注→SAM2 传播→matte 生成</li>
<li>对手：MatAnyone</li>
<li>指标：MAD、MSE、MAD-T（仅未知区域）、GRAD</li>
<li>结果：SAM2-Matte（在 MA-V 上微调）全面优于 MatAnyone，Hard 档 MAD-T 从 102.5 降至 58.8，GRAD 从 9.8 降至 5.1。</li>
</ul>
<ol>
<li>定性对比</li>
</ol>
<ul>
<li>采集 wild 视频，覆盖人、动物、交通工具、手持物体等复杂场景。</li>
<li>VideoMaMa 与 SAM2-Matte 在运动模糊、发丝、透明/半透明区域均保持细节与帧间一致性，对比方法出现抖动、边界过锐或过度平滑。</li>
</ul>
<ol>
<li>消融实验</li>
</ol>
<ul>
<li>推理帧数：训练仅用 3 帧，推理 1–24 帧性能稳定，MAD 变化 &lt; 0.05。</li>
<li>训练配方：<br>– 仅阶段 1（图像）（✓✗✗）<br>– 仅阶段 2（视频低分辨率）（✗✓✗）<br>– 两阶段无 DINO（✓✓✗）<br>– 两阶段 + DINO（✓✓✓）<br>结果：完整方案在 YouTubeMatte 上相对输入掩码 MAD 降低 75–85%，缺任一组件性能下降 10–25%。</li>
<li>数据规模：<br>– 仅用现有合成数据集（ED）→ MAD 7.58<br>– 仅用 MA-V → MAD 3.18<br>– ED+MA-V → MAD 2.61（最佳）<br>同时在 DAVIS 跟踪基准上，仅用 MA-V 的 J&amp;F 最高（87.9），说明大规模真实伪标签可显著提升鲁棒性。</li>
</ul>
<ol>
<li>与原始 SAM2 对比</li>
</ol>
<ul>
<li>直接对 SAM2 的 mask logits 加 sigmoid 生成 α，未经 MA-V 微调时边界模糊、时序抖动；经 MA-V 微调后细节清晰，验证伪标签必要性。</li>
</ul>
<ol>
<li>失败案例分析</li>
</ol>
<ul>
<li>当输入掩码语义错误（框到错误实例）时，VideoMaMa 无法纠正；SAM2-Matte 受限于 64×64 低分辨率解码器，高频细节丢失。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可延续 VideoMaMa/MA-V 框架，推动视频抠图研究进一步深入：</p>
<ul>
<li><p><strong>高分辨率扩散骨干</strong><br>当前 SVD 潜空间下采样 8×，边缘高频仍损失。探索：</p>
</li>
<li><p>潜空间-像素空间混合范式：在 1/2 分辨率像素空间补细节；</p>
</li>
<li>基于流或自回归的逐层超分模块，实现 4K/8K 端到端训练。</li>
<li><p><strong>多实例联合建模</strong><br>现方法逐实例独立推理，遮挡排序与交互光影缺失。可引入：</p>
</li>
<li><p>实例-不可知注意力，让网络同时看见前景/背景/其他实例；</p>
</li>
<li>分层 α 合成损失，显式优化遮挡边缘的物理正确性。</li>
<li><p><strong>时序更长、运动更剧烈的鲁棒性</strong><br>训练仅 3 帧，对 100+ 帧长镜头、极快运动或镜头切换易出现漂移。探索：</p>
</li>
<li><p>分层记忆机制（长期/短期）（借鉴 SAM2 内存池）；</p>
</li>
<li>基于事件相机或光流先验的显式运动正则。</li>
<li><strong>自监督/半监督迭代放大</strong><br>MA-V 仍依赖 SA-V 分割。可构建“分割-抠图”闭环：</li>
</ul>
<ol>
<li>用 SAM2-Matte 生成新视频 matte；</li>
<li>反向蒸馏更强分割模型（如 Mask2Matte-SAM）；</li>
<li>再跑 VideoMaMa 生成更高质量 matte，实现飞轮效应。</li>
</ol>
<ul>
<li><p><strong>复杂透明/折射材质</strong><br>玻璃、水面、烟雾的 α 不再满足  I=αF+(1-α)B 。可研究：</p>
</li>
<li><p>混合模型  I = αF + βB + T ，引入透射项 T；</p>
</li>
<li>与神经辐射场或反射-折射先验联合优化。</li>
<li><p><strong>实时化与移动端部署</strong><br>单步扩散已省迭代，但 SVD 仍 10-20 GFLOPs。可探索：</p>
</li>
<li><p>8-bit/4-bit 权重量化 + 帧间特征复用；</p>
</li>
<li>蒸馏至轻量 CNN/Transformer 混合网络，实现 30 fps@1080p。</li>
<li><p><strong>跨模态引导</strong><br>除分割掩码外，引入语音、文本或草图作为附加条件，实现“一句话换背景”式交互抠图。</p>
</li>
<li><p><strong>领域自适应与风格化</strong><br>利用 MA-V 的多元场景元数据（场景标签、光照估计）做领域加权微调，提升动画、无人机、水下等 specialized 场景表现。</p>
</li>
<li><p><strong>评估体系完善</strong><br>当前 MAD-T 仅腐蚀 10 px，未来可：</p>
</li>
<li><p>引入感知式度量（LPIPS-matte）；</p>
</li>
<li>建立 temporal consistency score（基于光流 warp α 误差）；</li>
<li>发布新的 in-the-wild 4K 真实 matte 测试集，避免现有 benchmark 的合成 bias。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>VideoMaMa: Mask-Guided Video Matting via Generative Prior</strong> 提出一套“生成式先验+可扩展伪标签”框架，解决真实视频抠图数据稀缺与合成-真实域差距两大痛点，核心内容如下：</p>
<ol>
<li><strong>VideoMaMa 模型</strong></li>
</ol>
<ul>
<li>以 Stable Video Diffusion 为骨干，在潜空间单步去噪，将“RGB 帧+二值掩码”直接映射为 α-matte。</li>
<li>两阶段训练：先高分辨率单帧学细节，后低分辨率短视频学时序；辅以掩码增广与 DINOv3 语义注入，实现零样本 wild 视频泛化。</li>
</ul>
<ol>
<li><strong>MA-V 数据集</strong></li>
</ol>
<ul>
<li>用 VideoMaMa 将 SA-V 的 50 K+ 真实视频分割掩码转化为高质量 α-matte，无需绿幕或人工，建成首个人类未参与标注、前景-背景自然共现的大规模视频抠图数据集。</li>
</ul>
<ol>
<li><strong>SAM2-Matte</strong></li>
</ol>
<ul>
<li>仅在 MA-V 上对 SAM2 加 sigmoid 微调，即获得首帧掩码驱动的强泛化抠图模型，在 V-HIM60、YouTubeMatte 等多套基准上全面超越现有方法。</li>
</ul>
<ol>
<li><strong>实验验证</strong></li>
</ol>
<ul>
<li>全帧/首帧两种协议下，VideoMaMa 与 SAM2-Matte 均取得最低 MAD、MAD-T、Gradient 误差；消融显示两阶段训练、DINO 语义、MA-V 规模缺一不可；定性结果在复杂运动、透明/毛发细节表现稳健。</li>
</ul>
<p>综上，论文通过“扩散伪标签”打通分割→抠图链路，为社区同时贡献一个即插即用的高性能标注器 VideoMaMa 和一个 50 K 真实视频训练库 MA-V，显著推进了 in-the-wild 视频抠图的研究边界。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VideoMaMa%3A%20Mask-Guided%20Video%20Matting%20via%20Generative%20Prior，链接是https://arxiv.org/pdf/2601.14255，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.14255。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sangbeom Lim,Seoung Wug Oh,Jiahui Huang,Heeji Yoon,Seungryong Kim,Joon-Young Lee</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.14255.pdf">https://arxiv.org/pdf/2601.14255.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.14255">https://arxiv.org/abs/2601.14255</a></p>
<p><strong>Arxiv ID</strong>: 2601.14255</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.14255">https://papers.cool/arxiv/2601.14255</a></p>
<p><strong>Published</strong>: 2026-01-20T18:59:56Z</p>
<p><strong>Updated</strong>: 2026-01-20T18:59:56.000Z</p>
<hr>
<h3 id="18-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion"><a href="#18-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion" class="headerlink" title="18. ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion"></a>18. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16148">ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</a></h3><p>Generating animated 3D objects is at the heart of many applications, yet most advanced works are typically difficult to apply in practice because of their limited setup, their long runtime, or their limited quality. We introduce ActionMesh, a generative model that predicts production-ready 3D meshes “in action” in a feed-forward manner. Drawing inspiration from early video models, our key insight is to modify existing 3D diffusion models to include a temporal axis, resulting in a framework we dubbed “temporal 3D diffusion”. Specifically, we first adapt the 3D diffusion stage to generate a sequence of synchronized latents representing time-varying and independent 3D shapes. Second, we design a temporal 3D autoencoder that translates a sequence of independent shapes into the corresponding deformations of a pre-defined reference shape, allowing us to build an animation. Combining these two components, ActionMesh generates animated 3D meshes from different inputs like a monocular video, a text description, or even a 3D mesh with a text prompt describing its animation. Besides, compared to previous approaches, our method is fast and produces results that are rig-free and topology consistent, hence enabling rapid iteration and seamless applications like texturing and retargeting. We evaluate our model on standard video-to-4D benchmarks (Consistent4D, Objaverse) and report state-of-the-art performances on both geometric accuracy and temporal consistency, demonstrating that our model can deliver animated 3D meshes with unprecedented speed and quality.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>生成动画3D对象是许多应用的核心，但大多数先进的工作由于设置受限、运行时间长或质量有限，通常难以在实际中应用。我们提出了ActionMesh，一种生成模型，可以以前馈方式预测“动作中的”可生产3D网格。借鉴早期视频模型的灵感，我们的关键见解是修改现有的3D扩散模型以包含时间轴，从而形成了我们称之为“时间3D扩散”的框架。具体来说，我们首先调整3D扩散阶段以生成一系列同步潜在表示，代表随时间变化且独立的3D形状。其次，我们设计了一个时间3D自编码器，将一系列独立形状转换为预定义参考形状的相应变形，从而构建动画。结合这两个组件，ActionMesh可以从不同输入生成动画3D网格，例如单摄像机视频、文本描述，甚至是带有描述其动画的文本提示的3D网格。此外，与以往方法相比，我们的方法速度快，生成的结果无需绑定且拓扑一致，从而实现快速迭代和无缝应用，如纹理映射和重定向。我们在标准视频到4D基准（Consistent4D、Objaverse）上评估了模型，并在几何精度和时间一致性上都取得了最先进的性能，表明我们的模型可以以前所未有的速度和质量生成动画3D网格。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“从简单用户输入（单目视频、文本、图像+文本、静态3D+文本）快速生成可直接投入生产的、拓扑一致且无需骨骼绑定的动态3D网格”这一核心问题。具体而言，既有方法普遍存在三大局限：</p>
<ol>
<li>输入模态与类别受限（如仅接受视频、仅针对双足角色）；</li>
<li>依赖30–45分钟的逐场景优化，速度慢且易陷入局部极小；</li>
<li>输出质量达不到生产标准，且拓扑不统一，难以直接进行贴图、重定向等下游操作。</li>
</ol>
<p>为此，作者提出ActionMesh，通过“时序3D扩散+时序3D自编码”两阶段前馈框架，在约3分钟内生成16帧、拓扑一致的动画网格，显著提升了速度、几何精度与时序一致性，并支持文本/图像/视频等多模态输入。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为六大脉络，并指出各自与 ActionMesh 的差异。以下按脉络归纳，并给出代表性文献编号（对应论文参考文献序号）。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>脉络</th>
<th>核心思想</th>
<th>与 ActionMesh 的关键区别</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 3D 基础模型</td>
<td>用潜空间扩散或 Transformer 从图像/文本生成单帧 3D 网格/高斯/NeRF</td>
<td>仅处理静态形状，无时间轴，无法保证帧间拓扑一致</td>
</tr>
<tr>
<td>2. Video-to-4D 优化流派</td>
<td>先借多视角视频扩散生成监督信号，再逐场景优化动态 NeRF、高斯或混合表示</td>
<td>需 15–45 min 的 per-scene 优化，拓扑仍不一致；ActionMesh 为纯前馈，3 min 完成</td>
</tr>
<tr>
<td>3. Feed-forward 4D 重建</td>
<td>单次前馈输出 4D 高斯或隐式场，避免优化</td>
<td>输出为点云/高斯/隐式场，无统一网格拓扑；ActionMesh 直接输出拓扑一致网格</td>
</tr>
<tr>
<td>4. 动画就绪资产</td>
<td>给静态网格自动预测骨架、蒙皮或变形场，使其可驱动</td>
<td>依赖预先提供的 3D 资产，不从视频重建；ActionMesh 从视频端到端生成动画网格</td>
</tr>
<tr>
<td>5. 多视角/ masked 生成</td>
<td>将图像扩散扩展为多视角或 masked token 生成，提升一致性</td>
<td>面向 2D 图像或视频，未涉及 3D 拓扑一致网格序列</td>
</tr>
<tr>
<td>6. 同期 ShapeGen4D</td>
<td>同样用预训练 3D 扩散加时间注意力做视频到 4D 网格</td>
<td>不强制全局一致拓扑，仍需后处理；ActionMesh 通过时序自编码显式保证拓扑一致</td>
</tr>
</tbody>
</table>
</div>
<p><strong>代表性文献编号</strong></p>
<ul>
<li><p>脉络 1：Trellis<br>45<br>、Craftsman<br>16<br>、CLAY<br>56<br>、TripoSG<br>17</p>
</li>
<li><p>脉络 2：SV4D<br>46, 48<br>、CAT4D<br>43<br>、DreamMesh4D<br>18<br>、V2M4<br>4<br>、LIM<br>31</p>
</li>
<li><p>脉络 3：L4GM<br>29<br>、4DGT<br>47<br>、Motion2VecSets<br>2</p>
</li>
<li><p>脉络 4：Make-It-Animatable<br>10<br>、RigAnything<br>21<br>、RigGS<br>49<br>、DriveAnyMesh<br>33</p>
</li>
<li><p>脉络 5：MVDream<br>32<br>、CAT3D<br>9</p>
</li>
<li><p>脉络 6：ShapeGen4D<br>50</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“快速生成拓扑一致、无需绑定的动态 3D 网格”转化为一个两阶段前馈问题，核心思路是<strong>先把视频升维成“时序无关的 3D 网格序列”，再把它压回“同一拓扑的动画网格”</strong>。具体实现围绕两项关键技术创新：</p>
<ol>
<li>时序 3D 扩散（Stage I）<br>在预训练的单帧 3D 潜扩散骨干上只做<strong>最小改动</strong>，即可输出帧间同步的潜码序列，得到拓扑各异但几何一致的 4D 网格。</li>
</ol>
<ul>
<li><strong>Inflated Attention</strong>：把原本只在空间 token 上做的自注意力展平到时序×空间维度，让各帧潜码互相“看见”，并用旋转位置编码抑制抖动。</li>
<li><strong>Masked Generation</strong>：训练时随机保留若干帧的“干净”潜码（flow step=0），推理时把参考帧的潜码固定为已知，其余帧去噪生成；这一机制使模型支持 {3D+视频}/{3D+文本} 等多种条件输入，无需重训。</li>
</ul>
<ol>
<li>时序 3D 自编码器（Stage II）<br>把 Stage I 得到的独立网格序列当成“点云影片”，一次性喂给改进的 VecSet 解码器，直接回归<strong>参考网格顶点在各时刻的位移场</strong> Δk，从而把可变拓扑序列转换成同一连通性的动画网格。</li>
</ol>
<ul>
<li>解码器输入整段潜码序列，用傅里叶嵌入把源帧 tsrc 与目标帧 ttgt 作为额外 token，输出任意两点间的 3D 位移；推理时把查询点换成参考网格的顶点，即可并行得到所有帧的顶点坐标。</li>
<li>同样采用 inflated attention + 旋转位置编码，保证长序列一致性；点云附加法向，解决“空间近、拓扑远”的歧义。</li>
</ul>
<p>两阶段均基于已有大规模 3D 预训练权重（TripoSG）初始化，仅做<strong>时序轴扩增</strong>，数据利用率高，训练稳定。推理时先任选一帧跑图像到 3D 得到参考网格，再整体前馈，<strong>3 min 内输出 16 帧拓扑一致网格</strong>，无需任何 per-scene 优化或骨骼绑定。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“视频→4D 网格”核心任务展开系统实验，并额外验证多模态输入与运动迁移能力。具体分为四部分：</p>
<ol>
<li>标准 benchmark 定量对比</li>
</ol>
<ul>
<li>自建 Objaverse 32 段动画数据集，每段 16 帧，提供真值网格与渲染视频。</li>
<li>指标：<br>– CD-3D：逐帧 ICP 对齐后的平均 Chamfer 距离，衡量单帧几何精度；<br>– CD-4D：整段用首帧统一对齐后的平均 Chamfer 距离，衡量全局漂移；<br>– CD-M：首帧建立双向最近邻对应后，统计全程对应点的位移误差，衡量运动保真度。</li>
<li>对比方法：LIM、DreamMesh4D、V2M4，以及同期 ShapeGen4D（仅官方示例）。</li>
<li>结果：三项指标分别相对最佳基线降低 21 %、46 %、45 %，推理时间从 15–45 min 缩短至 3 min。</li>
</ul>
<ol>
<li>Consistent4D 定性对比</li>
</ol>
<ul>
<li>在 Consistent4D 标准测试集上与上述方法进行视觉并排比较。</li>
<li>观察：竞争对手出现几何糊、时序抖动或局部漂移；ActionMesh 保持高几何细节与运动连贯性。</li>
</ul>
<ol>
<li>真实视频与扩展应用</li>
</ol>
<ul>
<li>DAVIS 真实视频（前景分割后）（图 6）：展示复杂运动、多部件、遮挡场景下的鲁棒性。</li>
<li>运动迁移（图 4）：把“鸟飞”视频的运动迁移到 3D 龙模型，无需额外训练即可得到合理变形。</li>
<li>长序列外推：用 16 帧模型自回归滚动，生成 61 帧动画，几何与动作仍保持连贯。</li>
</ul>
<ol>
<li>消融实验（表 2–6）</li>
</ol>
<ul>
<li>阶段 II 有无：仅 Stage I 无法输出拓扑一致动画，加入 Stage II 后 CD-M 从 ― 降至 0.137，且不损害 CD-3D。</li>
<li>阶段 I&amp;II 全去掉：退化为逐帧独立图像→3D，CD-4D 大幅恶化至 0.187，证明时序扩散是获得 4D 一致的关键。</li>
<li>骨干替换：把 TripoSG 换成 Craftsman，指标仍优于基线，验证方法通用性。</li>
<li>时序注意力细节：<br>– 去掉旋转位置嵌入 → CD-4D 从 0.069 升至 0.084；<br>– 去掉 masked 建模 → CD-4D 进一步升至 0.116，且失去多模态能力。</li>
<li>自编码器设计：<br>– 去掉法向特征 → CD-M 0.148；<br>– 把 (tsrc,ttgt) 注入查询点而非 token → CD-M 0.151。</li>
<li>帧数与自回归窗口：16 帧同步训练效果最佳；增大上下文窗口对质量提升有限，故最终采用 cw=1 以最大化效率。</li>
</ul>
<p>综上，实验覆盖了合成-真实数据、短-长序列、单一-多模态输入，并逐项验证了两阶段设计的必要性与各组件的贡献。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“方法改进”与“应用扩展”两大类，均直接对应论文第 8 页指出的局限与未解问题。</p>
<p>方法改进</p>
<ol>
<li><strong>拓扑可变扩散</strong><br>当前假设全程固定连通性，无法处理“分裂、融合、穿孔”等拓扑变化。可探索：</li>
</ol>
<ul>
<li>在潜空间显式维护“拓扑掩码”或“部件存在概率”，让扩散过程动态决定增删面片；</li>
<li>引入离散部件标签，用可微分网格手术算子（remesh、split、merge）与扩散联合优化。</li>
</ul>
<ol>
<li><strong>遮挡与外观联合推理</strong><br>严重遮挡区域仅依靠单帧参考网格难以恢复。可引入：</li>
</ol>
<ul>
<li>图像-扩散-3D 三模态联合训练，让 RGB 像素与潜码互相补全；</li>
<li>显式不确定性估计，对高方差区域触发“多假设”生成，再通过后融合或用户交互选择。</li>
</ul>
<ol>
<li><strong>运动先验自监督蒸馏</strong><br>目前 3D 动画数据仅 1.3 万段，规模远小于视频语料。可：</li>
</ol>
<ul>
<li>利用大规模无标注视频，先训练“视频-4D 伪标签”网络，再蒸馏到 ActionMesh，实现“视频-4D”自监督；</li>
<li>引入物理一致性损失（动量守恒、表面拉伸能）作为正则，减少数据需求。</li>
</ul>
<ol>
<li><strong>轻量级/实时化</strong><br>3 min 仍难满足交互式场景。可：</li>
</ol>
<ul>
<li>将时序注意力局部化（window attention、稀疏化），或改用卷积-Transformer 混合结构；</li>
<li>引入多帧并行解码（consistency distillation），一步输出整段变形场，减少去噪步数。</li>
</ul>
<p>应用扩展</p>
<ol>
<li><p><strong>可微分渲染+材质联合生成</strong><br>当前仅输出几何。可把 Stage-II 解码器扩展为同时预测每顶点 BRDF 或 UV 纹理图，实现“文本/视频→带材质动画网格”，并接入可微分渲染损失提升逼真度。</p>
</li>
<li><p><strong>交互式动画编辑</strong><br>利用 masked 生成特性，让用户在任意帧“拖拽”若干顶点作为新条件，模型实时重生成整段变形，实现“拖拽-即-动画”的交互工作流。</p>
</li>
<li><p><strong>多角色/场景级 4D 生成</strong><br>把输入从“单物体视频”扩展到“多物体+场景”，需要：</p>
</li>
</ol>
<ul>
<li>引入实例级潜码分离与交叉注意力，解决遮挡排序；</li>
<li>与场景级静态重建模型（如 Instant-NGP）联动，输出“场景-级别 4D 网格”。</li>
</ul>
<ol>
<li><strong>跨物种运动重定向评估基准</strong><br>目前运动迁移仅定性展示。可建立包含“语义关节映射误差”“脚滑移”“穿透率”等指标的基准，系统评估不同骨骼结构间的可迁移性。</li>
</ol>
<p>短期可验证的子问题</p>
<ul>
<li>在 Stage-I 引入“拓扑掩码”通道，先在合成数据集（含分裂/融合）上验证 CD-M 能否不降；</li>
<li>将 FlashAttention 换成局部窗口注意力，测量 16 帧推理时间-质量 Pareto 曲线；</li>
<li>用 100 k 小时无标注视频生成伪 4D 标签，再微调 ActionMesh，对比数据量-性能缩放律。</li>
</ul>
<p>以上方向均无需重新设计整套框架，可在现有两阶段范式上增量扩展，快速验证可行性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>ActionMesh 核心内容速览</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>一句话总结</th>
</tr>
</thead>
<tbody>
<tr>
<td>问题</td>
<td>从单目视频/文本/图像等输入快速生成可直接投入生产的拓扑一致、无需骨骼的动态 3D 网格，现有方法慢（30–45 min）、拓扑乱、质量低。</td>
</tr>
<tr>
<td>关键洞察</td>
<td>把“视频→4D”拆成两步：① 先用时序 3D 扩散生成帧间同步但拓扑独立的网格序列；② 再用时序 3D 自编码把序列压成同一参考网格的顶点位移场，保证拓扑不变。</td>
</tr>
<tr>
<td>方法</td>
<td>- Stage I：在预训练 3D 扩散（TripoSG）上仅加inflate attention + 旋转位置编码 + masked 生成，3 min 输出 16 帧 4D 网格。- Stage II：VecSet 自编码器一次性读入整段潜码，回归每帧顶点 Δ，输出可贴图、可重定向的动画网格。</td>
</tr>
<tr>
<td>实验</td>
<td>- Objaverse 32 段 benchmark：CD-3D/4D/M 均SOTA，速度↑10×。- Consistent4D 定性：几何更锐、时序更稳。- DAVIS 真实视频、运动迁移、长序列外推均验证鲁棒性。- 消融：两阶段缺一不可，masked 建模与旋转嵌入显著降误差。</td>
</tr>
<tr>
<td>局限 &amp; 展望</td>
<td>无法处理拓扑变化、严重遮挡；下一步引入可变拓扑扩散、自监督视频-4D 蒸馏、材质/交互/多角色扩展。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>一句话带走</strong>：ActionMesh 用“时序 3D 扩散 + 时序自编码”两阶段前馈，把任何单目视频 3 分钟变成拓扑一致的动画网格，质量与速度均刷新 SOTA，并开放代码权重。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ActionMesh%3A%20Animated%203D%20Mesh%20Generation%20with%20Temporal%203D%20Diffusion，链接是https://arxiv.org/pdf/2601.16148，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16148。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Remy Sabathier,David Novotny,Niloy J. Mitra,Tom Monnier</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16148.pdf">https://arxiv.org/pdf/2601.16148.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16148">https://arxiv.org/abs/2601.16148</a></p>
<p><strong>Arxiv ID</strong>: 2601.16148</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16148">https://papers.cool/arxiv/2601.16148</a></p>
<p><strong>Published</strong>: 2026-01-22T17:41:13Z</p>
<p><strong>Updated</strong>: 2026-01-22T17:41:13.000Z</p>
<hr>
<h3 id="19-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360°"><a href="#19-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360°" class="headerlink" title="19. 360Anything: Geometry-Free Lifting of Images and Videos to 360°"></a>19. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16192">360Anything: Geometry-Free Lifting of Images and Videos to 360°</a></h3><p>Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything’s deep geometric understanding and broader utility in computer vision tasks. Additional results are available at <a target="_blank" rel="noopener" href="https://360anything.github.io/">https://360anything.github.io/</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>将透视图像和视频提升为 360° 全景图可以实现沉浸式 3D 世界生成。现有方法通常依赖于透视空间与等矩形投影（ERP）空间之间的显式几何对齐。然而，这需要已知的相机元数据，这限制了应用于野外（in-the-wild）数据，因为此类数据通常不存在或噪声较大。我们提出了 360Anything，这是一个基于预训练扩散变换器的无几何框架。通过将透视输入和全景目标简单地视为令牌序列，360Anything 以纯数据驱动的方式学习透视到等矩形的映射，从而无需相机信息。我们的方法在图像和视频的透视到 360° 生成任务中均达到了最先进的性能，优于使用真实相机信息的先前工作。我们还追溯了 ERP 边界处接缝伪影的根本原因，这是由 VAE 编码器中的零填充引起的，并引入了循环潜编码以实现无缝生成。最后，我们在零样本相机视场和方向估计基准测试中展示了有竞争力的结果，证明了 360Anything 的深度几何理解能力及其在计算机视觉任务中的更广泛应用。更多结果可访问 <a target="_blank" rel="noopener" href="https://360anything.github.io/。">https://360anything.github.io/。</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“任意视角图像/视频到360°全景生成”这一任务中因<strong>缺乏相机参数</strong>而导致的鲁棒性不足问题。具体而言：</p>
<ul>
<li>现有方法普遍依赖<strong>显式几何对齐</strong>：先将透视输入投影到等距柱状（ERP）空间，再与目标全景做像素级拼接。</li>
<li>该流程需要准确的<strong>相机内参（FoV）与外参（姿态）</strong>；在真实场景中，这些元数据通常缺失或估计不准，导致误差累积、生成质量下降。</li>
<li>360Anything提出<strong>完全无需相机信息</strong>的解决方案：</li>
<li>将透视图与全景图都视为<strong>token序列</strong>，用预训练Diffusion Transformer（DiT）在序列维度拼接后做全局自注意力，<strong>数据驱动地学习透视↔全景映射</strong>。</li>
<li>训练阶段引入<strong>重力对齐规范化</strong>与<strong>Circular Latent Encoding</strong>，消除ERP边界接缝，实现端到端、可扩展、无几何先验的360°图像/视频外绘。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 360Anything 直接相关的研究可按以下四条主线梳理：</p>
<ol>
<li>全景图像生成</li>
</ol>
<ul>
<li>早期 GAN：CubeGAN、EpipolarGAN、Guided Co-Modulated GAN</li>
<li>自回归/Transformer：PanoLlama、Text2Light、OmniDreamer</li>
<li>扩散模型：Diffusion360、PanoDiffusion、CubeDiff、SphereDiffusion、TanDiT</li>
<li>共同局限：要么需要已知相机参数做像素级投影，要么假设输入 FoV=90°、姿态为零，难以应对“in-the-wild”数据。</li>
</ul>
<ol>
<li>全景视频生成</li>
</ol>
<ul>
<li>纯文本驱动：360DVD、PanoDiT、360-1M、PanoWan</li>
<li>透视→全景外绘：Imagine360、Argus、ViewPoint、VidPanos</li>
<li>共同局限：依赖外部相机估计器，且需在推理阶段用旋转增广或混合解码抑制接缝。</li>
</ul>
<ol>
<li>几何-无感 Transformer 生成</li>
</ol>
<ul>
<li>图像：DiT、FLUX、OmniGen</li>
<li>视频：Wan2.1、Stable-Video-Diffusion</li>
<li>3D 视角：DUSt3R、Geometry-Free View Synthesis</li>
<li>360Anything 首次将“序列拼接+DiT”范式引入全景任务，无需任何显式 3D 归纳偏置。</li>
</ul>
<ol>
<li>接缝/边界处理</li>
</ol>
<ul>
<li>推理阶段后处理：旋转去噪、混合解码、Circular Padding 仅用于解码器</li>
<li>360Anything 首次指出<strong>VAE 编码器零填充</strong>是接缝根源，提出训练阶段 Circular Latent Encoding，从源头消除不连续性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“透视→360°全景”任务重新定义为<strong>纯数据驱动的序列到序列学习问题</strong>，通过三项关键设计彻底摆脱对相机元数据的依赖，并根除 ERP 接缝：</p>
<ol>
<li>几何无感条件注入</li>
</ol>
<ul>
<li>不再做显式投影，而是将透视输入  x<em>(pers)  与带噪全景潜码  y^t</em>(equi)  沿<strong>序列维度拼接</strong>：</li>
</ul>
<p>textttDiT(concat([x<em>(pers), y^t</em>(equi)]))</p>
<ul>
<li>全局自注意力让模型<strong>自行推断</strong>透视图在全景画布中的位置与 FoV，实现端到端映射。</li>
</ul>
<ol>
<li>重力对齐的规范空间</li>
</ol>
<ul>
<li>训练数据全部拉回“直立”坐标系：先用 COLMAP 做帧间稳定，再用 GeoCalib 把重力方向对齐到 ERP 垂直轴。</li>
<li>推理时模型始终生成** canonical、重力向上**的全景，避免对任意姿态学习多种球面畸变，显著提升视觉一致性与几何质量。</li>
</ul>
<ol>
<li>Circular Latent Encoding（源头去接缝）</li>
</ol>
<ul>
<li>发现 ERP 边界接缝源于 VAE 编码器的<strong>零填充</strong>。</li>
<li>编码前对图像做<strong>循环填充</strong>→编码后丢弃填充区域，保证潜码在边界处<strong>循环连续</strong>：</li>
</ul>
<p>y^(pad)<em>(equi) = E!(concat([Y</em>(equi)[-w’:], Y<em>(equi), Y</em>(equi)[:w’]]))</p>
<ul>
<li>训练目标即无缝，无需推理阶段任何后处理。</li>
</ul>
<p>综合以上三点，360Anything 在<strong>不借助任何相机参数</strong>的情况下，达到图像/视频全景外绘新 SOTA，同时零-shot 估计的 FoV/姿态精度可与专用监督方法竞争。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕四个核心问题展开实验，涵盖图像、视频、相机标定与消融分析：</p>
<ol>
<li>全景图像外绘</li>
</ol>
<ul>
<li>数据集：Laval Indoor、SUN360</li>
<li>指标：FID、KID、CLIP-FID、FAED、CLIP-score</li>
<li>结果：360Anything 在所有指标上<strong>超越</strong> CubeDiff 等强基线，FAED 误差降低近 50%。</li>
</ul>
<ol>
<li>全景视频外绘</li>
</ol>
<ul>
<li>数据集：Argus 提供的 101 段真实与仿真轨迹视频</li>
<li>指标：PSNR、LPIPS、FVD、VBench（成像/美学/运动）</li>
<li>结果：全部指标<strong>大幅领先</strong> Imagine360、Argus、ViewPoint；FVD 相对最佳基线降低 ≈ 40%。</li>
</ul>
<ol>
<li>零-shot 相机标定评估</li>
</ol>
<ul>
<li>FoV 估计：NYUv2、ETH3D、iBims-1</li>
<li>平均误差 4.93°，<strong>优于多款监督方法</strong>，仅次于 DUSt3R、MoGe（差距 &lt; 2°）。</li>
<li>姿态估计：MegaDepth、LaMAR</li>
<li>Roll/Pitch 误差 <strong>&lt; 1°</strong> 差距落后于 GeoCalib，<strong>超越其余监督基线</strong>。</li>
</ul>
<ol>
<li>消融与鲁棒性</li>
</ol>
<ul>
<li>Circular Latent Encoding：接缝 Discontinuity Score 从 9.92→3.87（图像），35.5→13.3（视频）。</li>
<li>相机增广：随机 FoV/姿态训练<strong>提升</strong>标准视角下的 FID/FAED，证明泛化性。</li>
<li>规范空间训练：与非 canonical 对比，FVD 降低 ≈ 15%，VBench 全面上升。</li>
<li>真实轨迹数据：加入 20% 真实相机轨迹后，大运动视频不再出现重力方向漂移。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>长时序全景视频</strong><br>当前模型仅支持 81 帧，受限于 DiT 的上下文长度。可尝试将 360Anything 与最新“双向→因果”蒸馏方案（Diffusion Forcing、Self-Forcing 等）结合，把上下文窗口扩展到数百帧，实现<strong>分钟级</strong>沉浸式 3D 世界视频。</p>
</li>
<li><p><strong>高分辨率全景上采样</strong><br>直接套用现有透视视频超分模型会在 ERP 边界重新引入接缝，且球面结构被扭曲。需要研究<strong>球面保持</strong>的上采样或扩散式超分，以生成 4K/8K 无缝全景。</p>
</li>
<li><p><strong>动态场景与物理一致性</strong><br>目前 3D 重建仅针对静态场景。未来可引入 4D 高斯散射或时空神经辐射场，利用 360Anything 输出的<strong>动态全景视频</strong>进行<strong>可交互 4D 场景</strong>重建，并提升物理合理性（反射、流体、刚体运动）。</p>
</li>
<li><p><strong>多模态条件扩展</strong><br>除单张图像/视频外，可探索<strong>文本+草图+深度图</strong>联合条件，或<strong>语音驱动</strong>的 360° 故事生成，实现更灵活的沉浸式内容创作。</p>
</li>
<li><p><strong>显式几何与隐式先验的融合</strong><br>虽然完全无几何先验带来鲁棒性，但在<strong>极端稀少视角</strong>（&lt; 30° FoV）或<strong>重复纹理场景</strong>下可能模糊。可研究“<strong>可选几何提示</strong>”机制：当外部估计器置信度高时，以软注意力形式注入粗略深度或姿态，进一步提升精度。</p>
</li>
<li><p><strong>跨数据集泛化与偏见消除</strong><br>训练数据 90% 为室内合成场景，导致户外自然场景或特殊光照下仍可能出现伪影。需构建<strong>覆盖室内外、昼夜、多种相机传感器</strong>的大规模全景数据集，并引入对抗或域适应策略减轻生成偏见（如三脚架、黑边）。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>360Anything：无需几何先验的任意透视图像/视频→360°全景生成</strong></p>
<ul>
<li><p><strong>问题</strong><br>现有透视→全景方法依赖显式相机参数（FoV、姿态）做像素级投影，一旦元数据缺失或估计不准即失效，且 ERP 边界接缝严重。</p>
</li>
<li><p><strong>方法</strong></p>
</li>
</ul>
<ol>
<li>序列拼接：透视图与带噪全景潜码沿 token 维度拼接，DiT 全局自注意力<strong>数据驱动</strong>学习映射，无需任何相机信息。</li>
<li>重力对齐：训练数据全部规范化至“直立”坐标系，推理时自动输出 canonical 全景。</li>
<li>Circular Latent Encoding：VAE 编码前循环填充→编码后裁边，从源头消除接缝。</li>
</ol>
<ul>
<li><strong>结果</strong></li>
<li>图像外绘：Laval/SUN360 上 FID、KID、FAED 全面刷新 SOTA，FAED 误差降 50%。</li>
<li>视频外绘：Argus 基准 PSNR↑3+ dB，FVD 降 ≈40%，VBench 全指标第一。</li>
<li>零-shot 标定：FoV 误差 &lt;5°，姿态误差 &lt;1°，媲美专用监督网络。</li>
<li>消融：接缝分数降 60%+；随机相机增广与 canonical 训练均显著提升泛化。</li>
<li><strong>意义</strong><br>首次实现<strong>完全端到端、无相机参数、无缝且可扩展</strong>的透视→360°图像与视频外绘，并支持后续 3D/4D 场景重建，为沉浸式世界生成提供新基线。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是360Anything%3A%20Geometry-Free%20Lifting%20of%20Images%20and%20Videos%20to%20360%C2%B0，链接是https://arxiv.org/pdf/2601.16192，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16192。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ziyi Wu,Daniel Watson,Andrea Tagliasacchi,David J. Fleet,Marcus A. Brubaker,Saurabh Saxena</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16192.pdf">https://arxiv.org/pdf/2601.16192.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16192">https://arxiv.org/abs/2601.16192</a></p>
<p><strong>Arxiv ID</strong>: 2601.16192</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16192">https://papers.cool/arxiv/2601.16192</a></p>
<p><strong>Published</strong>: 2026-01-22T18:45:59Z</p>
<p><strong>Updated</strong>: 2026-01-22T18:45:59.000Z</p>
<hr>
<h3 id="20-Agentic-Uncertainty-Quantification"><a href="#20-Agentic-Uncertainty-Quantification" class="headerlink" title="20. Agentic Uncertainty Quantification"></a>20. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15703">Agentic Uncertainty Quantification</a></h3><p>Although AI agents have demonstrated impressive capabilities in long-horizon reasoning, their reliability is severely hampered by the ``Spiral of Hallucination,’’ where early epistemic errors propagate irreversibly. Existing methods face a dilemma: uncertainty quantification (UQ) methods typically act as passive sensors, only diagnosing risks without addressing them, while self-reflection mechanisms suffer from continuous or aimless corrections. To bridge this gap, we propose a unified Dual-Process Agentic UQ (AUQ) framework that transforms verbalized uncertainty into active, bi-directional control signals. Our architecture comprises two complementary mechanisms: System 1 (Uncertainty-Aware Memory, UAM), which implicitly propagates verbalized confidence and semantic explanations to prevent blind decision-making; and System 2 (Uncertainty-Aware Reflection, UAR), which utilizes these explanations as rational cues to trigger targeted inference-time resolution only when necessary. This enables the agent to balance efficient execution and deep deliberation dynamically. Extensive experiments on closed-loop benchmarks and open-ended deep research tasks demonstrate that our training-free approach achieves superior performance and trajectory-level calibration. We believe this principled framework AUQ represents a significant step towards reliable agents.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管人工智能代理在长期推理中展示了令人印象深刻的能力，但它们的可靠性受到“幻觉螺旋”严重制约，即早期的认知错误会不可逆地传播。现有方法面临两难境地：不确定性量化（UQ）方法通常仅作为被动传感器，只诊断风险而不加以解决，而自我反思机制则容易陷入持续或无目的的校正。为弥合这一差距，我们提出了统一的双过程代理不确定性量化（AUQ）框架，将口头表达的不确定性转化为主动的、双向的控制信号。我们的架构包含两种互补机制：系统1（不确定性感知记忆，UAM），能隐式传播口头表达的置信度和语义解释，以防止盲目决策；系统2（不确定性感知反思，UAR），将这些解释作为理性提示，仅在必要时触发针对性的推理期解决。这使代理能够动态平衡高效执行与深入思考。在闭环基准测试和开放性深度研究任务上的大量实验证明，我们的无训练方法在性能和轨迹级校准方面都具有优越表现。我们相信，这一原则性框架AUQ代表了迈向可靠代理的重要一步。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>长程自主智能体在执行任务时因早期认知错误不可逆传播而导致的“幻觉螺旋”可靠性瓶颈</strong>。具体而言：</p>
<ul>
<li><strong>问题背景</strong>：由大模型驱动的自主智能体在长程推理与工具调用中表现突出，但一旦出现早期 grounding 错误，该错误会被后续步骤当作“真实上下文”不断放大，最终使整个轨迹失效（即“幻觉螺旋”）。</li>
<li><strong>现有局限</strong>：</li>
<li>被动式不确定性量化（UQ）仅诊断风险，不主动修正；</li>
<li>自反思机制缺乏明确信号，常陷入盲目或无止境的修正循环。</li>
<li><strong>核心目标</strong>：将<strong>被动的不确定性诊断</strong>转化为<strong>主动的双向控制信号</strong>，在轨迹层面既抑制错误固化（正向约束），又在检测到高风险时触发有针对性的推理时修正（逆向校准），从而提升长程可靠性与任务成功率。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在附录 A.1 与第 1 段中系统梳理了相关研究，可归纳为四大脉络：</p>
<ol>
<li><strong>LLM 不确定性量化（UQ）</strong></li>
</ol>
<ul>
<li>经典温度缩放、Logit-based 方法（Guo et al. 2017）</li>
<li>语义熵、Verbalized Confidence（Lin et al. 2022；Tian et al. 2023；Groot &amp; Valdenegro-Toro 2024）</li>
<li>局限：仅对单轮生成做静态诊断，未嵌入多步轨迹控制</li>
</ul>
<ol>
<li><strong>自主智能体与错误传播分析</strong></li>
</ol>
<ul>
<li>ReAct、Reflexion 等推理-行动框架（Yao et al. 2022b；Shinn et al. 2023）</li>
<li>“幻觉螺旋”现象刻画（Zhang et al. 2024；Dziri et al. 2023）</li>
<li>失败归因与调试层方法（Zhu et al. 2025；Cemri et al. 2025）</li>
<li>特点：事后诊断或经验回放，缺乏运行时干预机制</li>
</ul>
<ol>
<li><strong>智能体不确定性建模</strong></li>
</ol>
<ul>
<li>UProp / SAUP：序列不确定性传播数学建模（Duan et al. 2025；Zhao et al. 2025）</li>
<li>工具交互不确定性（Liu et al. 2024a；Lymperopoulos &amp; Sarathy 2025）</li>
<li>共性：聚焦风险量化，未提供可执行的修正策略</li>
</ul>
<ol>
<li><strong>自反思与测试时计算</strong></li>
</ol>
<ul>
<li>Self-Refine、CRITIC、Self-Reflection（Madaan et al. 2023；Gou et al. 2023；Renze &amp; Guven 2024）</li>
<li>训练式轨迹校准 STeCa（Wang et al. 2025）</li>
<li>问题：无 ground-truth 时触发盲目，或需昂贵微调</li>
</ul>
<p>AUQ 与上述工作的区别：首次将<strong>不确定性量化</strong>与<strong>双过程控制</strong>耦合，实现<strong>无训练、轨迹级、主动式</strong>的错误传播抑制与推理时修正。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>Dual-Process Agentic Uncertainty Quantification (AUQ)</strong> 框架，将“口头表达的不确定性”转化为<strong>双向控制信号</strong>，在轨迹层面同时完成“抑制错误固化”与“定向修正”两个目标。核心机制分为三步：</p>
<ol>
<li>形式化分解<br>把智能体 UQ 拆成互补的两大数学问题：</li>
</ol>
<ul>
<li><strong>Forward</strong>：估计轨迹累计有效性</li>
</ul>
<p>P(V<em>t|h_t)=f</em>(fp)!(P(V<em>(t-1)|h</em>(t-1)),π(a_t|h_t))</p>
<ul>
<li><strong>Inverse</strong>：当  P(V_t|h_t)&lt;δ  时，求解最优修正动作</li>
</ul>
<p>a^*=argmax_a ∫ P(a|z,h_t),P(z|Succ,h_t),dz</p>
<ol>
<li>双过程架构</li>
</ol>
<ul>
<li><strong>System 1 (UAM)</strong>：默认快路径</li>
<li>每步生成  (a_t,hat c_t,hat e_t) ，把  hat c_t,hat e_t  写回上下文</li>
<li>利用注意力“软约束”抑制后续高承诺动作，实现<strong>不确定性前向传播</strong></li>
<li><strong>System 2 (UAR)</strong>：触发慢路径</li>
<li>当  hat c_t&lt;τ  时，以  hat e_t  为<strong>理性线索</strong>，启动 Best-of-N 反思</li>
<li>采用一致性加权得分</li>
</ul>
<p>S<em>(cons)(a)=(1) / (N)∑</em>(k=1)^N hat c<em>(new)^((k))·Il(a</em>(new)^((k))equiv ar)</p>
<p>选取高置信且语义一致的动作，实现<strong>推理时逆向校准</strong></p>
<ul>
<li>切换函数</li>
</ul>
<p>π<em>(dual)(a|h_t)= π</em>(fwd)(a|h<em>t,M_t), &amp; hat c_tgeτ[2pt] π</em>(∈v)(a|h_t), &amp; hat c_t&lt;τ</p>
<p>保证只在高不确定步骤消耗额外计算。</p>
<ol>
<li>轨迹级评估与记忆扩展</li>
</ol>
<ul>
<li>提出  Phi<em>(last),Phi</em>(avg),Phi_(min)  三种轨迹置信聚合，配合 T-ECE、T-BS、AUROC 进行校准与判别力评测</li>
<li>有限上下文场景下，若反射后仍  hat c&lt;τ ，自动加载全量历史再执行一次 System 2，实现<strong>分层防御</strong>而不过度消耗长上下文</li>
</ul>
<p>通过“<strong>前向记忆约束 + 逆向反射修正</strong>”的闭环，AUQ 无需额外训练即可在长程任务中显著降低幻觉传播、提升成功率与校准度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕 <strong>“闭环决策”</strong> 与 <strong>“开放式深度研究”</strong> 两大场景展开，共覆盖 <strong>3 个基准、8 个模型、6 类基线</strong> 与 <strong>3 种消融变体</strong>，从 <strong>性能、校准、判别力、成本</strong> 四个维度系统验证 AUQ 的有效性。</p>
<ol>
<li>数据集与任务</li>
</ol>
<ul>
<li>ALFWorld：140 个 embodied 家务任务，评估长程规划一致性</li>
<li>WebShop：140 个电商购物任务，评估噪声环境下的鲁棒性</li>
<li>DeepResearch Bench：100 个博士级研究题，评估开放式综合报告质量（RACE 四维度评分）</li>
</ul>
<ol>
<li>对比方法</li>
</ol>
<ul>
<li>纯 System-1：ReAct</li>
<li>无不确定性自反思：Self-Reflection</li>
<li>跨回合经验回放：Reflexion</li>
<li>纯采样集成：CoT-SC</li>
<li>单过程消融：Forward-UQ（UAM-only）、Inverse-UQ（UAR-only）</li>
</ul>
<ol>
<li>模型谱系</li>
</ol>
<ul>
<li>闭源：GPT-5.1、GPT-4.1、GPT-4o、Gemini-2.5-Pro/Flash</li>
<li>开源：Qwen3-235B、DeepSeek-V3.1<br>全部保持 <strong>zero-shot + 无微调</strong></li>
</ul>
<ol>
<li>主要定量结果</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>指标</th>
<th>ReAct</th>
<th>Reflexion</th>
<th>CoT-SC</th>
<th>AUQ(双过程)</th>
</tr>
</thead>
<tbody>
<tr>
<td>ALFWorld</td>
<td>Success Rate</td>
<td>63.6 %</td>
<td>67.9 %</td>
<td>69.5 %</td>
<td>74.3 %</td>
</tr>
<tr>
<td>WebShop</td>
<td>Success Rate</td>
<td>29.3 %</td>
<td>30.7 %</td>
<td>37.1 %</td>
<td>42.9 %</td>
</tr>
<tr>
<td>DeepResearch</td>
<td>RACE 总分</td>
<td>—</td>
<td>49.71</td>
<td>50.62</td>
<td>52.09</td>
</tr>
</tbody>
</table>
</div>
<p>校准与判别力（Trajectory-ECE ↓ / AUROC ↑）</p>
<ul>
<li>ALFWorld T-ECE：ReAct 0.255 → AUQ <strong>0.093</strong></li>
<li>WebShop AUROC：ReAct 0.608 → AUQ <strong>0.755</strong></li>
</ul>
<ol>
<li>关键消融与敏感性</li>
</ol>
<ul>
<li>τ ∈{0.8,0.85,0.9,0.95}，τ≈0.9 处出现 <strong>Pareto 最优</strong>（成功率-成本拐点）</li>
<li>有限记忆（h=5）下，Adaptive Memory Expansion 为 GPT-5.1 带来 <strong>+17.9 %</strong> 绝对增益</li>
<li>与 ReAct 对比：AUQ 修正 <strong>14.3 %</strong> 原失败轨迹，仅 <strong>3.6 %</strong> 成功轨迹出现回归，净修复比 ≈4:1</li>
</ul>
<ol>
<li>成本效率</li>
</ol>
<ul>
<li>平均失败轨迹步数：ReAct 50 步 vs AUQ 26.4 步</li>
<li>Effective Cost-per-Success 下降 <strong>≈20 %</strong>，反射开销被“早停”节省的环境交互费抵消</li>
</ul>
<ol>
<li>定性案例</li>
</ol>
<ul>
<li>ALFWorld：System-2 检测到缺失 desklamp，打破“拿碗-放碗”死循环，8 步完成 vs 基线 50 步超时</li>
<li>DeepResearch：System-2 针对“定义标准模糊”升级工具为 academic_search，新增区域/行业细分查询，报告 Insight 得分从 49.45 提升至 54.21</li>
</ul>
<p>实验结论：AUQ 在 <strong>零训练</strong> 条件下，跨模型、跨任务、跨场景一致地提升成功率、降低校准误差，并维持最优成本效率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可视为 AUQ 框架的直接延伸或深层扩展，均围绕“<strong>更细粒度、更自适应、更分布式、更可信</strong>”展开：</p>
<ol>
<li><strong>动态阈值与风险预算</strong><br>将静态 τ 升级为 <strong>step-level 自适应函数</strong></li>
</ol>
<p>τ_t = f(action-type, remaining-budget, stakes)</p>
<p>引入强化学习或元控制器，实现“高价值 API 调用前强制低 τ，纯推理步骤放宽 τ”的在线风险预算。</p>
<ol>
<li><strong>多智能体不确定性传播</strong><br>把 Forward/Inverse 公式从单 agent 推广到 <strong>多 agent 图</strong></li>
</ol>
<p>P(V<em>t^((i))|h_t^((1..N))) = f</em>(μlti)!l(P(V<em>(t-1)^((j))|h</em>(t-1)^((j))), π^((i))(a_t^((i))|h_t^((i)), neighbor-c)r)</p>
<p>研究“个体幻觉”如何通过消息传递演变为<strong>系统级共识幻觉</strong>，并设计分布式反射协议。</p>
<ol>
<li><strong>持续学习与记忆演化</strong><br>当前 UAM 仅在同一条轨迹内保留  (c,e) ；可探索 <strong>跨任务 episodic memory</strong></li>
</ol>
<ul>
<li>用检索增强记忆库  M_(global)  存储高置信失败/成功案例</li>
<li>在下次遇到相似  e  时直接检索已验证计划，减少重复反射开销。</li>
</ul>
<ol>
<li><strong>细粒度不确定性分解</strong><br>把单一  c  拆成 <strong>多维置信向量</strong></li>
</ol>
<p>(c<em>(fact), c</em>(plan), c<em>(tool), c</em>(temporal))</p>
<p>让 System-2 针对具体维度触发<strong>靶向修正</strong>（如仅重新检索事实或仅重排计划步骤）。</p>
<ol>
<li><strong>外部验证器与工具链闭环</strong><br>将“口头解释” e  自动映射为<strong>可执行验证脚本</strong></li>
</ol>
<ul>
<li>调用符号 solver、知识图谱或代码解释器对  e  中提到的“缺失数据”进行<strong>真值检验</strong></li>
<li>把检验结果作为观测  o_(verify)  重新送入 Inverse-UQ，形成<strong>双循环反思</strong>。</li>
</ul>
<ol>
<li><strong>小模型不确定性蒸馏</strong><br>针对 &lt;7B 模型 verbalized  c  校准度下降问题，设计<strong>大→小蒸馏</strong></li>
</ol>
<ul>
<li>用大模型生成  (c,e)  伪标签，训练小模型<strong>元置信头</strong></li>
<li>保持 AUQ 框架无需改动即可在边缘端部署。</li>
</ul>
<ol>
<li><strong>安全与对齐风险</strong></li>
</ol>
<ul>
<li>研究“<strong>过度自信攻击</strong>”：恶意提示诱导 System-2 产生 delusional confirmation，使  c↑  但内容错误</li>
<li>引入<strong>对抗性阈值扰动</strong>测试，量化 AUQ 在<strong>安全关键域</strong>（医疗、法律）下的可靠性边界。</li>
</ul>
<ol>
<li><strong>环境非平稳与概念漂移</strong><br>当外部工具 API 返回值分布随时间漂移时，Forward-UQ 的递归乘积会<strong>系统性低估</strong>风险。可引入<strong>漂移检测器</strong></li>
</ol>
<p>Delta<em>t = KL(P</em>(tool)(o<em>t) | P</em>(ref))</p>
<p>一旦  Delta_t&gt;eta ，自动重置历史置信权重，防止“<strong>置信冻结</strong>”。</p>
<ol>
<li><p><strong>多模态不确定性融合</strong><br>将文本  c  与视觉、音频模态的<strong>传感器不确定性</strong>统一纳入同一双过程框架，研究跨模态幻觉的耦合传播与联合修正。</p>
</li>
<li><p><strong>可解释性与人类协同</strong><br>开发<strong>可视化仪表盘</strong>，实时展示  c_t 、触发原因  e_t  及 System-2 修正路径，支持人类<strong>即时干预或回滚</strong>，实现“人在回路”的混合主动控制。</p>
</li>
</ol>
<p>这些方向既可直接嵌入现有 AUQ 代码库进行消融，也可作为独立子课题展开理论与实证研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Dual-Process Agentic Uncertainty Quantification (AUQ)</strong>，用<strong>不确定性驱动</strong>的双系统架构解决长程智能体“幻觉螺旋”可靠性瓶颈，核心内容可概括为 <strong>“一个框架、两条路径、三类实验、四项贡献”</strong>：</p>
<p>1. 一个框架：AUQ</p>
<ul>
<li><strong>零训练</strong>、<strong>推理时</strong>生效</li>
<li>把口头置信  (hat c, hat e)  变成<strong>双向控制信号</strong></li>
<li><strong>正向</strong>：抑制早期错误固化</li>
<li><strong>逆向</strong>：触发定向修正</li>
</ul>
<p>2. 两条路径：System 1 ↔ System 2</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>路径</th>
<th>触发条件</th>
<th>机制</th>
<th>作用</th>
</tr>
</thead>
<tbody>
<tr>
<td>System 1 (UAM)</td>
<td>默认执行</td>
<td>上下文保留  (hat c,hat e) ，注意力软约束</td>
<td>不确定性前向传播，防盲目承诺</td>
</tr>
<tr>
<td>System 2 (UAR)</td>
<td>hat c&lt;τ</td>
<td>以  hat e  为线索，Best-of-N + 一致性加权  S_(cons)</td>
<td>推理时逆向校准，定向纠错</td>
</tr>
</tbody>
</table>
</div>
<p><strong>切换函数</strong></p>
<p>π<em>(dual)(a|h_t)= π</em>(fwd) &amp; hat c<em>tgeτ[4pt] π</em>(∈v) &amp; hat c_t&lt;τ</p>
<p>3. 三类实验</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>数据</th>
<th>指标</th>
<th>主要结果</th>
</tr>
</thead>
<tbody>
<tr>
<td>闭环决策</td>
<td>ALFWorld 140 任务</td>
<td>Success Rate ↑T-ECE ↓</td>
<td>74.3 % vs 63.6 %0.093 vs 0.255</td>
</tr>
<tr>
<td>噪声环境</td>
<td>WebShop 140 任务</td>
<td>Success Rate ↑AUROC ↑</td>
<td>42.9 % vs 29.3 %0.755 vs 0.608</td>
</tr>
<tr>
<td>开放研究</td>
<td>DeepResearch 100 题</td>
<td>RACE 总分 ↑</td>
<td>52.09 vs 49.71 (SOTA)</td>
</tr>
</tbody>
</table>
</div>
<p>4. 四项贡献</p>
<ol>
<li><strong>形式化</strong>：首次把智能体 UQ 拆成 <strong>Forward 传播</strong> 与 <strong>Inverse 校准</strong> 两个互补数学问题</li>
<li><strong>架构</strong>：提出 <strong>双过程 Agent 架构</strong>，无训练即可动态平衡“快执行-慢反思”</li>
<li><strong>指标</strong>：设计 <strong>轨迹级校准指标</strong> (T-ECE / T-BS / AUROC)，解决传统 token 级指标失效问题</li>
<li><strong>验证</strong>：跨 8 模型、3 基准、6 基线一致提升 <strong>成功率+校准度+成本效率</strong>，并给出 <strong>Pareto 最优 τ≈0.9</strong></li>
</ol>
<p><strong>一句话总结</strong>：AUQ 把“我知道我不知道”变成“我立刻修正”，在长程任务中实现 <strong>更高成功率、更低幻觉、更省计算</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic%20Uncertainty%20Quantification，链接是https://arxiv.org/pdf/2601.15703，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15703。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jiaxin Zhang,Prafulla Kumar Choubey,Kung-Hsiang Huang,Caiming Xiong,Chien-Sheng Wu</p>
<p><strong>Categories</strong>: cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15703.pdf">https://arxiv.org/pdf/2601.15703.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15703">https://arxiv.org/abs/2601.15703</a></p>
<p><strong>Arxiv ID</strong>: 2601.15703</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15703">https://papers.cool/arxiv/2601.15703</a></p>
<p><strong>Published</strong>: 2026-01-22T07:16:26Z</p>
<p><strong>Updated</strong>: 2026-01-22T07:16:26.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202601/2026-01-25/">https://wdndev.github.io/daily/hf/202601/2026-01-25/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-EvoCUA-Evolving-Computer-Use-Agents-via-Learning-from-Scalable-Synthetic-Experience"><span class="toc-text">1. EvoCUA: Evolving Computer Use Agents via Learning from Scalable Synthetic Experience</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-HERMES-KV-Cache-as-Hierarchical-Memory-for-Efficient-Streaming-Video-Understanding"><span class="toc-text">2. HERMES: KV Cache as Hierarchical Memory for Efficient Streaming Video Understanding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-The-Flexibility-Trap-Why-Arbitrary-Order-Limits-Reasoning-Potential-in-Diffusion-Language-Models"><span class="toc-text">3. The Flexibility Trap: Why Arbitrary Order Limits Reasoning Potential in Diffusion Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-LLM-in-Sandbox-Elicits-General-Agentic-Intelligence"><span class="toc-text">4. LLM-in-Sandbox Elicits General Agentic Intelligence</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-BayesianVLA-Bayesian-Decomposition-of-Vision-Language-Action-Models-via-Latent-Action-Queries"><span class="toc-text">5. BayesianVLA: Bayesian Decomposition of Vision Language Action Models via Latent Action Queries</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-Scaling-Text-to-Image-Diffusion-Transformers-with-Representation-Autoencoders"><span class="toc-text">6. Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Stable-DiffCoder-Pushing-the-Frontier-of-Code-Diffusion-Large-Language-Model"><span class="toc-text">7. Stable-DiffCoder: Pushing the Frontier of Code Diffusion Large Language Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-SAMTok-Representing-Any-Mask-with-Two-Words"><span class="toc-text">8. SAMTok: Representing Any Mask with Two Words</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-Learning-to-Discover-at-Test-Time"><span class="toc-text">9. Learning to Discover at Test Time</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-Qwen3-TTS-Technical-Report"><span class="toc-text">10. Qwen3-TTS Technical Report</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-Terminal-Bench-Benchmarking-Agents-on-Hard-Realistic-Tasks-in-Command-Line-Interfaces"><span class="toc-text">11. Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-OpenVision-3-A-Family-of-Unified-Visual-Encoder-for-Both-Understanding-and-Generation"><span class="toc-text">12. OpenVision 3: A Family of Unified Visual Encoder for Both Understanding and Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Rethinking-Composed-Image-Retrieval-Evaluation-A-Fine-Grained-Benchmark-from-Image-Editing"><span class="toc-text">13. Rethinking Composed Image Retrieval Evaluation: A Fine-Grained Benchmark from Image Editing</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-Towards-Automated-Kernel-Generation-in-the-Era-of-LLMs"><span class="toc-text">14. Towards Automated Kernel Generation in the Era of LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-PROGRESSLM-Towards-Progress-Reasoning-in-Vision-Language-Models"><span class="toc-text">15. PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-Cosmos-Policy-Fine-Tuning-Video-Models-for-Visuomotor-Control-and-Planning"><span class="toc-text">16. Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#17-VideoMaMa-Mask-Guided-Video-Matting-via-Generative-Prior"><span class="toc-text">17. VideoMaMa: Mask-Guided Video Matting via Generative Prior</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-ActionMesh-Animated-3D-Mesh-Generation-with-Temporal-3D-Diffusion"><span class="toc-text">18. ActionMesh: Animated 3D Mesh Generation with Temporal 3D Diffusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-360Anything-Geometry-Free-Lifting-of-Images-and-Videos-to-360%C2%B0"><span class="toc-text">19. 360Anything: Geometry-Free Lifting of Images and Videos to 360°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20-Agentic-Uncertainty-Quantification"><span class="toc-text">20. Agentic Uncertainty Quantification</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>