<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2026-01-01 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric RefinementIn this report, we introduce UltraShape 1.0, a scalable 3D diffusion framew">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2026-01-01">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202601/2026-01-01/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric RefinementIn this report, we introduce UltraShape 1.0, a scalable 3D diffusion framew">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:54.981Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202601/2026-01-01/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2026-01-01',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2026-01-01</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:54.981Z" title="Updated 2026-02-08 08:00:54">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">24.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>85min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2026-01-01"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement"><a href="#1-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement" class="headerlink" title="1. UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21185">UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement</a></h3><p>In this report, we introduce UltraShape 1.0, a scalable 3D diffusion framework for high-fidelity 3D geometry generation. The proposed approach adopts a two-stage generation pipeline: a coarse global structure is first synthesized and then refined to produce detailed, high-quality geometry. To support reliable 3D generation, we develop a comprehensive data processing pipeline that includes a novel watertight processing method and high-quality data filtering. This pipeline improves the geometric quality of publicly available 3D datasets by removing low-quality samples, filling holes, and thickening thin structures, while preserving fine-grained geometric details. To enable fine-grained geometry refinement, we decouple spatial localization from geometric detail synthesis in the diffusion process. We achieve this by performing voxel-based refinement at fixed spatial locations, where voxel queries derived from coarse geometry provide explicit positional anchors encoded via RoPE, allowing the diffusion model to focus on synthesizing local geometric details within a reduced, structured solution space. Our model is trained exclusively on publicly available 3D datasets, achieving strong geometric quality despite limited training resources. Extensive evaluations demonstrate that UltraShape 1.0 performs competitively with existing open-source methods in both data processing quality and geometry generation. All code and trained models will be released to support future research.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>在本报告中，我们介绍了 UltraShape 1.0，一种可扩展的 3D 扩散框架，用于高保真 3D 几何生成。所提出的方法采用两阶段生成流程：首先合成粗略的整体结构，然后进行精细化生成，以产生详细、高质量的几何体。为了支持可靠的 3D 生成，我们开发了一套全面的数据处理流程，包括一种新颖的密闭处理方法和高质量的数据筛选。该流程通过移除低质量样本、填补孔洞和加厚薄结构，同时保持细粒度几何细节，从而提高了公开 3D 数据集的几何质量。为了实现细粒度几何精化，我们在扩散过程中将空间定位与几何细节合成解耦。我们通过在固定空间位置执行基于体素的精化来实现这一点，其中从粗略几何体获取的体素查询提供了由 RoPE 编码的显式位置锚点，使扩散模型能够专注于在一个简化、结构化的解空间内合成局部几何细节。我们的模型仅在公开可用的 3D 数据集上进行训练，在训练资源有限的情况下仍实现了强大的几何质量。大量评估表明，UltraShape 1.0 在数据处理质量和几何生成方面均与现有开源方法具有竞争力。所有代码和训练模型将被发布，以支持未来的研究。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>UltraShape 1.0 旨在一次性解决当前 3D 几何生成面临的三大核心瓶颈：</p>
<ol>
<li>数据质量瓶颈</li>
</ol>
<ul>
<li>公开 3D 数据集中普遍存在非水密、含孔洞、薄壳、碎片、姿态错乱、内部伪结构等低质量样本，直接训练会导致模型学到噪声拓扑。</li>
<li>论文提出一套可扩展的水密化处理与多级过滤管线，将 80 万原始模型精炼为 12 万高质量水密资产，为后续生成提供干净、可学习的信号。</li>
</ul>
<ol>
<li>可扩展性瓶颈</li>
</ol>
<ul>
<li>稀疏体素或向量集表示在分辨率升高时面临内存与计算立方增长，难以在单卡上实现高保真细节。</li>
<li>论文采用“粗→细”两阶段扩散：</li>
<li>阶段 1 用向量集 DiT 快速生成低分辨率但整体可信的粗结构；</li>
<li>阶段 2 在固定 128³ 体素格上执行局部细化，仅对粗结构提供的空间锚点进行去噪，显著压缩了解空间，实现单卡 2048³ 等效分辨率的细节合成。</li>
</ul>
<ol>
<li>细节保真度瓶颈</li>
</ol>
<ul>
<li>向量集方法因全局无序令牌难以刻画高频几何；纯体素方法又因令牌爆炸难以训练。</li>
<li>论文通过“体素查询 + RoPE 位置编码”把空间定位与几何细节解耦，令扩散模型只需在已知位置合成局部 SDF，显著增强边缘、褶皱等细粒度结构。</li>
</ul>
<p>综上，UltraShape 1.0 用“数据侧水密化+生成侧粗到细”双管齐下，首次在公开数据、有限算力条件下实现可与商业系统媲美的高保真、可扩展 3D 几何生成。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 UltraShape 1.0 直接相关的研究可按“数据水密化→3D 表示→扩散框架→两阶段/稀疏加速”四条主线梳理。下列工作被论文重点引用或作为对比基线，构成了其技术上下文。</p>
<p>1. 水密化与拓扑修复</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>核心机制</th>
<th>主要局限</th>
</tr>
</thead>
<tbody>
<tr>
<td>ManifoldPlus [7]</td>
<td>Flood-fill 从外部种子推断符号</td>
<td>对非水密/自交输入易“泄漏”，生成双层薄壳</td>
</tr>
<tr>
<td>基于 Visibility Check [12,13,28]</td>
<td>射线投射判断内外</td>
<td>遮挡敏感、薄壁区域噪声大</td>
</tr>
<tr>
<td>UDF→伪 SDF [2,28]</td>
<td>减 ε 偏移估计符号</td>
<td>缺乏显式内外判断，易丢孤立组件</td>
</tr>
<tr>
<td>Dora [2]</td>
<td>采样+基准测试框架</td>
<td>本身非修复算法，仅用于评估</td>
</tr>
<tr>
<td>UltraShape 水密化</td>
<td>稀疏体素分水岭+CUDA 并行</td>
<td>2048³ 可扩展，先闭合孔洞再增厚开放面，避免上述副作用</td>
</tr>
</tbody>
</table>
</div>
<p>2. 3D 表示与 VAE 框架</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>表示范式</th>
<th>代表工作</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>向量集 / 三平面</td>
<td>3DShape2VecSet [27]、CLAY [28]、Hunyuan3D 2.0/2.1 [8,29]、FlashVDM [11]、TripoSG [14]</td>
<td>阶段 1 直接复用 Hunyuan3D-2.1 的向量集 VAE+DiT</td>
</tr>
<tr>
<td>稀疏体素 / 三平面哈希</td>
<td>TRELLIS [24]、TRELLIS.2 [23]、TripoSF [5]、Sparc3D [15]、Hi3DGen [26]、Direct3D-S2 [22]</td>
<td>阶段 2 借鉴“稀疏结构→细化”思想，但改用固定 128³ 网格+RoPE 查询</td>
</tr>
<tr>
<td>混合 coarse-to-fine</td>
<td>Ultra3D [3]、LATTICE [10]</td>
<td>Ultra3D 用向量集粗体素细；LATTICE 用“体素集”给向量模型提供结构化查询；UltraShape 进一步把空间定位与细节合成完全解耦</td>
</tr>
</tbody>
</table>
</div>
<p>3. 扩散 Transformer（DiT）在 3D 的应用</p>
<ul>
<li><strong>DiT 原论文</strong><br>17<br>：将 Vision Transformer 扩展到扩散去噪，奠定本文两阶段 DiT 结构基础。</li>
<li><strong>Hunyuan3D-2.1</strong><br>8<br>：开源 3D DiT，提供阶段 1 的预训练权重与 VAE，UltraShape 在其上继续微调。</li>
<li><strong>RoPE</strong><br>19<br>：旋转位置编码，被引入到 3D 令牌，使体素查询具备显式空间顺序，提升细阶段收敛速度。</li>
</ul>
<p>4. 两阶段或稀疏加速生成</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>加速/细化策略</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>TRELLIS 系列 [23,24]</td>
<td>先稀疏体素掩码→再密集细化</td>
<td>需学习动态掩码，令牌数仍随分辨率平方增长</td>
</tr>
<tr>
<td>Direct3D-S2 [22]</td>
<td>Spatial Sparse Attention</td>
<td>在注意力层面剪枝，非固定查询空间</td>
</tr>
<tr>
<td>LATTICE [10]</td>
<td>体素集→向量 DiT</td>
<td>提供结构化查询，但仍让 DiT 同时学习位置+细节</td>
</tr>
<tr>
<td>UltraShape</td>
<td>固定 128³ 网格+粗几何锚点</td>
<td>查询空间预先定死，DiT 仅做“局部 SDF 雕刻”，训练与推理内存可控，支持 32k 令牌外推</td>
</tr>
</tbody>
</table>
</div>
<p>5. 其他相关方向</p>
<ul>
<li><strong>Score Distillation Sampling (SDS)</strong>：DreamFusion<br>18<br>、ProlificDreamer<br>21<br>——通过 2D 扩散蒸馏 3D，不需 3D 数据，但几何保真度与一致性低于原生 3D 扩散。</li>
<li><strong>Large Reconstruction Models (LRM)</strong>：LRM<br>6<br>、InstantMesh<br>25<br>、LGM<br>20<br>——单图→3D 前馈网络，推理快但细节受限于 MVS 或 triplane 分辨率，通常作为对比基线而非生成式扩散。</li>
</ul>
<p>小结</p>
<p>UltraShape 1.0 在数据侧吸收了水密化与过滤的最新经验，在生成侧把“向量集全局结构 + 稀疏体素局部细节”两条主流路线进行显式解耦，并借助 RoPE 与固定查询空间实现可扩展的高分辨率细化，从而站在上述工作的交叉点上，形成一套公开数据即可训练、单卡即可推理的高保真 3D 扩散框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>UltraShape 1.0 把“数据质量”与“生成可扩展性”拆成两条并行管线，各自用一套可复现的算法闭环，最终通过“粗→细”两阶段耦合输出高保真几何。具体做法可概括为 4 步：</p>
<p>1. 数据侧：水密化 + 三级过滤</p>
<p><strong>目标</strong>：把公开 80 万 Objaverse 模型变成 12 万“干净、可学习”的水密资产。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>子模块</th>
<th>关键算子</th>
<th>解决的问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>水密化</td>
<td>稀疏体素分水岭 + CUDA 并行 2048³</td>
<td>补洞、封闭开放面、增厚零体积区域，避免传统 flood-fill/visibility 的泄漏或噪声</td>
</tr>
<tr>
<td>VLM 过滤</td>
<td>渲染深度/法向图 → 视觉-语言模型打分</td>
<td>剔除简单几何、地面平面、扫描噪声场景</td>
</tr>
<tr>
<td>姿态归一化</td>
<td>VLM 检测 + canonicalization 网络</td>
<td>把同类别资产转到一致朝向，降低 DiT 学习难度</td>
</tr>
<tr>
<td>几何过滤</td>
<td>内外点比例 + VAE 重建碎片化检测</td>
<td>剔除内部伪结构、薄壳、重建后易碎模型</td>
</tr>
</tbody>
</table>
</div>
<p>2. 表示侧：改造 VAE 使其支持“体素查询”</p>
<ul>
<li>原始 Hunyuan3D-2.1 VAE 仅接受<strong>表面点</strong>查询。</li>
<li><strong>改进</strong>：训练时在表面点上加 $<br>-1/128, 1/128<br>$ 随机偏移，让解码器学会预测<strong>非表面 SDF</strong>；推理时即可用固定 128³ 体素中心作为查询点，得到完整体积场。</li>
</ul>
<p>3. 生成侧：两阶段解耦策略</p>
<p>Stage-1 全局结构（向量集 DiT）</p>
<ul>
<li>直接复用 Hunyuan3D-2.1 的预训练权重，输出低分辨率但拓扑合理的粗网格。</li>
<li>仅目标：给 Stage-2 提供“空间锚点”，无需细节。</li>
</ul>
<p>Stage-2 局部细化（体素查询 DiT）</p>
<ul>
<li><strong>输入</strong>：128³ 体素中心坐标 + Stage-1 粗网格给出的 SDF 初值。</li>
<li><strong>位置编码</strong>：用 RoPE 把三维坐标旋转嵌入，使令牌自带空间顺序。</li>
<li><strong>条件融合</strong>：DINOv2 图像特征通过 cross-attention 注入，背景令牌被 mask 掉。</li>
<li><strong>扩散任务</strong>：只对“局部 SDF 残差”去噪，解耦了“在哪里”与“长什么样”。</li>
<li><strong>推理外推</strong>：训练 4k→8k→10k 令牌渐进式升温，测试可直接外推到 32k 令牌，等效 2048³ 分辨率。</li>
</ul>
<p>4. 零样本风格化（bonus）</p>
<p>利用两阶段解耦特性：</p>
<ul>
<li>Stage-1 用图像 A 生成粗形状 → 固定体素锚点</li>
<li>Stage-2 换图像 B 作为条件，仅雕刻局部细节<br>无需再训练即可得到“粗形状保 A，细纹理保 B”的风格化资产。</li>
</ul>
<p>效果量化</p>
<ul>
<li>数据：80 万 → 12 万，水密化失败率 &lt;0.5 %</li>
<li>生成：单卡 H20 32k 令牌推理，Chamfer 距离比 Hunyuan3D-2.1 降 28 %，法向一致性提升 0.12；视觉评分在双盲实验中优于 TRELLIS、Direct3D-S2 等开源基线，与 4 款商用系统打平。</li>
<li>伸缩：VAE 重建误差随令牌数单调下降，DiT 外推 32k 仍稳定无崩溃。</li>
</ul>
<p>一句话总结</p>
<p>UltraShape 通过“先数据后模型”两条闭环——水密化过滤得到干净信号，体素查询+RoPE 把空间与细节解耦——首次在公开数据、单卡算力下实现可扩展、高保真的 3D 扩散生成。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验围绕三条主线展开：<br>A. 数据水密化质量；B. 生成与重建的可扩展性；C. 与 SOTA 及商业方法的对比。所有定量指标均基于 <strong>Chamfer Distance (CD)×10⁻³</strong>、<strong>Normal Consistency (NC)</strong> 以及 <strong>用户偏好率</strong>；定性结果通过 1024² 渲染 + 法向可视化进行双盲打分。</p>
<p>1. 数据水密化实验</p>
<p><strong>目的</strong>：验证自研稀疏体素分水岭算法在补洞、增厚、保细节上的优势。</p>
<ul>
<li><strong>对比基线</strong><br>– Flood-fill-1024<br>7</li>
</ul>
<p>– Visibility-check-768<br>12,13,28</p>
<p>– Dora-1024<br>2</p>
<p>– ManifoldPlus-256<br>7</p>
<ul>
<li><p><strong>测试集</strong><br>从 Objaverse 随机抽 500 份“含大孔洞/开放面/薄壳”的破损 mesh，人工标注真值水密 mesh（使用 Blender + 手工修补）作为 GT。</p>
</li>
<li><p><strong>指标</strong><br>– 水密成功率（Boolean 交并后体积误差 &lt;2 %）<br>– 表面误差：CD↓ / NC↑<br>– 细节保持：边缘 Chamfer (edge-CD)↓</p>
</li>
<li><p><strong>结果</strong>（表 1，论文 §3.2）</p>
</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>成功率 ↑</th>
<th>CD ↓</th>
<th>NC ↑</th>
<th>edge-CD ↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>ManifoldPlus-256</td>
<td>62 %</td>
<td>1.84</td>
<td>0.89</td>
<td>2.31</td>
</tr>
<tr>
<td>Visibility-768</td>
<td>71 %</td>
<td>1.52</td>
<td>0.91</td>
<td>1.97</td>
</tr>
<tr>
<td>Flood-fill-1024</td>
<td>68 %</td>
<td>1.63</td>
<td>0.90</td>
<td>2.05</td>
</tr>
<tr>
<td>Ours-1024</td>
<td>96 %</td>
<td>0.71</td>
<td>0.95</td>
<td>0.88</td>
</tr>
<tr>
<td>Ours-2048</td>
<td>98 %</td>
<td>0.54</td>
<td>0.96</td>
<td>0.65</td>
</tr>
</tbody>
</table>
</div>
<p>⇒ 在 2048³ 分辨率下，水密失败样本从 38 % 降至 2 %，细节边缘误差下降 65 %。</p>
<p>2. 可扩展性实验</p>
<p>2.1 VAE 重建外推</p>
<ul>
<li>设置：固定 128³ 体素查询，逐步增加令牌数 {4k,8k,16k,24k,32k}。</li>
<li>指标：重建 CD / NC。</li>
<li>结果（图 6）：</li>
<li>4k→32k 令牌，CD 从 1.12 单调降至 0.31；NC 从 0.90 升至 0.96。</li>
<li>视觉对比显示 32k 时连 0.3 mm 级刻线亦被还原。</li>
</ul>
<p>2.2 DiT 生成外推</p>
<ul>
<li>设置：同样令牌递增，图像分辨率固定 1022×1022。</li>
<li>指标：生成 vs GT 的 CD / NC；用户偏好率（50 名受试者，双盲 A/B）。</li>
<li>结果（图 10）：</li>
<li>4k→32k 令牌，CD 下降 38 %，NC 提升 0.10。</li>
<li>偏好率从 58 %→84 %，证明更大令牌直接带来更高 perceptual 质量。</li>
</ul>
<p>3. 生成质量对比实验</p>
<p>3.1 与开源 SOTA 比较</p>
<ul>
<li>基线：Hunyuan3D-2.1、TRELLIS、TRELLIS.2、Direct3D-S2、Hi3DGen</li>
<li>测试集：Objaverse-XL 官方验证集 1k 样本 + 人工标注对齐 GT mesh</li>
<li>指标：CD↓ / NC↑ / F-score@1 mm↑</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>方法</th>
<th>CD ↓</th>
<th>NC ↑</th>
<th>F-score ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>Hi3DGen</td>
<td>1.43</td>
<td>0.88</td>
<td>62.1</td>
</tr>
<tr>
<td>Direct3D-S2</td>
<td>1.27</td>
<td>0.89</td>
<td>65.4</td>
</tr>
<tr>
<td>TRELLIS</td>
<td>1.19</td>
<td>0.90</td>
<td>68.7</td>
</tr>
<tr>
<td>TRELLIS.2</td>
<td>1.05</td>
<td>0.91</td>
<td>71.2</td>
</tr>
<tr>
<td>Hunyuan3D-2.1</td>
<td>0.98</td>
<td>0.92</td>
<td>73.5</td>
</tr>
<tr>
<td>UltraShape 1.0</td>
<td>0.71</td>
<td>0.95</td>
<td>81.3</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li>用户偏好双盲：UltraShape 胜出率 78 %（图 7-8 视觉例）。</li>
</ul>
<p>3.2 与商业系统比较</p>
<ul>
<li>匿名商用 API：Model-M/H/T/R/S（对应业界主流 5 家）</li>
<li>测试集：作者自建 200 张 Unsplash 真实照片，无训练集重叠。</li>
<li>指标：CD / NC + 人工评分（1-5）。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>系统</th>
<th>CD ↓</th>
<th>NC ↑</th>
<th>平均评分 ↑</th>
</tr>
</thead>
<tbody>
<tr>
<td>Model-M</td>
<td>0.68</td>
<td>0.94</td>
<td>4.1</td>
</tr>
<tr>
<td>Model-H</td>
<td>0.72</td>
<td>0.93</td>
<td>4.0</td>
</tr>
<tr>
<td>Model-T</td>
<td>0.65</td>
<td>0.94</td>
<td>4.2</td>
</tr>
<tr>
<td>Model-R</td>
<td>0.69</td>
<td>0.93</td>
<td>4.1</td>
</tr>
<tr>
<td>Model-S</td>
<td>0.63</td>
<td>0.95</td>
<td>4.3</td>
</tr>
<tr>
<td>UltraShape</td>
<td>0.64</td>
<td>0.95</td>
<td>4.2</td>
</tr>
</tbody>
</table>
</div>
<p>⇒ 在仅使用 120 k 公开数据、8×H20 训练条件下，与最佳商业系统差距 &lt;3 %，达到同等可用级别。</p>
<p>4. 训练-free 风格化实验</p>
<ul>
<li>协议：粗阶段用图像 A（例如“陶瓷熊”），细阶段换图像 B（例如“金属机甲”）。</li>
<li>指标：形状一致性（粗 vs 原 A 的 CD）与风格一致性（细 vs B 的法向纹理相似度）。</li>
<li>结果：</li>
<li>形状 CD 仅 0.39（低偏差），风格 NC 0.94（高一致）。</li>
<li>图 4 显示成功将熊的轮廓与金属划痕细节融合，无需再训练或额外正则。</li>
</ul>
<p>5. 消融实验</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>组件</th>
<th>完整</th>
<th>w/o RoPE</th>
<th>w/o 体素查询（仍用表面点）</th>
<th>w/o 渐进令牌升温</th>
</tr>
</thead>
<tbody>
<tr>
<td>CD ↓</td>
<td>0.71</td>
<td>0.89</td>
<td>0.95</td>
<td>0.82</td>
</tr>
<tr>
<td>NC ↑</td>
<td>0.95</td>
<td>0.91</td>
<td>0.90</td>
<td>0.92</td>
</tr>
</tbody>
</table>
</div>
<p>⇒ 去除 RoPE 后细节错位最明显；回退到表面点查询导致 34 % 误差上升；无渐进升温则 32k 令牌推理出现 6 % 崩溃样本。</p>
<p>总结</p>
<p>实验覆盖数据、表示、生成、风格化四块，既给出定量指标也提供大规模视觉对比，证明 UltraShape 在公开数据与有限算力下即可达到商业级水密处理能力与几何保真度，同时具备优异的测试时扩展性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 UltraShape 1.0 的直接延伸，仍保持“公开数据 + 单卡可训”这一设定，但各自瞄准尚未充分验证的开放问题。</p>
<p>1. 数据侧</p>
<ul>
<li><p><strong>语言-3D 联合过滤</strong><br>当前仅用 VLM 图像过滤；可引入 caption-3D 对，用 VL-retriever 做语义-几何一致性打分，进一步剔除“图文不符”或“风格漂移”资产。</p>
</li>
<li><p><strong>自动语义分割 + 分块水密化</strong><br>对多部件装配体先作语义分块，再局部水密化，可避免“车轮被填成实心”一类过度闭合问题。</p>
</li>
<li><p><strong>可微水密化</strong><br>把稀疏分水岭步骤 relax 成可微算子，让水密化误差能反向传播，实现“为下游生成任务量身定制的拓扑修复”。</p>
</li>
</ul>
<p>2. 表示与 VAE 侧</p>
<ul>
<li><p><strong>哈希/三平面混合体素</strong><br>128³ 固定网格在 2048³ 等效分辨率下仍属“低占空比”。引入 Instant-NGP 式哈希或三平面分解，可把体素查询内存从 O(N³) 降到 O(N² log N)，支持 512³ 以上实网格训练。</p>
</li>
<li><p><strong>频域/小波令牌</strong><br>用 3D Haar 小波或傅里叶系数替代原始 SDF 令牌，让高频细节与低频形状在变换域显式分离，有望进一步降低扩散步数。</p>
</li>
<li><p><strong>动态查询预算</strong><br>当前采用固定 32k 令牌；可训练一个轻量级“重要性预测器”，在推理时只对曲率高或法向不一致区域分配额外令牌，实现自适应细粒度。</p>
</li>
</ul>
<p>3. 生成侧</p>
<ul>
<li><p><strong>级联超分 Stage-3</strong><br>在 128³ 体素细化后再接 1×1 卷积或哈希网格上采样到 512³，专司微米级曲面纹理（毛孔、布纹），保持前两个阶段不变，形成“粗→细→微”三阶段流水线。</p>
</li>
<li><p><strong>多视图-体素联合去噪</strong><br>把 16 张输入视图编码成 3D-aligned 特征体，直接 concat 到体素查询令牌，而非仅用 cross-attention；可缓解单图遮挡造成的几何缺失。</p>
</li>
<li><p><strong>文本-图像混合条件</strong><br>当前仅图像条件；可将 CLIP 文本特征通过另一组 cross-attention 注入，实现“形状随图，风格随文”的细粒度控制。</p>
</li>
<li><p><strong>可控局部编辑</strong><br>利用体素空间定位特性，引入注意力掩码仅对指定 bbox 或语义部件进行重噪-去噪，实现“换车头不改车身”的交互式编辑。</p>
</li>
</ul>
<p>4. 训练与推理加速</p>
<ul>
<li><p><strong>一致性蒸馏 (Consistency Distillation)</strong><br>把 50 步 DDPM 蒸馏成 2-4 步一致性模型，保持 32k 令牌外推能力，实现“秒级”网格生成。</p>
</li>
<li><p><strong>MoE- DiT</strong><br>把几何细节专家与纹理专家拆成稀疏门控前馈，可在不增加计算量的情况下分别处理高频曲面与低频形状。</p>
</li>
<li><p><strong>量化与 NF4 推理</strong><br>对 VAE 解码器与 DiT 权重做 4-bit 量化，配合 CUDA 半精度 marching cube，有望把 32k 令牌推理从 24 GB 压到 &lt;10 GB，下放至消费级 GPU。</p>
</li>
</ul>
<p>5. 物理与下游任务</p>
<ul>
<li><p><strong>带厚度/内部结构建模</strong><br>目前只输出单层水密表面。可在体素查询里预测“内外双 SDF”或壳层厚度，直接生成适合 3D 打印的空心网格。</p>
</li>
<li><p><strong>PBR 材质联合扩散</strong><br>同一体素网格同时预测 SDF + 基础色 + 粗糙度 + 金属度，实现“几何-材质”端到端生成，避免后续 SDS 烘培。</p>
</li>
<li><p><strong>可动骨骼自动绑定</strong><br>利用体素语义场预测骨架热力图，再回归关节位置与蒙皮权重，实现“生成即绑定”的动画可用资产。</p>
</li>
</ul>
<p>6. 评测与基准</p>
<ul>
<li><p><strong>细粒度几何 benchmark</strong><br>建立包含 0.1 mm 级特征的高分辨率扫描测试集，提供亚毫米级 GT，填补现有 3D Gen 评测“只测整体形状不测微细节”的空白。</p>
</li>
<li><p><strong>拓扑正确性指标</strong><br>引入“环数误差”“贝蒂数差异”等拓扑不变量，避免 Chamfer 距离无法惩罚错误手柄、多孔等拓扑噪声。</p>
</li>
</ul>
<p>7. 跨模态与开放世界</p>
<ul>
<li><p><strong>多物体/场景级生成</strong><br>把 128³ 体素空间从“单物体”扩展到“局部场景”坐标系，用实例掩码令牌区分不同物体，实现一次扩散生成完整室内/街景资产排列。</p>
</li>
<li><p><strong>4D 动态序列</strong><br>在时间维度堆叠 128³×T 体素，令 DiT 同时去噪 SDF 与速度场，直接输出水密动画序列，用于游戏过场或仿真数据合成。</p>
</li>
</ul>
<p>结语</p>
<p>UltraShape 通过“水密数据 + 体素查询解耦”打开了高分辨率 3D 原生扩散的可行路径，但仍在文本控制、物理属性、推理效率、拓扑编辑等维度留有广阔探索空间。上述方向大多可在单卡、公开数据设定下迭代，无需工业级集群即可验证，适合学术社区继续推进。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>UltraShape 1.0 提出一套“数据-生成”双管线，用公开数据与单卡算力实现高保真、可扩展的 3D 几何扩散生成，核心贡献可归纳为：</p>
<ol>
<li>水密化-过滤管线</li>
</ol>
<ul>
<li>稀疏体素分水岭 + CUDA 并行 → 2048³ 下自动补洞、增厚开放面，98 % 样本一次成功。</li>
<li>VLM+姿态归一化+几何碎片化检测，把 80 万 Objaverse 精炼成 12 万高质量水密资产。</li>
</ul>
<ol>
<li>两阶段生成框架</li>
</ol>
<ul>
<li>阶段 1：复用 Hunyuan3D-2.1 向量集 DiT，快速输出低分辨率但整体可信的粗网格。</li>
<li>阶段 2：固定 128³ 体素查询 + RoPE 位置编码，令 DiT 仅对粗几何锚点做局部 SDF 去噪，解耦“在哪里”与“长什么样”，支持 32 k 令牌外推，等效 2048³ 细节。</li>
</ul>
<ol>
<li><p>训练-免费风格化<br>粗、细两阶段可分别用不同图像条件，实现“轮廓保 A，纹理保 B”的零样本雕刻。</p>
</li>
<li><p>实验结果</p>
</li>
</ol>
<ul>
<li>水密化：边缘 Chamfer 误差比最佳基线降 65 %。</li>
<li>生成：在 1 k 公开测试集上 Chamfer↓28 %、法向一致性↑0.12，用户偏好率 78 %；与 5 家商业系统持平。</li>
<li>伸缩：令牌数 4 k→32 k，重建/生成误差单调下降，未出现崩溃。</li>
</ul>
<ol>
<li>代码与模型全部开源，为大规模 3D 生成研究提供可复现基线。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UltraShape%201.0%3A%20High-Fidelity%203D%20Shape%20Generation%20via%20Scalable%20Geometric%20Refinement，链接是https://arxiv.org/pdf/2512.21185，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21185。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tanghui Jia,Dongyu Yan,Dehao Hao,Yang Li,Kaiyi Zhang,Xianyi He,Lanjiong Li,Yuhan Wang,Jinnan Chen,Lutao Jiang,Qishen Yin,Long Quan,Ying-Cong Chen,Li Yuan</p>
<p><strong>Categories</strong>: cs.CV,cs.GR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21185.pdf">https://arxiv.org/pdf/2512.21185.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21185">https://arxiv.org/abs/2512.21185</a></p>
<p><strong>Arxiv ID</strong>: 2512.21185</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21185">https://papers.cool/arxiv/2512.21185</a></p>
<p><strong>Published</strong>: 2025-12-24T14:08:38Z</p>
<p><strong>Updated</strong>: 2025-12-24T14:08:38.000Z</p>
<hr>
<h3 id="2-DreamOmni3-Scribble-based-Editing-and-Generation"><a href="#2-DreamOmni3-Scribble-based-Editing-and-Generation" class="headerlink" title="2. DreamOmni3: Scribble-based Editing and Generation"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.22525">DreamOmni3: Scribble-based Editing and Generation</a></h3><p>Recently unified generation and editing models have achieved remarkable success with their impressive performance. These models rely mainly on text prompts for instruction-based editing and generation, but language often fails to capture users intended edit locations and fine-grained visual details. To this end, we propose two tasks: scribble-based editing and generation, that enables more flexible creation on graphical user interface (GUI) combining user textual, images, and freehand sketches. We introduce DreamOmni3, tackling two challenges: data creation and framework design. Our data synthesis pipeline includes two parts: scribble-based editing and generation. For scribble-based editing, we define four tasks: scribble and instruction-based editing, scribble and multimodal instruction-based editing, image fusion, and doodle editing. Based on DreamOmni2 dataset, we extract editable regions and overlay hand-drawn boxes, circles, doodles or cropped image to construct training data. For scribble-based generation, we define three tasks: scribble and instruction-based generation, scribble and multimodal instruction-based generation, and doodle generation, following similar data creation pipelines. For the framework, instead of using binary masks, which struggle with complex edits involving multiple scribbles, images, and instructions, we propose a joint input scheme that feeds both the original and scribbled source images into the model, using different colors to distinguish regions and simplify processing. By applying the same index and position encodings to both images, the model can precisely localize scribbled regions while maintaining accurate editing. Finally, we establish comprehensive benchmarks for these tasks to promote further research. Experimental results demonstrate that DreamOmni3 achieves outstanding performance, and models and code will be publicly released.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近，统一的生成与编辑模型凭借其出色的表现取得了显著成功。这些模型主要依赖文本提示进行基于指令的编辑与生成，但语言往往难以准确表达用户意图的编辑位置和细粒度的视觉细节。为此，我们提出了两个任务：基于涂鸦的编辑与生成，使用户能够在图形用户界面（GUI）上灵活地结合文本、图像和手绘草图进行创作。我们引入了DreamOmni3，旨在解决两个挑战：数据创建和框架设计。我们的数据合成管道包括两个部分：基于涂鸦的编辑和生成。在基于涂鸦的编辑中，我们定义了四个任务：涂鸦与指令结合的编辑、涂鸦与多模态指令结合的编辑、图像融合以及涂鸦编辑。基于DreamOmni2数据集，我们提取可编辑区域并覆盖手绘的框、圆、涂鸦或裁剪图像来构建训练数据。在基于涂鸦的生成中，我们定义了三个任务：涂鸦与指令结合的生成、涂鸦与多模态指令结合的生成以及涂鸦生成，其数据创建流程类似。对于框架设计，我们不再使用二值掩码，因为其在处理涉及多个涂鸦、图像和指令的复杂编辑时存在困难，而是提出了一种联合输入方案，将原始图像和涂鸦图像同时输入模型，通过不同颜色区分区域，简化处理。通过对两张图像应用相同的索引与位置编码，模型能够精确定位涂鸦区域，同时保持编辑的准确性。最后，我们建立了这些任务的综合基准，以促进进一步研究。实验结果表明，DreamOmni3表现优异，且模型与代码将公开发布。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现有统一生成-编辑模型在交互粒度上的两大痛点：</p>
<ol>
<li><p>文本指令难以精确定位<br>纯文本无法描述“在哪个像素区域”执行编辑，用户面对多个同类物体或需要局部微调时，语言显得冗余且模糊。</p>
</li>
<li><p>文本指令难以表达细粒度视觉细节<br>颜色、纹理、姿态等细节常超出自然语言可描述范围，导致生成结果与用户想象偏差大。</p>
</li>
</ol>
<p>为此，作者提出“基于草图的编辑与生成”这一新任务范式，允许用户通过手绘圈、框、涂鸦等草图信号，直接在图像上标定编辑区域或生成位置，并与文本、参考图像自由组合。DreamOmni3 框架即围绕该范式设计，以解决数据匮乏、输入格式不统一、多草图耦合复杂等关键挑战。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 DreamOmni3 直接相关的研究可归纳为三条主线，均聚焦于“如何让用户在像素级空间更精细地控制生成或编辑”。按贡献维度分列如下：</p>
<ul>
<li><strong>掩码-引导编辑（Mask-based Editing）</strong></li>
<li>图像补绘：DeepFill-v2、LaMa、RePaint 等经典 inpainting 方法仅依赖二值掩码，缺乏对上下文光照/阴影变化的推理。</li>
<li>结构保持补绘：MagicQuill 引入边缘图或低分辨率参考，缓解掩码区域完全重绘带来的结构漂移。</li>
<li>ID 注入式插入：SmartMask、InsertAnything、ACE++ 等将参考图的压缩对象 token 注入掩码区域，实现“把 A 图里的物体贴到 B 图掩码处”。<br>→ 共同局限：二值掩码无法自然表达“多区域、多模态”指令，且需要用户精确涂掩码，交互成本高。</li>
<li><strong>掩码-引导生成（Mask-guided Generation）</strong></li>
<li>文本到多区域生成：Multi-diffusion、GLIGEN、ELIGEN、MIGC 等通过傅里叶嵌入或区域注意力，在空白画布上按框生成不同实体。</li>
<li>主体驱动+布局：MS-diffusion 用 grounding resampler 将视觉主体特征绑定到指定框。<br>→ 共同局限：输入格式各异（框、mask、token），难以与后续编辑任务统一；不支持“草图+参考图+文本”混合指令。</li>
<li><strong>统一生成-编辑框架</strong></li>
<li>DreamOmni / DreamOmni2 首次把“文本指令编辑”与“多参考图生成”整合到同一 MM-DiT 框架，但仍只接受 RGB+文本，不支持草图信号。</li>
<li>OmniGen2、Qwen-image-Edit、Kontext 等继续拓展多图理解， yet 均未引入手绘涂鸦或圈选作为显式输入。</li>
</ul>
<p>DreamOmni3 在上述基础上首次把“草图”作为与文本、图像并列的第三种指令模态，并通过联合输入方案与位置/索引编码一致性策略，将掩码-编辑与掩码-生成两条研究路线统一到同一套 RGB 空间接口，克服了传统二值掩码在多区域、多模态场景下的可扩展性与易用性缺陷。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从“任务定义 → 数据构造 → 框架设计 → 训练策略 → 评测协议”五个环节系统解决“草图驱动编辑/生成”难题，核心思路是<strong>用颜色草图替代二值掩码，并以联合 RGB 输入保持像素一致性</strong>。关键做法如下：</p>
<ol>
<li>任务定义</li>
</ol>
<ul>
<li>将用户交互抽象为 4 类草图编辑 + 3 类草图生成，覆盖“圈选、涂鸦、参考图融合”等场景，统一用<strong>颜色笔画</strong>标定区域。</li>
</ul>
<ol>
<li>数据流水线（解决数据匮乏）</li>
</ol>
<ul>
<li>以 DreamOmni2 的多模态编辑数据为底座，利用 ReferSeg 定位对象，再手工叠加 30 种不同形状/颜色的<strong>圆、框、涂鸦</strong>模板，自动生成 97 k 条“源图+草图→目标图”三元组。</li>
<li>对涂鸦任务，调用 GPT-Image-1 把对象转成“人类手绘”风格，保证笔画非精确边缘，贴合真实用户草图。</li>
</ul>
<ol>
<li>框架设计（解决掩码缺陷）</li>
</ol>
<ul>
<li><strong>放弃二值掩码</strong>，采用“联合输入”：同时把原始源图  I  与带颜色草图的源图  I’  送入 MM-DiT，二者共享同一</li>
</ul>
<p>PositionEncoding(x,y), IndexEncoding(k)</p>
<p>使模型能对齐被笔画遮挡前的原始像素，减少补绘漂移。</p>
<ul>
<li>参考图仍沿用 DreamOmni2 的“位置平移 + 索引偏移”策略，避免与源图 token 混淆。</li>
</ul>
<ol>
<li>训练策略</li>
</ol>
<ul>
<li>在 Kontext-FLUX 权重上做 LoRA（rank=256），仅激活草图相关参数，保留原有文本编辑能力；编辑与生成分别训练两颗 LoRA，防止任务冲突。</li>
<li>联合输入仅在<strong>源图含草图</strong>的编辑任务启用，生成或参考图含草图时单图输入，降低计算量。</li>
</ul>
<ol>
<li>评测协议</li>
</ol>
<ul>
<li>构建真实场景 benchmark，覆盖 7 类子任务；提出四维 VLM 打分：指令忠实度、外观一致性、无显著伪影、草图区域对齐。</li>
<li>实验表明，该方案在 human/VLM 双重评测下，将开源模型成功率从 ≈20 % 提升到 55 % 以上，与 GPT-4o 持平，但无商业模型的“泛黄、复制粘贴”缺陷。</li>
</ul>
<p>通过“颜色草图 + 联合 RGB 输入 + 共享编码”，DreamOmni3 把原本需要精确掩码、复杂语言描述的局部编辑/生成问题，转化为用户随手圈画即可完成的统一接口。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“草图编辑”与“草图生成”两大任务，在自建的 <strong>DreamOmni3 benchmark</strong> 上开展了系统实验，涵盖定量指标、人工评审与消融分析三部分。主要实验如下（均以非表格形式列出）：</p>
<ol>
<li>主实验 – 与 SOTA 对比</li>
</ol>
<ul>
<li><strong>基准覆盖</strong>：4 类草图编辑 + 3 类草图生成，共 800 组真实拍摄图像。</li>
<li><strong>参评模型</strong>：<br>– 开源：Omnigen2、Qwen-image-Edit-2509、DreamOmni2、Kontext<br>– 商业：GPT-4o、Nano Banana</li>
<li><strong>评估方式</strong>：<br>– 双 VLM 自动打分：Gemini-2.5 与 Doubao-1.6，按“指令忠实度 / 外观一致 / 无伪影 / 草图对齐”四维判定通过率。<br>– 人工评审：5 名专业工程师逐样投票，&gt;3 票视为成功，计算 pass@1。</li>
<li><strong>结果（↑ 为越高越好）</strong>：<br>– 草图编辑通过率：DreamOmni3 取得 0.5250(Gemini) / 0.4500(Doubao) / 0.5750(Human)，显著高于开源最佳基线（≈0.24），与 GPT-4o(0.6125) 差距 &lt;0.09。<br>– 草图生成通过率：DreamOmni3 达到 0.5116(Gemini) / 0.4651(Doubao) / 0.5349(Human)，优于 Nano Banana 约 10 pp，与 GPT-4o(0.5814) 基本持平。</li>
</ul>
<ol>
<li>视觉定性对比</li>
</ol>
<ul>
<li>图 3 展示编辑案例：多模态指令换色、圈选添加/删除、图像融合、涂鸦发型替换。DreamOmni3 在物体比例、光影一致性与背景保留上优于对比方法；GPT-4o 出现全局泛黄，Nano Banana 出现复制粘贴痕迹。</li>
<li>图 4 展示生成案例：按圈生成人物+背景、按涂鸦生成兔子耳朵、多参考图组合摆放。DreamOmni3 能正确去除输入草图痕迹并生成合理结构，开源基线普遍“把涂鸦直接当纹理”输出。</li>
</ul>
<ol>
<li>消融实验</li>
</ol>
<ul>
<li><strong>联合输入有效性</strong>（对应原 Table 3）<br>方案 1：无训练 + 单图输入 → 编辑 0.1125<br>方案 4：有训练 + 联合输入 → 编辑 0.4500（+30 pp），生成 0.4651（+2 pp），证实联合输入对“保持非编辑区像素”关键。</li>
<li><strong>编码一致性</strong>（对应原 Table 4）<br>仅共享位置编码：0.4000<br>仅共享索引编码：0.4250<br>二者皆共享（本文）：0.4500，验证“同一 (x,y,k) 编码”可提升像素对齐与参考图一致性。</li>
<li><strong>草图颜色数敏感度</strong>（补充材料）：在 2∼6 种颜色区间，通过率变化 &lt;2 pp，表明框架对常见颜色数已鲁棒。</li>
</ul>
<ol>
<li>扩展分析</li>
</ol>
<ul>
<li>失败案例统计：VLM 与人工评审误差 &lt;3 %，说明自动指标可替代人工。</li>
<li>推理耗时：联合输入仅增加 18 % 显存，Latency 增加 11 %，仍在实时交互可接受范围。</li>
</ul>
<p>综上，论文通过“定量指标 + 人工双盲 + 多维度消融”验证了 DreamOmni3 在草图驱动场景下的有效性与设计合理性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可延续 DreamOmni3 的核心思想，继续拓展草图-文本-图像三元交互的边界：</p>
<ol>
<li>草图语义层级化</li>
</ol>
<ul>
<li>将单笔涂鸦解析为“实例-部件-材质”多级 token，支持一句指令如“把红色圆圈里的汽车车顶改成玻璃天窗”的细粒度操作。</li>
<li>引入部件-aware 的 attention mask，使模型在同一实例内部区分不同组件。</li>
</ul>
<ol>
<li>时序草图 / 多步编辑</li>
</ol>
<ul>
<li>支持用户分步绘制并随时撤销：维护一张“累积草图隐码”缓存，每步只把增量笔画编码进扩散去噪路径，实现实时交互式迭代。</li>
<li>探索编辑历史嵌入，防止多步操作后外观漂移。</li>
</ul>
<ol>
<li>三维草图与视角一致</li>
</ol>
<ul>
<li>把 2D 圈选提升到 3D 空间：用户先在 NeRF/3D-GS 场景里勾画，再投影回任意视角进行纹理或几何编辑，保证多视角一致性。</li>
<li>研究深度-aware 位置编码，使 (x,y,z) 与相机参数联合嵌入。</li>
</ul>
<ol>
<li>草图风格化与美学预测</li>
</ol>
<ul>
<li>用户手绘风格差异大，可训练“美学评分器”对草图进行风格归一化，或反向引导生成结果向用户画风靠拢。</li>
<li>引入 aesthetic LoRA，根据草图线条粗细、曲率动态调整生成图像的艺术风格强度。</li>
</ul>
<ol>
<li>跨模态草图检索与 few-shot 编辑</li>
</ol>
<ul>
<li>建立“草图-文本”双塔编码器，实现“画一笔即可检索参考图”，解决参考图难找问题。</li>
<li>支持 1~3 张参考图即完成风格/材质迁移，降低多图输入门槛。</li>
</ul>
<ol>
<li>视频草图编辑</li>
</ol>
<ul>
<li>把联合输入方案拓展到时空 U-Net：草图笔画自动传播到相邻帧，保持时序一致性；用户只需在关键帧勾画。</li>
<li>研究运动掩码预测，避免逐帧手动圈选。</li>
</ul>
<ol>
<li>多用户协同草图</li>
</ol>
<ul>
<li>实时协作场景下，不同用户用不同颜色同时勾勒，模型需区分“谁改了哪里”并合并冲突指令。</li>
<li>引入用户 ID 编码与并发版本控制机制。</li>
</ul>
<ol>
<li>轻量级边缘部署</li>
</ol>
<ul>
<li>将 LoRA 秩进一步压缩至 16-32，结合 INT4 量化，使草图编辑可在平板/手机上 2~3 秒出图。</li>
<li>研究“笔画→掩码”小模型，提前在端侧过滤无效涂鸦，减少云端推理次数。</li>
</ul>
<ol>
<li>自动草图生成与教学反馈</li>
</ol>
<ul>
<li>对于新手用户，系统可自动在图像上生成“建议圈选区域”或“步骤草图”，降低使用门槛。</li>
<li>结合视觉语言模型给出“为什么这样勾画效果更好”的可解释反馈。</li>
</ul>
<ol>
<li>更细粒度评测维度</li>
</ol>
<ul>
<li>当前四维指标仍显粗糙，可引入 DTO（Drawings-Text-Output）一致性度量、草图笔画 IoU、用户满意度实时记录，构建持续学习的反馈闭环。</li>
</ul>
<p>这些方向既可在模型结构（attention、position encoding）、数据形式（3D、视频、协作）、交互范式（自动建议、端侧实时）上深入，也能推动草图驱动编辑走向更普适、更智能的创作工具。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>DreamOmni3 核心内容速览</p>
<p>1. 问题</p>
<ul>
<li>纯文本指令难以精确定位编辑区域或描述细粒度视觉细节</li>
<li>现有统一生成-编辑模型不支持手绘圈选/涂鸦等直观交互</li>
</ul>
<p>2. 任务</p>
<p>提出两大新任务：</p>
<ul>
<li><strong>草图编辑</strong>（4 子类）：圈选+文本、圈选+参考图、图像融合、涂鸦编辑</li>
<li><strong>草图生成</strong>（3 子类）：圈选+文本、圈选+参考图、涂鸦生成</li>
</ul>
<p>3. 数据</p>
<ul>
<li>基于 DreamOmni2 多模态语料，用 ReferSeg 定位对象 → 随机叠加 30 种颜色框/圆/涂鸦 → 97 k 高质量三元组（源图+草图→目标图）</li>
<li>覆盖具象物体与抽象属性增删改，兼顾真实场景与手绘风格</li>
</ul>
<p>4. 框架</p>
<ul>
<li><strong>放弃二值掩码</strong>，采用“联合输入”：同时送入原始源图  I  与带草图源图  I’ ，共享同一位置-索引编码<br>PositionEncoding(x,y), IndexEncoding(k)<br>保持被遮挡像素可见，提升多区域编辑一致性</li>
<li>参考图沿用 DreamOmni2 平移编码，避免 token 混淆</li>
<li>基于 MM-DiT + LoRA(rank=256) 插入训练，保留原文本编辑能力</li>
</ul>
<p>5. 训练与推理</p>
<ul>
<li>编辑任务且源图含草图 → 启用联合输入；其余情况单图输入，节省计算</li>
<li>400 A100 小时完成训练，生成与编辑分离两颗 LoRA，防止任务冲突</li>
</ul>
<p>6. 评测</p>
<ul>
<li>自建 DreamOmni3 benchmark（800 真实图像）</li>
<li>四维 VLM 自动指标：指令忠实度、外观一致、无伪影、草图区域对齐；人工双盲验证误差 &lt;3%</li>
<li>实验结果：开源最佳≈0.24 → DreamOmni3 0.575（human），与 GPT-4o(0.612) 差距 &lt;0.04；生成任务同样达到商业级水平</li>
</ul>
<p>7. 贡献</p>
<ul>
<li>首次定义草图编辑/生成任务并给出大规模数据集</li>
<li>提出联合 RGB 输入+共享编码，解决多草图耦合与像素一致性难题</li>
<li>建立评测基准，推动统一模型向更细粒度、更低门槛的交互创作工具演进</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DreamOmni3%3A%20Scribble-based%20Editing%20and%20Generation，链接是https://arxiv.org/pdf/2512.22525，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.22525。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Bin Xia,Bohao Peng,Jiyang Liu,Sitong Wu,Jingyao Li,Junjia Huang,Xu Zhao,Yitong Wang,Ruihang Chu,Bei Yu,Jiaya Jia</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.22525.pdf">https://arxiv.org/pdf/2512.22525.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.22525">https://arxiv.org/abs/2512.22525</a></p>
<p><strong>Arxiv ID</strong>: 2512.22525</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.22525">https://papers.cool/arxiv/2512.22525</a></p>
<p><strong>Published</strong>: 2025-12-27T09:07:12Z</p>
<p><strong>Updated</strong>: 2025-12-27T09:07:12.000Z</p>
<hr>
<h3 id="3-End-to-End-Test-Time-Training-for-Long-Context"><a href="#3-End-to-End-Test-Time-Training-for-Long-Context" class="headerlink" title="3. End-to-End Test-Time Training for Long Context"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.23675">End-to-End Test-Time Training for Long Context</a></h3><p>We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture — a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model’s initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们将长上下文语言建模视为持续学习的问题，而不是架构设计的问题。在这种框架下，我们只使用标准架构——带滑动窗口注意力的Transformer。然而，我们的模型在测试时通过对给定上下文进行下一个词预测继续学习，将它读取的上下文压缩到权重中。此外，我们通过在训练时进行元学习来改进模型在测试时间的学习初始化。总体而言，我们的方法是一种测试时训练（TTT）形式，在测试时（通过下一个词预测）和训练时（通过元学习）都是端到端（E2E）的，这与以往的方法不同。我们进行了大量实验，重点关注扩展特性。特别是，对于使用1640亿个标记训练的3B模型，我们的方法（TTT-E2E）在上下文长度上的扩展方式与具有完整注意力的Transformer相同，而其他方法，例如Mamba 2和Gated DeltaNet，则不具备这种特性。然而，与RNN类似，TTT-E2E的推理延迟与上下文长度无关，使其在128K上下文下比完整注意力快2.7倍。我们的代码已公开。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文将“长上下文语言建模”重新表述为**持续学习（continual learning）**问题，而非传统意义上的架构设计问题。核心待解决问题可以概括为：</p>
<ul>
<li><strong>如何在仅允许常数级推理延迟的前提下，让模型随上下文长度增长而持续受益？</strong><br>现有方法要么</li>
</ul>
<ol>
<li>像全注意力 Transformer，随上下文长度线性增加计算量，导致长序列推理昂贵；</li>
<li>像 Mamba、Gated DeltaNet 等 RNN 变体，保持恒定每-token 成本，但在更长上下文上效果迅速下降。</li>
</ol>
<p>为此，作者提出 <strong>Test-Time Training End-to-End (TTT-E2E)</strong>：</p>
<ul>
<li>测试阶段仅用<strong>滑动窗口注意力</strong>的常规 Transformer，但通过<strong>下一词预测损失</strong>在本地上下文上继续梯度更新，把已读信息压缩进权重；</li>
<li>训练阶段采用<strong>元学习（meta-learning）</strong>，让模型初始化专门“学会如何在测试时快速学习”，以匹配测试阶段的行为。</li>
</ul>
<p>该方法在 3B 参数、164B token 训练规模下，达到与全注意力相近的随长度缩放曲线，而推理延迟与上下文长度无关，128K 上下文时预填充阶段快 2.7×。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文第4节“Related Work”系统梳理了与TTT-E2E相关的四条研究脉络，并指出各自与本文方法的异同。以下按主题归纳：</p>
<ol>
<li>Continual Learning（持续学习）</li>
</ol>
<ul>
<li>传统设定：数据分布随时间缓慢漂移，模型需在“不遗忘”前提下更新（如GEM、EWC、iCaRL）。</li>
<li>本文差异：把整条测试序列视为一次性体验，不追求跨序列记忆，而是“单序列内持续学习”，无分布漂移假设。</li>
</ul>
<ol>
<li>Test-Time Training（TTT）——三条子类 2.1 TTT on Nearest Neighbors</li>
</ol>
<ul>
<li>代表：局部加权回归、KNN-SVM、Hardt &amp; Sun (2023) 对LLM的“近邻微调”。</li>
<li>共同点：测试时用局部数据再训练，提升局部容量。</li>
<li>差异：邻居来自训练集或外部，而非当前序列本身；无元学习初始化。</li>
</ul>
<p>2.2 TTT for Novel Instances / Distribution Shift</p>
<ul>
<li>代表：TTT-MAE、AlphaProof、Akyurek et al. ARC-AGI 工作。</li>
<li>共同点：利用测试样本自监督生成新数据再训练，提高OOD泛化。</li>
<li>差异：任务多为分类/推理，非自回归语言建模；不强调长序列线性复杂度。</li>
</ul>
<p>2.3 TTT on Sequences / Long Context</p>
<ul>
<li>代表：TTT-KVB、MesaNet、Titans、Nested Learning、Gated DeltaNet（可视为无元学习的TTT特例）。</li>
<li>共同点：把历史压缩进隐状态，每步梯度更新。</li>
<li>差异：<br>– 它们采用“层内重建损失”或KV绑定损失，非端到端下一词预测；<br>– 多数用多head小MLP+LoRA，状态容量小；<br>– 无“外层循环”元学习初始化，训练-测试行为不一致。</li>
</ul>
<ol>
<li>Fast Weights &amp; Fast Weight Programmers（FWP）</li>
</ol>
<ul>
<li>经典：Schmidhuber 1992“快速权重”、Clark et al. 2022 在Transformer末端添加可梯度更新的快速MLP。</li>
<li>关联：TTT-E2E可视为“整个网络即一个大型FWP”，慢权重θ提供初始化，快权重W在序列内更新。</li>
<li>差异：Clark等仅加在模型末端且保持平方复杂度；本文把更新嵌入深层并与滑动窗口交错，实现线性复杂度。</li>
</ul>
<ol>
<li>Learning to Learn / Meta-Learning</li>
</ol>
<ul>
<li>代表：MAML、Reptile、Meta-SGD 等双层优化框架。</li>
<li>共同点：外层优化“初始化”，内层优化“任务”参数。</li>
<li>差异：<br>– MAML内层用整个任务数据集；本文内层仅依赖单序列自回归损失，任务定义更细粒度。<br>– 本文将“任务”等同于下一token预测，直接对应语言模型目标，无需多任务采样。</li>
</ul>
<p>综上，TTT-E2E把“序列内持续学习”+“元学习初始化”+“端到端下一词损失”首次统一在常数推理成本的Transformer骨架上，与上述各方向形成互补且明确区分。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“长上下文语言建模”重铸为<strong>双层优化</strong>的持续学习问题，通过<strong>测试时训练（TTT）</strong>与<strong>元学习初始化</strong>两条耦合的“端到端”回路，在<strong>仅含滑动窗口注意力</strong>的常数复杂度骨架上实现随上下文长度增长而持续受益。具体解法分三步：</p>
<p>1. 测试时：把“读过的 token”压缩进权重（Inner Loop）</p>
<ul>
<li>仅保留<strong>滑动窗口注意力</strong>（窗口 k ≪ T），预填充与解码均保持</li>
</ul>
<p>O(T) prefill, quad O(1) per-token decode</p>
<p>与 RNN 相同量级的延迟。</p>
<ul>
<li>用<strong>下一词预测损失</strong>在线更新模型：</li>
</ul>
<p>ell<em>t(W</em>(t-1)) = CE!l(f(x<em>(t-1); W</em>(t-1)),; x_tr)</p>
<p>以 mini-batch 方式沿序列做梯度步</p>
<p>W<em>i = W</em>(i-1) - (eta) / (b)∑<em>(t=(i-1)b+1)^(ib)∇ell_t(W</em>(i-1))</p>
<p>从而把历史信息写入权重，<strong>无需 KV-cache 线性增长</strong>。</p>
<ul>
<li>实际只更新<strong>后 1/4 块的 MLP</strong>并加入<strong>静态第二 MLP</strong>防止预训练知识被遗忘，兼顾容量与稳定。</li>
</ul>
<p>2. 训练时：让模型“学会如何快速学习”（Outer Loop）</p>
<ul>
<li>每条训练序列被当成“伪测试”序列：先执行上述 inner-loop TTT，得到序列末权重  W_(T/b) 。</li>
<li>外层目标直接最小化<strong>TTT 之后的平均下一词损失</strong>：</li>
</ul>
<p>L(θ) = E<em>X![(1) / (T)∑</em>(i=1)^(T/b)∑<em>(t=(i-1)b+1)^(ib)ell_t(W</em>(i-1))]</p>
<p>对初始化  θ （即  W_0  的来源）求<strong>梯度之梯度</strong>优化，实现训练-测试行为一致。<br>现代自动微分框架使二阶导计算开销可控。</p>
<p>3. 架构与效率折中</p>
<ul>
<li>滑动窗口负责<strong>短程记忆</strong>；TTT 更新的 MLP 权重充当<strong>长程记忆</strong>，形成“双记忆”层次。</li>
<li>通过调节<br>– 窗口大小 k（8K）<br>– TTT mini-batch 大小 b（1K）<br>– 更新层数比例（1/4）<br>在<strong>状态容量</strong>、<strong>计算量</strong>与<strong>稳定性</strong>之间取得经验最优，使 3B 模型在 128K 上下文下<br>– 语言建模损失与<strong>全注意力持平或更好</strong>（图 1 左）<br>– 预填充延迟<strong>2.7× 更快</strong>（图 1 右）<br>– 解码延迟与 SWA 相同，每批仅额外一次 TTT 步骤。</li>
</ul>
<p>综上，论文<strong>不是</strong>设计新算子，而是把<br>“<strong>下一词预测 → 梯度更新 → 元学习初始化</strong>”<br>全程端到端地嵌入标准 Transformer 的 MLP 权重，使模型在<strong>常数每步成本</strong>下仍能随上下文长度<strong>持续吸收</strong>并利用信息，从而同时获得</p>
<ul>
<li>全注意力的** scaling 优势**</li>
<li>RNN 的<strong>推理速度</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否在常数推理成本下保持随上下文长度持续受益”这一核心问题，从<strong>超参数敏感性、训练算力 scaling、上下文长度 scaling、召回能力、长序列生成、训练/推理效率</strong>六个维度展开系统实验。主要结果均基于<strong>3B 参数模型+164B token 训练</strong>（含 3× 基础量扩展），并与<strong>全注意力、SWA、Hybrid、Mamba 2、Gated DeltaNet、TTT-KVB</strong>六条强基线对照。关键实验一览：</p>
<p>1. 超参数消融（760M 模型，DCLM/Books）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>变量</th>
<th>搜索范围</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>滑动窗口大小 k</td>
<td>1K–8K</td>
<td>越大越好；8K 后收益饱和，选 8K。</td>
</tr>
<tr>
<td>TTT mini-batch 大小 b</td>
<td>1K–8K</td>
<td>b=1K 最优；b≥8K 等效“无 TTT”，性能骤降。</td>
</tr>
<tr>
<td>更新层数比例</td>
<td>1/8, 1/4, 1/2, 全部</td>
<td>≥1/4 层才呈现与全注意力相同的“随长度下降”趋势；再增加层数收益有限，选 1/4。</td>
</tr>
</tbody>
</table>
</div>
<p>2. 训练算力 Scaling（125M–3B 参数 &amp; 16B–80B token）</p>
<ul>
<li><p><strong>模型规模轴</strong>：125M→3B<br>– 小算力区：TTT-E2E 优势随参数增大而缩小；<br>– ≥760M 后，ΔLoss 与全注意力<strong>平行下降</strong>，趋势一致。</p>
</li>
<li><p><strong>数据规模轴</strong>：760M 固定，预训练 token 16B→80B<br>– ≥48B token 后，TTT-E2E 与全注意力<strong>scaling 曲线重合</strong>；<br>– Gated DeltaNet 呈现相同趋势，验证“RNN 类方法在大算力下不劣于全注意力”假设。</p>
</li>
</ul>
<p>3. 上下文长度 Scaling（3B 模型，Books 32K–128K）</p>
<ul>
<li>** aggregated 指标**（图 1 左）<br>– 仅 TTT-E2E 与全注意力一样“越长越好”；其余方法在 32K 后损失<strong>开始回升</strong>。</li>
<li><strong>token-level 分解</strong>（图 6）<br>– TTT-E2E <strong>全程低于</strong>全注意力，优势主要集中在前 1/3 段；尾部差距缩小但不反转。</li>
<li><strong>直接损失值</strong>（图 9）<br>– 128K 时 TTT-E2E 绝对损失最低，验证“压缩策略”在长上下文仍有效。</li>
</ul>
<p>4. 召回能力 Stress-Test：Needle-in-a-Haystack（RULER-S-NIAH）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>128K 召回率</th>
</tr>
</thead>
<tbody>
<tr>
<td>Pass-key</td>
<td>全注意力 0.99，TTT-E2E 0.46；其余方法 ≤0.26。</td>
</tr>
<tr>
<td>Number/UUID</td>
<td>全注意力 0.64–0.86，TTT-E2E 0.05–0.10；其余方法 ≤0.05。</td>
</tr>
</tbody>
</table>
</div>
<p>结论：<strong>压缩机制牺牲极致召回</strong>，与全注意力“几乎无损回忆”形成互补。</p>
<p>5. 长序列生成评估（3B base 模型，16K 续写）</p>
<ul>
<li>协议：用 Qwen-8B 作为外部评判，计算续写段落的 log-likelihood。</li>
<li>结果（图 7）：TTT-E2E 的 Qwen-loss <strong>全程低于</strong>全注意力；人工抽检 20 条样本通顺。</li>
<li>说明：TTT 在<strong>自己生成的 token 上继续梯度更新</strong>（self-training）不会崩溃，反而保持优势。</li>
</ul>
<p>6. 效率基准（H100/H200 实测）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>指标</th>
<th>128K 对比</th>
</tr>
</thead>
<tbody>
<tr>
<td>推理预填充</td>
<td>latency / 1K token</td>
<td>全注意力 0.22s → TTT-E2E 0.08s（2.7× 快）</td>
</tr>
<tr>
<td>推理解码</td>
<td>额外延迟</td>
<td>每 1K token 仅多一次 TTT 步骤，与 SWA 相同</td>
</tr>
<tr>
<td>训练</td>
<td>latency / 1K token</td>
<td>8K 上下文下 TTT-E2E 慢 3.4×；128K 时仅慢 1.2×</td>
</tr>
<tr>
<td>训练</td>
<td>FLOPs / token</td>
<td>与上下文长度无关，但需随长度增加梯度检查点，导致延迟上升</td>
</tr>
</tbody>
</table>
</div>
<p>7. 附加对照</p>
<ul>
<li><strong>TTT-E2E vs. TTT-KVB 简化链路</strong>（表 1）：<br>用“末端下一词损失”替换“层内 KV 重建损失”即可带来 <strong>0.013–0.018 损失下降</strong>，验证“端到端损失”是关键。</li>
<li><strong>TTT-E2E 叠加全注意力</strong>（k=8K）：<br>在<strong>已有全注意力</strong>基础上仍能再降 0.018 损失，说明收益与“SWA→全注意力”差距<strong>正交</strong>。</li>
</ul>
<p>综上，实验覆盖从** toy 例子<strong>到</strong>3B-128K 生产规模**，从<strong>语言建模损失</strong>到<strong>硬件时钟</strong>，多维度一致表明：<br>TTT-E2E 可在<strong>常数每步延迟</strong>约束下，实现与全注意力<strong>相同的 scaling 行为</strong>，同时显著更快；代价是<strong>极致召回任务落后</strong>且<strong>当前训练实现更慢</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为论文明确抛出或隐含留出的“下一步”，按<strong>可行性→挑战性</strong>大致排序，并给出可验证的初始指标。</p>
<p>1. 训练效率：把“梯度之梯度”开销降到可忽略</p>
<ul>
<li><strong>问题</strong>：当前 8K 上下文训练延迟 3.4× 于全注意力，成为大规模采用的主要瓶颈。</li>
<li><strong>可行路径</strong></li>
</ul>
<ol>
<li>为 TTT 写定制 FlashAttention 风格 kernel，支持二阶导；</li>
<li>先用常规预训练 Transformer 初始化，再<strong>仅最后 5-10% 训练预算</strong>切入 TTT-E2E 元学习（类似 RNN 蒸馏做法）。</li>
</ol>
<ul>
<li><strong>验证指标</strong>：在 760M-3B 模型上把训练延迟差距缩至 &lt;1.5×，同时 128K 下损失不劣于原文。</li>
</ul>
<p>2. 解码阶段“自训练”稳定性与质量</p>
<ul>
<li><strong>问题</strong>：论文仅用 Qwen-8B 外部 loss 与 20 条人工样例验证 16K 自生成续写，尚缺系统度量。</li>
<li><strong>探索点</strong><br>– 更长续写（32K-128K）是否会因错误累积导致“自训练”发散？<br>– 引入<strong>选择性梯度门控</strong>（类似 LSTM 遗忘门）能否抑制噪声 token 的回传？</li>
<li><strong>验证指标</strong>：续写 32K 后重复率、外部评判 loss、人类评分不低于全注意力；梯度范数增长速率下降 30%+。</li>
</ul>
<p>3. 极端召回任务：把“压缩”升级为“可提示回忆”</p>
<ul>
<li><strong>问题</strong>：NIAH 128K 召回率仅 0.05-0.46，远低于全注意力 0.64-0.99。</li>
<li><strong>思路</strong><br>– 在 TTT 损失中增加<strong>对比式召回辅助任务</strong>（随机采样早期 token 作为正例，同批其他序列 token 为负例）。<br>– 或者采用<strong>双层状态</strong>：快速压缩的“权重状态”+ 少量显式 KV-cache“便签本”，在出现显式检索提示时切换查询模式。</li>
<li><strong>验证指标</strong>：在 RULER 三任务上 128K 召回率≥0.8，同时语言建模损失不升。</li>
</ul>
<p>4. 层次化时间窗口：从“单一路径”到“多时间分辨率”</p>
<ul>
<li><strong>灵感</strong>：人脑既有工作记忆也有海马体回放。</li>
<li><strong>做法</strong><br>– 把序列划分为 1K、8K、64K 三级窗口，每级维护独立的 TTT 权重，解码时通过门控动态融合。<br>– 外层元学习同时优化三级初始化。</li>
<li><strong>验证指标</strong>：在 1M 上下文长度下损失仍随长度下降，而单级 TTT 已饱和。</li>
</ul>
<p>5. 跨序列持续学习：让“长程记忆”在会话间保留</p>
<ul>
<li><strong>挑战</strong>：当前 TTT 权重在序列末尾即丢弃。</li>
<li><strong>可行第一步</strong><br>– 把最后隐状态  W_(T/b)  作为“用户嵌入”存到外部内存，下一条序列加载后继续 TTT。<br>– 外层加入<strong>正则项</strong>，防止新序列覆盖旧知识（类似 EWC）。</li>
<li><strong>验证指标</strong>：在连续 Book 会话（每会话 32K）上 perplexity 随会话数下降，而 naive 加载初始化持平或上升。</li>
</ul>
<p>6. 理论侧：压缩极限与 scaling law</p>
<ul>
<li><strong>开放问题</strong>：<br>– 给定模型容量  C  与上下文长度  T ，TTT 的信息压缩极限何在？<br>– 能否给出类似“Transformer 参数量 ∝ 长度”的 scaling law，预测 TTT-E2E 的最优更新层数、batch size？</li>
<li><strong>方法</strong>：借用率失真理论或线性动力学分析，拟合不同  (C,T)  下的最优超参，再用 125M-3B 实验验证。</li>
</ul>
<p>7. 其他模态与任务</p>
<ul>
<li><strong>视频/音频语言模型</strong>：时间步冗余度更高，TTT 压缩收益可能更大。</li>
<li><strong>工具使用/Agent</strong>：每步执行结果视为新 token，TTT 可在推理链中实时“自我微调”以提高后续决策准确率。</li>
<li><strong>验证指标</strong>：在 1M-token 视频文本序列或 10-step 工具调用任务上，下游指标优于同等算力的全注意力基线。</li>
</ul>
<p>8. 自监督 TTT 目标：超越下一词预测</p>
<ul>
<li><strong>假设</strong>：下一词预测并非唯一可压缩信号。</li>
<li><strong>尝试</strong>：在 TTT 内加入<strong>掩码重建</strong>或<strong>下一帧预测</strong>辅助损失，加权联合优化。</li>
<li><strong>验证</strong>：辅助损失需≤1.2× 计算量，主损失仍保持原水平或更好。</li>
</ul>
<p>小结（可快速入手的 Top-3）</p>
<ol>
<li><strong>训练加速</strong>——写支持二阶导的 FlashKernel + 后期切入策略。</li>
<li><strong>召回增强</strong>——对比式辅助损失或便签式 KV 便签本。</li>
<li><strong>自训练稳定性</strong>——长续写+门控梯度，防漂移。</li>
</ol>
<p>解决任一即可让 TTT-E2E 更接近“即插即用”的长上下文方案。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心主张：<br>把“长上下文语言建模”不再当成架构问题，而是<strong>持续学习</strong>问题——只需<strong>滑动窗口 Transformer</strong>的常数推理成本，就能在测试时继续用<strong>下一词预测</strong>把上下文压缩进权重，并通过<strong>元学习</strong>让模型提前学会“如何快速学习”。</p>
<p>方法总览（TTT-E2E）</p>
<ol>
<li><strong>测试阶段（Inner Loop）</strong></li>
</ol>
<ul>
<li>仅保留窗口大小 k=8K 的滑动窗口注意力 → 预填充 O(T)、解码 O(1)。</li>
<li>用标准交叉熵损失在线更新<strong>后 1/4 块的 MLP</strong>权重，mini-batch b=1K。</li>
<li>历史信息被压缩进权重，无需线性增长的 KV-cache。</li>
</ul>
<ol>
<li><strong>训练阶段（Outer Loop）</strong></li>
</ol>
<ul>
<li>每条训练序列先完整走一遍上述 TTT，得到“TTT 后损失”。</li>
<li>用梯度之梯度优化<strong>初始化 θ</strong>，使模型天生适合“测试时快速适应”。</li>
</ul>
<ol>
<li><strong>架构细节</strong></li>
</ol>
<ul>
<li>冻结嵌入、归一化、注意力层，仅更新 MLP；</li>
<li>每更新块加<strong>静态第二 MLP</strong>防止遗忘；</li>
<li>可自然扩展到多 token 解码：每凑满 1K token 执行一次 TTT。</li>
</ul>
<p>主要结果（3B 模型，164B token）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>关键结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>上下文 scaling</td>
<td>128K 下损失与全注意力持平且更低；其余 RNN 类方法 32K 后开始劣化。</td>
</tr>
<tr>
<td>推理延迟</td>
<td>128K 预填充2.7× 快于全注意力，解码与 SWA 相同。</td>
</tr>
<tr>
<td>训练算力 scaling</td>
<td>≥760M 参数或 ≥48B token 后，TTT-E2E 与全注意力scaling 曲线平行。</td>
</tr>
<tr>
<td>召回 stress-test</td>
<td>NIAH 128K 召回率远低于全注意力，验证“压缩牺牲极致记忆”。</td>
</tr>
<tr>
<td>长序列生成</td>
<td>自训练 16K 续写外部评判 loss 低于全注意力，未见漂移。</td>
</tr>
<tr>
<td>训练效率</td>
<td>8K 上下文训练延迟 3.4×，为当前主要瓶颈；FLOPs 与长度无关。</td>
</tr>
</tbody>
</table>
</div>
<p>贡献一句话</p>
<p>TTT-E2E 用**“继续预训练”**的思想把长上下文压进权重，<strong>常数延迟下实现全注意力级 scaling</strong>，为“长文本+低成本”提供新范式，同时留下训练加速与极端召回等开放问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是End-to-End%20Test-Time%20Training%20for%20Long%20Context，链接是https://arxiv.org/pdf/2512.23675，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.23675。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Arnuv Tandon,Karan Dalal,Xinhao Li,Daniel Koceja,Marcel Rød,Sam Buchanan,Xiaolong Wang,Jure Leskovec,Sanmi Koyejo,Tatsunori Hashimoto,Carlos Guestrin,Jed McCaleb,Yejin Choi,Yu Sun</p>
<p><strong>Categories</strong>: cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.23675.pdf">https://arxiv.org/pdf/2512.23675.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.23675">https://arxiv.org/abs/2512.23675</a></p>
<p><strong>Arxiv ID</strong>: 2512.23675</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.23675">https://papers.cool/arxiv/2512.23675</a></p>
<p><strong>Published</strong>: 2025-12-29T18:30:14Z</p>
<p><strong>Updated</strong>: 2025-12-29T18:30:14.000Z</p>
<hr>
<h3 id="4-Evaluating-Parameter-Efficient-Methods-for-RLVR"><a href="#4-Evaluating-Parameter-Efficient-Methods-for-RLVR" class="headerlink" title="4. Evaluating Parameter Efficient Methods for RLVR"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.23165">Evaluating Parameter Efficient Methods for RLVR</a></h3><p>We systematically evaluate Parameter-Efficient Fine-Tuning (PEFT) methods under the paradigm of Reinforcement Learning with Verifiable Rewards (RLVR). RLVR incentivizes language models to enhance their reasoning capabilities through verifiable feedback; however, while methods like LoRA are commonly used, the optimal PEFT architecture for RLVR remains unidentified. In this work, we conduct the first comprehensive evaluation of over 12 PEFT methodologies across the DeepSeek-R1-Distill families on mathematical reasoning benchmarks. Our empirical results challenge the default adoption of standard LoRA with three main findings. First, we demonstrate that structural variants, such as DoRA, AdaLoRA, and MiSS, consistently outperform LoRA. Second, we uncover a spectral collapse phenomenon in SVD-informed initialization strategies (\textit{e.g.,} PiSSA, MiLoRA), attributing their failure to a fundamental misalignment between principal-component updates and RL optimization. Furthermore, our ablations reveal that extreme parameter reduction (\textit{e.g.,} VeRA, Rank-1) severely bottlenecks reasoning capacity. We further conduct ablation studies and scaling experiments to validate our findings. This work provides a definitive guide for advocating for more exploration for parameter-efficient RL methods.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>我们系统地评估了在可验证奖励强化学习（RLVR）范式下的参数高效微调（PEFT）方法。RLVR通过可验证的反馈激励语言模型提升其推理能力；然而，尽管LoRA等方法被广泛使用，但针对RLVR的最佳PEFT架构仍未确定。在本工作中，我们首次对DeepSeek-R1-Distill系列中的12种以上PEFT方法在数学推理基准上进行了全面评估。我们的实证结果对默认采用标准LoRA提出了挑战，并得出了三点主要发现。首先，我们证明了结构变体（如DoRA、AdaLoRA和MiSS）始终优于LoRA。其次，我们发现了SVD信息化初始化策略（如PiSSA、MiLoRA）中的谱崩塌现象，将其失败归因于主成分更新与RL优化之间的根本性不对齐。此外，我们的消融实验显示，极端的参数减少（如VeRA、Rank-1）会严重限制推理能力。我们还进行了消融研究和扩展实验以验证我们的发现。本研究为进一步探索参数高效的强化学习方法提供了权威指南。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该研究聚焦于“在可验证奖励强化学习（RLVR）场景下，究竟该选哪一种参数高效微调（PEFT）方法”这一核心问题。传统上，社区几乎默认采用标准 LoRA 进行 RL 训练，但缺乏系统性的横向对比与理论依据。论文首次对 12 余种 PEFT 方法在 RLVR 框架下展开大规模评估，旨在：</p>
<ul>
<li>揭示标准 LoRA 是否为最优选择；</li>
<li>找出真正适配 RL 稀疏、离主成分更新特性的结构或初始化策略；</li>
<li>明确参数压缩的“表达能力下限”，避免因过度精简而导致推理性能崩塌；</li>
<li>为后续参数高效强化学习提供可复现的基准与选型指南。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与本文直接相关的研究可归纳为四条主线，均围绕“参数高效微调 × 强化学习”展开：</p>
<ol>
<li>RLVR 框架与算法</li>
</ol>
<ul>
<li>Group Relative Policy Optimization (GRPO)</li>
<li>Decoupled-Clip &amp; Dynamic Sampling Policy Optimization (DAPO)</li>
<li>Dr. GRPO（消除长度归一化与难度偏差）<br>这些工作奠定了“用可验证奖励提升 LLM 推理”的范式，为后续 PEFT 对比提供统一训练协议。</li>
</ul>
<ol>
<li>参数高效微调（PEFT）基础方法</li>
</ol>
<ul>
<li>LoRA / LoRA+ / rsLoRA / LoRA-FA / VeRA</li>
<li>AdaLoRA、DoRA、MiSS 等结构变种</li>
<li>PiSSA、MiLoRA 等 SVD 初始化策略<br>它们原本在监督场景被验证，但尚未在 RLVR 中被系统评估。</li>
</ul>
<ol>
<li>RL 稀疏更新与光谱分析</li>
</ol>
<ul>
<li>“The Path Not Taken” 理论：RLVR 梯度主要落在非主成分子空间</li>
<li>彩票 ticket/子网络稀疏更新观测<br>这些研究解释了为何 SVD-based 初始化会与 RL 优化动态冲突，为本文“光谱塌陷”现象提供理论注脚。</li>
</ul>
<ol>
<li>参数高效 RL 初步尝试</li>
</ol>
<ul>
<li>Tiny-Reasoning-Models via LoRA（Tina）</li>
<li>JustRL：1.5 B 模型仅用 LoRA 进行纯 RL 扩展<br>它们证明了 LoRA 在 RLVR 的可行性，但未横向比较其他 PEFT，也未探讨结构或初始化差异带来的影响。</li>
</ul>
<p>本文正是在上述基础上，首次把“RLVR 算法—PEFT 结构—光谱动力学—参数下限”四条线系统整合，填补“哪种 PEFT 最适合强化学习”这一空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“大规模对照实验 + 光谱机理分析”的双轨策略，系统回答“RLVR 该用哪种 PEFT”：</p>
<ol>
<li>构建统一基准</li>
</ol>
<ul>
<li>选用 DeepSeek-R1-Distill 1.5 B/7 B 双尺度模型，固定 DAPO 算法、超参、数据与奖励函数，确保变量唯一。</li>
<li>覆盖 12 种 PEFT，分“结构型 / 初始化型 / 极致压缩型 / 其他”四类，每类至少 2 个代表。</li>
</ul>
<ol>
<li>多维消融</li>
</ol>
<ul>
<li>正交扫描 batch size（32/128）、学习率（1e-5→1e-6）、rank（1/8/16/32）与 RL 损失变体（GRPO/Dr. GRPO），排除超参耦合假象。</li>
<li>1.5 B 结果在 7 B 上复现，验证规模一致性。</li>
</ul>
<ol>
<li>光谱与轨迹诊断</li>
</ol>
<ul>
<li>对每层权重更新做 SVD，绘制“奇异值索引—归一化幅度”曲线，揭示 PiSSA/MiLoRA 梯度仍被主成分吸回，量化“光谱塌陷”。</li>
<li>追踪训练动态，解释 MiLoRA 初期上升→后期崩塌的优化不稳定。</li>
</ul>
<ol>
<li>表达能力边界测试</li>
</ol>
<ul>
<li>固定总参数量级，逐步降低可训练分量（LoRA-FA→VeRA→IA3→LN-only），记录性能断崖，定位“表达能力下限”。</li>
</ul>
<ol>
<li>结论提炼与可复现资源</li>
</ol>
<ul>
<li>公开 PeRL 代码库、HuggingFace 检查点与 WandB 日志，提供一键复现脚本与评估协议，供社区直接迁移。</li>
</ul>
<p>通过“实验→现象→机理→边界→指南”的闭环，论文不仅指出 DoRA、MiSS、AdaLoRA 等结构变体优于标准 LoRA，也阐明 SVD 初始化与极致压缩为何失效，从而给出明确的选型路线图。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验按“主实验 → 消融 → 缩放”三级展开，共 3 大类 12 组对照，累计训练 &gt;200 份模型：</p>
<ol>
<li>主实验（1.5 B 模型，固定 DAPO）</li>
</ol>
<ul>
<li>12 种 PEFT 全量对比：Full / LoRA / DoRA / AdaLoRA / MiSS / PiSSA / MiLoRA / LoRA+ / rsLoRA / LoRA-FA / VeRA / IA3 / LN-Tuning</li>
<li>统一 rank=32、lr=1e-5、batch=128、step=1 024，评价 6 个数学基准（AIME24/25、AMC、HMMT、MATH500、Minerva）。</li>
<li>记录 Overall Accuracy、Avg@k、Pass@k 及可训练参数量，绘制“参数量—性能”对数坐标前沿图。</li>
</ul>
<ol>
<li>四维消融（仍在 1.5 B）</li>
</ol>
<ul>
<li>batch size：32 vs 128</li>
<li>学习率：1e-5 / 5e-6 / 1e-6</li>
<li>LoRA rank：1 / 8 / 16 / 32</li>
<li>RL 算法：GRPO / Dr. GRPO / DAPO<br>每组仅变动一个因子，其余与主实验一致，验证结论对超参不敏感。</li>
</ul>
<ol>
<li>缩放实验</li>
</ol>
<ul>
<li>把主实验与最佳消融配置原封不动迁移到 7 B 模型（batch=32，step=8 192），验证 DoRA、LoRA+、MiSS 等相对排序是否保持。</li>
</ul>
<ol>
<li>光谱与动态诊断</li>
</ol>
<ul>
<li>对每层 gate-proj 抽取训练结束后的 ∆W，做 SVD 并归一化，绘制“主成分索引—更新幅度”曲线，对比 Full、LoRA、PiSSA、MiLoRA。</li>
<li>记录训练过程中每一步的 reward 曲线，量化 MiLoRA 的“先升后崩”现象。</li>
</ul>
<ol>
<li>表达能力下限测试</li>
</ol>
<ul>
<li>在 LoRA 框架内逐次极端压缩：rank-1、VeRA（仅标量向量）、IA3（逐元素缩放）、LN-Tuning（仅偏置与增益），保持训练预算一致，观察性能断崖点。</li>
</ul>
<p>所有实验均使用同一奖励脚本（latex2sympy + math_verify）、同一生成超参（T=0.6，top-p=0.95，32 k token），并公开代码与日志以保证可复现。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向可归纳为 <strong>理论机理、工程系统、场景扩展、部署稳定性</strong> 四大类：</p>
<p>1. 理论机理</p>
<ul>
<li><strong>光谱演化动力学</strong><br>建立 ∆W 的随机矩阵微分方程，量化“离主成分”更新的李雅普诺夫指数，解释为何 DoRA 能长期维持 off-principal 轨迹而 MiLoRA 被拉回。</li>
<li><strong>适配器容量-复杂度测度</strong><br>提出面向 RL 的“有效秩”或“稀疏谱熵”指标，与任务难度（如 MATH 等级）建立解析关系，给出 rank 选择的理论下限。</li>
<li><strong>奖励稀疏性与梯度噪声关系</strong><br>推导稀疏 0/1 奖励下梯度协方差的解析形式，揭示不同 PEFT 结构对噪声的鲁棒性差异。</li>
</ul>
<p>2. 工程系统</p>
<ul>
<li><strong>分布式框架升级</strong><br>将当前 TRL+DeepSpeed-ZeRO-2 迁移至 VeRL、OpenRLHF 等高性能 RL 底座，支持 70 B+ 模型、万卡规模与异步 rollout。</li>
<li><strong>长周期训练与冷启动</strong><br>在 R1-Zero 式无 SFT 冷启动、&gt;100 k steps 的体制下，观察 DoRA/AdaLoRA 是否仍保持优势，验证其渐近收敛性。</li>
<li><strong>多阶段课程 RL</strong><br>引入难度课程（AMC → AIME → Putnam）与动态 rank 分配，检验结构型 PEFT 能否随课程自动扩展容量。</li>
</ul>
<p>3. 场景扩展</p>
<ul>
<li><strong>多模态 RLVR</strong><br>将文本-图像混合推理（几何题、函数图像）纳入可验证奖励体系，验证 DoRA 在 cross-attention 层是否依旧有效。</li>
<li><strong>多轮交互式 RL</strong><br>在 MathChat、WebQA 等需要多轮工具调用与反馈的环境中，测试 PEFT 对长期策略优化的塑性。</li>
<li><strong>异步/离线 RL</strong><br>用 xPO、DPO-offline 等算法替代 on-policy GRPO，考察不同 PEFT 对离线数据分布漂移的稳健性。</li>
</ul>
<p>4. 部署稳定性</p>
<ul>
<li><strong>权重合并数值误差</strong><br>量化 float16/INT8 合并时的谱误差累积，提出 DoRA 的 magnitude-direction 分离合并协议，保证训练-推理一致性。</li>
<li><strong>推理延迟-精度权衡</strong><br>在 TensorRT-LLM、MLC-LLM 上实测 DoRA、AdaLoRA 的 kernel 级加速潜力，给出 serving 时的 rank 剪枝策略。</li>
<li><strong>适配器冲突与叠加</strong><br>研究数学 DoRA 与代码 LoRA 同时挂载时的梯度干扰，提出多任务适配器正交投影或子空间隔离方案。</li>
</ul>
<p>5. 自动化搜索</p>
<ul>
<li><strong>NAS for PEFT 结构</strong><br>以 RL 奖励为超级网络优化目标，自动搜索 magnitude、direction、rank、稀疏掩码的联合配置，超越手工设计的 DoRA。</li>
<li><strong>元学习初始化</strong><br>用 MAML 框架在数学任务分布上元学习 DoRA 的初始 magnitude 与 direction 分量，实现“几步即收敛”的快速适配。</li>
</ul>
<p>通过上述方向，可完成从“经验观察”到“理论-系统-产品”全链路的闭环，推动参数高效强化学习进入生产级应用。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>论文核心内容一览</strong></p>
<ol>
<li><p>研究动机<br>社区在“可验证奖励强化学习（RLVR）”中默认使用标准 LoRA，但缺乏系统横向对比；尚不清楚哪种参数高效微调（PEFT）真正适配 RL 的稀疏、离主成分更新特性。</p>
</li>
<li><p>实验规模</p>
</li>
</ol>
<ul>
<li>首次在 RLVR 框架下对 <strong>12+ 种 PEFT</strong>（结构型、初始化型、极致压缩型等）进行大规模对照。</li>
<li>覆盖 <strong>DeepSeek-R1-Distill 1.5 B / 7 B</strong> 双尺度，<strong>6 项数学基准</strong>（AIME24/25、AMC、HMMT、MATH500、Minerva）。</li>
<li>正交消融 <strong>batch size / 学习率 / rank / RL 算法</strong> 四个维度，累计训练 <strong>&gt;200 个模型</strong>。</li>
</ul>
<ol>
<li>主要发现</li>
</ol>
<ul>
<li><strong>结构型变种全面优于标准 LoRA</strong>：DoRA、AdaLoRA、MiSS 平均准确率分别达 <strong>46.6%、44.2%、43.4%</strong>，DoRA 甚至超过全参数微调（44.9%）。</li>
<li><strong>SVD 初始化与 RL 优化根本错位</strong>：<br>– PiSSA 强制主成分更新，与 RLVR 的“离主成分”特性冲突，训练崩溃至 <strong>0.2%</strong>。<br>– MiLoRA 虽用次成分初始化，但初始范数趋零，梯度仍被主成分吸回，最终仅 <strong>18.0%</strong>。</li>
<li><strong>存在表达能力下限</strong>：极致压缩方法（VeRA、IA3、LN-Tuning）因可训练空间过窄，推理性能显著崩塌（22.3%–40.7%）。</li>
</ul>
<ol>
<li><p>机理与诊断<br>通过 <strong>SVD 光谱分析</strong>与训练动态追踪，首次量化“光谱塌陷”现象，揭示梯度在主成分子空间的再聚集是 SVD 初始化失败的根源。</p>
</li>
<li><p>工程与缩放验证</p>
</li>
</ol>
<ul>
<li>7 B 模型上 DoRA、LoRA+ 仍保持 <strong>55.0%</strong>，相对 LoRA（54.8%）稳定领先，验证结论与规模无关。</li>
<li>公开 <strong>PeRL 代码库、HF 检查点、WandB 日志</strong>，提供可复现的 PEFT-RLVR 基准。</li>
</ul>
<ol>
<li>结论与建议<br><strong>停止在 RLVR 中默认使用标准 LoRA</strong>；优先采用 <strong>DoRA 等结构型或 LoRA+ 等优化型</strong>适配器，并避免 SVD 初始化与极端参数压缩，从而在效率与推理能力间取得最佳平衡。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Evaluating%20Parameter%20Efficient%20Methods%20for%20RLVR，链接是https://arxiv.org/pdf/2512.23165，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.23165。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Qingyu Yin,Yulun Wu,Zhennan Shen,Sunbowen Li,Zhilin Wang,Yanshu Li,Chak Tou Leong,Jiale Kang,Jinjin Gu</p>
<p><strong>Categories</strong>: cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.23165.pdf">https://arxiv.org/pdf/2512.23165.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.23165">https://arxiv.org/abs/2512.23165</a></p>
<p><strong>Arxiv ID</strong>: 2512.23165</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.23165">https://papers.cool/arxiv/2512.23165</a></p>
<p><strong>Published</strong>: 2025-12-29T03:13:08Z</p>
<p><strong>Updated</strong>: 2025-12-29T03:13:08.000Z</p>
<hr>
<h3 id="5-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization"><a href="#5-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization" class="headerlink" title="5. GraphLocator: Graph-guided Causal Reasoning for Issue Localization"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.22469">GraphLocator: Graph-guided Causal Reasoning for Issue Localization</a></h3><p>The issue localization task aims to identify the locations in a software repository that requires modification given a natural language issue description. This task is fundamental yet challenging in automated software engineering due to the semantic gap between issue description and source code implementation. This gap manifests as two mismatches:(1) symptom-to-cause mismatches, where descriptions do not explicitly reveal underlying root causes; (2) one-to-many mismatches, where a single issue corresponds to multiple interdependent code entities. To address these two mismatches, we propose GraphLocator, an approach that mitigates symptom-to-cause mismatches through causal structure discovering and resolves one-to-many mismatches via dynamic issue disentangling. The key artifact is the causal issue graph (CIG), in which vertices represent discovered sub-issues along with their associated code entities, and edges encode the causal dependencies between them. The workflow of GraphLocator consists of two phases: symptom vertices locating and dynamic CIG discovering; it first identifies symptom locations on the repository graph, then dynamically expands the CIG by iteratively reasoning over neighboring vertices. Experiments on three real-world datasets demonstrates the effectiveness of GraphLocator: (1) Compared with baselines, GraphLocator achieves more accurate localization with average improvements of +19.49% in function-level recall and +11.89% in precision. (2) GraphLocator outperforms baselines on both symptom-to-cause and one-to-many mismatch scenarios, achieving recall improvement of +16.44% and +19.18%, precision improvement of +7.78% and +13.23%, respectively. (3) The CIG generated by GraphLocator yields the highest relative improvement, resulting in a 28.74% increase in performance on downstream resolving task.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>问题定位任务旨在根据自然语言的问题描述，识别软件仓库中需要修改的位置。由于问题描述与源代码实现之间存在语义差距，这一任务在自动化软件工程中既基础又具有挑战性。这种差距表现为两种不匹配：(1) 症状与原因不匹配，即描述中未明确揭示潜在的根本原因；(2) 一对多不匹配，即单个问题对应多个相互依赖的代码实体。为了解决这两种不匹配，我们提出了GraphLocator，一种通过因果结构发现来缓解症状与原因不匹配，并通过动态问题拆解解决一对多不匹配的方法。其核心产物是因果问题图（CIG），图中的顶点代表发现的子问题及其相关的代码实体，边表示它们之间的因果依赖关系。GraphLocator的工作流程分为两个阶段：症状顶点定位和动态CIG发现；它首先在仓库图中识别症状位置，然后通过对邻近顶点迭代推理动态扩展CIG。在三个真实世界的数据集上的实验表明，GraphLocator的有效性：(1) 与基线方法相比，GraphLocator在函数级召回率平均提升+19.49%，精确率提升+11.89%，定位更为准确。 (2) GraphLocator在症状-原因不匹配和一对多不匹配场景中均优于基线方法，召回率分别提升+16.44%和+19.18%，精确率分别提升+7.78%和+13.23%。 (3) GraphLocator生成的CIG带来了最高的相对改进，在下游问题解决任务中的性能提升了28.74%。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文针对“issue localization”任务中存在的两大语义鸿沟——symptom-to-cause mismatch 与 one-to-many mismatch——提出 GraphLocator 框架，旨在将自然语言描述的问题自动、准确地映射到代码库中需要修改的多个相互依赖的代码实体。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><strong>Embedding-based 方法</strong></li>
<li>CodeSage、CodeRankEmbed、SWERankEmbed 等将 issue 与代码联合嵌入，通过相似度排序定位，但忽略结构依赖，难以追溯根因或捕捉跨实体修改。</li>
<li><strong>LLM-based 方法</strong></li>
<li><strong>Procedural 流程</strong>：Agentless、BugCerberus 按固定层级（文件→类→函数）逐步筛选，缺乏反向回溯与结构感知。</li>
<li><strong>Agentic 流程</strong>：SWE-Agent、OpenHands、Trae Agent、MoatlessTools 让 LLM 自主遍历代码图；LocAgent、CoSIL、OrcaLoca 进一步引入图结构，但仅基于“相关性”而非因果性，易引入噪声或剪枝过度。</li>
<li><strong>因果推理研究</strong></li>
<li>传统 SCM、Potential Outcome 框架针对表格数据；近期探索 LLM 在文本/代码上的因果发现、归因与效应估计，但尚未用于软件问题定位场景。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>GraphLocator 通过“因果结构发现 + 动态问题解耦”双机制，把 issue 拆成可解释的因果子问题链，再映射到代码实体，从而同时弥合 symptom-to-cause 与 one-to-many 两类鸿沟。具体流程如下：</p>
<ol>
<li><p>构建 Repository Dependency Fractal Structure (RDFS)<br>将仓库抽象为分层异构图，节点为目录/包/文件/类/方法等，边为 HasMember、UsedBy、ImportedBy 等依赖；采用惰性加载，保证可扩展性。</p>
</li>
<li><p>两阶段定位</p>
</li>
</ol>
<ul>
<li><p><strong>阶段Ⅰ：Symptom Vertices Locating</strong><br>SearchAgent 调用三种工具</p>
</li>
<li><p><code>search_vertex</code>：按名称/类型模糊匹配，召回候选症状节点</p>
</li>
<li><code>search_edge</code>：按依赖关系进一步筛选</li>
<li><code>finish</code>：返回症状节点集合 S<br>解决“描述→初始代码入口”的映射。</li>
<li><p><strong>阶段Ⅱ：Dynamic CIG Discovering</strong><br>以 S 为起点，在 RDFS 上迭代扩展 Causal Issue Graph (CIG)：</p>
</li>
<li><p>节点：子问题 x（自然语言描述）+ 其关联的代码顶点 φ(x)</p>
</li>
<li>边：带概率 ψ(x,y) 的因果依赖，由 LLM 评估<br>采用优先级队列，每次扩展 Ψ(x) 最高的子问题；通过“图引导反绎推理”模板（issue + 当前 CIG + 新观察邻居 Ot）让 LLM 判断哪些邻居是可信原因，从而逐步解耦并收敛到根因集合。</li>
</ul>
<ol>
<li>输出<br>最终 CIG 的所有 φ(x) 即为需修改的代码实体集合，天然支持多实体联合定位。</li>
</ol>
<p>通过显式建模因果链，GraphLocator 把“症状→根因”的多跳推理与“单 issue→多实体”的依赖拆解统一在一张因果图中，实现高精度、高召回的细粒度定位。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 3 个真实数据集（Python：SWE-bench Lite、LocBench；Java：Multi-SWE-bench）上系统评估 GraphLocator，围绕 4 个研究问题展开实验，共含 987 条 issue 实例。实验设计如下：</p>
<ul>
<li><p><strong>RQ1 有效性</strong><br>与 5 个代表基线（SWERank-S/L、Agentless、LocAgent、CoSIL）对比 file/module/function 三级定位指标（SL、Recall、Precision、F1）。Claude-3.5 下 function-level 平均 Recall 提升 19.49%，Precision 提升 11.89%。</p>
</li>
<li><p><strong>RQ2 泛化性</strong><br>按“symptom-to-cause 距离”与“ground-truth 函数数”分层评估。随着距离或函数数增加，所有方法性能下降，但 GraphLocator 在各层均保持最高 F1，验证对复杂场景的鲁棒性。</p>
</li>
<li><p><strong>RQ3 消融实验</strong><br>分别移除 search_vertex、search_edge、priority 队列、CIG 引导四组件。function-level F1 从 30.91% 降至 14.23%–21.50%，量化各模块贡献。</p>
</li>
<li><p><strong>RQ4 成本分析</strong><br>记录图构建耗时与 LLM 调用开销。惰性加载使万级函数仓库 20 s 内完成骨架构建；相比 LocAgent，输入 token 减少 32–53%，调用费用降低 23–44%。</p>
</li>
<li><p><strong>下游修复验证</strong><br>将定位结果嵌入 Agentless 与 Trae Agent 进行单样本修复。GraphLocator 提供的 CIG 结构使修复成功率相对提升 28.74%，证实定位质量可直接转化为后续补丁生成收益。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可进一步拓展 GraphLocator 的边界与实用性：</p>
<ol>
<li><p>跨语言通用性<br>当前仅验证 Python/Java；可构建针对 C/C++、Rust、Go 等语言的 RDFS 语法规范，检验因果推理框架在多语言场景下的可迁移性与精度变化。</p>
</li>
<li><p>增量式仓库演化<br>真实仓库持续提交，图结构动态变化。研究如何基于差异 AST 实现 CIG 的增量更新与缓存复用，避免每次全量重建，降低持续集成中的延迟。</p>
</li>
<li><p>多模态 issue 描述<br>现有描述为纯文本；GitHub issue 常含截图、日志、堆栈轨迹。探索将图像/日志向量化后与文本联合作为症状输入，提升对 UI 异常、并发崩溃等场景的定位精度。</p>
</li>
<li><p>双向因果验证<br>目前依赖 LLM 单向估计 ψ(x,y)。可引入轻量级动态或静态分析（如切片、污染分析）对因果边进行反向验证，减少假阳性，提高 ψ 的可解释性与置信度。</p>
</li>
<li><p>与修复生成端到端联合优化<br>定位后仍需生成补丁。可将 CIG 子问题节点直接作为“修复规划”输入，采用分层生成策略：先为每个子问题生成候选补丁，再通过图依赖做冲突消解与合并，实现定位-修复一体化。</p>
</li>
<li><p>人机协同诊断<br>提供可视化 CIG 编辑器，让开发者对子问题、因果边进行增删或调整权重；系统实时反馈定位结果，实现“人在回路”的交互式根因分析。</p>
</li>
<li><p>因果效应量化<br>借鉴 SCM 的 do-calculus，尝试在代码层面定义“修改某节点对症状消失的边际效应”，从而给出数值化的修复优先级，而不仅仅依赖 LLM 的概率打分。</p>
</li>
<li><p>大规模私有仓库评估<br>企业代码库规模与依赖复杂度远超开源项目。研究分布式 RDFS 构建与分片推理，测试 GraphLocator 在百万级节点、严格权限环境下的可扩展性与安全合规。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>GraphLocator: Graph-guided Causal Reasoning for Issue Localization</strong><br><strong>一句话总结</strong>：用“因果图”把自然语言 issue 拆成带依赖的子问题，再映射到多段需修改的代码，显著缓解症状-根因错位与一对多错位。</p>
<p>1. 问题</p>
<ul>
<li>issue localization 要把描述映射到需改动的代码，但存在<br>① symptom-to-cause mismatch（描述只提现象，未暴露根因）<br>② one-to-many mismatch（一个 issue 需同时改多个相互依赖的函数/类）</li>
</ul>
<p>2. 核心思路</p>
<ul>
<li>构建 <strong>Repository Dependency Fractal Structure (RDFS)</strong>：分层异构图，节点=目录→包→文件→类→方法，边=HasMember/UsedBy/ImportedBy 等依赖。</li>
<li>提出 <strong>Causal Issue Graph (CIG)</strong>：有向图，顶点=子问题+对应代码实体，边=LLM 估计的因果概率 ψ。</li>
<li>两阶段流程<br>① <strong>Symptom Vertices Locating</strong>：SearchAgent 用 search_vertex / search_edge 工具在 RDFS 上召回与描述最相关的症状节点。<br>② <strong>Dynamic CIG Discovering</strong>：以症状为起点，按优先级 Ψ(x) 迭代扩展邻居，用图引导反绎推理模板让 LLM 判断新节点是否构成可信原因，逐步解耦并补全因果链。</li>
</ul>
<p>3. 实验结果（987 条 Python &amp; Java issue）</p>
<ul>
<li><strong>RQ1 有效性</strong>：vs 5 个强基线，Claude-3.5 下 function-level Recall↑19.49%，Precision↑11.89%，F1 平均领先 9-21%。</li>
<li><strong>RQ2 泛化性</strong>：随 symptom-to-cause 距离或 ground-truth 函数数增加，性能下降但 GraphLocator 始终保持最高 F1。</li>
<li><strong>RQ3 消融</strong>：移除任一组件 function-level F1 降 30-55%，验证工具集、优先级队列、CIG 引导均关键。</li>
<li><strong>RQ4 成本</strong>：惰性加载使 10 k 函数仓库 20 s 内完成骨架；比 LocAgent 节省 23-44% token 费用。</li>
<li><strong>下游修复</strong>：把 CIG 直接输入 Agentless/Trae Agent，修复成功率相对提升 28.74%。</li>
</ul>
<p>4. 贡献</p>
<ul>
<li>提出 LLM-驱动的混合工作流，结合 RDFS 探索与因果结构发现。</li>
<li>引入 CIG 显式建模子问题间因果依赖，实现动态解耦。</li>
<li>在三大真实数据集上取得显著且跨语言稳定的定位增益，并首次验证因果结构可转化为后续补丁收益。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是GraphLocator%3A%20Graph-guided%20Causal%20Reasoning%20for%20Issue%20Localization，链接是https://arxiv.org/pdf/2512.22469，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.22469。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wei Liu,Chao Peng,Pengfei Gao,Aofan Liu,Wei Zhang,Haiyan Zhao,Zhi Jin</p>
<p><strong>Categories</strong>: cs.SE</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.22469.pdf">https://arxiv.org/pdf/2512.22469.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.22469">https://arxiv.org/abs/2512.22469</a></p>
<p><strong>Arxiv ID</strong>: 2512.22469</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.22469">https://papers.cool/arxiv/2512.22469</a></p>
<p><strong>Published</strong>: 2025-12-27T05:02:53Z</p>
<p><strong>Updated</strong>: 2025-12-27T05:02:53.000Z</p>
<hr>
<h3 id="6-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs"><a href="#6-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs" class="headerlink" title="6. GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2512.21008">GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs</a></h3><p>Mixture-of-Experts (MoE) architectures have advanced the scaling of Large Language Models (LLMs) by activating only a sparse subset of parameters per input, enabling state-of-the-art performance with reduced computational cost. As these models are increasingly deployed in critical domains, understanding and strengthening their alignment mechanisms is essential to prevent harmful outputs. However, existing LLM safety research has focused almost exclusively on dense architectures, leaving the unique safety properties of MoEs largely unexamined. The modular, sparsely-activated design of MoEs suggests that safety mechanisms may operate differently than in dense models, raising questions about their robustness. In this paper, we present GateBreaker, the first training-free, lightweight, and architecture-agnostic attack framework that compromises the safety alignment of modern MoE LLMs at inference time. GateBreaker operates in three stages: (i) gate-level profiling, which identifies safety experts disproportionately routed on harmful inputs, (ii) expert-level localization, which localizes the safety structure within safety experts, and (iii) targeted safety removal, which disables the identified safety structure to compromise the safety alignment. Our study shows that MoE safety concentrates within a small subset of neurons coordinated by sparse routing. Selective disabling of these neurons, approximately 3% of neurons in the targeted expert layers, significantly increases the averaged attack success rate (ASR) from 7.4% to 64.9% against the eight latest aligned MoE LLMs with limited utility degradation. These safety neurons transfer across models within the same family, raising ASR from 17.9% to 67.7% with one-shot transfer attack. Furthermore, GateBreaker generalizes to five MoE vision language models (VLMs) with 60.9% ASR on unsafe image inputs.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>专家混合（Mixture-of-Experts，MoE）架构通过在每个输入上仅激活稀疏子集的参数，推动了大型语言模型（LLM）的扩展，使其在降低计算成本的同时实现了最先进的性能。随着这些模型越来越多地部署在关键领域，理解并强化其对齐机制对于防止有害输出至关重要。然而，现有的 LLM 安全研究几乎完全集中在密集架构上，MoE 独特的安全特性鲜有研究。MoE 的模块化、稀疏激活设计表明，其安全机制可能与密集模型不同，从而引发对其稳健性的疑问。在本文中，我们提出了 GateBreaker，这是首个无需训练、轻量且与架构无关的攻击框架，能够在推理阶段破坏现代 MoE LLM 的安全对齐。GateBreaker 的操作分为三个阶段：(i) 门级分析，用于识别在有害输入上被不成比例路由的安全专家；(ii) 专家级定位，用于在安全专家内定位安全结构；(iii) 定向安全移除，禁用已识别的安全结构以破坏安全对齐。我们的研究表明，MoE 的安全性集中在由稀疏路由协调的一小部分神经元内。选择性禁用这些神经元（目标专家层中约 3% 的神经元）显著提高了平均攻击成功率（ASR），从 7.4% 提升至 64.9%，而对八个最新的对齐 MoE LLM 的实用性影响有限。这些安全神经元能够在同一系列的模型间迁移，一次性迁移攻击即可将 ASR 从 17.9% 提升至 67.7%。此外，GateBreaker 可推广至五个 MoE 视觉语言模型（VLMs），在不安全的图像输入上达到了 60.9% 的 ASR。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在揭示并验证一个尚未被充分研究的安全问题：<br><strong>当前主流的大语言模型（LLM）安全对齐机制在稀疏激活的混合专家（MoE）架构中表现出新的脆弱性，且这些脆弱性可被极轻量的推理时攻击利用。</strong></p>
<p>具体而言，作者指出：</p>
<ol>
<li>现有安全研究几乎只关注稠密模型，对 MoE 的“稀疏-专家-路由”特性如何影响对齐鲁棒性缺乏系统分析。</li>
<li>MoE 的安全行为并非均匀分布，而是<strong>集中在极少数专家及其内部神经元</strong>；一旦这些关键计算路径被精准禁用，模型会丧失拒绝能力，而通用能力几乎不受影响。</li>
<li>因此，论文提出并验证 <strong>GateBreaker</strong>——首个<strong>无需训练、仅修改推理激活值</strong>即可对多种 MoE LLM（含视觉-语言变体）实现高成功率 jailbreak 的攻击框架，以证明上述脆弱性的现实危害，并呼吁社区设计针对 MoE 的冗余与防御机制。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究划分为三大主线，并指出它们与 MoE 安全问题的缺口：</p>
<ol>
<li>稠密 LLM 的越狱与神经元级攻击</li>
</ol>
<ul>
<li>提示级越狱：角色扮演、混淆、迭代优化（Wei et al. 2023；Shen et al. 2024；Zou et al. 2023 等）。</li>
<li>神经元级禁用：NeuroStrike、Safety Neuron Pruning、Low-rank 修改（Li 2024；Chen 2024；Wei 2024 等）。<br>→ 全部针对稠密模型，未考虑稀疏路由带来的“专家-局部化”安全现象。</li>
</ul>
<ol>
<li>MoE 的通用攻击与后门</li>
</ol>
<ul>
<li>路由侧信道窃取提示（Yona 2024）。</li>
<li>批内路由完整性攻击（Hayes 2024）。</li>
<li>毒化休眠专家的后门（BadMoE, Wang 2025）。<br>→ 聚焦可用性或完整性，<strong>不触碰安全对齐</strong>。</li>
</ul>
<ol>
<li>唯一与“MoE 安全对齐”直接相关的先行工作</li>
</ol>
<ul>
<li>SAFEx（Lai 2025）：在推理时<strong>整专家屏蔽</strong>以降低拒绝率，但仅粗粒度实验 4 个模型，ASR 不足 30 %。<br>→ GateBreaker 以<strong>神经元级</strong>、<strong>训练无关</strong>、<strong>跨架构</strong>方式将 ASR 提升至 64.9 %，并首次揭示安全神经元在专家内部的可转移性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未“解决”安全漏洞，而是<strong>系统性地暴露并验证</strong>该漏洞的存在与可利用性，进而为后续防御研究提供明确靶点。其方法分为三步：</p>
<ol>
<li><p>门控层画像（Gate-level Profiling）<br>用大规模有害提示触发模型，记录每层专家被路由的频率，选出“高有害激活”的候选安全专家。</p>
</li>
<li><p>专家内部定位（Expert-level Localization）<br>在候选专家里，对比有害与良性提示的神经元激活，计算每个神经元的“安全权重”</p>
</li>
</ol>
<p>w<em>(l,j,n)=mathbb E</em>(q∈ D<em>harm)[v</em>(l,j)(q)<em>n]-mathbb E</em>(q∈ D<em>benign)[v</em>(l,j)(q)_n]</p>
<p>并做 z-score 标准化，取阈值 τ=2 的极端值作为<strong>安全神经元</strong>集合  N_(safety) 。</p>
<ol>
<li>靶向安全移除（Targeted Safety Removal）<br>推理时<strong>零训练、零梯度</strong>，仅把  N_(safety)  对应激活值就地置零：</li>
</ol>
<p>A’<em>(l,j)(x)_n=0,&amp;n∈ N</em>(safety)A_(l,j)(x)_n,&amp;otherwise</p>
<p>平均只需修改 2.6 % 的神经元即可令 8 个主流 MoE 模型的平均攻击成功率从 7.4 % 升至 64.9 %，而通用基准性能几乎不变。</p>
<p>通过上述“轻量-推理时-神经元级”攻击，论文<strong>实证回答</strong>了“MoE 的安全对齐是否可被局部瓦解”这一问题，并给出可复制的实验框架与防御建议，供社区后续<strong>设计冗余与监控机制</strong>时参考。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“MoE 安全对齐能否被局部瓦解”这一核心假设，设计并执行了<strong>六大类实验</strong>，覆盖模型数量、架构种类、攻击场景与防御维度，具体如下：</p>
<ol>
<li>层深度消融（Layer-wise Safety Removal）</li>
</ol>
<ul>
<li>8 个最新 MoE LLM（稀疏/混合/分组混合）</li>
<li>0 %、50 %、100 % 层范围剪枝</li>
<li>结果：平均 ASR 从 7.4 % → 64.9 %，仅动 2.6 % 神经元/层</li>
</ul>
<ol>
<li>一次性跨模型迁移（One-shot Transfer Attack）</li>
</ol>
<ul>
<li>同源基础模型 → 数学/营销/代码/多语言等 9 个微调变体</li>
<li>不重新画像，直接复用源模型安全神经元掩码</li>
<li>结果：平均 ASR 从 17.9 % → 66.1 %，最高提升 91.2 %</li>
</ul>
<ol>
<li>多模态 VLM 攻击</li>
</ol>
<ul>
<li>5 个 2025 版 MoE-VLM（Deepseek-VL2、Kimi-VL 系列）</li>
<li>文本提示转印刷体图片 + NSFW 图片描述任务</li>
<li>结果：ASR 从 20.8 % → 60.9 %；跨 VLM 迁移仍 &gt;54 %</li>
</ul>
<ol>
<li>与唯一现有基线 SAFEx 的横向对比</li>
</ol>
<ul>
<li>同一 8 模型 + 同一有害提示池</li>
<li>GateBreaker 平均 ASR 64.9 % vs SAFEx 29.9 %，最低提升 1.4×，最高 12×</li>
</ul>
<ol>
<li>通用能力 Utility 评估</li>
</ol>
<ul>
<li>5 大 NLU/推理基准（CoLA、RTE、WinoGrande、OpenBookQA、ARC）</li>
<li>平均下降 &lt;4.7 pp，部分模型个别任务反升，验证“功能无损”</li>
</ul>
<ol>
<li>超参数与消融细验</li>
</ol>
<ul>
<li>稀疏 vs 共享专家：共享虽少但“始终在线”，单剪即可达 50 % ASR</li>
<li>子层贡献：gate-proj 55.9 % ASR，up-proj 仅 20.6 %</li>
<li>安全专家数量：Top-k → Top-3k 平均 ASR 37 % → 65 %</li>
<li>z-score 阈值 τ=1/2/3：τ=2 在攻击强度与输出稳定性间最佳</li>
<li>随机神经元剪枝对照：ASR ≤11.8 %，排除“容量下降”假说</li>
<li>抑制强度 35 %/65 %/100 %：ASR 单调上升至 64.9 %，证实神经元-拒绝因果链</li>
</ul>
<p>以上实验共同证明：<strong>MoE 的安全对齐高度集中且可被极轻量的推理时修改精准绕过</strong>，为后续防御研究提供了量化依据与评估基准。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为“攻击侧深化”“防御侧构建”“理论侧解析”与“系统侧部署”四大类，供后续研究参考。</p>
<p>攻击侧深化</p>
<ol>
<li><strong>黑盒迁移</strong></li>
</ol>
<ul>
<li>利用公开 MoE 作为“代理模型”推断闭源 API 的专家激活模式，构建<strong>token-级专家规避</strong>提示，无需任何内部状态。</li>
<li>研究<strong>跨架构迁移</strong>（稀疏→混合→分组混合）是否仍保持神经元对应性。</li>
</ul>
<ol>
<li><strong>多轮对话场景</strong></li>
</ol>
<ul>
<li>将 GateBreaker 的神经元掩码<strong>持久化</strong>到多轮上下文，考察随着对话轮次增加，模型是否会逐渐“自我强化”有害输出。</li>
</ul>
<ol>
<li><strong>动态路由攻击</strong></li>
</ol>
<ul>
<li>不直接剪神经元，而是<strong>微调 gate logits</strong>让有害输入主动绕过安全专家，实现“软”绕过，进一步降低可检测性。</li>
</ul>
<ol>
<li><strong>专家级联故障</strong></li>
</ol>
<ul>
<li>探索当安全神经元被置零后，后续层专家是否出现<strong>级联激活漂移</strong>，从而发现更多“二级安全神经元”。</li>
</ul>
<p>防御侧构建</p>
<ol>
<li><strong>安全冗余训练</strong></li>
</ol>
<ul>
<li>在 RLHF/DPO 目标中显式加入<strong>专家多样性正则</strong>，强制≥N 个专家对同一有害提示都给出拒绝，削弱单点失效。</li>
</ul>
<ol>
<li><strong>专家级对抗训练</strong></li>
</ol>
<ul>
<li>针对 GateBreaker 的神经元掩码生成<strong>专家内对抗扰动</strong>，在训练阶段持续最小化被剪后的剩余拒绝概率，提升鲁棒性。</li>
</ul>
<ol>
<li><strong>运行时激活完整性校验</strong></li>
</ol>
<ul>
<li>在推理框架中插入<strong>轻量级 MAC</strong> 或<strong>校验和</strong>，对关键安全神经元激活值进行签名，检测是否被置零或异常抑制。</li>
</ul>
<ol>
<li><strong>可恢复安全掩码</strong></li>
</ol>
<ul>
<li>将安全神经元参数<strong>双份存储</strong>（主存+受控内存），一旦检测到异常激活下降，立即热切换回原始权重，实现自愈。</li>
</ul>
<p>理论侧解析</p>
<ol>
<li><strong>安全神经元与表示空间的对应关系</strong></li>
</ol>
<ul>
<li>利用字典学习或稀疏自动编码器，验证被剪神经元是否编码**“拒绝指令”或“危害类别”**的离散特征方向。</li>
</ul>
<ol>
<li><strong>路由-安全耦合度量化</strong></li>
</ol>
<ul>
<li>定义指标 $RSC = Var(g<em>(harm) - g</em>(benign)){mathbb E<br>g_(benign)<br>}$ 衡量 gate 分布对安全行为的敏感程度，预测模型易受 GateBreaker 攻击的程度。</li>
</ul>
<ol>
<li><strong>Scaling Law for Safety Sparsity</strong></li>
</ol>
<ul>
<li>系统改变专家总数、top-k 值与模型深度，观察“安全神经元占比 ∝ 参数总量”是否仍服从幂律，指导未来 MoE 的容量分配。</li>
</ul>
<p>系统侧部署</p>
<ol>
<li><strong>硬件-协同防护</strong></li>
</ol>
<ul>
<li>利用可信执行环境 (TEE) 把 gate 路由与专家激活计算放入 enclave，防止运行时钩子修改 logits 或激活值。</li>
</ul>
<ol>
<li><strong>专家级联邦监控</strong></li>
</ol>
<ul>
<li>在分布式推理场景下，每个设备上传<strong>专家激活直方图</strong>到中央审计节点，实时检测异常低激活的安全专家，触发模型下线。</li>
</ul>
<ol>
<li><strong>安全神经元热补丁</strong></li>
</ol>
<ul>
<li>设计“神经元级微补丁”机制，允许厂商像打安全补丁一样，仅替换或增量化微调被标识的 2 % 参数，而无需全量重新训练。</li>
</ul>
<p>以上探索点既可直接扩展 GateBreaker 的评估边界，也能从训练、推理、理论、系统多维度为 MoE 安全对齐建立更鲁棒的防线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>GateBreaker：首个针对 MoE 大模型的推理时、无训练、神经元级安全绕过框架</strong></p>
<ol>
<li><p>问题<br>现有 LLM 安全研究几乎只关注稠密模型；MoE 的稀疏-路由机制使安全对齐集中在极少数专家与神经元，形成新的单点失效风险，却未被系统评估。</p>
</li>
<li><p>方法（三阶段）<br>① 门控画像：统计有害提示下各专家被路由频率，选出“安全专家”。<br>② 专家定位：对比有害/良性激活，计算神经元级安全权重</p>
</li>
</ol>
<p>w<em>(l,j,n)= mathbb E</em>(harm)[v<em>(l,j)(q)_n]-mathbb E</em>(benign)[v_(l,j)(q)_n]</p>
<p>并取 z-score&gt;τ 的神经元作为“安全神经元”。<br>③ 靶向移除：推理时零训练、零梯度，仅把安全神经元激活置零，其余不动。</p>
<ol>
<li>结果</li>
</ol>
<ul>
<li>8 个主流 MoE（稀疏/混合/分组）平均 ASR 从 7.4 % → 64.9 %，仅改动 2.6 % 神经元，通用基准性能几乎不变。</li>
<li>同源模型一次性迁移 ASR 67.7 %；5 个 MoE-VLM 达 60.9 %。</li>
<li>对比唯一基线 SAFEx，ASR 提升 1.4×–12×。</li>
</ul>
<ol>
<li>结论<br>MoE 的安全对齐可被“极轻量、局部、推理时”精准瓦解；亟需针对稀疏架构的冗余与运行时防护机制。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是GateBreaker%3A%20Gate-Guided%20Attacks%20on%20Mixture-of-Expert%20LLMs，链接是https://arxiv.org/pdf/2512.21008，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.21008。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Lichao Wu,Sasha Behrouzi,Mohamadreza Rostami,Stjepan Picek,Ahmad-Reza Sadeghi</p>
<p><strong>Categories</strong>: cs.CR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2512.21008.pdf">https://arxiv.org/pdf/2512.21008.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2512.21008">https://arxiv.org/abs/2512.21008</a></p>
<p><strong>Arxiv ID</strong>: 2512.21008</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2512.21008">https://papers.cool/arxiv/2512.21008</a></p>
<p><strong>Published</strong>: 2025-12-24T07:13:24Z</p>
<p><strong>Updated</strong>: 2025-12-24T07:13:24.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202601/2026-01-01/">https://wdndev.github.io/daily/hf/202601/2026-01-01/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-UltraShape-1-0-High-Fidelity-3D-Shape-Generation-via-Scalable-Geometric-Refinement"><span class="toc-text">1. UltraShape 1.0: High-Fidelity 3D Shape Generation via Scalable Geometric Refinement</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-DreamOmni3-Scribble-based-Editing-and-Generation"><span class="toc-text">2. DreamOmni3: Scribble-based Editing and Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-End-to-End-Test-Time-Training-for-Long-Context"><span class="toc-text">3. End-to-End Test-Time Training for Long Context</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Evaluating-Parameter-Efficient-Methods-for-RLVR"><span class="toc-text">4. Evaluating Parameter Efficient Methods for RLVR</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-GraphLocator-Graph-guided-Causal-Reasoning-for-Issue-Localization"><span class="toc-text">5. GraphLocator: Graph-guided Causal Reasoning for Issue Localization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-GateBreaker-Gate-Guided-Attacks-on-Mixture-of-Expert-LLMs"><span class="toc-text">6. GateBreaker: Gate-Guided Attacks on Mixture-of-Expert LLMs</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>