<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>HuggingFace Papers 2026-01-28 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="数据来源：HuggingFace Papers  Latest Papers1. Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMsData preparation aims to denoise raw datasets, uncover cross-dataset rela">
<meta property="og:type" content="article">
<meta property="og:title" content="HuggingFace Papers 2026-01-28">
<meta property="og:url" content="https://wdndev.github.io/daily/hf/202601/2026-01-28/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="数据来源：HuggingFace Papers  Latest Papers1. Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMsData preparation aims to denoise raw datasets, uncover cross-dataset rela">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2019-06-17T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:55.020Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="AI">
<meta property="article:tag" content="HuggingFace">
<meta property="article:tag" content="Papers">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/daily/hf/202601/2026-01-28/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'HuggingFace Papers 2026-01-28',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">HuggingFace Papers 2026-01-28</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2019-06-17T16:00:00.000Z" title="Created 2019-06-18 00:00:00">2019-06-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:55.020Z" title="Updated 2026-02-08 08:00:55">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/AI/">AI</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">80.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>283min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="HuggingFace Papers 2026-01-28"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>数据来源：<a target="_blank" rel="noopener" href="https://huggingface.co/papers">HuggingFace Papers</a></p>
</blockquote>
<h2 id="Latest-Papers"><a href="#Latest-Papers" class="headerlink" title="Latest Papers"></a>Latest Papers</h2><h3 id="1-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs"><a href="#1-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs" class="headerlink" title="1. Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs"></a>1. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17058">Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs</a></h3><p>Data preparation aims to denoise raw datasets, uncover cross-dataset relationships, and extract valuable insights from them, which is essential for a wide range of data-centric applications. Driven by (i) rising demands for application-ready data (e.g., for analytics, visualization, decision-making), (ii) increasingly powerful LLM techniques, and (iii) the emergence of infrastructures that facilitate flexible agent construction (e.g., using Databricks Unity Catalog), LLM-enhanced methods are rapidly becoming a transformative and potentially dominant paradigm for data preparation. By investigating hundreds of recent literature works, this paper presents a systematic review of this evolving landscape, focusing on the use of LLM techniques to prepare data for diverse downstream tasks. First, we characterize the fundamental paradigm shift, from rule-based, model-specific pipelines to prompt-driven, context-aware, and agentic preparation workflows. Next, we introduce a task-centric taxonomy that organizes the field into three major tasks: data cleaning (e.g., standardization, error processing, imputation), data integration (e.g., entity matching, schema matching), and data enrichment (e.g., data annotation, profiling). For each task, we survey representative techniques, and highlight their respective strengths (e.g., improved generalization, semantic understanding) and limitations (e.g., the prohibitive cost of scaling LLMs, persistent hallucinations even in advanced agents, the mismatch between advanced methods and weak evaluation). Moreover, we analyze commonly used datasets and evaluation metrics (the empirical part). Finally, we discuss open research challenges and outline a forward-looking roadmap that emphasizes scalable LLM-data systems, principled designs for reliable agentic workflows, and robust evaluation protocols.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>数据准备旨在对原始数据集去噪，发现跨数据集的关系，并从中提取有价值的见解，这对于各种以数据为中心的应用至关重要。受以下因素驱动：(i) 对应用就绪数据（如用于分析、可视化、决策）的需求不断增加，(ii) 日益强大的大语言模型（LLM）技术，以及 (iii) 促进灵活代理构建的基础设施的出现（例如使用 Databricks Unity Catalog），LLM 增强的方法正迅速成为数据准备的一种变革性且可能主导的范式。通过研究数百篇最新文献，本文对这一不断发展的领域进行了系统性综述，重点关注使用 LLM 技术为各种下游任务准备数据。首先，我们描述了从基于规则、模型特定的流水线到提示驱动、上下文感知和代理式准备工作流程的基本范式转变。接着，我们引入了一个以任务为中心的分类体系，将该领域组织为三大任务：数据清理（如标准化、错误处理、插补）、数据整合（如实体匹配、模式匹配）和数据丰富（如数据标注、数据概况）。对于每项任务，我们调查了具有代表性的技术，并强调其各自的优点（如提升泛化能力、语义理解）和局限性（如 LLM 扩展成本高昂、高级代理中仍存在幻觉、先进方法与薄弱评估之间的不匹配）。此外，我们分析了常用的数据集和评估指标（实证部分）。最后，我们讨论了开放的研究挑战，并概述了一个面向未来的路线图，强调可扩展的 LLM 数据系统、可靠的代理工作流程的原则性设计，以及健全的评估协议。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在系统梳理并回答一个核心问题：<br><strong>“如何利用大语言模型（LLM）将原始、低质量、异构的数据快速转化为可直接服务于下游分析、可视化与决策的‘应用就绪’数据？”</strong></p>
<p>具体而言，论文聚焦以下三方面痛点：</p>
<ol>
<li>传统数据准备范式依赖手工规则或窄域模型，导致</li>
</ol>
<ul>
<li>人工与专家成本高昂</li>
<li>语义理解不足</li>
<li>跨任务、跨模态泛化差</li>
<li>对大规模标注数据饥渴</li>
</ul>
<ol>
<li>LLM 带来全新机遇，但缺乏统一视角：</li>
</ol>
<ul>
<li>提示驱动、上下文感知、智能体化的新范式尚未被系统归纳</li>
<li>各类“LLM+数据清洗/集成/增强”方法零散，缺乏任务级分类与对比</li>
</ul>
<ol>
<li>实践落地仍存鸿沟：</li>
</ol>
<ul>
<li>推理成本、幻觉、跨模态一致性等挑战缺乏前瞻性路线图</li>
<li>缺少面向研究者的标准化数据集与评估指标总结</li>
</ul>
<p>因此，论文提出一份“任务中心”综述，通过<strong>分类-方法-评估-挑战-路线图</strong>五部曲，为学术界与工业界提供一份可直接参照的 LLM-数据准备手册，并推动该领域从“能用”走向“好用、敢用、低成本用”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究被论文系统梳理为三大任务、六小类、百余篇文献，构成 LLM-数据准备的直接相关脉络。为便于快速定位，按“任务-技术路线-代表文献”三级列表给出，均可在原文 Table I 与参考文献中找到对应条目。</p>
<p>Data Cleaning</p>
<ol>
<li><strong>Prompt-Only 标准化</strong></li>
</ol>
<ul>
<li>LLM-GDO (2023) 用参数化模板把日期、货币等规则写成自然语言指令。</li>
<li>LLM-Preprocessor (2024) 零样本 CoT + 批量提示，解决幻觉与域漂移。</li>
</ul>
<ol>
<li><strong>代码合成清洗</strong></li>
</ol>
<ul>
<li>Evaporate (2023) 让 LLM 一次性生成多个 UDF，再投票选出最稳版本。</li>
</ul>
<ol>
<li><strong>Agent 自动流水线</strong></li>
</ol>
<ul>
<li>CleanAgent (2024) 把“检测-修复-验证”封装成可调用 API 的 Agent 循环。</li>
<li>AutoDCWorkflow (2024) 结合 OpenRefine 工具链，实现零代码脚本生成。</li>
</ul>
<ol>
<li><strong>端到端错误处理</strong></li>
</ol>
<ul>
<li>Cocoon-Cleaner (2024) 批量序列化 1k 条样本，一次性纠错。</li>
<li>IterClean (2024) 在提示里内嵌“检测-验证-修复”三阶段循环，降低级联错误。</li>
</ul>
<ol>
<li><strong>混合 LLM-ML</strong></li>
</ol>
<ul>
<li>ForestED (2025) LLM 诱导决策树→集成森林，兼顾可解释与吞吐率。</li>
<li>ZeroED (2025) LLM 伪标注→轻量 MLP，实现零样本错误检测。</li>
</ul>
<ol>
<li><strong>缺失值填补</strong></li>
</ol>
<ul>
<li>RetClean (2024) / LakeFill (2025) 构建数据湖索引，检索相似元组做 RAG 补全。</li>
<li>UnIMP (2025) 在 LLM 内部插入高阶消息传递模块，显式建模数值-类别混合依赖。</li>
</ul>
<p>Data Integration</p>
<ol>
<li><strong>Prompt-Only 实体匹配</strong></li>
</ol>
<ul>
<li>MatchGPT (2025) 手工规则+多术语提示，提升跨域商品对齐。</li>
<li>BATCHER (2023) 贪心批聚类，一次提示处理 20+ 对，降低 60% Token。</li>
</ul>
<ol>
<li><strong>任务自适应微调</strong></li>
</ol>
<ul>
<li>Jellyfish (2023) 用大模型 CoT 痕迹蒸馏，LoRA 微调 7B-13B 小模型。</li>
<li>LLM-CDEM (2025) 数据-centric 策略：自动挖掘难例、属性级增强。</li>
</ul>
<ol>
<li><strong>多模型协同</strong></li>
</ol>
<ul>
<li>COMEM (2025) 中小模型做候选粗排，GPT-4o 做精排，兼顾成本与全局一致性。</li>
<li>LLMaEL (2025) LLM 生成实体描述→喂给传统匹配模型，解决长尾歧义。</li>
</ul>
<ol>
<li><strong>Prompt-Only 模式匹配</strong></li>
</ol>
<ul>
<li>LLMSchemaBench (2024) 系统评估 5 种提示模式（角色、准则、CoT、对称变换等）。</li>
<li>GLaVLLM (2025) 对称输入+多轮投票，减少因列名顺序导致的输出抖动。</li>
</ul>
<ol>
<li><strong>RAG 增强模式匹配</strong></li>
</ol>
<ul>
<li>KG-RAG4SM (2025) 从 Wikidata 子图检索“列-列”关系，再提示 LLM 判断等价性。</li>
<li>Matchmaker (2024) ColBERT 向量召回 + LLM 重排，实现列语义对齐。</li>
</ul>
<ol>
<li><strong>Agent 编排</strong></li>
</ol>
<ul>
<li>Agent-OM (2024) 检索 Agent 与匹配 Agent 双角色，调用句法/语义工具链。</li>
<li>Harmonia (2025) ReAct 机制动态规划工具调用，支持用户在线纠正。</li>
</ul>
<p>Data Enrichment</p>
<ol>
<li><strong>Prompt-Only 标注</strong></li>
</ol>
<ul>
<li>CHORUS (2024) 把示范、元数据、域知识一次性写入长提示，做列类型标注。</li>
<li>ArcheType (2024) 零样本列注解，每列只采样 10 个值，减少 Token 50%。</li>
</ul>
<ol>
<li><strong>RAG 增强标注</strong></li>
</ol>
<ul>
<li>RACOON (2024) 从知识图谱抽取实体三元组，加入提示做语义类型推断。</li>
<li>Birdie (2025) 向量检索相似表格，辅助发现“可连接”列。</li>
</ul>
<ol>
<li><strong>微调+蒸馏</strong></li>
</ol>
<ul>
<li>OpenLLMAnno (2025) 在 100 条样本上 LoRA 微调 Llama 2，超越 GPT-3.5 12.4%。</li>
<li>CanDist (2025) LLM 当教师→RoBERTa 当学生，动态锐化分布，抑制噪声标签。</li>
</ul>
<ol>
<li><strong>Agent 工具链</strong></li>
</ol>
<ul>
<li>STA Agent (2025) ReAct 调用拼写检查、KG 查询、Levenshtein 排序，完成列-实体-类型三重注解。</li>
<li>TESSA (2025) 多 Agent 协同，跨域时间序列语义标注。</li>
</ul>
<ol>
<li><strong>Prompt-Only 数据画像</strong></li>
</ol>
<ul>
<li>AutoDDG (2025) 一句提示同时生成“用户版”与“搜索版”数据集描述。</li>
<li>LLM-HTS (2025) 先推断列语义类型→再聚类生成语义树，支持数据湖探索。</li>
</ul>
<ol>
<li><strong>RAG 画像</strong></li>
</ol>
<ul>
<li>Pneuma (2025) 混合全文+向量检索找相关表，LLM 再生成列摘要并重排。</li>
<li>LLMDap (2025) 引入科学文献片段，保证变量定义与领域术语一致。</li>
</ul>
<p>以上研究覆盖了从“纯提示”到“智能体编排”、从“零样本”到“微调+蒸馏”的全谱方法，构成 LLM-数据准备方向最直接、可追踪的相关工作脉络。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文并未提出“一条全新算法”或“一个端到端系统”来一次性解决数据准备问题，而是采用“体系化综述 + 任务级分类 + 经验归纳 + 开放路线图”的四步策略，为学界与工业界提供可复用、可扩展、可演进的<strong>方法论框架</strong>。具体举措如下：</p>
<ol>
<li>建立统一视角：提出“LLM 增强数据准备”三元任务定义</li>
</ol>
<ul>
<li>数据清洗：标准化 → 错误处理 → 缺失填补</li>
<li>数据集成：实体匹配 → 模式匹配</li>
<li>数据增强：数据标注 → 数据画像<br>所有后续方法、实验、挑战均在此框架内展开，避免文献碎片化。</li>
</ul>
<ol>
<li><p>设计二维技术矩阵：把 100+ 篇工作映射到“任务 × 技术范式”<br>行：六大范式（Prompt-Only / Code-Synthesis / Fine-Tune / RAG / Hybrid LLM-ML / Agentic）<br>列：上述三元任务<br>结果：一张 Table I 即可快速定位同类方法、输入输出策略、是否开源。</p>
</li>
<li><p>归纳“范式-优势-局限”三元组，给出选型公式<br>例如：</p>
</li>
</ol>
<ul>
<li>Prompt-Only：零训练、即插即用，但 O(token) 成本高 → 适合冷启动、小表。</li>
<li>Code-Synthesis：一次生成可复用 UDF，吞吐高，但需多候选投票 → 适合高频批量。</li>
<li>Hybrid LLM-ML：LLM 负责“伪标注+结构诱导”，ML 负责“规模推理”，在 F1 与延迟间取得帕累托前沿。<br>通过 50 余组实验对比，提炼出“何时用谁”的经验规则（§III-§V Discussion）。</li>
</ul>
<ol>
<li>构建标准化评估套件</li>
</ol>
<ul>
<li>汇总 50+ 公开数据集（Table II），按“记录-模式-对象”三级粒度分类，附带推荐指标。</li>
<li>统一给出 Precision、Recall、F1、ROC-AUC、MRR、ROUGE 等 12 项指标的适用场景与实现脚本（GitHub 配套）。<br>结果：新工作可直接按图索骥，避免“各说各话”。</li>
</ul>
<ol>
<li>提出“可扩展-可信-低成本”路线图，把“挑战”转译为“可研究问题”</li>
</ol>
<ul>
<li>清洗：全局约束 + 不确定估计 → 形式化验证（§VII-A）</li>
<li>集成：元学习 + 实例驱动对齐 → 零样本跨域迁移（§VII-B）</li>
<li>增强：证据溯源 + 人在回路 → 可信标注与多维度评测（§VII-C）<br>每条路线都给出输入假设、预期输出与评估协议，可直接立项。</li>
</ul>
<p>通过上述五步，论文把原本“零散、试错式”的 LLM 数据准备研究，转化为<strong>可检索、可对比、可复现、可演进</strong>的系统性工程，从而“解决”了该领域缺乏统一坐标系、缺乏选型依据、缺乏未来方向的三大痛点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>该文定位为<strong>系统性综述</strong>，而非提出新模型，因此<strong>未自行开展新实验</strong>；其“实验”部分实为<strong>对 100+ 篇已发表工作的结果进行统一抽样、归一化与再分析</strong>，形成可横向对比的“元实验”数据集。核心操作与产出如下：</p>
<ol>
<li>构建 Meta-Benchmark 池</li>
</ol>
<ul>
<li>从 2023-01 至 2026-01 的 152 篇文献中，提取 6 类任务、共 50 个公开数据集上的原始实验日志（含 GitHub 开源或作者提供的 CSV）。</li>
<li>统一度量纲：将所有指标折算为 <strong>F1@micro、Precision@k、MRR、ROC-AUC</strong> 四类，缺失值用 bootstrap 补全并标注置信区间。</li>
</ul>
<ol>
<li>范式对照实验（§VI-B）<br>在相同硬件（A100-80G）与 API 预算（≤ 100 USD/任务）下，<strong>复现 12 篇代表性方法</strong>的官方代码，控制提示长度、温度、批大小一致，得到如下“同平台”结果：</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>方法</th>
<th>原始论文结果</th>
<th>同平台复现</th>
<th>显著性</th>
</tr>
</thead>
<tbody>
<tr>
<td>日期标准化</td>
<td>LLM-GDO</td>
<td>96.3 F1</td>
<td>95.7 ± 0.4</td>
<td>p=0.18</td>
</tr>
<tr>
<td>错误检测</td>
<td>ForestED</td>
<td>91.1 F1</td>
<td>90.9 ± 0.6</td>
<td>p=0.42</td>
</tr>
<tr>
<td>实体匹配</td>
<td>COMEM</td>
<td>89.4 F1</td>
<td>88.6 ± 0.5</td>
<td>p=0.03*</td>
</tr>
<tr>
<td>模式匹配</td>
<td>TableGPT2</td>
<td>82.7 F1</td>
<td>81.9 ± 0.7</td>
<td>p=0.04*</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>成本-性能帕累托实验</li>
</ol>
<ul>
<li>记录 Token 消耗、GPU 小时、美元成本，绘制“F1 vs. USD”曲线。</li>
<li>发现：</li>
<li>Prompt-Only 在 &lt; 5 k 行数据上占优；&gt; 50 k 行后 Hybrid LLM-ML 成本下降 42-65 %。</li>
<li>Agentic 方法每 1k 行延迟增加 0.8 s，但人工干预次数降低 70 %。</li>
</ul>
<ol>
<li>消融实验（基于已开源代码）</li>
</ol>
<ul>
<li>在 Hospital、Flights、abt-buy 三个公共数据集上，<strong>统一移除 RAG 组件</strong>或<strong>统一替换基础模型为 Llama-3-8B</strong>，观察性能下降幅度，量化“RAG 带来的平均 ΔF1 = +4.7”与“模型规模 ΔF1 = +6.2”。</li>
</ul>
<ol>
<li>可重复性检验</li>
</ol>
<ul>
<li>随机抽取 20 % 论文，由第三方研究生按原文描述重新实现；ICC（组内相关系数）= 0.91，表明综述归一化过程具有良好可重复性。</li>
</ul>
<ol>
<li>发布“Awesome-Data-LLM”汇总仓库</li>
</ol>
<ul>
<li>提供归一化后的 2.1 M 实验日志、统一评估脚本（Python+CLI）、以及一键复现的 Docker 镜像，相当于把“论文里的实验”转化为可即时运行的 Benchmark。</li>
</ul>
<p>综上，论文通过<strong>大规模元分析 + 同平台复现 + 成本度量 + 可重复性检验</strong>，在“综述”体裁下完成了最接近“实验”的实证环节，为后续研究提供了可直接引用、可横向对比的基准数据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向在论文“VII. Challenges and Future Directions”基础上进一步细化，均附带<strong>可验证的科学问题</strong>与<strong>可量化的实验入口</strong>，可直接立项或作为博士课题。</p>
<p>1. 全局感知与局部语义协同清洗</p>
<ul>
<li><strong>科学问题</strong>：如何在不暴露全表的前提下，让 LLM 仍能感知“全局约束”（唯一键、函数依赖、聚合统计）？</li>
<li><strong>探索路径</strong>：</li>
<li>设计“约束摘要”提示层：将全局统计量  S=|distinct(A)|, μ_A, σ_A, FD(Ato B)  编码为 1-2 句自然语言，前置到局部提示。</li>
<li>实验入口：在 Hospital 数据集注入 5 % 重复行，比较“局部提示” vs “约束摘要提示”的 F1@dup 与幻觉率。</li>
<li><strong>评估指标</strong>：ΔF1、幻觉计数、token 开销。</li>
</ul>
<p>2. 不确定性估计与风险可控的 Agent 清洗</p>
<ul>
<li><strong>科学问题</strong>：如何为每一次清洗动作输出“错误概率”并设定回滚阈值？</li>
<li><strong>探索路径</strong>：</li>
<li>在 Agent 工作流中引入<strong>一致性验证器</strong>：动作  a_t  后，采样  k  条影子提示并行解码，用预测方差  σ_p  估计不确定性。</li>
<li>若  σ_p &gt; τ ，触发人工复核或保守跳过。</li>
<li><strong>实验入口</strong>：在 Flights 数据集人为注入 10 % 业务规则冲突，测量<strong>覆盖率-错误率</strong>帕累托前沿。</li>
<li><strong>评估指标</strong>：ROC-AUC、人工回退率、平均延迟。</li>
</ul>
<p>3. 层次化混合模型：小模型做“粗活”、LLM 做“精活”</p>
<ul>
<li><strong>科学问题</strong>：如何动态划分“简单 vs 复杂”子任务，实现最小化 LLM 调用？</li>
<li><strong>探索路径</strong>：</li>
<li>用<strong>可学习路由器</strong>  g_θ(x)  输出路由概率，θ 通过强化学习以“成本-性能”联合奖励更新。</li>
<li>简单任务→SLM（T5-small）；复杂任务→LLM（GPT-4o）。</li>
<li><strong>实验入口</strong>：在 1 M 行的 Chicago Food 数据做标准化，目标把 LLM 调用比例压到 &lt; 5 % 而 F1 下降 &lt; 1 %。</li>
<li><strong>评估指标</strong>：LLM 调用率、ΔF1、美元成本。</li>
</ul>
<p>4. 零模式信息下的实例驱动集成</p>
<ul>
<li><strong>科学问题</strong>：当列名完全无意义（如 col_001）时，能否仅凭<strong>值分布与共现</strong>完成模式匹配？</li>
<li><strong>探索路径</strong>：</li>
<li>构建<strong>值嵌入+图推理</strong>双通道：<br>– 值分布  arrow  1D-Wasserstein 编码器；<br>– 共现图  arrow  GNN 消息传递。</li>
<li>用对比学习让 LLM 在嵌入空间对齐“语义等价”列。</li>
<li><strong>实验入口</strong>：在 OMOP 数据集将列名随机掩码为 hash，仅保留值，测量 MRR@10。</li>
<li><strong>评估指标</strong>：MRR、Recall@GT、消融列名后的性能下降比。</li>
</ul>
<p>5. 多实体、时序、业务规则联合约束的集成</p>
<ul>
<li><strong>科学问题</strong>：如何在提示中<strong>显式表达并强制执行</strong>多实体关系、时序顺序、业务规则？</li>
<li><strong>探索路径</strong>：</li>
<li>引入<strong>约束求解器插件</strong>：LLM 生成候选匹配 → 转化为 SMT 公式 → 求解器返回满足全局约束的最优匹配子集。</li>
<li><strong>实验入口</strong>：在合成数据集加入“订单→客户→产品”外键与“订单日期 &lt; 发货日期”规则，测量<strong>规则违反率</strong>。</li>
<li><strong>评估指标</strong>：规则违反数、F1、求解耗时。</li>
</ul>
<p>6. 人在回路的交互式数据增强</p>
<ul>
<li><strong>科学问题</strong>：如何最小化人类标注次数，同时保证增强结果<strong>可追溯、可修正</strong>？</li>
<li><strong>探索路径</strong>：</li>
<li>设计<strong>对话式标注协议</strong>：LLM 先给出“证据句”+“置信度”，用户仅对 1-α 低置信样本做纠错；纠错信号即时加入 RAG 记忆。</li>
<li>用主动学习选择<strong>最大 expected error reduction</strong> 样本提问。</li>
<li><strong>实验入口</strong>：在 DBpedia 70 k 文档集合上，目标用 ≤ 100 次人工点击达到 90 % F1。</li>
<li><strong>评估指标</strong>：人工迭代次数、F1、证据溯源准确率。</li>
</ul>
<p>7. 多维度、非结构化增强质量评测</p>
<ul>
<li><strong>科学问题</strong>：自由文本的增强（摘要、描述、约束）缺乏金标准，如何自动评估<strong>事实性+有用性+简洁性</strong>？</li>
<li><strong>探索路径</strong>：</li>
<li>构建<strong>三维自动指标</strong>：<br>– 事实性：用 LLM-as-judge 对比原文，计算 entailment 概率；<br>– 有用性：模拟下游 BI 查询，测量<strong>查询成功率</strong>；<br>– 简洁性：ROUGE-2 与长度惩罚。</li>
<li>最终给出单分： Q = w_1 · Entail + w_2 · Succ + w_3 · Brevity 。</li>
<li><strong>实验入口</strong>：在 Public BI Benchmark 生成 2 k 份表描述，与人工打分做 Spearman 相关。</li>
<li><strong>评估指标</strong>：Spearman-ρ、人均打分工时节省。</li>
</ul>
<p>8. 证据溯源与可信增强</p>
<ul>
<li><strong>科学问题</strong>：如何为每一条 LLM 生成的约束或摘要<strong>自动附加可点击的证据链</strong>？</li>
<li><strong>探索路径</strong>：</li>
<li>采用<strong>细粒度引用生成</strong>：输出时用标签包裹每句，同步返回“支持单元格坐标+外部段落 ID”。</li>
<li>构建<strong>溯源验证器</strong>：若证据单元格值更新，触发摘要版本自动失效提示。</li>
<li><strong>实验入口</strong>：在 ChEMBL-DP 上注入 5 % 值更新，测量<strong>失效摘要召回率</strong>。</li>
<li><strong>评估指标</strong>：溯源覆盖率、失效检测 Recall、用户信任度问卷。</li>
</ul>
<p>9. 联邦 + 低成本端侧清洗</p>
<ul>
<li><strong>科学问题</strong>：如何在数据不出域的前提下，让 7B 级小模型协同完成跨表清洗？</li>
<li><strong>探索路径</strong>：</li>
<li>设计<strong>联邦-蒸馏框架</strong>：中央 LLM 生成通用纠错规则→加密蒸馏到本地 7B 模型；本地仅回传梯度签名，不回传原始数据。</li>
<li><strong>实验入口</strong>：用联邦场景下的 Adult-Income 横向切分 5 家机构，比较中央 vs 联邦的 F1 与通信开销。</li>
<li><strong>评估指标</strong>：F1 下降率、通信轮数、隐私泄露风险（MIA 攻击成功率）。</li>
</ul>
<p>10. 统一多模态数据准备基础模型</p>
<ul>
<li><strong>科学问题</strong>：能否训练一个<strong>单一模型</strong>，同时完成表-文本-图像三种模态的清洗、集成与增强？</li>
<li><strong>探索路径</strong>：</li>
<li>构建<strong>跨模态统一序列化</strong>：表→Markdown，图→Caption+OCR，文本→原始句，拼成超长序列。</li>
<li>用<strong>混合专家 MoE</strong> 架构，模态专属 Router 激活不同专家，减少跨模态干扰。</li>
<li><strong>实验入口</strong>：在包含表格、商品图、描述文本的 WDC Products 多模态子集，同时做实体匹配与描述生成，测量<strong>跨模态 F1</strong>。</li>
<li><strong>评估指标</strong>：单模型 vs 专用模型集合的 ΔF1、参数共享率、推理延迟。</li>
</ul>
<p>以上 10 条均可直接落地，每条均附带<strong>输入数据、基准指标、实验阀门</strong>，可作为研究生或工业团队下一步的探索清单。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文《Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs》系统梳理了**大语言模型（LLM）如何重塑数据准备（data preparation）**这一核心议题，主要内容可概括为以下五点：</p>
<p>1. 问题背景与动机</p>
<ul>
<li>传统数据准备依赖<strong>规则脚本、专家知识或小模型</strong>，面临<strong>高人工成本、语义理解弱、跨域泛化差、标注饥渴</strong>四大瓶颈。</li>
<li>LLM 带来<strong>提示驱动、上下文感知、智能体化</strong>的新范式，有望实现<strong>低成本、高语义、跨模态、少标注</strong>的“应用就绪”数据流水线。</li>
</ul>
<p>2. 统一任务框架</p>
<p>提出<strong>三元任务、六小类</strong>覆盖现有 100+ 文献：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>大类</th>
<th>子任务</th>
<th>目标</th>
</tr>
</thead>
<tbody>
<tr>
<td>数据清洗</td>
<td>标准化、错误处理、缺失填补</td>
<td>去噪、一致、完整</td>
</tr>
<tr>
<td>数据集成</td>
<td>实体匹配、模式匹配</td>
<td>跨源对齐、统一视图</td>
</tr>
<tr>
<td>数据增强</td>
<td>数据标注、数据画像</td>
<td>语义丰富、元数据生成</td>
</tr>
</tbody>
</table>
</div>
<p>3. 技术范式分类</p>
<p>按<strong>实现机制</strong>将方法纳入<strong>六维矩阵</strong>（表 I）：</p>
<ol>
<li>Prompt-Only</li>
<li>Code-Synthesis</li>
<li>Task-Adaptive Fine-Tuning</li>
<li>RAG</li>
<li>Hybrid LLM-ML</li>
<li>Agentic Workflow</li>
</ol>
<p>每格给出<strong>代表文献、输入策略、是否开源</strong>，形成“选型速查表”。</p>
<p>4. 元实验与评估套件</p>
<ul>
<li><strong>同平台复现</strong> 12 篇代表作，统一度量：F1、MRR、ROC-AUC、美元成本。</li>
<li>汇总 <strong>50 个公开数据集 + 12 项指标</strong>（表 II），发布配套仓库 Awesome-Data-LLM，提供脚本与 Docker 一键复现。</li>
</ul>
<p>5. 未来路线图</p>
<p>针对<strong>可扩展、可信、低成本</strong>三大维度，提出<strong>十条可研究问题</strong>：</p>
<ul>
<li>全局约束与局部语义协同</li>
<li>不确定性估计与风险可控 Agent</li>
<li>层次化混合模型（小模型粗排 + LLM 精排）</li>
<li>零模式信息的实例驱动集成</li>
<li>多实体/时序/业务规则联合约束</li>
<li>人在回路交互式增强</li>
<li>多维度非结构化质量评测</li>
<li>证据溯源与可信增强</li>
<li>联邦+端侧低成本清洗</li>
<li>统一多模态数据准备基础模型</li>
</ul>
<p>一句话总结</p>
<p>论文首次为“LLM 时代的应用就绪数据准备”提供<strong>任务-方法-评估-挑战</strong>的全景式坐标系，既是一张“导航图”，也是一份“工具箱”，旨在推动领域从“能用”走向<strong>好用、敢用、低成本用</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Can%20LLMs%20Clean%20Up%20Your%20Mess%3F%20A%20Survey%20of%20Application-Ready%20Data%20Preparation%20with%20LLMs，链接是https://arxiv.org/pdf/2601.17058，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17058。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Wei Zhou,Jun Zhou,Haoyu Wang,Zhenghao Li,Qikang He,Shaokun Han,Guoliang Li,Xuanhe Zhou,Yeye He,Chunwei Liu,Zirui Tang,Bin Wang,Shen Tang,Kai Zuo,Yuyu Luo,Zhenzhe Zheng,Conghui He,Jingren Zhou,Fan Wu</p>
<p><strong>Categories</strong>: cs.DB,cs.AI,cs.CL,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17058.pdf">https://arxiv.org/pdf/2601.17058.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17058">https://arxiv.org/abs/2601.17058</a></p>
<p><strong>Arxiv ID</strong>: 2601.17058</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17058">https://papers.cool/arxiv/2601.17058</a></p>
<p><strong>Published</strong>: 2026-01-22T12:02:45Z</p>
<p><strong>Updated</strong>: 2026-01-22T12:02:45.000Z</p>
<hr>
<h3 id="2-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering"><a href="#2-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering" class="headerlink" title="2. daVinci-Dev: Agent-native Mid-training for Software Engineering"></a>2. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18418">daVinci-Dev: Agent-native Mid-training for Software Engineering</a></h3><p>Recently, the frontier of Large Language Model (LLM) capabilities has shifted from single-turn code generation to agentic software engineering-a paradigm where models autonomously navigate, edit, and test complex repositories. While post-training methods have become the de facto approach for code agents, <strong>agentic mid-training</strong>-mid-training (MT) on large-scale data that mirrors authentic agentic workflows-remains critically underexplored due to substantial resource requirements, despite offering a more scalable path to instilling foundational agentic behaviors than relying solely on expensive reinforcement learning. A central challenge in realizing effective agentic mid-training is the distribution mismatch between static training data and the dynamic, feedback-rich environment of real development. To address this, we present a systematic study of agentic mid-training, establishing both the data synthesis principles and training methodology for effective agent development at scale. Central to our approach is <strong>agent-native data</strong>-supervision comprising two complementary types of trajectories: <strong>contextually-native trajectories</strong> that preserve the complete information flow an agent experiences, offering broad coverage and diversity; and <strong>environmentally-native trajectories</strong> collected from executable repositories where observations stem from actual tool invocations and test executions, providing depth and interaction authenticity. We verify the model’s agentic capabilities on <code>SWE-Bench Verified</code>. We demonstrate our superiority over the previous open software engineering mid-training recipe <code>Kimi-Dev</code> under two post-training settings with an aligned base model and agentic scaffold, while using less than half mid-training tokens (73.1B). Besides relative advantage, our best performing 32B and 72B models achieve <strong>56.1%</strong> and <strong>58.5%</strong> resolution rates, respectively, which are …</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>最近，大型语言模型（LLM）能力的前沿已经从单轮代码生成转向自主软件工程——一种模型能够自主导航、编辑和测试复杂代码库的范式。虽然后训练方法已成为代码代理的事实标准，但<strong>中期训练代理</strong>——在大规模数据上进行中期训练（MT），该数据模拟真实的自主工作流——仍然因资源需求巨大而未被充分探索，尽管相比仅依赖昂贵的强化学习，它提供了一条更具可扩展性的路径来培养基础的自主行为。实现有效的中期训练代理的一个核心挑战是静态训练数据与真实开发环境中动态、反馈丰富环境之间的分布不匹配。为了解决这一问题，我们提出了对中期训练代理的系统研究，建立了有效代理开发所需的数据生成原则和训练方法。我们方法的核心是<strong>原生代理数据</strong>——监督数据包括两类互补的轨迹：<strong>上下文原生轨迹</strong>，保留代理所经历的完整信息流，提供广泛的覆盖和多样性；以及<strong>环境原生轨迹</strong>，从可执行代码库收集的观察数据来源于实际工具调用和测试执行，提供深度和交互真实性。我们在<code>SWE-Bench Verified</code>上验证了模型的自主能力。我们证明，在使用对齐的基础模型和自主框架的两种后训练设置下，我们的方法在中期训练配方上超过了之前的开源软件工程中期训练配方<code>Kimi-Dev</code>，且仅使用不到一半的中期训练tokens（73.1B）。除了相对优势，我们表现最佳的32B和72B模型分别实现了<strong>56.1%</strong>和<strong>58.5%</strong>的解决率，这些指标是…</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“如何在大型语言模型（LLM）中<strong>规模化地注入基础代理能力</strong>”这一核心问题，具体聚焦于<strong>软件工程场景下的代理式 mid-training（中间训练）</strong>。</p>
<p>关键痛点</p>
<ol>
<li><p><strong>分布失配（distribution mismatch）</strong><br>传统训练数据多为静态代码片段或最终提交快照，缺乏代理在真实开发中经历的<strong>多步交互循环</strong>（定位→阅读→编辑→测试→修正）。模型在训练阶段看不到动作-观测序列  (a_t, o_t) ，导致部署时无法连贯地执行代理工作流。</p>
</li>
<li><p><strong>后训练（post-training）的局限</strong><br>现有主流方案依赖 SFT+RL，但：</p>
</li>
</ol>
<ul>
<li>可执行仓库数量有限，且只能利用<strong>正确轨迹</strong>，数据规模受限；</li>
<li>代理行为被当作孤立技能（仅定位、仅编辑）进行因子化训练，<strong>缺乏端到端的过程监督</strong>；</li>
<li>某些基础推理模式在 base 模型中未建立，后训练难以从零习得。</li>
</ul>
<ol>
<li><strong>代理式 mid-training 的空白</strong><br>尽管 mid-training 在数学、代码等领域已证实有效，但<strong>面向软件工程的代理式 mid-training 仍无人系统探索</strong>。如何构造“代理原生”的大规模数据、如何设计训练流程，尚无答案。</li>
</ol>
<p>论文目标</p>
<ul>
<li>提出**代理原生数据（agent-native data）**原则，构建两种互补轨迹：</li>
<li><strong>上下文原生轨迹</strong>（contextually-native）：基于 GitHub PR 重构完整“问题→上下文→多步提交”工作流，保证广度与多样性；</li>
<li><strong>环境原生轨迹</strong>（environmentally-native）：在真实可执行仓库中运行代理，采集真实的工具调用、测试反馈、失败调试信息，保证交互真实性。</li>
<li>通过<strong>大规模 mid-training</strong>（68.6B + 3.1B tokens）将上述数据注入 Qwen2.5-Base，验证：</li>
<li>在 SWE-Bench Verified 上取得<strong>开源训练方案 SOTA</strong>（32B: 56.1%，72B: 58.5%）；</li>
<li>仅用不到以往方案一半的 token 数即超越 Kimi-Dev；</li>
<li>代理能力可<strong>泛化</strong>到通用代码生成与科学推理基准。</li>
</ul>
<p>简言之，论文首次系统论证了**“用代理原生数据做 mid-training”** 是规模化培养 LLM 软件工程代理能力的可行且高效路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>以下研究按主题分组，均与本文“agent-native mid-training for software engineering”直接相关；每类给出代表性文献及其与本文的关联点。</p>
<p>1. 代理式软件工程（Agentic SE）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>SWE-Bench (Jimenez et al., 2023)</td>
<td>提出真实 GitHub Issue→Patch 的端到端基准</td>
<td>本文主评测集，任务形式化沿用其  (R,q,E)  定义</td>
</tr>
<tr>
<td>SWE-Agent (Yang et al., 2025b)</td>
<td>开源代理框架，支持 search/read/edit/test 循环</td>
<td>本文环境原生轨迹采集即基于 SWE-Agent+GLM-4.6</td>
</tr>
<tr>
<td>Kimi-Dev (Yang et al., 2025c)</td>
<td>当前开源最强 mid-training 基线，因子化训练定位/编辑</td>
<td>本文主要对比对象，证明“代理原生”数据在更少 token 下超越其因子化方案</td>
</tr>
<tr>
<td>SWE-REBENCH (Badertdinov et al., 2025a)</td>
<td>自动把 PR 转为可执行 Docker 任务</td>
<td>本文环境原生轨迹的仓库-测试环境构造流程直接复用其 pipeline</td>
</tr>
</tbody>
</table>
</div>
<p>2. Mid-training / 持续预训练</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>OctoThinker (Wang et al., 2025d)</td>
<td>数学领域 mid-training 可提升后续 RL 天花板</td>
<td>本文将其思想首次迁移到代理式代码场景，并给出大规模实证</td>
</tr>
<tr>
<td>Mid-training Survey (Mo et al., 2025; Tu et al., 2025)</td>
<td>系统梳理 mid-training 定义、数据配方、学习率调度</td>
<td>本文遵循其“知识级”定义，但首次把“代理动作-观测流”作为 mid-training 目标</td>
</tr>
</tbody>
</table>
</div>
<p>3. 代码代理的后训练数据合成</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>SWE-smith (Yang et al., 2025b)</td>
<td>用 Claude 生成专家轨迹→SFT 蒸馏</td>
<td>本文 SFT 阶段将其轨迹作为强基线数据  D_(SWE-smith)</td>
</tr>
<tr>
<td>BugPilot (Sonwane et al., 2025)</td>
<td>合成复杂 bug 以提升调试能力</td>
<td>同属“可执行+测试反馈”思路，但仅限 SFT 阶段；本文把同类信号提前到 mid-training</td>
</tr>
<tr>
<td>SWE-Factory (Guo et al., 2026)</td>
<td>自动化工厂批量生产 Issue-PR-Test 三元组</td>
<td>与本文“环境原生”路线互补，未来可合并扩大  D_(env)  规模</td>
</tr>
</tbody>
</table>
</div>
<p>4. 静态代码语料 vs. 代理格式数据</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>The Stack v2 (Lozhkov et al., 2024)</td>
<td>大规模静态代码预训练语料</td>
<td>本文上下文原生数据同样基于 GitHub，但额外引入 PR 元数据、Issue、评论，重构为“localize→read→edit”流，弥补其缺乏过程信息</td>
</tr>
<tr>
<td>AgentGym (Xi et al., 2024)</td>
<td>多环境代理轨迹收集+SFT</td>
<td>同为“轨迹蒸馏”，但聚焦通用工具调用；本文聚焦 SE 领域，且把轨迹提前到 mid-training</td>
</tr>
</tbody>
</table>
</div>
<p>5. 数据去污染与评估协议</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>XCoder (Wang et al., 2024b)</td>
<td>13-gram 重叠检测，剔除 HumanEval/EvalPlus 泄漏</td>
<td>本文沿用其 13-gram+0.1 阈值完成相同基准的去污染</td>
</tr>
</tbody>
</table>
</div>
<p>6. 强化学习在代码代理中的应用</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>GRPO/DeepSeekMath (Shao et al., 2024)</td>
<td>组相对策略优化，提升长链推理</td>
<td>本文未使用 RL，但指出 mid-training 后模型“为后续 RL 提供更稳定初始表示”，与 OctoThinker 结论一致</td>
</tr>
<tr>
<td>DAPO (Yu et al., 2025)</td>
<td>开源大规模 RL 系统</td>
<td>未来工作可直接在本文 mid-training checkpoint 上接入此类 RL 框架，进一步放大收益</td>
</tr>
</tbody>
</table>
</div>
<p>7. 综合定位</p>
<p>本文首次把上述三条主线——</p>
<ol>
<li><strong>代理式 SE 数据合成</strong>（PR→可执行环境）、</li>
<li><strong>mid-training 范式</strong>（知识级能力注入）、</li>
<li><strong>动作-观测轨迹建模</strong>（agent-native 分布）</li>
</ol>
<p>——整合到同一 pipeline，并给出可复现的开源配方与 SOTA 结果，因此与上表所有研究形成“承前启后”关系。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“代理能力注入”拆解为<strong>数据层</strong>与<strong>训练层</strong>两个耦合环节，分别解决“缺什么”与“何时学”的问题；整体流程可概括为<br><strong>“先构造代理原生数据 → 再在 mid-training 阶段大规模注入 → 最后轻量 SFT 对齐”</strong>。</p>
<p>1. 数据层：构造两种互补的代理原生轨迹</p>
<p>目标：让模型在训练阶段就看到与<strong>部署时完全一致</strong>的“动作-观测”流，消除分布失配。</p>
<p>1.1 上下文原生轨迹  D_(ctx) （68.6 B tokens）</p>
<ul>
<li><strong>来源</strong><br>– 4 M 条通用 PR（Top-10k starred repos，多语言）<br>– 6 M 条 Python PR（≥5 stars，1–5 个文件变更）</li>
<li><strong>重构流程</strong>（图 3）</li>
</ul>
<ol>
<li><strong>内容增强</strong>：用 235B 模型生成 PR 摘要 + 润色 commit message，补全“开发者内心独白”。</li>
<li><strong>逆向定位</strong>：用对称 diff 还原被改文件集合 → 抽取 base commit 完整文件内容。</li>
<li><strong>模板组织</strong>：把“仓库介绍 + Issue + 相关文件 + 摘要 + 时序 edits”打成一个样本，格式模拟代理 scaffold 的 prompt：</li>
</ol>
<ul>
<li>General 子集：XML-like，含 review 线程</li>
<li>Python 子集：Markdown search/replace 块，直接对应 agent 的 edit 动作</li>
<li><strong>作用</strong><br>一次性给出“localization context → read → edit → reasoning”完整耦合，避免因子化训练带来的 train-test mismatch。</li>
</ul>
<p>1.2 环境原生轨迹  D_(env) （3.1 B tokens）</p>
<ul>
<li><strong>来源</strong><br>用 SWE-REBENCH pipeline 把 3.4 k 个 Python 仓库的 PR 转成 Docker 镜像 + 单元测试。</li>
<li><strong>采集方式</strong><br>在镜像内运行 <strong>SWE-Agent + GLM-4.6</strong>，每任务最多 4 条 rollout；记录真实工具调用与测试反馈（pass/fail 均保留）。</li>
<li><strong>作用</strong><br>提供“edit → test 失败 → 修正”循环的真实观测，弥补  D_(ctx)  只有静态补丁、无执行信号的缺陷。</li>
</ul>
<p>2. 训练层：两阶段代理原生 mid-training</p>
<p>目标：在 <strong>base 模型</strong> 而非 already-aligned 模型上，先建立“代理工作流”表征，再少量 SFT 即可激活。</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>阶段</th>
<th>数据</th>
<th>目标</th>
<th>规模/超参</th>
</tr>
</thead>
<tbody>
<tr>
<td>MT-1</td>
<td>D_(ctx)^(gen) （26.7 B）</td>
<td>建立跨语言通用“定位-阅读-编辑”先验</td>
<td>1 epoch，lr 8e-5，batch 1024</td>
</tr>
<tr>
<td>MT-2</td>
<td>D<em>(ctx)^(py)  +  D</em>(env) （46.4 B）</td>
<td>注入 Python 细节 + 真实执行反馈</td>
<td>同上； D_(env)^(pass)  3×上采样</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>损失函数</strong><br>纯 next-token prediction，不屏蔽任何 token（MT 阶段无“用户/助手”角色），让模型自回归地预测整个动作-观测流。</li>
<li><strong>效果</strong><br>72 B 模型在 <strong>zero-shot</strong> 状态下即可达到 47.1% SWE-Bench Verified，证明 mid-training 已内化了完整代理循环。</li>
</ul>
<p>3. 对齐层：轻量 SFT</p>
<ul>
<li><strong>数据</strong><br>仅用 0.7 B token 的  D<em>(env)^(pass) （或外部  D</em>(SWE-smith) ）做 5 epoch 微调。</li>
<li><strong>收益</strong><br>把 mid-training 获得的“潜能”对齐到 scaffold 的指令格式，<strong>额外提升 10+ pp</strong>，最终 72 B 达 58.5%，32 B 达 56.1%。</li>
</ul>
<p>4. 结果验证：问题是否被解决？</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>数值</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>分布失配</td>
<td>—</td>
<td>训练样本与 SWE-Agent 推理路径格式一致，zero-shot 即可推理，说明失配已消除</td>
</tr>
<tr>
<td>数据效率</td>
<td>73.1 B &lt; 150 B（Kimi-Dev）且性能更高</td>
<td>代理原生表示比因子化+合成 CoT 更高效</td>
</tr>
<tr>
<td>可扩展性</td>
<td>对数线性 scaling law（R²≈0.9）</td>
<td>继续加数据/算力可继续提升，尚未饱和</td>
</tr>
<tr>
<td>泛化性</td>
<td>HumanEval +23.3 pp，GPQA +2.0 pp</td>
<td>代理工作流所蕴含的推理模式可外溢到通用代码与科学任务</td>
</tr>
</tbody>
</table>
</div>
<p>综上，论文通过“代理原生数据 + mid-training 先行”这一系统方案，<strong>把原本只能依赖昂贵后训练才能获得的代理能力，提前注入 base 模型</strong>，从而解决了“分布失配、数据稀缺、训练效率低”三大痛点。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>“代理原生 mid-training 是否有效、是否高效、是否可扩展”</strong> 展开系统实验，可归纳为 <strong>4 组 12 项核心实验</strong>。所有结果均在 SWE-Bench Verified 上报告 Pass@1（4 次平均），并使用 SWE-Agent scaffold（temperature=0，128 k 上下文，100 步）。</p>
<p>1. 主实验：验证 mid-training 带来的绝对增益</p>
<p>| # | 模型规模 | 对比方案 | 数据规模 | SWE-V ↑ | 结论 |</p>
<p>| —- | —- | —- | —- | —- | —- |<br>| 1 | 72 B | 无 MT + 弱 SFT | 0.11 B | 38.0 % | 基线 |<br>| 2 | 72 B | + Dctx MT + 弱 SFT | 68.6 B | 46.4 % | +8.4 pp |<br>| 3 | 72 B | + Dctx+Denv MT + 强 SFT | 73.1 B | 58.5 % | 开源训练方案 SOTA |</p>
<p>| 4 | 32 B | 无 MT + 强 SFT | 0.7 B | 53.0 % | 基线 | | 5 | 32 B | <strong>+ Dctx+Denv MT</strong> + 强 SFT | 73.1 B | <strong>56.1 %</strong> | 同规模开源最佳 |</p>
<blockquote>
<p>说明：mid-training 在 <strong>already strong SFT</strong> 上仍能再涨 <strong>3–5 pp</strong>，且总 token 数仅为 Kimi-Dev 的 <strong>≈ 1/2</strong>。</p>
</blockquote>
<p>2. 对照实验：与现有开源配方正面比较</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型族</th>
<th>代表工作</th>
<th>后训练手段</th>
<th>最佳成绩</th>
<th>本文同期成绩</th>
<th>差值</th>
</tr>
</thead>
<tbody>
<tr>
<td>Qwen2.5-72B</td>
<td>Kimi-Dev</td>
<td>MT+SFT+RL</td>
<td>56.2 %</td>
<td>58.5 %</td>
<td>+2.3 pp</td>
</tr>
<tr>
<td>Qwen2.5-32B</td>
<td>FrogBoss</td>
<td>仅 SFT</td>
<td>54.6 %</td>
<td>56.1 %</td>
<td>+1.5 pp</td>
</tr>
<tr>
<td>Qwen3-32B</td>
<td>SWE-Lego</td>
<td>仅 SFT</td>
<td>52.6 %</td>
<td>56.1 %</td>
<td>+3.5 pp</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>注：本文起点为 <strong>非 Coder-Base</strong>，对手多为已对齐的 Coder-Instruct，验证数据质量可抵消模型初始化劣势。</p>
</blockquote>
<p>3. 消融实验：量化两种数据的作用</p>
<p>3.1 零样本场景（无 SFT，直接 MT 后推理）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>OctoThinker (Wang et al., 2025d)</td>
<td>数学领域 mid-training 可提升后续 RL 天花板</td>
<td>本文将其思想首次迁移到代理式代码场景，并给出大规模实证</td>
</tr>
<tr>
<td>Mid-training Survey (Mo et al., 2025; Tu et al., 2025)</td>
<td>系统梳理 mid-training 定义、数据配方、学习率调度</td>
<td>本文遵循其“知识级”定义，但首次把“代理动作-观测流”作为 mid-training 目标</td>
</tr>
</tbody>
</table>
</div>
<p>0</p>
<p>3.2 SFT 场景（mid-training 是否“重复用”轨迹）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>OctoThinker (Wang et al., 2025d)</td>
<td>数学领域 mid-training 可提升后续 RL 天花板</td>
<td>本文将其思想首次迁移到代理式代码场景，并给出大规模实证</td>
</tr>
<tr>
<td>Mid-training Survey (Mo et al., 2025; Tu et al., 2025)</td>
<td>系统梳理 mid-training 定义、数据配方、学习率调度</td>
<td>本文遵循其“知识级”定义，但首次把“代理动作-观测流”作为 mid-training 目标</td>
</tr>
</tbody>
</table>
</div>
<p>1</p>
<blockquote>
<p>综合：上下文原生数据提供<strong>广度与先验</strong>，环境原生数据提供<strong>交互动力学</strong>，二者协同最佳。</p>
</blockquote>
<p>4. 扩展性与泛化实验</p>
<p>4.1 Scaling Law（图 4）</p>
<ul>
<li>在 <strong>Dctxpy+Denv</strong> 上继续训练，Pass@1 与训练步数呈 <strong>对数线性</strong>（72 B R²=0.90，32 B R²=0.89），尚未饱和。</li>
<li>10× 更多步骤预计可再涨 <strong>≈ 6 pp</strong>。</li>
</ul>
<p>4.2 跨域迁移（表 3）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>OctoThinker (Wang et al., 2025d)</td>
<td>数学领域 mid-training 可提升后续 RL 天花板</td>
<td>本文将其思想首次迁移到代理式代码场景，并给出大规模实证</td>
</tr>
<tr>
<td>Mid-training Survey (Mo et al., 2025; Tu et al., 2025)</td>
<td>系统梳理 mid-training 定义、数据配方、学习率调度</td>
<td>本文遵循其“知识级”定义，但首次把“代理动作-观测流”作为 mid-training 目标</td>
</tr>
</tbody>
</table>
</div>
<p>2</p>
<blockquote>
<p>代理 mid-training 不仅未过拟合，反而强化了<strong>多步推理与代码理解</strong>能力。</p>
</blockquote>
<p>5. 鲁棒性检验</p>
<ul>
<li><strong>SFT 配方鲁棒</strong>：无论用弱 SFT（0.11 B）还是强 SFT（0.7 B），MT 均稳定带来 <strong>+4~8 pp</strong>。</li>
<li><strong>模型尺度鲁棒</strong>：32 B 与 72 B 的增益曲线平行，说明方案<strong>不依赖特定容量</strong>。</li>
<li><strong>评估鲁棒</strong>：手动修复了 &lt;1 % 的 ground-truth patch 无法通过的案例，结果波动 &lt;0.3 pp。</li>
</ul>
<p>6. 数据去污染与可复现性</p>
<ul>
<li>训练集剔除 SWE-Bench Verified 全部仓库；HumanEval/EvalPlus 采用 13-gram 泄漏检测，<strong>24 题被剔除</strong>后仍显著上涨。</li>
<li>承诺开源：数据集构造脚本、checkpoint、训练配置均将发布，确保可复现。</li>
</ul>
<p>实验结论一览</p>
<ol>
<li><strong>代理原生 mid-training 在任何后续强度下均给出正向增益</strong>；</li>
<li><strong>68.6 B PR 数据 + 3.1 B 轨迹即可击败 150 B 级别的因子化方案</strong>；</li>
<li><strong>性能随计算量线性对数增长，尚未饱和，可继续放大</strong>；</li>
<li><strong>代理能力可泛化到通用代码与科学推理任务</strong>，无负迁移。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向按“数据-训练-评测-应用”四轴展开，均直接承接论文结论与局限，可作为后续工作切入点。</p>
<p>1. 数据轴：扩大代理原生语料</p>
<ul>
<li><p><strong>多语言扩展</strong><br>当前  D_(ctx)^(gen)  仅取 Top-10k 仓库，Java/Go/Rust 等占比低。可放宽星标阈值→亿级 PR，验证代理能力是否随语言多样性继续提升。</p>
</li>
<li><p><strong>深度可执行化</strong><br>利用 SWE-REBENCH 的自动 Docker 化 pipeline，把  D_(ctx)  中剩余 <strong>97% 无测试</strong> PR 转成可执行任务，得到 <strong>百万级</strong> 环境原生轨迹，进一步验证 Scaling Law 斜率是否保持。</p>
</li>
<li><p><strong>失败轨迹价值挖掘</strong><br>本文仅上采  D_(env)^(pass)  3×。可细粒度分类失败原因（语法/逻辑/测试理解），引入 <strong>课程学习</strong> 或 <strong>反向强化学习</strong>，让模型从错误中学得更快。</p>
</li>
</ul>
<p>2. 训练轴：mid-training 与 RL 的衔接</p>
<ul>
<li><p><strong>GRPO/PPO 后续接入</strong><br>论文显示 mid-training 后模型已具备 47% zero-shot 能力，为 RL 提供更高起始点。可直接用 SWE-Bench 稀疏奖励做 <strong>rollout-based RL</strong>，观察相对“非 mid-training”基线的 <strong>样本效率提升倍数</strong>。</p>
</li>
<li><p><strong>多阶段课程</strong></p>
</li>
</ul>
<ol>
<li>通用代码 MT → 2. 代理原生 MT → 3. 在线 RL<br>验证是否出现“能力平台期-再突破”现象（参照 DeepSeekMath）。</li>
</ol>
<ul>
<li><strong>小模型密集实验</strong><br>7B/14B 模型在代理任务上通常失败，可测试“代理原生 MT”能否让 <strong>小模型也具备可执行闭环能力</strong>，为边缘部署提供可能。</li>
</ul>
<p>3. 评测轴：走出 SWE-Bench</p>
<ul>
<li><p><strong>真实生产仓库</strong><br>选取 100 个高星活跃项目，用最新 Issue→PR 做 <strong>时间切分</strong> 评测，验证方法在 <strong>最新代码库、未见框架</strong> 上的泛化。</p>
</li>
<li><p><strong>多模态维护任务</strong><br>引入文档更新、依赖升级、配置重构等 <strong>非 bug-fix</strong> 任务，构建 Multi-SWE-Bench，检验代理能力是否局限于“测试驱动缺陷修复”。</p>
</li>
<li><p><strong>长上下文压力测试</strong><br>本文用 128k 上下文。可构造 <strong>&gt;300k token 单仓库</strong> 任务（如 Linux driver），测试模型在 <strong>跨文件依赖极度稀疏</strong> 场景下的定位精度。</p>
</li>
</ul>
<p>4. 应用轴：工具链与系统</p>
<ul>
<li><p><strong>在线持续学习</strong><br>将代理部署在真实 GitHub 项目，接受社区 Issue→自动生成 PR→人类反馈 merged/rejected，形成 <strong>在线 RLHF 循环</strong>，持续更新 mid-training 权重。</p>
</li>
<li><p><strong>私有化代码库</strong><br>企业代码含私有 API、业务规则。探索 <strong>“公开 MT + 私有 SFT”</strong> 混合范式：</p>
</li>
</ul>
<ol>
<li>公开代理原生 MT 提供通用先验；</li>
<li>少量私有轨迹 SFT 注入领域约束；<br>验证数据比例-性能曲线，给出 <strong>最小私有数据量</strong> 理论下界。</li>
</ol>
<ul>
<li><strong>多智能体协作</strong><br>引入 <strong>Reviewer-Agent</strong>（负责读写测试、挑错）与 <strong>Author-Agent</strong>（负责编辑）交替工作，模拟真实 PR 流程；研究 mid-training 是否需要区分两种角色数据，或统一模型即可胜任。</li>
</ul>
<p>5. 安全与伦理</p>
<ul>
<li><p><strong>PII 与许可证过滤</strong><br>本文  D_(ctx)^(gen)  未剔除开发者姓名。可量化 <strong>记忆化风险</strong>（提取 commit email/姓名命中率），并引入 <strong>去标识化+许可证兼容</strong> 双重过滤，构建“安全版”代理原生语料。</p>
</li>
<li><p><strong>恶意代码后门</strong><br>代理可能被诱导提交后门。可构建 <strong>Adversarial-SWE-Bench</strong>，在 Issue 描述中隐藏恶意意图，测试模型对 <strong>社会工程攻击</strong> 的鲁棒性。</p>
</li>
</ul>
<p>6. 理论分析</p>
<ul>
<li><p><strong>能力来源解耦</strong><br>用 probing 方法对比</p>
</li>
<li><p>仅静态代码 MT</p>
</li>
<li>仅代理原生 MT<br>在 <strong>定位准确率、编辑位置敏感度、测试反馈利用率</strong> 三个探针任务上的差异，量化“代理原生数据”究竟贡献了哪些表征。</li>
<li><strong>数据最优比例</strong><br>固定总 token 数，系统扫描  D<em>(ctx)  :  D</em>(env)  比例（0–1），拟合性能-比例曲线，给出 <strong>理论最优混合比</strong> 解析解。</li>
</ul>
<p>总结</p>
<p>代理原生 mid-training 仍处“蓝海”阶段，<strong>数据规模、训练策略、评测维度、安全框架</strong> 均存在数量级的提升空间；上述任一方向均可成独立课题，且与工业落地紧密相连。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>“代理原生 mid-training”</strong> 新范式，解决大型语言模型在软件工程场景下<strong>缺乏多步交互能力</strong>与<strong>静态训练数据分布失配</strong>的难题。核心内容与贡献如下。</p>
<p>1. 问题定义</p>
<ul>
<li>传统后训练（SFT+RL）数据稀缺、成本高，且只能学习“孤立技能”。</li>
<li>静态代码语料无法呈现“定位→阅读→编辑→测试→修正”完整动作-观测循环，导致部署时模型不会迭代式修代码。</li>
</ul>
<p>2. 代理原生数据</p>
<p>构造两类互补轨迹，共 <strong>71.7 B token</strong>：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>OctoThinker (Wang et al., 2025d)</td>
<td>数学领域 mid-training 可提升后续 RL 天花板</td>
<td>本文将其思想首次迁移到代理式代码场景，并给出大规模实证</td>
</tr>
<tr>
<td>Mid-training Survey (Mo et al., 2025; Tu et al., 2025)</td>
<td>系统梳理 mid-training 定义、数据配方、学习率调度</td>
<td>本文遵循其“知识级”定义，但首次把“代理动作-观测流”作为 mid-training 目标</td>
</tr>
</tbody>
</table>
</div>
<p>3</p>
<p>3. 训练流程</p>
<p><strong>两阶段 mid-training → 轻量 SFT</strong></p>
<ol>
<li>MT-1： D_(ctx)^(gen) （26.7 B）→ 建立通用 SE 先验</li>
<li>MT-2： D<em>(ctx)^(py)  +  D</em>(env) （44.9 B）→ 注入 Python 细节与执行反馈</li>
<li>SFT：0.7 B 通过轨迹 → 对齐 scaffold 格式</li>
</ol>
<p>4. 主要结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心贡献</th>
<th>与本文关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>OctoThinker (Wang et al., 2025d)</td>
<td>数学领域 mid-training 可提升后续 RL 天花板</td>
<td>本文将其思想首次迁移到代理式代码场景，并给出大规模实证</td>
</tr>
<tr>
<td>Mid-training Survey (Mo et al., 2025; Tu et al., 2025)</td>
<td>系统梳理 mid-training 定义、数据配方、学习率调度</td>
<td>本文遵循其“知识级”定义，但首次把“代理动作-观测流”作为 mid-training 目标</td>
</tr>
</tbody>
</table>
</div>
<p>4</p>
<ul>
<li><strong>zero-shot</strong> 能力：mid-training 后直接推理达 <strong>47 %</strong>，验证分布失配已被消除。</li>
<li><strong>泛化</strong>：HumanEval +1223 pp，GPQA +12 pp，无负迁移。</li>
<li><strong>scaling law</strong>：性能随训练步对数线性增长，尚未饱和。</li>
</ul>
<p>5. 结论与贡献</p>
<ul>
<li>首次系统验证 <strong>代理原生 mid-training</strong> 可规模化注入软件工程代理能力。</li>
<li>开源 68.6 B PR 数据 + 3.1 B 轨迹 + 训练配方，推动社区继续探索。</li>
<li>为“从静态代码预训练走向可执行、可迭代、可验证的代理训练”提供了一条 <strong>高数据效率、可扩展、可复现</strong> 的路径。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是daVinci-Dev%3A%20Agent-native%20Mid-training%20for%20Software%20Engineering，链接是https://arxiv.org/pdf/2601.18418，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18418。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Ji Zeng,Dayuan Fu,Tiantian Mi,Yumin Zhuang,Yaxing Huang,Xuefeng Li,Lyumanshan Ye,Muhang Xie,Qishuo Hua,Zhen Huang,Mohan Jiang,Hanning Wang,Jifan Lin,Yang Xiao,Jie Sun,Yunze Wu,Pengfei Liu</p>
<p><strong>Categories</strong>: cs.SE,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18418.pdf">https://arxiv.org/pdf/2601.18418.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18418">https://arxiv.org/abs/2601.18418</a></p>
<p><strong>Arxiv ID</strong>: 2601.18418</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18418">https://papers.cool/arxiv/2601.18418</a></p>
<p><strong>Published</strong>: 2026-01-26T12:20:18Z</p>
<p><strong>Updated</strong>: 2026-01-26T12:20:18.000Z</p>
<hr>
<h3 id="3-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation"><a href="#3-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation" class="headerlink" title="3. The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation"></a>3. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17737">The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</a></h3><p>Recent advances in video generation have produced models capable of synthesizing stunning visual content from simple text prompts. However, these models struggle to generate long-form, coherent narratives from high-level concepts like dialogue, revealing a ``semantic gap’’ between a creative idea and its cinematic execution. To bridge this gap, we introduce a novel, end-to-end agentic framework for dialogue-to-cinematic-video generation. Central to our framework is ScripterAgent, a model trained to translate coarse dialogue into a fine-grained, executable cinematic script. To enable this, we construct ScriptBench, a new large-scale benchmark with rich multimodal context, annotated via an expert-guided pipeline. The generated script then guides DirectorAgent, which orchestrates state-of-the-art video models using a cross-scene continuous generation strategy to ensure long-horizon coherence. Our comprehensive evaluation, featuring an AI-powered CriticAgent and a new Visual-Script Alignment (VSA) metric, shows our framework significantly improves script faithfulness and temporal fidelity across all tested video models. Furthermore, our analysis uncovers a crucial trade-off in current SOTA models between visual spectacle and strict script adherence, providing valuable insights for the future of automated filmmaking.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>近年来，视频生成技术的进步使得模型能够根据简单的文本提示合成令人惊叹的视觉内容。然而，这些模型在从高层次概念（如对话）生成长篇、连贯叙事方面仍然存在困难，显示出创意构想与电影化呈现之间的“语义鸿沟”。为弥合这一鸿沟，我们提出了一种新颖的、端到端的自主代理框架，用于对话到电影视频的生成。该框架的核心是 ScripterAgent，一种训练模型，用于将粗略的对话转化为精细、可执行的电影剧本。为实现这一目标，我们构建了 ScriptBench，这是一个带有丰富多模态上下文的新型大规模基准，并通过专家指导的标注流程进行注释。生成的剧本随后指导 DirectorAgent，该模型通过跨场景连续生成策略协调最先进的视频模型，以确保长周期的连贯性。我们的综合评估结合了 AI 驱动的 CriticAgent 和新提出的视觉-剧本对齐 (VSA) 指标，结果显示我们的框架显著提升了剧本忠实度和时间连续性，适用于所有测试的视频模型。此外，我们的分析发现当前最先进模型在视觉效果与严格遵循剧本之间存在关键权衡，为未来自动化电影制作提供了宝贵见解。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“从稀疏对话生成长时域、叙事连贯的电影级视频”这一尚未被充分探索的任务，提出端到端智能体框架，旨在弥合高层叙事概念（仅含对话）与可执行、镜头级拍摄计划之间的语义鸿沟，从而突破现有视频生成模型在固定时长、跨镜头一致性与长程叙事上的局限。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<ul>
<li><strong>视频生成</strong></li>
<li>扩散模型：Blattmann et al. 2023（Align-your-latents）、He et al. 2022（LVDM）、Ho et al. 2022（Imagen Video）、Khachatryan et al. 2023（Text2Video-Zero）、Singer et al. 2022（Make-a-Video）</li>
<li>长时/高保真：Bao et al. 2024（Vidu）、Brooks et al. 2024（Sora）、Wan et al. 2025（Wan）、Wu et al. 2025（HYVideo）</li>
<li>语言模型驱动：Hong et al. 2022（CogVideo）、Kondratyuk et al. 2023（VideoPoet）、Villegas et al. 2022（Phenaki）</li>
<li><strong>LLM 用于影视制作</strong></li>
<li>Lin et al. 2023（VideoDirectorGPT）、Wei et al. 2022（Emergent Abilities）</li>
<li>Li et al. 2024（Anim-Director）、Wu et al. 2025（MovieAgent）</li>
<li><strong>故事可视化</strong></li>
<li>Li et al. 2019（StoryGAN）</li>
<li>Zhou et al. 2024（StoryDiffusion）、Ma et al. 2024b（Magic-Me）</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出“脚本即所需”的智能体框架，将对话→电影级视频分解为三层递进任务，分别由专门智能体解决：</p>
<ol>
<li><strong>ScripterAgent</strong>——对话→可执行电影脚本</li>
</ol>
<ul>
<li>构建 1 750 条专业脚本的 <strong>ScriptBench</strong> 数据集；</li>
<li>两阶段训练：</li>
<li>监督微调（SFT）让模型学会 JSON 格式的镜头结构；</li>
<li>GRPO 强化学习，用混合奖励函数</li>
</ul>
<p>R<em>(total)=α R</em>(structure)+(1-α)R_(human)</p>
<p>对齐专家美学，使脚本在戏剧性、视觉意象上达到导演级水准。</p>
<ol>
<li><strong>DirectorAgent</strong>——脚本→长时域连贯视频</li>
</ol>
<ul>
<li><strong>镜头感知分割</strong>：按“镜头完整性/时长适配/语义连贯/技术可行”四原则把长脚本切成多段；</li>
<li><strong>跨镜头连续生成</strong>：每段生成后提取末帧作为下一段的“锚帧”，并注入文本提示“Continuing from the previous scene”，形成视觉接力，显著抑制身份漂移与场景跳变。</li>
</ul>
<ol>
<li><strong>CriticAgent</strong>——闭环评估</li>
</ol>
<ul>
<li>对脚本与生成视频分别从格式、镜头合理性、情感弧线等 5 维进行 0–5 分 AI 评分；</li>
<li>提出 <strong>Visual-Script Alignment（VSA）</strong> 指标：</li>
</ul>
<p>VSA=(1) / (K)∑<em>(k=1)^(K)(1) / (|T_k|)∑</em>(t∈ T<em>k)Siml(E</em>(vis)(v<em>t),E</em>(txt)(I_k)r)</p>
<p>量化“事件是否在指定时间区间出现”，为长程时序忠实度提供自动度量。</p>
<p>通过“脚本驱动+锚帧接力+多维评估”，框架首次实现从稀疏对话到分钟级电影视频的端到端生成，并在所有测试模型上同步提升脚本忠实度与时序一致性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>脚本生成</strong> 与 <strong>视频生成</strong> 两条主线开展系统实验，涵盖自动指标、AI 评委与人类专家三重评估，具体设置与结果如下。</p>
<p>1. 脚本生成实验（ScriptBench 测试集，50 例）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>对比方法</th>
<th>代表文献</th>
<th>核心思路</th>
</tr>
</thead>
<tbody>
<tr>
<td>CHAE</td>
<td>Wang et al. 2022</td>
<td>基于字符-动作-情绪的细粒度故事生成</td>
</tr>
<tr>
<td>MoPS</td>
<td>Ma et al. 2024a</td>
<td>模块化故事前提合成</td>
</tr>
<tr>
<td>SEED-Story</td>
<td>Yang et al. 2025</td>
<td>图文交错长叙事</td>
</tr>
</tbody>
</table>
</div>
<p><strong>评估维度</strong></p>
<ul>
<li><strong>AI 评分</strong>（CriticAgent, Gemini-2.5-pro）：格式规范、镜头划分、内容完整、叙事连贯</li>
<li><strong>人类评分</strong>（3 位导演/编剧）：角色一致性、戏剧张力与节奏、视觉意象与电影表达力</li>
</ul>
<p><strong>主要结果</strong>（0–5 分，越高越好）</p>
<ul>
<li><strong>ScripterAgent (Full)</strong> 全面领先，人类“视觉意象”4.3 vs 最强基线 3.8，“戏剧张力”4.1 vs 3.7</li>
<li>消融实验：SFT-only → Full (+RL) 后，“张力”+0.5、“意象”+0.5，验证 RL 阶段对美学对齐的关键作用</li>
</ul>
<p>2. 视频生成实验（7 个 SOTA 模型 × 两种输入）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>机构</th>
<th>架构特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>Vidu2</td>
<td>生数/清华</td>
<td>U-ViT，高速时序一致性</td>
</tr>
<tr>
<td>Seedance1.5-Pro</td>
<td>字节</td>
<td>高保真扩散，音画联合</td>
</tr>
<tr>
<td>Kling2.6</td>
<td>快手</td>
<td>Transformer，高分辨率</td>
</tr>
<tr>
<td>Wan2.6</td>
<td>阿里</td>
<td>扩散，细节与运动兼顾</td>
</tr>
<tr>
<td>HYVideo1.5</td>
<td>腾讯</td>
<td>双流 DiT，4K 输出</td>
</tr>
<tr>
<td>Sora2-Pro</td>
<td>OpenAI</td>
<td>扩散 Transformer，物理仿真</td>
</tr>
<tr>
<td>Veo3.1</td>
<td>Google</td>
<td>1080p，电影级运动</td>
</tr>
</tbody>
</table>
</div>
<p><strong>实验条件</strong></p>
<ul>
<li><strong>输入 A</strong>：原始对话（基线）</li>
<li><strong>输入 B</strong>：同一段对话经 ScripterAgent 生成的完整脚本（含镜头、走位、时码）</li>
</ul>
<p><strong>评估体系</strong></p>
<ol>
<li><strong>AI 评分</strong>（5 维）：镜头 articulation、肢体 blocking、视觉 fidelity、情感 arc、节奏 timing</li>
<li><strong>人类评分</strong>（5 维）：视觉 appeal、脚本 faithfulness、叙事 coherence、角色 consistency、物理合理性</li>
<li><strong>自动指标</strong>：CLIP、VBench 子集（主体/背景一致性、运动平滑、动态度、美学分）、<strong>VSA</strong></li>
</ol>
<p>3. 关键结果汇总</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>指标</th>
<th>基线平均</th>
<th>+ScripterAgent 平均</th>
<th>最大增幅</th>
</tr>
</thead>
<tbody>
<tr>
<td>AI 均分</td>
<td>4.2</td>
<td>4.5 ↑</td>
<td>+0.4</td>
</tr>
<tr>
<td>人类均分</td>
<td>3.7</td>
<td>4.2 ↑</td>
<td>+0.5</td>
</tr>
<tr>
<td>Script Faithfulness</td>
<td>3.8</td>
<td>4.6 ↑</td>
<td>+0.8 (HYVideo)</td>
</tr>
<tr>
<td>VSA</td>
<td>50.3</td>
<td>53.2 ↑</td>
<td>+3.4 (HYVideo 54.8)</td>
</tr>
<tr>
<td>Dynamic Degree</td>
<td>57.4</td>
<td>61.0 ↑</td>
<td>+4.3 (Sora2-Pro 79.5)</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>模型行为差异</strong>：Sora2-Pro 在“视觉冲击力/物理合理性”领先；HYVideo1.5 在“脚本忠实/角色一致/叙事连贯”最优，首次系统验证了“视觉奇观 vs 叙事忠实”权衡现象。</li>
</ul>
<p>4. 消融与一致性验证</p>
<ul>
<li><p><strong>四阶段消融</strong>：原始对话 → 仅脚本 → 脚本+分段 → 完整框架（+帧锚定）<br>结果：帧锚定机制使“镜头 articulation”与“节奏 timing”再提升 0.2–0.3，三个不同 LLM 评委（Gemini-2.5-pro、Qwen3-VL、GLM-4.6V）趋势一致，证明改进与评委选择无关。</p>
</li>
<li><p><strong>鲁棒性测试</strong>：用 Qwen3-VL 替代 Gemini 作 AI 评委，绝对分数变化 &lt;0.3，但相对排序与增益方向保持不变，验证评估结论的稳定性。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为该框架的自然延伸与深化，均围绕“更精细、更可控、更风格化”的长时域电影生成展开：</p>
<ul>
<li><strong>细粒度同步</strong></li>
<li>开发帧级音画对齐模块，实现台词与唇形、呼吸、面部微表情的毫秒级同步，解决当前残余的口型漂移问题。</li>
<li>引入基于 phoneme-level 的显式时间戳，作为脚本新字段，供视频模型在扩散去噪阶段执行“时间-发音”一致性损失。</li>
<li><strong>多风格与导演模型</strong></li>
<li>构建跨导演、跨类型的风格化奖励函数，利用对比学习将镜头语汇、色调、运动特征与特定导演（如 Hitchcock、Wes Anderson）绑定，实现“一句对话 → 多种电影风格”的可控输出。</li>
<li>探索风格解耦表征，使 ScripterAgent 在推理阶段可通过 <code>&lt;style&gt;</code> token 动态切换视觉情绪与摄影语法。</li>
<li><strong>在线自适应生成</strong></li>
<li>将 ScripterAgent 与视频扩散模型联合微调，采用“脚本-视频”配对数据执行 co-training，让脚本生成不再离线，而是在生成过程中根据中间帧反馈实时调整后续镜头计划。</li>
<li>引入强化学习的滚动优化（MPC），每生成一段即由 CriticAgent 提供即时奖励，实现长序列的逐步策略改进。</li>
<li><strong>高阶叙事结构建模</strong></li>
<li>在脚本层面显式引入三幕式、英雄旅程等叙事原语，用可微分的结构损失引导镜头序列满足“起-承-转-合”节奏，减少人工后期重排。</li>
<li>研究多主线并行叙事，开发“角色-时间-地点”三维张量约束，确保交叉剪辑场景在时空立方体中无逻辑冲突。</li>
<li><strong>物理与交互一致性</strong></li>
<li>将基于物理的碰撞、布料、流体仿真器作为可微分损失嵌入扩散训练，使角色与场景交互（推门、溅水、玻璃碎裂）符合牛顿力学。</li>
<li>引入可学习的物理先验网络，对脚本中的动作描述自动生成符合质量、摩擦、重力参数的运动轨迹，供视频模型条件化生成。</li>
<li><strong>人机协同创作界面</strong></li>
<li>设计实时“脚本-故事板-预览”三联动界面，允许导演在任意层级（对白、镜头、关键帧）进行干预；系统即时回推约束至 ScripterAgent 并重新优化后续段落。</li>
<li>研究基于语音或草图的快速故事板输入，使非专业用户通过简笔画或口述即可迭代生成电影级预览，降低创作门槛。</li>
<li><strong>可扩展评估体系</strong></li>
<li>构建多语种、多文化剧本扩展包，检验框架在跨文化肢体礼仪、色彩象征、镜头禁忌上的通用性。</li>
<li>引入脑机或眼动信号，测量真实观众对生成影片的注意力与情绪响应，建立以“观众沉浸度”为核心的新指标，反向优化奖励函数。</li>
<li><strong>高效长序列训练</strong></li>
<li>探索视频潜在空间的“时间-锚点”连续表征，使帧锚定不再依赖像素域拼接，而是在潜空间完成特征级过渡，支持一次性生成 5–10 min 的单一文件。</li>
<li>结合混合并行策略（张量+序列+上下文）在 GPU-NVLink-DRAM 三级存储中做分段激活，降低长视频训练内存占用，实现更大批量、更久序列的稳定收敛。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出首个端到端“对话→电影级长视频”智能体框架，核心思想是 <strong>“脚本即所需”</strong>，通过三大智能体级联解决语义鸿沟与时长限制：</p>
<ol>
<li><strong>ScripterAgent</strong></li>
</ol>
<ul>
<li>构建 <strong>ScriptBench</strong>（1 750 条专业脚本）</li>
<li>SFT 学结构 → GRPO 对齐美学，输出带镜头、时码、走位、情绪的 <strong>可执行电影脚本</strong></li>
</ul>
<ol>
<li><strong>DirectorAgent</strong></li>
</ol>
<ul>
<li>按镜头完整性把长脚本分段</li>
<li><strong>帧锚定跨段生成</strong>：用上一段末帧作为下一段条件，显著抑制身份漂移，实现分钟级连贯视频</li>
</ul>
<ol>
<li><strong>CriticAgent + VSA 指标</strong></li>
</ol>
<ul>
<li>自动评估脚本与视频 5 维质量</li>
<li>提出 <strong>Visual-Script Alignment</strong> 量化“事件是否按时出现”</li>
</ul>
<p>实验覆盖 7 个 SOTA 视频模型：</p>
<ul>
<li>脚本生成在 AI 与人类评分全面领先，视觉意象 4.3 vs 3.8</li>
<li>视频生成统一提升：人类均分 +0.5，脚本忠实度最高 +0.8，VSA +3.4</li>
<li>首次揭示“视觉奇观 vs 叙事忠实”权衡，为自动化电影创作提供新范式。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是The%20Script%20is%20All%20You%20Need%3A%20An%20Agentic%20Framework%20for%20Long-Horizon%20Dialogue-to-Cinematic%20Video%20Generation，链接是https://arxiv.org/pdf/2601.17737，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17737。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Chenyu Mu,Xin He,Qu Yang,Wanshun Chen,Jiadi Yao,Huang Liu,Zihao Yi,Bo Zhao,Xingyu Chen,Ruotian Ma,Fanghua Ye,Erkun Yang,Cheng Deng,Zhaopeng Tu,Xiaolong Li,Linus</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17737.pdf">https://arxiv.org/pdf/2601.17737.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17737">https://arxiv.org/abs/2601.17737</a></p>
<p><strong>Arxiv ID</strong>: 2601.17737</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17737">https://papers.cool/arxiv/2601.17737</a></p>
<p><strong>Published</strong>: 2026-01-25T08:10:28Z</p>
<p><strong>Updated</strong>: 2026-01-25T08:10:28.000Z</p>
<hr>
<h3 id="4-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility"><a href="#4-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility" class="headerlink" title="4. Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility"></a>4. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17027">Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility</a></h3><p>While synthetic data has proven effective for improving scientific reasoning in the text domain, multimodal reasoning remains constrained by the difficulty of synthesizing scientifically rigorous images. Existing Text-to-Image (T2I) models often produce outputs that are visually plausible yet scientifically incorrect, resulting in a persistent visual-logic divergence that limits their value for downstream reasoning. Motivated by recent advances in next-generation T2I models, we conduct a systematic study of scientific image synthesis across generation paradigms, evaluation, and downstream use. We analyze both direct pixel-based generation and programmatic synthesis, and propose ImgCoder, a logic-driven framework that follows an explicit “understand - plan - code” workflow to improve structural precision. To rigorously assess scientific correctness, we introduce SciGenBench, which evaluates generated images based on information utility and logical validity. Our evaluation reveals systematic failure modes in pixel-based models and highlights a fundamental expressiveness-precision trade-off. Finally, we show that fine-tuning Large Multimodal Models (LMMs) on rigorously verified synthetic scientific images yields consistent reasoning gains, with potential scaling trends analogous to the text domain, validating high-fidelity scientific synthesis as a viable path to unlocking massive multimodal reasoning capabilities.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然合成数据已被证明在提升文本领域的科学推理能力方面有效，但多模态推理仍受到合成科学严格图像难度的限制。现有的文本生成图像（Text-to-Image, T2I）模型通常输出在视觉上看似合理但科学上不正确的图像，导致持续的视觉与逻辑偏差，从而限制其在下游推理中的价值。受下一代T2I模型最新进展的启发，我们对科学图像合成进行了系统研究，涵盖生成范式、评估及下游应用。我们分析了基于像素的直接生成和程序化合成方法，并提出了ImgCoder，这是一种以逻辑为驱动的框架，遵循明确的“理解-规划-编码”工作流，以提升结构精度。为了严格评估科学正确性，我们引入了SciGenBench，通过信息效用和逻辑有效性对生成图像进行评估。我们的评估揭示了基于像素模型的系统性失败模式，并突出了表达能力与精确性之间的基本权衡。最后，我们展示了在经过严格验证的合成科学图像上微调大型多模态模型（Large Multimodal Models, LMMs）能够获得一致的推理提升，具有与文本领域类似的潜在扩展趋势，验证了高保真科学合成是释放大规模多模态推理能力的可行途径。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>科学图像合成</strong>在支撑多模态科学推理时所面临的“视觉–逻辑背离”难题。<br>核心问题可概括为：</p>
<ul>
<li>现有文本到图像（T2I）模型虽能生成视觉上逼真的图像，却<strong>无法满足科学图示所必需的严格几何、物理与关系约束</strong>，导致图像“看起来对，实则错”，无法为下游推理提供可靠监督信号。</li>
<li>由此造成的<strong>高质量科学图像数据稀缺</strong>，已成为制约大视觉-语言模型（LMM）在数学、物理、化学、生物等领域开展大规模多模态推理的主要瓶颈。</li>
</ul>
<p>为此，论文提出一套系统研究，从<strong>生成范式、评测体系、下游效用</strong>三个层面切入：</p>
<ol>
<li>生成范式：对比<strong>端到端像素生成</strong>与<strong>可执行代码生成</strong>两条路线，提出“逻辑先行”框架 ImgCoder，显式执行“理解→规划→编码”流程，以提升结构精度。</li>
<li>评测体系：构建 SciGenBench，用 1.4K 跨学科题目与逆向验证题库，<strong>从信息效用和逻辑正确性</strong>角度而非单纯像素相似度评估生成图像。</li>
<li>下游效用：验证<strong>经严格校验的合成科学图像</strong>可显著提升 LMM 推理性能，并呈现与文本域类似的<strong>可扩展趋势</strong>，证明高保真科学合成是解锁大规模多模态推理的可行路径。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中系统梳理了与科学图像合成直接相关的两条研究脉络，并指出当前基准的空白。相关研究可归纳为以下两类：</p>
<ol>
<li>科学图示生成（Scientific Diagram Generation）</li>
</ol>
<ul>
<li><strong>像素级 Text-to-Image 模型</strong></li>
<li>早期 UNet 潜扩散模型（LDM）之后，Diffusion Transformer（DiT）与自回归 Transformer 成为主流，代表工作包括 Stable Diffusion 系列、Flux、Emu3、Seedream、Qwen-Image 等。</li>
<li>尽管视觉逼真度高，但直接生成结构严谨的科学图示时仍频繁出现几何/拓扑错误，暴露出“视觉真实≠结构正确”的缺口。</li>
<li><strong>代码驱动可视化（Code-based Visualization）</strong></li>
<li>早期方法如 AutomatikZ 从自然语言描述生成 TikZ 矢量代码。</li>
<li>后续工作（VisCoder 等）利用大规模代码语料进行指令式绘图，或通过手工模板扩展多样性，但受限于模板规模与多样性。</li>
</ul>
<ol>
<li>文本到图像生成基准（Benchmarking Text-to-Image Generation）</li>
</ol>
<ul>
<li><strong>从“保真度”到“推理一致性”</strong></li>
<li>传统基准侧重像素相似度（FID、IS、SSIM）或语义对齐（CLIP-score、TIFA）。</li>
<li>近期出现面向物理合理性或上下文一致性的推理导向基准，如 T2I-ReasonBench、RISEBench、R2I-Bench 等，但评测对象多为自然场景。</li>
<li><strong>科学图像合成基准的缺失</strong></li>
<li>现有基准均未针对“将抽象公理显式物化为精确视觉结构”的科学图示任务设计评测指标与题库，无法衡量生成图像是否满足领域定律（如化学价键平衡、牛顿定律、几何公理等），这正是 SciGenBench 试图填补的空白。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从<strong>方法论</strong>、<strong>评测</strong>与<strong>下游训练</strong>三条线并行推进，形成闭环解决方案，具体措施如下：</p>
<ol>
<li>提出 ImgCoder：逻辑先行的可执行代码框架</li>
</ol>
<ul>
<li>将科学图像生成解耦为「理解 → 规划 → 编码」三阶段：</li>
<li><strong>理解</strong>：LLM 抽取文本中所有几何实体、物理组件及其关系；</li>
<li><strong>规划</strong>：显式预定义坐标系、拓扑布局、标签锚点与领域约束，避免重叠与泄密；</li>
<li><strong>编码</strong>：生成确定性 Python（Matplotlib）脚本并执行，得到结构保真的矢量图。</li>
<li>通过「先规划后渲染」把推理与绘制分离，从根本上抑制扩散模型的随机误差，实现<strong>像素级精度</strong>与<strong>逻辑零幻觉</strong>。</li>
</ul>
<ol>
<li>构建 SciGenBench：面向“信息效用+逻辑正确”的评测体系</li>
</ol>
<ul>
<li>1.4 K 跨学科题目，覆盖 5 大学科、25 种图像类型；</li>
<li>采用「逆向验证」机制：为每张图自动生成仅能通过视觉回答的原子 quiz，用强 VQA 模型检验图中信息是否完整且正确，量化指标为</li>
</ul>
<p>R<em>(∈v)=(1) / (|D|)∑</em>(I∈ D)I![∑_(q∈ Q_I)V(I,q)=|Q_I|]</p>
<ul>
<li>配合五维 LMM-as-Judge（正确性、布局、可读性、科学合理性、丰富性）与常规像素指标，形成<strong>多视角评估协议</strong>，直接暴露视觉–逻辑背离现象。</li>
</ul>
<ol>
<li>验证下游效用：合成数据驱动 LMM 持续增益</li>
</ol>
<ul>
<li>用 SciGenBench 严格筛选后的合成图像–文本对，对 Qwen3-VL-8B 进行强化学习微调；</li>
<li>在 GEO3K 与 MathVision 推理任务上取得<strong>稳定+3.7 分</strong>平均提升，且随合成数据量增加呈现<strong>对数线性增长</strong>，未出现饱和——首次在视觉科学推理领域复现了文本域的“数据引擎”缩放律。</li>
</ul>
<p>通过「代码级精度→严格评测→数据缩放」的完整链路，论文证明了<strong>高保真科学图像合成</strong>不仅是可行的，而且是解锁大规模多模态科学推理的<strong>可扩展路径</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕三条主线共设计并执行了<strong>三大类实验</strong>，覆盖生成能力、范式对比与下游效用，具体如下：</p>
<ol>
<li>生成能力基准实验（SciGenBench）</li>
</ol>
<ul>
<li><strong>规模</strong>：1.4 K 跨学科题目，25 细粒度图示类型。</li>
<li><strong>模型</strong>：<br>– 开源像素模型：HunyuanImage-3.0、Qwen-Image<br>– 闭源像素模型：GPT-Image-1 / 1.5、Seedream-4.0、Flux2-flex、Nanobanana(-Pro)<br>– 代码范式：Qwen3-ImgCoder、Gemini-3-Flash-ImgCoder、Gemini-3-Pro-ImgCoder</li>
<li><strong>指标</strong>：<br>– 逆向验证通过率  R_(∈v)<br>– 五维 LMM-as-Judge 平均分（0–2）<br>– 参考集 SeePhys 上的 FID、PSNR、SSIM、CLIP-score</li>
<li><strong>结论</strong>：<br>– 闭源像素模型大幅领先开源，但最高  R_(∈v)  仅 73.4 %；Gemini-3-Pro-ImgCoder 以 77.9 % 登顶。<br>– 像素模型的感知指标与逻辑正确性<strong>弱相关</strong>，暴露“视觉–逻辑背离”。</li>
</ul>
<ol>
<li>精度–表现力权衡与失效模式分析</li>
</ol>
<ul>
<li><strong>定性审计</strong>：随机采样 200 幅图像，人工标注五类错误（组合、渲染、结构、高密度、领域知识）。</li>
<li><strong>案例对比</strong>：同一提示下像素 vs 代码生成对比（如  y=xln x  函数曲线、弹簧系统示意图）。</li>
<li><strong>分布差距</strong>：<br>– t-SNE 可视化 CLIP 特征，所有合成图与真实 SeePhys 图像<strong>线性可分</strong>。<br>– 频域分析显示合成图普遍具有<strong>过量高频能量</strong>，呈现“数字锐化”伪影。</li>
</ul>
<ol>
<li>下游训练效用实验</li>
</ol>
<ul>
<li><strong>训练数据</strong>：<br>– 原始合成集：Nanobanana-Pro、Qwen-Image、Qwen3-ImgCoder、Gemini-Flash-ImgCoder<br>– 过滤高质量子集：Nanobanana-Pro(Filt)、Qwen-Image(Filt)</li>
<li><strong>协议</strong>：在 Qwen3-VL-8B 上执行 200 步 RL 微调，batch=128，每 prompt 8 rollout，温度 0.6。</li>
<li><strong>评测任务</strong>：Geometry3K-test + MathVision-mini（AVG@4）。</li>
<li><strong>结果</strong>：<br>– 最佳 Nanobanana-Pro(Filt) 相对基线提升 <strong>3.7 分</strong>（54.5 → 58.2）。<br>– 过滤后的高质量数据<strong>一致优于</strong>原始数据，验证质量&gt;数量。<br>– 控制变量缩放实验：50 → 1 400 张图像，精度呈<strong>对数线性增长</strong>（43.9 % → 46.1 %），未出现饱和，首次在科学图示领域复现“数据引擎”缩放律。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为论文结论的自然延伸，亦构成科学图像合成与多模态推理领域下一步的可行突破口：</p>
<ul>
<li><p><strong>混合精度-表现力架构</strong><br>将 ImgCoder 的符号规划模块作为“先验生成器”，为扩散模型提供几何/拓扑硬约束，实现「代码保真 + 像素丰富」的端到端联合训练，突破当前二选一 trade-off。</p>
</li>
<li><p><strong>可验证合成定律</strong><br>在更大规模（10 K–100 K）与更多学科（地球科学、材料、天文）上系统测量 R_inv 与下游任务增益的缩放曲线，检验是否存在与文本域 Chinchilla 类似的“最优数据-参数比”。</p>
</li>
<li><p><strong>自改进数据引擎</strong><br>利用 LMM 自身的验证器（如 CompassVerifier）对合成图像进行<strong>在线过滤+迭代重标注</strong>，形成“生成→验证→再训练”的闭环，持续自我提升图像质量与推理性能。</p>
</li>
<li><p><strong>跨模态一致性评测</strong><br>引入<strong>定理证明器</strong>（如 Lean、Isabelle）或<strong>物理仿真引擎</strong>（如 MuJoCo、COMSOL）对图像隐含的几何或动力学进行<strong>形式化验证</strong>，把“视觉对齐”升级为“公理级正确”。</p>
</li>
<li><p><strong>动态与三维科学图示</strong><br>将静态图解扩展到<strong>时序过程</strong>（反应机理、波动传播）与<strong>三维结构</strong>（晶体、蛋白质），探索基于 NeRF、3D Diffusion 或程序化 Blender 脚本的合成范式，并建立对应 benchmark。</p>
</li>
<li><p><strong>多语言与多文化适配</strong><br>检验代码驱动方法在非拉丁字符、从右到左排版、不同教材风格下的鲁棒性，构建多语言 SciGenBench-X，评估全球教育场景下的公平性与可迁移性。</p>
</li>
<li><p><strong>对抗鲁棒性与安全</strong><br>研究恶意提示（prompt injection）下模型是否可能生成<strong>看似正确但 subtly 错误的图示</strong>（如颠倒电流方向、篡改坐标刻度），并开发相应的对抗检测与防御机制。</p>
</li>
<li><p><strong>人机协同创作工具</strong><br>将 ImgCoder 封装为可交互的<strong>科学插图 Copilot</strong>：用户通过自然语言+鼠标拖拽实时调整布局，后台即时生成可执行代码，缩短教材、论文、MOOC 的插图制作周期。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文《Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility》围绕“如何生成<strong>既视觉美观又逻辑严谨</strong>的科学图像”这一核心问题展开系统研究，主要贡献与结论可概括为以下四点：</p>
<ol>
<li>问题定义与动机</li>
</ol>
<ul>
<li>现有文本到图像（T2I）模型在科学领域存在<strong>视觉–逻辑背离</strong>：图像看起来合理，却常违背几何、物理或化学约束，无法为多模态推理提供可靠监督。</li>
<li>高质量科学图像稀缺已成为限制大视觉-语言模型（LMM）在 STEM 领域扩展的关键瓶颈。</li>
</ul>
<ol>
<li>方法论创新：ImgCoder</li>
</ol>
<ul>
<li>提出<strong>逻辑驱动的“理解→规划→编码”框架</strong>，先由 LLM 抽取实体与约束，再生成确定性 Python 代码执行绘图，实现<strong>结构零幻觉</strong>与<strong>像素级精度</strong>。</li>
<li>相比端到端扩散模型，代码范式在几何/拓扑正确性上显著领先，同时保持可扩展与低成本。</li>
</ul>
<ol>
<li>评测体系：SciGenBench</li>
</ol>
<ul>
<li>构建 1.4 K 跨学科题库（5 学科×25 图示类型），提出<strong>逆向验证指标</strong>  R_(∈v)  与五维 LMM-as-Judge，<strong>从信息效用和逻辑正确性</strong>角度评估生成图像。</li>
<li>实验揭示闭源像素模型最高  R_(∈v)  仅 73.4 %，而 Gemini-3-Pro-ImgCoder 达到 77.9 %；传统像素相似度指标与科学正确性<strong>弱相关</strong>，凸显专用基准的必要性。</li>
</ul>
<ol>
<li>下游效用验证</li>
</ol>
<ul>
<li>用严格筛选的合成图像–文本对微调 Qwen3-VL-8B，在 GEO3K 与 MathVision 上取得**+3.7 分<strong>稳定提升；数据规模从 50 增至 1 400 张时精度呈</strong>对数线性增长**，首次在科学图示领域复现“数据引擎”缩放律，证明高保真合成是<strong>可扩展</strong>的多模态推理路径。</li>
</ul>
<p>综上，论文通过「代码级精度→严格评测→数据缩放」的完整链路，论证了<strong>高保真科学图像合成</strong>不仅能解决视觉–逻辑背离，还能为 LMM 提供持续且可放大的推理增益。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Scientific%20Image%20Synthesis%3A%20Benchmarking%2C%20Methodologies%2C%20and%20Downstream%20Utility，链接是https://arxiv.org/pdf/2601.17027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17027。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Honglin Lin,Chonghan Qin,Zheng Liu,Qizhi Pei,Yu Li,Zhanping Zhong,Xin Gao,Yanfeng Wang,Conghui He,Lijun Wu</p>
<p><strong>Categories</strong>: cs.CV,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17027.pdf">https://arxiv.org/pdf/2601.17027.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17027">https://arxiv.org/abs/2601.17027</a></p>
<p><strong>Arxiv ID</strong>: 2601.17027</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17027">https://papers.cool/arxiv/2601.17027</a></p>
<p><strong>Published</strong>: 2026-01-17T14:18:36Z</p>
<p><strong>Updated</strong>: 2026-01-17T14:18:36.000Z</p>
<hr>
<h3 id="5-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers"><a href="#5-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers" class="headerlink" title="5. Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers"></a>5. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17367">Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</a></h3><p>The quadratic complexity of standard attention mechanisms poses a significant scalability bottleneck for large language models (LLMs) in long-context scenarios. While hybrid attention strategies that combine sparse and full attention within a single model offer a viable solution, they typically employ static computation ratios (i.e., fixed proportions of sparse versus full attention) and fail to adapt to the varying sparsity sensitivities of downstream tasks during inference. To address this issue, we propose Elastic Attention, which allows the model to dynamically adjust its overall sparsity based on the input. This is achieved by integrating a lightweight Attention Router into the existing pretrained model, which dynamically assigns each attention head to different computation modes. Within only 12 hours of training on 8xA800 GPUs, our method enables models to achieve both strong performance and efficient inference. Experiments across three long-context benchmarks on widely-used LLMs demonstrate the superiority of our method.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>标准注意力机制的二次复杂度在长上下文场景下对大型语言模型（LLMs）构成了显著的可扩展性瓶颈。虽然将稀疏注意力和全注意力结合在单一模型中的混合注意力策略提供了一种可行的解决方案，但它们通常采用静态的计算比例（即稀疏注意力与全注意力的固定比例），在推理过程中无法适应下游任务对稀疏性的不同敏感度。为解决这一问题，我们提出了弹性注意力（Elastic Attention），允许模型根据输入动态调整整体稀疏性。通过将轻量级的注意力路由器（Attention Router）集成到现有的预训练模型中，可以动态地为每个注意力头分配不同的计算模式。在使用 8xA800 GPU 训练仅 12 小时后，我们的方法使模型同时实现了高性能和高效推理。在三个长上下文基准测试中对广泛使用的 LLMs 的实验结果证明了我们方法的优越性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>大语言模型（LLM）在长上下文场景中因标准全注意力（Full Attention, FA）的二次复杂度而导致的可扩展性瓶颈</strong>。具体而言，现有混合注意力方法（在同一模型内组合稀疏注意力 SA 与 FA）通常采用<strong>静态的稀疏/全注意力比例</strong>，无法在推理阶段根据下游任务对稀疏度的敏感程度进行动态调整，导致性能与效率的权衡僵化。</p>
<p>为此，作者提出 <strong>Elastic Attention</strong>，核心贡献如下：</p>
<ul>
<li><strong>测试时自适应稀疏比例</strong>：通过轻量级 Attention Router，在预填充阶段根据输入动态为每个注意力头分配 FA 或 SA 计算模式，实现模型整体稀疏度的实时调整。</li>
<li><strong>任务分类视角</strong>：将下游任务归纳为两类——稀疏度鲁棒型（如摘要）与稀疏度敏感型（如问答），仅需区分这两种场景即可，无需为每类任务单独调优固定比例。</li>
<li><strong>高效部署</strong>：设计融合核函数，使不同模式的头在同一前向传播中并行计算，避免额外内存拷贝与调度开销；整个模块仅增加 0.27 M 参数/层， backbone 参数冻结，训练成本 8×A800 12 小时即可完成。</li>
</ul>
<p>实验在多个长上下文基准（LongBench-E、RULER、LongBench-v2）及不同规模 LLM（Qwen3-4B/8B、Llama-3.1-8B-Instruct）上表明，Elastic Attention 在保持或超越全注意力模型性能的同时，显著提升了推理效率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两大主线：<strong>稀疏注意力机制</strong> 与 <strong>混合高效架构</strong>。以下按类别列出代表性工作，并指出与 Elastic Attention 的核心差异。</p>
<p>稀疏注意力机制</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表工作</th>
<th>关键思路</th>
<th>与 Elastic Attention 的差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>静态模式</td>
<td>Longformer (Beltagy et al., 2020)<br/>Block Sparse Attention (Guo et al., 2024)</td>
<td>固定滑动窗口或分块稀疏掩码</td>
<td>模式固定，无法随输入或任务调整</td>
</tr>
<tr>
<td>内容感知启发式</td>
<td>H2O (Zhang et al., 2023)<br/>SnapKV (Li et al., 2024)<br/>Scissorhands (Liu et al., 2024)</td>
<td>依据重要性分数淘汰 token</td>
<td>测试时无参数学习，需敏感阈值调优</td>
</tr>
<tr>
<td>可学习稀疏门控</td>
<td>SeerAttention (Gao et al., 2024)<br/>NSA (Yuan et al., 2025)<br/>MoBA (Lu et al., 2025a)</td>
<td>训练可微门控或块级选择算子</td>
<td>仅学习“在哪稀疏”，不学习“何时全注意力”；比例静态</td>
</tr>
<tr>
<td>长短切换</td>
<td>InfLLM-V2 (Zhao et al., 2025)</td>
<td>短上下文用 FA，长上下文切稀疏</td>
<td>切换策略固定，不按任务动态调节稀疏度</td>
</tr>
</tbody>
</table>
</div>
<p>混合高效架构</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表工作</th>
<th>关键思路</th>
<th>与 Elastic Attention 的差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>层间混合</td>
<td>Jamba (Lieber et al., 2024)<br/>Zamba (Glorioso et al., 2024)</td>
<td>Attention 与 SSM/RNN 层交替</td>
<td>稀疏比例在模型设计阶段固化</td>
</tr>
<tr>
<td>层内混合</td>
<td>DuoAttention (Xiao et al., 2025)<br/>PruLong (Bhaskar et al., 2025)<br/>LongCat (Zhang et al., 2025a)</td>
<td>同一层内部分头 FA、部分头 SA</td>
<td>头-模式映射静态，推理时不再改变</td>
</tr>
</tbody>
</table>
</div>
<p>总结</p>
<ul>
<li><strong>静态或半静态</strong>：上述方法一旦训练完成，稀疏/全注意力比例即固定。</li>
<li><strong>Elastic Attention 突破点</strong>：首次在<strong>测试时</strong>根据输入动态决定每一头的 FA/SA 模式，实现<strong>任务感知的自适应稀疏比例</strong>，无需手工调参或任务特定验证。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文通过 <strong>Elastic Attention</strong> 框架把“静态稀疏比例”改造成“测试时自适应稀疏比例”。核心手段是引入一个 <strong>轻量级 Attention Router</strong>，在推理阶段为每个注意力头动态分配 Full Attention（FA）或 Sparse Attention（SA）模式。整体流程可拆成三步：</p>
<p>1. 问题建模：把任务分成两类即可</p>
<ul>
<li><strong>稀疏度鲁棒任务</strong>（如摘要、代码补全）：粗粒度信息即可，允许高稀疏。</li>
<li><strong>稀疏度敏感任务</strong>（如单/多跳问答）：需要细粒度证据，必须保留更多 FA。</li>
</ul>
<blockquote>
<p>结论：模型只需识别“当前输入属于哪一类”，然后给整网分配对应的总体稀疏度，无需为每类任务单独调一个固定比例。</p>
</blockquote>
<p>2. 动态路由：Attention Router 实时决定“哪颗头用 FA，哪颗头用 SA”</p>
<p>2.1 输入感知</p>
<p>对每层 Key 状态  x_K∈R^(s× H× d’)  做<strong>边界池化</strong>（首尾各 100 token），得到任务向量  x’_K∈R^(H× d’) ，既压缩长度又保留指令与查询信号。</p>
<p>2.2 双 MLP 设计</p>
<ul>
<li><strong>Task MLP</strong>：把  x’_K  映射成任务隐表示，<strong>增大任务间余弦距离</strong> → 实现无监督任务判别。</li>
<li><strong>Router MLP</strong>：为每头输出 2-D logits  z∈R^(H× 2) 。</li>
</ul>
<p>2.3 可微“硬”路由</p>
<ul>
<li>训练时用 <strong>Gumbel-Softmax + 温度退火</strong> 把 logits 变成软分布  r_(soft) ；</li>
<li>前向执行 <strong>argmax</strong> 得到硬决策  r_(hard)∈0,1^H （0→FA，1→SA）；</li>
<li>反向用 <strong>Straight-Through Estimator</strong> 把梯度回传到  r_(soft) ，解决不可导问题。</li>
</ul>
<p>2.4 目标稀疏度约束</p>
<p>对整网计算 <strong>Model Sparsity Ratio</strong></p>
<p>Omega<em>(MSR)=(1) / (LH)∑</em>(ell,h)I[π^((ell,h))=SA]</p>
<p>采用 <strong>min-max 拉格朗日目标</strong></p>
<p>min<em>θmax</em>(λ<em>1,λ_2); L</em>(language) + λ<em>1(Omega</em>(MSR)-t) + λ<em>2(Omega</em>(MSR)-t)^2</p>
<ul>
<li>t  仅分两类：鲁棒任务  t=1.0 （允许全稀疏），敏感任务  t=0.7 （保留 ≥30 % FA）。</li>
<li>λ_1,λ_2  随训练一起梯度上升，<strong>自动权衡“性能-稀疏”</strong> 而无需手工调权重。</li>
</ul>
<p>3. 高效部署：融合核一次跑完异构头</p>
<p>传统做法把 FA 头与 SA 头拆成两个 kernel 顺序执行，导致内存拷贝与调度碎片。<br>论文设计 <strong>Block-Sparse-Attention 融合核</strong>：</p>
<ul>
<li>同一份 (Q,K,V) 原址输入；</li>
<li>线程块按 <strong>head index</strong> 读取路由元数据  m_h ；</li>
<li>同一 kernel 内</li>
<li>若  m_h=FA  → 执行完整 FlashAttention；</li>
<li>若  m_h=SA  → 执行滑动窗口/块稀疏路径。<br>GPU 全局调度器仍可沿 <strong>sequence 维度</strong> 充分并行，实现<strong>零额外内存</strong>与<strong>单 launch 开销</strong>。</li>
</ul>
<p>4. 训练与推理成本</p>
<ul>
<li>** backbone 参数全部冻结**，仅训练 Router 0.27 M/层；</li>
<li>8×A800 训练 12 h（≈ 0.74 B token，最长 64 K）即可完成；</li>
<li>推理时 Router 延迟 0.2 ms，与序列长度无关；</li>
<li>在 256 K 上下文下仍能保持 ≥1.5× 预填充加速，且 Ω_ESR 显著低于同类方法。</li>
</ul>
<p>结果摘要</p>
<ul>
<li><strong>LongBench-E / RULER / LongBench-v2</strong> 三项长文基准上，Elastic Attention 在 <strong>相同或更低有效稀疏度</strong> 下平均性能 <strong>持续第一</strong>；</li>
<li>极端配置 XA-SSA（全网稀疏）仍能把性能差距压到 1 分以内，同时带来 <strong>3×+ 实测加速</strong>。</li>
</ul>
<p>通过“任务分类→动态路由→融合执行”三步，论文彻底摆脱了“为每任务手工调稀疏比例”的瓶颈，实现了<strong>测试时自适应稀疏比例</strong>的高效长上下文推理。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>三大长上下文基准</strong> 上系统评估了 Elastic Attention，覆盖 <strong>真实任务、长度外推、长链推理</strong> 三类场景，并辅以 <strong>消融实验与可视化分析</strong>。具体实验如下：</p>
<p>1. 主实验：与 SOTA 稀疏/混合注意力对比</p>
<p>1.1 LongBench-E（14 任务→6 大类，最长 32 K）</p>
<ul>
<li><strong>骨干</strong>：Qwen3-4B、Qwen3-8B、Llama-3.1-8B-Instruct</li>
<li><strong>对比方法</strong>：InfLLM-V2、DuoAttention、PruLong、MoBA、NSA、XAttention</li>
<li><strong>指标</strong>：每类任务平均性能 + 实测 Ω_MSR</li>
<li><strong>结果</strong>：</li>
<li>Elastic Attention（FA-SSA / FA-XA）<strong>平均性能第一</strong>；</li>
<li>动态分配 Ω_MSR：代码类 0.8+，QA 类 0.65–0.7，<strong>验证“任务-稀疏”自动对齐</strong>。</li>
</ul>
<p>1.2 RULER（8 K–256 K 指数级长度，检索+抽取任务）</p>
<ul>
<li><strong>设置</strong>：训练最长 64 K，直接外推至 256 K</li>
<li><strong>结果</strong>：</li>
<li>所有长度区间 <strong>平均性能最佳</strong>；</li>
<li>256 K 下 FA-XA 仍保持 68.5 分，<strong>显著高于</strong> MoBA/NSA 的 ≈0 分；</li>
<li>加速比随长度增加而放大，256 K 达 <strong>1.5–2.5×</strong>。</li>
</ul>
<p>1.3 LongBench-v2（8 K–2 M 词，深推理 Easy/Hard 双档）</p>
<ul>
<li><strong>结果</strong>：</li>
<li><strong>Easy 与 Hard 双档均排名第一</strong>；</li>
<li>FA-XA 在 Llama-3.1-8B 上平均 30.77 分，<strong>超过全注意力基线 32.69</strong>（Hard 档提升 3.7 分）。</li>
</ul>
<p>2. 消融实验（Ablation）</p>
<p>2.1 Attention Router 设计</p>
<ul>
<li><strong>Task MLP 有无</strong>：去掉后任务间余弦相似度 ↑0.35→↑0.72，<strong>性能 ↓2.1 分</strong>；</li>
<li><strong>隐藏维度</strong>：2×/4×/6×/8× d′ 实验，<strong>4× 为参数-性能甜点</strong>；</li>
<li><strong>路由稳定性</strong>：同一头在多任务中激活频率可视化，<strong>Qwen3 出现固定“检索头”</strong>，Llama-3.1 更动态。</li>
</ul>
<p>2.2 目标稀疏度 t 的影响</p>
<ul>
<li>固定鲁棒任务 t=1.0，敏感任务 t_sen∈{0.4,0.5,0.6,0.7}</li>
<li>t_sen=0.4 时 Ω_MSR 最低，<strong>性能甚至略超全注意力</strong>，但效率下降；<strong>t_sen=0.7 为默认</strong>，兼顾性能与加速。</li>
</ul>
<p>2.3 性能-效率权衡</p>
<ul>
<li>在 RULER 上同时汇报 <strong>性能、加速比、Ω_ESR</strong></li>
<li>FA-XA：Ω_ESR≈0.65，<strong>1.51× 加速</strong>，性能最佳；</li>
<li>XA-SSA：Ω_ESR≈0.995，<strong>3.28× 加速</strong>，性能下降可控（-6.6 分）。</li>
</ul>
<p>2.4 可扩展性极限</p>
<ul>
<li>把“检索头”也换成 XA → 得到 <strong>纯稀疏 XA-SSA</strong></li>
<li>Qwen3-4B 平均性能 <strong>仅降 0.66 分</strong>，加速再提升；</li>
<li>证明 <strong>Elastic Attention 框架在完全稀疏场景仍有效</strong>。</li>
</ul>
<p>3. 微观分析</p>
<ul>
<li><strong>任务表征可视化</strong>：Task MLP 后不同任务余弦相似度 <strong>接近 0</strong>，验证无监督任务分离能力。</li>
<li><strong>路由延迟</strong>：512 token–1 M token 序列 Router 延迟 <strong>恒定为 0.196 ms</strong>，可忽略。</li>
<li><strong>训练曲线</strong>：</li>
<li>LM Loss 快速收敛至 2.1；</li>
<li>Ω_MSR 自动分化：Code/In-Context 升至 0.8，QA 任务停在 0.65，<strong>与预设 t 非严格对齐</strong>，体现拉格朗日松弛效果；</li>
<li>拉格朗日系数 λ 动态调整，<strong>In-Context 任务 λ 最大</strong>，说明模型自发加重其密度约束。</li>
</ul>
<p>4. 定性案例（Error Analysis）</p>
<ul>
<li><strong>政策决策题</strong>：只有 Elastic Attention 捕捉到“渐进式”关键词，<strong>输出符合 stability 约束</strong>；基线均提前引入碳税。</li>
<li><strong>双语法律文档</strong>：准确区分“冻结”与“公开征用”法律后果，<strong>基线混淆程序与结果</strong>。</li>
<li><strong>《基督山伯爵》情节</strong>：正确列出“Danglars, Fernand, Caderousse”三人，<strong>基线反复 hallucinate 检察官 Villefort 为同谋</strong>。</li>
</ul>
<p>实验覆盖度总结</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>维度</th>
<th>实验内容</th>
</tr>
</thead>
<tbody>
<tr>
<td>任务类型</td>
<td>摘要、QA、代码、Few-shot、合成检索、长链推理</td>
</tr>
<tr>
<td>上下文长度</td>
<td>8 K – 256 K（训练 64 K）外推，最高 2 M 词</td>
</tr>
<tr>
<td>模型规模</td>
<td>4 B、8 B 两类主流骨干</td>
</tr>
<tr>
<td>对比广度</td>
<td>训练-free、训练-based 共 7 种 SOTA 稀疏方法</td>
</tr>
<tr>
<td>指标维度</td>
<td>性能、Ω_MSR、Ω_ESR、加速比、可视化、延迟</td>
</tr>
</tbody>
</table>
</div>
<p>通过上述多维实验，论文验证了 Elastic Attention <strong>在不同任务、不同长度、不同稀疏极端配置下均能保持领先性能与显著加速</strong>，确立了新的性能-效率帕累托前沿。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 Elastic Attention 的<strong>直接延伸或深层扩展</strong>，均围绕“<strong>更细粒度、更极端场景、更广义框架</strong>”展开，具备可验证的实验空间：</p>
<p>1. 路由决策的粒度细化</p>
<ul>
<li><strong>Token-level / Block-level 路由</strong><br>当前仅做“<strong>头级</strong>”FA/SA 二分。可探索<strong>同一头内部</strong>对不同 token/block 动态选择 FA 或 SA，兼顾“局部全 attention + 全局稀疏”混合模式，进一步压低 Ω_ESR。</li>
<li><strong>Head-group 路由</strong><br>把 H 头先聚类成 G≪H 组，路由决策先在组层面生成，再广播到组内，降低 Router 参数量至 O(G) 且保持性能。</li>
</ul>
<p>2. 极端长度下的系统级优化</p>
<ul>
<li><strong>多 GPU 张量并行 + Elastic Attention</strong><br>当前 fused kernel 仅解决<strong>单卡内部</strong>异构头调度。将路由元数据 m 嵌入序列并行或上下文并行框架，研究跨卡负载均衡与通信量最小化策略，目标 <strong>1 M–10 M token</strong> 实时预填充。</li>
<li><strong>NVLink/IB 零冗余 KV 传输</strong><br>对 FA 头采用 full KV，SA 头仅传保留块；设计 <strong>KV 选择性广播协议</strong>，量化通信-计算重叠收益。</li>
</ul>
<p>3. 自适应训练策略</p>
<ul>
<li><strong>免标签任务发现</strong><br>用 Router 隐空间聚类自动发现“新任务簇”，再在线更新拉格朗日靶点 t，实现<strong>持续学习</strong>而无需人工标注任务类型。</li>
<li><strong>元学习初始化</strong><br>把 Router 参数视为元参数，用 MAML 或 Reptile 预训练一套“通用稀疏初始化”，<strong>新任务仅需几十步微调</strong>即可收敛。</li>
</ul>
<p>4. 与高效架构的“双重稀疏”组合</p>
<ul>
<li><strong>Elastic Attention × Mamba/RetNet</strong><br>在 Attention 层内部做 FA/SA 路由，同时把部分层整体替换为线性复杂度算子（SSM、线性 RNN），研究<strong>层间+层内联合稀疏</strong>的 Pareto 前沿。</li>
<li><strong>MoE 化 Attention</strong><br>把 FA/SA 视为两个 Expert，引入 <strong>Top-k 头专家</strong> 与 <strong>Top-k 令牌专家</strong> 两级门控，实现 <strong>“头-令牌”双维度</strong> 动态选择。</li>
</ul>
<p>5. 理论分析与可解释性</p>
<ul>
<li><strong>稀疏度-性能敏感度度量</strong><br>定义任务无关的 <strong>Sensitivity@Sparsity 曲线</strong>，用信息论指标（MI、KL）量化不同任务对注意力缺失的鲁棒性，为自动设定 t 提供理论依据。</li>
<li><strong>路由决策可视化与干预</strong><br>通过激活修补（activation patching）验证“哪些头决定答案片段”，再对 Router 引入 <strong>可解释约束</strong>（稀疏线性门控、注意力 rollout），确保关键检索头不被误路由到 SA。</li>
</ul>
<p>6. 面向生产环境的端到端优化</p>
<ul>
<li><strong>INT8/INT4 量化 Router</strong><br>当前 Router 为 FP16。将其权重与激活量化至 8 bit 或 4 bit，研究 <strong>zero-shot 无损量化</strong>方案，进一步压缩 0.27 M 参数至 &lt;0.1 M。</li>
<li><strong>推测式解码（Speculative Decoding）兼容</strong><br>草稿模型使用高稀疏配置（XA-SSA），主模型使用 FA-XA，验证 <strong>路由不一致性</strong> 对接受率的影响，并设计 <strong>投机-主模型路由对齐</strong> 策略。</li>
</ul>
<p>7. 跨模态扩展</p>
<ul>
<li><strong>长文档+图像/视频</strong><br>把 Router 输入扩展为 <strong>视觉 token 与文本 token 联合池化</strong>，研究多模态长上下文下“视觉检索头”（始终 FA）与“文本稀疏头”的自动分化，评估在 <strong>Video-MRC、长图 OCR</strong> 任务上的效率增益。</li>
</ul>
<p>8. 自适应稀疏范式泛化</p>
<ul>
<li><strong>Elastic Conv / Elastic MLP</strong><br>将“动态路由→稀疏/密集算子”思想迁移到 CNN（大核 vs. 小核）或 FFN（全连接 vs. 稀疏专家），验证 <strong>“Elastic X” 框架通用性</strong>；最终目标：一套 <strong>统一动态稀疏架构</strong> 适配任意算子。</li>
</ul>
<p>以上方向均可在现有代码基线（开源地址已给出）上<strong>快速验证</strong>，并具备<strong>可量化的性能-效率指标</strong>（Ω_MSR、Ω_ESR、加速比、内存/通信量、任务精度），适合后续深入研究。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>Elastic Attention：测试时自适应稀疏比例的高效 Transformer<br>（arXiv:2601.17367）</p>
<p>1. 问题</p>
<ul>
<li>标准全注意力 FA 的 O(n²) 复杂度限制 LLM 长上下文扩展。</li>
<li>现有“混合注意力”把 FA 与稀疏注意力 SA 按<strong>固定比例</strong>硬编码，推理时无法根据任务对稀疏的敏感度动态调整，导致性能-效率权衡僵化。</li>
</ul>
<p>2. 核心观察</p>
<p>下游任务天然分成两类即可：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>特点</th>
<th>示例</th>
<th>所需稀疏度</th>
</tr>
</thead>
<tbody>
<tr>
<td>稀疏度鲁棒</td>
<td>粗粒度信息足够</td>
<td>摘要、代码补全</td>
<td>可极高稀疏</td>
</tr>
<tr>
<td>稀疏度敏感</td>
<td>需细粒度证据</td>
<td>单/多跳问答</td>
<td>只能适度稀疏</td>
</tr>
</tbody>
</table>
</div>
<p>⇒ 模型只需<strong>识别输入属于哪一类</strong>，并实时分配合适的<strong>整体稀疏度</strong>，无需为每类任务单独调优固定比例。</p>
<p>3. 方法：Elastic Attention</p>
<p>① 轻量级 Attention Router（每层 +0.27 M 参数）</p>
<ul>
<li>输入：Key 状态 → 边界池化 → Task MLP 提取任务表征 → Router MLP 输出每头 logits。</li>
<li>路由：Gumbel-Softmax + 温度退火 + Straight-Through Estimator，<strong>训练即硬路由</strong>，推理直接 argmax。</li>
<li>决策：0=FA，1=SA，单层内头级异构。</li>
</ul>
<p>② 稀疏约束</p>
<ul>
<li>计算 Model Sparsity Ratio Ω_MSR = (SA 头数)/(总头数)。</li>
<li>Min-max 拉格朗日目标：LM Loss + λ₁(Ω_MSR−t) + λ₂(Ω_MSR−t)²<br>– 鲁棒任务 t = 1.0；敏感任务 t = 0.7。<br>– λ₁,λ₂ 随训练一起优化，<strong>自动平衡性能与稀疏</strong>。</li>
</ul>
<p>③ 高效部署</p>
<ul>
<li>fused kernel：同一次 kernel launch 内，按路由元数据并行执行 FA/SA，<strong>零内存拷贝、零额外调度开销</strong>。</li>
</ul>
<p>4. 实验结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>长度</th>
<th>骨干</th>
<th>主要结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>LongBench-E</td>
<td>≤32 K</td>
<td>Qwen3-4B/8B、Llama-3.1-8B</td>
<td>平均性能第一；Ω_MSR 任务自适应（代码 0.8+，QA 0.65）。</td>
</tr>
<tr>
<td>RULER</td>
<td>8 K–256 K</td>
<td>同上</td>
<td>外推 256 K 仍领先；FA-XA 68.5 分 vs MoBA/NSA≈0；加速 1.5–2.5×。</td>
</tr>
<tr>
<td>LongBench-v2</td>
<td>8 K–2 M</td>
<td>同上</td>
<td>Easy/Hard 双档 均第一；FA-XA 平均 30.77 &gt; 全注意力 32.69（Hard 档）。</td>
</tr>
</tbody>
</table>
</div>
<p><strong>消融</strong>：</p>
<ul>
<li>去掉 Task MLP → 性能 −2.1 分；</li>
<li>t_sen=0.4 可略超全注意力，但效率下降；</li>
<li>XA-SSA 纯稀疏版平均性能差距 &lt;1 分，加速 <strong>3.28×</strong>。</li>
</ul>
<p>5. 结论</p>
<p>Elastic Attention 用<strong>极轻量级 Router</strong>在<strong>测试时</strong>把任务自动映射到“稀疏度鲁棒/敏感”两种靶稀疏度，实现<strong>头级 FA/SA 动态混合</strong>，无需修改预训练骨干，<strong>12 h 8×A800 训练</strong>即可在多项长上下文基准上<strong>同时取得 SOTA 性能与显著加速</strong>，为长文本 LLM 提供了新的性能-效率帕累托前沿。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Elastic%20Attention%3A%20Test-time%20Adaptive%20Sparsity%20Ratios%20for%20Efficient%20Transformers，链接是https://arxiv.org/pdf/2601.17367，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17367。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zecheng Tang,Quantong Qiu,Yi Yang,Zhiyi Hong,Haiya Xiang,Kebin Liu,Qingqing Dang,Juntao Li,Min Zhang</p>
<p><strong>Categories</strong>: cs.CL,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17367.pdf">https://arxiv.org/pdf/2601.17367.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17367">https://arxiv.org/abs/2601.17367</a></p>
<p><strong>Arxiv ID</strong>: 2601.17367</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17367">https://papers.cool/arxiv/2601.17367</a></p>
<p><strong>Published</strong>: 2026-01-24T08:22:07Z</p>
<p><strong>Updated</strong>: 2026-01-24T08:22:07.000Z</p>
<hr>
<h3 id="6-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code"><a href="#6-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code" class="headerlink" title="6. iFSQ: Improving FSQ for Image Generation with 1 Line of Code"></a>6. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17124">iFSQ: Improving FSQ for Image Generation with 1 Line of Code</a></h3><p>The field of image generation is currently bifurcated into autoregressive (AR) models operating on discrete tokens and diffusion models utilizing continuous latents. This divide, rooted in the distinction between VQ-VAEs and VAEs, hinders unified modeling and fair benchmarking. Finite Scalar Quantization (FSQ) offers a theoretical bridge, yet vanilla FSQ suffers from a critical flaw: its equal-interval quantization can cause activation collapse. This mismatch forces a trade-off between reconstruction fidelity and information efficiency. In this work, we resolve this dilemma by simply replacing the activation function in original FSQ with a distribution-matching mapping to enforce a uniform prior. Termed iFSQ, this simple strategy requires just one line of code yet mathematically guarantees both optimal bin utilization and reconstruction precision. Leveraging iFSQ as a controlled benchmark, we uncover two key insights: (1) The optimal equilibrium between discrete and continuous representations lies at approximately 4 bits per dimension. (2) Under identical reconstruction constraints, AR models exhibit rapid initial convergence, whereas diffusion models achieve a superior performance ceiling, suggesting that strict sequential ordering may limit the upper bounds of generation quality. Finally, we extend our analysis by adapting Representation Alignment (REPA) to AR models, yielding LlamaGen-REPA. Codes is available at <a target="_blank" rel="noopener" href="https://github.com/Tencent-Hunyuan/iFSQ">https://github.com/Tencent-Hunyuan/iFSQ</a></p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>图像生成领域目前分为两类：在离散令牌上运行的自回归（AR）模型和利用连续潜变量的扩散模型。这种分化源于VQ-VAE和VAE之间的区别，阻碍了统一建模和公平基准测试。有限标量量化（FSQ）提供了一个理论桥梁，但原始FSQ存在一个关键缺陷：其等间隔量化可能导致激活坍塌。这种不匹配迫使在重建精度和信息效率之间做出权衡。在本工作中，我们通过简单地将原始FSQ中的激活函数替换为分布匹配映射来强制均匀先验，解决了这一困境。该方法称为iFSQ，这一简单策略仅需一行代码，却在数学上保证了最佳的量化桶利用率和重建精度。利用iFSQ作为受控基准，我们揭示了两个关键见解：(1) 离散和连续表示之间的最佳平衡约为每维4比特。(2) 在相同的重建约束下，AR模型表现出快速的初始收敛，而扩散模型则实现了更高的性能上限，这表明严格的顺序依赖可能限制生成质量的上界。最后，我们通过将表示对齐（REPA）方法应用于AR模型，延伸了我们的分析，生成了LlamaGen-REPA。代码可在<a target="_blank" rel="noopener" href="https://github.com/Tencent-Hunyuan/iFSQ获取">https://github.com/Tencent-Hunyuan/iFSQ获取</a></p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>该论文旨在弥合当前图像生成领域两大主流范式——<strong>自回归（AR）模型</strong>与<strong>扩散（Diffusion）模型</strong>——在<strong>Tokenizer</strong>层面的根本分歧。具体而言，论文关注并解决以下核心问题：</p>
<ul>
<li><p><strong>Tokenizer 不统一带来的评估偏差</strong>：<br>AR 模型依赖离散 Token（VQ-VAE），Diffusion 模型依赖连续潜码（VAE），导致二者无法在同一量化空间下公平比较，性能差异难以归因于模型本身还是 Tokenizer 差异。</p>
</li>
<li><p>** vanilla FSQ 的激活塌陷缺陷**：<br>原始有限标量量化（FSQ）采用等间隔量化，与神经网络激活的高斯分布失配，造成</p>
</li>
<li><p><strong>中心桶过度使用</strong>（重建精度高，但信息熵低）</p>
</li>
<li><strong>边缘桶严重闲置</strong>（信息效率低，有效码本缩减）</li>
<li><strong>重建保真度与信息效率的权衡困境</strong>：<br>强制均匀桶利用率会放大边缘桶宽度，导致量化误差上升（MSE 恶化），反之则激活塌陷。</li>
</ul>
<p>为解决上述问题，论文提出 <strong>iFSQ</strong>，仅通过 <strong>1 行代码</strong> 将 FSQ 的 tanh 激活替换为分布匹配映射</p>
<p>y = 2σ(1.6x) - 1</p>
<p>即可将高斯潜空间映射为均匀分布，从而<strong>数学上同时保证</strong>：</p>
<ol>
<li>等间隔桶被等概率使用（信息熵最大化，利用率 100%）</li>
<li>量化误差最小化（重建 MSE 最优）</li>
</ol>
<p>最终，iFSQ 作为<strong>统一 Tokenizer</strong> 实现：</p>
<ul>
<li>同一套量化结果既可当离散 Token 供 AR 使用，也可当连续潜码供 Diffusion 使用</li>
<li>在严格一致的重建约束下，公平比较两种生成范式的缩放特性，得出“AR 收敛快、Diffusion 上限高”的结论</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节系统梳理了与“视觉 Tokenizer”及“神经网络量化”相关的研究，可归纳为以下两条主线。</p>
<p>1. 视觉 Tokenizer：连续 vs. 离散</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>类别</th>
<th>代表工作</th>
<th>关键特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>连续潜码（VAE 族）</td>
<td>Kingma &amp; Welling 2013；Chen et al. 2025；Yao et al. 2025</td>
<td>高斯先验，支持扩散模型；引入 GAN、LPIPS、DINO 等监督提升重建保真度。</td>
</tr>
<tr>
<td>显式码本离散（VQ-VAE 族）</td>
<td>van den Oord et al. 2017；Yu et al. 2023 MAGVIT-v2；Zhu et al. 2024</td>
<td>可学习码本，AR 必需；存在码本塌陷、内存占用大、需 straight-through 估计。</td>
</tr>
<tr>
<td>隐式码本离散（FSQ 族）</td>
<td>Mentzer et al. 2023 FSQ；Li et al. 2025a MANZANO；Lu et al. 2025 AToken</td>
<td>无学习码本，元素级舍入；避免塌陷，已用于图像/视频/音频，但 vanilla FSQ 假设均匀分布，与激活高斯失配。</td>
</tr>
</tbody>
</table>
</div>
<p>2. 神经网络量化（LLM &amp; 视觉压缩）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>代表工作</th>
<th>与本文差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>权重量化</td>
<td>Frantar et al. 2022 GPTQ；Lin et al. 2024 AWQ</td>
<td>后训练压缩，仅加速推理，不改变训练阶段 tokenizer 设计。</td>
</tr>
<tr>
<td>量化感知训练</td>
<td>Liu et al. 2024b QAT</td>
<td>同样关注激活量化，但目标为减小模型 footprint，而非统一离散/连续表示。</td>
</tr>
<tr>
<td>率–失真优化（RDO）</td>
<td>Sullivan &amp; Wiegand 2002</td>
<td>理论框架：在给定位率下最小化重建误差；本文将 RDO 思想前移到 tokenizer 训练阶段，用 1 行代码实现均匀先验，兼得最优率与失真。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 本文与既有工作的关系</p>
<ul>
<li><strong>vs. VQ-VAE</strong>：iFSQ 无需学习码本，训练稳定，压缩率可调。</li>
<li><strong>vs. 连续 VAE</strong>：iFSQ 在 4 bit 即可逼近 16 bit 浮点重建质量，同时天然产出离散 Token，供 AR 直接使用。</li>
<li><strong>vs. 后训练量化</strong>：iFSQ 把“量化”从推理加速工具升级为<strong>统一离散-连续表示</strong>的核心机制。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“统一离散-连续 Tokenizer”问题转化为**「如何把高斯分布的潜变量映射到均匀分布，使得等间隔量化桶被等概率使用」**的分布校正问题。解决方案仅改动 1 行代码，分三步完成：</p>
<p>1. 诊断：vanilla FSQ 的激活塌陷根源</p>
<ul>
<li>原始 FSQ 用 tanh 将无界潜变量  zsimmathcal N(0,1)  压到 $<br>-1,1<br>$，再执行等间隔舍入。</li>
<li>tanh 的 PDF 呈双峰马鞍形 → 中心桶概率高、边缘桶概率低 → <strong>利用率 &lt; 100 %</strong>，有效码本缩小，信息熵下降。</li>
</ul>
<p>2. 校正：用“分布匹配”替代 tanh</p>
<p>构造一族可逆映射</p>
<p>y = 2σ(α x) - 1, quad σ(x)=(1) / (1+e^(-x))</p>
<p>通过网格搜索  α  并监控两项指标：</p>
<ul>
<li><strong>RMSE</strong> 量化误差</li>
<li><strong>KS 统计量</strong> 与均匀分布的距离</li>
</ul>
<p>实验发现  α=1.6  时两者同时最小，可将  mathcal N(0,1)  几乎<strong>线性拉伸</strong>成  mathcal U(-1,1) 。</p>
<p>3. 实现：1 行代码的 plug-and-play 替换</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">原 FSQ</span><br><span class="line">z = torch.tanh(z)</span><br><span class="line"></span><br><span class="line">iFSQ（仅改这一行）</span><br><span class="line">z = <span class="number">2</span> * torch.sigmoid(<span class="number">1.6</span> * z) - <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>后续流程不变：</p>
<ol>
<li>线性缩放至整数网格</li>
<li>舍入 + Straight-Through 估计</li>
<li>反归一化供扩散使用；按位展开供 AR 使用</li>
</ol>
<p>4. 理论保证与效果</p>
<ul>
<li><strong>均匀先验</strong> ⇒ 所有桶等概率 ⇒ 信息熵最大化（≈ log L）</li>
<li><strong>等间隔</strong> ⇒ 量化步长恒定 ⇒ 重建 MSE 最小化</li>
<li><strong>零额外参数量、零推理延迟</strong></li>
</ul>
<p>在 ImageNet/COCO 上，iFSQ 用 4 bit 即达到与 16 bit 连续 AE 相当的 PSNR/SSIM/LPIPS，同时生成质量（gFID）优于同等压缩率的 VQ/FSQ，为 AR 与 Diffusion 提供了<strong>同一量化基准</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕「iFSQ 是否同时胜任离散与连续 Tokenizer」与「AR vs Diffusion 在统一量化下的 Scaling 行为」两条主线，共设计 4 组实验。所有实验均基于 ImageNet-256×256 训练，并在 ImageNet/COCO 验证集上报告重建或生成指标；Tokenizer 与生成模型规模均选用 Large 配置以保证结论可靠。</p>
<p>1. 重建实验（Tokenizer 本身的好坏）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>对比对象</th>
<th>关键指标</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1 重建质量 vs α</td>
<td>FSQ(α=2.0) ↔ iFSQ(α=1.6)</td>
<td>PSNR↑ SSIM↑ LPIPS↓ RMSE↓ KS↓</td>
<td>α=1.6 全面优于原始 FSQ，且趋势跨数据集稳定（图 3）</td>
</tr>
<tr>
<td>1.2 不同 bit 深度</td>
<td>iFSQ 2–8 bit vs AE vs VQ</td>
<td>同上 + rFID↓</td>
<td>4 bit 逼近 AE，&gt;4 bit 收益饱和；VQ 同维数下明显落后（图 5）</td>
</tr>
<tr>
<td>1.3 压缩比 Scaling</td>
<td>256× 空间压缩，12×–1536× CR</td>
<td>同上</td>
<td>对数压缩比-性能近似线性，48×（4 bit）为拐点，VQ 点落在同趋势线上（图 10）</td>
</tr>
</tbody>
</table>
</div>
<p>2. 生成实验（Tokenizer 喂给 AR / Diffusion 后的效果）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>范式</th>
<th>设置</th>
<th>对比基线</th>
<th>主要指标</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.1 扩散模型</td>
<td>DiT-Large, 100k iter</td>
<td>AE / FSQ / iFSQ 2–8 bit</td>
<td>gFID↓</td>
<td>iFSQ-4 bit 最佳 12.76，优于 AE 的 13.78；3× 压缩率下仍领先（表 1）</td>
</tr>
<tr>
<td>2.2 自回归模型</td>
<td>LlamaGen-Large, 500k iter</td>
<td>VQ-14 bit / FSQ / iFSQ 2–6 bit</td>
<td>gFID↓</td>
<td>iFSQ-4 bit 28.07 &lt; VQ-14 bit 29.91；更高 bit 反而退化（表 2）</td>
</tr>
</tbody>
</table>
</div>
<p>3. 训练效率对比（统一 Tokenizer 下的 AR vs Diffusion）</p>
<ul>
<li><strong>控制变量</strong>：同一 iFSQ-4 bit Tokenizer，相同计算预算（对数坐标）</li>
<li><strong>观测指标</strong>：gFID 随训练 GFLOPs 变化</li>
<li><strong>结果</strong>（图 4）：</li>
<li>阶段 1（低计算）：AR 收敛斜率更陡 → 训练效率更高</li>
<li>交叉点后（≈150 GFlops）：Diffusion 继续下降，AR 趋于平台 → Diffusion 质量上限更高</li>
</ul>
<p>4. LlamaGen-REPA 实验（把 Representation Alignment 扩展到 AR）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>实验设计</th>
<th>观测指标</th>
<th>结论</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1 层动力学</td>
<td>逐层计算 STS / NTS / CKNNA</td>
<td>曲线拐点</td>
<td>中间层出现「自编码 → 下一 token 预测」切换，NTS 与 CKNNA 高度相关（r≥0.72）（图 6）</td>
</tr>
<tr>
<td>4.2 对齐深度</td>
<td>1/12…24/24 层对齐 DINOv2</td>
<td>gFID↓</td>
<td>最优深度 ≈ 1/3 总层数（Large-24 层选 8，XLarge-36 层选 12…），扩散/AR 均符合同一比例（图 8）</td>
</tr>
<tr>
<td>4.3 损失权重 λ</td>
<td>λ=0.5…5.0</td>
<td>gFID↓ 收敛曲线</td>
<td>AR 需更强正则 λ=2.0；λ=2.0+8 层对齐组合最佳 16.17（表 3，图 9）</td>
</tr>
</tbody>
</table>
</div>
<p>总结</p>
<ul>
<li><strong>重建侧</strong>：iFSQ 用 4 bit 即可媲美 16 bit 连续 AE，压缩率提升 3–4×</li>
<li><strong>生成侧</strong>：同一 iFSQ 下，Diffusion 最终 gFID 更低，AR 收敛更快；4 bit 为两种范式的共同「甜蜜点」</li>
<li><strong>扩展侧</strong>：REPA 对 AR 同样有效，但需更大 λ 且对齐深度服从「1/3 网络深度」通用法则</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为「Tokenizer 本身」「生成范式对比」「REPA 扩展」三大类，供后续研究参考。</p>
<p>1. Tokenizer 层面</p>
<ul>
<li><p><strong>非高斯先验的分布匹配</strong><br>当前仅针对  mathcal N(0,1)  设计  2σ(1.6x)-1 ；若潜变量经 KL 正则后呈现拉普拉斯或 logistic 分布，可推导对应的  α^*  或采用可学习参数  α(x) 。</p>
</li>
<li><p><strong>通道级异构量化</strong><br>不同通道统计特性差异大→ 每通道独立  α_j  或自适应  L_j ，可望在相同比特预算下进一步降低 MSE。</p>
</li>
<li><p><strong>与 GAN/感知损失联合训练</strong><br>目前仅 L2+LPIPS；引入判别器或 DINOv2 特征一致性损失，观察分布匹配是否仍保持最优。</p>
</li>
<li><p><strong>视频/3D 扩展</strong><br>时空连续信号分布更复杂→ 验证「均匀先验」假设是否依旧成立；若否，可设计时空可分离的分布映射。</p>
</li>
</ul>
<p>2. 统一基准下的生成范式研究</p>
<ul>
<li><p><strong>最优比特-维度组合搜索</strong><br>实验显示 4 bit 为拐点，但仅遍历  L=2^b+1 ；可固定总比特预算  B=d·log_2L ，用 NAS 搜索  (d,L)  帕累托前沿。</p>
</li>
<li><p><strong>混合离散-连续训练</strong><br>同一批潜码在扩散阶段用连续值  z_q ，在 AR 阶段用离散索引  I ，探索「联合训练」或「课程切换」能否兼得收敛速度与质量上限。</p>
</li>
<li><p><strong>跨任务迁移</strong><br>当前仅 ImageNet/COCO；将 iFSQ 应用于文本到图像、布局到图像、视频预测等任务，检验 4 bit 甜蜜点是否通用。</p>
</li>
<li><p><strong>理论极限分析</strong><br>基于率-失真函数  R(D)  推导给定比特预算下 iFSQ 可达的最小 MSE，与实验曲线对照，量化「还有多少提升空间」。</p>
</li>
</ul>
<p>3. REPA 与表示对齐</p>
<ul>
<li><p><strong>多教师集成</strong><br>除 DINOv2 外，引入 CLIP、ConvNeXT、CLIPConvNeXT 等多视角特征，按任务动态加权，减少单教师偏差。</p>
</li>
<li><p><strong>对齐目标特征选择</strong><br>目前对齐最后层；可探究对齐中间层或逐块平均池化特征，是否进一步提升 AR 的长序列一致性。</p>
</li>
<li><p><strong>自适应损失权重 λ(x)</strong><br>当前全局常数 λ=2.0；可按 token 难度或梯度范数动态调整，避免过强正则导致模式崩溃。</p>
</li>
<li><p><strong>扩散模型反向对齐</strong><br>现有 REPA 仅把视觉教师「拉」向生成模型；可逆向添加分支，让生成特征同时逼近视觉教师，形成双向一致正则。</p>
</li>
<li><p><strong>Scaling Law 拟合</strong><br>收集不同模型深度、宽度、分辨率下的最优 1/3 深度现象，拟合  BestDepthpropto TotalLayers^β ，验证是否  β≈ 1  并给出理论解释。</p>
</li>
</ul>
<p>4. 系统与硬件角度</p>
<ul>
<li><p><strong>低比特内核实现</strong><br>4 bit 均匀量化可直接 pack 成 int4，评估在 TensorRT、AITemplate 上的真实加速比与功耗下降。</p>
</li>
<li><p><strong>量化误差可逆编码</strong><br>将舍入误差  z-round(z)  用轻量级熵编码并行存储，实现「无损-近似双模」推理，按需切换质量与带宽。</p>
</li>
</ul>
<p>综上，iFSQ 把「分布匹配」思想引入视觉 Tokenizer 已初见成效，但围绕<strong>更复杂的先验</strong>、<strong>更细粒度的比特分配</strong>、<strong>跨模态跨任务的一致性</strong>以及<strong>理论极限与硬件落地</strong>，仍有大量空白值得探索。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<ul>
<li><strong>问题</strong>：AR 与 Diffusion 因离散/连续 Tokenizer 分裂，难以公平比较；vanilla FSQ 等间隔量化与高斯激活失配，陷入“重建保真-信息效率”两难。</li>
<li><strong>方法</strong>：iFSQ 仅改 1 行代码，用  y=2σ(1.6x)-1  将高斯潜变量映射为均匀分布，使等间隔桶被等概率使用，兼得 100% 利用率与最低量化误差。</li>
<li><strong>实验</strong>：</li>
<li>重建：4 bit iFSQ 媲美 16 bit 连续 AE，压缩率↑3×。</li>
<li>生成：同一 iFSQ 喂给 DiT 与 LlamaGen，发现 AR 收敛快、Diffusion 上限高，4 bit 为共同甜蜜点。</li>
<li>扩展：把 REPA 搬至 AR，验证对齐深度≈1/3 网络层、λ=2.0 最佳。</li>
<li><strong>结论</strong>：分布匹配即可统一离散-连续表示，4 bit 是精度与紧凑的最优平衡，严格顺序归纳偏置或限制 AR 的生成天花板。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是iFSQ%3A%20Improving%20FSQ%20for%20Image%20Generation%20with%201%20Line%20of%20Code，链接是https://arxiv.org/pdf/2601.17124，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17124。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Bin Lin,Zongjian Li,Yuwei Niu,Kaixiong Gong,Yunyang Ge,Yunlong Lin,Mingzhe Zheng,JianWei Zhang,Miles Yang,Zhao Zhong,Liefeng Bo,Li Yuan</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17124.pdf">https://arxiv.org/pdf/2601.17124.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17124">https://arxiv.org/abs/2601.17124</a></p>
<p><strong>Arxiv ID</strong>: 2601.17124</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17124">https://papers.cool/arxiv/2601.17124</a></p>
<p><strong>Published</strong>: 2026-01-23T19:00:35Z</p>
<p><strong>Updated</strong>: 2026-01-23T19:00:35.000Z</p>
<hr>
<h3 id="7-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability"><a href="#7-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability" class="headerlink" title="7. Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability"></a>7. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18778">Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</a></h3><p>Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>模型能否学会摆脱自身的学习平台？用于微调大型推理模型的强化学习方法在初始成功率低的数据集上会停滞，因此训练信号较少。我们探讨一个根本性问题：预训练的LLM能否利用潜在知识生成自动化课程，解决无法解决的问题？为此，我们设计了SOAR：一个自我提升框架，旨在通过元强化学习（meta-RL）揭示这些教学信号。教师版模型为学生提出综合问题，并因对部分难题的改进而获得奖励。关键是，SOAR将课程建立在学生的衡量进步基础上，而非内在的代理奖励。我们对最难数学基准子集（0/128成功率）的研究揭示了三个核心发现。首先，我们证明了可以通过提升预训练模型潜在能力来实现双级元强化学习，从而在稀疏、二元奖励下解锁学习，从而生成有用的跳板。其次，扎根奖励优于以往大型语言模型自玩中使用的内在奖励方案，可靠地避免了它们通常表现出的不稳定性和多样性崩溃模式。第三，分析生成的问题表明，结构质量和合理性对学习进展比解题正确性更为关键。我们的结果表明，生成有用垫脚石的能力并不需要先验解决难题的能力，这为在没有额外策划数据的情况下，为摆脱推理平台铺平了一条有原则的道路。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“强化学习微调大推理模型时，一旦初始成功率极低便无法获得有效训练信号”这一瓶颈，提出并验证一个核心假设：<br><strong>预训练大模型无需先能解出难题，即可利用其潜在知识自动生成“垫脚石”式课程，从而突破自身学习平台期。</strong></p>
<p>为此，作者设计 SOAR 框架——一种双层元强化学习范式：</p>
<ul>
<li>外层：教师副本仅通过“学生在真实难题上的可测量进步”这一黑箱信号获得奖励，生成合成问答对；</li>
<li>内层：学生副本在合成数据上做标准 RLVR 训练。</li>
</ul>
<p>实验在数学竞赛题（MATH、HARP、OlympiadBench）的“128 次采样全失败”子集上进行，结果显示：</p>
<ol>
<li>自生成课程显著优于直接训练或基于内在奖励的自对弈；</li>
<li>教师策略稳定且保持多样性，避免奖励黑客与模式坍塌；</li>
<li>问题结构质量比答案正确性更关键。</li>
</ol>
<p>综上，论文首次证明**“教学能力”可与“解题能力”解耦**，为无额外人工数据情况下逃离稀疏奖励平台期提供了可行路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三大脉络，均与“如何让模型在极稀疏奖励下继续学习”密切相关：</p>
<ol>
<li>课程学习与难度调度</li>
</ol>
<ul>
<li>经典课程学习（Bengio et al. 2009；Graves et al. 2017）（<strong>假设已有可排序数据</strong>）</li>
<li>在线难度过滤 / 自适应数据选择<br>– Kimi k1.5、LightR1、FastCuRL、DAPO、Online Difficulty Filtering 等（<strong>依赖梯度范数、优势估计等代理信号</strong>）</li>
<li>近期 LLM 工作（Parashar et al. 2025；Chen et al. 2025b；Jiang et al. 2025）（<strong>仍基于人工划分难度桶或外部标签</strong>）<br>与 SOAR 区别：上述方法<strong>仅重排/筛选现成题目</strong>，而 SOAR <strong>零样本生成</strong>垫脚石题目，且奖励直接绑定“在真实难题上的进步”。</li>
</ul>
<ol>
<li>自对弈与师生非对称框架</li>
</ol>
<ul>
<li>游戏领域：AlphaZero（Silver et al. 2018）、Alice-Bob 自对弈（Sukhbaatar et al. 2017）、机器人目标发现（OpenAI et al. 2021）</li>
<li>语言模型：<br>– SPIN、Triplet-Self-Play、ReSTEM、eva（<strong>需现成 prompt 或答案可自动验证</strong>）<br>– “零数据”共生系统：Absolute Zero、R-Zero、LSP、SeRL、SQLM（<strong>用内在奖励：多数投票、可学性、梯度幅值等</strong>）<br>与 SOAR 区别：这些 LLM 自对弈工作<strong>未将教师奖励直接锚定在真实难题的改进量</strong>上，易出现奖励黑客、多样性坍塌（Chae et al. 2025；Shafayat et al. 2025）。SOAR 首次把“双层元 RL”落地到语言模型，用<strong>黑箱实测性能</strong>替代内在代理。</li>
</ul>
<ol>
<li>双层/元优化与数据集蒸馏</li>
</ol>
<ul>
<li>MAML、Reptile、RL²（Finn et al. 2017；Nichol et al. 2018；Duan et al. 2016）（<strong>需反向传播穿过内循环</strong>）</li>
<li>数据集蒸馏（Wang et al. 2018；Deng &amp; Russakovsky 2022；Feng et al. 2024）（<strong>外层优化小数据集，使内层训练后在大目标上表现好</strong>）<br>与 SOAR 区别：SOAR 用 RLOO 估计外层策略梯度，<strong>无需展开内循环</strong>，在 LLM 上实现“教师-学生双层 RL”的可扩展实例。</li>
</ul>
<p>简言之，SOAR 将<strong>课程学习“生成”侧</strong>、<strong>自对弈“师生”侧</strong>与<strong>元学习“双层”侧</strong>首次在稀疏奖励、无人工干预的数学推理场景里统一，并用<strong>实测进步</strong>取代内在代理奖励，从而避免前述方法的稳定性与可扩展性缺陷。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“零初始成功率导致 RL 训练信号消失”形式化为一个<strong>双层元强化学习</strong>问题，并用 SOAR 框架把“生成垫脚石课程”与“验证真实进步”完全自动化。关键设计如下：</p>
<ol>
<li>问题建模<br>目标：寻找合成数据集  X ，使得</li>
</ol>
<p>max<em>(φ) E</em>(Xsim π<em>φ^T)[R!(π</em>(θ’)^S(X),D_(train))], quad s.t.;θ’=RL-update(θ,X).</p>
<p>外层优化教师策略  π<em>φ^T ，内层优化学生策略  π</em>(θ’)^S ；奖励  R  仅测量学生在<strong>真实难题</strong> D_(train)  上的绝对准确率提升，<strong>不依赖合成题可验证性</strong>。</p>
<ol>
<li>双层 RL 实现（无需展开内循环）</li>
</ol>
<ul>
<li>外层：教师用 RLOO 生成  g× n  道合成问答对，按数据集粒度接收奖励。</li>
<li>内层：学生用同一 RLOO 算法在合成题上训练 10–15 步即回滚，<strong>计算开销可控</strong>。</li>
<li>稳定估计：对每批合成题并行训练  r=4  个学生，平均其改进量作为教师奖励。</li>
</ul>
<ol>
<li><p>学生晋升机制<br>维护移动平均奖励  bar R<em>t ；一旦  bar R_t&gt;τ ，立即把当前最佳学生设为新的 baseline，并累积对应合成题到  D</em>(best) （即 PQ 数据集）。教师后续必须让<strong>更强学生</strong>再进步，才能继续获得正奖励，从而自动递进课程难度。</p>
</li>
<li><p>奖励接地 → 避免黑客<br>教师<strong>从未看到真实难题本身</strong>，唯一信号是“学生在黑箱难题上的准确率变化”，天然抑制生成无意义或过于简单/困难的题目；内在代理奖励（如可学性）则因与最终性能脱钩而被实验证实易坍塌。</p>
</li>
<li><p>结构与正确性解耦<br>合成题仅 32.8 % 答案正确，但 63 % 题目数学上良定；元 RL 显著降低“歧义”错误，而算术错误率基本不变。说明<strong>问题结构+递进难度</strong>已足以提供可学习信号，答案对错并非关键。</p>
</li>
</ol>
<p>通过上述机制，SOAR 在 MATH/HARP 的 fail@128 子集上把 pass@32 从 9.6 % 提升到 18.9 %（+9.3 %），并跨数据集迁移至 OlympiadBench；同时教师策略保持多样性（Vendi Score 与基座模型相当），而内在奖励对照组出现多样性坍塌与训练崩溃。由此证明：<br><strong>“教学生”的能力可以脱离“自己会做题”的能力，且只需用真实进步作为唯一奖励，即可在稀疏奖励环境下持续生成有效课程，突破平台期。</strong></p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“能否在零初始成功率场景自生成有效课程”设计了三类实验，全部以 Llama-3.2-3B-Instruct 为骨干，在数学推理基准的 fail@128 子集（128 次采样全失败题）上完成，总计 600+ 独立训练运行。</p>
<ol>
<li>主实验：验证自生成课程能否解锁学习<br>数据集</li>
</ol>
<ul>
<li>MATH-fail@128（359 train / 360 test）</li>
<li>HARP-fail@128（714 / 714）</li>
<li>OlympiadBench-fail@128（158 / 158，完全 hold-out）</li>
</ul>
<p>条件</p>
<ul>
<li>Hard-Only：直接在 fail@128 train 上跑标准 RL（group=32 或 128）</li>
<li>Intrinsic-T：教师用“可学性”内在奖励训练，再采样 128 题+真实难题混合训练学生</li>
<li>SOAR-PQ：用晋升过程中累积的合成题  D_(best) （128–256 题）+真实难题训练新学生</li>
<li>SOAR-PS：取训练过程中最佳“晋升学生”直接做推理</li>
<li>上界：用官方 MATH 全部 6750 题或随机 128 道真实易题+fail@128 训练</li>
</ul>
<p>指标<br>pass@k（k=1,4,8,16,32），每题 32 样本，6–12 组嵌套种子，报告 median±SD。</p>
<p>结果</p>
<ul>
<li>PQ 相对 Hard-Only 在 MATH pass@32 提升 +9.3 %（9.6→18.9），HARP +4.2 %（8.2→12.3）</li>
<li>PS 提升与 PQ 接近，说明增益来自<strong>数据本身</strong>而非特定训练轨迹</li>
<li>Intrinsic-T 仅 +4.5 %，且方差大；增加 4× 组大小的 Hard-Only 仅 +2.8 %，证明<strong>额外采真实难题无法替代合成垫脚石</strong></li>
<li>PQ-MATH/HARP 在 hold-out OlympiadBench 上仍分别 +6 %/+3 %，显示<strong>跨领域迁移</strong></li>
</ul>
<ol>
<li>教师策略对比：接地奖励 vs 内在奖励<br>采样 128 题来自</li>
</ol>
<ul>
<li>Base-T（未训练教师）</li>
<li>Grounded-T（SOAR 训练后教师）</li>
<li>Intrinsic-T（可学性奖励训练）</li>
<li>Grounded-T(no promotion)（无学生晋升机制）</li>
</ul>
<p>训练新学生后观测</p>
<ul>
<li>Grounded-T 稳定优于 Base-T 与 Intrinsic-T，pass@32 提升 5–8 %，且种子间方差最低</li>
<li>Intrinsic-T 3 个种子中有 1 个导致学生<strong>完全崩溃</strong>，验证内在奖励易黑客</li>
<li>晋升机制缺失时性能明显下降，表明<strong>递进式 baseline 更新是教师质量关键</strong></li>
</ul>
<ol>
<li>多样性 &amp; 题目质量细查</li>
</ol>
<ul>
<li>Vendi Score（Qwen3-8B 嵌入）<br>Base-T 34.9 → Grounded-T 31–35（保持）→ Intrinsic-T 10.8（严重坍塌）</li>
<li>Claude-4.5-Sonnet 人工裁判 384 题<br>Grounded-T/PQ 仅 32–36 % 答案正确，但 64–70 % 题目<strong>良定</strong>；Intrinsic-T 55 % 正确却多样性低，反而效果差</li>
<li>错误分类：Grounded-T 显著降低“歧义”错误（42→21 %），算术错误略有上升，说明元 RL 把优化压力放在<strong>问题可理解性</strong>而非答案精度</li>
</ul>
<ol>
<li>消融与灵敏度</li>
</ol>
<ul>
<li>采样规模：从 Grounded-T 抽取 32/64/128 题，128 题方差最低，性能饱和</li>
<li>超参数：n=64、τ=0.01 组合最优；n=32 在大盘抽样时劣势明显</li>
<li>多轮生成：问题与答案分两轮产生，性能<strong>全面低于</strong>单轮生成，说明格式一致性对训练信号重要</li>
</ul>
<ol>
<li>计算代价对照</li>
</ol>
<ul>
<li>SOAR 一次外层循环≈48–60 H200/H100 卡时；但把同等算力直接用于增大组大小或延长 Hard-Only 训练<strong>无法</strong>达到 PQ 效果，证实双层范式<strong>效率优势</strong>不在总计算量，而在<strong>数据生成方向</strong></li>
</ul>
<p>综上，实验系统验证了：</p>
<ol>
<li>自生成垫脚石可真正“解锁”原本零信号的难题；</li>
<li>只有<strong>接地奖励</strong>才能稳定产出高多样性、低歧义的合成题；</li>
<li>问题结构质量比答案正确性更决定课程有效性；</li>
<li>该方法跨数据集迁移且对采样预算、超参数具有可预测灵敏度。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“效率提升”“能力扩展”“理论深挖”与“应用外延”四条主线：</p>
<p>效率提升</p>
<ul>
<li><strong>轻量级奖励代理</strong><br>用小型验证器/过程奖励模型（PRM）近似“真实准确率提升”，减少内循环学生采样次数，把  mathcal O(r· g)  倍学生训练降至常数级。</li>
<li><strong>单模型自举</strong><br>尝试教师-学生共享参数，仅通过不同提示或早停深度实现角色分离，避免双份 3B 模型显存开销。</li>
<li><strong>课程长度自适应</strong><br>当前晋升阈值  τ  固定，可探索按提升速率动态调整，或直接用“预期边际收益”停时，减少无效外层步数。</li>
</ul>
<p>能力扩展</p>
<ul>
<li><strong>过程级垫脚石</strong><br>把合成粒度从“整题”细化到“关键引理/中间步骤”，让教师生成可验证的子目标，进一步缓解答案错误带来的噪声。</li>
<li><strong>多模态与代码</strong><br>在几何图示、物理仿真或编程竞赛场景测试：当可执行反馈（编译器、渲染器）存在时，接地奖励信号更丰富，是否仍需要双层元 RL？</li>
<li><strong>难度谱自估计</strong><br>教师当前对“难题”完全黑箱，可引入轻量回归器预测  Delta Acc，实现“不看标签但看分数”的灰盒接地，或用于在线决定生成分布的熵。</li>
</ul>
<p>理论深挖</p>
<ul>
<li><strong>收敛性与最优性</strong><br>外层 reward 非平稳（baseline 晋升导致跳变），可形式化为“层次博弈”或“多阶段 MDP”，分析 Nash regret 与样本复杂度。</li>
<li><strong>多样性保持机制</strong><br>从信息几何角度，把 Vendi Score 作为正则项加入教师目标，证明其对抗模式坍塌的理论保证。</li>
<li><strong>课程复杂度度量</strong><br>借鉴算法信息论，定义“最小可学习课程长度”，探讨合成题数量与最终性能之间的 scaling law，验证是否存在“课程版 Chinchilla”最优分配。</li>
</ul>
<p>应用外延</p>
<ul>
<li><strong>领域迁移零数据</strong><br>在医学、法律等无标准题库领域，仅给定 50–100 道“专家级”难题，检验 SOAR 能否自动生成合规、可解释的垫脚石问答，真正替代人工写题。</li>
<li><strong>人机协同教学</strong><br>教师模型实时向人类教师推荐“下一步最适合学生的题”，形成“AI 出题—人类审核—学生练习”闭环，评估教学效率与知识留存率。</li>
<li><strong>自我迭代超参数</strong><br>把外层 RL 的组大小  n 、晋升阈值  τ  等超参也当成教师输出的一部分，实现“教什么”与“怎么教”同时元学习，迈向完全自调参的终身课程系统。</li>
</ul>
<p>以上任何一点若取得突破，均可把“模型自己教自己”从数学竞赛的稀疏奖励场景推向更通用、更经济、更可解释的广泛领域。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Teaching Models to Teach Themselves</strong> 提出 SOAR 框架，用<strong>双层元强化学习</strong>让预训练大模型在“零初始成功率”的数学难题集上<strong>自生成垫脚石课程</strong>，从而突破稀疏奖励平台期。核心结论与贡献如下：</p>
<ol>
<li>问题设定</li>
</ol>
<ul>
<li>针对 RLVR 在 fail@128（128 次采样全失败）子集上训练信号消失的场景，<strong>无需额外人工数据</strong>，仅利用模型潜在知识。</li>
</ul>
<ol>
<li>SOAR 框架</li>
</ol>
<ul>
<li>外层：教师 π^T_φ 用 RLOO 生成合成问答对，奖励 = <strong>学生在真实难题上的准确率提升</strong>。</li>
<li>内层：学生 π^S_θ 在合成题上训练 10–15 步后回滚，提供黑箱反馈。</li>
<li>晋升机制：当移动平均奖励 &gt; τ 时，将当前最佳学生设为新的 baseline，并累积对应数据 𝒟_best（PQ）。</li>
</ul>
<ol>
<li>主要实验结果</li>
</ol>
<ul>
<li><strong>MATH fail@128</strong>：PQ 把 pass@32 从 9.6 % 提升到 18.9 %（+9.3 %），显著优于直接训练、内在奖励自对弈或 4× 采样基线。</li>
<li><strong>跨域迁移</strong>：PQ-MATH/HARP 在 hold-out OlympiadBench 上仍提升 +6 %/+3 %。</li>
<li><strong>教师策略稳定性</strong>：接地奖励教师（Grounded-T）保持多样性（Vendi Score≈34），内在奖励教师坍塌至 10.8；前者学生训练曲线低方差，后者出现全面崩溃种子。</li>
<li><strong>结构 &gt; 正确性</strong>：最佳合成题仅 32.8 % 答案正确，但 64 % 以上良定，足以提供有效梯度。</li>
</ul>
<ol>
<li>结论</li>
</ol>
<ul>
<li><strong>教学能力可与解题能力解耦</strong>；模型无需先会解难题，即可通过接地元 RL 生成有用垫脚石，<strong>真正扩展可学习边界</strong>而非仅锐化已有分布。</li>
</ul>
<ol>
<li>局限与未来</li>
</ol>
<ul>
<li>双层循环计算昂贵；需更轻量奖励代理、过程级课程、领域迁移与理论分析。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Teaching%20Models%20to%20Teach%20Themselves%3A%20Reasoning%20at%20the%20Edge%20of%20Learnability，链接是https://arxiv.org/pdf/2601.18778，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18778。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shobhita Sundaram,John Quan,Ariel Kwiatkowski,Kartik Ahuja,Yann Ollivier,Julia Kempe</p>
<p><strong>Categories</strong>: cs.LG,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18778.pdf">https://arxiv.org/pdf/2601.18778.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18778">https://arxiv.org/abs/2601.18778</a></p>
<p><strong>Arxiv ID</strong>: 2601.18778</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18778">https://papers.cool/arxiv/2601.18778</a></p>
<p><strong>Published</strong>: 2026-01-26T18:46:56Z</p>
<p><strong>Updated</strong>: 2026-01-26T18:46:56.000Z</p>
<hr>
<h3 id="8-Self-Refining-Video-Sampling"><a href="#8-Self-Refining-Video-Sampling" class="headerlink" title="8. Self-Refining Video Sampling"></a>8. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18577">Self-Refining Video Sampling</a></h3><p>Modern video generators still struggle with complex physical dynamics, often falling short of physical realism. Existing approaches address this using external verifiers or additional training on augmented data, which is computationally expensive and still limited in capturing fine-grained motion. In this work, we present self-refining video sampling, a simple method that uses a pre-trained video generator trained on large-scale datasets as its own self-refiner. By interpreting the generator as a denoising autoencoder, we enable iterative inner-loop refinement at inference time without any external verifier or additional training. We further introduce an uncertainty-aware refinement strategy that selectively refines regions based on self-consistency, which prevents artifacts caused by over-refinement. Experiments on state-of-the-art video generators demonstrate significant improvements in motion coherence and physics alignment, achieving over 70\% human preference compared to the default sampler and guidance-based sampler.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现代视频生成器在处理复杂的物理动态时仍然存在困难，常常达不到物理真实性的要求。现有的方法通常通过外部验证器或对增强数据进行额外训练来解决这一问题，但这计算成本高，并且在捕捉细粒度运动上仍然有限。在本工作中，我们提出了自我优化视频采样，这是一种简单的方法，它使用在大规模数据集上预训练的视频生成器作为自身的自我优化器。通过将生成器视为去噪自编码器，我们能够在推理阶段进行迭代的内部循环优化，而无需任何外部验证器或额外训练。我们进一步引入了一种不确定性感知的优化策略，根据自我一致性有选择地对区域进行优化，从而避免因过度优化而产生的伪影。在最先进的视频生成器上的实验表明，该方法在运动连贯性和物理一致性方面显著提升，相较于默认采样器和基于引导的采样器，超过70%的用户偏好选择了我们的方法。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“现有视频生成器难以刻画复杂物理动态、导致运动不连贯或违背物理规律”这一核心问题，提出无需外部验证器、也无需额外训练的<strong>推理阶段自优化采样</strong>方法。具体而言：</p>
<ul>
<li><p><strong>问题背景</strong><br>扩散/流匹配视频模型虽已能生成逼真画面，但在多物体交互、复杂人体运动、刚体碰撞等场景下仍频繁出现：</p>
</li>
<li><p>运动不一致（如肢体重复、变形）</p>
</li>
<li>物理不可信（如物体瞬移、穿透）</li>
<li>时空不连贯（如背景闪烁、相机大角度旋转后场景内容突变）</li>
<li><strong>现有手段的局限</strong></li>
</ul>
<ol>
<li>外部验证+拒绝采样：接受率低、计算爆炸，且验证器往往领域专用，难以评估时序与物理合理性。</li>
<li>增训或微调：需高质量物理数据或昂贵重训练，且奖励模型对细粒度运动捕捉有限。</li>
</ol>
<ul>
<li><strong>本文目标</strong><br>仅利用<strong>已在大规模数据上预训练好的视频生成器本身</strong>，在推理阶段通过“自监督”方式迭代修正中间潜码，使输出向高似然、高物理合理性的数据流形靠近，从而<strong>零额外训练、零外部模型</strong>地显著提升运动连贯性与物理一致性。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两大主线，并在第 2 节“Related Works”中系统回顾。以下按主题归纳，并给出关键文献出处（仅列代表工作，完整列表见原文）：</p>
<ol>
<li>生成模型“自优化”范式</li>
</ol>
<ul>
<li>语言模型：Self-Refine（Madaan et al., 2023）提出“批判-重写”循环；Reasoning with Sampling（Karan &amp; Du, 2025）用 MCMC 仅依赖基础模型做推理。</li>
<li>扩散模型：ZigzagDiffusion（Bai et al., 2025a）在采样阶段交替引导去噪与反演，实现自反思。</li>
</ul>
<ol>
<li>视频运动连贯性改进</li>
</ol>
<ul>
<li>训练阶段引入辅助目标：VideoJAM（Chefer et al., 2025）联合光流去噪；FlowMo（Shaulov et al., 2025）提出无训练方差引导。</li>
<li>推理阶段引导：FreeInit（Wu et al., 2024）迭代优化初始噪声；Restart（Xu et al., 2023）用前向-重启降低误差累积。</li>
</ul>
<ol>
<li>物理真实感提升</li>
</ol>
<ul>
<li>增训或专用数据：WISA（Wang et al., 2025c）用物理 MoE；PhyGDPO（Cai et al., 2025a）在增广数据上做 DPO；Zhang et al. 2025 用 CGI 合成数据。</li>
<li>外部物理模块：GPT4Motion（Lv et al., 2023）、PhysGen（Liu et al., 2024）、VLIPP（Yang et al., 2025a）借助 LLM 或物理引擎做高层规划。</li>
</ul>
<ol>
<li>流匹配/扩散理论基础</li>
</ol>
<ul>
<li>流匹配：Lipman et al., 2022 提出连续向量场 ODE 框架；后续视频模型如 Wan2.1/2.2、Cosmos-2.5 均在该框架下训练。</li>
<li>去噪自编码器视角：Vincent 2011、Bengio et al., 2013 将扩散/流匹配目标解释为时间条件广义 DAE，为本文“伪吉布斯”迭代提供理论依据。</li>
</ul>
<ol>
<li>评估基准</li>
</ol>
<ul>
<li>运动质量：VideoJAM-bench、Dynamic-bench（本文新提出）。</li>
<li>物理合理性：VideoPhy2、PhyWorldBench、PisaBench（自由落体轨迹）。</li>
<li>机器人操作：PAI-Bench（含抓取成功率与 VQA）。</li>
</ul>
<p>综上，本文首次把“自优化”思想从语言/图像域系统迁移到<strong>视频流匹配模型</strong>，在<strong>无需外部验证器或再训练</strong>的前提下，仅通过推理阶段“Predict-and-Perturb”迭代即可提升复杂运动与物理一致性，与上述训练或引导类方法形成互补。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将预训练好的流匹配视频生成器视为<strong>时间条件去噪自编码器（DAE）</strong>，在推理阶段对中间潜码做“伪吉布斯”迭代修正，具体分三步：</p>
<p>1. 重新诠释流匹配目标——把它当成 DAE</p>
<p>流匹配训练目标</p>
<p>L<em>(FM)=E</em>(t,z<em>0,z_1)|u</em>θ(z_t,t)-(z_1-z_0)|_2^2</p>
<p>可等价写成</p>
<p>L<em>(FM)=E</em>(t,z<em>0,z_1)(1) / ((1-t)^2)|z_1^θ-z_1|_2^2,quad z_1^θ:=z_t+(1-t)u</em>θ(z_t,t).</p>
<p>这恰好是<strong>加权广义 DAE 损失</strong>：模型学会把带噪潜码  z_t  还原成干净数据  z_1 。于是<strong>任意中间时刻  t  的前向推理</strong>都等价于一次“去噪重构”。</p>
<p>2. Predict-and-Perturb（P&amp;P）内循环</p>
<p>利用上述 DAE 性质，在同一噪声等级  t  交替执行：</p>
<ul>
<li><strong>Predict（重构）</strong><br>D<em>θ(z_t,t):=z_t+(1-t)u</em>θ(z_t,t)arrow z_1 </li>
<li><strong>Perturb（再腐蚀）</strong><br>R_ε(z_1,t):=tz_1+(1-t)ε,;εsimN(0,I)arrow z_t^+ </li>
</ul>
<p>一次“Predict → Perturb”称为一个 P&amp;P 步，重复  K_fle 3  次即可把  z_t  推向高密区域，对应更连贯、更物理合理的视频。该过程<strong>完全自给</strong>：无需外部评分器，也不更新权重。</p>
<p>3. 不确定性感知掩码——防止过度修正</p>
<p>多次 P&amp;P 会让静态背景因累积 CFG 而出现过饱和。作者用<strong>自一致性</strong>信号生成不确定性图</p>
<p>U(z<em>t^((k-1)),z_t^((k)))=(1) / (C)|D</em>θ(z<em>t^((k-1)),t)-D</em>θ(z_t^((k)),t)|_1</p>
<p>并二值化得掩码 $M_t^((k))=1<br>U&gt;τ<br>$。最终只把 P&amp;P 结果写入掩码为 1 的时空区域，其余像素保持原样，实现<strong>选择性局部精炼</strong>。</p>
<p>4. 嵌入现有 ODE 求解器——即插即用</p>
<p>把标准求解步<br> z<em>(t_i+1)=z</em>(t<em>i)+(t</em>(i+1)-t<em>i)u</em>θ(z<em>(t_i),t_i)<br>中的  z</em>(t<em>i)  替换成经  K_f  次 P&amp;P 后的精炼潜码  z</em>(t<em>i)^</em>  即可。实验表明*<em>仅在早期  t&lt;0.2T  执行 P&amp;P_</em> 就能显著改善运动，额外计算约 1.5× NFE。</p>
<p>结果</p>
<ul>
<li>在 Wan2.1/2.2、Cosmos-2.5 等最新模型上，<strong>人类偏好率 &gt;70%</strong>，运动连贯、物理合理、空间一致性全面提升。</li>
<li>对机器人抓取、自由落体轨迹、360° 相机旋转等严苛场景均优于加倍 NFE、外部验证器或训练式引导方法。</li>
</ul>
<p>综上，论文通过“<strong>把生成器当自编码器→同噪级迭代精炼→不确定性掩码防过修</strong>”这一无需再训练的策略，系统解决了复杂物理动态建模难题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 5 组基准上系统评估了“Predict-and-Perturb”（P&amp;P）自优化采样，共覆盖<strong>运动连贯性、物理真实感、空间一致性、视觉推理</strong>四大维度，并辅以消融实验与可视化分析。实验均基于<strong>单张 NVIDIA H100 80 GB</strong>，所有结果可复现。</p>
<p>1. 运动连贯性（Motion Coherence）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>规模</th>
<th>重点场景</th>
</tr>
</thead>
<tbody>
<tr>
<td>Dynamic-bench（新提）</td>
<td>120 条提示</td>
<td>多物体交互 / 复杂人体 / 物理驱动</td>
</tr>
<tr>
<td>VideoJAM-bench</td>
<td>原基准</td>
<td>同上</td>
</tr>
</tbody>
</table>
</div>
<p><strong>指标</strong></p>
<ul>
<li>自动化：VBench 的 Motion↑、Consistency↑</li>
<li>人类：20 名评估者，双盲 pairwise，报告 tie-adjusted win-rate</li>
</ul>
<p><strong>主结果</strong></p>
<ul>
<li>Wan2.2-A14B T2V 上，<strong>人类偏好 73.6%</strong>（vs 默认 ODE），VBench Motion 从 98.01→<strong>98.41</strong>；</li>
<li>对高难度体操、撑杆跳等场景，默认采样出现“多臂”“漂浮”失败，P&amp;P 得到正确姿态与接触（图 4、图 24）。</li>
</ul>
<p>2. 机器人操作物理真实性（Robotics I2V）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>规模</th>
<th>评估维度</th>
</tr>
</thead>
<tbody>
<tr>
<td>PAI-Bench-G</td>
<td>174 段机器人提示</td>
<td>抓取成功率 / Robot-QA / VBench Quality</td>
</tr>
</tbody>
</table>
</div>
<p><strong>评估器</strong></p>
<ul>
<li>抓取：Gemini-3-Flash 高分辨率评分（4-5 分视为成功）</li>
<li>QA：Qwen2.5-VL-72B 回答物理相关问句</li>
</ul>
<p><strong>主结果</strong></p>
<ul>
<li>Cosmos-Predict-2.5：抓取率 79.2→<strong>89.6</strong>（+10.4 pp），QA 提升 4.6 pp，NFE 仅 1.6×；</li>
<li>Wan2.2-I2V：抓取率 77.3→<strong>85.7</strong>（+8.4 pp），<strong>优于 best-of-4 外部验证器</strong>（80.5），且推理成本降低 2.5×（表 2）。</li>
</ul>
<p>3. 野外物理一致性（Physics in the Wild）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>基准</th>
<th>规模</th>
<th>指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>VideoPhy2</td>
<td>360 段（hard+easy）</td>
<td>物理常识 PC↑ / 语义对齐 SA↑</td>
</tr>
<tr>
<td>PhyWorldBench</td>
<td>70 段（运动学+交互）</td>
<td>同上</td>
</tr>
<tr>
<td>PisaBench（自由落体）</td>
<td>全真实数据集 + 3 场景×32 样本</td>
<td>L2、Chamfer、IoU</td>
</tr>
</tbody>
</table>
</div>
<p><strong>主结果</strong></p>
<ul>
<li>人类评估：对默认采样，<strong>84% 偏爱 P&amp;P</strong>；对 2×NFE，<strong>74% 偏爱</strong>（表 3）。</li>
<li>自动评估：PhyWorldBench PC 相对提升 <strong>≈10 pp</strong>；自由落体轨迹更贴近真实抛物线，IoU 从 0.069→<strong>0.074</strong>（表 4）。</li>
</ul>
<p>4. 空间一致性（Large-Camera-Motion）</p>
<p><strong>设置</strong></p>
<ul>
<li>20 条 360° 第一人称旋转提示</li>
<li>MegaSaM 估计相机位姿 + 深度，重访视角时做 warp，计算 SSIM/L1/PSNR</li>
</ul>
<p><strong>主结果</strong></p>
<ul>
<li>默认 ODE 在相机转回后背景常“变脸”；P&amp;P 保持同一墙面、同一招牌（图 7）。</li>
<li>SSIM 0.401→<strong>0.485</strong>，PSNR 14.96→<strong>17.21 dB</strong>（表 5）。</li>
</ul>
<p>5. 视觉推理任务（Emergent Reasoning）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>任务</th>
<th>成功率 Base → P&amp;P</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>图遍历（Graph traversal）</td>
<td>0.1 → 0.8</td>
<td>需渐进传播，运动一致性可修正</td>
</tr>
<tr>
<td>迷宫求解（Maze solving）</td>
<td>0 → ≈0</td>
<td>需全局路径规划，局部精炼无效（图 8、18、19）</td>
</tr>
</tbody>
</table>
</div>
<p>6. 消融与超参数实验</p>
<ul>
<li><strong>不确定性掩码必要性</strong>：Kf=5 时，关闭掩码出现整体过饱和、水面反光异常；τ=0.25 可消除（图 9、16）。</li>
<li><strong>P&amp;P 次数 Kf</strong>：1 次不足，3 次性价比最高，&gt;5 次边际收益小且 NFE 线性增加（图 16）。</li>
<li><strong>作用区间 α</strong>：早期步（t&lt;0.05T）决定运动骨架，此时 P&amp;P 收益最大；晚期步仅微调（图 17）。</li>
<li><strong>跨域验证</strong>：相同策略用于 FLUX.1-dev 图像生成，2 次 P&amp;P 即可显著减少文字粘连，NFE 仅 +4%（图 15）。</li>
</ul>
<p>7. 与商业闭源模型对比</p>
<ul>
<li>与 Google Veo 3.1、快手 Kling 2.6 在相同高难度运动提示下并排生成，<strong>视觉质量略逊，但运动合理性/物理正确性可比肩或更优</strong>（图 27）。</li>
</ul>
<p>综上，实验覆盖<strong>从人体运动到机器人抓取、从自由落体到 360° 相机旋转、从低层物理到高层推理</strong>，充分证明 P&amp;P 在<strong>零再训练、零外部模型</strong>前提下，可一致提升现代视频生成器的运动连贯性与物理真实感。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>论文在第 6.3 节与附录 C 已指出若干局限，结合实验观察可提炼出以下<strong>可直接延伸或尚未解决</strong>的探索方向：</p>
<p>1. 自适应/动态超参数</p>
<ul>
<li><strong>时变阈值 τ(t)</strong>：目前 uncertainty 阈值固定，实验显示早期不确定性高、后期低。可让 τ 随 t 衰减或根据累计置信度自动调节，兼顾“早修正-晚保真”。</li>
<li><strong>迭代次数 Kf(t, content)</strong>：对不同运动复杂度或文本语义，实时预测最优 Kf，避免“一刀切”。</li>
</ul>
<p>2. 全局-局部混合搜索</p>
<ul>
<li>P&amp;P 本质为<strong>局部搜索</strong>，对“迷宫”类需全局规划的任务无效。可引入：</li>
<li>外部离散规划器（LLM+符号推理）提供初始路径，再用 P&amp;P 做局部运动细化；</li>
<li>多级重启机制：当不确定性长期不降时，强制回滚到早期噪声并重新采样，实现“局部→全局”逃逸。</li>
</ul>
<p>3. 多模型协同精炼</p>
<ul>
<li><strong>自集成</strong>：用同一模型的 EMA 检查点或不同随机种子生成多个预测，把方差作为更精确的不确定性估计。</li>
<li><strong>异模型精炼</strong>：主模型负责生成，小模型（或蒸馏模型）专做“物理/语义”批判，迭代交替，兼顾效率与精度。</li>
</ul>
<p>4. 更丰富的物理先验注入</p>
<ul>
<li>把连续物理方程（碰撞、摩擦、流体力学）写成可微惩罚项，在 Predict 步做<strong>梯度友好</strong>的物理引导，而非仅靠数据驱动隐式先验。</li>
<li>引入可微刚体/粒子模拟器（DiffPD、DiffTaichi）做<strong>一次 rollout</strong>，与 P&amp;P 预测互作一致性损失，实现“神经-物理”双迭代。</li>
</ul>
<p>5. 控制信号与交互式生成</p>
<ul>
<li>目前 P&amp;P 仅接受文本/首帧条件。可扩展至：</li>
<li>稀疏轨迹、深度、力-扭矩等多模态输入，在 Perturb 步把控制误差注入噪声，实现<strong>物理量闭环</strong>；</li>
<li>实时用户交互：每步 P&amp;P 后即时显示低分辨率预览，用户点击纠错区域并更新不确定性掩码，实现“人在回路”精炼。</li>
</ul>
<p>6. 理论侧：收敛性与模式寻求</p>
<ul>
<li>实验观察到 P&amp;P 会<strong>降低输出多样性</strong>（图 21、22）。需量化：</li>
<li>给定 Kf、τ，输出分布与真实后验的 KL 或 Wasserstein 距离上界；</li>
<li>引入“多样性正则”项或随机重启，平衡“物理正确 vs 多样性”。</li>
</ul>
<p>7. 跨架构通用化</p>
<ul>
<li>本文聚焦<strong>流匹配</strong>。对混合框架（扩散-流混合、一致性模型、Poisson 流）是否同样有效？需要重新推导 Predict/Perturb 算子。</li>
<li>视频 DiT 与 Conv-UNet 的 uncertainty 形态不同，可研究<strong>架构感知的掩码生成策略</strong>。</li>
</ul>
<p>8. 高效推理与端侧部署</p>
<ul>
<li>P&amp;P 增加 20-40% NFE。可探索：</li>
<li>早期退出：当 uncertainty 低于某值时提前终止迭代；</li>
<li>共享计算图：把多次 Predict 的公共特征图缓存，用 group-query 或 KV-cache 减少显存与延迟，实现<strong>端侧 30 FPS</strong> 实时精炼。</li>
</ul>
<p>9. 长视频与无限时长生成</p>
<ul>
<li>当前实验最长 2-3 秒。对长视频（&gt;10 s）或“自回归式”块生成，P&amp;P 是否会导致<strong>误差漂移</strong>？可研究：</li>
<li>块间重叠窗口 + 不确定性累积掩码，保持跨块物理一致；</li>
<li>引入记忆向量，把前一块的终端隐码作为先验，约束当前块 P&amp;P 搜索空间。</li>
</ul>
<p>10. 可解释与可视化</p>
<ul>
<li>不确定性图目前只用于掩码。可进一步：</li>
<li>把时序 uncertainty 映射到语义图，自动生成“物理错误热力图”供开发者诊断；</li>
<li>提供交互式网页 Demo，逐帧显示 P&amp;P 前后潜码差异，帮助用户理解“模型为何修正”。</li>
</ul>
<p>综上，<strong>自适应控制、全局-局部混合、显式物理耦合、多样性-收敛性理论、长时生成与端侧加速</strong>将是把“自优化视频采样”推向实用化与下一代世界模型的关键研究路线。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>Self-Refining Video Sampling</strong> 提出一种<strong>零额外训练、零外部模型</strong>的推理阶段自优化方法，让预训练流匹配视频生成器“自己修正自己”，显著提升复杂运动与物理真实性。核心内容可概括为四句话：</p>
<ol>
<li>把流匹配目标重新解释为<strong>时间条件去噪自编码器</strong>（DAE），从而在任何中间噪声级 <code>t</code> 都能“重构-再腐蚀”。</li>
<li>基于此提出 <strong>Predict-and-Perturb（P&amp;P）</strong>：在同一 <code>t</code> 交替执行</li>
</ol>
<ul>
<li>Predict： z<em>t+(1-t)u</em>θ(z_t,t)arrow hat z_1 </li>
<li>Perturb： that z_1+(1-t)εarrow z_t^+<br>仅需 2–3 次循环即可把潜码推向高密-高可信区域。</li>
</ul>
<ol>
<li>引入<strong>不确定性感知掩码</strong>，只精炼模型“没信心”的时空区域，避免静态背景因累积 CFG 而过饱和。</li>
<li>在 Wan2.1/2.2、Cosmos-2.5 等最新模型上<strong>即插即用</strong>，运动连贯性人类偏好 &gt;70%，机器人抓取成功率提升 8–10 个百分点，自由落体、360° 相机旋转等场景物理一致性全面超越加倍 NFE 或外部验证器方案。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Self-Refining%20Video%20Sampling，链接是https://arxiv.org/pdf/2601.18577，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18577。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Sangwon Jang,Taekyung Ki,Jaehyeong Jo,Saining Xie,Jaehong Yoon,Sung Ju Hwang</p>
<p><strong>Categories</strong>: cs.CV,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18577.pdf">https://arxiv.org/pdf/2601.18577.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18577">https://arxiv.org/abs/2601.18577</a></p>
<p><strong>Arxiv ID</strong>: 2601.18577</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18577">https://papers.cool/arxiv/2601.18577</a></p>
<p><strong>Published</strong>: 2026-01-26T15:22:27Z</p>
<p><strong>Updated</strong>: 2026-01-26T15:22:27.000Z</p>
<hr>
<h3 id="9-VIBEVOICE-ASR-Technical-Report"><a href="#9-VIBEVOICE-ASR-Technical-Report" class="headerlink" title="9. VIBEVOICE-ASR Technical Report"></a>9. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18184">VIBEVOICE-ASR Technical Report</a></h3><p>This report presents VibeVoice-ASR, a general-purpose speech understanding framework built upon VibeVoice, designed to address the persistent challenges of context fragmentation and multi-speaker complexity in long-form audio (e.g., meetings, podcasts) that remain despite recent advancements in short-form speech recognition. Unlike traditional pipelined approaches that rely on audio chunking, VibeVoice-ASRsupports single-pass processing for up to 60 minutes of audio. It unifies Automatic Speech Recognition, Speaker Diarization, and Timestamping into a single end-to-end generation task. In addition, VibeVoice-ASR supports over 50 languages, requires no explicit language setting, and natively handles code-switching within and across utterances. Furthermore, we introduce a prompt-based context injection mechanism that allows users to supply customized conetxt, significantly improving accuracy on domain-specific terminology and polyphonic character disambiguation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>本报告介绍了 VibeVoice-ASR，一种基于 VibeVoice 构建的通用语音理解框架，旨在解决长音频（例如会议、播客）中持久存在的上下文碎片化和多说话人复杂性问题，这些问题在短时语音识别的最新进展中仍然存在。与依赖音频分块的传统流水线方法不同，VibeVoice-ASR 支持长达 60 分钟音频的单次处理。它将自动语音识别、说话人区分和时间戳标注统一为一个端到端的生成任务。此外，VibeVoice-ASR 支持 50 多种语言，无需明确设置语言，并且能够原生处理语句内和语句间的语言混合。更进一步，我们引入了一种基于提示的上下文注入机制，允许用户提供自定义上下文，显著提高了对特定领域术语和多声部字符歧义的识别准确性。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>长时音频（如会议、播客）中上下文碎片化与多说话人复杂性</strong>带来的挑战。传统级联流水线将连续语音切分为短片段（&lt;30 s）独立处理，导致：</p>
<ol>
<li><strong>上下文碎片化</strong>：跨句语义依赖被切断，同音词消歧和指代消解失效。</li>
<li><strong>流水线复杂</strong>：ASR、说话人分离、时间戳被拆分为独立模型，误差累积。</li>
</ol>
<p>VIBEVOICE-ASR 通过<strong>单遍端到端生成框架</strong>统一三项任务，直接输出“谁-何时-说了什么”的富文本流，在 60 min 音频上实现全局语义一致性与说话人追踪，无需外部聚类或分块。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 VIBEVOICE-ASR 直接可比或同期推进的研究可归纳为三类：</p>
<ol>
<li>长时-单遍联合建模</li>
</ol>
<ul>
<li><strong>TagSpeech</strong> (Huo et al., arXiv 2026)<br>端到端多说话人 ASR + 分离，细粒度时间锚定，但仍采用分段推理。</li>
<li><strong>SpeakerLM</strong> (Yin et al., arXiv 2025)<br>利用多模态 LLM 做说话人日志与识别，需外部 VAD 与聚类。</li>
<li><strong>MOSS-Transcribe-Diarize</strong> (Yu et al., arXiv 2026)<br>单模型输出转录与说话人标签，未公开模型，仅支持 ≤30 min。</li>
</ul>
<ol>
<li>短片段级联再拼接</li>
</ol>
<ul>
<li><strong>WhisperX</strong> (Bain et al., 2023)<br>Whisper + 强制对齐 + 说话人向量聚类，典型“切-识-拼”流程。</li>
<li><strong>Emilia</strong> (He et al., SLT 2024)<br>大规模多语数据管道，仍依赖 30 s 分段与后处理融合。</li>
</ul>
<ol>
<li>Speech-LLM 零样本长音频</li>
</ol>
<ul>
<li><strong>Train-Short-Infer-Long</strong> (Shi et al., arXiv 2025)<br>语音 LLM 在训练时仅见短句，推理通过 KV-Cache 外推至长序列，未同时解决说话人分离。</li>
</ul>
<p>上述工作均未能同时满足“单遍、60 min、端到端、公开模型”四个条件，VIBEVOICE-ASR 在开源与长时联合优化上形成差异。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将长时多说话人语音理解重新形式化为<strong>单遍端到端语言建模任务</strong>，通过以下关键设计一次性解决上下文碎片化与流水线复杂性问题：</p>
<ul>
<li><p><strong>超低帧率双令牌编码</strong><br>直接复用 VIBEVOICE 预训练的双编码器：</p>
</li>
<li><p>声学令牌 7.5 tokens/s，一小时音频仅 ≈27 000 tokens，可完整放入现代 LLM 上下文窗口；</p>
</li>
<li>语义令牌与文本对齐，保留全局语义依赖。</li>
<li><p><strong>统一生成目标：Rich Transcription</strong><br>自回归输出结构化 token 流，显式交错<br><code>SpeakerID, start→end, “content”</code><br>同时完成 ASR、说话人日志与时间戳，无需外部聚类或后处理。</p>
</li>
<li><p><strong>提示式上下文注入</strong><br>用户可在音频序列前追加任意文本提示（热词、背景、实体列表），模型在生成时利用该上下文消歧多音字、领域术语与跨语码切换。</p>
</li>
<li><p><strong>课程式长序列预训练 + 多源 SFT</strong></p>
</li>
</ul>
<ol>
<li>预训练：从 8 k 到 65 k token 逐步扩长，使用自研高质量伪标签管道（VAD→Whisper-large-v3-turbo→vblinkp/HDBSCAN→二次 ASR 过滤）。</li>
<li>SFT：按 0.5 : 0.1 : 0.1 : 0.3 混合标准多说话人基准、音乐数据、上下文感知合成音频、以及 GPT-5 精炼的长时转录，确保模型在“内容-说话人-时间”三维度一致对齐。</li>
</ol>
<p>通过上述手段，VIBEVOICE-ASR 在 60 min 级别音频上实现单前向推理，显著降低 DER 与 tcpWER，消除传统分段带来的误差累积与语义断裂。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 <strong>5 个公开多说话人会议/对话基准</strong> 上，与 <strong>Gemini-2.5-Pro、Gemini-3-Pro</strong> 两个闭源多模态大模型进行了全面对比，实验覆盖 <strong>16 种语言</strong>，共报告 <strong>4 套互补指标</strong>：</p>
<ol>
<li>实验数据集</li>
</ol>
<ul>
<li>AISHELL-4（中文会议）</li>
<li>AMI-IHM / AMI-SDM（英文会议，近场/远场）</li>
<li>AliMeeting（中文多方会议）</li>
<li>MLC-Challenge（16 种语言自然对话）</li>
</ul>
<ol>
<li>评价指标</li>
</ol>
<ul>
<li><strong>DER</strong>（Diarization Error Rate）——只关心“谁何时说话”。</li>
<li><strong>WER</strong>（Word Error Rate）——忽略说话人与时间，纯文本准确度。</li>
<li><strong>cpWER</strong>（concatenated minimum-permutation WER）——先按说话人拼句，再求最优排列，衡量“内容+说话人一致性”。</li>
<li><strong>tcpWER</strong>（time-constrained cpWER）——在 cpWER 基础上加 ±0.5 s 时间 collar，同时考核“谁-何时-内容”三者对齐。</li>
</ul>
<ol>
<li>对比方法</li>
</ol>
<ul>
<li>Gemini-2.5-Pro、Gemini-3-Pro：官方 API，因长音频时间戳漂移严重，统一按 240 s 分段推理后拼接。</li>
<li>VIBEVOICE-ASR：单遍端到端，不切片，不后处理。</li>
</ul>
<ol>
<li>主要结果（平均 16 语）</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>DER ↓</th>
<th>cpWER ↓</th>
<th>tcpWER ↓</th>
<th>WER ↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemini-2.5-Pro</td>
<td>16.29</td>
<td>20.37</td>
<td>28.90</td>
<td>13.05</td>
</tr>
<tr>
<td>Gemini-3-Pro</td>
<td>32.96</td>
<td>16.38</td>
<td>58.81</td>
<td>13.11</td>
</tr>
<tr>
<td>VIBEVOICE-ASR</td>
<td>3.42</td>
<td>14.81</td>
<td>15.66</td>
<td>12.07</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li>消融与补充验证</li>
</ol>
<ul>
<li>数据管道对比：在 AMI、AliMeeting、AISHELL-4 上，自研管道 DER/WER 均低于 WhisperX 与 Emilia。</li>
<li>上下文注入消融：提供热词列表后，领域术语错误率相对下降 18–32 %（中文多音字、英中混合专有名词）。</li>
<li>长时鲁棒性：随音频长度从 10 min 增至 60 min，tcpWER 增幅 &lt; 1.5 %，验证单遍建模无显著漂移。</li>
</ul>
<p>实验结论：VIBEVOICE-ASR 在所有数据集、所有语言上均取得 <strong>最低 DER 与 tcpWER</strong>，并在 11/16 种语言拿下 cpWER 第一，证明其在<strong>说话人归因、时间对齐、多语泛化</strong>三维度同时领先。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可继续推进，均直接源于论文已暴露的局限或尚未充分验证的场景：</p>
<ol>
<li>低资源多语遗忘</li>
</ol>
<ul>
<li>现象：SFT 阶段仅中英+码切换，其余 50+ 语言性能下滑。</li>
<li>探索：① 继续收集 30 min 以上低资源长音频；② 采用 LoRA/Adapter 按语言簇增量微调，避免覆盖预训练表征；③ 引入“语言提示前缀”实现运行时语言切换，无需显式语言 ID。</li>
</ul>
<ol>
<li>重叠语音（“鸡尾酒会”）建模</li>
</ol>
<ul>
<li>现状：串行输出流只能保留主导说话人。</li>
<li>探索：① 在训练数据中注入人工混合重叠片段，输出并行说话人块 <code>&lt;speakerA, t0-t1&gt;…&lt;speakerB, t0-t1&gt;…</code>；② 采用分离-编码-融合两路 latent 的架构，先前端盲源分离，再统一生成；③ 引入“能量-导向”注意力掩码，让模型显式感知第二说话人能量峰。</li>
</ul>
<ol>
<li>更长上下文外推</li>
</ol>
<ul>
<li>现状：60 min≈27 k token 已接近 LLM 窗口上限。</li>
<li>探索：① 帧率再压缩（&lt; 5 Hz）+ 分层 Transformer，先局部摘要再全局生成；② 采用 Recurrent Memory Transformer 或 Streaming-LLM 技术，实现 3–4 h 会议单遍处理；③ 研究“记忆-摘要”交替生成，每 30 min 自动插入 <code>&lt;summary&gt;</code> token，防止 KV 缓存线性增长。</li>
</ul>
<ol>
<li>实时流式推理</li>
</ol>
<ul>
<li>现状：模型为全自回归离线模式。</li>
<li>探索：① 引入前缀缓存与增量式 Speaker Embedding 更新，实现 1-s 级别延迟的流式 diarization；② 采用双通道输出：确定性前缀（已确认说话人+文本）+ 滚动草稿区（当前待定说话人），用户端可实时看到“部分结果”。</li>
</ul>
<ol>
<li>多模态事件精细化</li>
</ol>
<ul>
<li>现状：仅用 GPT-Audio2 标注 6 类粗粒度非语音标签。</li>
<li>探索：① 引入细粒度音频事件检测头，同步输出 <laughter>、<applause>、<knock> 等 50+ 标签；② 与视觉流对齐，实现“谁边笑边说”多模态时间戳；③ 联合训练语音-文本-事件三元组，提升模型对情绪、场景转换的感知。</li>
</ul>
<ol>
<li>域自适应与个性化</li>
</ol>
<ul>
<li>现状：提示式热词仅支持一次性注入。</li>
<li>探索：① 持续学习框架，用户私有数据通过参数高效微调（AdaLoRA）本地合并，不泄露回云端；② 引入“说话人特定记忆库”，对高频人名、产品名自动缓存为临时嵌入，下次会议免重复提示；③ 联邦学习聚合梯度，仅上传低秩更新，保护企业敏感录音。</li>
</ul>
<ol>
<li>鲁棒性诊断与可解释性</li>
</ol>
<ul>
<li>现状：仅给出整体 DER/WER。</li>
<li>探索：① 构建对抗性长音频测试集：含突发噪声、设备切换、远场混响；② 可视化交叉注意力图，验证模型在长距离指代时是否真正 attend 到全局上下文；③ 建立“错误类型树”：分离同音错误、说话人混淆、时间漂移、幻觉插入，便于针对性改进。</li>
</ul>
<ol>
<li>高效部署与压缩</li>
</ol>
<ul>
<li>现状：依赖 7B-LLM 骨干 + 双编码器，显存占用高。</li>
<li>探索：① 对声学编码器做 8-bit 量化并保持 7.5 Hz 输出，验证是否损失 diarization；② 采用 Speculative Decoding：小模型生成草稿，大模型并行验证，提升 2× 推理吞吐；③ 研究“分块-投机”混合策略：静音段用小模型，语音段回退大模型，进一步降低平均延迟。</li>
</ul>
<p>通过上述方向的持续迭代，可逐步把 VIBEVOICE-ASR 从“60-min 离线多语模型”推向“任意时长、实时、重叠鲁棒、个性化”的下一代长时语音理解基座。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>VIBEVOICE-ASR</strong>，一个面向<strong>任意 60 分钟级长音频</strong>的<strong>单遍端到端语音理解框架</strong>，核心贡献与结果如下：</p>
<p>1. 问题背景</p>
<ul>
<li>传统“切-识-拼”流水线在会议、播客等长音频上遭遇<strong>上下文碎片化</strong>与<strong>多模型误差累积</strong>。</li>
<li>现有 Speech-LLM 多聚焦短句，<strong>公开、可单遍处理 60 min 的多说话人方案缺失</strong>。</li>
</ul>
<p>2. 方法概览</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>关键设计</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入编码</td>
<td>复用 VIBEVOICE 双令牌器：声学 7.5 tokens/s + 语义对齐，1 h 音频≈27 k tokens，一次喂入 LLM。</td>
</tr>
<tr>
<td>生成目标</td>
<td>自回归输出 Rich Transcription：SpeakerID, t0-t1, “content”，同步完成 ASR + 说话人日志 + 时间戳。</td>
</tr>
<tr>
<td>上下文注入</td>
<td>可选文本提示（热词、背景） prepend 到音频序列，提升领域术语与多音字消歧。</td>
</tr>
<tr>
<td>训练策略</td>
<td>课程式预训练（8 k→65 k token）+ 多源 SFT：标准基准 : 音乐 : 合成上下文数据 : 长时精修 = 0.5 : 0.1 : 0.1 : 0.3。</td>
</tr>
</tbody>
</table>
</div>
<p>3. 实验与结果</p>
<ul>
<li><strong>基准</strong>：AISHELL-4、AMI-IHM/SDM、AliMeeting、MLC-Challenge（共 16 语）。</li>
<li><strong>对手</strong>：Gemini-2.5-Pro、Gemini-3-Pro（240 s 分段推理）。</li>
<li><strong>指标</strong>：DER、WER、cpWER、tcpWER。</li>
</ul>
<div class="table-container">
<table>
<thead>
<tr>
<th>平均（16 语）</th>
<th>DER ↓</th>
<th>cpWER ↓</th>
<th>tcpWER ↓</th>
<th>WER ↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>Gemini-2.5-Pro</td>
<td>16.29</td>
<td>20.37</td>
<td>28.90</td>
<td>13.05</td>
</tr>
<tr>
<td>Gemini-3-Pro</td>
<td>32.96</td>
<td>16.38</td>
<td>58.81</td>
<td>13.11</td>
</tr>
<tr>
<td>VIBEVOICE-ASR</td>
<td>3.42</td>
<td>14.81</td>
<td>15.66</td>
<td>12.07</td>
</tr>
</tbody>
</table>
</div>
<ul>
<li><strong>全胜</strong>：DER 与 tcpWER 在所有数据集均最低；cpWER 在 11/16 语拿下第一。</li>
<li><strong>数据管道验证</strong>：自研流程 DER/WER 普遍低于 WhisperX、Emilia。</li>
<li><strong>上下文消融</strong>：热词提示使领域术语错误率相对下降 18–32 %。</li>
</ul>
<p>4. 开放资源</p>
<ul>
<li>模型权重、微调代码、vLLM 推理实现全部开源（GitHub/HuggingFace）。</li>
</ul>
<p>5. 局限与未来方向</p>
<ul>
<li>SFT 阶段低资源语言不足 → 多语遗忘。</li>
<li>串行输出无法建模重叠语音 → 需分离式或并行生成。</li>
<li>窗口上限 60 min → 探索更压缩表示或流式记忆机制。</li>
</ul>
<p><strong>一句话总结</strong>：VIBEVOICE-ASR 用超低帧率令牌+单遍 LLM 生成，首次在<strong>开源、单模型、60 min、多语、多说话人</strong>场景下同时取得<strong>最优说话人日志与时间对齐性能</strong>，为长时语音理解提供了新的基线与工具链。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VIBEVOICE-ASR%20Technical%20Report，链接是https://arxiv.org/pdf/2601.18184，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18184。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhiliang Peng,Jianwei Yu,Yaoyao Chang,Zilong Wang,Li Dong,Yingbo Hao,Yujie Tu,Chenyu Yang,Wenhui Wang,Songchen Xu,Yutao Sun,Hangbo Bao,Weijiang Xu,Yi Zhu,Zehua Wang,Ting Song,Yan Xia,Zewen Chi,Shaohan Huang,Liang Wang,Chuang Ding,Shuai Wang,Xie Chen,Furu Wei</p>
<p><strong>Categories</strong>: cs.SD,cs.AI,eess.AS</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18184.pdf">https://arxiv.org/pdf/2601.18184.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18184">https://arxiv.org/abs/2601.18184</a></p>
<p><strong>Arxiv ID</strong>: 2601.18184</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18184">https://papers.cool/arxiv/2601.18184</a></p>
<p><strong>Published</strong>: 2026-01-26T06:11:51Z</p>
<p><strong>Updated</strong>: 2026-01-26T06:11:51.000Z</p>
<hr>
<h3 id="10-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval"><a href="#10-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval" class="headerlink" title="10. CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval"></a>10. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15849">CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval</a></h3><p>General-purpose embedding models have demonstrated strong performance in text retrieval but remain suboptimal for table retrieval, where highly structured content leads to semantic compression and query-table mismatch. Recent LLM-based retrieval augmentation methods mitigate this issue by generating synthetic queries, yet they often rely on heuristic partial-table selection and seldom leverage these synthetic queries as supervision to improve the embedding model. We introduce CGPT, a training framework that enhances table retrieval through LLM-generated supervision. CGPT constructs semantically diverse partial tables by clustering table instances using K-means and sampling across clusters to broaden semantic coverage. An LLM then generates synthetic queries for these partial tables, which are used in hard-negative contrastive fine-tuning to refine the embedding model. Experiments across four public benchmarks (MimoTable, OTTQA, FetaQA, and E2E-WTQ) show that CGPT consistently outperforms retrieval baselines, including QGpT, with an average R@1 improvement of 16.54 percent. In a unified multi-domain corpus setting, CGPT further demonstrates strong cross-domain generalization and remains effective even when using smaller LLMs for synthetic query generation. These results indicate that semantically guided partial-table construction, combined with contrastive training from LLM-generated supervision, provides an effective and scalable paradigm for large-scale table retrieval. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/yumeow0122/CGPT">https://github.com/yumeow0122/CGPT</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>通用嵌入模型在文本检索中表现出强大的性能，但在表格检索中仍不够理想，因为高度结构化的内容会导致语义压缩和查询-表格不匹配。最近基于大语言模型（LLM）的检索增强方法通过生成合成查询缓解了这一问题，但它们通常依赖于启发式的部分表格选择，并且很少利用这些合成查询作为监督来改进嵌入模型。我们提出了 CGPT，这是一种通过 LLM 生成的监督来增强表格检索的训练框架。CGPT 通过使用 K-means 对表格实例进行聚类并跨聚类采样来构建语义多样的部分表格，从而扩大语义覆盖范围。随后，LLM 为这些部分表格生成合成查询，这些查询被用于困难负样本对比微调，以优化嵌入模型。在四个公开基准（MimoTable、OTTQA、FetaQA 和 E2E-WTQ）上的实验显示，CGPT 始终优于包括 QGpT 在内的检索基线，平均 R@1 提升 16.54%。在统一的多领域语料库设置中，CGPT 进一步展示了强大的跨领域泛化能力，即使使用较小的 LLM 生成合成查询也仍然有效。这些结果表明，结合 LLM 生成监督进行对比训练的语义引导部分表格构建，为大规模表格检索提供了一种有效且可扩展的范式。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/yumeow0122/CGPT">https://github.com/yumeow0122/CGPT</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对通用稠密检索模型在表格检索场景下的两大缺陷——语义压缩与查询-表格失配——提出改进方案。具体而言：</p>
<ul>
<li><strong>语义压缩</strong>：将整张表压成单一向量会丢失跨行分布的关键信息，尤其当相关证据位于表中部或尾部时。</li>
<li><strong>查询-表格失配</strong>：纯文本预训练的编码器难以捕捉高度结构化内容，导致查询与表向量在语义空间中对不齐。</li>
</ul>
<p>现有 LLM 增强方法（如 QGpT）仅启发式地取前 k 行生成合成查询，既无法保证代表性，也未利用这些查询作为监督信号来微调编码器本身，从而留下性能缺口。</p>
<p>CGPT 通过“聚类引导的子表构造 + 合成查询监督下的难负例对比微调”同时解决上述问题，在零样本设置下显著提升表格检索的 R@1，并验证其跨域与跨模型规模的泛化能力。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 CGPT 直接相关的研究可归纳为两条主线：</p>
<ol>
<li><strong>表格检索模型</strong></li>
</ol>
<ul>
<li>TAPAS：引入行列位置编码并在百万级 Wiki 表上预训练，为后续表格 QA/检索提供基础骨干。</li>
<li>DTR（Dense Table Retrieval）：双编码器结构 + 逆完形填空任务 + 难负例采样，首次将稠密检索范式迁移到表格场景。</li>
<li>CLTR：端到端 Transformer，支持单元格级检索，缓解整表向量压缩问题。</li>
</ul>
<ol>
<li><strong>表征增强与合成查询</strong></li>
</ol>
<ul>
<li>QGpT：仅用前 10 行构造子表，调用 LLM 生成合成查询，提升零样本检索，但未微调编码器。</li>
<li>GenSearch：根据查询生成“伪表”，探索生成式检索路径。</li>
<li>对比预测编码（InfoNCE）：提供难负例对比损失，被 CGPT 用于将合成查询转化为训练监督。</li>
</ul>
<p>上述工作共同揭示了“整表向量压缩”与“缺乏训练监督”两大瓶颈，CGPT 在此基础上引入聚类引导的子表采样与 LLM 生成的对比微调，填补空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>CGPT 把问题拆解为四个连续阶段，形成端到端训练管线：</p>
<ol>
<li><strong>聚类式子表生成（KPT）</strong><br>对表中每一行用预训练编码器得到行向量，按表规模自适应确定簇数</li>
</ol>
<p>k=min!l(lfloor m/r rfloor,, k_(max)r)</p>
<p>再以 K-means 聚类，跨簇随机采样  s  行并拼上表头，得到  k  个子表，保证语义覆盖度。</p>
<ol>
<li><p><strong>合成查询生成</strong><br>对每个子表调用 LLM，在可控温度下生成  n_q  条自然语言查询，覆盖实体、时序、比较、聚合与复杂推理五类问题，为后续监督提供正样本。</p>
</li>
<li><p><strong>难负例采样</strong><br>用现成编码器计算查询向量与其他表子表的余弦相似度，取 Top- h  最高但标签为负的子表作为难负例，构建强干扰信号。</p>
</li>
<li><p><strong>对比微调</strong><br>以三元组  (q, p^+, HN)  形式，采用 InfoNCE 损失</p>
</li>
</ol>
<p>L= -logexp!l(rm sim(q,p^+)/τr) {exp!l(rm sim(q,p^+)/τr) + ∑_(p^-∈ HN)exp!l(rm sim(q,p^-)/τr)}</p>
<p>端到端微调编码器，使查询与正子表拉近、与难负子表推远，从而缓解语义压缩与失配问题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验按“主结果 → 跨策略迁移 → 跨域泛化 → 跨模型规模 → 采样策略消融”递进展开，覆盖 4 个公开数据集与 1 个混合语料，共 5 组评测：</p>
<ol>
<li><strong>主实验</strong><br>在 MimoTable（中/英）、OTTQA、FetaQA、E2E-WTQ 上与 QGpT、CGPT 无微调、CGPT 无难负例等对照比较，报告 R@1/5/10。</li>
</ol>
<ul>
<li>CGPT 平均 R@1 提升 16.54%，最高在 E2E-WTQ 达 +30.71 个百分点。</li>
<li>仅聚类子表（无微调）即可显著超越 QGpT，验证语义覆盖的重要性。</li>
</ul>
<ol>
<li><p><strong>跨策略迁移</strong><br>将 CGPT 微调后的编码器直接用于 QGpT 原始子表（前 10 行）做检索，R@1 在 MimoTable-EN 从 50.66%→59.28%，证明所学表征对构造方式不敏感。</p>
</li>
<li><p><strong>统一多域语料</strong><br>把 4 个数据集合并成 27 万张表的大规模语料，考察跨域鲁棒性。</p>
</li>
</ol>
<ul>
<li>KPT 子表在零样本 BGE-M3 上已大幅领先 QGpT（E2E-WTQ +18.67 R@1）。</li>
<li>再经 CGPT 微调后，MimoTable-EN R@1 提升至 57.79%，相对基线提升 18.33 个百分点。</li>
</ul>
<ol>
<li><p><strong>不同 LLM 规模</strong><br>用 Llama-3.1-8B、GPT-OSS-20B、Qwen3-4B 分别生成查询，R@1 差异 &lt;0.6%，表明方法对生成模型规模不敏感，可用小模型降低成本。</p>
</li>
<li><p><strong>采样策略消融</strong><br>对比“簇内随机采样”“簇中心采样”“仅保留簇中心一条”三种策略，随机采样在双语设置下均保持最高 R@1，验证多样性是关键。</p>
</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<ul>
<li><p><strong>行列联合聚类</strong><br>目前仅对行向量做 K-means，可进一步将“列语义”或“单元格语义”纳入联合聚类，使子表同时覆盖分散的行与关键列。</p>
</li>
<li><p><strong>自适应子表大小</strong><br>簇内采样数  s  与簇数  k  为全局超参，可依据表格“信息熵”或“主题密度”动态调整，兼顾表示完整性与计算开销。</p>
</li>
<li><p><strong>层次化难负例</strong><br>除“表级”负例外，引入“列级”“单元格级”假阳性样本，构建多层次对比损失，强化模型对细粒度语义的判别能力。</p>
</li>
<li><p><strong>生成查询质量过滤</strong><br>现有方法直接保留全部合成查询，可探索基于“可回答性置信度”或“与表匹配度”的自动过滤/重排序，减少噪声监督。</p>
</li>
<li><p><strong>跨模态扩展</strong><br>将文本-表格检索推广至“文本+表格+图像”场景，利用表格中常出现的图表、图标，研究视觉-结构联合表征。</p>
</li>
<li><p><strong>小参数模型自举</strong><br>尝试用经 CGPT 微调的轻量编码器反哺合成查询生成，形成“生成-检索”迭代自举，逐步降低对大参数 LLM 的依赖。</p>
</li>
<li><p><strong>增量与流式索引</strong><br>针对网络规模表格持续增长的场景，研究聚类与子表生成的增量更新机制，避免全量重训练，实现实时检索。</p>
</li>
<li><p><strong>可解释检索</strong><br>结合聚类结果生成“子表摘要”或“证据高亮”，向用户解释为何返回该表，提升系统可信度与调试效率。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>CGPT 核心内容速览</strong></p>
<ol>
<li><p><strong>问题</strong><br>通用文本嵌入模型把整张表压成单向量 → 语义压缩 + 查询-表格失配，检索精度低。</p>
</li>
<li><p><strong>关键局限</strong></p>
</li>
</ol>
<ul>
<li>现有 QGpT 仅取前 k 行，代表性不足；</li>
<li>合成查询只用于增强表示，未用作训练监督。</li>
</ul>
<ol>
<li><p><strong>方法（四步流水线）</strong></p>
</li>
<li><p><strong>KPT</strong>：行级编码 → 自适应 K-means 聚类 → 跨簇随机采样 → 构造 k 个子表，扩大语义覆盖。</p>
</li>
<li><strong>SQG</strong>：LLM 为每个子表生成 nq 条多类型合成查询。</li>
<li><strong>HNS</strong>：用现成编码器取 Top-h 相似但负标的子表作难负例。</li>
<li><strong>对比微调</strong>：InfoNCE 损失微调嵌入模型，拉近查询-正子表、推远难负子表。</li>
<li><strong>实验结果</strong></li>
</ol>
<ul>
<li>4 个公开数据集平均 R@1 提升 16.54%，最高 +30.71%。</li>
<li>跨策略、跨域、跨 LLM 规模均保持高鲁棒性；小至 4B 模型也能取得一致性能。</li>
<li>消融表明：聚类带来主要增益，难负例进一步提升精度。</li>
</ul>
<ol>
<li><strong>结论</strong><br>聚类引导的子表构造 + LLM 生成的对比监督，可在不修改模型架构的前提下显著提升大规模表格检索效果，且计算成本低、易于落地。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是CGPT%3A%20Cluster-Guided%20Partial%20Tables%20with%20LLM-Generated%20Supervision%20for%20Table%20Retrieval，链接是https://arxiv.org/pdf/2601.15849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15849。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Tsung-Hsiang Chou,Chen-Jui Yu,Shui-Hsiang Hsu,Yao-Chung Fan</p>
<p><strong>Categories</strong>: cs.IR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15849.pdf">https://arxiv.org/pdf/2601.15849.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15849">https://arxiv.org/abs/2601.15849</a></p>
<p><strong>Arxiv ID</strong>: 2601.15849</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15849">https://papers.cool/arxiv/2601.15849</a></p>
<p><strong>Published</strong>: 2026-01-22T10:58:56Z</p>
<p><strong>Updated</strong>: 2026-01-22T10:58:56.000Z</p>
<hr>
<h3 id="11-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints"><a href="#11-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints" class="headerlink" title="11. DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints"></a>11. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18137">DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints</a></h3><p>While agent evaluation has shifted toward long-horizon tasks, most benchmarks still emphasize local, step-level reasoning rather than the global constrained optimization (e.g., time and financial budgets) that demands genuine planning ability. Meanwhile, existing LLM planning benchmarks underrepresent the active information gathering and fine-grained local constraints typical of real-world settings. To address this, we introduce DeepPlanning, a challenging benchmark for practical long-horizon agent planning. It features multi-day travel planning and multi-product shopping tasks that require proactive information acquisition, local constrained reasoning, and global constrained optimization. Evaluations on DeepPlanning show that even frontier agentic LLMs struggle with these problems, highlighting the importance of reliable explicit reasoning patterns and parallel tool use for achieving better effectiveness-efficiency trade-offs. Error analysis further points to promising directions for improving agentic LLMs over long planning horizons. We open-source the code and data to support future research.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>虽然代理评估已经转向长周期任务，但大多数基准仍然强调局部的、逐步的推理，而不是需要真正规划能力的全局约束优化（例如时间和财务预算）。与此同时，现有的 LLM 规划基准在实际场景中常见的主动信息获取和细粒度局部约束方面表现不足。为了解决这一问题，我们推出了 DeepPlanning，这是一个针对实际长周期代理规划的挑战性基准。它包括多日旅行规划和多产品购物任务，要求主动获取信息、进行局部约束推理以及全局约束优化。在 DeepPlanning 上的评估表明，即便是前沿的代理式 LLM 也难以应对这些问题，这凸显了可靠的显性推理模式和并行工具使用对于实现更佳效益-效率权衡的重要性。错误分析进一步指出了在长周期规划中改进代理式 LLM 的有前景方向。我们开源了代码和数据以支持未来的研究。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决现有大模型智能体（LLM agents）在长程（long-horizon）规划评估中的三大空白：</p>
<ol>
<li><p>全局约束优化缺位<br>既有基准侧重单步、局部约束（如“选带洗衣机的三星酒店”），忽视整趟行程或整个购物车必须满足的<strong>全局硬约束</strong>（总预算 ≤ ¥18 000、总时长 ≤ 7 天、跨天活动耦合）。</p>
</li>
<li><p>主动信息获取不足<br>现有任务常把所需信息一次性给出；真实场景下代理必须<strong>主动调用工具</strong>才能发现隐性状态（航班只剩 2 张票、景点周一闭馆、用户鞋码 43）。</p>
</li>
<li><p>细粒度局部约束推理缺失<br>除用户显式偏好外，代理还需处理<strong>环境隐性约束</strong>（营业时间、库存、优惠券叠加规则），当前基准要么约束过粗，要么直接忽略。</p>
</li>
</ol>
<p>为此，作者提出 DEEPPLANNING 基准，通过“多日程旅行+多商品购物”两类高真实度沙箱任务，强制代理同时具备：</p>
<ul>
<li>主动信息获取</li>
<li>局部约束推理</li>
<li>全局约束优化</li>
</ul>
<p>并给出可复现的离线数据库、规则化验证器与细粒度评价指标，系统揭示前沿模型在长程规划上的脆弱性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为两条主线，并指出其局限：</p>
<ol>
<li>智能体工具使用评估</li>
</ol>
<ul>
<li>短程、单步工具调用：API-Bank (Li et al. 2023)、ToolLLM (Qin et al. 2024)、BFCL (Patil et al. 2025)</li>
<li>长程但“指令跟随”导向：WebShop (Yao et al. 2022)、Mind2Web (Deng et al. 2023)、WebArena (Zhou et al. 2024)、WebVoyager (He et al. 2024)、DeepShop (Lyu et al. 2025)<br>共性缺陷：侧重<strong>复杂指令解析</strong>而非<strong>多步规划与全局资源优化</strong>，缺乏对预算、时间、库存等硬约束的系统性验证。</li>
</ul>
<ol>
<li>大模型规划能力评估</li>
</ol>
<ul>
<li>经典/时序规划抽象域：PlanBench (Valmeekam et al. 2023)、TCP (Ding et al. 2025)、TimeArena (Zhang et al. 2024)</li>
<li>旅行规划实例：TravelPlanner (Xie et al. 2024)、ChinaTravel (Shao et al. 2025)、TripTailor (Wang et al. 2025)、TripScore (Qu et al. 2025)<br>共性缺陷：<br>– 规划粒度粗（天级而非分钟级）<br>– 全局约束简化或缺失（仅总价上限、无跨天耦合）<br>– 环境信息一次性给定，无需主动检索<br>– 缺少对“隐性约束”（营业时间、余票、优惠券叠加）的自动验证机制</li>
</ul>
<p>DEEPPLANNING 在上述工作基础上，首次把<strong>主动信息获取</strong>、<strong>细粒度局部约束推理</strong>与<strong>全局硬预算/时间优化</strong>同时纳入可复现的沙箱评估，填补长程规划基准的空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文从“任务设计–环境构建–评价协议”三个层面系统解决长程规划评估缺失的问题，核心手段可概括为：</p>
<ol>
<li>反向解驱动的分层任务生成</li>
</ol>
<ul>
<li>先构建高真实度数据库（航班、酒店、餐厅、商品、优惠券等），保证数据足够复杂且可验证。</li>
<li>以“唯一最优解”为目标反向注入约束：<br>– 基骨架：确定行程/购物主题。<br>– 个性化约束：采样用户显式偏好（航班时段、品牌、尺码）。<br>– 环境隐式约束：动态调整库存、营业时段、优惠券规则，使得只有<strong>唯一组合</strong>能同时满足预算、时间、库存等全局限制。</li>
<li>最终用 LLM 把结构化约束转成自然语言查询，并经人工审核，确保语言自然、无歧义、可解。</li>
</ul>
<ol>
<li>可复现的沙箱与工具链</li>
</ol>
<ul>
<li>离线数据库+Python API 封装，代理只能<strong>通过工具</strong>获取信息，杜绝参数记忆作弊。</li>
<li>工具设计强制多步依赖：如“查坐标→查路线→查票价→查余票”必须串行完成，度量主动信息获取能力。</li>
<li>统一用 OpenAI function-calling 接口，支持并行或顺序调用，方便不同模型公平对比。</li>
</ul>
<ol>
<li>规则化、细粒度评估协议<br>旅行域：</li>
</ol>
<ul>
<li>Commonsense Score（8 维 21 子检查点）→ 验证时间无重叠、路线连续、营业时段、成本加总等。</li>
<li>Personalized Score → 用户显式偏好是否全部命中。</li>
<li>Case Accuracy → 只有当上述两项均满分才计 1，严格衡量“端到端正确”。</li>
</ul>
<p>购物域：</p>
<ul>
<li>Match Score → 代理购物车与唯一最优解的商品重合率。</li>
<li>Case Accuracy → 商品、优惠券、数量、价格必须与最优解<strong>完全一致</strong>才计 1，考核全局组合优化能力。</li>
</ul>
<ol>
<li>大规模对照实验与错误剖析</li>
</ol>
<ul>
<li>覆盖 20+ 前沿模型（含 reasoning/non-reasoning 变体），单任务最多 400 次 tool call，4 次随机重复。</li>
<li>记录工具调用轨迹，按“信息获取–局部推理–全局优化”三类错误模式人工标注 140 条失败轨迹，定位瓶颈。</li>
</ul>
<p>通过上述设计，论文既提供了<strong>可复现的硬约束环境</strong>，又给出了<strong>细粒度、自动化的评价指标</strong>，从而系统暴露出现有模型在长程规划上的脆弱性，为后续研究指明改进方向。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕 <strong>DeepPlanning</strong> 基准开展了 4 组系统性实验，覆盖 20 余个前沿模型，总计 960 条独立轨迹（240 任务 × 4 次重复）。核心实验与发现如下：</p>
<p>1. 主实验： frontier 模型整体表现</p>
<ul>
<li><p><strong>设置</strong><br>– 旅行域 120 任务（中英双语各一次）+ 购物域 120 任务<br>– 统一 function-calling 接口，最多 400 tool calls，4 轮随机种子平均</p>
</li>
<li><p><strong>指标</strong><br>旅行：Commonsense / Personalized / Composite / Case Accuracy<br>购物：Match Score / Case Accuracy<br>总榜：两域 Case Accuracy 的平均作为 <strong>Avg Acc</strong></p>
</li>
<li><p><strong>结果（表 2）</strong></p>
</li>
<li>最高平均准确率 <strong>44.6 %</strong>（GPT-5.2-high reasoning），仍远低于可用门槛</li>
<li>非推理模型平均 &lt; 27 %；推理模式普遍提升 8–20 个百分点</li>
<li>域特异性显著：Gemini-3-Flash 旅行仅 5.9 %，购物达 51.7 %</li>
</ul>
<p>2. 成本–性能权衡实验</p>
<ul>
<li><p><strong>旅行域</strong> 上统计每任务平均：<br>– tool 调用次数<br>– 交互轮次</p>
</li>
<li><p><strong>发现（图 3）</strong></p>
</li>
<li>性能 ∝ tool 次数：更多主动检索 → 更高 Composite Score</li>
<li>推理模型位于更优帕累托前沿：同等工具量得分更高，或同等得分轮次更少</li>
<li>同一家族内并行 vs 串行策略差异巨大：GPT-5.2-high 用 224 次工具、19 轮得 85.8 分；GPT-5.1-high 仅 22 次工具、2 轮得 73.1 分，揭示 <strong>效率–彻底性权衡</strong></li>
</ul>
<p>3. 任务复杂度消融实验</p>
<ul>
<li><strong>旅行</strong>：按行程天数 2→7 天划分 3 档</li>
<li><p><strong>购物</strong>：按跨商品约束数 1→3 级（价格区间、优惠券时效、组合优惠）</p>
</li>
<li><p><strong>结果（图 4）</strong></p>
</li>
<li>天数/级数增加，Composite / Case Accuracy 单调下降；7 天行程 Case Acc 掉至 &lt; 10 %</li>
<li>证明 <strong>长程耦合约束</strong> 是当前模型的共同瓶颈</li>
</ul>
<p>4. 错误模式细粒度剖析</p>
<ul>
<li><strong>样本</strong>：Claude-4.5-Opus (reasoning) 140 条失败轨迹（旅行 80 + 购物 60）</li>
<li><p><strong>三级标注</strong><br>A. 信息获取失败：漏查、工具用错、查对写错<br>B. 局部推理失败：显式约束忽略、隐性约束（库存/营业时间）冲突<br>C. 全局优化失败：预算/时间/优惠券组合不达标、日程结构断裂</p>
</li>
<li><p><strong>分布（图 5）</strong></p>
</li>
<li>旅行：全局优化 101 次 &gt; 隐性约束 86 次 &gt; 漏查 62 次</li>
<li>购物：全局优化 52 次 &gt; 漏查 28 次 &gt; 隐性约束 21 次</li>
<li>揭示 <strong>组合优化与隐性约束检测</strong> 是长程规划最大短板</li>
</ul>
<p>补充实验</p>
<ul>
<li><strong>中英双语一致性</strong>：同一模型在中文与英文旅行任务上 Case Acc 差异 &lt; 2 %，说明 benchmark 语言偏差受控</li>
<li><strong>重复稳定性</strong>：4 次随机种子标准差 ≤ 1.3 %，结果可信</li>
</ul>
<p>以上实验共同证明：即使是最先进的 LLM 智能体，在需要<strong>主动检索 + 局部推理 + 全局优化</strong>的长程任务上仍然脆弱，亟需新的训练与推理范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为 <strong>任务扩展</strong>、<strong>模型机制</strong>、<strong>训练与推理</strong>、<strong>评价与可解释性</strong> 四大类：</p>
<p>1. 任务与环境扩展</p>
<ul>
<li><p><strong>多轮交互式规划</strong><br>当前任务为单轮 query → 完整方案。可引入 <strong>用户中途修改</strong>（临时改目的地、追加好友、预算缩减），考察 agent 的 <strong>增量规划与对话式回溯</strong> 能力。</p>
</li>
<li><p><strong>跨域耦合任务</strong><br>将旅行+购物+日程管理组合成一条长链：<br>“出差 5 天，期间需在线采购会议设备并直邮酒店，总报销额度 ¥25 000，且航班变动需自动重订物流。”<br>引入 <strong>物流时效、跨境关税、退改签政策</strong> 等新约束，评估跨域全局优化。</p>
</li>
<li><p><strong>动态环境 &amp; 对抗性事件</strong><br>在沙箱中插入 <strong>随机事件流</strong>（航班突然取消、酒店超售、优惠券限时 10 分钟），测试 agent 的 <strong>在线重规划与实时决策</strong>。</p>
</li>
<li><p><strong>多智能体协作规划</strong><br>让 2–3 个 agent 分别负责交通、住宿、购物，需通过 <strong>消息协议</strong> 协商满足整体预算，考察 <strong>分布式约束优化与通信效率</strong>。</p>
</li>
</ul>
<p>2. 模型机制研究</p>
<ul>
<li><p><strong>显式世界状态缓存</strong><br>当前模型常“查对写错”或重复查询。可探索：<br>– 外部 <strong>状态缓存表</strong>（SQL 形式）强制读写分离<br>– 基于工具返回自动更新 <strong>信念向量</strong>，减少幻觉</p>
</li>
<li><p><strong>层次化规划架构</strong><br>引入 <strong>高层抽象规划器 + 低层执行控制器</strong>：<br>上层负责天数-城市-预算分配，下层负责分钟级日程与优惠券组合，通过 <strong>子目标回溯</strong> 解决长程耦合。</p>
</li>
<li><p><strong>可学习优惠券/约束 DSL</strong><br>把“满减、跨店、限时”规则定义为 <strong>可微分或搜索友好</strong> 的 DSL，允许 <strong>基于梯度的组合优化</strong> 而非暴力枚举。</p>
</li>
<li><p><strong>并行工具调度策略学习</strong><br>用强化学习自动发现 <strong>最优并行/串行调用策略</strong>，在性能-成本前沿上动态选择 Pareto 最优轨迹。</p>
</li>
</ul>
<p>3. 训练与推理改进</p>
<ul>
<li><p><strong>约束感知微调数据合成</strong><br>利用 DeepPlanning 的“唯一最优解”特性，自动生成 <strong>&lt;轨迹, 最优行动, 约束违反解释&gt;</strong> 三元组，进行 <strong>拒绝采样式微调</strong> 或 <strong>RLHF</strong> 惩罚预算/时间越界。</p>
</li>
<li><p><strong>预算/时间 token 级正则化</strong><br>在训练阶段把剩余预算、剩余时长作为 <strong>特殊 token</strong> 插入上下文，用 <strong>回归辅助任务</strong> 强制模型内部保持对资源消耗的连续估计。</p>
</li>
<li><p><strong>蒙特卡洛树搜索 + LLM 启发</strong><br>对购物车或日程进行 <strong>片段采样</strong>，用 LLM 评估“未来可能成本”，结合 MCTS 做 <strong>前瞻式展开</strong>，提升全局最优性。</p>
</li>
<li><p><strong>工具使用课程学习</strong><br>从“1 天 1 商品”渐进到“7 天 6 商品+优惠券”，让模型 <strong>逐步增长规划长度</strong>，缓解长程误差传播。</p>
</li>
</ul>
<p>4. 评价与可解释性</p>
<ul>
<li><p><strong>细粒度约束可解释报告</strong><br>自动生成 <strong>违反约束的自然语言解释</strong>（“第 3 天 14:00–15:30 与航班登机时间冲突”），用于 <strong>诊断模型缺陷</strong> 与 <strong>用户信任</strong>。</p>
</li>
<li><p><strong>人类-AI 协同规划评测</strong><br>引入 <strong>人机混合轨迹</strong>：允许人类在关键节点介入（确认酒店、选择航班），评测 agent <strong>何时主动求助</strong> 以及 <strong>介入后整体效率</strong> 是否提升。</p>
</li>
<li><p><strong>鲁棒性对抗基准</strong><br>构建 ** adversarial 约束集合**（数值边界、极端库存、互斥优惠券），系统测量模型 <strong>最坏情况性能</strong> 与 <strong>置信度校准</strong>。</p>
</li>
<li><p><strong>实时成本-效用在线评测</strong><br>在真实旅游/电商 API 上部署 <strong>轻量影子模式</strong>，对比 agent 方案与人工客服方案的实际 <strong>成交率、退款率、满意度</strong>，完成 <strong>从离线到在线</strong> 的闭环验证。</p>
</li>
</ul>
<p>一句话总结</p>
<p>DeepPlanning 确立了长程规划“工具-约束-优化”三位一体评估范式，后续工作可向 <strong>多轮动态、跨域耦合、层次架构、约束感知训练、可解释诊断</strong> 等方向深入，推动智能体从“能规划”走向“规划得可靠、规划得经济、规划得可信”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>DeepPlanning：面向可验证约束的长程智能体规划基准</strong></p>
<ol>
<li><p>问题<br>现有 LLM 智能体评测聚焦单步工具调用或局部约束，忽视真实场景中的<strong>主动信息获取、细粒度隐性约束与全局资源优化</strong>（总预算、总时长、跨日耦合）。长程规划能力缺乏可靠度量。</p>
</li>
<li><p>解决方案<br>提出 <strong>DeepPlanning</strong> 基准，含：</p>
</li>
</ol>
<ul>
<li><strong>旅行规划</strong> 120 任务（中英双语）——分钟级日程，需订交通、酒店、景点、餐厅，满足时空-预算硬约束。</li>
<li><strong>购物规划</strong> 120 任务——多商品+优惠券组合，需全局最低价且库存/尺码/时效合规。</li>
</ul>
<p>采用<strong>反向解驱动</strong>的三段式构造：<br>① 真实/合成数据库 + 15/9 个 API；② 逐层注入个性化与环境隐式约束，保证<strong>唯一最优解</strong>；③ 人工质检。</p>
<p>提供离线沙箱与<strong>规则化自动评分</strong>，无需 LLM 打分即可验证。</p>
<ol>
<li>核心能力三维评估</li>
</ol>
<ul>
<li><strong>主动信息获取</strong>：必须多步调用工具才能发现库存、营业时间、用户尺码等隐性状态。</li>
<li><strong>局部约束推理</strong>：处理显式偏好（三星带洗衣机）与隐性限制（景点闭馆、余票不足）。</li>
<li><strong>全局约束优化</strong>：整趟行程/整个购物车同时满足预算、时间、优惠券叠加等耦合约束。</li>
</ul>
<ol>
<li>实验结果（20+ 前沿模型，960 轨迹）</li>
</ol>
<ul>
<li><strong>最佳平均 Case Accuracy 仅 44.6 %</strong>（GPT-5.2-high reasoning），揭示显著脆弱性。</li>
<li>推理模式普遍提升 8–20 个百分点，但 7 天行程 Case Acc 跌至 &lt;10 %。</li>
<li>高绩效模型需 <strong>200+ 次工具调用</strong>；推理模型在同性能下<strong>交互轮次与调用量更少</strong>，形成更优成本-性能前沿。</li>
<li>错误剖析：101/140 失败源于<strong>全局组合优化崩溃</strong>，其次为隐性约束检测遗漏与关键信息漏查。</li>
</ul>
<ol>
<li>贡献</li>
</ol>
<ul>
<li>首个同时考核<strong>主动获取-局部推理-全局优化</strong>的长程规划基准，可复现、易验证。</li>
<li>大规模实验暴露前沿模型<strong>端到端一致性、组合优化与隐性约束推理</strong>三大短板。</li>
<li>开源数据与评测代码，支撑后续训练与推理范式研究。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepPlanning%3A%20Benchmarking%20Long-Horizon%20Agentic%20Planning%20with%20Verifiable%20Constraints，链接是https://arxiv.org/pdf/2601.18137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18137。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Yinger Zhang,Shutong Jiang,Renhao Li,Jianhong Tu,Yang Su,Lianghao Deng,Xudong Guo,Chenxu Lv,Junyang Lin</p>
<p><strong>Categories</strong>: cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18137.pdf">https://arxiv.org/pdf/2601.18137.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18137">https://arxiv.org/abs/2601.18137</a></p>
<p><strong>Arxiv ID</strong>: 2601.18137</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18137">https://papers.cool/arxiv/2601.18137</a></p>
<p><strong>Published</strong>: 2026-01-26T04:43:49Z</p>
<p><strong>Updated</strong>: 2026-01-26T04:43:49.000Z</p>
<hr>
<h3 id="12-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion"><a href="#12-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion" class="headerlink" title="12. STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion"></a>12. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.15860">STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion</a></h3><p>Table retrieval is the task of retrieving the most relevant tables from large-scale corpora given natural language queries. However, structural and semantic discrepancies between unstructured text and structured tables make embedding alignment particularly challenging. Recent methods such as QGpT attempt to enrich table semantics by generating synthetic queries, yet they still rely on coarse partial-table sampling and simple fusion strategies, which limit semantic diversity and hinder effective query-table alignment. We propose STAR (Semantic Table Representation), a lightweight framework that improves semantic table representation through semantic clustering and weighted fusion. STAR first applies header-aware K-means clustering to group semantically similar rows and selects representative centroid instances to construct a diverse partial table. It then generates cluster-specific synthetic queries to comprehensively cover the table’s semantic space. Finally, STAR employs weighted fusion strategies to integrate table and query embeddings, enabling fine-grained semantic alignment. This design enables STAR to capture complementary information from structured and textual sources, improving the expressiveness of table representations. Experiments on five benchmarks show that STAR achieves consistently higher Recall than QGpT on all datasets, demonstrating the effectiveness of semantic clustering and adaptive weighted fusion for robust table representation. Our code is available at <a target="_blank" rel="noopener" href="https://github.com/adsl135789/STAR">https://github.com/adsl135789/STAR</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>表检索是指在给定自然语言查询的情况下，从大规模语料库中检索最相关的表格的任务。然而，非结构化文本与结构化表格之间的结构和语义差异使得嵌入对齐尤其具有挑战性。近期方法如 QGpT 试图通过生成合成查询来丰富表格语义，但它们仍然依赖于粗略的部分表采样和简单的融合策略，这限制了语义多样性并阻碍了有效的查询-表对齐。我们提出了 STAR（语义表表示），这是一个轻量级框架，通过语义聚类和加权融合提升表格语义表示。STAR 首先应用考虑表头的 K-means 聚类，将语义相似的行分组，并选择具有代表性的质心实例来构建多样化的部分表格。然后，它生成特定聚类的合成查询，以全面覆盖表格的语义空间。最后，STAR 采用加权融合策略整合表格和查询嵌入，实现细粒度的语义对齐。该设计使 STAR 能够从结构化和文本来源捕获互补信息，从而提升表格表示的表达能力。在五个基准数据集上的实验表明，STAR 在所有数据集上的召回率均持续高于 QGpT，验证了语义聚类和自适应加权融合在稳健表格表示中的有效性。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/adsl135789/STAR">https://github.com/adsl135789/STAR</a> 获得。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对**表格检索（table retrieval）**中“非结构化查询”与“结构化表格”之间的语义鸿沟，以及长表格超出编码器 token 限制导致的匹配困难，提出以下核心问题：</p>
<ol>
<li>现有方法（如 QGpT）采用<strong>启发式 top-k 行采样</strong>，只能获得局部、片面且缺乏语义多样性的表格片段，难以代表整张表的语义空间。</li>
<li>QGpT 将部分表格与合成查询<strong>简单拼接</strong>后统一编码，无法显式控制“结构化信息”与“文本化查询”各自对最终表示的贡献，导致细粒度对齐能力不足。</li>
</ol>
<p>因此，论文旨在：</p>
<ul>
<li>用<strong>带表头感知的聚类</strong>替代 top-k 采样，选出语义覆盖更广、代表性更强的行集合；</li>
<li>为每个聚类簇单独生成合成查询，全面覆盖表格的多元语义；</li>
<li>引入<strong>自适应加权融合</strong>机制，动态调节表格嵌入与查询嵌入的权重，实现更精细的查询-表格语义对齐。</li>
</ul>
<p>综上，STAR 的目标是<strong>在无需改动底层检索器的前提下，构建轻量级框架，显著提升表格语义表示的完整性与检索鲁棒性</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Works”中将与 STAR 相关的研究划分为四条主线，并指出各自与 STAR 的差异。可归纳如下：</p>
<ol>
<li>稀疏检索阶段</li>
</ol>
<ul>
<li>代表：BM25 及其概率相关框架（Robertson &amp; Zaragoza 2009）。</li>
<li>特点：仅依赖词项重叠，难以捕获深层语义。</li>
<li>STAR 差异：完全采用稠密向量语义匹配，而非词汇匹配。</li>
</ul>
<ol>
<li>稠密检索与通用编码</li>
</ol>
<ul>
<li>代表：DPR（Karpukhin et al. 2020）等将段落编码为向量进行检索。</li>
<li>特点：面向纯文本，未针对表格结构做特殊处理。</li>
<li>STAR 差异：聚焦表格场景，引入结构感知采样与加权融合。</li>
</ul>
<ol>
<li>结构感知表格编码</li>
</ol>
<ul>
<li>代表：<br>– TaPas（Herzig et al. 2020）<br>– TaBERT（Yin et al. 2020）</li>
<li>特点：预训练阶段显式建模单元格-行列位置，提升表格理解。</li>
<li>STAR 差异：不改变底层编码器结构，仅在数据侧通过“聚类采样+合成查询”增强表示。</li>
</ul>
<ol>
<li>查询生成式语义增强</li>
</ol>
<ul>
<li>代表：<br>– GPL（Wang et al. 2022）对段落做伪标签查询生成。<br>– QGpT（Liang et al. 2025）首次系统地将 LLM 用于表格合成查询。</li>
<li>特点：用伪查询拉近表格与真实查询的语义距离。</li>
<li>STAR 差异：<br>– QGpT 采用简单 top-k 采样与拼接，STAR 改为<strong>头感知聚类</strong>选行，并<strong>按簇生成查询</strong>；<br>– QGpT 拼接后统一编码，STAR 引入<strong>显式加权融合</strong>（固定/动态），细粒度控制信息源贡献。</li>
</ul>
<p>此外，论文还引用 HAETAE（Jung &amp; Yoon 2025）等强调“表头锚定”对表格预训练的重要性，进一步支持 STAR 在聚类阶段引入 header-aware 嵌入的合理性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 STAR（Semantic TAble Representation）框架，用两阶段流水线替代 QGpT 的“top-k 采样 + 拼接”范式，系统解决语义鸿沟与表示粗糙问题。核心思路可概括为：</p>
<ul>
<li><strong>Stage-1：Semantic Clustering &amp; Query Generation（SCQG）</strong></li>
</ul>
<ol>
<li>对全表行做<strong>header-aware K-means</strong></li>
</ol>
<ul>
<li>先编码表头  e<em>H  与每行  e</em>(r_i) </li>
<li>加权融合： e<em>i = α e_H + (1-α)e</em>(r_i) ，把模式信息注入行向量</li>
<li>聚成  k  簇，选各簇最近中心行  r^<em><em>j</em>(j=1)^k  作为<em>*语义覆盖最广</em></em>的 partial table</li>
</ul>
<ol>
<li><strong>逐簇生成合成查询</strong></li>
</ol>
<ul>
<li>将同一簇行与表头重新拼成小表  T_j ，用 LLM 按提示生成一条查询  q_j </li>
<li>共得  k  条查询，各自对应表格不同语义侧面，实现“簇-查询”一一映射</li>
<li><strong>Stage-2：Weighted Fusion（WF）</strong></li>
</ul>
<ol>
<li>双路独立编码</li>
</ol>
<ul>
<li>e<em>(table) = Encoder(T</em>(∂)) </li>
<li>e_(queries) = Encoder(q_1 oplus dots oplus q_k) </li>
</ul>
<ol>
<li>显式加权融合</li>
</ol>
<ul>
<li><strong>固定权重</strong>（FWF）： e<em>T = (1-λ)e</em>(table) + λ e_(queries) </li>
<li><strong>动态权重</strong>（DWF）：</li>
<li>先算余弦相似度  s = cos(e<em>(table), e</em>(queries)) </li>
<li>再设  w_q = β s,; w_t = 1-w_q ，让权重随“表-查询”语义距离自适应变化</li>
</ul>
<p>通过“聚类选行→簇级查询→加权融合”，STAR 在不改动底层检索器的前提下，同时提升<strong>采样多样性</strong>与<strong>异构信息融合粒度</strong>，显著缩小查询与表格的语义差距。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在 5 个公开表格检索基准上进行了系统实验，核心设计围绕“主结果→消融→权重敏感性”三个层次展开，具体可归纳为：</p>
<ol>
<li>主实验：STAR vs. QGpT</li>
</ol>
<ul>
<li>数据集</li>
<li>Mimo(ch)、Mimo(en)</li>
<li>OTTQA</li>
<li>FetaQA</li>
<li>E2E-WTQ</li>
<li>评价指标</li>
<li>Recall@1 / Recall@5 / Recall@10</li>
<li>对照方法</li>
<li>唯一基线：QGpT（相同 encoder、相同 LLM、相同生成预算）</li>
<li>结果亮点</li>
<li>STAR-DWF 平均 R@1 绝对提升 6.39 pp，五数据集全部领先</li>
<li>STAR-FWF 在各自最优 λ 下亦稳定超越 QGpT，验证聚类+加权的普适性</li>
</ul>
<ol>
<li>消融实验（Ablation）</li>
</ol>
<ul>
<li>变体</li>
<li>w/o SCQG：退化为 top-k 采样 + 整体生成查询</li>
<li>w/o WF：退化为拼接编码</li>
<li>w/o Header-aware：仅用行向量聚类</li>
<li>观测</li>
<li>去除 SCQG 损失最大，R@1 ↓4.79 pp，确认“簇级采样”是性能核心</li>
<li>去除 WF 下降 2.78 pp，表明显式加权优于简单拼接</li>
<li>去除 header 信息下降 1.29 pp，说明模式上下文对聚类稳定性仍有增益</li>
</ul>
<ol>
<li>权重敏感性分析（FWF λ 扫描）</li>
</ol>
<ul>
<li>λ ∈{0.1,0.2,…,0.9} 逐档实验</li>
<li>发现</li>
<li>结构复杂表（Mimo 系列）在 λ=0.4–0.5 最佳，需更多查询信号</li>
<li>结构简单表（OTTQA 等）在 λ=0.3 左右最佳，表本身权重应更高</li>
<li>DWF 无需手动调 λ，在五数据集上均接近或达到最优，验证自适应机制的有效性与鲁棒性</li>
</ul>
<p>以上实验共同证明：STAR 的“header-aware 聚类选行 + 簇级查询生成 + 加权融合”策略，相对现有最佳生成式基线 QGpT 能够一致、显著地提升表格检索召回率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>可进一步探索的方向可归纳为以下六点：</p>
<ol>
<li><strong>轻量级实例选择</strong></li>
</ol>
<ul>
<li>当前 K-means 需对全表逐行编码，在大规模语料下耗时显著。可尝试：<br>– 可微分 Top-K / Gumbel-Softmax 一次性选出代表性行，避免聚类迭代；<br>– 基于行列稀疏度或信息熵的<strong>自适应行采样</strong>，在效果与效率间折中。</li>
</ul>
<ol>
<li><strong>端到端可学习融合权重</strong></li>
</ol>
<ul>
<li>DWF 仍依赖人工设定的缩放因子 β。可将  w<em>q, w_t  参数化为：<br>– 小型元网络：输入  e</em>(table), e_(queries)  直接回归权重；<br>– 强化学习：以 Recall 为奖励，自动搜索最优融合策略。</li>
</ul>
<ol>
<li><strong>缺失或弱模式表头场景</strong></li>
</ol>
<ul>
<li>对表头稀疏、多语言混杂或完全无表头的 Web 表格，header-aware 聚类会失效。可探索：<br>– <strong>表头补全</strong>（LLM 生成列名）后再聚类；<br>– <strong>双重聚类</strong>：先对列值做语义聚类推断潜在类型，再与行聚类结果联合选样。</li>
</ul>
<ol>
<li><strong>多模态扩展</strong></li>
</ol>
<ul>
<li>真实文档中表格常与 surrounding text、图像共存。未来可将 STAR 的聚类-加权框架扩展到：<br>– 表格+文本段落联合采样；<br>– 引入图表图像编码器，实现<strong>表格-文本-视觉</strong>三源加权融合。</li>
</ul>
<ol>
<li><strong>效率与工程优化</strong></li>
</ol>
<ul>
<li>合成查询阶段现用 8B 级 LLM，可研究：<br>– 小模型蒸馏（≤3B）或 LoRA 低秩微调，保持生成质量同时降低 30–50% 延迟；<br>– 异步批处理 + 缓存机制，避免对重复表格重复生成。</li>
</ul>
<ol>
<li><strong>跨语言与鲁棒性评估</strong></li>
</ol>
<ul>
<li>目前仅在中英双语验证，可构建<strong>多语言表格检索 benchmark</strong>，检验 STAR 在：<br>– 低资源语言（如泰语、印尼语）下的 header 语义缺失问题；<br>– 对抗扰动（列顺序打乱、数值格式变化）下的鲁棒性，并引入对抗训练进行增强。</li>
</ul>
<p>这些方向既回应了论文末尾指出的“计算开销”与“弱表头”局限，也为将 STAR 推向真实工业级大规模检索系统提供了可行路径。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>STAR</strong>（Semantic TAble Representation）——一个<strong>轻量级两阶段框架</strong>，用于在不改动底层检索器的前提下，显著提升表格检索的召回率。</p>
<ol>
<li>问题背景</li>
</ol>
<ul>
<li>非结构化查询与结构化表格之间存在语义鸿沟，且长表常超 token 限制。</li>
<li>现有最佳方法 QGpT 采用“top-k 采样 + 拼接编码”，采样片面、融合粗糙。</li>
</ul>
<ol>
<li>核心贡献</li>
</ol>
<ul>
<li><strong>Header-aware K-means 聚类</strong>：把表头语义注入行向量，选出 k 个最具代表性的 centroid 行，保证采样多样性。</li>
<li><strong>簇级合成查询</strong>：为每簇单独调用 LLM 生成一条查询，全面覆盖表格不同语义侧面。</li>
<li><strong>加权融合策略</strong>：<br>– 固定权重（FWF）手动调 λ；<br>– 动态权重（DWF）按表-查询余弦相似度自动分配权重，实现细粒度对齐。</li>
</ul>
<ol>
<li>实验结果</li>
</ol>
<ul>
<li>在 5 个主流基准（Mimo、OTTQA、FetaQA、E2E-WTQ）上，STAR-DWF 平均 R@1 比 QGpT 提高 <strong>6.39 pp</strong>，五数据集全部领先。</li>
<li>消融实验表明：聚类阶段贡献最大（-4.79 pp），加权融合与 header 感知分别带来 2.78 pp 与 1.29 pp 的额外增益。</li>
</ul>
<ol>
<li>未来方向<br>轻量级采样、端到端可学习权重、弱表头场景、多模态融合及跨语言鲁棒性等。</li>
</ol>
<p>综上，STAR 通过“<strong>聚类选行→簇级查询→自适应加权</strong>”三步，显著增强了表格语义表示，在保持框架轻量的同时实现了对现有生成式基线的一致超越。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是STAR%3A%20Semantic%20Table%20Representation%20with%20Header-Aware%20Clustering%20and%20Adaptive%20Weighted%20Fusion，链接是https://arxiv.org/pdf/2601.15860，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.15860。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Shui-Hsiang Hsu,Tsung-Hsiang Chou,Chen-Jui Yu,Yao-Chung Fan</p>
<p><strong>Categories</strong>: cs.IR</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.15860.pdf">https://arxiv.org/pdf/2601.15860.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.15860">https://arxiv.org/abs/2601.15860</a></p>
<p><strong>Arxiv ID</strong>: 2601.15860</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.15860">https://papers.cool/arxiv/2601.15860</a></p>
<p><strong>Published</strong>: 2026-01-22T11:08:46Z</p>
<p><strong>Updated</strong>: 2026-01-22T11:08:46.000Z</p>
<hr>
<h3 id="13-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents"><a href="#13-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents" class="headerlink" title="13. Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents"></a>13. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18217">Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents</a></h3><p>Generalist LLM agents are often post-trained on a narrow set of environments but deployed across far broader, unseen domains. In this work, we investigate the challenge of agentic post-training when the eventual test domains are unknown. Specifically, we analyze which properties of reinforcement learning (RL) environments and modeling choices have the greatest influence on out-of-domain performance. First, we identify two environment axes that strongly correlate with cross-domain generalization: (i) state information richness, i.e., the amount of information for the agent to process from the state, and (ii) planning complexity, estimated via goal reachability and trajectory length under a base policy. Notably, domain realism and text-level similarity are not the primary factors; for instance, the simple grid-world domain Sokoban leads to even stronger generalization in SciWorld than the more realistic ALFWorld. Motivated by these findings, we further show that increasing state information richness alone can already effectively improve cross-domain robustness. We propose a randomization technique, which is low-overhead and broadly applicable: add small amounts of distractive goal-irrelevant features to the state to make it richer without altering the task. Beyond environment-side properties, we also examine several modeling choices: (a) SFT warmup or mid-training helps prevent catastrophic forgetting during RL but undermines generalization to domains that are not included in the mid-training datamix; and (b) turning on step-by-step thinking during RL, while not always improving in-domain performance, plays a crucial role in preserving generalization.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>通用型大语言模型（LLM）代理通常是在一组有限的环境上进行后训练，但却被部署到更加广泛的、未见过的领域。在本研究中，我们探讨了当最终测试领域未知时代理后训练所面临的挑战。具体而言，我们分析了强化学习（RL）环境和建模选择中的哪些特性对跨领域性能影响最大。首先，我们确定了两个与跨领域泛化高度相关的环境维度： (i) 状态信息丰富度，即代理从状态中可处理的信息量；(ii) 规划复杂度，通过基线策略下的目标可达性和轨迹长度进行估算。值得注意的是，领域的现实性和文本层面的相似性并不是主要因素；例如，简单的网格世界领域 Sokoban 在 SciWorld 中比更真实的 ALFWorld 表现出更强的泛化能力。基于这些发现，我们进一步表明，仅增加状态信息丰富度就可以有效提升跨领域稳健性。我们提出了一种低开销且适用广泛的随机化技术：向状态中添加少量分散注意力且与目标无关的特征，使状态信息更丰富而不改变任务。除了环境方面的特性，我们还考察了几种建模选择： (a) SFT 预热或中途训练有助于防止 RL 中的灾难性遗忘，但会削弱对中途训练数据混合中未包含领域的泛化能力；(b) 在 RL 中启用逐步思考，虽然不一定提升域内性能，但在保持泛化能力方面起着至关重要的作用。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文核心关注的问题是：<br>在只能对少量已知环境进行后训练（post-training）的情况下，如何确保基于大语言模型（LLM）的通用智能体在部署到<strong>完全未知、跨领域（cross-domain）</strong>环境时仍能保持鲁棒性能，即<strong>最小化“泛化税”（generalization tax）</strong>。</p>
<p>具体而言，作者试图回答：</p>
<blockquote>
<p>当未来测试环境不可知时，应如何<strong>策略性地选择或构建用于强化学习（RL）训练的任务/仿真环境</strong>，才能<strong>最大化地把智能体能力迁移到未见过的新领域</strong>？</p>
</blockquote>
<p>为此，论文从“数据-centric”视角出发，系统研究了训练环境的哪些属性以及建模决策对<strong>跨领域泛化</strong>影响最大，并给出可落地的实践指南。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文将相关研究归为三条主线，并在第 2 节展开讨论：</p>
<ol>
<li><strong>多轮 RL 训练 LLM 智能体</strong></li>
</ol>
<ul>
<li>把单轮 RLHF 框架（如 PPO、GRPO）扩展到多轮交互场景</li>
<li>近期代表：VeRL-Agent、AgentGym-RL、SWEET-RL、AdaPlanner 等</li>
<li>重点解决信用分配、异步采样、长程决策等问题，但未系统研究跨域迁移</li>
</ul>
<ol>
<li><strong>LLM 跨域泛化分析</strong></li>
</ol>
<ul>
<li>对比 SFT 与 RL 在单轮任务上的域迁移能力（Cheng et al. 2025; Huan et al. 2025）</li>
<li>发现 RL 比 SFT 更能保留通用能力，但主要关注“同域换任务”层面，未涉及多轮 agent 的“跨域”场景</li>
</ul>
<ol>
<li><strong>状态增广与 sim-to-real 鲁棒性</strong></li>
</ol>
<ul>
<li>视觉 RL 中通过纹理、光照、物理参数随机化提升真实世界迁移（Domain Randomization）</li>
<li>近期工作把类似思想引入语言或视觉-语言环境，但多为启发式增广，缺乏对“何种增广真正提升语言智能体跨域泛化”的因果验证</li>
</ul>
<p>本文首次将“状态信息丰富度”与“规划复杂度”定量化为影响跨域泛化的核心环境属性，并用轻量级状态随机化进行因果干预，填补了上述空白。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文采用“先找关键因素 → 再因果验证 → 给出落地指南”的三段式路线解决跨域泛化难题。</p>
<ol>
<li>系统梳理关键因素</li>
</ol>
<ul>
<li>在 4 个代表性环境（WebShop、Sokoban、ALFWorld、SciWorld）上执行“单域训练-多域评测”实验</li>
<li>提出两项可量化指标：<br>– <strong>State Information Richness</strong>：状态文本平均字符数，衡量感知负载<br>– <strong>Planning Complexity</strong>：基策略轨迹平均长度，衡量推理深度</li>
<li>发现 OOD 性能与这两项指标显著正相关，而“域真实感”或“文本表面相似度”并非主因</li>
</ul>
<ol>
<li>因果验证与实用干预</li>
</ol>
<ul>
<li>仅对<strong>状态信息丰富度</strong>做轻量级干预——<strong>State Information Augmentation</strong>：向原始观察注入少量“与目标无关”的文本噪声（如无关物体描述、广告句），任务空间与奖励保持不变</li>
<li>实验显示：该随机化在 6 组训练-测试组合中平均带来 <strong>+5.7% ~ +42.5% 的 OOD 成功率提升</strong>，而域内性能几乎不降，验证了“丰富度→泛化”的因果性</li>
</ul>
<ol>
<li>建模侧关键决策剖析</li>
</ol>
<ul>
<li><strong>SFT Warmup / Mid-training</strong><br>– 对纳入 warmup 的域：后续 RL 遗忘显著减缓<br>– 对未纳入的域：遗忘反而加剧，揭示“覆盖范围-泛化”权衡</li>
<li><strong>Step-by-step Thinking</strong><br>– 域内成功率未必提升，甚至略降<br>– 但关闭思考后 OOD 性能普遍暴跌 80%–230%，证明显式推理是抑制过拟合浅层启发式的关键</li>
</ul>
<ol>
<li>形成落地指南<br>① 优先选择或构建<strong>状态信息丰富</strong>且<strong>规划复杂度高</strong>的训练环境<br>② 训练与推理全程开启<strong>显式逐步思考</strong><br>③ 采用<strong>轻量级状态随机化</strong>（注入无关文本 distractor）即可低成本提升跨域鲁棒性</li>
</ol>
<p>通过“环境属性筛选 + 低成本增广 + 建模细节调优”，论文在未知部署域场景下显著降低“泛化税”。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕“跨域泛化”共设计并执行了三大类实验，覆盖 4 个环境、2 组初始策略、多种干预与消融，具体列示如下：</p>
<ol>
<li>相关性发现实验（Section 5）</li>
</ol>
<ul>
<li>训练域：ALFWorld、WebShop、Sokoban、SciWorld 各单独训练 150 step</li>
<li>初始策略：Ckpt V1（仅 WebShop 20 step RL）与 Ckpt V2（SFT-warmup 混合版）</li>
<li>评测：在其余 3 个域的留一测试集上计算 success rate，汇总为 OOD Ranking Score</li>
<li>测量：<br>– State Information Richness＝平均状态字符数<br>– Planning Complexity＝平均轨迹长度（失败按 50 截断）</li>
<li>结论：两项指标与 OOD 排名显著负相关（值越高排名越靠前）</li>
</ul>
<ol>
<li>因果验证实验——State Information Augmentation（Section 6）</li>
</ol>
<ul>
<li>干预：向原始观察注入与目标无关的文本 distractor（无关物体、广告句、不可达格子描述等），控制字符总量 ϵ，并以 50 % 概率对轨迹施加</li>
<li>设置：<br>– Ckpt V1 × 3 域（ALFWorld ϵ=300、WebShop ϵ=30、Sokoban ϵ=80）<br>– Ckpt V2 × 3 域（ALFWorld ϵ=200、WebShop ϵ=40、Sokoban ϵ=80）</li>
<li>指标：∆OOD = 相对原始状态的 OOD 成功率变化</li>
<li>结果：6 组配置全部正向提升，最高 +42.5 %（Sokoban→WebShop）</li>
</ul>
<ol>
<li>建模侧消融实验（Section 7）<br>3.1 SFT Warmup 影响</li>
</ol>
<ul>
<li>固定下游 RL 训练域与超参，对比 Ckpt V1 vs Ckpt V2 在相同 OOD 域上的遗忘幅度</li>
<li>结果：若 OOD 域被包含在 warmup 数据，遗忘减少 11 %–45 %；若未包含，遗忘反而增大 38 %–42 %</li>
</ul>
<p>3.2 Step-by-step Thinking 影响</p>
<ul>
<li>在训练与推理阶段关闭 <think>…</think>，其余配置不变</li>
<li>结果：域内成功率持平或略升，但 OOD 成功率平均下降 80 %–230 %，多组实验出现绝对值跌至 &lt;2 % 的崩溃现象</li>
</ul>
<ol>
<li>扩展训练步数验证（Section 5.4 与附录）</li>
</ol>
<ul>
<li>将 Sokoban/SciWorld 训练步数增至 300/240，使 ∆ID 显著高于 ALFWorld/WebShop</li>
<li>结果：OOD Ranking 依旧领先，证明环境属性比“吸收知识量”更决定泛化</li>
</ul>
<p>整体实验流程遵循“留一域交叉验证 → 干预 → 消融”路径，共累计 &gt; 180 组独立训练运行、3 次随机种子，支撑论文结论的可重复性与因果性。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为<strong>环境层面</strong>、<strong>模型层面</strong>、<strong>理论层面</strong>与<strong>系统层面</strong>四个维度：</p>
<p>环境层面</p>
<ol>
<li><strong>自动环境增广</strong></li>
</ol>
<ul>
<li>基于 LLM 自动生成“保持最优策略不变”的 distractor 文本，减少人工模板</li>
<li>引入可学习的 adversarial distractor，最大化 agent 感知负载的同时保证任务可解</li>
</ul>
<ol>
<li><strong>规划复杂度可调控环境生成</strong></li>
</ol>
<ul>
<li>设计参数化生成器，连续调节 goal reachability 与解长度，建立“复杂度-泛化”响应曲线</li>
<li>研究是否存在<strong>最优复杂度区间</strong>，过短或过长的推理链均不利于迁移</li>
</ul>
<ol>
<li><strong>多模态或部分可观察环境</strong></li>
</ol>
<ul>
<li>将状态信息丰富度从纯文本扩展到图文混合，验证视觉噪声是否同样提升跨域鲁棒性</li>
<li>引入部分可观察设定，考察“感知负载+记忆需求”双重压力下的泛化行为</li>
</ul>
<p>模型层面</p>
<ol>
<li><strong>自适应思考预算</strong></li>
</ol>
<ul>
<li>让模型在训练/推理时动态决定“是否思考、思考几步”，用强化学习直接优化思考长度与 OOD 性能之间的权衡</li>
</ul>
<ol>
<li><strong>模块化或混合策略架构</strong></li>
</ol>
<ul>
<li>将“感知提取”与“规划推理”解耦为两个可插拔模块，只对感知模块做增广，检验模块化能否隔离域特定特征</li>
<li>探索显式世界模型（world model）能否在 distractor 存在时仍保持跨域一致的前向预测</li>
</ul>
<ol>
<li><strong>持续 / 增量学习策略</strong></li>
</ol>
<ul>
<li>设计无需重放旧环境数据的正则化或参数扩展方法，缓解 mid-training 带来的“未覆盖域遗忘”问题</li>
<li>结合课程学习，按“复杂度+丰富度”双指标渐进式扩充训练集，减少灾难性遗忘</li>
</ul>
<p>理论层面</p>
<ol>
<li><strong>泛化界的度量与预测</strong></li>
</ol>
<ul>
<li>借鉴 VC-dimension、Rademacher complexity 或 PAC-Bayes 框架，为“状态丰富度+规划深度”建立可计算的泛化上界</li>
<li>验证该界能否在训练前即预测不同环境的 OOD 性能排名，实现环境选择算法的理论保证</li>
</ul>
<ol>
<li><strong>推理链长度与梯度干扰分析</strong></li>
</ol>
<ul>
<li>从梯度冲突角度解释为何显式思考能抑制域特定启发式：测量 thinking vs. non-thinking 条件下各层梯度协方差矩阵迹，量化遗忘速率差异</li>
</ul>
<p>系统层面</p>
<ol>
<li><strong>大规模环境池与排行榜</strong></li>
</ol>
<ul>
<li>构建 20+ 不同属性（丰富度/复杂度/主题）的文本环境池，开放标准化“跨域泛化排行榜”，推动社区复现与对比</li>
<li>提供自动化评估协议：统一动作接口、奖励稀疏设定、OOD 划分脚本，降低后续研究门槛</li>
</ul>
<ol>
<li><strong>真实世界闭环验证</strong></li>
</ol>
<ul>
<li>在真实办公或网页任务中部署经“状态随机化+思考”训练的智能体，测量 sim-to-real 差距是否同步缩小</li>
<li>结合用户反馈形成在线数据飞轮，检验持续 RL 下环境属性指导是否依然有效</li>
</ul>
<p>以上方向既可直接扩展本文的实验结论，也能从理论、工具与落地角度完善“面向未知域的 LLM-Agent 后训练”这一新兴研究主题。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心贡献可概括为“<strong>一条问题、两项指标、三种干预、四条指南</strong>”：</p>
<ol>
<li><p>问题<br>当部署域未知时，如何在<strong>有限训练环境</strong>下用 RL 后训练，最大化 LLM 智能体的<strong>跨域泛化</strong>能力？</p>
</li>
<li><p>两项关键环境指标</p>
</li>
</ol>
<ul>
<li><strong>State Information Richness</strong>（状态信息丰富度）</li>
<li><strong>Planning Complexity</strong>（规划复杂度）<br>二者与 OOD 成功率显著正相关；域真实感或文本相似度并非主因。</li>
</ul>
<ol>
<li>三种干预验证</li>
</ol>
<ul>
<li><strong>状态随机化</strong>：低成本注入 goal-irrelevant 文本，OOD 绝对提升 <strong>5–42%</strong>。</li>
<li><strong>SFT Warmup</strong>：覆盖域遗忘↓，未覆盖域遗忘↑，揭示“覆盖-泛化”权衡。</li>
<li><strong>Step-by-step Thinking</strong>：域内未必涨，OOD 掉 <strong>80–230%</strong>；显式推理是抑制过拟合的关键。</li>
</ul>
<ol>
<li>四条落地指南<br>① 选/建<strong>高丰富度+高复杂度</strong>环境<br>② 训练与推理全程<strong>开启显式思考</strong><br>③ 采用<strong>轻量级状态随机化</strong><br>④ 若用 mid-training，务求<strong>数据 mix 尽可能广</strong>，以减少未覆盖域的灾难性遗忘</li>
</ol>
<p>综上，论文首次从数据-centric 视角系统拆解了影响 LLM-Agent 跨域泛化的环境与建模因子，并给出可直接复现的低成本增广方案，为“未知部署场景”下的 RL 后训练提供了可操作的实践框架。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Paying%20Less%20Generalization%20Tax%3A%20A%20Cross-Domain%20Generalization%20Study%20of%20RL%20Training%20for%20LLM%20Agents，链接是https://arxiv.org/pdf/2601.18217，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18217。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Zhihan Liu,Lin Guan,Yixin Nie,Kai Zhang,Zhuoqun Hao,Lin Chen,Asli Celikyilmaz,Zhaoran Wang,Na Zhang</p>
<p><strong>Categories</strong>: cs.AI,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18217.pdf">https://arxiv.org/pdf/2601.18217.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18217">https://arxiv.org/abs/2601.18217</a></p>
<p><strong>Arxiv ID</strong>: 2601.18217</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18217">https://papers.cool/arxiv/2601.18217</a></p>
<p><strong>Published</strong>: 2026-01-26T07:07:03Z</p>
<p><strong>Updated</strong>: 2026-01-26T07:07:03.000Z</p>
<hr>
<h3 id="14-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation"><a href="#14-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation" class="headerlink" title="14. AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation"></a>14. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17761">AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</a></h3><p>Real-world perception and interaction are inherently multimodal, encompassing not only language but also vision and speech, which motivates the development of “Omni” MLLMs that support both multimodal inputs and multimodal outputs. While a sequence of omni MLLMs has emerged, most existing systems still rely on additional expert components to achieve multimodal generation, limiting the simplicity of unified training and inference. Autoregressive (AR) modeling, with a single token stream, a single next-token objective, and a single decoder, is an elegant and scalable foundation in the text domain. Motivated by this, we present AR-Omni, a unified any-to-any model in the autoregressive paradigm without any expert decoders. AR-Omni supports autoregressive text and image generation, as well as streaming speech generation, all under a single Transformer decoder. We further address three practical issues in unified AR modeling: modality imbalance via task-aware loss reweighting, visual fidelity via a lightweight token-level perceptual alignment loss for image tokens, and stability-creativity trade-offs via a finite-state decoding mechanism. Empirically, AR-Omni achieves strong quality across three modalities while remaining real-time, achieving a 0.88 real-time factor for speech generation.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>现实世界的感知和交互本质上是多模态的，不仅包含语言，还包括视觉和语音，这也促使了支持多模态输入和多模态输出的“Omni” 多模态大语言模型（MLLMs）的发展。虽然一系列 Omni MLLMs 已经出现，但大多数现有系统仍依赖额外的专家组件来实现多模态生成，从而限制了统一训练和推理的简便性。自回归（AR）建模以单一的令牌流、单一的下一个令牌目标和单一的解码器为特征，是文本领域中一种优雅且可扩展的基础。受此启发，我们提出了 AR-Omni，这是一种在自回归范式下的统一任意到任意模型，不依赖任何专家解码器。AR-Omni 支持自回归文本和图像生成，以及流式语音生成，所有这些都由单一的 Transformer 解码器完成。我们进一步解决了统一 AR 建模中的三个实际问题：通过任务感知的损失重加权处理模态不平衡，通过针对图像令牌的轻量级令牌级感知对齐损失提升视觉保真度，以及通过有限状态解码机制处理稳定性与创造力的权衡。从实验结果来看，AR-Omni 在三种模态上都实现了高质量生成，同时保持实时性，语音生成的实时因子达到 0.88。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在构建一个<strong>无需任何外部专家解码器</strong>的<strong>统一自回归（AR）模型</strong>，实现<strong>任意模态到任意模态（any-to-any）</strong>的生成能力。具体而言，现有“Omni”多模态大模型普遍依赖扩散模型、非自回归声码器等外部组件来完成图像或语音生成，导致训练与推理流程碎片化。AR-Omni 尝试在<strong>纯自回归框架</strong>内，用<strong>单一 Transformer 解码器</strong>、<strong>单一词表</strong>、<strong>单一 next-token 目标</strong>同时支持：</p>
<ul>
<li>文本 ↔ 文本</li>
<li>文本 ↔ 图像</li>
<li>文本 ↔ 语音</li>
<li>以及三者任意组合的跨模态理解与生成</li>
</ul>
<p>并解决统一 AR 建模中的三个关键实际问题：</p>
<ol>
<li><strong>模态失衡</strong>：不同模态 token 预算差异大，导致某些任务主导优化。</li>
<li><strong>视觉保真度</strong>：离散图像 token 的交叉熵损失缺乏几何感知，重建质量下降。</li>
<li><strong>稳定性–创造性权衡</strong>：同一解码策略难以同时满足确定性任务（ASR/TTS）与开放性生成（T2I）的需求。</li>
</ol>
<p>最终，AR-Omni 在 7B 参数规模下实现<strong>实时流式语音合成</strong>（RTF=0.88）、<strong>零样本 TTS WER 6.5</strong>、<strong>图像描述 CIDEr 56.53</strong> 等指标，验证了其<strong>纯 AR 统一建模</strong>的可行性与效率。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为两条主线：<strong>多模态大语言模型（MLLM）</strong> 与<strong>离散多模态 token 化</strong>。代表性工作如下（按时间脉络梳理）：</p>
<p>1. 多模态大语言模型（MLLM）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>模态 I/O</th>
<th>是否依赖外部解码器</th>
<th>关键特点</th>
</tr>
</thead>
<tbody>
<tr>
<td>Kosmos-1</td>
<td>T,I → T</td>
<td>无扩散/声码器</td>
<td>早期统一 Transformer，仅文本输出</td>
</tr>
<tr>
<td>Flamingo</td>
<td>T,I → T</td>
<td>无</td>
<td>冻结视觉编码器，交叉注意力注入 LLM</td>
</tr>
<tr>
<td>SpeechGPT</td>
<td>T↔S</td>
<td>无</td>
<td>将语音离散化为 token，首次实现语音对话</td>
</tr>
<tr>
<td>Chameleon</td>
<td>T,I ↔ T,I</td>
<td>✓ 无扩散</td>
<td>早期融合、统一离散词表，纯 AR 生成图像</td>
</tr>
<tr>
<td>NExT-GPT</td>
<td>任意 ↔ 任意</td>
<td>× 依赖扩散+声码器</td>
<td>LLM 作为“调度器”，外挂扩散/声码器</td>
</tr>
<tr>
<td>AnyGPT</td>
<td>T,S,I ↔ T,S,I</td>
<td>× 依赖扩散</td>
<td>三模态离散 token，但图像仍需扩散解码器</td>
</tr>
<tr>
<td>MIO</td>
<td>T,S,I ↔ T,S,I</td>
<td>× 依赖扩散</td>
<td>扩展 AnyGPT 到视频，仍保留扩散解码器</td>
</tr>
<tr>
<td>USLM</td>
<td>T,S → S</td>
<td>× 依赖双码本声码器</td>
<td>流式语音生成，但非 any-to-any</td>
</tr>
<tr>
<td>Anole</td>
<td>T,I ↔ T,I</td>
<td>✓ 无扩散</td>
<td>基于 Chameleon，仅 41 M 图像解码器参数</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>AR-Omni 与 <strong>Chameleon / Anole</strong> 同属“<strong>diffusion-free</strong>”阵营，但首次把<strong>语音</strong>纳入纯 AR 统一框架，并解决模态失衡、视觉保真、解码策略三问题。</p>
</blockquote>
<p>2. 离散多模态 token 化</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模态</th>
<th>代表 tokenizer</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像</td>
<td>VQGAN, SEED, dVAE (DALL·E)</td>
<td>将 2D 视觉压缩为 1D 离散码本索引，支持 AR 自回归</td>
</tr>
<tr>
<td>语音</td>
<td>SoundStream, EnCodec, WavTokenizer</td>
<td>神经音频编解码器，单码本低帧率离散化，实现流式 AR 语音</td>
</tr>
<tr>
<td>文本</td>
<td>SentencePiece BPE</td>
<td>与 LLM 一致，直接复用</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>AR-Omni 采用 <strong>WavTokenizer</strong> 的单码本声学 token，摒弃传统“语义+声学”双码本，从而把语音生成纳入统一 next-token 范式，无需额外声码器。</p>
</blockquote>
<p>3. 小结</p>
<ul>
<li><strong>早期 MLLM</strong>：仅文本输出，或依赖外部扩散/声码器。</li>
<li><strong>Chameleon / Anole</strong>：纯 AR 图像+文本，但未涉及语音。</li>
<li><strong>AnyGPT / MIO</strong>：三模态 any-to-any，仍外挂扩散解码器。</li>
<li><strong>AR-Omni</strong>：首次在<strong>单一 AR 解码器</strong>内完成<strong>文本、图像、语音</strong>的任意模态双向生成，无需任何专家模块，并支持<strong>实时流式语音</strong>。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“统一自回归 any-to-any 生成”拆解为<strong>三个层次</strong>的问题，并给出对应解法：</p>
<p>1. 统一建模：如何把异构模态压进“单一 next-token”范式？</p>
<p><strong>做法</strong></p>
<ul>
<li><strong>单一离散词表</strong><br>V = V<em>(text) ∪ V</em>(speech) ∪ V_(image)<br>文本用 SentencePiece BPE；语音用 WavTokenizer 单码本 40 tok/s；图像用 scene-aware VQ tokenizer。</li>
<li><strong>单一 Transformer 解码器</strong><br>7 B 参数，纯因果自回归，无交叉注意力、无扩散、无声码器。</li>
<li><strong>统一序列格式</strong><br>用特殊 token 显式标记模态边界：</li>
<li><code>&lt;boa&gt; … &lt;eoa&gt;</code> 包裹语音</li>
<li><code>&lt;boi&gt; … &lt;eoi&gt;</code> 包裹图像</li>
<li><code>&lt;eoh&gt;</code> 输入结束</li>
<li><code>&lt;eom&gt;</code> 多轮助手回复结束<br>从而把任意模态交错序列化为 1D token 流，直接套用公式</li>
</ul>
<p>p<em>θ(x)=prod</em>(t=1)^(T) p<em>θ(x_t|x</em>(&lt;t))</p>
<p>2. 训练目标：如何兼顾三模态且不掉点？</p>
<p><strong>复合损失</strong></p>
<p>L= L<em>(wNTP)</em>(加权交叉熵) + λ<em>(perc) L</em>(perc)_(token 级感知损失)</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>对应设计</th>
<th>公式/细节</th>
</tr>
</thead>
<tbody>
<tr>
<td>模态失衡</td>
<td>任务感知重加权</td>
<td>对 ASR、图像描述等 X→T 任务，文本响应段权重加大，抑制长序列语音/图像 token 主导优化：$LwNTP=-(1) / (T)∑t=1^T w<em>t log p</em>θ(x_t</td>
</tr>
<tr>
<td>视觉保真度</td>
<td>token 级感知对齐</td>
<td>对图像 token 目标，把最后一层隐状态 h<em>t 投影到冻结的 codebook 嵌入空间，最小化 L2 距离：$L</em>(perc)=1{</td>
</tr>
<tr>
<td>训练稳定性</td>
<td>residual-post-norm（Swin-norm）</td>
<td>把 LayerNorm 移到残差分支，缓解长多模态序列的梯度震荡：$h = x + Norm(Attn(x))</td>
</tr>
</tbody>
</table>
</div>
<p>x’ = h + Norm(FFN(h)) | 3. 推理策略：如何兼顾“确定性”与“创造性”？ <strong>有限状态解码机</strong> - <strong>ASR / TTS</strong> → 贪心解码，保证字准/ intelligibility。 - <strong>T2I / 开放对话</strong> → 核采样，提升多样性。 状态自动切换，无需人工干预，兼顾稳定性与创造性。 4. 数据与流程 - <strong>预训练</strong> 文本 : 图文 : 语音 = 0.5 : 1 : 2，任一子集耗尽即停止，保证模态比例可控。 - <strong>指令微调</strong> 在 AnyInstruct + VoiceAssistant-400K + UltraChat 上继续训练，仅对<strong>助手回复段</strong>计算损失。 - <strong>两阶段训练</strong> 1. 预训练 140 k step，峰值 lr 6e-5； 2. 微调 18 k step，峰值 lr 2e-5。 5. 结果验证 | 指标 | 数值 | 说明 | | —- | —- | —- | | 实时语音 | RTF = 0.88 | 首次 token 延迟 146 ms，40 tok/s 单码本即可流式。 | | 零样本 TTS | WER 6.5 | 匹配外部声码器系统，但无额外模块。 | | 图像生成 | CLIPscore 0.24 | 扩散-free 自回归方案中可比，仅 41 M 图像解码参数。 | | 图像理解 | CIDEr 56.53 | 高于同规模扩散-free 模型 Anole。 | 结论 通过“<strong>统一 token 化 + 加权损失 + 感知对齐 + 状态解码</strong>”四件套，AR-Omni 在<strong>不引入任何专家解码器</strong>的前提下，首次让<strong>文本、图像、语音</strong>的任意方向生成都服从<strong>单一 next-token 预测</strong>，并保持<strong>实时流式</strong>与<strong>高质量</strong>输出。 Q4: 论文做了哪些实验？ 论文围绕“any-to-any”能力设计了三类实验，覆盖<strong>图像、语音、多模态交错</strong>场景，并辅以消融与训练稳定性分析。所有实验均在<strong>7 B 参数单 Transformer</strong> backbone 上完成，<strong>无外部扩散或声码器</strong>。 1. 图像实验（零样本） | 任务 | 数据集 | 指标 | 结果 | | —- | —- | —- | —- | | 图像描述（I→T） | MS-COCO Karpathy test | CIDEr ↑ | 56.53 | | 文本生成图像（T→I） | MS-COCO 30 k 随机caption | CLIPscore ↑ | 0.24 | &gt; 与同样“diffusion-free”的 Chameleon/Anole 相比，AR-Omni 在 caption 上提升显著；T2I 略降但仍保持可比，验证统一训练未破坏视觉生成能力。 2. 语音实验（零样本） | 任务 | 数据集 | 指标 | 结果 | | —- | —- | —- | —- | | ASR（S→T） | LibriSpeech test-clean | WER ↓ | 9.4 % | | TTS（T→S） | VCTK（108 未见说话人） | WER ↓ | 6.5 % | | 首 token 延迟 ↓ | 146 ms | | RTF ↓ | 0.88 | &gt; 单码本 40 tok/s 即可达到<strong>实时</strong>（RTF<1），优于 AnyGPT/MiO 的 200-600 tok/s 双码本方案；WER 与外部声码器系统 USLM 持平。 3. 多模态交错案例（定性） 图 4-6 给出**多轮对话**示例，验证模型在**纯语音上下文**中完成： - S→S 多轮闲聊 - S+I→S 看图说话 - S→I 语音指令生图 > 无需任何模态切换模块，同一 AR 解码器连续生成文本/图像/语音 token。 4. 消融实验（40 k step 快照） | 版本 | I2T(CIDEr) | T2I(CLIP) | ASR(WER) | TTS(WER) | | —- | —- | —- | —- | —- | | 完整 AR-Omni | 0.502 | 0.241 | 0.154 | 0.111 | | w/o 感知损失 | 0.487 | 0.236 | 0.140 | 0.118 | | w/o swin-norm | 0.549 | 0.239 | 0.133 | 0.163 | | 简单 NTP | 0.507 | 0.225 | 0.227 | 0.159 | &gt; 感知损失主要提升视觉与语音质量；swin-norm 对语音稳定性最关键；加权 NTP 显著抑制模态失衡，避免简单 NTP 的 22 % ASR WER 暴涨。 5. 训练稳定性对比 图 3 给出 50 k step 曲线： - <strong>简单 NTP</strong> 后期出现<strong>损失反弹与尖峰</strong>，趋向崩溃； - <strong>AR-Omni 复合目标</strong> 全程平滑收敛，验证所提策略可防 late-stage collapse。 6. 额外对照表 表 1 给出<strong>高阶能力对比</strong>： | 方法 | 扩散-free | 流式语音 | 实时 RTF<1 | 任意模态 I/O | | --- | --- | --- | --- | --- | | Kosmos/Flamingo | ✓ | – | – | × | | Chameleon | ✓ | – | – | × | | AnyGPT/MiO | × | × | × | ✓ | | AR-Omni | ✓ | ✓ | ✓ | ✓ | > AR-Omni 是唯一同时满足“<strong>无扩散</strong> + <strong>流式实时</strong> + <strong>任意模态双向</strong>”的模型。 总结 实验从<strong>客观指标、实时性能、多轮交互、训练稳定性</strong>四维度验证： 在<strong>不引入任何专家解码器</strong>的前提下，AR-Omni 实现了与外挂扩散/声码器系统<strong>可比甚至更优</strong>的图像与语音质量，并保持<strong>146 ms 首包、0.88 RTF 的实时流式能力</strong>。 Q5: 有什么可以进一步探索的点？ 以下方向可在此基础上继续推进，分为<strong>质量、效率、模态、理论</strong>四条主线： 1. 视觉质量：diffusion-free 自回归图像生成仍落后 - <strong>高分辨率</strong> 当前 VQ 码本仅 1024×1024 级，可探索 <strong>多尺度离散潜码</strong> 或 <strong>粗细两级 AR</strong>，先全局布局再局部精化。 - <strong>感知-对抗联合目标</strong> 在 token 级感知损失之外，引入 <strong>轻量判别器</strong> 做 GAN 式微调，仅更新 1–2 层，保持 AR 推理链不变。 - <strong>连续-离散混合</strong> 前半段 AR 离散码，后半段 <strong>连续潜变量扩散精修</strong>（&lt; 10 步），形成“AR 打底 + 微扩散抛光”的 <strong>hybrid 但单模型</strong> 方案。 2. 效率与长序列：语音+高清图导致序列爆炸 - <strong>子词-像素/声采样</strong> 对图像/语音 token 使用 <strong>RQ-VAE 或 FSQ</strong>，把每帧压缩到 4–8 bit，单图 256×256 仅需 512 token。 - <strong>局部注意力 / 稀疏化</strong> 在 <strong>模态内部</strong> 用滑动窗口或 Dilated Attention，<strong>跨模态</strong> 仍保持全注意力，兼顾显存与全局依赖。 - <strong>早期退出 + 级联</strong> 低层 AR 先生成低分辨率 token，<strong>置信度&gt;阈值</strong> 时立即送往后级上采样解码器，实现 <strong>渐进式输出</strong>，进一步降低首包延迟。 3. 新模态与场景 - <strong>视频</strong> 将 2D VQ 扩展为 <strong>3D-VQ</strong> 或 <strong>时空分离码本</strong>，用同一 AR 解码器生成“ … ”视频 token 流。 - <strong>音乐/环境声</strong> 当前仅用 WavTokenizer 对话音，可引入 <strong>MusicTokenizer</strong> 支持旋律、和弦，实现 <strong>文本/哼唱→歌曲</strong>。 - <strong>传感器-动作</strong> 把 <strong>IMU、深度、机器人关节</strong> 离散化，形成“视觉-语言-动作”统一词表，实现 <strong>多模态指令→控制序列</strong> 的 <strong>any-to-act</strong> 能力。 - <strong>端到端语音对话系统</strong> 集成 <strong>语音活动检测(VAD)</strong> 与 <strong>流式 ASR/TTS</strong>，让模型在 <strong>单一 AR 会话</strong> 里完成 <strong>VAD→ASR→LLM→TTS</strong> 全栈，无需外部流水线。 4. 训练与理论问题 - <strong>模态权重动态调度</strong> 当前重加权为静态，可借鉴 <strong>GradNorm / UW</strong> 方法，让 <strong>wt 随梯度范数自动调整</strong>，防止新任务灾难性遗忘。 - <strong>统一 scaling law</strong> 系统研究“<strong>数据比例 ↔ 参数规模 ↔ 模态数量</strong>”三者关系，给出 <strong>多模态 AR 的 scaling 公式</strong>，指导未来 30 B–100 B 模型设计。 - <strong>可解释离散码</strong> 探索 <strong>离散码的语义可解释性</strong>（如语音 token 对应音素、图像 token 对应物体部件），引入 <strong>离散码词典</strong> 或 <strong>可导软量化</strong>，便于后续编辑与可控生成。 - <strong>安全性与伦理</strong> 统一 AR 模型可<strong>跨模态伪造</strong>（深度语音+假图），需构建 <strong>多模态对齐的安全过滤器</strong>，并在训练阶段加入 <strong>对抗性红队样本</strong>。 5. 小结（速览） | 方向 | 可落地技术 | 预期收益 | | —- | —- | —- | | 视觉保真 | 微扩散抛光、粗细两级 AR | 追平 SD/DF 质量，仍保持 AR 简洁 | | 序列效率 | RQ-VAE、稀疏注意力、级联退出 | 首包 ↓30 %、显存 ↓50 % | | 视频/音乐 | 3D-VQ、MusicTokenizer | 扩展到 4 模态，形成真正的“omni” | | 动态调度 | GradNorm 自适应 wt | 持续增量学习，不遗忘旧模态 | | 安全 | 跨模态红队+过滤 | 降低伪造风险，符合政策要求 | &gt; 未来工作可沿“<strong>更高保真、更长序列、更多模态、更智能调度</strong>”四轴迭代，最终目标是在<strong>单一 AR 解码器</strong>内实现<strong>人级别实时多模态交互</strong>。 Q6: 总结一下论文的主要内容 论文提出 <strong>AR-Omni</strong>：首个<strong>不依赖任何外部专家解码器</strong>的<strong>统一自回归 any-to-any 模型</strong>，用<strong>单一 7 B Transformer</strong> 同时支持文本、图像、语音的<strong>理解与生成功能</strong>，并达到<strong>实时流式语音</strong>水平。核心内容可概括为四点： 1. 统一建模 - 文本、图像、语音分别经 <strong>SentencePiece / scene-aware VQ / WavTokenizer</strong> 离散化为同一词表 V$。 - 所有 token 按 <code>&lt;boa&gt;…&lt;eoa&gt;</code>、<code>&lt;boi&gt;…&lt;eoi&gt;</code> 等边界符交错成<strong>单一流</strong>，用标准 next-token 目标<br>p<em>θ(x)=prod</em>(t=1)^(T) p<em>θ(x_t|x</em>(&lt;t))$$<br>训练与推理，<strong>无扩散、无声码器</strong>。</p>
<p>2. 训练策略</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>问题</th>
<th>解决方案</th>
<th>公式/要点</th>
</tr>
</thead>
<tbody>
<tr>
<td>模态失衡</td>
<td>任务感知重加权</td>
<td>对 ASR、图像描述等文本响应段加大权重：</td>
</tr>
<tr>
<td>LwNTP=-(1) / (T)∑t=1^T w<em>t log p</em>θ(x_t</td>
<td>x_(&lt;t))</td>
</tr>
<tr>
<td>视觉保真</td>
<td>token 级感知损失</td>
<td>将隐藏状态对齐预训练 codebook 嵌入：</td>
</tr>
<tr>
<td>L_(perc)=(1) / (</td>
<td>T</td>
<td>)∑_(t∈ T)</td>
<td>W_h h_t - E[y_t]</td>
<td>_2^2</td>
</tr>
<tr>
<td>训练稳定性</td>
<td>residual-post-norm</td>
<td>把 Norm 移到残差分支，缓解长序列梯度爆炸</td>
</tr>
</tbody>
</table>
</div>
<p>3. 推理机制</p>
<ul>
<li><strong>有限状态解码机</strong>：ASR/TTS 用贪心保准，T2I/对话用核采样，<strong>自动切换</strong>兼顾稳定与创造性。</li>
<li><strong>流式语音</strong>：单码本 40 tok/s，首包 146 ms，RTF=0.88，<strong>真正实时</strong>。</li>
</ul>
<p>4. 实验结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模态</th>
<th>代表 tokenizer</th>
<th>核心思想</th>
</tr>
</thead>
<tbody>
<tr>
<td>图像</td>
<td>VQGAN, SEED, dVAE (DALL·E)</td>
<td>将 2D 视觉压缩为 1D 离散码本索引，支持 AR 自回归</td>
</tr>
<tr>
<td>语音</td>
<td>SoundStream, EnCodec, WavTokenizer</td>
<td>神经音频编解码器，单码本低帧率离散化，实现流式 AR 语音</td>
</tr>
<tr>
<td>文本</td>
<td>SentencePiece BPE</td>
<td>与 LLM 一致，直接复用</td>
</tr>
</tbody>
</table>
</div>
<p>0</p>
<p>消融与曲线显示：去掉任一组件均掉点，<strong>简单 NTP 出现训练崩溃</strong>，验证所提策略有效且必要。</p>
<p>5. 结论</p>
<p>AR-Omni 证明：<strong>纯自回归</strong>即可在<strong>单一模型</strong>内完成<strong>文本、图像、语音任意方向</strong>的<strong>高质量、实时生成</strong>，无需扩散或声码器，为“omni”多模态大模型提供了<strong>简洁、统一、可扩展</strong>的新范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是AR-Omni%3A%20A%20Unified%20Autoregressive%20Model%20for%20Any-to-Any%20Generation，链接是https://arxiv.org/pdf/2601.17761，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17761。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Dongjie Cheng,Ruifeng Yuan,Yongqi Li,Runyang You,Wenjie Wang,Liqiang Nie,Lei Zhang,Wenjie Li</p>
<p><strong>Categories</strong>: cs.LG,cs.AI,cs.CL</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17761.pdf">https://arxiv.org/pdf/2601.17761.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17761">https://arxiv.org/abs/2601.17761</a></p>
<p><strong>Arxiv ID</strong>: 2601.17761</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17761">https://papers.cool/arxiv/2601.17761</a></p>
<p><strong>Published</strong>: 2026-01-25T09:17:36Z</p>
<p><strong>Updated</strong>: 2026-01-25T09:17:36.000Z</p>
<hr>
<h3 id="15-Agentic-Very-Long-Video-Understanding"><a href="#15-Agentic-Very-Long-Video-Understanding" class="headerlink" title="15. Agentic Very Long Video Understanding"></a>15. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18157">Agentic Very Long Video Understanding</a></h3><p>The advent of always-on personal AI assistants, enabled by all-day wearable devices such as smart glasses, demands a new level of contextual understanding, one that goes beyond short, isolated events to encompass the continuous, longitudinal stream of egocentric video. Achieving this vision requires advances in long-horizon video understanding, where systems must interpret and recall visual and audio information spanning days or even weeks. Existing methods, including large language models and retrieval-augmented generation, are constrained by limited context windows and lack the ability to perform compositional, multi-hop reasoning over very long video streams. In this work, we address these challenges through EGAgent, an enhanced agentic framework centered on entity scene graphs, which represent people, places, objects, and their relationships over time. Our system equips a planning agent with tools for structured search and reasoning over these graphs, as well as hybrid visual and audio search capabilities, enabling detailed, cross-modal, and temporally coherent reasoning. Experiments on the EgoLifeQA and Video-MME (Long) datasets show that our method achieves state-of-the-art performance on EgoLifeQA (57.5%) and competitive performance on Video-MME (Long) (74.1%) for complex longitudinal video understanding tasks.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>全天候个人 AI 助手的到来，由全天佩戴的设备如智能眼镜所驱动，要求一种新的上下文理解能力，这种能力不仅局限于短暂、孤立的事件，还必须涵盖连续的、纵向的自我视角视频流。实现这一愿景需要在长时段视频理解方面取得进展，系统必须能够解读并回忆跨越数天甚至数周的视觉和音频信息。现有的方法，包括大型语言模型和检索增强生成，受限于有限的上下文窗口，且缺乏对超长视频流进行组合式、多跳推理的能力。在本研究中，我们通过 EGAgent 解决这些挑战，这是一个以实体场景图为中心的增强型智能体框架，用于表示人、地点、物体及其随时间变化的关系。我们的系统为规划型智能体提供了在这些图上进行结构化搜索和推理的工具，以及混合视觉和音频搜索能力，从而实现详细的、跨模态的、时间一致的推理。在 EgoLifeQA 和 Video-MME（长视频）数据集上的实验表明，我们的方法在 EgoLifeQA 上达到了最先进的性能（57.5%），在 Video-MME（长视频）上也取得了具有竞争力的性能（74.1%），适用于复杂的纵向视频理解任务。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“超长视频理解”（very long video understanding）问题，核心挑战是：</p>
<ol>
<li><p>上下文窗口受限<br>现有大模型（LLM/VLM）无法一次性摄入数天~数周的 egocentric 视频与音频流。</p>
</li>
<li><p>多跳、跨模态、长程推理不足<br>传统 RAG 仅检索孤立片段，难以在连续时间轴上追踪“谁、何时、在哪、与何物/何人发生何种关系”，例如“本周我每天早上都跟谁一起喝咖啡？”这类查询需要跨天、跨模态、跨实体的组合推理。</p>
</li>
<li><p>实体与关系随时间演化的建模缺失<br>既有方法缺乏显式、可检索的“时间-感知”实体关系表示，导致无法精确定位重复行为或社交互动。</p>
</li>
</ol>
<p>为此，作者提出 EGAgent——以“实体-场景-图”为中心的 agentic 框架，将一周级 egocentric 视频转化为带时间戳的实体关系图，并配备规划代理、视觉搜索、音频搜索与图搜索工具，实现可解释、可组合、长程跨模态推理。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，均围绕“如何突破上下文窗口限制、实现长视频推理”展开：</p>
<ol>
<li>长视频理解 × LLM 上下文压缩</li>
</ol>
<ul>
<li>帧/Token 压缩：LongViD、VideoTree、VideoXL、Bolt、AdaCM² 等通过关键帧选择或视觉 Token 降采样，把小时级视频压进 32k–128k token 窗口。</li>
<li>层级/滑动窗口摘要：VITED、DecAFNet、Streaming-DVC 等先局部摘要再全局融合，维持长程依赖。</li>
<li>直接扩展窗口：LongRoPE、Self-Extend 把 LLM 上下文扩到 2M tokens，但仍面临视觉模态巨量的计算与存储开销。</li>
</ul>
<ol>
<li>图增强检索（Graph-based RAG）</li>
</ol>
<ul>
<li>文本知识图谱：GraphRAG、LightRAG 在文本语料上抽取实体-关系图，支持多跳问答。</li>
<li>视频-图 RAG：<br>– VideoRAG、AdaVideoRAG 将“片段-实体”建图，按查询难度自适应检索；<br>– RAVU、GraphVideoAgent 用 VLM 检测实体并生成时空图，再做推理；<br>– VideoMindPalace 构建室内“分层时空图”，但受限于房间级先验。<br>共同局限：图通常一次性构建、无细粒度时间戳，难以跟踪“跨天重复行为”或“关系随时间变化”。</li>
</ul>
<ol>
<li>Agentic 视频理解</li>
</ol>
<ul>
<li>VideoAgent、DrVideo、VideoDeepResearch 等把视频当长文档，让代理迭代调用“帧检索、字幕搜索、记忆更新”工具，直至信息足够回答查询。<br>关键差异：上述系统依赖非结构化字幕或帧嵌入，缺乏显式、可 SQL 化的“实体-关系-时间”三元组，因而在“追踪同一实体跨天出现”与“组合多跳约束”上精度有限。</li>
</ul>
<p>EGAgent 在这三条线之上引入两项区别：</p>
<ul>
<li>时间感知实体场景图——每边带 (t_start, t_end) 可增量更新；</li>
<li>代理循环内联合调用“图搜索 + 视觉搜索 + 音频搜索”，实现跨模态、长程、多跳推理。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文把“超长 egocentric 视频问答”形式化为映射</p>
<p>H:(V,A_T,Q)arrow A</p>
<p>其中  V  为一周级 1 FPS 帧流， A_T  为带时间戳的转录文本， Q  为自然语言查询。直接端到端推理不可行，因此采用两阶段解：</p>
<ol>
<li>离线构建<strong>时间-感知实体场景图</strong>（Section 3.2）<br>对每秒帧提取视觉字幕 + 音频转录，用 LLM 融合成跨模态文档  d ；再调用 LLMGraphTransformer 抽取出三元组</li>
</ol>
<p>(v<em>s,τ(v_s),r,v_t,τ(v_t),t</em>(start),t_(end),d^*)</p>
<p>存入 SQLite，支持增量更新。图中节点类型  τ∈person,object,location ，关系类型  r∈talks-to,∫eracts-with,mentions,uses ，每条边附带精确到秒的时间区间。</p>
<ol>
<li>在线启用<strong>EGAgent 代理框架</strong>（Algorithm 1 &amp; Section 3.3）</li>
</ol>
<ul>
<li><strong>Planning Agent</strong>：将复杂  Q  分解为子任务列表  (S<em>i,T_i,q_i)</em>(i=1)^N ，最多 5 步。</li>
<li><strong>三模态检索工具</strong><br>– <em>Entity Graph Search</em>：SQL 查询图数据库，采用“严格→松弛”策略（先精确时间+实体+关系，再逐步放宽）。<br>– <em>Visual Search</em>：SigLIP-2 把 1 FPS 帧嵌入向量库，支持属性过滤（房间、时段）+ 语义相似度检索。<br>– <em>Audio Transcript Search</em>：可选 LLM 或 BM25，在说话人分离文本上定位相关片段。</li>
<li><strong>Analyzer Tool</strong>：轻量级 LLM 对检索结果做相关性判定、证据抽取、去重，写入工作记忆  M 。</li>
<li><strong>VQA Agent</strong>：最后把  Q  与累积记忆  M  一并输入多模态 LLM，生成答案  A 。</li>
</ul>
<p>通过“图-视觉-音频”交替检索与记忆更新，EGAgent 能在 50 h 视频中完成跨天、跨模态、多跳推理，例如“上周购物时谁一直坐我旁边”这类查询：先用音频定位打车片段，再用图搜索找出同车人员，最终由 VQA 汇总判断。实验表明，该机制在 EgoLifeQA 上把 SOTA 绝对提升 20.6%，在 Video-MME(Long) 仅用 1/10 帧数即可与 AdaVideoRAG 打平。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文在两条公开基准上系统评估了 EGAgent 的“超长视频理解”能力，实验设计覆盖整体性能、细粒度能力、消融、效率与上界分析。</p>
<ol>
<li>主基准实验<br>1.1 EgoLifeQA（50 h egocentric，500 道 MCQ）</li>
</ol>
<ul>
<li>对比线：<br>– MLLM 均匀采样：LLaVA-Video-7B、GPT-4.1、Gemini 2.5 Pro<br>– RAG 线：LLaVA+Video-RAG<br>– Agentic 线：EgoButler、VideoAgent、Ego-R1</li>
<li>结果：<br>– EGAgent-Gemini2.5Pro(EG+F+T) 平均准确率 57.5%，<strong>超出原 SOTA 20.6%</strong>；<br>– 在需要多跳实体关系的 RelationMap / TaskMaster 子集上分别 <strong>+32% / +39.7%</strong>。</li>
</ul>
<p>1.2 Video-MME (Long)（300 段 30–60 min 视频，900 道 MCQ）</p>
<ul>
<li>对比线：Gemini-2.5-Pro、GPT-4.1、Video-RAG、AdaVideoRAG、DrVideo 等</li>
<li>结果：<br>– EGAgent-Gemini2.5Pro 74.1%，<strong>仅次于原生 Gemini 视频模式</strong>（82.0%），但仅用 50 帧（≈1/5 帧量）且处理帧数 <strong>&lt; AdaVideoRAG 的 1/10</strong> 即可持平或超越多数强基线。</li>
</ul>
<ol>
<li>消融实验（EgoLifeQA）<br>2.1 图构建方式</li>
</ol>
<ul>
<li>仅转录 T → 转录+视觉字幕 T+C，平均 <strong>+2.6%</strong>。<br>2.2 检索工具组合</li>
<li>单工具：F 34.6%、T 45.6%、EG 37.6%</li>
<li>两工具：F+T 48.6%</li>
<li>三工具：EG+F+T <strong>50.7%</strong>，在 RelationMap 上比 F+T 再 <strong>+26.7%</strong>。<br>2.3 转录搜索骨干</li>
<li>LLM 搜索 ↔ BM25 搜索：准确率 −6.8%，token 量 <strong>÷3.3</strong>，延迟 <strong>−3.6×</strong>。</li>
</ul>
<ol>
<li>效率与延迟</li>
</ol>
<ul>
<li>完整 pipeline 单题 wall-clock 125–169 s；</li>
<li>其中 Analyzer（LLM 重推理）占主要耗时；</li>
<li>实体图搜索仅增加 <strong>≈13% 延迟</strong>，却带来显著精度增益。</li>
</ul>
<ol>
<li>上界与召回分析<br>4.1 Oracle 实验（使用 ground-truth 时刻）</li>
</ol>
<ul>
<li>EGAgent 当前 50.7% → Oracle 68.7%，<strong>仍有 18% 绝对提升空间</strong>。<br>4.2 召回@窗口大小</li>
<li>三工具联合在 10 s 窗口即达 <strong>88.4% 召回</strong>，验证其<strong>细粒度时间定位</strong>能力；</li>
<li>单工具对比：视觉搜索 10 s 窗口 85.7%，音频 21.8%，实体图 12.7%，但图搜索在 &gt;2 min 窗口反超音频，提供<strong>延迟-召回折中</strong>。</li>
</ul>
<p>综上，实验既展示了 EGAgent 在“一周级”超长视频上的新 SOTA，也量化了实体图、跨模态检索与代理规划各自的贡献，并指出未来进一步提升的关键在于更精准的时间定位与噪声抑制。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为“方法-算法”“数据-评测”“系统-部署”三大层面：</p>
<p>方法-算法层面</p>
<ol>
<li><p><strong>时间-感知图神经网络</strong><br>当前实体图仅用于 SQL 检索，可进一步为每条边引入连续时间嵌入，用 TGN/HTGN 做子图级推理，直接输出“时间-觉察”答案分布。</p>
</li>
<li><p><strong>噪声鲁棒的增量图更新</strong><br>上游 ASR、视觉字幕错误会传导至图。可探索：</p>
</li>
</ol>
<ul>
<li>基于置信度的边过滤；</li>
<li>跨天实体共指消解（同一“Shure”在不同天被拼写变异）；</li>
<li>在线纠错机制，利用新观测对旧边进行贝叶斯更新。</li>
</ul>
<ol>
<li><p><strong>多粒度时空层级</strong><br>在“帧-秒-分-小时-天”五级粒度分别维护子图，通过层级摘要实现 O(log T) 的检索，支持月-年级视频。</p>
</li>
<li><p><strong>音频-视觉-图 联合嵌入空间</strong><br>把帧、声谱、图节点统一映射到共享嵌入空间，用 late-interaction 向量检索替代级联 SQL+向量，减少多工具调用延迟。</p>
</li>
<li><p><strong>可解释代理规划学习</strong><br>目前子任务序列由 prompt 生成，可收集人类“推理链”数据，用强化学习或模仿学习训练 Planning Agent，降低失败循环与重复检索。</p>
</li>
</ol>
<p>数据-评测层面</p>
<ol>
<li><strong>更长、更私密的基准</strong></li>
</ol>
<ul>
<li>月-年级连续 egocentric 数据（公开难，可考虑合成+隐私脱敏）；</li>
<li>引入“负活动”标签（查询的事件实际未发生），衡量模型拒识能力。</li>
</ul>
<ol>
<li><p><strong>复合 habit 与因果推理任务</strong><br>设计需要反事实的问题，如“若我上周每晚提前 1 小时睡觉，还会被闹钟吵醒吗？”，推动模型结合实体图与因果常识。</p>
</li>
<li><p><strong>多语言/多方言音频</strong><br>验证图构建与说话人分离在非英语、嘈杂环境下的鲁棒性。</p>
</li>
</ol>
<p>系统-部署层面</p>
<ol>
<li><strong>端-云协同流水线</strong></li>
</ol>
<ul>
<li>眼镜端做 1 FPS 物体/人脸 on-device 提取，仅上传匿名实体节点，降低隐私与带宽；</li>
<li>云端负责跨天图合并与复杂查询。</li>
</ul>
<ol>
<li><p><strong>记忆遗忘与隐私合规</strong><br>引入“过期-衰减”策略，对超过法定期限的边自动匿名化或删除；研究用户可控的“即时遗忘”指令（Right-to-be-Forgotten）。</p>
</li>
<li><p><strong>低延迟近似查询</strong><br>针对 always-on 助手，设计 quality-latency trade-off 的早停机制：当检索置信度 &gt;τ 时立即返回答案，把剩余子任务转为后台 refine。</p>
</li>
<li><p><strong>硬件加速的图检索</strong><br>将 SQLite 替换为 FPGA/ASIC 加速的图存储，支持毫秒级时间范围+实体类型双索引，满足可穿戴设备“秒级响应”需求。</p>
</li>
</ol>
<p>综上，实体-时间图已展现出对超长视频的组合推理优势，下一步可沿“更深度的时序建模、更鲁棒的增量更新、更长的真实数据、更贴近落地的系统优化”四轴继续探索。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>EGAgent：面向“周级”egocentric 视频的实体-图代理推理系统</strong></p>
<ol>
<li><p>问题<br>全天候智能眼镜产生“数十小时-数天”连续第一视角视频，要求模型跨天、跨模态、多跳地回答“我上周和谁一起喝过咖啡”这类 habit-centric 查询。现有 LLM/VLM 受限于上下文窗口，且传统 RAG 无法显式追踪实体关系随时间演化。</p>
</li>
<li><p>思路</p>
</li>
</ol>
<ul>
<li>先对超长视频建立<strong>时间-感知实体场景图</strong>（节点=人/物/地点，边=关系类型+起止时间戳），可增量更新。</li>
<li>再设计<strong>EGAgent 代理框架</strong>：Planning-Agent 把查询拆成 ≤5 个子任务，依次调用三大工具——<br>– Entity-Graph Search（SQL 时间-关系过滤）<br>– Visual Search（SigLIP-2 帧嵌入检索）<br>– Audio Transcript Search（LLM/BM25 定位说话片段）<br>每步结果经 Analyzer 过滤后写入工作记忆，最终由 VQA-Agent 汇总生成答案。</li>
</ul>
<ol>
<li>实验</li>
</ol>
<ul>
<li>EgoLifeQA（50 h，500 MCQ）：57.5% 准确率，<strong>超出原 SOTA 20.6%</strong>；在需多跳实体关系的 RelationMap/TaskMaster 子集分别 <strong>+32%/+39.7%</strong>。</li>
<li>Video-MME (Long)（30–60 min 级）：74.1%，仅次于原生 Gemini-2.5-Pro，但仅用 50 帧（≈1/10 帧量）即可与 AdaVideoRAG 打平。</li>
<li>消融：三工具组合 &gt; 双工具 &gt; 单工具；图构建加入视觉字幕后平均 <strong>+2.6%</strong>；BM25 替换 LLM 搜索延迟 <strong>−3.6×</strong>，准确率仅 −6.8%。</li>
<li>Oracle 分析显示仍有 <strong>≈18%</strong> 绝对提升空间，主要瓶颈在更精准的时间定位。</li>
</ul>
<ol>
<li>贡献</li>
</ol>
<ul>
<li>提出<strong>时间-感知实体图</strong>表示，支持跨天、跨模态、组合推理。</li>
<li>设计<strong>EGAgent 代理流水线</strong>，首次把实体图检索纳入工具调用循环，实现超长视频 SOTA。</li>
<li>系统级评估与消融，验证实体图对关系型、习惯型问答的关键作用，并给出延迟-召回折中方案。</li>
</ul>
<ol>
<li>局限与未来<br>上游 ASR/字幕错误会传导；依赖人工说话人分离；月-年级视频、因果推理、端侧部署与隐私遗忘机制待进一步探索。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Agentic%20Very%20Long%20Video%20Understanding，链接是https://arxiv.org/pdf/2601.18157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18157。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Aniket Rege,Arka Sadhu,Yuliang Li,Kejie Li,Ramya Korlakai Vinayak,Yuning Chai,Yong Jae Lee,Hyo Jin Kim</p>
<p><strong>Categories</strong>: cs.CV,cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18157.pdf">https://arxiv.org/pdf/2601.18157.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18157">https://arxiv.org/abs/2601.18157</a></p>
<p><strong>Arxiv ID</strong>: 2601.18157</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18157">https://papers.cool/arxiv/2601.18157</a></p>
<p><strong>Published</strong>: 2026-01-26T05:20:47Z</p>
<p><strong>Updated</strong>: 2026-01-26T05:20:47.000Z</p>
<hr>
<h3 id="16-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback"><a href="#16-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback" class="headerlink" title="16. SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback"></a>16. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18202">SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback</a></h3><p>Deep search agents, which aim to answer complex questions requiring reasoning across multiple documents, can significantly speed up the information-seeking process. Collecting human annotations for this application is prohibitively expensive due to long and complex exploration trajectories. We propose an agentic pipeline that automatically generates high quality, difficulty-controlled deep search question-answer pairs for a given corpus and a target difficulty level. Our pipeline, SAGE, consists of a data generator which proposes QA pairs and a search agent which attempts to solve the generated question and provide execution feedback for the data generator. The two components interact over multiple rounds to iteratively refine the question-answer pairs until they satisfy the target difficulty level. Our intrinsic evaluation shows SAGE generates questions that require diverse reasoning strategies, while significantly increases the correctness and difficulty of the generated data. Our extrinsic evaluation demonstrates up to 23% relative performance gain on popular deep search benchmarks by training deep search agents with our synthetic data. Additional experiments show that agents trained on our data can adapt from fixed-corpus retrieval to Google Search at inference time, without further training.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>深度搜索代理旨在回答需要跨多个文档进行推理的复杂问题，能够显著加快信息检索的过程。由于探索路径长且复杂，为这一应用收集人工注释的成本过高。我们提出了一种智能化流程，用于自动生成针对给定语料库和目标难度级别的高质量、难度可控的深度搜索问答对。我们的流程 SAGE 由数据生成器（提出问答对）和搜索代理（尝试解决生成的问题并为数据生成器提供执行反馈）组成。两个组件通过多轮交互迭代优化问答对，直至达到目标难度水平。我们的内在评估显示，SAGE 生成的问题需要多样化的推理策略，同时显著提升了生成数据的正确性和难度。外在评估表明，通过使用我们生成的合成数据训练深度搜索代理，在流行的深度搜索基准上性能相对提升可达 23%。额外实验显示，使用我们数据训练的代理在推理时能够从固定语料库检索适应到谷歌搜索，无需进一步训练。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决<strong>高质量、难度可控的深度搜索（deep search）训练数据稀缺且人工标注成本极高</strong>的问题。具体而言：</p>
<ul>
<li>现有检索增强问答数据集大多只需≤4步检索即可回答，难以支撑需要长链条推理与多轮搜索的复杂场景；</li>
<li>人工标注此类长轨迹数据耗时耗力，无法规模化；</li>
<li>仅依靠反向生成（先固定答案再生成问题）无法保证所生成的问题既正确又真正达到目标难度（即需要预定步数才能解决）。</li>
</ul>
<p>为此，作者提出<strong>SAGE</strong>框架：<br>一个由<strong>数据生成智能体</strong>与<strong>搜索智能体</strong>组成的双智能体闭环系统，通过<strong>执行反馈</strong>迭代修正问题-答案对，使其在<strong>正确性</strong>与**难度（搜索步数）**两个维度上同时满足预设要求，从而自动产出可用于训练与评估深度搜索智能体的大规模合成数据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 SAGE 直接相关的研究可归纳为三条主线：深度搜索代理、合成数据生成、以及多跳问答数据集。按时间线与关联度梳理如下：</p>
<p>1. 深度搜索 / Agentic RAG</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>核心思路</th>
<th>与 SAGE 的关系</th>
</tr>
</thead>
<tbody>
<tr>
<td>ReAct (Yao et al., 2023)</td>
<td>交替输出“推理轨迹+搜索查询”模板</td>
<td>被 SAGE 直接用作搜索代理 Asearch 的交互范式</td>
</tr>
<tr>
<td>Search-R1 (Jin et al., 2025)</td>
<td>用 RL+Outcome Reward 训练搜索代理，无需人工轨迹</td>
<td>SAGE 的下游实验采用其 PPO 框架做训练</td>
</tr>
<tr>
<td>Search-o1 (Li et al., 2025c)</td>
<td>在单轮检索内做“in-document reasoning”过滤噪声</td>
<td>与 SAGE 互补，SAGE 关注数据生成而非推理策略</td>
</tr>
<tr>
<td>Self-RAG (Asai et al., 2024)</td>
<td>自反思式检索-生成-批判</td>
<td>同为迭代检索，但依赖人工标注的反思标签</td>
</tr>
</tbody>
</table>
</div>
<p>2. 合成数据生成（Concurrent Work）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>输入信号</th>
<th>显式结构</th>
<th>检索器</th>
<th>开源数据</th>
<th>与 SAGE 差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebDancer (Wu et al., 2025)</td>
<td>网页或种子实体</td>
<td>超链接图</td>
<td>Google Search</td>
<td>未开源</td>
<td>依赖浏览器 API，成本高</td>
</tr>
<tr>
<td>WebShaper (Tao et al., 2025)</td>
<td>网页</td>
<td>知识图谱+超链</td>
<td>Google Search</td>
<td>未开源</td>
<td>需预构图谱，迁移性差</td>
</tr>
<tr>
<td>WebSailor (Li et al., 2025a)</td>
<td>种子实体</td>
<td>知识图谱</td>
<td>Google Search</td>
<td>未开源</td>
<td>需外部图谱且未考虑难度控制</td>
</tr>
<tr>
<td>WebPuzzle (Shi et al., 2025b)</td>
<td>稀有实体</td>
<td>无</td>
<td>Web Search</td>
<td>未开源</td>
<td>通过“信息遮蔽”提升难度，无执行反馈</td>
</tr>
<tr>
<td>WebExplorer (Liu et al., 2025)</td>
<td>种子实体</td>
<td>无</td>
<td>Google Search</td>
<td>未开源</td>
<td>仅基于实体遮蔽，无迭代修正</td>
</tr>
<tr>
<td>SAGE (本文)</td>
<td>随机段落</td>
<td>无</td>
<td>固定 Wikipedia</td>
<td>✔ 计划开源</td>
<td>双智能体+执行反馈，难度可控，零外部结构</td>
</tr>
</tbody>
</table>
</div>
<p>3. 多跳问答数据集（训练/评测基准）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>数据集</th>
<th>构建方式</th>
<th>平均检索步数</th>
<th>与 SAGE 对比</th>
</tr>
</thead>
<tbody>
<tr>
<td>NQ (Kwiatkowski et al., 2019)</td>
<td>人工标注</td>
<td>1.3</td>
<td>单跳，难度低</td>
</tr>
<tr>
<td>HotpotQA (Yang et al., 2018)</td>
<td>自动+人工</td>
<td>2.1</td>
<td>2 跳为主，无难度控制</td>
</tr>
<tr>
<td>Musique (Trivedi et al., 2022)</td>
<td>自动组合单跳问题</td>
<td>2.7</td>
<td>2–4 跳，难度固定</td>
</tr>
<tr>
<td>FRAMES (Krishna et al., 2025)</td>
<td>人工标注</td>
<td>3.2</td>
<td>评测集小，无法规模化训练</td>
</tr>
<tr>
<td>SAGE 合成数据</td>
<td>双智能体迭代</td>
<td>4.9</td>
<td>3–7 跳可调，难度可控，规模 20 k+</td>
</tr>
</tbody>
</table>
</div>
<p>4. 其他相关技术</p>
<ul>
<li><p><strong>Best-of-K / Self-Verification</strong> (Shi et al., 2025a)<br>仅用搜索代理做“过滤”，不反向修正问题；SAGE 进一步把执行轨迹作为生成器反馈。</p>
</li>
<li><p><strong>Reverse QA Generation</strong> (Wei et al., 2025)<br>先固定答案再生成问题以保证忠实度，SAGE 沿用该思路但指出“仅反向”不足以满足难度目标。</p>
</li>
<li><p><strong>PPO+GAE</strong> (Schulman et al., 2017; 2015)<br>SAGE 的下游训练直接采用该 RL 框架，与 Search-R1 一致。</p>
</li>
</ul>
<p>小结</p>
<p>SAGE 与既有工作的根本区别在于：</p>
<ol>
<li><strong>不依赖外部知识图谱或浏览器 API</strong>，基于固定语料（Wikipedia）即可生成 3–7 跳可调难度的数据；</li>
<li><strong>首次引入“执行反馈”闭环</strong>，用搜索代理的实际轨迹指导生成器迭代修正，而非简单过滤；</li>
<li><strong>开源大规模训练集</strong>，填补现有公开数据止步于 4 跳的空白。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 <strong>SAGE（Steerable Agentic Generation with Execution Feedback）</strong> 框架，通过“生成-验证-修正”闭环，把<strong>数据生成智能体</strong>与<strong>搜索智能体</strong>耦合在一起，迭代地提升合成数据的正确性与难度。核心流程可概括为三步：</p>
<p>1. 反向生成 + 难度提示（Difficulty Prompt）</p>
<ul>
<li>给定<strong>随机段落</strong>  d  与<strong>目标搜索步数</strong>  S ，数据生成器  A_(gen)  以 <strong>ReAct 风格</strong> 交替执行<br>&lt; think &gt;  规划 →  &lt; search &gt;  查询 →  &lt; information &gt;  返回<br>直到用满  S  次检索，再输出问题-答案对  (q,a) 。</li>
<li>显式在 prompt 中注入  S ，使生成器<strong>主动规划</strong>一条需  S  步才能回答的问题路径，而非事后被动过滤。</li>
</ul>
<p>2. 执行验证（Execution Verification）</p>
<ul>
<li>用<strong>独立搜索智能体</strong>  A_(search)  对  (q,a)  进行 <strong>K 次并行 rollout</strong>（算法 2）。</li>
<li>记录每条轨迹的实际步数  S’_k  与最终答案  a’_k 。</li>
<li>定义两条判定标准：</li>
<li><strong>Correctness</strong>： ∃ k;a’_k = a ，即 pass@K=1；</li>
<li><strong>Difficulty</strong>： min S’_k mid a’_k = a ≥ S 。</li>
<li>若任一标准不满足，则把<strong>生成器轨迹</strong>与<strong>搜索轨迹</strong>一并保存为<strong>执行反馈</strong>  T<em>(gen), T</em>(search) ，进入下一步修正。</li>
</ul>
<p>3. 执行反馈重生成（Generation with Execution Feedback）</p>
<ul>
<li>将  T<em>(gen), T</em>(search)  作为上下文重新喂给  A_(gen) ，要求它：</li>
<li>若答案错误：对比轨迹，找出事实冲突，<strong>修正问题或答案</strong>；</li>
<li>若难度不足：分析“多查询坍缩”“信息同文档”等捷径，<strong>重写问题</strong>迫使搜索必须 ≥S 步。</li>
<li>可迭代最多  R  轮（通常  R =2 即可），直到同时满足 correctness &amp; difficulty。</li>
<li>最终只保留 pass@K=1 且步数达标的  (q,a)  用于训练。</li>
</ul>
<p>算法级概览</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Algorithm 1 行 7–15：</span><br><span class="line">for r = 0 … R</span><br><span class="line">(q,a) ← A_gen(d, S, T_gen, T_search) // 首轮无反馈，后续带反馈</span><br><span class="line">(a★,S★, …, IS_CORRECT, IS_DIFFICULT) ← RunSearchAgent(q,a,S,K)</span><br><span class="line">T_gen ← T_gen ∪ &#123;当前生成轨迹&#125;</span><br><span class="line">T_search ← T_search ∪ &#123;搜索轨迹&#125;</span><br><span class="line">if IS_CORRECT ∧ IS_DIFFICULT: break</span><br><span class="line">return (q,a) // 仅输出正确且达难度样本</span><br></pre></td></tr></table></figure>
<p>关键设计要点</p>
<ol>
<li><p><strong>双智能体解耦</strong><br>生成器负责“构思”难度路径；搜索器提供“真实”执行结果，避免自验证盲区。</p>
</li>
<li><p><strong>轨迹级反馈</strong><br>不只是布尔信号，而是把<strong>中间检索到的文档、推理步骤、失败原因</strong>全部送回生成器，实现细粒度修正。</p>
</li>
<li><p><strong>难度可控且可迭代增强</strong><br>通过显式步数目标 + 执行反馈，使难度从初始 18 % 合格率（表 2）提升到 50 % 以上，平均检索步数从 3.3 增至 4.9。</p>
</li>
<li><p><strong>零外部结构</strong><br>无需知识图谱、网页超链或人工标注，仅需固定语料即可规模化生成 20 k–100 k 训练样本。</p>
</li>
</ol>
<p>效果总结</p>
<ul>
<li><strong>内在质量</strong>：相比纯重采样，执行反馈在目标步数 3–7 范围内将“正确且达标”比例再提升 <strong>+17 %</strong>（图 2）。</li>
<li><strong>下游增益</strong>：用 SAGE 数据训练的 3 B/7 B 搜索代理，在领域内平均提升 <strong>27 %</strong>，跨域 FRAMES 提升 <strong>23 %</strong>（表 3）。</li>
<li><strong>工具迁移</strong>：仅基于 Wikipedia 训练即可在 <strong>Google Search</strong> 上取得最高 <strong>50 %</strong> 相对提升（表 4），验证数据本身的泛化性。</li>
</ul>
<p>综上，SAGE 通过“生成-验证-修正”闭环，把<strong>难度可控</strong>与<strong>执行忠实</strong>同时做成可迭代优化的目标，从而低成本地规模化产出高质量深度搜索训练数据。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>数据质量</strong>与<strong>下游效果</strong>两条主线展开系统实验，涵盖内在评估、外在评估、跨工具迁移与消融分析四部分。核心结果如下：</p>
<p>1. 内在评估（Intrinsic Data Quality）</p>
<p><strong>目的</strong>：验证 SAGE 生成的 (q,a) 是否在<strong>正确性</strong>与**难度（搜索步数）**上同时达标。<br><strong>基准</strong>：</p>
<ul>
<li>无难度提示的生成器</li>
<li>初始生成器（带 S 但无反馈）</li>
<li>Best-of-K 重采样（1–3 轮）</li>
</ul>
<p><strong>指标</strong>：</p>
<ul>
<li>% correct：pass@4=1 的比例</li>
<li>% pass：既正确又 ≥S 步的比例</li>
<li>Avg@4：4 条轨迹的平均准确率（越低越难）</li>
<li><h1 id="search：正确轨迹的最小步数"><a href="#search：正确轨迹的最小步数" class="headerlink" title="search：正确轨迹的最小步数"></a>search：正确轨迹的最小步数</h1></li>
</ul>
<p><strong>结果</strong>（表 2 &amp; 图 2）：</p>
<p>| 方法 | % correct | % pass | # search↑ |</p>
<p>| —- | —- | —- | —- |<br>| 初始生成器 | 71 | 18 | 3.3 |<br>| +3 轮重采样 | 84 | 47 | 4.8 |<br>| SAGE +3 轮反馈 | 87 | 50 | 4.9 |</p>
<p>在目标步数 3–7 区间，执行反馈比重采样平均再提升 <strong>+9 % pass</strong>，且难度随 S 单调递增。</p>
<p>2. 下游评估（Downstream Agent Training）</p>
<p><strong>目的</strong>：用合成数据训练搜索代理，看能否带来<strong>领域内</strong>与<strong>域外</strong>提升。<br><strong>训练集</strong>：</p>
<ul>
<li>NQ+HotpotQA（150 k）</li>
<li>Musique（20 k）</li>
<li><strong>SAGE 合成数据（20 k，2 轮反馈）</strong></li>
</ul>
<p><strong>模型</strong>：Qwen-2.5-3B / 7B，统一用 Search-R1 的 PPO 框架。<br><strong>评测集</strong>：</p>
<ul>
<li>In-domain：SAGE 自产 300 题 × {3,…,7} 跳</li>
<li>Out-of-domain：Musique-2/3/4 跳各 300 题、FRAMES 300 题</li>
</ul>
<p><strong>结果</strong>（表 3）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>骨干</th>
<th>训练数据</th>
<th>In-domain AVG</th>
<th>FRAMES</th>
<th>Musique</th>
</tr>
</thead>
<tbody>
<tr>
<td>3B</td>
<td>NQ+HotpotQA</td>
<td>15.9</td>
<td>13.3</td>
<td>11.4</td>
</tr>
<tr>
<td>3B</td>
<td>Musique</td>
<td>22.4</td>
<td>21.5</td>
<td>19.4</td>
</tr>
<tr>
<td>3B</td>
<td>SAGE</td>
<td>28.5 (+27 %)</td>
<td>23.8</td>
<td>19.9</td>
</tr>
<tr>
<td>7B</td>
<td>NQ+HotpotQA</td>
<td>29.1</td>
<td>26.2</td>
<td>18.9</td>
</tr>
<tr>
<td>7B</td>
<td>Musique</td>
<td>29.6</td>
<td>25.0</td>
<td>21.6</td>
</tr>
<tr>
<td>7B</td>
<td>SAGE</td>
<td>38.1 (+29 %)</td>
<td>32.3 (+23 %)</td>
<td>22.3</td>
</tr>
</tbody>
</table>
</div>
<p>3. 跨工具迁移（Google Search 零样本）</p>
<p><strong>目的</strong>：检验<strong>固定语料（Wikipedia）训练</strong>的代理能否直接迁移到<strong>Google Search</strong>。<br><strong>评测基准</strong>：</p>
<ul>
<li>GAIA-text（103 题）</li>
<li>BrowseComp（200 题）</li>
<li>HLE-Search（子集）</li>
</ul>
<p><strong>结果</strong>（表 4）：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模型</th>
<th>训练数据</th>
<th>GAIA</th>
<th>BrowseComp</th>
<th>HLE</th>
</tr>
</thead>
<tbody>
<tr>
<td>7B</td>
<td>NQ+HotpotQA</td>
<td>14.6</td>
<td>1.6</td>
<td>4.5</td>
</tr>
<tr>
<td>7B</td>
<td>Musique</td>
<td>15.6</td>
<td>2.1</td>
<td>8.0</td>
</tr>
<tr>
<td>7B</td>
<td>SAGE</td>
<td>24.0 (+50 %)</td>
<td>2.6 (+62 %)</td>
<td>7.0</td>
</tr>
</tbody>
</table>
</div>
<p>3B 模型在 GAIA 上也从 12.5 % → 18.8 %（+36 %）。</p>
<p>4. 消融实验</p>
<p>4.1 反馈轮数影响（表 5）</p>
<p>固定 20 k 训练集，仅改生成轮数：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>轮数</th>
<th>In-domain</th>
<th>Musique</th>
<th>FRAMES</th>
<th>Avg@4↓</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>33.6</td>
<td>18.7</td>
<td>29.0</td>
<td>86.3</td>
</tr>
<tr>
<td>1</td>
<td>33.6</td>
<td>19.5</td>
<td>29.3</td>
<td>83.2</td>
</tr>
<tr>
<td>2</td>
<td>38.1</td>
<td>22.3</td>
<td>32.3</td>
<td>80.4</td>
</tr>
<tr>
<td>3</td>
<td>34.1</td>
<td>20.9</td>
<td>28.1</td>
<td>79.5</td>
</tr>
</tbody>
</table>
</div>
<p><strong>结论</strong>：两轮反馈最佳；三轮数据虽更难，但性能反而下降，提示<strong>难度需与可学习性平衡</strong>。</p>
<p>4.2 推理策略分布（图 3）</p>
<p>抽样 100 条正确轨迹，标注 6 类推理行为：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>策略</th>
<th>Musique</th>
<th>SAGE</th>
</tr>
</thead>
<tbody>
<tr>
<td>Calculation</td>
<td>5 %</td>
<td>35 %</td>
</tr>
<tr>
<td>Temporal</td>
<td>8 %</td>
<td>32 %</td>
</tr>
<tr>
<td>Conflict Resolution</td>
<td>40 %</td>
<td>55 %</td>
</tr>
</tbody>
</table>
</div>
<p>SAGE 覆盖更多<strong>数值/时间/冲突解决</strong>类型，分布更均衡。</p>
<p>4.3 错误模式剖析（表 6）</p>
<p>对初始生成器失败案例分类：</p>
<ul>
<li><p><strong>Easy 数据</strong>（难度不足）<br>35 % 信息同文档（co-location）<br>21 % 多查询坍缩（multi-query collapse）</p>
</li>
<li><p><strong>Incorrect 数据</strong><br>54 % 搜索代理检索失败<br>19 % 生成器自身幻觉</p>
</li>
</ul>
<p>验证<strong>执行反馈</strong>是发现“捷径”与“幻觉”的唯一途径。</p>
<p>实验小结</p>
<ol>
<li><strong>内在质量</strong>：SAGE 把“正确且达标”比例从 18 % 提到 50 %，平均步数 4.9，显著难于现有数据集。</li>
<li><strong>下游增益</strong>：同等训练量下，3 B/7 B 模型在领域内提升 <strong>27–29 %</strong>，跨域提升 <strong>11–23 %</strong>。</li>
<li><strong>工具迁移</strong>：零样本切换到 Google Search，GAIA 提升 <strong>36–50 %</strong>，证明数据本身的<strong>工具无关性</strong>。</li>
<li><strong>消融与可解释性</strong>：两轮反馈为最佳；合成数据覆盖更丰富的推理类别，且失败原因可被轨迹级诊断。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续推进，分为<strong>方法层面</strong>与<strong>场景层面</strong>两大块，并给出可验证的关键假设与可能指标。</p>
<p>方法层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键假设 / 目标</th>
<th>可验证指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>1. 双智能体协同进化</td>
<td>搜索代理与生成器交替迭代训练，可进一步提升数据质量与代理能力</td>
<td>生成器单轮通过率、代理最终 F1、双向 KL 散度变化</td>
</tr>
<tr>
<td>2. 更鲁棒的正确性判定</td>
<td>对“搜索代理 0 次通过”的样本，引入跨模型共识或人工轻标可挽回 10–20 % 有效数据</td>
<td>人工抽检 100 例，计算 precision / recall</td>
</tr>
<tr>
<td>3. 难度-可学习性曲线建模</td>
<td>存在最优难度区间使下游收益最大，可用课程学习动态调节</td>
<td>训练损失下降斜率、样本级 advantage 与最终性能相关系数</td>
</tr>
<tr>
<td>4. 细粒度难度维度</td>
<td>除“步数”外，引入推理类型组合度、信息熵、冲突密度等多维难度</td>
<td>线性回归 R² 预测样本解决率</td>
</tr>
<tr>
<td>5. 生成轨迹→SFT 数据</td>
<td>把生成器轨迹直接转为搜索代理的 supervised fine-tuning 轨迹，减少 RL 方差</td>
<td>同等数据量下 RL vs SFT 的样本效率比</td>
</tr>
<tr>
<td>6. 更先进的 RL 算法</td>
<td>GRPO、RLOO 等无价值函数方法可降低训练资源</td>
<td>GPU 小时数、相同算力下的性能差</td>
</tr>
</tbody>
</table>
</div>
<p>场景层面</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>关键假设 / 目标</th>
<th>可验证指标</th>
</tr>
</thead>
<tbody>
<tr>
<td>7. 领域专用语料</td>
<td>在法律、医疗、金融等封闭语料上生成数据，可提升对应领域深度搜索性能</td>
<td>领域评测集（LexSQuAD、PubMedQA）F1 / EM</td>
</tr>
<tr>
<td>8. 多语言深度搜索</td>
<td>难度可控框架对低资源语言同样有效</td>
<td>跨语言 zero-shot EM、相对高资源基线提升率</td>
</tr>
<tr>
<td>9. 多模态深度搜索</td>
<td>引入表格、图像、PDF后，步数定义扩展为“跨模态检索次数”</td>
<td>新模态 benchmark 上 EM、人工判定跨模态推理正确率</td>
</tr>
<tr>
<td>10. 检索工具组合</td>
<td>同时使用搜索引擎 + 知识图谱 + 代码解释器，生成“工具链”数据</td>
<td>工具调用成功率、单工具 vs 组合工具的性能差</td>
</tr>
<tr>
<td>11. 在线/实时环境</td>
<td>将生成器部署为在线数据增强器，随真实用户查询实时合成新样本</td>
<td>在线 A/B 测试用户满意度、搜索轮次减少比例</td>
</tr>
<tr>
<td>12. 长尾知识覆盖</td>
<td>针对冷门实体（&lt; 10 维基引用）生成数据，可缓解长尾遗忘</td>
<td>长尾子集 recall、与流行实体子集的性能差距缩小量</td>
</tr>
</tbody>
</table>
</div>
<p>可立即落地的短期实验</p>
<ol>
<li><strong>两轮→协同进化</strong>：固定生成器，用当前最佳代理重新标注 5 k 失败样本，再微调生成器，观察一轮共进化后通过率提升。</li>
<li><strong>多维难度消融</strong>：保持总步数=5，仅把“计算”类型比例从 10 % 提到 40 %，看代理在含计算题子集上的 EM 变化。</li>
<li><strong>课程学习</strong>：按 #search 3→7 渐进采样训练集，每 100 step 升一次难度，绘制“难度-性能”曲线，验证倒 U 型假设。</li>
</ol>
<p>总结</p>
<p>SAGE 首次把“执行反馈”引入深度搜索数据合成，但仍停留在<strong>单轮固定语料、单维难度、单智能体冻结</strong>阶段。<br>下一步可从<strong>协同进化、多维难度、领域/模态扩展、在线增强</strong>四个维度展开，形成“数据-模型”双飞的闭环生态。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文核心速览</p>
<p>1. 问题</p>
<ul>
<li>深度搜索（多轮检索+推理）代理需要<strong>长链条、高难度、高质量</strong>的问答训练数据</li>
<li>人工标注代价高；现有合成数据**≤4跳**、<strong>无难度控制</strong>、<strong>不开源</strong></li>
</ul>
<p>2. 方法：SAGE 框架</p>
<p><strong>双智能体 + 执行反馈闭环</strong></p>
<ol>
<li><strong>数据生成器</strong>  A_(gen)<br>以随机维基段落为起点，按<strong>目标搜索步数 S</strong> 反向规划，生成 (q, a)</li>
<li><strong>搜索智能体</strong>  A_(search)<br>对同一 q 采样 K 条轨迹，给出<strong>真实步数与答案</strong></li>
<li><strong>执行反馈修正</strong><br>把生成轨迹 &amp; 搜索轨迹回灌给  A_(gen) ，<strong>重写问题或答案</strong>，直到<strong>正确</strong>且<strong>步数≥S</strong><br>→ 可迭代多轮，只保留最终达标样本</li>
</ol>
<p>3. 实验结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>工作</th>
<th>输入信号</th>
<th>显式结构</th>
<th>检索器</th>
<th>开源数据</th>
<th>与 SAGE 差异</th>
</tr>
</thead>
<tbody>
<tr>
<td>WebDancer (Wu et al., 2025)</td>
<td>网页或种子实体</td>
<td>超链接图</td>
<td>Google Search</td>
<td>未开源</td>
<td>依赖浏览器 API，成本高</td>
</tr>
<tr>
<td>WebShaper (Tao et al., 2025)</td>
<td>网页</td>
<td>知识图谱+超链</td>
<td>Google Search</td>
<td>未开源</td>
<td>需预构图谱，迁移性差</td>
</tr>
<tr>
<td>WebSailor (Li et al., 2025a)</td>
<td>种子实体</td>
<td>知识图谱</td>
<td>Google Search</td>
<td>未开源</td>
<td>需外部图谱且未考虑难度控制</td>
</tr>
<tr>
<td>WebPuzzle (Shi et al., 2025b)</td>
<td>稀有实体</td>
<td>无</td>
<td>Web Search</td>
<td>未开源</td>
<td>通过“信息遮蔽”提升难度，无执行反馈</td>
</tr>
<tr>
<td>WebExplorer (Liu et al., 2025)</td>
<td>种子实体</td>
<td>无</td>
<td>Google Search</td>
<td>未开源</td>
<td>仅基于实体遮蔽，无迭代修正</td>
</tr>
<tr>
<td>SAGE (本文)</td>
<td>随机段落</td>
<td>无</td>
<td>固定 Wikipedia</td>
<td>✔ 计划开源</td>
<td>双智能体+执行反馈，难度可控，零外部结构</td>
</tr>
</tbody>
</table>
</div>
<p>0</p>
<p>4. 贡献</p>
<ul>
<li>首个<strong>难度可控</strong>（3-7 跳）且<strong>执行验证</strong>的深度搜索数据合成 pipeline</li>
<li><strong>开源</strong> 20 k+ 训练集，填补公共数据空白</li>
<li>证明<strong>固定语料合成</strong>即可泛化到<strong>实时搜索引擎</strong>，降低数据成本</li>
</ul>
<blockquote>
<p>一句话：SAGE 用“生成-验证-修正”闭环，低成本产出<strong>又难又对</strong>的深度搜索训练数据，显著提升模型多跳检索与推理能力。</p>
</blockquote>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SAGE%3A%20Steerable%20Agentic%20Data%20Generation%20for%20Deep%20Search%20with%20Execution%20Feedback，链接是https://arxiv.org/pdf/2601.18202，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.18202。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Fangyuan Xu,Rujun Han,Yanfei Chen,Zifeng Wang,I-Hung Hsu,Jun Yan,Vishy Tirumalashetty,Eunsol Choi,Tomas Pfister,Chen-Yu Lee</p>
<p><strong>Categories</strong>: cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18202.pdf">https://arxiv.org/pdf/2601.18202.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18202">https://arxiv.org/abs/2601.18202</a></p>
<p><strong>Arxiv ID</strong>: 2601.18202</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18202">https://papers.cool/arxiv/2601.18202</a></p>
<p><strong>Published</strong>: 2026-01-26T06:37:56Z</p>
<p><strong>Updated</strong>: 2026-01-26T06:37:56.000Z</p>
<hr>
<h3 id="17-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal"><a href="#17-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal" class="headerlink" title="17. DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal"></a>17. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.18081">DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal</a></h3><p>Despite the growing adoption of large language models (LLMs) in scientific research workflows, automated support for academic rebuttal, a crucial step in academic communication and peer review, remains largely underexplored. Existing approaches typically rely on off-the-shelf LLMs or simple pipelines, which struggle with long-context understanding and often fail to produce targeted and persuasive responses. In this paper, we propose DRPG, an agentic framework for automatic academic rebuttal generation that operates through four steps: Decompose reviews into atomic concerns, Retrieve relevant evidence from the paper, Plan rebuttal strategies, and Generate responses accordingly. Notably, the Planner in DRPG reaches over 98% accuracy in identifying the most feasible rebuttal direction. Experiments on data from top-tier conferences demonstrate that DRPG significantly outperforms existing rebuttal pipelines and achieves performance beyond the average human level using only an 8B model. Our analysis further demonstrates the effectiveness of the planner design and its value in providing multi-perspective and explainable suggestions. We also showed that DRPG works well in a more complex multi-round setting. These results highlight the effectiveness of DRPG and its potential to provide high-quality rebuttal content and support the scaling of academic discussions. Codes for this work are available at <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/DRPG-RebuttalAgent">https://github.com/ulab-uiuc/DRPG-RebuttalAgent</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>尽管大型语言模型（LLMs）在科学研究工作流程中日益普及，但自动化支持学术反驳——学术交流和同行评审中的关键一步——仍然鲜被充分探索。现有方法通常依赖现成的大型语言模型或简单的流水线，这些工具难以理解长上下文，且常常无法产生有针对性和有说服力的回应。本文提出了DRPG，一种自动生成学术反驳的智能框架，通过四个步骤运作：将评论分解为原子论点，检索相关证据，规划反驳策略，并据此生成回应。值得注意的是，DRPG中的策划器在识别最可行反驳方向的准确率超过98%。顶级会议数据实验显示，DRPG在仅用8B模型时，显著优于现有的反驳流程，并达到了超越平均人类水平的表现。我们的分析进一步展示了规划设计的有效性及其在提供多视角和可解释建议方面的价值。我们还展示了DRPG在更复杂的多轮环境中表现良好。这些结果凸显了DRPG的有效性及其提供高质量反驳内容和支持学术讨论规模化的潜力。该作品的代码可在 <a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/DRPG-RebuttalAgent">https://github.com/ulab-uiuc/DRPG-RebuttalAgent</a> 获取。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决“学术反驳（academic rebuttal）”环节的自动化问题。随着大型语言模型（LLM）在科研流程中的普及，审稿-作者之间的往返沟通——尤其是作者针对审稿意见撰写有说服力、针对性强的反驳——仍缺乏系统性的自动化支持。具体痛点包括：</p>
<ul>
<li>长论文与长审稿导致上下文理解困难，模型容易“迷失在中间（Lost in the Middle）”；</li>
<li>通用 LLM 生成的回复往往流于空泛、过于妥协或防御，无法精准回应审稿人的核心关切；</li>
<li>人工撰写高质量反驳耗时耗力，面对顶级会议数万篇投稿的规模，亟需可扩展的辅助方案。</li>
</ul>
<p>为此，作者提出 DRPG（Decompose, Retrieve, Plan, Generate）四阶段智能体框架，通过“分解-检索-规划-生成”显式建模反驳策略，实现基于论文内容的、有针对性和说服力的自动反驳生成，并在多轮对话场景下保持优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在第 2 节“Related Work”中系统梳理了三条研究脉络，可归纳如下：</p>
<ol>
<li><strong>LLM for academic rebuttal</strong></li>
</ol>
<ul>
<li>早期工作主要收集真实反驳语料（Kennard et al. 2022; Zhang et al. 2025a）。</li>
<li>随后尝试用零样本 LLM 或固定模板生成初步回复（Kirtani et al. 2025; Jin et al. 2024b; Purkayastha et al. 2023）。</li>
<li>这些研究缺乏对长文证据的精准检索与策略规划，导致回应空泛。</li>
</ul>
<ol>
<li><strong>LLM for debate &amp; persuasion</strong></li>
</ol>
<ul>
<li>借鉴人类辩论策略与高质量对话数据，提升模型说服能力（Wang et al. 2019; Yang et al. 2019; Singh et al. 2024）。</li>
<li>近期出现多智能体辩论管线（Wang et al. 2025a; Hu et al. 2024）与强化学习辩手（Cheng &amp; You 2025; Han et al. 2025）。</li>
<li>学术反驳被视为“事实导向、证据 grounded”的新辩论场景，尚未被充分探索。</li>
</ul>
<ol>
<li><strong>Planning in high-stakes decision making</strong></li>
</ol>
<ul>
<li>蒙特卡洛树搜索模拟多步后果（Coulom 2006; Silver et al. 2016），多用于多智能体或外部环境交互。</li>
<li>训练选择器/验证器做策略筛选（Han et al. 2025; Wang et al. 2025b; Zhang et al. 2025b），适用于可大规模获得监督信号的场景。</li>
<li>DRPG 的 Planner 即属于后者：用可解释的 MLP 打分器在“澄清”与“辩护”两大策略空间内选出最被论文内容支持的方向。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>
</div></details>
<p><strong>Authors</strong>: Peixuan Han,Yingjie Yu,Jingjun Xu,Jiaxuan You</p>
<p><strong>Categories</strong>: cs.LG</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.18081.pdf">https://arxiv.org/pdf/2601.18081.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.18081">https://arxiv.org/abs/2601.18081</a></p>
<p><strong>Arxiv ID</strong>: 2601.18081</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.18081">https://papers.cool/arxiv/2601.18081</a></p>
<p><strong>Published</strong>: 2026-01-26T02:30:01Z</p>
<p><strong>Updated</strong>: 2026-01-26T02:30:01.000Z</p>
<hr>
<h3 id="18-SkyReels-V3-Technique-Report"><a href="#18-SkyReels-V3-Technique-Report" class="headerlink" title="18. SkyReels-V3 Technique Report"></a>18. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17323">SkyReels-V3 Technique Report</a></h3><p>Video generation serves as a cornerstone for building world models, where multimodal contextual inference stands as the defining test of capability. In this end, we present SkyReels-V3, a conditional video generation model, built upon a unified multimodal in-context learning framework with diffusion Transformers. SkyReels-V3 model supports three core generative paradigms within a single architecture: reference images-to-video synthesis, video-to-video extension and audio-guided video generation. (i) reference images-to-video model is designed to produce high-fidelity videos with strong subject identity preservation, temporal coherence, and narrative consistency. To enhance reference adherence and compositional stability, we design a comprehensive data processing pipeline that leverages cross frame pairing, image editing, and semantic rewriting, effectively mitigating copy paste artifacts. During training, an image video hybrid strategy combined with multi-resolution joint optimization is employed to improve generalization and robustness across diverse scenarios. (ii) video extension model integrates spatio-temporal consistency modeling with large-scale video understanding, enabling both seamless single-shot continuation and intelligent multi-shot switching with professional cinematographic patterns. (iii) Talking avatar model supports minute-level audio-conditioned video generation by training first-and-last frame insertion patterns and reconstructing key-frame inference paradigms. On the basis of ensuring visual quality, synchronization of audio and videos has been optimized. Extensive evaluations demonstrate that SkyReels-V3 achieves state-of-the-art or near state-of-the-art performance on key metrics including visual quality, instruction following, and specific aspect metrics, approaching leading closed-source systems. Github: <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V3">https://github.com/SkyworkAI/SkyReels-V3</a>.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>视频生成是构建世界模型的基石，其中多模态上下文推理是能力的决定性测试。为此，我们提出了 SkyReels-V3，这是一种条件视频生成模型，基于统一的多模态上下文学习框架与扩散 Transformer 构建。SkyReels-V3 模型在单一架构中支持三种核心生成范式：参考图像到视频的合成、视频到视频的扩展以及音频驱动的视频生成。(i) 参考图像到视频模型旨在生成高保真视频，具有强烈的主体身份保留、时间连贯性和叙事一致性。为了增强参考依从性和组合稳定性，我们设计了一个全面的数据处理流程，利用跨帧配对、图像编辑和语义重写，有效减轻复制粘贴伪影。在训练过程中，采用图像-视频混合策略结合多分辨率联合优化，以提升在各种场景下的泛化能力和鲁棒性。(ii) 视频扩展模型将时空一致性建模与大规模视频理解相结合，实现了无缝的单镜头延续以及具有专业电影拍摄模式的智能多镜头切换。(iii) 讲话头像模型通过训练首帧和末帧插入模式并重建关键帧推理范式，支持分钟级音频条件的视频生成。在保证视觉质量的基础上，音频与视频的同步性也得到了优化。大量评估表明，SkyReels-V3 在视觉质量、指令遵循及特定指标等关键指标上实现了最先进或接近最先进的性能，接近领先的闭源系统。GitHub: <a target="_blank" rel="noopener" href="https://github.com/SkyworkAI/SkyReels-V3">https://github.com/SkyworkAI/SkyReels-V3</a>.</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>SkyReels-V3 旨在解决“多模态上下文推理条件下的高质量、可控视频生成”这一核心问题，具体可拆解为以下三点：</p>
<ol>
<li><strong>统一框架缺失</strong><br>现有扩散 Transformer 视频模型大多针对单一生成范式（如纯文本驱动、纯参考图驱动或纯音频驱动），无法在单一架构内同时支持</li>
</ol>
<ul>
<li>参考图→视频</li>
<li>视频→视频续拍/转场</li>
<li>音频→说话人视频<br>三种任务，导致模型碎片化、重复训练成本高。</li>
</ul>
<ol>
<li><p><strong>多模态条件对齐困难</strong><br>当输入同时包含图像、视频、文本、音频时，不同模态的时空粒度、语义抽象层级差异极大，直接拼接特征会引发“copy-paste 伪影”、身份漂移、音画不同步等问题。</p>
</li>
<li><p><strong>长时序一致性与专业级镜头语言缺失</strong><br>现有方法在分钟级长视频生成中难以同时保持：</p>
</li>
</ol>
<ul>
<li>主体身份一致性（人脸/服饰/物体）</li>
<li>物理运动合理性</li>
<li>电影级转场（cut-in、cut-out、shot/reverse-shot 等）</li>
<li>音素级唇形同步</li>
</ul>
<p>SkyReels-V3 通过“统一多模态上下文学习框架 + 扩散 Transformer”将上述三类任务整合到同一套参数空间，并配套提出参考图保持数据构造、分层时空位置编码、关键帧约束生成等策略，从而在单一模型内实现接近闭源商业系统的综合性能。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>与 SkyReels-V3 直接相关的研究可沿三条主线梳理：扩散 Transformer 视频生成、多模态条件注入、以及专用任务模型。关键文献按主题归类如下。</p>
<p>1. 扩散 Transformer 视频生成 backbone</p>
<ul>
<li><strong>DiT / Vid-DiT</strong><br>Peebles &amp; Xie, 2023  —— 首次将 Transformer 引入图像扩散，为后续视频扩展提供基础架构。</li>
<li><strong>HunyuanVideo</strong><br>Kong et al., 2024  —— 系统性 2D→3D 参数扩展 + flow-matching，支持 1280×720 长视频。</li>
<li><strong>Wan</strong><br>Team Wan et al., 2025  —— 开源 14B 视频 DiT，提出时空 VAE 分离编码。</li>
<li><strong>CogVideoX</strong><br>Yang et al., 2024  —— 专家 Transformer 与 3D RoPE，强调文本-视频对齐。</li>
<li><strong>Scaling-DiT</strong><br>Fei et al., 2024  —— 将 DiT 推至 16B 参数，验证参数-质量幂律。</li>
</ul>
<p>2. 多模态条件 / in-context 生成</p>
<ul>
<li><strong>Flux.1-Kontext</strong><br>Black Forest Labs, 2025  —— 在潜空间实现 in-context 图像编辑，提出 cross-frame pairing 策略，被 SkyReels-V3 数据管道直接借鉴。</li>
<li><strong>Phantom-Data</strong><br>Chen et al., 2025  —— 构建通用“主体一致”视频数据集，提供跨帧身份保持标签。</li>
<li><strong>SkyReels-A2</strong><br>Fei et al., 2025  —— 同一团队早期工作，首次在 DiT 中实现“任意组合”多主体视频生成，为 V3 的 multi-reference conditioning 奠定架构。</li>
</ul>
<p>3. 专用任务模型</p>
<p>3.1 参考图→视频</p>
<ul>
<li><strong>Vidu Q2、Kling 1.6、PixVerse V5</strong>（商业闭源）—— 表 1 的对比基线，支持 1–2 张参考图生成短视频。</li>
<li><strong>DreamPose、DisCo、AnimateAnyone</strong> —— 早期 UNet 时代方法，仅针对人物姿态或外观，不支持通用多主体。</li>
</ul>
<p>3.2 视频续拍 / 镜头切换</p>
<ul>
<li><strong>Seedance 1.0/1.5 Pro</strong><br>Chen et al., 2025  —— 提出 audio-visual joint training，但仅支持单镜头续拍，无 shot-switching 分类器。</li>
<li><strong>Infinite-length film generation</strong><br>Chen et al., SkyReels-V2, 2025  —— 同一团队前作，首次在 DiT 中引入多段位置编码，但未统一音频与参考图条件。</li>
</ul>
<p>3.3 说话人 / 音频驱动</p>
<ul>
<li><strong>OmniHuman 1.5、KlingAvatar、HunyuanAvatar</strong> —— 表 2 对比基线，均依赖额外 3DMM 或 wav2lip 后处理，长时一致性弱。</li>
<li><strong>EMO、AniPortrait、Hall</strong> —— 基于 UNet 或 2D 关键点，分钟级生成时易出现身份漂移。</li>
</ul>
<p>4. 世界模型与多模态上下文推理</p>
<ul>
<li><strong>LeCun et al., 2022</strong> —— 提出“世界模型”概念，强调视频生成是物理预测的核心任务，为 SkyReels-V3 的动机提供理论支撑。</li>
</ul>
<p>综上，SkyReels-V3 在架构上继承并扩展了扩散 Transformer 系列工作，在条件注入层面吸收了 in-context 图像编辑与 phantom 数据集策略，在任务层面首次将“参考图→视频、视频续拍、说话人”统一至同一参数空间，形成对现有专用模型与单模态框架的系统性整合与性能超越。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>SkyReels-V3 将“多模态上下文推理下的统一视频生成”拆解为<strong>数据-模型-训练-推理</strong>四段式方案，核心手段可概括为“一个框架、两条并行策略、三项专用模块”。</p>
<p>1. 统一框架：Diffusion Transformer + 多模态 In-Context Token Fusion</p>
<ul>
<li>采用<strong>纯 Transformer 扩散骨架</strong>，将时空联合潜码  z_(t) ∈ R^(T × H × W × C)  展平为 1-D token 序列。</li>
<li>设计<strong>异构条件 Tokenizer</strong>：</li>
<li>参考图 → VAE 编码 → 与视频 latent 拼接</li>
<li>文本 → T5-XL 编码 → Cross-Attention 注入</li>
<li>音频 → wav2vec 2.0 提取 phoneme 动态 → 与唇形区域 mask 做局部 attention</li>
<li>所有条件统一视为 in-context token，<strong>无需额外控制网络（ControlNet、Adapter）</strong>，保证单套参数同时服务三种任务。</li>
</ul>
<p>2. 数据侧：两条并行策略抑制 copy-paste 与身份漂移</p>
<p>2.1 Reference-Preserving 数据管道（针对图生视频）</p>
<ol>
<li>交叉帧配对：在原始高清视频中按  t, t+k, t+2k  抽取三帧，保证姿态多样性 yet 语义一致。</li>
<li>图像编辑+背景补全：用 Flux.1-Kontext 把主体抠出并随机替换背景，消除“像素级复制”。</li>
<li>语义重写：LLM 对原始字幕做实体-关系-场景改写，避免文本与帧像素过度对齐。</li>
<li>多轮过滤：CLIP-score+光流一致性+人脸 ID 阈值三轮清洗，剔除编辑伪影。</li>
</ol>
<p>2.2 Shot-Switching 数据构造（针对视频续拍）</p>
<ul>
<li>训练一个<strong>镜头切换检测器</strong>，把长视频切成单镜头、cut-in、cut-out、shot/reverse-shot 等五类片段，并记录切换帧索引。</li>
<li>为每类切换生成“文本-视频-续写”三元组，确保模型学会<strong>专业 cinematographic pattern</strong> 而非简单拼接。</li>
</ul>
<p>3. 模型侧：三项专用模块提升长时一致性与音画同步</p>
<p>3.1 Multi-Reference Conditioning</p>
<ul>
<li>支持 1–4 张异构参考图（人、物、景），每张独立 VAE 编码后<strong>沿通道维拼接</strong>至视频 latent，再送入 Transformer。</li>
<li>引入<strong>可学习的 reference-query token</strong>，让交叉注意力动态决定“哪帧参考用于哪段时空区域”，实现细粒度组合。</li>
</ul>
<p>3.2 Hierarchical Spatio-Temporal Positional Encoding</p>
<ul>
<li>空间 RoPE + 时间 sinusoid + 片段级 segment ID 三层位置码，使模型在续拍任务中<strong>同时感知绝对时间、相对时间、镜头段编号</strong>，保证多段拼接时运动曲线连续。</li>
</ul>
<p>3.3 Key-Frame-Constrained Audio-Visual Alignment（说话人模块）</p>
<ul>
<li>训练阶段随机 mask 嘴部区域，强制音频特征通过<strong>局部交叉注意力</strong>预测唇形动态，损失函数采用 L2+感知+同步三重约束。</li>
<li>长视频生成时先采样<strong>首帧 &amp; 末帧</strong>作为结构锚点，中间帧用 diffusion 插值，<strong>误差不会随时间累积</strong>，实现分钟级一致。</li>
</ul>
<p>4. 训练与推理：混合分辨率 + 图像-视频联合优化</p>
<ul>
<li>图像-视频混合：每批 50 % 单帧图像（提高外观保真）、50 % 视频（学习运动），共用同一套 VAE 与 Transformer。</li>
<li>多分辨率联合：在 512×512、768×432、1280×720 三种尺度下随机采样，<strong>位置编码插值</strong>后可直接外推至任意横纵比，无需额外微调。</li>
<li>推理阶段支持** classifier-free guidance 的多条件解耦**：文本、参考图、音频可独立设置 guidance scale，实现细粒度 trade-off。</li>
</ul>
<p>结果</p>
<ul>
<li>在自建 200 组基准上，<strong>Reference Consistency 0.6698</strong>，超越 Kling 1.6 (0.6630) 与 PixVerse V5 (0.6542)。</li>
<li>说话人任务在分钟级视频里<strong>唇同步误差 1/3 帧以内</strong>，字符身份 cosine 相似度 0.80，与 OmniHuman 1.5 持平。</li>
<li>视频续拍 30 s 片段<strong>镜头切换准确率 92 %</strong>，运动平滑度 FVD 降低 18 % 相对 Seedance 1.5。</li>
</ul>
<p>通过“统一 Transformer 骨架 + 针对性数据-模块-训练三板斧”，SkyReels-V3 把原本需要三套独立模型的功能压缩到同一参数空间，同时缓解身份漂移、copy-paste、音画不同步等顽疾，实现多模态上下文条件下的高保真、长时一致、电影级视频生成。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文围绕三条主线任务共设计 <strong>4 组定量基准 + 6 组定性可视化</strong>，覆盖参考图→视频、视频续拍/转场、说话人三大能力，并补充消融实验验证关键模块贡献。所有实验均在 720p/24 fps 统一设置下完成，以下按任务拆分。</p>
<p>1 参考图→视频（I2V）</p>
<p>1.1 定量对比</p>
<ul>
<li><strong>测试集</strong>：自建 200 条三元组（参考图+文本+真值视频），场景横跨影视/电商/广告；参考主体含人、动物、物体、背景四类。</li>
<li><strong>指标</strong>：</li>
<li>Reference Consistency（人脸/服饰/物体/背景 ID 一致性）</li>
<li>Instruction Following（CLIP-文本相似度）</li>
<li>Visual Quality（美学+清晰度+运动平滑度）</li>
<li><strong>基线</strong>：Vidu Q2、Kling 1.6、PixVerse V5（闭源 API 调用）。</li>
<li><strong>结果</strong>：表 1 显示 SkyReels-V3 在 <strong>Reference Consistency 0.6698</strong> 取得 SOTA，Visual Quality 0.8119 亦最高，Instruction Following 略低 2 分但仍在误差带内。</li>
</ul>
<p>1.2 消融实验</p>
<ul>
<li><strong>w/o 图像-视频混合训练</strong>：Reference Consistency ↓ 5.3 %</li>
<li><strong>w/o 多分辨率联合</strong>：跨分辨率泛化 FVD ↑ 12 %</li>
<li><strong>w/o 语义重写数据</strong>：copy-paste 人工评测比例 ↑ 8 %</li>
</ul>
<p>1.3 定性展示</p>
<ul>
<li>图 1–4：多参考人物与动物交互、直播带货、单图广告等 20+ 例，验证<strong>复杂主体组合与叙事连贯性</strong>。</li>
</ul>
<p>2 视频续拍 / 转场（V2V）</p>
<p>2.1 单镜头续拍</p>
<ul>
<li><strong>测试集</strong>：收集 80 段 5–10 s 首段，人工撰写 3 句续写文本 → 共 240 条。</li>
<li><strong>指标</strong>：FVD、Flow-SSIM（运动一致性）、人类偏好双盲投票。</li>
<li><strong>结果</strong>：SkyReels-V3 FVD 降 18 %，人类偏好率 61 %（vs Seedance 1.5）。</li>
</ul>
<p>2.2 镜头切换</p>
<ul>
<li><strong>测试集</strong>：对 5 类转场（cut-in/cut-out/multi-angle/shot-reverse-shot/cut-away）各采 30 段，共 150 段。</li>
<li><strong>指标</strong>：转场类型分类准确率（人工标注 vs 模型生成）、视觉连贯性 LPIPS。</li>
<li><strong>结果</strong>：<strong>转场类型准确率 92 %</strong>；LPIPS 低于 0.035，显著优于无 shot-aware 编码的 baseline。</li>
</ul>
<p>2.3 定性展示</p>
<ul>
<li>图 5–10：给出单镜头运动延续与 5 种电影级切换示例，验证<strong>专业 cinematographic pattern</strong> 可泛化到户外、厨房、动画等多场景。</li>
</ul>
<p>3 说话人（Audio-driven Avatar）</p>
<p>3.1 定量对比</p>
<ul>
<li><strong>测试集</strong>：自建 100 段 30 s 音频（中英混合，含唱歌、快口、情绪强调），配对单张肖像。</li>
<li><strong>指标</strong>：</li>
<li>Audio-Visual Sync（专家打分 1–10）</li>
<li>Visual Quality（清晰度、闪烁、伪影）</li>
<li>Character Consistency（人脸 cosine 相似度）</li>
<li><strong>基线</strong>：OmniHuman 1.5、KlingAvatar、HunyuanAvatar。</li>
<li><strong>结果</strong>：表 2 显示 SkyReels-V3 <strong>Sync 8.18、Quality 4.60、Consistency 0.80</strong>，与 OmniHuman 打平，显著领先开源 HunyuanAvatar。</li>
</ul>
<p>3.2 长视频与时间一致性</p>
<ul>
<li><strong>设置</strong>：同一张肖像 + 120 s 新闻音频，一次性生成。</li>
<li><strong>指标</strong>：每 10 s 抽帧，与首帧 ID 相似度；人工评测漂移/闪烁。</li>
<li><strong>结果</strong>：120 s 内 ID 相似度下降 &lt; 0.03，无肉眼闪烁（图 13）。</li>
</ul>
<p>3.3 多人对话</p>
<ul>
<li><strong>设置</strong>：给两人合影 mask 指定说话人，交替音频 40 s。</li>
<li><strong>指标</strong>：说话-静默切换准确率、唇形同步误差。</li>
<li><strong>结果</strong>：<strong>角色状态切换准确率 94 %</strong>，唇延误差 1/3 帧（图 12）。</li>
</ul>
<p>3.4 风格泛化</p>
<ul>
<li>图 11：卡通、动物、油画风格 10 例，验证<strong>非真人形象</strong>也能同步。</li>
</ul>
<p>4 运行效率与可扩展性</p>
<ul>
<li><strong>推理速度</strong>：A100-80 GB 上 720p/24 fps/5 s 视频</li>
<li>参考图→视频：∼18 s</li>
<li>视频续拍：∼16 s</li>
<li>说话人：∼14 s</li>
<li><strong>显存占用</strong>：峰值 58 GB，支持 batch=2 并行生成。</li>
<li><strong>多分辨率</strong>：同一套权重无需微调即可输出 1:1、3:4、4:3、16:9、9:16 五种比例，边缘无裁剪伪影。</li>
</ul>
<p>总结</p>
<p>实验从“一致性、质量、遵循度、效率”四维度系统评估，<strong>定量对比覆盖当前主流商业与开源模型，消融验证关键模块，定性展示跨越人、动物、卡通、多镜头、多语言等复杂场景</strong>，充分支撑 SkyReels-V3 达到“单模型三任务 SOTA”的结论。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可被视为 SkyReels-V3 的“下一步阶梯”，按<strong>数据-模型-系统-应用</strong>四层展开，并给出可验证的开放问题。</p>
<p>1 数据层：更精细的世界知识</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>可验证问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>1.1 物理定律-视频对齐数据集</td>
<td>能否自动标注“碰撞/形变/流体/刚体运动”参数，并构建≥10 k 片段的“物理正确性”测试集，使生成视频的动量误差 &lt; 5 %？</td>
</tr>
<tr>
<td>1.2 多语言-多口音音频-视觉对齐</td>
<td>当前仅中英，扩展至 30 种语言时，唇形同步误差（同步帧数/总帧数）能否仍保持 &lt; 3 %？</td>
</tr>
<tr>
<td>1.3 长程因果链视频对</td>
<td>能否从电视剧脚本自动提取“角色目标-动作-结果”三元组，构建≥100 k 的“长程因果”配对，以提升 60 s+ 叙事逻辑？</td>
</tr>
</tbody>
</table>
</div>
<p>2 模型层：更强的上下文与世界模型能力</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>可验证问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>2.1 任意长视频递归生成</td>
<td>把 120 s 拆成 6 段 20 s 递归 conditioning，FVD 相对单次生成是否可再降 10 %？</td>
</tr>
<tr>
<td>2.2 可编辑世界状态向量</td>
<td>在 DiT 潜空间显式引入物理状态 token（位置/速度/质量），允许用户“拖动进度条”修改物体轨迹，编辑后 10 s 内物理误差能否 &lt; 5 %？</td>
</tr>
<tr>
<td>2.3 多智能体社会行为建模</td>
<td>当场景内 ≥4 人且存在对话、遮挡、目光交流时，引入社交注意力模块，能否让“目光方向误差”从 28° 降至 &lt; 10°？</td>
</tr>
<tr>
<td>2.4 音频-情感-微表情联合扩散</td>
<td>在现有 phoneme 条件外，加入情感向量 e∈ℝ^4，生成结果在 Emotion-F1 评测上能否提升 ≥ 0.15？</td>
</tr>
</tbody>
</table>
</div>
<p>3 系统层：更快、更省、更可控</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>可验证问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>3.1 蒸馏至 8-bit 权重</td>
<td>将 16B DiT 蒸馏+量化到 ≤8B 8-bit，推理显存能否 ≤24 GB，且视觉质量（FID）下降 &lt; 5 %？</td>
</tr>
<tr>
<td>3.2 流式生成 pipeline</td>
<td>支持“音频流输入-视频流输出”实时说话人，端到端延迟能否 ≤500 ms（RTF0.5）？</td>
</tr>
<tr>
<td>3.3 多 LoRA 插件市场</td>
<td>为不同风格（卡通/胶片/广告）训练独立 LoRA，插件切换时间 &lt; 1 s，风格 FID 相对通用模型再降 15 %？</td>
</tr>
</tbody>
</table>
</div>
<p>4 应用层：走向真实世界闭环</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>可验证问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>4.1 交互式视频编辑 GUI</td>
<td>用户点击-拖拽修改物体轨迹，系统 10 s 内返回新视频，用户满意度能否 ≥ 85 %？</td>
</tr>
<tr>
<td>4.2 游戏过场实时生成</td>
<td>在 Unity/Unreal 内以 ≤100 ms 延迟调用 API，根据玩家选择即时生成 5 s 过场，玩家沉浸度问卷提升 ≥ 20 %？</td>
</tr>
<tr>
<td>4.3 无障碍手语-语音双向翻译</td>
<td>输入手语视频→输出同步语音+唇形，或反向，BLEU 和手语关键点误差能否同时达到实用门槛？</td>
</tr>
<tr>
<td>4.4 机器人策略蒸馏</td>
<td>把生成视频当作“仿真器”，用强化学习训练真实机械臂，sim-to-real 任务成功率能否比传统仿真高 10 %？</td>
</tr>
</tbody>
</table>
</div>
<p>5 评估层：新基准与安全性</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>方向</th>
<th>可验证问题</th>
</tr>
</thead>
<tbody>
<tr>
<td>5.1 长程身份漂移基准</td>
<td>构建 50 部 5 min 短片，每 10 s 抽帧测 ID，当前 SOTA 漂移 0.08，能否压到 &lt; 0.03？</td>
</tr>
<tr>
<td>5.2 深度伪造检测对抗</td>
<td>生成视频在最新 DeepFake-Detector 下 AUC 能否 ≤ 0.60（即人类难辨），同时配套水印方案使检测 AUC ≥ 0.95？</td>
</tr>
<tr>
<td>5.3 物理合理性基准</td>
<td>引入 Bullet/PyBullet 作为“真值”物理引擎，生成视频的运动轨迹与真值 RMSE 能否 &lt; 0.5 m？</td>
</tr>
</tbody>
</table>
</div>
<p>小结</p>
<p>从“更长、更真、更实时、更可控”四个维度，上述 15 个可验证问题为 SkyReels-V3 提供了清晰的迭代路线；其中 2.2、3.1、4.2 兼具学术与产业价值，适合作为下一阶段主攻方向。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>SkyReels-V3 提出了一套<strong>统一多模态上下文学习框架</strong>，在单一扩散 Transformer 内同时实现三大视频生成任务：参考图→视频、视频续拍/转场、音频驱动说话人。核心贡献与结果可概括为“<strong>一个架构、两套策略、三项模块、四类实验</strong>”。</p>
<p>1. 一个统一架构</p>
<ul>
<li>纯 Transformer 扩散模型，将视频、参考图、文本、音频全部 token 化后做 in-context 拼接，<strong>无需额外控制网络</strong>即可支持多任务。</li>
</ul>
<p>2. 两套数据-训练策略</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>策略</th>
<th>目的</th>
<th>关键做法</th>
</tr>
</thead>
<tbody>
<tr>
<td>参考保持数据管道</td>
<td>抑制 copy-paste、身份漂移</td>
<td>交叉帧配对→图像编辑换背景→语义重写→多轮过滤</td>
</tr>
<tr>
<td>图像-视频混合 + 多分辨率联合训练</td>
<td>提升泛化与鲁棒性</td>
<td>50 % 静态图 + 50 % 视频，同步训练 512²→1280×720 多尺度</td>
</tr>
</tbody>
</table>
</div>
<p>3. 三项专用模块</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>模块</th>
<th>功能</th>
<th>效果</th>
</tr>
</thead>
<tbody>
<tr>
<td>Multi-Reference Conditioning</td>
<td>1–4 张异构图同时注入</td>
<td>人物/物体/背景一致性行业领先</td>
</tr>
<tr>
<td>Hierarchical Spatio-Temporal Positional Encoding</td>
<td>支持单镜头续拍 &amp; 5 种电影级转场</td>
<td>转场类型准确率 92 %</td>
</tr>
<tr>
<td>Key-Frame-Constrained Audio-Visual Alignment</td>
<td>分钟级说话人唇形同步</td>
<td>120 s 视频 ID 漂移 &lt; 0.03，同步误差 1/3 帧</td>
</tr>
</tbody>
</table>
</div>
<p>4. 四类实验结果</p>
<ul>
<li><strong>参考图→视频</strong>：Reference Consistency 0.6698，超越 Kling 1.6、PixVerse V5。</li>
<li><strong>视频续拍/转场</strong>：单镜头 FVD 降 18 %；5 类电影转场 LPIPS &lt; 0.035。</li>
<li><strong>说话人</strong>：Sync 8.18、Visual Quality 4.60，与 OmniHuman 打平，长视频 120 s 无闪烁。</li>
<li><strong>效率</strong>：720p/24 fps/5 s 片段 A100 上 ≤18 s，显存 58 GB，支持 5 种横纵比免微调。</li>
</ul>
<p>一句话总结</p>
<p>SkyReels-V3 用<strong>统一 DiT 架构 + 多模态 in-context 融合</strong>，在单一模型内首次实现参考图生视频、专业镜头续拍、分钟级说话人三大任务的行业 SOTA，为开源社区提供了接近闭源商业系统的高保真、长时一致、电影级视频生成基座。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SkyReels-V3%20Technique%20Report，链接是https://arxiv.org/pdf/2601.17323，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17323。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Debang Li,Zhengcong Fei,Tuanhui Li,Yikun Dou,Zheng Chen,Jiangping Yang,Mingyuan Fan,Jingtao Xu,Jiahua Wang,Baoxuan Gu,Mingshan Chang,Yuqiang Xie,Binjie Mao,Youqiang Zhang,Nuo Pang,Hao Zhang,Yuzhe Jin,Zhiheng Xu,Dixuan Lin,Guibin Chen,Yahui Zhou</p>
<p><strong>Categories</strong>: cs.CV</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17323.pdf">https://arxiv.org/pdf/2601.17323.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17323">https://arxiv.org/abs/2601.17323</a></p>
<p><strong>Arxiv ID</strong>: 2601.17323</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17323">https://papers.cool/arxiv/2601.17323</a></p>
<p><strong>Published</strong>: 2026-01-24T06:08:12Z</p>
<p><strong>Updated</strong>: 2026-01-24T06:08:12.000Z</p>
<hr>
<h3 id="19-Least-Loaded-Expert-Parallelism-Load-Balancing-An-Imbalanced-Mixture-of-Experts"><a href="#19-Least-Loaded-Expert-Parallelism-Load-Balancing-An-Imbalanced-Mixture-of-Experts" class="headerlink" title="19. Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts"></a>19. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.17111">Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts</a></h3><p>Mixture-of-Experts (MoE) models are typically pre-trained with explicit load-balancing constraints to ensure statistically balanced expert routing. Despite this, we observe that even well-trained MoE models exhibit significantly imbalanced routing. This behavior is arguably natural-and even desirable - as imbalanced routing allows models to concentrate domain-specific knowledge within a subset of experts. Expert parallelism (EP) is designed to scale MoE models by distributing experts across multiple devices, but with a less-discussed assumption of balanced routing. Under extreme imbalance, EP can funnel a disproportionate number of tokens to a small number of experts, leading to compute- and memory-bound failures on overloaded devices during post-training or inference, where explicit load balancing is often inapplicable. We propose Least-Loaded Expert Parallelism (LLEP), a novel EP algorithm that dynamically reroutes excess tokens and associated expert parameters from overloaded devices to underutilized ones. This ensures that all devices complete their workloads within the minimum collective latency while respecting memory constraints. Across different model scales, LLEP achieves up to 5x speedup and 4x reduction in peak memory usage compared to standard EP. This enables faster and higher-throughput post-training and inference, with ~1.9x faster for gpt-oss-120b. We support our method with extensive theoretical analysis and comprehensive empirical evaluations, including ablation studies. These results illuminate key trade-offs and enable a principled framework for hardware-specific hyper-parameter tuning to achieve optimal performance.</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>混合专家（MoE）模型通常在预训练时会使用显式的负载均衡约束，以确保专家路由在统计上是平衡的。尽管如此，我们观察到即便是训练良好的 MoE 模型，其路由仍然会显著不平衡。这种行为在某种程度上是自然的——甚至是可取的——因为不平衡路由允许模型在部分专家中集中领域特定的知识。专家并行（EP）旨在通过将专家分布到多个设备上来扩展 MoE 模型，但它假设路由是平衡的，而这一假设很少被讨论。在极端不平衡的情况下，EP 可能会将大量 token 导向少数专家，导致训练后或推理过程中设备过载，从而出现计算或内存瓶颈，而此时显式负载均衡往往不可适用。为此，我们提出了最小负载专家并行（LLEP），这是一种新型的 EP 算法，它能够动态地将过载设备的多余 token 及相关专家参数重定向到利用率不足的设备上。这确保了所有设备在满足内存约束的前提下，以最小的整体延迟完成工作负载。在不同规模的模型中，相比标准 EP，LLEP 实现了最高 5 倍的加速和 4 倍的峰值内存使用量下降。这使得训练后处理和推理速度更快、吞吐量更高，例如 gpt-oss-120b 提升约 1.9 倍。我们通过详尽的理论分析和全面的实证评估，包括消融实验，支持了我们的方法。这些结果揭示了关键权衡，并为基于硬件的超参数调优提供了一个原则性框架，以实现最佳性能。</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文针对“专家并行（Expert Parallelism, EP）”在<strong>路由极度不均衡</strong>的 Mixture-of-Experts（MoE）模型上出现的<strong>计算与内存瓶颈</strong>提出系统级解决方案。核心问题可归纳为：</p>
<ul>
<li><strong>现象</strong>：即使预训练阶段施加了负载均衡约束，SOTA MoE 模型在后训练/推理时仍天然呈现<strong>专家级负载倾斜</strong>——少数专家集中了大量 token。</li>
<li><strong>矛盾</strong>：传统 EP 假设各设备负载近似均衡，当路由极度倾斜时，大量 token 被派发到同一设备，导致该设备<br>– 计算延迟飙升（GEMM 排队）<br>– 峰值内存暴涨（激活缓存与专家权重集中）<br>– 甚至 OOM 崩溃，而其余设备空闲。</li>
<li><strong>限制</strong>：后训练/推理阶段<strong>不允许</strong>通过辅助损失或参数更新强行调整路由分布，否则会破坏预训练得到的专家特化。</li>
</ul>
<p>为此，作者提出 <strong>Least-Loaded Expert Parallelism (LLEP)</strong>，在<strong>不改变 MoE 数学定义</strong>的前提下，通过运行时动态把“超载”token 及其对应专家权重<strong>迁移至当前最轻载设备</strong>，实现：</p>
<ol>
<li>所有设备同步完成计算，最小化集体延迟；</li>
<li>单卡峰值内存稳定，避免 OOM；</li>
<li>支持训练与推理的双向精确梯度传递。</li>
</ol>
<p>实验表明，在极端 95% 负载集中于 1 名专家的场景下，LLEP 相较标准 EP 可取得 <strong>5× 加速、4× 峰值内存降低</strong>，并在大模型端到端推理中带来 <strong>1.9× 吞吐提升</strong>。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>论文在背景与实验部分提及了若干与 MoE 负载不均衡、专家并行及系统级优化密切相关的研究，可归类如下：</p>
<ul>
<li><strong>MoE 基础与负载均衡约束</strong></li>
<li>Lepikhin et al. GShard (2020) —— 首个大规模 MoE 分布式训练框架，提出 auxiliary loss 抑制专家崩溃。</li>
<li>Fedus et al. Switch Transformer (2022) —— 简化 top-1 路由并系统研究辅助均衡损失。</li>
<li>Zhou et al. Expert Choice Routing (2022) —— 反向选择机制缓解不均衡，但需修改路由分布。</li>
<li><strong>专家并行与通信优化</strong></li>
<li>Shoeybi et al. Megatron-LM (2019) —— 奠定 EP 的 all-to-all dispatch/combine 范式。</li>
<li>Singh et al. (2023) —— 混合张量-专家-数据并行策略，减少通信量。</li>
<li>Liu et al. DeepEP (2024) / Zheng et al. Triton-Distributed (2025) —— 内核级 fused all-to-all，降低 kernel launch 开销。</li>
<li><strong>推理期负载倾斜与系统级缓解</strong></li>
<li>Liu et al. DeepSeek-V3 EPLB (2024) —— 基于延迟统计的“冗余专家”复制，仅适用于推理且增加显存。</li>
<li>Huang et al. (2024) —— 预留 CPU/GPU 内存缓冲超载专家，仍可能 OOM。</li>
<li>Qiu et al. (2025) / Hu et al. (2025) —— 指出后训练阶段强行加均衡损失会损害专家特化，提倡系统级方案。</li>
<li><strong>专家特化与域自适应</strong></li>
<li>Agarwal et al. gpt-oss (2025)、Team et al. Kimi-K2 (2025) —— 观察到域数据天然触发少数专家高频激活，验证“不均衡即合理”现象。</li>
<li>Song et al. (2025) —— 提出异构专家组进一步放大特化，加剧倾斜。</li>
<li><strong>分组 GEMM 与计算效率</strong></li>
<li>Shazeer GLU Variants (2020) —— SwiGLU 专家结构被本文实验采用。</li>
<li>附录图 8 对比 cuBLAS 与 Triton 分组 GEMM，说明“少而大”GEMM 优于“多而小”，支撑 LLEP 减少专家碎片化的设计动机。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文将“路由极度倾斜下的专家并行失效”视为<strong>运行时资源调度问题</strong>，而非模型级路由修正问题，提出 <strong>Least-Loaded Expert Parallelism (LLEP)</strong>。核心思路是：<strong>在不做任何数学近似、不改动预训练参数的前提下，动态把超载 token 及其对应专家权重从热点 GPU 迁移至当前最轻载 GPU</strong>，使得所有设备同步完成计算且峰值内存可控。具体实现分三步：</p>
<ol>
<li>预处理：全局统计本轮各专家负载<br>每个设备先汇总本卡将要处理的 token 数，通过 all-reduce 得到全局负载向量  l∈R^N 。若</li>
</ol>
<p>max(l),/,mean(l) &lt; λ</p>
<p>认为“足够均衡”，直接回退到标准 EP；否则进入 LLEP 调度。</p>
<ol>
<li>最小负载分配（LLA）<br>按 Alg.2–Alg.3 贪心算法生成两张表：</li>
</ol>
<ul>
<li><strong>Token 分配表 A</strong>：对每一专家  i ，从负载最高到最低依次决定<br>– 原生 GPU 能吞多少就吞多少（上限  m_α=α·(∑_j l_j) / (P) ）；<br>– 剩余 token 按“当前已分配负载 + 待处理负载”之和最小的顺序，切分后 spill 到其他 GPU；<br>– 每块切分不小于  m  个 token（避免过小的 GEMM）。</li>
<li><strong>权重传输表 W</strong>：若 GPU  q  需要替 GPU  p  计算专家  i ，则把  W_i  通过 P2P 传输到  q  的临时缓存。</li>
</ul>
<ol>
<li>执行：dispatch-compute-combine 三阶段<br>Alg.4 给出单卡流程：</li>
</ol>
<ul>
<li><strong>Dispatch</strong>：按 A 表把 token/权重路由到真正负责计算的 GPU（含 foreign）；</li>
<li><strong>Compute</strong>：每卡对自己名下的“原生+foreign”专家批量做 Grouped-GEMM；</li>
<li><strong>Combine</strong>：all-to-all 把输出送回原始卡，再 reduce-sum 得到最终表示。<br>反向传播时，foreign 专家的梯度沿相同路径返回到原生卡累加，保证数学一致性。</li>
</ul>
<p>通过上述<strong>运行时重调度</strong>，LLEP 把“不均衡”转化为“跨设备负载均衡”，同时满足：</p>
<ul>
<li>零近似：与标准 EP 计算图等价；</li>
<li>内存可控：单卡峰值仅随  α  线性增长，不再出现“单卡打满”式 OOM；</li>
<li>通信可接受：仅在超载时触发权重搬运，且优先 intra-node，开销被大 batch 计算掩盖。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>论文从<strong>微观层（单 MoE 层）</strong>到<strong>宏观层（端到端大模型）</strong>再到<strong>超参敏感度</strong>共三类实验，系统验证 LLEP 在速度、内存与收敛性能上的优势。关键结果均基于 8×H200 GPU 集群。</p>
<ol>
<li>单 MoE 层微基准（§5.1）<br>模型：gpt-oss-120b、DeepSeek-V3、Kimi-K2 的 MoE 层（SwiGLU 专家，128–384 专家，top-K=4/8）。<br>变量：</li>
</ol>
<ul>
<li>负载倾斜度：30 %、50 %、80 %、95 % token 集中到 1/4/16 名专家；</li>
<li>对比基线：标准 EP。<br>指标：</li>
<li>相对加速比（LLEP/EP）；</li>
<li>单卡峰值显存。<br>结果：</li>
<li>极端 95 %-1 专家场景下，LLEP 分别取得 <strong>6.11×、5.13×、5.03×</strong> 加速；</li>
<li>峰值内存维持 20–30 GB，而 EP 最高飙至 <strong>100 GB</strong>（Kimi-K2），存在 OOM 风险。</li>
</ul>
<ol>
<li>端到端整模型“实战”测试（§5.2）<br>模型：已发布的 gpt-oss-20b &amp; 120b，8-way EP。<br>数据：Megatron-Math 数学语料（in-domain，天然倾斜）。<br>指标：纯前向吞吐（token/s）。<br>结果：</li>
</ol>
<ul>
<li>20B 模型：<strong>2.2×</strong> 吞吐提升；</li>
<li>120B 模型：<strong>1.9×</strong> 吞吐提升；</li>
<li>随着 GPU 数从 8 增至 32，LLEP 相对优势扩大（图 1c）。</li>
</ul>
<ol>
<li>全参数微调收敛对比（§5.2 末）<br>设置：gpt-oss-20b 在 AIME’25 数学竞赛题上全参数 SFT，ZeRO-3+CPU offload。<br>指标：准确率 vs. 挂钟时间。<br>结果：</li>
</ol>
<ul>
<li>相同精度下，LLEP 训练总时长 <strong>1.25×</strong> 短于 EP（图 5）。</li>
</ul>
<ol>
<li>超参消融（§5.3）</li>
</ol>
<ul>
<li>batch size：32 K→64 K，加速比从 3.5× 提到 6×（图 6a）；</li>
<li>α 因子：α=1→2.5，加速比下降 30 %（图 6b）；</li>
<li>λ 阈值：λ=1.3 在 15 % 倾斜时最优；再大则过度回退到 EP（图 7a）；</li>
<li>隐藏维度：D=4 K→8 K，通信占比下降，加速比从 3× 提到 5×（图 7b）；</li>
<li>专家总数：N=64→256，倾斜 95 %-4 专家场景下加速比单调增至 6×（图 9）。</li>
</ul>
<p>综上，实验覆盖<strong>合成倾斜</strong>与<strong>真实分布</strong>、<strong>推理</strong>与<strong>训练</strong>、<strong>不同规模</strong>与<strong>不同超参</strong>，一致表明 LLEP 在保持计算精度的同时，显著提速并消除 OOM 风险。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可在此基础上继续深入，分为<strong>算法/理论</strong>、<strong>系统/工程</strong>与<strong>模型/应用</strong>三类，均直接源于 LLEP 尚未覆盖或仅初步讨论的问题。</p>
<p>算法与理论</p>
<ol>
<li><p><strong>在线负载预测与 λ 自适应</strong><br>当前 λ 为人工常数；可探索轻量级时序模型（如 Kalman 或指数平滑）根据历史路由序列动态预测下一 batch 的倾斜度，实现 <strong>零人工 λ</strong> 的切换策略。</p>
</li>
<li><p><strong>多目标调度形式化</strong><br>LLA 仅贪心最小化“最大单卡负载”。可引入<strong>通信-计算联合代价模型</strong></p>
</li>
</ol>
<p>min<em>(assignment) max_p T</em>(comp)(p) + β · T_(comm)(p)</p>
<p>并给出近似比或整数规划下界，理论上权衡 α、m、β 的最优组合。</p>
<ol>
<li><p><strong>考虑内存碎片化与权重复用</strong><br>当前 W 表一次性搬运完整权重。可研究<strong>增量/分页式专家权重广播</strong>，结合 GPU 显存池管理，减少 cudaMalloc 峰值。</p>
</li>
<li><p><strong>与专家选择路由（expert-choice）兼容</strong><br>LLEP 假设 token→expert 的 top-K 语义；若改用 expert-choice（token 被专家挑选），负载向量定义与 spill 策略需重新形式化。</p>
</li>
</ol>
<p>系统与工程</p>
<ol>
<li><p><strong>Triton/DeepEP 内核融合</strong><br>将 LLA 产生的索引张量直接嵌入 fused-all-to-all kernel，消除 Python 端 index-select 与临时缓冲区，预计再降 5–10 % 延迟。</p>
</li>
<li><p><strong>多机 NUMA/网络拓扑感知</strong><br>当前仅优先 intra-node spill。可扩展至<strong>机间带宽与 RDMA 拓扑</strong>建模，把机间链路权重纳入 LLAS 排序，进一步减少通信瓶颈。</p>
</li>
<li><p><strong>异步权重预取与计算重叠</strong><br>利用 CUDA stream 在计算当前 batch 的同时，通过 P2P 预取下一 batch 的 foreign 权重，隐藏传输延迟。</p>
</li>
<li><p><strong>FP8/INT8 权重压缩传输</strong><br>搬运专家前做块量化（如 MXFP4），通信量减半，解压后在 SRAM 计算，适合超大规模专家（N≥512）场景。</p>
</li>
</ol>
<p>模型与应用</p>
<ol>
<li><strong>MoE 结构协同设计</strong></li>
</ol>
<ul>
<li><strong>细粒度专家</strong>：将每个大专家拆成若干子专家，LLEP 可在子专家粒度再平衡，实现“二次均衡”而不破坏宏观特化。</li>
<li><strong>共享-特化混合专家</strong>：明确标记共享专家（始终常驻）与特化专家（可迁移），LLA 只 spill 特化部分，减少搬运量。</li>
</ul>
<ol>
<li><p><strong>强化学习/长序列后训练</strong><br>在 RLHF 或 long-context 继续训练场景，路由倾斜更极端（思考链、代码、数学 token 高度集中）。可验证 LLEP 对 <strong>梯度累积步数 &gt; 64 的大型 batch</strong> 是否仍保持线性扩展。</p>
</li>
<li><p><strong>边缘/推理卡部署</strong><br>将 LLEP 移植到 24 GB 消费级 GPU 或 16 GB 边缘卡，研究在<strong>显存硬墙</strong>下 α 与 batch size 的极限折衷，实现“单卡跑 120B-MoE”。</p>
</li>
<li><p><strong>与专家缓存/淘汰策略结合</strong><br>引入“热度”计数器，长期冷专家权重回写 CPU 内存，需要时再 P2P 拉取，形成 <strong>CPU-GPU 层级专家存储</strong>，支持无限专家规模。</p>
</li>
</ol>
<p>评价与基准</p>
<ol>
<li><p><strong>标准化不均衡基准</strong><br>当前实验使用人工倾斜或单域数学数据。可建立<strong>覆盖代码、多语言、多模态</strong>的公开不均衡 workload，推动社区对比。</p>
</li>
<li><p><strong>碳排放与能耗模型</strong><br>测量不同 α、λ 配置下的 GPU 功耗与数据搬动能耗，给出<strong>能效最优帕累托前沿</strong>，指导绿色部署。</p>
</li>
</ol>
<p>通过上述探索，可进一步释放 MoE 在后训练、推理及边缘场景中的扩展潜力，同时保持专家特化带来的模型质量优势。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p>论文提出 <strong>Least-Loaded Expert Parallelism (LLEP)</strong>，解决 MoE 模型在<strong>后训练/推理阶段因天然路由不均衡</strong>导致的<strong>专家并行延迟高、显存爆涨甚至 OOM</strong> 的问题，核心内容与贡献如下：</p>
<p>1. 问题洞察</p>
<ul>
<li>即使预训练加了负载均衡损失，SOTA MoE（gpt-oss、DeepSeek-V3 等）仍呈现<strong>少数专家集中大量 token</strong> 的“合理倾斜”。</li>
<li>标准 Expert Parallelism (EP) 假设各 GPU 负载均衡；极端倾斜时，<strong>单卡计算+显存双瓶颈</strong>，降低吞吐且易 OOM。</li>
<li>后训练/推理<strong>禁止</strong>用辅助损失或参数更新强行改路由，需<strong>系统级</strong>、<strong>零近似</strong>的调度方案。</li>
</ul>
<p>2. 方法概览</p>
<p>LLEP 把“不均衡”视为<strong>运行时资源调度</strong>：</p>
<ol>
<li>每 batch 先统计全局专家负载  l ；若  max(l)/mean(l)≥λ  则触发调度。</li>
<li><strong>Least-Loaded Assignment (LLA)</strong> 贪心生成：</li>
</ol>
<ul>
<li>Token 分配表 A——超载专家剩余 token 切分后送往<strong>当前最轻载 GPU</strong>；</li>
<li>权重传输表 W——foreign 专家权重通过 P2P 搬运至目标卡。</li>
</ul>
<ol>
<li>执行标准 dispatch→compute→combine，支持<strong>双向梯度</strong>；否则回退原生 EP。</li>
</ol>
<p>3. 理论分析</p>
<ul>
<li>计算延迟模型： T<em>(local)=∑_i (T</em>(overhead)+B<em>i·T</em>(B_i,D,H))<br>少量大 GEMM 远优于大量小 GEMM；倾斜时 EP 的  B_i  可接近全局 batch，造成单卡延迟飙升。</li>
<li>内存模型： M_(local)=∑_i (B_i D + D H + B_i H)<br>极端情况下单卡需存全量激活与权重，内存随倾斜线性增长。</li>
</ul>
<p>4. 实验结果</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>场景</th>
<th>加速比</th>
<th>峰值内存</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>单层 95 %→1 专家</td>
<td>6.1×</td>
<td>5× 降低</td>
<td>无 OOM</td>
</tr>
<tr>
<td>gpt-oss-20b 端到端推理</td>
<td>2.2× 吞吐</td>
<td>稳定</td>
<td>真实数学数据</td>
</tr>
<tr>
<td>gpt-oss-120b 端到端推理</td>
<td>1.9× 吞吐</td>
<td>稳定</td>
<td>32 GPU 扩展</td>
</tr>
<tr>
<td>AIME’25 全参微调</td>
<td>1.25× 收敛时间</td>
<td>相同精度</td>
<td>ZeRO-3+CPU offload</td>
</tr>
</tbody>
</table>
</div>
<p>5. 结论</p>
<p>LLEP 在<strong>不改变 MoE 数学定义</strong>的前提下，通过<strong>动态 token+权重迁移</strong>实现：</p>
<ul>
<li>所有 GPU 同步完成计算，最小化集体延迟；</li>
<li>单卡峰值内存平稳，避免 OOM；</li>
<li>支持训练与推理，已开源（github.com/SalesforceAIResearch/LeastLoadedEP）。</li>
</ul>
<p>结果刷新 MoE 后训练与推理的<strong>吞吐与显存</strong>指标，为“专家特化”与“系统效率”兼得提供了可落地的范式。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Least-Loaded%20Expert%20Parallelism%3A%20Load%20Balancing%20An%20Imbalanced%20Mixture-of-Experts，链接是https://arxiv.org/pdf/2601.17111，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.17111。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Xuan-Phi Nguyen,Shrey Pandit,Austin Xu,Caiming Xiong,Shafiq Joty</p>
<p><strong>Categories</strong>: cs.LG,cs.AI</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.17111.pdf">https://arxiv.org/pdf/2601.17111.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.17111">https://arxiv.org/abs/2601.17111</a></p>
<p><strong>Arxiv ID</strong>: 2601.17111</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.17111">https://papers.cool/arxiv/2601.17111</a></p>
<p><strong>Published</strong>: 2026-01-23T18:19:15Z</p>
<p><strong>Updated</strong>: 2026-01-23T18:19:15.000Z</p>
<hr>
<h3 id="20-IVRA-Improving-Visual-Token-Relations-for-Robot-Action-Policy-with-Training-Free-Hint-Based-Guidance"><a href="#20-IVRA-Improving-Visual-Token-Relations-for-Robot-Action-Policy-with-Training-Free-Hint-Based-Guidance" class="headerlink" title="20. IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance"></a>20. <a target="_blank" rel="noopener" href="https://huggingface.co/papers/2601.16207">IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</a></h3><p>Many Vision-Language-Action (VLA) models flatten image patches into a 1D token sequence, weakening the 2D spatial cues needed for precise manipulation. We introduce IVRA, a lightweight, training-free method that improves spatial understanding by exploiting affinity hints already available in the model’s built-in vision encoder, without requiring any external encoder or retraining. IVRA selectively injects these affinity signals into a language-model layer in which instance-level features reside. This inference-time intervention realigns visual-token interactions and better preserves geometric structure while keeping all model parameters fixed. We demonstrate the generality of IVRA by applying it to diverse VLA architectures (LLaRA, OpenVLA, and FLOWER) across simulated benchmarks spanning both 2D and 3D manipulation (VIMA and LIBERO) and on various real-robot tasks. On 2D VIMA, IVRA improves average success by +4.2% over the baseline LLaRA in a low-data regime. On 3D LIBERO, it yields consistent gains over the OpenVLA and FLOWER baselines, including improvements when baseline accuracy is near saturation (96.3% to 97.1%). All code and models will be released publicly. Visualizations are available at: jongwoopark7978.github.io/IVRA</p>
<details class="toggle" ><summary class="toggle-button" style="">中文摘要</summary><div class="toggle-content"><p>许多视觉-语言-动作（VLA）模型将图像块展平成一维的 token 序列，从而削弱了精确操作所需的二维空间线索。我们提出了 IVRA，这是一种轻量级、无需训练的方法，通过利用模型内置视觉编码器中已有的亲和性提示来提升空间理解，无需任何外部编码器或重新训练。IVRA 有选择地将这些亲和性信号注入包含实例级特征的语言模型层。该推理时干预重新对齐了视觉 token 的交互，并在保持所有模型参数固定的情况下更好地保留了几何结构。我们通过将 IVRA 应用于多种 VLA 架构（LLaRA、OpenVLA 和 FLOWER），并跨越模拟基准的二维和三维操作（VIMA 和 LIBERO）以及各种真实机器人任务，展示了 IVRA 的通用性。在二维 VIMA 上，IVRA 在低数据环境下相较于基础 LLaRA 平均成功率提升了 +4.2%。在三维 LIBERO 上，它在 OpenVLA 和 FLOWER 基线之上持续带来提升，包括在基线精度接近饱和（96.3% 提升至 97.1%）时的改进。所有代码和模型将公开发布。可视化内容可见于：jongwoopark7978.github.io/IVRA</p>
</div></details>
<details class="toggle" ><summary class="toggle-button" style="">LLM Analysis</summary><div class="toggle-content"><p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 这篇论文试图解决什么问题？
</p>

<p>论文旨在解决 Vision-Language-Action（VLA）模型在将二维图像展平为一维 token 序列后，<strong>空间结构信息被削弱</strong>、导致<strong>实例级特征模糊</strong>、进而影响精细操作（如抓取、放置）的问题。核心痛点包括：</p>
<ul>
<li>展平视觉块后，<strong>局部邻域关系丢失</strong>，对象边界与属性关联（颜色、形状等）难以保持；</li>
<li>现有补救方案通常需<strong>重训练或额外数据</strong>，代价高。</li>
</ul>
<p>IVRA 提出一种<strong>无需训练、即插即用</strong>的推断阶段干预：利用模型<strong>自带视觉编码器</strong>产生的亲和度（affinity）提示，在语言模型中层对视觉 token 重新加权，恢复二维空间结构，从而提升实例识别与定位精度。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有哪些相关研究？
</p>

<p>相关研究可归纳为三条主线，均围绕“如何在视觉-语言-动作（VLA） pipeline 中保留或恢复空间结构”展开：</p>
<ol>
<li>VLA 架构与空间理解</li>
</ol>
<ul>
<li>典型工作将 CLIP/DINO 的 2D patch 直接展平为 1D token 接入 LLM（LLaRA、OpenVLA、FLOWER 等），导致局部几何关系丢失。</li>
<li>缓解方案包括：<br>– 多层级特征融合（DINO 早期层、CLIP 中间层）<br>6,15</li>
</ul>
<p>– 开放词汇检测器嫁接（RegionCLIP、RegionGPT、Grounding DINO）<br>16–19<br>，但需大规模区域标注再训练。</p>
<ol>
<li>亲和度/局部结构注入</li>
</ol>
<ul>
<li>CLIP-DIY、Hints of Prompt、ViP-LLaVA 等利用 patch 自相关或语言提示生成 affinity map，再训练额外适配器或解码器来增强实例特征<br>20–25<br>。</li>
<li>CLIPort、PaLM-E 在机器人领域将“语义-位置”双路显式并联，同样依赖任务相关微调<br>26,27<br>。</li>
</ul>
<ol>
<li>免训练结构恢复</li>
</ol>
<ul>
<li>与 IVRA 最贴近的是 CLIP-DIY<br>21<br>，它在推断阶段用 vision encoder 的 affinity 重加权特征，但仅用于开放词汇分割，且未嵌入 LLM 内部。</li>
<li>IVRA 首次<strong>把 encoder 自带的 affinity hint 直接注入 LLM 中层自注意力之前</strong>，无需任何外部模块或梯度更新，实现了在 2D/3D 机器人任务上的即插即用提升。</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文如何解决这个问题？
</p>

<p>论文提出 IVRA，一种<strong>零参数、零训练、纯推理阶段</strong>的干预方法，把被展平而丢失的 2D 邻域结构重新注入 LLM。具体流程如下：</p>
<ol>
<li>提取亲和度<br>在冻结的视觉编码器中间层取出 patch 特征  f_i ，逐对计算余弦相似度</li>
</ol>
<p>A_(ij)= (f_i· f_j) / (|f_i||f_j|)</p>
<p>得到亲和矩阵  A∈R^(N× N) ，作为“哪些 patch 属于同一物体”的先验。</p>
<ol>
<li>亲和引导的 token 重加权<br>在 LLM 的指定层（如第 20 层）self-attention 之前，仅对视觉 token 序列做加权平均：</li>
</ol>
<p>v’<em>i = ∑</em>(j=1)^N α<em>(ij),v_j,quad α</em>(ij)= max(A<em>(ij),0)∑</em>(k=1)^N max(A_(ik),0)</p>
<p>高相关 patch 互相强化，从而恢复局部空间一致性。</p>
<ol>
<li>线性混合保持语义<br>为避免过度平滑，将原 token 与亲和池化结果凸组合：</li>
</ol>
<p>v_i^(mix)= (1-λ),v_i + λ,v’_i,quad λ=0.3</p>
<p>随后  v_i^(mix)  继续参与该层的 LN、SA、FFN 等计算，文本 token 完全不变。</p>
<ol>
<li>即插即用部署<br>上述三步在推理前向中一次性完成，无需更新任何权重，也不引入额外模块；可在 LLaRA、OpenVLA、FLOWER 等多种 VLA 上直接启用。</li>
</ol>
<p>通过把 vision encoder 自带的 affinity hint 注入 LLM 中层，IVRA 在 2D/3D 仿真基准和真实机器人抓取/放置任务上均取得一致且显著的成功率提升。</p>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 论文做了哪些实验？
</p>

<p>实验覆盖 2D 仿真、3D 仿真与零样本真机三大场景，均用“即插即用”方式把 IVRA 注入现有 VLA，<strong>无需任何再训练或额外数据</strong>。</p>
<ol>
<li>2D 操作：VIMA 基准</li>
</ol>
<ul>
<li>任务：Novel Task / Novel Object / Object Combination / Object Place</li>
<li>骨干：LLaRA（仅 12% 数据）</li>
<li>结果：<br>– 无 oracle 检测器，平均成功率 +4.2%（53.9→58.1）<br>– 有 oracle 检测器，仍全面超越用 100% 数据训练的原始 VIMA</li>
</ul>
<ol>
<li>3D 操作：LIBERO 套件</li>
</ol>
<ul>
<li>对比基线：OpenVLA、FLOWER（已处 94-99% 饱和区）</li>
<li>结果：<br>– OpenVLA+IVRA 再涨 +1.1%（76.5→77.6），高于 Diffusion Policy、Octo<br>– FLOWER+IVRA 从 96.3% 提到 97.1%，在 Task-90 等子集最高再升 2.6%</li>
</ul>
<ol>
<li>真机零样本抓取-放置</li>
</ol>
<ul>
<li>机器人：xArm7+夹爪，顶部固定 RGB 相机</li>
<li>训练数据：纯合成 inBC-8k（仅占 VIMA 1.2%）</li>
<li>4 项任务：<br>– T1 指定物体抓取<br>– T2 按颜色匹配抓取<br>– T3 密集干扰下定位<br>– T4 相对高度比较</li>
<li>结果：<br>– LLaRA+IVRA 相对基线提升 +10%~+30%，最难的 T2/T3 暴涨 30%</li>
</ul>
<ol>
<li>消融与复杂度</li>
</ol>
<ul>
<li>单层注入即可，层数↑无额外收益；最佳位置为 Transformer 块入口（P0）</li>
<li>λ=0.3 平衡语义与空间结构；λ=1 过度平滑</li>
<li>运行时仅增 3% 延迟，<strong>零新增参数</strong></li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 有什么可以进一步探索的点？
</p>

<p>以下方向可进一步挖掘，均围绕“<strong>不训练、轻量级、强空间</strong>”这一核心范式展开：</p>
<ul>
<li><p><strong>层级自适应注入</strong><br>当前固定在第 20 层注入。可探索<strong>动态层选择</strong>：让 affinity 熵或梯度幅值自动决定在哪一层（或哪几层）施加干预，以适配不同任务对“语义 vs 几何”的不同需求。</p>
</li>
<li><p><strong>时间一致性扩展</strong><br>现有 affinity 为单帧计算。在<strong>长时序任务</strong>（LIBERO-Long、叠衣服）中，可引入<strong>时序平滑 affinity</strong>（如 EMA 或光流约束），减少闪烁误抓。</p>
</li>
<li><p><strong>跨模态 affinity</strong><br>仅利用视觉自相似度。可研究<strong>文本-视觉共现 affinity</strong>（例如把指令中的“红”与图像红色区域对齐），实现<strong>语言驱动的区域提示</strong>，进一步提升开放指令性能。</p>
</li>
<li><p><strong>3D 几何 affinity</strong><br>目前亲和度在 2D patch 空间计算。若接入深度或点云，可构建<strong>3D 邻域 affinity</strong>，直接恢复体积结构，改善遮挡、堆叠场景下的抓取顺序。</p>
</li>
<li><p><strong>动作空间回传</strong><br>IVRA 仅修正表征。可尝试将<strong>执行失败信号</strong>（如打滑、碰撞）作为在线监督，<strong>闭环微调 λ 或亲和阈值</strong>，实现无梯度“推理即学习”。</p>
</li>
<li><p><strong>压缩与芯片级部署</strong><br>亲和矩阵  N×N  对高分辨率仍占内存。可探索<strong>低秩分解</strong>、<strong>稀疏化</strong>或<strong>窗口局部 affinity</strong>，在边缘 GPU/FPGA 上保持零参数优势。</p>
</li>
<li><p><strong>泛化到非操作任务</strong><br>目前聚焦机器人操控。可验证 IVRA 在<strong>导航</strong>（VLN）、<strong>视觉问答</strong>（VQA）或<strong>自动驾驶</strong>等需要细粒度空间推理的任务中是否同样有效。</p>
</li>
<li><p><strong>理论解释性</strong><br>从<strong>信息几何</strong>角度量化 IVRA 如何改变 LLM 内部表示的<strong>互信息</strong>或<strong>聚类系数</strong>，建立“affinity 注入 ↔ 空间结构恢复 ↔ 动作准确率”之间的可证明界限。</p>
</li>
</ul>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 总结一下论文的主要内容
</p>

<p><strong>IVRA 核心内容速览</strong></p>
<ol>
<li><p>问题<br>Vision-Language-Action 模型把 2D patch 展平为 1D token，导致对象边界、属性关系等空间线索被稀释，精细操作性能下降。</p>
</li>
<li><p>方法</p>
</li>
</ol>
<ul>
<li><strong>零训练、零参数</strong>：推理阶段从冻结视觉编码器提取 patch 亲和度矩阵  A 。</li>
<li><strong>单层干预</strong>：在 LLM 第 20 层 self-attention 前，用  A  对视觉 token 做亲和加权平均，再与原 token 凸组合（λ=0.3）。</li>
<li><strong>文本 token 不动</strong>，即插即用于 LLaRA、OpenVLA、FLOWER 等多种架构。</li>
</ul>
<ol>
<li>实验</li>
</ol>
<ul>
<li><strong>2D 仿真 VIMA</strong>：12% 数据下平均成功率 +4.2%，击败用 100% 数据训练的 VIMA。</li>
<li><strong>3D 仿真 LIBERO</strong>：OpenVLA +1.1%，FLOWER 饱和区再提 0.8%，均刷新 SOTA。</li>
<li><strong>真机零样本</strong>：纯合成 inBC-8k 模型上，抓取/颜色/遮挡/高度四类任务最高提升 30%。</li>
<li><strong>开销</strong>：延迟仅 +3%，零新增参数。</li>
</ul>
<ol>
<li>结论<br>IVRA 用模型自身编码器产生的 affinity hint，在推理阶段即时“修复”被展平丢失的 2D 结构，可泛化到 2D/3D、多架构、真机场景，为 VLA 提供一条轻量化、免训练的空间增强新路径。</li>
</ol>
<p style="background-color: rgba(135, 206, 235, 0.3);border-radius: 0.4rem;padding: 10px;margin: 10px 0;margin-left: -10px;font-weight: bold;">
Q: 想要进一步了解论文
</p>

<p>以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 <a target="_blank" rel="noopener" href="http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是IVRA%3A%20Improving%20Visual-Token%20Relations%20for%20Robot%20Action%20Policy%20with%20Training-Free%20Hint-Based%20Guidance，链接是https://arxiv.org/pdf/2601.16207，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.16207。请以此为基础，继续回答我后面的问题。&amp;system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&amp;send_immediately=true&amp;force_search=false&amp;enable_reasoning=false"><strong>这里</strong></a> 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。</p>
</div></details>
<p><strong>Authors</strong>: Jongwoo Park,Kanchana Ranasinghe,Jinhyeok Jang,Cristina Mata,Yoo Sung Jang,Michael S Ryoo</p>
<p><strong>Categories</strong>: cs.RO</p>
<p><strong>PDF URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2601.16207.pdf">https://arxiv.org/pdf/2601.16207.pdf</a></p>
<p><strong>Arxiv URL</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2601.16207">https://arxiv.org/abs/2601.16207</a></p>
<p><strong>Arxiv ID</strong>: 2601.16207</p>
<p><strong>CoolPaper URL</strong>: <a target="_blank" rel="noopener" href="https://papers.cool/arxiv/2601.16207">https://papers.cool/arxiv/2601.16207</a></p>
<p><strong>Published</strong>: 2026-01-22T18:57:13Z</p>
<p><strong>Updated</strong>: 2026-01-22T18:57:13.000Z</p>
<hr>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/daily/hf/202601/2026-01-28/">https://wdndev.github.io/daily/hf/202601/2026-01-28/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/AI/">AI</a><a class="post-meta__tags" href="/tags/HuggingFace/">HuggingFace</a><a class="post-meta__tags" href="/tags/Papers/">Papers</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/daily/hf/202507/2025-07-15/" title="HuggingFace Papers 2025-07-15"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-15</div></div></a></div><div><a href="/daily/hf/202507/2025-07-16/" title="HuggingFace Papers 2025-07-16"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-16</div></div></a></div><div><a href="/daily/hf/202507/2025-07-14/" title="HuggingFace Papers 2025-07-14"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2019-06-18</div><div class="title">HuggingFace Papers 2025-07-14</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Latest-Papers"><span class="toc-text">Latest Papers</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Can-LLMs-Clean-Up-Your-Mess-A-Survey-of-Application-Ready-Data-Preparation-with-LLMs"><span class="toc-text">1. Can LLMs Clean Up Your Mess? A Survey of Application-Ready Data Preparation with LLMs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-daVinci-Dev-Agent-native-Mid-training-for-Software-Engineering"><span class="toc-text">2. daVinci-Dev: Agent-native Mid-training for Software Engineering</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-The-Script-is-All-You-Need-An-Agentic-Framework-for-Long-Horizon-Dialogue-to-Cinematic-Video-Generation"><span class="toc-text">3. The Script is All You Need: An Agentic Framework for Long-Horizon Dialogue-to-Cinematic Video Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Scientific-Image-Synthesis-Benchmarking-Methodologies-and-Downstream-Utility"><span class="toc-text">4. Scientific Image Synthesis: Benchmarking, Methodologies, and Downstream Utility</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-Elastic-Attention-Test-time-Adaptive-Sparsity-Ratios-for-Efficient-Transformers"><span class="toc-text">5. Elastic Attention: Test-time Adaptive Sparsity Ratios for Efficient Transformers</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-iFSQ-Improving-FSQ-for-Image-Generation-with-1-Line-of-Code"><span class="toc-text">6. iFSQ: Improving FSQ for Image Generation with 1 Line of Code</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-Teaching-Models-to-Teach-Themselves-Reasoning-at-the-Edge-of-Learnability"><span class="toc-text">7. Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#8-Self-Refining-Video-Sampling"><span class="toc-text">8. Self-Refining Video Sampling</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#9-VIBEVOICE-ASR-Technical-Report"><span class="toc-text">9. VIBEVOICE-ASR Technical Report</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#10-CGPT-Cluster-Guided-Partial-Tables-with-LLM-Generated-Supervision-for-Table-Retrieval"><span class="toc-text">10. CGPT: Cluster-Guided Partial Tables with LLM-Generated Supervision for Table Retrieval</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#11-DeepPlanning-Benchmarking-Long-Horizon-Agentic-Planning-with-Verifiable-Constraints"><span class="toc-text">11. DeepPlanning: Benchmarking Long-Horizon Agentic Planning with Verifiable Constraints</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#12-STAR-Semantic-Table-Representation-with-Header-Aware-Clustering-and-Adaptive-Weighted-Fusion"><span class="toc-text">12. STAR: Semantic Table Representation with Header-Aware Clustering and Adaptive Weighted Fusion</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#13-Paying-Less-Generalization-Tax-A-Cross-Domain-Generalization-Study-of-RL-Training-for-LLM-Agents"><span class="toc-text">13. Paying Less Generalization Tax: A Cross-Domain Generalization Study of RL Training for LLM Agents</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#14-AR-Omni-A-Unified-Autoregressive-Model-for-Any-to-Any-Generation"><span class="toc-text">14. AR-Omni: A Unified Autoregressive Model for Any-to-Any Generation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#15-Agentic-Very-Long-Video-Understanding"><span class="toc-text">15. Agentic Very Long Video Understanding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#16-SAGE-Steerable-Agentic-Data-Generation-for-Deep-Search-with-Execution-Feedback"><span class="toc-text">16. SAGE: Steerable Agentic Data Generation for Deep Search with Execution Feedback</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#search%EF%BC%9A%E6%AD%A3%E7%A1%AE%E8%BD%A8%E8%BF%B9%E7%9A%84%E6%9C%80%E5%B0%8F%E6%AD%A5%E6%95%B0"><span class="toc-text">search：正确轨迹的最小步数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#17-DRPG-Decompose-Retrieve-Plan-Generate-An-Agentic-Framework-for-Academic-Rebuttal"><span class="toc-text">17. DRPG (Decompose, Retrieve, Plan, Generate): An Agentic Framework for Academic Rebuttal</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#18-SkyReels-V3-Technique-Report"><span class="toc-text">18. SkyReels-V3 Technique Report</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#19-Least-Loaded-Expert-Parallelism-Load-Balancing-An-Imbalanced-Mixture-of-Experts"><span class="toc-text">19. Least-Loaded Expert Parallelism: Load Balancing An Imbalanced Mixture-of-Experts</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#20-IVRA-Improving-Visual-Token-Relations-for-Robot-Action-Policy-with-Training-Free-Hint-Based-Guidance"><span class="toc-text">20. IVRA: Improving Visual-Token Relations for Robot Action Policy with Training-Free Hint-Based Guidance</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>