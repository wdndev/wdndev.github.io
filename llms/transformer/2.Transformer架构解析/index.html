<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Transformer架构解析 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="Transformer架构解析本文主要来自：The Annotated Transformer 论文地址: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1810.04805.pdf 代码链接：personal&#x2F;transformer · GitHub 1.Transformer架构图1.1 Transformer模型的作用基于seq2seq架构的transformer模型可以完成NLP领域研究的典">
<meta property="og:type" content="article">
<meta property="og:title" content="Transformer架构解析">
<meta property="og:url" content="https://wdndev.github.io/llms/transformer/2.Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="Transformer架构解析本文主要来自：The Annotated Transformer 论文地址: https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;1810.04805.pdf 代码链接：personal&#x2F;transformer · GitHub 1.Transformer架构图1.1 Transformer模型的作用基于seq2seq架构的transformer模型可以完成NLP领域研究的典">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2023-05-24T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:55.700Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/llms/transformer/2.Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Transformer架构解析',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Transformer架构解析</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-05-24T16:00:00.000Z" title="Created 2023-05-25 00:00:00">2023-05-25</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:55.700Z" title="Updated 2026-02-08 08:00:55">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Transformer/">Transformer</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">21.3k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>85min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Transformer架构解析"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="Transformer架构解析"><a href="#Transformer架构解析" class="headerlink" title="Transformer架构解析"></a>Transformer架构解析</h1><p>本文主要来自：<a target="_blank" rel="noopener" href="http://nlp.seas.harvard.edu/annotated-transformer/#hardware-and-schedule">The Annotated Transformer</a></p>
<p>论文地址: <a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf">https://arxiv.org/pdf/1810.04805.pdf</a></p>
<p>代码链接：<a target="_blank" rel="noopener" href="https://github.com/wdndev/personal/tree/main/transformer">personal/transformer · GitHub</a></p>
<h1 id="1-Transformer架构图"><a href="#1-Transformer架构图" class="headerlink" title="1.Transformer架构图"></a>1.Transformer架构图</h1><h2 id="1-1-Transformer模型的作用"><a href="#1-1-Transformer模型的作用" class="headerlink" title="1.1 Transformer模型的作用"></a>1.1 Transformer模型的作用</h2><p>基于seq2seq架构的transformer模型可以完成NLP领域研究的典型任务, 如机器翻译, 文本生成等. 同时又可以构建预训练语言模型，用于不同任务的迁移学习.</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1qh4y1o7UU">https://www.bilibili.com/video/BV1qh4y1o7UU</a></p>
<h2 id="1-2-Transformer总体架构"><a href="#1-2-Transformer总体架构" class="headerlink" title="1.2 Transformer总体架构"></a>1.2 Transformer总体架构</h2><p><img src="image/image_V6s3Iu8dFP.png" alt=""></p>
<h3 id="（1）输入部分"><a href="#（1）输入部分" class="headerlink" title="（1）输入部分"></a>（1）输入部分</h3><ul>
<li>源文本嵌入层及其位置编码器</li>
<li>目标文本嵌入层及其位置编码器</li>
</ul>
<p><img src="image/image_g26cBonBG7.png" alt=""></p>
<h3 id="（2）输出部分"><a href="#（2）输出部分" class="headerlink" title="（2）输出部分"></a>（2）输出部分</h3><ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
<p><img src="image/image_NV2DvH8m8o.png" alt=""></p>
<h3 id="（3）编码器"><a href="#（3）编码器" class="headerlink" title="（3）编码器"></a>（3）编码器</h3><ul>
<li>由N个编码器层堆叠而成</li>
<li>每个编码器层由两个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
<p><img src="image/image_6RlUw8Ldsk.png" alt=""></p>
<h3 id="（4）解码器部分"><a href="#（4）解码器部分" class="headerlink" title="（4）解码器部分:"></a>（4）解码器部分:</h3><ul>
<li>由N个解码器层堆叠而成</li>
<li>每个解码器层由三个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>
<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
<p><img src="image/image_IlV0Rf2hVU.png" alt=""></p>
<h1 id="2-输入部分Embeddings"><a href="#2-输入部分Embeddings" class="headerlink" title="2.输入部分Embeddings"></a>2.输入部分Embeddings</h1><h2 id="2-1文本嵌入层"><a href="#2-1文本嵌入层" class="headerlink" title="2.1文本嵌入层"></a>2.1文本嵌入层</h2><p>无论是源文本嵌入还是目标文本嵌入，都是为了将文本中词汇的数字表示转变为向量表示, 希望在这样的高维空间捕捉词汇间的关系.</p>
<h3 id="（1）实现"><a href="#（1）实现" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>初始化函数以<code>d_model</code>， 词嵌入维度, 和<code>vocab</code>， 词汇总数为参数, 内部主要使用了nn中的预定层Embedding进行词嵌入.</li>
<li>在forward函数中, 将输入x传入到Embedding的实例化对象中, 然后乘以一个根号下<code>d_model</code>进行缩放, 控制数值大小。</li>
<li>它的输出是文本嵌入后的结果。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch中变量封装函数Variable.</span></span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="comment"># 定义Embeddings类来实现文本嵌入层，这里s说明代表两个一模一样的嵌入层, 他们共享参数.</span></span><br><span class="line"><span class="comment"># 该类继承nn.Module, 这样就有标准层的一些功能, 这里我们也可以理解为一种模式, 我们自己实现的所有层都会这样去写.</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Embeddings</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;类的初始化函数, </span></span><br><span class="line"><span class="string">         d_model: 指词嵌入的维度, </span></span><br><span class="line"><span class="string">         vocab: 指词表的大小.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 接着就是使用super的方式指明继承nn.Module的初始化函数, 我们自己实现的所有层都会这样去写.</span></span><br><span class="line">        <span class="built_in">super</span>(Embeddings, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 之后就是调用nn中的预定义层Embedding, 获得一个词嵌入对象self.lut</span></span><br><span class="line">        <span class="variable language_">self</span>.lut = nn.Embedding(vocab, d_model)</span><br><span class="line">        <span class="comment"># 最后就是将d_model传入类中</span></span><br><span class="line">        <span class="variable language_">self</span>.d_model = d_model</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; x: 因为Embedding层是首层, 所以代表输入给模型的文本通过词汇映射后的张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将x传给self.lut并与根号下self.d_model相乘作为结果返回</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.lut(x) * math.sqrt(<span class="variable language_">self</span>.d_model)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试"><a href="#（2）测试" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment"># 词表大小是1000</span></span><br><span class="line">vocab = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 输入x是一个使用Variable封装的长整型张量, 形状是2 x 4</span></span><br><span class="line">x = Variable(torch.LongTensor([[<span class="number">100</span>,<span class="number">2</span>,<span class="number">421</span>,<span class="number">508</span>],[<span class="number">491</span>,<span class="number">998</span>,<span class="number">1</span>,<span class="number">221</span>]]))</span><br><span class="line">emb = Embeddings(d_model, vocab)</span><br><span class="line">embr = emb(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;embr:&quot;</span>, embr)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">embr: Variable containing:</span><br><span class="line">( 0 ,.,.) = </span><br><span class="line">  35.9321   3.2582 -17.7301  ...    3.4109  13.8832  39.0272</span><br><span class="line">   8.5410  -3.5790 -12.0460  ...   40.1880  36.6009  34.7141</span><br><span class="line"> -17.0650  -1.8705 -20.1807  ...  -12.5556 -34.0739  35.6536</span><br><span class="line">  20.6105   4.4314  14.9912  ...   -0.1342  -9.9270  28.6771</span><br><span class="line"></span><br><span class="line">( 1 ,.,.) = </span><br><span class="line">  27.7016  16.7183  46.6900  ...   17.9840  17.2525  -3.9709</span><br><span class="line">   3.0645  -5.5105  10.8802  ...  -13.0069  30.8834 -38.3209</span><br><span class="line">  33.1378 -32.1435  -3.9369  ...   15.6094 -29.7063  40.1361</span><br><span class="line"> -31.5056   3.3648   1.4726  ...    2.8047  -9.6514 -23.4909</span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure>
<h2 id="2-2-位置编码器PositionalEncoding"><a href="#2-2-位置编码器PositionalEncoding" class="headerlink" title="2.2 位置编码器PositionalEncoding"></a>2.2 位置编码器PositionalEncoding</h2><p>因为在Transformer的编码器结构中, 并没有针对词汇位置信息的处理，因此需要在Embedding层后加入位置编码器，<strong>将词汇位置不同可能会产生不同语义的信息加入到词嵌入张量中, 以弥补位置信息的缺失</strong>.</p>
<p>使用不同频率的正弦和余弦函数：</p>
<script type="math/tex; mode=display">
PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}})</script><script type="math/tex; mode=display">
PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中$pos$是位置，$i$ 是维度。也就是说，位置编码的每个维度对应于一个正弦曲线。 这些波长形成一个从 $2\pi$ 到 $10000 \cdot 2\pi$的集合级数。我们选择这个函数是因为我们假设它会让模型很容易学习对相对位置的关注，因为对任意确定的偏移k , $PE<em>{pos+k}$可以表示为 $PE</em>{pos}$的线性函数。</p>
<h3 id="（1）实现-1"><a href="#（1）实现-1" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>初始化函数以<code>d_model</code>, <code>dropout</code>, <code>max_len</code>为参数, 分别代表<code>d_model</code>: 词嵌入维度, <code>dropout</code>: 置0比率, <code>max_len</code>: 每个句子的最大长度.</li>
<li>forward函数中的输入参数为x, 是Embedding层的输出.</li>
<li>最终输出一个加入了位置编码信息的词嵌入张量.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义位置编码器类, 我们同样把它看做一个层, 因此会继承nn.Module    </span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PositionalEncoding</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, dropout, max_len=<span class="number">5000</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;位置编码器类的初始化函数, 共有三个参数, 分别是</span></span><br><span class="line"><span class="string">          d_model: 词嵌入维度, </span></span><br><span class="line"><span class="string">          dropout: 置0比率, </span></span><br><span class="line"><span class="string">          max_len: 每个句子的最大长度</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(PositionalEncoding, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 实例化nn中预定义的Dropout层, 并将dropout传入其中, 获得对象self.dropout</span></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line">        <span class="comment"># 初始化一个位置编码矩阵, 它是一个0阵，矩阵的大小是max_len x d_model.</span></span><br><span class="line">        pe = torch.zeros(max_len, d_model)</span><br><span class="line">        <span class="comment"># 初始化一个绝对位置矩阵, 在我们这里，词汇的绝对位置就是用它的索引去表示. </span></span><br><span class="line">        <span class="comment"># 所以我们首先使用arange方法获得一个连续自然数向量，然后再使用unsqueeze方法拓展向量维度使其成为矩阵， </span></span><br><span class="line">        <span class="comment"># 又因为参数传的是1，代表矩阵拓展的位置，会使向量变成一个max_len x 1 的矩阵， </span></span><br><span class="line">        position = torch.arange(<span class="number">0</span>, max_len).unsqueeze(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 绝对位置矩阵初始化之后，接下来就是考虑如何将这些位置信息加入到位置编码矩阵中，</span></span><br><span class="line">        <span class="comment"># 最简单思路就是先将max_len x 1的绝对位置矩阵， 变换成max_len x d_model形状，然后覆盖原来的初始位置编码矩阵即可， </span></span><br><span class="line">        <span class="comment"># 要做这种矩阵变换，就需要一个1xd_model形状的变换矩阵div_term，我们对这个变换矩阵的要求除了形状外，</span></span><br><span class="line">        <span class="comment"># 还希望它能够将自然数的绝对位置编码缩放成足够小的数字，有助于在之后的梯度下降过程中更快的收敛.  这样我们就可以开始初始化这个变换矩阵了.</span></span><br><span class="line">        <span class="comment"># 首先使用arange获得一个自然数矩阵， 但是细心的同学们会发现， 我们这里并没有按照预计的一样初始化一个1xd_model的矩阵， </span></span><br><span class="line">        <span class="comment"># 而是有了一个跳跃，只初始化了一半即1xd_model/2 的矩阵。 为什么是一半呢，其实这里并不是真正意义上的初始化了一半的矩阵，</span></span><br><span class="line">        <span class="comment"># 我们可以把它看作是初始化了两次，而每次初始化的变换矩阵会做不同的处理，第一次初始化的变换矩阵分布在正弦波上， 第二次初始化的变换矩阵分布在余弦波上， </span></span><br><span class="line">        <span class="comment"># 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上，组成最终的位置编码矩阵.</span></span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>) *</span><br><span class="line">                             -(math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        pe[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        pe[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 这样我们就得到了位置编码矩阵pe, pe现在还只是一个二维矩阵，要想和embedding的输出（一个三维张量）相加，</span></span><br><span class="line">        <span class="comment"># 就必须拓展一个维度，所以这里使用unsqueeze拓展维度.</span></span><br><span class="line">        pe = pe.unsqueeze(<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 最后把pe位置编码矩阵注册成模型的buffer，什么是buffer呢，</span></span><br><span class="line">        <span class="comment"># 我们把它认为是对模型效果有帮助的，但是却不是模型结构中超参数或者参数，不需要随着优化步骤进行更新的增益对象. </span></span><br><span class="line">        <span class="comment"># 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载.</span></span><br><span class="line">        <span class="variable language_">self</span>.register_buffer(<span class="string">&#x27;pe&#x27;</span>, pe)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; x, 表示文本序列的词嵌入表示 &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在相加之前我们对pe做一些适配工作， 将这个三维张量的第二维也就是句子最大长度的那一维将切片到与输入的x的第二维相同即x.size(1)，</span></span><br><span class="line">        <span class="comment"># 因为我们默认max_len为5000一般来讲实在太大了，很难有一条句子包含5000个词汇，所以要进行与输入张量的适配. </span></span><br><span class="line">        <span class="comment"># 最后使用Variable进行封装，使其与x的样式相同，但是它是不需要进行梯度求解的，因此把requires_grad设置成false.</span></span><br><span class="line">        x = x + Variable(<span class="variable language_">self</span>.pe[:, :x.size(<span class="number">1</span>)], </span><br><span class="line">                         requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># 最后使用self.dropout对象进行&#x27;丢弃&#x27;操作, 并返回结果.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.dropout(x)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-1"><a href="#（2）测试-1" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 词嵌入维度是512维</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment"># 置0比率为0.1</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"><span class="comment"># 句子最大长度</span></span><br><span class="line">max_len=<span class="number">60</span></span><br><span class="line"></span><br><span class="line">x = embr</span><br><span class="line">pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">pe_result = pe(x)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;pe_result:&quot;</span>, pe_result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">pe_result: Variable containing:</span><br><span class="line">( 0 ,.,.) = </span><br><span class="line"> -19.7050   0.0000   0.0000  ...  -11.7557  -0.0000  23.4553</span><br><span class="line">  -1.4668 -62.2510  -2.4012  ...   66.5860 -24.4578 -37.7469</span><br><span class="line">   9.8642 -41.6497 -11.4968  ...  -21.1293 -42.0945  50.7943</span><br><span class="line">   0.0000  34.1785 -33.0712  ...   48.5520   3.2540  54.1348</span><br><span class="line">( 1 ,.,.) = </span><br><span class="line">   7.7598 -21.0359  15.0595  ...  -35.6061  -0.0000   4.1772</span><br><span class="line"> -38.7230   8.6578  34.2935  ...  -43.3556  26.6052   4.3084</span><br><span class="line">  24.6962  37.3626 -26.9271  ...   49.8989   0.0000  44.9158</span><br><span class="line"> -28.8435 -48.5963  -0.9892  ...  -52.5447  -4.1475  -3.0450</span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br></pre></td></tr></table></figure>
<h3 id="（3）绘制词汇向量中特征的分布曲线"><a href="#（3）绘制词汇向量中特征的分布曲线" class="headerlink" title="（3）绘制词汇向量中特征的分布曲线"></a>（3）绘制词汇向量中特征的分布曲线</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一张15 x 5大小的画布</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment"># 实例化PositionalEncoding类得到pe对象, 输入参数是20和0</span></span><br><span class="line">pe = PositionalEncoding(<span class="number">20</span>, <span class="number">0</span>)</span><br><span class="line"><span class="comment"># 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数, </span></span><br><span class="line"><span class="comment"># 且这个tensor里的数值都是0, 被处理后相当于位置编码张量</span></span><br><span class="line">y = pe(Variable(torch.zeros(<span class="number">1</span>, <span class="number">100</span>, <span class="number">20</span>)))</span><br><span class="line"><span class="comment"># 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值</span></span><br><span class="line"><span class="comment"># 因为总共有20维之多, 我们这里只查看4，5，6，7维的值.</span></span><br><span class="line">plt.plot(np.arange(<span class="number">100</span>), y[<span class="number">0</span>, :, <span class="number">4</span>:<span class="number">8</span>].data.numpy())</span><br><span class="line"><span class="comment"># 在画布上填写维度提示信息</span></span><br><span class="line">plt.legend([<span class="string">&quot;dim %d&quot;</span>%p <span class="keyword">for</span> p <span class="keyword">in</span> [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]])</span><br></pre></td></tr></table></figure>
<p><img src="image/image_ZclAi-4PTQ.png" alt=""></p>
<p>效果分析：</p>
<ul>
<li>每条颜色的曲线代表某一个词汇中的特征在不同位置的含义.</li>
<li>保证同一词汇随着所在位置不同它对应位置嵌入向量会发生变化。</li>
<li>正弦波和余弦波的值域范围都是1到-1这又很好的控制了嵌入数值的大小, 有助于梯度的快速计算。</li>
</ul>
<h1 id="3-编码器"><a href="#3-编码器" class="headerlink" title="3.编码器"></a>3.编码器</h1><ul>
<li>由N个编码器层堆叠而成</li>
<li>每个编码器层由两个子层连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
<p><img src="image/image_-NDHADCxJy.png" alt=""></p>
<h2 id="3-1-掩码张量subsequent-mask"><a href="#3-1-掩码张量subsequent-mask" class="headerlink" title="3.1 掩码张量subsequent_mask"></a>3.1 掩码张量subsequent_mask</h2><p>掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量</p>
<p><strong>掩码张量的作用</strong>：在transformer中, 掩码张量的主要作用在应用attention时，有一些生成的attention张量中的值计算有可能已知了未来信息而得到的，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，但是理论上解码器的的输出却不是一次就能产生最终结果的，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用。</p>
<h3 id="（1）实现-2"><a href="#（1）实现-2" class="headerlink" title="（1）实现"></a>（1）实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">subsequent_mask</span>(<span class="params">size</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;生成向后遮掩的掩码张量, </span></span><br><span class="line"><span class="string">      参数size是掩码张量最后两个维度的大小, 它的最后两维形成一个方阵</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 首先定义掩码张量的形状</span></span><br><span class="line">    attn_shape = (<span class="number">1</span>, size, size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 然后使用np.ones方法向这个形状中添加1元素,形成上三角阵, 最后为了节约空间, </span></span><br><span class="line">    <span class="comment"># 再使其中的数据类型变为无符号8位整形unit8 </span></span><br><span class="line">    subsequent_mask = np.triu(np.ones(attn_shape), k=<span class="number">1</span>).astype(<span class="string">&#x27;uint8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后将numpy类型转化为torch中的tensor, 内部做一个1 - 的操作, </span></span><br><span class="line">    <span class="comment"># 在这个其实是做了一个三角阵的反转, subsequent_mask中的每个元素都会被1减, </span></span><br><span class="line">    <span class="comment"># 如果是0, subsequent_mask中的该位置由0变成1</span></span><br><span class="line">    <span class="comment"># 如果是1, subsequent_mask中的该位置由1变成0 </span></span><br><span class="line">    <span class="keyword">return</span> torch.from_numpy(<span class="number">1</span> - subsequent_mask)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-2"><a href="#（2）测试-2" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 生成的掩码张量的最后两维的大小</span></span><br><span class="line">size = <span class="number">5</span></span><br><span class="line">sm = subsequent_mask(size)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;sm:&quot;</span>, sm)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[1, 0, 0, 0, 0],</span><br><span class="line">         [1, 1, 0, 0, 0],</span><br><span class="line">         [1, 1, 1, 0, 0],</span><br><span class="line">         [1, 1, 1, 1, 0],</span><br><span class="line">         [1, 1, 1, 1, 1]]], dtype=torch.uint8)</span><br></pre></td></tr></table></figure>
<h3 id="（3）掩码张量的可视化"><a href="#（3）掩码张量的可视化" class="headerlink" title="（3）掩码张量的可视化"></a>（3）掩码张量的可视化</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">plt.figure(figsize=(<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">plt.imshow(subsequent_mask(<span class="number">20</span>)[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>
<p><img src="image/image_bHivDZ57nK.png" alt=""></p>
<p>效果分析:</p>
<ul>
<li>通过观察可视化方阵, 黄色是1的部分, 这里代表被遮掩, 紫色代表没有被遮掩的信息, 横坐标代表目标词汇的位置, 纵坐标代表可查看的位置;</li>
<li>我们看到, 在0的位置我们一看望过去都是黄色的, 都被遮住了，1的位置一眼望过去还是黄色, 说明第一次词还没有产生, 从第二个位置看过去, 就能看到位置1的词, 其他位置看不到, 以此类推.</li>
</ul>
<h2 id="3-2-注意力机制Attention"><a href="#3-2-注意力机制Attention" class="headerlink" title="3.2 注意力机制Attention"></a>3.2 注意力机制Attention</h2><h3 id="（1）什么是注意力"><a href="#（1）什么是注意力" class="headerlink" title="（1）什么是注意力"></a>（1）什么是注意力</h3><p>我们观察事物时，之所以能够快速判断一种事物（当然允许判断是错误的）, 是因为我们大脑能够很快把注意力放在事物最具有辨识度的部分从而作出判断，而并非是从头到尾的观察一遍事物后，才能有判断结果。正是基于这样的理论，就产生了注意力机制。</p>
<p>Attention功能可以描述为将query和一组key-value对映射到输出，其中query、key、value和输出都是向量。输出为value的加权和，其中每个value的权重通过query与相应key的兼容函数来计算。</p>
<p>具体来说，output 是 value 的一个加权和 —&gt; 输出的维度 ==  value 的维度。output 中 value 的权重 = 查询 query 和对应的 key 的相似度 ；权重等价于 query 和对应的 key 的相似度</p>
<p><img src="image/image_mo1QFm0-QD.png" alt=""></p>
<p>图中，红色表示value，蓝色表示key：</p>
<ul>
<li>给定q为黄色，靠近key的第一第二个，所以output更多偏向与value的第一和第二个；</li>
<li>给定q为绿色，靠近key的第二第三个，所以output更多偏向于value的第二和第三个。</li>
</ul>
<p>虽然 key-value 并没有变，但是随着 query 的改变，因为权重的分配不一样，导致 输出会有不一样，这就是注意力机制。</p>
<h3 id="（2）注意力计算规则"><a href="#（2）注意力计算规则" class="headerlink" title="（2）注意力计算规则"></a>（2）注意力计算规则</h3><p>需要三个指定的输入Q(query), K(key), V(value)， 然后通过公式得到注意力的计算结果, 这个结果代表<strong>query在key和value作用下的表示</strong>。而这个具体的计算规则有很多种，我这里只介绍我们用到的这一种.</p>
<script type="math/tex; mode=display">
Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V</script><h3 id="（3）Q-K-V的比喻解释"><a href="#（3）Q-K-V的比喻解释" class="headerlink" title="（3）Q, K, V的比喻解释"></a>（3）Q, K, V的比喻解释</h3><p>假如我们有一个问题： 给出一段文本，使用一些关键词对它进行描述!</p>
<p>为了方便统一正确答案，这道题可能预先已经给大家写出了一些关键词作为提示.其中这些给出的提示就可以看作是key， 而整个的文本信息就相当于是query，value的含义则更抽象，可以比作是你看到这段文本信息后，脑子里浮现的答案信息，这里我们又假设大家最开始都不是很聪明，第一次看到这段文本后脑子里基本上浮现的信息就只有提示这些信息，因此key与value基本是相同的，但是随着我们对这个问题的深入理解，通过我们的思考脑子里想起来的东西原来越多，并且能够开始对我们query也就是这段文本，提取关键信息进行表示.  这就是注意力作用的过程， 通过这个过程，我们最终脑子里的value发生了变化，根据提示key生成了query的关键词表示方法，也就是另外一种特征表示方法.</p>
<p>刚刚说到key和value一般情况下默认是相同，与query是不同的，这种是我们一般的注意力输入形式，但有一种特殊情况，就是我们query与key和value相同，这种情况我们称为<strong>自注意力机制</strong>，就如同我们的刚刚的例子， 使用一般注意力机制，是使用不同于给定文本的关键词表示它. 而自注意力机制，需要用给定文本自身来表达自己，也就是说你需要从给定文本中抽取关键词来表述它, 相当于对文本自身的一次特征提取.</p>
<h3 id="（4）注意力机制"><a href="#（4）注意力机制" class="headerlink" title="（4）注意力机制"></a>（4）注意力机制</h3><p>注意力机制是注意力计算规则能够应用的深度学习网络的载体, 除了注意力计算规则外, 还包括一些必要的全连接层以及相关张量处理, 使其与应用网络融为一体。使用自注意力计算规则的注意力机制称为自注意力机制。</p>
<p>将particular attention称之为“缩放的点积Attention”(Scaled Dot-Product Attention”)。其输入为query、key(维度是$d_k$)以及values(维度是$d_v$)。我们计算query和所有key的点积，然后对每个除以 $\sqrt{d_k}$ , 最后用softmax函数获得value的权重。</p>
<p><img src="image/image_oiPuFb7z05.png" alt=""></p>
<p>两个最常用的attention函数是加法attention(cite)和点积（乘法）attention。除了缩放因子 $\frac{1}{\sqrt{d_k}}$ ，点积Attention跟我们的平时的算法一样。加法attention使用具有单个隐层的前馈网络计算兼容函数。虽然理论上点积attention和加法attention复杂度相似，但在实践中，点积attention可以使用高度优化的矩阵乘法来实现，因此点积attention计算更快、更节省空间。</p>
<p>当 $d<em>k$ 的值比较小的时候，这两个机制的性能相近。当 $d_k$ 比较大时，加法attention比不带缩放的点积attention性能好 (cite)。我们怀疑，对于很大的 $d_k$ 值, 点积大幅度增长，将softmax函数推向具有极小梯度的区域。(为了说明为什么点积变大，假设$q$和$k$是独立的随机变量，均值为0，方差为1。那么它们的点积 $q \cdot k = \sum</em>{i=1}^{d_k} q_ik_i$ , 均值为0方差为$d_k$ )。为了抵消这种影响，我们将点积缩小 $\frac{1}{\sqrt{d_k}}$ 倍。</p>
<p><strong>为什么Attention中除以</strong>$\sqrt{d}$<strong> 这么重要？</strong></p>
<p>Attention的计算是在内积之后进行softmax，主要涉及的运算是$e^{q \cdot k}$，可以大致认为内积之后、softmax之前的数值在$-3\sqrt{d}$到$3\sqrt{d}$这个范围内，由于d通常都至少是64，所以$e^{3\sqrt{d}}$比较大而 $e^{-3\sqrt{d}}$比较小，因此经过softmax之后，Attention的分布非常接近一个one hot分布了，这带来严重的梯度消失问题，导致训练效果差。（例如y=softmax(x)在|x|较大时进入了饱和区，x继续变化y值也几乎不变，即饱和区梯度消失）</p>
<p>相应地，解决方法就有两个:</p>
<ol>
<li><p>像NTK参数化那样，在内积之后除以 $\sqrt{d}$，使q⋅k的方差变为1，对应$e^3,e^{−3}$都不至于过大过小，这样softmax之后也不至于变成one hot而梯度消失了，这也是常规的Transformer如BERT里边的Self Attention的做法</p>
</li>
<li><p>另外就是不除以 $\sqrt{d}$，但是初始化q,k的全连接层的时候，其初始化方差要多除以一个d，这同样能使得使q⋅k的初始方差变为1，T5采用了这样的做法。</p>
</li>
</ol>
<h3 id="（5）代码实现"><a href="#（5）代码实现" class="headerlink" title="（5）代码实现"></a>（5）代码实现</h3><ul>
<li>输入就是Q，K，V以及mask和dropout, mask用于掩码, dropout用于随机置0.</li>
<li>输出有两个, query的注意力表示以及注意力张量.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">query, key, value, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;注意力机制的实现, </span></span><br><span class="line"><span class="string">        输入分别是query, key, value, mask: 掩码张量, </span></span><br><span class="line"><span class="string">       dropout是nn.Dropout层的实例化对象, 默认为None</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将query的最后一个维度提取出来，代表的是词嵌入的维度</span></span><br><span class="line">    d_k = query.size(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 按照注意力公式, 将query与key的转置相乘, 这里面key是将最后两个维度进行转置, </span></span><br><span class="line">    <span class="comment"># 再除以缩放系数根号下d_k, 这种计算方法也称为缩放点积注意力计算.</span></span><br><span class="line">    <span class="comment"># 得到注意力得分张量scores</span></span><br><span class="line">    scores = torch.matmul(query, key.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 接着判断是否使用掩码张量</span></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 使用tensor的masked_fill方法, 将掩码张量和scores张量每个位置一一比较, </span></span><br><span class="line">        <span class="comment"># 如果掩码张量处为0，则对应的scores张量用-1e9这个值来替换, 如下演示</span></span><br><span class="line">        scores = scores.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e9</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对scores的最后一维进行softmax操作, 使用F.softmax方法, 第一个参数是softmax对象, 第二个是目标维度.</span></span><br><span class="line">    <span class="comment"># 这样获得最终的注意力张量</span></span><br><span class="line">    p_attn = scores.softmax(dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 之后判断是否使用dropout进行随机置0</span></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 将p_attn传入dropout对象中进行&#x27;丢弃&#x27;处理</span></span><br><span class="line">        p_attn = dropout(p_attn)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后, 根据公式将p_attn与value张量相乘获得最终的query注意力表示, 同时返回注意力张量</span></span><br><span class="line">    <span class="keyword">return</span> torch.matmul(p_attn, value), p_attn</span><br></pre></td></tr></table></figure>
<h3 id="（6）测试"><a href="#（6）测试" class="headerlink" title="（6）测试"></a>（6）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> embedding <span class="keyword">import</span> Embeddings</span><br><span class="line"><span class="keyword">from</span> positional_encoding <span class="keyword">import</span> PositionalEncoding</span><br><span class="line"></span><br><span class="line">  d_model = <span class="number">512</span></span><br><span class="line">  dropout = <span class="number">0.1</span></span><br><span class="line">  max_len = <span class="number">60</span></span><br><span class="line"></span><br><span class="line">  vocab = <span class="number">1000</span></span><br><span class="line">  emb = Embeddings(d_model, vocab)</span><br><span class="line">  <span class="built_in">input</span> = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">9</span>]]))</span><br><span class="line">  embr = emb(<span class="built_in">input</span>)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;embr: &quot;</span>, embr)</span><br><span class="line">  <span class="built_in">print</span>(embr.shape)</span><br><span class="line"></span><br><span class="line">  pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">  pe_res = pe(embr)</span><br><span class="line"></span><br><span class="line">  query = key = value = pe_res</span><br><span class="line">  attn, p_attn = attention(query, key, value)</span><br><span class="line"></span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;attn shape:&quot;</span>, attn.shape)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;p_attn:&quot;</span>, p_attn)</span><br><span class="line">  <span class="built_in">print</span>(<span class="string">&quot;p_attn shape:&quot;</span>, p_attn.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">attn: tensor([[[ -6.9986, -22.1325,  44.3268,  ...,   0.0000,  -2.9953,  -3.8844],</span><br><span class="line">         [ 57.2318,   5.9384,   0.0000,  ...,  38.5518,   0.5860,  12.5283],</span><br><span class="line">         [-56.2970,   0.0000,  -4.3592,  ...,  26.0355,  -2.3129,   0.0000],</span><br><span class="line">         [ -2.1557,   3.4803, -36.6878,  ...,  15.8174, -21.3978, -30.9041]],</span><br><span class="line"></span><br><span class="line">        [[-57.3073, -25.4691,  -5.3997,  ...,  26.0355,  -2.3131,  14.3969],</span><br><span class="line">         [  6.9673, -32.6722,  39.3464,  ...,  -5.4699, -10.4042,  -4.4331],</span><br><span class="line">         [ 57.3072,   4.8757,  -5.6530,  ...,  38.5518,   0.5862,   0.0000],</span><br><span class="line">         [ 18.3028,  -9.1978,  59.9258,  ...,  -9.5552, -45.4553,   0.0000]]],</span><br><span class="line">       grad_fn=&lt;UnsafeViewBackward&gt;)</span><br><span class="line">attn shape: torch.Size([2, 4, 512])</span><br><span class="line">p_attn: tensor([[[1., 0., 0., 0.],</span><br><span class="line">         [0., 1., 0., 0.],</span><br><span class="line">         [0., 0., 1., 0.],</span><br><span class="line">         [0., 0., 0., 1.]],</span><br><span class="line"></span><br><span class="line">        [[1., 0., 0., 0.],</span><br><span class="line">         [0., 1., 0., 0.],</span><br><span class="line">         [0., 0., 1., 0.],</span><br><span class="line">         [0., 0., 0., 1.]]], grad_fn=&lt;SoftmaxBackward&gt;)</span><br><span class="line">p_attn shape: torch.Size([2, 4, 4])</span><br></pre></td></tr></table></figure>
<p>待用mask的输出效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">query = key = value = pe_result</span><br><span class="line"></span><br><span class="line"><span class="comment"># 令mask为一个2x4x4的零张量</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">2</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">attn, p_attn = attention(query, key, value, mask=mask)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;attn:&quot;</span>, attn)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;p_attn:&quot;</span>, p_attn)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># query的注意力表示:</span></span><br><span class="line">attn: Variable containing:</span><br><span class="line">( 0 ,.,.) = </span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line">   0.4284  -7.4741   8.8839  ...    1.5618   0.5063   0.5770</span><br><span class="line"></span><br><span class="line">( 1 ,.,.) = </span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">  -2.8890   9.9972 -12.9505  ...    9.1657  -4.6164  -0.5491</span><br><span class="line">[torch.FloatTensor of size 2x4x512]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力张量:</span></span><br><span class="line">p_attn: Variable containing:</span><br><span class="line">(0 ,.,.) = </span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line"></span><br><span class="line">(1 ,.,.) = </span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">  0.2500  0.2500  0.2500  0.2500</span><br><span class="line">[torch.FloatTensor of size 2x4x4]</span><br></pre></td></tr></table></figure>
<h2 id="3-3-多头注意力机制MultiHeadAttention"><a href="#3-3-多头注意力机制MultiHeadAttention" class="headerlink" title="3.3 多头注意力机制MultiHeadAttention"></a>3.3 多头注意力机制MultiHeadAttention</h2><h3 id="（1）什么是多头注意力机制"><a href="#（1）什么是多头注意力机制" class="headerlink" title="（1）什么是多头注意力机制"></a>（1）什么是多头注意力机制</h3><p>从多头注意力的结构图中，貌似这个所谓的多个头就是指多组线性变换层，其实并不是，我只有使用了一组线性变化层，即三个变换张量对Q，K，V分别进行线性变换，<strong>这些变换不会改变原有张量的尺寸</strong>，因此每个变换矩阵都是方阵，得到输出结果后，多头的作用才开始显现，每个头开始从词义层面分割输出的张量，也就是每个头都想获得一组Q，K，V进行注意力机制的计算，但是句子中的每个词的表示只获得一部分，也就是只分割了最后一维的词嵌入向量。这就是所谓的多头，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.</p>
<p>Multi-head attention允许模型共同关注来自不同位置的不同表示子空间的信息，如果只有一个attention head，它的平均值会削弱这个信息。</p>
<script type="math/tex; mode=display">
MultiHead(Q,K,V)=Concat(head_1,...,head_h)W^O  \\
where ~ head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)</script><p>其中映射由权重矩阵完成：$ W^Q<em>i \in \mathbb{R}^{d</em> \times d<em>k}<br>  $ , $W^K_i \in \mathbb{R}^{d</em>{\text{model}} \times d<em>k}$ , $W^V_i \in \mathbb{R}^{d</em>{\text{model}} \times d<em>v}$ 和 $W^O_i \in \mathbb{R}^{hd_v \times d</em>{\text{model}} }$ 。</p>
<p><img src="image/image_saBcwnCQZW.png" alt=""></p>
<h3 id="（2）多头注意力作用"><a href="#（2）多头注意力作用" class="headerlink" title="（2）多头注意力作用"></a>（2）多头注意力作用</h3><p>这种结构设计能<strong>让每个注意力机制去优化每个词汇的不同特征部分</strong>，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.</p>
<p><strong>为什么要做多头注意力机制呢</strong>？</p>
<ul>
<li>一个 dot product 的注意力里面，没有什么可以学的参数。具体函数就是内积，为了识别不一样的模式，希望有不一样的计算相似度的办法。加性 attention 有一个权重可学，也许能学到一些内容。</li>
<li>multi-head attention 给 h 次机会去学习 不一样的投影的方法，使得在投影进去的度量空间里面能够去匹配不同模式需要的一些相似函数，然后把 h 个 heads 拼接起来，最后再做一次投影。</li>
<li>每一个头 hi 是把 Q,K,V 通过 可以学习的 Wq, Wk, Wv 投影到 dv 上，再通过注意力函数，得到 headi。&#x20;</li>
</ul>
<h3 id="（3）实现"><a href="#（3）实现" class="headerlink" title="（3）实现"></a>（3）实现</h3><ul>
<li>因为多头注意力机制中需要使用多个相同的线性层, 首先实现了克隆函数clones.</li>
<li>clones函数的输入是module，N，分别代表克隆的目标层，和克隆个数.</li>
<li>clones函数的输出是装有N个克隆层的Module列表.</li>
<li>接着实现MultiHeadedAttention类, 它的初始化函数输入是h, d_model, dropout分别代表头数，词嵌入维度和置零比率.</li>
<li>它的实例化对象输入是Q, K, V以及掩码张量mask.</li>
<li>它的实例化对象输出是通过多头注意力机制处理的Q的注意力表示.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clones</span>(<span class="params">model, N</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    用于生成相同网络层的克隆函数, </span></span><br><span class="line"><span class="string">        - module表示要克隆的目标网络层, </span></span><br><span class="line"><span class="string">        - N代表需要克隆的数量</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 在函数中, 我们通过for循环对module进行N次深度拷贝, 使其每个module成为独立的层,</span></span><br><span class="line">    <span class="comment"># 然后将其放在nn.ModuleList类型的列表中存放.</span></span><br><span class="line">    <span class="keyword">return</span> nn.ModuleList([copy.deepcopy(model) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(N)])</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, head, embedding_dim, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiHeadAttention, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 多头注意力机制</span></span><br><span class="line"><span class="string">            - head代表头数</span></span><br><span class="line"><span class="string">            - embedding_dim代表词嵌入的维度， </span></span><br><span class="line"><span class="string">           - dropout代表进行dropout操作时置0比率，默认是0.1</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 在函数中，首先使用了一个测试中常用的assert语句，判断h是否能被d_model整除，</span></span><br><span class="line">        <span class="comment"># 这是因为我们之后要给每个头分配等量的词特征.也就是embedding_dim/head个.</span></span><br><span class="line">        <span class="keyword">assert</span> embedding_dim % head == <span class="number">0</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头获得的分割词向量维度d_k</span></span><br><span class="line">        <span class="variable language_">self</span>.d_k = embedding_dim // head</span><br><span class="line">        <span class="comment"># 传入头数head</span></span><br><span class="line">        <span class="variable language_">self</span>.head = head</span><br><span class="line"></span><br><span class="line">         <span class="comment"># 然后获得线性层对象，通过nn的Linear实例化，</span></span><br><span class="line">         <span class="comment"># 它的内部变换矩阵是embedding_dim x embedding_dim，</span></span><br><span class="line">         <span class="comment"># 然后使用clones函数克隆四个，为什么是四个呢，</span></span><br><span class="line">         <span class="comment"># 这是因为在多头注意力中，Q，K，V各需要一个，</span></span><br><span class="line">         <span class="comment"># 最后拼接的矩阵还需要一个，因此一共是四个.</span></span><br><span class="line">        <span class="variable language_">self</span>.linears = clones(nn.Linear(embedding_dim, embedding_dim), <span class="number">4</span>)</span><br><span class="line">        <span class="comment"># self.attn为None，它代表最后得到的注意力张量，现在还没有结果所以为None.</span></span><br><span class="line">        <span class="variable language_">self</span>.attn = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 输入参数有四个，前三个就是注意力机制需要的Q, K, V，</span></span><br><span class="line"><span class="string">            最后一个是注意力机制中可能需要的mask掩码张量，默认是None.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 如果存在掩码张量mask</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 使用unsqueeze拓展维度</span></span><br><span class="line">            mask = mask.unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着，我们获得一个batch_size的变量，他是query尺寸的第1个数字，代表有多少条样本.</span></span><br><span class="line">        batch_size = query.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 之后就进入多头处理环节</span></span><br><span class="line">        <span class="comment"># 首先利用zip将输入QKV与三个线性层组到一起，然后使用for循环，</span></span><br><span class="line">        <span class="comment"># 将输入QKV分别传到线性层中，做完线性变换后，</span></span><br><span class="line">        <span class="comment"># 开始为每个头分割输入，这里使用view方法对线性变换的结果进行维度重塑，多加了一个维度h，代表头数，</span></span><br><span class="line">        <span class="comment"># 这样就意味着每个头可以获得一部分词特征组成的句子，其中的-1代表自适应维度，</span></span><br><span class="line">        <span class="comment"># 计算机会根据这种变换自动计算这里的值.然后对第二维和第三维进行转置操作，</span></span><br><span class="line">        <span class="comment"># 为了让代表句子长度维度和词向量维度能够相邻，</span></span><br><span class="line">        <span class="comment"># 这样注意力机制才能找到词义与句子位置的关系，</span></span><br><span class="line">        <span class="comment"># 从attention函数中可以看到，利用的是原始输入的倒数第一和第二维.</span></span><br><span class="line">        <span class="comment"># 这样我们就得到了每个头的输入.</span></span><br><span class="line">        query, key, value = [</span><br><span class="line">            model(x).view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.head, <span class="variable language_">self</span>.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">            <span class="keyword">for</span> model, x <span class="keyword">in</span> <span class="built_in">zip</span>(<span class="variable language_">self</span>.linears, (query, key, value))</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 得到每个头的输入后，接下来就是将他们传入到attention中，</span></span><br><span class="line">        <span class="comment"># 这里直接调用我们之前实现的attention函数.同时也将mask和dropout传入其中.</span></span><br><span class="line">        x, <span class="variable language_">self</span>.attn = attention(query, key, value, mask=mask, dropout=<span class="variable language_">self</span>.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 通过多头注意力计算后，我们就得到了每个头计算结果组成的4维张量，</span></span><br><span class="line">        <span class="comment"># 我们需要将其转换为输入的形状以方便后续的计算，</span></span><br><span class="line">        <span class="comment"># 因此这里开始进行第一步处理环节的逆操作，</span></span><br><span class="line">        <span class="comment"># 先对第二和第三维进行转置，然后使用contiguous方法，</span></span><br><span class="line">        <span class="comment"># 这个方法的作用就是能够让转置后的张量应用view方法，否则将无法直接使用，</span></span><br><span class="line">        <span class="comment"># 所以，下一步就是使用view重塑形状，变成和输入形状相同.</span></span><br><span class="line">        x = x.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, <span class="variable language_">self</span>.head * <span class="variable language_">self</span>.d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 最后使用线性层列表中的最后一个线性层对输入进行线性变换得到最终的多头注意力结构的输出.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.linears[-<span class="number">1</span>](x)</span><br></pre></td></tr></table></figure>
<h3 id="（4）测试"><a href="#（4）测试" class="headerlink" title="（4）测试"></a>（4）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line">max_len = <span class="number">60</span></span><br><span class="line">vocab = <span class="number">1000</span></span><br><span class="line">emb = Embeddings(d_model, vocab)</span><br><span class="line"><span class="built_in">input</span> = Variable(torch.LongTensor([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">4</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">9</span>]]))</span><br><span class="line">embr = emb(<span class="built_in">input</span>)</span><br><span class="line"></span><br><span class="line">pe = PositionalEncoding(d_model, dropout, max_len)</span><br><span class="line">pe_res = pe(embr)</span><br><span class="line"></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">embedding_dim = <span class="number">512</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">query = key = value = pe_res</span><br><span class="line"></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">mha = MultiHeadAttention(head, embedding_dim, dropout)</span><br><span class="line">mha_res = mha(query, key, value, mask)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[-2.3411, -0.8430, -4.1038,  ...,  1.4731, -0.7992,  0.9026],</span><br><span class="line">         [-5.1657, -2.4703, -7.4543,  ...,  0.8810,  0.3061,  0.6387],</span><br><span class="line">         [-4.2553, -0.1940, -6.1963,  ..., -3.4095,  0.6791, -0.6660],</span><br><span class="line">         [-4.8889, -4.0475, -5.9836,  ..., -3.4044,  0.5312,  0.7642]],</span><br><span class="line"></span><br><span class="line">        [[-4.8633,  2.5490, -6.3160,  ..., -1.7124, -2.2730,  0.7630],</span><br><span class="line">         [-5.1141,  2.4704, -4.4557,  ...,  2.4667, -0.3286,  0.8127],</span><br><span class="line">         [-8.8165,  1.9820, -6.3692,  ..., -1.9055,  2.4552, -6.4086],</span><br><span class="line">         [-6.2969,  2.9008, -1.2483,  ...,  0.1594, -4.0804,  0.0228]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h2 id="3-4-前馈全连接层PositionwiseFeedForward"><a href="#3-4-前馈全连接层PositionwiseFeedForward" class="headerlink" title="3.4 前馈全连接层PositionwiseFeedForward"></a>3.4 前馈全连接层PositionwiseFeedForward</h2><p>在Transformer中前馈全连接层就是具有两层线性层的全连接网络.</p>
<h3 id="（1）前馈全连接层作用"><a href="#（1）前馈全连接层作用" class="headerlink" title="（1）前馈全连接层作用"></a>（1）前馈全连接层作用</h3><p>考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.</p>
<script type="math/tex; mode=display">
FFN(x)=max(0, ~ xW_1+b_1)W_2+b_2</script><blockquote>
<p>Position就是序列中每个token，<code>Position-wise</code> 就是把MLP对每个token作用一次，且作用的是同一个MLP。</p>
</blockquote>
<h3 id="（2）代码实现"><a href="#（2）代码实现" class="headerlink" title="（2）代码实现"></a>（2）代码实现</h3><ul>
<li>实例化参数为<code>d_model</code>, <code>d_ff</code>, <code>dropout</code>, 分别代表词嵌入维度, 线性变换维度, 和置零比率.</li>
<li>输入参数x, 表示上层的输出.</li>
<li>输出是经过2层线性网络变换的特征表示.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PositionwiseFeedForward</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现前馈全连接层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, d_ff, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(PositionwiseFeedForward, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - d_model: 线性层的输入维度也是第二个线性层的输出维度，</span></span><br><span class="line"><span class="string">                    因为我们希望输入通过前馈全连接层后输入和输出的维度不变. </span></span><br><span class="line"><span class="string">           - d_ff: 第二个线性层的输入维度和第一个线性层的输出维度. </span></span><br><span class="line"><span class="string">           - dropout: 置0比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="variable language_">self</span>.w1 = nn.Linear(d_model, d_ff)</span><br><span class="line">        <span class="variable language_">self</span>.w2 = nn.Linear(d_ff, d_model)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(p=dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># 首先经过第一个线性层，然后使用Funtional中relu函数进行激活,</span></span><br><span class="line">        <span class="comment"># 之后再使用dropout进行随机置0，最后通过第二个线性层w2，返回最终结果.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.w2(<span class="variable language_">self</span>.dropout(<span class="variable language_">self</span>.w1(x).relu()))</span><br></pre></td></tr></table></figure>
<h3 id="（3）测试"><a href="#（3）测试" class="headerlink" title="（3）测试"></a>（3）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line"><span class="comment"># 线性变化的维度</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mha_res.shape)</span><br><span class="line">ff_result = ff(mha_res)</span><br><span class="line"><span class="built_in">print</span>(ff_result)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-2.8747, -0.1289, -0.4966,  ..., -1.2763, -2.0888,  0.3344],</span><br><span class="line">         [-1.2404,  0.3891,  1.3854,  ..., -1.2675, -1.8324,  0.2271],</span><br><span class="line">         [-1.6913,  1.2393,  0.1528,  ..., -0.7420, -2.5605,  0.9924],</span><br><span class="line">         [-3.4989,  1.4898, -0.7094,  ..., -1.1352, -1.9817,  0.4473]],</span><br><span class="line"></span><br><span class="line">        [[-2.0806,  0.1014,  1.4044,  ...,  0.1496, -2.4822, -1.5388],</span><br><span class="line">         [-3.5828,  0.3326,  1.2598,  ...,  0.8470, -2.5095,  0.1296],</span><br><span class="line">         [-2.7594, -0.2307,  1.4870,  ...,  1.1056, -2.3847, -1.6484],</span><br><span class="line">         [-1.1717, -1.2086,  0.6444,  ..., -0.5858, -3.3344, -0.9535]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h2 id="3-5-规范化层LayerNorm"><a href="#3-5-规范化层LayerNorm" class="headerlink" title="3.5 规范化层LayerNorm"></a>3.5 规范化层LayerNorm</h2><h3 id="（1）规范化层作用"><a href="#（1）规范化层作用" class="headerlink" title="（1）规范化层作用"></a>（1）规范化层作用</h3><p>它是所有深层网络模型都需要的标准网络层，因为<strong>随着网络层数的增加，通过多层的计算后参数可能开始出现过大或过小的情况</strong>，这样可能会导致学习过程出现异常，模型可能收敛非常的慢。 因此都<strong>会在一定层数后接规范化层进行数值的规范化</strong>，使其特征数值在合理范围内.</p>
<h3 id="（2）代码实现-1"><a href="#（2）代码实现-1" class="headerlink" title="（2）代码实现"></a>（2）代码实现</h3><ul>
<li>实例化参数有两个, <code>features</code>和<code>eps</code>，分别表示词嵌入特征大小，和一个足够小的数.</li>
<li>输入参数x代表来自上一层的输出.</li>
<li>输出就是经过规范化的特征表示.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LayerNorm</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现规范化层的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, features, eps=<span class="number">1e-6</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LayerNorm, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化参数</span></span><br><span class="line"><span class="string">            - features: 表示词嵌入的维度,</span></span><br><span class="line"><span class="string">            - eps: 一个足够小的数, 在规范化公式的分母中出现,防止分母为0.默认是1e-6.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 根据features的形状初始化两个参数张量a2，和b2，第一个初始化为1张量，</span></span><br><span class="line">        <span class="comment"># 也就是里面的元素都是1，第二个初始化为0张量，也就是里面的元素都是0，</span></span><br><span class="line">        <span class="comment"># 这两个张量就是规范化层的参数，因为直接对上一层得到的结果做规范化公式计算，</span></span><br><span class="line">        <span class="comment"># 将改变结果的正常表征，因此就需要有参数作为调节因子，使其即能满足规范化要求，</span></span><br><span class="line">        <span class="comment"># 又能不改变针对目标的表征.最后使用nn.parameter封装，代表他们是模型的参数。</span></span><br><span class="line">        <span class="variable language_">self</span>.a_2 = nn.Parameter(torch.ones(features))</span><br><span class="line">        <span class="variable language_">self</span>.b_2 = nn.Parameter(torch.zeros(features))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中，首先对输入变量x求其最后一个维度的均值，并保持输出维度与输入维度一致.</span></span><br><span class="line">        <span class="comment"># 接着再求最后一个维度的标准差，然后就是根据规范化公式，</span></span><br><span class="line">        <span class="comment"># 用x减去均值除以标准差获得规范化的结果，</span></span><br><span class="line">        <span class="comment"># 最后对结果乘以我们的缩放参数，即a2，*号代表同型点乘，即对应位置进行乘法操作，</span></span><br><span class="line">        <span class="comment"># 加上位移参数b2.返回即可.</span></span><br><span class="line">        mean = x.mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        std = x.std(-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.a_2 * (x - mean) / (std + <span class="variable language_">self</span>.eps) + <span class="variable language_">self</span>.b_2</span><br></pre></td></tr></table></figure>
<h3 id="（3）测试-1"><a href="#（3）测试-1" class="headerlink" title="（3）测试"></a>（3）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">features = d_model = <span class="number">512</span></span><br><span class="line">eps = <span class="number">1e-6</span></span><br><span class="line"></span><br><span class="line">x = ff_result</span><br><span class="line">ln = LayerNorm(features, eps)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(mha_res.shape)</span><br><span class="line">ln_result = ln(x)</span><br><span class="line"><span class="built_in">print</span>(ln_result)</span><br><span class="line"><span class="built_in">print</span>(ln_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-0.6481,  0.6222, -0.2731,  ...,  0.2868,  1.0175,  0.6720],</span><br><span class="line">         [-0.3056,  0.6657,  0.1862,  ...,  0.3972,  0.9435,  0.5340],</span><br><span class="line">         [-0.1928,  1.0572,  0.3111,  ..., -0.0410,  0.5555,  0.5671],</span><br><span class="line">         [-0.4334, -0.2361, -0.1477,  ...,  0.0923,  2.0700,  0.7032]],</span><br><span class="line"></span><br><span class="line">        [[ 1.0477, -0.7183,  0.0449,  ...,  1.6828,  0.3927,  0.5616],</span><br><span class="line">         [ 1.5125, -1.1870,  0.5266,  ...,  1.4665,  1.8670,  0.2973],</span><br><span class="line">         [ 0.8196, -2.3064, -0.2661,  ...,  1.0591,  1.1476, -0.2259],</span><br><span class="line">         [ 0.3523, -0.5912,  0.5318,  ...,  1.0312,  0.6859, -0.6222]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h3 id="（3）LayerNorm-和-BatchNorm"><a href="#（3）LayerNorm-和-BatchNorm" class="headerlink" title="（3）LayerNorm 和 BatchNorm"></a>（3）LayerNorm 和 BatchNorm</h3><h4 id="BatchNorm-简单的-2-维-情况（蓝色）"><a href="#BatchNorm-简单的-2-维-情况（蓝色）" class="headerlink" title="BatchNorm 简单的 2 维 情况（蓝色）"></a>BatchNorm 简单的 2 维 情况（蓝色）</h4><ul>
<li>每一行是一个样本 X，每一列是 一个 feature</li>
<li><strong>BatchNorm</strong>：每次把一列（1 个 feature）放在一个 mini-batch 里，均值变成 0， 方差变成 1 的标准化。</li>
<li><strong>How</strong>：（该列向量 - mini-batch 该列向量的均值）/（mini - batch 该列向量的方差）</li>
<li><strong>训练时</strong>：mini-batch 计算均值；</li>
<li><strong>测试时</strong>：使用全局均值、方差。</li>
<li>BatchNorm 还会学 $\lambda$$  \beta $，BatchNorm 可以通过学习将向量放缩成任意均值、任意方差 的一个向量。</li>
</ul>
<h4 id="Layernorm-（黄色）"><a href="#Layernorm-（黄色）" class="headerlink" title="Layernorm （黄色）"></a>Layernorm （黄色）</h4><ul>
<li>LayerNorm 跟 BatchNorm 在很多时候几乎是一样的，除了实现的方法有点不一样之外。</li>
<li><strong>LayerNorm</strong>：对每个样本做 Normalization（把每一行变成 均值为 0、方差为 1），不是对每个特征做 normalization。</li>
</ul>
<p><img src="image/image_jeoTTRYHi-.png" alt=""></p>
<h4 id="LayerNorm-在操作上-和-BatchNorm-二维输入-的关系"><a href="#LayerNorm-在操作上-和-BatchNorm-二维输入-的关系" class="headerlink" title="LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系"></a>LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系</h4><p>LayerNorm 整个把数据转置一次，放到 BatchNorm 里面出来的结果，再转置回去，基本上可以得到LayerNorm的结果。</p>
<h4 id="三维输入"><a href="#三维输入" class="headerlink" title="三维输入"></a>三维输入</h4><p>Transformer 和 RNN 里面：3 维输入。</p>
<ul>
<li>输入的是一个序列的样本，每个样本中有很多元素，是一个序列。</li>
<li>一个句子里面有 n 个词，每个词对应一个向量，+ 一个 batch —&gt; 3 维&#x20;</li>
<li>列 是 seq 序列长度 n；第 3 维 feature 是每个词额外的向量，d = 512 in transformer &#x20;</li>
</ul>
<p><strong>BatchNorm</strong> （蓝色线）：每次取一个特征，切一块，拉成一个向量，均值为 0 、方差为 1 的标准化。</p>
<p><strong>LayerNorm (橙色)</strong>：横着切</p>
<p><img src="image/image_nQvpI0YdcK.png" alt=""></p>
<h4 id="举例"><a href="#举例" class="headerlink" title="举例"></a>举例</h4><p>时序数据中 样本长度可能不一样。</p>
<p>举例分析：4个长度不一样的样本，0 填充到 max_len</p>
<p><strong>BatchNorm 切出来的结果</strong>（蓝色)</p>
<ul>
<li>BatchNorm 计算均值和方差，有效的是阴影部分，其余是 0</li>
<li>Mini-batch 的均值和方差：如果样本长度变化比较大的时候，每次计算小批量的均值和方差，均值和方差的抖动大。</li>
<li>局的均值和方差：测试时遇到一个特别长的全新样本 （最上方蓝色阴影块），训练时未见过，训练时计算的均值和方差可能不好用。</li>
</ul>
<p><strong>LayerNorm 切出来的结果</strong>（黄色）</p>
<ul>
<li>ayerNorm 每个样本自己算均值和方差，不需要存全局的均值和方差。</li>
<li>ayerNorm 更稳定，不管样本长还是短，均值和方差是在每个样本内计算。</li>
</ul>
<p><img src="image/image_P2kdW23lox.png" alt=""></p>
<h4 id="LayerNorm-和-BatchNorm-的例子理解：n-本书"><a href="#LayerNorm-和-BatchNorm-的例子理解：n-本书" class="headerlink" title="LayerNorm 和 BatchNorm 的例子理解：n 本书"></a>LayerNorm 和 BatchNorm 的例子理解：n 本书</h4><ul>
<li><strong>BatchNorm</strong>：n本书，每本书的第一页拿出来，根据 n 本书的第一页的字数均值 做 Norm</li>
<li><strong>LayerNorm</strong>：针对某一本书，这本书的每一页拿出来，根据次数每页的字数均值，自己做 Norm</li>
</ul>
<h2 id="3-6-子层连接结构SublayerConnection"><a href="#3-6-子层连接结构SublayerConnection" class="headerlink" title="3.6 子层连接结构SublayerConnection"></a>3.6 子层连接结构SublayerConnection</h2><p>如图所示，输入到每个子层以及规范化层的过程中，还使用了<strong>残差链接</strong>（跳跃连接），因此我们把这一部分结构整体叫做<strong>子层连接</strong>（代表子层及其链接结构），在每个编码器层中，都有两个子层，这两个子层加上周围的链接结构就形成了两个子层连接结构.</p>
<p><img src="image/image_-2lYYJLnn7.png" alt=""></p>
<h3 id="（1）代码实现"><a href="#（1）代码实现" class="headerlink" title="（1）代码实现"></a>（1）代码实现</h3><ul>
<li>类的初始化函数输入参数是<code>size</code>, <code>dropout</code>, 分别代表词嵌入大小和置零比率.</li>
<li>它的实例化对象输入参数是<code>x</code>, <code>sublayer</code>, 分别代表上一层输出以及子层的函数表示.</li>
<li>它的输出就是通过子层连接结构处理的输出.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SublayerConnection</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 子层连接结构的类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - size: 词嵌入维度的大小， </span></span><br><span class="line"><span class="string">            - dropout: 是对模型结构中的节点数进行随机抑制的比率， </span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(SublayerConnection, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 实例化了规范化对象self.norm</span></span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(size)</span><br><span class="line">        <span class="variable language_">self</span>.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, sublayer</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 前向逻辑函数中, 接收上一个层或者子层的输入作为第一个参数，</span></span><br><span class="line"><span class="string">           将该子层连接中的子层函数作为第二个参数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 我们首先对输出进行规范化，然后将结果传给子层处理，之后再对子层进行dropout操作，</span></span><br><span class="line">        <span class="comment"># 随机停止一些网络中神经元的作用，来防止过拟合. 最后还有一个add操作， </span></span><br><span class="line">        <span class="comment"># 因为存在跳跃连接，所以是将输入x与dropout后的子层输出结果相加作为最终的子层连接输出.</span></span><br><span class="line">        <span class="keyword">return</span> x + <span class="variable language_">self</span>.dropout(sublayer(<span class="variable language_">self</span>.norm(x)))</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-3"><a href="#（2）测试-3" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = pe_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">mha = MultiHeadAttention(head, embedding_dim, dropout)</span><br><span class="line"></span><br><span class="line">sublayer = <span class="keyword">lambda</span> x : mha(x, x, x, mask)</span><br><span class="line">size = <span class="number">512</span></span><br><span class="line"></span><br><span class="line">sc = SublayerConnection(size, dropout)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">sc_result = sc(x, sublayer)</span><br><span class="line"><span class="built_in">print</span>(sc_result)</span><br><span class="line"><span class="built_in">print</span>(sc_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[ -8.1109,   0.2217,   1.0194,  ...,  22.8768,  40.2057,  14.8928],</span><br><span class="line">         [ 31.5267,  33.8321,  29.1483,  ...,  17.2134, -16.1430,  -4.6771],</span><br><span class="line">         [  7.9553, -20.3707, -41.5156,  ..., -10.6798, -23.8106,   4.3206],</span><br><span class="line">         [-59.5128,  31.6802,   1.1462,  ..., -15.4225,  -2.6904,  45.0427]],</span><br><span class="line"></span><br><span class="line">        [[  6.9450, -18.4468, -42.5561,  ..., -10.3473, -23.6293,   4.3888],</span><br><span class="line">         [-38.2155,  46.7522, -22.8546,  ...,  28.8744,  -0.0767,  13.4702],</span><br><span class="line">         [ 31.4326,  33.3512,  29.2566,  ...,  17.6481, -16.0407,  -4.4314],</span><br><span class="line">         [-16.7070, -15.7774,  16.0646,  ..., -65.1326,   0.0000,   0.1910]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h2 id="3-7-编码器层EncoderLayer"><a href="#3-7-编码器层EncoderLayer" class="headerlink" title="3.7 编码器层EncoderLayer"></a>3.7 编码器层EncoderLayer</h2><p>作为编码器的组成单元,<strong> 每个编码器层完成一次对输入的特征提取过程</strong>, 即编码过程.</p>
<p><img src="image/image_WHGcn6ZmRg.png" alt=""></p>
<h3 id="（1）代码实现-1"><a href="#（1）代码实现-1" class="headerlink" title="（1）代码实现"></a>（1）代码实现</h3><ul>
<li>类的初始化函数共有4个, 第一个是<code>size</code>，其实就是我们词嵌入维度的大小. 第二个<code>self_attn</code>，之后我们将传入多头自注意力子层实例化对象, 并且是自注意力机制. 第三个是<code>feed_froward</code>, 之后我们将传入前馈全连接层实例化对象. 最后一个是置0比率<code>dropout</code>.</li>
<li>实例化对象的输入参数有2个，x代表来自上一层的输出, mask代表掩码张量.</li>
<li>它的输出代表经过整个编码层的特征表示.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 编码器层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - size: 词嵌入维度的大小，它也将作为我们编码器层的大小, </span></span><br><span class="line"><span class="string">            - self_attn: 传入多头自注意力子层实例化对象, 并且是自注意力机制, </span></span><br><span class="line"><span class="string">            - eed_froward: 传入前馈全连接层实例化对象,</span></span><br><span class="line"><span class="string">            - dropout: 置0比率</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.self_attn = self_attn</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = feed_forward</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="comment"># 如图所示, 编码器层中有两个子层连接结构, 所以使用clones函数进行克隆</span></span><br><span class="line">        <span class="variable language_">self</span>.sublayer = clones(SublayerConnection(size, dropout), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; x和mask，分别代表上一层的输出，和掩码张量mask</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment">#  首先通过第一个子层连接结构，其中包含多头自注意力子层，</span></span><br><span class="line">        <span class="comment"># 然后通过第二个子层连接结构，其中包含前馈全连接子层. 最后返回结果.</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x : <span class="variable language_">self</span>.self_attn(x, x, x, mask))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sublayer[<span class="number">1</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-4"><a href="#（2）测试-4" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">x = pe_result</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">self_attn = MultiHeadAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">el = EncoderLayer(size, self_attn, ff, dropout)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">el_result = el(x, mask)</span><br><span class="line"><span class="built_in">print</span>(el_result)</span><br><span class="line"><span class="built_in">print</span>(el_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-29.5621,  -2.0977,   2.6601,  ...,  33.7814, -41.1742, -10.7692],</span><br><span class="line">         [ 36.1439, -27.8296, -23.2643,  ...,  21.5115,  -4.6657,  14.0641],</span><br><span class="line">         [ -8.1926, -29.1385,  -2.2535,  ...,  -5.4215,   2.7747,  -0.4909],</span><br><span class="line">         [-25.4288, -14.2624, -22.5432,  ...,  -5.3338,   9.2610,   4.8978]],</span><br><span class="line"></span><br><span class="line">        [[ -9.4071, -27.6196,  -2.8486,  ...,  -5.1319,   2.3754,  14.1391],</span><br><span class="line">         [-15.6512,  -1.9466, -36.3869,  ...,  19.8941,  24.4394,  40.9649],</span><br><span class="line">         [ 36.5416, -28.4673, -22.8311,  ...,  21.5283,  -4.6554,  14.3312],</span><br><span class="line">         [-30.7237,  38.2961,  -8.4991,  ...,  57.8437,  11.2464,  -5.4290]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h2 id="3-8-编码器Encoder"><a href="#3-8-编码器Encoder" class="headerlink" title="3.8 编码器Encoder"></a>3.8 编码器Encoder</h2><p>编码器用于对输入进行指定的特征提取过程, 也称为编码, 由N个编码器层堆叠而成.</p>
<p><img src="image/image_hCQZA6gkw_.png" alt=""></p>
<h3 id="（1）实现-3"><a href="#（1）实现-3" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>类的初始化函数参数有两个，分别是<code>layer</code>和<code>N</code>，代表编码器层和编码器层的个数.</li>
<li>forward函数的输入参数也有两个, 和编码器层的forward相同, x代表上一层的输出, mask代码掩码张量.</li>
<li>编码器类的输出就是Transformer中编码器的特征提取表示, 它将成为解码器的输入的一部分.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Encoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现编码器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - layer: 编码器层</span></span><br><span class="line"><span class="string">            - N: 编码器层的个数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Encoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 使用clones函数克隆N个编码器层放在self.layers中</span></span><br><span class="line">        <span class="variable language_">self</span>.layers = clones(layer, N)</span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数的输入和编码器层相同, </span></span><br><span class="line"><span class="string">            - x: 上一层的输出, </span></span><br><span class="line"><span class="string">            - mask: 掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 首先就是对我们克隆的编码器层进行循环，每次都会得到一个新的x，</span></span><br><span class="line">        <span class="comment"># 这个循环的过程，就相当于输出的x经过了N个编码器层的处理. </span></span><br><span class="line">        <span class="comment"># 最后再通过规范化层的对象self.norm进行处理，最后返回结果. </span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-5"><a href="#（2）测试-5" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">x = pe_result</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">N = <span class="number">8</span></span><br><span class="line"></span><br><span class="line">self_attn = MultiHeadAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">layer = EncoderLayer(size, copy.deepcopy(self_attn), copy.deepcopy(ff), dropout)</span><br><span class="line"></span><br><span class="line">encoder = Encoder(layer, N)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">en_result = encoder(x, mask)</span><br><span class="line"><span class="built_in">print</span>(en_result)</span><br><span class="line"><span class="built_in">print</span>(en_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-0.7567, -1.2521, -0.2055,  ...,  0.8205, -1.2941, -2.0247],</span><br><span class="line">         [-0.0359,  0.9469,  0.0691,  ..., -0.6150,  0.4005, -0.1147],</span><br><span class="line">         [-1.3874,  0.9941,  0.1449,  ..., -0.3395,  1.3993, -2.0148],</span><br><span class="line">         [-0.5812, -0.6430,  2.1250,  ...,  1.8703, -0.1342,  0.6250]],</span><br><span class="line"></span><br><span class="line">        [[-1.4746,  1.0971, -0.0154,  ..., -0.3533,  1.4110, -1.8592],</span><br><span class="line">         [-0.5287, -1.6246,  0.7500,  ...,  0.4196,  0.8892,  0.2809],</span><br><span class="line">         [-0.1306,  0.8462,  0.0411,  ..., -0.5721,  0.4040, -0.1732],</span><br><span class="line">         [-0.8179, -1.3323, -0.7204,  ..., -0.4005,  0.5500, -0.0986]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h1 id="4-解码器"><a href="#4-解码器" class="headerlink" title="4.解码器"></a>4.解码器</h1><ul>
<li>由N个解码器层堆叠而成</li>
<li>每个解码器层由<strong>三个子层</strong>连接结构组成</li>
<li>第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接</li>
<li>第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接</li>
<li>第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接</li>
</ul>
<p><img src="image/image_0fQfOONvyp.png" alt=""></p>
<h2 id="4-1-解码器层DecoderLayer"><a href="#4-1-解码器层DecoderLayer" class="headerlink" title="4.1 解码器层DecoderLayer"></a>4.1 解码器层DecoderLayer</h2><p>作为解码器的组成单元, <strong>每个解码器层根据给定的输入向目标方向进行特征提取操作</strong>，即解码过程。</p>
<h3 id="（1）实现-4"><a href="#（1）实现-4" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>类的初始化函数的参数有5个, 分别是<code>size</code>，代表词嵌入的维度大小, 同时也代表解码器层的尺寸，第二个是<code>self_attn</code>，多头自注意力对象，也就是说这个注意力机制需要<code>Q=K=V</code>，第三个是<code>src_attn</code>，多头注意力对象，这里<code>Q!=K=V</code>， 第四个是前馈全连接层对象，最后就是<code>droupout</code>置0比率.</li>
<li>forward函数的参数有4个，分别是来自上一层的输入x，来自编码器层的语义存储变量mermory， 以及源数据掩码张量和目标数据掩码张量.</li>
<li>最终输出了由编码器输入和目标数据一同作用的特征提取结果.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoderLayer</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 解码器层</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, self_attn, src_attn, feed_forward, dropout</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - size：代表词嵌入的维度大小, 同时也代表解码器层的尺寸，</span></span><br><span class="line"><span class="string">            - self_attn： 多头自注意力对象，也就是说这个注意力机制需要Q=K=V， </span></span><br><span class="line"><span class="string">            - src_attn：多头注意力对象，这里Q!=K=V， </span></span><br><span class="line"><span class="string">            - feed_forward： 前馈全连接层对象，</span></span><br><span class="line"><span class="string">            - droupout：置0比率.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(DecoderLayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.self_attn = self_attn</span><br><span class="line">        <span class="variable language_">self</span>.src_attn = src_attn</span><br><span class="line">        <span class="variable language_">self</span>.feed_forward = feed_forward</span><br><span class="line">        <span class="variable language_">self</span>.dropout = dropout</span><br><span class="line">        <span class="comment"># 按照结构图使用clones函数克隆三个子层连接对象.</span></span><br><span class="line">        <span class="variable language_">self</span>.sublayer = clones(SublayerConnection(size, dropout), <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;forward函数中的参数有4个，</span></span><br><span class="line"><span class="string">            - x: 来自上一层的输入x，</span></span><br><span class="line"><span class="string">            - mermory: 来自编码器层的语义存储变量 </span></span><br><span class="line"><span class="string">            - src_mask: 源数据掩码张量</span></span><br><span class="line"><span class="string">            - tgt_mask: 目标数据掩码张量.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 将memory表示成m方便之后使用</span></span><br><span class="line">        m = memory</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 将x传入第一个子层结构，第一个子层结构的输入分别是x和self-attn函数，</span></span><br><span class="line">        <span class="comment"># 因为是自注意力机制，所以Q,K,V都是x，最后一个参数是目标数据掩码张量，</span></span><br><span class="line">        <span class="comment"># 这时要对目标数据进行遮掩，因为此时模型可能还没有生成任何目标数据，</span></span><br><span class="line">        <span class="comment"># 比如在解码器准备生成第一个字符或词汇时，我们其实已经传入了第一个字符以便计算损失，</span></span><br><span class="line">        <span class="comment"># 但是我们不希望在生成第一个字符时模型能利用这个信息，因此我们会将其遮掩，</span></span><br><span class="line">        <span class="comment"># 同样生成第二个字符或词汇时，</span></span><br><span class="line">        <span class="comment"># 模型只能使用第一个字符或词汇信息，第二个字符以及之后的信息都不允许被模型使用.</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">0</span>](x, <span class="keyword">lambda</span> x : <span class="variable language_">self</span>.self_attn(x, x, x, tgt_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 接着进入第二个子层，这个子层中常规的注意力机制，q是输入x; k，v是编码层输出memory， </span></span><br><span class="line">        <span class="comment"># 同样也传入source_mask，但是进行源数据遮掩的原因并非是抑制信息泄漏，</span></span><br><span class="line">        <span class="comment"># 而是遮蔽掉对结果没有意义的字符而产生的注意力值，</span></span><br><span class="line">        <span class="comment"># 以此提升模型效果和训练速度. 这样就完成了第二个子层的处理.</span></span><br><span class="line">        x = <span class="variable language_">self</span>.sublayer[<span class="number">1</span>](x, <span class="keyword">lambda</span> x : <span class="variable language_">self</span>.src_attn(x, m, m, src_mask))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 最后一个子层就是前馈全连接子层，经过它的处理后就可以返回结果</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.sublayer[<span class="number">2</span>](x, <span class="variable language_">self</span>.feed_forward)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-6"><a href="#（2）测试-6" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line"></span><br><span class="line">self_attn = src_attn = MultiHeadAttention(head, d_model, dropout)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line"><span class="comment"># x是来自目标数据的词嵌入表示, 但形式和源数据的词嵌入表示相同, 这里使用per充当.</span></span><br><span class="line">x = pe_result</span><br><span class="line"><span class="comment"># memory是来自编码器的输出</span></span><br><span class="line">memory = en_result</span><br><span class="line"><span class="comment"># 实际中source_mask和target_mask并不相同, 这里为了方便计算使他们都为mask</span></span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">src_mask = tgt_mask = mask</span><br><span class="line"></span><br><span class="line">dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)</span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">dl_result = dl(x, memory, src_mask, tgt_mask)</span><br><span class="line"><span class="built_in">print</span>(dl_result)</span><br><span class="line"><span class="built_in">print</span>(dl_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[  0.3128, -28.9028,  12.6505,  ..., -25.0090,   0.0671,  43.7430],</span><br><span class="line">         [-18.6592, -20.2816,  -0.2946,  ...,  30.0085, -15.1012, -14.5942],</span><br><span class="line">         [-19.6110,   0.1298,  -0.3384,  ..., -14.7287, -22.1352,  12.7321],</span><br><span class="line">         [-33.9689,   0.9680, -61.3009,  ..., -51.5810,  -8.8205,  -6.2392]],</span><br><span class="line"></span><br><span class="line">        [[-21.1268,  10.4200,   3.9523,  ..., -15.5449,  -0.2314,  12.6887],</span><br><span class="line">         [  6.3277,  27.3815,  43.6648,  ..., -21.2202, -48.8453, -20.5100],</span><br><span class="line">         [-18.5797, -21.5331, -38.0592,  ...,  29.7248, -15.0411,   0.4119],</span><br><span class="line">         [ 65.4318,  15.5895, -23.6869,  ..., -25.7464,  42.8896,  14.5587]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h2 id="4-2-解码器Decoder"><a href="#4-2-解码器Decoder" class="headerlink" title="4.2 解码器Decoder"></a>4.2 解码器Decoder</h2><p><strong>根据编码器的结果以及上一次预测的结果, 对下一次可能出现的’值’进行特征表示</strong>.</p>
<h3 id="（1）实现-5"><a href="#（1）实现-5" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>类的初始化函数的参数有两个，第一个就是解码器层<code>layer</code>，第二个是解码器层的个数<code>N</code>.</li>
<li>forward函数中的参数有4个，<code>x</code>代表目标数据的嵌入表示，<code>memory</code>是编码器层的输出，<code>src_mask</code>, <code>tgt_mask</code>代表源数据和目标数据的掩码张量.</li>
<li>输出解码过程的最终特征表示.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Decoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现解码器</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, layer, N</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - layer： 解码器层</span></span><br><span class="line"><span class="string">            - N：解码器层个数</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Decoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="comment"># 首先使用clones方法克隆了N个layer，然后实例化了一个规范化层. </span></span><br><span class="line">        <span class="comment"># 因为数据走过了所有的解码器层后最后要做规范化处理. </span></span><br><span class="line">        <span class="variable language_">self</span>.layers = clones(layer, N)</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.norm = LayerNorm(layer.size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, memory, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - x: 来自上一层的输入x，</span></span><br><span class="line"><span class="string">            - mermory: 来自编码器层的语义存储变量 </span></span><br><span class="line"><span class="string">            - src_mask: 源数据掩码张量</span></span><br><span class="line"><span class="string">            - tgt_mask: 目标数据掩码张量.</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 然后就是对每个层进行循环，当然这个循环就是变量x通过每一个层的处理，</span></span><br><span class="line">        <span class="comment"># 得出最后的结果，再进行一次规范化返回即可.</span></span><br><span class="line">        <span class="keyword">for</span> layer <span class="keyword">in</span> <span class="variable language_">self</span>.layers:</span><br><span class="line">            x = layer(x, memory, src_mask, tgt_mask)</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.norm(x)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-7"><a href="#（2）测试-7" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">size = d_model = <span class="number">512</span></span><br><span class="line">head = <span class="number">8</span></span><br><span class="line">d_ff = <span class="number">64</span></span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">attn = MultiHeadAttention(head, d_model)</span><br><span class="line">ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">layer = DecoderLayer(d_model, copy.deepcopy(attn), copy.deepcopy(attn), copy.deepcopy(ff), dropout)</span><br><span class="line"></span><br><span class="line">N = <span class="number">8</span></span><br><span class="line"><span class="comment"># 输入参数与解码器层的输入参数相同</span></span><br><span class="line">x = pe_result</span><br><span class="line">memory = en_result</span><br><span class="line">mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line">src_mask = tgt_mask = mask</span><br><span class="line"></span><br><span class="line">de = Decoder(layer, N)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(x.shape)</span><br><span class="line">de_result = de(x, memory, src_mask, tgt_mask)</span><br><span class="line"><span class="built_in">print</span>(de_result)</span><br><span class="line"><span class="built_in">print</span>(de_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[ 6.2605e-01, -1.6188e+00, -2.0886e+00,  ...,  1.0329e-01,</span><br><span class="line">          -9.3746e-01, -2.6656e-01],</span><br><span class="line">         [ 1.0500e-01, -2.6750e+00,  2.6044e+00,  ..., -5.4699e-01,</span><br><span class="line">          -7.5199e-02, -2.8667e-01],</span><br><span class="line">         [-1.6483e-02, -3.6539e-01, -3.1693e-01,  ..., -2.1838e-01,</span><br><span class="line">           5.6952e-01,  1.4017e+00],</span><br><span class="line">         [-4.9258e-01, -9.2657e-01, -1.3348e-01,  ..., -2.1710e-01,</span><br><span class="line">           1.3200e+00,  1.3176e+00]],</span><br><span class="line"></span><br><span class="line">        [[-2.6396e-03, -2.1476e-01, -2.9699e-01,  ..., -2.0103e-02,</span><br><span class="line">           5.1760e-01,  1.5096e+00],</span><br><span class="line">         [-4.1995e-01, -2.5207e+00, -1.1587e-01,  ..., -4.2679e-01,</span><br><span class="line">           7.7861e-01,  1.7993e-02],</span><br><span class="line">         [ 2.2358e-02, -2.7497e+00,  2.5735e+00,  ..., -4.2572e-01,</span><br><span class="line">          -4.1822e-01, -1.9397e-01],</span><br><span class="line">         [ 2.8288e-01,  4.5199e-01,  3.6352e-01,  ...,  2.1069e+00,</span><br><span class="line">          -8.3942e-01,  3.1137e-02]]], grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h1 id="5-输出部分"><a href="#5-输出部分" class="headerlink" title="5.输出部分"></a>5.输出部分</h1><ul>
<li>线性层</li>
<li>softmax层</li>
</ul>
<p><img src="image/image_GROtcgUedV.png" alt=""></p>
<h2 id="5-1-线性层"><a href="#5-1-线性层" class="headerlink" title="5.1 线性层"></a>5.1 线性层</h2><p>通过对上一步的线性变化得到指定维度的输出, 也就是<strong>转换维度</strong>的作用.</p>
<h2 id="5-2-softmax层"><a href="#5-2-softmax层" class="headerlink" title="5.2 softmax层"></a>5.2 softmax层</h2><p>使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.</p>
<h2 id="5-3-线性层和softmax层的类-Generator"><a href="#5-3-线性层和softmax层的类-Generator" class="headerlink" title="5.3 线性层和softmax层的类 Generator"></a>5.3 线性层和softmax层的类 Generator</h2><h3 id="（1）实现-6"><a href="#（1）实现-6" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>初始化函数的输入参数有两个, <code>d_model</code>代表词嵌入维度, <code>vocab_size</code>代表词表大小.</li>
<li>forward函数接受上一层的输出.</li>
<li>最终获得经过线性层和softmax层处理的结果.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Generator</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 生成器类</span></span><br><span class="line"><span class="string">        将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构</span></span><br><span class="line"><span class="string">        因此把类的名字叫做Generator, 生成器类</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, vocab_size</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - d_model: 词嵌入维度</span></span><br><span class="line"><span class="string">            - vocab_size: 词表的总大小</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(Generator, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.project = nn.Linear(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - x: 上一层的输出张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中, 首先使用上一步得到的self.project对x进行线性变化, </span></span><br><span class="line">        <span class="comment"># 然后使用F中已经实现的log_softmax进行的softmax处理.</span></span><br><span class="line">        <span class="comment"># 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.</span></span><br><span class="line">        <span class="comment"># log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, </span></span><br><span class="line">        <span class="comment"># 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.</span></span><br><span class="line">        <span class="keyword">return</span> log_softmax(<span class="variable language_">self</span>.project(x), dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-8"><a href="#（2）测试-8" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">d_model = <span class="number">512</span></span><br><span class="line">vocab_size = <span class="number">1000</span></span><br><span class="line"><span class="comment"># 输入x是上一层网络的输出, 我们使用来自解码器层的输出</span></span><br><span class="line">x = de_result</span><br><span class="line"></span><br><span class="line">gen = Generator(d_model, vocab_size)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">gen_result = gen(x)</span><br><span class="line"><span class="built_in">print</span>(gen_result)</span><br><span class="line"><span class="built_in">print</span>(gen_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.Size([2, 4, 512])</span><br><span class="line">tensor([[[-7.6141, -7.1042, -6.0669,  ..., -6.8734, -7.0652, -7.3009],</span><br><span class="line">         [-7.1238, -6.8357, -7.2459,  ..., -7.1225, -8.0530, -6.9380],</span><br><span class="line">         [-7.5266, -7.6293, -8.1937,  ..., -7.6398, -7.8350, -7.6071],</span><br><span class="line">         [-7.4972, -7.2781, -7.3025,  ..., -6.6653, -6.4995, -6.9529]],</span><br><span class="line"></span><br><span class="line">        [[-7.5171, -7.0331, -8.0956,  ..., -7.4018, -7.6130, -7.9539],</span><br><span class="line">         [-6.4713, -7.4932, -6.8351,  ..., -6.6046, -8.0713, -7.4401],</span><br><span class="line">         [-7.3482, -6.6409, -7.5268,  ..., -7.1031, -8.2056, -7.2852],</span><br><span class="line">         [-8.1393, -7.1066, -7.4460,  ..., -6.9347, -6.3511, -6.9577]]],</span><br><span class="line">       grad_fn=&lt;LogSoftmaxBackward&gt;)</span><br><span class="line">torch.Size([2, 4, 1000])</span><br></pre></td></tr></table></figure>
<h1 id="6-模型构建"><a href="#6-模型构建" class="headerlink" title="6.模型构建"></a>6.模型构建</h1><h2 id="6-1-编码器-解码器EncoderDecoder"><a href="#6-1-编码器-解码器EncoderDecoder" class="headerlink" title="6.1 编码器-解码器EncoderDecoder"></a>6.1 编码器-解码器EncoderDecoder</h2><p>大部分神经序列转换模型都有一个编码器-解码器结构。编码器把一个输入序列$(x_1, …, x_n)$映射到一个连续的表示 $z=(z_1, .., z_n)$中。解码器对z中的每个元素，生成输出序列$(y_1, …, y_m)$，一个时间步生成一个元素。在每一步中，模型都是自回归的，在生成下一个结果时，会将先前生成的结构加入输入序列来一起预测。（自回归模型的特点）</p>
<h3 id="（1）实现-7"><a href="#（1）实现-7" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>类的初始化函数传入5个参数, 分别是编码器对象, 解码器对象, 源数据嵌入函数, 目标数据嵌入函数, 以及输出部分的类别生成器对象.</li>
<li>类中共实现三个函数, <code>forward</code>, <code>encode</code>, <code>decode</code></li>
<li>forward是主要逻辑函数, 有四个参数, source代表源数据, target代表目标数据, source_mask和target_mask代表对应的掩码张量.</li>
<li>encode是编码函数, 以source和source_mask为参数.</li>
<li>decode是解码函数, 以memory即编码器的输出, source_mask, target, target_mask为参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">EncoderDecoder</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 实现编码器-解码器结构</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, encoder, decoder, src_embed, tgt_embed, generator</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 初始化</span></span><br><span class="line"><span class="string">            - encoder： 编码器对象</span></span><br><span class="line"><span class="string">            - decoder： 解码器对象</span></span><br><span class="line"><span class="string">            - src_embed：源数据嵌入函数</span></span><br><span class="line"><span class="string">            - tgt_embed： 目标数据嵌入函数</span></span><br><span class="line"><span class="string">            - generator：输出部分类别生成器</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="built_in">super</span>(EncoderDecoder, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.encoder = encoder</span><br><span class="line">        <span class="variable language_">self</span>.decoder = decoder</span><br><span class="line">        <span class="variable language_">self</span>.src_embed = src_embed</span><br><span class="line">        <span class="variable language_">self</span>.tgt_embed = tgt_embed</span><br><span class="line">        <span class="variable language_">self</span>.generator = generator</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, src, tgt, src_mask, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">            - src：源数据</span></span><br><span class="line"><span class="string">            - tgt：目标数据</span></span><br><span class="line"><span class="string">            - src_mask：源数据的掩码张量</span></span><br><span class="line"><span class="string">            - tgt_mask：目标数据的掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="comment"># 在函数中, 将source, source_mask传入编码函数, 得到结果后,</span></span><br><span class="line">        <span class="comment"># 与source_mask，target，和target_mask一同传给解码函数.</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decode(<span class="variable language_">self</span>.encode(src, src_mask), src_mask, tgt, tgt_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">encode</span>(<span class="params">self, src, src_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 编码函数</span></span><br><span class="line"><span class="string">            - src：源数据</span></span><br><span class="line"><span class="string">            - src_mask：源数据的掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.encoder(<span class="variable language_">self</span>.src_embed(src), src_mask)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decode</span>(<span class="params">self, memory, src_mask, tgt, tgt_mask</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot; 解码函数</span></span><br><span class="line"><span class="string">            - memory：经历编码器编码后的输出张量</span></span><br><span class="line"><span class="string">            - src_mask：源数据的掩码张量</span></span><br><span class="line"><span class="string">            - tgt：目标数据</span></span><br><span class="line"><span class="string">            - tgt_mask：目标数据的掩码张量</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.decoder(<span class="variable language_">self</span>.tgt_embed(tgt), memory, src_mask, tgt_mask)</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-9"><a href="#（2）测试-9" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">vocab_size = <span class="number">1000</span></span><br><span class="line">d_model = <span class="number">512</span></span><br><span class="line">encoder = en</span><br><span class="line">decoder = de</span><br><span class="line">src_embed = nn.Embedding(vocab_size, d_model)</span><br><span class="line">tgt_embed = nn.Embedding(vocab_size, d_model)</span><br><span class="line">generator = Generator(d_model, vocab_size)</span><br><span class="line"></span><br><span class="line">src = tgt = Variable(torch.LongTensor([[<span class="number">100</span>, <span class="number">2</span>, <span class="number">421</span>, <span class="number">508</span>], [<span class="number">491</span>, <span class="number">998</span>, <span class="number">1</span>, <span class="number">221</span>]]))</span><br><span class="line">src_mask = tgt_mask = Variable(torch.zeros(<span class="number">8</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">ed = EncoderDecoder(encoder, decoder, src_embed, tgt_embed, generator)</span><br><span class="line"></span><br><span class="line">ed_result = ed(src, tgt, src_mask, tgt_mask)</span><br><span class="line"><span class="built_in">print</span>(ed_result)</span><br><span class="line"><span class="built_in">print</span>(ed_result.shape)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tensor([[[ 0.3879,  0.1344, -0.5700,  ..., -0.2206, -0.7505, -0.3314],</span><br><span class="line">         [-0.7957,  0.8759, -0.5033,  ..., -1.3409, -1.4451, -0.3243],</span><br><span class="line">         [ 0.3513,  0.8083,  0.1246,  ..., -0.4443, -1.3551, -1.3547],</span><br><span class="line">         [ 0.0050,  0.6573, -1.1390,  ...,  0.1529, -1.5487, -0.8990]],</span><br><span class="line"></span><br><span class="line">        [[-0.1515,  1.9247, -0.0315,  ..., -0.5945, -2.7363, -1.2481],</span><br><span class="line">         [-0.6422,  1.5250,  0.7561,  ..., -1.4778, -1.2162, -2.2946],</span><br><span class="line">         [ 0.0163,  1.8034,  0.1408,  ..., -0.4170, -1.7017, -1.6474],</span><br><span class="line">         [ 0.2880, -0.0269, -0.1636,  ..., -0.7687, -1.3453, -0.8909]]],</span><br><span class="line">       grad_fn=&lt;AddBackward0&gt;)</span><br><span class="line">torch.Size([2, 4, 512])</span><br></pre></td></tr></table></figure>
<h2 id="6-2-Transformer模型make-model"><a href="#6-2-Transformer模型make-model" class="headerlink" title="6.2 Transformer模型make_model"></a>6.2 Transformer模型make_model</h2><h3 id="（1）实现-8"><a href="#（1）实现-8" class="headerlink" title="（1）实现"></a>（1）实现</h3><ul>
<li>有7个参数，分别是源数据特征(词汇)总数，目标数据特征(词汇)总数，编码器和解码器堆叠数，词向量映射维度，前馈全连接网络中变换矩阵的维度，多头注意力结构中的多头数，以及置零比率dropout.</li>
<li>该函数最后返回一个构建好的模型对象.</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">make_model</span>(<span class="params">src_vocab, tgt_vocab, N=<span class="number">6</span>, d_model=<span class="number">512</span>, d_ff=<span class="number">2048</span>, h=<span class="number">8</span>, dropout=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 构建transformer模型</span></span><br><span class="line"><span class="string">        - src_vocab: 源数据特征(词汇)总数</span></span><br><span class="line"><span class="string">        - tgt_vocab: 目标数据特征(词汇)总数</span></span><br><span class="line"><span class="string">        - N: 编码器和解码器堆叠数</span></span><br><span class="line"><span class="string">        - d_model: 词向量映射维度</span></span><br><span class="line"><span class="string">        - d_ff: 前馈全连接网络中变换矩阵的维度</span></span><br><span class="line"><span class="string">        - h : 多头注意力结构中的多头数</span></span><br><span class="line"><span class="string">        - dropout: 置零比率</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="comment"># 首先得到一个深度拷贝命令，接下来很多结构都需要进行深度拷贝，</span></span><br><span class="line">    <span class="comment"># 来保证他们彼此之间相互独立，不受干扰.</span></span><br><span class="line">    c = copy.deepcopy</span><br><span class="line">    <span class="comment"># 实例化了多头注意力类，得到对象attn</span></span><br><span class="line">    attn = MultiHeadAttention(h, d_model)</span><br><span class="line">    <span class="comment"># 然后实例化前馈全连接类，得到对象ff </span></span><br><span class="line">    ff = PositionwiseFeedForward(d_model, d_ff, dropout)</span><br><span class="line">    <span class="comment"># 实例化位置编码类，得到对象position</span></span><br><span class="line">    position = PositionalEncoding(d_model, dropout)</span><br><span class="line">    <span class="comment"># 根据结构图, 最外层是EncoderDecoder，在EncoderDecoder中，</span></span><br><span class="line">    <span class="comment"># 分别是编码器层，解码器层，源数据Embedding层和位置编码组成的有序结构，</span></span><br><span class="line">    <span class="comment"># 目标数据Embedding层和位置编码组成的有序结构，以及类别生成器层. </span></span><br><span class="line">    <span class="comment"># 在编码器层中有attention子层以及前馈全连接子层，</span></span><br><span class="line">    <span class="comment"># 在解码器层中有两个attention子层以及前馈全连接层.</span></span><br><span class="line">    model = EncoderDecoder(</span><br><span class="line">        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),</span><br><span class="line">        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),</span><br><span class="line">        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),</span><br><span class="line">        Generator(d_model, tgt_vocab)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 模型结构完成后，接下来就是初始化模型中的参数，比如线性层中的变换矩阵</span></span><br><span class="line">    <span class="comment"># 这里一但判断参数的维度大于1，则会将其初始化成一个服从均匀分布的矩阵，</span></span><br><span class="line">    <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters():</span><br><span class="line">        <span class="keyword">if</span> p.dim() &gt; <span class="number">1</span>:</span><br><span class="line">            nn.init.xavier_uniform_(p)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>
<h3 id="（2）测试-10"><a href="#（2）测试-10" class="headerlink" title="（2）测试"></a>（2）测试</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">src_vocab = <span class="number">11</span></span><br><span class="line">tgt_vocab = <span class="number">11</span></span><br><span class="line"></span><br><span class="line">model= make_model(src_vocab, tgt_vocab)</span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br></pre></td><td class="code"><pre><span class="line">EncoderDecoder(</span><br><span class="line">  (encoder): Encoder(</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (0): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (1): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (2): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (3): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (4): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (5): EncoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">    )</span><br><span class="line">    (norm): LayerNorm()</span><br><span class="line">  )</span><br><span class="line">  (decoder): Decoder(</span><br><span class="line">    (layers): ModuleList(</span><br><span class="line">      (0): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (1): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (2): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (3): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (4): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (2): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">        )</span><br><span class="line">      )</span><br><span class="line">      (5): DecoderLayer(</span><br><span class="line">        (self_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (src_attn): MultiHeadAttention(</span><br><span class="line">          (linears): ModuleList(</span><br><span class="line">            (0): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (1): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (2): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">            (3): Linear(in_features=512, out_features=512, bias=True)</span><br><span class="line">          )</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (feed_forward): PositionwiseFeedForward(</span><br><span class="line">          (w1): Linear(in_features=512, out_features=2048, bias=True)</span><br><span class="line">          (w2): Linear(in_features=2048, out_features=512, bias=True)</span><br><span class="line">          (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">        )</span><br><span class="line">        (sublayer): ModuleList(</span><br><span class="line">          (0): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">          (1): SublayerConnection(</span><br><span class="line">            (norm): LayerNorm()</span><br><span class="line">            (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">          )</span><br><span class="line">  (src_embed): Sequential(</span><br><span class="line">    (0): Embeddings(</span><br><span class="line">      (lut): Embedding(8316, 512)</span><br><span class="line">    )</span><br><span class="line">    (1): PositionalEncoding(</span><br><span class="line">      (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (tgt_embed): Sequential(</span><br><span class="line">    (0): Embeddings(</span><br><span class="line">      (lut): Embedding(6385, 512)</span><br><span class="line">    )</span><br><span class="line">    (1): PositionalEncoding(</span><br><span class="line">      (dropout): Dropout(p=0.1, inplace=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (generator): Generator(</span><br><span class="line">    (project): Linear(in_features=512, out_features=6385, bias=True)</span><br><span class="line">  )</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="6-3-Inference"><a href="#6-3-Inference" class="headerlink" title="6.3 Inference"></a>6.3 Inference</h2><p>在这里，我们用生成模型的预测。 我们尝试使用我们的transformer 来记住输入。 正如您将看到的那样，由于模型尚未训练，输出是随机生成的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inference_test</span>():</span><br><span class="line">    test_model = make_model(<span class="number">11</span>, <span class="number">11</span>, <span class="number">2</span>)</span><br><span class="line">    test_model.<span class="built_in">eval</span>()</span><br><span class="line">    src = torch.LongTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]])</span><br><span class="line">    src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    memory = test_model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">9</span>):</span><br><span class="line">        out = test_model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = test_model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.empty(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Example Untrained Model Prediction:&quot;</span>, ys)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_tests</span>():</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        inference_test()</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Example Untrained Model Prediction: tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[0, 3, 4, 4, 4, 4, 4, 4, 4, 4]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0, 10, 10, 10,  3,  2,  5,  7,  9,  6]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  4,  3,  6, 10, 10,  2,  6,  2,  2]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  9,  0,  1,  5, 10,  1,  5, 10,  6]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  1,  5,  1, 10,  1, 10, 10, 10, 10]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  1, 10,  9,  9,  9,  9,  9,  1,  5]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  3,  1,  5, 10, 10, 10, 10, 10, 10]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[ 0,  3,  5, 10,  5, 10,  4,  2,  4,  2]])</span><br><span class="line">Example Untrained Model Prediction: tensor([[0, 5, 6, 2, 5, 6, 2, 6, 2, 2]])</span><br></pre></td></tr></table></figure>
<h1 id="7-测试运行"><a href="#7-测试运行" class="headerlink" title="7.测试运行"></a>7.测试运行</h1><h2 id="7-1-copy任务简介"><a href="#7-1-copy任务简介" class="headerlink" title="7.1 copy任务简介"></a>7.1 copy任务简介</h2><p><strong>任务描述</strong>：针对数字序列进行学习, 学习的最终目标是使输出与输入的序列相同. 如输入[1, 5, 8, 9, 3], 输出也是[1, 5, 8, 9, 3].</p>
<p>任务意义：copy任务在模型基础测试中具有重要意义，因为copy操作对于模型来讲是一条明显规律, 因此模型能否在短时间内，小数据集中学会它，可以帮助我们断定模型所有过程是否正常，是否已具备基本学习能力。</p>
<h2 id="7-2-模型基本测试"><a href="#7-2-模型基本测试" class="headerlink" title="7.2 模型基本测试"></a>7.2 模型基本测试</h2><h3 id="（1）构建数据集生成器"><a href="#（1）构建数据集生成器" class="headerlink" title="（1）构建数据集生成器"></a>（1）构建数据集生成器</h3><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">data_generator</span>(<span class="params">V, batch_size, num_batch</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot; 该函数用于随机生成copy任务的数据</span></span><br><span class="line"><span class="string">        - V: 随机生成数字的最大值+1</span></span><br><span class="line"><span class="string">        - batch: 每次输送给模型更新一次参数的数据量</span></span><br><span class="line"><span class="string">        - num_batch: 一共输送num_batch次完成一轮</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_batch):</span><br><span class="line">        <span class="comment"># 在循环中使用randint方法随机生成[1, V)的整数, </span></span><br><span class="line">        <span class="comment"># 分布在(batch, 10)形状的矩阵中, </span></span><br><span class="line">        data = torch.randint(<span class="number">1</span>, V, size=(batch_size, <span class="number">10</span>))</span><br><span class="line">        <span class="comment"># 接着使数据矩阵中的第一列数字都为1, 这一列也就成为了起始标志列, </span></span><br><span class="line">        <span class="comment"># 当解码器进行第一次解码的时候, 会使用起始标志列作为输入.</span></span><br><span class="line">        data[:, <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">        <span class="comment"># 因为是copy任务, 所有source与target是完全相同的, 且数据样本作用变量不需要求梯度</span></span><br><span class="line">        <span class="comment"># 因此requires_grad设置为False</span></span><br><span class="line">        src = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        tgt = data.requires_grad_(<span class="literal">False</span>).clone().detach()</span><br><span class="line">        <span class="comment"># 使用Batch对source和target进行对应批次的掩码张量生成, 最后使用yield返回</span></span><br><span class="line">        <span class="keyword">yield</span> Batch(src, tgt, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">num_batch = <span class="number">30</span></span><br><span class="line">res = data_generator(V, batch_size, num_batch)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;generator object data_generator at 0x00000245A1AD4BA0&gt;</span><br></pre></td></tr></table></figure>
<h3 id="（2）获得Transformer模型及优化器和损失函数"><a href="#（2）获得Transformer模型及优化器和损失函数" class="headerlink" title="（2）获得Transformer模型及优化器和损失函数"></a>（2）获得Transformer模型及优化器和损失函数</h3><h4 id="实现-1"><a href="#实现-1" class="headerlink" title="实现"></a>实现</h4><p><strong>损失函数计算</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleLossCompute</span>:</span><br><span class="line">    <span class="string">&quot;损失函数计算&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, generator, criterion</span>):</span><br><span class="line">        <span class="variable language_">self</span>.generator = generator</span><br><span class="line">        <span class="variable language_">self</span>.criterion = criterion</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, x, y, norm</span>):</span><br><span class="line">        x = <span class="variable language_">self</span>.generator(x)</span><br><span class="line">        sloss = <span class="variable language_">self</span>.criterion(</span><br><span class="line">                x.contiguous().view(-<span class="number">1</span>, x.size(-<span class="number">1</span>)), y.contiguous().view(-<span class="number">1</span>)</span><br><span class="line">            ) / norm</span><br><span class="line">        <span class="keyword">return</span> sloss.data * norm, sloss</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>标签平滑</strong></p>
<p>在训练过程中，我们使用的label平滑的值为\epsilon_{ls}=0.1 (cite)。这让模型不易理解，因为模型学得更加不确定，但提高了准确性和BLEU得分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LabelSmoothing</span>(nn.Module):</span><br><span class="line">    <span class="string">&quot;Implement label smoothing.&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, size, padding_idx, smoothing=<span class="number">0.0</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(LabelSmoothing, <span class="variable language_">self</span>).__init__()</span><br><span class="line">        <span class="variable language_">self</span>.criterion = nn.KLDivLoss(reduction=<span class="string">&quot;sum&quot;</span>)</span><br><span class="line">        <span class="variable language_">self</span>.padding_idx = padding_idx</span><br><span class="line">        <span class="variable language_">self</span>.confidence = <span class="number">1.0</span> - smoothing</span><br><span class="line">        <span class="variable language_">self</span>.smoothing = smoothing</span><br><span class="line">        <span class="variable language_">self</span>.size = size</span><br><span class="line">        <span class="variable language_">self</span>.true_dist = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, target</span>):</span><br><span class="line">        <span class="keyword">assert</span> x.size(<span class="number">1</span>) == <span class="variable language_">self</span>.size</span><br><span class="line">        true_dist = x.data.clone()</span><br><span class="line">        true_dist.fill_(<span class="variable language_">self</span>.smoothing / (<span class="variable language_">self</span>.size - <span class="number">2</span>))</span><br><span class="line">        true_dist.scatter_(<span class="number">1</span>, target.data.unsqueeze(<span class="number">1</span>), <span class="variable language_">self</span>.confidence)</span><br><span class="line">        true_dist[:, <span class="variable language_">self</span>.padding_idx] = <span class="number">0</span></span><br><span class="line">        mask = torch.nonzero(target.data == <span class="variable language_">self</span>.padding_idx)</span><br><span class="line">        <span class="keyword">if</span> mask.dim() &gt; <span class="number">0</span>:</span><br><span class="line">            true_dist.index_fill_(<span class="number">0</span>, mask.squeeze(), <span class="number">0.0</span>)</span><br><span class="line">        <span class="variable language_">self</span>.true_dist = true_dist</span><br><span class="line">        <span class="keyword">return</span> <span class="variable language_">self</span>.criterion(x, true_dist.clone().detach())</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">20</span></span><br><span class="line">num_batch = <span class="number">30</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 获得模型优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(</span><br><span class="line">    model.parameters(), lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion)</span><br></pre></td></tr></table></figure>
<h4 id="标签平滑示例"><a href="#标签平滑示例" class="headerlink" title="标签平滑示例"></a>标签平滑示例</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用LabelSmoothing实例化一个crit对象.</span></span><br><span class="line"><span class="comment"># 第一个参数size代表目标数据的词汇总数, 也是模型最后一层得到张量的最后一维大小</span></span><br><span class="line"><span class="comment"># 这里是5说明目标词汇总数是5个. 第二个参数padding_idx表示要将那些tensor中的数字</span></span><br><span class="line"><span class="comment"># 替换成0, 一般padding_idx=0表示不进行替换. 第三个参数smoothing, 表示标签的平滑程度</span></span><br><span class="line"><span class="comment"># 如原来标签的表示值为1, 则平滑后它的值域变为[1-smoothing, 1+smoothing].</span></span><br><span class="line">crit = LabelSmoothing(size=<span class="number">5</span>, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定一个任意的模型最后输出预测结果和真实结果</span></span><br><span class="line">predict = Variable(torch.FloatTensor([[<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>],</span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>], </span><br><span class="line">                            [<span class="number">0</span>, <span class="number">0.2</span>, <span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标签的表示值是0，1，2</span></span><br><span class="line">target = Variable(torch.LongTensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将predict, target传入到对象中</span></span><br><span class="line">crit(predict, target)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制标签平滑图像</span></span><br><span class="line">plt.imshow(crit.true_dist)</span><br><span class="line">plt.waitforbuttonpress()</span><br></pre></td></tr></table></figure>
<p><img src="image/image_imustA3jpO.png" alt=""></p>
<p>标签平滑图像分析:</p>
<ul>
<li>我们目光集中在黄色小方块上, 它相对于横坐标横跨的值域就是标签平滑后的正向平滑值域, 我们可以看到大致是从0.5到2.5.</li>
<li>它相对于纵坐标横跨的值域就是标签平滑后的负向平滑值域, 我们可以看到大致是从-0.5到1.5, 总的值域空间由原来的[0, 2]变成了[-0.5, 2.5].</li>
</ul>
<h3 id="（3）运行模型进行训练和评估"><a href="#（3）运行模型进行训练和评估" class="headerlink" title="（3）运行模型进行训练和评估"></a>（3）运行模型进行训练和评估</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">80</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 获得模型优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), </span><br><span class="line">                             lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion)</span><br><span class="line">lr_scheduler = LambdaLR(optimizer=optimizer,</span><br><span class="line">    lr_lambda=<span class="keyword">lambda</span> step: rate(step, model_size=model.src_embed[<span class="number">0</span>].d_model, </span><br><span class="line">                                factor=<span class="number">1.0</span>, warmup=<span class="number">400</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">     <span class="comment"># 模型使用训练模式, 所有参数将被更新</span></span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">20</span>), model, loss,</span><br><span class="line">        optimizer, lr_scheduler, mode=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型使用评估模式, 参数将不会变化 </span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">5</span>), model, loss,</span><br><span class="line">        DummyOptimizer(), DummyScheduler(), mode=<span class="string">&quot;eval&quot;</span>)[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<h3 id="（4）使用模型进行贪婪解码"><a href="#（4）使用模型进行贪婪解码" class="headerlink" title="（4）使用模型进行贪婪解码"></a>（4）使用模型进行贪婪解码</h3><p>为简单起见，此代码使用贪婪解码预测翻译。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">greedy_decode</span>(<span class="params">model, src, src_mask, max_len, start_symbol</span>):</span><br><span class="line">    memory = model.encode(src, src_mask)</span><br><span class="line">    ys = torch.zeros(<span class="number">1</span>, <span class="number">1</span>).fill_(start_symbol).type_as(src.data)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(max_len - <span class="number">1</span>):</span><br><span class="line">        out = model.decode(</span><br><span class="line">            memory, src_mask, ys, subsequent_mask(ys.size(<span class="number">1</span>)).type_as(src.data)</span><br><span class="line">        )</span><br><span class="line">        prob = model.generator(out[:, -<span class="number">1</span>])</span><br><span class="line">        _, next_word = torch.<span class="built_in">max</span>(prob, dim=<span class="number">1</span>)</span><br><span class="line">        next_word = next_word.data[<span class="number">0</span>]</span><br><span class="line">        ys = torch.cat(</span><br><span class="line">            [ys, torch.zeros(<span class="number">1</span>, <span class="number">1</span>).type_as(src.data).fill_(next_word)], dim=<span class="number">1</span></span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">return</span> ys</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">V = <span class="number">11</span></span><br><span class="line">batch_size = <span class="number">80</span></span><br><span class="line">model = make_model(V, V, N=<span class="number">2</span>, d_model=<span class="number">256</span>, d_ff=<span class="number">512</span>, h=<span class="number">4</span>, dropout=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 获得模型优化器</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), </span><br><span class="line">                             lr=<span class="number">0.5</span>, betas=(<span class="number">0.9</span>, <span class="number">0.98</span>), eps=<span class="number">1e-9</span>)</span><br><span class="line"><span class="comment"># 使用LabelSmoothing获得标签平滑对象</span></span><br><span class="line">criterion = LabelSmoothing(size=V, padding_idx=<span class="number">0</span>, smoothing=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># 使用SimpleLossCompute获得利用标签平滑结果的损失计算方法</span></span><br><span class="line">loss = SimpleLossCompute(model.generator, criterion)</span><br><span class="line">lr_scheduler = LambdaLR(optimizer=optimizer,</span><br><span class="line">    lr_lambda=<span class="keyword">lambda</span> step: rate(step, model_size=model.src_embed[<span class="number">0</span>].d_model, </span><br><span class="line">                                factor=<span class="number">1.0</span>, warmup=<span class="number">400</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">     <span class="comment"># 模型使用训练模式, 所有参数将被更新</span></span><br><span class="line">    model.train()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">20</span>), model, loss,</span><br><span class="line">        optimizer, lr_scheduler, mode=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 模型使用评估模式, 参数将不会变化 </span></span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    run_epoch(data_generator(V, batch_size, <span class="number">5</span>), model, loss,</span><br><span class="line">        DummyOptimizer(), DummyScheduler(), mode=<span class="string">&quot;eval&quot;</span>)[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">src = torch.LongTensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">max_len = src.shape[<span class="number">1</span>]</span><br><span class="line">src_mask = torch.ones(<span class="number">1</span>, <span class="number">1</span>, max_len)</span><br><span class="line">result = greedy_decode(model, src, src_mask, max_len=max_len, start_symbol=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])</span><br></pre></td></tr></table></figure>
<h1 id="8-Transformer常见问题"><a href="#8-Transformer常见问题" class="headerlink" title="8.Transformer常见问题"></a>8.Transformer常见问题</h1><h2 id="8-1-Transformer和RNN"><a href="#8-1-Transformer和RNN" class="headerlink" title="8.1 Transformer和RNN"></a>8.1 Transformer和RNN</h2><p>最简单情况：没有残差连接、没有 layernorm、 attention 单头、没有投影。看和 RNN 区别</p>
<ul>
<li>attention 对输入做一个加权和，加权和 进入 point-wise MLP。（画了多个红色方块 MLP， 是一个权重相同的 MLP）</li>
<li>point-wise MLP 对 每个输入的点 做计算，得到输出。</li>
<li>attention 作用：把整个序列里面的信息抓取出来，做一次汇聚 aggregation</li>
</ul>
<p><img src="image/image_MkcmTq99sT.png" alt=""></p>
<p>RNN 跟 transformer <strong>异：如何传递序列的信</strong>息</p>
<p>RNN 是把上一个时刻的信息输出传入下一个时候做输入。Transformer 通过一个 attention 层，去全局的拿到整个序列里面信息，再用 MLP 做语义的转换。</p>
<p>RNN 跟 transformer <strong>同：语义空间的转换 + 关注点</strong></p>
<p>用一个线性层 or 一个 MLP 来做语义空间的转换。</p>
<p><strong>关注点</strong>：怎么有效的去使用序列的信息。</p>
<h2 id="8-2-一些细节"><a href="#8-2-一些细节" class="headerlink" title="8.2 一些细节"></a>8.2 一些细节</h2><p><strong>Transformer为何使用多头注意力机制？</strong>（为什么不使用一个头）</p>
<ul>
<li>多头保证了transformer可以注意到不同子空间的信息，捕捉到更加丰富的特征信息。可以类比CNN中同时使用<strong>多个滤波器</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征/信息。</strong></li>
</ul>
<p><strong>Transformer为什么Q和K使用不同的权重矩阵生成，为何不能使用同一个值进行自身的点乘？</strong> （注意和第一个问题的区别）</p>
<ul>
<li>使用Q/K/V不相同可以保证在不同空间进行投影，增强了表达能力，提高了泛化能力。</li>
<li>同时，由softmax函数的性质决定，实质做的是一个soft版本的arg max操作，得到的向量接近一个one-hot向量（接近程度根据这组数的数量级有所不同）。如果令Q=K，那么得到的模型大概率会得到一个类似单位矩阵的attention矩阵，<strong>这样self-attention就退化成一个point-wise线性映射</strong>。这样至少是违反了设计的初衷。</li>
</ul>
<p><strong>Transformer计算attention的时候为何选择点乘而不是加法？两者计算复杂度和效果上有什么区别？</strong></p>
<ul>
<li>K和Q的点乘是为了得到一个attention score 矩阵，用来对V进行提纯。K和Q使用了不同的W_k, W_Q来计算，可以理解为是在不同空间上的投影。正因为有了这种不同空间的投影，增加了表达能力，这样计算得到的attention score矩阵的泛化能力更高。</li>
<li>为了计算更快。矩阵加法在加法这一块的计算量确实简单，但是作为一个整体计算attention的时候相当于一个隐层，整体计算量和点积相似。在效果上来说，从实验分析，两者的效果和dk相关，dk越大，加法的效果越显著。</li>
</ul>
<p><strong>为什么在进行softmax之前需要对attention进行scaled（为什么除以dk的平方根）</strong>，并使用公式推导进行讲解</p>
<ul>
<li>这取决于softmax函数的特性，如果softmax内计算的数数量级太大，会输出近似one-hot编码的形式，导致梯度消失的问题，所以需要scale</li>
<li>那么至于为什么需要用维度开根号，假设向量q，k满足各分量独立同分布，均值为0，方差为1，那么qk点积均值为0，方差为dk，从统计学计算，若果让qk点积的方差控制在1，需要将其除以dk的平方根，是的softmax更加平滑</li>
</ul>
<p><strong>在计算attention score的时候如何对padding做mask操作？</strong></p>
<ul>
<li>padding位置置为负无穷(一般来说-1000就可以)，再对attention score进行相加。对于这一点，涉及到batch_size之类的，具体的大家可以看一下实现的源代码，位置在这里：<a href="https://link.zhihu.com/?target=https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720" title="https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720">https://github.com/huggingface/transformers/blob/aa6a29bc25b663e1311c5c4fb96b004cf8a6d2b6/src/transformers/modeling_bert.py#L720</a></li>
<li>padding位置置为负无穷而不是0，是因为后续在softmax时，$e^0=1$，不是0，计算会出现错误；而$e^{-\infty} = 0$，所以取负无穷</li>
</ul>
<p><strong>为什么在进行多头注意力的时候需要对每个head进行降维？</strong>（可以参考上面一个问题）</p>
<ul>
<li>将原有的<strong>高维空间转化为多个低维空间</strong>并再最后进行拼接，形成同样维度的输出，借此丰富特性信息<ul>
<li>基本结构：Embedding + Position Embedding，Self-Attention，Add + LN，FN，Add + LN</li>
</ul>
</li>
</ul>
<p><strong>为何在获取输入词向量之后需要对矩阵乘以embedding size的开方？意义是什么？</strong></p>
<ul>
<li>embedding matrix的初始化方式是xavier init，这种方式的方差是1/embedding size，因此乘以embedding size的开方使得embedding matrix的方差是1，在这个scale下可能更有利于embedding matrix的收敛。</li>
</ul>
<p><strong>简单介绍一下Transformer的位置编码？有什么意义和优缺点？</strong></p>
<ul>
<li>因为self-attention是位置无关的，无论句子的顺序是什么样的，通过self-attention计算的token的hidden embedding都是一样的，这显然不符合人类的思维。因此要有一个办法能够在模型中表达出一个token的位置信息，transformer使用了固定的positional encoding来表示token在句子中的绝对位置信息。</li>
</ul>
<p><strong>你还了解哪些关于位置编码的技术，各自的优缺点是什么？</strong>（参考上一题）</p>
<ul>
<li>相对位置编码（RPE）1.在计算attention score和weighted value时各加入一个可训练的表示相对位置的参数。2.在生成多头注意力时，把对key来说将绝对位置转换为相对query的位置3.复数域函数，已知一个词在某个位置的词向量表示，可以计算出它在任何位置的词向量表示。前两个方法是词向量+位置编码，属于亡羊补牢，复数域是生成词向量的时候即生成对应的位置信息。</li>
</ul>
<p><strong>简单讲一下Transformer中的残差结构以及意义。</strong></p>
<ul>
<li>就是ResNet的优点，解决梯度消失</li>
</ul>
<p><strong>为什么transformer块使用LayerNorm而不是BatchNorm？LayerNorm 在Transformer的位置是哪里？</strong></p>
<ul>
<li>LN：针对每个样本序列进行Norm，没有样本间的依赖。对一个序列的不同特征维度进行Norm</li>
<li>CV使用BN是认为channel维度的信息对cv方面有重要意义，如果对channel维度也归一化会造成不同通道信息一定的损失。而同理nlp领域认为句子长度不一致，并且各个batch的信息没什么关系，因此只考虑句子内信息的归一化，也就是LN。</li>
</ul>
<p><strong>简答讲一下BatchNorm技术，以及它的优缺点。</strong></p>
<ul>
<li>优点：<ul>
<li>第一个就是可以解决内部协变量偏移，简单来说训练过程中，各层分布不同，增大了学习难度，BN缓解了这个问题。当然后来也有论文证明BN有作用和这个没关系，而是可以使<strong>损失平面更加的平滑</strong>，从而加快的收敛速度。</li>
<li>第二个优点就是缓解了<strong>梯度饱和问题</strong>（如果使用sigmoid激活函数的话），加快收敛。</li>
</ul>
</li>
<li>缺点：<ul>
<li>第一个，batch_size较小的时候，效果差。这一点很容易理解。BN的过程，使用 整个batch中样本的均值和方差来模拟全部数据的均值和方差，在batch_size 较小的时候，效果肯定不好。</li>
<li>第二个缺点就是 BN 在RNN中效果比较差。</li>
</ul>
</li>
</ul>
<p><strong>简单描述一下Transformer中的前馈神经网络？使用了什么激活函数？相关优缺点？</strong></p>
<ul>
<li>ReLU</li>
</ul>
<script type="math/tex; mode=display">
FFN(x)=max(0,~ xW_1+b_1)W_2+b_2</script><p><strong>Encoder端和Decoder端是如何进行交互的？</strong>（在这里可以问一下关于seq2seq的attention知识）</p>
<ul>
<li>Cross Self-Attention，Decoder提供Q，Encoder提供K，V</li>
</ul>
<p><strong>Decoder阶段的多头自注意力和encoder的多头自注意力有什么区别？</strong>（为什么需要decoder自注意力需要进行 sequence mask)</p>
<ul>
<li>让输入序列只看到过去的信息，不能让他看到未来的信息</li>
</ul>
<p><strong>Transformer的并行化提现在哪个地方？Decoder端可以做并行化吗？</strong></p>
<ul>
<li>Encoder侧：模块之间是串行的，一个模块计算的结果做为下一个模块的输入，互相之前有依赖关系。从每个模块的角度来说，注意力层和前馈神经层这两个子模块单独来看都是可以并行的，不同单词之间是没有依赖关系的。</li>
<li>Decode引入sequence mask就是为了并行化训练，Decoder推理过程没有并行，只能一个一个的解码，很类似于RNN，这个时刻的输入依赖于上一个时刻的输出。</li>
</ul>
<p><strong>简单描述一下wordpiece model 和 byte pair encoding，有实际应用过吗？</strong></p>
<ul>
<li>传统词表示方法无法很好的处理未知或罕见的词汇（OOV问题），传统词tokenization方法不利于模型学习词缀之间的关系”</li>
<li>BPE（字节对编码）或二元编码是一种简单的数据压缩形式，其中最常见的一对连续字节数据被替换为该数据中不存在的字节。后期使用时需要一个替换表来重建原始数据。</li>
<li>优点：可以有效地平衡词汇表大小和步数（编码句子所需的token次数）。</li>
<li>缺点：基于贪婪和确定的符号替换，不能提供带概率的多个分片结果。</li>
</ul>
<p><strong>Transformer训练的时候学习率是如何设定的？Dropout是如何设定的，位置在哪里？Dropout 在测试的需要有什么需要注意的吗？</strong></p>
<ul>
<li>Dropout测试的时候记得对输入整体呈上dropout的比率</li>
</ul>
<p><strong>引申一个关于bert问题，bert的mask为何不学习transformer在attention处进行屏蔽score的技巧？</strong></p>
<ul>
<li>BERT和transformer的目标不一致，bert是语言的预训练模型，需要充分考虑上下文的关系，而transformer主要考虑句子中第i个元素与前i-1个元素的关系。</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/llms/transformer/2.Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90/">https://wdndev.github.io/llms/transformer/2.Transformer架构解析/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/llms/llms_idx/" title="LLMs 目录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-05</div><div class="title">LLMs 目录</div></div></a></div><div><a href="/paper_reading/2.1.Transformer/" title="论文精读 Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-31</div><div class="title">论文精读 Transformer</div></div></a></div><div><a href="/paper_reading/2.2.BERT/" title="论文精读 BERT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-06</div><div class="title">论文精读 BERT</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Transformer%E6%9E%B6%E6%9E%84%E8%A7%A3%E6%9E%90"><span class="toc-text">Transformer架构解析</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Transformer%E6%9E%B6%E6%9E%84%E5%9B%BE"><span class="toc-text">1.Transformer架构图</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-Transformer%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%9C%E7%94%A8"><span class="toc-text">1.1 Transformer模型的作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-Transformer%E6%80%BB%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="toc-text">1.2 Transformer总体架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86"><span class="toc-text">（1）输入部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86"><span class="toc-text">（2）输出部分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-text">（3）编码器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E8%A7%A3%E7%A0%81%E5%99%A8%E9%83%A8%E5%88%86"><span class="toc-text">（4）解码器部分:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E8%BE%93%E5%85%A5%E9%83%A8%E5%88%86Embeddings"><span class="toc-text">2.输入部分Embeddings</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E6%96%87%E6%9C%AC%E5%B5%8C%E5%85%A5%E5%B1%82"><span class="toc-text">2.1文本嵌入层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95"><span class="toc-text">（2）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%99%A8PositionalEncoding"><span class="toc-text">2.2 位置编码器PositionalEncoding</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-1"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-1"><span class="toc-text">（2）测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%BB%98%E5%88%B6%E8%AF%8D%E6%B1%87%E5%90%91%E9%87%8F%E4%B8%AD%E7%89%B9%E5%BE%81%E7%9A%84%E5%88%86%E5%B8%83%E6%9B%B2%E7%BA%BF"><span class="toc-text">（3）绘制词汇向量中特征的分布曲线</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E7%BC%96%E7%A0%81%E5%99%A8"><span class="toc-text">3.编码器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E6%8E%A9%E7%A0%81%E5%BC%A0%E9%87%8Fsubsequent-mask"><span class="toc-text">3.1 掩码张量subsequent_mask</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-2"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-2"><span class="toc-text">（2）测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%8E%A9%E7%A0%81%E5%BC%A0%E9%87%8F%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">（3）掩码张量的可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6Attention"><span class="toc-text">3.2 注意力机制Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">（1）什么是注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%A1%E7%AE%97%E8%A7%84%E5%88%99"><span class="toc-text">（2）注意力计算规则</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Q-K-V%E7%9A%84%E6%AF%94%E5%96%BB%E8%A7%A3%E9%87%8A"><span class="toc-text">（3）Q, K, V的比喻解释</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">（4）注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%885%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">（5）代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%886%EF%BC%89%E6%B5%8B%E8%AF%95"><span class="toc-text">（6）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6MultiHeadAttention"><span class="toc-text">3.3 多头注意力机制MultiHeadAttention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6"><span class="toc-text">（1）什么是多头注意力机制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E4%BD%9C%E7%94%A8"><span class="toc-text">（2）多头注意力作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%AE%9E%E7%8E%B0"><span class="toc-text">（3）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E6%B5%8B%E8%AF%95"><span class="toc-text">（4）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E5%89%8D%E9%A6%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82PositionwiseFeedForward"><span class="toc-text">3.4 前馈全连接层PositionwiseFeedForward</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%89%8D%E9%A6%88%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82%E4%BD%9C%E7%94%A8"><span class="toc-text">（1）前馈全连接层作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">（2）代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%B5%8B%E8%AF%95"><span class="toc-text">（3）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E8%A7%84%E8%8C%83%E5%8C%96%E5%B1%82LayerNorm"><span class="toc-text">3.5 规范化层LayerNorm</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E8%A7%84%E8%8C%83%E5%8C%96%E5%B1%82%E4%BD%9C%E7%94%A8"><span class="toc-text">（1）规范化层作用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-text">（2）代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%B5%8B%E8%AF%95-1"><span class="toc-text">（3）测试</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89LayerNorm-%E5%92%8C-BatchNorm"><span class="toc-text">（3）LayerNorm 和 BatchNorm</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BatchNorm-%E7%AE%80%E5%8D%95%E7%9A%84-2-%E7%BB%B4-%E6%83%85%E5%86%B5%EF%BC%88%E8%93%9D%E8%89%B2%EF%BC%89"><span class="toc-text">BatchNorm 简单的 2 维 情况（蓝色）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Layernorm-%EF%BC%88%E9%BB%84%E8%89%B2%EF%BC%89"><span class="toc-text">Layernorm （黄色）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LayerNorm-%E5%9C%A8%E6%93%8D%E4%BD%9C%E4%B8%8A-%E5%92%8C-BatchNorm-%E4%BA%8C%E7%BB%B4%E8%BE%93%E5%85%A5-%E7%9A%84%E5%85%B3%E7%B3%BB"><span class="toc-text">LayerNorm 在操作上 和 BatchNorm (二维输入) 的关系</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E7%BB%B4%E8%BE%93%E5%85%A5"><span class="toc-text">三维输入</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BE%E4%BE%8B"><span class="toc-text">举例</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#LayerNorm-%E5%92%8C-BatchNorm-%E7%9A%84%E4%BE%8B%E5%AD%90%E7%90%86%E8%A7%A3%EF%BC%9An-%E6%9C%AC%E4%B9%A6"><span class="toc-text">LayerNorm 和 BatchNorm 的例子理解：n 本书</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-6-%E5%AD%90%E5%B1%82%E8%BF%9E%E6%8E%A5%E7%BB%93%E6%9E%84SublayerConnection"><span class="toc-text">3.6 子层连接结构SublayerConnection</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">（1）代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-3"><span class="toc-text">（2）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-7-%E7%BC%96%E7%A0%81%E5%99%A8%E5%B1%82EncoderLayer"><span class="toc-text">3.7 编码器层EncoderLayer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0-1"><span class="toc-text">（1）代码实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-4"><span class="toc-text">（2）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-8-%E7%BC%96%E7%A0%81%E5%99%A8Encoder"><span class="toc-text">3.8 编码器Encoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-3"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-5"><span class="toc-text">（2）测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E8%A7%A3%E7%A0%81%E5%99%A8"><span class="toc-text">4.解码器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E8%A7%A3%E7%A0%81%E5%99%A8%E5%B1%82DecoderLayer"><span class="toc-text">4.1 解码器层DecoderLayer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-4"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-6"><span class="toc-text">（2）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E8%A7%A3%E7%A0%81%E5%99%A8Decoder"><span class="toc-text">4.2 解码器Decoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-5"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-7"><span class="toc-text">（2）测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E8%BE%93%E5%87%BA%E9%83%A8%E5%88%86"><span class="toc-text">5.输出部分</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E7%BA%BF%E6%80%A7%E5%B1%82"><span class="toc-text">5.1 线性层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-softmax%E5%B1%82"><span class="toc-text">5.2 softmax层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E7%BA%BF%E6%80%A7%E5%B1%82%E5%92%8Csoftmax%E5%B1%82%E7%9A%84%E7%B1%BB-Generator"><span class="toc-text">5.3 线性层和softmax层的类 Generator</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-6"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-8"><span class="toc-text">（2）测试</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA"><span class="toc-text">6.模型构建</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E7%BC%96%E7%A0%81%E5%99%A8-%E8%A7%A3%E7%A0%81%E5%99%A8EncoderDecoder"><span class="toc-text">6.1 编码器-解码器EncoderDecoder</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-7"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-9"><span class="toc-text">（2）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Transformer%E6%A8%A1%E5%9E%8Bmake-model"><span class="toc-text">6.2 Transformer模型make_model</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%AE%9E%E7%8E%B0-8"><span class="toc-text">（1）实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%B5%8B%E8%AF%95-10"><span class="toc-text">（2）测试</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-Inference"><span class="toc-text">6.3 Inference</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E6%B5%8B%E8%AF%95%E8%BF%90%E8%A1%8C"><span class="toc-text">7.测试运行</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-copy%E4%BB%BB%E5%8A%A1%E7%AE%80%E4%BB%8B"><span class="toc-text">7.1 copy任务简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E6%A8%A1%E5%9E%8B%E5%9F%BA%E6%9C%AC%E6%B5%8B%E8%AF%95"><span class="toc-text">7.2 模型基本测试</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%9E%84%E5%BB%BA%E6%95%B0%E6%8D%AE%E9%9B%86%E7%94%9F%E6%88%90%E5%99%A8"><span class="toc-text">（1）构建数据集生成器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0"><span class="toc-text">实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B5%8B%E8%AF%95"><span class="toc-text">测试</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E8%8E%B7%E5%BE%97Transformer%E6%A8%A1%E5%9E%8B%E5%8F%8A%E4%BC%98%E5%8C%96%E5%99%A8%E5%92%8C%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-text">（2）获得Transformer模型及优化器和损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0-1"><span class="toc-text">实现</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91%E7%A4%BA%E4%BE%8B"><span class="toc-text">标签平滑示例</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%AE%AD%E7%BB%83%E5%92%8C%E8%AF%84%E4%BC%B0"><span class="toc-text">（3）运行模型进行训练和评估</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E4%BD%BF%E7%94%A8%E6%A8%A1%E5%9E%8B%E8%BF%9B%E8%A1%8C%E8%B4%AA%E5%A9%AA%E8%A7%A3%E7%A0%81"><span class="toc-text">（4）使用模型进行贪婪解码</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Transformer%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-text">8.Transformer常见问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-Transformer%E5%92%8CRNN"><span class="toc-text">8.1 Transformer和RNN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82"><span class="toc-text">8.2 一些细节</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>