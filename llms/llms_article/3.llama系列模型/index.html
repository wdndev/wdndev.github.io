<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLaMA系列模型架构 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.LLama1.1 简介Open and Efficient Foundation Language Models (Open但没完全Open的LLaMA)&#x20; 2023年2月，Meta（原Facebook）推出了LLaMA大模型，使用了1.4T token进行训练，虽然最大模型只有65B，但在相关评测任务上的效果可以媲美甚至超过千亿级大模型，被认为是近期开源大模型百花⻬放的开端之一，“">
<meta property="og:type" content="article">
<meta property="og:title" content="LLaMA系列模型架构">
<meta property="og:url" content="https://wdndev.github.io/llms/llms_article/3.llama%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="1.LLama1.1 简介Open and Efficient Foundation Language Models (Open但没完全Open的LLaMA)&#x20; 2023年2月，Meta（原Facebook）推出了LLaMA大模型，使用了1.4T token进行训练，虽然最大模型只有65B，但在相关评测任务上的效果可以媲美甚至超过千亿级大模型，被认为是近期开源大模型百花⻬放的开端之一，“">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2023-12-04T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:10.225Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="LLMs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/llms/llms_article/3.llama%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLaMA系列模型架构',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">LLaMA系列模型架构</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-04T16:00:00.000Z" title="Created 2023-12-05 00:00:00">2023-12-05</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:10.225Z" title="Updated 2025-11-02 07:46:10">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLMs/">LLMs</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">2.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>12min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLaMA系列模型架构"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="1-LLama"><a href="#1-LLama" class="headerlink" title="1.LLama"></a>1.LLama</h1><h2 id="1-1-简介"><a href="#1-1-简介" class="headerlink" title="1.1 简介"></a>1.1 简介</h2><p>Open and Efficient Foundation Language Models (Open但没完全Open的LLaMA)&#x20;</p>
<p>2023年2月，Meta（原Facebook）推出了LLaMA大模型，使用了1.4T token进行训练，虽然最大模型只有65B，但在相关评测任务上的效果可以媲美甚至超过千亿级大模型，被认为是近期开源大模型百花⻬放的开端之一，“羊驼”系列模型及其生态快速发展。</p>
<p>LLaMA 所采用的 Transformer 结构和细节，与标准的 Transformer 架构不同的地方包括采用了<strong>前置层归一化（Pre-normalization）</strong>并使用 <strong>RMSNorm 归一化函数</strong> （Normalizing Function）、激活函数更换为<strong> SwiGLU</strong>，并使用了<strong>旋转位置嵌入（RoP）</strong>，整体 Transformer 架构与 GPT-2 类似。</p>
<p><img src="image/image_V-mY0ArIbu.png" alt=""></p>
<h2 id="1-2-RMSNorm归一化函数"><a href="#1-2-RMSNorm归一化函数" class="headerlink" title="1.2 RMSNorm归一化函数"></a>1.2 RMSNorm归一化函数</h2><p><strong>为了使得模型训练过程更加稳定</strong>，GPT-2 相较于 GPT 就引入了<strong>前置层归一化方法</strong>，将第一个层归一化移动到多头自注意力层之前，第二个层归一化也移动到了全连接层之前，同时残差连接的位置也调整到了多头自注意力层与全连接层之后。层归一化中也采用了 <strong>RMSNorm 归一化函数</strong>。 针对输入向量 aRMSNorm 函数计算公式如下</p>
<script type="math/tex; mode=display">
R M S(a)=\sqrt{\frac{1}{n} \sum_{i=1}^{n} a_{i}^{2}}</script><script type="math/tex; mode=display">
\bar{a}_{i}=\frac{a_{i}}{R M S(\boldsymbol{a})}</script><p>此外，RMSNorm 还可以引入可学习的缩放因子 $ g<em><br>i  $和偏移参数 $b_i$，从而得到 $\bar{a}</em>{i}=\frac{a<em>{i}}{\operatorname{RMS}(\boldsymbol{a})} g</em>{i}+b_{i}$。 RMSNorm 在 HuggingFace Transformer 库中代码实现如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LlamaRMSNorm</span>(nn.Module):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size, eps=<span class="number">1e-6</span></span>): </span><br><span class="line">    <span class="string">&quot;&quot;&quot; </span></span><br><span class="line"><span class="string">    LlamaRMSNorm is equivalent to T5LayerNorm </span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span> </span><br><span class="line">    <span class="built_in">super</span>().__init__() </span><br><span class="line">    <span class="variable language_">self</span>.weight = nn.Parameter(torch.ones(hidden_size)) </span><br><span class="line">    <span class="variable language_">self</span>.variance_epsilon = eps <span class="comment"># eps 防止取倒数之后分母为 0 </span></span><br><span class="line">  </span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, hidden_states</span>): </span><br><span class="line">    input_dtype = hidden_states.dtype </span><br><span class="line">    variance = hidden_states.to(torch.float32).<span class="built_in">pow</span>(<span class="number">2</span>).mean(-<span class="number">1</span>, keepdim=<span class="literal">True</span>) </span><br><span class="line">    hidden_states = hidden_states * torch.rsqrt(variance + <span class="variable language_">self</span>.variance_epsilon) <span class="comment"># weight 是末尾乘的可训练参数, 即 g_i </span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> (<span class="variable language_">self</span>.weight * hidden_states).to(input_dtype)</span><br></pre></td></tr></table></figure>
<h2 id="1-3-SwiGLU计划函数"><a href="#1-3-SwiGLU计划函数" class="headerlink" title="1.3 SwiGLU计划函数"></a>1.3 SwiGLU计划函数</h2><p>SwiGLU激活函数是相较于 ReLU 函数在大部分评测中都有不少提升。在 LLaMA 中全连接层 使用带有 SwiGLU 激活函数的 FFN（Position-wise Feed-Forward Network）的计算公式如下：</p>
<script type="math/tex; mode=display">
\operatorname{FFN}_{\text {SwiGLU }}\left(\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{V}, \boldsymbol{W}_{2}\right)=\operatorname{SwiGLU}(\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{V}) \boldsymbol{W}_{2}</script><script type="math/tex; mode=display">
\operatorname{SwiGLU}(\boldsymbol{x}, \boldsymbol{W}, \boldsymbol{V})=\operatorname{Swish}_{\beta}(x \boldsymbol{W}) \otimes \boldsymbol{x} \boldsymbol{V}</script><script type="math/tex; mode=display">
\operatorname{Swish}_{\beta}(\boldsymbol{x})=\boldsymbol{x} \sigma(\boldsymbol{\beta} \boldsymbol{x})</script><p>其中，$σ(x)$ 是 Sigmoid 函数。下图给出了 Swish 激活函数在参数 $β$ 不同取值下的形状。可以看 到当 $β$ 趋近于 0 时，Swish 函数趋近于线性函数 $y = x$，当 $ β  $趋近于无穷大时，Swish 函数趋近于 ReLU 函数，$β$ 取值为 1 时，Swish 函数是光滑且非单调。在 HuggingFace 的 Transformer 库中 Swish1 函数使用 silu 函数代替。</p>
<p><img src="image/image_3VP6MMucO_.png" alt=""></p>
<p>LLaMA中直接将FFN中的ReLU替换为SwiGLU，并将维度放缩为$(2/3) ⋅ 4d$</p>
<p><img src="image/image_GfepsrgfTq.png" alt=""></p>
<h2 id="1-4-旋转位置嵌入（RoPE）"><a href="#1-4-旋转位置嵌入（RoPE）" class="headerlink" title="1.4 旋转位置嵌入（RoPE）"></a>1.4 旋转位置嵌入（RoPE）</h2><p>在位置编码上，使用旋转位置嵌入（Rotary Positional Embeddings，RoPE）代替原有的绝 对位置编码。RoPE 借助了<strong>复数的思想</strong>，出发点是<strong>通过绝对位置编码的方式实现相对位置编码</strong>。其目标是通过下述运算来给 <code>q</code>，<code>k</code> 添加绝对位置信息：</p>
<script type="math/tex; mode=display">
\tilde{\boldsymbol{q}}_{m}=f(\boldsymbol{q}, m), \tilde{\boldsymbol{k}}_{n}=f(\boldsymbol{k}, n)</script><p>经过上述操作后，$\tilde{\boldsymbol{q}}<em>{m}$和$\tilde{\boldsymbol{k}}</em>{n}$就带有位置m和n的绝对位置信息。</p>
<p>最终可以得到二维情况下用复数表示的 RoPE：</p>
<script type="math/tex; mode=display">
f(\boldsymbol{q}, m)=R_{f}(\boldsymbol{q}, m) e^{i \Theta_{f}(\boldsymbol{q}, m)}=\|\boldsymbol{q}\| e^{i(\Theta(\boldsymbol{q})+m \theta)}=\boldsymbol{q} e^{i m \theta}</script><p>根据复数乘法的几何意义，上述变换实际上是对应向量旋转，所以位置向量称为“旋转式位置编 码”。还可以使用矩阵形式表示</p>
<script type="math/tex; mode=display">
f(\boldsymbol{q}, m)=\left(\begin{array}{cc}\cos m \theta & -\sin \cos m \theta \\ \sin m \theta & \cos m \theta\end{array}\right)\left(\begin{array}{l}\boldsymbol{q}_{0} \\ \boldsymbol{q}_{1}\end{array}\right)</script><p>根据内积满足线性叠加的性质，任意偶数维的 RoPE，都可以表示为二维情形的拼接，即：</p>
<script type="math/tex; mode=display">
f(\boldsymbol{q}, m)=\underbrace{\left(\begin{array}{ccccccc}\cos m \theta_{0} & -\sin m \theta_{0} & 0 & 0 & \cdots & 0 & 0 \\ \sin m \theta_{0} & \cos m \theta_{0} & 0 & 0 & \cdots & 0 & 0 \\ 0 & 0 & \cos m \theta_{1} & -\sin m \theta_{1} & \cdots & 0 & 0 \\ 0 & 0 & \sin m \theta_{1} & \cos m \theta_{1} & \cdots & 0 & 0 \\ \cdots & \cdots & \cdots & \cdots & \ddots & \cdots & \cdots \\ 0 & 0 & 0 & 0 & \cdots & \cos m \theta_{d / 2-1} & -\sin m \theta_{d / 2-1} \\ 0 & 0 & 0 & 0 & \cdots & \sin m \theta_{d / 2-1} & \cos m \theta_{d / 2-1}\end{array}\right)}_{\boldsymbol{R}_{d}}\left(\begin{array}{c}\boldsymbol{q}_{0} \\ \boldsymbol{q}_{1} \\ \boldsymbol{q}_{2} \\ \boldsymbol{q}_{3} \\ \cdots \\ \boldsymbol{q}_{d-2} \\ \boldsymbol{q}_{d-1}\end{array}\right)</script><p><img src="image/image_FT9IKVPWqd.png" alt=""></p>
<p>RoPE 在 HuggingFace Transformer 库中代码实现如下所示：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">precompute_freqs_cis</span>(<span class="params">dim: <span class="built_in">int</span>, end: <span class="built_in">int</span>, constant: <span class="built_in">float</span> = <span class="number">10000.0</span></span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    计算cos和sin的值，cos值在实部，sin值在虚部，类似于 cosx+j*sinx</span></span><br><span class="line"><span class="string">    :param dim: q,k,v的最后一维，一般为emb_dim/head_num</span></span><br><span class="line"><span class="string">    :param end: 句长length</span></span><br><span class="line"><span class="string">    :param constant： 这里指10000</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    复数计算 torch.polar(a, t)输出， a*(cos(t)+j*sin(t))</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="comment"># freqs: 计算 1/(10000^(2i/d) )，将结果作为参数theta</span></span><br><span class="line">    <span class="comment"># 形式化为 [theta_0, theta_1, ..., theta_(d/2-1)]</span></span><br><span class="line">    freqs = <span class="number">1.0</span> / (constant ** (torch.arange(<span class="number">0</span>, dim, <span class="number">2</span>)[: (dim // <span class="number">2</span>)].<span class="built_in">float</span>() / dim)) <span class="comment"># [d/2]</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算m</span></span><br><span class="line">    t = torch.arange(end, device=freqs.device)  <span class="comment"># [length]</span></span><br><span class="line">    <span class="comment"># 计算m*theta</span></span><br><span class="line">    freqs = torch.outer(t, freqs).<span class="built_in">float</span>()  <span class="comment"># [length, d/2]</span></span><br><span class="line">    <span class="comment"># freqs形式化为 [m*theta_0, m*theta_1, ..., m*theta_(d/2-1)],其中 m=0,1,...,length-1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算cos(m*theta)+j*sin(m*theta)</span></span><br><span class="line">    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  <span class="comment"># complex64</span></span><br><span class="line">    <span class="comment"># freqs_cis: [cos(m*theta_0)+j*sin(m*theta_0),  cos(m*theta_1)+j*sin(m*theta_1),), ..., cos(m*theta_(d/2-1))+j*sin(m*theta_(d/2-1))]</span></span><br><span class="line">    <span class="comment"># 其中j为虚数单位， m=0,1,...,length-1</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis <span class="comment"># [length, d/2]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">reshape_for_broadcast</span>(<span class="params">freqs_cis: torch.Tensor, x: torch.Tensor</span>):</span><br><span class="line">    ndim = x.ndim</span><br><span class="line">    <span class="keyword">assert</span> <span class="number">0</span> &lt;= <span class="number">1</span> &lt; ndim</span><br><span class="line">    <span class="keyword">assert</span> freqs_cis.shape == (x.shape[<span class="number">1</span>], x.shape[-<span class="number">1</span>])</span><br><span class="line">    shape = [d <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == ndim - <span class="number">1</span> <span class="keyword">else</span> <span class="number">1</span> <span class="keyword">for</span> i, d <span class="keyword">in</span> <span class="built_in">enumerate</span>(x.shape)] <span class="comment"># (1, length, 1, d/2)</span></span><br><span class="line">    <span class="keyword">return</span> freqs_cis.view(*shape) <span class="comment"># [1, length, 1, d/2]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rotary_emb</span>(<span class="params">xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor,</span>):</span><br><span class="line">    <span class="comment"># 先将xq维度变为[bs, length, head,  d/2, 2], 利用torch.view_as_complex转变为复数</span></span><br><span class="line">    <span class="comment"># xq:[q0, q1, .., q(d-1)] 转变为 xq_: [q0+j*q1, q2+j*q3, ..., q(d-2)+j*q(d-1)]</span></span><br><span class="line">    xq_ = torch.view_as_complex(xq.<span class="built_in">float</span>().reshape(*xq.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>)) <span class="comment"># [bs, length, head, d/2]</span></span><br><span class="line">    <span class="comment"># 同样的，xk_:[k0+j*k1, k2+j*k3, ..., k(d-2)+j*k(d-1)]</span></span><br><span class="line">    xk_ = torch.view_as_complex(xk.<span class="built_in">float</span>().reshape(*xk.shape[:-<span class="number">1</span>], -<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"></span><br><span class="line">    freqs_cis = reshape_for_broadcast(freqs_cis, xq_) <span class="comment"># [1, length, 1, d/2]</span></span><br><span class="line">    <span class="comment"># 下式xq_ * freqs_cis形式化输出，以第一个为例, 如下</span></span><br><span class="line">    <span class="comment"># (q0+j*q1)(cos(m*theta_0)+j*sin(m*theta_0)) = q0*cos(m*theta_0)-q1*sin(m*theta_0) + j*(q1*cos(m*theta_0)+q0*sin(m*theta_0))</span></span><br><span class="line">    <span class="comment"># 上式的实部为q0*cos(m*theta_0)-q1*sin(m*theta_0)，虚部为q1*cos(m*theta_0)+q0*sin(m*theta_0)</span></span><br><span class="line">    <span class="comment"># 然后通过torch.view_as_real函数，取出实部和虚部，维度由[bs, length, head, d/2]变为[bs, length, head, d/2, 2]，最后一维放实部与虚部</span></span><br><span class="line">    <span class="comment"># 最后经flatten函数将维度拉平，即[bs, length, head, d]</span></span><br><span class="line">    <span class="comment"># 此时xq_out形式化为 [实部0，虚部0，实部1，虚部1，..., 实部(d/2-1), 虚部(d/2-1)]</span></span><br><span class="line">    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(<span class="number">3</span>) <span class="comment"># [bs, length, head, d]</span></span><br><span class="line">    <span class="comment"># 即为新生成的q</span></span><br><span class="line"></span><br><span class="line">    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(<span class="number">3</span>)</span><br><span class="line">    <span class="keyword">return</span> xq_out.type_as(xq), xk_out.type_as(xk)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="comment"># (bs, length, head, d)</span></span><br><span class="line">    q = torch.randn((<span class="number">2</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">32</span>))  <span class="comment"># q=[q0, q1, .., qd-1]</span></span><br><span class="line">    k = torch.randn((<span class="number">2</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">32</span>))</span><br><span class="line">    v = torch.randn((<span class="number">2</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">32</span>))</span><br><span class="line">    freqs_cis= precompute_freqs_cis(dim=<span class="number">32</span>, end=<span class="number">10</span>, constant= <span class="number">10000.0</span>)</span><br><span class="line">    <span class="comment"># print(freqs_cis.detach().numpy())</span></span><br><span class="line"></span><br><span class="line">    q_new, k_new = apply_rotary_emb(xq=q, xk=k, freqs_cis=freqs_cis)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h1 id="2-Alpaca"><a href="#2-Alpaca" class="headerlink" title="2.Alpaca"></a>2.Alpaca</h1><h2 id="2-1-简介"><a href="#2-1-简介" class="headerlink" title="2.1 简介"></a>2.1 简介</h2><p>Stanford Alpaca: An Instruction-following LLaMA Model</p>
<p>Alpaca是在<strong>LLaMA基础上使用52K指令数据精调的预训练模型</strong>，作者只用了不到600美元的成本训练出了该模型（数据$500 + 机器$100）。初步实验结果表明Alpaca可以达到与OpenAI text-davinci-003相匹敌的效果</p>
<h2 id="2-2-微调方法"><a href="#2-2-微调方法" class="headerlink" title="2.2 微调方法"></a>2.2 微调方法</h2><ol>
<li>第一步：构造175条self-instruct 种子示例任务</li>
<li>第二步：基于上述种子任务，利 用text-davinci-003爬取指令数据</li>
<li>第三步：使用爬取下来的52K指令 数据在LLaMA上进行精调，最终 得到Alpaca</li>
</ol>
<p><img src="image/image_j15EAfp-zb.png" alt=""></p>
<h2 id="2-3-Self-instruct数据构造"><a href="#2-3-Self-instruct数据构造" class="headerlink" title="2.3 Self-instruct数据构造"></a>2.3 Self-instruct数据构造</h2><p>首先由人工构造175条种子数据</p>
<figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="string">&quot;seed_task_25&quot;</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;perfect_numbers&quot;</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">&quot;instruction&quot;</span><span class="punctuation">:</span> <span class="string">&quot;Find the four smallest perfect numbers.&quot;</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="attr">&quot;instances&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="punctuation">&#123;</span> <span class="attr">&quot;input&quot;</span><span class="punctuation">:</span> <span class="string">&quot;&quot;</span><span class="punctuation">,</span> <span class="attr">&quot;output&quot;</span><span class="punctuation">:</span> <span class="string">&quot;6, 28, 496, and 8128”&#125;], </span></span><br><span class="line"><span class="string">  &quot;</span>is_classification<span class="string">&quot;: false</span></span><br><span class="line"><span class="string">&#125;</span></span><br></pre></td></tr></table></figure>
<p>将“爬取要求”和种子数据进行适当组合，送入textdavinci-003，要求生成类似的指令数据。要求包括：提升指令多样性、包含真实数据、字数 要求、语言要求、拒绝不合适指令等</p>
<h2 id="2-4-指令数据格式"><a href="#2-4-指令数据格式" class="headerlink" title="2.4 指令数据格式"></a>2.4 指令数据格式</h2><ul>
<li><code>instruction</code>: 描述模型需要执行的指令内容</li>
<li><code>input</code>（可选）: 任务上下文或输入信息，例如当指令是“对文章进行总结”，则input是文章内容</li>
<li><code>output</code>: 由text-davinci-003生成的针对指令的回复</li>
</ul>
<p><img src="image/image_pPtBso1few.png" alt=""></p>
<h1 id="3-Llama-2"><a href="#3-Llama-2" class="headerlink" title="3.Llama-2"></a>3.Llama-2</h1><h2 id="3-1-简介"><a href="#3-1-简介" class="headerlink" title="3.1 简介"></a>3.1 简介</h2><p>Llama 2: Open Foundation and Fine-Tuned Chat Models&#x20;</p>
<p>2023年7月，Meta推出了Llama-2开源大模型，并且推出了Llama-2-Chat对话模型</p>
<p>与一代LLaMA主要区别体现在<strong>更多的训练数据、更⻓的上下文窗口、GQA技术</strong>等</p>
<p><img src="image/image_2nhkpLZZzT.png" alt=""></p>
<p>模型结构的变动主要是体现在<strong>GQA</strong>和<strong>FFN</strong>缩放上</p>
<ul>
<li><strong>MHA改成GQA</strong>：整体参数量会有减少</li>
<li><strong>FFN模块矩阵维度有扩充</strong>：增强泛化能力，整体参数量增加</li>
<li><strong>上下文长度是llama两倍</strong>(长度从2048-&gt;4096) 训练语料增加约 40%，体现在1.4T-&gt;2.0T的Tokens llama2-34B和llama2-70B使用了GQA，加速模型训练和推理速度</li>
</ul>
<h2 id="3-2-GQA"><a href="#3-2-GQA" class="headerlink" title="3.2 GQA"></a>3.2 GQA</h2><p>GQA和MQA都是注意力的变体，其中多个查询头关注相同的键和值头，以减少推理过程中 KV 缓存的大小，并可以显著提高推理吞吐量。</p>
<p>MHA、GQA、MQA的区别和联系，具体的优点如下：</p>
<ul>
<li><code>Mutil-Head Attention</code> 因为自回归模型生成回答时，需要前面生成的KV缓存起来，来加速计算。</li>
<li><code>Multi-Query Attention</code> 多个头之间可以共享KV对，因此速度上非常有优势，实验验证大约减少30-40%吞吐。</li>
<li><code>Group Query Attention</code> 没有像MQA那么极端，将query分组，组内共享KV，效果接近MQA，速度上与MQA可比较。</li>
</ul>
<p><img src="image/image_SFLwTUXOiI.png" alt=""></p>
<p>Llama-2中使用了8个KV映射，即GQA-8，<strong>GQA在多数任务上与MHA效果相当，且平均效果优于MQA；GQA和MQA均比MHA有更好的吞吐量</strong></p>
<h2 id="3-3-源码"><a href="#3-3-源码" class="headerlink" title="3.3 源码"></a>3.3 源码</h2><p><img src="image/image_WANwLmpxNK.png" alt=""></p>
<h1 id="4-Code-Llama"><a href="#4-Code-Llama" class="headerlink" title="4.Code Llama"></a>4.Code Llama</h1><h2 id="4-1-简介"><a href="#4-1-简介" class="headerlink" title="4.1 简介"></a>4.1 简介</h2><p>2023年8月24日，Meta推出了面向代码的可商用大模型Code Llama，包含三个大小版本（7B/13B/34B）</p>
<p>支持多种编程语言，包括Python、C++、Java、PHP、Typescript (Javascript)、C#和Bash</p>
<p>亮点：</p>
<ul>
<li>免费供学术研究和商用</li>
<li>支持100K上下文</li>
<li>“神秘”34B版接近GPT-4效果</li>
</ul>
<h2 id="4-2-模型训练流程"><a href="#4-2-模型训练流程" class="headerlink" title="4.2 模型训练流程"></a>4.2 模型训练流程</h2><p><img src="image/image_bjYO7Dnhe_.png" alt=""></p>
<h2 id="4-3-Code-Infilling-Task-（7B-13B-only）"><a href="#4-3-Code-Infilling-Task-（7B-13B-only）" class="headerlink" title="4.3 Code Infilling Task （7B/13B only）"></a>4.3 Code Infilling Task （7B/13B only）</h2><p>任务目标：根据代码的上下文，预测残缺部分的代码</p>
<p>方法：</p>
<ul>
<li>从完整的代码中选择一部分进行掩码（mask）并替换为<code>&lt;MASK&gt;</code>符号，构成上下文</li>
<li>利用自回归的方法，根据上下文信息预测解码出被mask的代码部分</li>
</ul>
<p><img src="image/image_PEeja7_w0_.png" alt=""></p>
<h1 id="5-总结"><a href="#5-总结" class="headerlink" title="5.总结"></a>5.总结</h1><p><strong>LLaMA</strong>&#x20;</p>
<ul>
<li>开源大模型繁荣发展的开端，一系列相关工作均基于LLaMA开展</li>
<li>模型规模7B、13B、33B、65B满足了开发者和研究者的不同需求</li>
</ul>
<p><strong>Alpaca</strong>：通过少量的指令精调赋予LLaMA指令理解与执行的能力</p>
<p><strong>Llama-2</strong></p>
<ul>
<li>LLaMA的二代模型，相关模型性能进一步提升，模型可商用</li>
<li>推出官方对⻬的Chat版本模型，采用了完整的RLHF链条</li>
</ul>
<p><strong>Code Llama</strong>：专注于代码能力的LLaMA模型，最好的模型代码能力接近GPT-4效果，模型可商用</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/llms/llms_article/3.llama%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/">https://wdndev.github.io/llms/llms_article/3.llama系列模型/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/LLMs/">LLMs</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/llms/llms_idx/" title="LLMs 目录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-05</div><div class="title">LLMs 目录</div></div></a></div><div><a href="/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/" title="LLMs 推理优化技术"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-03</div><div class="title">LLMs 推理优化技术</div></div></a></div><div><a href="/llms/llms_article/2.%E4%B8%BB%E6%B5%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%BB%86%E8%8A%82/" title="主流大语言模型的技术原理细节"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-03</div><div class="title">主流大语言模型的技术原理细节</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-LLama"><span class="toc-text">1.LLama</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E7%AE%80%E4%BB%8B"><span class="toc-text">1.1 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-RMSNorm%E5%BD%92%E4%B8%80%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-text">1.2 RMSNorm归一化函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-SwiGLU%E8%AE%A1%E5%88%92%E5%87%BD%E6%95%B0"><span class="toc-text">1.3 SwiGLU计划函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-%E6%97%8B%E8%BD%AC%E4%BD%8D%E7%BD%AE%E5%B5%8C%E5%85%A5%EF%BC%88RoPE%EF%BC%89"><span class="toc-text">1.4 旋转位置嵌入（RoPE）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-Alpaca"><span class="toc-text">2.Alpaca</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E7%AE%80%E4%BB%8B"><span class="toc-text">2.1 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-%E5%BE%AE%E8%B0%83%E6%96%B9%E6%B3%95"><span class="toc-text">2.2 微调方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Self-instruct%E6%95%B0%E6%8D%AE%E6%9E%84%E9%80%A0"><span class="toc-text">2.3 Self-instruct数据构造</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-%E6%8C%87%E4%BB%A4%E6%95%B0%E6%8D%AE%E6%A0%BC%E5%BC%8F"><span class="toc-text">2.4 指令数据格式</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Llama-2"><span class="toc-text">3.Llama-2</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%AE%80%E4%BB%8B"><span class="toc-text">3.1 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-GQA"><span class="toc-text">3.2 GQA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%BA%90%E7%A0%81"><span class="toc-text">3.3 源码</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-Code-Llama"><span class="toc-text">4.Code Llama</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1-%E7%AE%80%E4%BB%8B"><span class="toc-text">4.1 简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-text">4.2 模型训练流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-3-Code-Infilling-Task-%EF%BC%887B-13B-only%EF%BC%89"><span class="toc-text">4.3 Code Infilling Task （7B&#x2F;13B only）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E6%80%BB%E7%BB%93"><span class="toc-text">5.总结</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>