<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLMs 推理优化技术 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="原文链接：Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog   堆叠Transformer层以创建大型模型可以获得更好的准确性、few-shot学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（经常性成本）。当今最流行的大型语言">
<meta property="og:type" content="article">
<meta property="og:title" content="LLMs 推理优化技术">
<meta property="og:url" content="https://wdndev.github.io/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="原文链接：Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog   堆叠Transformer层以创建大型模型可以获得更好的准确性、few-shot学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（经常性成本）。当今最流行的大型语言">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2023-12-02T16:00:00.000Z">
<meta property="article:modified_time" content="2026-02-08T00:00:55.223Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="LLMs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLMs 推理优化技术',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-02-08 08:00:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">942</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">25</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">16</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/hot_news/index"><i class="fa-fw fas fa-brands fa-message"></i><span> HotNews</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">LLMs 推理优化技术</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-12-02T16:00:00.000Z" title="Created 2023-12-03 00:00:00">2023-12-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-02-08T00:00:55.223Z" title="Updated 2026-02-08 08:00:55">2026-02-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLMs/">LLMs</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">6.7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>20min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLMs 推理优化技术"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><blockquote>
<p>原文链接：<a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/mastering-llm-techniques-inference-optimization/" title="Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog">Mastering LLM Techniques: Inference Optimization | NVIDIA Technical Blog</a></p>
</blockquote>
<p><img src="image/image_YZh-nrxyKn.png" alt=""></p>
<p>堆叠Transformer层以创建大型模型可以获得更好的准确性、few-shot学习能力，甚至在各种语言任务中具有接近人类的涌现能力。这些基础模型的训练成本很高，而且在推理过程中可能需要大量的内存和计算（经常性成本）。当今最流行的大型语言模型（LLM）的大小可以达到数百亿到数千亿个参数，并且根据用例的不同，可能需要摄入长输入（或上下文），这也会增加开销。</p>
<p>这篇文章讨论了LLM推理中最紧迫的挑战，以及一些实用的解决方案。读者应该对transformer架构和注意力机制有一个基本的了解。</p>
<h1 id="1-理解LLM推理"><a href="#1-理解LLM推理" class="headerlink" title="1.理解LLM推理"></a>1.理解LLM推理</h1><p>大多数流行的only-decode LLM（例如 GPT-3）都是针对因果建模目标进行预训练的，本质上是作为下一个词预测器。这些 LLM 将一系列tokens作为输入，并自回归生成后续tokens，直到满足停止条件（例如，生成tokens数量的限制或遇到停止词）或直到生成特殊的 <code>&lt;end&gt;</code> 标记生成结束的tokens。该过程涉及两个阶段：预填充阶段和解码阶段。</p>
<p>请注意，tokens是模型处理的语言的原子部分。一个tokens大约是四个英文字符。所有自然语言在输入模型之前都会转换为toikens。</p>
<h2 id="1-1-预填充阶段或处理输入"><a href="#1-1-预填充阶段或处理输入" class="headerlink" title="1.1 预填充阶段或处理输入"></a>1.1 预填充阶段或处理输入</h2><p>在预填充阶段，LLM处理输入token以计算中间状态（keys和value），用于生成“第一个”token。每个新的token都依赖于所有先前的token，但由于输入的全部已知，因此在运算上，都是高度并行化矩阵运算，可以有效地使用GPU。</p>
<h2 id="1-2-解码阶段或生成输出"><a href="#1-2-解码阶段或生成输出" class="headerlink" title="1.2 解码阶段或生成输出"></a>1.2 解码阶段或生成输出</h2><p>在解码阶段，LLM一次自回归生成一个输出token，直到满足停止条件。每个输出tokens都需要直到之前迭代的所有输出状态（keys和values）。这与预填充输入处理相比，就像矩阵向量运算未充分利用GPU计算能力。数据（weights, keys, values, activations） 从内存传输到GPU的速度决定了延迟，而不是计算实际时间消耗。即，这是一个内存限制操作。</p>
<p>本文中的许多推理挑战和相应的解决方案都涉及此解码阶段的优化：高效的注意力模块、有效管理键和值等。</p>
<p>不同的LLMs可能使用不同的tokenizers，因此比较它们之间的输出tokens可能并不简单。在比较推理吞吐量时，即使两个 LLMs每秒输出的tokens相似，如果它们使用不同的tokenizers，也可能不相等。这是因为相应的tokens可能代表不同数量的字符。</p>
<h2 id="1-3-批处理（Batching）"><a href="#1-3-批处理（Batching）" class="headerlink" title="1.3 批处理（Batching）"></a>1.3 批处理（Batching）</h2><p>提高 GPU 利用率和有效吞吐量的最简单方法是通过<strong>批处理</strong>。由于多个请求使用相同的模型，因此权重的内存成本被分散。大批量数据传输到 GPU 一次处理，将提高GPU资源的利用率。</p>
<p>然而，批量大小只能增加到一定限制，此时可能会导致内存溢出。为了防止这种情况发生，需要查看键值 (KV) 缓存和 LLM 内存要求。</p>
<p>传统批处理（也称为静态批处理， static batching）不是最佳的。这是因为<strong>对于批次中的每个请求，LLM 可能会生成不同数量的tokens，并且不同tokens有不同的执行时间</strong>。因此，批次中的所有请求都必须等待最长token的处理完成，而生成长度的巨大差异可能会加剧这种情况。有一些方法可以缓解这种情况，例如稍动态批处理。</p>
<h2 id="1-4-KV缓存"><a href="#1-4-KV缓存" class="headerlink" title="1.4 KV缓存"></a>1.4 KV缓存</h2><p>解码阶段的一种常见优化是 KV 缓存。解码阶段在每个时间步生成单个token，但每个token依赖于之前token的键和值张量（包括预填充时计算的输入tokens的 KV 张量，以及当前时间步之前计算的任何新 KV 张量） 。</p>
<p>为了避免在每个时间步重新计算所有tokens的这些张量，<strong>可以将它们缓存在 GPU 内存中</strong>。每次迭代，当需要计算新token时，它们都会被添加到正在运行的缓存中，以便在下一次迭代中使用。在一些实现中，模型的每一层都有一个KV缓存。</p>
<p><img src="image/image_LmKhNv__Og.png" alt=""></p>
<blockquote>
<p>图1 KV缓存机制</p>
</blockquote>
<h2 id="1-5-LLM内存需求"><a href="#1-5-LLM内存需求" class="headerlink" title="1.5 LLM内存需求"></a>1.5 LLM内存需求</h2><p>实际上，LLM对GPU显存的需求主要是模型权重和KV缓存：</p>
<ul>
<li><strong>模型权重</strong>：模型参数占用内存。例如，具有 70 亿个参数的模型（例如 Llama2-7B），以 16 位精度（FP16 或 BF16）加载，将占用大约 <code>7B * sizeof(FP16) ~= 14 GB</code> 的内存。</li>
<li><strong>KV缓存</strong>：自注意力张量的缓存占用内存，避免冗余计算。</li>
</ul>
<p>使用批处理时，批处理中每个请求的 KV 缓存仍然必须单独分配，并且可能会占用大量内存。下面的公式描述了 KV 缓存的大小，适用于当今最常见的 LLM 架构。</p>
<script type="math/tex; mode=display">
每个token的KV缓存大小(字节) = 2 * (num\_layers) * (num\_heads * dim\_head) *  precision\_in\_bytes</script><p>第一个因子 2 代表 K 和 V 矩阵。通常，<code>(num_heads * dim_head)</code>的值与Transformer的<code>hidden_​​size</code>（或模型的维度，<code>d_model</code>）相同。这些模型属性通常可以在配置文件中找到。</p>
<p>输入批次中输入序列中的每个tokens都需要此内存大小。假设半精度，KV缓存的总大小由以下公式给出:</p>
<script type="math/tex; mode=display">
总KV缓存大小(字节)=(batch\_size) * (sequence\_length) * 2 * (num\_layers) * (hidden\_size) *  sizeof(FP16)</script><p>例如，对于 16 位精度的 Llama 2 7B 模型，批量大小为 <code>1</code>，KV 缓存的大小将为 <code>1 * 4096 * 2 * 32 * 4096 * 2</code> 字节，即约 <code>2 GB</code>。</p>
<p>高效的管理 KV 缓存是一项具有挑战性的工作。内存需求随着批量大小和序列长度线性增长，可以快速扩展。因此，它限制了可服务的吞吐量，并对长上下文输入提出了挑战。这就是本文中介绍的多项优化背后的动机。</p>
<h1 id="2-模型并行化扩展LLM"><a href="#2-模型并行化扩展LLM" class="headerlink" title="2.模型并行化扩展LLM"></a>2.模型并行化扩展LLM</h1><p>减少模型权重在每设备的显存占用的一种方法是<strong>将模型分布在多个 GPU 上</strong>。分散内存和计算可以运行更大的模型或更大批量的输入。模型并行化是训练或推理模型所必需的，模型并行化需要比单个设备更多的内存，用来训练和推理（延迟或吞吐量）。根据模型权重的划分方式，有多种方法可以并行化模型。</p>
<p>请注意，数据并行性也是一种经常在与下面列出的其他技术相同的的技术。在这种情况下，模型的权重被复制到多个设备上，并且输入的（全局）批量大小在每个设备上被分成微批次。它通过处理较大的批次来减少总体执行时间。然而，这是一种训练时间优化，在推理过程中不太相关。</p>
<h2 id="2-1-Pipeline并行"><a href="#2-1-Pipeline并行" class="headerlink" title="2.1 Pipeline并行"></a>2.1 Pipeline并行</h2><p>Pipeline并行化<strong>将模型（垂直）分片为块，其中每个块包含在单独设备上执行的层的子集</strong>。图 2a 说明了四路Pipeline，其中模型按顺序分区，并且所有层的四分之一子集在每个设备上执行。一个设备上的一组操作的输出被传递到下一个设备，后者继续执行后续块。$F_n$和 $B_n$分别表示设备 $n$ 上的前向传播和后向传播。每个设备上存储模型权重的内存需求被分成四份。</p>
<p>该方法的缺点是，由于处理的顺序性质，<strong>某些设备或层在等待前一层的输出（激活、梯度）时可能保持空闲状态</strong>。这会导致前向和后向传递效率低下或出现“Pipeline bubbles”。在图 2b 中，白色空白区域是Pipeline并行性产生的Pipeline bubbles，其中设备闲置且未得到充分利用。</p>
<p><strong>微批处理可以在一定程度上缓解这种情况</strong>，如图 2c 所示。输入的全局批次大小被分成子批次，这些子批次被一一处理，最后累积梯度。请注意，$F<em>{n,m}$ 和 $B</em>{n,m}$ 分别表示设备<code>n</code>上<code>m</code>批次的前向和后向传递。<strong>这种方法缩小了管道气泡的尺寸，但并没有完全消除它们</strong>。</p>
<p><img src="image/image_PkhYDpFHjZ.png" alt=""></p>
<blockquote>
<p>图2 Pipeline并行，</p>
</blockquote>
<h2 id="2-2-Tensor并行"><a href="#2-2-Tensor并行" class="headerlink" title="2.2 Tensor并行"></a>2.2 Tensor并行</h2><p>Tensor并行化<strong>将模型的各个层（水平）分片为更小的、独立的计算块，这些计算块可以在不同的设备上执行</strong>。Transformer的主要组成部分，注意力块和多层感知器（MLP）层是可以利用Tensor并行化的。在多头注意力块中，每个头或一组头可以分配给不同的设备，以便它们可以独立且并行地计算。</p>
<p><img src="image/image_Z1kiW5PtoV.png" alt=""></p>
<blockquote>
<p>图3 Tensor并行化MLP和自注意力</p>
</blockquote>
<p>图 3a 显示了两层 MLP Tensor并行的示例，每一层都由一个圆角框表示。在第一层中，权重矩阵$A$分为$A_1$和$A_2$ 。对于输入X，可以在同一批次不同设备上计算$XA_1$ 和$ XA_2  $，其中，f是identity 操作。这将每个设备上存储权重的内存需求减半。归约操作$g$组合了第二层的输出。</p>
<p>图 3b 是自注意力层中Tensor并行的示例。多个注意力头本质上是并行的，并且可以跨设备分割。</p>
<h2 id="2-3-Sequence并行"><a href="#2-3-Sequence并行" class="headerlink" title="2.3 Sequence并行"></a>2.3 Sequence并行</h2><p>Tensor并行化是有局限性，它需要将层划分为独立的、可管理的块，不适用于 <code>LayerNorm</code>和 <code>Dropout</code>等操作，而是在tensor并行中复制。虽然 <code>LayerNorm</code>和 <code>Dropout</code>的计算成本较低，但它们确实需要大量内存来存储（冗余）激活。</p>
<p>如<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2205.05198.pdf" title="Reducing Activation Recomputation in Large Transformer Models">Reducing Activation Recomputation in Large Transformer Models</a>所示，这些操作在输入序列中是独立的，并且这些操作<strong>可以沿着“序列维度”进行分区</strong>，从而提高内存效率。这称为序列并行性。</p>
<p><img src="image/image_RE2vXpBHJ5.png" alt=""></p>
<blockquote>
<p>图4，transformer层的tensor并行化和sequence并行化</p>
</blockquote>
<p>模型并行技术不是唯一的，可以结合使用。它们可以帮助扩展和减少 LLM 的每 GPU 内存占用量，但也有专门针对注意力模块的优化技术。</p>
<h1 id="3-注意力机制优化"><a href="#3-注意力机制优化" class="headerlink" title="3.注意力机制优化"></a>3.注意力机制优化</h1><p>缩放点积注意力 (SDPA， scaled dot-product attention) 操作将<code>query</code>和<code>key</code>对映射到输出，如论文<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1706.03762.pdf" title="Attention Is All You Need">Attention Is All You Need</a>所述。</p>
<h2 id="3-1-多头注意力（MHA）"><a href="#3-1-多头注意力（MHA）" class="headerlink" title="3.1 多头注意力（MHA）"></a>3.1 多头注意力（MHA）</h2><p>作为 SDPA 的增强，三个变换张量对Q，K，V分别进行线性变换，<strong>这些变换不会改变原有张量的尺寸</strong>，使模型能够共同关注来自不同位置的不同表示子空间的信息。这些子空间是独立学习的，使模型能够更丰富地理解输入中的不同位置。</p>
<p>如图 5 所示，多个并行注意力操作的输出被拼接后线性投影以组合起来。每个并行注意力层称为“头”，这种方法称为多头注意力（MHA）。</p>
<p>当使用八个并行注意力头时，每个注意力头的维度都会减少（例如 $d_model/8$）。这使得计算成本与单头注意力相似。</p>
<p><img src="image/image_RFewX-Wp-I.png" alt=""></p>
<blockquote>
<p>图5 缩放点积注意力（左）和多头注意力（右）的图示，并行的多个 SDPA 头</p>
</blockquote>
<h2 id="3-2-多查询注意力（MQA）"><a href="#3-2-多查询注意力（MQA）" class="headerlink" title="3.2 多查询注意力（MQA）"></a>3.2 多查询注意力（MQA）</h2><p>MHA 的推理优化之一称为多查询注意力 (MQA)，如 Fast Transformer Decoding 中提出的，<strong>在多个注意力头之间共享键和值</strong>。与以前一样，查询向量仍然被投影多次。</p>
<p>虽然 MQA 中完成的计算量与 MHA 相同，但从内存读取的数据量（键、值）只是以前的一小部分。当受内存带宽限制时，这可以实现更好的计算利用率。它还减少了内存中 KV 缓存的大小，为更大的批量大小留出了空间。</p>
<p>key头的减少会带来潜在的准确性下降。此外，需要在推理时利用这种优化的模型需要在启用 MQA 的情况下进行训练（或至少使用大约 5% 的训练量进行微调）。</p>
<h2 id="3-3-分组注意力（GQA）"><a href="#3-3-分组注意力（GQA）" class="headerlink" title="3.3 分组注意力（GQA）"></a>3.3 分组注意力（GQA）</h2><p>分组查询注意力 (GQA) 通过将键和值投影到几组查询头，在 MHA 和 MQA 之间取得平衡（图 6）。在每个组中，它的行为类似于多查询注意力。</p>
<p>图 6 显示多头注意力有多个键值头（左）。分组查询注意力（中心）的键值头多于一个，但少于查询头的数量，这是内存需求和模型质量之间的平衡。多查询注意力（右）具有单个键值头，有助于节省内存。</p>
<p><img src="image/image_R47Naw43sH.png" alt=""></p>
<p>最初使用 MHA 训练的模型可以使用原始训练计算的一小部分通过 GQA 进行“升级训练”。它们获得接近 MHA 的质量，同时保持接近 MQA 的计算效率。 Llama 2 70B 是利用 GQA 的模型示例。</p>
<p><strong>MQA 和 GQA 等优化通过减少存储的key头和value头的数量来帮助减少 KV 缓存所需的内存</strong>。 KV 缓存的管理方式可能仍然效率低下。与优化注意力模块本身不同，下一节将介绍一种更高效的 KV 缓存管理技术。</p>
<h2 id="3-4-Flash-attention"><a href="#3-4-Flash-attention" class="headerlink" title="3.4 Flash attention"></a>3.4 Flash attention</h2><p>优化注意力机制的另一种方法是<strong>修改某些计算的顺序，以更好地利用 GPU 的内存层次结构</strong>。神经网络通常用层来描述，大多数实现也以这种方式布局，每次按顺序对输入数据进行一种计算。这并不总是能带来最佳性能，因为对已经进入内存层次结构的更高、性能更高级别的值进行更多计算可能是有益的。</p>
<p>在实际计算过程中将多个层融合在一起可以最大限度地减少 GPU 需要读取和写入内存的次数，并将需要相同数据的计算分组在一起，即使它们是神经网络中不同层的一部分。</p>
<p>一种非常流行的融合是 FlashAttention，这是一种 I/O 感知精确注意算法，详细信息请参阅 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2205.14135" title="FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness">FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</a>。精确注意力意味着它在数学上与标准多头注意力相同（具有可用于多查询和分组查询注意力的变体），因此可以无需修改即可交换到现有的模型架构，甚至是已经训练的模型。</p>
<p>I/O 感知意味着在将操作融合在一起时，它会考虑前面讨论的一些内存移动成本。特别是，FlashAttention 使用“平铺”一次性完全计算并写出最终矩阵的一小部分，而不是分步对整个矩阵进行部分计算，写出中间的中间值。</p>
<p>图 7 显示了 40 GB GPU 上的平铺 FlashAttention 计算模式和内存层次结构。右图显示了对注意力机制的不同组件进行融合和重新排序所带来的相对加速。</p>
<p><img src="image/image_0qq38on2gP.png" alt=""></p>
<blockquote>
<p>图7 40 GB GPU 上的平铺 FlashAttention 计算模式和内存层次结构</p>
</blockquote>
<h1 id="4-KV缓存的分页高效管理"><a href="#4-KV缓存的分页高效管理" class="headerlink" title="4.KV缓存的分页高效管理"></a>4.KV缓存的分页高效管理</h1><p>有时，KV 缓存会静态地“过度配置”(over-provisioned)，以考虑最大可能的输入（支持的序列长度），因为输入的大小是不可预测的。例如，如果模型支持的最大序列长度为 2,048，则<strong>无论请求中输入和生成的输出的大小如何，都将在内存中保留大小为 2,048 的数据。该空间可以是连续分配的，并且通常其中大部分未被使用，从而导致内存浪费或碎片</strong>。该保留空间在请求的生命周期内被占用。</p>
<p><img src="image/image_0EJDxQn6Es.png" alt=""></p>
<blockquote>
<p>图8 由于过度配置和低效的 KV 缓存管理而导致的内存浪费和碎片</p>
</blockquote>
<p>受操作系统分页的启发，PagedAttention 算法能够<strong>将连续的键和值存储在内存中的不连续空间中</strong>。它将每个请求的 KV 缓存划分为代表固定数量token的块，这些块可以不连续存储。</p>
<p>在注意力计算期间，使用根据记录索引获取这些块。当新的token产生时，就会进行新的区块分配。这些块的大小是固定的，消除了因不同请求需要不同分配等挑战而产生的低效率。这极大地限制了内存浪费，从而实现了更大的批量大小（从而提高了吞吐量）。</p>
<h1 id="5-模型优化技术"><a href="#5-模型优化技术" class="headerlink" title="5.模型优化技术"></a>5.模型优化技术</h1><p>到目前为止，我们已经讨论了 LLM 消耗内存的不同方式、跨多个不同 GPU 分配内存的一些方式，以及优化注意力机制和 KV 缓存。还有多种模型优化技术可以通过修改模型权重本身来减少每个 GPU 上的内存使用。 GPU 还具有专用硬件来加速这些修改值的运算，从而为模型提供更多加速。</p>
<h2 id="5-1-量化（Quantization）"><a href="#5-1-量化（Quantization）" class="headerlink" title="5.1 量化（Quantization）"></a>5.1 量化（Quantization）</h2><p><strong>量化是降低模型权重和激活精度的过程</strong>。大多数模型都以 32 或 16 位精度进行训练，其中每个参数和激活元素占用 32 或 16 位内存（单精度浮点）。然而，大多数深度学习模型可以用每个值八个甚至更少的位来有效表示。</p>
<p>图 9 显示了一种可能的量化方法之前和之后的值分布。在这种情况下，舍入会丢失一些精度，并且剪裁会丢失一些动态范围，从而允许以更小的格式表示值。</p>
<p><img src="image/image_M1RfRhceOy.png" alt=""></p>
<blockquote>
<p>图9 一种可能的量化方法之前和之后的值分布</p>
</blockquote>
<p>降低模型的精度可以带来多种好处。如果模型占用的内存空间较少，则可以在相同数量的硬件上安运行更大的模型。量化还意味着可以在相同的带宽上传输更多参数，这有助于加速带宽有限的模型。</p>
<p>LLM 有许多不同的量化技术，涉及降低激活、权重或两者的精度。量化权重要简单得多，因为它们在训练后是固定的。然而，这可能会留下一些性能问题，因为激活仍然保持在更高的精度。 GPU 没有用于乘以 INT8 和 FP16 数字的专用硬件，因此必须将权重转换回更高精度以进行实际运算。</p>
<p>还可以量化激活、Transformer块和网络层的输入，但这也有其自身的挑战。激活向量通常包含异常值，有效地增加了它们的动态范围，并使以比权重更低的精度表示这些值变得更具挑战性。</p>
<p>一种选择是通过模型传递代表性数据集并选择以比其他激活更高的精度表示某些激活来找出这些异常值可能出现的位置 (<code>LLM.int8()</code>)。另一种选择是借用易于量化的权重的动态范围，并在激活中重用该范围。</p>
<h2 id="5-2-稀疏（Sparsity）"><a href="#5-2-稀疏（Sparsity）" class="headerlink" title="5.2 稀疏（Sparsity）"></a>5.2 稀疏（Sparsity）</h2><p>与量化类似，事实证明，许多深度学习模型对于修剪或用 <code>0</code> 本身替换某些接近 <code>0</code> 的值具有鲁棒性。稀疏矩阵是许多元素为 0 的矩阵。这些矩阵可以用压缩形式表示，比完整的稠密矩阵占用的空间更少。</p>
<p><img src="image/image_UKiDga-iHn.png" alt=""></p>
<blockquote>
<p>图10，以压缩格式表示的稀疏矩阵，由非零数据值及其相应的两位索引组成</p>
</blockquote>
<p>GPU 尤其具有针对某种结构化稀疏性的硬件加速，其中每四个值中有两个由零表示。稀疏表示还可以与量化相结合，以实现更大的执行速度。寻找以稀疏格式表示大型语言模型的最佳方法仍然是一个活跃的研究领域，并为未来提高推理速度提供了一个有希望的方向。</p>
<h2 id="5-3-蒸馏（Distillation）"><a href="#5-3-蒸馏（Distillation）" class="headerlink" title="5.3 蒸馏（Distillation）"></a>5.3 蒸馏（Distillation）</h2><p>缩小模型大小的另一种方法是通过称为蒸馏的过程<strong>将其知识转移到较小的模型</strong>。此过程涉及训练较小的模型（称为学生）来模仿较大模型（教师）的行为。</p>
<p>蒸馏模型的成功例子包括 <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1910.01108" title="DistilBERT">DistilBERT</a>，它将 BERT 模型压缩了 40%，同时保留了 97% 的语言理解能力，速度提高了 60%。</p>
<p>虽然LLMs中的蒸馏是一个活跃的研究领域，但神经网络的一般方法首次在<a target="_blank" rel="noopener" href="https://arxiv.org/abs/1503.02531" title="Distilling the Knowledge in a Neural Network">Distilling the Knowledge in a Neural Network</a>中提出：</p>
<ul>
<li>学生网络经过训练，可以反映较大教师网络的性能，使用损失函数来测量其输出之间的差异。该目标还可能包括将学生的输出与真实标签进行匹配的原始损失函数。</li>
<li>匹配的教师输出可以是最后一层（称为 <code>logits</code>）或中间层激活。</li>
</ul>
<p>图 11 显示了知识蒸馏的总体框架。教师的 <code>logits</code>是学生使用蒸馏损失进行优化的软目标。其他蒸馏方法可能会使用其他损失措施来从老师那里“蒸馏”知识。</p>
<p><img src="image/image_N8rcSMB0Rs.png" alt=""></p>
<blockquote>
<p>图11，知识蒸馏的通用框架</p>
</blockquote>
<p>蒸馏的另一种方法是使用教师合成的数据对LLMs学生进行监督培训，这在人工注释稀缺或不可用时特别有用。一步一步蒸馏！更进一步，除了作为基本事实的标签之外，还从LLMs教师那里提取基本原理。这些基本原理作为中间推理步骤，以数据有效的方式培训规模较小的LLMs。</p>
<p>值得注意的是，当今许多最先进的LLMs都拥有限制性许可证，禁止使用他们的成果来训练其他LLMs，这使得找到合适的教师模型具有挑战性。</p>
<h1 id="6-模型服务技术"><a href="#6-模型服务技术" class="headerlink" title="6.模型服务技术"></a>6.模型服务技术</h1><p>模型执行通常受内存带宽限制，特别是权重中的带宽限制。即使在应用了前面描述的所有模型优化之后，它仍然很可能受到内存限制。因此，在加载模型权重时尽可能多地处理它们。换句话说，尝试并行。可以采取两种方法：</p>
<ul>
<li>动态批处理(<strong>In-flight batching</strong>) ：同时执行多个不同的请求。</li>
<li>预测推理(<strong>Speculative inference</strong>) ：并行执行序列的多个不同步骤以尝试节省时间。</li>
</ul>
<h2 id="6-1-动态批处理（In-flight-batching）"><a href="#6-1-动态批处理（In-flight-batching）" class="headerlink" title="6.1 动态批处理（In-flight batching）"></a>6.1 动态批处理（<strong>In-flight batching</strong>）</h2><p>LLMs 具有一些独特的执行特征，这些特征可能导致在实践中难以有效地处理批量请求。一个模型可以同时用于多种不同的任务。从聊天机器人中的简单问答响应到文档摘要或代码块的生成，工作负载是高度动态的，输出大小变化几个数量级。</p>
<p>这种多功能性使得批处理请求并有效地并行执行它们变得具有挑战性，这是服务神经网络的常见优化。这可能会导致某些请求比其他请求更早完成。</p>
<p>为了管理这些动态负载，许多LLMs 服务解决方案包括一种称<strong>为连续或动态批处理的优化调度技术</strong>。这利用了这样一个事实：<strong>LLMs的整个文本生成过程可以分解为模型上的多次执行迭代</strong>。</p>
<p>通过动态批处理，服务器运行时会<strong>立即从批处理中剔除已完成的序列，而不是等待整个批处理完成后再继续处理下一组请求</strong>。然后，它开始执行新请求，而其他请求仍在进行中。因此，动态批处理可以极大地提高实际用例中 GPU 的整体利用率。</p>
<h2 id="6-2-预测推理（Speculative-inference）"><a href="#6-2-预测推理（Speculative-inference）" class="headerlink" title="6.2 预测推理（Speculative inference）"></a>6.2 预测推理（<strong>Speculative inference</strong>）</h2><p>预测推理也称为推测采样、辅助生成或分块并行解码，是并行执行 LLM 的另一种方式。通常，GPT 风格的大语言模型是自回归模型，逐个生成文本标记。</p>
<p>生成的每个标记都依赖于它之前的所有标记来提供上下文。这意味着在常规执行中，<strong>不可能从同一个序列并行生成多个token，必须等待第 n 个token生成后才能生成 n+1 个token</strong>。</p>
<p>图 12 显示了预测推理的示例，其中临时模型临时预测并行验证或拒绝的多个未来步骤。在这种情况下，临时模型中的前两个预测token被接受，而最后一个在继续生成之前被拒绝并删除。</p>
<p><img src="image/image_yJilh0txbj.png" alt=""></p>
<blockquote>
<p>图12， 预测推理示例</p>
</blockquote>
<p>预测性抽样提供了一种解决方法。这种方法的基本思想是使用一些“更便宜”的过程来生成几个token长的临时序列。然后，并行执行多个步骤的主要“验证”模型，使用廉价临时序列作为需要的执行步骤的“预测”上下文。</p>
<p>如果验证模型生成与临时序列相同的token，那么就知道接受这些token作为输出。否则，可以丢弃第一个不匹配标记之后的所有内容，并使用新的临时序列重复该过程。</p>
<p>如何生成临时token有许多不同的选项，每个选项都有不同的权衡。可以训练多个模型，或在单个预训练模型上微调多个头，以预测未来多个步骤的标记。或者，可以使用小型模型作为临时模型，使用更大、功能更强大的模型作为验证器。</p>
<h1 id="7-结论"><a href="#7-结论" class="headerlink" title="7.结论"></a>7.结论</h1><p>这篇文章概述了许多最流行的解决方案，以帮助高效地优化和服务LLMs，无论是在数据中心还是在 PC 边缘。其中许多技术都经过优化并通过 NVIDIA TensorRT-LLM 提供，这是一个开源库，由 TensorRT 深度学习编译器以及优化的内核、预处理和后处理步骤以及多 GPU/多节点通信原语组成，可在 NVIDIA 上实现突破性的性能GPU。</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/">https://wdndev.github.io/llms/llms_article/1.llm推理优化技术/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/LLMs/">LLMs</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/llms/llms_idx/" title="LLMs 目录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-05</div><div class="title">LLMs 目录</div></div></a></div><div><a href="/llms/llms_article/2.%E4%B8%BB%E6%B5%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%BB%86%E8%8A%82/" title="主流大语言模型的技术原理细节"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-03</div><div class="title">主流大语言模型的技术原理细节</div></div></a></div><div><a href="/llms/llms_article/3.llama%E7%B3%BB%E5%88%97%E6%A8%A1%E5%9E%8B/" title="LLaMA系列模型架构"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-05</div><div class="title">LLaMA系列模型架构</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E7%90%86%E8%A7%A3LLM%E6%8E%A8%E7%90%86"><span class="toc-text">1.理解LLM推理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-%E9%A2%84%E5%A1%AB%E5%85%85%E9%98%B6%E6%AE%B5%E6%88%96%E5%A4%84%E7%90%86%E8%BE%93%E5%85%A5"><span class="toc-text">1.1 预填充阶段或处理输入</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E8%A7%A3%E7%A0%81%E9%98%B6%E6%AE%B5%E6%88%96%E7%94%9F%E6%88%90%E8%BE%93%E5%87%BA"><span class="toc-text">1.2 解码阶段或生成输出</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88Batching%EF%BC%89"><span class="toc-text">1.3 批处理（Batching）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-KV%E7%BC%93%E5%AD%98"><span class="toc-text">1.4 KV缓存</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-5-LLM%E5%86%85%E5%AD%98%E9%9C%80%E6%B1%82"><span class="toc-text">1.5 LLM内存需求</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C%E5%8C%96%E6%89%A9%E5%B1%95LLM"><span class="toc-text">2.模型并行化扩展LLM</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-Pipeline%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.1 Pipeline并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2-Tensor%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.2 Tensor并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-Sequence%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.3 Sequence并行</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E4%BC%98%E5%8C%96"><span class="toc-text">3.注意力机制优化</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88MHA%EF%BC%89"><span class="toc-text">3.1 多头注意力（MHA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E5%A4%9A%E6%9F%A5%E8%AF%A2%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88MQA%EF%BC%89"><span class="toc-text">3.2 多查询注意力（MQA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E5%88%86%E7%BB%84%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88GQA%EF%BC%89"><span class="toc-text">3.3 分组注意力（GQA）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-Flash-attention"><span class="toc-text">3.4 Flash attention</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-KV%E7%BC%93%E5%AD%98%E7%9A%84%E5%88%86%E9%A1%B5%E9%AB%98%E6%95%88%E7%AE%A1%E7%90%86"><span class="toc-text">4.KV缓存的分页高效管理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E6%A8%A1%E5%9E%8B%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">5.模型优化技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E9%87%8F%E5%8C%96%EF%BC%88Quantization%EF%BC%89"><span class="toc-text">5.1 量化（Quantization）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E7%A8%80%E7%96%8F%EF%BC%88Sparsity%EF%BC%89"><span class="toc-text">5.2 稀疏（Sparsity）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E8%92%B8%E9%A6%8F%EF%BC%88Distillation%EF%BC%89"><span class="toc-text">5.3 蒸馏（Distillation）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-%E6%A8%A1%E5%9E%8B%E6%9C%8D%E5%8A%A1%E6%8A%80%E6%9C%AF"><span class="toc-text">6.模型服务技术</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-%E5%8A%A8%E6%80%81%E6%89%B9%E5%A4%84%E7%90%86%EF%BC%88In-flight-batching%EF%BC%89"><span class="toc-text">6.1 动态批处理（In-flight batching）</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E9%A2%84%E6%B5%8B%E6%8E%A8%E7%90%86%EF%BC%88Speculative-inference%EF%BC%89"><span class="toc-text">6.2 预测推理（Speculative inference）</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E7%BB%93%E8%AE%BA"><span class="toc-text">7.结论</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2026 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>