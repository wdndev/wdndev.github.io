<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>LLMs公开课 - 5.高效训练&amp;模型压缩 | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="1.背景介绍1.1 CPU &amp; GPU预训练语言模型以每年十倍的速度增大，越大的模型往往表现出更好的性能；但为了训练这些模型耗费也越来越昂贵，训练代码变得更复杂。 希望让训练过程变得更加简单，训练变得更高效，并且训练更加廉价。 首先要分析GPU内存；其次理解在多张显卡之间的合作模式是怎样的。  深度学习中最常见的矩阵乘法和向量加法适合于用GPU来计算。 &#x20; CPU和GPU的合作方">
<meta property="og:type" content="article">
<meta property="og:title" content="LLMs公开课 - 5.高效训练&amp;模型压缩">
<meta property="og:url" content="https://wdndev.github.io/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="1.背景介绍1.1 CPU &amp; GPU预训练语言模型以每年十倍的速度增大，越大的模型往往表现出更好的性能；但为了训练这些模型耗费也越来越昂贵，训练代码变得更复杂。 希望让训练过程变得更加简单，训练变得更高效，并且训练更加廉价。 首先要分析GPU内存；其次理解在多张显卡之间的合作模式是怎样的。  深度学习中最常见的矩阵乘法和向量加法适合于用GPU来计算。 &#x20; CPU和GPU的合作方">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2024-01-06T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:10.498Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="LLMs">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLMs公开课 - 5.高效训练&模型压缩',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">LLMs公开课 - 5.高效训练&amp;模型压缩</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2024-01-06T16:00:00.000Z" title="Created 2024-01-07 00:00:00">2024-01-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:10.498Z" title="Updated 2025-11-02 07:46:10">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/LLMs/">LLMs</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">7.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>22min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLMs公开课 - 5.高效训练&amp;模型压缩"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="1-背景介绍"><a href="#1-背景介绍" class="headerlink" title="1.背景介绍"></a>1.背景介绍</h1><h2 id="1-1-CPU-amp-GPU"><a href="#1-1-CPU-amp-GPU" class="headerlink" title="1.1 CPU &amp; GPU"></a>1.1 CPU &amp; GPU</h2><p>预训练语言模型以每年十倍的速度增大，越大的模型往往表现出更好的性能；但为了训练这些模型耗费也越来越昂贵，训练代码变得更复杂。</p>
<p>希望让训练过程变得更加简单，训练变得更高效，并且训练更加廉价。</p>
<p>首先要分析GPU内存；其次理解在多张显卡之间的合作模式是怎样的。</p>
<p><img src="image/image_iJFHeDFErq.png" alt=""></p>
<p>深度学习中最常见的矩阵乘法和向量加法适合于用GPU来计算。 &#x20;</p>
<p>CPU和GPU的合作方法通过CPU发送一些控制信号去控制GPU进行计算。 &#x20;</p>
<p><img src="image/image_hj3GbT8YLj.png" alt=""></p>
<p>如果想把模型的向量加法或矩阵乘法放到GPU中计算的话，需要把这些数据从CPU上拷贝到GPU上(<code>.cuda</code>)。</p>
<p>显卡中有哪些显存的组成。</p>
<h2 id="1-2-显存组成"><a href="#1-2-显存组成" class="headerlink" title="1.2 显存组成"></a>1.2 显存组成</h2><h4 id="（1）参数"><a href="#（1）参数" class="headerlink" title="（1）参数"></a>（1）参数</h4><p>为了加速模型的前向传播，需要把模型所有的参数都放到显卡中。</p>
<p><img src="image/image_kZdam7XmAL.png" alt=""></p>
<h4 id="（2）梯度"><a href="#（2）梯度" class="headerlink" title="（2）梯度"></a>（2）梯度</h4><p>在反向传播过程中，计算得到的梯度也保存到显卡中。</p>
<p><img src="image/image_y7R04D69wF.png" alt=""></p>
<h4 id="（3）中间结果"><a href="#（3）中间结果" class="headerlink" title="（3）中间结果"></a>（3）中间结果</h4><p>模型的中间计算结果，比如线性层 $y=Wx$，为了计算反向传播，需要在前向传播时在显卡中保存模型的输入(中间结果)。</p>
<p><img src="image/image_rRZkYTL9GR.png" alt=""></p>
<h4 id="（4）优化器"><a href="#（4）优化器" class="headerlink" title="（4）优化器"></a>（4）优化器</h4><p>第四部分，在显存中占大头的一部分，就是优化器，比如<code>Adam</code>，需要保存模型的梯度，和相关的历史信息<code>(m_t,v_t)</code>。它们的参数量是和梯度等数量级的。</p>
<p><img src="image/image_FtBMNH9bhx.png" alt=""></p>
<p>这四部分是预训练模型在显卡中主要的四个组成部分。 &#x20;</p>
<h4 id="（5）示例"><a href="#（5）示例" class="headerlink" title="（5）示例"></a>（5）示例</h4><p>一个11B参数的预训练语言模型，每个需要用float类型(FP32)来存储 &#x20;</p>
<script type="math/tex; mode=display">
\frac{11 * 10^{9} * 4(F P 32)}{1024^{3}} \approx 40 G B</script><p>光模型参数就占用了40GB的显存。</p>
<h1 id="2-模型训练优化方式"><a href="#2-模型训练优化方式" class="headerlink" title="2.模型训练优化方式"></a>2.模型训练优化方式</h1><h2 id="2-1-数据并行"><a href="#2-1-数据并行" class="headerlink" title="2.1 数据并行"></a>2.1 数据并行</h2><h3 id="1）协作通信"><a href="#1）协作通信" class="headerlink" title="(1）协作通信"></a>(1）协作通信</h3><h4 id="0）参数服务器"><a href="#0）参数服务器" class="headerlink" title="0）参数服务器"></a>0）参数服务器</h4><ol>
<li>有一个参数服务器。</li>
<li>前向传播</li>
</ol>
<ul>
<li>在每台设备上复制该参数</li>
<li>每个副本处理输入的一部分。</li>
</ul>
<ol>
<li>反向传播</li>
</ol>
<ul>
<li>每个副本的梯度取平均值。</li>
<li>平均梯度用于更新参数服务器</li>
</ul>
<p>在数据并行过程中，有一个参数服务器，它保持了模型的参数，以及完整的数据。前向传播过程中，参数服务器上的参数会被复制到所有的显卡上，这样每张显卡上都得到了和参数服务器一样的参数。然后把数据分成三份，每张显卡用这部分数据进行前向传播&amp;反向传播，得到各自的梯度，为了让模型学到这份数据的所有知识，需要把这些梯度信息进行聚合。这里用了一个取平均操作，然后让聚合好的参数去更新模型。就能学到这三部分数据合起来完整的知识。</p>
<p>参数服务器可以在0号显卡上，从0号显卡把模型的参数复制到1,2,3号显卡。这就像一个广播过程；而从1,2,3号显卡上对模型的梯度进行聚合(或规约)，把规约的结果放到服务器0号显卡上。</p>
<p><img src="image/image_Zej5DL5gWW.png" alt=""></p>
<h4 id="1）Broadcast-x20"><a href="#1）Broadcast-x20" class="headerlink" title="1）Broadcast &#x20;"></a>1）Broadcast &#x20;</h4><p>广播算子做的事情就是<strong>把数据从其中的一张显卡上传到其他所有的显卡上</strong>。可以看到通过广播之后，在原本第二张显卡上的<code>in</code>这个向量广播到所有显卡上变成了<code>out</code>向量。</p>
<p><img src="image/image_Y3ji-ayVqk.png" alt=""></p>
<h4 id="2）Reduce"><a href="#2）Reduce" class="headerlink" title="2）Reduce"></a>2）Reduce</h4><p>规约(Reduce)。规约有很多种种类，可以是<strong>求和、平均、最值</strong>等。会把各张显卡上的数据进行一个规约，然后把规约得到的结果放到一张指定的显卡里面。比如这里把规约的结果放到2号显卡里面。假设规约操作是求和，那么2号显卡最终得到的<code>out=int0+in1+in2+in3</code>。</p>
<p><img src="image/image_Q2dAvOKAf2.png" alt=""></p>
<h4 id="3）All-Reduce"><a href="#3）All-Reduce" class="headerlink" title="3）All Reduce"></a>3）All Reduce</h4><p>All Reduce。比规约多了一个All。在规约的基础上，<strong>把规约得到的结果告诉所有的显卡(All)</strong>。也就是说，最后得到的结果里面，每张显卡上都会得到完全一样的<code>out=in0+in1+in2+in3</code>。</p>
<p><img src="image/image_HVVERl5_RM.png" alt=""></p>
<h4 id="4）Reduce-Scatter"><a href="#4）Reduce-Scatter" class="headerlink" title="4）Reduce Scatter"></a>4）Reduce Scatter</h4><p>Reduce Scatter。和All Reduce的相同之处在于，都会把规约得到的结果发送给所有的显卡。不同之处在于，<strong>Reduce Scatter最后每张显卡上只得到了一部分的规约结果</strong>。比如0号显卡就会得到<code>in0</code>的前<code>1/4</code>的参数＋<code>in1</code>的前<code>1/4</code>参数＋<code>in2</code>的前<code>1/4</code>参数＋<code>in3</code>的前<code>1/4</code>参数。而3号显卡会得到<code>in0</code>的最后<code>1/4</code>的参数＋<code>in1</code>的最后<code>1/4</code>参数＋<code>in2</code>的最后<code>1/4</code>参数＋<code>in3</code>的最后<code>1/4</code>参数。</p>
<p><img src="image/image_AV3bl7SvL_.png" alt=""></p>
<h4 id="5）All-Gather"><a href="#5）All-Gather" class="headerlink" title="5）All Gather"></a>5）All Gather</h4><p>收集(All Gather)，<strong>拼接每张显卡上的结果</strong>。比如<code>in0</code>拼接<code>in1</code>拼接<code>in2</code>拼接<code>in3</code>得到0号显卡的<code>out</code>，然后广播到所有显卡上。</p>
<p><img src="image/image_TGSaKNJtGn.png" alt=""></p>
<h3 id="（2）数据并行"><a href="#（2）数据并行" class="headerlink" title="（2）数据并行"></a>（2）数据并行</h3><p>可以看到数据并行有两个核心点。</p>
<ol>
<li><strong>通过把数据分成很多份</strong>，让每张显卡计算得到各自梯度之后，为了得到所有数据的知识，需要把这些梯度进行一个规约操作。</li>
<li>通过使用参数服务器，让规约后的梯度去更新参数服务器上的参数。然后通过广播的操作，让每张显卡上<strong>同步</strong>得到更新之后的参数。</li>
</ol>
<h3 id="（3）分布式数据并行"><a href="#（3）分布式数据并行" class="headerlink" title="（3）分布式数据并行"></a>（3）分布式数据并行</h3><p>而分布式参数并行对此进行了优化，<strong>舍弃了专门的参数服务器</strong>，让每张显卡各自去完成参数的更新，保证它们参数更新之后的结果一致。</p>
<p>具体来说，初始时，每张显卡上都有一个相同的模型参数，得到了一部分数据。通过前向传播&amp;反向传播得到各自的梯度信息，然后对梯度信息进行一个规约。为了让每张显卡都得到相同的梯度信息，使用All Reduce，它会把规约结果告诉所有的显卡。这样，每一张显卡上都能得到完整的规约之后的梯度，每张显卡都有一样的参数，就可以分别通过模型的优化器进行更新。每轮更新之后，既然参数一样，梯度一样，优化器之前的历史信息一样，那么更新之后，各张显卡上的参数也会保持一致。</p>
<p><img src="image/image_n__Rw74uPL.png" alt=""></p>
<p>带来的显存上的优化</p>
<p>中间结果是一个和<code>batch</code>乘以句子长度和模型维度相关的显存占用。在使用数据并存的时候，把一批数据分成了很多份，让每张显卡只处理其中的一部分数据。每张显卡上所处理的<code>batch</code>大小就降低到了原来的显卡数量(n)分之一。通过把输入的维度进行了降低，那么模型整体的中间结果量也会进行降低。</p>
<p>缺点：数据较少时，参数，梯度，优化器都会保存到显卡上。</p>
<p><img src="image/image_iUyIpzpaFZ.png" alt=""></p>
<h2 id="2-2模型并行"><a href="#2-2模型并行" class="headerlink" title="2.2模型并行"></a>2.2模型并行</h2><p>一张显卡上无法存放模型的所有参数，那么就想办法把一个模型分成很多个小的部分。</p>
<p>比如针对线性层矩阵乘法的例子，假设有一个<code>3×2</code>的矩阵。它乘上一个 <code>2×1</code>的向量，那么本质上可以把它的结果分成三部分。</p>
<p>这里的 <code>3×2</code>的矩阵就是线性层中的参数 <code>W</code>，向量就是线性层的输入。可以通过矩阵乘法的性质，把模型的参数横向切成很多份(n)，最后得到线性层的结果就是很多个这样小的矩阵乘上线性层的输入，最后把结果进行拼接。</p>
<p>通过这样的方式，线性层的参数就可以划分到多张显卡上。同时需要保证多张显卡上模型的输入是一样的。那么就不能使用数据并行的方式对数据进行划分。</p>
<script type="math/tex; mode=display">
\left[\begin{array}{ll}1 & 2 \\ 3 & 4 \\ 5 & 6\end{array}\right]\left[\begin{array}{l}x \\ y\end{array}\right]=\left[\begin{array}{c}1 x+2 y \\ 0 \\ 0\end{array}\right]+\left[\begin{array}{c}0 \\ 3 x+4 y \\ 0\end{array}\right]+\left[\begin{array}{c}0 \\ 0 \\ 5 x+6 y\end{array}\right]</script><script type="math/tex; mode=display">
\begin{aligned} \mathbf{y}_{A} & =W_{A \times B} \mathbf{x}_{B} \\ & =\left[W_{\frac{A}{n} \times B}^{(1)} ; W_{\frac{A}{n} \times B}^{(2)} ; \cdots ; W_{\frac{A}{n} \times B}^{(n)}\right] \mathbf{x}_{B} \\ & =\left[W_{\frac{A}{n} \times B}^{(1)} \mathbf{x}_{B} ; W_{\frac{A}{n} \times B}^{(2)} \mathbf{x}_{B} ; \cdots ; W_{\frac{A}{n} \times B}^{(n)} \mathbf{x}_{B}\right]\end{aligned}</script><p><strong>需要保证每张显卡上的输入是一样的，是同样一批数据</strong>，<strong>这里对线性层参数进行划分</strong>。每张显卡上得到线性层参数矩阵的一小部分，通过这一小部分参数和数据进行矩阵乘法，就得到了很多个子结果。这里通过All Gather收集算子进行拼接，然后广播给所有的显卡。</p>
<p>这样，每张显卡上只需要保存原来的N分之一的模型参数，N是显卡数量。由于只保留了这么一小部分参数，梯度也只需要保留这么多，同时优化器也只需要保持同样级别的参数量。但模型计算的中间结果没有减少，这也是该方法的一个弊端。当batch size很大的时候，仍然会出现显存溢出的问题。</p>
<p><img src="image/image_Fr2DQZv1oj.png" alt=""></p>
<h2 id="2-3-ZeRO"><a href="#2-3-ZeRO" class="headerlink" title="2.3 ZeRO"></a>2.3 ZeRO</h2><p>Zero Redundancy优化器是基于<strong>数据并行</strong>建立的一套框架，在数据并行中需要对模型的梯度进行规约。为了保证每轮迭代之后每张显卡上的参数仍然是一致的。就让每张显卡都得到了规约后的参数。然后每张显卡各自进行更新。</p>
<p>可以发现每张显卡用的是同样的一批数据，和同样的一批梯度去进行参数更新。那么它们各自去进行参数优化，是不是就带来了计算上的重复和冗余。</p>
<p>为了消除这样的冗余，那么<strong>每张显卡只获得一部分的梯度，然后只更新一部分参数</strong>。这样多张显卡通过合作的方式来更新模型的完整参数。</p>
<p><img src="image/image_Ma5KyykqyG.png" alt=""></p>
<h3 id="（1）ZeRO-Stage-1-x20"><a href="#（1）ZeRO-Stage-1-x20" class="headerlink" title="（1）ZeRO-Stage 1 &#x20;"></a>（1）ZeRO-Stage 1 &#x20;</h3><p>具体来说，由于是基于数据并行的架构，因此<strong>每张显卡上保存了完整的模型参数</strong>。有一部分数据，通过前向传播&amp;反向传播得到各自的梯度。之后在规约的时候，不是使用All Reduce的方式，而是使用<strong>Reduce Scatter</strong>让每张显卡得到一部分reduce的结果。这样让<strong>每张显卡上得到的部分梯度去更新对应的部分模型参数，最后通过收集的操作All Gather将每张显卡分工合作之后的结果告诉所有的显卡</strong>。这样，每张显卡上得到了完全一样的参数和一致的结果。</p>
<p><img src="image/image_wLbzyqvGlW.png" alt=""></p>
<h3 id="（2）ZeRO-Stage-2-x20"><a href="#（2）ZeRO-Stage-2-x20" class="headerlink" title="（2）ZeRO-Stage 2 &#x20;"></a>（2）ZeRO-Stage 2 &#x20;</h3><p>在第2阶段中，进行了一个优化。在第1阶段中，需要在反向传播得到所有梯度之后，对梯度进行Reduce Scatter，然后让每张显卡上各得到一部分规约后的梯度<code>Gradient*</code>。 原来的梯度就不需要保存在显卡上了。<strong>在第1阶段，在反向传播结束之后，才把这个梯度移除</strong>。那可以在反向传播的过程中先把<code>Gradient*</code>算出来，然后把之前一步的<code>Gradient</code>删掉。</p>
<p><img src="image/image_S3573Un-4H.png" alt=""></p>
<h3 id="（3）ZeRO-Stage-3"><a href="#（3）ZeRO-Stage-3" class="headerlink" title="（3）ZeRO-Stage 3"></a>（3）ZeRO-Stage 3</h3><p>在第3阶段，<strong>对模型的参数进一步划分</strong>。因为每张显卡上只保留了一部分梯度去进行参数更新，参数更新也只更新一部分的模型参数。这样，<strong>实际上每张显卡可以只保存它自己参数更新所负责的那一部分参数</strong>。在FP\&amp;BP的过程中，需要的时候，把模型的参数进行一个All Gather的操作， 用完之后，就可以将参数从显卡中释放。</p>
<p>注意：反向传播也需要模型完整的参数</p>
<p><img src="image/image_kXpY1CXu7H.png" alt=""></p>
<h3 id="（4）总结"><a href="#（4）总结" class="headerlink" title="（4）总结"></a>（4）总结</h3><p>比较一下这三个阶段的显存占比：</p>
<p><img src="image/image_nuslQPD0jv.png" alt=""></p>
<p>在第1阶段中，每张显卡只需要处理一部分的模型梯度，优化器降低到了原来的显卡数分之一，同时把中间结果的量也降低到原来的卡数分之一； &#x20;</p>
<p>第2阶段中，进一步地把模型的梯度划分提前，把Reduce Scatter提前到了反向传播的过程中，实际上不需要保留完整的梯度。 &#x20;</p>
<p>第3阶段中，进一步地划分参数。</p>
<p>通过这三部分的优化，显卡上的四大组成部分：参数、梯度、优化器和中间结果都得到了划分，每张显卡只需要保持自己的那部分参数。</p>
<h2 id="2-4-Pipeline并行"><a href="#2-4-Pipeline并行" class="headerlink" title="2.4 Pipeline并行"></a>2.4 Pipeline并行</h2><p>与模型的并行方法有类似之处，模型并行的方法通过把线性层分成很多个小的矩阵，然后把这些小的矩阵分到各张显卡上。 &#x20;</p>
<p>而对流水线的并行方法，<strong>把模型的不同层分给不同的显卡</strong>。比如有一个三层的Transformer，可以把Transformer的第一层分到第一张显卡上；第二层分到第二张显卡上，等等。 &#x20;</p>
<p>进行前向传播的过程中，需要在第一张显卡上完成第一层的模型计算，然后把计算结果告诉第二张显卡，第二章显卡进行计算，再把计算结果传给下一张显卡。 &#x20;</p>
<p>可以看到，这样的方法，显存占比都得到了划分，因为每张显卡上只保留了某些层的参数，也只用保留对应的梯度。虽然没有使用数据并行的方法，但模型层数变少了，这样中间结果也得到了减少。 &#x20;</p>
<p>但这种方法存在的弊端在于，0号显卡计算的时候，1号和2号显卡实际上处于空闲的状态。</p>
<p><img src="image/image_lOIrlGft7h.png" alt=""></p>
<h2 id="2-5-优化技术细节"><a href="#2-5-优化技术细节" class="headerlink" title="2.5 优化技术细节"></a>2.5 优化技术细节</h2><h3 id="（1）混合精度"><a href="#（1）混合精度" class="headerlink" title="（1）混合精度"></a>（1）混合精度</h3><p>比如C语言中有<code>float</code>类型、<code>double</code>类型和<code>long double</code>类型。数值表示范围依次增大。</p>
<p><code>double</code>类型比<code>float</code>类型有更大的表示范围和更高的有效位精度，但是<code>double</code>类型的计算会更慢。 &#x20;</p>
<p>同理<code>FP16</code>和<code>FP32</code>是一样的，前者的数值表示范围和有效位数更小，同时计算会更快。 &#x20;</p>
<p>在**一般模型的训练中，可能使用<code>FP32</code>**<strong>作为默认训练参数的表示</strong>。实际上，模型的参数一般不会超过千这个数量级，那么完全可以使用<code>FP16</code>。</p>
<p>那能否从<code>FP32</code>转到<code>FP16</code>得到运行速度上的提升呢？其实会面临一个问题，在参数更新的时候，<code>权重=梯度*学习率</code>，一般学习率是比较小的：<code>1e-5</code>、<code>1e-3</code>等。而<code>FP16</code>能表示的最小值，是<code>1e-5</code>数量级的数，假如梯度乘上学习率低于<code>FP16</code>的表示范围，那么参数更新量就会产生丢失(下溢)。</p>
<p>那么既然<code>FP32</code>能达到出更高的表示范围，<strong>可以把</strong>**<code>FP16</code><strong><strong>的梯度乘上学习率得到的参数更新量表示为</strong></strong><code>FP32</code>**，但模型的参数是更低精度的<code>FP16</code>。那无法直接把参数更新量加到模型参数上，<strong>此时需要在优化器上额外保留单精度(**</strong><code>FP32</code><strong>**)的一个参数。</strong></p>
<p><img src="image/image_CJEG0f2EID.png" alt=""></p>
<p>在一般的模型训练中，模型会有<code>FP32</code>的参数和<code>FP32</code>的梯度，然后优化器会使用<code>FP32</code>的梯度进行参数优化。</p>
<p>而在混合精度训练中，为了加速模型的前向传播&amp;反向传播，模型中会使用半精度(<code>FP16</code>)的参数，和半精度的梯度，把梯度传到优化器里进行优化器的更新。<strong>同时把优化器的更新量保存为</strong>**<code>FP32</code><strong><strong>类型，把这个</strong></strong><code>FP32</code><strong><strong>类型通过优化器里临时创建的</strong></strong><code>FP32</code>**<strong>参数进行累积，之后转回到FP16的参数来与模型进行计算。</strong></p>
<h3 id="（2）Offloading"><a href="#（2）Offloading" class="headerlink" title="（2）Offloading"></a>（2）Offloading</h3><p>以Adam为例，<strong>优化器的参数量会是模型参数量两倍的关系</strong>，显然它是一个显存占用的大头。能否把它从显卡中移除呢？</p>
<p><img src="image/image_DgKmRdhBmk.png" alt=""></p>
<p>其实是可以的，<strong>可以把它从显卡上移到CPU上</strong>。 &#x20;</p>
<p>这样需要先把模型参数的梯度从显卡中传给CPU，在CPU上进行优化器的优化，将优化的结果传回显卡上。在使用了ZeRO3梯度优化之后，参数划分为显卡数分之一，通过把一张显卡绑定到多张CPU上，就可以让每张CPU上的计算量足够低，能让CPU不成为模型训练的瓶颈。</p>
<h3 id="（3）Overlapping"><a href="#（3）Overlapping" class="headerlink" title="（3）Overlapping"></a>（3）Overlapping</h3><p>通信的计算的重叠。在GPU中的内存操作一般是<strong>异步的</strong>，<strong>可以提前给内存发送一个请求，可以去进行其他的计算，其他计算完成之后，对那个内存请求进行接收</strong>。</p>
<p>在模型前向传播过程中，需要把Layer1的参数通过Gather操作，然后对Layer2的参数进行优化。在获得完Layer1参数之后，在Layer1前向传播计算过程中，异步地把Layer2参数的获得进行提前。在Layer1前向传播计算完之后，Layer2的参数也已经获得，那么就可以马上进行Layer2前向传播计算。</p>
<p><img src="image/image_x0Z1otLirj.png" alt=""></p>
<h3 id="（4）Checkpointing"><a href="#（4）Checkpointing" class="headerlink" title="（4）Checkpointing"></a>（4）Checkpointing</h3><p>Checkpointing就是检查点，就像单机游戏中的存档。</p>
<p>为了支持模型的反向传播，需要把模型计算的所有中间结果保持在显卡中，是否可以通过存档的方式进行优化。</p>
<p>即<strong>不把所有结果都保持到显卡中，而只保持一定的存档点</strong>。</p>
<p><img src="image/image_962n-kMp6b.png" alt=""></p>
<p>以Transformer为例，只保留Transformer大层的输入作为检查点，在反向传播过程中，那么如何为大层中的线性层梯度进行计算。此时可以通过<strong>重计算</strong>，就是说通过Transformer每个大层的输入，在反向传播过程中，重新对它进行一个前向的传播。临时得到每个大层里面所有线性层的输入，那么得到了中间结果，就可以进行反向传播。 &#x20;</p>
<p>完成了这一层的反向传播之后，就可以把检查点和临时重计算的中间结果从显存中清理掉。这样就不需要保存那么多中间结果。</p>
<h2 id="2-6-BMTrain——使用介绍"><a href="#2-6-BMTrain——使用介绍" class="headerlink" title="2.6 BMTrain——使用介绍"></a>2.6 BMTrain——使用介绍</h2><p>本小节介绍BMTrain性能上的提升。</p>
<p><img src="image/image_HNYBj_eL7i.png" alt=""></p>
<p>据说可以使用更少的机器，达到更快的速度。</p>
<p><a target="_blank" rel="noopener" href="https://colab.research.google.com/drive/1H-T7PmTjdcgwYUFfMikfxZ4_bMWRKu8h?usp=sharing" title="bmtrain_demo.ipynb - Colaboratory (google.com)">bmtrain_demo.ipynb - Colaboratory (google.com)</a></p>
<p><img src="image/image_8bXbGlYObE.png" alt=""></p>
<p>使用上也简单，替换一些包名前缀。就可以用到前面提到的一些技术。</p>
<h1 id="3-模型压缩"><a href="#3-模型压缩" class="headerlink" title="3.模型压缩"></a>3.模型压缩</h1><p>背景就是大模型的规模增长非常快。</p>
<p><img src="image/image_IuMy2GOKk9.png" alt=""></p>
<p>接下来介绍模型压缩的一些技术，目的是希望把大规模的模型压缩成更小规模。 &#x20;</p>
<p><img src="image/image_o0NJoIrwG0.png" alt=""></p>
<h2 id="3-1-知识蒸馏（Knowledge-Distillation）"><a href="#3-1-知识蒸馏（Knowledge-Distillation）" class="headerlink" title="3.1 知识蒸馏（Knowledge Distillation）"></a>3.1 知识蒸馏（Knowledge Distillation）</h2><p>什么是知识 ？</p>
<p>这里知识指的是<strong>模型的参数本身</strong>，本质是把模型从输入映射到输出的过程。知识蒸馏就是想把这种映射能力从大模型迁移到小模型上。</p>
<p><img src="image/image_bOKtx3jymv.png" alt=""></p>
<p>soft target比gold labels提供了更多的信息</p>
<p>对于输入数据，会有大模型作为Teacher，它会算出当前数据的预测结果，logits。 &#x20;</p>
<p>同时，该数据也可以输入给一个小得多的Student模型，该模型对于数据也能给出logits，知识蒸馏想做的事情是让这两个logits尽可能地接近。</p>
<p><img src="image/image_6ChngCARys.png" alt=""></p>
<h3 id="（1）PKD"><a href="#（1）PKD" class="headerlink" title="（1）PKD"></a>（1）PKD</h3><p>第一篇关于预训练模型的知识蒸馏工作称为PKD，它是面向BERT做的知识蒸馏。</p>
<blockquote>
<p>Sun et al. Patient Knowledge Distillation for BERT Model Compression. EMNLP 2019.</p>
</blockquote>
<p>它针<strong>对传统的知识蒸馏进行改进，让student模型可以从teacher模型中间层进行学习</strong>。 &#x20;</p>
<p>PKD针对模型很多层都有输出，或者说隐藏状态。它想做的事情是<strong>让student模型的隐藏状态和教师的尽可能接近</strong>。而不是仅拟合最终的输出。</p>
<p><img src="image/image_pih5x3mhf_.png" alt=""></p>
<h3 id="（2）TinyBERT"><a href="#（2）TinyBERT" class="headerlink" title="（2）TinyBERT"></a>（2）TinyBERT</h3><p>还有一个非常有代表性的工作是，TinyBERT。它进一步地推广了能学习的信号。<strong>从Teacher模型中找到了更多的可用于知识蒸馏的中间表示。</strong> 比如输入的嵌入向量以及Attention矩阵。</p>
<blockquote>
<p>Jiao et al. TinyBERT: Distilling BERT for Natural Language Understanding. Findings of EMNLP 2020</p>
</blockquote>
<p><img src="image/image_7_KbPnl6sM.png" alt=""></p>
<h2 id="3-2-模型剪枝"><a href="#3-2-模型剪枝" class="headerlink" title="3.2 模型剪枝"></a>3.2 模型剪枝</h2><p>这里剪枝做的事情，比如对于参数矩阵<code>W</code>，可能有很多元素非常接近于0。那么是否可以把这些参数丢掉。 &#x20;</p>
<p>核心是<strong>去除参数冗余部分</strong>，去除的依据是根据重要性，重要性最直观的依据是看元素绝对值大小，如果非常接近于0，那么就认为它不重要。</p>
<p>剪枝分为<strong>结构化剪枝</strong>和<strong>非结构化剪枝</strong>。 &#x20;</p>
<p>现在比较有用的是结构化剪枝，它考虑一次性删除矩阵中的一行/一列/一块。这样删掉之后矩阵还是一个比较规整的形状，从而比较利于并行化计算。</p>
<p><img src="image/image_ZX4JQKdfAn.png" alt=""></p>
<p>权重剪枝效果</p>
<ul>
<li>30-40%的权值可以被丢弃而不影响BERT的普适性(剪枝预训练)</li>
<li>对下游任务进行微调不会改变其性质(剪枝下游)</li>
</ul>
<p><img src="image/image_4lcwv3ArA5.png" alt=""></p>
<p>注意力剪枝（结构化）</p>
<ul>
<li>切除一个头</li>
<li>定义注意头的重要性分数</li>
</ul>
<script type="math/tex; mode=display">
I_{h}=\mathbb{E}_{x \sim X}\left|\operatorname{Att}_{h}(x)^{T} \frac{\partial \mathcal{L}(x)}{\partial \operatorname{Att}_{h}(x)}\right|</script><p>针对注意力中的冗余。如果把某个注意力head丢掉，观察对与机器翻译和语言理解任务上的影响，从图中可以看到，这种做法不一定会对模型造成负面的影响，甚至很多时候还带来结果的提升。</p>
<p><img src="image/image_JFX7_N7uH_.png" alt=""></p>
<p>在不同的模型上迭代地剪枝头(蓝线)</p>
<p><img src="image/image_-C3dqehV4P.png" alt=""></p>
<ul>
<li>层剪枝(结构化)</li>
<li>将dropout从权重扩展到层</li>
<li>训练：随机dropout层</li>
<li>测试：选择任意深度的sub-network</li>
</ul>
<p><img src="image/image_YQW30KDnGC.png" alt=""></p>
<h2 id="3-3-模型量化"><a href="#3-3-模型量化" class="headerlink" title="3.3 模型量化"></a>3.3 模型量化</h2><p>标准的神经网络数值计算是浮点计算，那么表示的位数相对多一些。观察发现，神经网络其实不需要这么高的精度，所以可以把浮点的表示转换成定精度的表示。</p>
<p><img src="image/image_ZVhlgGJNQM.png" alt=""></p>
<p>随着位数的降低，准确率的变化：</p>
<p><img src="image/image_-_SeUJMrWM.png" alt=""></p>
<h2 id="3-4-其他方法"><a href="#3-4-其他方法" class="headerlink" title="3.4 其他方法"></a>3.4 其他方法</h2><h3 id="（1）权重共享"><a href="#（1）权重共享" class="headerlink" title="（1）权重共享"></a>（1）权重共享</h3><p>ALBERT：两种参数缩减技术&#x20;</p>
<ul>
<li>将大的词表向量分解为两个小矩阵&#x20;</li>
<li>跨层参数共享</li>
</ul>
<blockquote>
<p>Lan et al. ALBERT: A Lite BERT for Self-supervised Learning of Language Representations. ICLR 2020.</p>
</blockquote>
<p><img src="image/image_jAJhCjUWew.png" alt=""></p>
<h3 id="（2）低阶近似（Low-rank-Approximation）"><a href="#（2）低阶近似（Low-rank-Approximation）" class="headerlink" title="（2）低阶近似（Low-rank Approximation）"></a>（2）低阶近似（Low-rank Approximation）</h3><script type="math/tex; mode=display">
\begin{array}{c}D=U \Sigma V^{\top} \in \mathbb{R}^{m \times n}, \quad m \geq n \quad \Sigma=: \operatorname{diag}\left(\sigma_{1}, \ldots, \sigma_{m}\right) \\ \widehat{D}^{*}=U_{1} \Sigma_{1} V_{1}^{\top}\end{array}</script><p>难以直接进行低秩近似</p>
<p>分解输入矩阵</p>
<p><img src="image/image_4iyN7F-eNd.png" alt=""></p>
<h3 id="（3）Architecture-Search"><a href="#（3）Architecture-Search" class="headerlink" title="（3）Architecture Search"></a>（3）Architecture Search</h3><p>Transformer架构是否是完美的？</p>
<ul>
<li>基于Transformer的神经结构搜索</li>
<li>预定义几个简单模块</li>
<li>对每个架构进行几个小时的训练</li>
</ul>
<blockquote>
<p>So et al. Primer: Searching for Efficient Transformersfor Language Modeling. NeurIPS 2021.</p>
</blockquote>
<p><img src="image/image_GQLZ5yGD_y.png" alt=""></p>
<p>两种高效的架构</p>
<p><img src="image/image_ItShAgwLyr.png" alt=""></p>
<h1 id="4-BMCook"><a href="#4-BMCook" class="headerlink" title="4.BMCook"></a>4.BMCook</h1><p>与现有的压缩工具包相比，BMCook支持所有主流的PLM加速方法</p>
<p><img src="image/image_rEGvhlD4vf.png" alt=""></p>
<ul>
<li>用几行代码实现不同的压缩方法</li>
<li>压缩方法可以以任何方式组合到极端加速</li>
</ul>
<p><img src="image/image_l1adDQ9qSe.png" alt=""></p>
<ul>
<li>BMCook的核心：模型压缩配置文件</li>
<li>用几行代码实现多种方法</li>
</ul>
<p><img src="image/image_fzXxIxQE1G.png" alt=""></p>
<p>蒸馏配置，支持MSE和CE损耗</p>
<p><img src="image/image_2ql7-vdv7T.png" alt=""></p>
<p>模型剪枝配置，支持非结构化剪枝</p>
<p><img src="image/image_2npDUZd_rI.png" alt=""></p>
<p>模型量化配置，更换所有线性模块</p>
<p><img src="image/image_klIoW-zp-r.png" alt=""></p>
<h1 id="5-BMInf"><a href="#5-BMInf" class="headerlink" title="5.BMInf"></a>5.BMInf</h1><p>BMInf是OpenBMB发布的第一个工具包。</p>
<p>Github repo: <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/BMInf" title="https://github.com/OpenBMB/BMInf">https://github.com/OpenBMB/BMInf</a></p>
<p>主要的目的是能在便宜的GPU，比如GTX 1060上，也能运行起来大模型。</p>
<p>消费级显卡运行大模型困难：</p>
<ol>
<li>高内存占用；</li>
<li>计算能力；</li>
</ol>
<h2 id="5-1-深入理解Transformer"><a href="#5-1-深入理解Transformer" class="headerlink" title="5.1 深入理解Transformer"></a>5.1 深入理解Transformer</h2><p>来深入分析模型，看如何优化模型。</p>
<p><img src="image/image_lfoHGz1KNm.png" alt=""></p>
<p>Transformer模型中主要的就是<strong>线性层</strong>，比如对于CMP-2中90%的参数都是在线性层中。</p>
<p>所以先来针对线性层。<strong>在允许一些精度损失的前提下，来优化线性层的运算效率</strong>。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/" title="https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/">https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/</a></p>
</blockquote>
<p><img src="image/image_igof4x4AGk.png" alt=""></p>
<p>目前常用的是<code>FP32</code>，但目前模型比较大，为了降低开销，逐渐在训练过程中引入<code>FP16</code>。</p>
<blockquote>
<p><code>FP16</code>示例：1.001， -1.001<br><code>FP8</code>示例：1.0， 1.25， 1.5</p>
</blockquote>
<p><code>INT8</code>：范围更小，但更准确</p>
<p><img src="image/image_EqH9HU_k8o.png" alt=""></p>
<p>为了进一步降低开销，有没有可能使用INT8来表示参数。</p>
<h2 id="5-2-量化"><a href="#5-2-量化" class="headerlink" title="5.2 量化"></a>5.2 量化</h2><p>使用整数来模拟浮点矩阵运算。</p>
<p>首先<strong>找到矩阵里面最大的那个数，然后缩放到<code>-127~127</code>，得到缩放系数。然后把浮点矩阵中所有元素除以该缩放系数</strong>，每个元素值经过四舍五入就能得到新的整数。这样可以把浮点数矩阵拆成缩放系数和一个整数矩阵。</p>
<p>就让能让矩阵中值从<code>FP16</code>变成了<code>INT8</code>。</p>
<p><img src="image/image_GHpC0ySs7e.png" alt=""></p>
<p>在完成了矩阵量化之后，如果用INT8来模拟矩阵乘法呢？</p>
<p>针对线性层来说，分别对它的<strong>输入和权重进行量化，就可以得到两个INT8的矩阵和对应的缩放系数</strong>。接着在这两个INT8的矩阵中进行矩阵乘法。这会得到一个整数结果，但该结果INT8是存不下来的，此时会用INT32来存储。同时针对缩放系数进行一个标量惩罚，得到一个新的缩放系数，然后把整数结果乘上这个新缩放系数还原成浮点数。</p>
<p><img src="image/image_vZFQQ83VJu.png" alt=""></p>
<p>但是该方法直接应用在Transformer上效果不理想。因为Transformer中矩阵太大，使用一个缩放因子有点困难。</p>
<p>此需要更加精细的量化方法。<strong>可以将量化的粒度从原来的整个矩阵变成一行或一列，计算单行/列的缩放系数。</strong> 这种方法能在Transformer上达到不错的效果。</p>
<p>使用这种方法可以使模型大小优化一半(11G)，但还是不能放到GTX 1060(6G)上。</p>
<p><img src="image/image_vM2Ps3NzHD.png" alt=""></p>
<h2 id="5-3-内存分配"><a href="#5-3-内存分配" class="headerlink" title="5.3 内存分配"></a>5.3 内存分配</h2><p>借鉴操作系统中<strong>虚拟内存</strong>机制。 &#x20;</p>
<p>在进行一个百亿模型推理的时候，实际上<strong>并不会同时用到这11G的参数</strong>，每次只用一部分。比如每次只计算一层，实际上只用到了这一层的参数。那些暂时不用计算的层没必要一直放到GPU上。</p>
<p>这种方法在<code>CUDA6</code>中被实现了。</p>
<p><img src="image/image_kKEU5LcgRq.png" alt=""></p>
<p><strong>如果能在计算一层的同时去加载另一层参</strong>数，那么理论上只需要两层，就可以让整个模型完美地运行起来。比如我们在计算第0层的时候，同时加载第1层。这样第0层计算完之后，就可以释放第0层所占的空间，去加载第1层的参数进行计算，同时加载第2层参数。</p>
<p><img src="image/image_IddWpXvxQr.png" alt=""></p>
<p>但实际操作上遇到了一些问题，</p>
<p>实际上<strong>传输一层参数的时间远远超过了计算该层参数所用的时间</strong>。如果只放两层参数的话，虽然占用空间小，但花费的时间反而特别长。那是否可以多放几层，来减少加载参数所用的开销。</p>
<p>假设<strong>一块GPU上能放n层参数，那么可以固定n-2层在GPU上，多余的2层空间用于调度</strong>。</p>
<p>那现在的问题是，哪些层固定？</p>
<p><img src="image/image_wWVZMGCERz.png" alt=""></p>
<p>假如两层需要从CPU加载，左边的方案是固定7,8,9，调度6和10。  右边是固定6,8,10，调度7个9。 &#x20;</p>
<p>这两种方法的区别在于，<strong>要加载的层之间的间隔</strong>，左边是间隔了3层，右边是间隔1层。</p>
<p>那么左边的方案肯定不会差于右边的，因为我们在加载完第6层之后，中间留下第7、8、9层计算的时间来加载第10层。即留给加载第10层的时间更长。</p>
<p>所以要<strong>尽量扩大需要加载的两层之间的间隔</strong>。</p>
<p><img src="image/image_1u1AB2tOUu.png" alt=""></p>
<h2 id="5-4-使用介绍"><a href="#5-4-使用介绍" class="headerlink" title="5.4 使用介绍"></a>5.4 使用介绍</h2><p>在实现了上面的技术(BMInf包)之后，终于可以把百亿参数模型放到GTX1060上运行起来。</p>
<p><img src="image/image_-jhc7BI2Xa.png" alt=""></p>
<p>那么这么好的工具包怎么使用呢？</p>
<p><img src="image/image_Iv8Tr0mL2m.png" alt=""></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/llms/llms_course/5.%E9%AB%98%E6%95%88%E8%AE%AD%E7%BB%83_%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/">https://wdndev.github.io/llms/llms_course/5.高效训练_模型压缩/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/LLMs/">LLMs</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/llms/llms_idx/" title="LLMs 目录"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-05</div><div class="title">LLMs 目录</div></div></a></div><div><a href="/llms/llms_article/1.llm%E6%8E%A8%E7%90%86%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF/" title="LLMs 推理优化技术"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-03</div><div class="title">LLMs 推理优化技术</div></div></a></div><div><a href="/llms/llms_article/2.%E4%B8%BB%E6%B5%81%E5%A4%A7%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86%E7%BB%86%E8%8A%82/" title="主流大语言模型的技术原理细节"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-12-03</div><div class="title">主流大语言模型的技术原理细节</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E8%83%8C%E6%99%AF%E4%BB%8B%E7%BB%8D"><span class="toc-text">1.背景介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-CPU-amp-GPU"><span class="toc-text">1.1 CPU &amp; GPU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-%E6%98%BE%E5%AD%98%E7%BB%84%E6%88%90"><span class="toc-text">1.2 显存组成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E5%8F%82%E6%95%B0"><span class="toc-text">（1）参数</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%A2%AF%E5%BA%A6"><span class="toc-text">（2）梯度</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E4%B8%AD%E9%97%B4%E7%BB%93%E6%9E%9C"><span class="toc-text">（3）中间结果</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-text">（4）优化器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%EF%BC%885%EF%BC%89%E7%A4%BA%E4%BE%8B"><span class="toc-text">（5）示例</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F"><span class="toc-text">2.模型训练优化方式</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1-%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.1 数据并行</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1%EF%BC%89%E5%8D%8F%E4%BD%9C%E9%80%9A%E4%BF%A1"><span class="toc-text">(1）协作通信</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#0%EF%BC%89%E5%8F%82%E6%95%B0%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-text">0）参数服务器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1%EF%BC%89Broadcast-x20"><span class="toc-text">1）Broadcast  </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2%EF%BC%89Reduce"><span class="toc-text">2）Reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3%EF%BC%89All-Reduce"><span class="toc-text">3）All Reduce</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4%EF%BC%89Reduce-Scatter"><span class="toc-text">4）Reduce Scatter</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#5%EF%BC%89All-Gather"><span class="toc-text">5）All Gather</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-text">（2）数据并行</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E5%88%86%E5%B8%83%E5%BC%8F%E6%95%B0%E6%8D%AE%E5%B9%B6%E8%A1%8C"><span class="toc-text">（3）分布式数据并行</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E6%A8%A1%E5%9E%8B%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.2模型并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-3-ZeRO"><span class="toc-text">2.3 ZeRO</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89ZeRO-Stage-1-x20"><span class="toc-text">（1）ZeRO-Stage 1  </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89ZeRO-Stage-2-x20"><span class="toc-text">（2）ZeRO-Stage 2  </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89ZeRO-Stage-3"><span class="toc-text">（3）ZeRO-Stage 3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89%E6%80%BB%E7%BB%93"><span class="toc-text">（4）总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-4-Pipeline%E5%B9%B6%E8%A1%8C"><span class="toc-text">2.4 Pipeline并行</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-5-%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF%E7%BB%86%E8%8A%82"><span class="toc-text">2.5 优化技术细节</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%B7%B7%E5%90%88%E7%B2%BE%E5%BA%A6"><span class="toc-text">（1）混合精度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89Offloading"><span class="toc-text">（2）Offloading</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Overlapping"><span class="toc-text">（3）Overlapping</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89Checkpointing"><span class="toc-text">（4）Checkpointing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-6-BMTrain%E2%80%94%E2%80%94%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D"><span class="toc-text">2.6 BMTrain——使用介绍</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9"><span class="toc-text">3.模型压缩</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-%E7%9F%A5%E8%AF%86%E8%92%B8%E9%A6%8F%EF%BC%88Knowledge-Distillation%EF%BC%89"><span class="toc-text">3.1 知识蒸馏（Knowledge Distillation）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89PKD"><span class="toc-text">（1）PKD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89TinyBERT"><span class="toc-text">（2）TinyBERT</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E6%A8%A1%E5%9E%8B%E5%89%AA%E6%9E%9D"><span class="toc-text">3.2 模型剪枝</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E6%A8%A1%E5%9E%8B%E9%87%8F%E5%8C%96"><span class="toc-text">3.3 模型量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-%E5%85%B6%E4%BB%96%E6%96%B9%E6%B3%95"><span class="toc-text">3.4 其他方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB"><span class="toc-text">（1）权重共享</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%BD%8E%E9%98%B6%E8%BF%91%E4%BC%BC%EF%BC%88Low-rank-Approximation%EF%BC%89"><span class="toc-text">（2）低阶近似（Low-rank Approximation）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Architecture-Search"><span class="toc-text">（3）Architecture Search</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-BMCook"><span class="toc-text">4.BMCook</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-BMInf"><span class="toc-text">5.BMInf</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Transformer"><span class="toc-text">5.1 深入理解Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-%E9%87%8F%E5%8C%96"><span class="toc-text">5.2 量化</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="toc-text">5.3 内存分配</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-4-%E4%BD%BF%E7%94%A8%E4%BB%8B%E7%BB%8D"><span class="toc-text">5.4 使用介绍</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>