{"timestamp":"2025-07-15T15:56:20.904Z","totalItems":40,"items":[{"id":"arxiv-2507.10535v1-1752594979339","title":"CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks","description":"Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.","url":"https://arxiv.org/abs/2507.10535v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:56:29.000Z","metadata":{"arxivId":"2507.10535v1","authors":"Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan","categories":"cs.CL, cs.AI, cs.SE","published":"2025-07-14T17:56:29Z","pdfUrl":"https://arxiv.org/pdf/2507.10535v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10535v1","rank":1,"domain":"LLM"}},{"id":"arxiv-2507.10472v1-1752594979339","title":"MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking","description":"This paper introduces an innovative Applicant Tracking System (ATS) enhanced by a novel Robotic process automation (RPA) framework or as further referred to as MLAR. Traditional recruitment processes often encounter bottlenecks in resume screening and candidate shortlisting due to time and resource constraints. MLAR addresses these challenges employing Large Language Models (LLMs) in three distinct layers: extracting key characteristics from job postings in the first layer, parsing applicant resume to identify education, experience, skills in the second layer, and similarity matching in the third layer. These features are then matched through advanced semantic algorithms to identify the best candidates efficiently. Our approach integrates seamlessly into existing RPA pipelines, automating resume parsing, job matching, and candidate notifications. Extensive performance benchmarking shows that MLAR outperforms the leading RPA platforms, including UiPath and Automation Anywhere, in high-volume resume-processing tasks. When processing 2,400 resumes, MLAR achieved an average processing time of 5.4 seconds per resume, reducing processing time by approximately 16.9% compared to Automation Anywhere and 17.1% compared to UiPath. These results highlight the potential of MLAR to transform recruitment workflows by providing an efficient, accurate, and scalable solution tailored to modern hiring needs.","url":"https://arxiv.org/abs/2507.10472v1","source":"ArXiv Domain","timestamp":"2025-07-14T16:53:19.000Z","metadata":{"arxivId":"2507.10472v1","authors":"Mohamed T. Younes, Omar Walid, Mai Hassan, Ali Hamdi","categories":"cs.CL","published":"2025-07-14T16:53:19Z","pdfUrl":"https://arxiv.org/pdf/2507.10472v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10472v1","rank":2,"domain":"LLM"}},{"id":"arxiv-2504.12355v2-1752594979339","title":"Leveraging Large Language Models for Multi-Class and Multi-Label Detection of Drug Use and Overdose Symptoms on Social Media","description":"Drug overdose remains a critical global health issue, often driven by misuse of opioids, painkillers, and psychiatric medications. Traditional research methods face limitations, whereas social media offers real-time insights into self-reported substance use and overdose symptoms. This study proposes an AI-driven NLP framework trained on annotated social media data to detect commonly used drugs and associated overdose symptoms. Using a hybrid annotation strategy with LLMs and human annotators, we applied traditional ML models, neural networks, and advanced transformer-based models. Our framework achieved 98% accuracy in multi-class and 97% in multi-label classification, outperforming baseline models by up to 8%. These findings highlight the potential of AI for supporting public health surveillance and personalized intervention strategies.","url":"https://arxiv.org/abs/2504.12355v2","source":"ArXiv Domain","timestamp":"2025-04-16T02:33:19.000Z","metadata":{"arxivId":"2504.12355v2","authors":"Muhammad Ahmad, Fida Ullah, Ummhy Habiba, ldar Batyrshin, Grigori Sidorov","categories":"cs.CL, cs.AI, cs.SI","published":"2025-04-16T02:33:19Z","pdfUrl":"https://arxiv.org/pdf/2504.12355v2.pdf","abstractUrl":"https://arxiv.org/abs/2504.12355v2","rank":3,"domain":"LLM"}},{"id":"arxiv-2507.10445v1-1752594979339","title":"Referential ambiguity and clarification requests: comparing human and LLM behaviour","description":"In this work we examine LLMs' ability to ask clarification questions in task-oriented dialogues that follow the asynchronous instruction-giver/instruction-follower format. We present a new corpus that combines two existing annotations of the Minecraft Dialogue Corpus -- one for reference and ambiguity in reference, and one for SDRT including clarifications -- into a single common format providing the necessary information to experiment with clarifications and their relation to ambiguity. With this corpus we compare LLM actions with original human-generated clarification questions, examining how both humans and LLMs act in the case of ambiguity. We find that there is only a weak link between ambiguity and humans producing clarification questions in these dialogues, and low correlation between humans and LLMs. Humans hardly ever produce clarification questions for referential ambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce more clarification questions for referential ambiguity, but less so for task uncertainty. We question if LLMs' ability to ask clarification questions is predicated on their recent ability to simulate reasoning, and test this with different reasoning approaches, finding that reasoning does appear to increase question frequency and relevancy.","url":"https://arxiv.org/abs/2507.10445v1","source":"ArXiv Domain","timestamp":"2025-07-14T16:28:00.000Z","metadata":{"arxivId":"2507.10445v1","authors":"Chris Madge, Matthew Purver, Massimo Poesio","categories":"cs.CL, cs.AI","published":"2025-07-14T16:28:00Z","pdfUrl":"https://arxiv.org/pdf/2507.10445v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10445v1","rank":4,"domain":"LLM"}},{"id":"arxiv-2507.05285v2-1752594979339","title":"Beyond classical and contemporary models: a transformative AI framework for student dropout prediction in distance learning using RAG, Prompt engineering, and Cross-modal fusion","description":"Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors,and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., \"isolation,\" \"workload anxiety\"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk pro-files. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems","url":"https://arxiv.org/abs/2507.05285v2","source":"ArXiv Domain","timestamp":"2025-07-04T21:41:43.000Z","metadata":{"arxivId":"2507.05285v2","authors":"Miloud Mihoubi, Meriem Zerkouk, Belkacem Chikhaoui","categories":"cs.CL, cs.AI, cs.CY, cs.IR, I.2.7; I.2.1; K.3.1","published":"2025-07-04T21:41:43Z","pdfUrl":"https://arxiv.org/pdf/2507.05285v2.pdf","abstractUrl":"https://arxiv.org/abs/2507.05285v2","rank":5,"domain":"LLM"}},{"id":"arxiv-2410.06238v2-1752594979339","title":"EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration","description":"Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.","url":"https://arxiv.org/abs/2410.06238v2","source":"ArXiv Domain","timestamp":"2024-10-08T17:54:03.000Z","metadata":{"arxivId":"2410.06238v2","authors":"Allen Nie, Yi Su, Bo Chang, Jonathan N. Lee, Ed H. Chi, Quoc V. Le, Minmin Chen","categories":"cs.LG, cs.AI, cs.CL","published":"2024-10-08T17:54:03Z","pdfUrl":"https://arxiv.org/pdf/2410.06238v2.pdf","abstractUrl":"https://arxiv.org/abs/2410.06238v2","rank":6,"domain":"LLM"}},{"id":"arxiv-2506.22791v2-1752594979339","title":"ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models","description":"Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.","url":"https://arxiv.org/abs/2506.22791v2","source":"ArXiv Domain","timestamp":"2025-06-28T07:25:12.000Z","metadata":{"arxivId":"2506.22791v2","authors":"Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren","categories":"cs.CL, cs.DB","published":"2025-06-28T07:25:12Z","pdfUrl":"https://arxiv.org/pdf/2506.22791v2.pdf","abstractUrl":"https://arxiv.org/abs/2506.22791v2","rank":7,"domain":"LLM"}},{"id":"arxiv-2507.10300v1-1752594979339","title":"FaceLLM: A Multimodal Large Language Model for Face Understanding","description":"Multimodal large language models (MLLMs) have shown remarkable performance in vision-language tasks. However, existing MLLMs are primarily trained on generic datasets, limiting their ability to reason on domain-specific visual cues such as those in facial images. In particular, tasks that require detailed understanding of facial structure, expression, emotion, and demographic features remain underexplored by MLLMs due to the lack of large-scale annotated face image-text datasets. In this work, we introduce FaceLLM, a multimodal large language model trained specifically for facial image understanding. To construct the training data, we propose a novel weakly supervised pipeline that uses ChatGPT with attribute-aware prompts to generate high-quality question-answer pairs based on images from the FairFace dataset. The resulting corpus, called FairFaceGPT, covers a diverse set of attributes including expression, pose, skin texture, and forensic information. Our experiments demonstrate that FaceLLM improves the performance of MLLMs on various face-centric tasks and achieves state-of-the-art performance. This work highlights the potential of synthetic supervision via language models for building domain-specialized MLLMs, and sets a precedent for trustworthy, human-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM models are publicly available in the project page.","url":"https://arxiv.org/abs/2507.10300v1","source":"ArXiv Domain","timestamp":"2025-07-14T14:04:14.000Z","metadata":{"arxivId":"2507.10300v1","authors":"Hatef Otroshi Shahreza, Sébastien Marcel","categories":"cs.CV, cs.AI, cs.CL","published":"2025-07-14T14:04:14Z","pdfUrl":"https://arxiv.org/pdf/2507.10300v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10300v1","rank":8,"domain":"LLM"}},{"id":"arxiv-2502.12992v2-1752594979339","title":"B-cos LM: Efficiently Transforming Pre-trained Language Models for Improved Explainability","description":"Post-hoc explanation methods for black-box models often struggle with faithfulness and human interpretability due to the lack of explainability in current neural architectures. Meanwhile, B-cos networks have been introduced to improve model explainability by proposing an architecture that removes bias terms and promotes input-weight alignment. Although B-cos networks have shown success in building explainable systems, their application has so far been limited to computer vision models and their associated training pipelines. In this work, we introduce B-cos LMs, i.e., B-cos language models (LMs) empowered for natural language processing (NLP) tasks. Our approach directly transforms pre-trained language models into B-cos LMs by combining B-cos conversion and task fine-tuning, improving efficiency compared to previous methods. Our automatic and human evaluation results demonstrate that B-cos LMs produce more faithful and human interpretable explanations than post-hoc methods, while maintaining task performance comparable to conventional fine-tuning. Our in-depth analysis explores how B-cos LMs differ from conventionally fine-tuned models in their learning processes and explanation patterns. Finally, we are also the first to explore the transformation of decoder-only models to B-cos LMs for generation tasks.","url":"https://arxiv.org/abs/2502.12992v2","source":"ArXiv Domain","timestamp":"2025-02-18T16:13:08.000Z","metadata":{"arxivId":"2502.12992v2","authors":"Yifan Wang, Sukrut Rao, Ji-Ung Lee, Mayank Jobanputra, Vera Demberg","categories":"cs.CL, cs.AI","published":"2025-02-18T16:13:08Z","pdfUrl":"https://arxiv.org/pdf/2502.12992v2.pdf","abstractUrl":"https://arxiv.org/abs/2502.12992v2","rank":9,"domain":"LLM"}},{"id":"arxiv-2507.10216v1-1752594979340","title":"Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects","description":"As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.","url":"https://arxiv.org/abs/2507.10216v1","source":"ArXiv Domain","timestamp":"2025-07-14T12:33:07.000Z","metadata":{"arxivId":"2507.10216v1","authors":"Renad Al-Monef, Hassan Alhuzali, Nora Alturayeif, Ashwag Alasmari","categories":"cs.CL, cs.AI","published":"2025-07-14T12:33:07Z","pdfUrl":"https://arxiv.org/pdf/2507.10216v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10216v1","rank":10,"domain":"LLM"}},{"id":"arxiv-2507.10522v1-1752594980062","title":"DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology","description":"We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system for automated scientific synthesis that supports recursive, depth- and breadth-controlled exploration of original research questions -- enhancing search diversity and nuance in the retrieval of relevant scientific literature. Unlike conventional retrieval-augmented generation pipelines, DeepResearch enables user-controllable synthesis with transparent reasoning and parameter-driven configurability, facilitating high-throughput integration of domain-specific evidence while maintaining analytical rigor. Applied to 49 ecological research questions, DeepResearch achieves up to a 21-fold increase in source integration and a 14.9-fold rise in sources integrated per 1,000 words. High-parameter settings yield expert-level analytical depth and contextual diversity. Source code available at: https://github.com/sciknoworg/deep-research.","url":"https://arxiv.org/abs/2507.10522v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:47:28.000Z","metadata":{"arxivId":"2507.10522v1","authors":"Jennifer D'Souza, Endres Keno Sander, Andrei Aioanei","categories":"cs.AI, cs.CL, cs.MA","published":"2025-07-14T17:47:28Z","pdfUrl":"https://arxiv.org/pdf/2507.10522v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10522v1","rank":1,"domain":"Agent"}},{"id":"arxiv-2505.00684v2-1752594980062","title":"Visual Test-time Scaling for GUI Agent Grounding","description":"We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\\% on Screenspot-pro and 24+\\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.","url":"https://arxiv.org/abs/2505.00684v2","source":"ArXiv Domain","timestamp":"2025-05-01T17:45:59.000Z","metadata":{"arxivId":"2505.00684v2","authors":"Tiange Luo, Lajanugen Logeswaran, Justin Johnson, Honglak Lee","categories":"cs.CV, cs.AI, cs.LG","published":"2025-05-01T17:45:59Z","pdfUrl":"https://arxiv.org/pdf/2505.00684v2.pdf","abstractUrl":"https://arxiv.org/abs/2505.00684v2","rank":2,"domain":"Agent"}},{"id":"arxiv-2507.10457v1-1752594980062","title":"Logic layer Prompt Control Injection (LPCI): A Novel Security Vulnerability Class in Agentic Systems","description":"The integration of large language models (LLMs) into enterprise systems has created a new class of covert security vulnerabilities, particularly within logic-execution layers and persistent-memory contexts. In this paper, we introduce Logic-Layer Prompt Control Injection (LPCI), a novel attack category in which encoded, delayed, and conditionally triggered payloads are embedded in memory, vector stores, or tool outputs. These payloads can bypass conventional input filters and trigger unauthorised behaviour across sessions.","url":"https://arxiv.org/abs/2507.10457v1","source":"ArXiv Domain","timestamp":"2025-07-14T16:37:05.000Z","metadata":{"arxivId":"2507.10457v1","authors":"Hammad Atta, Ken Huang, Manish Bhatt, Kamal Ahmed, Muhammad Aziz Ul Haq, Yasir Mehmood","categories":"cs.CR, cs.AI, cs.LG","published":"2025-07-14T16:37:05Z","pdfUrl":"https://arxiv.org/pdf/2507.10457v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10457v1","rank":3,"domain":"Agent"}},{"id":"arxiv-2503.16465v3-1752594980062","title":"OS-Kairos: Adaptive Interaction for MLLM-Powered GUI Agents","description":"Autonomous graphical user interface (GUI) agents powered by multimodal large language models have shown great promise. However, a critical yet underexplored issue persists: over-execution, where the agent executes tasks in a fully autonomous way, without adequate assessment of its action confidence to compromise an adaptive human-agent collaboration. This poses substantial risks in complex scenarios, such as those involving ambiguous user instructions, unexpected interruptions, and environmental hijacks. To address the issue, we introduce OS-Kairos, an adaptive GUI agent capable of predicting confidence levels at each interaction step and efficiently deciding whether to act autonomously or seek human intervention. OS-Kairos is developed through two key mechanisms: (i) collaborative probing that annotates confidence scores at each interaction step; (ii) confidence-driven interaction that leverages these confidence scores to elicit the ability of adaptive interaction. Experimental results show that OS-Kairos substantially outperforms existing models on our curated dataset featuring complex scenarios, as well as on established benchmarks such as AITZ and Meta-GUI, with 24.59\\%$\\sim$87.29\\% improvements in task success rate. OS-Kairos facilitates an adaptive human-agent collaboration, prioritizing effectiveness, generality, scalability, and efficiency for real-world GUI interaction. The dataset and codes are available at https://github.com/Wuzheng02/OS-Kairos.","url":"https://arxiv.org/abs/2503.16465v3","source":"ArXiv Domain","timestamp":"2025-02-26T12:31:16.000Z","metadata":{"arxivId":"2503.16465v3","authors":"Pengzhou Cheng, Zheng Wu, Zongru Wu, Aston Zhang, Zhuosheng Zhang, Gongshen Liu","categories":"cs.HC, cs.AI","published":"2025-02-26T12:31:16Z","pdfUrl":"https://arxiv.org/pdf/2503.16465v3.pdf","abstractUrl":"https://arxiv.org/abs/2503.16465v3","rank":4,"domain":"Agent"}},{"id":"arxiv-2409.10320v3-1752594980062","title":"SEAL: Towards Safe Autonomous Driving via Skill-Enabled Adversary Learning for Closed-Loop Scenario Generation","description":"Verification and validation of autonomous driving (AD) systems and components is of increasing importance, as such technology increases in real-world prevalence. Safety-critical scenario generation is a key approach to robustify AD policies through closed-loop training. However, existing approaches for scenario generation rely on simplistic objectives, resulting in overly-aggressive or non-reactive adversarial behaviors. To generate diverse adversarial yet realistic scenarios, we propose SEAL, a scenario perturbation approach which leverages learned objective functions and adversarial, human-like skills. SEAL-perturbed scenarios are more realistic than SOTA baselines, leading to improved ego task success across real-world, in-distribution, and out-of-distribution scenarios, of more than 20%. To facilitate future research, we release our code and tools: https://github.com/cmubig/SEAL","url":"https://arxiv.org/abs/2409.10320v3","source":"ArXiv Domain","timestamp":"2024-09-16T14:33:21.000Z","metadata":{"arxivId":"2409.10320v3","authors":"Benjamin Stoler, Ingrid Navarro, Jonathan Francis, Jean Oh","categories":"cs.RO, cs.AI, cs.LG","published":"2024-09-16T14:33:21Z","pdfUrl":"https://arxiv.org/pdf/2409.10320v3.pdf","abstractUrl":"https://arxiv.org/abs/2409.10320v3","rank":5,"domain":"Agent"}},{"id":"arxiv-2507.10281v1-1752594980062","title":"Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence","description":"Tables are fundamental in domains such as finance, healthcare, and public administration, yet real-world table tasks often involve noise, structural heterogeneity, and semantic complexity--issues underexplored in existing research that primarily targets clean academic datasets. This survey focuses on LLM-based Table Agents, which aim to automate table-centric workflows by integrating preprocessing, reasoning, and domain adaptation. We define five core competencies--C1: Table Structure Understanding, C2: Table and Query Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze and compare current approaches. In addition, a detailed examination of the Text-to-SQL Agent reveals a performance gap between academic benchmarks and real-world scenarios, especially for open-source models. Finally, we provide actionable insights to improve the robustness, generalization, and efficiency of LLM-based Table Agents in practical settings.","url":"https://arxiv.org/abs/2507.10281v1","source":"ArXiv Domain","timestamp":"2025-07-14T13:48:13.000Z","metadata":{"arxivId":"2507.10281v1","authors":"Jiaming Tian, Liyao Li, Wentao Ye, Haobo Wang, Lingxin Wang, Lihua Yu, Zujie Ren, Gang Chen, Junbo Zhao","categories":"cs.AI, cs.DB","published":"2025-07-14T13:48:13Z","pdfUrl":"https://arxiv.org/pdf/2507.10281v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10281v1","rank":6,"domain":"Agent"}},{"id":"arxiv-2507.10142v1-1752594980062","title":"Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review","description":"Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in coordinating multiple agents across simulated benchmarks and constrained scenarios. However, its deployment in real-world multi-agent systems (MAS) remains limited, primarily due to the complex and dynamic nature of such environments. These challenges arise from multiple interacting sources of variability, including fluctuating agent populations, evolving task goals, and inconsistent execution conditions. Together, these factors demand that MARL algorithms remain effective under continuously changing system configurations and operational demands. To better capture and assess this capacity for adjustment, we introduce the concept of \\textit{adaptability} as a unified and practically grounded lens through which to evaluate the reliability of MARL algorithms under shifting conditions, broadly referring to any changes in the environment dynamics that may occur during learning or execution. Centred on the notion of adaptability, we propose a structured framework comprising three key dimensions: learning adaptability, policy adaptability, and scenario-driven adaptability. By adopting this adaptability perspective, we aim to support more principled assessments of MARL performance beyond narrowly defined benchmarks. Ultimately, this survey contributes to the development of algorithms that are better suited for deployment in dynamic, real-world multi-agent systems.","url":"https://arxiv.org/abs/2507.10142v1","source":"ArXiv Domain","timestamp":"2025-07-14T10:39:17.000Z","metadata":{"arxivId":"2507.10142v1","authors":"Siyi Hu, Mohamad A Hady, Jianglin Qiao, Jimmy Cao, Mahardhika Pratama, Ryszard Kowalczyk","categories":"cs.AI, cs.LG, cs.MA","published":"2025-07-14T10:39:17Z","pdfUrl":"https://arxiv.org/pdf/2507.10142v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10142v1","rank":7,"domain":"Agent"}},{"id":"arxiv-2507.10073v1-1752594980062","title":"Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires","description":"Are AI systems truly representing human values, or merely averaging across them? Our study suggests a concerning reality: Large Language Models (LLMs) fail to represent diverse cultural moral frameworks despite their linguistic capabilities. We expose significant gaps between AI-generated and human moral intuitions by applying the Moral Foundations Questionnaire across 19 cultural contexts. Comparing multiple state-of-the-art LLMs' origins against human baseline data, we find these models systematically homogenize moral diversity. Surprisingly, increased model size doesn't consistently improve cultural representation fidelity. Our findings challenge the growing use of LLMs as synthetic populations in social science research and highlight a fundamental limitation in current AI alignment approaches. Without data-driven alignment beyond prompting, these systems cannot capture the nuanced, culturally-specific moral intuitions. Our results call for more grounded alignment objectives and evaluation metrics to ensure AI systems represent diverse human values rather than flattening the moral landscape.","url":"https://arxiv.org/abs/2507.10073v1","source":"ArXiv Domain","timestamp":"2025-07-14T08:59:26.000Z","metadata":{"arxivId":"2507.10073v1","authors":"Simon Münker","categories":"cs.CL, cs.AI","published":"2025-07-14T08:59:26Z","pdfUrl":"https://arxiv.org/pdf/2507.10073v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10073v1","rank":8,"domain":"Agent"}},{"id":"arxiv-2408.11415v2-1752594980062","title":"Political Bias in LLMs: Unaligned Moral Values in Agent-centric Simulations","description":"Contemporary research in social sciences increasingly utilizes state-of-the-art generative language models to annotate or generate content. While these models achieve benchmark-leading performance on common language tasks, their application to novel out-of-domain tasks remains insufficiently explored. To address this gap, we investigate how personalized language models align with human responses on the Moral Foundation Theory Questionnaire. We adapt open-source generative language models to different political personas and repeatedly survey these models to generate synthetic data sets where model-persona combinations define our sub-populations. Our analysis reveals that models produce inconsistent results across multiple repetitions, yielding high response variance. Furthermore, the alignment between synthetic data and corresponding human data from psychological studies shows a weak correlation, with conservative persona-prompted models particularly failing to align with actual conservative populations. These results suggest that language models struggle to coherently represent ideologies through in-context prompting due to their alignment process. Thus, using language models to simulate social interactions requires measurable improvements in in-context optimization or parameter manipulation to align with psychological and sociological stereotypes properly.","url":"https://arxiv.org/abs/2408.11415v2","source":"ArXiv Domain","timestamp":"2024-08-21T08:20:41.000Z","metadata":{"arxivId":"2408.11415v2","authors":"Simon Münker","categories":"cs.CL, cs.AI","published":"2024-08-21T08:20:41Z","pdfUrl":"https://arxiv.org/pdf/2408.11415v2.pdf","abstractUrl":"https://arxiv.org/abs/2408.11415v2","rank":9,"domain":"Agent"}},{"id":"arxiv-2507.10000v1-1752594980062","title":"On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model","description":"Since Searle's work deconstructing intent and intentionality in the realm of philosophy, the practical meaning of intent has received little attention in science and technology. Intentionality and context are both central to the scope of Promise Theory's model of Semantic Spacetime, used as an effective Tiny Language Model. One can identify themes and concepts from a text, on a low level (without knowledge of the specific language) by using process coherence as a guide. Any agent process can assess superficially a degree of latent `intentionality' in data by looking for anomalous multi-scale anomalies and assessing the work done to form them. Scale separation can be used to sort parts into `intended' content and `ambient context', using the spacetime coherence as a measure. This offers an elementary but pragmatic interpretation of latent intentionality for very low computational cost, and without reference to extensive training or reasoning capabilities. The process is well within the reach of basic organisms as it does not require large scale artificial probabilistic batch processing. The level of concept formation depends, however, on the memory capacity of the agent.","url":"https://arxiv.org/abs/2507.10000v1","source":"ArXiv Domain","timestamp":"2025-07-14T07:34:58.000Z","metadata":{"arxivId":"2507.10000v1","authors":"Mark Burgess","categories":"cs.AI, cs.CL, I.2.11; F.4.1; I.2.4; G.2.2","published":"2025-07-14T07:34:58Z","pdfUrl":"https://arxiv.org/pdf/2507.10000v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10000v1","rank":10,"domain":"Agent"}},{"id":"arxiv-2507.10535v1-1752594980576","title":"CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks","description":"Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.","url":"https://arxiv.org/abs/2507.10535v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:56:29.000Z","metadata":{"arxivId":"2507.10535v1","authors":"Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan","categories":"cs.CL, cs.AI, cs.SE","published":"2025-07-14T17:56:29Z","pdfUrl":"https://arxiv.org/pdf/2507.10535v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10535v1","rank":1,"domain":"Evaluation"}},{"id":"arxiv-2507.10502v1-1752594980576","title":"Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop","description":"Artificial intelligence holds immense promise for transforming biology, yet a lack of standardized, cross domain, benchmarks undermines our ability to build robust, trustworthy models. Here, we present insights from a recent workshop that convened machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics to tackle this gap. We identify major technical and systemic bottlenecks such as data heterogeneity and noise, reproducibility challenges, biases, and the fragmented ecosystem of publicly available resources and propose a set of recommendations for building benchmarking frameworks that can efficiently compare ML models of biological systems across tasks and data modalities. By promoting high quality data curation, standardized tooling, comprehensive evaluation metrics, and open, collaborative platforms, we aim to accelerate the development of robust benchmarks for AI driven Virtual Cells. These benchmarks are crucial for ensuring rigor, reproducibility, and biological relevance, and will ultimately advance the field toward integrated models that drive new discoveries, therapeutic insights, and a deeper understanding of cellular systems.","url":"https://arxiv.org/abs/2507.10502v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:25:28.000Z","metadata":{"arxivId":"2507.10502v1","authors":"Elizabeth Fahsbender, Alma Andersson, Jeremy Ash, Polina Binder, Daniel Burkhardt, Benjamin Chang, Georg K. Gerber, Anthony Gitter, Patrick Godau, Ankit Gupta, Genevieve Haliburton, Siyu He, Trey Ideker, Ivana Jelic, Aly Khan, Yang-Joon Kim, Aditi Krishnapriyan, Jon M. Laurent, Tianyu Liu 28, Emma Lundberg, Shalin B. Mehta, Rob Moccia, Angela Oliveira Pisco, Katherine S. Pollard, Suresh Ramani, Julio Saez-Rodriguez, Yasin Senbabaoglu, Elana Simon, Srinivasan Sivanandan, Gustavo Stolovitzky, Marc Valer, Bo Wang, Xikun Zhang, James Zou, Katrina Kalantar","categories":"cs.LG, cs.AI","published":"2025-07-14T17:25:28Z","pdfUrl":"https://arxiv.org/pdf/2507.10502v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10502v1","rank":2,"domain":"Evaluation"}},{"id":"arxiv-2507.10492v1-1752594980576","title":"BenchReAD: A systematic benchmark for retinal anomaly detection","description":"Retinal anomaly detection plays a pivotal role in screening ocular and systemic diseases. Despite its significance, progress in the field has been hindered by the absence of a comprehensive and publicly available benchmark, which is essential for the fair evaluation and advancement of methodologies. Due to this limitation, previous anomaly detection work related to retinal images has been constrained by (1) a limited and overly simplistic set of anomaly types, (2) test sets that are nearly saturated, and (3) a lack of generalization evaluation, resulting in less convincing experimental setups. Furthermore, existing benchmarks in medical anomaly detection predominantly focus on one-class supervised approaches (training only with negative samples), overlooking the vast amounts of labeled abnormal data and unlabeled data that are commonly available in clinical practice. To bridge these gaps, we introduce a benchmark for retinal anomaly detection, which is comprehensive and systematic in terms of data and algorithm. Through categorizing and benchmarking previous methods, we find that a fully supervised approach leveraging disentangled representations of abnormalities (DRA) achieves the best performance but suffers from significant drops in performance when encountering certain unseen anomalies. Inspired by the memory bank mechanisms in one-class supervised learning, we propose NFM-DRA, which integrates DRA with a Normal Feature Memory to mitigate the performance degradation, establishing a new SOTA. The benchmark is publicly available at https://github.com/DopamineLcy/BenchReAD.","url":"https://arxiv.org/abs/2507.10492v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:13:08.000Z","metadata":{"arxivId":"2507.10492v1","authors":"Chenyu Lian, Hong-Yu Zhou, Zhanli Hu, Jing Qin","categories":"cs.CV, cs.AI, cs.LG","published":"2025-07-14T17:13:08Z","pdfUrl":"https://arxiv.org/pdf/2507.10492v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10492v1","rank":3,"domain":"Evaluation"}},{"id":"arxiv-2507.10469v1-1752594980576","title":"An Empirical Evaluation of AI-Powered Non-Player Characters' Perceived Realism and Performance in Virtual Reality Environments","description":"Advancements in artificial intelligence (AI) have significantly enhanced the realism and interactivity of non-player characters (NPCs) in virtual reality (VR), creating more engaging and believable user experiences. This paper evaluates AI-driven NPCs within a VR interrogation simulator, focusing on their perceived realism, usability, and system performance. The simulator features two AI-powered NPCs, a suspect, and a partner, using GPT-4 Turbo to engage participants in a scenario to determine the suspect's guilt or innocence. A user study with 18 participants assessed the system using the System Usability Scale (SUS), Game Experience Questionnaire (GEQ), and a Virtual Agent Believability Questionnaire, alongside latency measurements for speech-to-text (STT), text-to-speech (TTS), OpenAI GPT-4 Turbo, and overall (cycle) latency. Results showed an average cycle latency of 7 seconds, influenced by the increasing conversational context. Believability scored 6.67 out of 10, with high ratings in behavior, social relationships, and intelligence but moderate scores in emotion and personality. The system achieved a SUS score of 79.44, indicating good usability. These findings demonstrate the potential of large language models to improve NPC realism and interaction in VR while highlighting challenges in reducing system latency and enhancing emotional depth. This research contributes to the development of more sophisticated AI-driven NPCs, revealing the need for performance optimization to achieve increasingly immersive virtual experiences.","url":"https://arxiv.org/abs/2507.10469v1","source":"ArXiv Domain","timestamp":"2025-07-14T16:50:29.000Z","metadata":{"arxivId":"2507.10469v1","authors":"Mikko Korkiakoski, Saeid Sheikhi, Jesper Nyman, Jussi Saariniemi, Kalle Tapio, Panos Kostakos","categories":"cs.HC, cs.AI, cs.MM","published":"2025-07-14T16:50:29Z","pdfUrl":"https://arxiv.org/pdf/2507.10469v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10469v1","rank":4,"domain":"Evaluation"}},{"id":"arxiv-2407.15600v2-1752594980576","title":"A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism","description":"Neural architecture search (NAS) enables researchers to automatically explore vast search spaces and find efficient neural networks. But NAS suffers from a key bottleneck, i.e., numerous architectures need to be evaluated during the search process, which requires a lot of computing resources and time. In order to improve the efficiency of NAS, a series of methods have been proposed to reduce the evaluation time of neural architectures. However, they are not efficient enough and still only focus on the accuracy of architectures. In addition to the classification accuracy, more efficient and smaller network architectures are required in real-world applications. To address the above problems, we propose the SMEM-NAS, a pairwise comparison relation-assisted multi-objective evolutionary algorithm based on a multi-population mechanism. In the SMEM-NAS, a surrogate model is constructed based on pairwise comparison relations to predict the accuracy ranking of architectures, rather than the absolute accuracy. Moreover, two populations cooperate with each other in the search process, i.e., a main population guides the evolution, while a vice population expands the diversity. Our method aims to provide high-performance models that take into account multiple optimization objectives. We conduct a series of experiments on the CIFAR-10, CIFAR-100 and ImageNet datasets to verify its effectiveness. With only a single GPU searching for 0.17 days, competitive architectures can be found by SMEM-NAS which achieves 78.91% accuracy with the MAdds of 570M on the ImageNet. This work makes a significant advance in the important field of NAS. Our code is publicly available at https://github.com/ccz-enas/SMEM-NAS.","url":"https://arxiv.org/abs/2407.15600v2","source":"ArXiv Domain","timestamp":"2024-07-22T12:46:22.000Z","metadata":{"arxivId":"2407.15600v2","authors":"Yu Xue, Pengcheng Jiang, Chenchen Zhu, MengChu Zhou, Mohamed Wahib, Moncef Gabbouj","categories":"cs.NE, cs.AI","published":"2024-07-22T12:46:22Z","pdfUrl":"https://arxiv.org/pdf/2407.15600v2.pdf","abstractUrl":"https://arxiv.org/abs/2407.15600v2","rank":5,"domain":"Evaluation"}},{"id":"arxiv-2410.06238v2-1752594980576","title":"EVOLvE: Evaluating and Optimizing LLMs For In-Context Exploration","description":"Despite their success in many domains, large language models (LLMs) remain under-studied in scenarios requiring optimal decision-making under uncertainty. This is crucial as many real-world applications, ranging from personalized recommendations to healthcare interventions, demand that LLMs not only predict but also actively learn to make optimal decisions through exploration. In this work, we measure LLMs' (in)ability to make optimal decisions in bandits, a state-less reinforcement learning setting relevant to many applications. We develop a comprehensive suite of environments, including both context-free and contextual bandits with varying task difficulties, to benchmark LLMs' performance. Motivated by the existence of optimal exploration algorithms, we propose efficient ways to integrate this algorithmic knowledge into LLMs: by providing explicit algorithm-guided support during inference; and through algorithm distillation via in-context demonstrations and fine-tuning, using synthetic data generated from these algorithms. Impressively, these techniques allow us to achieve superior exploration performance with smaller models, surpassing larger models on various tasks. We conducted an extensive ablation study to shed light on various factors, such as task difficulty and data representation, that influence the efficiency of LLM exploration. Additionally, we conduct a rigorous analysis of the LLM's exploration efficiency using the concept of regret, linking its ability to explore to the model size and underlying algorithm.","url":"https://arxiv.org/abs/2410.06238v2","source":"ArXiv Domain","timestamp":"2024-10-08T17:54:03.000Z","metadata":{"arxivId":"2410.06238v2","authors":"Allen Nie, Yi Su, Bo Chang, Jonathan N. Lee, Ed H. Chi, Quoc V. Le, Minmin Chen","categories":"cs.LG, cs.AI, cs.CL","published":"2024-10-08T17:54:03Z","pdfUrl":"https://arxiv.org/pdf/2410.06238v2.pdf","abstractUrl":"https://arxiv.org/abs/2410.06238v2","rank":6,"domain":"Evaluation"}},{"id":"arxiv-2505.12864v3-1752594980576","title":"LEXam: Benchmarking Legal Reasoning on 340 Law Exams","description":"Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/","url":"https://arxiv.org/abs/2505.12864v3","source":"ArXiv Domain","timestamp":"2025-05-19T08:48:12.000Z","metadata":{"arxivId":"2505.12864v3","authors":"Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus","categories":"cs.CL, cs.AI, cs.LG, 68T50, I.2","published":"2025-05-19T08:48:12Z","pdfUrl":"https://arxiv.org/pdf/2505.12864v3.pdf","abstractUrl":"https://arxiv.org/abs/2505.12864v3","rank":7,"domain":"Evaluation"}},{"id":"arxiv-2507.10223v1-1752594980576","title":"ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users","description":"Prosthetic legs play a pivotal role in clinical rehabilitation, allowing individuals with lower-limb amputations the ability to regain mobility and improve their quality of life. Gait analysis is fundamental for optimizing prosthesis design and alignment, directly impacting the mobility and life quality of individuals with lower-limb amputations. Vision-based machine learning (ML) methods offer a scalable and non-invasive solution to gait analysis, but face challenges in correctly detecting and analyzing prosthesis, due to their unique appearances and new movement patterns. In this paper, we aim to bridge this gap by introducing a multi-purpose dataset, namely ProGait, to support multiple vision tasks including Video Object Segmentation, 2D Human Pose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from four above-knee amputees when testing multiple newly-fitted prosthetic legs through walking trials, and depicts the presence, contours, poses, and gait patterns of human subjects with transfemoral prosthetic legs. Alongside the dataset itself, we also present benchmark tasks and fine-tuned baseline models to illustrate the practical application and performance of the ProGait dataset. We compared our baseline models against pre-trained vision models, demonstrating improved generalizability when applying the ProGait dataset for prosthesis-specific tasks. Our code is available at https://github.com/pittisl/ProGait and dataset at https://huggingface.co/datasets/ericyxy98/ProGait.","url":"https://arxiv.org/abs/2507.10223v1","source":"ArXiv Domain","timestamp":"2025-07-14T12:40:57.000Z","metadata":{"arxivId":"2507.10223v1","authors":"Xiangyu Yin, Boyuan Yang, Weichen Liu, Qiyao Xue, Abrar Alamri, Goeran Fiedler, Wei Gao","categories":"cs.CV, cs.AI","published":"2025-07-14T12:40:57Z","pdfUrl":"https://arxiv.org/pdf/2507.10223v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10223v1","rank":8,"domain":"Evaluation"}},{"id":"arxiv-2507.10216v1-1752594980576","title":"Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects","description":"As large language models (LLMs) become increasingly central to Arabic NLP applications, evaluating their understanding of regional dialects and cultural nuances is essential, particularly in linguistically diverse settings like Saudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark specifically designed to assess LLMs performance across major Saudi dialects. \\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six distinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage, Cultural Interpretation, and Location Recognition. These questions are derived from a curated dataset of dialectal words, phrases, and proverbs sourced from various regions of Saudi Arabia. We evaluate several state-of-the-art LLMs, including multilingual and Arabic-specific models. We also provide detailed insights into their capabilities and limitations. Our results reveal notable performance gaps, particularly in tasks requiring cultural inference or contextual understanding. Our findings highlight the urgent need for dialect-aware training and culturally aligned evaluation methodologies to improve LLMs performance in real-world Arabic applications.","url":"https://arxiv.org/abs/2507.10216v1","source":"ArXiv Domain","timestamp":"2025-07-14T12:33:07.000Z","metadata":{"arxivId":"2507.10216v1","authors":"Renad Al-Monef, Hassan Alhuzali, Nora Alturayeif, Ashwag Alasmari","categories":"cs.CL, cs.AI","published":"2025-07-14T12:33:07Z","pdfUrl":"https://arxiv.org/pdf/2507.10216v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10216v1","rank":9,"domain":"Evaluation"}},{"id":"arxiv-2507.10200v1-1752594980576","title":"Natural Language-based Assessment of L2 Oral Proficiency using LLMs","description":"Natural language-based assessment (NLA) is an approach to second language assessment that uses instructions - expressed in the form of can-do descriptors - originally intended for human examiners, aiming to determine whether large language models (LLMs) can interpret and apply them in ways comparable to human assessment. In this work, we explore the use of such descriptors with an open-source LLM, Qwen 2.5 72B, to assess responses from the publicly available S&I Corpus in a zero-shot setting. Our results show that this approach - relying solely on textual information - achieves competitive performance: while it does not outperform state-of-the-art speech LLMs fine-tuned for the task, it surpasses a BERT-based model trained specifically for this purpose. NLA proves particularly effective in mismatched task settings, is generalisable to other data types and languages, and offers greater interpretability, as it is grounded in clearly explainable, widely applicable language descriptors.","url":"https://arxiv.org/abs/2507.10200v1","source":"ArXiv Domain","timestamp":"2025-07-14T12:13:50.000Z","metadata":{"arxivId":"2507.10200v1","authors":"Stefano Bannò, Rao Ma, Mengjie Qian, Siyuan Tang, Kate Knill, Mark Gales","categories":"eess.AS, cs.AI, cs.CL","published":"2025-07-14T12:13:50Z","pdfUrl":"https://arxiv.org/pdf/2507.10200v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10200v1","rank":10,"domain":"Evaluation"}},{"id":"arxiv-2507.10552v1-1752594980901","title":"Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder","description":"Camera traps are revolutionising wildlife monitoring by capturing vast amounts of visual data; however, the manual identification of individual animals remains a significant bottleneck. This study introduces a fully self-supervised approach to learning robust chimpanzee face embeddings from unlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision Transformers on automatically mined face crops, eliminating the need for identity labels. Our method demonstrates strong open-set re-identification performance, surpassing supervised baselines on challenging benchmarks such as Bossou, despite utilising no labelled data during training. This work underscores the potential of self-supervised learning in biodiversity monitoring and paves the way for scalable, non-invasive population studies.","url":"https://arxiv.org/abs/2507.10552v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:59:59.000Z","metadata":{"arxivId":"2507.10552v1","authors":"Vladimir Iashin, Horace Lee, Dan Schofield, Andrew Zisserman","categories":"cs.CV, cs.AI, cs.LG","published":"2025-07-14T17:59:59Z","pdfUrl":"https://arxiv.org/pdf/2507.10552v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10552v1","rank":1,"domain":"AI"}},{"id":"arxiv-2507.10548v1-1752594980901","title":"EmbRACE-3K: Embodied Reasoning and Action in Complex Environments","description":"Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities.","url":"https://arxiv.org/abs/2507.10548v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:59:46.000Z","metadata":{"arxivId":"2507.10548v1","authors":"Mingxian Lin, Wei Huang, Yitang Li, Chengjie Jiang, Kui Wu, Fangwei Zhong, Shengju Qian, Xin Wang, Xiaojuan Qi","categories":"cs.CV, cs.AI, cs.CL","published":"2025-07-14T17:59:46Z","pdfUrl":"https://arxiv.org/pdf/2507.10548v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10548v1","rank":2,"domain":"AI"}},{"id":"arxiv-2507.10547v1-1752594980901","title":"Quantize-then-Rectify: Efficient VQ-VAE Training","description":"Visual tokenizers are pivotal in multimodal large models, acting as bridges between continuous inputs and discrete tokens. Nevertheless, training high-compression-rate VQ-VAEs remains computationally demanding, often necessitating thousands of GPU hours. This work demonstrates that a pre-trained VAE can be efficiently transformed into a VQ-VAE by controlling quantization noise within the VAE's tolerance threshold. We present \\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs to enable rapid VQ-VAE training with minimal computational overhead. By integrating \\textbf{channel multi-group quantization} to enlarge codebook capacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ compresses ImageNet images into at most 512 tokens while sustaining competitive reconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training costs by over two orders of magnitude relative to state-of-the-art approaches: ReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours, whereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental results show that ReVQ achieves superior efficiency-reconstruction trade-offs.","url":"https://arxiv.org/abs/2507.10547v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:59:41.000Z","metadata":{"arxivId":"2507.10547v1","authors":"Borui Zhang, Qihang Rao, Wenzhao Zheng, Jie Zhou, Jiwen Lu","categories":"cs.CV, cs.LG","published":"2025-07-14T17:59:41Z","pdfUrl":"https://arxiv.org/pdf/2507.10547v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10547v1","rank":3,"domain":"AI"}},{"id":"arxiv-2507.10546v1-1752594980901","title":"Disentangling Neural Disjunctive Normal Form Models","description":"Neural Disjunctive Normal Form (DNF) based models are powerful and interpretable approaches to neuro-symbolic learning and have shown promising results in classification and reinforcement learning settings without prior knowledge of the tasks. However, their performance is degraded by the thresholding of the post-training symbolic translation process. We show here that part of the performance degradation during translation is due to its failure to disentangle the learned knowledge represented in the form of the networks' weights. We address this issue by proposing a new disentanglement method; by splitting nodes that encode nested rules into smaller independent nodes, we are able to better preserve the models' performance. Through experiments on binary, multiclass, and multilabel classification tasks (including those requiring predicate invention), we demonstrate that our disentanglement method provides compact and interpretable logical representations for the neural DNF-based models, with performance closer to that of their pre-translation counterparts. Our code is available at https://github.com/kittykg/disentangling-ndnf-classification.","url":"https://arxiv.org/abs/2507.10546v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:59:33.000Z","metadata":{"arxivId":"2507.10546v1","authors":"Kexin Gu Baugh, Vincent Perreault, Matthew Baugh, Luke Dickens, Katsumi Inoue, Alessandra Russo","categories":"cs.LG, cs.AI","published":"2025-07-14T17:59:33Z","pdfUrl":"https://arxiv.org/pdf/2507.10546v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10546v1","rank":4,"domain":"AI"}},{"id":"arxiv-2507.10542v1-1752594980901","title":"ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions","description":"Generating high-fidelity real-time animated sequences of photorealistic 3D head avatars is important for many graphics applications, including immersive telepresence and movies. This is a challenging problem particularly when rendering digital avatar close-ups for showing character's facial microfeatures and expressions. To capture the expressive, detailed nature of human heads, including skin furrowing and finer-scale facial movements, we propose to couple locally-defined facial expressions with 3D Gaussian splatting to enable creating ultra-high fidelity, expressive and photorealistic 3D head avatars. In contrast to previous works that operate on a global expression space, we condition our avatar's dynamics on patch-based local expression features and synthesize 3D Gaussians at a patch level. In particular, we leverage a patch-based geometric 3D face model to extract patch expressions and learn how to translate these into local dynamic skin appearance and motion by coupling the patches with anchor points of Scaffold-GS, a recent hierarchical scene representation. These anchors are then used to synthesize 3D Gaussians on-the-fly, conditioned by patch-expressions and viewing direction. We employ color-based densification and progressive training to obtain high-quality results and faster convergence for high resolution 3K training images. By leveraging patch-level expressions, ScaffoldAvatar consistently achieves state-of-the-art performance with visually natural motion, while encompassing diverse facial expressions and styles in real time.","url":"https://arxiv.org/abs/2507.10542v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:59:03.000Z","metadata":{"arxivId":"2507.10542v1","authors":"Shivangi Aneja, Sebastian Weiss, Irene Baeza, Prashanth Chandran, Gaspard Zoss, Matthias Nießner, Derek Bradley","categories":"cs.GR, cs.AI, cs.CV","published":"2025-07-14T17:59:03Z","pdfUrl":"https://arxiv.org/pdf/2507.10542v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10542v1","rank":5,"domain":"AI"}},{"id":"arxiv-2507.10541v1-1752594980901","title":"REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once","description":"Recent Large Reasoning Models (LRMs) have achieved remarkable progress on task-specific benchmarks, yet their evaluation methods remain constrained by isolated problem-solving paradigms. Existing benchmarks predominantly assess single-question reasoning through sequential testing, resulting critical limitations: (1) vulnerability to data contamination and less challenging (e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual creation of new questions with large human efforts, (2) failure to evaluate models under multi-context pressure, a key requirement for real-world deployment. To bridge this gap, we present REST (Reasoning Evaluation through Simultaneous Testing), a stress-testing framework that concurrently exposes LRMs to multiple problems simultaneously. Beyond basic reasoning, REST specifically evaluates several under-tested capabilities: contextual priority allocation, cross-problem interference resistance, and dynamic cognitive load management. Our evaluation reveals several striking findings: Even state-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance degradation under stress testing. Crucially, REST demonstrates stronger discriminative power than existing benchmarks, revealing pronounced performance differences among models that exhibit similar, near-ceiling performance under single-question evaluations. Some key mechanistic insights emerge from our analysis: (1) the \"overthinking trap\" is a critical factor contributing to the performance degradation; (2) the models trained with \"long2short\" technique preserve more accuracy of their single-problem performance under REST, outperforming standard-trained counterparts. These results establish REST as a cost-efficient, future-proof evaluation paradigm that better reflects real-world reasoning demands while reducing reliance on continuous human annotation.","url":"https://arxiv.org/abs/2507.10541v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:58:47.000Z","metadata":{"arxivId":"2507.10541v1","authors":"Zhuoshi Pan, Qizhi Pei, Yu Li, Qiyao Sun, Zinan Tang, H. Vicky Zhao, Conghui He, Lijun Wu","categories":"cs.CL","published":"2025-07-14T17:58:47Z","pdfUrl":"https://arxiv.org/pdf/2507.10541v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10541v1","rank":6,"domain":"AI"}},{"id":"arxiv-2507.10540v1-1752594980901","title":"Fusing LLM Capabilities with Routing Data","description":"The rapid advancement of large language models (LLMs) has created a vibrant ecosystem of diverse architectures, each with unique strengths due to differences in design, training data, and objectives. However, most applications still rely on a single backend model, limiting coverage of capabilities and leading to inefficiencies in performance and token cost when tackling complex tasks. We highlight an underexploited opportunity: LLM routing data, produced when hosting platforms route diverse queries to different models, which can reveal comparative strengths across tasks. To address this, we propose FusionBench, a comprehensive routing benchmark covering 14 tasks across five domains with 20 open-source LLMs (8B to 671B parameters), capturing 103M tokens and summarizing reusable thought templates from top models. Building on this, we introduce FusionFactory, a systematic fusion framework with three levels: (1) query-level fusion, tailoring routers for each query using both direct responses and reasoning-augmented outputs; (2) thought-level fusion, leveraging abstract templates derived from top-performing LLMs' answers to similar queries; and (3) model-level fusion, transferring capabilities between models via distillation, using top responses or highest judge scores as training data. Experiments show FusionFactory consistently outperforms the best individual LLM across all 14 benchmarks, with optimal fusion configurations varying by benchmark, demonstrating the value of systematic LLM fusion in harnessing complementary strengths and improving overall performance.","url":"https://arxiv.org/abs/2507.10540v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:58:02.000Z","metadata":{"arxivId":"2507.10540v1","authors":"Tao Feng, Haozhen Zhang, Zijie Lei, Pengrui Han, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jiaxuan You","categories":"cs.LG","published":"2025-07-14T17:58:02Z","pdfUrl":"https://arxiv.org/pdf/2507.10540v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10540v1","rank":7,"domain":"AI"}},{"id":"arxiv-2507.10539v1-1752594980902","title":"Graph World Model","description":"World models (WMs) demonstrate strong capabilities in prediction, generation, and planning tasks. Existing WMs primarily focus on unstructured data and cannot leverage the ubiquitous structured data, often represented as graphs, in the digital world. While multiple graph foundation models have been proposed, they focus on graph learning tasks and cannot extend to diverse multi-modal data and interdisciplinary tasks. To address these challenges, we propose the Graph World Model (GWM), a world model that supports both unstructured and graph-structured states with multi-modal information and represents diverse tasks as actions. The core of a GWM is a generic message-passing algorithm to aggregate structured information, either over a unified multi-modal token space by converting multi-modal data into text (GWM-T) or a unified multi-modal embedding space by modality-specific encoders (GWM-E). Notably, GWM introduces action nodes to support diverse tasks, where action nodes are linked to other nodes via direct reference or similarity computation. Extensive experiments on six tasks from diverse domains, including multi-modal generation and matching, recommendation, graph prediction, multi-agent, retrieval-augmented generation, and planning and optimization, show that the same GWM outperforms or matches domain-specific baselines' performance, benefits from multi-hop structures, and demonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our code for GWM is released at https://github.com/ulab-uiuc/GWM.","url":"https://arxiv.org/abs/2507.10539v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:57:45.000Z","metadata":{"arxivId":"2507.10539v1","authors":"Tao Feng, Yexin Wu, Guanyu Lin, Jiaxuan You","categories":"cs.LG","published":"2025-07-14T17:57:45Z","pdfUrl":"https://arxiv.org/pdf/2507.10539v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10539v1","rank":8,"domain":"AI"}},{"id":"arxiv-2507.10536v1-1752594980902","title":"On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance","description":"In this work, we analyze the optimization behaviour of common private learning optimization algorithms under heavy-tail class imbalanced distribution. We show that, in a stylized model, optimizing with Gradient Descent with differential privacy (DP-GD) suffers when learning low-frequency classes, whereas optimization algorithms that estimate second-order information do not. In particular, DP-AdamBC that removes the DP bias from estimating loss curvature is a crucial component to avoid the ill-condition caused by heavy-tail class imbalance, and empirically fits the data better with $\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the least frequent classes on both controlled experiments and real data respectively.","url":"https://arxiv.org/abs/2507.10536v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:57:08.000Z","metadata":{"arxivId":"2507.10536v1","authors":"Qiaoyue Tang, Alain Zhiyanov, Mathias Lécuyer","categories":"cs.LG","published":"2025-07-14T17:57:08Z","pdfUrl":"https://arxiv.org/pdf/2507.10536v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10536v1","rank":9,"domain":"AI"}},{"id":"arxiv-2507.10535v1-1752594980902","title":"CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks","description":"Large Language Models (LLMs) have significantly advanced the state-of-the-art in various coding tasks. Beyond directly answering user queries, LLMs can also serve as judges, assessing and comparing the quality of responses generated by other models. Such an evaluation capability is crucial both for benchmarking different LLMs and for improving response quality through response ranking. However, despite the growing adoption of the LLM-as-a-Judge paradigm, its effectiveness in coding scenarios remains underexplored due to the absence of dedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a benchmark explicitly designed to evaluate the performance of LLM-as-a-Judge models across three critical coding tasks: code generation, code repair, and unit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge models, we find that recent thinking models significantly outperform non-thinking models on our carefully designed code judging tasks. Notably, even relatively small thinking models, such as Qwen3-8B, can outperform specially trained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still exhibit significant randomness in their judgment of coding tasks. For pairwise judging tasks, simply changing the order in which responses are presented can substantially impact accuracy. In addition, when judging code and unit tests written by different LLMs, LLM-as-a-Judge models also show variance in performance. This sensitivity raises concerns about the reliability and consistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal prompting strategies for LLM-as-a-Judge. We find that using pair-wise comparison outperforms scalar point-wise judging. Furthermore, retaining comments and reasoning in the full, unprocessed LLM response leads to improved judge performance.","url":"https://arxiv.org/abs/2507.10535v1","source":"ArXiv Domain","timestamp":"2025-07-14T17:56:29.000Z","metadata":{"arxivId":"2507.10535v1","authors":"Hongchao Jiang, Yiming Chen, Yushi Cao, Hung-yi Lee, Robby T. Tan","categories":"cs.CL, cs.AI, cs.SE","published":"2025-07-14T17:56:29Z","pdfUrl":"https://arxiv.org/pdf/2507.10535v1.pdf","abstractUrl":"https://arxiv.org/abs/2507.10535v1","rank":10,"domain":"AI"}}]}