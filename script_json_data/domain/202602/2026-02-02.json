{"timestamp":"2026-02-02T00:01:08.881Z","totalItems":40,"items":[{"id":"arxiv-2601.22159v1-1769990143839","title":"RedSage: A Cybersecurity Generalist LLM","description":"Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.","url":"https://arxiv.org/abs/2601.22159v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:57.000Z","metadata":{"arxivId":"2601.22159v1","authors":"Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani","categories":"cs.CR, cs.AI, cs.CL","published":"2026-01-29T18:59:57Z","pdfUrl":"https://arxiv.org/pdf/2601.22159v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22159v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22159v1","rank":1,"domain":"LLM","zh_summary":"网络安全运营需要支持多样化工作流程且不暴露敏感数据的辅助大型语言模型。现有解决方案要么依赖存在隐私风险的专有API，要么依赖缺乏域名适配的开放模型。为弥合这一差距，我们通过大规模网络过滤和手工收集高质量资源，策划了118亿个以网络安全为重点的持续预训练数据标记，涵盖2.86万份文档，涵盖框架、攻击性技术和安全工具。基于此，我们设计了一个代理增强流程，模拟专家工作流程，生成26.6万多回合网络安全样本，用于监督式微调。结合通用开源的大型语言模型数据，这些资源使得RedSage的培训成为可能，RedSage是一款开源、可本地部署的网络安全助手，具备域感知的预培训和后培训功能。为了严格评估这些模型，我们推出了RedSage-Bench，这是一个基准测试，包含3万条选择题和240条开放式问答题，涵盖网络安全知识、技能和工具专长。RedSage还会根据已建立的网络安全基准（如CTI-Bench、CyberMetric、SECURE）和通用大型语言模型基准进行进一步评估，以评估更广泛的概括性。在8B尺度上，RedSage持续取得更优成绩，在网络安全基准测试中领先基线模型最高+5.59分，在开放大型语言模型排行榜任务中高出+5.05分。这些发现表明，领域感知型代理增强以及前后培训不仅能提升网络安全专长，还有助于提升整体推理能力和指令遵循能力。所有模型、数据集和代码均公开。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. **现有网络安全大模型的训练与部署缺陷**\n\n- 专有模型依赖云端 API，存在敏感数据外泄风险；\n- 开源模型缺乏针对网络安全场景的充分预训练与后训练，导致领域知识薄弱；\n- 已有工作往往只关注单一训练阶段（仅预训练或仅监督微调），且公开数据规模有限，难以支撑本地化部署的通用型安全助手。\n2. **网络安全评测基准的覆盖不足**\n\n- 主流基准侧重知识问答，缺少对**工具操作能力**与**开放式问答质量**的系统性评估；\n- 缺乏同时覆盖知识、技能、工具三维能力的大规模高质量评测集。\n3. **数据与 pipeline 的封闭性**\n\n- 先前研究多未公开训练数据与代码，阻碍社区复现与进一步研究。\n\n为此，作者提出 **RedSage**：\n\n- 构建 **11.8 B token** 的网络安全持续预训练语料（CyberFineWeb + RedSage-Seed）；\n- 设计 **agentic 增强 pipeline**，将 28 K 权威文档转化为 **266 K 轮次对话**用于监督微调；\n- 推出 **RedSage-Bench**（30 K 多选题 + 240 开放式问答），系统评估知识、技能与工具 proficiency；\n- 训练得到 **8 B 参数可本地部署**的开源模型，在网络安全与通用基准上均取得 **+5 分以上** 的提升，并全面公开数据、模型与代码。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统梳理了相关研究，可归纳为两条主线：**网络安全评测基准**与**网络安全领域大模型/数据集**。\n\n1. 网络安全评测基准\n\n- **知识型基准**\n- SecEval（Li et al., 2023）：2 K 多选题，覆盖 9 类安全主题。\n- CyberMetric（Tihanyi et al., 2024）：10 K 多选题，基于 RAG 与专家验证。\n- CyberBench（Liu et al., 2024）：除问答外还包含 NER、摘要、分类任务。\n- SECURE（Bhusal et al., 2024）：面向工控系统，聚焦风险推理与漏洞分析。\n- CS-Eval（Yu et al., 2024）：42 子类，分知识-能力-应用三级认知维度。\n- SecBench（Jing et al., 2025）：44 K 多选题 + 3 K 简答题，中英双语。\n- **应用与智能体基准**\n- CTI-Bench（Alam et al., 2024）：威胁情报任务，含 CVE↔CWE 映射、CVSS 预测、攻击技战术识别。\n- CyberSecEval（Wan et al., 2024）：8 类风险场景，如漏洞利用生成、提示注入。\n- NYU-CTF（Shao et al., 2024）、CyBench（Zhang et al., 2025）：交互式夺旗赛，评估逆向、Web 利用等红队能力。\n- **共性局限**：多数基准仅覆盖知识或应用，**缺乏对安全工具操作与开放式问答质量的细粒度评估**。\n2. 网络安全领域大模型与数据集\n\n- **早期编码器模型**\n- CyBERT、SecureBERT、CTI-BERT：基于 BERT 的域内微调，任务特定，数据未公开。\n- **大模型时代的持续预训练/微调**\n- PRIMUS（Yu et al., 2025）：2.75 B token 预训练 + 835 条 SFT，基于 Llama-3.1-8B。\n- Foundation-Sec-8B（Kassianik et al., 2025）：5.1 B token 预训练，数据未开源。\n- DeepHat（2025）：宣称 1 M+ Q&A 微调，无预训练阶段。\n- Lily-Cybersecurity-7B（Sego Lily Labs, 2024）：22 K 手工对话微调。\n- Cyber-DAP（Salahuddin et al., 2025）：119 M token 小规模持续预训练。\n- SecGemini：谷歌闭源模型，实时威胁情报，数据与权重均未发布。\n- **共性局限**：\n- 仅聚焦单一训练阶段（预训练或 SFT），**未联合优化**；\n- 数据规模或质量有限，**无 agentic 增强**；\n- **未同时公开模型、数据与代码**，可复现性差。\n\nRedSage 与上述工作的区别\n\n- 首次将**大规模持续预训练（11.7 B token）+ 代理增强 SFT（266 K 对话）+ 偏好对齐**完整 pipeline 开源；\n- 提出同时覆盖**知识、技能、工具**且包含**开放式问答质量评分**的 RedSage-Bench；\n- 8 B 规模即可本地部署，在网络安全与通用基准上均取得新 SOTA。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“数据-训练-评测”三位一体的系统化方案，一次性补齐了领域预训练、高质量后训练与严格评测的缺口，具体路径如下：\n\n1\\. 构建 11.8 B token 网络安全持续预训练语料\n\n| 子语料 | 来源与处理 | 规模 | 关键特点 |\n| --- | --- | --- | --- |\n| CyberFineWeb | 在 FineWeb（17 T token）上用 ModernBERT 分类器过滤→全球 MinHash 去重→按时间片混合 30 % FineWeb-Edu 防止灾难性遗忘 | 13 M 文档 / 11.7 B token | 覆盖 2013-2024 公开 Web 安全内容，保持通用知识 |\n| RedSage-Seed | 人工精选 28 637 篇权威资源（MITRE、OWASP、Kali、CTF Write-up、tldr 等）→ Markdown 结构化 | 150 M token | 高质量、可溯源、三分类：Knowledge / Skills / Tools |\n| RedSage-Dump | 补充 NVD、RFC、NIST、安全新闻、教育门户等公开数据 | 700 M token | 增强漏洞、协议、合规等长尾知识 |\n\n2\\. Agentic 增强生成 266 K 多轮对话用于后训练\n\n- **Planner Agent**：对 Seed 数据分块→自动抽取技能集（如“漏洞分析”“命令生成”）并输出多样化增强策略（Q&A、角色扮演、步骤演练等）。\n- **Augmenter Agent**：按策略将每块种子实例化为**角色驱动、多轮、可执行**的对话，严格保持技术深度与事实一致。\n- **质量控制**：格式、一致性、相关性三重过滤；最终得到 266 K 对话（353 M token），样本扩大 9.2×，token 扩大 2.3×。\n\n3\\. 两阶段后训练\n\n1. **SFT**：RedSage-Conv + 通用指令集 SmolTalk2（非推理子集）→兼顾网络安全专精与广泛指令遵循。\n2. **DPO**：采用开源 Tulu3 偏好数据做直接偏好优化，提升回答有用性与安全性。\n\n4\\. 建立 RedSage-Bench 全面评测\n\n| 维度 | 题型 | 规模 | 构建方式 | 质量保障 |\n| --- | --- | --- | --- | --- |\n| 知识 | MCQ | 30 K | 用强 LLM 对 Seed 生成→双阶段 LLM 验证（结构+评分）→人工抽查 | 得分>8 且类别均衡 |\n| 技能 | Open-QA | 240 | Agentic 计划→QA 生成→双 LLM 交叉验证→人工终审 | 0-10 质量评分+事实正确性 |\n| 工具 | —— | —— | 同上 | 同上 |\n\n5\\. 训练与评测结果\n\n- **基座**：Qwen3-8B → 持续预训练 → RedSage-8B-Base\n- **指令版**：SFT → RedSage-8B-Ins；再加 DPO → RedSage-8B-DPO\n\n**效果**（8 B 模型）\n\n- 网络安全基准平均 +5.59（最高 84.6 % vs 79.0 %）\n- Open LLM Leaderboard 平均 +5.05（74.3 % vs 69.3 %）\n- 32 B 规模轻量 QLoRA 实验亦获一致提升，验证 pipeline 可扩展。\n\n6\\. 开源与可复现\n\n- 全部语料、对话、Benchmark、训练配置（Axolotl YAML）、评测脚本（LightEval）一次性公开；\n- 提供去污染与伦理使用协议，支持本地 GPU 部署，实现“隐私-安全-可控”的工业级安全助手。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“模型性能”** 与 **“方法可复现/可扩展”** 两条主线，共执行了 4 组实验，覆盖 20+ 公开基准、3 类模型规模、累计 50 余万个测试样本。\n\n1\\. RedSage-Bench 自评实验\n\n**目的**：验证领域定制预训练 + 代理增强 SFT 的绝对增益。\n**任务与指标**\n\n- MCQ（30 k题，0-shot）：宏观准确率 + 知识/技能/工具 三类细粒度准确率。\n- Open-QA（240题，0-shot）：LLM-as-Judge 打分（0–10）+ 事实正确率（T/F）。\n\n**对照组**\n\n- 通用基线：Llama-3.1-8B、Qwen3-8B\n- 安全专用基线：Foundation-Sec-8B、Llama-Primus、DeepHat-V1-7B 等\n\n**主要结果**\n\n- RedSage-8B-Ins 在 MCQ 宏观准确率上比最强 8 B 通用模型 Qwen3-8B 提升 **+3.88 pp**；DPO 版本保持 **+2.98 pp**。\n- Open-QA 平均正确率领先次优模型 **+7 pp**，质量分领先 **+0.07**（6.50 vs 6.43）。\n- 工具类题目最难（中位数质量分最低），RedSage 仍保持显著优势。\n\n2\\. 公开网络安全基准外推实验\n\n**目的**：检验领域知识能否泛化到外部评测。\n**基准列表（7 个）**\nCyberMetric-500、SecBench-En、MMLU-CSec、SECURE（MEAT/CWET/KCV）、CTI-Bench（MCQ+RCM）、SecEval\n\n**结果（表 5）**\n\n- 5-shot 基座阶段：RedSage-8B-Base 平均 **84.6 %**（+3.75 pp > Qwen3-8B-Base）。\n- 0-shot 指令阶段：RedSage-8B-Ins **81.3 %**（+5.59 pp > Qwen3-8B），接近 32 B 模型（82.3 %），距离 GPT-5 差 5 pp。\n\n3\\. 通用大模型 Leaderboard 实验\n\n**目的**：验证“领域强化”是否损害通用能力。\n**基准（7 项）**\nARC-C、HellaSwag、TruthfulQA、MMLU、WinoGrande、GSM8K、IFEval\n\n**结果（表 6）**\n\n- RedSage-8B-DPO 平均 **74.3 %**，领先所有 8 B 对照（最高基线 69.3 %），并超过 Qwen3-32B（73.2 %）。\n- GSM8K 数学推理 **86.1 %**（+8.4 pp > 最强 8 B 基线），说明 Seed 数据中的 CTF/Write-up 逻辑链同样提升通用推理。\n\n4\\. 规模扩展与消融实验\n\n4.1 32 B 轻量微调（QLoRA）\n\n- 仅用 RedSage-Seed + 50 % RedSage-Conv，1 % 可训练参数，4 小时训练。\n- RedSage-32B-LoRA-Ins 在 RedSage-MCQ 达 **87.5 %**（+2.0 pp > 全量 8 B 版本），外部安全基准平均 **82.9 %**，验证 pipeline 随模型容量线性受益。\n\n4.2 消融：预训练语料贡献\n\n- **CyberFineWeb-only** vs **Seed-only** vs **CFW+Seed**\n- Seed-only 在需要权威知识的 CTI-RCM、MMLU-CSec 上最高；\n- CFW-only 在 SecBench、CyberMetric 等 Web 知识型基准上最高；\n- 二者组合取得最佳宏观平均，证明互补性。\n\n4.3 消融：后训练策略\n\n- SFT → DPO 连续提升 IFEval（指令遵循）（79.9 → 83.4 pp），但 MCQ 准确率略降（85.7 → 84.8），显示偏好对齐以牺牲少量闭卷正确率为代价换取开放式可用性。\n\n实验总结\n\n- 在 20+ 基准、50 万级样本上，RedSage 8 B 模型**同时**实现：\n- 网络安全任务 **SOTA**（+5.6 pp）\n- 通用任务 **领先同规模模型**（+5.0 pp）\n- 代理增强数据 + 持续预训练的策略可**零修改**迁移到 32 B 模型，继续提升性能，证明方法的可扩展性与复现价值。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**数据、模型、评测、应用与安全**五大主题，并给出可落地的研究切入点。\n\n1\\. 数据层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 多语言/地域威胁情报 | 将 CyberFineWeb 过滤 pipeline 扩展到 Common Crawl 其他语言子集，构建「中文-阿拉伯语-俄语」平行安全语料。 | 捕捉非英语地下论坛/漏洞交易信息，提升全球威胁可见度。 |\n| 多模态安全数据 | 把 CVE 公告中的 PoC 视频、攻击流程图、恶意软件截图与文本对齐，形成图文交错预训练语料。 | 支持「图-文」混合问答，例如“根据这张 Wireshark 截图指出攻击阶段”。 |\n| 实时威胁增量更新 | 设计「时间窗口」持续预训练策略，每周注入新 CVE、新 ATT&CK Technique，避免模型“过期”。 | 解决当前静态语料无法反映 0-day 爆发的问题。 |\n\n2\\. 模型层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 工具调用能力 | 引入 ToolLLM 式 API 模板，把 Kali 工具封装成可调用函数，训练模型生成「工具链+参数」并实时执行。 | 实现真正的“红队助手”——输入目标 IP，输出 nmap → nuclei → metasploit 自动链。 |\n| 推理-行动循环 | 结合 ReAct / Reflexion 框架，让模型在 CTF 环境中自我验证 flag 结果，根据失败反馈迭代 payload。 | 突破当前仅文本生成、无环境反馈的局限，迈向自主渗透代理。 |\n| 小参数高效化 | 用 1-3 B 模型 + 动态 LoRA 合并，实现「手机端本地安全助手」；研究量化后是否仍能保持 exploit 语法精度。 | 满足现场应急、无网络场景下的便携需求。 |\n\n3\\. 评测层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 动态 CTF 基准 | 与 CTFd 平台对接，每月自动拉取新比赛题目，形成「活基准」；指标从准确率改为夺旗数/用时。 | 防止静态题目过拟合，提供可无限扩展的攻防评测。 |\n| 红-蓝对抗评分 | 建立「双模型对抗」协议：RedSage-Red vs RedSage-Blue，用 MITRE CALDERA 记录 ATT&CK 覆盖度，计算累积 TTP 成功数。 | 量化模型在真实杀伤链中的攻防贡献度。 |\n| 幻觉与一致性审计 | 引入「事实性链式追踪」数据集：对模型输出的每一条命令/漏洞描述给出可执行验证脚本，统计可复现率。 | 解决当前 LLM 安全建议“看似合理却不可执行”的痛点。 |\n\n4\\. 应用层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| SOAR 编排自动生成 | 将 SIEM 告警作为输入，模型输出 Splunk Phantom / Microsoft Sentinel Playbook JSON，直接导入执行。 | 缩短 SOC 平均响应时间（MTTR）。 |\n| 漏洞补丁联合生成 | 给定 CVE 描述 + 受影响函数，模型同时输出（1）补丁 diff（2）回归测试用例（3）CI 集成 YAML。 | 实现「一键修复」DevSecOps 工作流。 |\n| 安全培训个性化 | 利用对话历史评估学员薄弱点，动态生成「专属靶场」Docker Compose 场景与提示脚本。 | 提升企业安全培训效率，降低人力成本。 |\n\n5\\. 安全与伦理层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 可控生成护栏 | 在模型内部集成「双重控制码」：当请求包含 exploit、RAT、后门等关键字时，自动切换到「只给出防御/检测建议」模式。 | 降低双重用途风险，满足欧盟 AI Act 等监管要求。 |\n| 训练数据毒化检测 | 研究若攻击者在 Common Crawl 植入恶意网页并被 CyberFineWeb 吸收，如何触发异常行为；构建毒化样本检测器。 | 防止“上游投毒”导致模型输出有害 payload。 |\n| 可解释威胁推理 | 要求模型在给出攻击建议时，同步输出对应的 MITRE ATT&CK Technique ID 与置信度，并提供引用段落。 | 方便审计与责任追溯，增强安全员信任度。 |\n\n快速起步建议（3 个月可完成）\n\n1. 用现有 RedSage-8B-DPO + ToolLLM 模板，收集 5 K 条「工具调用-执行结果」对话，微调出 RedSage-Tools 原型。\n2. 在小型 CTF 平台（如 PicoMini）运行自动夺旗实验，记录 flag 率并与人类选手基线对比。\n3. 将动态评测结果开源，形成「RedSage-LiveBench」月度榜单，吸引社区提交新模型与新题。\n\n通过以上探索，可逐步把 RedSage 从“知识问答”升级为「可解释、可执行、可自我进化」的下一代网络安全智能体。\n\nQ6: 总结一下论文的主要内容\n\nRedSage：开源、本地化、8 B 参数的网络安全通用大模型\n——数据-训练-评测全流程总结\n\n1\\. 要解决的问题\n\n- 专有安全 LLM 依赖云端 API，敏感数据外泄风险高；\n- 开源模型缺乏大规模**域内预训练**+**高质量后训练**，安全知识不足；\n- 现有基准缺**工具操作**与**开放问答质量**维度，且数据/代码多未公开。\n\n2\\. 核心贡献\n\n| 内容 | 规模 | 亮点 |\n| --- | --- | --- |\n| CyberFineWeb | 11.7 B token | 2013-24 Common Crawl → ModernBERT 过滤 → 全局去重 → 30 % 通用知识回放 |\n| RedSage-Seed | 28 k 权威文档 / 150 M token | MITRE、OWASP、Kali、CTF Write-up 等，Markdown 结构化 |\n| Agentic 增强对话 | 266 k 多轮 / 353 M token | Planner+Augmenter 双智能体，把种子转为可执行、角色驱动对话 |\n| RedSage-Bench | 30 k MCQ + 240 开放问答 | 覆盖知识-技能-工具；双阶段 LLM 验证+人工终审，去污染 |\n| RedSage-8B 模型 | 8 B 参数 | Qwen3-Base → 持续预训练 → SFT → DPO；完全开源 |\n\n3\\. 实验结果（↑ vs 最强同规模模型）\n\n- **网络安全基准**平均 **+5.6 pp**（84.6 % vs 79.0 %）\n- **通用 LLM Leaderboard**平均 **+5.0 pp**（74.3 % vs 69.3 %）\n- **开放问答**正确率 **+7 pp**，质量分 **+0.07**\n- **32 B 轻量 QLoRA**继续提升，验证 pipeline 可扩展\n\n4\\. 可用性与影响\n\n- 8 B 规模 → 单卡 24 GB GPU 可部署，**本地私有**运行\n- 代码+数据+Benchmark 全部开放，支持复现与二次开发\n- 首次实现“**大规模域内预训练 + 代理增强后训练 + 三维评测**”一体化，为社区提供可落地的网络安全通用助手基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RedSage%3A%20A%20Cybersecurity%20Generalist%20LLM，链接是https://arxiv.org/pdf/2601.22159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22159。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22157v1-1769990143839","title":"Discovering Hidden Gems in Model Repositories","description":"Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.","url":"https://arxiv.org/abs/2601.22157v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:55.000Z","metadata":{"arxivId":"2601.22157v1","authors":"Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen","categories":"cs.LG, cs.CL","published":"2026-01-29T18:59:55Z","pdfUrl":"https://arxiv.org/pdf/2601.22157v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22157v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22157v1","rank":2,"domain":"LLM","zh_summary":"公共仓库托管了数百万经过微调的模型，但社区使用仍然过度集中于少数基础检查点。我们探讨了这种集中现象是反映了高效的市场选择，还是优质模型被系统性忽视。通过对超过2000个模型的广泛评估，我们展示了“隐藏的宝石”普遍存在——这些不受欢迎的微调模型显著优于其热门对应模型。值得注意的是，在Llama-3.1-8B家族中，我们发现一些很少被下载的检查点能够将数学性能从83.2%提升至96.0%，而无需增加推理成本。然而，通过对每个上传模型进行穷尽评估来发现这些模型在计算上是不可行的。因此，我们将模型发现问题表述为多臂赌博机问题，并通过使用共享查询集和激进的淘汰策略来加速顺序减半搜索算法。我们的方法仅需每个候选模型50次查询即可检索到顶级模型，使发现速度加快了50倍以上。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对公共模型仓库（如 Hugging Face）中“用户集中下载极少数热门模型，而大量模型无人问津”这一现象，提出并回答两个核心问题：\n\n1. 热门模型是否真的就是同系列中性能最好的？\n2. 如果不是，如何在百万级模型中高效找出那些“被埋没的优等生”？\n\n为此，作者\n\n- 在 4 个主流模型树（Qwen-2.5-3B/7B、Mistral-7B、Llama-3.1-8B）上系统评估了 2000+ 微调/checkpoint，证明“隐藏宝石”（下载量极低却显著优于官方热门版本的模型）普遍存在；\n- 将“大海捞针”式的模型搜索形式化为固定预算的最佳臂识别（Best-Arm Identification）多臂老虎机问题，提出加速版 Sequential Halving 算法，仅 50 次查询即可把候选池缩小到 top-3，速度提升 50× 以上，平均性能提升 4.5% 以上。\n\nQ2: 有哪些相关研究？\n\n与本文密切相关的研究可归纳为以下四条主线，并在关键处给出对应文献：\n\n1. 模型群体与权重空间学习\n\n- 将神经网络权重视为数据点，直接学习其语义表征以预测性能、功能或血缘，无需运行推理：\n- Schürholt et al. 2021, 2022, 2024\n- Horwitz et al. 2025a,b,c\n- Kahana et al. 2025a,b\n- Navon et al. 2023a,b；Kofinas et al. 2024\n- 研究参数空间对称性/置换不变性，为权重空间距离与检索提供理论支撑：\n- Lim et al. 2023, 2024；Zhao et al. 2025\n2. 模型选择、路由与排行榜压缩\n\n- 传统 leaderboard 方法：在固定基准上一次性评估所有候选，计算量随模型数线性增长（Perlitz et al. 2024；Tamura et al. 2025）。\n- 多 LLM 路由系统：训练元控制器把输入动态分配给不同专家模型（Hu et al. 2024；Muqeeth et al. 2024）。\n- 小样本/高效基准：tinybenchmarks、label-efficient selection（Polo et al. 2024；Ashury-Tahan et al. 2024）。\n3. 固定预算最佳臂识别（Best-Arm Identification, BAI）\n\n- 纯探索型多臂老虎机：Successive Rejects、Sequential Halving、UCB-E、Bayesian Elimination 等（Audibert & Bubeck 2010；Karnin et al. 2013；Atsidakou et al. 2022）。\n- 本文在此基础上提出“共享查询集 + 激进剪枝”的域专用加速方案。\n4. 模型合并与群体优化\n\n- 权重平均/融合：Model Soups、WiSE-FT、TIES-Merging（Wortsman et al. 2022；Yadav et al. 2024）。\n- 协同下降与 swarm 优化：ColD Fusion、Model Swarms（Don-Yehiya et al. 2023；Feng et al. 2024, 2025）。\n\n这些工作共同构成了“如何在庞大模型生态中快速定位高价值个体”的研究背景，而本文首次系统论证了“隐藏宝石”现象的存在，并给出可扩展的纯探索检索算法。\n\nQ3: 论文如何解决这个问题？\n\n论文将“在百万级模型库里找出被埋没的高性能微调”这一难题拆成两步解决：\n\n1. 证明问题存在——“隐藏宝石”确实大量存在\n\n- 在 4 棵主流模型树（Qwen-2.5-3B/7B、Mistral-7B、Llama-3.1-8B）上统一采样 2 500 条 RouterBench 查询，对 2 000+ 个下游微调/适配器做相同推理成本的对照实验。\n- 定义“隐藏宝石”三条件：\n- 下载量不在前 1 %（Obscurity）\n- 性能位于前 1 %（Excellence）\n- 严格超过最受欢迎模型的得分（Dominance）\n- 结果：每棵树都发现满足上述条件的模型，例如 Llama-3.1-8B 一棵树上某微调把 GSM8K 从 83.2 % 提升到 96.0 %，而月下载量仅三位数，从而否定“市场已高效选出最优模型”的假说。\n2. 让搜索可负担——把“ exhaustive 评估”变成“固定预算最佳臂识别”\n\n- 形式化：给定模型树  T=m_1,…,m_K  与总查询预算  B ，目标是最小化简单遗憾\n\nr_B = max_(i) μ_i - μ_(hat imath)\n\n其中  μ_i  为模型  m_i  在任务上的真实准确率， hat imath  是算法停时后返回的模型。\n- 基础算法：Sequential Halving（SH）——多轮淘汰，每轮把剩余模型用相同查询数评估，淘汰后 50 %。\n- 提出两项领域专用加速：\n1. 相关采样（Correlated Sampling）\n每轮强制所有幸存模型回答**同一批**查询，用共享题集消减题目难度差异带来的方差，使得排序信噪比更高。\n2. 激进剪枝调度（Aggressive Elimination Schedule）\n观察到 90 % 上传模型质量极低，首轮即把候选池从  K  直接压到 100，并把 60 % 预算砸在这一步；后续每轮查询量翻倍，保证精英模型尽早获得高置信度估计。\n- 复杂度：每个候选只需约 50 次查询即可以 > 90 % 概率锁定 top-3 模型，对比 exhaustive 评估的 2 500 次，加速 50× 以上。\n3. 实验验证\n在 10、25、50、100、200 次查询/模型五种预算下重复 100 轮，平均 rank 与准确率均显著优于 Uniform、UCB、SR、TTTS、BayesElim 等 8 条基线；50 次查询即可逼近“全局最优”性能，且跨四棵树一致有效。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“隐藏宝石是否存在”与“能否高效找到”两条主线，共完成以下四类实验：\n\n1. 大规模对照评估——验证隐藏宝石存在性\n\n- 模型池：从 4 棵主流模型树（Qwen-2.5-3B、Qwen-2.5-7B、Mistral-7B、Llama-3.1-8B）中各抽取 ≈400 个全量微调 + ≈400 个适配器，过滤掉下载/运行失败案例后，实得 2 000+ 候选。\n- 评测协议：统一使用 RouterBench 的 2 500 题子集（ARC-C、Winogrande、MMLU、MBPP、GSM8K 各 500 题），所有模型在相同 prompt、相同解码参数（greedy, max\\_length 50/512）下推理。\n- 结果：\n- 每棵树均发现满足“Obscurity+Excellence+Dominance”三条件的隐藏宝石，例如\n– Llama-3.1-8B 树某数学微调 GSM8K 准确率 96.0 %，比官方 Instruct 提升 12.8 %，月下载量仅 930。\n– Mistral-7B 树某通用微调 RouterBench 提升 14.0 %，月下载量仅 1。\n- 超过 90 % 的宝石在模型卡片中**没有任何**对应任务性能记录，文本检索无法发现。\n2. 模型质量分布统计——支撑“激进剪枝”假设\n对每棵树分别绘制 GSM8K、MBPP、RouterBench 的累积准确率曲线，发现 60–99 % 的模型距离最佳模型 > 10 % 绝对分，说明“绝大多数候选可早期快速淘汰”。\n\n3. 固定预算 Best-Arm Identification 主实验——验证搜索效率\n\n- 设置 5 档预算：每模型平均 10、25、50、100、200 次查询，总预算  B = K × N 。\n- 对比 8 条基线：Uniform、UCB、UCB-StdDev、UCB-E、Successive Rejects、TTTS、BayesElim、标准 Sequential Halving。\n- 评价指标：100 次随机种子下的\n– 检索模型在完整 2 500 题上的**真实平均 rank**（↓越好）\n– 检索模型的**真实准确率**（↑越好）\n- 结果（50 查询示例）：\n\n| 树 | 基线最佳 rank/acc | 本文方法 rank/acc |\n| --- | --- | --- |\n| Qwen-3B | 30.0 / 0.721 | 3.5 / 0.729 |\n| Qwen-7B | 28.9 / 0.784 | 3.6 / 0.790 |\n| Mistral-7B | 4.0 / 0.693 | 1.6 / 0.695 |\n| Llama-8B | 29.9 / 0.720 | 3.0 / 0.736 |\n| 在全部 20 组（4 树×5 预算）设定中，本文方法均取得最低平均 rank 与最高平均准确率；50 次查询即可逼近“全局最优”性能（Oracle 第 1 行）。 |\n\n4. 消融实验——量化两项改进各自贡献\n\n- 仅替换调度器（保留随机抽样）：10 查询平均 rank 提升约 30 位。\n- 仅去掉相关采样（保留激进剪枝）：50 查询平均 rank 再降 2–3 位。\n- 两者结合取得最佳结果，验证“早期共享题集 + 快速削至 100 候选”缺一不可。\n5. 超预算鲁棒性实验\n用 25 查询的本文方法 vs 50 查询的基线，以及 50 查询的本文方法 vs 100 查询的基线，前者仍显著优于后者，说明加速效果并非单纯靠“砸预算”。\n\n6. 文档缺失分析——解释为何宝石被埋没\n手动检查 24 颗已发现宝石：19 颗无任何性能文档，3 颗仅提供无关任务（如东南亚多语）结果，仅 2 颗有微弱相关记录，文本检索几乎无法召回。\n\n综上，实验从“存在性→分布特性→检索效率→组件贡献→现实障碍”五个维度系统支撑了论文主张。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“方法改进”“场景扩展”“理论分析”与“生态研究”四类，供后续工作参考：\n\n方法改进\n\n1. **权重空间先验 + 老虎机**\n利用权重空间表征网络（Schürholt 2021；Horwitz 2025a）预测初始性能 μ̂\\_i，再用 μ̂\\_i 作为 SH/UCB 的先验均值，实现“零查询冷启动”与查询自适应分配。\n\n2. **任务-查询联合选择**\n将查询选择（Polo 2024；Zouhar 2025）纳入 Bandit 框架，每轮同时决策“测哪些模型”与“用哪些题”，实现预算双重压缩。\n\n3. **多目标 Bandit**\n同时优化准确率、推理延迟、显存占用，形成 Pareto 前沿，而非单指标最佳臂。\n\n4. **在线更新与漂移检测**\n仓库持续上传新版本，可把算法扩展为“非静态 Bandit”，用漂移检测（如 CUSUM）触发重评估，避免模型过时。\n\n场景扩展\n\n5. **多模态/多语言树**\n将实验从纯文本 LLM 扩展到 Vision-Language、Audio 或 200+ 语种模型树，验证宝石现象是否依旧成立。\n\n6. **垂直领域深度挖掘**\n针对医学、法律、金融等低资源但高风险的垂直任务，构建领域专用 500-题速评集，看是否能挖出超越通用榜前十的“小透明”。\n\n7. **指令-攻击-鲁棒性三重评估**\n除了任务准确率，同时评估指令跟随能力（IFEval）与对抗鲁棒性（AdvGLUE），检验宝石是否存在“偏科”或安全隐患。\n\n理论分析\n\n8. **样本复杂度下界**\n在“共享查询 + 早期剪枝”条件下，推导新的 Best-Arm 样本复杂度下界，并与标准 SH 的 Ω(H log K) 比较，给出理论加速比。\n\n9. **方差缩减量刻画**\n量化 Correlated Sampling 对 Var\nμ̂_i − μ̂_j\n 的缩减系数，与题目难度分布的熵建立显式关系，指导查询集构造。\n\n10. **早期剪枝错误率控制**\n用多重假设检验框架（如 FDR）设定首轮淘汰阈值，给出“好模型被误杀”概率的上界，保证 aggressive 剪枝的可靠性。\n\n生态与工具\n\n11. **社区实时推荐系统**\n把算法封装成 Hugging Face Space 或 CLI 插件，用户输入任务描述 + 预算，云端 5 分钟返回 top-5 候选，并生成可复现的评测报告。\n\n12. **宝石成因挖掘**\n对隐藏宝石进行训练数据重建（Salama 2025）与超参反演，分析“数据配方”或“学习率调度”是否与性能跃升显著相关，形成“如何复现宝石”指南。\n\n13. **供应链图谱**\n结合模型血缘恢复（Horwitz 2025c；Yu 2025）与下载-性能二部图，研究“上游基础模型 → 下游微调”网络结构，量化信息孤岛与发现瓶颈。\n\n14. **可持续评估基准**\n构建动态最小可评测集（Minimal Viable Benchmark），每年用信息增益指标淘汰 10 % 冗余题目，降低整个社区评估碳排。\n\n一句话总结\n\n从“更快、更准、更全面”到“为何存在、如何再现、怎样服务社区”，上述任意一条均可作为下一步的探索切口。\n\nQ6: 总结一下论文的主要内容\n\n**主要内容速览**\n\n1. **问题**\n公共仓库（HF）月下载量 95 % 集中在 0.0015 % 的“明星”模型；用户默认选官方 checkpoint，未必最优。\n\n2. **发现**\n在 4 棵主流模型树共 2 000+ 微调上系统评测 → 每棵树均存在“隐藏宝石”：\n\n- 下载量排后 99 %，却**严格优于**最受欢迎版本\n- Llama-3.1-8B 数学任务提升 12.8 %（83.2 → 96.0），零额外推理成本\n- 90 % 宝石无任何性能文档，文本检索无法发现\n\n3. **方法**\n将“大海捞针”建模为**固定预算 Best-Arm Identification 多臂老虎机**：\n\n- 基础：Sequential Halving 多轮淘汰\n- 加速 1：Correlated Sampling——每轮所有候选答同一批题，消减难度方差\n- 加速 2：Aggressive Elimination——首轮即削至 100 候选，60 % 预算砸前筛\n→ 50 次查询/模型即可锁定 top-3，速度提升 50×，平均准确率 +4.5 %\n\n4. **实验**\n\n- 5 档预算（10–200 查询）（×）4 模型树（×）8 条基线，100 轮重复\n- 本文方法在所有设定下均取得**最低平均 rank**与**最高真实准确率**\n- 消融与超预算对比证实两项改进各自贡献且可减半预算\n\n5. **结论**\n公共仓库并非“高效市场”；大量高性能微调被埋没。提出的加速 Sequential Halving 使“挖宝石”在常规算力范围内即可实现，为社区模型选择提供实用工具。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovering%20Hidden%20Gems%20in%20Model%20Repositories，链接是https://arxiv.org/pdf/2601.22157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22157。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22156v1-1769990143839","title":"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts","description":"Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data","url":"https://arxiv.org/abs/2601.22156v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:53.000Z","metadata":{"arxivId":"2601.22156v1","authors":"Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu","categories":"cs.CL, cs.AI, cs.LG","published":"2026-01-29T18:59:53Z","pdfUrl":"https://arxiv.org/pdf/2601.22156v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22156v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22156v1","rank":3,"domain":"LLM","zh_summary":"混合 Transformer 架构结合了 softmax 注意力模块和循环神经网络（RNN），在长上下文建模中表现出理想的性能与吞吐量折衷，但由于从零进行大规模预训练的成本过高，其应用和研究受到限制。一些近期研究表明，预训练的 softmax 注意力模块可以通过参数传递和知识蒸馏转换为 RNN 模块。然而，这些转换方法需要大量训练数据（超过 100 亿个 token），且得到的混合模型在长上下文下的表现也较差，而混合模型在这种场景下本可以比基于 Transformer 的模型获得显著的推理加速。在本文中，我们提出了 HALO（通过层优化实现的混合注意力），这是一种将 Transformer 模型蒸馏为 RNN-注意力混合模型的流程。随后，我们提出了 HypeNet，一种通过新颖的位置编码方案（称为 HyPE）和各种架构改进实现出色长度泛化能力的混合架构。我们使用 HALO 将 Qwen3 系列转换为 HypeNet，实现了与原始 Transformer 模型相当的性能，同时在长上下文性能和效率上表现出优势。该转换仅需 23 亿个 token，不到其预训练数据的 0.01%。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. 现有混合 Transformer-RNN 架构的“高门槛”预训练成本\n混合模型（交错 softmax attention 与线性 RNN 层）在长上下文场景下兼具性能与吞吐优势，但通常需从头预训练数百亿级 tokens，对学术团队极不友好。\n\n2. 已有蒸馏方案的两大瓶颈\n\n- 数据量依旧巨大：先前将预训练 Transformer 蒸馏为混合架构的方法仍需 ≥10 B tokens。\n- 长上下文 recall 严重退化：蒸馏后的混合模型在 128 K 以上长度任务上性能骤降，丧失混合架构本应具备的推理效率优势。\n3. 目标\n提出一种**极低成本**（<3 B tokens）的跨架构蒸馏流程，使预训练 Transformer 可直接转换为**长上下文表现更强、推理更高效**的混合模型，而无需从头预训练。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为三类，均围绕“如何把预训练 Transformer 转化为线性复杂度的序列建模结构”展开：\n\n1. 混合架构（Hybrid Attention-RNN）\n\n- Jamba (Lieber et al., 2024)\n- MiniMax-01 (MiniMax et al., 2025)\n- Qwen3-Next (Qwen, 2025)\n- Kimi-Linear (Kimi et al., 2025)\n- NVIDIA Nemotron 3 (NVIDIA et al., 2025)\n共同点：从头预训练百亿级参数，attention 层与 RNN/SSM 层交错；未提供低成本转换方案。\n2. 蒸馏式“Attention → Hybrid”方法\n\n| 方法 | 发布 | 训练 tokens | 关键指标 |\n| --- | --- | --- | --- |\n| Mamba-in-the-Llama (Wang et al., 2025b) | 2024-08 | 20 B | 均匀替换 attention 层，长程 recall 下降明显 |\n| SMART (Yang et al., 2026) | 2025-05 | >7 B | 基于输出分布漂移挑选保留层 |\n| RAD (Hoshino et al., 2025) | 2025-05 | 20 B | 提出冗余度度量，仍需大量数据 |\n| Jet-Nemotron (Gu et al., 2025) | 2025-08 | 400 B | 任务性能下降指导层选择，成本最高 |\n| KL-LS (Li et al., 2025) | 2025-12 | 25 B | 逐层重跑蒸馏，用 KL 散度定重要性 |\n\n3. 纯 RNN/线性注意力蒸馏\n\n- RADLADS (Goldstein et al., 2025) — 本文 Stage-1 的对齐策略即在其基础上改进。\n- LoLCATs (Zhang et al., 2025)、Bick et al. (2025) — 将 attention 整体替换为线性形式，不涉及混合。\n\n上述工作或为“高成本预训练”，或为“十亿级 token 蒸馏且长上下文性能退化”。本文 HALO 首次把训练数据压缩到 2.3 B tokens 以内，同时通过 HyPE 与 Lightning Attention 等改进显著提升了 128 K–1 M 长度的 recall。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **HALO（Hybrid Attention via Layer Optimization）** 流水线与 **HypeNet** 架构，从“蒸馏效率”与“长程泛化”两条路径同时突破：\n\n1. 极低成本蒸馏流水线（HALO）\n\n- **Stage-0 权重热启动**：直接把 Teacher 的  W_q,W_k,W_v,W_o  填入线性 RNN 的对应投影，省去随机初始化。\n- **Stage-1 隐状态对齐**：仅训练 RNN 层，最小化\n\nmathcal L_(stage1)^((l))=MSE!(Y^((l))_(teacher),; RNN^((l))(X^((l-1))))\n\n用 320 M tokens 即可得到“逐层替身”。\n- **Stage-1.5 重要性评分**：一次性推理得到每层被替换后的\n\ns_i=max_j(R_(orig)-R^((i))_j)max_j(C_(orig)-C^((i))_j)+varepsilon\n\n按  s_i  排序保留 top-k（k=⌊L/4⌋）attention 层，无需反复重训。\n- **Stage-2 端到端蒸馏**：固定 Teacher，用 1 B tokens 做 KL 蒸馏\n\nmathcal L_(stage2)=D_(KL)!(f_(orig)parallel f_(hybrid))\n\n- **Stage-3 长上下文微调**：再用 1 B tokens、16 K 长度、低 LR 微调，总开销 2.3 B tokens。\n2. 面向长程泛化的 HypeNet 架构\n\n- **HyPE 位置编码**\n– Attention 层用 **NoPE**，保证无训练长度外推；\n– RNN 层用 **RoPE**，弥补局部位置信息；\n– 推理时对 attention logits 做**动态缩放**\n\ntilde s_t=log_a(t+a),quad a∈500,600,900\n\n抑制长序列熵增导致的注意力分散。\n- **四项结构微调**\n➊ RNN 也加 QK-Norm；➋ GQA→MHA 解除 KV 头共享；➌ 给 RNN/Attention 都加输出门；➍ 去掉短卷积以减少 CUDA kernel 开销。\n- **通用 RNN Mixer 插槽**\n只要满足\n\nS_t=F_t S_(t-1)+k_t^top v_t,quad y_t=q_t S_t W_o\n\n即可接入；实验表明 **Lightning Attention**（数据无关 forget gate）在长度泛化与吞吐间取得最佳平衡。\n3. 结果\n把 Qwen3-1.7B/4B/8B 用 HALO 转成 HypeNet 后，\n\n- 128 K 上下文 NIAH 平均准确率 >90 %，而 Jet-Nemotron、KL-LS 同期方法在 128 K 已跌至 <30 %；\n- 512 K 长度下解码速度提升 3.0×，预填充 3.4×，且显存占用随序列长度线性增长，Teacher 在 1 M 长度即 OOM。\n\n综上，论文通过“分层重要性筛选 + 轻量对齐 + HyPE 缩放”将蒸馏数据压缩两个数量级，并首次使蒸馏型混合模型在长上下文召回任务上超越原始 Transformer。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 6 组实验，覆盖「主结果—消融—效率」三个层次，全部在 1×A800-80 GB 上完成，代码与 checkpoint 已开源。\n\n1. 主实验：Qwen3 → HypeNet 蒸馏\n\n- 模型规模：1.7 B → 2 B、4 B → 5 B、8 B → 9 B\n- 训练数据：FineWeb-Edu 共 2.3 B tokens（Stage-1 320 M + Stage-2 1 B + Stage-3 1 B）\n- 评测任务\n– CSR：ARC-E/C、HellaSwag、WinoGrande、PIQA、LAMBADA、MMLU 共 7 项零样本任务，取归一化准确率平均值。\n– 长程 recall：RULER 的 NIAH-Single-1/2/3，长度 32 K–256 K；额外测到 1 M。\n- 对比基线\n– 原始 Qwen3（Teacher，128 K 用 YaRN 外推）\n– 同期蒸馏混合模型：Jet-Nemotron-2 B（400 B tokens）、KL-LS-GDN-3 B（25 B tokens）\n- 关键结果\n– 128 K 平均 NIAH 90 %↑，而 Jet-Nemotron 与 KL-LS 分别跌至 0 % 与 14 %。\n– CSR 平均仅比 Teacher 低 1–2 分，但 128 K 上下文吞吐提升 2.4–3.0×，显存节省 40 % 以上。\n2. 从头训练消融：验证 HyPE 与 Mixer 选型\n\n- 500 M 参数、20 B tokens、4 K 上下文训练，测 512–256 K 外推。\na) 位置编码对比\n– 全 RoPE、全 NoPE、Attn-RoPE+RNN-NoPE、SWAN-GPT、HyPE\n– 结果：HyPE 在 64× 训练长度仍保持 93.5 % NIAH，显著优于仅 RoPE（20 %）与仅 NoPE（60 %）。\nb) RNN Mixer 对比\n– Lightning、Mamba2、GLA、GDN、RWKV-7\n– Lightning 在 128 K 取得 95 % NIAH，同时预填充速度比 Mamba2 快 3.6×。\n3. HALO 架构消融：逐组件剔除\n在 Qwen3-1.7 B→2 B 的同一 Teacher 上，保留其他条件不变，仅移除下列模块：\n\n- ➊ RNN-RoPE → 128 K NIAH 从 79.9 % 降至 47.9 %\n- ➋ RNN-QK-Norm → 128 K 降至 17.3 %\n- ➌ GQA→MHA → 128 K 略升 83.5 %，但 256 K 从 74.3 % 降至 58 %\n- ➍ 输出门（RNN 或 Attention）（-1.5～-5.4 分不等）\n证明四项改进均对长程 recall 有正向作用。\n4. HALO 层选择策略对比\n同一 HALO 流程，仅替换“保留哪些 attention 层”的算法：\n\n- 均匀分布、仅后半均匀、Jet-Nemotron 任务下降法、KL-LS 逐层重蒸馏法\n- 结果：本文提出的 recall/CSR 比值评分法在 8 K–256 K NIAH 上平均领先 8–15 分，且零额外训练成本。\n5. 训练超参消融\na) Stage-1 数据量：320 M 已饱和，再增到 1.3 B 反而过拟合。\nb) Stage-2 峰值 LR：1e-4 最佳；RADLADS 推荐的常数 1e-5 导致 128 K NIAH 掉到 60 %。\n\n6. 效率实测\n\n- 测解码延迟（TPOT）与预填充时间，batch=1、BF16、Flash-Attention-2 / Triton kernel。\n- 512 K 上下文：HypeNet-1.7 B 解码 3.0×，预填充 3.4×；Qwen3 在 1 M 长度 OOM，HypeNet 仍可用 73 GB。\n- 各 Mixer 预填充对比：Lightning 在 256 K 单 token 时延 6 ms，与 SWA-512 持平，比 Mamba2 快 4× 以上。\n\n综上，实验从「可比性能+更低成本」「更长外推」「更快推理」三个维度系统验证了 HALO 与 HypeNet 的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下问题在论文中尚未完全解决，可作为后续工作的直接延伸：\n\n1. 指令遵循与对齐恢复\nHALO 仅用 FineWeb-Edu 预训练语料，导致原模型的指令跟随、对话安全等后训练能力被稀释。\n\n- 如何设计「低成本、数据高效」的对齐恢复阶段（例如 LoRA+RLHF、DPO）？\n- 是否需要保留部分原始对话数据或引入合成指令数据？\n2. 层选择与压缩极限\n目前固定保留 25 % attention 层；若继续降低至 15 % 甚至 5 %，recall 性能下降曲线如何？\n\n- 能否用可微结构搜索（NAS）一次性联合优化「层类型 + 层位置 + 头数」？\n- 动态/自适应深度：推理时依据输入长度自动决定启用多少 attention 层。\n3. 长度外推机制再深化\nHyPE 的 logits 缩放仅使用一个全局超参 a。\n\n- 能否让缩放系数 st 与内容或头维度相关，实现「头相关、token 相关」的自适应缩放？\n- 进一步结合相对位置惩罚（ALiBi、xPos）或频域校正，实现 2 M+ 长度无损外推。\n4. 多模态与工具调用\n本文仅验证文本模态。\n\n- 视觉/语音交叉注意力层是否同样适用 HALO 的「重要性评分」筛选？\n- 工具调用、代码解释器等需要强召回的场景，RNN 层是否会出现「关键 API 遗忘」？\n5. 异构硬件与量化协同\nHypeNet 的 KV-cache 已大幅缩减，但新增输出门与 MHA 带来 10 % 参数量上升。\n\n- 与 4-bit/2-bit 量化、KV-cache 4-bit 压缩、Flash-Linear Attention 专用 CUDA kernel 联合优化，能否在边缘 GPU 实现 1 M 上下文 30 tokens/s？\n- 探索「遗忘门常量化」后是否可直接用 INT8 矩阵乘法，进一步提速。\n6. 持续学习与领域适应\n蒸馏后模型继续在新领域增量预训练时，RNN 层与 attention 层的「可塑性」差异显著。\n\n- 如何设计不同的学习率或正则策略，防止 RNN 层过早遗忘旧知识？\n- 能否用 expansion/compression 框架动态增长或剪枝 RNN 隐状态维度？\n7. 理论分析\n\n- 对 HyPE 给出长度泛化的严格证明：NoPE attention 的误差界与 RNN 的 receptive field 如何耦合？\n- Lightning Attention 的数据无关 forget gate 与数据相关 gate 的「长度外推」能力差异，能否用谱范数或梯度流解释？\n8. 非 Transformer 教师模型\n目前 HALO 假设教师为纯 Transformer。\n\n- 若教师本身已是混合结构（如 Jamba），如何递归蒸馏「混合 → 更稀疏混合」？\n- 对于 MoE、RetNet、TTT 等不同范式，HALO 的「重要性评分」是否仍然有效？\n9. 自动超参搜索\n缩放底数 a、峰值 LR、层比例 k 目前靠网格搜索。\n\n- 用贝叶斯优化或强化学习把「a + LR + k」联合搜索，目标函数为 Pareto（性能, 吞吐, 显存），能否再节省 30 % 搜索时间？\n10. 绿色 AI 评估\n给出端到端碳排与美元成本：2.3 B tokens × A800 GPU hours = ? kg CO₂，与从头预训练相比减少几个数量级？\n建立「每美元准确率」「每克碳 recall」指标，推动社区采用蒸馏而非重训。\n\n这些方向既有理论价值（长度泛化、重要性度量），也能直接落地（指令恢复、硬件量化、多模态），可作为后续研究的切入点。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：混合 Transformer-RNN 模型需百亿级从头预训练，现有蒸馏法仍耗 10 B+ tokens 且长上下文 recall 暴跌。\n- **方法**：提出 **HALO** 流水线（2.3 B tokens）（权重热启动→隐状态对齐→基于 recall/CSR 重要性选层→KL 蒸馏→长上下文微调），并设计 **HypeNet** 架构：\n– **HyPE**：Attention 层用 NoPE 保长度外推，RNN 层用 RoPE 补位置信息，加动态 logits 缩放。\n– 四项结构微调：RNN 加 QK-Norm、GQA→MHA、输出门、去短卷积。\n– 通用 RNN Mixer 插槽，Lightning Attention 表现最佳。\n- **实验**：Qwen3 1.7/4/8 B→2/5/9 B HypeNet，128 K 上下文 NIAH>90 %（同期方法<30 %），CSR 与 Teacher 持平，512 K 解码 3.0×、预填充 3.4× 提速，1 M 长度 Teacher OOM 而 HypeNet 仍可行。\n- **结论**：首次实现“极低数据成本蒸馏 + 卓越长程性能”的混合模型，为高效长上下文 LLM 提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid%20Linear%20Attention%20Done%20Right%3A%20Efficient%20Distillation%20and%20Effective%20Architectures%20for%20Extremely%20Long%20Contexts，链接是https://arxiv.org/pdf/2601.22156，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22156。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22155v1-1769990143839","title":"UEval: A Benchmark for Unified Multimodal Generation","description":"We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.","url":"https://arxiv.org/abs/2601.22155v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22155v1","authors":"Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu","categories":"cs.CV, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22155v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22155v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22155v1","rank":4,"domain":"LLM","zh_summary":"我们介绍了 UEval，这是一个用于评估统一模型的基准，即能够生成图像和文本的模型。UEval 包含 1,000 个由专家策划的问题，这些问题要求模型输出同时包含图像和文本，来源于 8 个现实任务。我们策划的问题覆盖了广泛的推理类型，从逐步指南到教材说明。评估开放式多模态生成并非易事，因为简单的“使用大型语言模型作为评判者”的方法可能会忽略细微差别。不同于以往依赖多模态大型语言模型（MLLM）来评估图像质量或文本准确性的工作，我们在 UEval 中设计了基于评分标准的评分系统。对于每个问题，提供参考图像和文本答案给 MLLM 生成初始评分标准，该标准包含多个评估标准，然后由人工专家进行完善和验证。总的来说，UEval 包含 10,417 个验证过的评分标准，实现了可扩展且细粒度的自动评分。UEval 对当前的统一模型具有挑战性：GPT-5-Thinking 的得分仅为 66.4（满分 100），而最好的开源模型仅达到 49.1。我们观察到推理型模型通常优于非推理型模型，将推理型模型的推理路径迁移到非推理型模型会显著缩小差距。这表明，推理能力可能对于需要复杂多模态理解和生成的任务非常重要。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在填补“统一多模态生成”评测的空白。现有基准主要将视觉任务割裂为两类独立范式：\n\n- 视觉问答（VQA）：输入图像→输出文本\n- 文生图（T2I）：输入文本→输出图像\n\n二者均未考察**同一查询需同时生成图像与文本**的真实场景，例如“请画出示意图并解释潮汐成因”。为此，作者提出 UEval 基准，核心贡献与目标可归纳为：\n\n1. 构建评测集\n\n- 1 000 条专家筛选问题，覆盖 8 类真实任务（空间、教科书、图表、论文、艺术、生活、技术、运动）。\n- 每条样本要求模型**联合输出图像+文本**，并附带人工校验的参考答案。\n2. 设计细粒度评分协议\n\n- 采用“数据相关”的 rubric 机制：针对每题自动生成十余条评分细则（共 10 417 条），由人类二次校准。\n- 用前沿 MLLM 作评委，按细则逐项打分，实现可扩展、可复现的自动评估。\n3. 验证基准难度与洞察\n\n- 闭源最佳模型 GPT-5-Thinking 仅得 66.4/100，开源最佳仅 49.1，显示任务极具挑战性。\n- 发现“推理型”模型显著优于非推理型，且将推理轨迹迁移给非推理模型可缩小差距，提示**思维链机制对多模态生成同样关键**。\n\n综上，论文首次系统评估了**统一模型在“同时生成图像与文本”上的推理与生成能力**，并揭示当前模型在跨模态一致性、多步规划等方面仍存在显著不足。\n\nQ2: 有哪些相关研究？\n\n与 UEval 直接相关的研究可划分为三条主线：\n\n1. 统一多模态理解与生成模型；\n2. 多模态评测基准；\n3. 细粒度/数据相关评分方法。\n\n1\\. 统一多模态模型\n\n- **Transfusion** (Zhou et al., 2025a) —— 同一 Transformer 内自回归预测文本、扩散生成图像。\n- **Emu3/Emu3.5** (Wang et al., 2024; Cui et al., 2025) —— 纯下一 token 预测实现文本-图像双向生成。\n- **DreamLLM** (Dong et al., 2024) —— 协同训练 LLM 与扩散解码器，支持交错图文生成。\n- **Janus/Janus-Pro** (Wu et al., 2025a; Chen et al., 2025d) —— 解耦视觉编码，统一自回归框架。\n- **Show-o2** (Xie et al., 2025) —— 离散扩散+自回归混合，支持图文交错输出。\n- **BAGEL** (Deng et al., 2025) —— 连续/离散潜变量统一训练，兼顾理解与生成。\n\n2\\. 多模态评测基准\n\n- **VQA 系列**\n- VQA-v2 (Goyal et al., 2017)\n- OK-VQA (Marino et al., 2019)\n- MMBench (Liu et al., 2024b)\n- MMMU (Yue et al., 2024)\n- BLINK (Fu et al., 2024) —— 侧重视觉感知细节。\n- **文生图评测**\n- T2I-CompBench (Huang et al., 2023) —— 组合属性一致性。\n- Geneval (Ghosh et al., 2023) —— 对象级对齐。\n- ScienceT2I (Li et al., 2025a) —— 科学概念可视化。\n- **交错/统一生成评测**\n- OpenLEAF (An et al., 2024) —— 开放域交错图文生成。\n- MMIE (Xia et al., 2025) —— 大规模交错理解基准。\n- RealUnify (Shi et al., 2025b) —— 统一模型综合探针。\n- Uni-MMMU (Zou et al., 2025) —— 多学科统一评测。\n\n3\\. 细粒度或数据相关评分\n\n- **HealthBench** (Arora et al., 2025) —— 针对医疗问答自动生成 rubric，人类二次校验；UEval 的 rubric 流程受此启发。\n- **VDC** (Chai et al., 2025) —— 引入样本相关评分提示，缓解通用 prompt 带来的偏差。\n\n上述工作为 UEval 提供了模型架构、评测维度与自动化评分三方面的理论与方法基础。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 设计可扩展评分协议 + 大规模实验诊断”三步解决统一多模态生成缺乏标准评测的问题。\n\n1. 构建 UEval 基准\n\n- 任务层面：精选 8 类真实场景（空间、教科书、图表、论文、艺术、生活、技术、运动），共 1 000 题；每题必须同时输出图像与文本。\n- 数据层面：人工撰写参考文本+检索/绘制参考图像，形成高质量答案对。\n- 划分类型：\n– 闭式任务（空间/教科书/图表/论文）——答案客观、明确；\n– 开式任务（艺术/生活/技术/运动）——允许多种合理步骤图。\n2. 设计数据相关 Rubric 评分协议\n\n- 自动初稿：用 Gemini-2.5-Pro 针对“问题+参考答案”生成 10 余条细粒度准则（图像/文本约各占一半）。\n- 人工精修：两轮审核去重、补漏，最终得到 10 417 条无歧义准则。\n- 自动评分：再以 Gemini-2.5-Pro 作评委，逐条判定模型响应是否满足准则，计算满足率作为最终得分；与人评 Pearson r = 0.88，≈90% 准则一致。\n3. 大规模实验与诊断\n\n- 评测 9 个最新统一模型（5 开源 + 4 闭源），给出 8 任务排行榜，揭示：\n– 闭源最佳 GPT-5-Thinking 仅 66.4/100，开源最佳 Emu3.5 仅 49.1，基准足够困难。\n– 多步规划类任务（艺术、生活等）得分显著低于知识类任务；模型常出现步骤图序号错乱、视觉一致性差。\n– 推理模型全面优于非推理模型；把 GPT-5-Thinking 的 Chain-of-Thought 轨迹拼到非推理模型 prompt 后，其图像准确率明显提升（对弱模型无效），证实“推理”对跨模态生成的重要性。\n\n通过以上三步，论文首次实现了对“统一模型同时生成图像+文本”能力的标准化、可复现且细粒度的评测，并量化揭示了现有模型的短板与改进方向。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「统一多模态生成」共开展 4 组实验，覆盖模型性能、评分可靠性、裁判模型差异与推理迁移效果。\n\n1. 主评测实验（UEval 排行榜）\n\n- 被测模型：9 个最新统一模型\n– 开源：Janus-Pro、Show-o2、MMaDA、BAGEL、Emu3.5\n– 闭源：Gemini-2.0-Flash、Gemini-2.5-Flash、GPT-5-Instant、GPT-5-Thinking\n- 指标：以 Gemini-2.5-Pro 为评委，按 10 417 条 rubric 计算「满足率」。\n- 结果：\n– 闭源全面领先，GPT-5-Thinking 平均 66.4，开源最佳 Emu3.5 仅 49.1。\n– 多步规划任务（art/life/tech/exercise）得分远低于知识任务（space/textbook/diagram/paper）。\n– 图像得分普遍低于文本得分；开源模型图像平均 <15，文本亦低于纯文本模型。\n2. 人类一致性验证\n\n- 随机抽取 10 % GPT-5-Thinking 输出，由人工独立标注每条 rubric 是否满足。\n- 指标：准则级匹配率、样本级 Pearson r。\n- 结果：≈90 % 准则匹配，r = 0.88，证明 LLM-as-a-judge 可替代人工。\n3. 不同评委模型敏感性实验\n\n- 用 6 个额外 MLLM（GPT-5-Thinking、Seed1.6-Vision、Qwen3-VL-235B-Thinking/Instruct、GLM-4.1V-Thinking）对同一批 GPT-5-Thinking 响应再评分。\n- 结果：\n– Gemini-2.5-Pro、GPT-5-Thinking、Qwen3-VL-235B-Thinking 给出高度一致分数；\n– Seed1.6-Vision、GLM-4.1V-Thinking 在开放任务打分显著偏高，提示评委选择影响结论。\n4. 推理轨迹迁移实验\n\n- 方法：将 GPT-5-Thinking 产生的 Chain-of-Thought 段落直接追加到原 prompt，再喂给非推理模型（GPT-5-Instant、Gemini-2.5-Flash、BAGEL）。\n- 指标：图像准确率变化（人工盲审）。\n- 结果：\n– 前两者生成的图像更接近参考答案（如自由女神像皇冠内部结构），得分提升；\n– BAGEL 等较弱模型无改善，说明「有效利用推理信号」本身需要足够强的生成能力。\n\n上述实验系统验证了 UEval 的难度、评分可靠性，并首次量化证明「推理能力」对统一多模态生成的正向作用。\n\nQ5: 有什么可以进一步探索的点？\n\n可进一步探索的方向（按研究类别分组）\n\n1\\. 基准与评测协议\n\n- **动态/可扩展 rubric**\n当前 rubric 为静态人工校验，可探索“迭代式”或“对抗式”生成：让评委模型与候选模型多轮博弈，自动发现新失败模式并即时补充准则。\n\n- **多图像一致性指标**\n开式任务含多图序列，可引入“跨帧 CLIP 相似度”“对象检测 ID 一致性”等自动度量，与 rubric 打分联合拟合，减少评委模型开销。\n\n- **人类-模型混合评审**\n对高分段样本（>90）仍显模糊，可采用“模型预筛 + 人工终审”的主动学习循环，进一步压缩人工成本。\n\n2\\. 模型能力诊断\n\n- **细粒度错误归因**\n将 rubric 准则按“视觉生成-文本生成-跨模态对齐”三级标签分类，统计失败分布，定位是“画不对”还是“说不对”抑或“图文不符”。\n\n- **多步规划深度扩展**\n当前最长 5–6 步，可构造 10+ 步的复杂流程（实验协议、手工折纸等），检验模型长程一致性及中间步骤可复现性。\n\n- **多图联合扩散 vs. 自回归**\n对同一问题分别用“一次扩散出整图网格”与“逐图自回归”两种策略，评估视觉一致性、推理链完整性，寻找最佳生成范式。\n\n3\\. 推理机制\n\n- **推理轨迹可迁移性上限**\n系统研究“评委模型-生成模型”能力 gap 阈值：当生成模型参数量/数据量低于何值时，外部推理轨迹不再有效。\n\n- **视觉思维链（Visual-CoT）**\n让模型在生成每幅图像前输出“中间草图”或“布局描述”，形成显式视觉推理链，检验能否进一步提升一致性与可解释性。\n\n- **逆向推理：文本→图像→文本**\n先让模型生成图像，再屏蔽原 prompt 仅让另一模型依据图像生成解释，循环评估可重建性，量化图像的信息完备度。\n\n4\\. 数据与任务\n\n- **引入时序/动态内容**\n将静态图扩展为短视频片段（3–5 秒），要求模型生成“关键帧+解说”，评测统一模型对时序因果的理解。\n\n- **跨语言与文化**\n将 prompt 与 rubric 本地化到多语言、多文化场景（如中国古建筑、阿拉伯图案），观察模型是否出现文化偏差或视觉符号误用。\n\n- **专业领域深度**\n在医学、法律、机械维修等高风险领域收集小样本，用 UEval 式 rubric 评估，验证统一模型在专业场景下的可靠性。\n\n5\\. 效率与鲁棒性\n\n- **评委模型效率优化**\n探索“小模型蒸馏 + 准则级早期退出”策略，在保持 0.85+ 人类相关度的前提下，将评审成本降低一个数量级。\n\n- **对抗攻击与鲁棒性**\n对图像生成加入隐写扰动或对文本加入同义词扰动，观察评委模型是否被误导，建立鲁棒性评分曲线。\n\n6\\. 统一模型改进\n\n- **联合训练目标加权**\n依据 UEval 的图像-文本 rubric 失败率，动态调整训练损失权重，实现“哪里弱训哪里”的自适应优化。\n\n- **检索增强生成（RAG-for-Generation）**\n允许模型在生成前检索参考图或知识图谱，检验外部记忆能否弥补多步规划与细节一致性缺陷。\n\n这些方向可分别从基准、诊断、推理、数据、鲁棒性与训练等角度推进，进一步揭示统一多模态模型的能力边界与改进路径。\n\nQ6: 总结一下论文的主要内容\n\n**UEval: A Benchmark for Unified Multimodal Generation** 提出首个系统评估“同一 prompt 下同时生成图像+文本”能力的基准，核心内容如下：\n\n1. 问题定义\n现有基准仅分别评测 VQA（图→文）或 T2I（文→图），忽视真实场景需联合输出图文。UEval 填补该空白。\n\n2. 基准构建\n\n- 1 000 道专家筛选题，覆盖 8 类任务：空间、教科书、图表、论文（闭式）；艺术、生活、技术、运动（开式）。\n- 每题配参考图像+文本，人工校验。\n3. 评分协议\n\n- 数据相关 rubric：Gemini-2.5-Pro 针对每题自动生成十余条细则→人工精修，共 10 417 条。\n- 用同一 MLLM 评委按细则逐项打分，满足率即最终得分；与人评 Pearson r = 0.88。\n4. 主实验结果\n\n- 9 个统一模型参评：闭源最佳 GPT-5-Thinking 66.4/100，开源最佳 Emu3.5 仅 49.1，显示基准挑战大。\n- 多步规划任务得分远低于知识任务；模型常出现步骤图序号错乱、图文不一致。\n- 文本生成普遍强于图像；开源模型图像得分常 <15。\n5. 推理迁移实验\n将 GPT-5-Thinking 的 Chain-of-Thought 轨迹追加到非推理模型 prompt，其图像准确率显著提升，表明推理能力对多模态生成关键。\n\n6. 结论\nUEval 提供可扩展、细粒度的统一多模态生成评测，量化揭示当前模型在跨模态一致性、复杂规划与图像质量上的显著不足，为未来研究指明改进方向。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UEval%3A%20A%20Benchmark%20for%20Unified%20Multimodal%20Generation，链接是https://arxiv.org/pdf/2601.22155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22155。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22154v1-1769990143839","title":"Exploring Reasoning Reward Model for Agents","description":"Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.","url":"https://arxiv.org/abs/2601.22154v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22154v1","authors":"Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue","categories":"cs.AI, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22154v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22154v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22154v1","rank":5,"domain":"LLM","zh_summary":"主动强化学习（Agentic RL）在使智能体执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍然依赖基于稀疏结果的奖励来进行训练。这种反馈无法区分中间推理质量，导致训练结果不理想。在本文中，我们提出了智能体推理奖励模型（Agent-RRM），这是一种多方面的奖励模型，为智能体轨迹生成结构化反馈，包括 (1) 明确的推理轨迹，(2) 聚焦的批评，通过突出推理缺陷提供改进指导，以及 (3) 评估过程表现的总体评分。利用这些信号，我们系统地研究了三种整合策略：Reagent-C（文本增强的优化）、Reagent-R（奖励增强的指导）和 Reagent-U（统一反馈整合）。在 12 个多样化基准上的广泛评估表明，Reagent-U 能显著提升性能，在 GAIA 上达到 43.7%，在 WebWalkerQA 上达到 46.2%，验证了我们的推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来研究。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**Agentic Reinforcement Learning（Agentic RL）**中**稀疏、仅依赖结果（outcome-based）的奖励信号**所带来的两大核心缺陷：\n\n1. **无法区分中间推理质量**\n在长程、多步工具调用任务里，只要最终答案错误，整条轨迹就被视为完全失败，导致成功的前置步骤被埋没，训练信号过于粗粒度。\n\n2. **缺乏可操作的改进指引**\n纯标量奖励只给出“好/坏”评价，不提供具体哪里出错、如何修正的信息，使得智能体难以针对性改进推理与工具使用策略。\n\n为此，作者提出**Agent Reasoning Reward Model（Agent-RRM）**，一次性输出三种结构化信号：\n\n- 显式推理轨迹 `<think>`\n- 针对性批评 `<critique>`\n- 整体质量评分 `<score>`\n\n并基于该多维度反馈设计三种训练变体（Reagent-C / R / U），系统验证**密集推理奖励**在提升智能体推理与工具使用能力上的有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究划分为两条主线，并指出各自与本文工作的差异。以下按两条主线归纳代表性文献及其与 Agent-RRM 的关系。\n\n2.1 Agentic Reinforcement Learning\n\n**核心问题**：现有方法几乎全部采用**稀疏、结果导向**的奖励，无法对长程轨迹的中间步骤进行细粒度监督。\n\n| 代表工作 | 主要贡献 | 与本文差异 |\n| --- | --- | --- |\n| Search-R1 (Jin et al., 2025) | 用 RL 训练 LLM 交替执行搜索与推理，提升检索增强推理性能 | 仅依赖最终答案正确性作为奖励，无中间反馈 |\n| WebSailor (Li et al., 2025b) | 将 RL 扩展到长程网页导航，降低信息搜寻不确定性 | 同样使用稀疏结果奖励，无法纠正中间步骤错误 |\n| Agent0 (Xia et al., 2025) | 无需人工监督，让工具感知行为通过共进化自动涌现 | 未引入显式推理质量评估，训练信号仍稀疏 |\n\n2.2 Reward Modeling\n\n**核心问题**：现有奖励模型要么只给**步骤级标量分**（标注成本高、易被 hack），要么只做**成对偏好比较**（无法提供可执行改进建议）。\n\n| 代表工作 | 奖励信号形式 | 与本文差异 |\n| --- | --- | --- |\n| Atom-Searcher (Deng et al., 2025) | 直接拿 30B 模型给每一步打 0/1 分 | 仅步级标量，无语言解释，易奖励黑客 |\n| PPR (Xu et al., 2025) | 用预定义原则给步骤打分 | 同样是步级标量，缺乏可操作 critique |\n| RM-R1 (Chen et al., 2025d) | 先生成推理 rubric 再给出评分 | 面向通用问答，未针对 agent 工具链设计 |\n| R1-Reward (Zhang et al., 2025b) | 多模态推理奖励模型，引入稳定 RL 算法 | 仅输出标量，无文本 critique 用于 refinement |\n\n小结\n\n- **Agentic RL 侧**：首次把“**推理轨迹 + 批评文本 + 标量分**”统一引入 agent 训练流程。\n- **Reward Model 侧**：首次为 agent 设计**无需真值**即可生成**可执行批评**的多维奖励模型。\n\nQ3: 论文如何解决这个问题？\n\n论文将“稀疏结果奖励 → 细粒度推理奖励”的转化拆成三步：\n\n1. 设计**多维奖励模型** Agent-RRM；\n2. 构建**覆盖四种任务类型**的高质量数据集；\n3. 提出**三种集成策略**系统地把推理奖励喂给智能体。\n\n以下按方法论（Method）顺序展开。\n\n3.3 数据集构造：先解决“有信号”问题\n\n- **Reagent-RL-709K**\n– 来源：公开 QA + 自采复杂任务，共 70.9 万条。\n– 过滤：三阶段去歧义、去重、难度采样，保证轨迹多样性。\n\n- **Reagent-SFT-55.6K**\n– 用 DeepSeek-V3.1 生成轨迹，**只保留最终答案正确**的 5.56 万条，作为冷启动 SFT 数据。\n\n- **Agent-RRM 专用标注**\n– 在 709 K 上采样 118 K 轨迹，用 GPT-4o-mini-120B 标注三元组 `<think>｜<critique>｜<score>`，得到\n– Reagent-RRM-SFT-28K\n– Reagent-RRM-RL-90K\n\n3.4 Agent-RRM：再解决“信号可信”问题\n\n两阶段训练：\n\n1. SFT 阶段：在 28 K 上让 8 B 模型学会输出结构化三元组。\n2. GRPO 阶段：在 90 K 上用强化学习微调，使\n- 推理轨迹自洽（降低幻觉）\n- 标量分与人工排序一致性最高（Kendall τ 提升 0.12）\n\n**输出格式**（训练/推理一致）：\n\n```text\n<think>…逐步检查工具调用与逻辑链…</think>\n<critique>…指出具体错误：漏 browse、误用工具、循环调用…</critique>\n<score>0.32</score>\n```\n\n3.5 Reagent：最后解决“信号用好”问题\n\n三种集成策略对应三条消融曲线：\n\n| 变体 | 利用的信号 | 训练方式 | 关键公式 |\n| --- | --- | --- | --- |\n| Reagent-C | 仅 <critique> | 零参更新，in-context 二次生成 |  o_i^((2))sim π_θ(·mid q,o_i^((1)),c_i)  |\n| Reagent-R | 仅 <score> | 传统 GRPO，把 score 当密集奖励 |  R_i=R_(rule)+λ·Agent-RRM(q,o_i)  |\n| Reagent-U | <critique>+<score> | 统一池化优化：初始+修正轨迹一起算优势 | 见下方公式 |\n\n**Reagent-U 统一目标**\n\nJ_U(θ)=E[(1) / (2G)∑_(k=1)^2∑_(i=1)^G[min!(r_i^((k))(θ)A_i^((k)),clip_ε)-β D_(KL)^((i,k))]]\n\n其中\n\n-  k∈1,2  对应“初始轨迹”与“经 critique 修正后的轨迹”\n- 优势  A_i^((k))  在 **混合池**  G_(pool)  内标准化，保证模型同时优化“一次做对”与“被提示后能改”两种能力。\n\n实验验证“信号有效”\n\n- **Reagent-C** 在 GSM8K 上把 94.6 → 94.9，**无需训练**即可修正 47 % 的初级计算错误。\n- **Reagent-R** 在 Bamboogle 上比纯规则奖励提升 11.2 个百分点，验证密集奖励缓解稀疏性。\n- **Reagent-U** 在 12 个基准上**全部领先**，GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相对 8 B 基线平均提升 18.4 %，证明两种信号**协同增益**最大。\n\n一句话总结\n\n论文通过“**多维奖励模型 → 数据 → 三种集成策略**”的闭环，把原本只能告诉智能体“错”的稀疏信号，升级为同时告诉它“**哪里错、为何错、如何改**”的密集推理奖励，从而系统性地提升了长程工具使用与推理能力。\n\nQ4: 论文做了哪些实验？\n\n论文在第 4 节“Experiments”中围绕 **12 个基准、3 类任务、3 种变体、4 组消融** 展开系统评估，可概括为“**一个主表 + 三个深度分析 + 两个参数实验**”。\n\n1\\. 主实验：12 基准全面比较\n\n**任务分组与指标**\n\n- **数学推理**：AIME24/25、MATH500、GSM8K        pass@1\n- **知识密集型**：HotpotQA、2Wiki、Bamboogle、MuSiQue  pass@1\n- **通用 Agent & 搜索**：GAIA、WebWalkerQA、HLE、xbench pass@1\n\n**对照维度**\n\n- 闭源：o3, o1-preview, Claude-4-Sonnet, DeepResearch\n- 开源同规模（≤8 B）：WebThinker、ARPO、VerlTool …\n- 开源大尺度（≤32 B）：QwQ-32B、DeepSeek-R1-671B、Search-o1 …\n- 开源过程奖励：Atom-Searcher、PPR-Instruct\n\n**核心结果**（表 1+2 汇总）\n\n| 模型 | GAIA | WebWalkerQA | Bamboogle | AIME24 | 平均提升 |\n| --- | --- | --- | --- | --- | --- |\n| Qwen3-8B 基线 | 21.4 | 29.0 | 53.6 | 46.7 | — |\n| Reagent-C | 25.2 | 35.5 | 61.6 | 56.7 | +6.8 |\n| Reagent-R | 36.9 | 45.3 | 72.8 | 53.3 | +11.4 |\n| Reagent-U | 43.7 | 46.2 | 76.8 | 60.0 | +18.4 |\n\n2\\. 深度分析 1：Textual-Critique 真有用吗？\n\n- **实验**：Reagent-C 零参更新，仅把 critique 作为 prompt 让模型再答一次。\n- **结果**：12 个数据集**全部上涨**，GSM8K 94.6→94.9，GAIA 21.4→25.2；**首次失败中 47 % 在第二次答对**。\n- **结论**：critique 提供**可执行诊断**，无需训练即可纠正工具误用与逻辑跳跃。\n\n3\\. 深度分析 2：Model-based Reward 能缓解稀疏性吗？\n\n- **实验**：固定 λ=0.3，比较 Reagent-R（规则+模型分） vs Reagent w/o Agent-RRM（仅规则）。\n- **结果**：Bamboogle +11.2，xbench +9.0，平均 +7.6 个百分点。\n- **结论**：密集**标量奖励**显著改善训练信号，但仍低于 Reagent-U，说明**纯分数不如“分数+文字”**。\n\n4\\. 深度分析 3：Unified Feedback 是否产生协同效应？\n\n- **实验**：Reagent-U 同时优化初始与 critique-修正轨迹，优势在混合池统一标准化。\n- **结果**：\n– 在**需要≥5 步工具调用**的 GAIA Lv3 上，Reagent-U 16.7 %，比 Reagent-R 高 10 个百分点。\n– 在**跨模态**的 GAIA-full（165 题）上，pass@1 38.8 %，比 MCP-R1 高 1.2 个百分点，验证**非文本任务同样受益**。\n- **结论**：scalar 信号提供**全局优化方向**，critique 信号提供**局部修正动作**，二者联合产生**1+1>2**的协同增益。\n\n5\\. 参数实验\n\n5.1 奖励权重 λ 扫描（图 3）\n\n- 范围：λ∈\n0,0.5\n；任务：AIME24（数学）、xbench（深度搜索）。\n- 结果：λ≈0.2–0.4 区域性能 plateau；λ≥0.5 开始下降，**过高权重会牺牲最终答案准确性**。\n\n5.2 训练步数与收敛性（附录 B）\n\n- 300 步 GRPO 已收敛，继续训练未见明显提升，说明**数据量与模型规模匹配**，未出现过度拟合。\n\n6\\. 案例可视化（附录 D）\n\n- **Case 1（GAIA 搜索）**：critique 指出“未 browse 权威页→假设错误”，第二次调用 browse 拿到 15 Mt 精确值，答案从 Cannikin→Bravo。\n- **Case 2（GSM8K 数学）**：critique 指出“Python 第一次无 print 浪费调用”，第二次去掉多余调用并纠正“总工时÷人数”逻辑，答案从 47.25→189。\n\n一句话总结\n\n实验从“**能不能用→管不管用→好不好用→怎么用最好**”四个层次验证：\n**多维推理奖励模型 + 统一集成训练** 在 12 基准、跨模态、长程工具调用场景下**全面领先**，且**零参 critique 即可带来显著推理修正**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下可探索方向按“**规模-数据-任务-信号-理论**”五层归纳，均直接对应论文第 6 节 Limitations 的开放问题，并给出可落地的下一步实验设计。\n\n1\\. 规模：更大参数与分布式 RL\n\n- **问题**：全文仅在 8 B 模型验证，协同效应是否会随规模放大？\n- **探索点**\n– 在 32 B/70 B 基座重复 Reagent-U 训练，观察“λ-性能”曲线是否左移（更强模型需要更少 critique 即可收敛）。\n– 采用 **Model-parallel + Rollout-parallel** 框架，把 Agent-RRM 与 Policy 解耦到不同 GPU 节点，验证千亿级模型下是否出现奖励过拟合（reward overfitting）。\n\n2\\. 数据：开放环境 vs 静态基准\n\n- **问题**：12 个基准均为**静态题库**，工具集固定，无法反映真实世界工具漂移。\n- **探索点**\n– 引入 **Live-API 环境**（如 Google Search 动态排名、WolframAlpha 付费限额），构建非平稳奖励，测试 Agent-RRM 的**在线自适应**能力。\n– 设计 **Adversarial Tool-Response** 数据集：故意返回错位、截断或冲突信息，量化 critique 对“工具幻觉”的纠错上限。\n\n3\\. 任务：长周期闭环与多智能体\n\n- **问题**：现有 episode 最长 30 步，且为单 agent。\n- **探索点**\n– **Science-of-Chemistry** 场景：让 agent 连续运行 100+ 步完成“文献检索→实验设计→代码仿真→结果验证”闭环，用 Agent-RRM 每 10 步做一次 critique，观察**长程信用分配**是否仍有效。\n– **Multi-Agent Debate**：把 critique 角色独立为第三方裁判，与其他 agent 的 solution 进行**多轮对抗式 refinement**，检验 critique 语言是否出现**策略性误导**（reward hacking 新形态）。\n\n4\\. 信号： critique 的粒度与格式\n\n- **问题**：固定三段式输出可能不是最优。\n- **探索点**\n– **Token-Level 奖励**：把 `<critique>` 拆成**逐句批评**，用指针网络对齐到具体动作，实现**子步骤级**优势估计：\n\nA_t = R_(sentence)(t) - μσ\n\n– **Multimodal-Critique**：当观测含图/音频时，让 Agent-RRM 输出**圈图 mask** 或**音频时间戳**，验证**视觉定位**能否进一步降低修正成本。\n\n5\\. 理论：奖励可解释性与收敛保证\n\n- **问题**：Agent-RRM 的自洽性缺乏形式化刻画。\n- **探索点**\n– 建立 **Critique Consistency Index (CCI)**：同一轨迹两次采样 critique 的**语义相似度**，用 CCI 与最终性能做皮尔逊检验，量化“**可解释→可改进**”的因果链。\n– 在**离线 RL** 框架（如 CQL）下，把 Agent-RRM 当作 Q-function 正则项，推导**误差上界**，回答“**推理奖励误差 ε 对策略性能影响是 O(ε) 还是 O(ε²)**”。\n\n快速启动清单（8 卡 A800 可完成）\n\n1. 用 32 B 模型复现 Reagent-U，记录 λ∈{0.1,0.3,0.5} 的 GAIA 性能 → 验证规模效应。\n2. 在 Live-API 环境跑 500 条长 episode（≥50 步），统计 critique 成功纠正“工具返回漂移”的比例 → 验证鲁棒性。\n3. 把 critique 拆成句子级嵌入，与动作序列做注意力对齐，重新计算优势 → 验证更细粒度信号是否带来额外 2–3 个百分点。\n\n一句话总结\n\n未来工作可沿“**更大模型、更乱环境、更长链条、更细信号、更严理论**”五轴推进，把 Agent-RRM 从“静态题库好”推向**开放世界可信**。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Agent-RRM**（Agent Reasoning Reward Model）与 **Reagent** 训练框架，解决 Agentic RL 中“稀疏结果奖励无法区分中间推理质量、缺乏可执行改进信号”的核心痛点。主要贡献与内容可概括为以下四点：\n\n1. 多维推理奖励模型\nAgent-RRM 对每条轨迹输出三元组：\n\n- `<think>`：逐步推理轨迹\n- `<critique>`：可执行的错误指正\n- `<score>`：0–1 整体质量分\n无需真值即可提供密集、语言化、可解释的反馈。\n2. 高质量数据集\n构建 70.9 万 RL 样本（Reagent-RL-709K）与 5.56 万 SFT 样本（Reagent-SFT-55.6K），并标注 11.8 万条结构化评判（Reagent-RRM-SFT/RL-28K/90K），覆盖数学、多模态、网页搜索、复杂工具四类任务。\n\n3. 三种集成策略\n\n- Reagent-C：零参更新，用 critique 做 in-context 二次生成\n- Reagent-R：GRPO 训练，规则奖励 + Agent-RRM 分数\n- Reagent-U：统一池化初始与修正轨迹，同步优化 scalar 与语言信号\n4. 系统实验\n在 12 个基准（GAIA、WebWalkerQA、AIME、HotpotQA 等）上，Reagent-U 取得新 SOTA：GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相比 8 B 基线平均提升 18.4 %；零参的 Reagent-C 也能稳定上涨，验证多维推理奖励的普适性与可解释性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Reasoning%20Reward%20Model%20for%20Agents，链接是https://arxiv.org/pdf/2601.22154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22154。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22149v1-1769990143839","title":"DynaWeb: Model-Based Reinforcement Learning of Web Agents","description":"The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.","url":"https://arxiv.org/abs/2601.22149v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:07.000Z","metadata":{"arxivId":"2601.22149v1","authors":"Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu","categories":"cs.CL, cs.AI","published":"2026-01-29T18:59:07Z","pdfUrl":"https://arxiv.org/pdf/2601.22149v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22149v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22149v1","rank":6,"domain":"LLM","zh_summary":"由大型语言模型（LLM）和强化学习（RL）驱动的自主网页代理的发展，代表了通用人工智能助手迈出的重要一步。然而，与实时互联网的交互在训练这些代理时带来了严重的障碍，因为这种方式效率低、成本高且风险重重。基于模型的强化学习（MBRL）通过学习环境的世界模型以实现模拟交互，提供了一种有前景的解决方案。本文提出了 DynaWeb，一种新颖的 MBRL 框架，通过与经过训练以预测代理动作下自然网页表示的网页世界模型进行交互来训练网页代理。该模型作为一个合成的网页环境，在其中代理策略可以通过生成大量的回滚动作轨迹来进行“模拟”，从而实现高效的在线强化学习。除了自由策略回滚之外，DynaWeb 还结合了来自训练数据的真实专家轨迹，这些轨迹在训练过程中随机穿插在策略内回滚中，以提高稳定性和样本效率。在具有挑战性的 WebArena 和 WebVoyager 基准测试中进行的实验表明，DynaWeb 一贯且显著地提升了最先进开源网页代理模型的性能。我们的研究结果证明，通过“想象”训练网页代理是可行的，为扩展在线代理强化学习提供了一种可扩展且高效的方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何在不与真实互联网交互的前提下，高效、安全地训练具备在线强化学习能力的网页智能体”这一核心问题。\n具体而言，现有基于大模型的网页智能体虽可通过在线 RL 显著提升鲁棒性与长程决策能力，但直接依赖真实网页交互会带来三大瓶颈：\n\n1. 效率低：每次策略更新都需大量实时页面请求，训练周期长。\n2. 成本高：频繁访问真实站点易触发验证码、限流或封禁，需额外代理与清洗开销。\n3. 风险大：策略探索可能产生不可逆副作用（误下单、删数据、泄露隐私等），难以规模化。\n\n为此，作者提出将“世界模型”从传统推理工具升级为**在线 RL 的训练环境**，通过模型化互联网动力学，使智能体在“想象”中完成策略优化，从而把对真实环境的采样需求降到最低，同时保留在线 RL 的探索与信用分配优势。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均与“网页智能体”或“世界模型”密切相关，但尚未将二者以\\*\\*在线模型强化学习（MBRL）\\*\\*方式深度耦合：\n\n- **网页智能体训练范式**\n- 提示工程与推理框架：ReAct、Cognitive Kernel、MCP 等通过链式思维或结构化提示提升单步决策。\n- 监督/模仿学习：NNetNav、Go-Browse、Explorer、InSTA 等收集或合成高质量轨迹，用 SFT 训练。\n- 在线 RL：WebAgent-R1、WebRL、WorkForceAgent-R1、WebDancer 等直接在真实环境采样，用 GRPO、DAPO 等算法优化，但需大量真实交互，成本高。\n- **世界模型在网页场景的辅助应用**\n- 推理时规划：WebDreamer、WMA、RAP、WKM 用大模型模拟下一步网页状态，供树搜索或价值估计，**仅用于推断阶段**，不更新策略。\n- 离线数据增强：WebEvolver、Explorer 用世界模型生成额外轨迹，再离线蒸馏到策略，**与 on-policy 优化解耦**。\n- **通用世界模型与 MBRL 基础**\n- 视觉/游戏领域：Dreamer、SimPLe、IRLA、Diffusion World Models 等将环境动力学模型嵌入 RL 循环，实现“想象训练”。\n- 文本/决策抽象：RWKM、RAP、Diffusion for World Modeling 等把 LLM 当作隐式环境模型，但尚未在网页这类高动态、部分可观察场景做在线策略优化。\n\n综上，已有工作要么停留在“**推理阶段**”使用世界模型，要么“**离线生成**”伪轨迹后再蒸馏；DynaWeb 首次把**专门训练的网页世界模型**作为**可微环境**，与真实专家轨迹混合，直接用于 on-policy 强化学习，填补了“网页 MBRL”空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 DynaWeb，一套**面向网页任务的在线模型强化学习（MBRL）框架**，把“真实网页交互”替换为“可学习的网页世界模型 + 少量真实专家轨迹”的混合训练流程，核心步骤如下：\n\n1. 训练网页世界模型（WWM）\n\n- 将网页状态表示为**可访问性树**  o_t 。\n- 用监督数据  (o_t, a_t, o_(t+1))  训练大模型  p_φ  先输出**自然语言状态变更描述**  Delta ，再自回归生成下一帧树  hat o_(t+1) ，损失为\n\nL_φ = -∑ log p_φ(r,Delta mid I,o_t,a_t)\n\n- 得到的  p_φ  充当**可反复采样的网页“服务器”**，无需访问真实站点即可展开多步交互。\n2. 想象 rollout：策略与世界模型闭环\n\n- 策略  π_θ  以当前真实或想象观测为输入，采样动作  a_tsim π_θ(· mid hat o_(1:t),h_(1:t-1),a_(1:t-1),q) 。\n- 世界模型立即返回  hat o_(t+1)sim p_φ(· mid hat o_t,a_t,q) ，形成**想象轨迹**\n\nhatτ = (hat o_1,h_1,a_1,dots,hat o_T,h_T,a_T)\n\n- 终止时用模型自评给出稀疏奖励  hat r(hatτ,q)∈0,1 ，作为整条轨迹的回报  G(hatτ) 。\n3. 混合真实专家轨迹\n\n- 每次梯度更新按 50 % 比例**随机混入**来自 NNetNav SFT 数据集的**真实专家轨迹**，与想象 rollout 一起构成 batch。\n- 真实轨迹提供**无偏状态转移与奖励信号**，抑制世界模型幻觉，稳定训练。\n4. 序列级策略优化（GSPO）\n\n- 对每条轨迹计算单序列优势  hat A_i = G(hatτ_i) - bar G 。\n- 采用**序列级重要性权重**\n\ns_i(θ)=exp!l((1) / (|y_i|)∑_k logπ_θ(y_(ik)mid·s){π_(θ_old)(y_(ik)mid·s)}r)\n\n代入 clipped objective\n\nJ_(GSPO)(θ)=mathbb E![(1) / (G)∑_(i=1)^G min!l(s_i(θ)hat A_i, clip(s_i(θ),1!-!varepsilon,1!+!varepsilon)hat A_ir)]\n\n- 实现**长程信用分配**且避免 token 级方差爆炸。\n5. 控制想象深度与误差\n\n- 实验发现 dream length 4–5 步最佳，过长会累积模型误差；同时限制单条 rollout 最大 5 步并早停终端状态，兼顾探索深度与仿真保真度。\n\n通过上述设计，DynaWeb 把**策略优化完全搬到世界模型内部**，仅依赖少量真实轨迹做正则化，从而\n\n- 将真实环境采样需求降低一个数量级；\n- 避免不可逆操作与封禁风险；\n- 在 WebArena 与 WebVoyager 上相对最强基线分别提升 **+16.1 %** 和 **+18.7 %** 成功率，验证“用想象训练网页智能体”的可行性与扩展性。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“**能否用想象 rollout 替代真实网页交互来训练高性能网页智能体**”展开，分三部分：\n\n1. 主实验：WebArena & WebVoyager 成功率对比\n\n- 基准\n– 零样本提示：Llama-3.1-8B-Instruct+CoT、GPT-4o\n– 监督微调：NNetNav、Go-Browse\n– 离线 RL：WebRL\n– 推理时世界模型：ITL（每步模拟选最优动作，不训练策略）\n- 结果\n– WebArena（812 任务，5 站点）\nDynaWeb-8B 平均 SR **31.0 %**，相对离线 RL **+16.1 %**，在 Reddit/GitLab/CMS/Shopping 四域均第一。\n– WebVoyager（643 开放站点）\nDynaWeb 平均 SR **38.7 %**，显著高于次佳基线 WebRL（32.6 %），在 Amazon/BBC/Map 等 9 个站点领先。\n2. 消融实验：关键设计对性能的影响\n\n- Dream length 消融\n固定其余超参，令最大想象步数 1–8 变化；SR 在 4–5 步处峰值，过短欠探索，过长累积幻觉。\n- 真实轨迹比例消融\n0 %–100 % 替换想象 rollout；40 % 真实数据即可显著超越纯 SFT，>60 % 后收益饱和，验证“少量真实轨迹正则化”即可。\n- 世界模型训练必要性\n用**冻结 GPT-oss-120b** 替代微调后的 WWM；WebArena SR 从 31.0 % → 20.9 %，WebVoyager 35.4 % → 28.6 %，表明**显式环境动力学训练**不可或缺。\n3. 训练过程分析\n\n- 训练曲线：混入真实轨迹后，GSPO 的序列级 variance 更小，收敛更快。\n- 错误类型统计：相比纯想象策略，DynaWeb 将“页面元素幻觉”与“无效动作”两类错误各降低约 30 %。\n\n综上，实验系统验证了\n\n- 想象 rollout 可直接用于 on-policy RL；\n- 适度真实数据 + 适度 dream length 是性能拐点；\n- 专门训练的网页世界模型是提升的关键。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步扩展 DynaWeb 的想象力驱动范式：\n\n- **长程高保真世界模型**\n- 研究层级或扩散式架构，将页面视觉截图与 Accessibility Tree 联合建模，降低 5 步后的幻觉累积，实现 ≥20 步稳定 rollout。\n- 引入“页面差异压缩”或 Delta-VAE，仅对变化节点建模，减少自回归长度与误差传播。\n- **多模态世界模型**\n- 同时接受 HTML、CSS、屏幕截图、DOM 变化事件，统一文本与视觉空间，缓解纯文本树对动态 UI（Canvas、SVG、Shadow-DOM）信息丢失问题。\n- 探索视觉-动作对齐预训练，提升对图标、悬浮动画等细粒度交互的预测精度。\n- **可自我批评的世界模型**\n- 让同一模型输出“下一状态 + 预测不确定性/置信度”，策略据此自适应决定是否请求真实环境验证，形成**选择性查询**（selective query）机制，进一步压缩真实交互预算。\n- 引入对比式或能量模型，对世界模型置信度进行校准，防止过度乐观幻觉。\n- **层次化想象与策略分解**\n- 将任务分解为子目标（URL 导航、表单填充、结果提取），训练**高层子目标世界模型**与**低层原子动作世界模型**，实现“子目标→原始动作”两级想象，降低长程信用分配难度。\n- 结合选项框架（Options），在想象空间先做子目标规划，再展开低层动作 rollout。\n- **持续学习与领域增量**\n- 研究世界模型在线微调策略，避免新站点数据导致旧站点灾难性遗忘；可采用 EWC、LoRA-merging 或经验回放。\n- 探索任务-站点混合分布外推（OOD）检测，自动触发世界模型局部更新，保持长期一致性。\n- **安全与可验证想象**\n- 引入形式化约束（如不变量：价格≥0、表单不可越权提交），在世界模型采样阶段即屏蔽非法状态，减少策略探索危险动作。\n- 结合合约或沙箱 API，让世界模型对敏感操作（支付、写库）返回“虚拟确认”而非真实执行，实现零风险训练。\n- **跨环境迁移与元世界模型**\n- 收集 100+ 站点数据，训练**通用网页世界模型**，再针对新站点少量微调，验证“预训练-微调”范式能否快速适应未知站点动力学。\n- 研究元学习（MAML、Prompt-tuning）初始化，使世界模型在新站点仅用 50 条轨迹即可适配。\n- **更细粒度奖励模型**\n- 当前仅用终端 0/1 奖励，可训练**稠密价值模型**对想象中间状态打分，实现单步 TD 学习，进一步缩短真实轨迹需求。\n- 探索人工偏好 + 自动规则混合的奖励函数，缓解稀疏奖励导致的想象轨迹价值估计方差。\n- **系统级优化**\n- 世界模型与策略模型联合蒸馏为单一小模型，降低推理延迟，实现端侧部署。\n- 研究异步并行想象：GPU 集群同时展开数千条 rollout，用 Ray/vLLM 动态调度，提升训练吞吐。\n- **评测扩展**\n- 构建更长 horizon（≥50 步）与多会话（跨天登录、购物车保留）任务集，检验世界模型对**状态持久化与动态会话**的建模能力。\n- 引入对抗评测：在想象与真实环境间随机切换，测量策略对模型误差的鲁棒性，推动世界模型-策略协同优化。\n\n这些方向分别从模型架构、训练策略、安全约束、系统效率到评测维度出发，可进一步释放“想象驱动网页 RL”的潜力，并推广至更广泛的数字界面自动化任务。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **DynaWeb**：首个面向网页任务的**在线模型强化学习（MBRL）框架**，用“想象”替代昂贵、高风险的真实互联网交互，实现高效、安全的智能体训练。\n\n1. 问题\n在线 RL 可提升网页智能体的鲁棒性与长程决策，但直接访问真实站点效率低、成本高、风险大，难以规模化。\n\n2. 解法\n\n- 训练**网页世界模型**  p_φ ，输入当前可访问性树  o_t  与动作  a_t ，自回归输出状态变更描述  Delta  并生成下一帧树  hat o_(t+1) ，充当可无限采样的**虚拟网站**。\n- 策略  π_θ  与世界模型闭环，展开多步**想象轨迹**  hatτ ，用模型自评稀疏奖励  hat r(hatτ,q)  进行**序列级策略优化（GSPO）**。\n- 训练中**随机混入 40 % 真实专家轨迹**，提供无偏状态转移，抑制幻觉并稳定学习。\n- 控制想象深度 4–5 步，平衡探索与仿真保真。\n3. 实验\n\n- WebArena（812 任务）：DynaWeb 平均成功率 **31.0 %**，相对最强离线 RL 基线 **+16.1 %**。\n- WebVoyager（643 开放站点）：**38.7 %**，显著优于 GPT-4o、WebRL 等。\n- 消融：真实数据比例、dream length、世界模型训练必要性均被验证为性能拐点。\n4. 结论\nDynaWeb 首次证明**专门训练的世界模型可直接用于 on-policy 网页 RL**，在零风险、低成本的前提下持续提升开源智能体，为“想象驱动”的通用数字代理提供可扩展路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaWeb%3A%20Model-Based%20Reinforcement%20Learning%20of%20Web%20Agents，链接是https://arxiv.org/pdf/2601.22149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22149。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22146v1-1769990143839","title":"FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale","description":"Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .","url":"https://arxiv.org/abs/2601.22146v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:58:47.000Z","metadata":{"arxivId":"2601.22146v1","authors":"Ajay Patel, Colin Raffel, Chris Callison-Burch","categories":"cs.CL, cs.LG","published":"2026-01-29T18:58:47Z","pdfUrl":"https://arxiv.org/pdf/2601.22146v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22146v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22146v1","rank":7,"domain":"LLM","zh_summary":"由于监督训练数据有限，大型语言模型（LLM）通常通过自监督的“预测下一个单词”目标在大量非结构化文本数据上进行预训练。为了使得到的模型对用户有用，它还会在数量远小于前者的“指令调优”数据上进行进一步训练，这些数据由指令与响应的监督训练示例组成。为了克服有限的监督数据量，我们提出了一种方法，可以将互联网规模的预训练文档中的知识转化为数十亿条合成指令与回答训练对。生成的数据集称为 FineInstructions，使用了约 1800 万条从真实用户编写的查询和提示创建的指令模板。这些指令模板会与非结构化预训练语料库中的人工编写源文档匹配并实例化。通过在这种规模上生成的“监督”合成训练数据，LLM 可以从零开始仅通过指令调优目标进行预训练，这与预计的 LLM 下游使用（响应用户提示）更为一致。我们进行了受控的逐令牌训练实验，发现在 FineInstructions 上预训练，在标准基准测试中衡量自由形式响应质量时，优于标准预训练和其他提出的合成预训练技术。我们的资源可在 https://huggingface.co/fineinstructions 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决大规模语言模型（LLM）预训练阶段与下游“指令-回答”使用场景之间分布不一致、且高质量有监督数据稀缺的问题。传统预训练依赖无监督“下一个词预测”目标，而指令微调阶段只能利用少量人工标注的指令-回答对。作者提出 FineInstructions 流水线，将互联网级预训练文档自动转化为**十亿级合成指令-回答对**，使得模型**从预训练开始就完全以指令-回答形式进行有监督学习**，从而：\n\n- 让预训练数据更贴近真实用户提示分布\n- 提升知识吸收效率，减少在噪声文本上的计算浪费\n- 在同等 token 预算下，显著优于标准预训练及现有合成数据方法\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为四类，均围绕“如何在预训练阶段引入更多任务信号或合成数据”展开：\n\n1. 预训练阶段直接生成合成数据\n\n- Gunasekar et al. (2023) “Textbooks are all you need”：用 LLM 生成高质量教科书式文本进行预训练。\n- Ben Allal et al. (2024) Cosmopedia：用模型生成教科书、博文、Wiki 等多种合成文体。\n- Abdin et al. (2024) Phi-4：迭代生成-过滤合成数据，持续提升小规模模型性能。\n- Yang et al. (2025) Synthetic Continued Pre-training：在持续预训练中混入合成领域文本。\n2. 文档→问答/指令转换\n\n- Cheng et al. (2024) Instruction Pre-Training (IPT)：训练一个“文档→指令-回答”合成器，把预训练语料改成问答对。\n- Su et al. (2024) Nemotron-CC：用 LLM 对 CommonCrawl 文档执行重述、摘要、Q&A 等多任务合成。\n- Maini et al. (2024) WRAP：将网页重述为“高质量再版”后再预训练。\n3. 数据重加权/过滤与混合\n\n- Xie et al. (2023) DoReMi、Albalak et al. (2023)、Ye et al. (2025) Data Mixing Laws：在线或离线优化预训练数据混合比例。\n- Penedo et al. (2023, 2024) RefinedWeb/FineWeb：用模型打分过滤低质量网页。\n4. 指令模板与弱监督\n\n- Sanh et al. (2021) P3、Wei et al. (2021) FLAN、Mishra et al. (2022) NaturalInstructions：手工/众包数百至数千模板，做零样本任务泛化。\n- Köksal et al. (2024) LongForm：用“反向指令”配对长文档与合成指令。\n- Ratner et al. (2017) Snorkel：用弱监督将大量无标注文本转化为有标注数据，FineInstructions 将其思想扩展到预训练规模。\n\nFineInstructions 与上述工作的核心区别：\n\n- 模板规模放大到 **~1 800 万**真实用户查询提炼的模板，而非手工数百条；\n- 整个预训练阶段**完全以指令-回答形式**进行，而非仅在下游微调阶段使用指令数据；\n- 通过“高斯池化”局部语义匹配+摘录答案，保证回答**有源可溯**，减少幻觉。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **FineInstructions** 流水线，把互联网级无监督语料自动转换成**十亿级合成指令-回答对**，使预训练全程以“指令→回答”这一监督目标完成，彻底取代传统“下一个词预测”。核心步骤如下：\n\n1. 模板化真实用户查询\n\n- 采集 ≈1 800 万条真实用户提问（WildChat、Reddit QA、GooAQ 等）。\n- 用蒸馏后的 1B 模型将查询泛化为带 `<fi>` 占位符的指令模板，并生成“兼容文档描述”。\n2. 文档-模板精准匹配\n\n- 用 BGE-M3 编码“兼容文档描述”与文档本身，构建 FAISS 索引。\n- 提出 **高斯池化**：对同一文档产出 K=5 个局部嵌入 + 1 个全局嵌入，可检索与不同段落相关的模板，实现**全文覆盖**。\n3. 实例化与答案摘录\n\n- 对每篇文档召回 6 个兼容模板，用 3B 蒸馏模型完成：\n– 把模板变量填成文档中的真实实体/概念；\n– 从文档**原句摘录** ≥80 % 的 token 作为答案，允许少量衔接词，显著降低生成成本与幻觉风险。\n- 引入 `<excerpt>` 标签，仅解码标签与省略号，批量展开原文，减少自回归长度。\n4. 质量评判与过滤\n\n- 用 3.8B 的 Flow Judge 对每条合成样本打分（1–5 Likert），保留 ≥4 分的数据，最终得到 **10 亿+ 高质量指令-回答对**。\n5. 纯指令形式预训练\n\n- 按“token 对 token”原则，用上述数据从零训练 1.8B 模型（同样步数/算力），完全以\n\n```\nInstruction: …\n\nAnswer: …\n```\n\n格式进行监督学习，无需传统自监督 CLM 目标。\n\n通过上述流程，FineInstructions 把预训练语料“改写”成与下游用户提示同分布的大规模监督信号，从而在 MixEval、MT-Bench-101、AlpacaEval 上相对标准预训练取得 **30 %–70 % 相对提升**，并优于 IPT、Nemotron-CC 等同类合成数据方法。\n\nQ4: 论文做了哪些实验？\n\n论文在控制“相同语料、相同 token 预算”的前提下，系统比较了 FineInstructions 与多条基线在不同规模模型上的效果。实验分为**主实验**与**补充实验**两部分：\n\n主实验（1.8 B 参数，单 epoch）\n\n| 数据集 | 方法 | MixEval Acc↑ | MT-Bench-101 ↑ | AlpacaEval Win Rate↑ |\n| --- | --- | --- | --- | --- |\n| IPT 23 B | 标准预训练 | 17.8 / 14.0 | 1.9 | 73.6 % |\n| IPT 23 B | IPT | 19.8 / 16.7 | 2.4 | 68.2 % |\n| IPT 23 B | FineInstructions | 31.7 / 19.2 | 2.8 | – |\n| Nemotron-CC 300 B | 标准预训练 | 24.0 / 17.1 | 3.5 | 63.6 % |\n| Nemotron-CC 300 B | WRAP | 22.8 / 18.4 | 3.6 | 65.1 % |\n| Nemotron-CC 300 B | Q&A-only | 27.1 / 18.9 | 3.4 | 76.1 % |\n| Nemotron-CC 300 B | Nemotron-CC | 24.5 / 16.7 | 3.6 | 65.9 % |\n| Nemotron-CC 300 B | FineInstructions | 33.0 / 21.8 | 3.9 | – |\n\n- **↑** 表示越高越好；MixEval 给出 Standard/Hard 双列结果。\n- FineInstructions 在 **知识型**（MixEval）与**开放型**（AlpacaEval、MT-Bench-101）任务上均显著优于所有基线。\n\n补充实验\n\n1. **评判过滤消融**\n保留 vs 移除 Flow-Judge（≥4 分）过滤：\n– IPT 语料：AlpacaEval 胜率从 46.5 % → 73.6 %\n– Nemotron-CC 语料：AlpacaEval 胜率从 61.4 % → 63.6 %\n证明过滤阶段持续提升质量。\n\n2. **模型规模扩展**（300 M → 1.8 B → 7 B，相同 token 预算）\n\n| 规模 | 方法 | MixEval | MT-Bench-101 | AlpacaEval Win |\n| --- | --- | --- | --- | --- |\n| 300 M | Nemotron-CC | 18.7 | 13.9 | 57.5 % |\n| 300 M | FineInstructions | 31.9 | 22.0 | – |\n| 1.8 B | Nemotron-CC | 24.5 | 16.7 | 65.9 % |\n| 1.8 B | FineInstructions | 33.0 | 21.8 | – |\n| 7 B | Nemotron-CC | 32.7 | 21.5 | 56.6 % |\n| 7 B | FineInstructions | 42.6 | 25.8 | – |\n| 相同参数下全面领先；300 M 的 FineInstructions 模型即可超过 1.8 B 的基线，体现数据效率。 |\n\n3. **多样性分析**\n\n- 在 1.08 B 指令中，共使用 4.3 M 个**唯一模板**，单模板占比 <0.09 %，长尾分布符合幂律\n y = 16,891 · x^(0.24) (R^2=0.96) 。\n- 零样本分类显示：科学 36.6 %、医学 10.4 %、数学 0.58 %、代码 0.25 %、需推理任务 11 % 等，覆盖广泛。\n4. **局部嵌入相关性验证**\n高斯池化 chunk 索引 k 与答案摘录位置 Pearson 相关系数 **0.99**，说明匹配机制精准定位到文档相关片段。\n\n结论性证据\n\n- **token-for-token** 控制下，FineInstructions 在 3 项人工一致性基准上均取得**绝对最高**分数。\n- 数据只需合成一次，后续训练可直接复用，**收敛更快、算力更省**。\n- 小规模模型借助该数据即可达到**大一号参数规模**的基线性能，验证了其“以数据换参数”的潜力。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可继续推进，分为**数据侧、模型侧、评测侧与理论侧**四大类：\n\n数据侧\n\n1. **最优混合比例搜索**\n将模板来源、领域、难度、长度作为可控变量，用 DoReMi/数据混合定律方法搜索使验证集损失最小化的配比，避免真实分布偏差。\n2. **多语言/跨语言扩展**\n现有模板与文档均以英文为主，可采集多语言论坛查询，检验“指令-回答”预训练是否同样提升低资源语言性能。\n3. **多轮对话级联**\n当前仅单轮指令；可把同一文档生成多轮追问-回答链，构建对话式预训练语料，测试是否改善多轮一致性。\n4. **引入工具调用与代码执行**\n对含代码、数学推导的文档，生成“指令→代码片段→执行结果”三元组，使预训练模型自带工具使用信号。\n5. **动态难度课程**\n按模板复杂度（ 个数、推理深度）或 judge 分数做课程学习，观察是否进一步加速收敛或提升推理任务。\n\n模型侧\n\n6. **缩放法则联合拟合**\n同时变化参数规模（0.3 B→13 B）与数据规模（10 B→1 T tokens），拟合  L(N,D)  曲面，看 FineInstructions 是否改变 Chinchilla 最优分配。\n7. **继续预训练 vs 从头训练**\n在已有 LLM 上用 FineInstructions 做继续预训练，比较与“从零开始”哪种更能保持原有能力并提升指令遵循。\n8. **MoE/稀疏模型验证**\n稀疏模型对数据质量更敏感，验证 FineInstructions 能否在同等激活参数下获得更大提升。\n9. **小模型+大推理预算**\n结合 speculative decoding、自洽性投票，检验小参数 FineInstructions 模型能否在推理阶段“跑赢”大参数基线。\n\n评测侧\n\n10. **长尾真实任务 benchmark**\n构建以“推荐、建议、操作步骤、故障排查”为主的用户查询集，弥补现有基准偏重知识问答的空白。\n11. **鲁棒性与安全性探针**\n引入 Prompt-Inject、Instruction-Inversion、Harmful Request 探针，观察纯指令预训练是否意外降低对齐程度。\n12. **校准度与置信度**\n测量模型对答案的置信概率与实际正确率之间的 ECE，检验“摘录为主”是否改善校准。\n\n理论侧\n\n13. **知识吸收效率量化**\n在同等 FLOPs 下，比较标准预训练与 FineInstructions 的“知识探测任务”学习曲线，验证“分布对齐→样本效率提升”的假设。\n14. **表征几何分析**\n用 CKA、Procrustes 距离比较两种预训练得到的表示空间，看指令格式是否诱导更稀疏、更可解释的表征。\n15. **幻觉率系统测量**\n针对同一批文档，让模型闭卷回答，人工标注幻觉比例，量化“80 % 摘录策略”对幻觉的真实抑制幅度。\n\n实施层面\n\n- 开放模板-文档-答案三元组检索接口，允许社区定向抽取特定领域（法律、金融、医学）继续研究。\n- 将 Gaussian Pooling 层实现为即插即用模块，供其他长文档检索任务复用。\n\n这些探索可进一步释放“预训练即指令学习”范式的潜力，并回答其缩放边界、安全边界与最优数据配方。\n\nQ6: 总结一下论文的主要内容\n\n**FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale**\n**一句话总结**：把互联网语料自动变成十亿级“指令-回答”对，让大模型从零开始就用“监督式指令学习”完成预训练，同等算力下全面优于传统自监督。\n\n1\\. 问题\n\n- 传统 LLM 分两阶段：\n① 自监督“下一个词”预训练（海量无标注文本）→ ② 少量指令微调。\n- 阶段①数据噪声大、与下游“回答用户提问”分布不一致，知识吸收效率低；阶段②数据稀缺且易蒸馏失效。\n\n2\\. 方法（FineInstructions 流水线）\n\n1. **模板池**\n采集 ≈1 800 万真实用户查询 → 蒸馏成带 `<fi>` 占位符的通用指令模板与“兼容文档描述”。\n\n2. **文档-模板匹配**\n用 BGE-M3 编码；提出 **高斯池化** 为每篇长文档生成 1 个全局 + 5 个局部嵌入，召回与不同段落相关的模板。\n\n3. **实例化与答案摘录**\n3B 蒸馏模型把模板变量填成文档实体，并**原句摘录 ≥80 %** 内容作为答案，仅用 `<excerpt>` 标签减少解码长度，降低幻觉与成本。\n\n4. **质量过滤**\n3.8 B Flow Judge 打分，保留 ≥4 分样本，最终得到 **10 亿+ 指令-回答对**。\n\n5. **纯指令预训练**\n从零训练 1.8 B 模型，全程格式：\n\n```\nInstruction: …\n\nAnswer: …\n```\n\ntoken-for-token 替换原语料，不再用“下一个词预测”。\n\n3\\. 实验结果（控制相同语料、相同 token 量）\n\n| 基准 | 标准预训练 | 最佳基线 | FineInstructions（提升） |\n| --- | --- | --- | --- |\n| MixEval | 17.8 / 24.0 | 19.8 / 27.1 | 31.7 / 33.0 (+69 % / +39 %) |\n| MT-Bench-101 | 1.9 / 3.5 | 2.4 / 3.6 | 2.8 / 3.9 |\n| AlpacaEval 胜率 | 63–74 % | 65–76 % | 持续优于所有基线 |\n\n- 300 M 模型即可打败 1.8 B 基线，7 B 模型再提 10 个百分点，验证“以数据换参数”。\n\n4\\. 贡献\n\n1. 首个**十亿级真实用户查询模板库**（≈1 800 万）。\n2. 端到端流水线，把原始预训练语料**一次性转化为高质量指令-回答对**。\n3. 证明**预训练全程用监督式指令目标**即可超越传统自监督 + 微调范式，数据与代码全部开源。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是FineInstructions%3A%20Scaling%20Synthetic%20Instructions%20to%20Pre-Training%20Scale，链接是https://arxiv.org/pdf/2601.22146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22146。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2510.23596v3-1769990143839","title":"Think Twice: Branch-and-Rethink Reasoning Reward Model","description":"Large language models (LLMs) increasingly rely on thinking models that externalize intermediate steps and allocate extra test-time compute, with think-twice strategies showing that a deliberate second pass can elicit stronger reasoning. In contrast, most reward models (RMs) still compress many quality dimensions into a single scalar in one shot, a design that induces judgment diffusion: attention spreads across evaluation criteria, yielding diluted focus and shallow analysis. We introduce branch-and-rethink (BR-RM), a two-turn RM that transfers the think-twice principle to reward modeling. Turn 1 performs adaptive branching, selecting a small set of instance-critical dimensions (such as factuality and safety) and sketching concise, evidence-seeking hypotheses. Turn 2 executes branch-conditioned rethinking, a targeted reread that tests those hypotheses and scrutinizes only what matters most. We train with GRPO-style reinforcement learning over structured two-turn traces using a simple binary outcome reward with strict format checks, making the approach compatible with standard RLHF pipelines. By converting all-at-once scoring into focused, second-look reasoning, BR-RM reduces judgment diffusion and improves sensitivity to subtle yet consequential errors while remaining practical and scalable. Experimental results demonstrate that our model achieves state-of-the-art performance on three challenging reward modeling benchmarks across diverse domains.","url":"https://arxiv.org/abs/2510.23596v3","source":"ArXiv Domain","timestamp":"2025-10-27T17:58:07.000Z","metadata":{"arxivId":"2510.23596v3","authors":"Yizhu Jiao, Jiaqi Zeng, Julien Veron Vialard, Oleksii Kuchaiev, Jiawei Han, Olivier Delalleau","categories":"cs.CL","published":"2025-10-27T17:58:07Z","pdfUrl":"https://arxiv.org/pdf/2510.23596v3.pdf","abstractUrl":"https://arxiv.org/abs/2510.23596v3","coolPaperUrl":"https://papers.cool/arxiv/2510.23596v3","rank":8,"domain":"LLM","zh_summary":"大型语言模型（LLMs）越来越依赖于将中间步骤外化并分配额外测试时计算的思维模型，‘二次思考’策略表明，经过深思熟虑的第二遍处理可以激发更强的推理能力。相比之下，大多数奖励模型（RMs）仍然在一次性处理过程中将多个质量维度压缩为单一标量，这种设计会导致判断扩散：注意力分散在各个评估标准上，从而导致关注点稀释和分析浅显。我们提出了分支重思（BR-RM）方法，这是一种将‘二次思考’原则应用于奖励建模的两轮RM。第一轮进行自适应分支，选择一小组实例中关键的维度（如真实性和安全性），并初步勾勒出简明、证据导向的假设。第二轮执行基于分支的重思，即针对性地重新阅读，验证这些假设，并仅审查最重要的内容。我们使用基于GRPO风格的强化学习，对结构化的两轮轨迹进行训练，并采用带有严格格式检查的简单二元结果奖励，使该方法能够兼容标准的RLHF流程。通过将一次性评分转化为集中、二次审视的推理，BR-RM减少了判断扩散，提高了对微妙但重要错误的敏感性，同时保持了实用性和可扩展性。实验结果表明，我们的模型在跨不同领域的三个具有挑战性的奖励建模基准上实现了最先进的性能。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有奖励模型（Reward Model, RM）在“一锤定音”式单步打分过程中出现的 **判断扩散（judgment diffusion）** 问题：\n\n- 单步 RM 被迫同时关注所有质量维度（事实性、安全性、逻辑、风格等），导致注意力被摊薄，难以对关键错误进行深度审查；\n- 结果表现为 **聚焦稀释（focus dilution）** 与 **浅层分析（shallow analysis）**，容易遗漏细微但高代价的事实或逻辑错误，且易被表面风格干扰。\n\n为此，作者将“三思而后行”的推理原则从生成模型迁移到奖励建模，提出 **Branch-and-Rethink 推理奖励模型（BR-RM）**，把一次 holistic 打分转化为 **两阶段显式推理**：\n\n1. **Turn 1 自适应分支**：针对当前实例动态挑选 1–3 个最关键维度，并草拟简洁的“问题假设”；\n2. **Turn 2 条件再思**：以分支结果为条件，仅对这些维度进行靶向重读与验证，输出最终偏好。\n\n通过结构化两回合生成与 GRPO 强化学习训练，BR-RM 在减少判断扩散的同时，提升了对细微错误的敏感度，并在三个挑战性奖励模型基准上取得 SOTA。\n\nQ2: 有哪些相关研究？\n\n论文在 §2 与实验部分系统回顾了三大类密切相关的工作，可归纳如下：\n\n1. 推理型语言模型（Reasoning LMs）\n\n- 链式思维 / 草稿纸 / 树状探索：Nye et al. 2021；Wei et al. 2022；Yao et al. 2023\n- 反思-修正与多轮“想两次”：Shinn et al. 2023；Tian et al. 2025；Snell et al. 2024\n- 数学或代码领域强化推理：DeepSeek-Math/R1（Shao et al. 2024；DeepSeek-AI et al. 2025）\n2. 奖励模型（RM）基础框架\n\n- 标量 RM：Christiano et al. 2017；Stiennon et al. 2020；Ouyang et al. 2022；Bai et al. 2022\n- 生成式 RM（GenRM）：Mahan et al. 2024；Zhang et al. 2025；Ye et al. 2025\n- 推理式 RM（ReasonRM）：Chen et al. 2025（RM-R1）；Guo et al. 2025（Reward Reasoning Model）；Whitehouse et al. 2025（J1）；Saha et al. 2025（EvalPlanner）\n3. 偏好数据与评测基准\n\n- 大规模偏好数据集：HelpSteer3（Wang et al. 2025）、Skywork-Reward（Liu et al. 2024, 2025）、Code-Preference-Pairs（Vezora 2024）、Math-Step-DPO（Lai et al. 2024）\n- 综合评测基准：RewardBench（Lambert et al. 2025）、RM-Bench（Liu et al. 2025）、RMB（Zhou et al. 2025）\n\n这些研究共同构成了 BR-RM 的对比基线与方法论背景。\n\nQ3: 论文如何解决这个问题？\n\n论文将“一锤定音”式单步奖励建模重构为 **两阶段显式推理流程**，通过“先分支-再反思”机制强制模型把有限测试时算力集中到最可能出错的地方，从而抑制判断扩散。具体做法如下：\n\n1\\. 任务重定义\n\n- 不再学习标量函数  R_φ(x,y) ，而是训练策略  π_θ  生成 **两回合结构化推理轨迹**  τ=τ_1circτ_2 ，最终从  τ_2  解析出二元偏好  hat z∈1,2 。\n- 训练数据保持标准 RLHF 形式  mathcal D=(x_i,y_(i,1),y_(i,2),z_i) ，但监督信号仅依赖  hat z  是否等于  z_i 。\n\n2\\. 两阶段生成框架（Branch-and-Rethink）\n\n| 阶段 | 目标 | 关键输出 | 解决痛点 |\n| --- | --- | --- | --- |\n| Turn 1 自适应分支 | 抑制“聚焦稀释” | 从 9 个候选维度  mathcal C  动态挑选  mathcal C_(sel) （1–3 个），并针对每份回复生成“问题草图”  α_j  | 强制模型先判断“该看什么”，而非“什么都看” |\n| Turn 2 条件再思 | 抑制“浅层分析” | 以  τ_1=(mathcal C_(sel),α_1,α_2)  为条件，结合任务层次  mathcal H_(task) （如 Correctness > Process > Presentation）对 flagged 维度做靶向重读，输出深度比较与最终决策  hat z  | 算力只花在已标记的高风险点上，实现“第二次带着问题读” |\n\n3\\. 强化学习训练\n\n- 采用 **GRPO**（Group Relative Policy Optimization）——PPO 的变体，无需价值网络，用组内均值-方差归一化优势。\n- 奖励函数仅两项：\n\nR(τ)=R_(format)(τ)_(格式惩罚)+ w·R_(outcome)(τ)_(二元正确性)\n\n-  R_(format) ：若两回合输出不符合预定义 JSON 结构，一次性罚 −100；\n-  R_(outcome) ：最终  hat z  正确得 0，错误得 −1（权重  w=10 ）。\n- 同一奖励值同时回传给  τ_1  与  τ_2  的所有 token，实现多回合优化而无需修改 RL 基础设施。\n\n4\\. 训练数据与实验验证\n\n- 混合四类专业偏好数据：HelpSteer3（通用）、Skywork（安全）、Code-Preference（代码）、Math-Step-DPO（数学）。\n- 在 RewardBench、RM-Bench、RMB 三大基准上，8B/14B 的 BR-RM 平均准确率分别达 82.6/84.2，超越同规模 Scalar RM、GenRM 与 ReasonRM 基线。\n- 消融实验表明：\n- 去掉 Turn 2 性能平均掉 2–4 点，验证“第二次看”不可或缺；\n- 去掉分支条件后 Turn 2 重新扩散，验证“带着问题看”是关键；\n- 格式惩罚与二元奖励设计最简洁有效，复杂分数或中间奖励反而引入噪声。\n\n通过把“全局一次打分”拆成“先挑重点-再深度验证”，BR-RM 用有限的测试时算力实现了对关键错误的精准捕捉，从而系统性地缓解了判断扩散问题。\n\nQ4: 论文做了哪些实验？\n\n论文在 §4 与附录 E 系统报告了 4 组实验，覆盖主结果、消融、奖励设计、数据贡献与案例可视化，具体列示如下：\n\n1\\. 主实验：三基准全面评测\n\n- **基准**\n- RewardBench（Lambert et al. 2025）\n- RM-Bench（Liu et al. 2025）\n- RMB（Zhou et al. 2025）\n- **对照组**\n- Scalar RM：SteerLM、Eurus、InternLM2、Skywork、Nemotron-4、INF-ORM 等 9 个\n- Generative RM：Claude-3.5、GPT-4o、Gemini-1.5、Skywork-Critic 等 5 个\n- Reasoning RM：JudgeLRM、DeepSeek-GRM、RM-R1 家族、EvalPlanner、J1、RRM-32B 等 10 个\n- **结果**\n- BR-RM-Qwen-14B 在三基准上分别取得 92.1 / 85.9 / 74.7，平均 84.2，**整体 SOTA**；\n- 8B 版本平均 82.6，**超越多数 70B 级基线**，验证两阶段推理的参数效率优势。\n\n2\\. 消融实验（表 2）\n\n| 变体 | 说明 | 平均降幅 |\n| --- | --- | --- |\n| Branching Only | 去掉 Turn 2 | −2.6 (14B) |\n| Unconditioned Rethink | Turn 2 无分支条件 | −1.7 |\n| Single-Turn | 两回合提示合并为一次生成 | −0.9 |\n\n结果量化显示“分支+条件再思”双组件均不可或缺，Turn 2 贡献最大。\n\n3\\. 奖励函数烧蚀（表 3）\n\n| 奖励设计 | 相对平均降幅 | 主要问题 |\n| --- | --- | --- |\n| 去掉格式惩罚 | −0.9 | 模型跳过推理结构 |\n| 改为 −3…+3 连续分数 | −1.7 | 训练-评测目标错位 |\n| 给 Branch 单独中间奖励 | −4.3 | 信号稀疏、训练不稳定 |\n\n验证“二元正确性+强格式约束”最简洁有效。\n\n4\\. 训练数据源烧蚀（表 4）\n\n| 移除数据集 | 主要影响 | RMB 降幅 |\n| --- | --- | --- |\n| HelpSteer3 | 通用事实性下降 | −4.4 |\n| Skywork | 安全维度减弱 | −5.0 |\n| Math-Step-DPO | 推理错误敏感度降低 | −2.1 |\n| Code-Preference | 代码风格-正确性区分减弱 | −0.7 |\n\n表明领域专用数据对对应维度至关重要，通用数据提供广度。\n\n5\\. 判断扩散初步研究（附录 A / 图 3）\n\n- 与 RRM-32B 在 RM-Bench 四个子集各 100 例对比**token 维度分布**。\n- 基线模型把 20–30 % 生成摊到“写作清晰度”等非关键维度；BR-RM 把 ≥ 70 % 算力集中到任务关键维度（如代码任务的 Implementation+Precision）。\n- 量化验证了“分支”机制确实实现实例级自适应聚焦。\n\n6\\. 案例研究（附录 F / 图 6）\n\n- C++ 函数 `unique_digits` 实现质量对比：两回复均无致命 bug，但一份含可运行 main 函数。\n- Turn 1 选中 Implementation / Precision / Instruction 三维；Turn 2 用层次 Correctness>Process>Presentation 给出细微排序，成功区分“可用”与“更优”版本，展示框架对 RL 细粒度奖励信号的适用性。\n\n综上，实验从宏观性能、微观烧蚀到可视化分析形成完整证据链，证明“分支-再思”结构在参数规模、训练稳定性与评测鲁棒性上均优于现有单步或单回合奖励模型。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“工具增强”“动态策略”“理论分析”与“系统扩展”四条线，供后续研究参考：\n\n1\\. 工具增强：把“再思”从文本扩展到可执行验证\n\n- **检索-增强事实核查**：Turn 2 调用搜索引擎或知识库 API，对 Turn 1 标记的“可疑事实”进行外部取证，减少幻觉漏检。\n- **代码解释器集成**：对标记为 Computation Precision 的片段即时运行单元测试，用运行时错误信号作为额外奖励通道，实现“推理-执行”闭环。\n- **多模态证据**：若输入含图像/表格，可引入视觉模型对图表一致性进行交叉验证，提升 STEM 与医学领域可靠性。\n\n2\\. 动态策略：让“是否再思、再思多深”成为策略的一部分\n\n- **自适应停止准则**：用模型不确定性（predictive entropy 或 token-level confidence）作为门控，对易样本跳过 Turn 2，对难样本展开三、四回合迭代，实现“算力-精度”在线权衡。\n- **可学习的分支空间**：目前  mathcal C  为人工定义的 9 维，可让模型在 meta-prompt 层自行提出新维度或组合子维度，形成任务专用的“评价词汇表”。\n- **层次化再思**：对极难样本引入“树状再思”，即 Turn 2 可生成多条相互竞争的深度分析路径，再用投票/排序机制得出最终偏好，借鉴 Tree-of-Thoughts 思路。\n\n3\\. 理论分析：理解“分支-再思”为何有效\n\n- **注意力熵度量**：量化 Turn 1 前后注意力分布的熵值变化，验证“分支”是否确实降低维度间注意力耦合度。\n- **错误发现率（FDR）框架**：将 BR-RM 视为多重假设检验过程，分析在固定假阳性率下，两阶段方法相比单步对 subtle error 的检验功效（power）。\n- **Scaling Law 实验**：固定训练算力，变化 Turn 2 的生成长度与采样数，拟合“测试时算力-性能”曲线，验证推理时扩展是否遵循与预训练不同的指数律。\n\n4\\. 系统扩展：把两阶段评判嵌入更大对齐管线\n\n- **在线 RL 循环**：将 BR-RM 作为实时奖励信号源，用于 PPO/DPO 迭代训练策略模型，观察是否减少传统 RM 的“奖励黑客”现象，并监测训练稳定性。\n- **多 RM 集成**：训练多个专业化 BR-RM（分别偏向安全、代码、数学），用门控网络或加权集成输出最终奖励，实现“专家评委”协同。\n- **人类-模型协同标注**：用 Turn 1 生成的问题草图辅助人类标注员快速定位需要仔细审查的片段，降低人工标注成本并提升一致性。\n- **可解释性接口**：将  τ_1  的维度选择与  τ_2  的对比推理可视化，作为产品级“评分说明书”，增强用户对模型判决的信任与调试能力。\n\n5\\. 风险与价值对齐：再思过程本身的安全\n\n- **恶意再思**：Turn 2 可能因强化学习奖励而故意夸大微小缺陷以高判负分，需研究奖励攻击与防御。\n- **价值观漂移**：当分支空间由模型自动生成时，需监控是否隐含不当维度（如歧视性标准），引入宪法 AI 或规则过滤器进行约束。\n\n综上，BR-RM 把“评判”转化为显式推理任务，为后续引入外部工具、自适应算力分配与可解释对齐提供了天然接口；上述任一方向均可单独成文或组合推进。\n\nQ6: 总结一下论文的主要内容\n\n**论文核心贡献一句话**：\n将“三思而后行”的推理机制从生成模型迁移到奖励模型，用**两阶段显式推理**取代单步 holistic 打分，显著缓解**判断扩散**，在三大基准上取得新 SOTA。\n\n1\\. 问题\n\n- 现有 RM 一次性输出标量，注意力摊薄→**判断扩散**（漏掉 subtle error、易受风格干扰）。\n- 即使 ReasonRM 也多为单回合、固定维度，无法**实例级聚焦**。\n\n2\\. 方法：Branch-and-Rethink Reward Model（BR-RM）\n\n| 阶段 | 目标 | 关键输出 |\n| --- | --- | --- |\n| Turn 1 自适应分支 | 挑重点 | 从 9 维选 1–3 个关键维度  mathcal C_(sel)  + 问题草图  α_j  |\n| Turn 2 条件再思 | 深度审 | 以  τ_1  为条件，只审 flagged 维度，输出最终偏好  hat z∈1,2  |\n\n- 训练：GRPO 强化学习，**二元正确性+格式惩罚**即可稳定收敛。\n- 推理：固定算力预算内，70 % token 集中在实例关键维度。\n\n3\\. 实验结果\n\n- **RewardBench / RM-Bench / RMB**\nBR-RM-Qwen-14B：92.1 / 85.9 / 74.7，**平均 84.2 整体 SOTA**；8B 版本亦超越多数 70B 基线。\n- 消融：去掉 Turn 2 掉 2–4 点；去掉分支条件掉 1–3 点；验证“分支+再思”缺一不可。\n- 奖励烧蚀：格式惩罚与二元奖励最简洁；连续分数或中间奖励均引入噪声。\n- 数据烧蚀：HelpSteer3 保广度，Skywork 保安全，Math-Step 保推理，各贡献 2–5 点。\n- 可视化：token 分布显示 BR-RM 把 ≥70 % 算力集中到任务关键维度，基线仅 20–30 %。\n\n4\\. 意义\n\n- **同一“think twice”原则**既提升 solver 也提升 judge，可无缝接入现有 RLHF 管线。\n- 为后续引入**外部工具、自适应算力、可解释接口**提供自然框架。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Think%20Twice%3A%20Branch-and-Rethink%20Reasoning%20Reward%20Model，链接是https://arxiv.org/pdf/2510.23596，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23596。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2509.21670v4-1769990143839","title":"MORPH: PDE Foundation Models with Arbitrary Data Modality","description":"We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters, MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.","url":"https://arxiv.org/abs/2509.21670v4","source":"ArXiv Domain","timestamp":"2025-09-25T22:38:36.000Z","metadata":{"arxivId":"2509.21670v4","authors":"Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Alexander Scheinker, Diane Oyen, Nathan Debardeleben, Earl Lawrence, Ayan Biswas","categories":"cs.CV, cs.AI, cs.LG, physics.comp-ph","published":"2025-09-25T22:38:36Z","pdfUrl":"https://arxiv.org/pdf/2509.21670v4.pdf","abstractUrl":"https://arxiv.org/abs/2509.21670v4","coolPaperUrl":"https://papers.cool/arxiv/2509.21670v4","rank":9,"domain":"LLM","zh_summary":"我们介绍了 MORPH，一种适用于偏微分方程（PDE）的模态无关、自回归的基础模型。MORPH 基于卷积视觉变换器（Convolutional Vision Transformer）骨干构建，能够无缝处理不同数据模态（1D--3D）和不同分辨率的异构时空数据集，以及包含混合标量和向量分量的多个场。该架构结合了：(i) 分量卷积，可联合处理标量和向量通道以捕捉局部交互，(ii) 跨场注意力（inter-field cross-attention），能够对不同物理场之间的信息进行建模并选择性传播，(iii) 轴向注意力（axial attentions），沿单独的空间和时间轴分解全时空自注意力，从而在保留表达能力的同时减少计算负担。我们在多样化的异构 PDE 数据集上对多种模型变体进行预训练，并评估其在一系列下游预测任务上的迁移能力。通过全模型微调和参数高效的低秩适配器，MORPH 的表现优于从零训练的模型。在广泛评估中，MORPH 与强基线以及近期最先进模型不相上下甚至表现更好。总体而言，这些能力为从科学观测的异构和多模态特性中学习提供了灵活且强大的骨干，为可扩展且数据高效的科学机器学习开辟了道路。源代码、数据集和模型可在 https://github.com/lanl/MORPH 公共访问。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**科学机器学习（SciML）中构建通用偏微分方程（PDE）基础模型**所面临的两大核心瓶颈：\n\n1. **数据异构性**\n现有PDE数据集在空间维度（1D–3D）、分辨率、标量/矢量场混合、物理场数量等方面差异巨大，且规模可达TB级。传统方法需对数据进行填充或重采样以统一形状，导致计算与存储开销爆炸，且难以纳入3D或高分辨率2D数据（自注意力复杂度 O(L^2) 随序列长度 L 二次增长）。\n\n2. **部分可观测性**\n实验或仿真往往只能提供稀疏、低维的观测（如1D探针、2D切片），而真实物理过程是3+1维连续时空演化。现有基础模型多假设2D笛卡尔网格、固定场类型，无法直接利用1D或3D数据，造成信息浪费与泛化盲区。\n\n为此，作者提出**MORPH**——一个**形状无关（shape-agnostic）**、自回归的PDE基础模型，其目标是在**不重新设计架构**的前提下，统一处理任意维度、任意分辨率、混合标量/矢量场的多物理PDE数据，实现：\n\n- **预训练阶段**：从六类异构数据（1D可压缩NS、2D不可压缩NS、2D反应-扩散、2D浅水、3D可压缩NS、3D磁流体）中学习通用演化算子。\n- **下游任务**：通过全参数微调或LoRA高效适配，在七项新任务（1D Burgers、1D反应-扩散、2D Kolmogorov流、3D湍流冷却等）上零样本或少量样本即可超越从头训练的专用模型。\n\n总结：MORPH试图用**单一架构**解决“**如何在部分、异构、多模态的PDE数据上预训练，并低成本迁移到任意下游物理问题**”这一SciML基础模型核心挑战。\n\nQ2: 有哪些相关研究？\n\n论文第2节“Related Work”将相关研究划分为两大主线，并指出它们与MORPH的差异。可归纳为以下四类：\n\n1. 专用PDE代理模型（Standalone Surrogates）\n\n- 数据驱动：3D-CNN、ConvLSTM、DCGAN、GNN\n- 物理嵌入：PINNs、神经算子（DeepONet、FNO、WNO、LNO、PINO）\n- 生成式：GAN、扩散模型（score-based）、潜空间演化（VAE+Transformer/LSTM/DeepONet）\n共同点：针对**单一PDE族**重新训练，无法跨物理迁移。\n2. 早期迁移/多任务尝试\n\n- 深度迁移算子学习（Deep Transfer Operator Learning）\n- 上下文算子学习（In-context Operator Learning）\n局限：仅在**同维度、同网格**任务间迁移，未解决异构数据。\n3. 近期PDE基础模型（Foundation Models）\n\n| 模型 | 可处理维度 | 预训练数据 | 关键限制 |\n| --- | --- | --- | --- |\n| MPP | 仅2D | 2D多物理 | 固定2D网格，无法1D/3D |\n| DPOT | 仅2D | 2D+时序10步 | 需10步上下文，不支持3D |\n| POSEIDON | 仅2D | 2D多尺度 | 需Swin窗口，形状固定 |\n| PDEformer | 仅1D | 1D方程 | 无法处理2D/3D |\n| 共同瓶颈： |\n\n- 维度、分辨率、标量/矢量场**需重新设计架构**；\n- 3D体积数据导致序列长度爆炸，被显式排除在预训练之外；\n- 1D实验信号（探针、地震道）被强制填充到2D，浪费算力。\n4. 与MORPH最相关的“异构”讨论\n\n- Ye et al. 2024、McCabe et al. 2024 均指出“**扩展到3D及混合维度**”是开放难题；\n- MORPH首次把**1D–3D、任意分辨率、混合场**统一进**同一套权重**，并通过轴向注意力将复杂度从 O((TDHW)^2) 降至 O(T^2+D^2+H^2+W^2) ，填补了这一空白。\n\n简言之，既有研究要么“专模专用”，要么“二维打天下”，而MORPH首次系统性地把**形状无关、多维度、多物理、部分可观测**作为第一优先级设计目标。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“一格式、三机制、四变体”的系统设计，把异构、部分可观测的 PDE 数据塞进同一个自回归 Vision Transformer，实现“任何维度、任何分辨率、任何场类型”端到端预训练与低成本迁移。具体方案如下：\n\n1\\. 统一格式：UPTF-7\n\n- 语义保留的 7D 张量模板\n (N,T,F,C,D,H,W) \n-  F ：物理场数目（可运行时变化）\n-  C ：场分量数（标量=1，矢量≥1；混合时广播到最大  C ）\n- 零填充仅发生在**GPU 端 mini-batch 加载瞬间**，原始 TB 级数据仍按原生格式存盘，避免存储爆炸。\n- 结果：1D 探针、2D 切片、3D 体数据可同批训练，无需改图、改代码或重采样。\n\n2\\. 三大架构机制\n\n| 机制 | 作用 | 实现要点 | 复杂度/优势 |\n| --- | --- | --- | --- |\n| Component-wise 3D 卷积 | 局部空间关联+标/矢混合 | 只在  C  维滑动，输出  F  张特征图； 1×1×1  升维→ 3×3×3  深度卷积 | 参数随  F  线性增长，诱导局部偏置，提升小样本效率 |\n| Inter-field Cross-Attention | 跨物理场信息筛选与融合 | 单查询向量  q∈R^(E)  对  F  个场做 MH-CrossAttn，输出单一融合场 |  O(F)  替代  O(F^2) ，抑制尺度失配带来的伪激活；运行时场数可变 |\n| 4D 轴向注意力 | 全局时空依赖，内存可控 | 将完整自注意力拆成四支独立 1D-MHA：时间、深度、高度、宽度；残差相加 | 复杂度从  O((TDHW)^2)  降到  Ol(T^2+D^2+H^2+W^2r) ，支持 3D 高分辨批量训练 |\n\n3\\. 参数高效迁移：LoRA\n\n- 仅对 Attention 与 MLP 的稠密矩阵引入低秩分解\n W'=W_0+(α) / (r)BA,;A∈R^(r× k),B∈R^(d× r) \n- MORPH-L（480 M）中仅 77 M 可训练，即可匹配全参数微调效果，首次把 LLM 的“冻结主干+小插件”范式搬到 PDE 基础模型。\n\n4\\. 四尺度模型变体\n\n| 模型 | 参数量 | 注意力维度 | 头数 | 深度 | 适用场景 |\n| --- | --- | --- | --- | --- | --- |\n| TI (Tiny) | 7 M | 256 | 4 | 4 | 单卡/快速实验 |\n| S (Small) | 30 M | 512 | 8 | 4 | 单卡/生产 |\n| M (Medium) | 126 M | 768 | 12 | 8 | 多卡 |\n| L (Large) | 480 M | 1024 | 16 | 16 | 多节点+LoRA |\n\n5\\. 训练与数据策略\n\n- **平衡任务采样**：按轨迹数  N_i  反比加权，避免小数据集被遗忘。\n- **ReVIN 归一化**：预计算全局均值/方差，训练与推理阶段可逆，消除分布漂移。\n- **流式+自定义分片**：六数据集并行 DataLoader，DDP 下按样本级取模分片，支持 TB 级数据外存训练。\n- **AR(1) 自回归**：严格遵循初值问题语义，减少曝光偏差；也可扩展为 AR(p) 或变阶。\n\n6\\. 效果验证\n\n- **零样本**：在预训练集上直接推理，平均误差优于或接近专用模型。\n- **全样本微调**：7 个下游任务全部刷新 SOTA（NRMSE/VRMSE）。\n- **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 256 条轨迹（<1 %）即可打败全量训练的独立模型。\n- **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调 200 epoch，MSE/RMSE 低于 122 M 的 DPOT-FM-M 与 158 M 的 POSEIDON-FM-B。\n\n综上，论文用“UPTF-7 统一表示 + 卷积-交叉-轴向三重高效算子 + LoRA 插件式迁移”组合拳，把以往只能分而治之的 1D/2D/3D、标量/矢量、多物理问题纳入**同一套权重**，在数据与算力双重稀缺场景下仍能实现跨物理、跨维度、跨分辨率的强泛化，从而系统性地解决了 PDE 基础模型的异构与部分可观测难题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“**异构预训练 → 零样本/少样本迁移 → 数据与算力稀缺场景**”设计了四层实验，覆盖 13 个数据集、4 个模型尺度、3 类对照组。核心结果汇总如下（行内公式统一用  ...  格式）。\n\n1\\. 预训练（Pretraining）\n\n**目的**：验证 MORPH 能否在 6 类异构数据上同时收敛且不遗忘。\n\n| 数据集 | 维度 | 场类型 | 轨迹数 | 时间步 | 空间分辨率 |\n| --- | --- | --- | --- | --- | --- |\n| 1D-CFD | 1D | 1×矢量+2×标量 | 10 k | 100 | 1024 |\n| 2D-DR | 2D | 2×标量 | 1 k | 100 |  128^2  |\n| 2D-CFD-IC | 2D | 1×矢量 | 80 | 1000 |  512^2  |\n| 2D-SW | 2D | 1×标量 | 1 k | 100 |  128^2  |\n| 3D-CFD | 3D | 1×矢量+2×标量 | 200 | 21 |  128^3  |\n| 3D-MHD | 3D | 2×矢量+1×标量 | 97 | 100 |  64^3  |\n\n- **训练步数**：TI/S 200k，M 100k，L 125k 梯度步。\n- **采样策略**：按轨迹数  N_i  反比加权，3D-MHD 被采样概率 0.31，1D-CFD 0.08。\n- **监控指标**：训练/验证 MSE，无早期停止（loss 仍在下降）。\n\n2\\. 零样本评估（Zero-shot）\n\n**设置**：直接用预训练权重在预训练集上做单步预测，不更新参数。\n\n| 指标 | 1D-CFD | 2D-DR | 2D-CFD-IC | 2D-SW | 3D-CFD | 3D-MHD |\n| --- | --- | --- | --- | --- | --- | --- |\n| NRMSE ↓  | 0.0506 | 0.1112 | 0.0858 | 0.0045 | 0.1161 | – |\n| VRMSE ↓  | – | – | – | – | – | 0.285 |\n\n- MORPH-FM-S 在 4/6 数据集上**优于或接近**专用 standalone 模型；\n- 首次展示 1D→3D 跨维度零样本推理能力（对比基线只能 2D）。\n\n3\\. 下游微调（Full-shot Fine-tuning）\n\n**7 个新物理任务**，100–150 epoch，全参数或 LoRA。\n\n| 数据集 | 维度 | 任务特色 | 最优结果 |\n| --- | --- | --- | --- |\n| 1D-DR | 1D | 反应-扩散前沿 | NRMSE= 1.25×10^(-3) （TI） |\n| 1D-BE | 1D | Burgers 激波 | NRMSE=0.0302（S） |\n| 2D-CFD | 2D | 可压缩 NS  512^2  | NRMSE=0.0423（L-LoRA） |\n| 2D-GSDR | 2D | Gray-Scott 斑图 | VRMSE=0.00725（S） |\n| 2D-FNS-KF | 2D | Kolmogorov 强迫湍流 | MSE=0.00089（S） |\n| 3D-CFD-Turb | 3D | 湍流  64^3  | NRMSE=0.0713（M） |\n| 3D-TGC | 3D | 自引力冷却 | VRMSE=0.0395（L-LoRA） |\n\n- **全部 7 项**均取得**新 SOTA**（对比 FNO、U-Net、MPP、DPOT、POSEIDON）。\n- LoRA 仅调 77 M 参数即超越全参数 M 模型，验证参数高效迁移。\n\n4\\. 数据与算力稀缺实验\n\n4.1 少数据\n\n- **1D-DR**：用 25 % 轨迹微调 100 epoch，RMSE 已**低于**全量训练的 standalone-TI。\n- **2D-FNS-KF**：用 256 条轨迹（<1 %）微调 100 epoch，RMSE 同样**低于**全量 standalone-S。\n\n4.2 少算力\n\n- 固定 128 条轨迹、200 epoch，对比同等微调预算下的 SOTA 基础模型：\n\n| 模型 | 参数量 | MSE | RMSE |\n| --- | --- | --- | --- |\n| DPOT-FM-M | 122 M | 0.0301 | 0.176 |\n| POSEIDON-FM-B | 158 M | 0.0017 | 0.0412 |\n| MORPH-FM-S | 30 M | 0.00162 | 0.0401 |\n\n- **4× 更轻参数**仍取得最低误差，证明在**数据+算力双稀缺**场景优势最大。\n\n5\\. 长时程自回归 rollout\n\n- **Shallow-Water 10 步**：MORPH-SS-S NRMSE 比 TI 低 28 %，无发散。\n- **FNS-KF 10 步**：MORPH-FM-S 每步 MSE 均低于 FM-TI，误差累积缓慢。\n\n6\\. 消融与系统实验\n\n- \\*\\* patch size\\*\\*：8×8×8 在 3D 显存与精度间最佳；降至 4×4×4 序列长度翻倍，训练时间↑2.3×，收益<2 %。\n- **轴向 vs 全时空注意力**：3D-CFD batch 下显存节省 5.7×，速度↑3.2×，NRMSE 差距<1 %。\n- **平衡采样 vs 均匀采样**：无加权时 3D-MHD NRMSE 恶化 18 %，证实反比加权必要性。\n\n7\\. 可重复性\n\n- 代码、权重、训练日志已开源（GitHub + HuggingFace）。\n- 提供 PyTorch 数据加载、ReVIN 统计、LoRA 配置、rollout 脚本，确保指标可复现。\n\n综上，实验从**预训练收敛 → 零样本泛化 → 全量微调 SOTA → 数据/算力稀缺优势 → 长时稳定性**五个层面，系统验证了 MORPH 在异构 PDE 数据上的通用性与实用性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 MORPH 框架的“直接延伸”或“范式升级”，均围绕其**形状无关、部分可观测、参数高效**三大特性展开，具有明确的研究价值与可行性。\n\n1\\. 维度与几何外推\n\n- **非笛卡尔网格**：曲线坐标、 immersed boundary、 unstructured tet-grid。\n→ 把轴向注意力泛化到“图-轴向混合”：先按节点局部连通做图注意力，再沿用户自定义的“几何轴”做轴向注意力，保持  O(N)  复杂度。\n- **4D+ 时空-参数空间**：将 PDE 参数  (μ)  视为第 5 维，实现“一推理直接扫参数平面”，迈向**超参数快速寻优**。\n\n2\\. 物理一致性层\n\n- **守恒-辛-对称嵌入**：在轴向注意力后接入**显式守恒投影层**（ rho,v,E  Moment Constraint）或**辛子网络**，确保 rollout 全程质量、动量、能量误差有界。\n- **可微分边界条件**：把 BC 编码为可学习的软约束项  L_(BC) = ∫_(∂Omega) |u_θ(x,t) - g(x,t)|^2 ，随注意力反向传播，实现“零样本”边界迁移。\n\n3\\. 多保真与实验-仿真融合\n\n- **保真度轴 (Fidelity Axis)**：在 UPTF-7 再增一维  M （Mesh-level 或 Noise-level），同批次混入低分辨/含噪实验信号与高分辨仿真，用**轴向注意力天然的分支结构**做保真度-自适应加权。\n- **物理-数据双驱动微调**：当实验只测到 1D 探针时，把仿真 3D 场作为 teacher，通过**轴向知识蒸馏**让模型在探针位置输出与实验一致，同时保持 3D 场预测合理。\n\n4\\. 参数高效范式升级\n\n- **LoRA 组合路由**：\n- 冻结共享主干，为每个物理系**自动学习路由权重**  w_i  组合多个 LoRA 插件：\n\nDelta W = ∑_(i=1)^k w_i (α_i) / (r_i) B_i A_i\n\n- 实现“一个主干 + 数十个插件”覆盖全物理库，插件总大小 <10 % 主干。\n- **渐进式扩展**：\n- 先训练 7 M 模型 → 用 function-preserving 放大到 30 M → 126 M → 480 M，对比“直接训练大模型”在数据-算力稀缺时的样本效率提升。\n\n5\\. 长时程与混沌行为\n\n- **Koopman 轴向注意力**：在时间轴引入**线性 Koopman 层**  z_(t+1) = K z_t ，再残差接非线性轴向注意力，缓解多步 rollout 的 Lyapunov 漂移。\n- **自回归策略搜索**：目前固定 AR(1)。可探索**可变阶调度**  p_t sim π_θ(error_t) ，用强化学习最小化累积误差，实现“自适应步长”。\n\n6\\. 逆向与不确定性\n\n- **Bayes-LoRA**：把 LoRA 矩阵  A,B  设为**随机变量**，采用 SVGD 或 NF 推断后验，实现**轻量级不确定性量化**；仅需  2r(d+k)  个采样参数而非全模型。\n- **稀疏观测反演**：利用形状无关编码器，把稀疏探针数据直接映射到潜码，再解码完整 3D 场，实现**实验-在线重构**。\n\n7\\. 跨模态与多物理新基准\n\n- **实验-仿真混合基准**：发布一套同时包含\n- 1D 探针（压力-时序）\n- 2D PIV 切片\n- 3D LES 仿真\n的公开数据集，推动社区研究“部分可观测”设定。\n- **多物理耦合新任务**：\n- 辐射-磁流体 (radiation-MHD)\n- 流-固-热耦合\n- 等离子体-壁面相互作用\n验证 MORPH 在“真实多物理”下的零样本能力。\n\n8\\. 系统与硬件协同\n\n- **轴向注意力核融合**：将四维轴向 MHA 写成单一 CUDA kernel，利用共享内存缓存  D,H,W  切片，实测可再提速 1.8–2.2×。\n- **边缘部署**：把 TI 模型 (<30 MB) 编译到 TensorRT-Lite，在 NVIDIA Jetson 上实现**毫秒级 128³ 3D 场在线预测**，服务实验闭环控制。\n\n9\\. 理论基础\n\n- **形状无关通用逼近定理**：证明对任意  d∈1,2,3  与任意可测域  Omega⊂R^d ，MORPH 编码器-解码器对  M_θ: L^2(Omega)to L^2(Omega)  在恰当 Sobolev 范数下稠密。\n- **轴向注意力复杂度-精度权衡**：给出  O(T^2+D^2+H^2+W^2)  与 full  O((TDHW)^2)  的谱范数误差上界，指导用户按硬件自动选择分支数。\n\n10\\. 伦理与双用\n\n- **湍流武器化风险**：建立“物理模型权重出口”审查清单，对高保真 3D 可压缩 NS 权重加**水印+差分隐私**，防止直接用于超高速武器设计。\n\n简言之，MORPH 把“维度墙”和“数据墙”打通后，下一步可在**几何、物理、不确定性、系统、理论**五个层面继续深挖，形成“**形状无关+物理严格+参数插件+边缘实时**”的下一代科学基础模型生态。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **MORPH**——首个**形状无关**的偏微分方程（PDE）基础模型，用统一架构端到端处理 **1D–3D、任意分辨率、混合标量/矢量场**的异构科学数据，并在**零样本、全样本、数据-算力稀缺**场景下均取得 SOTA 或可比性能。核心内容可概括为 **“一格式、三机制、四变体、五验证”**：\n\n1️⃣ 一格式：UPTF-7\n\n- 7D 张量  (N,T,F,C,D,H,W)  把任意 PDE 数据语义化封装，**磁盘保持原生格式**，GPU 端按需零填充，解决 TB 级存储与维度差异。\n\n2️⃣ 三机制\n\n| 机制 | 作用 | 复杂度 |\n| --- | --- | --- |\n| Component-wise 3D 卷积 | 联合提取标/矢量局部特征 |  O(F)  |\n| Inter-field Cross-Attention | 跨物理场选择性融合为单一场 |  O(F)  |\n| 4D 轴向注意力 | 时间+空间三轴分别 1D-MHA，全局感受野 |  O(T^2+D^2+H^2+W^2)  |\n\n3️⃣ 四变体\n\n| 模型 | 参数量 | 适用 |\n| --- | --- | --- |\n| TI | 7 M | 单卡快速实验 |\n| S | 30 M | 生产级 |\n| M | 126 M | 多卡 |\n| L | 480 M | 多节点+LoRA |\n\n4️⃣ 五验证\n\n1. **异构预训练**：6 数据集（1D–3D）同时收敛，无灾难遗忘。\n2. **零样本推理**：在预训练集直接推理，**4/6 任务**优于专用模型。\n3. **下游微调**：7 项新物理任务**全部刷新 SOTA**（NRMSE/VRMSE）。\n4. **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 <1 % 数据即**超越全量训练模型**。\n5. **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调下，MSE/RMSE **低于 122 M 的 DPOT 与 158 M 的 POSEIDON**。\n\n5️⃣ 贡献一句话\n\nMORPH 用**统一架构**打通 1D–3D、任意分辨率、多物理场的壁垒，首次在**部分可观测、TB 级异构数据**上实现**低成本预训练+参数高效迁移**，为科学机器学习提供**通用、轻量、强泛化**的 PDE 基础模型基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MORPH%3A%20Shape-agnostic%20PDE%20Foundation%20Models，链接是https://arxiv.org/pdf/2509.21670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.21670。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22141v1-1769990143839","title":"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data","description":"In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.","url":"https://arxiv.org/abs/2601.22141v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:56:41.000Z","metadata":{"arxivId":"2601.22141v1","authors":"Grzegorz Stefanski, Alberto Presta, Michal Byra","categories":"cs.AI, cs.CV, cs.LG","published":"2026-01-29T18:56:41Z","pdfUrl":"https://arxiv.org/pdf/2601.22141v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22141v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22141v1","rank":10,"domain":"LLM","zh_summary":"在剪枝中，彩票假说（Lottery Ticket Hypothesis）提出，大型网络包含稀疏子网络，或称“中奖彩票”，可以独立训练以匹配其密集网络的性能。然而，大多数现有方法假设存在一个适用于所有输入的通用中奖彩票，忽略了真实数据的固有异质性。在本工作中，我们提出了“路由彩票（Routing the Lottery, RTL）”，一种自适应剪枝框架，可发现多个专门化子网络，称为自适应彩票，每个彩票针对一个类别、语义聚类或环境条件进行定制。在多样化的数据集和任务中，RTL 在平衡准确率和召回率上持续优于单模态和多模态基线，并且使用的参数数量比独立模型少多达 10 倍，同时表现出语义对齐。此外，我们识别了子网络崩塌（subnetwork collapse），即在激进剪枝下性能下降，并提出了子网络相似性评分，可实现无标签的过度稀疏诊断。总体而言，我们的结果将剪枝重新定义为一种将模型结构与数据异质性对齐的机制，为更加模块化和上下文感知的深度学习铺平了道路。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对“彩票假设（Lottery Ticket Hypothesis, LTH）”在真实世界数据上的局限：现有剪枝方法几乎都为所有输入共享同一张全局稀疏掩码，忽视数据本身的异质性。为此，作者提出 Routing the Lottery（RTL）框架，核心目标可归纳为：\n\n- **突破单一全局掩码**：不再寻找“一张通用彩票”，而是为不同类别、语义簇或环境条件发现多张**专用子网络**（adaptive tickets）。\n- **用剪枝实现动态特化**：仅通过剪枝、不引入额外路由网络或参数，即可让不同数据子集激活不同稀疏路径，实现结构级、上下文感知的推理。\n- **兼顾性能与效率**：在参数规模比独立模型少一个数量级的前提下，取得优于单掩码及多模型基线的平衡准确率与召回率。\n- **提供无标签诊断工具**：揭示“子网络塌陷”现象，并引入掩码相似度指标，可在无标签条件下预判过度稀疏化导致的性能骤降。\n\n简言之，论文将剪枝从“静态压缩”重塑为“对齐数据异构性的动态机制”，为构建模块化、可解释且轻量的深度学习模型提供新途径。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统回顾了与“ Routing the Lottery（RTL）”密切相关的四条研究脉络，可概括为：\n\n- **经典与最新剪枝**\n- 早期基于敏感度：Optimal Brain Damage / Surgeon（LeCun et al. 1989；Hassibi et al. 1993）\n- 非结构化剪枝：Han et al. 2015；Yang et al. 2017, 2018（能耗感知）\n- 正则化与稀疏训练：Sreenivasan et al. 2022；Tartaglione et al. 2020, 2022；Benbaki et al. 2023\n- 理论极限：Zhang et al. 2024（权重幅值+网络锐度决定剪枝极限）\n- 专用场景：LLM 剪枝（Frantar & Alistarh 2023；Sun et al. 2023；Lu et al. 2024；Wei et al. 2024）、CNN 剪枝（Zhao et al. 2023）、脉冲网络（Shi et al. 2024）\n- **彩票假设（LTH）及其扩展**\n- 原始 IMP：Frankle & Carbin 2019\n- 初始化/重绕：Frankle et al. 2021；Renda et al. 2020\n- 理论分析：Tartaglione 2022；Burkholz et al. 2021；Sakamoto & Sato 2022；Paul et al. 2022\n- 算法加速：Wang et al. 2023；Lin et al. 2023\n→ **共同点**：均寻找**单张全局掩码**，未考虑数据异构性。\n- **动态稀疏训练**\nSET（Mocanu et al. 2017）、SNFS（Dettmers & Zettlemoyer 2020）、RigL（Evci et al. 2020）在训练过程中**重新生长**连接，但仍维护**单个子网络**，未显式特化不同数据子集。\n\n- **条件计算与混合专家（MoE）**\n- 稀疏门控 MoE（Shazeer et al. 2017；Fedus et al. 2022）\n- 条件卷积（Yang et al. 2019）\n→ 依赖**额外路由网络**与大量辅助参数，计算开销高；RTL 与之正交：仅用剪枝+掩码路由，无需新增参数或复杂门控。\n\n综上，现有工作要么固守“单掩码”假设，要么引入繁重路由机制；RTL 首次将“多专子网络”理念引入纯剪枝框架，填补了“数据异构-结构适配”这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“数据异构→单一掩码性能受限”这一核心瓶颈转化为\\*\\*“联合发现多张专用稀疏子网络”\\*\\*的优化问题，并给出完整实现流程。关键步骤与对应设计如下：\n\n1\\. 问题重定义：从单掩码到掩码集合\n\n- 给定数据集  D  被划分为  K  个子集  D_k_(k=1)^K （类别、语义簇或环境）。\n- 目标：联合学习  K  个二元掩码  m_k ，使得\n\nmin_(m_k) ∑_(k=1)^K mathbb E_((x,y)sim D_k)[ell(y,f(x;m_kodotθ))] quads.t.quad |m_k|_0le s\n\n即**每个子集  D_k  拥有自己稀疏度上限  s  的专用子网络**，而所有子网络共享同一初始化参数  θ 。\n\n2\\. 自适应“彩票”提取（Adaptive Ticket Extraction）\n\n- **逐簇迭代 magnitude pruning**（算法 1）：\n1. 对子集  d_k  训练  T  步得到临时权重  θ_T^((k)) ；\n2. 按幅度剪去比例  p  的权重，生成  m_k ；\n3. 权重**回滚**到共享初始  θ_0 ，保留掩码；\n4. 循环直到所有  m_k  达到目标稀疏度  s 。\n- **效果**：每张“彩票”都在同一初始化基础上、仅针对对应数据分布被“刮开”，天然具备**参数共享+结构特化**双重属性。\n\n3\\. 联合重训练（Joint Retraining）\n\n- **同步小批量采样**：对大小不同的  D_k  进行循环补全，保证每 epoch 各子网络梯度更新次数相同。\n- **掩码梯度屏蔽**（公式 (7)）：\n\nθ arrow θ - eta · (∇_θ L(f(x;m_kodotθ),y)odot m_k)\n\n仅更新  m_k  保留的权重，避免子网络间互相覆盖或灾难性遗忘。\n- **结果**：在**单一张密集参数张量  θ ** 上同时精炼  K  条稀疏路径，实现**推理时按上下文（标签/环境）直接选掩码**，无需额外路由网络。\n\n4\\. 无标签诊断：子网络塌陷预警\n\n- 定义掩码 Jaccard 相似度\n\nJ(m_i,m_j)=(|m_i∩ m_j|) / (|m_i∪ m_j|)\n\n- 实验发现：当  J  随剪枝率急剧上升时，各子网络开始共享大量权重→**结构特化消失→性能骤降**（称之“子网络塌陷”）。\n- **用途**：无需标签即可通过监控  J  判断是否过度稀疏，指导早停或调整稀疏预算。\n\n5\\. 端到端总结\n\n1. **划分数据**→2. **逐簇剪+回滚**得  K  张掩码→3. **联合重训练**共享权重→4. **推理时按上下文选掩码**→5. **用掩码相似度监控塌陷**。\n\n通过上述流程，RTL 仅利用**剪枝+掩码路由**便让不同输入走不同稀疏路径，实现“结构即路由”的轻量级异构推理，在参数量少一个数量级的情况下持续超越单掩码/独立模型基线。\n\nQ4: 论文做了哪些实验？\n\n论文从“可控分类→语义聚类→单图语义→真实语音→诊断分析”五个层次递进验证 Routing the Lottery（RTL）的通用性与可解释性。具体实验设置与目的如下：\n\n1 CIFAR-10 类别专用子网络（理想划分）\n\n- **目的**：验证“每类一张彩票”在干净标签下的上限收益。\n- **协议**：10 类各对应 1 个子网络；与 (i) 单全局 IMP 掩码 (ii) 每类独立 IMP（无共享）对比；稀疏度 25 % / 50 % / 75 %。\n- **观测指标**：balanced accuracy / precision / recall / 剩余参数量。\n- **关键结论**：RTL 在所有稀疏度均取得最高 balanced accuracy 与 recall，参数仅为独立模型的 ≈ 1/10。\n\n2 CIFAR-100 语义聚类（噪声划分）\n\n- **目的**：测试 RTL 对“不完美、重叠”数据划分的鲁棒性。\n- **协议**：用 CLIP+UMAP+HDBSCAN 将 100 类聚成 8 个语义簇；其余同 Exp-1。\n- **结论**：即使簇边界模糊，RTL 仍全程领先；召回率优势尤为显著，说明特化对噪声划分仍有效。\n\n3 隐式神经表示（INR）——**单图内部语义区域**\n\n- **目的**：验证 RTL 可细化到“**一张图像内的不同语义区域**”。\n- **任务**：用坐标 MLP 将连续 (x,y) 映射到 RGB，重建 10 张 ADE20K 图像。\n- **划分**：利用语义分割掩码，每幅图按“对象类别”定义 K 个子集（4–15 类）。\n- **指标**：平均 PSNR；参数计数。\n- **结论**：25 %/50 %/75 % 稀疏下 RTL 分别领先单掩码 IMP ≈ +2.9 dB / +2.5 dB / +2.2 dB；可视化显示边界更清晰、颜色更稳定。\n\n4 真实环境语音增强（DNS + TAU 场景）\n\n- **目的**：检验 RTL 在非分类、真实噪声环境下的通用性。\n- **数据**：干净语音（DNS Challenge 2020）× 三种环境噪声（室内、户外、交通，TAU 2020）。\n- **划分**：按噪声场景定义 3 个子集；U-Net 型复数比值掩码网络。\n- **指标**：SI-SNRi；参数量。\n- **结论**：RTL 在所有稀疏度均获最高 SI-SNRi（25 % 时 +0.36 dB 以上），参数仅 32 K，远低于独立模型的 84 K；谱图可视化显示谐波结构保留更好。\n\n5 诊断性分析：子网络塌陷 & 语义对齐\n\n5.1 子网络塌陷\n\n- **方法**：跟踪 CIFAR-10/100 各子网络 balanced accuracy 与平均掩码 Jaccard 相似度随剪枝率变化。\n- **发现**：accuracy 骤降点与 Jaccard 急剧上升点高度重合→“结构重叠”可作为**无标签早停信号**。\n\n5.2 语义对齐\n\n- **方法**：计算 CIFAR-10 类间 WordNet 路径相似度，与深层/浅层掩码相似度做 Spearman 相关。\n- **发现**：\n– 浅层掩码普遍相似（共享低层特征），深层掩码相似度与语义距离正相关；\n– 相关强度随训练进程与稀疏度增加而提升→RTL 剪出的结构**自发编码语义层级**。\n\n总结\n\n实验覆盖**理想标签→模糊聚类→图像内部→真实声学→无标签诊断**，任务模态包括分类、坐标回归、语音增强；指标兼顾精度、参数量、可视化与相关性分析，系统证明 RTL 在性能、效率与可解释性上的优势。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 RTL 框架的自然延伸或深层追问，均直接对应论文已暴露的边界与现象，具备可验证的实验入口：\n\n1 动态路由 vs. 静态掩码\n\n- **问题**：RTL 目前依赖“上下文标签”做硬掩码选择，若环境或类别未知即失效。\n- **探索**：在共享 backbone 上训练一个**无参数开销的轻量门控**（如一次前向后的激活统计），实现**输入驱动的软路由**，比较与固定掩码的精度-延迟折中。\n\n2 稀疏度自动分配\n\n- **问题**：各子网络现用统一稀疏预算  s  ，但不同簇/环境可能天生复杂度不同。\n- **探索**：以掩码相似度上升斜率或验证损失为信号，**逐簇自适应调整目标稀疏度**；可建模为约束优化或强化学习，验证能否在总参数不变下进一步提升平均精度。\n\n3 跨任务迁移与增量簇\n\n- **问题**：现实场景常出现新类别/新噪声环境。\n- **探索**：\n– 保持旧掩码冻结，用**侧向添加的稀疏 Adapter** 学习新簇，检验是否出现“稀疏灾难性遗忘”；\n– 或采用**超网络生成掩码**，以簇嵌入为条件，实现连续增量学习而无需重训练全部子网络。\n\n4 子网络 collapse 的理论阈值\n\n- **问题**：实验观察到 Jaccard 拐点 ≈ 0.6 时性能骤降，但该值是否随架构、任务、优化器而变？\n- **探索**：在随机图与稀疏神经网络理论框架下，推导**保持子网络可区分性的最大允许重叠**；给出与层宽、深度、剪枝因子相关的解析或半解析阈值，为自动早停提供理论依据。\n\n5 与参数高效微调（PEFT）正交组合\n\n- **问题**：大模型时代更关注“如何少调参”。\n- **探索**：将 RTL 的掩码特化思想迁移到 LLM，**仅对每层部分神经元做簇专用剪枝**，其余权重保持预训练不变；对比 LoRA/AdaLoRA，看能否在下游多任务上同时获得稀疏与低秩的复合收益。\n\n6 非语义划分下的鲁棒性极限\n\n- **问题**：目前聚类仍部分依赖语义标签或 CLIP 嵌入。\n- **探索**：用**完全随机或对抗性划分**制造最难场景，测量 RTL 相对单掩码的增益衰减曲线；界定“数据划分质量→RTL 有效性”的临界条件，指导实际部署时的划分策略选择。\n\n7 硬件-感知掩码同步\n\n- **问题**：不同子网络导致不规则稀疏，难以在通用加速器上获得实测加速。\n- **探索**：在掩码提取阶段加入**结构化约束**（block/block-column 稀疏或 N:M 细粒度），检验是否仍能保持 specialization 优势；配合真实 GPU/NNIE 推理库测量端到端延迟与能耗，验证“理论参数少”→“实测速度快”的闭环。\n\n8 语义对齐的逆向利用\n\n- **问题**：深层掩码相似度与 WordNet 距离正相关。\n- **探索**：**反过来用掩码相似度指导零样本/少样本分类**——未见类别的掩码可通过语义近邻插值生成，检验其作为权重先验能否提升低样本场景下的泛化能力。\n\n这些方向既可直接复用 RTL 的代码框架，也能从理论、系统、应用三端填补“稀疏特化”尚存的空白，为后续研究提供可落地的实验入口。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：传统 Lottery Ticket 剪枝只用一张全局稀疏掩码，忽视数据异构，导致在类别、语义或环境差异大的场景下性能受限。\n- **方法**：提出 Routing the Lottery（RTL）——\n1. 将数据划分为 K 个子集（类别/聚类/环境）；\n2. 对每子集独立 magnitude-prune 并回滚到共享初始化，得到 K 张专用掩码；\n3. 联合重训练：在统一参数张量上按掩码屏蔽梯度，仅更新各子网络存活权重；\n4. 推理时按上下文选择对应掩码，实现“结构即路由”的轻量化推断。\n- **发现**：\n– 在 CIFAR-10/100、ADE20K-INR、真实语音增强任务中，RTL 以≈1/10 参数量持续优于单掩码与独立模型；\n– 提出“子网络塌陷”现象：掩码 Jaccard 相似度陡升可作为无标签早停信号；\n– 深层掩码相似度与 WordNet 语义距离正相关，表明剪枝自发编码语义层级。\n- **结论**：RTL 把剪枝从静态压缩重塑为对齐数据异构的动态机制，为构建模块化、可解释且高效的深度模型提供了新路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Routing%20the%20Lottery%3A%20Adaptive%20Subnetworks%20for%20Heterogeneous%20Data，链接是https://arxiv.org/pdf/2601.22141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22141。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22159v1-1769990225065","title":"RedSage: A Cybersecurity Generalist LLM","description":"Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.","url":"https://arxiv.org/abs/2601.22159v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:57.000Z","metadata":{"arxivId":"2601.22159v1","authors":"Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani","categories":"cs.CR, cs.AI, cs.CL","published":"2026-01-29T18:59:57Z","pdfUrl":"https://arxiv.org/pdf/2601.22159v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22159v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22159v1","rank":1,"domain":"Agent","zh_summary":"网络安全运营需要支持多样化工作流程且不暴露敏感数据的辅助大型语言模型。现有解决方案要么依赖存在隐私风险的专有API，要么依赖缺乏域名适配的开放模型。为弥合这一差距，我们通过大规模网络过滤和手工收集高质量资源，策划了118亿个以网络安全为重点的持续预训练数据标记，涵盖2.86万份文档，涵盖框架、攻击性技术和安全工具。基于此，我们设计了一个代理增强流程，模拟专家工作流程，生成26.6万多回合网络安全样本，用于监督式微调。结合通用开源的大型语言模型数据，这些资源使得RedSage的培训成为可能，RedSage是一款开源、可本地部署的网络安全助手，具备域感知的预培训和后培训功能。为了严格评估这些模型，我们推出了RedSage-Bench，这是一个基准测试，包含3万条选择题和240条开放式问答题，涵盖网络安全知识、技能和工具专长。RedSage还会根据已建立的网络安全基准（如CTI-Bench、CyberMetric、SECURE）和通用大型语言模型基准进行进一步评估，以评估更广泛的概括性。在8B尺度上，RedSage持续取得更优成绩，在网络安全基准测试中领先基线模型最高+5.59分，在开放大型语言模型排行榜任务中高出+5.05分。这些发现表明，领域感知型代理增强以及前后培训不仅能提升网络安全专长，还有助于提升整体推理能力和指令遵循能力。所有模型、数据集和代码均公开。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. **现有网络安全大模型的训练与部署缺陷**\n\n- 专有模型依赖云端 API，存在敏感数据外泄风险；\n- 开源模型缺乏针对网络安全场景的充分预训练与后训练，导致领域知识薄弱；\n- 已有工作往往只关注单一训练阶段（仅预训练或仅监督微调），且公开数据规模有限，难以支撑本地化部署的通用型安全助手。\n2. **网络安全评测基准的覆盖不足**\n\n- 主流基准侧重知识问答，缺少对**工具操作能力**与**开放式问答质量**的系统性评估；\n- 缺乏同时覆盖知识、技能、工具三维能力的大规模高质量评测集。\n3. **数据与 pipeline 的封闭性**\n\n- 先前研究多未公开训练数据与代码，阻碍社区复现与进一步研究。\n\n为此，作者提出 **RedSage**：\n\n- 构建 **11.8 B token** 的网络安全持续预训练语料（CyberFineWeb + RedSage-Seed）；\n- 设计 **agentic 增强 pipeline**，将 28 K 权威文档转化为 **266 K 轮次对话**用于监督微调；\n- 推出 **RedSage-Bench**（30 K 多选题 + 240 开放式问答），系统评估知识、技能与工具 proficiency；\n- 训练得到 **8 B 参数可本地部署**的开源模型，在网络安全与通用基准上均取得 **+5 分以上** 的提升，并全面公开数据、模型与代码。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统梳理了相关研究，可归纳为两条主线：**网络安全评测基准**与**网络安全领域大模型/数据集**。\n\n1. 网络安全评测基准\n\n- **知识型基准**\n- SecEval（Li et al., 2023）：2 K 多选题，覆盖 9 类安全主题。\n- CyberMetric（Tihanyi et al., 2024）：10 K 多选题，基于 RAG 与专家验证。\n- CyberBench（Liu et al., 2024）：除问答外还包含 NER、摘要、分类任务。\n- SECURE（Bhusal et al., 2024）：面向工控系统，聚焦风险推理与漏洞分析。\n- CS-Eval（Yu et al., 2024）：42 子类，分知识-能力-应用三级认知维度。\n- SecBench（Jing et al., 2025）：44 K 多选题 + 3 K 简答题，中英双语。\n- **应用与智能体基准**\n- CTI-Bench（Alam et al., 2024）：威胁情报任务，含 CVE↔CWE 映射、CVSS 预测、攻击技战术识别。\n- CyberSecEval（Wan et al., 2024）：8 类风险场景，如漏洞利用生成、提示注入。\n- NYU-CTF（Shao et al., 2024）、CyBench（Zhang et al., 2025）：交互式夺旗赛，评估逆向、Web 利用等红队能力。\n- **共性局限**：多数基准仅覆盖知识或应用，**缺乏对安全工具操作与开放式问答质量的细粒度评估**。\n2. 网络安全领域大模型与数据集\n\n- **早期编码器模型**\n- CyBERT、SecureBERT、CTI-BERT：基于 BERT 的域内微调，任务特定，数据未公开。\n- **大模型时代的持续预训练/微调**\n- PRIMUS（Yu et al., 2025）：2.75 B token 预训练 + 835 条 SFT，基于 Llama-3.1-8B。\n- Foundation-Sec-8B（Kassianik et al., 2025）：5.1 B token 预训练，数据未开源。\n- DeepHat（2025）：宣称 1 M+ Q&A 微调，无预训练阶段。\n- Lily-Cybersecurity-7B（Sego Lily Labs, 2024）：22 K 手工对话微调。\n- Cyber-DAP（Salahuddin et al., 2025）：119 M token 小规模持续预训练。\n- SecGemini：谷歌闭源模型，实时威胁情报，数据与权重均未发布。\n- **共性局限**：\n- 仅聚焦单一训练阶段（预训练或 SFT），**未联合优化**；\n- 数据规模或质量有限，**无 agentic 增强**；\n- **未同时公开模型、数据与代码**，可复现性差。\n\nRedSage 与上述工作的区别\n\n- 首次将**大规模持续预训练（11.7 B token）+ 代理增强 SFT（266 K 对话）+ 偏好对齐**完整 pipeline 开源；\n- 提出同时覆盖**知识、技能、工具**且包含**开放式问答质量评分**的 RedSage-Bench；\n- 8 B 规模即可本地部署，在网络安全与通用基准上均取得新 SOTA。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“数据-训练-评测”三位一体的系统化方案，一次性补齐了领域预训练、高质量后训练与严格评测的缺口，具体路径如下：\n\n1\\. 构建 11.8 B token 网络安全持续预训练语料\n\n| 子语料 | 来源与处理 | 规模 | 关键特点 |\n| --- | --- | --- | --- |\n| CyberFineWeb | 在 FineWeb（17 T token）上用 ModernBERT 分类器过滤→全球 MinHash 去重→按时间片混合 30 % FineWeb-Edu 防止灾难性遗忘 | 13 M 文档 / 11.7 B token | 覆盖 2013-2024 公开 Web 安全内容，保持通用知识 |\n| RedSage-Seed | 人工精选 28 637 篇权威资源（MITRE、OWASP、Kali、CTF Write-up、tldr 等）→ Markdown 结构化 | 150 M token | 高质量、可溯源、三分类：Knowledge / Skills / Tools |\n| RedSage-Dump | 补充 NVD、RFC、NIST、安全新闻、教育门户等公开数据 | 700 M token | 增强漏洞、协议、合规等长尾知识 |\n\n2\\. Agentic 增强生成 266 K 多轮对话用于后训练\n\n- **Planner Agent**：对 Seed 数据分块→自动抽取技能集（如“漏洞分析”“命令生成”）并输出多样化增强策略（Q&A、角色扮演、步骤演练等）。\n- **Augmenter Agent**：按策略将每块种子实例化为**角色驱动、多轮、可执行**的对话，严格保持技术深度与事实一致。\n- **质量控制**：格式、一致性、相关性三重过滤；最终得到 266 K 对话（353 M token），样本扩大 9.2×，token 扩大 2.3×。\n\n3\\. 两阶段后训练\n\n1. **SFT**：RedSage-Conv + 通用指令集 SmolTalk2（非推理子集）→兼顾网络安全专精与广泛指令遵循。\n2. **DPO**：采用开源 Tulu3 偏好数据做直接偏好优化，提升回答有用性与安全性。\n\n4\\. 建立 RedSage-Bench 全面评测\n\n| 维度 | 题型 | 规模 | 构建方式 | 质量保障 |\n| --- | --- | --- | --- | --- |\n| 知识 | MCQ | 30 K | 用强 LLM 对 Seed 生成→双阶段 LLM 验证（结构+评分）→人工抽查 | 得分>8 且类别均衡 |\n| 技能 | Open-QA | 240 | Agentic 计划→QA 生成→双 LLM 交叉验证→人工终审 | 0-10 质量评分+事实正确性 |\n| 工具 | —— | —— | 同上 | 同上 |\n\n5\\. 训练与评测结果\n\n- **基座**：Qwen3-8B → 持续预训练 → RedSage-8B-Base\n- **指令版**：SFT → RedSage-8B-Ins；再加 DPO → RedSage-8B-DPO\n\n**效果**（8 B 模型）\n\n- 网络安全基准平均 +5.59（最高 84.6 % vs 79.0 %）\n- Open LLM Leaderboard 平均 +5.05（74.3 % vs 69.3 %）\n- 32 B 规模轻量 QLoRA 实验亦获一致提升，验证 pipeline 可扩展。\n\n6\\. 开源与可复现\n\n- 全部语料、对话、Benchmark、训练配置（Axolotl YAML）、评测脚本（LightEval）一次性公开；\n- 提供去污染与伦理使用协议，支持本地 GPU 部署，实现“隐私-安全-可控”的工业级安全助手。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“模型性能”** 与 **“方法可复现/可扩展”** 两条主线，共执行了 4 组实验，覆盖 20+ 公开基准、3 类模型规模、累计 50 余万个测试样本。\n\n1\\. RedSage-Bench 自评实验\n\n**目的**：验证领域定制预训练 + 代理增强 SFT 的绝对增益。\n**任务与指标**\n\n- MCQ（30 k题，0-shot）：宏观准确率 + 知识/技能/工具 三类细粒度准确率。\n- Open-QA（240题，0-shot）：LLM-as-Judge 打分（0–10）+ 事实正确率（T/F）。\n\n**对照组**\n\n- 通用基线：Llama-3.1-8B、Qwen3-8B\n- 安全专用基线：Foundation-Sec-8B、Llama-Primus、DeepHat-V1-7B 等\n\n**主要结果**\n\n- RedSage-8B-Ins 在 MCQ 宏观准确率上比最强 8 B 通用模型 Qwen3-8B 提升 **+3.88 pp**；DPO 版本保持 **+2.98 pp**。\n- Open-QA 平均正确率领先次优模型 **+7 pp**，质量分领先 **+0.07**（6.50 vs 6.43）。\n- 工具类题目最难（中位数质量分最低），RedSage 仍保持显著优势。\n\n2\\. 公开网络安全基准外推实验\n\n**目的**：检验领域知识能否泛化到外部评测。\n**基准列表（7 个）**\nCyberMetric-500、SecBench-En、MMLU-CSec、SECURE（MEAT/CWET/KCV）、CTI-Bench（MCQ+RCM）、SecEval\n\n**结果（表 5）**\n\n- 5-shot 基座阶段：RedSage-8B-Base 平均 **84.6 %**（+3.75 pp > Qwen3-8B-Base）。\n- 0-shot 指令阶段：RedSage-8B-Ins **81.3 %**（+5.59 pp > Qwen3-8B），接近 32 B 模型（82.3 %），距离 GPT-5 差 5 pp。\n\n3\\. 通用大模型 Leaderboard 实验\n\n**目的**：验证“领域强化”是否损害通用能力。\n**基准（7 项）**\nARC-C、HellaSwag、TruthfulQA、MMLU、WinoGrande、GSM8K、IFEval\n\n**结果（表 6）**\n\n- RedSage-8B-DPO 平均 **74.3 %**，领先所有 8 B 对照（最高基线 69.3 %），并超过 Qwen3-32B（73.2 %）。\n- GSM8K 数学推理 **86.1 %**（+8.4 pp > 最强 8 B 基线），说明 Seed 数据中的 CTF/Write-up 逻辑链同样提升通用推理。\n\n4\\. 规模扩展与消融实验\n\n4.1 32 B 轻量微调（QLoRA）\n\n- 仅用 RedSage-Seed + 50 % RedSage-Conv，1 % 可训练参数，4 小时训练。\n- RedSage-32B-LoRA-Ins 在 RedSage-MCQ 达 **87.5 %**（+2.0 pp > 全量 8 B 版本），外部安全基准平均 **82.9 %**，验证 pipeline 随模型容量线性受益。\n\n4.2 消融：预训练语料贡献\n\n- **CyberFineWeb-only** vs **Seed-only** vs **CFW+Seed**\n- Seed-only 在需要权威知识的 CTI-RCM、MMLU-CSec 上最高；\n- CFW-only 在 SecBench、CyberMetric 等 Web 知识型基准上最高；\n- 二者组合取得最佳宏观平均，证明互补性。\n\n4.3 消融：后训练策略\n\n- SFT → DPO 连续提升 IFEval（指令遵循）（79.9 → 83.4 pp），但 MCQ 准确率略降（85.7 → 84.8），显示偏好对齐以牺牲少量闭卷正确率为代价换取开放式可用性。\n\n实验总结\n\n- 在 20+ 基准、50 万级样本上，RedSage 8 B 模型**同时**实现：\n- 网络安全任务 **SOTA**（+5.6 pp）\n- 通用任务 **领先同规模模型**（+5.0 pp）\n- 代理增强数据 + 持续预训练的策略可**零修改**迁移到 32 B 模型，继续提升性能，证明方法的可扩展性与复现价值。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**数据、模型、评测、应用与安全**五大主题，并给出可落地的研究切入点。\n\n1\\. 数据层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 多语言/地域威胁情报 | 将 CyberFineWeb 过滤 pipeline 扩展到 Common Crawl 其他语言子集，构建「中文-阿拉伯语-俄语」平行安全语料。 | 捕捉非英语地下论坛/漏洞交易信息，提升全球威胁可见度。 |\n| 多模态安全数据 | 把 CVE 公告中的 PoC 视频、攻击流程图、恶意软件截图与文本对齐，形成图文交错预训练语料。 | 支持「图-文」混合问答，例如“根据这张 Wireshark 截图指出攻击阶段”。 |\n| 实时威胁增量更新 | 设计「时间窗口」持续预训练策略，每周注入新 CVE、新 ATT&CK Technique，避免模型“过期”。 | 解决当前静态语料无法反映 0-day 爆发的问题。 |\n\n2\\. 模型层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 工具调用能力 | 引入 ToolLLM 式 API 模板，把 Kali 工具封装成可调用函数，训练模型生成「工具链+参数」并实时执行。 | 实现真正的“红队助手”——输入目标 IP，输出 nmap → nuclei → metasploit 自动链。 |\n| 推理-行动循环 | 结合 ReAct / Reflexion 框架，让模型在 CTF 环境中自我验证 flag 结果，根据失败反馈迭代 payload。 | 突破当前仅文本生成、无环境反馈的局限，迈向自主渗透代理。 |\n| 小参数高效化 | 用 1-3 B 模型 + 动态 LoRA 合并，实现「手机端本地安全助手」；研究量化后是否仍能保持 exploit 语法精度。 | 满足现场应急、无网络场景下的便携需求。 |\n\n3\\. 评测层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 动态 CTF 基准 | 与 CTFd 平台对接，每月自动拉取新比赛题目，形成「活基准」；指标从准确率改为夺旗数/用时。 | 防止静态题目过拟合，提供可无限扩展的攻防评测。 |\n| 红-蓝对抗评分 | 建立「双模型对抗」协议：RedSage-Red vs RedSage-Blue，用 MITRE CALDERA 记录 ATT&CK 覆盖度，计算累积 TTP 成功数。 | 量化模型在真实杀伤链中的攻防贡献度。 |\n| 幻觉与一致性审计 | 引入「事实性链式追踪」数据集：对模型输出的每一条命令/漏洞描述给出可执行验证脚本，统计可复现率。 | 解决当前 LLM 安全建议“看似合理却不可执行”的痛点。 |\n\n4\\. 应用层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| SOAR 编排自动生成 | 将 SIEM 告警作为输入，模型输出 Splunk Phantom / Microsoft Sentinel Playbook JSON，直接导入执行。 | 缩短 SOC 平均响应时间（MTTR）。 |\n| 漏洞补丁联合生成 | 给定 CVE 描述 + 受影响函数，模型同时输出（1）补丁 diff（2）回归测试用例（3）CI 集成 YAML。 | 实现「一键修复」DevSecOps 工作流。 |\n| 安全培训个性化 | 利用对话历史评估学员薄弱点，动态生成「专属靶场」Docker Compose 场景与提示脚本。 | 提升企业安全培训效率，降低人力成本。 |\n\n5\\. 安全与伦理层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 可控生成护栏 | 在模型内部集成「双重控制码」：当请求包含 exploit、RAT、后门等关键字时，自动切换到「只给出防御/检测建议」模式。 | 降低双重用途风险，满足欧盟 AI Act 等监管要求。 |\n| 训练数据毒化检测 | 研究若攻击者在 Common Crawl 植入恶意网页并被 CyberFineWeb 吸收，如何触发异常行为；构建毒化样本检测器。 | 防止“上游投毒”导致模型输出有害 payload。 |\n| 可解释威胁推理 | 要求模型在给出攻击建议时，同步输出对应的 MITRE ATT&CK Technique ID 与置信度，并提供引用段落。 | 方便审计与责任追溯，增强安全员信任度。 |\n\n快速起步建议（3 个月可完成）\n\n1. 用现有 RedSage-8B-DPO + ToolLLM 模板，收集 5 K 条「工具调用-执行结果」对话，微调出 RedSage-Tools 原型。\n2. 在小型 CTF 平台（如 PicoMini）运行自动夺旗实验，记录 flag 率并与人类选手基线对比。\n3. 将动态评测结果开源，形成「RedSage-LiveBench」月度榜单，吸引社区提交新模型与新题。\n\n通过以上探索，可逐步把 RedSage 从“知识问答”升级为「可解释、可执行、可自我进化」的下一代网络安全智能体。\n\nQ6: 总结一下论文的主要内容\n\nRedSage：开源、本地化、8 B 参数的网络安全通用大模型\n——数据-训练-评测全流程总结\n\n1\\. 要解决的问题\n\n- 专有安全 LLM 依赖云端 API，敏感数据外泄风险高；\n- 开源模型缺乏大规模**域内预训练**+**高质量后训练**，安全知识不足；\n- 现有基准缺**工具操作**与**开放问答质量**维度，且数据/代码多未公开。\n\n2\\. 核心贡献\n\n| 内容 | 规模 | 亮点 |\n| --- | --- | --- |\n| CyberFineWeb | 11.7 B token | 2013-24 Common Crawl → ModernBERT 过滤 → 全局去重 → 30 % 通用知识回放 |\n| RedSage-Seed | 28 k 权威文档 / 150 M token | MITRE、OWASP、Kali、CTF Write-up 等，Markdown 结构化 |\n| Agentic 增强对话 | 266 k 多轮 / 353 M token | Planner+Augmenter 双智能体，把种子转为可执行、角色驱动对话 |\n| RedSage-Bench | 30 k MCQ + 240 开放问答 | 覆盖知识-技能-工具；双阶段 LLM 验证+人工终审，去污染 |\n| RedSage-8B 模型 | 8 B 参数 | Qwen3-Base → 持续预训练 → SFT → DPO；完全开源 |\n\n3\\. 实验结果（↑ vs 最强同规模模型）\n\n- **网络安全基准**平均 **+5.6 pp**（84.6 % vs 79.0 %）\n- **通用 LLM Leaderboard**平均 **+5.0 pp**（74.3 % vs 69.3 %）\n- **开放问答**正确率 **+7 pp**，质量分 **+0.07**\n- **32 B 轻量 QLoRA**继续提升，验证 pipeline 可扩展\n\n4\\. 可用性与影响\n\n- 8 B 规模 → 单卡 24 GB GPU 可部署，**本地私有**运行\n- 代码+数据+Benchmark 全部开放，支持复现与二次开发\n- 首次实现“**大规模域内预训练 + 代理增强后训练 + 三维评测**”一体化，为社区提供可落地的网络安全通用助手基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RedSage%3A%20A%20Cybersecurity%20Generalist%20LLM，链接是https://arxiv.org/pdf/2601.22159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22159。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22156v1-1769990225065","title":"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts","description":"Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data","url":"https://arxiv.org/abs/2601.22156v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:53.000Z","metadata":{"arxivId":"2601.22156v1","authors":"Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu","categories":"cs.CL, cs.AI, cs.LG","published":"2026-01-29T18:59:53Z","pdfUrl":"https://arxiv.org/pdf/2601.22156v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22156v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22156v1","rank":2,"domain":"Agent","zh_summary":"混合 Transformer 架构结合了 softmax 注意力模块和循环神经网络（RNN），在长上下文建模中表现出理想的性能与吞吐量折衷，但由于从零进行大规模预训练的成本过高，其应用和研究受到限制。一些近期研究表明，预训练的 softmax 注意力模块可以通过参数传递和知识蒸馏转换为 RNN 模块。然而，这些转换方法需要大量训练数据（超过 100 亿个 token），且得到的混合模型在长上下文下的表现也较差，而混合模型在这种场景下本可以比基于 Transformer 的模型获得显著的推理加速。在本文中，我们提出了 HALO（通过层优化实现的混合注意力），这是一种将 Transformer 模型蒸馏为 RNN-注意力混合模型的流程。随后，我们提出了 HypeNet，一种通过新颖的位置编码方案（称为 HyPE）和各种架构改进实现出色长度泛化能力的混合架构。我们使用 HALO 将 Qwen3 系列转换为 HypeNet，实现了与原始 Transformer 模型相当的性能，同时在长上下文性能和效率上表现出优势。该转换仅需 23 亿个 token，不到其预训练数据的 0.01%。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. 现有混合 Transformer-RNN 架构的“高门槛”预训练成本\n混合模型（交错 softmax attention 与线性 RNN 层）在长上下文场景下兼具性能与吞吐优势，但通常需从头预训练数百亿级 tokens，对学术团队极不友好。\n\n2. 已有蒸馏方案的两大瓶颈\n\n- 数据量依旧巨大：先前将预训练 Transformer 蒸馏为混合架构的方法仍需 ≥10 B tokens。\n- 长上下文 recall 严重退化：蒸馏后的混合模型在 128 K 以上长度任务上性能骤降，丧失混合架构本应具备的推理效率优势。\n3. 目标\n提出一种**极低成本**（<3 B tokens）的跨架构蒸馏流程，使预训练 Transformer 可直接转换为**长上下文表现更强、推理更高效**的混合模型，而无需从头预训练。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为三类，均围绕“如何把预训练 Transformer 转化为线性复杂度的序列建模结构”展开：\n\n1. 混合架构（Hybrid Attention-RNN）\n\n- Jamba (Lieber et al., 2024)\n- MiniMax-01 (MiniMax et al., 2025)\n- Qwen3-Next (Qwen, 2025)\n- Kimi-Linear (Kimi et al., 2025)\n- NVIDIA Nemotron 3 (NVIDIA et al., 2025)\n共同点：从头预训练百亿级参数，attention 层与 RNN/SSM 层交错；未提供低成本转换方案。\n2. 蒸馏式“Attention → Hybrid”方法\n\n| 方法 | 发布 | 训练 tokens | 关键指标 |\n| --- | --- | --- | --- |\n| Mamba-in-the-Llama (Wang et al., 2025b) | 2024-08 | 20 B | 均匀替换 attention 层，长程 recall 下降明显 |\n| SMART (Yang et al., 2026) | 2025-05 | >7 B | 基于输出分布漂移挑选保留层 |\n| RAD (Hoshino et al., 2025) | 2025-05 | 20 B | 提出冗余度度量，仍需大量数据 |\n| Jet-Nemotron (Gu et al., 2025) | 2025-08 | 400 B | 任务性能下降指导层选择，成本最高 |\n| KL-LS (Li et al., 2025) | 2025-12 | 25 B | 逐层重跑蒸馏，用 KL 散度定重要性 |\n\n3. 纯 RNN/线性注意力蒸馏\n\n- RADLADS (Goldstein et al., 2025) — 本文 Stage-1 的对齐策略即在其基础上改进。\n- LoLCATs (Zhang et al., 2025)、Bick et al. (2025) — 将 attention 整体替换为线性形式，不涉及混合。\n\n上述工作或为“高成本预训练”，或为“十亿级 token 蒸馏且长上下文性能退化”。本文 HALO 首次把训练数据压缩到 2.3 B tokens 以内，同时通过 HyPE 与 Lightning Attention 等改进显著提升了 128 K–1 M 长度的 recall。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **HALO（Hybrid Attention via Layer Optimization）** 流水线与 **HypeNet** 架构，从“蒸馏效率”与“长程泛化”两条路径同时突破：\n\n1. 极低成本蒸馏流水线（HALO）\n\n- **Stage-0 权重热启动**：直接把 Teacher 的  W_q,W_k,W_v,W_o  填入线性 RNN 的对应投影，省去随机初始化。\n- **Stage-1 隐状态对齐**：仅训练 RNN 层，最小化\n\nmathcal L_(stage1)^((l))=MSE!(Y^((l))_(teacher),; RNN^((l))(X^((l-1))))\n\n用 320 M tokens 即可得到“逐层替身”。\n- **Stage-1.5 重要性评分**：一次性推理得到每层被替换后的\n\ns_i=max_j(R_(orig)-R^((i))_j)max_j(C_(orig)-C^((i))_j)+varepsilon\n\n按  s_i  排序保留 top-k（k=⌊L/4⌋）attention 层，无需反复重训。\n- **Stage-2 端到端蒸馏**：固定 Teacher，用 1 B tokens 做 KL 蒸馏\n\nmathcal L_(stage2)=D_(KL)!(f_(orig)parallel f_(hybrid))\n\n- **Stage-3 长上下文微调**：再用 1 B tokens、16 K 长度、低 LR 微调，总开销 2.3 B tokens。\n2. 面向长程泛化的 HypeNet 架构\n\n- **HyPE 位置编码**\n– Attention 层用 **NoPE**，保证无训练长度外推；\n– RNN 层用 **RoPE**，弥补局部位置信息；\n– 推理时对 attention logits 做**动态缩放**\n\ntilde s_t=log_a(t+a),quad a∈500,600,900\n\n抑制长序列熵增导致的注意力分散。\n- **四项结构微调**\n➊ RNN 也加 QK-Norm；➋ GQA→MHA 解除 KV 头共享；➌ 给 RNN/Attention 都加输出门；➍ 去掉短卷积以减少 CUDA kernel 开销。\n- **通用 RNN Mixer 插槽**\n只要满足\n\nS_t=F_t S_(t-1)+k_t^top v_t,quad y_t=q_t S_t W_o\n\n即可接入；实验表明 **Lightning Attention**（数据无关 forget gate）在长度泛化与吞吐间取得最佳平衡。\n3. 结果\n把 Qwen3-1.7B/4B/8B 用 HALO 转成 HypeNet 后，\n\n- 128 K 上下文 NIAH 平均准确率 >90 %，而 Jet-Nemotron、KL-LS 同期方法在 128 K 已跌至 <30 %；\n- 512 K 长度下解码速度提升 3.0×，预填充 3.4×，且显存占用随序列长度线性增长，Teacher 在 1 M 长度即 OOM。\n\n综上，论文通过“分层重要性筛选 + 轻量对齐 + HyPE 缩放”将蒸馏数据压缩两个数量级，并首次使蒸馏型混合模型在长上下文召回任务上超越原始 Transformer。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 6 组实验，覆盖「主结果—消融—效率」三个层次，全部在 1×A800-80 GB 上完成，代码与 checkpoint 已开源。\n\n1. 主实验：Qwen3 → HypeNet 蒸馏\n\n- 模型规模：1.7 B → 2 B、4 B → 5 B、8 B → 9 B\n- 训练数据：FineWeb-Edu 共 2.3 B tokens（Stage-1 320 M + Stage-2 1 B + Stage-3 1 B）\n- 评测任务\n– CSR：ARC-E/C、HellaSwag、WinoGrande、PIQA、LAMBADA、MMLU 共 7 项零样本任务，取归一化准确率平均值。\n– 长程 recall：RULER 的 NIAH-Single-1/2/3，长度 32 K–256 K；额外测到 1 M。\n- 对比基线\n– 原始 Qwen3（Teacher，128 K 用 YaRN 外推）\n– 同期蒸馏混合模型：Jet-Nemotron-2 B（400 B tokens）、KL-LS-GDN-3 B（25 B tokens）\n- 关键结果\n– 128 K 平均 NIAH 90 %↑，而 Jet-Nemotron 与 KL-LS 分别跌至 0 % 与 14 %。\n– CSR 平均仅比 Teacher 低 1–2 分，但 128 K 上下文吞吐提升 2.4–3.0×，显存节省 40 % 以上。\n2. 从头训练消融：验证 HyPE 与 Mixer 选型\n\n- 500 M 参数、20 B tokens、4 K 上下文训练，测 512–256 K 外推。\na) 位置编码对比\n– 全 RoPE、全 NoPE、Attn-RoPE+RNN-NoPE、SWAN-GPT、HyPE\n– 结果：HyPE 在 64× 训练长度仍保持 93.5 % NIAH，显著优于仅 RoPE（20 %）与仅 NoPE（60 %）。\nb) RNN Mixer 对比\n– Lightning、Mamba2、GLA、GDN、RWKV-7\n– Lightning 在 128 K 取得 95 % NIAH，同时预填充速度比 Mamba2 快 3.6×。\n3. HALO 架构消融：逐组件剔除\n在 Qwen3-1.7 B→2 B 的同一 Teacher 上，保留其他条件不变，仅移除下列模块：\n\n- ➊ RNN-RoPE → 128 K NIAH 从 79.9 % 降至 47.9 %\n- ➋ RNN-QK-Norm → 128 K 降至 17.3 %\n- ➌ GQA→MHA → 128 K 略升 83.5 %，但 256 K 从 74.3 % 降至 58 %\n- ➍ 输出门（RNN 或 Attention）（-1.5～-5.4 分不等）\n证明四项改进均对长程 recall 有正向作用。\n4. HALO 层选择策略对比\n同一 HALO 流程，仅替换“保留哪些 attention 层”的算法：\n\n- 均匀分布、仅后半均匀、Jet-Nemotron 任务下降法、KL-LS 逐层重蒸馏法\n- 结果：本文提出的 recall/CSR 比值评分法在 8 K–256 K NIAH 上平均领先 8–15 分，且零额外训练成本。\n5. 训练超参消融\na) Stage-1 数据量：320 M 已饱和，再增到 1.3 B 反而过拟合。\nb) Stage-2 峰值 LR：1e-4 最佳；RADLADS 推荐的常数 1e-5 导致 128 K NIAH 掉到 60 %。\n\n6. 效率实测\n\n- 测解码延迟（TPOT）与预填充时间，batch=1、BF16、Flash-Attention-2 / Triton kernel。\n- 512 K 上下文：HypeNet-1.7 B 解码 3.0×，预填充 3.4×；Qwen3 在 1 M 长度 OOM，HypeNet 仍可用 73 GB。\n- 各 Mixer 预填充对比：Lightning 在 256 K 单 token 时延 6 ms，与 SWA-512 持平，比 Mamba2 快 4× 以上。\n\n综上，实验从「可比性能+更低成本」「更长外推」「更快推理」三个维度系统验证了 HALO 与 HypeNet 的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下问题在论文中尚未完全解决，可作为后续工作的直接延伸：\n\n1. 指令遵循与对齐恢复\nHALO 仅用 FineWeb-Edu 预训练语料，导致原模型的指令跟随、对话安全等后训练能力被稀释。\n\n- 如何设计「低成本、数据高效」的对齐恢复阶段（例如 LoRA+RLHF、DPO）？\n- 是否需要保留部分原始对话数据或引入合成指令数据？\n2. 层选择与压缩极限\n目前固定保留 25 % attention 层；若继续降低至 15 % 甚至 5 %，recall 性能下降曲线如何？\n\n- 能否用可微结构搜索（NAS）一次性联合优化「层类型 + 层位置 + 头数」？\n- 动态/自适应深度：推理时依据输入长度自动决定启用多少 attention 层。\n3. 长度外推机制再深化\nHyPE 的 logits 缩放仅使用一个全局超参 a。\n\n- 能否让缩放系数 st 与内容或头维度相关，实现「头相关、token 相关」的自适应缩放？\n- 进一步结合相对位置惩罚（ALiBi、xPos）或频域校正，实现 2 M+ 长度无损外推。\n4. 多模态与工具调用\n本文仅验证文本模态。\n\n- 视觉/语音交叉注意力层是否同样适用 HALO 的「重要性评分」筛选？\n- 工具调用、代码解释器等需要强召回的场景，RNN 层是否会出现「关键 API 遗忘」？\n5. 异构硬件与量化协同\nHypeNet 的 KV-cache 已大幅缩减，但新增输出门与 MHA 带来 10 % 参数量上升。\n\n- 与 4-bit/2-bit 量化、KV-cache 4-bit 压缩、Flash-Linear Attention 专用 CUDA kernel 联合优化，能否在边缘 GPU 实现 1 M 上下文 30 tokens/s？\n- 探索「遗忘门常量化」后是否可直接用 INT8 矩阵乘法，进一步提速。\n6. 持续学习与领域适应\n蒸馏后模型继续在新领域增量预训练时，RNN 层与 attention 层的「可塑性」差异显著。\n\n- 如何设计不同的学习率或正则策略，防止 RNN 层过早遗忘旧知识？\n- 能否用 expansion/compression 框架动态增长或剪枝 RNN 隐状态维度？\n7. 理论分析\n\n- 对 HyPE 给出长度泛化的严格证明：NoPE attention 的误差界与 RNN 的 receptive field 如何耦合？\n- Lightning Attention 的数据无关 forget gate 与数据相关 gate 的「长度外推」能力差异，能否用谱范数或梯度流解释？\n8. 非 Transformer 教师模型\n目前 HALO 假设教师为纯 Transformer。\n\n- 若教师本身已是混合结构（如 Jamba），如何递归蒸馏「混合 → 更稀疏混合」？\n- 对于 MoE、RetNet、TTT 等不同范式，HALO 的「重要性评分」是否仍然有效？\n9. 自动超参搜索\n缩放底数 a、峰值 LR、层比例 k 目前靠网格搜索。\n\n- 用贝叶斯优化或强化学习把「a + LR + k」联合搜索，目标函数为 Pareto（性能, 吞吐, 显存），能否再节省 30 % 搜索时间？\n10. 绿色 AI 评估\n给出端到端碳排与美元成本：2.3 B tokens × A800 GPU hours = ? kg CO₂，与从头预训练相比减少几个数量级？\n建立「每美元准确率」「每克碳 recall」指标，推动社区采用蒸馏而非重训。\n\n这些方向既有理论价值（长度泛化、重要性度量），也能直接落地（指令恢复、硬件量化、多模态），可作为后续研究的切入点。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：混合 Transformer-RNN 模型需百亿级从头预训练，现有蒸馏法仍耗 10 B+ tokens 且长上下文 recall 暴跌。\n- **方法**：提出 **HALO** 流水线（2.3 B tokens）（权重热启动→隐状态对齐→基于 recall/CSR 重要性选层→KL 蒸馏→长上下文微调），并设计 **HypeNet** 架构：\n– **HyPE**：Attention 层用 NoPE 保长度外推，RNN 层用 RoPE 补位置信息，加动态 logits 缩放。\n– 四项结构微调：RNN 加 QK-Norm、GQA→MHA、输出门、去短卷积。\n– 通用 RNN Mixer 插槽，Lightning Attention 表现最佳。\n- **实验**：Qwen3 1.7/4/8 B→2/5/9 B HypeNet，128 K 上下文 NIAH>90 %（同期方法<30 %），CSR 与 Teacher 持平，512 K 解码 3.0×、预填充 3.4× 提速，1 M 长度 Teacher OOM 而 HypeNet 仍可行。\n- **结论**：首次实现“极低数据成本蒸馏 + 卓越长程性能”的混合模型，为高效长上下文 LLM 提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid%20Linear%20Attention%20Done%20Right%3A%20Efficient%20Distillation%20and%20Effective%20Architectures%20for%20Extremely%20Long%20Contexts，链接是https://arxiv.org/pdf/2601.22156，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22156。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22154v1-1769990225065","title":"Exploring Reasoning Reward Model for Agents","description":"Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.","url":"https://arxiv.org/abs/2601.22154v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22154v1","authors":"Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue","categories":"cs.AI, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22154v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22154v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22154v1","rank":3,"domain":"Agent","zh_summary":"主动强化学习（Agentic RL）在使智能体执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍然依赖基于稀疏结果的奖励来进行训练。这种反馈无法区分中间推理质量，导致训练结果不理想。在本文中，我们提出了智能体推理奖励模型（Agent-RRM），这是一种多方面的奖励模型，为智能体轨迹生成结构化反馈，包括 (1) 明确的推理轨迹，(2) 聚焦的批评，通过突出推理缺陷提供改进指导，以及 (3) 评估过程表现的总体评分。利用这些信号，我们系统地研究了三种整合策略：Reagent-C（文本增强的优化）、Reagent-R（奖励增强的指导）和 Reagent-U（统一反馈整合）。在 12 个多样化基准上的广泛评估表明，Reagent-U 能显著提升性能，在 GAIA 上达到 43.7%，在 WebWalkerQA 上达到 46.2%，验证了我们的推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来研究。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**Agentic Reinforcement Learning（Agentic RL）**中**稀疏、仅依赖结果（outcome-based）的奖励信号**所带来的两大核心缺陷：\n\n1. **无法区分中间推理质量**\n在长程、多步工具调用任务里，只要最终答案错误，整条轨迹就被视为完全失败，导致成功的前置步骤被埋没，训练信号过于粗粒度。\n\n2. **缺乏可操作的改进指引**\n纯标量奖励只给出“好/坏”评价，不提供具体哪里出错、如何修正的信息，使得智能体难以针对性改进推理与工具使用策略。\n\n为此，作者提出**Agent Reasoning Reward Model（Agent-RRM）**，一次性输出三种结构化信号：\n\n- 显式推理轨迹 `<think>`\n- 针对性批评 `<critique>`\n- 整体质量评分 `<score>`\n\n并基于该多维度反馈设计三种训练变体（Reagent-C / R / U），系统验证**密集推理奖励**在提升智能体推理与工具使用能力上的有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究划分为两条主线，并指出各自与本文工作的差异。以下按两条主线归纳代表性文献及其与 Agent-RRM 的关系。\n\n2.1 Agentic Reinforcement Learning\n\n**核心问题**：现有方法几乎全部采用**稀疏、结果导向**的奖励，无法对长程轨迹的中间步骤进行细粒度监督。\n\n| 代表工作 | 主要贡献 | 与本文差异 |\n| --- | --- | --- |\n| Search-R1 (Jin et al., 2025) | 用 RL 训练 LLM 交替执行搜索与推理，提升检索增强推理性能 | 仅依赖最终答案正确性作为奖励，无中间反馈 |\n| WebSailor (Li et al., 2025b) | 将 RL 扩展到长程网页导航，降低信息搜寻不确定性 | 同样使用稀疏结果奖励，无法纠正中间步骤错误 |\n| Agent0 (Xia et al., 2025) | 无需人工监督，让工具感知行为通过共进化自动涌现 | 未引入显式推理质量评估，训练信号仍稀疏 |\n\n2.2 Reward Modeling\n\n**核心问题**：现有奖励模型要么只给**步骤级标量分**（标注成本高、易被 hack），要么只做**成对偏好比较**（无法提供可执行改进建议）。\n\n| 代表工作 | 奖励信号形式 | 与本文差异 |\n| --- | --- | --- |\n| Atom-Searcher (Deng et al., 2025) | 直接拿 30B 模型给每一步打 0/1 分 | 仅步级标量，无语言解释，易奖励黑客 |\n| PPR (Xu et al., 2025) | 用预定义原则给步骤打分 | 同样是步级标量，缺乏可操作 critique |\n| RM-R1 (Chen et al., 2025d) | 先生成推理 rubric 再给出评分 | 面向通用问答，未针对 agent 工具链设计 |\n| R1-Reward (Zhang et al., 2025b) | 多模态推理奖励模型，引入稳定 RL 算法 | 仅输出标量，无文本 critique 用于 refinement |\n\n小结\n\n- **Agentic RL 侧**：首次把“**推理轨迹 + 批评文本 + 标量分**”统一引入 agent 训练流程。\n- **Reward Model 侧**：首次为 agent 设计**无需真值**即可生成**可执行批评**的多维奖励模型。\n\nQ3: 论文如何解决这个问题？\n\n论文将“稀疏结果奖励 → 细粒度推理奖励”的转化拆成三步：\n\n1. 设计**多维奖励模型** Agent-RRM；\n2. 构建**覆盖四种任务类型**的高质量数据集；\n3. 提出**三种集成策略**系统地把推理奖励喂给智能体。\n\n以下按方法论（Method）顺序展开。\n\n3.3 数据集构造：先解决“有信号”问题\n\n- **Reagent-RL-709K**\n– 来源：公开 QA + 自采复杂任务，共 70.9 万条。\n– 过滤：三阶段去歧义、去重、难度采样，保证轨迹多样性。\n\n- **Reagent-SFT-55.6K**\n– 用 DeepSeek-V3.1 生成轨迹，**只保留最终答案正确**的 5.56 万条，作为冷启动 SFT 数据。\n\n- **Agent-RRM 专用标注**\n– 在 709 K 上采样 118 K 轨迹，用 GPT-4o-mini-120B 标注三元组 `<think>｜<critique>｜<score>`，得到\n– Reagent-RRM-SFT-28K\n– Reagent-RRM-RL-90K\n\n3.4 Agent-RRM：再解决“信号可信”问题\n\n两阶段训练：\n\n1. SFT 阶段：在 28 K 上让 8 B 模型学会输出结构化三元组。\n2. GRPO 阶段：在 90 K 上用强化学习微调，使\n- 推理轨迹自洽（降低幻觉）\n- 标量分与人工排序一致性最高（Kendall τ 提升 0.12）\n\n**输出格式**（训练/推理一致）：\n\n```text\n<think>…逐步检查工具调用与逻辑链…</think>\n<critique>…指出具体错误：漏 browse、误用工具、循环调用…</critique>\n<score>0.32</score>\n```\n\n3.5 Reagent：最后解决“信号用好”问题\n\n三种集成策略对应三条消融曲线：\n\n| 变体 | 利用的信号 | 训练方式 | 关键公式 |\n| --- | --- | --- | --- |\n| Reagent-C | 仅 <critique> | 零参更新，in-context 二次生成 |  o_i^((2))sim π_θ(·mid q,o_i^((1)),c_i)  |\n| Reagent-R | 仅 <score> | 传统 GRPO，把 score 当密集奖励 |  R_i=R_(rule)+λ·Agent-RRM(q,o_i)  |\n| Reagent-U | <critique>+<score> | 统一池化优化：初始+修正轨迹一起算优势 | 见下方公式 |\n\n**Reagent-U 统一目标**\n\nJ_U(θ)=E[(1) / (2G)∑_(k=1)^2∑_(i=1)^G[min!(r_i^((k))(θ)A_i^((k)),clip_ε)-β D_(KL)^((i,k))]]\n\n其中\n\n-  k∈1,2  对应“初始轨迹”与“经 critique 修正后的轨迹”\n- 优势  A_i^((k))  在 **混合池**  G_(pool)  内标准化，保证模型同时优化“一次做对”与“被提示后能改”两种能力。\n\n实验验证“信号有效”\n\n- **Reagent-C** 在 GSM8K 上把 94.6 → 94.9，**无需训练**即可修正 47 % 的初级计算错误。\n- **Reagent-R** 在 Bamboogle 上比纯规则奖励提升 11.2 个百分点，验证密集奖励缓解稀疏性。\n- **Reagent-U** 在 12 个基准上**全部领先**，GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相对 8 B 基线平均提升 18.4 %，证明两种信号**协同增益**最大。\n\n一句话总结\n\n论文通过“**多维奖励模型 → 数据 → 三种集成策略**”的闭环，把原本只能告诉智能体“错”的稀疏信号，升级为同时告诉它“**哪里错、为何错、如何改**”的密集推理奖励，从而系统性地提升了长程工具使用与推理能力。\n\nQ4: 论文做了哪些实验？\n\n论文在第 4 节“Experiments”中围绕 **12 个基准、3 类任务、3 种变体、4 组消融** 展开系统评估，可概括为“**一个主表 + 三个深度分析 + 两个参数实验**”。\n\n1\\. 主实验：12 基准全面比较\n\n**任务分组与指标**\n\n- **数学推理**：AIME24/25、MATH500、GSM8K        pass@1\n- **知识密集型**：HotpotQA、2Wiki、Bamboogle、MuSiQue  pass@1\n- **通用 Agent & 搜索**：GAIA、WebWalkerQA、HLE、xbench pass@1\n\n**对照维度**\n\n- 闭源：o3, o1-preview, Claude-4-Sonnet, DeepResearch\n- 开源同规模（≤8 B）：WebThinker、ARPO、VerlTool …\n- 开源大尺度（≤32 B）：QwQ-32B、DeepSeek-R1-671B、Search-o1 …\n- 开源过程奖励：Atom-Searcher、PPR-Instruct\n\n**核心结果**（表 1+2 汇总）\n\n| 模型 | GAIA | WebWalkerQA | Bamboogle | AIME24 | 平均提升 |\n| --- | --- | --- | --- | --- | --- |\n| Qwen3-8B 基线 | 21.4 | 29.0 | 53.6 | 46.7 | — |\n| Reagent-C | 25.2 | 35.5 | 61.6 | 56.7 | +6.8 |\n| Reagent-R | 36.9 | 45.3 | 72.8 | 53.3 | +11.4 |\n| Reagent-U | 43.7 | 46.2 | 76.8 | 60.0 | +18.4 |\n\n2\\. 深度分析 1：Textual-Critique 真有用吗？\n\n- **实验**：Reagent-C 零参更新，仅把 critique 作为 prompt 让模型再答一次。\n- **结果**：12 个数据集**全部上涨**，GSM8K 94.6→94.9，GAIA 21.4→25.2；**首次失败中 47 % 在第二次答对**。\n- **结论**：critique 提供**可执行诊断**，无需训练即可纠正工具误用与逻辑跳跃。\n\n3\\. 深度分析 2：Model-based Reward 能缓解稀疏性吗？\n\n- **实验**：固定 λ=0.3，比较 Reagent-R（规则+模型分） vs Reagent w/o Agent-RRM（仅规则）。\n- **结果**：Bamboogle +11.2，xbench +9.0，平均 +7.6 个百分点。\n- **结论**：密集**标量奖励**显著改善训练信号，但仍低于 Reagent-U，说明**纯分数不如“分数+文字”**。\n\n4\\. 深度分析 3：Unified Feedback 是否产生协同效应？\n\n- **实验**：Reagent-U 同时优化初始与 critique-修正轨迹，优势在混合池统一标准化。\n- **结果**：\n– 在**需要≥5 步工具调用**的 GAIA Lv3 上，Reagent-U 16.7 %，比 Reagent-R 高 10 个百分点。\n– 在**跨模态**的 GAIA-full（165 题）上，pass@1 38.8 %，比 MCP-R1 高 1.2 个百分点，验证**非文本任务同样受益**。\n- **结论**：scalar 信号提供**全局优化方向**，critique 信号提供**局部修正动作**，二者联合产生**1+1>2**的协同增益。\n\n5\\. 参数实验\n\n5.1 奖励权重 λ 扫描（图 3）\n\n- 范围：λ∈\n0,0.5\n；任务：AIME24（数学）、xbench（深度搜索）。\n- 结果：λ≈0.2–0.4 区域性能 plateau；λ≥0.5 开始下降，**过高权重会牺牲最终答案准确性**。\n\n5.2 训练步数与收敛性（附录 B）\n\n- 300 步 GRPO 已收敛，继续训练未见明显提升，说明**数据量与模型规模匹配**，未出现过度拟合。\n\n6\\. 案例可视化（附录 D）\n\n- **Case 1（GAIA 搜索）**：critique 指出“未 browse 权威页→假设错误”，第二次调用 browse 拿到 15 Mt 精确值，答案从 Cannikin→Bravo。\n- **Case 2（GSM8K 数学）**：critique 指出“Python 第一次无 print 浪费调用”，第二次去掉多余调用并纠正“总工时÷人数”逻辑，答案从 47.25→189。\n\n一句话总结\n\n实验从“**能不能用→管不管用→好不好用→怎么用最好**”四个层次验证：\n**多维推理奖励模型 + 统一集成训练** 在 12 基准、跨模态、长程工具调用场景下**全面领先**，且**零参 critique 即可带来显著推理修正**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下可探索方向按“**规模-数据-任务-信号-理论**”五层归纳，均直接对应论文第 6 节 Limitations 的开放问题，并给出可落地的下一步实验设计。\n\n1\\. 规模：更大参数与分布式 RL\n\n- **问题**：全文仅在 8 B 模型验证，协同效应是否会随规模放大？\n- **探索点**\n– 在 32 B/70 B 基座重复 Reagent-U 训练，观察“λ-性能”曲线是否左移（更强模型需要更少 critique 即可收敛）。\n– 采用 **Model-parallel + Rollout-parallel** 框架，把 Agent-RRM 与 Policy 解耦到不同 GPU 节点，验证千亿级模型下是否出现奖励过拟合（reward overfitting）。\n\n2\\. 数据：开放环境 vs 静态基准\n\n- **问题**：12 个基准均为**静态题库**，工具集固定，无法反映真实世界工具漂移。\n- **探索点**\n– 引入 **Live-API 环境**（如 Google Search 动态排名、WolframAlpha 付费限额），构建非平稳奖励，测试 Agent-RRM 的**在线自适应**能力。\n– 设计 **Adversarial Tool-Response** 数据集：故意返回错位、截断或冲突信息，量化 critique 对“工具幻觉”的纠错上限。\n\n3\\. 任务：长周期闭环与多智能体\n\n- **问题**：现有 episode 最长 30 步，且为单 agent。\n- **探索点**\n– **Science-of-Chemistry** 场景：让 agent 连续运行 100+ 步完成“文献检索→实验设计→代码仿真→结果验证”闭环，用 Agent-RRM 每 10 步做一次 critique，观察**长程信用分配**是否仍有效。\n– **Multi-Agent Debate**：把 critique 角色独立为第三方裁判，与其他 agent 的 solution 进行**多轮对抗式 refinement**，检验 critique 语言是否出现**策略性误导**（reward hacking 新形态）。\n\n4\\. 信号： critique 的粒度与格式\n\n- **问题**：固定三段式输出可能不是最优。\n- **探索点**\n– **Token-Level 奖励**：把 `<critique>` 拆成**逐句批评**，用指针网络对齐到具体动作，实现**子步骤级**优势估计：\n\nA_t = R_(sentence)(t) - μσ\n\n– **Multimodal-Critique**：当观测含图/音频时，让 Agent-RRM 输出**圈图 mask** 或**音频时间戳**，验证**视觉定位**能否进一步降低修正成本。\n\n5\\. 理论：奖励可解释性与收敛保证\n\n- **问题**：Agent-RRM 的自洽性缺乏形式化刻画。\n- **探索点**\n– 建立 **Critique Consistency Index (CCI)**：同一轨迹两次采样 critique 的**语义相似度**，用 CCI 与最终性能做皮尔逊检验，量化“**可解释→可改进**”的因果链。\n– 在**离线 RL** 框架（如 CQL）下，把 Agent-RRM 当作 Q-function 正则项，推导**误差上界**，回答“**推理奖励误差 ε 对策略性能影响是 O(ε) 还是 O(ε²)**”。\n\n快速启动清单（8 卡 A800 可完成）\n\n1. 用 32 B 模型复现 Reagent-U，记录 λ∈{0.1,0.3,0.5} 的 GAIA 性能 → 验证规模效应。\n2. 在 Live-API 环境跑 500 条长 episode（≥50 步），统计 critique 成功纠正“工具返回漂移”的比例 → 验证鲁棒性。\n3. 把 critique 拆成句子级嵌入，与动作序列做注意力对齐，重新计算优势 → 验证更细粒度信号是否带来额外 2–3 个百分点。\n\n一句话总结\n\n未来工作可沿“**更大模型、更乱环境、更长链条、更细信号、更严理论**”五轴推进，把 Agent-RRM 从“静态题库好”推向**开放世界可信**。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Agent-RRM**（Agent Reasoning Reward Model）与 **Reagent** 训练框架，解决 Agentic RL 中“稀疏结果奖励无法区分中间推理质量、缺乏可执行改进信号”的核心痛点。主要贡献与内容可概括为以下四点：\n\n1. 多维推理奖励模型\nAgent-RRM 对每条轨迹输出三元组：\n\n- `<think>`：逐步推理轨迹\n- `<critique>`：可执行的错误指正\n- `<score>`：0–1 整体质量分\n无需真值即可提供密集、语言化、可解释的反馈。\n2. 高质量数据集\n构建 70.9 万 RL 样本（Reagent-RL-709K）与 5.56 万 SFT 样本（Reagent-SFT-55.6K），并标注 11.8 万条结构化评判（Reagent-RRM-SFT/RL-28K/90K），覆盖数学、多模态、网页搜索、复杂工具四类任务。\n\n3. 三种集成策略\n\n- Reagent-C：零参更新，用 critique 做 in-context 二次生成\n- Reagent-R：GRPO 训练，规则奖励 + Agent-RRM 分数\n- Reagent-U：统一池化初始与修正轨迹，同步优化 scalar 与语言信号\n4. 系统实验\n在 12 个基准（GAIA、WebWalkerQA、AIME、HotpotQA 等）上，Reagent-U 取得新 SOTA：GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相比 8 B 基线平均提升 18.4 %；零参的 Reagent-C 也能稳定上涨，验证多维推理奖励的普适性与可解释性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Reasoning%20Reward%20Model%20for%20Agents，链接是https://arxiv.org/pdf/2601.22154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22154。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22149v1-1769990225065","title":"DynaWeb: Model-Based Reinforcement Learning of Web Agents","description":"The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.","url":"https://arxiv.org/abs/2601.22149v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:07.000Z","metadata":{"arxivId":"2601.22149v1","authors":"Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu","categories":"cs.CL, cs.AI","published":"2026-01-29T18:59:07Z","pdfUrl":"https://arxiv.org/pdf/2601.22149v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22149v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22149v1","rank":4,"domain":"Agent","zh_summary":"由大型语言模型（LLM）和强化学习（RL）驱动的自主网页代理的发展，代表了通用人工智能助手迈出的重要一步。然而，与实时互联网的交互在训练这些代理时带来了重大挑战，因为这种交互效率低、成本高且充满风险。基于模型的强化学习（MBRL）通过学习环境的世界模型以实现模拟交互，提供了一种有前景的解决方案。本文介绍了 DynaWeb，一种新型的 MBRL 框架，通过与训练好的网页世界模型进行交互来训练网页代理，该模型能够根据代理的动作预测自然网页表示。该模型充当一个合成网页环境，使代理策略可以通过生成大量的回滚动作轨迹来“演练”，从而实现高效的在线强化学习。除了自由策略回滚之外，DynaWeb 还结合了来自训练数据的真实专家轨迹，这些轨迹在训练过程中随机穿插在策略内回滚中，以提高稳定性和样本效率。在具有挑战性的 WebArena 和 WebVoyager 基准测试中进行的实验表明，DynaWeb 一贯且显著地提升了最先进开源网页代理模型的性能。我们的研究结果证明，通过想象训练网页代理是可行的，为大规模扩展在线代理强化学习提供了一种高效且可扩展的方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何在不与真实互联网交互的前提下，高效、安全地训练具备在线强化学习能力的网页智能体”这一核心问题。\n具体而言，现有基于大模型的网页智能体虽可通过在线 RL 显著提升鲁棒性与长程决策能力，但直接依赖真实网页交互会带来三大瓶颈：\n\n1. 效率低：每次策略更新都需大量实时页面请求，训练周期长。\n2. 成本高：频繁访问真实站点易触发验证码、限流或封禁，需额外代理与清洗开销。\n3. 风险大：策略探索可能产生不可逆副作用（误下单、删数据、泄露隐私等），难以规模化。\n\n为此，作者提出将“世界模型”从传统推理工具升级为**在线 RL 的训练环境**，通过模型化互联网动力学，使智能体在“想象”中完成策略优化，从而把对真实环境的采样需求降到最低，同时保留在线 RL 的探索与信用分配优势。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均与“网页智能体”或“世界模型”密切相关，但尚未将二者以\\*\\*在线模型强化学习（MBRL）\\*\\*方式深度耦合：\n\n- **网页智能体训练范式**\n- 提示工程与推理框架：ReAct、Cognitive Kernel、MCP 等通过链式思维或结构化提示提升单步决策。\n- 监督/模仿学习：NNetNav、Go-Browse、Explorer、InSTA 等收集或合成高质量轨迹，用 SFT 训练。\n- 在线 RL：WebAgent-R1、WebRL、WorkForceAgent-R1、WebDancer 等直接在真实环境采样，用 GRPO、DAPO 等算法优化，但需大量真实交互，成本高。\n- **世界模型在网页场景的辅助应用**\n- 推理时规划：WebDreamer、WMA、RAP、WKM 用大模型模拟下一步网页状态，供树搜索或价值估计，**仅用于推断阶段**，不更新策略。\n- 离线数据增强：WebEvolver、Explorer 用世界模型生成额外轨迹，再离线蒸馏到策略，**与 on-policy 优化解耦**。\n- **通用世界模型与 MBRL 基础**\n- 视觉/游戏领域：Dreamer、SimPLe、IRLA、Diffusion World Models 等将环境动力学模型嵌入 RL 循环，实现“想象训练”。\n- 文本/决策抽象：RWKM、RAP、Diffusion for World Modeling 等把 LLM 当作隐式环境模型，但尚未在网页这类高动态、部分可观察场景做在线策略优化。\n\n综上，已有工作要么停留在“**推理阶段**”使用世界模型，要么“**离线生成**”伪轨迹后再蒸馏；DynaWeb 首次把**专门训练的网页世界模型**作为**可微环境**，与真实专家轨迹混合，直接用于 on-policy 强化学习，填补了“网页 MBRL”空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 DynaWeb，一套**面向网页任务的在线模型强化学习（MBRL）框架**，把“真实网页交互”替换为“可学习的网页世界模型 + 少量真实专家轨迹”的混合训练流程，核心步骤如下：\n\n1. 训练网页世界模型（WWM）\n\n- 将网页状态表示为**可访问性树**  o_t 。\n- 用监督数据  (o_t, a_t, o_(t+1))  训练大模型  p_φ  先输出**自然语言状态变更描述**  Delta ，再自回归生成下一帧树  hat o_(t+1) ，损失为\n\nL_φ = -∑ log p_φ(r,Delta mid I,o_t,a_t)\n\n- 得到的  p_φ  充当**可反复采样的网页“服务器”**，无需访问真实站点即可展开多步交互。\n2. 想象 rollout：策略与世界模型闭环\n\n- 策略  π_θ  以当前真实或想象观测为输入，采样动作  a_tsim π_θ(· mid hat o_(1:t),h_(1:t-1),a_(1:t-1),q) 。\n- 世界模型立即返回  hat o_(t+1)sim p_φ(· mid hat o_t,a_t,q) ，形成**想象轨迹**\n\nhatτ = (hat o_1,h_1,a_1,dots,hat o_T,h_T,a_T)\n\n- 终止时用模型自评给出稀疏奖励  hat r(hatτ,q)∈0,1 ，作为整条轨迹的回报  G(hatτ) 。\n3. 混合真实专家轨迹\n\n- 每次梯度更新按 50 % 比例**随机混入**来自 NNetNav SFT 数据集的**真实专家轨迹**，与想象 rollout 一起构成 batch。\n- 真实轨迹提供**无偏状态转移与奖励信号**，抑制世界模型幻觉，稳定训练。\n4. 序列级策略优化（GSPO）\n\n- 对每条轨迹计算单序列优势  hat A_i = G(hatτ_i) - bar G 。\n- 采用**序列级重要性权重**\n\ns_i(θ)=exp!l((1) / (|y_i|)∑_k logπ_θ(y_(ik)mid·s){π_(θ_old)(y_(ik)mid·s)}r)\n\n代入 clipped objective\n\nJ_(GSPO)(θ)=mathbb E![(1) / (G)∑_(i=1)^G min!l(s_i(θ)hat A_i, clip(s_i(θ),1!-!varepsilon,1!+!varepsilon)hat A_ir)]\n\n- 实现**长程信用分配**且避免 token 级方差爆炸。\n5. 控制想象深度与误差\n\n- 实验发现 dream length 4–5 步最佳，过长会累积模型误差；同时限制单条 rollout 最大 5 步并早停终端状态，兼顾探索深度与仿真保真度。\n\n通过上述设计，DynaWeb 把**策略优化完全搬到世界模型内部**，仅依赖少量真实轨迹做正则化，从而\n\n- 将真实环境采样需求降低一个数量级；\n- 避免不可逆操作与封禁风险；\n- 在 WebArena 与 WebVoyager 上相对最强基线分别提升 **+16.1 %** 和 **+18.7 %** 成功率，验证“用想象训练网页智能体”的可行性与扩展性。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“**能否用想象 rollout 替代真实网页交互来训练高性能网页智能体**”展开，分三部分：\n\n1. 主实验：WebArena & WebVoyager 成功率对比\n\n- 基准\n– 零样本提示：Llama-3.1-8B-Instruct+CoT、GPT-4o\n– 监督微调：NNetNav、Go-Browse\n– 离线 RL：WebRL\n– 推理时世界模型：ITL（每步模拟选最优动作，不训练策略）\n- 结果\n– WebArena（812 任务，5 站点）\nDynaWeb-8B 平均 SR **31.0 %**，相对离线 RL **+16.1 %**，在 Reddit/GitLab/CMS/Shopping 四域均第一。\n– WebVoyager（643 开放站点）\nDynaWeb 平均 SR **38.7 %**，显著高于次佳基线 WebRL（32.6 %），在 Amazon/BBC/Map 等 9 个站点领先。\n2. 消融实验：关键设计对性能的影响\n\n- Dream length 消融\n固定其余超参，令最大想象步数 1–8 变化；SR 在 4–5 步处峰值，过短欠探索，过长累积幻觉。\n- 真实轨迹比例消融\n0 %–100 % 替换想象 rollout；40 % 真实数据即可显著超越纯 SFT，>60 % 后收益饱和，验证“少量真实轨迹正则化”即可。\n- 世界模型训练必要性\n用**冻结 GPT-oss-120b** 替代微调后的 WWM；WebArena SR 从 31.0 % → 20.9 %，WebVoyager 35.4 % → 28.6 %，表明**显式环境动力学训练**不可或缺。\n3. 训练过程分析\n\n- 训练曲线：混入真实轨迹后，GSPO 的序列级 variance 更小，收敛更快。\n- 错误类型统计：相比纯想象策略，DynaWeb 将“页面元素幻觉”与“无效动作”两类错误各降低约 30 %。\n\n综上，实验系统验证了\n\n- 想象 rollout 可直接用于 on-policy RL；\n- 适度真实数据 + 适度 dream length 是性能拐点；\n- 专门训练的网页世界模型是提升的关键。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步扩展 DynaWeb 的想象力驱动范式：\n\n- **长程高保真世界模型**\n- 研究层级或扩散式架构，将页面视觉截图与 Accessibility Tree 联合建模，降低 5 步后的幻觉累积，实现 ≥20 步稳定 rollout。\n- 引入“页面差异压缩”或 Delta-VAE，仅对变化节点建模，减少自回归长度与误差传播。\n- **多模态世界模型**\n- 同时接受 HTML、CSS、屏幕截图、DOM 变化事件，统一文本与视觉空间，缓解纯文本树对动态 UI（Canvas、SVG、Shadow-DOM）信息丢失问题。\n- 探索视觉-动作对齐预训练，提升对图标、悬浮动画等细粒度交互的预测精度。\n- **可自我批评的世界模型**\n- 让同一模型输出“下一状态 + 预测不确定性/置信度”，策略据此自适应决定是否请求真实环境验证，形成**选择性查询**（selective query）机制，进一步压缩真实交互预算。\n- 引入对比式或能量模型，对世界模型置信度进行校准，防止过度乐观幻觉。\n- **层次化想象与策略分解**\n- 将任务分解为子目标（URL 导航、表单填充、结果提取），训练**高层子目标世界模型**与**低层原子动作世界模型**，实现“子目标→原始动作”两级想象，降低长程信用分配难度。\n- 结合选项框架（Options），在想象空间先做子目标规划，再展开低层动作 rollout。\n- **持续学习与领域增量**\n- 研究世界模型在线微调策略，避免新站点数据导致旧站点灾难性遗忘；可采用 EWC、LoRA-merging 或经验回放。\n- 探索任务-站点混合分布外推（OOD）检测，自动触发世界模型局部更新，保持长期一致性。\n- **安全与可验证想象**\n- 引入形式化约束（如不变量：价格≥0、表单不可越权提交），在世界模型采样阶段即屏蔽非法状态，减少策略探索危险动作。\n- 结合合约或沙箱 API，让世界模型对敏感操作（支付、写库）返回“虚拟确认”而非真实执行，实现零风险训练。\n- **跨环境迁移与元世界模型**\n- 收集 100+ 站点数据，训练**通用网页世界模型**，再针对新站点少量微调，验证“预训练-微调”范式能否快速适应未知站点动力学。\n- 研究元学习（MAML、Prompt-tuning）初始化，使世界模型在新站点仅用 50 条轨迹即可适配。\n- **更细粒度奖励模型**\n- 当前仅用终端 0/1 奖励，可训练**稠密价值模型**对想象中间状态打分，实现单步 TD 学习，进一步缩短真实轨迹需求。\n- 探索人工偏好 + 自动规则混合的奖励函数，缓解稀疏奖励导致的想象轨迹价值估计方差。\n- **系统级优化**\n- 世界模型与策略模型联合蒸馏为单一小模型，降低推理延迟，实现端侧部署。\n- 研究异步并行想象：GPU 集群同时展开数千条 rollout，用 Ray/vLLM 动态调度，提升训练吞吐。\n- **评测扩展**\n- 构建更长 horizon（≥50 步）与多会话（跨天登录、购物车保留）任务集，检验世界模型对**状态持久化与动态会话**的建模能力。\n- 引入对抗评测：在想象与真实环境间随机切换，测量策略对模型误差的鲁棒性，推动世界模型-策略协同优化。\n\n这些方向分别从模型架构、训练策略、安全约束、系统效率到评测维度出发，可进一步释放“想象驱动网页 RL”的潜力，并推广至更广泛的数字界面自动化任务。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **DynaWeb**：首个面向网页任务的**在线模型强化学习（MBRL）框架**，用“想象”替代昂贵、高风险的真实互联网交互，实现高效、安全的智能体训练。\n\n1. 问题\n在线 RL 可提升网页智能体的鲁棒性与长程决策，但直接访问真实站点效率低、成本高、风险大，难以规模化。\n\n2. 解法\n\n- 训练**网页世界模型**  p_φ ，输入当前可访问性树  o_t  与动作  a_t ，自回归输出状态变更描述  Delta  并生成下一帧树  hat o_(t+1) ，充当可无限采样的**虚拟网站**。\n- 策略  π_θ  与世界模型闭环，展开多步**想象轨迹**  hatτ ，用模型自评稀疏奖励  hat r(hatτ,q)  进行**序列级策略优化（GSPO）**。\n- 训练中**随机混入 40 % 真实专家轨迹**，提供无偏状态转移，抑制幻觉并稳定学习。\n- 控制想象深度 4–5 步，平衡探索与仿真保真。\n3. 实验\n\n- WebArena（812 任务）：DynaWeb 平均成功率 **31.0 %**，相对最强离线 RL 基线 **+16.1 %**。\n- WebVoyager（643 开放站点）：**38.7 %**，显著优于 GPT-4o、WebRL 等。\n- 消融：真实数据比例、dream length、世界模型训练必要性均被验证为性能拐点。\n4. 结论\nDynaWeb 首次证明**专门训练的世界模型可直接用于 on-policy 网页 RL**，在零风险、低成本的前提下持续提升开源智能体，为“想象驱动”的通用数字代理提供可扩展路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaWeb%3A%20Model-Based%20Reinforcement%20Learning%20of%20Web%20Agents，链接是https://arxiv.org/pdf/2601.22149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22149。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2509.21670v4-1769990225065","title":"MORPH: PDE Foundation Models with Arbitrary Data Modality","description":"We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters, MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.","url":"https://arxiv.org/abs/2509.21670v4","source":"ArXiv Domain","timestamp":"2025-09-25T22:38:36.000Z","metadata":{"arxivId":"2509.21670v4","authors":"Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Alexander Scheinker, Diane Oyen, Nathan Debardeleben, Earl Lawrence, Ayan Biswas","categories":"cs.CV, cs.AI, cs.LG, physics.comp-ph","published":"2025-09-25T22:38:36Z","pdfUrl":"https://arxiv.org/pdf/2509.21670v4.pdf","abstractUrl":"https://arxiv.org/abs/2509.21670v4","coolPaperUrl":"https://papers.cool/arxiv/2509.21670v4","rank":5,"domain":"Agent","zh_summary":"我们介绍了 MORPH，一种适用于偏微分方程（PDE）的模态无关、自回归的基础模型。MORPH 基于卷积视觉变换器（Convolutional Vision Transformer）骨干，能够无缝处理不同数据模态（1D–3D）、不同分辨率的异质时空数据集，以及包含标量和向量成分的多场数据。该架构结合了 (i) 分量卷积，可以联合处理标量和向量通道以捕捉局部交互；(ii) 场间交叉注意力，可建模并选择性地传播不同物理场之间的信息；(iii) 轴向注意力，将完整的时空自注意力沿单独的空间和时间轴进行分解，以降低计算负担，同时保留表达能力。我们对多个模型变体在多样的异质 PDE 数据集上进行了预训练，并评估其在一系列下游预测任务中的迁移能力。通过全模型微调和参数高效的低秩适配器，MORPH 的表现优于从零训练的模型。在广泛的评估中，MORPH 能够匹配或超越强基线模型及近期的最先进模型。综合来看，这些功能为从科学观测的异质和多模态特性中进行学习提供了灵活且强大的骨干结构，为可扩展且数据高效的科学机器学习铺平了道路。源代码、数据集和模型均公开可在 https://github.com/lanl/MORPH 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**科学机器学习（SciML）中构建通用偏微分方程（PDE）基础模型**所面临的两大核心瓶颈：\n\n1. **数据异构性**\n现有PDE数据集在空间维度（1D–3D）、分辨率、标量/矢量场混合、物理场数量等方面差异巨大，且规模可达TB级。传统方法需对数据进行填充或重采样以统一形状，导致计算与存储开销爆炸，且难以纳入3D或高分辨率2D数据（自注意力复杂度 O(L^2) 随序列长度 L 二次增长）。\n\n2. **部分可观测性**\n实验或仿真往往只能提供稀疏、低维的观测（如1D探针、2D切片），而真实物理过程是3+1维连续时空演化。现有基础模型多假设2D笛卡尔网格、固定场类型，无法直接利用1D或3D数据，造成信息浪费与泛化盲区。\n\n为此，作者提出**MORPH**——一个**形状无关（shape-agnostic）**、自回归的PDE基础模型，其目标是在**不重新设计架构**的前提下，统一处理任意维度、任意分辨率、混合标量/矢量场的多物理PDE数据，实现：\n\n- **预训练阶段**：从六类异构数据（1D可压缩NS、2D不可压缩NS、2D反应-扩散、2D浅水、3D可压缩NS、3D磁流体）中学习通用演化算子。\n- **下游任务**：通过全参数微调或LoRA高效适配，在七项新任务（1D Burgers、1D反应-扩散、2D Kolmogorov流、3D湍流冷却等）上零样本或少量样本即可超越从头训练的专用模型。\n\n总结：MORPH试图用**单一架构**解决“**如何在部分、异构、多模态的PDE数据上预训练，并低成本迁移到任意下游物理问题**”这一SciML基础模型核心挑战。\n\nQ2: 有哪些相关研究？\n\n论文第2节“Related Work”将相关研究划分为两大主线，并指出它们与MORPH的差异。可归纳为以下四类：\n\n1. 专用PDE代理模型（Standalone Surrogates）\n\n- 数据驱动：3D-CNN、ConvLSTM、DCGAN、GNN\n- 物理嵌入：PINNs、神经算子（DeepONet、FNO、WNO、LNO、PINO）\n- 生成式：GAN、扩散模型（score-based）、潜空间演化（VAE+Transformer/LSTM/DeepONet）\n共同点：针对**单一PDE族**重新训练，无法跨物理迁移。\n2. 早期迁移/多任务尝试\n\n- 深度迁移算子学习（Deep Transfer Operator Learning）\n- 上下文算子学习（In-context Operator Learning）\n局限：仅在**同维度、同网格**任务间迁移，未解决异构数据。\n3. 近期PDE基础模型（Foundation Models）\n\n| 模型 | 可处理维度 | 预训练数据 | 关键限制 |\n| --- | --- | --- | --- |\n| MPP | 仅2D | 2D多物理 | 固定2D网格，无法1D/3D |\n| DPOT | 仅2D | 2D+时序10步 | 需10步上下文，不支持3D |\n| POSEIDON | 仅2D | 2D多尺度 | 需Swin窗口，形状固定 |\n| PDEformer | 仅1D | 1D方程 | 无法处理2D/3D |\n| 共同瓶颈： |\n\n- 维度、分辨率、标量/矢量场**需重新设计架构**；\n- 3D体积数据导致序列长度爆炸，被显式排除在预训练之外；\n- 1D实验信号（探针、地震道）被强制填充到2D，浪费算力。\n4. 与MORPH最相关的“异构”讨论\n\n- Ye et al. 2024、McCabe et al. 2024 均指出“**扩展到3D及混合维度**”是开放难题；\n- MORPH首次把**1D–3D、任意分辨率、混合场**统一进**同一套权重**，并通过轴向注意力将复杂度从 O((TDHW)^2) 降至 O(T^2+D^2+H^2+W^2) ，填补了这一空白。\n\n简言之，既有研究要么“专模专用”，要么“二维打天下”，而MORPH首次系统性地把**形状无关、多维度、多物理、部分可观测**作为第一优先级设计目标。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“一格式、三机制、四变体”的系统设计，把异构、部分可观测的 PDE 数据塞进同一个自回归 Vision Transformer，实现“任何维度、任何分辨率、任何场类型”端到端预训练与低成本迁移。具体方案如下：\n\n1\\. 统一格式：UPTF-7\n\n- 语义保留的 7D 张量模板\n (N,T,F,C,D,H,W) \n-  F ：物理场数目（可运行时变化）\n-  C ：场分量数（标量=1，矢量≥1；混合时广播到最大  C ）\n- 零填充仅发生在**GPU 端 mini-batch 加载瞬间**，原始 TB 级数据仍按原生格式存盘，避免存储爆炸。\n- 结果：1D 探针、2D 切片、3D 体数据可同批训练，无需改图、改代码或重采样。\n\n2\\. 三大架构机制\n\n| 机制 | 作用 | 实现要点 | 复杂度/优势 |\n| --- | --- | --- | --- |\n| Component-wise 3D 卷积 | 局部空间关联+标/矢混合 | 只在  C  维滑动，输出  F  张特征图； 1×1×1  升维→ 3×3×3  深度卷积 | 参数随  F  线性增长，诱导局部偏置，提升小样本效率 |\n| Inter-field Cross-Attention | 跨物理场信息筛选与融合 | 单查询向量  q∈R^(E)  对  F  个场做 MH-CrossAttn，输出单一融合场 |  O(F)  替代  O(F^2) ，抑制尺度失配带来的伪激活；运行时场数可变 |\n| 4D 轴向注意力 | 全局时空依赖，内存可控 | 将完整自注意力拆成四支独立 1D-MHA：时间、深度、高度、宽度；残差相加 | 复杂度从  O((TDHW)^2)  降到  Ol(T^2+D^2+H^2+W^2r) ，支持 3D 高分辨批量训练 |\n\n3\\. 参数高效迁移：LoRA\n\n- 仅对 Attention 与 MLP 的稠密矩阵引入低秩分解\n W'=W_0+(α) / (r)BA,;A∈R^(r× k),B∈R^(d× r) \n- MORPH-L（480 M）中仅 77 M 可训练，即可匹配全参数微调效果，首次把 LLM 的“冻结主干+小插件”范式搬到 PDE 基础模型。\n\n4\\. 四尺度模型变体\n\n| 模型 | 参数量 | 注意力维度 | 头数 | 深度 | 适用场景 |\n| --- | --- | --- | --- | --- | --- |\n| TI (Tiny) | 7 M | 256 | 4 | 4 | 单卡/快速实验 |\n| S (Small) | 30 M | 512 | 8 | 4 | 单卡/生产 |\n| M (Medium) | 126 M | 768 | 12 | 8 | 多卡 |\n| L (Large) | 480 M | 1024 | 16 | 16 | 多节点+LoRA |\n\n5\\. 训练与数据策略\n\n- **平衡任务采样**：按轨迹数  N_i  反比加权，避免小数据集被遗忘。\n- **ReVIN 归一化**：预计算全局均值/方差，训练与推理阶段可逆，消除分布漂移。\n- **流式+自定义分片**：六数据集并行 DataLoader，DDP 下按样本级取模分片，支持 TB 级数据外存训练。\n- **AR(1) 自回归**：严格遵循初值问题语义，减少曝光偏差；也可扩展为 AR(p) 或变阶。\n\n6\\. 效果验证\n\n- **零样本**：在预训练集上直接推理，平均误差优于或接近专用模型。\n- **全样本微调**：7 个下游任务全部刷新 SOTA（NRMSE/VRMSE）。\n- **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 256 条轨迹（<1 %）即可打败全量训练的独立模型。\n- **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调 200 epoch，MSE/RMSE 低于 122 M 的 DPOT-FM-M 与 158 M 的 POSEIDON-FM-B。\n\n综上，论文用“UPTF-7 统一表示 + 卷积-交叉-轴向三重高效算子 + LoRA 插件式迁移”组合拳，把以往只能分而治之的 1D/2D/3D、标量/矢量、多物理问题纳入**同一套权重**，在数据与算力双重稀缺场景下仍能实现跨物理、跨维度、跨分辨率的强泛化，从而系统性地解决了 PDE 基础模型的异构与部分可观测难题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“**异构预训练 → 零样本/少样本迁移 → 数据与算力稀缺场景**”设计了四层实验，覆盖 13 个数据集、4 个模型尺度、3 类对照组。核心结果汇总如下（行内公式统一用  ...  格式）。\n\n1\\. 预训练（Pretraining）\n\n**目的**：验证 MORPH 能否在 6 类异构数据上同时收敛且不遗忘。\n\n| 数据集 | 维度 | 场类型 | 轨迹数 | 时间步 | 空间分辨率 |\n| --- | --- | --- | --- | --- | --- |\n| 1D-CFD | 1D | 1×矢量+2×标量 | 10 k | 100 | 1024 |\n| 2D-DR | 2D | 2×标量 | 1 k | 100 |  128^2  |\n| 2D-CFD-IC | 2D | 1×矢量 | 80 | 1000 |  512^2  |\n| 2D-SW | 2D | 1×标量 | 1 k | 100 |  128^2  |\n| 3D-CFD | 3D | 1×矢量+2×标量 | 200 | 21 |  128^3  |\n| 3D-MHD | 3D | 2×矢量+1×标量 | 97 | 100 |  64^3  |\n\n- **训练步数**：TI/S 200k，M 100k，L 125k 梯度步。\n- **采样策略**：按轨迹数  N_i  反比加权，3D-MHD 被采样概率 0.31，1D-CFD 0.08。\n- **监控指标**：训练/验证 MSE，无早期停止（loss 仍在下降）。\n\n2\\. 零样本评估（Zero-shot）\n\n**设置**：直接用预训练权重在预训练集上做单步预测，不更新参数。\n\n| 指标 | 1D-CFD | 2D-DR | 2D-CFD-IC | 2D-SW | 3D-CFD | 3D-MHD |\n| --- | --- | --- | --- | --- | --- | --- |\n| NRMSE ↓  | 0.0506 | 0.1112 | 0.0858 | 0.0045 | 0.1161 | – |\n| VRMSE ↓  | – | – | – | – | – | 0.285 |\n\n- MORPH-FM-S 在 4/6 数据集上**优于或接近**专用 standalone 模型；\n- 首次展示 1D→3D 跨维度零样本推理能力（对比基线只能 2D）。\n\n3\\. 下游微调（Full-shot Fine-tuning）\n\n**7 个新物理任务**，100–150 epoch，全参数或 LoRA。\n\n| 数据集 | 维度 | 任务特色 | 最优结果 |\n| --- | --- | --- | --- |\n| 1D-DR | 1D | 反应-扩散前沿 | NRMSE= 1.25×10^(-3) （TI） |\n| 1D-BE | 1D | Burgers 激波 | NRMSE=0.0302（S） |\n| 2D-CFD | 2D | 可压缩 NS  512^2  | NRMSE=0.0423（L-LoRA） |\n| 2D-GSDR | 2D | Gray-Scott 斑图 | VRMSE=0.00725（S） |\n| 2D-FNS-KF | 2D | Kolmogorov 强迫湍流 | MSE=0.00089（S） |\n| 3D-CFD-Turb | 3D | 湍流  64^3  | NRMSE=0.0713（M） |\n| 3D-TGC | 3D | 自引力冷却 | VRMSE=0.0395（L-LoRA） |\n\n- **全部 7 项**均取得**新 SOTA**（对比 FNO、U-Net、MPP、DPOT、POSEIDON）。\n- LoRA 仅调 77 M 参数即超越全参数 M 模型，验证参数高效迁移。\n\n4\\. 数据与算力稀缺实验\n\n4.1 少数据\n\n- **1D-DR**：用 25 % 轨迹微调 100 epoch，RMSE 已**低于**全量训练的 standalone-TI。\n- **2D-FNS-KF**：用 256 条轨迹（<1 %）微调 100 epoch，RMSE 同样**低于**全量 standalone-S。\n\n4.2 少算力\n\n- 固定 128 条轨迹、200 epoch，对比同等微调预算下的 SOTA 基础模型：\n\n| 模型 | 参数量 | MSE | RMSE |\n| --- | --- | --- | --- |\n| DPOT-FM-M | 122 M | 0.0301 | 0.176 |\n| POSEIDON-FM-B | 158 M | 0.0017 | 0.0412 |\n| MORPH-FM-S | 30 M | 0.00162 | 0.0401 |\n\n- **4× 更轻参数**仍取得最低误差，证明在**数据+算力双稀缺**场景优势最大。\n\n5\\. 长时程自回归 rollout\n\n- **Shallow-Water 10 步**：MORPH-SS-S NRMSE 比 TI 低 28 %，无发散。\n- **FNS-KF 10 步**：MORPH-FM-S 每步 MSE 均低于 FM-TI，误差累积缓慢。\n\n6\\. 消融与系统实验\n\n- \\*\\* patch size\\*\\*：8×8×8 在 3D 显存与精度间最佳；降至 4×4×4 序列长度翻倍，训练时间↑2.3×，收益<2 %。\n- **轴向 vs 全时空注意力**：3D-CFD batch 下显存节省 5.7×，速度↑3.2×，NRMSE 差距<1 %。\n- **平衡采样 vs 均匀采样**：无加权时 3D-MHD NRMSE 恶化 18 %，证实反比加权必要性。\n\n7\\. 可重复性\n\n- 代码、权重、训练日志已开源（GitHub + HuggingFace）。\n- 提供 PyTorch 数据加载、ReVIN 统计、LoRA 配置、rollout 脚本，确保指标可复现。\n\n综上，实验从**预训练收敛 → 零样本泛化 → 全量微调 SOTA → 数据/算力稀缺优势 → 长时稳定性**五个层面，系统验证了 MORPH 在异构 PDE 数据上的通用性与实用性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 MORPH 框架的“直接延伸”或“范式升级”，均围绕其**形状无关、部分可观测、参数高效**三大特性展开，具有明确的研究价值与可行性。\n\n1\\. 维度与几何外推\n\n- **非笛卡尔网格**：曲线坐标、 immersed boundary、 unstructured tet-grid。\n→ 把轴向注意力泛化到“图-轴向混合”：先按节点局部连通做图注意力，再沿用户自定义的“几何轴”做轴向注意力，保持  O(N)  复杂度。\n- **4D+ 时空-参数空间**：将 PDE 参数  (μ)  视为第 5 维，实现“一推理直接扫参数平面”，迈向**超参数快速寻优**。\n\n2\\. 物理一致性层\n\n- **守恒-辛-对称嵌入**：在轴向注意力后接入**显式守恒投影层**（ rho,v,E  Moment Constraint）或**辛子网络**，确保 rollout 全程质量、动量、能量误差有界。\n- **可微分边界条件**：把 BC 编码为可学习的软约束项  L_(BC) = ∫_(∂Omega) |u_θ(x,t) - g(x,t)|^2 ，随注意力反向传播，实现“零样本”边界迁移。\n\n3\\. 多保真与实验-仿真融合\n\n- **保真度轴 (Fidelity Axis)**：在 UPTF-7 再增一维  M （Mesh-level 或 Noise-level），同批次混入低分辨/含噪实验信号与高分辨仿真，用**轴向注意力天然的分支结构**做保真度-自适应加权。\n- **物理-数据双驱动微调**：当实验只测到 1D 探针时，把仿真 3D 场作为 teacher，通过**轴向知识蒸馏**让模型在探针位置输出与实验一致，同时保持 3D 场预测合理。\n\n4\\. 参数高效范式升级\n\n- **LoRA 组合路由**：\n- 冻结共享主干，为每个物理系**自动学习路由权重**  w_i  组合多个 LoRA 插件：\n\nDelta W = ∑_(i=1)^k w_i (α_i) / (r_i) B_i A_i\n\n- 实现“一个主干 + 数十个插件”覆盖全物理库，插件总大小 <10 % 主干。\n- **渐进式扩展**：\n- 先训练 7 M 模型 → 用 function-preserving 放大到 30 M → 126 M → 480 M，对比“直接训练大模型”在数据-算力稀缺时的样本效率提升。\n\n5\\. 长时程与混沌行为\n\n- **Koopman 轴向注意力**：在时间轴引入**线性 Koopman 层**  z_(t+1) = K z_t ，再残差接非线性轴向注意力，缓解多步 rollout 的 Lyapunov 漂移。\n- **自回归策略搜索**：目前固定 AR(1)。可探索**可变阶调度**  p_t sim π_θ(error_t) ，用强化学习最小化累积误差，实现“自适应步长”。\n\n6\\. 逆向与不确定性\n\n- **Bayes-LoRA**：把 LoRA 矩阵  A,B  设为**随机变量**，采用 SVGD 或 NF 推断后验，实现**轻量级不确定性量化**；仅需  2r(d+k)  个采样参数而非全模型。\n- **稀疏观测反演**：利用形状无关编码器，把稀疏探针数据直接映射到潜码，再解码完整 3D 场，实现**实验-在线重构**。\n\n7\\. 跨模态与多物理新基准\n\n- **实验-仿真混合基准**：发布一套同时包含\n- 1D 探针（压力-时序）\n- 2D PIV 切片\n- 3D LES 仿真\n的公开数据集，推动社区研究“部分可观测”设定。\n- **多物理耦合新任务**：\n- 辐射-磁流体 (radiation-MHD)\n- 流-固-热耦合\n- 等离子体-壁面相互作用\n验证 MORPH 在“真实多物理”下的零样本能力。\n\n8\\. 系统与硬件协同\n\n- **轴向注意力核融合**：将四维轴向 MHA 写成单一 CUDA kernel，利用共享内存缓存  D,H,W  切片，实测可再提速 1.8–2.2×。\n- **边缘部署**：把 TI 模型 (<30 MB) 编译到 TensorRT-Lite，在 NVIDIA Jetson 上实现**毫秒级 128³ 3D 场在线预测**，服务实验闭环控制。\n\n9\\. 理论基础\n\n- **形状无关通用逼近定理**：证明对任意  d∈1,2,3  与任意可测域  Omega⊂R^d ，MORPH 编码器-解码器对  M_θ: L^2(Omega)to L^2(Omega)  在恰当 Sobolev 范数下稠密。\n- **轴向注意力复杂度-精度权衡**：给出  O(T^2+D^2+H^2+W^2)  与 full  O((TDHW)^2)  的谱范数误差上界，指导用户按硬件自动选择分支数。\n\n10\\. 伦理与双用\n\n- **湍流武器化风险**：建立“物理模型权重出口”审查清单，对高保真 3D 可压缩 NS 权重加**水印+差分隐私**，防止直接用于超高速武器设计。\n\n简言之，MORPH 把“维度墙”和“数据墙”打通后，下一步可在**几何、物理、不确定性、系统、理论**五个层面继续深挖，形成“**形状无关+物理严格+参数插件+边缘实时**”的下一代科学基础模型生态。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **MORPH**——首个**形状无关**的偏微分方程（PDE）基础模型，用统一架构端到端处理 **1D–3D、任意分辨率、混合标量/矢量场**的异构科学数据，并在**零样本、全样本、数据-算力稀缺**场景下均取得 SOTA 或可比性能。核心内容可概括为 **“一格式、三机制、四变体、五验证”**：\n\n1️⃣ 一格式：UPTF-7\n\n- 7D 张量  (N,T,F,C,D,H,W)  把任意 PDE 数据语义化封装，**磁盘保持原生格式**，GPU 端按需零填充，解决 TB 级存储与维度差异。\n\n2️⃣ 三机制\n\n| 机制 | 作用 | 复杂度 |\n| --- | --- | --- |\n| Component-wise 3D 卷积 | 联合提取标/矢量局部特征 |  O(F)  |\n| Inter-field Cross-Attention | 跨物理场选择性融合为单一场 |  O(F)  |\n| 4D 轴向注意力 | 时间+空间三轴分别 1D-MHA，全局感受野 |  O(T^2+D^2+H^2+W^2)  |\n\n3️⃣ 四变体\n\n| 模型 | 参数量 | 适用 |\n| --- | --- | --- |\n| TI | 7 M | 单卡快速实验 |\n| S | 30 M | 生产级 |\n| M | 126 M | 多卡 |\n| L | 480 M | 多节点+LoRA |\n\n4️⃣ 五验证\n\n1. **异构预训练**：6 数据集（1D–3D）同时收敛，无灾难遗忘。\n2. **零样本推理**：在预训练集直接推理，**4/6 任务**优于专用模型。\n3. **下游微调**：7 项新物理任务**全部刷新 SOTA**（NRMSE/VRMSE）。\n4. **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 <1 % 数据即**超越全量训练模型**。\n5. **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调下，MSE/RMSE **低于 122 M 的 DPOT 与 158 M 的 POSEIDON**。\n\n5️⃣ 贡献一句话\n\nMORPH 用**统一架构**打通 1D–3D、任意分辨率、多物理场的壁垒，首次在**部分可观测、TB 级异构数据**上实现**低成本预训练+参数高效迁移**，为科学机器学习提供**通用、轻量、强泛化**的 PDE 基础模型基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MORPH%3A%20Shape-agnostic%20PDE%20Foundation%20Models，链接是https://arxiv.org/pdf/2509.21670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.21670。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22141v1-1769990225065","title":"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data","description":"In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.","url":"https://arxiv.org/abs/2601.22141v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:56:41.000Z","metadata":{"arxivId":"2601.22141v1","authors":"Grzegorz Stefanski, Alberto Presta, Michal Byra","categories":"cs.AI, cs.CV, cs.LG","published":"2026-01-29T18:56:41Z","pdfUrl":"https://arxiv.org/pdf/2601.22141v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22141v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22141v1","rank":6,"domain":"Agent","zh_summary":"在剪枝中，彩票假说（Lottery Ticket Hypothesis）提出，大型网络包含稀疏子网络，或者称为“中奖票”，这些子网络可以单独训练，以匹配密集网络的性能。然而，大多数现有方法假设所有输入共享一个通用的中奖票，忽略了现实世界数据的固有异质性。在本工作中，我们提出了“路由彩票”（Routing the Lottery, RTL）——一种自适应剪枝框架，它可以发现多种专用子网络，称为自适应票，每个子网络针对特定类别、语义簇或环境条件进行优化。在多样的数据集和任务中，RTL在平衡精度和召回率上始终优于单模型和多模型基线，同时使用的参数最多比独立模型少10倍，并表现出语义对齐性。此外，我们识别了子网络崩塌现象，即在激进剪枝下性能下降，并引入了子网络相似度评分，从而实现无需标签即可诊断过度稀疏化问题。总体而言，我们的结果将剪枝重新定义为一种使模型结构与数据异质性对齐的机制，为更加模块化和上下文感知的深度学习奠定了基础。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对“彩票假设（Lottery Ticket Hypothesis, LTH）”在真实世界数据上的局限：现有剪枝方法几乎都为所有输入共享同一张全局稀疏掩码，忽视数据本身的异质性。为此，作者提出 Routing the Lottery（RTL）框架，核心目标可归纳为：\n\n- **突破单一全局掩码**：不再寻找“一张通用彩票”，而是为不同类别、语义簇或环境条件发现多张**专用子网络**（adaptive tickets）。\n- **用剪枝实现动态特化**：仅通过剪枝、不引入额外路由网络或参数，即可让不同数据子集激活不同稀疏路径，实现结构级、上下文感知的推理。\n- **兼顾性能与效率**：在参数规模比独立模型少一个数量级的前提下，取得优于单掩码及多模型基线的平衡准确率与召回率。\n- **提供无标签诊断工具**：揭示“子网络塌陷”现象，并引入掩码相似度指标，可在无标签条件下预判过度稀疏化导致的性能骤降。\n\n简言之，论文将剪枝从“静态压缩”重塑为“对齐数据异构性的动态机制”，为构建模块化、可解释且轻量的深度学习模型提供新途径。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统回顾了与“ Routing the Lottery（RTL）”密切相关的四条研究脉络，可概括为：\n\n- **经典与最新剪枝**\n- 早期基于敏感度：Optimal Brain Damage / Surgeon（LeCun et al. 1989；Hassibi et al. 1993）\n- 非结构化剪枝：Han et al. 2015；Yang et al. 2017, 2018（能耗感知）\n- 正则化与稀疏训练：Sreenivasan et al. 2022；Tartaglione et al. 2020, 2022；Benbaki et al. 2023\n- 理论极限：Zhang et al. 2024（权重幅值+网络锐度决定剪枝极限）\n- 专用场景：LLM 剪枝（Frantar & Alistarh 2023；Sun et al. 2023；Lu et al. 2024；Wei et al. 2024）、CNN 剪枝（Zhao et al. 2023）、脉冲网络（Shi et al. 2024）\n- **彩票假设（LTH）及其扩展**\n- 原始 IMP：Frankle & Carbin 2019\n- 初始化/重绕：Frankle et al. 2021；Renda et al. 2020\n- 理论分析：Tartaglione 2022；Burkholz et al. 2021；Sakamoto & Sato 2022；Paul et al. 2022\n- 算法加速：Wang et al. 2023；Lin et al. 2023\n→ **共同点**：均寻找**单张全局掩码**，未考虑数据异构性。\n- **动态稀疏训练**\nSET（Mocanu et al. 2017）、SNFS（Dettmers & Zettlemoyer 2020）、RigL（Evci et al. 2020）在训练过程中**重新生长**连接，但仍维护**单个子网络**，未显式特化不同数据子集。\n\n- **条件计算与混合专家（MoE）**\n- 稀疏门控 MoE（Shazeer et al. 2017；Fedus et al. 2022）\n- 条件卷积（Yang et al. 2019）\n→ 依赖**额外路由网络**与大量辅助参数，计算开销高；RTL 与之正交：仅用剪枝+掩码路由，无需新增参数或复杂门控。\n\n综上，现有工作要么固守“单掩码”假设，要么引入繁重路由机制；RTL 首次将“多专子网络”理念引入纯剪枝框架，填补了“数据异构-结构适配”这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“数据异构→单一掩码性能受限”这一核心瓶颈转化为\\*\\*“联合发现多张专用稀疏子网络”\\*\\*的优化问题，并给出完整实现流程。关键步骤与对应设计如下：\n\n1\\. 问题重定义：从单掩码到掩码集合\n\n- 给定数据集  D  被划分为  K  个子集  D_k_(k=1)^K （类别、语义簇或环境）。\n- 目标：联合学习  K  个二元掩码  m_k ，使得\n\nmin_(m_k) ∑_(k=1)^K mathbb E_((x,y)sim D_k)[ell(y,f(x;m_kodotθ))] quads.t.quad |m_k|_0le s\n\n即**每个子集  D_k  拥有自己稀疏度上限  s  的专用子网络**，而所有子网络共享同一初始化参数  θ 。\n\n2\\. 自适应“彩票”提取（Adaptive Ticket Extraction）\n\n- **逐簇迭代 magnitude pruning**（算法 1）：\n1. 对子集  d_k  训练  T  步得到临时权重  θ_T^((k)) ；\n2. 按幅度剪去比例  p  的权重，生成  m_k ；\n3. 权重**回滚**到共享初始  θ_0 ，保留掩码；\n4. 循环直到所有  m_k  达到目标稀疏度  s 。\n- **效果**：每张“彩票”都在同一初始化基础上、仅针对对应数据分布被“刮开”，天然具备**参数共享+结构特化**双重属性。\n\n3\\. 联合重训练（Joint Retraining）\n\n- **同步小批量采样**：对大小不同的  D_k  进行循环补全，保证每 epoch 各子网络梯度更新次数相同。\n- **掩码梯度屏蔽**（公式 (7)）：\n\nθ arrow θ - eta · (∇_θ L(f(x;m_kodotθ),y)odot m_k)\n\n仅更新  m_k  保留的权重，避免子网络间互相覆盖或灾难性遗忘。\n- **结果**：在**单一张密集参数张量  θ ** 上同时精炼  K  条稀疏路径，实现**推理时按上下文（标签/环境）直接选掩码**，无需额外路由网络。\n\n4\\. 无标签诊断：子网络塌陷预警\n\n- 定义掩码 Jaccard 相似度\n\nJ(m_i,m_j)=(|m_i∩ m_j|) / (|m_i∪ m_j|)\n\n- 实验发现：当  J  随剪枝率急剧上升时，各子网络开始共享大量权重→**结构特化消失→性能骤降**（称之“子网络塌陷”）。\n- **用途**：无需标签即可通过监控  J  判断是否过度稀疏，指导早停或调整稀疏预算。\n\n5\\. 端到端总结\n\n1. **划分数据**→2. **逐簇剪+回滚**得  K  张掩码→3. **联合重训练**共享权重→4. **推理时按上下文选掩码**→5. **用掩码相似度监控塌陷**。\n\n通过上述流程，RTL 仅利用**剪枝+掩码路由**便让不同输入走不同稀疏路径，实现“结构即路由”的轻量级异构推理，在参数量少一个数量级的情况下持续超越单掩码/独立模型基线。\n\nQ4: 论文做了哪些实验？\n\n论文从“可控分类→语义聚类→单图语义→真实语音→诊断分析”五个层次递进验证 Routing the Lottery（RTL）的通用性与可解释性。具体实验设置与目的如下：\n\n1 CIFAR-10 类别专用子网络（理想划分）\n\n- **目的**：验证“每类一张彩票”在干净标签下的上限收益。\n- **协议**：10 类各对应 1 个子网络；与 (i) 单全局 IMP 掩码 (ii) 每类独立 IMP（无共享）对比；稀疏度 25 % / 50 % / 75 %。\n- **观测指标**：balanced accuracy / precision / recall / 剩余参数量。\n- **关键结论**：RTL 在所有稀疏度均取得最高 balanced accuracy 与 recall，参数仅为独立模型的 ≈ 1/10。\n\n2 CIFAR-100 语义聚类（噪声划分）\n\n- **目的**：测试 RTL 对“不完美、重叠”数据划分的鲁棒性。\n- **协议**：用 CLIP+UMAP+HDBSCAN 将 100 类聚成 8 个语义簇；其余同 Exp-1。\n- **结论**：即使簇边界模糊，RTL 仍全程领先；召回率优势尤为显著，说明特化对噪声划分仍有效。\n\n3 隐式神经表示（INR）——**单图内部语义区域**\n\n- **目的**：验证 RTL 可细化到“**一张图像内的不同语义区域**”。\n- **任务**：用坐标 MLP 将连续 (x,y) 映射到 RGB，重建 10 张 ADE20K 图像。\n- **划分**：利用语义分割掩码，每幅图按“对象类别”定义 K 个子集（4–15 类）。\n- **指标**：平均 PSNR；参数计数。\n- **结论**：25 %/50 %/75 % 稀疏下 RTL 分别领先单掩码 IMP ≈ +2.9 dB / +2.5 dB / +2.2 dB；可视化显示边界更清晰、颜色更稳定。\n\n4 真实环境语音增强（DNS + TAU 场景）\n\n- **目的**：检验 RTL 在非分类、真实噪声环境下的通用性。\n- **数据**：干净语音（DNS Challenge 2020）× 三种环境噪声（室内、户外、交通，TAU 2020）。\n- **划分**：按噪声场景定义 3 个子集；U-Net 型复数比值掩码网络。\n- **指标**：SI-SNRi；参数量。\n- **结论**：RTL 在所有稀疏度均获最高 SI-SNRi（25 % 时 +0.36 dB 以上），参数仅 32 K，远低于独立模型的 84 K；谱图可视化显示谐波结构保留更好。\n\n5 诊断性分析：子网络塌陷 & 语义对齐\n\n5.1 子网络塌陷\n\n- **方法**：跟踪 CIFAR-10/100 各子网络 balanced accuracy 与平均掩码 Jaccard 相似度随剪枝率变化。\n- **发现**：accuracy 骤降点与 Jaccard 急剧上升点高度重合→“结构重叠”可作为**无标签早停信号**。\n\n5.2 语义对齐\n\n- **方法**：计算 CIFAR-10 类间 WordNet 路径相似度，与深层/浅层掩码相似度做 Spearman 相关。\n- **发现**：\n– 浅层掩码普遍相似（共享低层特征），深层掩码相似度与语义距离正相关；\n– 相关强度随训练进程与稀疏度增加而提升→RTL 剪出的结构**自发编码语义层级**。\n\n总结\n\n实验覆盖**理想标签→模糊聚类→图像内部→真实声学→无标签诊断**，任务模态包括分类、坐标回归、语音增强；指标兼顾精度、参数量、可视化与相关性分析，系统证明 RTL 在性能、效率与可解释性上的优势。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 RTL 框架的自然延伸或深层追问，均直接对应论文已暴露的边界与现象，具备可验证的实验入口：\n\n1 动态路由 vs. 静态掩码\n\n- **问题**：RTL 目前依赖“上下文标签”做硬掩码选择，若环境或类别未知即失效。\n- **探索**：在共享 backbone 上训练一个**无参数开销的轻量门控**（如一次前向后的激活统计），实现**输入驱动的软路由**，比较与固定掩码的精度-延迟折中。\n\n2 稀疏度自动分配\n\n- **问题**：各子网络现用统一稀疏预算  s  ，但不同簇/环境可能天生复杂度不同。\n- **探索**：以掩码相似度上升斜率或验证损失为信号，**逐簇自适应调整目标稀疏度**；可建模为约束优化或强化学习，验证能否在总参数不变下进一步提升平均精度。\n\n3 跨任务迁移与增量簇\n\n- **问题**：现实场景常出现新类别/新噪声环境。\n- **探索**：\n– 保持旧掩码冻结，用**侧向添加的稀疏 Adapter** 学习新簇，检验是否出现“稀疏灾难性遗忘”；\n– 或采用**超网络生成掩码**，以簇嵌入为条件，实现连续增量学习而无需重训练全部子网络。\n\n4 子网络 collapse 的理论阈值\n\n- **问题**：实验观察到 Jaccard 拐点 ≈ 0.6 时性能骤降，但该值是否随架构、任务、优化器而变？\n- **探索**：在随机图与稀疏神经网络理论框架下，推导**保持子网络可区分性的最大允许重叠**；给出与层宽、深度、剪枝因子相关的解析或半解析阈值，为自动早停提供理论依据。\n\n5 与参数高效微调（PEFT）正交组合\n\n- **问题**：大模型时代更关注“如何少调参”。\n- **探索**：将 RTL 的掩码特化思想迁移到 LLM，**仅对每层部分神经元做簇专用剪枝**，其余权重保持预训练不变；对比 LoRA/AdaLoRA，看能否在下游多任务上同时获得稀疏与低秩的复合收益。\n\n6 非语义划分下的鲁棒性极限\n\n- **问题**：目前聚类仍部分依赖语义标签或 CLIP 嵌入。\n- **探索**：用**完全随机或对抗性划分**制造最难场景，测量 RTL 相对单掩码的增益衰减曲线；界定“数据划分质量→RTL 有效性”的临界条件，指导实际部署时的划分策略选择。\n\n7 硬件-感知掩码同步\n\n- **问题**：不同子网络导致不规则稀疏，难以在通用加速器上获得实测加速。\n- **探索**：在掩码提取阶段加入**结构化约束**（block/block-column 稀疏或 N:M 细粒度），检验是否仍能保持 specialization 优势；配合真实 GPU/NNIE 推理库测量端到端延迟与能耗，验证“理论参数少”→“实测速度快”的闭环。\n\n8 语义对齐的逆向利用\n\n- **问题**：深层掩码相似度与 WordNet 距离正相关。\n- **探索**：**反过来用掩码相似度指导零样本/少样本分类**——未见类别的掩码可通过语义近邻插值生成，检验其作为权重先验能否提升低样本场景下的泛化能力。\n\n这些方向既可直接复用 RTL 的代码框架，也能从理论、系统、应用三端填补“稀疏特化”尚存的空白，为后续研究提供可落地的实验入口。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：传统 Lottery Ticket 剪枝只用一张全局稀疏掩码，忽视数据异构，导致在类别、语义或环境差异大的场景下性能受限。\n- **方法**：提出 Routing the Lottery（RTL）——\n1. 将数据划分为 K 个子集（类别/聚类/环境）；\n2. 对每子集独立 magnitude-prune 并回滚到共享初始化，得到 K 张专用掩码；\n3. 联合重训练：在统一参数张量上按掩码屏蔽梯度，仅更新各子网络存活权重；\n4. 推理时按上下文选择对应掩码，实现“结构即路由”的轻量化推断。\n- **发现**：\n– 在 CIFAR-10/100、ADE20K-INR、真实语音增强任务中，RTL 以≈1/10 参数量持续优于单掩码与独立模型；\n– 提出“子网络塌陷”现象：掩码 Jaccard 相似度陡升可作为无标签早停信号；\n– 深层掩码相似度与 WordNet 语义距离正相关，表明剪枝自发编码语义层级。\n- **结论**：RTL 把剪枝从静态压缩重塑为对齐数据异构的动态机制，为构建模块化、可解释且高效的深度模型提供了新路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Routing%20the%20Lottery%3A%20Adaptive%20Subnetworks%20for%20Heterogeneous%20Data，链接是https://arxiv.org/pdf/2601.22141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22141。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22139v1-1769990225065","title":"Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers","description":"Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \\emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\\% higher accuracy, 22.90\\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}","url":"https://arxiv.org/abs/2601.22139v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:56:12.000Z","metadata":{"arxivId":"2601.22139v1","authors":"Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang","categories":"cs.CL, cs.AI","published":"2026-01-29T18:56:12Z","pdfUrl":"https://arxiv.org/pdf/2601.22139v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22139v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22139v1","rank":7,"domain":"Agent","zh_summary":"以推理为导向的大型语言模型（LLMs）在思维链（Chain-of-Thought，CoT）提示方面取得了显著进展，但它们仍然受限于一种\\emph{盲目自我思考}范式：即使关键信息缺失或含糊不清，仍需进行大量内在推理。我们提出了主动交互推理（PIR），这是一种新的推理范式，将大型语言模型从被动的求解者转变为主动探询者，将推理与澄清交织在一起。与主要通过查询外部环境来解决知识不确定性的现有搜索或工具框架不同，PIR通过与用户的直接交互，针对基础和意图层面的不确定性。PIR通过两个核心组件实现：（1）一个具备不确定性意识的监督微调过程，赋予模型交互式推理能力;（2）基于用户模拟器的策略优化框架，由复合奖励驱动，使模型行为与用户意图保持一致。数学推理、代码生成和文档编辑的广泛实验表明，PIR始终优于强基线，准确率提升多达32.70%，通过率提升22.90%，BLEU提升41.36，同时减少近一半的推理计算和不必要的交互回合。对事实知识、问答和缺失前提情景的进一步可靠性评估证实了PIR的强推广性和稳健性。模型和代码公开可访问：\\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对现有推理型大语言模型（reasoning-oriented LLM）在“盲目自思考”（blind self-thinking）范式下的根本缺陷：\n即便用户指令缺失关键前提或存在歧义，模型仍会在内部执行冗长链式思考，导致幻觉、结论错位、算力浪费，并迫使用户事后多轮纠正。\n\n为此，作者提出 Proactive Interactive Reasoning（PIR）新范式，将模型从“被动求解器”转变为“主动询问者”，使其在推理过程中实时检测不确定性并主动向用户发起澄清，从而在源头对齐用户意图，减少无效推理与交互轮次。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为两条主线，均旨在缓解“盲目自思考”带来的幻觉、过思考或意图错位，但各自侧重点与局限性不同：\n\n1. **推理增强型 LLM**\n\n- **代表工作**：GPT-o1、DeepSeek-R1、Search-R1、ToolRL、Graph-R1 等\n- **核心思路**：通过检索、工具调用或外部知识库为中间步骤提供可验证证据，降低事实层幻觉。\n- **与 PIR 的区别**：主要处理**知识不确定性**（knowledge uncertainty），假设用户意图已完整；当歧义源自**前提缺失或意图不明**时，仍会继续无效自思考。\n2. **交互式 / 澄清式 LLM**\n\n- **代表工作**：CLAM、STaR-GATE、CollabLLM、CIRCLE、CLARINET 等\n- **核心思路**：用模板提示、合成歧义数据或强化学习训练模型“何时提问”，以减少多轮纠正。\n- **与 PIR 的区别**：\n1. 多为**被动响应**——等待用户发起纠正，而非在推理链中**主动插入澄清**；\n2. 未将“提问”显式嵌入内部推理流程，导致“为何问、问什么”决策不透明；\n3. 缺乏统一框架同时优化**任务正确性**与**交互效率**。\n\nPIR 在以上两类工作基础上，首次把“意图层不确定性”纳入推理链内部决策，通过不确定性感知的监督微调 + 用户模拟器强化学习，实现“边推理边提问”的主动对齐机制。\n\nQ3: 论文如何解决这个问题？\n\n论文将“盲目自思考”问题形式化为**意图-前提不确定性下的无效推理**，并提出两阶段框架 **PIR（Proactive Interactive Reasoning）**，把模型从“被动求解器”改造成“主动询问者”。核心流程如下：\n\n阶段一：交互能力激活（Interactive Capability Activation）\n\n1. **不确定性感知数据增广**\n用冻结教师模型生成原始 CoT，逐句计算归一化预测熵\n\nPE_(norm)(s_j|x)=-(1) / (|s_j|)∑_i log p(z_i|s_(<j),x)\n\n取 Top-k% 高熵位置作为“澄清点”，插入 `<asking>…</asking>` 问题与 `<response>…</response>` 模拟回答，得到“思考-询问-回应”交错轨迹\n\ny=t_1,(a_1,r_1),dots,t_m,(a_m,r_m),O\n\n2. **冷启动监督微调**\n在增广轨迹上做标准自回归损失\n\nL_(SFT)=-∑_(j=1)^L log p_θ(y_j|x,y_(<j))\n\n使模型学会“何时问、如何问、如何整合回答”。\n\n阶段二：用户意图对齐（User-Intent Alignment）\n\n1. **用户模拟器驱动的 GRPO（US-GRPO）**\n\n- 用指令遵循 LLM 充当**动态用户模拟器**  S ，内置真实意图  I 。\n- 策略  π_θ  联合采样轨迹\n\nysim p(y|x,I)=prod_n π_θ(t_n,a_n|h_n)· S(r_n|a_n,I)\n\n- 采用**无价值函数**的组相对策略优化\n\nJ_(GRPO)(θ)=E_(xsimD)![(1) / (G)∑_(i=1)^G∑_(n=1)^(N_i)π_θ(t_n^((i)),a_n^((i))|h_n^((i)))π_(old)(·)A_i -β D_(KL)(π_θ|π_(ref))]\n\n仅对  (t_n,a_n)  计算梯度，屏蔽  r_n  防止泄漏。\n2. **复合奖励函数**\n总奖励\n\nR(y)=R_(output)(o,g)+R_(reason)(r,o,g)\n\n- **输出奖励**：答对才给基线分\n\nR_(output)=S_(base)·I(o=g)\n\n- **推理奖励**：仅当答案正确时生效，鼓励**有用且高效**的澄清\n\nR_(reason)=I(o=g)·I_(ask)![S_(base)· E(r)· H_(LLM)(r)]\n\n其中\n\nE(r)=N_(max)-nN_(max)-1quad(交互效率)\n\n H_(LLM)(r)  由强 LLM 评判澄清质量，\n0,1\n 归一化。\n\n效果\n\n- 数学、代码、文档编辑三类交互基准上，**准确率↑ 9.8–32.7%**，**token 消耗↓ ~2 k/任务**，**无效交互轮次减半**。\n- 在缺失前提测试（MIP）等零样本场景，**准确率↑ 1.6–2.9×**，验证“先问后推”可泛化到非交互环境。\n\nQ4: 论文做了哪些实验？\n\n论文在**交互式场景**与**非交互式场景**两大维度共 8 个基准上进行了系统实验，量化验证 PIR 对“盲目自思考”的缓解效果与泛化能力。主要实验一览如下（按场景归类）：\n\n1\\. 交互式任务实验（3 个基准）\n\n| 数据集 | 任务类型 | 评价指标 | 主要结果 |\n| --- | --- | --- | --- |\n| MATH-Chat | 多轮数学解题 | Accuracy (ACC) + Tokens + TTR + Help. | 21.30→32.70 (+11.4 pp) |\n| BigCodeBench-Chat | 多轮代码生成 | Pass Rate (PR) + Tokens + TTR + Help. | 19.70→22.90 (+3.2 pp) |\n| MediumDocEdit-Chat | 多轮文档编辑 | BLEU + Tokens + TTR + Help. | 28.00→41.36 (+13.36 BLEU) |\n\n- **对比基线**：Instruct LLM、Proactive Prompt、STaR-GATE、CollabLLM 以及消融版 PIR（仅 SFT 或仅 RL）。\n- **关键发现**：\n- PIR 平均节省约 **2 k tokens/任务**，**交互轮次减半**（TTR↓ 50%）。\n- US-GRPO 阶段相对纯 SFT 在真实交互环境提升 **>10 pp**，证明强化学习对“问得准”至关重要。\n\n2\\. 用户模拟器与奖励消融（基于 MATH-Chat）\n\n| 变量 | 设定 | 结果摘要 |\n| --- | --- | --- |\n| 模拟器质量 | Llama-3.1-8B ↔ GPT-4o-mini | ACC 32.7→34.0，TTR 1.8→2.3，问题信息量↑ |\n| 奖励组分 | 去输出/去推理/仅效率/仅有用 | 复合奖励平衡后取得最佳 ACC-TTR 折中 |\n\n3\\. 非交互式泛化实验（5 个基准，零样本 CoT）\n\n| 场景 | 数据集 | 指标 | 结果亮点 |\n| --- | --- | --- | --- |\n| 事实知识 | MMLU / MMLU-Pro | EM | 60.1→62.5（小幅↑，知识已内禀） |\n| 开放问答 | TriviaQA / SQuAD | EM | 20.1→45.5（↑ 26.7 pp，需外部信息） |\n| 缺失前提 | MIP-GSM8K / MIP-MATH | ACC | 8.6→17.4 / 7.7→25.0（↑ 2-3×，显著抗幻觉） |\n\n- **对比对象**：DeepSeek-R1-Distill-7B、GPT-4o-mini。\n- **结论**：PIR 学会**按需提问**——当参数知识足够时克制交互；当上下文/前提缺失时主动澄清，从而在不同不确定性结构下均表现稳健。\n\n4\\. 案例可视化（Case Study）\n\n- 数学题缺失方程、代码生成需求模糊两个实例显示：\n- 传统推理模型**盲推 2000 tokens**后给出错位答案；\n- PIR 在\\*\\*<300 tokens\\*\\*内先问清关键前提，再精准求解，用户体验与计算效率同步提升。\n\n综上，实验覆盖**交互-非交互**、**不同不确定性来源**、**多任务多指标**，一致表明 PIR 能在**提升准确率**的同时**降低推理与交互成本**，并具备跨场景泛化能力。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 PIR 框架的直接延伸或深层扩展，均围绕“主动交互式推理”尚未充分解决的挑战展开：\n\n1. **真实人类交互漂移**\n\n- 当前用户模拟器基于 LLM，分布与真实人群存在差距（语言噪声、动态意图、少数群体行为）。\n- 可探索：\n- 在线强化学习（on-policy RL）用真实用户反馈持续微调；\n- 分层模拟器（ persona + 意图 + 噪声）+ 对抗式训练，提升分布外鲁棒性。\n2. **安全与价值对齐**\n\n- PIR 未经过暴力、自残、仇恨言论等安全筛选；主动提问可能诱导用户泄露敏感信息。\n- 可探索：\n- 将安全策略嵌入提问策略空间，形成“安全-提问”联合策略；\n- 引入 Constitutional AI / RLHF 约束，对澄清问题进行实时毒性过滤。\n3. **多模态交互**\n\n- 文本-only 澄清限制了对图表、公式、代码截图等歧义源的探测。\n- 可探索：\n- 视觉-语言 PIR：在 VLM 内部推理链中插入 `<asking visual=\"...\">` 节点，支持用户圈画或指代图像区域；\n- 统一熵度量：跨文本、图像、结构化数据计算一致的不确定性信号。\n4. **提问成本与信息价值权衡**\n\n- 复合奖励虽兼顾效率与有用性，但仍是人工加权；不同任务、不同用户偏好下的 Pareto 前沿尚不清晰。\n- 可探索：\n- 基于信息论的价值函数  V(a) = I(Answer; User Reply | a) ，动态决定“是否值得问”；\n- 元强化学习（Meta-RL）快速适配个人提问预算与耐心系数。\n5. **长程记忆与持续一致性**\n\n- 多轮澄清后，模型需追踪累积约束并在后续推理中一致应用。\n- 可探索：\n- 将交互历史压缩为可检索的“约束向量库”，用记忆增强 Transformer 或检索-推理分离架构；\n- 引入逻辑一致性检查器（theorem verifier / code interpreter）对每条新约束做可满足性验证。\n6. **异构工具链下的统一提问**\n\n- 当模型可调用搜索引擎、计算器、API 时，不确定性来源进一步细分（知识 vs 工具 vs 意图）。\n- 可探索：\n- 构建“工具-意图”双层不确定性检测器，先决定“问用户”还是“查工具”；\n- 用分层强化学习（HRL）将“提问动作”与“工具调用动作”置于同一动作空间联合优化。\n7. **可解释性与可控性**\n\n- 当前模型以隐式熵值触发提问，缺乏人类可读的解释。\n- 可探索：\n- 生成“提问理由”子句，用自然语言或形式化规格说明缺失前提；\n- 允许用户直接编辑触发规则（如“当涉及医疗剂量时请一定问我”），实现个性化策略注入。\n8. **低资源语言与领域迁移**\n\n- 实验仅覆盖英文数学/代码/编辑场景。\n- 可探索：\n- 跨语言不确定性对齐：高资源语言训练提问策略，低资源语言做零样本迁移；\n- 领域专用模拟器（法律、医疗、金融）+ 小样本 RL，检验提问策略是否需重训练或仅需 prompt 调节。\n\n通过上述方向的深入，可逐步把 PIR 从“文本-英文-安全未审”的实验室原型，推向“多模态、多语言、强安全、个性化”的下一代人机协同推理基础模型。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Proactive Interactive Reasoning（PIR）**，把推理型大语言模型从“盲目自思考”转变为“边推理边主动提问”，解决因用户意图/前提缺失导致的幻觉、过思考与交互低效问题。核心内容可概括为：\n\n1\\. 问题与动机\n\n- 现有推理 LLM 依赖长链 CoT，即使信息不足也持续内耗，造成**算力浪费+结论错位**。\n- 检索/工具增强仅缓解**知识不确定性**，对**意图不确定性**无能为力。\n\n2\\. PIR 框架（两阶段）\n\n① 交互能力激活\n\n- **不确定性感知数据增广**：用预测熵  PE_(norm)  定位高不确定推理步，插入“问-答”对，生成 think-ask-respond 轨迹。\n- **冷启动监督微调**：在增广数据上自回归训练，使模型学会何时问、如何问。\n\n② 用户意图对齐\n\n- **US-GRPO**：用可配置的用户模拟器与组相对策略优化，无需价值函数即可微调提问策略。\n- **复合奖励**： R = R_(output) + R_(reason) ，只在答对时激活效率与有用性奖励，防止幻觉或偷懒。\n\n3\\. 实验结果\n\n| 场景 | 指标提升 | 资源节省 |\n| --- | --- | --- |\n| MATH-Chat | ACC +11.4 pp | tokens −54% |\n| BigCodeBench-Chat | PR +3.2 pp | tokens −50% |\n| DocEdit-Chat | BLEU +13.4 | TTR −50% |\n| 缺失前提测试 | ACC ↑ 2–3× | tokens −60% |\n\n4\\. 贡献\n\n- 首次形式化“盲目自思考”问题并提出 PIR 范式。\n- 提出不确定性感知增广 + US-GRPO 联合优化方法。\n- 在交互与非交互基准上同时实现**更高准确率、更低推理与交互成本**，为下一代人机协同推理模型提供新基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Reasoning%20While%20Asking%3A%20Transforming%20Reasoning%20Large%20Language%20Models%20from%20Passive%20Solvers%20to%20Proactive%20Inquirers，链接是https://arxiv.org/pdf/2601.22139，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22139。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2508.13187v3-1769990225065","title":"\"Not in My Backyard\": LLMs Uncover Online and Offline Social Biases Against Homelessness","description":"Homelessness is a persistent social challenge, impacting millions worldwide. Over 876,000 people experienced homelessness (PEH) in the U.S. in 2025. Social bias is a significant barrier to alleviation, shaping public perception and influencing policymaking. Given that online textual media and offline city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases against PEH. We present a new, manually-annotated multi-domain dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across ten U.S. cities. Our 16-category multi-label taxonomy creates a challenging long-tail classification problem: some categories appear in less than 1% of samples, while others exceed 70%. We find that small human-annotated datasets (1,702 samples) are insufficient for training effective classifiers, whether used to fine-tune encoder models or as few-shot examples for LLMs. To address this, we use GPT-4.1 to generate pseudo-labels on a larger unlabeled corpus. Training on this expanded dataset enables even small encoder models (ModernBERT, 150M parameters) to achieve 35.23 macro-F1, approaching GPT-4.1's 41.57. This demonstrates that \\textbf{data quantity matters more than model size}, enabling low-cost, privacy-preserving deployment without relying on commercial APIs. Our results reveal that negative bias against PEH is prevalent both offline and online (especially on Reddit), with \"not in my backyard\" narratives showing the highest engagement. These findings uncover a type of ostracism that directly impacts poverty-reduction policymaking and provide actionable insights for practitioners addressing homelessness.","url":"https://arxiv.org/abs/2508.13187v3","source":"ArXiv Domain","timestamp":"2025-08-14T17:58:34.000Z","metadata":{"arxivId":"2508.13187v3","authors":"Jonathan A. Karr, Benjamin F. Herbst, Matthew L. Sisk, Xueyun Li, Ting Hua, Matthew Hauenstein, Georgina Curto, Nitesh V. Chawla","categories":"cs.CY, cs.AI, cs.CL","published":"2025-08-14T17:58:34Z","pdfUrl":"https://arxiv.org/pdf/2508.13187v3.pdf","abstractUrl":"https://arxiv.org/abs/2508.13187v3","coolPaperUrl":"https://papers.cool/arxiv/2508.13187v3","rank":8,"domain":"Agent","zh_summary":"无家可归是一个持续存在的社会挑战，影响着全球数百万人。2025年，美国有超过876,000人经历了无家可归（PEH）。社会偏见是缓解措施的重要障碍，塑造公众认知并影响政策制定。鉴于线上文本媒体和线下市议会话语反映了并影响部分公众舆论，这为识别和追踪针对PEH的社会偏见提供了宝贵的见解。我们呈现一个新的、手动注释的多域数据集，汇编自Reddit、X（前身为Twitter）、新闻文章以及美国十个城市的市议会会议记录。我们采用16类多标签分类法，形成了一个具有挑战性的长尾分类问题：有些类别的样本比例低于1%，而另一些则超过70%。我们发现，少量人工注释数据集（1702个样本）不足以训练有效的分类器，无论是用于微调编码器模型还是作为LLM的少数样本样本。为此，我们使用 GPT-4.1 在更大的未标记语料库上生成伪标签。在扩展数据集上训练使得即使是小型编码模型（ModernBERT，1.5亿参数）也能达到35.23宏F1，接近GPT-4.1的41.57。这表明 \\textbf{数据数量比模型大小更重要}，使得低成本、保护隐私的部署成为可能，无需依赖商业 API。我们的结果显示，针对PEH的负面偏见在线下和线上（尤其是在Reddit上）普遍存在，“别在我家后院”的叙事显示了最高的互动。这些发现揭示了一种直接影响减贫政策制定的排斥现象，并为应对无家可归问题的从业者提供了可作的见解。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n这篇论文试图解决的问题是如何利用大型语言模型（LLMs）来识别和量化在线和离线公共话语中的无家可归者（PEH）偏见，并通过影响公众舆论来缓解无家可归问题。具体而言，论文的目标包括：\n\n- 开发新的指标来衡量无家可归偏见，以便为政策制定提供信息。\n- 通过提高公众对无家可归偏见的认识，减少有害偏见的传播。\n- 增强生成性人工智能技术在解决社会问题时的公平性和伦理应用。\n- 探索不同城市和数据源之间的偏见差异，以及这些偏见与实际无家可归水平之间的潜在联系。\n\nQ2: 有哪些相关研究？\n\n论文中提到了以下几类相关研究：\n\n评估和基准测试LLMs作为分类器\n\n- **零样本和少样本学习**：研究评估了LLMs在低资源或新颖场景（如零样本和少样本学习）中的分类能力，包括其准确性、一致性和泛化能力。例如，GLUE和BIG-Bench等基准测试提供了关于核心语言能力的基础见解，这些能力对于分类任务至关重要。\n- **模型公平性和偏见**：HELM评估模型在多个维度上的表现，包括公平性和偏见，而不仅仅是准确性。同时，研究也关注LLMs自身的“固有偏见”，例如代表性偏见和有害内容生成，这些可能会影响分类结果。\n\n社会影响力和政策导向的数据收集\n\n- **NLP在政策分析中的应用**：自然语言处理（NLP）工具被用于解析政治活动、分析立法、跟踪公众情绪和研究政策效果，改变了研究人员和政策制定者处理文本数据的方式。\n- **AI中的偏见缓解**：研究致力于减轻AI系统中的偏见，以确保这些工具能够减少而不是加剧社会差异。\n\n使用AI检测和分类社会偏见\n\n- **在线话语中的偏见检测**：先前的研究评估了LLMs作为分类器在检测针对贫困人群（通常称为恐贫症）的偏见方面的有效性，特别是在在线话语中。例如，国际比较研究揭示了在线公众舆论中对贫困的刑事化现象，还有研究提出了针对恐贫症的全面分类体系。\n- **无家可归偏见的分类**：一些研究展示了LLMs在检测与社会经济因素相关的公众态度变化方面的潜力，例如通过分析推文来发现无家可归人口数量与有害概括之间的相关性。\n\nQ3: 论文如何解决这个问题？\n\n为了解决如何利用大型语言模型（LLMs）来识别和量化无家可归者（PEH）偏见的问题，论文采取了以下步骤：\n\n数据收集与处理\n\n- **多模态数据集构建**：从Reddit、X（前身为Twitter）、新闻文章和美国10个城市的市议会会议记录中收集了2015年至2025年间与无家可归相关的数据，构建了一个多模态数据集。这些数据涵盖了在线和离线的公共话语，为研究提供了丰富的素材。\n- **数据匿名化**：使用spaCy和pydeidentify工具对数据进行匿名化处理，以保护个人隐私，确保数据中不包含个人身份信息（PII）。\n\n偏见分类框架\n\n- **扩展分类标准**：基于先前的研究（如OATH框架），扩展了多模态PEH偏见分类标准，增加了新的分类类别，如“提出一个真诚的问题”、“提出一个修辞问题”、“提供一个事实或主张”等，以更全面地捕捉无家可归偏见的多样性。\n\n模型评估与选择\n\n- **人类标注基线**：由三位人类标注者使用定义的多模态PEH偏见分类类别对数据集进行标注，创建了一个手动标注的基线。通过与南本德市的领域专家密切合作，确保了标注的一致性和准确性。\n- **模型比较**：评估了多种模型，包括本地LLMs（如Llama 3.2 3B Instruct、Qwen 2.5 7B Instruct和Phi4 Instruct Mini）和闭源API模型（如GPT-4.1、Gemini 2.5 Pro和Grok-4），以及BERT模型。通过零样本学习和少样本学习（in-context learning）两种方式来测试这些模型在分类任务中的表现，并与人类标注者的结果进行比较。\n- **性能评估**：使用宏观F1分数（macro-F1）和微观F1分数（micro-F1）来评估模型性能，考虑到数据集中类别不平衡的问题，宏观F1分数能够更公平地评估模型在不同类别上的表现。\n\n结果分析与应用\n\n- **模型选择与应用**：根据模型在加权平均宏观F1分数上的表现，选择了表现最佳的模型（如GPT-4.1）来对整个数据集进行分类，以生成无家可归偏见的分类结果。\n- **偏见分析**：分析了不同城市和不同数据源之间的偏见差异，以及这些偏见与实际无家可归水平之间的潜在联系。通过相关性分析，揭示了不同偏见类别之间的关系，例如“解决方案/干预措施”与“有害概括”之间的负相关性，以及“表达意见”与“应得/不应得”之间的正相关性。\n- **政策建议**：基于分析结果，提出了通过影响公众舆论来缓解无家可归问题的潜在途径，强调了在不同城市和媒体平台上采取针对性干预措施的必要性。\n\nQ4: 论文做了哪些实验？\n\n论文中进行了以下实验：\n\n数据收集与处理\n\n- **多模态数据集构建**：从Reddit、X（前身为Twitter）、新闻文章和美国10个城市的市议会会议记录中收集了2015年至2025年间与无家可归相关的数据，构建了一个多模态数据集。\n- **数据匿名化**：使用spaCy和pydeidentify工具对数据进行匿名化处理，以保护个人隐私，确保数据中不包含个人身份信息（PII）。\n\n偏见分类框架\n\n- **扩展分类标准**：基于先前的研究（如OATH框架），扩展了多模态PEH偏见分类标准，增加了新的分类类别，如“提出一个真诚的问题”、“提出一个修辞问题”、“提供一个事实或主张”等，以更全面地捕捉无家可归偏见的多样性。\n\n模型评估与选择\n\n- **人类标注基线**：由三位人类标注者使用定义的多模态PEH偏见分类类别对数据集进行标注，创建了一个手动标注的基线。通过与南本德市的领域专家密切合作，确保了标注的一致性和准确性。\n- **模型比较**：评估了多种模型，包括本地LLMs（如Llama 3.2 3B Instruct、Qwen 2.5 7B Instruct和Phi4 Instruct Mini）和闭源API模型（如GPT-4.1、Gemini 2.5 Pro和Grok-4），以及BERT模型。通过零样本学习和少样本学习（in-context learning）两种方式来测试这些模型在分类任务中的表现，并与人类标注者的结果进行比较。\n- **性能评估**：使用宏观F1分数（macro-F1）和微观F1分数（micro-F1）来评估模型性能，考虑到数据集中类别不平衡的问题，宏观F1分数能够更公平地评估模型在不同类别上的表现。\n\n结果分析与应用\n\n- **模型选择与应用**：根据模型在加权平均宏观F1分数上的表现，选择了表现最佳的模型（如GPT-4.1）来对整个数据集进行分类，以生成无家可归偏见的分类结果。\n- **偏见分析**：分析了不同城市和不同数据源之间的偏见差异，以及这些偏见与实际无家可归水平之间的潜在联系。通过相关性分析，揭示了不同偏见类别之间的关系，例如“解决方案/干预措施”与“有害概括”之间的负相关性，以及“表达意见”与“应得/不应得”之间的正相关性。\n- **政策建议**：基于分析结果，提出了通过影响公众舆论来缓解无家可归问题的潜在途径，强调了在不同城市和媒体平台上采取针对性干预措施的必要性。\n\n具体实验结果\n\n- **模型性能比较**：在表3中，展示了不同模型在不同数据源上的宏观F1分数和微观F1分数。结果显示，GPT-4.1在加权平均宏观F1分数上表现最佳，因此被选为对整个数据集进行分类的模型。\n- **类别级别的F1分数**：在表4中，展示了GPT-4.1模型在不同类别上的F1分数，包括零样本学习和少样本学习的结果。这些结果表明，少样本学习在某些类别上能够显著提高模型的性能，尤其是在那些模型本身表现不佳的类别上。\n- **相关性分析**：在图2中，展示了不同偏见类别之间的相关性矩阵。分析发现，某些类别之间存在显著的正相关或负相关关系，这些关系可能对政策制定者在制定缓解无家可归问题的策略时具有指导意义。\n- **城市和数据源的偏见差异**：在图3和图4中，分别展示了不同城市和不同数据源之间的偏见差异。结果显示，不同城市和不同媒体平台上的偏见分布存在显著差异，这强调了在制定政策时需要考虑具体的社会和文化背景。\n\nQ5: 有什么可以进一步探索的点？\n\n论文在解决无家可归偏见检测问题上取得了显著进展，但仍有一些可以进一步探索的点，以深化研究并提高解决方案的有效性。以下是一些潜在的研究方向：\n\n数据集扩展与多样性\n\n- **增加数据源**：当前数据集主要来自Reddit、X（前身为Twitter）、新闻文章和市议会会议记录。可以考虑纳入更多类型的数据源，如社交媒体平台（如Facebook、Instagram）、在线论坛、博客、评论区等，以更全面地捕捉无家可归偏见的多样性。\n- **跨文化研究**：目前研究仅限于美国的10个城市。可以扩展到其他国家和地区，以探索不同文化背景下无家可归偏见的表现形式和差异。这将有助于开发更具普适性的偏见检测模型。\n- **时间跨度扩展**：虽然数据集涵盖了2015年至2025年的数据，但可以进一步扩展时间跨度，以观察长期趋势和变化。这有助于理解无家可归偏见如何随时间演变，以及社会事件和政策变化对其的影响。\n\n模型改进与优化\n\n- **多模态融合**：当前研究主要集中在文本数据上。可以探索多模态融合方法，将文本、图像、音频等多种模态的数据结合起来，以更全面地捕捉无家可归偏见。例如，分析与无家可归相关的新闻图片或视频中的偏见。\n- **模型集成**：虽然论文中评估了多种LLMs，但可以进一步探索模型集成方法，结合多个模型的优势，以提高偏见检测的准确性和鲁棒性。\n- **自适应学习**：开发能够自适应学习的模型，以自动调整分类策略，适应不同数据源和城市的特点。这可以通过元学习或强化学习等技术实现。\n\n偏见分类与理解\n\n- **细粒度偏见分类**：当前的偏见分类框架虽然已经较为全面，但仍可以进一步细化。例如，可以增加更多具体的偏见类型，如针对特定群体（如退役军人、残疾人）的偏见，以更精确地捕捉偏见的表现形式。\n- **因果关系分析**：除了相关性分析，可以进一步探索偏见与无家可归水平之间的因果关系。例如，通过因果推断方法，分析某些偏见是否会导致更严重的无家可归问题，或者某些政策是否能够有效减少偏见。\n- **偏见的动态变化**：研究偏见在不同时间尺度上的动态变化，以及这些变化如何受到社会事件、政策变化和公众舆论的影响。这有助于开发能够实时监测和预警偏见变化的系统。\n\n应用与干预\n\n- **实时偏见监测**：开发实时偏见监测系统，能够及时发现和预警无家可归偏见的出现和传播。这可以为政策制定者和社会组织提供及时的信息，以便采取有效的干预措施。\n- **干预策略评估**：评估不同干预策略（如公众教育活动、政策改革、社交媒体宣传等）对减少无家可归偏见的效果。通过实验设计和随机对照试验，确定哪些策略最有效，并为政策制定提供科学依据。\n- **社区参与与合作**：加强与社区组织、非营利机构和政策制定者的合作，将研究成果转化为实际的干预措施。通过社区参与，确保干预措施能够真正满足当地社区的需求，并取得实际效果。\n\n伦理与社会影响\n\n- **伦理审查与透明度**：进一步加强伦理审查，确保数据收集和模型应用符合伦理标准。同时，提高模型的透明度和可解释性，使公众能够理解和信任偏见检测系统。\n- **社会影响评估**：评估偏见检测和干预措施对社会的影响，包括对无家可归者、公众和政策制定者的长期影响。通过社会影响评估，确保研究和应用能够真正促进社会公平和正义。\n\n通过这些进一步的研究方向，可以更全面地理解和解决无家可归偏见问题，为缓解无家可归问题提供更有力的支持。\n\nQ6: 总结一下论文的主要内容\n\n本文的核心内容是利用大型语言模型（LLMs）来识别和量化在线和离线公共话语中的无家可归者（PEH）偏见，并通过影响公众舆论来缓解无家可归问题。研究的主要贡献包括构建了一个多模态数据集，开发了偏见分类框架，评估了多种LLMs模型的性能，并分析了不同城市和数据源之间的偏见差异及其与实际无家可归水平的潜在联系。\n\n研究背景\n\n无家可归是一个全球性的社会挑战，影响着数百万人。美国在2024年记录了超过770,000人无家可归，这是有史以来最高的数字。社会对无家可归者的偏见是一个重大障碍，影响着公众认知和政策制定。在线和市议会的讨论反映了公众舆论的一部分，为识别和追踪社会偏见提供了宝贵的见解。\n\n研究方法\n\n1. **数据收集与处理**：从Reddit、X（前身为Twitter）、新闻文章和美国10个城市的市议会会议记录中收集了2015年至2025年间与无家可归相关的数据，构建了一个多模态数据集。使用spaCy和pydeidentify工具对数据进行匿名化处理，以保护个人隐私。\n2. **偏见分类框架**：基于先前的研究（如OATH框架），扩展了多模态PEH偏见分类标准，增加了新的分类类别，如“提出一个真诚的问题”、“提出一个修辞问题”、“提供一个事实或主张”等，以更全面地捕捉无家可归偏见的多样性。\n3. **模型评估与选择**：评估了多种模型，包括本地LLMs（如Llama 3.2 3B Instruct、Qwen 2.5 7B Instruct和Phi4 Instruct Mini）和闭源API模型（如GPT-4.1、Gemini 2.5 Pro和Grok-4），以及BERT模型。通过零样本学习和少样本学习（in-context learning）两种方式来测试这些模型在分类任务中的表现，并与人类标注者的结果进行比较。\n4. **性能评估**：使用宏观F1分数（macro-F1）和微观F1分数（micro-F1）来评估模型性能，考虑到数据集中类别不平衡的问题，宏观F1分数能够更公平地评估模型在不同类别上的表现。\n\n实验结果\n\n1. **模型性能比较**：在表3中，展示了不同模型在不同数据源上的宏观F1分数和微观F1分数。结果显示，GPT-4.1在加权平均宏观F1分数上表现最佳，因此被选为对整个数据集进行分类的模型。\n2. **类别级别的F1分数**：在表4中，展示了GPT-4.1模型在不同类别上的F1分数，包括零样本学习和少样本学习的结果。这些结果表明，少样本学习在某些类别上能够显著提高模型的性能，尤其是在那些模型本身表现不佳的类别上。\n3. **相关性分析**：在图2中，展示了不同偏见类别之间的相关性矩阵。分析发现，某些类别之间存在显著的正相关或负相关关系，这些关系可能对政策制定者在制定缓解无家可归问题的策略时具有指导意义。\n4. **城市和数据源的偏见差异**：在图3和图4中，分别展示了不同城市和不同数据源之间的偏见差异。结果显示，不同城市和不同媒体平台上的偏见分布存在显著差异，这强调了在制定政策时需要考虑具体的社会和文化背景。\n\n结论\n\n研究结果表明，LLMs在识别和分类无家可归偏见方面具有显著潜力，尤其是在少样本学习的帮助下，本地LLMs的性能可以接近闭源模型。通过分析不同城市和数据源之间的偏见差异，研究强调了在制定政策时需要考虑具体的社会和文化背景。此外，研究还揭示了不同偏见类别之间的相关性，为政策制定者提供了有价值的见解。最终，这项工作旨在提高公众对无家可归偏见的认识，减少有害偏见的传播，并增强生成性人工智能技术在解决社会问题时的公平性和伦理应用。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Combating%20Homelessness%20Stigma%20with%20LLMs%3A%20A%20New%20Multi-Modal%20Dataset%20for%20Bias%20Detection，链接是https://arxiv.org/pdf/2508.13187，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2508.13187。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22137v1-1769990225065","title":"PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training","description":"Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.","url":"https://arxiv.org/abs/2601.22137v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:55:46.000Z","metadata":{"arxivId":"2601.22137v1","authors":"Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney","categories":"cs.LG, cs.AI, math.NA, math.OC","published":"2026-01-29T18:55:46Z","pdfUrl":"https://arxiv.org/pdf/2601.22137v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22137v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22137v1","rank":9,"domain":"Agent","zh_summary":"矩阵函数如平方根、逆根和正交化在神经网络训练的预条件梯度方法中起着核心作用。这推动了迭代算法的发展，这些算法避免了显式特征分解，而主要依赖矩阵乘法，因此非常适合现代 GPU 加速器。我们提出了 PRISM（用于矩阵函数计算的多项式拟合和随机迭代草图），这是一个用于加速矩阵函数计算迭代算法的通用框架。PRISM 将自适应多项式逼近与随机草图结合：在每次迭代中，它通过一个草图化最小二乘问题拟合当前谱的多项式替代函数，并以最小开销自适应当前实例。我们将 PRISM 应用于加速类似 Newton-Schulz 的矩阵平方根和正交化迭代，这些是机器学习中的核心原语。与以往方法不同，PRISM 无需显式的谱界或奇异值估计；并且它会自动适应不断变化的谱。实证结果表明，当整合到 Shampoo 和 Muon 优化器中时，PRISM 可以加速训练。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nPRISM 旨在解决**大规模神经网络训练中矩阵函数（如平方根、逆根、极分解）计算代价高且对谱分布敏感**的问题。具体而言：\n\n- **现有迭代方法依赖先验谱信息**（如奇异值上下界），而真实场景中这些量难以廉价获取；若预设区间与实际谱失配，收敛速度会急剧下降。\n- **手工设计的多项式加速策略**（如 PolarExpress）仅针对单一谱分布优化，跨任务迁移性差。\n- **高阶矩阵迭代**（Newton–Schulz、Chebyshev 等）在 GPU 上虽以 GEMM 为主，但初始收敛慢，导致训练 wall-clock 时间瓶颈。\n\nPRISM 提出**无分布假设、实例自适应**的加速框架：\n\n1. 把各类经典迭代统一写成“多项式残差更新”形式（Part I）。\n2. 每步用**随机草图最小二乘**实时拟合最优多项式系数 αk，使更新自动匹配当前迭代矩阵的实际谱（Part II）。\n3. 额外开销仅  O(n^2 log n) ，远低于  O(n^3)  的 GEMM 主计算，却能在理论保持二次收敛的同时，在 Marchenko-Pastur 或重尾谱上获得一致加速。\n\n最终，**无需知道任何奇异值边界**，PRISM 即可在 Shampoo、Muon 等优化器里稳定地缩短矩阵函数计算时间，从而加速整个神经网络训练过程。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为四条主线，均围绕“GPU 友好的矩阵函数迭代”与“谱自适应加速”展开：\n\n1. GEMM-only 迭代与 Newton–Schulz 家族\n\n- Kenney & Laub 1991、Higham 1997、Higham et al. 2004/2005 给出矩阵符号、平方根、极分解的经典 Newton–Schulz、Padé、Halley/QDWH 迭代，奠定“仅用矩阵乘法即可收敛”的理论基础。\n- Nakatsukasa & Higham 2012、Nakatsukasa & Freund 2016 利用 Zolotarev 有理函数将迭代次数降至 2 步，但仍需显式谱界。\n2. 深度学习优化器内的矩阵函数\n\n- Shampoo (Gupta et al. 2018) 用逆平方根预处理，原始实现依赖 SVD。\n- Muon (Jordan et al. 2024) 将梯度矩阵极分解作为动量正交化步骤，重新点燃对“无 SVD 迭代”的兴趣。\n- PolarExpress (Amsel et al. 2025)、CANS (Grishina et al. 2025) 针对 Muon 设计 minimax 最优多项式，却需预设奇异值区间 \nℓ,u\n；谱失配时反而减速（论文图 1）。\n- DION (Ahn et al. 2025) 解决分布式场景下的正交化通信瓶颈，但未处理自适应加速。\n3. 随机化数值线性代数（RandNLA）\n\n- Mahoney 2011、Woodruff 2014、Drineas & Mahoney 2018 系统发展草图最小二乘、子空间嵌入，为 PRISM 的  O(n^2log n)  多项式拟合提供理论工具。\n- Balabanov & Nouy 2019 给出 OSE 概率保证，被 PRISM 直接引用以控制 αk 的近似误差。\n4. 谱自适应与多项式逼近\n\n- Chen et al. 2011 提出在 Krylov 子空间上最小二乘拟合  f(A)b  的多项式，但面向通用矩阵函数而非 GPU 迭代。\n- Fan et al. 2019/2020 的“spectrum-adapted polynomial”需预先估计全部极值特征值，开销与 SVD 相当。\n- Kim et al. 2025 用 Monte-Carlo Tree Search 搜索最优多项式/有理函数，仍假设已知奇异值分布。\n\nPRISM 首次将上述方向整合：**以 RandNLA 的草图最小二乘实现“无先验谱信息的在线多项式优化”**，从而把经典 Newton–Schulz、Chebyshev、Inverse Newton 等迭代统一加速为实例自适应、GPU 友好、理论收敛保证的框架。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 PRISM（Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation）框架，将“矩阵函数迭代”转化为“在线多项式逼近 + 随机草图最小二乘”问题，从而**无需先验谱信息即可在 GPU 上实现实例自适应加速**。核心步骤如下：\n\n1\\. 统一视角：把经典迭代写成“残差多项式更新”\n\n对任意目标函数  T(A) （符号、平方根、极分解等），标量形式恒有\n\nT(a)=x f(xi), quad xi=xi(x,a).\n\n取  d  阶 Taylor 多项式  f_d(xi)  得迭代\n\nx_(k+1)=x_k f_d(xi_k).\n\n矩阵版\n\nX_(k+1)=X_k f_d(R_k), quad R_k=xi(X_k,A)\n\n即 Newton–Schulz、Chebyshev、Inverse Newton 等算法（论文表 1）。\n该“Part I”模板把**不同函数/不同阶数**全部纳入同一形式，为后续统一加速奠定基础。\n\n2\\. 在线多项式重拟合：每步最优 αk\n\nTaylor 多项式只在  xi≈ 0  处精确；若  R_k  有特征值远离 0，收敛变慢。\nPRISM 仅用**单额外参数**修正最高次项：\n\ng_d(xi;α)=f_(d-1)(xi)+αxi^d.\n\n每步求解\n\nα_k^_=argmin_(α∈[ell,u])|I-X_(k+1)^2|_F^2, quad X_(k+1)=X_k g_d(R_k;α)\n\n等价于在\\*_当前矩阵的全部特征值__  λ_i  上最小二乘拟合  (λ_i,f(λ_i)) 。\n\n- 目标函数是关于  α  的**四次多项式**  m(α)=∑ c_iα^i ，系数  c_i  仅含  tr(R_k^j)  类型项，可用  O(d)  次矩阵乘法得到。\n- 区间 $\nell,u\n 仅由 d 决定（例如 d=1 时 \n1/2,1\n$），**与输入谱无关**，保证收敛速率不低于原迭代（定理 1）。\n\n3\\. 随机草图：把  O(n^3)  降到  O(n^2log n) \n\n直接求  c_i  需计算  R_k^(4d+2) ，代价高于一次 Newton–Schulz 迭代。\nPRISM 引入  p× n  高斯草图矩阵  S_k （ p=O(log n) ），改为\n\ntildeα_k=argmin_(α∈[ell,u])|S_k(I-X_(k+1)^2)|_F^2.\n\n- 系数  tilde c_i  只含  tr(S_k R_k^j S_k^top) ，**右侧累乘**即可在  O(n^2 p)  时间完成。\n-  S_k  是  (6,ε,δ) -OSE，以  ≥ 1-δ  概率保证\n\n|R_(k+1)|_2le 2.95|R_k|_2^2 quad⇒quad |R_k|_2le |R_0|_2^(k-3)\n\n（定理 2），**收敛阶与原迭代相同**，额外开销仅  tilde O(n^2) 。\n\n4\\. 端到端流程（单步伪代码）\n\n```\n输入 Xk, Rk = ξ(Xk,A)\n1. 构造 m(α)=‖Sk(I−Xk gd(Rk;α)2)‖F^2 // O(n^2 p)\n2. 解析解 m′(α)=0 得 tildeαk ∈[ell,u] // O(1)\n3. Xk+1 = Xk gd(Rk;tildeαk) // O(n^3) GEMM\n```\n\n5\\. 理论保证\n\n- **确定性版本**（定理 1）： |I-X_k^2|_2le |I-A^2|_2^(2^k-2) ，**二次收敛不低于经典 Newton–Schulz**。\n- **草图版本**（定理 2）：以高概率保持同样阶数，\\*\\* sketch 不损失渐进收敛率\\*\\*。\n\n6\\. 实验验证\n\n- 在 Gaussian、重尾 HTMP 矩阵上，**wall-clock 时间一致快于 PolarExpress 与经典 Newton–Schulz**（图 3–4）。\n- 嵌入 Shampoo 与 Muon 优化器后，CIFAR-10/100 与 GPT-2 训练曲线**相同迭代数下收敛更快**，或与 AdamW 相比**验证损失更低**（图 5–6）。\n\n综上，PRISM 通过\n“**Taylor 模板 → 每步最优 αk → 草图最小二乘 → 理论保证**”\n这一完整链条，**把“对谱分布敏感”的手工调参难题转化为几乎零额外成本的在线自适应计算**，从而在大规模神经网络训练中实现通用、鲁棒、GPU 友好的矩阵函数加速。\n\nQ4: 论文做了哪些实验？\n\n论文共执行三类实验，覆盖**纯矩阵收敛测试**、**优化器内嵌加速**与**消融/对比分析**，全部在单精度 (FP32) 与 NVIDIA A100 GPU 上完成。\n\n1\\. 矩阵级收敛：PRISM 本身是否更快？\n\n| 实验对象 | 矩阵类型 | 变量 | 观测指标 |\n| --- | --- | --- | --- |\n| 极分解  UV^T  | 标准 Gaussian  A∈R^(n× m)  | 长宽比  γ=n/m∈1,4,50  | wall-clock 时间 vs.  |I-X_k^top X_k|_F  |\n| 极分解  UV^T  | 重尾 HTMP 随机矩阵 (Hodgkinson et al. 2025) | 尾指数  kappa∈0.1,0.5,100  | 同上 |\n| 平方根  A^(1/2),A^(-1/2)  | Wishart  A=G^top G  与 HTMP | 同上 |  |I-X_k^(-2)A|_F  |\n\n**结果**\n\n- 图 3-4：在所有  γ  与  kappa  下，PRISM 5 阶 Newton–Schulz 的 **GPU 时间-误差曲线** 始终最靠左，平均比 PolarExpress 再快 **1.2-1.8×**，比经典 Newton–Schulz 快 **2-4×**。\n- 图 D.1-D.4（附录）：以“迭代次数”为横轴时，PRISM 同样迭代数下误差更小，说明**加速不是单纯因为每步更便宜**，而是真正减少了迭代量。\n- 右侧面板实时绘制了 PRISM 自动学到的  α_k ：不同谱分布下曲线显著不同，验证**实例自适应**机制生效。\n\n2\\. 端到端训练：嵌入主流优化器\n\n| 优化器 | 模型/数据 | 矩阵函数调用 | 对比方法 | 观测指标 |\n| --- | --- | --- | --- | --- |\n| Shampoo | ResNet-20 / CIFAR-10 | 每步  L_t^(-1/2),R_t^(-1/2)  (p=2) | 特征分解、PolarExpress、PRISM | 前 50 epoch 验证准确率 vs. 实际训练时间 |\n| Shampoo | ResNet-32 / CIFAR-100 | 同上 | 同上 | 同上 |\n| Muon | GPT-2 Large (10 层, 1024 维, 200M FineWeb tokens) | 每步梯度矩阵极分解 | AdamW、PolarExpress、PRISM-3、PRISM-5 | 训练/验证 Loss vs. 时间，最终验证 perplexity |\n\n**结果**\n\n- 图 5：Shampoo 场景下，PRISM 在 **相同 wall-clock 时间** 内准确率最高；到 2000 s 时比 PolarExpress 高 **1.5-2%**，比特征分解高 **3-4%**。\n- 图 6：GPT-2 训练 200M tokens 后，PRISM-5 验证 loss **5.03**，PolarExpress **5.45**，AdamW **6.87**；PRISM-3 在中间迭代数更少的情况下仍优于 PolarExpress。\n\n3\\. 算法变种与消融\n\n| 目的 | 设置 | 关键发现 |\n| --- | --- | --- |\n| 高阶 vs. 低阶 | PRISM-3 (d=1) 与 PRISM-5 (d=2) 各跑 5 迭代 | 在 GPT-2 上 PRISM-5 最终 loss 更低，但 PRISM-3 已显著优于 PolarExpress，说明框架通用，不依赖特定阶数。 |\n| Newton 迭代对比 | PRISM-accelerated DB Newton (§A.2) vs. PRISM-NS | 图 D.5：DB Newton 收敛迭代数更少，但因每步需 Cholesky 逆，整体 GPU 时间略慢于 PRISM-NS；留待未来做数值稳定性改进。 |\n| 草图维度灵敏度 | p ∈ {5,10,20,50} 对随机 Gaussian 矩阵 | d=1 时 p=5 已观测不到性能损失，与定理 2 “p=O(log n)” 吻合。 |\n\n实验总结\n\n- **矩阵层面**：PRISM 在 Gaussian、Wishart、重尾谱上均取得一致且显著的 wall-clock 加速。\n- **训练层面**：嵌入 Shampoo/Muon 后，**相同训练时间获得更高准确率/更低 perplexity**，证明矩阵函数加速直接转化为端到端收益。\n- **消融层面**：框架对阶数、草图宽度、算法变种均鲁棒，**自适应系数 αk 是真正带来增益的关键**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 PRISM 框架的“直接延伸”或“跨界嫁接”，均围绕**更激进的速度、更复杂的函数、更极端的规模、更稳固的理论**四条主线展开。\n\n1\\. 更高阶/混合阶多项式：能否“自动选阶”？\n\n- 目前只尝试 d=1,2；当矩阵条件数极大时，**高阶或分段多项式**可能进一步压缩迭代数。\n- 可探索 **Multi-armed bandit / MCTS** 在每步从 {3,5,7,…}∪{rational} 中选最佳更新，**把 PRISM 的“单参数 αk”升级为“结构+参数”联合自适应**。\n\n2\\. 分布式 & 超算规模：通信与数值稳定性\n\n- PRISM 的草图只在单卡上做  S_k R_k^j ；在 **FSDP/DeepSpeed** 场景下， R_k  被切分到多节点。\n→ 设计 **并行草图**（CountSketch 或 TensorSketch）使通信量与  p  无关，同时保持 OSE 性质。\n- DB Newton 每步需全局 Cholesky；可研究 **Newton–Schulz + Newton 混合调度**：在条件数恶劣时局部切到 Newton 步，兼顾速度与稳定性。\n\n3\\. 非对称/非正定矩阵：理论尚留空白\n\n- 定理 1-2 要求  A^2  对称；对 **一般梯度矩阵  G∈R^(m× n) ** 的极分解，需建立 **随机奇异值草图** 下的收敛保证。\n- 可引入 **two-sided sketching**  S_k G T_k^top  同时估计左右奇异子空间，再拟合 **双向多项式**  g_d(Sigma;α) 。\n\n4\\. 矩阵指数、对数、ϕ-函数等“非代数”函数\n\n- 目前仅覆盖  (1-xi)^(-1/2)  型代数核；**矩阵指数  exp(A) 、对数  log(I+A) 、ϕ-functions** 出现在指数积分、Transformer 长度外推、高斯过程似然。\n- 思路：把 **Padé 或 Laplace 反变换** 写成迭代残差形式，再用 PRISM **逐步拟合有理函数系数**；需重新推导草图后的系数公式与强凸性分析。\n\n5\\. 与低秩结构的协同\n\n- 训练后期梯度矩阵常 **数值低秩**；可 **先草图截断秩-r 因子  U_r,V_r,Sigma_r ，再在  R^(r× r)  上执行 PRISM 迭代**，复杂度从  O(n^3)  降到  O(nr^2+r^3) 。\n- 需研究 **“草图-截断-迭代”三阶段耦合误差** 是否仍保持二次收敛。\n\n6\\. 学习率、权重衰减与 αk 的联合动态\n\n- 目前 αk 仅看矩阵谱；**优化器超参数（lr, β, wd）会改变  R_k  的演化路径**。\n- 可构建 **differentiable outer-loop**：把 αk 视为可学习标量，用 **超梯度/演化策略** 最小化验证 loss，实现 **“矩阵内迭代 + 优化器外迭代”双时间尺度自适应**。\n\n7\\. 硬件级微优化\n\n- 系数计算阶段  S_k R_k^j  是 **GEMV 链**；可融合到 **CUTLASS 3.x** 的 warp-level GEMM，一次性把  R_k^2,R_k^3,dots  打到 shared-memory 复用。\n- 探索 **half-precision 草图 + TF32 迭代** 的混合精度路径，在保持 OSE 常数的同时再省 30-40% 显存。\n\n8\\. 理论深挖：随机迭代非凸性 & 全局收敛\n\n- 当前定理只给 **单调  |R_k|_2  下降率**；随机草图下  α_k  是随机变量，需建立 **随机 Lyapunov 函数** 或 **鞅浓度不等式**，证明 **高概率二次终止时间**。\n- 对重尾谱，可研究 **heavy-tailed random matrix theory** 意义下的 **期望收敛率 vs. 尾指数 κ** 的定量关系。\n\n9\\. 跨领域移植\n\n- **计算物理**：用 PRISM 加速 **密度泛函理论** 中的  S^(-1/2) （重叠矩阵逆平方根），对比传统 Chebyshev 或 Hotelling 迭代。\n- **计算机图形学**：把 **协方差平方根** 用于可微渲染的 BSDF 重采样，看是否能在实时路径追踪里取代 SVD-Whitening。\n- **联邦学习**：客户端只上传草图  S_k R_k^j ，服务器聚合后回传  α_k ，实现 **通信高效的分布式预处理**。\n\n10\\. 自动代码生成 & 编译器\n\n- 给定目标函数  f(xi)  与阶数  d ，自动推导 **符号系数  c_i(tr(R^j)) ** 并生成 **CUDA-C / Triton** 内核，实现 **“矩阵函数即服务”** 的 DSL。\n- 结合 **MLIR** 的 linalg 方言，让框架在 **不同尺寸、不同精度、不同架构** 上自动选择最优草图与 tile 参数。\n\n一句话总结\n\nPRISM 把“矩阵函数迭代”变成“在线多项式学习”，因此**凡是需要快速、可微、硬件友好且谱未知的  f(A)  场景**，都值得用上述思路再探一探。\n\nQ6: 总结一下论文的主要内容\n\nPRISM：\n**P**olynomial-fitting **R**andomized **I**terative **S**ketching for **M**atrix functions——一条“无先验谱知识”即可在 GPU 上快速计算矩阵函数的统一框架。\n\n1\\. 背景痛点\n\n- 深度网络优化器（Shampoo、Muon）需反复算  A^(1/2), A^(-1/2), UV^top  等矩阵函数。\n- 传统 Newton–Schulz、Chebyshev 虽 GEMM-friendly，但**初始收敛慢**；最新手工加速法（PolarExpress 等）又**依赖预设奇异值区间**，一旦失配反而减速。\n\n2\\. 核心思路（两阶段元算法）\n\n| Part I：统一迭代模板 | Part II：在线加速 |\n| --- | --- |\n| 把“符号/平方根/极分解”全部写成 X_(k+1)=X_k f_d(R_k)  | 每步用随机草图最小二乘实时重拟合 g_d(xi;α)=f_(d-1)(xi)+αxi^d  |\n| 仅需矩阵乘法，无需 SVD | 求  α_k  开销  tilde O(n^2) ，远小于一次 GEMM |\n\n3\\. 理论结果\n\n- **定理 1**（确定性）： |I-X_k^2|_2le |I-A^2|_2^(2^k-2) ，二次收敛不低于经典迭代。\n- **定理 2**（随机草图）： p=O(log n)  时仍保持二次收敛，失败概率  δ 。\n\n4\\. 实验效果\n\n- **纯矩阵测试**：Gaussian、Wishart、重尾谱上比 PolarExpress 再快 **1.2-1.8×**，比经典 Newton–Schulz 快 **2-4×**。\n- **端到端训练**：\n– Shampoo 预处理 ResNet-20/32 → 相同时间准确率↑**1.5-2%**\n– Muon 训练 GPT-2 Large → 最终验证 loss 从 6.87→5.03，优于 PolarExpress 与 AdamW。\n\n5\\. 贡献一句话\n\nPRISM 把“矩阵函数迭代”变成“在线多项式学习”，**无需知道任何奇异值边界**，即可在 GPU 上稳定、廉价、一致地加速神经网络训练。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PRISM%3A%20Distribution-free%20Adaptive%20Computation%20of%20Matrix%20Functions%20for%20Accelerating%20Neural%20Network%20Training，链接是https://arxiv.org/pdf/2601.22137，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22137。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22136v1-1769990225065","title":"StepShield: When, Not Whether to Intervene on Rogue Agents","description":"Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.","url":"https://arxiv.org/abs/2601.22136v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:55:46.000Z","metadata":{"arxivId":"2601.22136v1","authors":"Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli","categories":"cs.LG, cs.AI, cs.CR, cs.SE","published":"2026-01-29T18:55:46Z","pdfUrl":"https://arxiv.org/pdf/2601.22136v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22136v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22136v1","rank":10,"domain":"Agent","zh_summary":"现有的病原体安全基准报告了二元准确性，混淆了早期干预与尸检分析。在第8步标记违规的检测器可实现干预;在第48步报告的，仅具法医价值。这一区别至关重要，但现有的基准无法衡量它。我们引入了StepShield，这是首个评估何时检测违规的基准测试，而不仅仅是是否被检测。StepShield包含9,213条代码特工轨迹，包括1,278对经过细致注释的训练对和一套7,935轨迹测试集，具有真实的8.1%的盗版率。流氓行为基于六类真实安全事件。我们提出了三个新的时间指标：早期干预率（EIR）、干预差距和代币节省。令人惊讶的是，我们的评估显示，基于LLM的评判机能实现59%的EIR，而静态分析仪仅有26%，这一性能差距是2.3倍，而标准准确度指标完全看不见。我们还进一步证明，早期检测具有直接的经济效益：我们的级联HybridGuard检测器将监控成本降低75%，并在企业规模五年内预计可节省1.08亿美元的费用。通过将评估重点从是否转向何时，StepShield为构建更安全、更具经济效益的AI代理奠定了新的基础。代码和数据均以Apache 2.0许可证发布。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对现有智能体安全评测体系的盲区——“只能判定是否违规，却无法衡量何时发现违规”——提出并验证了“检测时效性”这一全新维度。核心问题可归纳为：\n\n- **传统二元准确率指标失效**：两个检测器在轨迹层面准确率相同，但一个在第4步报警、一个在第48步才报警，现有基准无法区分其干预价值。\n- **干预窗口被忽视**：早期报警可阻断后续有害动作，晚期报警仅具事后追溯意义，而当前评测体系对此差异零感知。\n- **缺乏细粒度数据与指标**：尚无公开数据集把“每一步是否违规”标注出来，也没有量化“多早算早”的评测标准。\n\nStepShield 通过以下手段解决上述问题：\n\n1. 发布 9 213 条代码智能体轨迹，每一步都标注是否首次出现违规，首次引入**步级标签**。\n2. 提出三项时效性指标：\n- **Early Intervention Rate (EIR)**：在首次违规步及之前报警的轨迹占比。\n- **Intervention Gap (IG)**：首次违规到报警的平均步数差。\n- **Tokens Saved**：因提前终止而节省的 token 比例，直接折算成本。\n3. 用新指标重新评估四类检测器，揭示 LLM-Judge 的 EIR 达 0.59，而静态规则仅 0.26，**2.3 倍的早期干预差距**被传统准确率完全掩盖。\n4. 进一步证明早期检测的经济价值：级联方案 HybridGuard 可削减 75% 监控成本，五年累计节省 1.08 亿美元。\n\n综上，论文把“何时干预”从隐性需求变成可测指标，为构建可实时阻断风险的智能体安全系统提供了基准、数据与量化框架。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为六大主线，并指出它们与 StepShield 的互补或差异关系。按主题归纳如下：\n\n1. 基础 LLM 智能体架构\n\n- ReAct（Yao et al. 2023）：交错推理与行动，奠定逐步执行范式。\n- Toolformer（Schick et al. 2023）：自监督学习调用外部工具。\n- 通用能力基准：AgentBench、GAIA、WebArena、OSWorld 等，聚焦任务成功率，**未涉及逐步安全标签**。\n2. 智能体安全基准（均为轨迹级、事后二分类）\n\n- Agent-SafetyBench、SafeArena、ToolEmu、R-Judge、SafeAgentBench、AgentHarm、TheAgentCompany 等。\n- 共同局限：仅给整条轨迹打“有害/无害”标签，**无法评估检测时机**。\n3. 护栏智能体（Guardrail Agents）\n\n- ShieldAgent：用逻辑推理强制安全策略，轨迹级召回 90.1%。\n- GuardAgent：动态生成护栏代码，医疗/网页场景准确率 98%/83%。\n- StepShield 提供**步级时效指标**，可用来评测上述护栏的“干预早晚”。\n4. 规格游戏与欺骗对齐\n\n- Krakovna et al. 2020 提出规格游戏概念；Anthropic 揭示谄媚与奖励篡改；Sleeper Agents（Hubinger et al. 2024）证明欺骗行为可躲过安全训练。\n- StepShield 的六类违规 taxonomy 直接取材于这些真实欺骗案例，强调**运行时逐步监控**的必要性。\n5. 步级评估（Step-level Evaluation）\n\n- 此前无公开“步级违规标签”数据集；StepShield 首次提供 9 213 条轨迹的逐步标注，使**过程奖励模型或预测式检测**成为可能。\n6. LLM-as-Judge / Agent-as-Judge\n\n- R-Judge、SafeArena 用 LLM 做轨迹级安全裁判；Agent-as-a-Judge 用智能体评智能体，节省 97% 评估成本。\n- StepShield 把 LLM-as-Judge 下探到**每一步实时打分**，实现“过程裁判”而非“事后裁判”。\n\n简言之，StepShield 与上述研究的关系是：借用它们的真实攻击样例与裁判范式，但在**步级标注、时效指标、早期干预评估**三个维度上填补了空白。\n\nQ3: 论文如何解决这个问题？\n\n论文把“何时干预”从理念变成可量化、可复现的整套方案，具体分五步实施：\n\n1. 构建步级标注数据集 StepShield\n\n- 9 213 条代码智能体轨迹，每步人工标注是否首次违规；\n- 639 对“违规-干净”配对，确保任务一致、减少混淆；\n- 8.1 % 真实违规率，模拟生产环境，避免平衡数据集对假阳性的掩盖。\n2. 设计时效性指标体系\n\n- **EIR**（Early Intervention Rate）：在首次违规步及之前报警的轨迹比例；\n- **IG**（Intervention Gap）：首次违规到报警的平均步数差；\n- **Tokens Saved**：提前终止所节省的 token 占比，直接换算成本。\n3. 提出“准确率-时效分离”定理\n形式化证明：存在两个检测器，轨迹级准确率差任意小，但 EIR 可相差 1（即 0 vs 1），从而**从理论上否定仅用准确率评估的可行性**。\n\n4. 实现并评测四类检测器\n\n- **StaticGuard**：毫秒级正则/关键字，<1 ms；\n- **ConstraintGuard**：显式约束检查，≈5 ms；\n- **LLMJudge**：每步调用 GPT-4.1-mini 做语义裁判，≈500 ms；\n- **HybridGuard**：级联前三者，不确定时再调用 LLM，平均 75 ms。\n实验结果显示 LLMJudge 的 EIR 达 0.59，静态规则仅 0.26，**2.3 倍早期干预差距**被传统准确率完全隐藏；HybridGuard 在成本下降 75 % 的同时仍保持 0.41 EIR 与最高准确率 0.66。\n5. 经济模型与规模推演\n建立成本公式\n\nC_(cost) = C_(detect) + C_(execute) · (1 - Tokens Saved)\n\n按 1 亿条轨迹/年起始、35 % 年增长率计算，HybridGuard 五年累计节省 1.08 亿美元；若考虑自注意力 O(n²) 的后期高阶 token 成本，实际节省可达 4.2 亿美元。\n\n通过以上五个环节，论文把“检测时机”转化为可测指标、可比较算法、可落地成本收益，从而**首次让‘何时干预’成为智能体安全评测的核心维度**。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“检测时效性”共设计并执行了 5 组实验，覆盖主评测、细粒度剖析、消融、失败案例与统计显著性五个层面。\n\n1. 主实验：四检测器全基准评测\n\n- 数据集：StepShield 7 935 条测试轨迹（8.1 % 违规）。\n- 指标：EIR、IG、Acc、F1、Tokens Saved。\n- 结果：\n– LLMJudge 获得最高 EIR 0.59，StaticGuard 仅 0.26，**2.3 倍差距**；\n– HybridGuard 准确率最高 0.66，但 EIR 降至 0.41，揭示“准确率-时效”权衡；\n– HybridGuard 单条成本 $0.025，比“无检测”下降 75 %。\n2. 违规类别细粒度剖析\n将 639 条违规轨迹按六类（UFO、SEC、RES、INV、TST、DEC）拆分，计算每类 EIR：\n\n- 语义型违规（DEC/INV/UFO）LLMJudge 显著领先，EIR 0.70–0.79；\n- 模式型违规（TST）StaticGuard 与 LLMJudge 差距缩小（0.49 vs 0.58）；\n- HybridGuard 在 DEC 类意外拿到 0.79，超过 LLMJudge，显示级联策略对“欺骗完成”特别有效。\n3. 难度级别（L1/L2/L3）消融\n\n- L1（关键词明显）LLMJudge EIR 0.78，StaticGuard 0.42；\n- L3（需跨步推理）所有检测器骤降，LLMJudge 仅 0.38，**确立“L3 难题”作为后续研究方向**。\n4. 超参数与敏感性消融\n\n- **Prompt 风格**：zero-shot / 2-shot / CoT，EIR 在 0.57–0.59 之间波动，证明 LLMJudge 对提示词稳健；\n- **模型规模**：GPT-4.1-mini → GPT-4 → GPT-4-Turbo，EIR 从 0.59 升至 0.68，显示规模直接转化为时效收益；\n- **上下文步数**：1/3/5/All，EIR 在 5 步后饱和，兼顾性能与延迟；\n- **级联阈值 θ**：0.5/0.7/0.9，θ=0.7 在成本与 EIR 之间取得最优平衡。\n5. 失败案例与统计显著性\n\n- 随机采样 100 条 LLMJudge 漏报/误报，手动归类：38 % 为 L3 subtle 违规，22 % 涉及多步推理；\n- 10 000 次 bootstrap 给出 95 % 置信区间，McNemar 检验显示所有 detector 间 EIR 差异 p < 0.001，**统计显著**。\n\n通过上述实验，论文不仅验证了“时效指标”可测、可用，还量化了早期干预带来的经济收益，为后续研究提供了明确的性能基准与改进方向。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接延伸 StepShield 的“时效评测”框架，或利用其步级标注打开新的研究空间：\n\n1. 跨领域步级数据集\n\n- 网页智能体：在 WebArena、OSWorld 上标注“点击/输入/导航”每一步的违规标签，检验时效指标是否依旧呈 2× 以上差距。\n- 多智能体协作：为对话或分布式工具调用场景定义“合谋违规”“链式违规”等新类别，观察早期检测是否随智能体数量指数级变难。\n2. L3 subtle 违规的专用检测器\n\n- 训练 Process Reward Model（PRM）：用 StepShield 的步级标签做监督，显式优化“首次违规步”的似然。\n- 跨步因果推理：引入因果图或事件时序模型，检测“前几步埋伏、后几步爆发”的 sleeper 行为。\n3. 预测式干预（Before-the-fact）\n\n- 定义“pre-EIR”指标：在 _t\\_rogue – k_ 步就触发报警，k>0 时仍计入有效干预；探索 k 与最终任务成功率之间的帕累托前沿。\n- 结合不确定性估计：当模型置信度低于某阈值即提前暂停，降低误中断。\n4. 动态级联策略学习\n\n- 把阈值 θ 做成状态相关函数 θ(s\\_t, c\\_t)，用强化学习优化长期成本节省，而非手工调 0.7。\n- 引入“early-exit” Transformer，对每一步动态决定继续深入推理还是立即报警。\n5. 经济模型细化\n\n- 非线性 token 成本：按自注意力 O(n²) 实测定价，重新计算 Tokens Saved，并引入“中断罚金”（用户侧体验损失）做多目标优化。\n- 企业异构场景：对高价值金融轨迹与低价值日志轨迹采用不同级联策略，验证节省曲线是否呈分段线性。\n6. 对抗与隐蔽性评估\n\n- 在 SHADE-Arena 的“隐藏破坏”任务上，用 StepShield 指标评测监测器能否在破坏意图尚未转化为可观察动作时提前报警。\n- 构建“对抗扰动步”数据集：对代码插入语义保留但迷惑性高的扰动（如变量重命名+分步 rm），测量 EIR 下降幅度。\n7. 隐私与治理\n\n- 研究步级监控日志本身带来的隐私泄露风险，设计“遗忘步”或差分隐私过滤器，在保持 EIR 的同时降低用户数据暴露。\n- 建立“干预可解释性”基准：要求检测器在报警同时提供自然语言理由，供人类审核，防止监控系统被滥用于过度审查。\n8. 实时部署工具链\n\n- 将 HybridGuard 封装为轻量级 sidecar 容器，嵌入 CI/CD 或 Jupyter 内核，收集真实生产环境的延迟、误中断率，闭环更新模型。\n- 开源社区挑战赛：固定计算预算（$0.01/轨迹）谁能在 StepShield 测试集上取得最高 EIR，推动算法与工程协同优化。\n\n通过上述探索，可把“何时干预”从离线评测扩展到在线决策、从代码智能体扩展到通用智能体、从单点检测扩展到预测-治理一体化系统。\n\nQ6: 总结一下论文的主要内容\n\n**StepShield：把“何时干预”变成可测指标的智能体安全基准**\n\n1. 问题\n现有安全基准只给整条轨迹打“是否违规”标签，无法区分第4步报警与第48步报警的差异，导致早期干预能力完全不可见。\n\n2. 解决方案\n\n- 发布 9 213 条代码智能体轨迹，**每步标注**首次违规点；\n- 提出三项**时效指标**：Early Intervention Rate (EIR)、Intervention Gap (IG)、Tokens Saved；\n- 形式化证明“准确率相同但 EIR 可相差 1”，确立时效指标的必要性；\n- 实现四款检测器（StaticGuard / ConstraintGuard / LLMJudge / HybridGuard），在统一基准上比较。\n\n3. 主要结果\n\n- LLMJudge 的 EIR 达 0.59，静态规则仅 0.26，**2.3 倍早期干预差距**被传统准确率完全隐藏；\n- HybridGuard 在保持最高准确率 0.66 的同时，单条成本 $0.025，比无检测节省 75 %；\n- 按 1 亿轨迹/年、35 % 年增长率测算，五年累计节省 **1.08 亿美元**。\n\n4. 贡献\n\n- 首个步级标注数据集 + 时效评测范式；\n- 揭示“准确率-时效”权衡，为部署提供量化依据；\n- 证明早期检测兼具安全与经济效益，推动智能体安全从“事后取证”走向“实时阻断”。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是StepShield%3A%20When%2C%20Not%20Whether%20to%20Intervene%20on%20Rogue%20Agents，链接是https://arxiv.org/pdf/2601.22136，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22136。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22159v1-1769990308644","title":"RedSage: A Cybersecurity Generalist LLM","description":"Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.","url":"https://arxiv.org/abs/2601.22159v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:57.000Z","metadata":{"arxivId":"2601.22159v1","authors":"Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani","categories":"cs.CR, cs.AI, cs.CL","published":"2026-01-29T18:59:57Z","pdfUrl":"https://arxiv.org/pdf/2601.22159v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22159v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22159v1","rank":1,"domain":"Evaluation","zh_summary":"网络安全运营需要支持多样化工作流程且不暴露敏感数据的辅助大型语言模型。现有解决方案要么依赖存在隐私风险的专有API，要么依赖缺乏域名适配的开放模型。为弥合这一差距，我们通过大规模网络过滤和手工收集高质量资源，策划了118亿个以网络安全为重点的持续预训练数据标记，涵盖2.86万份文档，涵盖框架、攻击性技术和安全工具。基于此，我们设计了一个代理增强流程，模拟专家工作流程，生成26.6万多回合网络安全样本，用于监督式微调。结合通用开源的大型语言模型数据，这些资源使得RedSage的培训成为可能，RedSage是一款开源、可本地部署的网络安全助手，具备域感知的预培训和后培训功能。为了严格评估这些模型，我们推出了RedSage-Bench，这是一个基准测试，包含3万条选择题和240条开放式问答题，涵盖网络安全知识、技能和工具专长。RedSage还会根据已建立的网络安全基准（如CTI-Bench、CyberMetric、SECURE）和通用大型语言模型基准进行进一步评估，以评估更广泛的概括性。在8B尺度上，RedSage持续取得更优成绩，在网络安全基准测试中领先基线模型最高+5.59分，在开放大型语言模型排行榜任务中高出+5.05分。这些发现表明，领域感知型代理增强以及前后培训不仅能提升网络安全专长，还有助于提升整体推理能力和指令遵循能力。所有模型、数据集和代码均公开。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. **现有网络安全大模型的训练与部署缺陷**\n\n- 专有模型依赖云端 API，存在敏感数据外泄风险；\n- 开源模型缺乏针对网络安全场景的充分预训练与后训练，导致领域知识薄弱；\n- 已有工作往往只关注单一训练阶段（仅预训练或仅监督微调），且公开数据规模有限，难以支撑本地化部署的通用型安全助手。\n2. **网络安全评测基准的覆盖不足**\n\n- 主流基准侧重知识问答，缺少对**工具操作能力**与**开放式问答质量**的系统性评估；\n- 缺乏同时覆盖知识、技能、工具三维能力的大规模高质量评测集。\n3. **数据与 pipeline 的封闭性**\n\n- 先前研究多未公开训练数据与代码，阻碍社区复现与进一步研究。\n\n为此，作者提出 **RedSage**：\n\n- 构建 **11.8 B token** 的网络安全持续预训练语料（CyberFineWeb + RedSage-Seed）；\n- 设计 **agentic 增强 pipeline**，将 28 K 权威文档转化为 **266 K 轮次对话**用于监督微调；\n- 推出 **RedSage-Bench**（30 K 多选题 + 240 开放式问答），系统评估知识、技能与工具 proficiency；\n- 训练得到 **8 B 参数可本地部署**的开源模型，在网络安全与通用基准上均取得 **+5 分以上** 的提升，并全面公开数据、模型与代码。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统梳理了相关研究，可归纳为两条主线：**网络安全评测基准**与**网络安全领域大模型/数据集**。\n\n1. 网络安全评测基准\n\n- **知识型基准**\n- SecEval（Li et al., 2023）：2 K 多选题，覆盖 9 类安全主题。\n- CyberMetric（Tihanyi et al., 2024）：10 K 多选题，基于 RAG 与专家验证。\n- CyberBench（Liu et al., 2024）：除问答外还包含 NER、摘要、分类任务。\n- SECURE（Bhusal et al., 2024）：面向工控系统，聚焦风险推理与漏洞分析。\n- CS-Eval（Yu et al., 2024）：42 子类，分知识-能力-应用三级认知维度。\n- SecBench（Jing et al., 2025）：44 K 多选题 + 3 K 简答题，中英双语。\n- **应用与智能体基准**\n- CTI-Bench（Alam et al., 2024）：威胁情报任务，含 CVE↔CWE 映射、CVSS 预测、攻击技战术识别。\n- CyberSecEval（Wan et al., 2024）：8 类风险场景，如漏洞利用生成、提示注入。\n- NYU-CTF（Shao et al., 2024）、CyBench（Zhang et al., 2025）：交互式夺旗赛，评估逆向、Web 利用等红队能力。\n- **共性局限**：多数基准仅覆盖知识或应用，**缺乏对安全工具操作与开放式问答质量的细粒度评估**。\n2. 网络安全领域大模型与数据集\n\n- **早期编码器模型**\n- CyBERT、SecureBERT、CTI-BERT：基于 BERT 的域内微调，任务特定，数据未公开。\n- **大模型时代的持续预训练/微调**\n- PRIMUS（Yu et al., 2025）：2.75 B token 预训练 + 835 条 SFT，基于 Llama-3.1-8B。\n- Foundation-Sec-8B（Kassianik et al., 2025）：5.1 B token 预训练，数据未开源。\n- DeepHat（2025）：宣称 1 M+ Q&A 微调，无预训练阶段。\n- Lily-Cybersecurity-7B（Sego Lily Labs, 2024）：22 K 手工对话微调。\n- Cyber-DAP（Salahuddin et al., 2025）：119 M token 小规模持续预训练。\n- SecGemini：谷歌闭源模型，实时威胁情报，数据与权重均未发布。\n- **共性局限**：\n- 仅聚焦单一训练阶段（预训练或 SFT），**未联合优化**；\n- 数据规模或质量有限，**无 agentic 增强**；\n- **未同时公开模型、数据与代码**，可复现性差。\n\nRedSage 与上述工作的区别\n\n- 首次将**大规模持续预训练（11.7 B token）+ 代理增强 SFT（266 K 对话）+ 偏好对齐**完整 pipeline 开源；\n- 提出同时覆盖**知识、技能、工具**且包含**开放式问答质量评分**的 RedSage-Bench；\n- 8 B 规模即可本地部署，在网络安全与通用基准上均取得新 SOTA。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“数据-训练-评测”三位一体的系统化方案，一次性补齐了领域预训练、高质量后训练与严格评测的缺口，具体路径如下：\n\n1\\. 构建 11.8 B token 网络安全持续预训练语料\n\n| 子语料 | 来源与处理 | 规模 | 关键特点 |\n| --- | --- | --- | --- |\n| CyberFineWeb | 在 FineWeb（17 T token）上用 ModernBERT 分类器过滤→全球 MinHash 去重→按时间片混合 30 % FineWeb-Edu 防止灾难性遗忘 | 13 M 文档 / 11.7 B token | 覆盖 2013-2024 公开 Web 安全内容，保持通用知识 |\n| RedSage-Seed | 人工精选 28 637 篇权威资源（MITRE、OWASP、Kali、CTF Write-up、tldr 等）→ Markdown 结构化 | 150 M token | 高质量、可溯源、三分类：Knowledge / Skills / Tools |\n| RedSage-Dump | 补充 NVD、RFC、NIST、安全新闻、教育门户等公开数据 | 700 M token | 增强漏洞、协议、合规等长尾知识 |\n\n2\\. Agentic 增强生成 266 K 多轮对话用于后训练\n\n- **Planner Agent**：对 Seed 数据分块→自动抽取技能集（如“漏洞分析”“命令生成”）并输出多样化增强策略（Q&A、角色扮演、步骤演练等）。\n- **Augmenter Agent**：按策略将每块种子实例化为**角色驱动、多轮、可执行**的对话，严格保持技术深度与事实一致。\n- **质量控制**：格式、一致性、相关性三重过滤；最终得到 266 K 对话（353 M token），样本扩大 9.2×，token 扩大 2.3×。\n\n3\\. 两阶段后训练\n\n1. **SFT**：RedSage-Conv + 通用指令集 SmolTalk2（非推理子集）→兼顾网络安全专精与广泛指令遵循。\n2. **DPO**：采用开源 Tulu3 偏好数据做直接偏好优化，提升回答有用性与安全性。\n\n4\\. 建立 RedSage-Bench 全面评测\n\n| 维度 | 题型 | 规模 | 构建方式 | 质量保障 |\n| --- | --- | --- | --- | --- |\n| 知识 | MCQ | 30 K | 用强 LLM 对 Seed 生成→双阶段 LLM 验证（结构+评分）→人工抽查 | 得分>8 且类别均衡 |\n| 技能 | Open-QA | 240 | Agentic 计划→QA 生成→双 LLM 交叉验证→人工终审 | 0-10 质量评分+事实正确性 |\n| 工具 | —— | —— | 同上 | 同上 |\n\n5\\. 训练与评测结果\n\n- **基座**：Qwen3-8B → 持续预训练 → RedSage-8B-Base\n- **指令版**：SFT → RedSage-8B-Ins；再加 DPO → RedSage-8B-DPO\n\n**效果**（8 B 模型）\n\n- 网络安全基准平均 +5.59（最高 84.6 % vs 79.0 %）\n- Open LLM Leaderboard 平均 +5.05（74.3 % vs 69.3 %）\n- 32 B 规模轻量 QLoRA 实验亦获一致提升，验证 pipeline 可扩展。\n\n6\\. 开源与可复现\n\n- 全部语料、对话、Benchmark、训练配置（Axolotl YAML）、评测脚本（LightEval）一次性公开；\n- 提供去污染与伦理使用协议，支持本地 GPU 部署，实现“隐私-安全-可控”的工业级安全助手。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“模型性能”** 与 **“方法可复现/可扩展”** 两条主线，共执行了 4 组实验，覆盖 20+ 公开基准、3 类模型规模、累计 50 余万个测试样本。\n\n1\\. RedSage-Bench 自评实验\n\n**目的**：验证领域定制预训练 + 代理增强 SFT 的绝对增益。\n**任务与指标**\n\n- MCQ（30 k题，0-shot）：宏观准确率 + 知识/技能/工具 三类细粒度准确率。\n- Open-QA（240题，0-shot）：LLM-as-Judge 打分（0–10）+ 事实正确率（T/F）。\n\n**对照组**\n\n- 通用基线：Llama-3.1-8B、Qwen3-8B\n- 安全专用基线：Foundation-Sec-8B、Llama-Primus、DeepHat-V1-7B 等\n\n**主要结果**\n\n- RedSage-8B-Ins 在 MCQ 宏观准确率上比最强 8 B 通用模型 Qwen3-8B 提升 **+3.88 pp**；DPO 版本保持 **+2.98 pp**。\n- Open-QA 平均正确率领先次优模型 **+7 pp**，质量分领先 **+0.07**（6.50 vs 6.43）。\n- 工具类题目最难（中位数质量分最低），RedSage 仍保持显著优势。\n\n2\\. 公开网络安全基准外推实验\n\n**目的**：检验领域知识能否泛化到外部评测。\n**基准列表（7 个）**\nCyberMetric-500、SecBench-En、MMLU-CSec、SECURE（MEAT/CWET/KCV）、CTI-Bench（MCQ+RCM）、SecEval\n\n**结果（表 5）**\n\n- 5-shot 基座阶段：RedSage-8B-Base 平均 **84.6 %**（+3.75 pp > Qwen3-8B-Base）。\n- 0-shot 指令阶段：RedSage-8B-Ins **81.3 %**（+5.59 pp > Qwen3-8B），接近 32 B 模型（82.3 %），距离 GPT-5 差 5 pp。\n\n3\\. 通用大模型 Leaderboard 实验\n\n**目的**：验证“领域强化”是否损害通用能力。\n**基准（7 项）**\nARC-C、HellaSwag、TruthfulQA、MMLU、WinoGrande、GSM8K、IFEval\n\n**结果（表 6）**\n\n- RedSage-8B-DPO 平均 **74.3 %**，领先所有 8 B 对照（最高基线 69.3 %），并超过 Qwen3-32B（73.2 %）。\n- GSM8K 数学推理 **86.1 %**（+8.4 pp > 最强 8 B 基线），说明 Seed 数据中的 CTF/Write-up 逻辑链同样提升通用推理。\n\n4\\. 规模扩展与消融实验\n\n4.1 32 B 轻量微调（QLoRA）\n\n- 仅用 RedSage-Seed + 50 % RedSage-Conv，1 % 可训练参数，4 小时训练。\n- RedSage-32B-LoRA-Ins 在 RedSage-MCQ 达 **87.5 %**（+2.0 pp > 全量 8 B 版本），外部安全基准平均 **82.9 %**，验证 pipeline 随模型容量线性受益。\n\n4.2 消融：预训练语料贡献\n\n- **CyberFineWeb-only** vs **Seed-only** vs **CFW+Seed**\n- Seed-only 在需要权威知识的 CTI-RCM、MMLU-CSec 上最高；\n- CFW-only 在 SecBench、CyberMetric 等 Web 知识型基准上最高；\n- 二者组合取得最佳宏观平均，证明互补性。\n\n4.3 消融：后训练策略\n\n- SFT → DPO 连续提升 IFEval（指令遵循）（79.9 → 83.4 pp），但 MCQ 准确率略降（85.7 → 84.8），显示偏好对齐以牺牲少量闭卷正确率为代价换取开放式可用性。\n\n实验总结\n\n- 在 20+ 基准、50 万级样本上，RedSage 8 B 模型**同时**实现：\n- 网络安全任务 **SOTA**（+5.6 pp）\n- 通用任务 **领先同规模模型**（+5.0 pp）\n- 代理增强数据 + 持续预训练的策略可**零修改**迁移到 32 B 模型，继续提升性能，证明方法的可扩展性与复现价值。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**数据、模型、评测、应用与安全**五大主题，并给出可落地的研究切入点。\n\n1\\. 数据层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 多语言/地域威胁情报 | 将 CyberFineWeb 过滤 pipeline 扩展到 Common Crawl 其他语言子集，构建「中文-阿拉伯语-俄语」平行安全语料。 | 捕捉非英语地下论坛/漏洞交易信息，提升全球威胁可见度。 |\n| 多模态安全数据 | 把 CVE 公告中的 PoC 视频、攻击流程图、恶意软件截图与文本对齐，形成图文交错预训练语料。 | 支持「图-文」混合问答，例如“根据这张 Wireshark 截图指出攻击阶段”。 |\n| 实时威胁增量更新 | 设计「时间窗口」持续预训练策略，每周注入新 CVE、新 ATT&CK Technique，避免模型“过期”。 | 解决当前静态语料无法反映 0-day 爆发的问题。 |\n\n2\\. 模型层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 工具调用能力 | 引入 ToolLLM 式 API 模板，把 Kali 工具封装成可调用函数，训练模型生成「工具链+参数」并实时执行。 | 实现真正的“红队助手”——输入目标 IP，输出 nmap → nuclei → metasploit 自动链。 |\n| 推理-行动循环 | 结合 ReAct / Reflexion 框架，让模型在 CTF 环境中自我验证 flag 结果，根据失败反馈迭代 payload。 | 突破当前仅文本生成、无环境反馈的局限，迈向自主渗透代理。 |\n| 小参数高效化 | 用 1-3 B 模型 + 动态 LoRA 合并，实现「手机端本地安全助手」；研究量化后是否仍能保持 exploit 语法精度。 | 满足现场应急、无网络场景下的便携需求。 |\n\n3\\. 评测层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 动态 CTF 基准 | 与 CTFd 平台对接，每月自动拉取新比赛题目，形成「活基准」；指标从准确率改为夺旗数/用时。 | 防止静态题目过拟合，提供可无限扩展的攻防评测。 |\n| 红-蓝对抗评分 | 建立「双模型对抗」协议：RedSage-Red vs RedSage-Blue，用 MITRE CALDERA 记录 ATT&CK 覆盖度，计算累积 TTP 成功数。 | 量化模型在真实杀伤链中的攻防贡献度。 |\n| 幻觉与一致性审计 | 引入「事实性链式追踪」数据集：对模型输出的每一条命令/漏洞描述给出可执行验证脚本，统计可复现率。 | 解决当前 LLM 安全建议“看似合理却不可执行”的痛点。 |\n\n4\\. 应用层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| SOAR 编排自动生成 | 将 SIEM 告警作为输入，模型输出 Splunk Phantom / Microsoft Sentinel Playbook JSON，直接导入执行。 | 缩短 SOC 平均响应时间（MTTR）。 |\n| 漏洞补丁联合生成 | 给定 CVE 描述 + 受影响函数，模型同时输出（1）补丁 diff（2）回归测试用例（3）CI 集成 YAML。 | 实现「一键修复」DevSecOps 工作流。 |\n| 安全培训个性化 | 利用对话历史评估学员薄弱点，动态生成「专属靶场」Docker Compose 场景与提示脚本。 | 提升企业安全培训效率，降低人力成本。 |\n\n5\\. 安全与伦理层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 可控生成护栏 | 在模型内部集成「双重控制码」：当请求包含 exploit、RAT、后门等关键字时，自动切换到「只给出防御/检测建议」模式。 | 降低双重用途风险，满足欧盟 AI Act 等监管要求。 |\n| 训练数据毒化检测 | 研究若攻击者在 Common Crawl 植入恶意网页并被 CyberFineWeb 吸收，如何触发异常行为；构建毒化样本检测器。 | 防止“上游投毒”导致模型输出有害 payload。 |\n| 可解释威胁推理 | 要求模型在给出攻击建议时，同步输出对应的 MITRE ATT&CK Technique ID 与置信度，并提供引用段落。 | 方便审计与责任追溯，增强安全员信任度。 |\n\n快速起步建议（3 个月可完成）\n\n1. 用现有 RedSage-8B-DPO + ToolLLM 模板，收集 5 K 条「工具调用-执行结果」对话，微调出 RedSage-Tools 原型。\n2. 在小型 CTF 平台（如 PicoMini）运行自动夺旗实验，记录 flag 率并与人类选手基线对比。\n3. 将动态评测结果开源，形成「RedSage-LiveBench」月度榜单，吸引社区提交新模型与新题。\n\n通过以上探索，可逐步把 RedSage 从“知识问答”升级为「可解释、可执行、可自我进化」的下一代网络安全智能体。\n\nQ6: 总结一下论文的主要内容\n\nRedSage：开源、本地化、8 B 参数的网络安全通用大模型\n——数据-训练-评测全流程总结\n\n1\\. 要解决的问题\n\n- 专有安全 LLM 依赖云端 API，敏感数据外泄风险高；\n- 开源模型缺乏大规模**域内预训练**+**高质量后训练**，安全知识不足；\n- 现有基准缺**工具操作**与**开放问答质量**维度，且数据/代码多未公开。\n\n2\\. 核心贡献\n\n| 内容 | 规模 | 亮点 |\n| --- | --- | --- |\n| CyberFineWeb | 11.7 B token | 2013-24 Common Crawl → ModernBERT 过滤 → 全局去重 → 30 % 通用知识回放 |\n| RedSage-Seed | 28 k 权威文档 / 150 M token | MITRE、OWASP、Kali、CTF Write-up 等，Markdown 结构化 |\n| Agentic 增强对话 | 266 k 多轮 / 353 M token | Planner+Augmenter 双智能体，把种子转为可执行、角色驱动对话 |\n| RedSage-Bench | 30 k MCQ + 240 开放问答 | 覆盖知识-技能-工具；双阶段 LLM 验证+人工终审，去污染 |\n| RedSage-8B 模型 | 8 B 参数 | Qwen3-Base → 持续预训练 → SFT → DPO；完全开源 |\n\n3\\. 实验结果（↑ vs 最强同规模模型）\n\n- **网络安全基准**平均 **+5.6 pp**（84.6 % vs 79.0 %）\n- **通用 LLM Leaderboard**平均 **+5.0 pp**（74.3 % vs 69.3 %）\n- **开放问答**正确率 **+7 pp**，质量分 **+0.07**\n- **32 B 轻量 QLoRA**继续提升，验证 pipeline 可扩展\n\n4\\. 可用性与影响\n\n- 8 B 规模 → 单卡 24 GB GPU 可部署，**本地私有**运行\n- 代码+数据+Benchmark 全部开放，支持复现与二次开发\n- 首次实现“**大规模域内预训练 + 代理增强后训练 + 三维评测**”一体化，为社区提供可落地的网络安全通用助手基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RedSage%3A%20A%20Cybersecurity%20Generalist%20LLM，链接是https://arxiv.org/pdf/2601.22159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22159。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22157v1-1769990308644","title":"Discovering Hidden Gems in Model Repositories","description":"Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of \"hidden gems\", unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.","url":"https://arxiv.org/abs/2601.22157v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:55.000Z","metadata":{"arxivId":"2601.22157v1","authors":"Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen","categories":"cs.LG, cs.CL","published":"2026-01-29T18:59:55Z","pdfUrl":"https://arxiv.org/pdf/2601.22157v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22157v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22157v1","rank":2,"domain":"Evaluation","zh_summary":"公共仓库托管了数百万经过微调的模型，但社区使用仍然过度集中于少数基础检查点。我们探讨了这种集中现象是反映了高效的市场选择，还是优质模型被系统性忽视。通过对超过2000个模型的广泛评估，我们展示了“隐藏的宝石”普遍存在——这些不受欢迎的微调模型显著优于其热门对应模型。值得注意的是，在Llama-3.1-8B家族中，我们发现一些很少被下载的检查点能够将数学性能从83.2%提升至96.0%，而无需增加推理成本。然而，通过对每个上传模型进行穷尽评估来发现这些模型在计算上是不可行的。因此，我们将模型发现问题表述为多臂老虎机问题，并通过使用共享查询集和激进淘汰策略加速序列抽半搜索算法。我们的方法每个候选模型只需50次查询就能检索到顶级模型，使发现速度提高50倍以上。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对公共模型仓库（如 Hugging Face）中“用户集中下载极少数热门模型，而大量模型无人问津”这一现象，提出并回答两个核心问题：\n\n1. 热门模型是否真的就是同系列中性能最好的？\n2. 如果不是，如何在百万级模型中高效找出那些“被埋没的优等生”？\n\n为此，作者\n\n- 在 4 个主流模型树（Qwen-2.5-3B/7B、Mistral-7B、Llama-3.1-8B）上系统评估了 2000+ 微调/checkpoint，证明“隐藏宝石”（下载量极低却显著优于官方热门版本的模型）普遍存在；\n- 将“大海捞针”式的模型搜索形式化为固定预算的最佳臂识别（Best-Arm Identification）多臂老虎机问题，提出加速版 Sequential Halving 算法，仅 50 次查询即可把候选池缩小到 top-3，速度提升 50× 以上，平均性能提升 4.5% 以上。\n\nQ2: 有哪些相关研究？\n\n与本文密切相关的研究可归纳为以下四条主线，并在关键处给出对应文献：\n\n1. 模型群体与权重空间学习\n\n- 将神经网络权重视为数据点，直接学习其语义表征以预测性能、功能或血缘，无需运行推理：\n- Schürholt et al. 2021, 2022, 2024\n- Horwitz et al. 2025a,b,c\n- Kahana et al. 2025a,b\n- Navon et al. 2023a,b；Kofinas et al. 2024\n- 研究参数空间对称性/置换不变性，为权重空间距离与检索提供理论支撑：\n- Lim et al. 2023, 2024；Zhao et al. 2025\n2. 模型选择、路由与排行榜压缩\n\n- 传统 leaderboard 方法：在固定基准上一次性评估所有候选，计算量随模型数线性增长（Perlitz et al. 2024；Tamura et al. 2025）。\n- 多 LLM 路由系统：训练元控制器把输入动态分配给不同专家模型（Hu et al. 2024；Muqeeth et al. 2024）。\n- 小样本/高效基准：tinybenchmarks、label-efficient selection（Polo et al. 2024；Ashury-Tahan et al. 2024）。\n3. 固定预算最佳臂识别（Best-Arm Identification, BAI）\n\n- 纯探索型多臂老虎机：Successive Rejects、Sequential Halving、UCB-E、Bayesian Elimination 等（Audibert & Bubeck 2010；Karnin et al. 2013；Atsidakou et al. 2022）。\n- 本文在此基础上提出“共享查询集 + 激进剪枝”的域专用加速方案。\n4. 模型合并与群体优化\n\n- 权重平均/融合：Model Soups、WiSE-FT、TIES-Merging（Wortsman et al. 2022；Yadav et al. 2024）。\n- 协同下降与 swarm 优化：ColD Fusion、Model Swarms（Don-Yehiya et al. 2023；Feng et al. 2024, 2025）。\n\n这些工作共同构成了“如何在庞大模型生态中快速定位高价值个体”的研究背景，而本文首次系统论证了“隐藏宝石”现象的存在，并给出可扩展的纯探索检索算法。\n\nQ3: 论文如何解决这个问题？\n\n论文将“在百万级模型库里找出被埋没的高性能微调”这一难题拆成两步解决：\n\n1. 证明问题存在——“隐藏宝石”确实大量存在\n\n- 在 4 棵主流模型树（Qwen-2.5-3B/7B、Mistral-7B、Llama-3.1-8B）上统一采样 2 500 条 RouterBench 查询，对 2 000+ 个下游微调/适配器做相同推理成本的对照实验。\n- 定义“隐藏宝石”三条件：\n- 下载量不在前 1 %（Obscurity）\n- 性能位于前 1 %（Excellence）\n- 严格超过最受欢迎模型的得分（Dominance）\n- 结果：每棵树都发现满足上述条件的模型，例如 Llama-3.1-8B 一棵树上某微调把 GSM8K 从 83.2 % 提升到 96.0 %，而月下载量仅三位数，从而否定“市场已高效选出最优模型”的假说。\n2. 让搜索可负担——把“ exhaustive 评估”变成“固定预算最佳臂识别”\n\n- 形式化：给定模型树  T=m_1,…,m_K  与总查询预算  B ，目标是最小化简单遗憾\n\nr_B = max_(i) μ_i - μ_(hat imath)\n\n其中  μ_i  为模型  m_i  在任务上的真实准确率， hat imath  是算法停时后返回的模型。\n- 基础算法：Sequential Halving（SH）——多轮淘汰，每轮把剩余模型用相同查询数评估，淘汰后 50 %。\n- 提出两项领域专用加速：\n1. 相关采样（Correlated Sampling）\n每轮强制所有幸存模型回答**同一批**查询，用共享题集消减题目难度差异带来的方差，使得排序信噪比更高。\n2. 激进剪枝调度（Aggressive Elimination Schedule）\n观察到 90 % 上传模型质量极低，首轮即把候选池从  K  直接压到 100，并把 60 % 预算砸在这一步；后续每轮查询量翻倍，保证精英模型尽早获得高置信度估计。\n- 复杂度：每个候选只需约 50 次查询即可以 > 90 % 概率锁定 top-3 模型，对比 exhaustive 评估的 2 500 次，加速 50× 以上。\n3. 实验验证\n在 10、25、50、100、200 次查询/模型五种预算下重复 100 轮，平均 rank 与准确率均显著优于 Uniform、UCB、SR、TTTS、BayesElim 等 8 条基线；50 次查询即可逼近“全局最优”性能，且跨四棵树一致有效。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“隐藏宝石是否存在”与“能否高效找到”两条主线，共完成以下四类实验：\n\n1. 大规模对照评估——验证隐藏宝石存在性\n\n- 模型池：从 4 棵主流模型树（Qwen-2.5-3B、Qwen-2.5-7B、Mistral-7B、Llama-3.1-8B）中各抽取 ≈400 个全量微调 + ≈400 个适配器，过滤掉下载/运行失败案例后，实得 2 000+ 候选。\n- 评测协议：统一使用 RouterBench 的 2 500 题子集（ARC-C、Winogrande、MMLU、MBPP、GSM8K 各 500 题），所有模型在相同 prompt、相同解码参数（greedy, max\\_length 50/512）下推理。\n- 结果：\n- 每棵树均发现满足“Obscurity+Excellence+Dominance”三条件的隐藏宝石，例如\n– Llama-3.1-8B 树某数学微调 GSM8K 准确率 96.0 %，比官方 Instruct 提升 12.8 %，月下载量仅 930。\n– Mistral-7B 树某通用微调 RouterBench 提升 14.0 %，月下载量仅 1。\n- 超过 90 % 的宝石在模型卡片中**没有任何**对应任务性能记录，文本检索无法发现。\n2. 模型质量分布统计——支撑“激进剪枝”假设\n对每棵树分别绘制 GSM8K、MBPP、RouterBench 的累积准确率曲线，发现 60–99 % 的模型距离最佳模型 > 10 % 绝对分，说明“绝大多数候选可早期快速淘汰”。\n\n3. 固定预算 Best-Arm Identification 主实验——验证搜索效率\n\n- 设置 5 档预算：每模型平均 10、25、50、100、200 次查询，总预算  B = K × N 。\n- 对比 8 条基线：Uniform、UCB、UCB-StdDev、UCB-E、Successive Rejects、TTTS、BayesElim、标准 Sequential Halving。\n- 评价指标：100 次随机种子下的\n– 检索模型在完整 2 500 题上的**真实平均 rank**（↓越好）\n– 检索模型的**真实准确率**（↑越好）\n- 结果（50 查询示例）：\n\n| 树 | 基线最佳 rank/acc | 本文方法 rank/acc |\n| --- | --- | --- |\n| Qwen-3B | 30.0 / 0.721 | 3.5 / 0.729 |\n| Qwen-7B | 28.9 / 0.784 | 3.6 / 0.790 |\n| Mistral-7B | 4.0 / 0.693 | 1.6 / 0.695 |\n| Llama-8B | 29.9 / 0.720 | 3.0 / 0.736 |\n| 在全部 20 组（4 树×5 预算）设定中，本文方法均取得最低平均 rank 与最高平均准确率；50 次查询即可逼近“全局最优”性能（Oracle 第 1 行）。 |\n\n4. 消融实验——量化两项改进各自贡献\n\n- 仅替换调度器（保留随机抽样）：10 查询平均 rank 提升约 30 位。\n- 仅去掉相关采样（保留激进剪枝）：50 查询平均 rank 再降 2–3 位。\n- 两者结合取得最佳结果，验证“早期共享题集 + 快速削至 100 候选”缺一不可。\n5. 超预算鲁棒性实验\n用 25 查询的本文方法 vs 50 查询的基线，以及 50 查询的本文方法 vs 100 查询的基线，前者仍显著优于后者，说明加速效果并非单纯靠“砸预算”。\n\n6. 文档缺失分析——解释为何宝石被埋没\n手动检查 24 颗已发现宝石：19 颗无任何性能文档，3 颗仅提供无关任务（如东南亚多语）结果，仅 2 颗有微弱相关记录，文本检索几乎无法召回。\n\n综上，实验从“存在性→分布特性→检索效率→组件贡献→现实障碍”五个维度系统支撑了论文主张。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“方法改进”“场景扩展”“理论分析”与“生态研究”四类，供后续工作参考：\n\n方法改进\n\n1. **权重空间先验 + 老虎机**\n利用权重空间表征网络（Schürholt 2021；Horwitz 2025a）预测初始性能 μ̂\\_i，再用 μ̂\\_i 作为 SH/UCB 的先验均值，实现“零查询冷启动”与查询自适应分配。\n\n2. **任务-查询联合选择**\n将查询选择（Polo 2024；Zouhar 2025）纳入 Bandit 框架，每轮同时决策“测哪些模型”与“用哪些题”，实现预算双重压缩。\n\n3. **多目标 Bandit**\n同时优化准确率、推理延迟、显存占用，形成 Pareto 前沿，而非单指标最佳臂。\n\n4. **在线更新与漂移检测**\n仓库持续上传新版本，可把算法扩展为“非静态 Bandit”，用漂移检测（如 CUSUM）触发重评估，避免模型过时。\n\n场景扩展\n\n5. **多模态/多语言树**\n将实验从纯文本 LLM 扩展到 Vision-Language、Audio 或 200+ 语种模型树，验证宝石现象是否依旧成立。\n\n6. **垂直领域深度挖掘**\n针对医学、法律、金融等低资源但高风险的垂直任务，构建领域专用 500-题速评集，看是否能挖出超越通用榜前十的“小透明”。\n\n7. **指令-攻击-鲁棒性三重评估**\n除了任务准确率，同时评估指令跟随能力（IFEval）与对抗鲁棒性（AdvGLUE），检验宝石是否存在“偏科”或安全隐患。\n\n理论分析\n\n8. **样本复杂度下界**\n在“共享查询 + 早期剪枝”条件下，推导新的 Best-Arm 样本复杂度下界，并与标准 SH 的 Ω(H log K) 比较，给出理论加速比。\n\n9. **方差缩减量刻画**\n量化 Correlated Sampling 对 Var\nμ̂_i − μ̂_j\n 的缩减系数，与题目难度分布的熵建立显式关系，指导查询集构造。\n\n10. **早期剪枝错误率控制**\n用多重假设检验框架（如 FDR）设定首轮淘汰阈值，给出“好模型被误杀”概率的上界，保证 aggressive 剪枝的可靠性。\n\n生态与工具\n\n11. **社区实时推荐系统**\n把算法封装成 Hugging Face Space 或 CLI 插件，用户输入任务描述 + 预算，云端 5 分钟返回 top-5 候选，并生成可复现的评测报告。\n\n12. **宝石成因挖掘**\n对隐藏宝石进行训练数据重建（Salama 2025）与超参反演，分析“数据配方”或“学习率调度”是否与性能跃升显著相关，形成“如何复现宝石”指南。\n\n13. **供应链图谱**\n结合模型血缘恢复（Horwitz 2025c；Yu 2025）与下载-性能二部图，研究“上游基础模型 → 下游微调”网络结构，量化信息孤岛与发现瓶颈。\n\n14. **可持续评估基准**\n构建动态最小可评测集（Minimal Viable Benchmark），每年用信息增益指标淘汰 10 % 冗余题目，降低整个社区评估碳排。\n\n一句话总结\n\n从“更快、更准、更全面”到“为何存在、如何再现、怎样服务社区”，上述任意一条均可作为下一步的探索切口。\n\nQ6: 总结一下论文的主要内容\n\n**主要内容速览**\n\n1. **问题**\n公共仓库（HF）月下载量 95 % 集中在 0.0015 % 的“明星”模型；用户默认选官方 checkpoint，未必最优。\n\n2. **发现**\n在 4 棵主流模型树共 2 000+ 微调上系统评测 → 每棵树均存在“隐藏宝石”：\n\n- 下载量排后 99 %，却**严格优于**最受欢迎版本\n- Llama-3.1-8B 数学任务提升 12.8 %（83.2 → 96.0），零额外推理成本\n- 90 % 宝石无任何性能文档，文本检索无法发现\n\n3. **方法**\n将“大海捞针”建模为**固定预算 Best-Arm Identification 多臂老虎机**：\n\n- 基础：Sequential Halving 多轮淘汰\n- 加速 1：Correlated Sampling——每轮所有候选答同一批题，消减难度方差\n- 加速 2：Aggressive Elimination——首轮即削至 100 候选，60 % 预算砸前筛\n→ 50 次查询/模型即可锁定 top-3，速度提升 50×，平均准确率 +4.5 %\n\n4. **实验**\n\n- 5 档预算（10–200 查询）（×）4 模型树（×）8 条基线，100 轮重复\n- 本文方法在所有设定下均取得**最低平均 rank**与**最高真实准确率**\n- 消融与超预算对比证实两项改进各自贡献且可减半预算\n\n5. **结论**\n公共仓库并非“高效市场”；大量高性能微调被埋没。提出的加速 Sequential Halving 使“挖宝石”在常规算力范围内即可实现，为社区模型选择提供实用工具。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Discovering%20Hidden%20Gems%20in%20Model%20Repositories，链接是https://arxiv.org/pdf/2601.22157，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22157。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22156v1-1769990308644","title":"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts","description":"Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data","url":"https://arxiv.org/abs/2601.22156v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:53.000Z","metadata":{"arxivId":"2601.22156v1","authors":"Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu","categories":"cs.CL, cs.AI, cs.LG","published":"2026-01-29T18:59:53Z","pdfUrl":"https://arxiv.org/pdf/2601.22156v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22156v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22156v1","rank":3,"domain":"Evaluation","zh_summary":"混合 Transformer 架构结合了 softmax 注意力模块和循环神经网络（RNN），在长上下文建模中表现出理想的性能与吞吐量折衷，但由于从零进行大规模预训练的成本过高，其应用和研究受到限制。一些近期研究表明，预训练的 softmax 注意力模块可以通过参数传递和知识蒸馏转换为 RNN 模块。然而，这些转换方法需要大量训练数据（超过 100 亿个 token），且得到的混合模型在长上下文下的表现也较差，而混合模型在这种场景下本可以比基于 Transformer 的模型获得显著的推理加速。在本文中，我们提出了 HALO（通过层优化实现的混合注意力），这是一种将 Transformer 模型蒸馏为 RNN-注意力混合模型的流程。随后，我们提出了 HypeNet，一种通过新颖的位置编码方案（称为 HyPE）和各种架构改进实现出色长度泛化能力的混合架构。我们使用 HALO 将 Qwen3 系列转换为 HypeNet，实现了与原始 Transformer 模型相当的性能，同时在长上下文性能和效率上表现出优势。该转换仅需 23 亿个 token，不到其预训练数据的 0.01%。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. 现有混合 Transformer-RNN 架构的“高门槛”预训练成本\n混合模型（交错 softmax attention 与线性 RNN 层）在长上下文场景下兼具性能与吞吐优势，但通常需从头预训练数百亿级 tokens，对学术团队极不友好。\n\n2. 已有蒸馏方案的两大瓶颈\n\n- 数据量依旧巨大：先前将预训练 Transformer 蒸馏为混合架构的方法仍需 ≥10 B tokens。\n- 长上下文 recall 严重退化：蒸馏后的混合模型在 128 K 以上长度任务上性能骤降，丧失混合架构本应具备的推理效率优势。\n3. 目标\n提出一种**极低成本**（<3 B tokens）的跨架构蒸馏流程，使预训练 Transformer 可直接转换为**长上下文表现更强、推理更高效**的混合模型，而无需从头预训练。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为三类，均围绕“如何把预训练 Transformer 转化为线性复杂度的序列建模结构”展开：\n\n1. 混合架构（Hybrid Attention-RNN）\n\n- Jamba (Lieber et al., 2024)\n- MiniMax-01 (MiniMax et al., 2025)\n- Qwen3-Next (Qwen, 2025)\n- Kimi-Linear (Kimi et al., 2025)\n- NVIDIA Nemotron 3 (NVIDIA et al., 2025)\n共同点：从头预训练百亿级参数，attention 层与 RNN/SSM 层交错；未提供低成本转换方案。\n2. 蒸馏式“Attention → Hybrid”方法\n\n| 方法 | 发布 | 训练 tokens | 关键指标 |\n| --- | --- | --- | --- |\n| Mamba-in-the-Llama (Wang et al., 2025b) | 2024-08 | 20 B | 均匀替换 attention 层，长程 recall 下降明显 |\n| SMART (Yang et al., 2026) | 2025-05 | >7 B | 基于输出分布漂移挑选保留层 |\n| RAD (Hoshino et al., 2025) | 2025-05 | 20 B | 提出冗余度度量，仍需大量数据 |\n| Jet-Nemotron (Gu et al., 2025) | 2025-08 | 400 B | 任务性能下降指导层选择，成本最高 |\n| KL-LS (Li et al., 2025) | 2025-12 | 25 B | 逐层重跑蒸馏，用 KL 散度定重要性 |\n\n3. 纯 RNN/线性注意力蒸馏\n\n- RADLADS (Goldstein et al., 2025) — 本文 Stage-1 的对齐策略即在其基础上改进。\n- LoLCATs (Zhang et al., 2025)、Bick et al. (2025) — 将 attention 整体替换为线性形式，不涉及混合。\n\n上述工作或为“高成本预训练”，或为“十亿级 token 蒸馏且长上下文性能退化”。本文 HALO 首次把训练数据压缩到 2.3 B tokens 以内，同时通过 HyPE 与 Lightning Attention 等改进显著提升了 128 K–1 M 长度的 recall。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **HALO（Hybrid Attention via Layer Optimization）** 流水线与 **HypeNet** 架构，从“蒸馏效率”与“长程泛化”两条路径同时突破：\n\n1. 极低成本蒸馏流水线（HALO）\n\n- **Stage-0 权重热启动**：直接把 Teacher 的  W_q,W_k,W_v,W_o  填入线性 RNN 的对应投影，省去随机初始化。\n- **Stage-1 隐状态对齐**：仅训练 RNN 层，最小化\n\nmathcal L_(stage1)^((l))=MSE!(Y^((l))_(teacher),; RNN^((l))(X^((l-1))))\n\n用 320 M tokens 即可得到“逐层替身”。\n- **Stage-1.5 重要性评分**：一次性推理得到每层被替换后的\n\ns_i=max_j(R_(orig)-R^((i))_j)max_j(C_(orig)-C^((i))_j)+varepsilon\n\n按  s_i  排序保留 top-k（k=⌊L/4⌋）attention 层，无需反复重训。\n- **Stage-2 端到端蒸馏**：固定 Teacher，用 1 B tokens 做 KL 蒸馏\n\nmathcal L_(stage2)=D_(KL)!(f_(orig)parallel f_(hybrid))\n\n- **Stage-3 长上下文微调**：再用 1 B tokens、16 K 长度、低 LR 微调，总开销 2.3 B tokens。\n2. 面向长程泛化的 HypeNet 架构\n\n- **HyPE 位置编码**\n– Attention 层用 **NoPE**，保证无训练长度外推；\n– RNN 层用 **RoPE**，弥补局部位置信息；\n– 推理时对 attention logits 做**动态缩放**\n\ntilde s_t=log_a(t+a),quad a∈500,600,900\n\n抑制长序列熵增导致的注意力分散。\n- **四项结构微调**\n➊ RNN 也加 QK-Norm；➋ GQA→MHA 解除 KV 头共享；➌ 给 RNN/Attention 都加输出门；➍ 去掉短卷积以减少 CUDA kernel 开销。\n- **通用 RNN Mixer 插槽**\n只要满足\n\nS_t=F_t S_(t-1)+k_t^top v_t,quad y_t=q_t S_t W_o\n\n即可接入；实验表明 **Lightning Attention**（数据无关 forget gate）在长度泛化与吞吐间取得最佳平衡。\n3. 结果\n把 Qwen3-1.7B/4B/8B 用 HALO 转成 HypeNet 后，\n\n- 128 K 上下文 NIAH 平均准确率 >90 %，而 Jet-Nemotron、KL-LS 同期方法在 128 K 已跌至 <30 %；\n- 512 K 长度下解码速度提升 3.0×，预填充 3.4×，且显存占用随序列长度线性增长，Teacher 在 1 M 长度即 OOM。\n\n综上，论文通过“分层重要性筛选 + 轻量对齐 + HyPE 缩放”将蒸馏数据压缩两个数量级，并首次使蒸馏型混合模型在长上下文召回任务上超越原始 Transformer。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 6 组实验，覆盖「主结果—消融—效率」三个层次，全部在 1×A800-80 GB 上完成，代码与 checkpoint 已开源。\n\n1. 主实验：Qwen3 → HypeNet 蒸馏\n\n- 模型规模：1.7 B → 2 B、4 B → 5 B、8 B → 9 B\n- 训练数据：FineWeb-Edu 共 2.3 B tokens（Stage-1 320 M + Stage-2 1 B + Stage-3 1 B）\n- 评测任务\n– CSR：ARC-E/C、HellaSwag、WinoGrande、PIQA、LAMBADA、MMLU 共 7 项零样本任务，取归一化准确率平均值。\n– 长程 recall：RULER 的 NIAH-Single-1/2/3，长度 32 K–256 K；额外测到 1 M。\n- 对比基线\n– 原始 Qwen3（Teacher，128 K 用 YaRN 外推）\n– 同期蒸馏混合模型：Jet-Nemotron-2 B（400 B tokens）、KL-LS-GDN-3 B（25 B tokens）\n- 关键结果\n– 128 K 平均 NIAH 90 %↑，而 Jet-Nemotron 与 KL-LS 分别跌至 0 % 与 14 %。\n– CSR 平均仅比 Teacher 低 1–2 分，但 128 K 上下文吞吐提升 2.4–3.0×，显存节省 40 % 以上。\n2. 从头训练消融：验证 HyPE 与 Mixer 选型\n\n- 500 M 参数、20 B tokens、4 K 上下文训练，测 512–256 K 外推。\na) 位置编码对比\n– 全 RoPE、全 NoPE、Attn-RoPE+RNN-NoPE、SWAN-GPT、HyPE\n– 结果：HyPE 在 64× 训练长度仍保持 93.5 % NIAH，显著优于仅 RoPE（20 %）与仅 NoPE（60 %）。\nb) RNN Mixer 对比\n– Lightning、Mamba2、GLA、GDN、RWKV-7\n– Lightning 在 128 K 取得 95 % NIAH，同时预填充速度比 Mamba2 快 3.6×。\n3. HALO 架构消融：逐组件剔除\n在 Qwen3-1.7 B→2 B 的同一 Teacher 上，保留其他条件不变，仅移除下列模块：\n\n- ➊ RNN-RoPE → 128 K NIAH 从 79.9 % 降至 47.9 %\n- ➋ RNN-QK-Norm → 128 K 降至 17.3 %\n- ➌ GQA→MHA → 128 K 略升 83.5 %，但 256 K 从 74.3 % 降至 58 %\n- ➍ 输出门（RNN 或 Attention）（-1.5～-5.4 分不等）\n证明四项改进均对长程 recall 有正向作用。\n4. HALO 层选择策略对比\n同一 HALO 流程，仅替换“保留哪些 attention 层”的算法：\n\n- 均匀分布、仅后半均匀、Jet-Nemotron 任务下降法、KL-LS 逐层重蒸馏法\n- 结果：本文提出的 recall/CSR 比值评分法在 8 K–256 K NIAH 上平均领先 8–15 分，且零额外训练成本。\n5. 训练超参消融\na) Stage-1 数据量：320 M 已饱和，再增到 1.3 B 反而过拟合。\nb) Stage-2 峰值 LR：1e-4 最佳；RADLADS 推荐的常数 1e-5 导致 128 K NIAH 掉到 60 %。\n\n6. 效率实测\n\n- 测解码延迟（TPOT）与预填充时间，batch=1、BF16、Flash-Attention-2 / Triton kernel。\n- 512 K 上下文：HypeNet-1.7 B 解码 3.0×，预填充 3.4×；Qwen3 在 1 M 长度 OOM，HypeNet 仍可用 73 GB。\n- 各 Mixer 预填充对比：Lightning 在 256 K 单 token 时延 6 ms，与 SWA-512 持平，比 Mamba2 快 4× 以上。\n\n综上，实验从「可比性能+更低成本」「更长外推」「更快推理」三个维度系统验证了 HALO 与 HypeNet 的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下问题在论文中尚未完全解决，可作为后续工作的直接延伸：\n\n1. 指令遵循与对齐恢复\nHALO 仅用 FineWeb-Edu 预训练语料，导致原模型的指令跟随、对话安全等后训练能力被稀释。\n\n- 如何设计「低成本、数据高效」的对齐恢复阶段（例如 LoRA+RLHF、DPO）？\n- 是否需要保留部分原始对话数据或引入合成指令数据？\n2. 层选择与压缩极限\n目前固定保留 25 % attention 层；若继续降低至 15 % 甚至 5 %，recall 性能下降曲线如何？\n\n- 能否用可微结构搜索（NAS）一次性联合优化「层类型 + 层位置 + 头数」？\n- 动态/自适应深度：推理时依据输入长度自动决定启用多少 attention 层。\n3. 长度外推机制再深化\nHyPE 的 logits 缩放仅使用一个全局超参 a。\n\n- 能否让缩放系数 st 与内容或头维度相关，实现「头相关、token 相关」的自适应缩放？\n- 进一步结合相对位置惩罚（ALiBi、xPos）或频域校正，实现 2 M+ 长度无损外推。\n4. 多模态与工具调用\n本文仅验证文本模态。\n\n- 视觉/语音交叉注意力层是否同样适用 HALO 的「重要性评分」筛选？\n- 工具调用、代码解释器等需要强召回的场景，RNN 层是否会出现「关键 API 遗忘」？\n5. 异构硬件与量化协同\nHypeNet 的 KV-cache 已大幅缩减，但新增输出门与 MHA 带来 10 % 参数量上升。\n\n- 与 4-bit/2-bit 量化、KV-cache 4-bit 压缩、Flash-Linear Attention 专用 CUDA kernel 联合优化，能否在边缘 GPU 实现 1 M 上下文 30 tokens/s？\n- 探索「遗忘门常量化」后是否可直接用 INT8 矩阵乘法，进一步提速。\n6. 持续学习与领域适应\n蒸馏后模型继续在新领域增量预训练时，RNN 层与 attention 层的「可塑性」差异显著。\n\n- 如何设计不同的学习率或正则策略，防止 RNN 层过早遗忘旧知识？\n- 能否用 expansion/compression 框架动态增长或剪枝 RNN 隐状态维度？\n7. 理论分析\n\n- 对 HyPE 给出长度泛化的严格证明：NoPE attention 的误差界与 RNN 的 receptive field 如何耦合？\n- Lightning Attention 的数据无关 forget gate 与数据相关 gate 的「长度外推」能力差异，能否用谱范数或梯度流解释？\n8. 非 Transformer 教师模型\n目前 HALO 假设教师为纯 Transformer。\n\n- 若教师本身已是混合结构（如 Jamba），如何递归蒸馏「混合 → 更稀疏混合」？\n- 对于 MoE、RetNet、TTT 等不同范式，HALO 的「重要性评分」是否仍然有效？\n9. 自动超参搜索\n缩放底数 a、峰值 LR、层比例 k 目前靠网格搜索。\n\n- 用贝叶斯优化或强化学习把「a + LR + k」联合搜索，目标函数为 Pareto（性能, 吞吐, 显存），能否再节省 30 % 搜索时间？\n10. 绿色 AI 评估\n给出端到端碳排与美元成本：2.3 B tokens × A800 GPU hours = ? kg CO₂，与从头预训练相比减少几个数量级？\n建立「每美元准确率」「每克碳 recall」指标，推动社区采用蒸馏而非重训。\n\n这些方向既有理论价值（长度泛化、重要性度量），也能直接落地（指令恢复、硬件量化、多模态），可作为后续研究的切入点。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：混合 Transformer-RNN 模型需百亿级从头预训练，现有蒸馏法仍耗 10 B+ tokens 且长上下文 recall 暴跌。\n- **方法**：提出 **HALO** 流水线（2.3 B tokens）（权重热启动→隐状态对齐→基于 recall/CSR 重要性选层→KL 蒸馏→长上下文微调），并设计 **HypeNet** 架构：\n– **HyPE**：Attention 层用 NoPE 保长度外推，RNN 层用 RoPE 补位置信息，加动态 logits 缩放。\n– 四项结构微调：RNN 加 QK-Norm、GQA→MHA、输出门、去短卷积。\n– 通用 RNN Mixer 插槽，Lightning Attention 表现最佳。\n- **实验**：Qwen3 1.7/4/8 B→2/5/9 B HypeNet，128 K 上下文 NIAH>90 %（同期方法<30 %），CSR 与 Teacher 持平，512 K 解码 3.0×、预填充 3.4× 提速，1 M 长度 Teacher OOM 而 HypeNet 仍可行。\n- **结论**：首次实现“极低数据成本蒸馏 + 卓越长程性能”的混合模型，为高效长上下文 LLM 提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid%20Linear%20Attention%20Done%20Right%3A%20Efficient%20Distillation%20and%20Effective%20Architectures%20for%20Extremely%20Long%20Contexts，链接是https://arxiv.org/pdf/2601.22156，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22156。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22155v1-1769990308644","title":"UEval: A Benchmark for Unified Multimodal Generation","description":"We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.","url":"https://arxiv.org/abs/2601.22155v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22155v1","authors":"Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu","categories":"cs.CV, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22155v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22155v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22155v1","rank":4,"domain":"Evaluation","zh_summary":"我们介绍了 UEval，这是一个用于评估统一模型的基准，即能够生成图像和文本的模型。UEval 包含 1,000 个由专家策划的问题，这些问题要求模型输出同时包含图像和文本，来源于 8 个现实任务。我们策划的问题覆盖了广泛的推理类型，从逐步指南到教材说明。评估开放式多模态生成并非易事，因为简单的“使用大型语言模型作为评判者”的方法可能会忽略细微差别。不同于以往依赖多模态大型语言模型（MLLM）来评估图像质量或文本准确性的工作，我们在 UEval 中设计了基于评分标准的评分系统。对于每个问题，提供参考图像和文本答案给 MLLM 生成初始评分标准，该标准包含多个评估标准，然后由人工专家进行完善和验证。总的来说，UEval 包含 10,417 个验证过的评分标准，实现了可扩展且细粒度的自动评分。UEval 对现有统一模型具有挑战性：GPT-5-Thinking 的得分仅为 100 分中 66.4 分，而表现最好的开源模型仅达到 49.1 分。我们观察到，推理模型通常优于非推理模型，将推理模型的推理轨迹转移到非推理模型上会显著缩小差距。这表明，推理能力可能对于需要复杂多模态理解和生成的任务非常重要。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在填补“统一多模态生成”评测的空白。现有基准主要将视觉任务割裂为两类独立范式：\n\n- 视觉问答（VQA）：输入图像→输出文本\n- 文生图（T2I）：输入文本→输出图像\n\n二者均未考察**同一查询需同时生成图像与文本**的真实场景，例如“请画出示意图并解释潮汐成因”。为此，作者提出 UEval 基准，核心贡献与目标可归纳为：\n\n1. 构建评测集\n\n- 1 000 条专家筛选问题，覆盖 8 类真实任务（空间、教科书、图表、论文、艺术、生活、技术、运动）。\n- 每条样本要求模型**联合输出图像+文本**，并附带人工校验的参考答案。\n2. 设计细粒度评分协议\n\n- 采用“数据相关”的 rubric 机制：针对每题自动生成十余条评分细则（共 10 417 条），由人类二次校准。\n- 用前沿 MLLM 作评委，按细则逐项打分，实现可扩展、可复现的自动评估。\n3. 验证基准难度与洞察\n\n- 闭源最佳模型 GPT-5-Thinking 仅得 66.4/100，开源最佳仅 49.1，显示任务极具挑战性。\n- 发现“推理型”模型显著优于非推理型，且将推理轨迹迁移给非推理模型可缩小差距，提示**思维链机制对多模态生成同样关键**。\n\n综上，论文首次系统评估了**统一模型在“同时生成图像与文本”上的推理与生成能力**，并揭示当前模型在跨模态一致性、多步规划等方面仍存在显著不足。\n\nQ2: 有哪些相关研究？\n\n与 UEval 直接相关的研究可划分为三条主线：\n\n1. 统一多模态理解与生成模型；\n2. 多模态评测基准；\n3. 细粒度/数据相关评分方法。\n\n1\\. 统一多模态模型\n\n- **Transfusion** (Zhou et al., 2025a) —— 同一 Transformer 内自回归预测文本、扩散生成图像。\n- **Emu3/Emu3.5** (Wang et al., 2024; Cui et al., 2025) —— 纯下一 token 预测实现文本-图像双向生成。\n- **DreamLLM** (Dong et al., 2024) —— 协同训练 LLM 与扩散解码器，支持交错图文生成。\n- **Janus/Janus-Pro** (Wu et al., 2025a; Chen et al., 2025d) —— 解耦视觉编码，统一自回归框架。\n- **Show-o2** (Xie et al., 2025) —— 离散扩散+自回归混合，支持图文交错输出。\n- **BAGEL** (Deng et al., 2025) —— 连续/离散潜变量统一训练，兼顾理解与生成。\n\n2\\. 多模态评测基准\n\n- **VQA 系列**\n- VQA-v2 (Goyal et al., 2017)\n- OK-VQA (Marino et al., 2019)\n- MMBench (Liu et al., 2024b)\n- MMMU (Yue et al., 2024)\n- BLINK (Fu et al., 2024) —— 侧重视觉感知细节。\n- **文生图评测**\n- T2I-CompBench (Huang et al., 2023) —— 组合属性一致性。\n- Geneval (Ghosh et al., 2023) —— 对象级对齐。\n- ScienceT2I (Li et al., 2025a) —— 科学概念可视化。\n- **交错/统一生成评测**\n- OpenLEAF (An et al., 2024) —— 开放域交错图文生成。\n- MMIE (Xia et al., 2025) —— 大规模交错理解基准。\n- RealUnify (Shi et al., 2025b) —— 统一模型综合探针。\n- Uni-MMMU (Zou et al., 2025) —— 多学科统一评测。\n\n3\\. 细粒度或数据相关评分\n\n- **HealthBench** (Arora et al., 2025) —— 针对医疗问答自动生成 rubric，人类二次校验；UEval 的 rubric 流程受此启发。\n- **VDC** (Chai et al., 2025) —— 引入样本相关评分提示，缓解通用 prompt 带来的偏差。\n\n上述工作为 UEval 提供了模型架构、评测维度与自动化评分三方面的理论与方法基础。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 设计可扩展评分协议 + 大规模实验诊断”三步解决统一多模态生成缺乏标准评测的问题。\n\n1. 构建 UEval 基准\n\n- 任务层面：精选 8 类真实场景（空间、教科书、图表、论文、艺术、生活、技术、运动），共 1 000 题；每题必须同时输出图像与文本。\n- 数据层面：人工撰写参考文本+检索/绘制参考图像，形成高质量答案对。\n- 划分类型：\n– 闭式任务（空间/教科书/图表/论文）——答案客观、明确；\n– 开式任务（艺术/生活/技术/运动）——允许多种合理步骤图。\n2. 设计数据相关 Rubric 评分协议\n\n- 自动初稿：用 Gemini-2.5-Pro 针对“问题+参考答案”生成 10 余条细粒度准则（图像/文本约各占一半）。\n- 人工精修：两轮审核去重、补漏，最终得到 10 417 条无歧义准则。\n- 自动评分：再以 Gemini-2.5-Pro 作评委，逐条判定模型响应是否满足准则，计算满足率作为最终得分；与人评 Pearson r = 0.88，≈90% 准则一致。\n3. 大规模实验与诊断\n\n- 评测 9 个最新统一模型（5 开源 + 4 闭源），给出 8 任务排行榜，揭示：\n– 闭源最佳 GPT-5-Thinking 仅 66.4/100，开源最佳 Emu3.5 仅 49.1，基准足够困难。\n– 多步规划类任务（艺术、生活等）得分显著低于知识类任务；模型常出现步骤图序号错乱、视觉一致性差。\n– 推理模型全面优于非推理模型；把 GPT-5-Thinking 的 Chain-of-Thought 轨迹拼到非推理模型 prompt 后，其图像准确率明显提升（对弱模型无效），证实“推理”对跨模态生成的重要性。\n\n通过以上三步，论文首次实现了对“统一模型同时生成图像+文本”能力的标准化、可复现且细粒度的评测，并量化揭示了现有模型的短板与改进方向。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「统一多模态生成」共开展 4 组实验，覆盖模型性能、评分可靠性、裁判模型差异与推理迁移效果。\n\n1. 主评测实验（UEval 排行榜）\n\n- 被测模型：9 个最新统一模型\n– 开源：Janus-Pro、Show-o2、MMaDA、BAGEL、Emu3.5\n– 闭源：Gemini-2.0-Flash、Gemini-2.5-Flash、GPT-5-Instant、GPT-5-Thinking\n- 指标：以 Gemini-2.5-Pro 为评委，按 10 417 条 rubric 计算「满足率」。\n- 结果：\n– 闭源全面领先，GPT-5-Thinking 平均 66.4，开源最佳 Emu3.5 仅 49.1。\n– 多步规划任务（art/life/tech/exercise）得分远低于知识任务（space/textbook/diagram/paper）。\n– 图像得分普遍低于文本得分；开源模型图像平均 <15，文本亦低于纯文本模型。\n2. 人类一致性验证\n\n- 随机抽取 10 % GPT-5-Thinking 输出，由人工独立标注每条 rubric 是否满足。\n- 指标：准则级匹配率、样本级 Pearson r。\n- 结果：≈90 % 准则匹配，r = 0.88，证明 LLM-as-a-judge 可替代人工。\n3. 不同评委模型敏感性实验\n\n- 用 6 个额外 MLLM（GPT-5-Thinking、Seed1.6-Vision、Qwen3-VL-235B-Thinking/Instruct、GLM-4.1V-Thinking）对同一批 GPT-5-Thinking 响应再评分。\n- 结果：\n– Gemini-2.5-Pro、GPT-5-Thinking、Qwen3-VL-235B-Thinking 给出高度一致分数；\n– Seed1.6-Vision、GLM-4.1V-Thinking 在开放任务打分显著偏高，提示评委选择影响结论。\n4. 推理轨迹迁移实验\n\n- 方法：将 GPT-5-Thinking 产生的 Chain-of-Thought 段落直接追加到原 prompt，再喂给非推理模型（GPT-5-Instant、Gemini-2.5-Flash、BAGEL）。\n- 指标：图像准确率变化（人工盲审）。\n- 结果：\n– 前两者生成的图像更接近参考答案（如自由女神像皇冠内部结构），得分提升；\n– BAGEL 等较弱模型无改善，说明「有效利用推理信号」本身需要足够强的生成能力。\n\n上述实验系统验证了 UEval 的难度、评分可靠性，并首次量化证明「推理能力」对统一多模态生成的正向作用。\n\nQ5: 有什么可以进一步探索的点？\n\n可进一步探索的方向（按研究类别分组）\n\n1\\. 基准与评测协议\n\n- **动态/可扩展 rubric**\n当前 rubric 为静态人工校验，可探索“迭代式”或“对抗式”生成：让评委模型与候选模型多轮博弈，自动发现新失败模式并即时补充准则。\n\n- **多图像一致性指标**\n开式任务含多图序列，可引入“跨帧 CLIP 相似度”“对象检测 ID 一致性”等自动度量，与 rubric 打分联合拟合，减少评委模型开销。\n\n- **人类-模型混合评审**\n对高分段样本（>90）仍显模糊，可采用“模型预筛 + 人工终审”的主动学习循环，进一步压缩人工成本。\n\n2\\. 模型能力诊断\n\n- **细粒度错误归因**\n将 rubric 准则按“视觉生成-文本生成-跨模态对齐”三级标签分类，统计失败分布，定位是“画不对”还是“说不对”抑或“图文不符”。\n\n- **多步规划深度扩展**\n当前最长 5–6 步，可构造 10+ 步的复杂流程（实验协议、手工折纸等），检验模型长程一致性及中间步骤可复现性。\n\n- **多图联合扩散 vs. 自回归**\n对同一问题分别用“一次扩散出整图网格”与“逐图自回归”两种策略，评估视觉一致性、推理链完整性，寻找最佳生成范式。\n\n3\\. 推理机制\n\n- **推理轨迹可迁移性上限**\n系统研究“评委模型-生成模型”能力 gap 阈值：当生成模型参数量/数据量低于何值时，外部推理轨迹不再有效。\n\n- **视觉思维链（Visual-CoT）**\n让模型在生成每幅图像前输出“中间草图”或“布局描述”，形成显式视觉推理链，检验能否进一步提升一致性与可解释性。\n\n- **逆向推理：文本→图像→文本**\n先让模型生成图像，再屏蔽原 prompt 仅让另一模型依据图像生成解释，循环评估可重建性，量化图像的信息完备度。\n\n4\\. 数据与任务\n\n- **引入时序/动态内容**\n将静态图扩展为短视频片段（3–5 秒），要求模型生成“关键帧+解说”，评测统一模型对时序因果的理解。\n\n- **跨语言与文化**\n将 prompt 与 rubric 本地化到多语言、多文化场景（如中国古建筑、阿拉伯图案），观察模型是否出现文化偏差或视觉符号误用。\n\n- **专业领域深度**\n在医学、法律、机械维修等高风险领域收集小样本，用 UEval 式 rubric 评估，验证统一模型在专业场景下的可靠性。\n\n5\\. 效率与鲁棒性\n\n- **评委模型效率优化**\n探索“小模型蒸馏 + 准则级早期退出”策略，在保持 0.85+ 人类相关度的前提下，将评审成本降低一个数量级。\n\n- **对抗攻击与鲁棒性**\n对图像生成加入隐写扰动或对文本加入同义词扰动，观察评委模型是否被误导，建立鲁棒性评分曲线。\n\n6\\. 统一模型改进\n\n- **联合训练目标加权**\n依据 UEval 的图像-文本 rubric 失败率，动态调整训练损失权重，实现“哪里弱训哪里”的自适应优化。\n\n- **检索增强生成（RAG-for-Generation）**\n允许模型在生成前检索参考图或知识图谱，检验外部记忆能否弥补多步规划与细节一致性缺陷。\n\n这些方向可分别从基准、诊断、推理、数据、鲁棒性与训练等角度推进，进一步揭示统一多模态模型的能力边界与改进路径。\n\nQ6: 总结一下论文的主要内容\n\n**UEval: A Benchmark for Unified Multimodal Generation** 提出首个系统评估“同一 prompt 下同时生成图像+文本”能力的基准，核心内容如下：\n\n1. 问题定义\n现有基准仅分别评测 VQA（图→文）或 T2I（文→图），忽视真实场景需联合输出图文。UEval 填补该空白。\n\n2. 基准构建\n\n- 1 000 道专家筛选题，覆盖 8 类任务：空间、教科书、图表、论文（闭式）；艺术、生活、技术、运动（开式）。\n- 每题配参考图像+文本，人工校验。\n3. 评分协议\n\n- 数据相关 rubric：Gemini-2.5-Pro 针对每题自动生成十余条细则→人工精修，共 10 417 条。\n- 用同一 MLLM 评委按细则逐项打分，满足率即最终得分；与人评 Pearson r = 0.88。\n4. 主实验结果\n\n- 9 个统一模型参评：闭源最佳 GPT-5-Thinking 66.4/100，开源最佳 Emu3.5 仅 49.1，显示基准挑战大。\n- 多步规划任务得分远低于知识任务；模型常出现步骤图序号错乱、图文不一致。\n- 文本生成普遍强于图像；开源模型图像得分常 <15。\n5. 推理迁移实验\n将 GPT-5-Thinking 的 Chain-of-Thought 轨迹追加到非推理模型 prompt，其图像准确率显著提升，表明推理能力对多模态生成关键。\n\n6. 结论\nUEval 提供可扩展、细粒度的统一多模态生成评测，量化揭示当前模型在跨模态一致性、复杂规划与图像质量上的显著不足，为未来研究指明改进方向。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UEval%3A%20A%20Benchmark%20for%20Unified%20Multimodal%20Generation，链接是https://arxiv.org/pdf/2601.22155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22155。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22154v1-1769990308645","title":"Exploring Reasoning Reward Model for Agents","description":"Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.","url":"https://arxiv.org/abs/2601.22154v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22154v1","authors":"Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue","categories":"cs.AI, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22154v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22154v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22154v1","rank":5,"domain":"Evaluation","zh_summary":"主动强化学习（Agentic RL）在使智能体执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍然依赖基于稀疏结果的奖励来进行训练。这种反馈无法区分中间推理质量，导致训练结果不理想。在本文中，我们提出了智能体推理奖励模型（Agent-RRM），这是一种多方面的奖励模型，为智能体轨迹生成结构化反馈，包括 (1) 明确的推理轨迹，(2) 聚焦的批评，通过突出推理缺陷提供改进指导，以及 (3) 评估过程表现的总体评分。利用这些信号，我们系统地研究了三种整合策略：Reagent-C（文本增强的优化）、Reagent-R（奖励增强的指导）和 Reagent-U（统一反馈整合）。在 12 个多样化基准测试中的广泛评估表明，Reagent-U 带来了显著的性能提升，在 GAIA 上达到 43.7%，在 WebWalkerQA 上达到 46.2%，验证了我们推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来研究。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**Agentic Reinforcement Learning（Agentic RL）**中**稀疏、仅依赖结果（outcome-based）的奖励信号**所带来的两大核心缺陷：\n\n1. **无法区分中间推理质量**\n在长程、多步工具调用任务里，只要最终答案错误，整条轨迹就被视为完全失败，导致成功的前置步骤被埋没，训练信号过于粗粒度。\n\n2. **缺乏可操作的改进指引**\n纯标量奖励只给出“好/坏”评价，不提供具体哪里出错、如何修正的信息，使得智能体难以针对性改进推理与工具使用策略。\n\n为此，作者提出**Agent Reasoning Reward Model（Agent-RRM）**，一次性输出三种结构化信号：\n\n- 显式推理轨迹 `<think>`\n- 针对性批评 `<critique>`\n- 整体质量评分 `<score>`\n\n并基于该多维度反馈设计三种训练变体（Reagent-C / R / U），系统验证**密集推理奖励**在提升智能体推理与工具使用能力上的有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究划分为两条主线，并指出各自与本文工作的差异。以下按两条主线归纳代表性文献及其与 Agent-RRM 的关系。\n\n2.1 Agentic Reinforcement Learning\n\n**核心问题**：现有方法几乎全部采用**稀疏、结果导向**的奖励，无法对长程轨迹的中间步骤进行细粒度监督。\n\n| 代表工作 | 主要贡献 | 与本文差异 |\n| --- | --- | --- |\n| Search-R1 (Jin et al., 2025) | 用 RL 训练 LLM 交替执行搜索与推理，提升检索增强推理性能 | 仅依赖最终答案正确性作为奖励，无中间反馈 |\n| WebSailor (Li et al., 2025b) | 将 RL 扩展到长程网页导航，降低信息搜寻不确定性 | 同样使用稀疏结果奖励，无法纠正中间步骤错误 |\n| Agent0 (Xia et al., 2025) | 无需人工监督，让工具感知行为通过共进化自动涌现 | 未引入显式推理质量评估，训练信号仍稀疏 |\n\n2.2 Reward Modeling\n\n**核心问题**：现有奖励模型要么只给**步骤级标量分**（标注成本高、易被 hack），要么只做**成对偏好比较**（无法提供可执行改进建议）。\n\n| 代表工作 | 奖励信号形式 | 与本文差异 |\n| --- | --- | --- |\n| Atom-Searcher (Deng et al., 2025) | 直接拿 30B 模型给每一步打 0/1 分 | 仅步级标量，无语言解释，易奖励黑客 |\n| PPR (Xu et al., 2025) | 用预定义原则给步骤打分 | 同样是步级标量，缺乏可操作 critique |\n| RM-R1 (Chen et al., 2025d) | 先生成推理 rubric 再给出评分 | 面向通用问答，未针对 agent 工具链设计 |\n| R1-Reward (Zhang et al., 2025b) | 多模态推理奖励模型，引入稳定 RL 算法 | 仅输出标量，无文本 critique 用于 refinement |\n\n小结\n\n- **Agentic RL 侧**：首次把“**推理轨迹 + 批评文本 + 标量分**”统一引入 agent 训练流程。\n- **Reward Model 侧**：首次为 agent 设计**无需真值**即可生成**可执行批评**的多维奖励模型。\n\nQ3: 论文如何解决这个问题？\n\n论文将“稀疏结果奖励 → 细粒度推理奖励”的转化拆成三步：\n\n1. 设计**多维奖励模型** Agent-RRM；\n2. 构建**覆盖四种任务类型**的高质量数据集；\n3. 提出**三种集成策略**系统地把推理奖励喂给智能体。\n\n以下按方法论（Method）顺序展开。\n\n3.3 数据集构造：先解决“有信号”问题\n\n- **Reagent-RL-709K**\n– 来源：公开 QA + 自采复杂任务，共 70.9 万条。\n– 过滤：三阶段去歧义、去重、难度采样，保证轨迹多样性。\n\n- **Reagent-SFT-55.6K**\n– 用 DeepSeek-V3.1 生成轨迹，**只保留最终答案正确**的 5.56 万条，作为冷启动 SFT 数据。\n\n- **Agent-RRM 专用标注**\n– 在 709 K 上采样 118 K 轨迹，用 GPT-4o-mini-120B 标注三元组 `<think>｜<critique>｜<score>`，得到\n– Reagent-RRM-SFT-28K\n– Reagent-RRM-RL-90K\n\n3.4 Agent-RRM：再解决“信号可信”问题\n\n两阶段训练：\n\n1. SFT 阶段：在 28 K 上让 8 B 模型学会输出结构化三元组。\n2. GRPO 阶段：在 90 K 上用强化学习微调，使\n- 推理轨迹自洽（降低幻觉）\n- 标量分与人工排序一致性最高（Kendall τ 提升 0.12）\n\n**输出格式**（训练/推理一致）：\n\n```text\n<think>…逐步检查工具调用与逻辑链…</think>\n<critique>…指出具体错误：漏 browse、误用工具、循环调用…</critique>\n<score>0.32</score>\n```\n\n3.5 Reagent：最后解决“信号用好”问题\n\n三种集成策略对应三条消融曲线：\n\n| 变体 | 利用的信号 | 训练方式 | 关键公式 |\n| --- | --- | --- | --- |\n| Reagent-C | 仅 <critique> | 零参更新，in-context 二次生成 |  o_i^((2))sim π_θ(·mid q,o_i^((1)),c_i)  |\n| Reagent-R | 仅 <score> | 传统 GRPO，把 score 当密集奖励 |  R_i=R_(rule)+λ·Agent-RRM(q,o_i)  |\n| Reagent-U | <critique>+<score> | 统一池化优化：初始+修正轨迹一起算优势 | 见下方公式 |\n\n**Reagent-U 统一目标**\n\nJ_U(θ)=E[(1) / (2G)∑_(k=1)^2∑_(i=1)^G[min!(r_i^((k))(θ)A_i^((k)),clip_ε)-β D_(KL)^((i,k))]]\n\n其中\n\n-  k∈1,2  对应“初始轨迹”与“经 critique 修正后的轨迹”\n- 优势  A_i^((k))  在 **混合池**  G_(pool)  内标准化，保证模型同时优化“一次做对”与“被提示后能改”两种能力。\n\n实验验证“信号有效”\n\n- **Reagent-C** 在 GSM8K 上把 94.6 → 94.9，**无需训练**即可修正 47 % 的初级计算错误。\n- **Reagent-R** 在 Bamboogle 上比纯规则奖励提升 11.2 个百分点，验证密集奖励缓解稀疏性。\n- **Reagent-U** 在 12 个基准上**全部领先**，GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相对 8 B 基线平均提升 18.4 %，证明两种信号**协同增益**最大。\n\n一句话总结\n\n论文通过“**多维奖励模型 → 数据 → 三种集成策略**”的闭环，把原本只能告诉智能体“错”的稀疏信号，升级为同时告诉它“**哪里错、为何错、如何改**”的密集推理奖励，从而系统性地提升了长程工具使用与推理能力。\n\nQ4: 论文做了哪些实验？\n\n论文在第 4 节“Experiments”中围绕 **12 个基准、3 类任务、3 种变体、4 组消融** 展开系统评估，可概括为“**一个主表 + 三个深度分析 + 两个参数实验**”。\n\n1\\. 主实验：12 基准全面比较\n\n**任务分组与指标**\n\n- **数学推理**：AIME24/25、MATH500、GSM8K        pass@1\n- **知识密集型**：HotpotQA、2Wiki、Bamboogle、MuSiQue  pass@1\n- **通用 Agent & 搜索**：GAIA、WebWalkerQA、HLE、xbench pass@1\n\n**对照维度**\n\n- 闭源：o3, o1-preview, Claude-4-Sonnet, DeepResearch\n- 开源同规模（≤8 B）：WebThinker、ARPO、VerlTool …\n- 开源大尺度（≤32 B）：QwQ-32B、DeepSeek-R1-671B、Search-o1 …\n- 开源过程奖励：Atom-Searcher、PPR-Instruct\n\n**核心结果**（表 1+2 汇总）\n\n| 模型 | GAIA | WebWalkerQA | Bamboogle | AIME24 | 平均提升 |\n| --- | --- | --- | --- | --- | --- |\n| Qwen3-8B 基线 | 21.4 | 29.0 | 53.6 | 46.7 | — |\n| Reagent-C | 25.2 | 35.5 | 61.6 | 56.7 | +6.8 |\n| Reagent-R | 36.9 | 45.3 | 72.8 | 53.3 | +11.4 |\n| Reagent-U | 43.7 | 46.2 | 76.8 | 60.0 | +18.4 |\n\n2\\. 深度分析 1：Textual-Critique 真有用吗？\n\n- **实验**：Reagent-C 零参更新，仅把 critique 作为 prompt 让模型再答一次。\n- **结果**：12 个数据集**全部上涨**，GSM8K 94.6→94.9，GAIA 21.4→25.2；**首次失败中 47 % 在第二次答对**。\n- **结论**：critique 提供**可执行诊断**，无需训练即可纠正工具误用与逻辑跳跃。\n\n3\\. 深度分析 2：Model-based Reward 能缓解稀疏性吗？\n\n- **实验**：固定 λ=0.3，比较 Reagent-R（规则+模型分） vs Reagent w/o Agent-RRM（仅规则）。\n- **结果**：Bamboogle +11.2，xbench +9.0，平均 +7.6 个百分点。\n- **结论**：密集**标量奖励**显著改善训练信号，但仍低于 Reagent-U，说明**纯分数不如“分数+文字”**。\n\n4\\. 深度分析 3：Unified Feedback 是否产生协同效应？\n\n- **实验**：Reagent-U 同时优化初始与 critique-修正轨迹，优势在混合池统一标准化。\n- **结果**：\n– 在**需要≥5 步工具调用**的 GAIA Lv3 上，Reagent-U 16.7 %，比 Reagent-R 高 10 个百分点。\n– 在**跨模态**的 GAIA-full（165 题）上，pass@1 38.8 %，比 MCP-R1 高 1.2 个百分点，验证**非文本任务同样受益**。\n- **结论**：scalar 信号提供**全局优化方向**，critique 信号提供**局部修正动作**，二者联合产生**1+1>2**的协同增益。\n\n5\\. 参数实验\n\n5.1 奖励权重 λ 扫描（图 3）\n\n- 范围：λ∈\n0,0.5\n；任务：AIME24（数学）、xbench（深度搜索）。\n- 结果：λ≈0.2–0.4 区域性能 plateau；λ≥0.5 开始下降，**过高权重会牺牲最终答案准确性**。\n\n5.2 训练步数与收敛性（附录 B）\n\n- 300 步 GRPO 已收敛，继续训练未见明显提升，说明**数据量与模型规模匹配**，未出现过度拟合。\n\n6\\. 案例可视化（附录 D）\n\n- **Case 1（GAIA 搜索）**：critique 指出“未 browse 权威页→假设错误”，第二次调用 browse 拿到 15 Mt 精确值，答案从 Cannikin→Bravo。\n- **Case 2（GSM8K 数学）**：critique 指出“Python 第一次无 print 浪费调用”，第二次去掉多余调用并纠正“总工时÷人数”逻辑，答案从 47.25→189。\n\n一句话总结\n\n实验从“**能不能用→管不管用→好不好用→怎么用最好**”四个层次验证：\n**多维推理奖励模型 + 统一集成训练** 在 12 基准、跨模态、长程工具调用场景下**全面领先**，且**零参 critique 即可带来显著推理修正**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下可探索方向按“**规模-数据-任务-信号-理论**”五层归纳，均直接对应论文第 6 节 Limitations 的开放问题，并给出可落地的下一步实验设计。\n\n1\\. 规模：更大参数与分布式 RL\n\n- **问题**：全文仅在 8 B 模型验证，协同效应是否会随规模放大？\n- **探索点**\n– 在 32 B/70 B 基座重复 Reagent-U 训练，观察“λ-性能”曲线是否左移（更强模型需要更少 critique 即可收敛）。\n– 采用 **Model-parallel + Rollout-parallel** 框架，把 Agent-RRM 与 Policy 解耦到不同 GPU 节点，验证千亿级模型下是否出现奖励过拟合（reward overfitting）。\n\n2\\. 数据：开放环境 vs 静态基准\n\n- **问题**：12 个基准均为**静态题库**，工具集固定，无法反映真实世界工具漂移。\n- **探索点**\n– 引入 **Live-API 环境**（如 Google Search 动态排名、WolframAlpha 付费限额），构建非平稳奖励，测试 Agent-RRM 的**在线自适应**能力。\n– 设计 **Adversarial Tool-Response** 数据集：故意返回错位、截断或冲突信息，量化 critique 对“工具幻觉”的纠错上限。\n\n3\\. 任务：长周期闭环与多智能体\n\n- **问题**：现有 episode 最长 30 步，且为单 agent。\n- **探索点**\n– **Science-of-Chemistry** 场景：让 agent 连续运行 100+ 步完成“文献检索→实验设计→代码仿真→结果验证”闭环，用 Agent-RRM 每 10 步做一次 critique，观察**长程信用分配**是否仍有效。\n– **Multi-Agent Debate**：把 critique 角色独立为第三方裁判，与其他 agent 的 solution 进行**多轮对抗式 refinement**，检验 critique 语言是否出现**策略性误导**（reward hacking 新形态）。\n\n4\\. 信号： critique 的粒度与格式\n\n- **问题**：固定三段式输出可能不是最优。\n- **探索点**\n– **Token-Level 奖励**：把 `<critique>` 拆成**逐句批评**，用指针网络对齐到具体动作，实现**子步骤级**优势估计：\n\nA_t = R_(sentence)(t) - μσ\n\n– **Multimodal-Critique**：当观测含图/音频时，让 Agent-RRM 输出**圈图 mask** 或**音频时间戳**，验证**视觉定位**能否进一步降低修正成本。\n\n5\\. 理论：奖励可解释性与收敛保证\n\n- **问题**：Agent-RRM 的自洽性缺乏形式化刻画。\n- **探索点**\n– 建立 **Critique Consistency Index (CCI)**：同一轨迹两次采样 critique 的**语义相似度**，用 CCI 与最终性能做皮尔逊检验，量化“**可解释→可改进**”的因果链。\n– 在**离线 RL** 框架（如 CQL）下，把 Agent-RRM 当作 Q-function 正则项，推导**误差上界**，回答“**推理奖励误差 ε 对策略性能影响是 O(ε) 还是 O(ε²)**”。\n\n快速启动清单（8 卡 A800 可完成）\n\n1. 用 32 B 模型复现 Reagent-U，记录 λ∈{0.1,0.3,0.5} 的 GAIA 性能 → 验证规模效应。\n2. 在 Live-API 环境跑 500 条长 episode（≥50 步），统计 critique 成功纠正“工具返回漂移”的比例 → 验证鲁棒性。\n3. 把 critique 拆成句子级嵌入，与动作序列做注意力对齐，重新计算优势 → 验证更细粒度信号是否带来额外 2–3 个百分点。\n\n一句话总结\n\n未来工作可沿“**更大模型、更乱环境、更长链条、更细信号、更严理论**”五轴推进，把 Agent-RRM 从“静态题库好”推向**开放世界可信**。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Agent-RRM**（Agent Reasoning Reward Model）与 **Reagent** 训练框架，解决 Agentic RL 中“稀疏结果奖励无法区分中间推理质量、缺乏可执行改进信号”的核心痛点。主要贡献与内容可概括为以下四点：\n\n1. 多维推理奖励模型\nAgent-RRM 对每条轨迹输出三元组：\n\n- `<think>`：逐步推理轨迹\n- `<critique>`：可执行的错误指正\n- `<score>`：0–1 整体质量分\n无需真值即可提供密集、语言化、可解释的反馈。\n2. 高质量数据集\n构建 70.9 万 RL 样本（Reagent-RL-709K）与 5.56 万 SFT 样本（Reagent-SFT-55.6K），并标注 11.8 万条结构化评判（Reagent-RRM-SFT/RL-28K/90K），覆盖数学、多模态、网页搜索、复杂工具四类任务。\n\n3. 三种集成策略\n\n- Reagent-C：零参更新，用 critique 做 in-context 二次生成\n- Reagent-R：GRPO 训练，规则奖励 + Agent-RRM 分数\n- Reagent-U：统一池化初始与修正轨迹，同步优化 scalar 与语言信号\n4. 系统实验\n在 12 个基准（GAIA、WebWalkerQA、AIME、HotpotQA 等）上，Reagent-U 取得新 SOTA：GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相比 8 B 基线平均提升 18.4 %；零参的 Reagent-C 也能稳定上涨，验证多维推理奖励的普适性与可解释性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Reasoning%20Reward%20Model%20for%20Agents，链接是https://arxiv.org/pdf/2601.22154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22154。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22151v1-1769990308645","title":"Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing","description":"Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation. The code is open source at https://github.com/TUDa-HWAI/NN2Logic","url":"https://arxiv.org/abs/2601.22151v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:50.000Z","metadata":{"arxivId":"2601.22151v1","authors":"Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang","categories":"cs.LG, eess.SY","published":"2026-01-29T18:59:50Z","pdfUrl":"https://arxiv.org/pdf/2601.22151v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22151v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22151v1","rank":6,"domain":"Evaluation","zh_summary":"神经网络已经成功应用于各种资源受限的边缘设备，这些设备通常由于电源有限而使用中央处理器（CPU）而非图形处理器（GPU）。最先进的研究仍然集中在高效执行大量乘加（MAC）操作上。然而，CPU本身并不擅长大规模执行这类数学运算，因为它们更适合执行控制流逻辑，即计算机算法。为了提高神经网络在CPU上的计算效率，本文提出将其转换为逻辑流进行执行。具体来说，神经网络首先被转换为等效的决策树，然后从中选择具有常量叶子的决策路径，并压缩成逻辑流。这些逻辑流由if和else结构以及减少的MAC操作组成。实验结果表明，在模拟的RISC-V CPU上，延迟最多可以降低14.9%，且不会降低准确性。代码开源于https://github.com/TUDa-HWAI/NN2Logic","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“在仅含通用 CPU 的边缘设备上运行神经网络时，因大量乘累加（MAC）操作而导致的延迟过高”这一核心问题，提出将网络执行形态从“张量-算术”转变为“逻辑-控制流”。具体而言，其目标可归纳为：\n\n- 突破现有算法层与编译层优化仍以“减少 MAC 数量”为中心的惯性思维；\n- 利用 CPU 擅长分支/逻辑运算、但 MAC 单元稀缺的架构特点，把可确定的计算路径直接映射为 if-else 逻辑，从而显著降低推理延迟；\n- 在不损失模型精度的前提下，为资源受限场景提供一种“零硬件改动”的加速方案。\n\nQ2: 有哪些相关研究？\n\n相关研究可从“算法级压缩/加速”与“编译/硬件映射”两大脉络梳理，均围绕“减少或优化 MAC 操作”展开，但未触及“将网络执行彻底转为逻辑控制流”这一维度。\n\n1\\. 算法级压缩与动态推理\n\n- **剪枝**\n- DTMM：在极弱 IoT 设备上通过结构化剪枝部署 TinyML 模型。\n- Class-Aware Pruning：按类别敏感度剪枝，保持精度同时削减运算量。\n- PowerPruning：联合权重与激活剪枝，以功耗为优化目标。\n- **量化**\n- Sun et al. “Class-based Quantization”：对类别相关通道采用差异化位宽。\n- Zhuo et al. 低精度量化实证研究：分析 <8 bit 对 TinyML 的精度-功耗权衡。\n- **知识蒸馏**\n- Moslemi 等综述：将大网络知识迁移至小网络，减少后续推理 MAC。\n- **动态/早退机制**\n- MCUNet：为 MCU 设计的轻量搜索网络，仍基于 MAC 削减。\n- Early-Exit with Class Exclusion：样本在浅层即可被排除部分类别，降低后续计算。\n\n2\\. 编译与硬件映射优化\n\n- **AI 编译器**\n- Apache TVM、IREE、Torch-Compiler、MLIR：聚焦循环变换、内存复用、指令调度，以提升 MAC 吞吐。\n- **内存与指令调度**\n- Patch-based Inference：分块执行以降低峰值内存。\n- Deeploy：在异构 MCU 上调度小语言模型，侧重能耗-延迟联合优化。\n\n3\\. 网络-决策树转换（与本工作方法最相邻）\n\n- Aytekin “Neural Networks are Decision Trees”：提供将前馈网络等价转为决策树的理论框架，但未涉及“提取逻辑路径+混合执行”及 CPU 优化目标。\n\n综上，现有研究始终将神经网络视为“MAC 密集型算子图”，通过削减或调度 MAC 来提升效率；本文首次把“可确定路径”转为纯分支逻辑，与剩余路径构成混合执行，从而直接契合 CPU 的硬件优势。\n\nQ3: 论文如何解决这个问题？\n\n整体思路：把“张量-算术密集”的神经网络改造成“分支-逻辑为主、MAC 为辅”的混合程序，使 CPU 发挥其擅长控制流、不擅长大规模 MAC 的架构特性。流程分三步，对应原文第 II 章 A-C。\n\n1\\. 网络 → 决策树（Training-Data 限定）\n\n- 以神经元激活条件（如 ReLU:  y_i>0 ）作为决策节点，按训练样本实际遍历的分支逐层展开，建成等价决策树。\n- 未被任何训练样本访问的分支直接剪去，降低树规模。\n\n2\\. 决策树 → 常数叶路径（逻辑流）\n\n- **常数叶判定**：对每片叶子，建立混合整数规划（MIP）检验“是否存在输入使该叶输出非此常数类”。若不可行，则该叶为常数叶。\n- **IIS 提取**：用 Gurobi 计算不可行子系统（IIS），得到“最少约束集”即可锁定该常数叶。\n- **逻辑压缩**：仅保留 IIS 中的不等式（如  y_0<0 ），映射为 C 语言的 if-else 串；该路径后续无需再算任何 MAC。\n\n3\\. 混合执行（逻辑流 + 原网络）\n\n- 推理时优先顺序检查所有已提取的逻辑流；任一匹配立即返回结果。\n- 若均不匹配，则退回到剩余路径，按原始网络计算（仍含 MAC）。\n- 由于逻辑流出口提前终止，整体平均时钟周期减少；最坏情况仅增加少量分支判断，最大延迟增幅可控。\n\n结果\n\n- Ibex RISC-V 仿真显示：最小延迟 ↓52.2%，平均延迟 ↓14.9%，准确率无损。\n- 第二、三层延迟最高 ↓39.3%，验证“逻辑流”主要替换了深层 MAC。\n\n通过“等价决策树 → 常数叶路径提取 → 分支逻辑化”这一完整链条，论文把部分神经网络执行转化为 CPU 最擅长的 if-else 控制流，从而解决边缘 CPU 上 MAC 效率低、延迟高的问题。\n\nQ4: 论文做了哪些实验？\n\n实验目的：验证“逻辑流混合执行”在真实 RISC-V CPU 仿真环境下能否降低延迟且保持精度。\n实验设计要点与结果如下（均取自原文 Table I 与第 III 章描述）。\n\n1\\. 测试对象\n\n- **模型**：3 层全连接网络，层宽分别随数据集变化；权重已统一做 8-bit 量化。\n- **数据集**\n- MNIST\\*：将原 10 类手写数字改为“偶 / 奇”二分类，共 60 k 训练、10 k 推理样本。\n- Occupancy I：10 维传感器特征，二分类“有人 / 无人”，训练 4 k、推理 1 k。\n- Occupancy II：20 维特征，二分类，训练 9 k、推理 2 k。\n\n2\\. 对照设置\n\n- **Reference**：同一网络、同一量化，由作者框架生成“纯 MAC”版 C 代码，启用同等神经元顺序与编译优化，排除解释器开销。\n- **Hybrid**：采用本文方法，将可提取的逻辑流转为 if-else，其余路径仍走 Reference 代码。\n- **平台**：lowRISC Ibex RISC-V 核心周期精确仿真器，无额外加速扩展。\n\n3\\. 观测指标\n\n- 精度（Acc）：Reference vs Hybrid 是否一致。\n- 延迟：记录 10 k/1 k/2 k 推理样本的周期数，取 Min/Avg/Max。\n- 层加速贡献：仅统计 Layer 2+3 的周期降幅。\n- 逻辑流出口比例：多少样本提前由 if-else 分支返回结果。\n\n4\\. 主要结果（相对 Reference）\n\n| 数据集 | 精度变化 | Min↓ | Avg↓ | Max↑ | Layer2+3↓ | 逻辑流出口 |\n| --- | --- | --- | --- | --- | --- | --- |\n| MNIST* | 0 | 4.8 % | 0.7 % | −0.1 % | 19.1 % | 15.1 % |\n| Occupancy I | 0 | 52.2 % | 14.9 % | −1.2 % | 39.3 % | 34.1 % |\n| Occupancy II | 0 | 25.5 % | 5.2 % | −0.6 % | 25.1 % | 12.7 % |\n\n- 所有网络均无精度损失。\n- 平均延迟最高减少 14.9 %（Occ-I），最小延迟最高减少 52.2 %。\n- 仅第二、三层延迟最高削减 39.3 %，与逻辑流主要替换深层决策一致。\n- 最多 34 % 样本通过纯分支路径完成推理，验证“逻辑流”实际生效。\n\n5\\. 实验结论\n\n在无任何硬件修改、无精度损失的条件下，混合逻辑流执行能把边缘 CPU 上的神经网络推理周期显著压缩，最小延迟降幅可达一半，平均延迟降幅近 15 %，从而验证了论文方法的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n可进一步探索的方向按“深度-广度-硬件”三条主线展开，均围绕“如何把逻辑流思路推向更大规模、更复杂场景并保持收益”这一核心问题。\n\n1\\. 深度维度：从浅层网络到深层 / 大规模 CNN-Transformer\n\n- **层级树分解**\n对 ResNet、ViT 等深层网络直接建单一决策树会爆炸。可逐块（block-wise）或按残差分支建立局部决策树，再串联成“决策森林-逻辑流”链。\n\n- **注意力逻辑化**\n将 softmax 分数与 Q-K 点积阈值化，转化为“若注意力 > τ 则执行对应 Value 路径”的稀疏逻辑，减少 O(n²) MAC。\n\n- **动态提前出口 + 逻辑流**\n早退网络已能判断“是否足够置信”；可把各出口后的子网络分别转成逻辑流，实现“分层早退 + 分支推理”双重加速。\n\n2\\. 广度维度：任务、精度与自动化\n\n- **多类 / 目标检测 / 语义分割**\n常数叶定义需从“单类独占”放松到“Top-k 类别集合恒定”，引入“集合-不可行”MIP 或 SAT 编码；检测/分割的回归输出可改用“区间恒定”判定。\n\n- **混合精度-逻辑联合搜索**\n将量化位宽与 IIS 约束联合优化：位宽越低，决策边界越粗糙，可能生成更多常数叶；可用 NAS 风格搜索 Pareto 前沿。\n\n- **全自动工具链**\n当前需人工设定“提取比例”与退回到原始网络的阈值。可强化学习自动决策“提取哪几条路径、何时回退”，以最小化期望延迟。\n\n3\\. 硬件维度：从单核 RISC-V 到异构与 FPGA\n\n- **指令集扩展**\n在 RISC-V 加入“比较-分支组合”指令（如 compare-and-branch-if-IIS），把多条不等式打包成一条定制指令，减少取指开销。\n\n- **逻辑流协处理器**\n将 IIS 判断做成可配置布尔单元阵列，挂在 CPU 旁作“决策加速器”；只有未命中才唤醒主核做 MAC，实现μW 级待机。\n\n- **FPGA 决策流水线**\n把提取出的逻辑流直接综合成 FPGA 查找表（LUT），形成“可重编程决策引擎”，与 CPU 共享片上 SRAM；对批量推理可流水化。\n\n4\\. 理论与鲁棒性\n\n- **对抗样本与分布漂移**\n逻辑流依赖训练数据遍历剪枝；当测试分布偏移，常数叶可能失效。可引入不确定性区间，让 IIS 保留“ε-松弛”，再在线回退。\n\n- **可解释性增强**\nIIS 本身即最小充分条件，天然适合生成人类可读的“规则解释”。可结合因果推理，输出“为何样本被判定为 A”的简短逻辑语句。\n\n5\\. 系统级整合\n\n- **与操作系统调度协同**\n在 RTOS 内把“逻辑流出口”标记为高速中断，允许高优先级任务立即抢占；对低优先级任务则继续用 MAC 路径，提升整体实时性。\n\n- **TinyML 模型库复用**\n建立“逻辑流友好”模型 zoo：提供预训练 + 已提取 IIS 的模型，开发者可直接链接 .c 文件，无需重新跑 MIP。\n\n通过上述方向，可将“神经网络→逻辑流”从当前的三层量化网络推广到现代深度架构、复杂任务与多元硬件平台，同时保持甚至进一步提升边缘 CPU 上的延迟与能耗优势。\n\nQ6: 总结一下论文的主要内容\n\n**论文核心**：提出“神经网络→逻辑流”新范式，把可在训练数据上证明为“常数输出”的决策路径转成纯 if-else 分支，在边缘 CPU 上优先执行，以减少 MAC、降低延迟。\n\n方法流程\n\n1. **网络转决策树**\n以神经元激活条件（如 ReLU>0）为节点，仅保留训练样本遍历过的分支，构建等价决策树。\n\n2. **提取常数叶路径**\n对每片叶子建立 MIP 可行性检验；若某类永远胜出，则用 Gurobi 求出最小不可行约束集（IIS），将其映射为精简逻辑表达式。\n\n3. **混合执行**\n推理时先顺序匹配所有逻辑流；若命中立即返回，否则退回原始网络做标准 MAC 计算。\n\n实验结果（Ibex RISC-V 仿真）\n\n- 3 个量化 3 层网络、MNIST\\* 与两个 Occupancy 数据集，零精度损失。\n- 最小延迟 ↓52.2%，平均延迟 ↓14.9%；第二、三层延迟最高 ↓39.3%。\n- 最多 34% 样本通过逻辑流提前退出。\n\n贡献\n\n- 首次将“逻辑控制流”作为神经网络在 CPU 上的主要执行形态，突破仅优化 MAC 的传统思路。\n- 提供完整工具链（开源），可在任意边缘 CPU 上生成混合 C 代码，无需硬件改动即得显著延迟收益。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Late%20Breaking%20Results%3A%20Conversion%20of%20Neural%20Networks%20into%20Logic%20Flows%20for%20Edge%20Computing，链接是https://arxiv.org/pdf/2601.22151，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22151。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22149v1-1769990308645","title":"DynaWeb: Model-Based Reinforcement Learning of Web Agents","description":"The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.","url":"https://arxiv.org/abs/2601.22149v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:07.000Z","metadata":{"arxivId":"2601.22149v1","authors":"Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu","categories":"cs.CL, cs.AI","published":"2026-01-29T18:59:07Z","pdfUrl":"https://arxiv.org/pdf/2601.22149v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22149v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22149v1","rank":7,"domain":"Evaluation","zh_summary":"由大型语言模型（LLM）和强化学习（RL）驱动的自主网页代理的发展，代表了通用人工智能助手迈出的重要一步。然而，与实时互联网的交互在训练这些代理时带来了重大挑战，因为这种交互效率低、成本高且充满风险。基于模型的强化学习（MBRL）通过学习环境的世界模型以实现模拟交互，提供了一种有前景的解决方案。本文介绍了 DynaWeb，一种新型的 MBRL 框架，通过与训练好的网页世界模型进行交互来训练网页代理，该模型能够根据代理的动作预测自然网页表示。该模型充当一个合成网页环境，使代理策略可以通过生成大量的回滚动作轨迹来“演练”，从而实现高效的在线强化学习。除了自由策略回滚之外，DynaWeb 还结合了来自训练数据的真实专家轨迹，这些轨迹在训练过程中随机穿插在策略内回滚中，以提高稳定性和样本效率。在具有挑战性的 WebArena 和 WebVoyager 基准测试中进行的实验表明，DynaWeb 一贯且显著地提升了最先进开源网页代理模型的性能。我们的研究结果证明，通过想象训练网页代理是可行的，为大规模扩展在线代理强化学习提供了一种高效且可扩展的方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何在不与真实互联网交互的前提下，高效、安全地训练具备在线强化学习能力的网页智能体”这一核心问题。\n具体而言，现有基于大模型的网页智能体虽可通过在线 RL 显著提升鲁棒性与长程决策能力，但直接依赖真实网页交互会带来三大瓶颈：\n\n1. 效率低：每次策略更新都需大量实时页面请求，训练周期长。\n2. 成本高：频繁访问真实站点易触发验证码、限流或封禁，需额外代理与清洗开销。\n3. 风险大：策略探索可能产生不可逆副作用（误下单、删数据、泄露隐私等），难以规模化。\n\n为此，作者提出将“世界模型”从传统推理工具升级为**在线 RL 的训练环境**，通过模型化互联网动力学，使智能体在“想象”中完成策略优化，从而把对真实环境的采样需求降到最低，同时保留在线 RL 的探索与信用分配优势。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均与“网页智能体”或“世界模型”密切相关，但尚未将二者以\\*\\*在线模型强化学习（MBRL）\\*\\*方式深度耦合：\n\n- **网页智能体训练范式**\n- 提示工程与推理框架：ReAct、Cognitive Kernel、MCP 等通过链式思维或结构化提示提升单步决策。\n- 监督/模仿学习：NNetNav、Go-Browse、Explorer、InSTA 等收集或合成高质量轨迹，用 SFT 训练。\n- 在线 RL：WebAgent-R1、WebRL、WorkForceAgent-R1、WebDancer 等直接在真实环境采样，用 GRPO、DAPO 等算法优化，但需大量真实交互，成本高。\n- **世界模型在网页场景的辅助应用**\n- 推理时规划：WebDreamer、WMA、RAP、WKM 用大模型模拟下一步网页状态，供树搜索或价值估计，**仅用于推断阶段**，不更新策略。\n- 离线数据增强：WebEvolver、Explorer 用世界模型生成额外轨迹，再离线蒸馏到策略，**与 on-policy 优化解耦**。\n- **通用世界模型与 MBRL 基础**\n- 视觉/游戏领域：Dreamer、SimPLe、IRLA、Diffusion World Models 等将环境动力学模型嵌入 RL 循环，实现“想象训练”。\n- 文本/决策抽象：RWKM、RAP、Diffusion for World Modeling 等把 LLM 当作隐式环境模型，但尚未在网页这类高动态、部分可观察场景做在线策略优化。\n\n综上，已有工作要么停留在“**推理阶段**”使用世界模型，要么“**离线生成**”伪轨迹后再蒸馏；DynaWeb 首次把**专门训练的网页世界模型**作为**可微环境**，与真实专家轨迹混合，直接用于 on-policy 强化学习，填补了“网页 MBRL”空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 DynaWeb，一套**面向网页任务的在线模型强化学习（MBRL）框架**，把“真实网页交互”替换为“可学习的网页世界模型 + 少量真实专家轨迹”的混合训练流程，核心步骤如下：\n\n1. 训练网页世界模型（WWM）\n\n- 将网页状态表示为**可访问性树**  o_t 。\n- 用监督数据  (o_t, a_t, o_(t+1))  训练大模型  p_φ  先输出**自然语言状态变更描述**  Delta ，再自回归生成下一帧树  hat o_(t+1) ，损失为\n\nL_φ = -∑ log p_φ(r,Delta mid I,o_t,a_t)\n\n- 得到的  p_φ  充当**可反复采样的网页“服务器”**，无需访问真实站点即可展开多步交互。\n2. 想象 rollout：策略与世界模型闭环\n\n- 策略  π_θ  以当前真实或想象观测为输入，采样动作  a_tsim π_θ(· mid hat o_(1:t),h_(1:t-1),a_(1:t-1),q) 。\n- 世界模型立即返回  hat o_(t+1)sim p_φ(· mid hat o_t,a_t,q) ，形成**想象轨迹**\n\nhatτ = (hat o_1,h_1,a_1,dots,hat o_T,h_T,a_T)\n\n- 终止时用模型自评给出稀疏奖励  hat r(hatτ,q)∈0,1 ，作为整条轨迹的回报  G(hatτ) 。\n3. 混合真实专家轨迹\n\n- 每次梯度更新按 50 % 比例**随机混入**来自 NNetNav SFT 数据集的**真实专家轨迹**，与想象 rollout 一起构成 batch。\n- 真实轨迹提供**无偏状态转移与奖励信号**，抑制世界模型幻觉，稳定训练。\n4. 序列级策略优化（GSPO）\n\n- 对每条轨迹计算单序列优势  hat A_i = G(hatτ_i) - bar G 。\n- 采用**序列级重要性权重**\n\ns_i(θ)=exp!l((1) / (|y_i|)∑_k logπ_θ(y_(ik)mid·s){π_(θ_old)(y_(ik)mid·s)}r)\n\n代入 clipped objective\n\nJ_(GSPO)(θ)=mathbb E![(1) / (G)∑_(i=1)^G min!l(s_i(θ)hat A_i, clip(s_i(θ),1!-!varepsilon,1!+!varepsilon)hat A_ir)]\n\n- 实现**长程信用分配**且避免 token 级方差爆炸。\n5. 控制想象深度与误差\n\n- 实验发现 dream length 4–5 步最佳，过长会累积模型误差；同时限制单条 rollout 最大 5 步并早停终端状态，兼顾探索深度与仿真保真度。\n\n通过上述设计，DynaWeb 把**策略优化完全搬到世界模型内部**，仅依赖少量真实轨迹做正则化，从而\n\n- 将真实环境采样需求降低一个数量级；\n- 避免不可逆操作与封禁风险；\n- 在 WebArena 与 WebVoyager 上相对最强基线分别提升 **+16.1 %** 和 **+18.7 %** 成功率，验证“用想象训练网页智能体”的可行性与扩展性。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“**能否用想象 rollout 替代真实网页交互来训练高性能网页智能体**”展开，分三部分：\n\n1. 主实验：WebArena & WebVoyager 成功率对比\n\n- 基准\n– 零样本提示：Llama-3.1-8B-Instruct+CoT、GPT-4o\n– 监督微调：NNetNav、Go-Browse\n– 离线 RL：WebRL\n– 推理时世界模型：ITL（每步模拟选最优动作，不训练策略）\n- 结果\n– WebArena（812 任务，5 站点）\nDynaWeb-8B 平均 SR **31.0 %**，相对离线 RL **+16.1 %**，在 Reddit/GitLab/CMS/Shopping 四域均第一。\n– WebVoyager（643 开放站点）\nDynaWeb 平均 SR **38.7 %**，显著高于次佳基线 WebRL（32.6 %），在 Amazon/BBC/Map 等 9 个站点领先。\n2. 消融实验：关键设计对性能的影响\n\n- Dream length 消融\n固定其余超参，令最大想象步数 1–8 变化；SR 在 4–5 步处峰值，过短欠探索，过长累积幻觉。\n- 真实轨迹比例消融\n0 %–100 % 替换想象 rollout；40 % 真实数据即可显著超越纯 SFT，>60 % 后收益饱和，验证“少量真实轨迹正则化”即可。\n- 世界模型训练必要性\n用**冻结 GPT-oss-120b** 替代微调后的 WWM；WebArena SR 从 31.0 % → 20.9 %，WebVoyager 35.4 % → 28.6 %，表明**显式环境动力学训练**不可或缺。\n3. 训练过程分析\n\n- 训练曲线：混入真实轨迹后，GSPO 的序列级 variance 更小，收敛更快。\n- 错误类型统计：相比纯想象策略，DynaWeb 将“页面元素幻觉”与“无效动作”两类错误各降低约 30 %。\n\n综上，实验系统验证了\n\n- 想象 rollout 可直接用于 on-policy RL；\n- 适度真实数据 + 适度 dream length 是性能拐点；\n- 专门训练的网页世界模型是提升的关键。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步扩展 DynaWeb 的想象力驱动范式：\n\n- **长程高保真世界模型**\n- 研究层级或扩散式架构，将页面视觉截图与 Accessibility Tree 联合建模，降低 5 步后的幻觉累积，实现 ≥20 步稳定 rollout。\n- 引入“页面差异压缩”或 Delta-VAE，仅对变化节点建模，减少自回归长度与误差传播。\n- **多模态世界模型**\n- 同时接受 HTML、CSS、屏幕截图、DOM 变化事件，统一文本与视觉空间，缓解纯文本树对动态 UI（Canvas、SVG、Shadow-DOM）信息丢失问题。\n- 探索视觉-动作对齐预训练，提升对图标、悬浮动画等细粒度交互的预测精度。\n- **可自我批评的世界模型**\n- 让同一模型输出“下一状态 + 预测不确定性/置信度”，策略据此自适应决定是否请求真实环境验证，形成**选择性查询**（selective query）机制，进一步压缩真实交互预算。\n- 引入对比式或能量模型，对世界模型置信度进行校准，防止过度乐观幻觉。\n- **层次化想象与策略分解**\n- 将任务分解为子目标（URL 导航、表单填充、结果提取），训练**高层子目标世界模型**与**低层原子动作世界模型**，实现“子目标→原始动作”两级想象，降低长程信用分配难度。\n- 结合选项框架（Options），在想象空间先做子目标规划，再展开低层动作 rollout。\n- **持续学习与领域增量**\n- 研究世界模型在线微调策略，避免新站点数据导致旧站点灾难性遗忘；可采用 EWC、LoRA-merging 或经验回放。\n- 探索任务-站点混合分布外推（OOD）检测，自动触发世界模型局部更新，保持长期一致性。\n- **安全与可验证想象**\n- 引入形式化约束（如不变量：价格≥0、表单不可越权提交），在世界模型采样阶段即屏蔽非法状态，减少策略探索危险动作。\n- 结合合约或沙箱 API，让世界模型对敏感操作（支付、写库）返回“虚拟确认”而非真实执行，实现零风险训练。\n- **跨环境迁移与元世界模型**\n- 收集 100+ 站点数据，训练**通用网页世界模型**，再针对新站点少量微调，验证“预训练-微调”范式能否快速适应未知站点动力学。\n- 研究元学习（MAML、Prompt-tuning）初始化，使世界模型在新站点仅用 50 条轨迹即可适配。\n- **更细粒度奖励模型**\n- 当前仅用终端 0/1 奖励，可训练**稠密价值模型**对想象中间状态打分，实现单步 TD 学习，进一步缩短真实轨迹需求。\n- 探索人工偏好 + 自动规则混合的奖励函数，缓解稀疏奖励导致的想象轨迹价值估计方差。\n- **系统级优化**\n- 世界模型与策略模型联合蒸馏为单一小模型，降低推理延迟，实现端侧部署。\n- 研究异步并行想象：GPU 集群同时展开数千条 rollout，用 Ray/vLLM 动态调度，提升训练吞吐。\n- **评测扩展**\n- 构建更长 horizon（≥50 步）与多会话（跨天登录、购物车保留）任务集，检验世界模型对**状态持久化与动态会话**的建模能力。\n- 引入对抗评测：在想象与真实环境间随机切换，测量策略对模型误差的鲁棒性，推动世界模型-策略协同优化。\n\n这些方向分别从模型架构、训练策略、安全约束、系统效率到评测维度出发，可进一步释放“想象驱动网页 RL”的潜力，并推广至更广泛的数字界面自动化任务。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **DynaWeb**：首个面向网页任务的**在线模型强化学习（MBRL）框架**，用“想象”替代昂贵、高风险的真实互联网交互，实现高效、安全的智能体训练。\n\n1. 问题\n在线 RL 可提升网页智能体的鲁棒性与长程决策，但直接访问真实站点效率低、成本高、风险大，难以规模化。\n\n2. 解法\n\n- 训练**网页世界模型**  p_φ ，输入当前可访问性树  o_t  与动作  a_t ，自回归输出状态变更描述  Delta  并生成下一帧树  hat o_(t+1) ，充当可无限采样的**虚拟网站**。\n- 策略  π_θ  与世界模型闭环，展开多步**想象轨迹**  hatτ ，用模型自评稀疏奖励  hat r(hatτ,q)  进行**序列级策略优化（GSPO）**。\n- 训练中**随机混入 40 % 真实专家轨迹**，提供无偏状态转移，抑制幻觉并稳定学习。\n- 控制想象深度 4–5 步，平衡探索与仿真保真。\n3. 实验\n\n- WebArena（812 任务）：DynaWeb 平均成功率 **31.0 %**，相对最强离线 RL 基线 **+16.1 %**。\n- WebVoyager（643 开放站点）：**38.7 %**，显著优于 GPT-4o、WebRL 等。\n- 消融：真实数据比例、dream length、世界模型训练必要性均被验证为性能拐点。\n4. 结论\nDynaWeb 首次证明**专门训练的世界模型可直接用于 on-policy 网页 RL**，在零风险、低成本的前提下持续提升开源智能体，为“想象驱动”的通用数字代理提供可扩展路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaWeb%3A%20Model-Based%20Reinforcement%20Learning%20of%20Web%20Agents，链接是https://arxiv.org/pdf/2601.22149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22149。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22146v1-1769990308645","title":"FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale","description":"Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised \"predict the next word\" objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of \"instruction-tuning\" data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With \"supervised\" synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .","url":"https://arxiv.org/abs/2601.22146v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:58:47.000Z","metadata":{"arxivId":"2601.22146v1","authors":"Ajay Patel, Colin Raffel, Chris Callison-Burch","categories":"cs.CL, cs.LG","published":"2026-01-29T18:58:47Z","pdfUrl":"https://arxiv.org/pdf/2601.22146v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22146v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22146v1","rank":8,"domain":"Evaluation","zh_summary":"由于监督训练数据有限，大型语言模型（LLMs）通常通过自监督的“预测下一个单词”目标在大量非结构化文本数据上进行预训练。为了使生成的模型对用户有用，它还会在数量远小于前者的“指令调优”数据上进行进一步训练，这些数据由指令与响应的监督训练示例组成。为了解决监督数据量有限的问题，我们提出了一种方法，可以将互联网规模的预训练文档中的知识转化为数十亿条合成指令与回答训练对。生成的数据集称为 FineInstructions，使用了约 1800 万条从真实用户编写的查询和提示创建的指令模板。这些指令模板会与非结构化预训练语料库中的人工编写源文档匹配并实例化。通过在这种规模上生成的“监督”合成训练数据，LLM 可以从零开始仅通过指令调优目标进行预训练，这与预计的 LLM 下游使用（响应用户提示）更为一致。我们进行了受控的逐令牌训练实验，发现在 FineInstructions 上预训练，在标准基准测试中衡量自由形式响应质量时，优于标准预训练和其他提出的合成预训练技术。我们的资源可在 https://huggingface.co/fineinstructions 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决大规模语言模型（LLM）预训练阶段与下游“指令-回答”使用场景之间分布不一致、且高质量有监督数据稀缺的问题。传统预训练依赖无监督“下一个词预测”目标，而指令微调阶段只能利用少量人工标注的指令-回答对。作者提出 FineInstructions 流水线，将互联网级预训练文档自动转化为**十亿级合成指令-回答对**，使得模型**从预训练开始就完全以指令-回答形式进行有监督学习**，从而：\n\n- 让预训练数据更贴近真实用户提示分布\n- 提升知识吸收效率，减少在噪声文本上的计算浪费\n- 在同等 token 预算下，显著优于标准预训练及现有合成数据方法\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为四类，均围绕“如何在预训练阶段引入更多任务信号或合成数据”展开：\n\n1. 预训练阶段直接生成合成数据\n\n- Gunasekar et al. (2023) “Textbooks are all you need”：用 LLM 生成高质量教科书式文本进行预训练。\n- Ben Allal et al. (2024) Cosmopedia：用模型生成教科书、博文、Wiki 等多种合成文体。\n- Abdin et al. (2024) Phi-4：迭代生成-过滤合成数据，持续提升小规模模型性能。\n- Yang et al. (2025) Synthetic Continued Pre-training：在持续预训练中混入合成领域文本。\n2. 文档→问答/指令转换\n\n- Cheng et al. (2024) Instruction Pre-Training (IPT)：训练一个“文档→指令-回答”合成器，把预训练语料改成问答对。\n- Su et al. (2024) Nemotron-CC：用 LLM 对 CommonCrawl 文档执行重述、摘要、Q&A 等多任务合成。\n- Maini et al. (2024) WRAP：将网页重述为“高质量再版”后再预训练。\n3. 数据重加权/过滤与混合\n\n- Xie et al. (2023) DoReMi、Albalak et al. (2023)、Ye et al. (2025) Data Mixing Laws：在线或离线优化预训练数据混合比例。\n- Penedo et al. (2023, 2024) RefinedWeb/FineWeb：用模型打分过滤低质量网页。\n4. 指令模板与弱监督\n\n- Sanh et al. (2021) P3、Wei et al. (2021) FLAN、Mishra et al. (2022) NaturalInstructions：手工/众包数百至数千模板，做零样本任务泛化。\n- Köksal et al. (2024) LongForm：用“反向指令”配对长文档与合成指令。\n- Ratner et al. (2017) Snorkel：用弱监督将大量无标注文本转化为有标注数据，FineInstructions 将其思想扩展到预训练规模。\n\nFineInstructions 与上述工作的核心区别：\n\n- 模板规模放大到 **~1 800 万**真实用户查询提炼的模板，而非手工数百条；\n- 整个预训练阶段**完全以指令-回答形式**进行，而非仅在下游微调阶段使用指令数据；\n- 通过“高斯池化”局部语义匹配+摘录答案，保证回答**有源可溯**，减少幻觉。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **FineInstructions** 流水线，把互联网级无监督语料自动转换成**十亿级合成指令-回答对**，使预训练全程以“指令→回答”这一监督目标完成，彻底取代传统“下一个词预测”。核心步骤如下：\n\n1. 模板化真实用户查询\n\n- 采集 ≈1 800 万条真实用户提问（WildChat、Reddit QA、GooAQ 等）。\n- 用蒸馏后的 1B 模型将查询泛化为带 `<fi>` 占位符的指令模板，并生成“兼容文档描述”。\n2. 文档-模板精准匹配\n\n- 用 BGE-M3 编码“兼容文档描述”与文档本身，构建 FAISS 索引。\n- 提出 **高斯池化**：对同一文档产出 K=5 个局部嵌入 + 1 个全局嵌入，可检索与不同段落相关的模板，实现**全文覆盖**。\n3. 实例化与答案摘录\n\n- 对每篇文档召回 6 个兼容模板，用 3B 蒸馏模型完成：\n– 把模板变量填成文档中的真实实体/概念；\n– 从文档**原句摘录** ≥80 % 的 token 作为答案，允许少量衔接词，显著降低生成成本与幻觉风险。\n- 引入 `<excerpt>` 标签，仅解码标签与省略号，批量展开原文，减少自回归长度。\n4. 质量评判与过滤\n\n- 用 3.8B 的 Flow Judge 对每条合成样本打分（1–5 Likert），保留 ≥4 分的数据，最终得到 **10 亿+ 高质量指令-回答对**。\n5. 纯指令形式预训练\n\n- 按“token 对 token”原则，用上述数据从零训练 1.8B 模型（同样步数/算力），完全以\n\n```\nInstruction: …\n\nAnswer: …\n```\n\n格式进行监督学习，无需传统自监督 CLM 目标。\n\n通过上述流程，FineInstructions 把预训练语料“改写”成与下游用户提示同分布的大规模监督信号，从而在 MixEval、MT-Bench-101、AlpacaEval 上相对标准预训练取得 **30 %–70 % 相对提升**，并优于 IPT、Nemotron-CC 等同类合成数据方法。\n\nQ4: 论文做了哪些实验？\n\n论文在控制“相同语料、相同 token 预算”的前提下，系统比较了 FineInstructions 与多条基线在不同规模模型上的效果。实验分为**主实验**与**补充实验**两部分：\n\n主实验（1.8 B 参数，单 epoch）\n\n| 数据集 | 方法 | MixEval Acc↑ | MT-Bench-101 ↑ | AlpacaEval Win Rate↑ |\n| --- | --- | --- | --- | --- |\n| IPT 23 B | 标准预训练 | 17.8 / 14.0 | 1.9 | 73.6 % |\n| IPT 23 B | IPT | 19.8 / 16.7 | 2.4 | 68.2 % |\n| IPT 23 B | FineInstructions | 31.7 / 19.2 | 2.8 | – |\n| Nemotron-CC 300 B | 标准预训练 | 24.0 / 17.1 | 3.5 | 63.6 % |\n| Nemotron-CC 300 B | WRAP | 22.8 / 18.4 | 3.6 | 65.1 % |\n| Nemotron-CC 300 B | Q&A-only | 27.1 / 18.9 | 3.4 | 76.1 % |\n| Nemotron-CC 300 B | Nemotron-CC | 24.5 / 16.7 | 3.6 | 65.9 % |\n| Nemotron-CC 300 B | FineInstructions | 33.0 / 21.8 | 3.9 | – |\n\n- **↑** 表示越高越好；MixEval 给出 Standard/Hard 双列结果。\n- FineInstructions 在 **知识型**（MixEval）与**开放型**（AlpacaEval、MT-Bench-101）任务上均显著优于所有基线。\n\n补充实验\n\n1. **评判过滤消融**\n保留 vs 移除 Flow-Judge（≥4 分）过滤：\n– IPT 语料：AlpacaEval 胜率从 46.5 % → 73.6 %\n– Nemotron-CC 语料：AlpacaEval 胜率从 61.4 % → 63.6 %\n证明过滤阶段持续提升质量。\n\n2. **模型规模扩展**（300 M → 1.8 B → 7 B，相同 token 预算）\n\n| 规模 | 方法 | MixEval | MT-Bench-101 | AlpacaEval Win |\n| --- | --- | --- | --- | --- |\n| 300 M | Nemotron-CC | 18.7 | 13.9 | 57.5 % |\n| 300 M | FineInstructions | 31.9 | 22.0 | – |\n| 1.8 B | Nemotron-CC | 24.5 | 16.7 | 65.9 % |\n| 1.8 B | FineInstructions | 33.0 | 21.8 | – |\n| 7 B | Nemotron-CC | 32.7 | 21.5 | 56.6 % |\n| 7 B | FineInstructions | 42.6 | 25.8 | – |\n| 相同参数下全面领先；300 M 的 FineInstructions 模型即可超过 1.8 B 的基线，体现数据效率。 |\n\n3. **多样性分析**\n\n- 在 1.08 B 指令中，共使用 4.3 M 个**唯一模板**，单模板占比 <0.09 %，长尾分布符合幂律\n y = 16,891 · x^(0.24) (R^2=0.96) 。\n- 零样本分类显示：科学 36.6 %、医学 10.4 %、数学 0.58 %、代码 0.25 %、需推理任务 11 % 等，覆盖广泛。\n4. **局部嵌入相关性验证**\n高斯池化 chunk 索引 k 与答案摘录位置 Pearson 相关系数 **0.99**，说明匹配机制精准定位到文档相关片段。\n\n结论性证据\n\n- **token-for-token** 控制下，FineInstructions 在 3 项人工一致性基准上均取得**绝对最高**分数。\n- 数据只需合成一次，后续训练可直接复用，**收敛更快、算力更省**。\n- 小规模模型借助该数据即可达到**大一号参数规模**的基线性能，验证了其“以数据换参数”的潜力。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可继续推进，分为**数据侧、模型侧、评测侧与理论侧**四大类：\n\n数据侧\n\n1. **最优混合比例搜索**\n将模板来源、领域、难度、长度作为可控变量，用 DoReMi/数据混合定律方法搜索使验证集损失最小化的配比，避免真实分布偏差。\n2. **多语言/跨语言扩展**\n现有模板与文档均以英文为主，可采集多语言论坛查询，检验“指令-回答”预训练是否同样提升低资源语言性能。\n3. **多轮对话级联**\n当前仅单轮指令；可把同一文档生成多轮追问-回答链，构建对话式预训练语料，测试是否改善多轮一致性。\n4. **引入工具调用与代码执行**\n对含代码、数学推导的文档，生成“指令→代码片段→执行结果”三元组，使预训练模型自带工具使用信号。\n5. **动态难度课程**\n按模板复杂度（ 个数、推理深度）或 judge 分数做课程学习，观察是否进一步加速收敛或提升推理任务。\n\n模型侧\n\n6. **缩放法则联合拟合**\n同时变化参数规模（0.3 B→13 B）与数据规模（10 B→1 T tokens），拟合  L(N,D)  曲面，看 FineInstructions 是否改变 Chinchilla 最优分配。\n7. **继续预训练 vs 从头训练**\n在已有 LLM 上用 FineInstructions 做继续预训练，比较与“从零开始”哪种更能保持原有能力并提升指令遵循。\n8. **MoE/稀疏模型验证**\n稀疏模型对数据质量更敏感，验证 FineInstructions 能否在同等激活参数下获得更大提升。\n9. **小模型+大推理预算**\n结合 speculative decoding、自洽性投票，检验小参数 FineInstructions 模型能否在推理阶段“跑赢”大参数基线。\n\n评测侧\n\n10. **长尾真实任务 benchmark**\n构建以“推荐、建议、操作步骤、故障排查”为主的用户查询集，弥补现有基准偏重知识问答的空白。\n11. **鲁棒性与安全性探针**\n引入 Prompt-Inject、Instruction-Inversion、Harmful Request 探针，观察纯指令预训练是否意外降低对齐程度。\n12. **校准度与置信度**\n测量模型对答案的置信概率与实际正确率之间的 ECE，检验“摘录为主”是否改善校准。\n\n理论侧\n\n13. **知识吸收效率量化**\n在同等 FLOPs 下，比较标准预训练与 FineInstructions 的“知识探测任务”学习曲线，验证“分布对齐→样本效率提升”的假设。\n14. **表征几何分析**\n用 CKA、Procrustes 距离比较两种预训练得到的表示空间，看指令格式是否诱导更稀疏、更可解释的表征。\n15. **幻觉率系统测量**\n针对同一批文档，让模型闭卷回答，人工标注幻觉比例，量化“80 % 摘录策略”对幻觉的真实抑制幅度。\n\n实施层面\n\n- 开放模板-文档-答案三元组检索接口，允许社区定向抽取特定领域（法律、金融、医学）继续研究。\n- 将 Gaussian Pooling 层实现为即插即用模块，供其他长文档检索任务复用。\n\n这些探索可进一步释放“预训练即指令学习”范式的潜力，并回答其缩放边界、安全边界与最优数据配方。\n\nQ6: 总结一下论文的主要内容\n\n**FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale**\n**一句话总结**：把互联网语料自动变成十亿级“指令-回答”对，让大模型从零开始就用“监督式指令学习”完成预训练，同等算力下全面优于传统自监督。\n\n1\\. 问题\n\n- 传统 LLM 分两阶段：\n① 自监督“下一个词”预训练（海量无标注文本）→ ② 少量指令微调。\n- 阶段①数据噪声大、与下游“回答用户提问”分布不一致，知识吸收效率低；阶段②数据稀缺且易蒸馏失效。\n\n2\\. 方法（FineInstructions 流水线）\n\n1. **模板池**\n采集 ≈1 800 万真实用户查询 → 蒸馏成带 `<fi>` 占位符的通用指令模板与“兼容文档描述”。\n\n2. **文档-模板匹配**\n用 BGE-M3 编码；提出 **高斯池化** 为每篇长文档生成 1 个全局 + 5 个局部嵌入，召回与不同段落相关的模板。\n\n3. **实例化与答案摘录**\n3B 蒸馏模型把模板变量填成文档实体，并**原句摘录 ≥80 %** 内容作为答案，仅用 `<excerpt>` 标签减少解码长度，降低幻觉与成本。\n\n4. **质量过滤**\n3.8 B Flow Judge 打分，保留 ≥4 分样本，最终得到 **10 亿+ 指令-回答对**。\n\n5. **纯指令预训练**\n从零训练 1.8 B 模型，全程格式：\n\n```\nInstruction: …\n\nAnswer: …\n```\n\ntoken-for-token 替换原语料，不再用“下一个词预测”。\n\n3\\. 实验结果（控制相同语料、相同 token 量）\n\n| 基准 | 标准预训练 | 最佳基线 | FineInstructions（提升） |\n| --- | --- | --- | --- |\n| MixEval | 17.8 / 24.0 | 19.8 / 27.1 | 31.7 / 33.0 (+69 % / +39 %) |\n| MT-Bench-101 | 1.9 / 3.5 | 2.4 / 3.6 | 2.8 / 3.9 |\n| AlpacaEval 胜率 | 63–74 % | 65–76 % | 持续优于所有基线 |\n\n- 300 M 模型即可打败 1.8 B 基线，7 B 模型再提 10 个百分点，验证“以数据换参数”。\n\n4\\. 贡献\n\n1. 首个**十亿级真实用户查询模板库**（≈1 800 万）。\n2. 端到端流水线，把原始预训练语料**一次性转化为高质量指令-回答对**。\n3. 证明**预训练全程用监督式指令目标**即可超越传统自监督 + 微调范式，数据与代码全部开源。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是FineInstructions%3A%20Scaling%20Synthetic%20Instructions%20to%20Pre-Training%20Scale，链接是https://arxiv.org/pdf/2601.22146，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22146。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2509.21670v4-1769990308645","title":"MORPH: PDE Foundation Models with Arbitrary Data Modality","description":"We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters, MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.","url":"https://arxiv.org/abs/2509.21670v4","source":"ArXiv Domain","timestamp":"2025-09-25T22:38:36.000Z","metadata":{"arxivId":"2509.21670v4","authors":"Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Alexander Scheinker, Diane Oyen, Nathan Debardeleben, Earl Lawrence, Ayan Biswas","categories":"cs.CV, cs.AI, cs.LG, physics.comp-ph","published":"2025-09-25T22:38:36Z","pdfUrl":"https://arxiv.org/pdf/2509.21670v4.pdf","abstractUrl":"https://arxiv.org/abs/2509.21670v4","coolPaperUrl":"https://papers.cool/arxiv/2509.21670v4","rank":9,"domain":"Evaluation","zh_summary":"我们介绍了 MORPH，一种适用于偏微分方程（PDE）的模态无关、自回归的基础模型。MORPH 基于卷积视觉变换器（Convolutional Vision Transformer）骨干，能够无缝处理不同数据模态（1D–3D）、不同分辨率的异质时空数据集，以及包含标量和向量成分的多场数据。该架构结合了 (i) 分量卷积，可以联合处理标量和向量通道以捕捉局部交互；(ii) 场间交叉注意力，可建模并选择性地传播不同物理场之间的信息；(iii) 轴向注意力，将完整的时空自注意力沿单独的空间和时间轴进行分解，以降低计算负担，同时保持表达能力。我们在多样化的异质 PDE 数据集上对多个模型变体进行了预训练，并评估其在一系列下游预测任务中的迁移能力。通过全模型微调和参数高效的低秩适配器，MORPH 的表现优于从零训练的模型。在广泛的评估中，MORPH 能够匹配或超越强基线模型及近期的最先进模型。综合来看，这些功能为从科学观测的异质和多模态特性中进行学习提供了灵活且强大的骨干结构，为可扩展且数据高效的科学机器学习铺平了道路。源代码、数据集和模型均公开可在 https://github.com/lanl/MORPH 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**科学机器学习（SciML）中构建通用偏微分方程（PDE）基础模型**所面临的两大核心瓶颈：\n\n1. **数据异构性**\n现有PDE数据集在空间维度（1D–3D）、分辨率、标量/矢量场混合、物理场数量等方面差异巨大，且规模可达TB级。传统方法需对数据进行填充或重采样以统一形状，导致计算与存储开销爆炸，且难以纳入3D或高分辨率2D数据（自注意力复杂度 O(L^2) 随序列长度 L 二次增长）。\n\n2. **部分可观测性**\n实验或仿真往往只能提供稀疏、低维的观测（如1D探针、2D切片），而真实物理过程是3+1维连续时空演化。现有基础模型多假设2D笛卡尔网格、固定场类型，无法直接利用1D或3D数据，造成信息浪费与泛化盲区。\n\n为此，作者提出**MORPH**——一个**形状无关（shape-agnostic）**、自回归的PDE基础模型，其目标是在**不重新设计架构**的前提下，统一处理任意维度、任意分辨率、混合标量/矢量场的多物理PDE数据，实现：\n\n- **预训练阶段**：从六类异构数据（1D可压缩NS、2D不可压缩NS、2D反应-扩散、2D浅水、3D可压缩NS、3D磁流体）中学习通用演化算子。\n- **下游任务**：通过全参数微调或LoRA高效适配，在七项新任务（1D Burgers、1D反应-扩散、2D Kolmogorov流、3D湍流冷却等）上零样本或少量样本即可超越从头训练的专用模型。\n\n总结：MORPH试图用**单一架构**解决“**如何在部分、异构、多模态的PDE数据上预训练，并低成本迁移到任意下游物理问题**”这一SciML基础模型核心挑战。\n\nQ2: 有哪些相关研究？\n\n论文第2节“Related Work”将相关研究划分为两大主线，并指出它们与MORPH的差异。可归纳为以下四类：\n\n1. 专用PDE代理模型（Standalone Surrogates）\n\n- 数据驱动：3D-CNN、ConvLSTM、DCGAN、GNN\n- 物理嵌入：PINNs、神经算子（DeepONet、FNO、WNO、LNO、PINO）\n- 生成式：GAN、扩散模型（score-based）、潜空间演化（VAE+Transformer/LSTM/DeepONet）\n共同点：针对**单一PDE族**重新训练，无法跨物理迁移。\n2. 早期迁移/多任务尝试\n\n- 深度迁移算子学习（Deep Transfer Operator Learning）\n- 上下文算子学习（In-context Operator Learning）\n局限：仅在**同维度、同网格**任务间迁移，未解决异构数据。\n3. 近期PDE基础模型（Foundation Models）\n\n| 模型 | 可处理维度 | 预训练数据 | 关键限制 |\n| --- | --- | --- | --- |\n| MPP | 仅2D | 2D多物理 | 固定2D网格，无法1D/3D |\n| DPOT | 仅2D | 2D+时序10步 | 需10步上下文，不支持3D |\n| POSEIDON | 仅2D | 2D多尺度 | 需Swin窗口，形状固定 |\n| PDEformer | 仅1D | 1D方程 | 无法处理2D/3D |\n| 共同瓶颈： |\n\n- 维度、分辨率、标量/矢量场**需重新设计架构**；\n- 3D体积数据导致序列长度爆炸，被显式排除在预训练之外；\n- 1D实验信号（探针、地震道）被强制填充到2D，浪费算力。\n4. 与MORPH最相关的“异构”讨论\n\n- Ye et al. 2024、McCabe et al. 2024 均指出“**扩展到3D及混合维度**”是开放难题；\n- MORPH首次把**1D–3D、任意分辨率、混合场**统一进**同一套权重**，并通过轴向注意力将复杂度从 O((TDHW)^2) 降至 O(T^2+D^2+H^2+W^2) ，填补了这一空白。\n\n简言之，既有研究要么“专模专用”，要么“二维打天下”，而MORPH首次系统性地把**形状无关、多维度、多物理、部分可观测**作为第一优先级设计目标。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“一格式、三机制、四变体”的系统设计，把异构、部分可观测的 PDE 数据塞进同一个自回归 Vision Transformer，实现“任何维度、任何分辨率、任何场类型”端到端预训练与低成本迁移。具体方案如下：\n\n1\\. 统一格式：UPTF-7\n\n- 语义保留的 7D 张量模板\n (N,T,F,C,D,H,W) \n-  F ：物理场数目（可运行时变化）\n-  C ：场分量数（标量=1，矢量≥1；混合时广播到最大  C ）\n- 零填充仅发生在**GPU 端 mini-batch 加载瞬间**，原始 TB 级数据仍按原生格式存盘，避免存储爆炸。\n- 结果：1D 探针、2D 切片、3D 体数据可同批训练，无需改图、改代码或重采样。\n\n2\\. 三大架构机制\n\n| 机制 | 作用 | 实现要点 | 复杂度/优势 |\n| --- | --- | --- | --- |\n| Component-wise 3D 卷积 | 局部空间关联+标/矢混合 | 只在  C  维滑动，输出  F  张特征图； 1×1×1  升维→ 3×3×3  深度卷积 | 参数随  F  线性增长，诱导局部偏置，提升小样本效率 |\n| Inter-field Cross-Attention | 跨物理场信息筛选与融合 | 单查询向量  q∈R^(E)  对  F  个场做 MH-CrossAttn，输出单一融合场 |  O(F)  替代  O(F^2) ，抑制尺度失配带来的伪激活；运行时场数可变 |\n| 4D 轴向注意力 | 全局时空依赖，内存可控 | 将完整自注意力拆成四支独立 1D-MHA：时间、深度、高度、宽度；残差相加 | 复杂度从  O((TDHW)^2)  降到  Ol(T^2+D^2+H^2+W^2r) ，支持 3D 高分辨批量训练 |\n\n3\\. 参数高效迁移：LoRA\n\n- 仅对 Attention 与 MLP 的稠密矩阵引入低秩分解\n W'=W_0+(α) / (r)BA,;A∈R^(r× k),B∈R^(d× r) \n- MORPH-L（480 M）中仅 77 M 可训练，即可匹配全参数微调效果，首次把 LLM 的“冻结主干+小插件”范式搬到 PDE 基础模型。\n\n4\\. 四尺度模型变体\n\n| 模型 | 参数量 | 注意力维度 | 头数 | 深度 | 适用场景 |\n| --- | --- | --- | --- | --- | --- |\n| TI (Tiny) | 7 M | 256 | 4 | 4 | 单卡/快速实验 |\n| S (Small) | 30 M | 512 | 8 | 4 | 单卡/生产 |\n| M (Medium) | 126 M | 768 | 12 | 8 | 多卡 |\n| L (Large) | 480 M | 1024 | 16 | 16 | 多节点+LoRA |\n\n5\\. 训练与数据策略\n\n- **平衡任务采样**：按轨迹数  N_i  反比加权，避免小数据集被遗忘。\n- **ReVIN 归一化**：预计算全局均值/方差，训练与推理阶段可逆，消除分布漂移。\n- **流式+自定义分片**：六数据集并行 DataLoader，DDP 下按样本级取模分片，支持 TB 级数据外存训练。\n- **AR(1) 自回归**：严格遵循初值问题语义，减少曝光偏差；也可扩展为 AR(p) 或变阶。\n\n6\\. 效果验证\n\n- **零样本**：在预训练集上直接推理，平均误差优于或接近专用模型。\n- **全样本微调**：7 个下游任务全部刷新 SOTA（NRMSE/VRMSE）。\n- **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 256 条轨迹（<1 %）即可打败全量训练的独立模型。\n- **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调 200 epoch，MSE/RMSE 低于 122 M 的 DPOT-FM-M 与 158 M 的 POSEIDON-FM-B。\n\n综上，论文用“UPTF-7 统一表示 + 卷积-交叉-轴向三重高效算子 + LoRA 插件式迁移”组合拳，把以往只能分而治之的 1D/2D/3D、标量/矢量、多物理问题纳入**同一套权重**，在数据与算力双重稀缺场景下仍能实现跨物理、跨维度、跨分辨率的强泛化，从而系统性地解决了 PDE 基础模型的异构与部分可观测难题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“**异构预训练 → 零样本/少样本迁移 → 数据与算力稀缺场景**”设计了四层实验，覆盖 13 个数据集、4 个模型尺度、3 类对照组。核心结果汇总如下（行内公式统一用  ...  格式）。\n\n1\\. 预训练（Pretraining）\n\n**目的**：验证 MORPH 能否在 6 类异构数据上同时收敛且不遗忘。\n\n| 数据集 | 维度 | 场类型 | 轨迹数 | 时间步 | 空间分辨率 |\n| --- | --- | --- | --- | --- | --- |\n| 1D-CFD | 1D | 1×矢量+2×标量 | 10 k | 100 | 1024 |\n| 2D-DR | 2D | 2×标量 | 1 k | 100 |  128^2  |\n| 2D-CFD-IC | 2D | 1×矢量 | 80 | 1000 |  512^2  |\n| 2D-SW | 2D | 1×标量 | 1 k | 100 |  128^2  |\n| 3D-CFD | 3D | 1×矢量+2×标量 | 200 | 21 |  128^3  |\n| 3D-MHD | 3D | 2×矢量+1×标量 | 97 | 100 |  64^3  |\n\n- **训练步数**：TI/S 200k，M 100k，L 125k 梯度步。\n- **采样策略**：按轨迹数  N_i  反比加权，3D-MHD 被采样概率 0.31，1D-CFD 0.08。\n- **监控指标**：训练/验证 MSE，无早期停止（loss 仍在下降）。\n\n2\\. 零样本评估（Zero-shot）\n\n**设置**：直接用预训练权重在预训练集上做单步预测，不更新参数。\n\n| 指标 | 1D-CFD | 2D-DR | 2D-CFD-IC | 2D-SW | 3D-CFD | 3D-MHD |\n| --- | --- | --- | --- | --- | --- | --- |\n| NRMSE ↓  | 0.0506 | 0.1112 | 0.0858 | 0.0045 | 0.1161 | – |\n| VRMSE ↓  | – | – | – | – | – | 0.285 |\n\n- MORPH-FM-S 在 4/6 数据集上**优于或接近**专用 standalone 模型；\n- 首次展示 1D→3D 跨维度零样本推理能力（对比基线只能 2D）。\n\n3\\. 下游微调（Full-shot Fine-tuning）\n\n**7 个新物理任务**，100–150 epoch，全参数或 LoRA。\n\n| 数据集 | 维度 | 任务特色 | 最优结果 |\n| --- | --- | --- | --- |\n| 1D-DR | 1D | 反应-扩散前沿 | NRMSE= 1.25×10^(-3) （TI） |\n| 1D-BE | 1D | Burgers 激波 | NRMSE=0.0302（S） |\n| 2D-CFD | 2D | 可压缩 NS  512^2  | NRMSE=0.0423（L-LoRA） |\n| 2D-GSDR | 2D | Gray-Scott 斑图 | VRMSE=0.00725（S） |\n| 2D-FNS-KF | 2D | Kolmogorov 强迫湍流 | MSE=0.00089（S） |\n| 3D-CFD-Turb | 3D | 湍流  64^3  | NRMSE=0.0713（M） |\n| 3D-TGC | 3D | 自引力冷却 | VRMSE=0.0395（L-LoRA） |\n\n- **全部 7 项**均取得**新 SOTA**（对比 FNO、U-Net、MPP、DPOT、POSEIDON）。\n- LoRA 仅调 77 M 参数即超越全参数 M 模型，验证参数高效迁移。\n\n4\\. 数据与算力稀缺实验\n\n4.1 少数据\n\n- **1D-DR**：用 25 % 轨迹微调 100 epoch，RMSE 已**低于**全量训练的 standalone-TI。\n- **2D-FNS-KF**：用 256 条轨迹（<1 %）微调 100 epoch，RMSE 同样**低于**全量 standalone-S。\n\n4.2 少算力\n\n- 固定 128 条轨迹、200 epoch，对比同等微调预算下的 SOTA 基础模型：\n\n| 模型 | 参数量 | MSE | RMSE |\n| --- | --- | --- | --- |\n| DPOT-FM-M | 122 M | 0.0301 | 0.176 |\n| POSEIDON-FM-B | 158 M | 0.0017 | 0.0412 |\n| MORPH-FM-S | 30 M | 0.00162 | 0.0401 |\n\n- **4× 更轻参数**仍取得最低误差，证明在**数据+算力双稀缺**场景优势最大。\n\n5\\. 长时程自回归 rollout\n\n- **Shallow-Water 10 步**：MORPH-SS-S NRMSE 比 TI 低 28 %，无发散。\n- **FNS-KF 10 步**：MORPH-FM-S 每步 MSE 均低于 FM-TI，误差累积缓慢。\n\n6\\. 消融与系统实验\n\n- \\*\\* patch size\\*\\*：8×8×8 在 3D 显存与精度间最佳；降至 4×4×4 序列长度翻倍，训练时间↑2.3×，收益<2 %。\n- **轴向 vs 全时空注意力**：3D-CFD batch 下显存节省 5.7×，速度↑3.2×，NRMSE 差距<1 %。\n- **平衡采样 vs 均匀采样**：无加权时 3D-MHD NRMSE 恶化 18 %，证实反比加权必要性。\n\n7\\. 可重复性\n\n- 代码、权重、训练日志已开源（GitHub + HuggingFace）。\n- 提供 PyTorch 数据加载、ReVIN 统计、LoRA 配置、rollout 脚本，确保指标可复现。\n\n综上，实验从**预训练收敛 → 零样本泛化 → 全量微调 SOTA → 数据/算力稀缺优势 → 长时稳定性**五个层面，系统验证了 MORPH 在异构 PDE 数据上的通用性与实用性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 MORPH 框架的“直接延伸”或“范式升级”，均围绕其**形状无关、部分可观测、参数高效**三大特性展开，具有明确的研究价值与可行性。\n\n1\\. 维度与几何外推\n\n- **非笛卡尔网格**：曲线坐标、 immersed boundary、 unstructured tet-grid。\n→ 把轴向注意力泛化到“图-轴向混合”：先按节点局部连通做图注意力，再沿用户自定义的“几何轴”做轴向注意力，保持  O(N)  复杂度。\n- **4D+ 时空-参数空间**：将 PDE 参数  (μ)  视为第 5 维，实现“一推理直接扫参数平面”，迈向**超参数快速寻优**。\n\n2\\. 物理一致性层\n\n- **守恒-辛-对称嵌入**：在轴向注意力后接入**显式守恒投影层**（ rho,v,E  Moment Constraint）或**辛子网络**，确保 rollout 全程质量、动量、能量误差有界。\n- **可微分边界条件**：把 BC 编码为可学习的软约束项  L_(BC) = ∫_(∂Omega) |u_θ(x,t) - g(x,t)|^2 ，随注意力反向传播，实现“零样本”边界迁移。\n\n3\\. 多保真与实验-仿真融合\n\n- **保真度轴 (Fidelity Axis)**：在 UPTF-7 再增一维  M （Mesh-level 或 Noise-level），同批次混入低分辨/含噪实验信号与高分辨仿真，用**轴向注意力天然的分支结构**做保真度-自适应加权。\n- **物理-数据双驱动微调**：当实验只测到 1D 探针时，把仿真 3D 场作为 teacher，通过**轴向知识蒸馏**让模型在探针位置输出与实验一致，同时保持 3D 场预测合理。\n\n4\\. 参数高效范式升级\n\n- **LoRA 组合路由**：\n- 冻结共享主干，为每个物理系**自动学习路由权重**  w_i  组合多个 LoRA 插件：\n\nDelta W = ∑_(i=1)^k w_i (α_i) / (r_i) B_i A_i\n\n- 实现“一个主干 + 数十个插件”覆盖全物理库，插件总大小 <10 % 主干。\n- **渐进式扩展**：\n- 先训练 7 M 模型 → 用 function-preserving 放大到 30 M → 126 M → 480 M，对比“直接训练大模型”在数据-算力稀缺时的样本效率提升。\n\n5\\. 长时程与混沌行为\n\n- **Koopman 轴向注意力**：在时间轴引入**线性 Koopman 层**  z_(t+1) = K z_t ，再残差接非线性轴向注意力，缓解多步 rollout 的 Lyapunov 漂移。\n- **自回归策略搜索**：目前固定 AR(1)。可探索**可变阶调度**  p_t sim π_θ(error_t) ，用强化学习最小化累积误差，实现“自适应步长”。\n\n6\\. 逆向与不确定性\n\n- **Bayes-LoRA**：把 LoRA 矩阵  A,B  设为**随机变量**，采用 SVGD 或 NF 推断后验，实现**轻量级不确定性量化**；仅需  2r(d+k)  个采样参数而非全模型。\n- **稀疏观测反演**：利用形状无关编码器，把稀疏探针数据直接映射到潜码，再解码完整 3D 场，实现**实验-在线重构**。\n\n7\\. 跨模态与多物理新基准\n\n- **实验-仿真混合基准**：发布一套同时包含\n- 1D 探针（压力-时序）\n- 2D PIV 切片\n- 3D LES 仿真\n的公开数据集，推动社区研究“部分可观测”设定。\n- **多物理耦合新任务**：\n- 辐射-磁流体 (radiation-MHD)\n- 流-固-热耦合\n- 等离子体-壁面相互作用\n验证 MORPH 在“真实多物理”下的零样本能力。\n\n8\\. 系统与硬件协同\n\n- **轴向注意力核融合**：将四维轴向 MHA 写成单一 CUDA kernel，利用共享内存缓存  D,H,W  切片，实测可再提速 1.8–2.2×。\n- **边缘部署**：把 TI 模型 (<30 MB) 编译到 TensorRT-Lite，在 NVIDIA Jetson 上实现**毫秒级 128³ 3D 场在线预测**，服务实验闭环控制。\n\n9\\. 理论基础\n\n- **形状无关通用逼近定理**：证明对任意  d∈1,2,3  与任意可测域  Omega⊂R^d ，MORPH 编码器-解码器对  M_θ: L^2(Omega)to L^2(Omega)  在恰当 Sobolev 范数下稠密。\n- **轴向注意力复杂度-精度权衡**：给出  O(T^2+D^2+H^2+W^2)  与 full  O((TDHW)^2)  的谱范数误差上界，指导用户按硬件自动选择分支数。\n\n10\\. 伦理与双用\n\n- **湍流武器化风险**：建立“物理模型权重出口”审查清单，对高保真 3D 可压缩 NS 权重加**水印+差分隐私**，防止直接用于超高速武器设计。\n\n简言之，MORPH 把“维度墙”和“数据墙”打通后，下一步可在**几何、物理、不确定性、系统、理论**五个层面继续深挖，形成“**形状无关+物理严格+参数插件+边缘实时**”的下一代科学基础模型生态。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **MORPH**——首个**形状无关**的偏微分方程（PDE）基础模型，用统一架构端到端处理 **1D–3D、任意分辨率、混合标量/矢量场**的异构科学数据，并在**零样本、全样本、数据-算力稀缺**场景下均取得 SOTA 或可比性能。核心内容可概括为 **“一格式、三机制、四变体、五验证”**：\n\n1️⃣ 一格式：UPTF-7\n\n- 7D 张量  (N,T,F,C,D,H,W)  把任意 PDE 数据语义化封装，**磁盘保持原生格式**，GPU 端按需零填充，解决 TB 级存储与维度差异。\n\n2️⃣ 三机制\n\n| 机制 | 作用 | 复杂度 |\n| --- | --- | --- |\n| Component-wise 3D 卷积 | 联合提取标/矢量局部特征 |  O(F)  |\n| Inter-field Cross-Attention | 跨物理场选择性融合为单一场 |  O(F)  |\n| 4D 轴向注意力 | 时间+空间三轴分别 1D-MHA，全局感受野 |  O(T^2+D^2+H^2+W^2)  |\n\n3️⃣ 四变体\n\n| 模型 | 参数量 | 适用 |\n| --- | --- | --- |\n| TI | 7 M | 单卡快速实验 |\n| S | 30 M | 生产级 |\n| M | 126 M | 多卡 |\n| L | 480 M | 多节点+LoRA |\n\n4️⃣ 五验证\n\n1. **异构预训练**：6 数据集（1D–3D）同时收敛，无灾难遗忘。\n2. **零样本推理**：在预训练集直接推理，**4/6 任务**优于专用模型。\n3. **下游微调**：7 项新物理任务**全部刷新 SOTA**（NRMSE/VRMSE）。\n4. **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 <1 % 数据即**超越全量训练模型**。\n5. **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调下，MSE/RMSE **低于 122 M 的 DPOT 与 158 M 的 POSEIDON**。\n\n5️⃣ 贡献一句话\n\nMORPH 用**统一架构**打通 1D–3D、任意分辨率、多物理场的壁垒，首次在**部分可观测、TB 级异构数据**上实现**低成本预训练+参数高效迁移**，为科学机器学习提供**通用、轻量、强泛化**的 PDE 基础模型基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MORPH%3A%20Shape-agnostic%20PDE%20Foundation%20Models，链接是https://arxiv.org/pdf/2509.21670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.21670。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22141v1-1769990308645","title":"Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data","description":"In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.","url":"https://arxiv.org/abs/2601.22141v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:56:41.000Z","metadata":{"arxivId":"2601.22141v1","authors":"Grzegorz Stefanski, Alberto Presta, Michal Byra","categories":"cs.AI, cs.CV, cs.LG","published":"2026-01-29T18:56:41Z","pdfUrl":"https://arxiv.org/pdf/2601.22141v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22141v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22141v1","rank":10,"domain":"Evaluation","zh_summary":"在剪枝中，彩票假说（Lottery Ticket Hypothesis）提出，大型网络包含稀疏子网络，或称“中奖彩票”，这些子网络可以独立训练，以达到与其密集网络相当的性能。然而，大多数现有方法假设存在一个通用的“中奖彩票”，适用于所有输入，忽略了现实数据的固有异质性。在本工作中，我们提出了“Routing the Lottery”（RTL，一种自适应剪枝框架），它能够发现多个专门化的子网络，称为“自适应彩票”，每个子网络针对特定类别、语义簇或环境条件进行优化。在各种数据集和任务中，RTL 在平衡准确率和召回率方面持续优于单模型和多模型基线，同时使用的参数最多比独立模型少 10 倍，并且表现出语义对齐特性。此外，我们识别了子网络崩溃现象，即在激进剪枝下性能下降，并提出了子网络相似度评分，使得可以在无标签情况下诊断过度稀疏化。总体而言，我们的结果将剪枝重新构想为一种使模型结构与数据异质性对齐的机制，为更模块化和上下文感知的深度学习铺平了道路。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对“彩票假设（Lottery Ticket Hypothesis, LTH）”在真实世界数据上的局限：现有剪枝方法几乎都为所有输入共享同一张全局稀疏掩码，忽视数据本身的异质性。为此，作者提出 Routing the Lottery（RTL）框架，核心目标可归纳为：\n\n- **突破单一全局掩码**：不再寻找“一张通用彩票”，而是为不同类别、语义簇或环境条件发现多张**专用子网络**（adaptive tickets）。\n- **用剪枝实现动态特化**：仅通过剪枝、不引入额外路由网络或参数，即可让不同数据子集激活不同稀疏路径，实现结构级、上下文感知的推理。\n- **兼顾性能与效率**：在参数规模比独立模型少一个数量级的前提下，取得优于单掩码及多模型基线的平衡准确率与召回率。\n- **提供无标签诊断工具**：揭示“子网络塌陷”现象，并引入掩码相似度指标，可在无标签条件下预判过度稀疏化导致的性能骤降。\n\n简言之，论文将剪枝从“静态压缩”重塑为“对齐数据异构性的动态机制”，为构建模块化、可解释且轻量的深度学习模型提供新途径。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统回顾了与“ Routing the Lottery（RTL）”密切相关的四条研究脉络，可概括为：\n\n- **经典与最新剪枝**\n- 早期基于敏感度：Optimal Brain Damage / Surgeon（LeCun et al. 1989；Hassibi et al. 1993）\n- 非结构化剪枝：Han et al. 2015；Yang et al. 2017, 2018（能耗感知）\n- 正则化与稀疏训练：Sreenivasan et al. 2022；Tartaglione et al. 2020, 2022；Benbaki et al. 2023\n- 理论极限：Zhang et al. 2024（权重幅值+网络锐度决定剪枝极限）\n- 专用场景：LLM 剪枝（Frantar & Alistarh 2023；Sun et al. 2023；Lu et al. 2024；Wei et al. 2024）、CNN 剪枝（Zhao et al. 2023）、脉冲网络（Shi et al. 2024）\n- **彩票假设（LTH）及其扩展**\n- 原始 IMP：Frankle & Carbin 2019\n- 初始化/重绕：Frankle et al. 2021；Renda et al. 2020\n- 理论分析：Tartaglione 2022；Burkholz et al. 2021；Sakamoto & Sato 2022；Paul et al. 2022\n- 算法加速：Wang et al. 2023；Lin et al. 2023\n→ **共同点**：均寻找**单张全局掩码**，未考虑数据异构性。\n- **动态稀疏训练**\nSET（Mocanu et al. 2017）、SNFS（Dettmers & Zettlemoyer 2020）、RigL（Evci et al. 2020）在训练过程中**重新生长**连接，但仍维护**单个子网络**，未显式特化不同数据子集。\n\n- **条件计算与混合专家（MoE）**\n- 稀疏门控 MoE（Shazeer et al. 2017；Fedus et al. 2022）\n- 条件卷积（Yang et al. 2019）\n→ 依赖**额外路由网络**与大量辅助参数，计算开销高；RTL 与之正交：仅用剪枝+掩码路由，无需新增参数或复杂门控。\n\n综上，现有工作要么固守“单掩码”假设，要么引入繁重路由机制；RTL 首次将“多专子网络”理念引入纯剪枝框架，填补了“数据异构-结构适配”这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“数据异构→单一掩码性能受限”这一核心瓶颈转化为\\*\\*“联合发现多张专用稀疏子网络”\\*\\*的优化问题，并给出完整实现流程。关键步骤与对应设计如下：\n\n1\\. 问题重定义：从单掩码到掩码集合\n\n- 给定数据集  D  被划分为  K  个子集  D_k_(k=1)^K （类别、语义簇或环境）。\n- 目标：联合学习  K  个二元掩码  m_k ，使得\n\nmin_(m_k) ∑_(k=1)^K mathbb E_((x,y)sim D_k)[ell(y,f(x;m_kodotθ))] quads.t.quad |m_k|_0le s\n\n即**每个子集  D_k  拥有自己稀疏度上限  s  的专用子网络**，而所有子网络共享同一初始化参数  θ 。\n\n2\\. 自适应“彩票”提取（Adaptive Ticket Extraction）\n\n- **逐簇迭代 magnitude pruning**（算法 1）：\n1. 对子集  d_k  训练  T  步得到临时权重  θ_T^((k)) ；\n2. 按幅度剪去比例  p  的权重，生成  m_k ；\n3. 权重**回滚**到共享初始  θ_0 ，保留掩码；\n4. 循环直到所有  m_k  达到目标稀疏度  s 。\n- **效果**：每张“彩票”都在同一初始化基础上、仅针对对应数据分布被“刮开”，天然具备**参数共享+结构特化**双重属性。\n\n3\\. 联合重训练（Joint Retraining）\n\n- **同步小批量采样**：对大小不同的  D_k  进行循环补全，保证每 epoch 各子网络梯度更新次数相同。\n- **掩码梯度屏蔽**（公式 (7)）：\n\nθ arrow θ - eta · (∇_θ L(f(x;m_kodotθ),y)odot m_k)\n\n仅更新  m_k  保留的权重，避免子网络间互相覆盖或灾难性遗忘。\n- **结果**：在**单一张密集参数张量  θ ** 上同时精炼  K  条稀疏路径，实现**推理时按上下文（标签/环境）直接选掩码**，无需额外路由网络。\n\n4\\. 无标签诊断：子网络塌陷预警\n\n- 定义掩码 Jaccard 相似度\n\nJ(m_i,m_j)=(|m_i∩ m_j|) / (|m_i∪ m_j|)\n\n- 实验发现：当  J  随剪枝率急剧上升时，各子网络开始共享大量权重→**结构特化消失→性能骤降**（称之“子网络塌陷”）。\n- **用途**：无需标签即可通过监控  J  判断是否过度稀疏，指导早停或调整稀疏预算。\n\n5\\. 端到端总结\n\n1. **划分数据**→2. **逐簇剪+回滚**得  K  张掩码→3. **联合重训练**共享权重→4. **推理时按上下文选掩码**→5. **用掩码相似度监控塌陷**。\n\n通过上述流程，RTL 仅利用**剪枝+掩码路由**便让不同输入走不同稀疏路径，实现“结构即路由”的轻量级异构推理，在参数量少一个数量级的情况下持续超越单掩码/独立模型基线。\n\nQ4: 论文做了哪些实验？\n\n论文从“可控分类→语义聚类→单图语义→真实语音→诊断分析”五个层次递进验证 Routing the Lottery（RTL）的通用性与可解释性。具体实验设置与目的如下：\n\n1 CIFAR-10 类别专用子网络（理想划分）\n\n- **目的**：验证“每类一张彩票”在干净标签下的上限收益。\n- **协议**：10 类各对应 1 个子网络；与 (i) 单全局 IMP 掩码 (ii) 每类独立 IMP（无共享）对比；稀疏度 25 % / 50 % / 75 %。\n- **观测指标**：balanced accuracy / precision / recall / 剩余参数量。\n- **关键结论**：RTL 在所有稀疏度均取得最高 balanced accuracy 与 recall，参数仅为独立模型的 ≈ 1/10。\n\n2 CIFAR-100 语义聚类（噪声划分）\n\n- **目的**：测试 RTL 对“不完美、重叠”数据划分的鲁棒性。\n- **协议**：用 CLIP+UMAP+HDBSCAN 将 100 类聚成 8 个语义簇；其余同 Exp-1。\n- **结论**：即使簇边界模糊，RTL 仍全程领先；召回率优势尤为显著，说明特化对噪声划分仍有效。\n\n3 隐式神经表示（INR）——**单图内部语义区域**\n\n- **目的**：验证 RTL 可细化到“**一张图像内的不同语义区域**”。\n- **任务**：用坐标 MLP 将连续 (x,y) 映射到 RGB，重建 10 张 ADE20K 图像。\n- **划分**：利用语义分割掩码，每幅图按“对象类别”定义 K 个子集（4–15 类）。\n- **指标**：平均 PSNR；参数计数。\n- **结论**：25 %/50 %/75 % 稀疏下 RTL 分别领先单掩码 IMP ≈ +2.9 dB / +2.5 dB / +2.2 dB；可视化显示边界更清晰、颜色更稳定。\n\n4 真实环境语音增强（DNS + TAU 场景）\n\n- **目的**：检验 RTL 在非分类、真实噪声环境下的通用性。\n- **数据**：干净语音（DNS Challenge 2020）× 三种环境噪声（室内、户外、交通，TAU 2020）。\n- **划分**：按噪声场景定义 3 个子集；U-Net 型复数比值掩码网络。\n- **指标**：SI-SNRi；参数量。\n- **结论**：RTL 在所有稀疏度均获最高 SI-SNRi（25 % 时 +0.36 dB 以上），参数仅 32 K，远低于独立模型的 84 K；谱图可视化显示谐波结构保留更好。\n\n5 诊断性分析：子网络塌陷 & 语义对齐\n\n5.1 子网络塌陷\n\n- **方法**：跟踪 CIFAR-10/100 各子网络 balanced accuracy 与平均掩码 Jaccard 相似度随剪枝率变化。\n- **发现**：accuracy 骤降点与 Jaccard 急剧上升点高度重合→“结构重叠”可作为**无标签早停信号**。\n\n5.2 语义对齐\n\n- **方法**：计算 CIFAR-10 类间 WordNet 路径相似度，与深层/浅层掩码相似度做 Spearman 相关。\n- **发现**：\n– 浅层掩码普遍相似（共享低层特征），深层掩码相似度与语义距离正相关；\n– 相关强度随训练进程与稀疏度增加而提升→RTL 剪出的结构**自发编码语义层级**。\n\n总结\n\n实验覆盖**理想标签→模糊聚类→图像内部→真实声学→无标签诊断**，任务模态包括分类、坐标回归、语音增强；指标兼顾精度、参数量、可视化与相关性分析，系统证明 RTL 在性能、效率与可解释性上的优势。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 RTL 框架的自然延伸或深层追问，均直接对应论文已暴露的边界与现象，具备可验证的实验入口：\n\n1 动态路由 vs. 静态掩码\n\n- **问题**：RTL 目前依赖“上下文标签”做硬掩码选择，若环境或类别未知即失效。\n- **探索**：在共享 backbone 上训练一个**无参数开销的轻量门控**（如一次前向后的激活统计），实现**输入驱动的软路由**，比较与固定掩码的精度-延迟折中。\n\n2 稀疏度自动分配\n\n- **问题**：各子网络现用统一稀疏预算  s  ，但不同簇/环境可能天生复杂度不同。\n- **探索**：以掩码相似度上升斜率或验证损失为信号，**逐簇自适应调整目标稀疏度**；可建模为约束优化或强化学习，验证能否在总参数不变下进一步提升平均精度。\n\n3 跨任务迁移与增量簇\n\n- **问题**：现实场景常出现新类别/新噪声环境。\n- **探索**：\n– 保持旧掩码冻结，用**侧向添加的稀疏 Adapter** 学习新簇，检验是否出现“稀疏灾难性遗忘”；\n– 或采用**超网络生成掩码**，以簇嵌入为条件，实现连续增量学习而无需重训练全部子网络。\n\n4 子网络 collapse 的理论阈值\n\n- **问题**：实验观察到 Jaccard 拐点 ≈ 0.6 时性能骤降，但该值是否随架构、任务、优化器而变？\n- **探索**：在随机图与稀疏神经网络理论框架下，推导**保持子网络可区分性的最大允许重叠**；给出与层宽、深度、剪枝因子相关的解析或半解析阈值，为自动早停提供理论依据。\n\n5 与参数高效微调（PEFT）正交组合\n\n- **问题**：大模型时代更关注“如何少调参”。\n- **探索**：将 RTL 的掩码特化思想迁移到 LLM，**仅对每层部分神经元做簇专用剪枝**，其余权重保持预训练不变；对比 LoRA/AdaLoRA，看能否在下游多任务上同时获得稀疏与低秩的复合收益。\n\n6 非语义划分下的鲁棒性极限\n\n- **问题**：目前聚类仍部分依赖语义标签或 CLIP 嵌入。\n- **探索**：用**完全随机或对抗性划分**制造最难场景，测量 RTL 相对单掩码的增益衰减曲线；界定“数据划分质量→RTL 有效性”的临界条件，指导实际部署时的划分策略选择。\n\n7 硬件-感知掩码同步\n\n- **问题**：不同子网络导致不规则稀疏，难以在通用加速器上获得实测加速。\n- **探索**：在掩码提取阶段加入**结构化约束**（block/block-column 稀疏或 N:M 细粒度），检验是否仍能保持 specialization 优势；配合真实 GPU/NNIE 推理库测量端到端延迟与能耗，验证“理论参数少”→“实测速度快”的闭环。\n\n8 语义对齐的逆向利用\n\n- **问题**：深层掩码相似度与 WordNet 距离正相关。\n- **探索**：**反过来用掩码相似度指导零样本/少样本分类**——未见类别的掩码可通过语义近邻插值生成，检验其作为权重先验能否提升低样本场景下的泛化能力。\n\n这些方向既可直接复用 RTL 的代码框架，也能从理论、系统、应用三端填补“稀疏特化”尚存的空白，为后续研究提供可落地的实验入口。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：传统 Lottery Ticket 剪枝只用一张全局稀疏掩码，忽视数据异构，导致在类别、语义或环境差异大的场景下性能受限。\n- **方法**：提出 Routing the Lottery（RTL）——\n1. 将数据划分为 K 个子集（类别/聚类/环境）；\n2. 对每子集独立 magnitude-prune 并回滚到共享初始化，得到 K 张专用掩码；\n3. 联合重训练：在统一参数张量上按掩码屏蔽梯度，仅更新各子网络存活权重；\n4. 推理时按上下文选择对应掩码，实现“结构即路由”的轻量化推断。\n- **发现**：\n– 在 CIFAR-10/100、ADE20K-INR、真实语音增强任务中，RTL 以≈1/10 参数量持续优于单掩码与独立模型；\n– 提出“子网络塌陷”现象：掩码 Jaccard 相似度陡升可作为无标签早停信号；\n– 深层掩码相似度与 WordNet 语义距离正相关，表明剪枝自发编码语义层级。\n- **结论**：RTL 把剪枝从静态压缩重塑为对齐数据异构的动态机制，为构建模块化、可解释且高效的深度模型提供了新路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Routing%20the%20Lottery%3A%20Adaptive%20Subnetworks%20for%20Heterogeneous%20Data，链接是https://arxiv.org/pdf/2601.22141，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22141。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22159v1-1769990387967","title":"RedSage: A Cybersecurity Generalist LLM","description":"Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.","url":"https://arxiv.org/abs/2601.22159v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:57.000Z","metadata":{"arxivId":"2601.22159v1","authors":"Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani","categories":"cs.CR, cs.AI, cs.CL","published":"2026-01-29T18:59:57Z","pdfUrl":"https://arxiv.org/pdf/2601.22159v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22159v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22159v1","rank":1,"domain":"VLM","zh_summary":"网络安全运营需要支持多样化工作流程且不暴露敏感数据的辅助大型语言模型。现有解决方案要么依赖存在隐私风险的专有API，要么依赖缺乏域名适配的开放模型。为弥合这一差距，我们通过大规模网络过滤和手工收集高质量资源，策划了118亿个以网络安全为重点的持续预训练数据标记，涵盖2.86万份文档，涵盖框架、攻击性技术和安全工具。基于此，我们设计了一个代理增强流程，模拟专家工作流程，生成26.6万多回合网络安全样本，用于监督式微调。结合通用开源的大型语言模型数据，这些资源使得RedSage的培训成为可能，RedSage是一款开源、可本地部署的网络安全助手，具备域感知的预培训和后培训功能。为了严格评估这些模型，我们推出了RedSage-Bench，这是一个基准测试，包含3万条选择题和240条开放式问答题，涵盖网络安全知识、技能和工具专长。RedSage还会根据已建立的网络安全基准（如CTI-Bench、CyberMetric、SECURE）和通用大型语言模型基准进行进一步评估，以评估更广泛的概括性。在8B尺度上，RedSage持续取得更优成绩，在网络安全基准测试中领先基线模型最高+5.59分，在开放大型语言模型排行榜任务中高出+5.05分。这些发现表明，领域感知型代理增强以及前后培训不仅能提升网络安全专长，还有助于提升整体推理能力和指令遵循能力。所有模型、数据集和代码均公开。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. **现有网络安全大模型的训练与部署缺陷**\n\n- 专有模型依赖云端 API，存在敏感数据外泄风险；\n- 开源模型缺乏针对网络安全场景的充分预训练与后训练，导致领域知识薄弱；\n- 已有工作往往只关注单一训练阶段（仅预训练或仅监督微调），且公开数据规模有限，难以支撑本地化部署的通用型安全助手。\n2. **网络安全评测基准的覆盖不足**\n\n- 主流基准侧重知识问答，缺少对**工具操作能力**与**开放式问答质量**的系统性评估；\n- 缺乏同时覆盖知识、技能、工具三维能力的大规模高质量评测集。\n3. **数据与 pipeline 的封闭性**\n\n- 先前研究多未公开训练数据与代码，阻碍社区复现与进一步研究。\n\n为此，作者提出 **RedSage**：\n\n- 构建 **11.8 B token** 的网络安全持续预训练语料（CyberFineWeb + RedSage-Seed）；\n- 设计 **agentic 增强 pipeline**，将 28 K 权威文档转化为 **266 K 轮次对话**用于监督微调；\n- 推出 **RedSage-Bench**（30 K 多选题 + 240 开放式问答），系统评估知识、技能与工具 proficiency；\n- 训练得到 **8 B 参数可本地部署**的开源模型，在网络安全与通用基准上均取得 **+5 分以上** 的提升，并全面公开数据、模型与代码。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统梳理了相关研究，可归纳为两条主线：**网络安全评测基准**与**网络安全领域大模型/数据集**。\n\n1. 网络安全评测基准\n\n- **知识型基准**\n- SecEval（Li et al., 2023）：2 K 多选题，覆盖 9 类安全主题。\n- CyberMetric（Tihanyi et al., 2024）：10 K 多选题，基于 RAG 与专家验证。\n- CyberBench（Liu et al., 2024）：除问答外还包含 NER、摘要、分类任务。\n- SECURE（Bhusal et al., 2024）：面向工控系统，聚焦风险推理与漏洞分析。\n- CS-Eval（Yu et al., 2024）：42 子类，分知识-能力-应用三级认知维度。\n- SecBench（Jing et al., 2025）：44 K 多选题 + 3 K 简答题，中英双语。\n- **应用与智能体基准**\n- CTI-Bench（Alam et al., 2024）：威胁情报任务，含 CVE↔CWE 映射、CVSS 预测、攻击技战术识别。\n- CyberSecEval（Wan et al., 2024）：8 类风险场景，如漏洞利用生成、提示注入。\n- NYU-CTF（Shao et al., 2024）、CyBench（Zhang et al., 2025）：交互式夺旗赛，评估逆向、Web 利用等红队能力。\n- **共性局限**：多数基准仅覆盖知识或应用，**缺乏对安全工具操作与开放式问答质量的细粒度评估**。\n2. 网络安全领域大模型与数据集\n\n- **早期编码器模型**\n- CyBERT、SecureBERT、CTI-BERT：基于 BERT 的域内微调，任务特定，数据未公开。\n- **大模型时代的持续预训练/微调**\n- PRIMUS（Yu et al., 2025）：2.75 B token 预训练 + 835 条 SFT，基于 Llama-3.1-8B。\n- Foundation-Sec-8B（Kassianik et al., 2025）：5.1 B token 预训练，数据未开源。\n- DeepHat（2025）：宣称 1 M+ Q&A 微调，无预训练阶段。\n- Lily-Cybersecurity-7B（Sego Lily Labs, 2024）：22 K 手工对话微调。\n- Cyber-DAP（Salahuddin et al., 2025）：119 M token 小规模持续预训练。\n- SecGemini：谷歌闭源模型，实时威胁情报，数据与权重均未发布。\n- **共性局限**：\n- 仅聚焦单一训练阶段（预训练或 SFT），**未联合优化**；\n- 数据规模或质量有限，**无 agentic 增强**；\n- **未同时公开模型、数据与代码**，可复现性差。\n\nRedSage 与上述工作的区别\n\n- 首次将**大规模持续预训练（11.7 B token）+ 代理增强 SFT（266 K 对话）+ 偏好对齐**完整 pipeline 开源；\n- 提出同时覆盖**知识、技能、工具**且包含**开放式问答质量评分**的 RedSage-Bench；\n- 8 B 规模即可本地部署，在网络安全与通用基准上均取得新 SOTA。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“数据-训练-评测”三位一体的系统化方案，一次性补齐了领域预训练、高质量后训练与严格评测的缺口，具体路径如下：\n\n1\\. 构建 11.8 B token 网络安全持续预训练语料\n\n| 子语料 | 来源与处理 | 规模 | 关键特点 |\n| --- | --- | --- | --- |\n| CyberFineWeb | 在 FineWeb（17 T token）上用 ModernBERT 分类器过滤→全球 MinHash 去重→按时间片混合 30 % FineWeb-Edu 防止灾难性遗忘 | 13 M 文档 / 11.7 B token | 覆盖 2013-2024 公开 Web 安全内容，保持通用知识 |\n| RedSage-Seed | 人工精选 28 637 篇权威资源（MITRE、OWASP、Kali、CTF Write-up、tldr 等）→ Markdown 结构化 | 150 M token | 高质量、可溯源、三分类：Knowledge / Skills / Tools |\n| RedSage-Dump | 补充 NVD、RFC、NIST、安全新闻、教育门户等公开数据 | 700 M token | 增强漏洞、协议、合规等长尾知识 |\n\n2\\. Agentic 增强生成 266 K 多轮对话用于后训练\n\n- **Planner Agent**：对 Seed 数据分块→自动抽取技能集（如“漏洞分析”“命令生成”）并输出多样化增强策略（Q&A、角色扮演、步骤演练等）。\n- **Augmenter Agent**：按策略将每块种子实例化为**角色驱动、多轮、可执行**的对话，严格保持技术深度与事实一致。\n- **质量控制**：格式、一致性、相关性三重过滤；最终得到 266 K 对话（353 M token），样本扩大 9.2×，token 扩大 2.3×。\n\n3\\. 两阶段后训练\n\n1. **SFT**：RedSage-Conv + 通用指令集 SmolTalk2（非推理子集）→兼顾网络安全专精与广泛指令遵循。\n2. **DPO**：采用开源 Tulu3 偏好数据做直接偏好优化，提升回答有用性与安全性。\n\n4\\. 建立 RedSage-Bench 全面评测\n\n| 维度 | 题型 | 规模 | 构建方式 | 质量保障 |\n| --- | --- | --- | --- | --- |\n| 知识 | MCQ | 30 K | 用强 LLM 对 Seed 生成→双阶段 LLM 验证（结构+评分）→人工抽查 | 得分>8 且类别均衡 |\n| 技能 | Open-QA | 240 | Agentic 计划→QA 生成→双 LLM 交叉验证→人工终审 | 0-10 质量评分+事实正确性 |\n| 工具 | —— | —— | 同上 | 同上 |\n\n5\\. 训练与评测结果\n\n- **基座**：Qwen3-8B → 持续预训练 → RedSage-8B-Base\n- **指令版**：SFT → RedSage-8B-Ins；再加 DPO → RedSage-8B-DPO\n\n**效果**（8 B 模型）\n\n- 网络安全基准平均 +5.59（最高 84.6 % vs 79.0 %）\n- Open LLM Leaderboard 平均 +5.05（74.3 % vs 69.3 %）\n- 32 B 规模轻量 QLoRA 实验亦获一致提升，验证 pipeline 可扩展。\n\n6\\. 开源与可复现\n\n- 全部语料、对话、Benchmark、训练配置（Axolotl YAML）、评测脚本（LightEval）一次性公开；\n- 提供去污染与伦理使用协议，支持本地 GPU 部署，实现“隐私-安全-可控”的工业级安全助手。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“模型性能”** 与 **“方法可复现/可扩展”** 两条主线，共执行了 4 组实验，覆盖 20+ 公开基准、3 类模型规模、累计 50 余万个测试样本。\n\n1\\. RedSage-Bench 自评实验\n\n**目的**：验证领域定制预训练 + 代理增强 SFT 的绝对增益。\n**任务与指标**\n\n- MCQ（30 k题，0-shot）：宏观准确率 + 知识/技能/工具 三类细粒度准确率。\n- Open-QA（240题，0-shot）：LLM-as-Judge 打分（0–10）+ 事实正确率（T/F）。\n\n**对照组**\n\n- 通用基线：Llama-3.1-8B、Qwen3-8B\n- 安全专用基线：Foundation-Sec-8B、Llama-Primus、DeepHat-V1-7B 等\n\n**主要结果**\n\n- RedSage-8B-Ins 在 MCQ 宏观准确率上比最强 8 B 通用模型 Qwen3-8B 提升 **+3.88 pp**；DPO 版本保持 **+2.98 pp**。\n- Open-QA 平均正确率领先次优模型 **+7 pp**，质量分领先 **+0.07**（6.50 vs 6.43）。\n- 工具类题目最难（中位数质量分最低），RedSage 仍保持显著优势。\n\n2\\. 公开网络安全基准外推实验\n\n**目的**：检验领域知识能否泛化到外部评测。\n**基准列表（7 个）**\nCyberMetric-500、SecBench-En、MMLU-CSec、SECURE（MEAT/CWET/KCV）、CTI-Bench（MCQ+RCM）、SecEval\n\n**结果（表 5）**\n\n- 5-shot 基座阶段：RedSage-8B-Base 平均 **84.6 %**（+3.75 pp > Qwen3-8B-Base）。\n- 0-shot 指令阶段：RedSage-8B-Ins **81.3 %**（+5.59 pp > Qwen3-8B），接近 32 B 模型（82.3 %），距离 GPT-5 差 5 pp。\n\n3\\. 通用大模型 Leaderboard 实验\n\n**目的**：验证“领域强化”是否损害通用能力。\n**基准（7 项）**\nARC-C、HellaSwag、TruthfulQA、MMLU、WinoGrande、GSM8K、IFEval\n\n**结果（表 6）**\n\n- RedSage-8B-DPO 平均 **74.3 %**，领先所有 8 B 对照（最高基线 69.3 %），并超过 Qwen3-32B（73.2 %）。\n- GSM8K 数学推理 **86.1 %**（+8.4 pp > 最强 8 B 基线），说明 Seed 数据中的 CTF/Write-up 逻辑链同样提升通用推理。\n\n4\\. 规模扩展与消融实验\n\n4.1 32 B 轻量微调（QLoRA）\n\n- 仅用 RedSage-Seed + 50 % RedSage-Conv，1 % 可训练参数，4 小时训练。\n- RedSage-32B-LoRA-Ins 在 RedSage-MCQ 达 **87.5 %**（+2.0 pp > 全量 8 B 版本），外部安全基准平均 **82.9 %**，验证 pipeline 随模型容量线性受益。\n\n4.2 消融：预训练语料贡献\n\n- **CyberFineWeb-only** vs **Seed-only** vs **CFW+Seed**\n- Seed-only 在需要权威知识的 CTI-RCM、MMLU-CSec 上最高；\n- CFW-only 在 SecBench、CyberMetric 等 Web 知识型基准上最高；\n- 二者组合取得最佳宏观平均，证明互补性。\n\n4.3 消融：后训练策略\n\n- SFT → DPO 连续提升 IFEval（指令遵循）（79.9 → 83.4 pp），但 MCQ 准确率略降（85.7 → 84.8），显示偏好对齐以牺牲少量闭卷正确率为代价换取开放式可用性。\n\n实验总结\n\n- 在 20+ 基准、50 万级样本上，RedSage 8 B 模型**同时**实现：\n- 网络安全任务 **SOTA**（+5.6 pp）\n- 通用任务 **领先同规模模型**（+5.0 pp）\n- 代理增强数据 + 持续预训练的策略可**零修改**迁移到 32 B 模型，继续提升性能，证明方法的可扩展性与复现价值。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**数据、模型、评测、应用与安全**五大主题，并给出可落地的研究切入点。\n\n1\\. 数据层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 多语言/地域威胁情报 | 将 CyberFineWeb 过滤 pipeline 扩展到 Common Crawl 其他语言子集，构建「中文-阿拉伯语-俄语」平行安全语料。 | 捕捉非英语地下论坛/漏洞交易信息，提升全球威胁可见度。 |\n| 多模态安全数据 | 把 CVE 公告中的 PoC 视频、攻击流程图、恶意软件截图与文本对齐，形成图文交错预训练语料。 | 支持「图-文」混合问答，例如“根据这张 Wireshark 截图指出攻击阶段”。 |\n| 实时威胁增量更新 | 设计「时间窗口」持续预训练策略，每周注入新 CVE、新 ATT&CK Technique，避免模型“过期”。 | 解决当前静态语料无法反映 0-day 爆发的问题。 |\n\n2\\. 模型层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 工具调用能力 | 引入 ToolLLM 式 API 模板，把 Kali 工具封装成可调用函数，训练模型生成「工具链+参数」并实时执行。 | 实现真正的“红队助手”——输入目标 IP，输出 nmap → nuclei → metasploit 自动链。 |\n| 推理-行动循环 | 结合 ReAct / Reflexion 框架，让模型在 CTF 环境中自我验证 flag 结果，根据失败反馈迭代 payload。 | 突破当前仅文本生成、无环境反馈的局限，迈向自主渗透代理。 |\n| 小参数高效化 | 用 1-3 B 模型 + 动态 LoRA 合并，实现「手机端本地安全助手」；研究量化后是否仍能保持 exploit 语法精度。 | 满足现场应急、无网络场景下的便携需求。 |\n\n3\\. 评测层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 动态 CTF 基准 | 与 CTFd 平台对接，每月自动拉取新比赛题目，形成「活基准」；指标从准确率改为夺旗数/用时。 | 防止静态题目过拟合，提供可无限扩展的攻防评测。 |\n| 红-蓝对抗评分 | 建立「双模型对抗」协议：RedSage-Red vs RedSage-Blue，用 MITRE CALDERA 记录 ATT&CK 覆盖度，计算累积 TTP 成功数。 | 量化模型在真实杀伤链中的攻防贡献度。 |\n| 幻觉与一致性审计 | 引入「事实性链式追踪」数据集：对模型输出的每一条命令/漏洞描述给出可执行验证脚本，统计可复现率。 | 解决当前 LLM 安全建议“看似合理却不可执行”的痛点。 |\n\n4\\. 应用层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| SOAR 编排自动生成 | 将 SIEM 告警作为输入，模型输出 Splunk Phantom / Microsoft Sentinel Playbook JSON，直接导入执行。 | 缩短 SOC 平均响应时间（MTTR）。 |\n| 漏洞补丁联合生成 | 给定 CVE 描述 + 受影响函数，模型同时输出（1）补丁 diff（2）回归测试用例（3）CI 集成 YAML。 | 实现「一键修复」DevSecOps 工作流。 |\n| 安全培训个性化 | 利用对话历史评估学员薄弱点，动态生成「专属靶场」Docker Compose 场景与提示脚本。 | 提升企业安全培训效率，降低人力成本。 |\n\n5\\. 安全与伦理层面\n\n| 方向 | 可探索点 | 潜在价值 |\n| --- | --- | --- |\n| 可控生成护栏 | 在模型内部集成「双重控制码」：当请求包含 exploit、RAT、后门等关键字时，自动切换到「只给出防御/检测建议」模式。 | 降低双重用途风险，满足欧盟 AI Act 等监管要求。 |\n| 训练数据毒化检测 | 研究若攻击者在 Common Crawl 植入恶意网页并被 CyberFineWeb 吸收，如何触发异常行为；构建毒化样本检测器。 | 防止“上游投毒”导致模型输出有害 payload。 |\n| 可解释威胁推理 | 要求模型在给出攻击建议时，同步输出对应的 MITRE ATT&CK Technique ID 与置信度，并提供引用段落。 | 方便审计与责任追溯，增强安全员信任度。 |\n\n快速起步建议（3 个月可完成）\n\n1. 用现有 RedSage-8B-DPO + ToolLLM 模板，收集 5 K 条「工具调用-执行结果」对话，微调出 RedSage-Tools 原型。\n2. 在小型 CTF 平台（如 PicoMini）运行自动夺旗实验，记录 flag 率并与人类选手基线对比。\n3. 将动态评测结果开源，形成「RedSage-LiveBench」月度榜单，吸引社区提交新模型与新题。\n\n通过以上探索，可逐步把 RedSage 从“知识问答”升级为「可解释、可执行、可自我进化」的下一代网络安全智能体。\n\nQ6: 总结一下论文的主要内容\n\nRedSage：开源、本地化、8 B 参数的网络安全通用大模型\n——数据-训练-评测全流程总结\n\n1\\. 要解决的问题\n\n- 专有安全 LLM 依赖云端 API，敏感数据外泄风险高；\n- 开源模型缺乏大规模**域内预训练**+**高质量后训练**，安全知识不足；\n- 现有基准缺**工具操作**与**开放问答质量**维度，且数据/代码多未公开。\n\n2\\. 核心贡献\n\n| 内容 | 规模 | 亮点 |\n| --- | --- | --- |\n| CyberFineWeb | 11.7 B token | 2013-24 Common Crawl → ModernBERT 过滤 → 全局去重 → 30 % 通用知识回放 |\n| RedSage-Seed | 28 k 权威文档 / 150 M token | MITRE、OWASP、Kali、CTF Write-up 等，Markdown 结构化 |\n| Agentic 增强对话 | 266 k 多轮 / 353 M token | Planner+Augmenter 双智能体，把种子转为可执行、角色驱动对话 |\n| RedSage-Bench | 30 k MCQ + 240 开放问答 | 覆盖知识-技能-工具；双阶段 LLM 验证+人工终审，去污染 |\n| RedSage-8B 模型 | 8 B 参数 | Qwen3-Base → 持续预训练 → SFT → DPO；完全开源 |\n\n3\\. 实验结果（↑ vs 最强同规模模型）\n\n- **网络安全基准**平均 **+5.6 pp**（84.6 % vs 79.0 %）\n- **通用 LLM Leaderboard**平均 **+5.0 pp**（74.3 % vs 69.3 %）\n- **开放问答**正确率 **+7 pp**，质量分 **+0.07**\n- **32 B 轻量 QLoRA**继续提升，验证 pipeline 可扩展\n\n4\\. 可用性与影响\n\n- 8 B 规模 → 单卡 24 GB GPU 可部署，**本地私有**运行\n- 代码+数据+Benchmark 全部开放，支持复现与二次开发\n- 首次实现“**大规模域内预训练 + 代理增强后训练 + 三维评测**”一体化，为社区提供可落地的网络安全通用助手基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RedSage%3A%20A%20Cybersecurity%20Generalist%20LLM，链接是https://arxiv.org/pdf/2601.22159，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22159。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22158v1-1769990387967","title":"One-step Latent-free Image Generation with Pixel Mean Flows","description":"Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose \"pixel MeanFlow\" (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.","url":"https://arxiv.org/abs/2601.22158v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:56.000Z","metadata":{"arxivId":"2601.22158v1","authors":"Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He","categories":"cs.CV","published":"2026-01-29T18:59:56Z","pdfUrl":"https://arxiv.org/pdf/2601.22158v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22158v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22158v1","rank":2,"domain":"VLM","zh_summary":"现代用于图像生成的扩散/流模型通常具有两个核心特征：（i）使用多步采样，以及（ii）在潜在空间中操作。近期的进展在每个方面都取得了令人鼓舞的成果，为实现无需潜变量的一步扩散/流铺平了道路。在本研究中，我们在这一目标上更进一步，提出了“像素均流”(pixel MeanFlow, pMF)。我们的核心指导思想是将网络输出空间和损失空间分别进行建模。网络的目标被设计在假定的低维图像流形上（即 x 预测），而损失则通过速度空间中的均流（MeanFlow）来定义。我们在图像流形和平均速度场之间引入了一个简单的变换。在实验中，pMF 在 ImageNet 上实现了强有力的一步无潜生成结果，在 256x256 分辨率下 FID 为 2.22，在 512x512 分辨率下 FID 为 2.48，填补了该领域的一个关键空白。我们希望本研究能进一步推动基于扩散/流的生成模型的边界。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在实现**一步（one-step）、无潜空间（latent-free）**的图像生成，把“多步采样”和“潜空间建模”这两个传统扩散/流模型的核心组件同时去掉，从而用**端到端**的方式直接从噪声生成像素级图像。具体而言，它要解决以下关键难题：\n\n1. 单步生成对网络容量要求极高\n在一步内完成从噪声到像素的映射，网络必须同时刻画整条概率流轨迹，而不再依赖多步数值积分。\n\n2. 像素空间维度灾难\n去掉 VAE tokenizer 后，输入/输出空间变为全分辨率像素，维度远高于潜空间，直接回归速度场  u  或  v  会遭遇“高维噪声回归”困境。\n\n3. 预测目标与损失空间不匹配\n以往方法要么在潜空间做速度回归（如 iMF），要么在像素空间做去噪回归（如 JiT），但二者难以同时满足“一步”与“无潜空间”要求。\n\n为此，作者提出 **pixel MeanFlow (pMF)**，核心思想是：\n\n- **预测空间**选择低维流形上的“去噪图像” x （x-prediction），降低网络学习难度；\n- **损失空间**仍放在速度场  v （v-loss），利用 MeanFlow 恒等式  v = u + (t − r)dot u  进行**空间转换**，通过  x → u → v  链式关系完成训练；\n- 引入感知损失（LPIPS）直接作用于像素级  x ，进一步提升质量。\n\n实验表明，pMF 在 ImageNet 256² 上一步生成 FID 达到 2.22，512² 达到 2.48，首次在这一严苛设定下与基于潜空间或多步的方法形成有力竞争，填补了“一步+无潜空间”研究的空白。\n\nQ2: 有哪些相关研究？\n\n论文第 2 节与第 4.5 节系统梳理了相关研究，可归纳为四大脉络：\n\n1. 扩散 / 流匹配基础\n\n- DDPM、DDIM、Score-SDE 系列：Sohl-Dickstein 2015、Ho 2020、Song 2021a/b\n- Flow Matching：Lipman 2023、Liu 2023、Albergo & Vanden-Eijnden 2023\n2. 潜空间扩散 / 流（降低维度+感知损失）\n\n- LDM / StableDiffusion：Rombach 2022\n- DiT、SiT、RAE-DiT：Peebles & Xie 2023、Ma 2024、Zheng 2026\n3. 像素空间扩散 / 流（无 tokenizer）\n\n- 早期 U-Net 像素扩散：Ho 2020、Nichol & Dhariwal 2021\n- 近期像素 Transformer：JiT、PixelDiT、PixelFlow、DIP、Deco —— 它们仍依赖多步采样\n4. 一步 / 少步生成（减少 NFE）\n\n- 蒸馏路线：Progressive Distillation、SD-Distill、Meng 2023\n- Consistency 系列：CM、CTM、Shortcut、sCM —— 一步但多数在潜空间\n- MeanFlow / iMF：Geng 2025a/b —— 一步潜空间速度回归\n- 一步像素 GAN：BigGAN-deep、StyleGAN-XL、GigaGAN\n\npMF 与上述工作的区别：\n\n- 同时满足 **1-NFE + 像素空间 + 无蒸馏**；\n- 预测目标采用 **x-prediction**（低维流形），而非直接回归高维速度场；\n- 通过 MeanFlow 恒等式完成 **x→u→v** 空间转换，实现 v-loss 训练。\n\nQ3: 论文如何解决这个问题？\n\n论文把“一步、无潜空间”生成拆解为**预测空间**与**损失空间**的分离设计，通过一条“x→u→v”链式转换完成端到端训练，具体步骤如下：\n\n1. 重新定义预测目标——低维流形上的“去噪图像”x\n利用 MeanFlow 平均速度场  u(z_t,r,t)  构造\n\nx(z_t,r,t) triangleq z_t - t· u(z_t,r,t)\n\n- 当  r=t  时， x  退化为 JiT 的“去噪图像”，天然位于图像流形；\n- 当  r=0  时， x  等于 ODE 轨迹终点  z_0sim p_(data) ，同样位于流形；\n- 对一般  0<r<t ，可视化显示  x  仍是近似干净或轻微模糊的图像，维度远低于噪声速度场。\n2. 网络直接输出 x，再用闭式公式“升维”到速度场\n给定网络输出  x_θ(z_t,r,t) ，立即得到\n\nu_θ(z_t,r,t)=(z_t - x_θ(z_t,r,t)) / (t)\n\n无需额外参数即可把低维  x  映射回高维  u 。\n\n3. 利用 MeanFlow 恒等式得到瞬时速度  v \n采用 iMF 的 JVP 技巧：\n\nV_θ = u_θ + (t-r)·stop-gradient!(t(mathrm d) / (mathrm dt)u_θ)\n\n该  V_θ  与 ground-truth  v=ε-x  在相同空间，可直接用标准  ell_2  损失训练：\n\nmathcal L_(pMF)=mathbb E_(t,r,x,ε)|V_θ - (ε-x)|^2\n\n4. 像素级感知损失无缝接入\n由于  x_θ  已是像素图像，可在  tle t_(thr)  时加入 LPIPS：\n\nmathcal L=mathcal L_(pMF)+λmathcal L_(perc)(x_θ,x)\n\n进一步提升细节保真度。\n\n5. 训练与采样算法\n\n- 训练：一次前向即可得  x_θto u_θto V_θ ，与任意 CFG 兼容（见 Alg. 1/2）。\n- 采样：只需单步 Euler： x_0 = x_θ(z_1,r,1) ，实现真正 1-NFE 生成。\n\n通过“预测低维  x 、损失高维  v ”的解耦设计，pMF 既规避了高维速度回归的容量灾难，又保留了流匹配的理论框架，在 ImageNet 256²/512² 上一步生成取得 2.22/2.48 FID，首次让“一步+无潜空间”成为可行且竞争性的方案。\n\nQ4: 论文做了哪些实验？\n\n论文从\\*\\* toy 验证 → 消融分析 → 系统对标\\*\\* 三个层次展开实验，全部围绕“1-NFE、像素空间、无蒸馏”这一严苛设定。主要结果如下表所示（仅列关键指标，单位 FID↓）：\n\n| 实验目的 | 关键条件 | 主要结果 |\n| --- | --- | --- |\n| 1. toy 验证（图 2） | 2-D Swiss-roll 投影到 D∈{2,8,16,512} | x-pred 在 D=512 仍可靠，u-pred 崩溃 |\n| 2. 预测目标对比（表 2） | ImageNet 64² vs 256²，相同网络容量 | 64²：x/u 相当（≈3.8）；256²：u-pred 暴增至 164.89，x-pred 仅 9.56 |\n| 3. 优化器（图 3a） | Adam vs Muon，160 epoch | Muon 收敛快且最终 9.56 → 8.71（320 epoch） |\n| 4. 感知损失（图 3b） | MSE → +LPIPS(VGG) → +LPIPS(ConvNeXt-V2) | 9.56 → 5.62 → 3.53 |\n| 5. 预条件器对比（表 3a） | linear / EDM / sCM 三种 pre-cond | 最佳 pre-cond 仅 13.81，x-pred 无 pre-cond 达 3.53 |\n| 6. 时间采样范围（表 3b） | 仅 r=t / 仅 r=0 / 两线 / 全三角 0≤r≤t | 前三者 106–389 FID，全三角 3.53 |\n| 7. 高分辨率固定序列长（表 4） | 256²→512²→1024²，patch 16→32→64，序列长保持 162 | 256²/3.53 → 512²/4.06 → 1024²/4.58 |\n| 8. 模型与训练时长缩放（表 5） | B/16 → L/16 → H/16，160→320→360 epoch | 3.53 → 2.52 → 2.22（H/16-360 epoch） |\n| 9. 256² 系统对标（表 6） | 同场 1-NFE 像素方法 + 多步/潜空间参考 | pMF-H/16 2.22 FID，优于 EPG 8.82 及所有 1-NFE GAN |\n| 10. 512² 系统对标（表 7） | 同上，512² | pMF-H/32 2.48 FID，计算量仅 272 Gflops，低于 StyleGAN-XL 的 2061 Gflops |\n| 11. 定性样本（图 4–6） | ImageNet 256²，uncurated 多类生成 | 视觉质量与多步潜空间模型难分高下 |\n\n综上，实验不仅验证了“x-pred + v-loss”这一核心设计的必要性，还证明其在**高分辨率、大模型、长训练、感知损失、CFG**等维度均具备良好扩展性，最终把“一步、无潜空间、无蒸馏”生成推向与主流多步/潜空间模型可竞争的 2.x FID 区间。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向值得进一步探索，按“理论-模型-系统-应用”四个层面列出：\n\n理论层面\n\n- **流形维度定量分析**\n目前仅依赖可视化与 toy 实验验证  x  位于低维流形。可引入局部本征维度估计或奇异谱分析，定量刻画  x(z_t,r,t)  随  (r,t)  的变化轨迹，为 x-prediction 提供更强的理论保证。\n\n- **双时间面  (r,t)  的采样策略**\n论文采用 logit-normal 均匀混合，尚未探索最优采样密度。可借鉴最优传输或变分推理思路，学习“在哪段  (r,t)  区域投放更多样本”以最小化积分误差。\n\n- **误差传播与 JVP 近似**\npMF 依赖 JVP 估计  (mathrm d) / (mathrm dt)u_θ ，其数值误差会随  (t-r)  放大。可研究高阶或伴随敏感度方法，降低单步近似的偏差。\n\n模型层面\n\n- **零阶保持 vs 高阶 ODE 扩展**\n当前仅做 1-NFE 零阶采样。可将 pMF 作为高阶 solver（Heun、DPMSolver）的初始值预测器，研究“用 pMF 做一步初始化 + 1–2 步修正”的混合方案，在 FID-速度帕累托前沿上取得更细粒度控制。\n\n- **动态 patch 尺寸 / 多分辨率训练**\n固定 patch 导致高分辨率时单 token 维度爆炸（1024² 达 12 288）。可引入可变形 patch 或傅立叶特征，让网络在不同区域自适应选择 token 粒度，兼顾全局结构与细节。\n\n- **条件注入方式升级**\n目前仅用类别向量 + CFG。可探索文本、图像-文本、布局、语义分割等更丰富的条件模态，并研究跨模态注意力与自适应归一化的最佳实践。\n\n- **生成-判别协同训练**\n虽然 pMF 无需对抗损失，但可引入轻量级判别器或扩散-判别混合目标，进一步压低 FID 并提升纹理高频细节。\n\n系统层面\n\n- **显存与延迟优化**\n大 patch 导致 token 长度固定为 162，但单 token 维度高达 12k，线性层权重成为显存瓶颈。可研究张量并行、激活检查点、权重量化（INT8/INT4）以及专为高维 token 设计的稀疏注意力。\n\n- **端侧部署与编译优化**\n1-NFE 模型天然适合移动端。可结合静态 shape 编译、operator fusion 与 NPU 指令集，把 pMF-H/32 压缩到 100 ms 以内 512² 出图，实现“手机端一步扩散”。\n\n- **与 latent decoder 开销对比再深化**\n论文已指出 SD-VAE 解码器 512² 需 1.2 TFlops。可进一步测量端到端能耗、DRAM 带宽占用，建立“无潜空间”在碳排放与推理成本上的量化优势。\n\n应用层面\n\n- **视频/3D/音频生成**\npMF 的 x-prediction 思想可迁移到时空立方体或神经辐射场：让网络直接输出低维“干净”体素或特征场，再通过微分恒等式映射到高维速度，实现一步视频帧或 3D 模型生成。\n\n- **可编辑与可控生成**\n利用 x 的“去噪图像”特性，可无缝接入图像编辑损失（如 CLIP-direction、StyleCLIP），实现单步文本驱动编辑；也可研究在 x 空间进行空间插值、属性解耦。\n\n- **数据-高效与零样本泛化**\n探索在极少量领域数据（如医学、卫星图像）上微调 pMF 的方案，验证 x-prediction 是否因低维流形假设而具备更强泛化与抗过拟合能力。\n\n一句话总结\n\npMF 把“预测目标”与“损失空间”解耦的思路不仅适用于一步像素图像，也可作为“高维观测-低维流形”生成问题的通用范式，等待在理论、算法、系统、跨模态等方向继续开花结果。\n\nQ6: 总结一下论文的主要内容\n\n**pixel MeanFlow (pMF)** 提出一种**一步、无潜空间、无蒸馏**的图像生成范式，核心是把网络“预测空间”与“损失空间”彻底解耦：\n\n1. 预测目标：低维流形上的“去噪图像”\n\nx(z_t,r,t)=z_t - t· u(z_t,r,t)\n\n网络直接输出像素级  x_θ ，降低高维回归难度。\n\n2. 损失空间：速度场 v-loss\n通过闭式链  x_θto u_θto v_θ  与 MeanFlow 恒等式\n\nv_θ=u_θ+(t-r),stop-gradient(dot u_θ)\n\n仍对瞬时速度  v  做  ell_2  回归，保持流匹配理论正确性。\n\n3. 像素感知损失即插即用\n x_θ  已是图像，可直接加 LPIPS，一步训练无需额外判别器。\n\n4. 实验结果\n\n- ImageNet 256²：1-NFE FID 2.22\n- ImageNet 512²：1-NFE FID 2.48\n首次让“一步+无潜空间”生成进入 2.x FID 俱乐部，与多步/潜空间模型竞争。\n5. 贡献总结\n\n- 理论：给出 x-prediction 在双时间面  (r,t)  上的流形论证\n- 算法：x→u→v 链式转换，兼顾可学习性与正确性\n- 系统：纯像素、1-NFE、低算力（272 Gflops@512²），无需 VAE 解码器开销\n- 代码：已开源（JAX/TPU 实现）\n\npMF 证明神经网络可在高维像素空间端到端完成一步生成，为“直接噪声→像素”这一终极映射提供了可行且高性能的解决方案。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是One-step%20Latent-free%20Image%20Generation%20with%20Pixel%20Mean%20Flows，链接是https://arxiv.org/pdf/2601.22158，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22158。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22156v1-1769990387967","title":"Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts","description":"Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data","url":"https://arxiv.org/abs/2601.22156v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:53.000Z","metadata":{"arxivId":"2601.22156v1","authors":"Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu","categories":"cs.CL, cs.AI, cs.LG","published":"2026-01-29T18:59:53Z","pdfUrl":"https://arxiv.org/pdf/2601.22156v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22156v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22156v1","rank":3,"domain":"VLM","zh_summary":"混合 Transformer 架构将 softmax 注意力模块与循环神经网络 (RNN) 结合，对于长上下文建模表现出理想的性能与吞吐量折衷，但由于从零进行大规模预训练的成本过高，其应用和研究受到限制。一些近期研究表明，预训练的 softmax 注意力模块可以通过参数传递和知识蒸馏转换为 RNN 模块。然而，这些转换方法需要大量训练数据（超过 100 亿个 token），且得到的混合模型在长上下文下的表现也较差，而混合模型在这种场景下本可以相较于基于 Transformer 的模型获得显著的推理速度提升。在本文中，我们提出了 HALO（Hybrid Attention via Layer Optimization，基于层优化的混合注意力），一个将 Transformer 模型蒸馏为 RNN-注意力混合模型的流程。随后，我们提出了 HypeNet，一种通过新颖的位置编码方案（命名为 HyPE）和各种架构改进实现出色长度泛化能力的混合架构。我们使用 HALO 将 Qwen3 系列转换为 HypeNet，实现了与原始 Transformer 模型相当的性能，同时在长上下文的表现和效率上更优。该转换仅需 23 亿个 tokens，不到其预训练数据量的 0.01%","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决以下核心问题：\n\n1. 现有混合 Transformer-RNN 架构的“高门槛”预训练成本\n混合模型（交错 softmax attention 与线性 RNN 层）在长上下文场景下兼具性能与吞吐优势，但通常需从头预训练数百亿级 tokens，对学术团队极不友好。\n\n2. 已有蒸馏方案的两大瓶颈\n\n- 数据量依旧巨大：先前将预训练 Transformer 蒸馏为混合架构的方法仍需 ≥10 B tokens。\n- 长上下文 recall 严重退化：蒸馏后的混合模型在 128 K 以上长度任务上性能骤降，丧失混合架构本应具备的推理效率优势。\n3. 目标\n提出一种**极低成本**（<3 B tokens）的跨架构蒸馏流程，使预训练 Transformer 可直接转换为**长上下文表现更强、推理更高效**的混合模型，而无需从头预训练。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为三类，均围绕“如何把预训练 Transformer 转化为线性复杂度的序列建模结构”展开：\n\n1. 混合架构（Hybrid Attention-RNN）\n\n- Jamba (Lieber et al., 2024)\n- MiniMax-01 (MiniMax et al., 2025)\n- Qwen3-Next (Qwen, 2025)\n- Kimi-Linear (Kimi et al., 2025)\n- NVIDIA Nemotron 3 (NVIDIA et al., 2025)\n共同点：从头预训练百亿级参数，attention 层与 RNN/SSM 层交错；未提供低成本转换方案。\n2. 蒸馏式“Attention → Hybrid”方法\n\n| 方法 | 发布 | 训练 tokens | 关键指标 |\n| --- | --- | --- | --- |\n| Mamba-in-the-Llama (Wang et al., 2025b) | 2024-08 | 20 B | 均匀替换 attention 层，长程 recall 下降明显 |\n| SMART (Yang et al., 2026) | 2025-05 | >7 B | 基于输出分布漂移挑选保留层 |\n| RAD (Hoshino et al., 2025) | 2025-05 | 20 B | 提出冗余度度量，仍需大量数据 |\n| Jet-Nemotron (Gu et al., 2025) | 2025-08 | 400 B | 任务性能下降指导层选择，成本最高 |\n| KL-LS (Li et al., 2025) | 2025-12 | 25 B | 逐层重跑蒸馏，用 KL 散度定重要性 |\n\n3. 纯 RNN/线性注意力蒸馏\n\n- RADLADS (Goldstein et al., 2025) — 本文 Stage-1 的对齐策略即在其基础上改进。\n- LoLCATs (Zhang et al., 2025)、Bick et al. (2025) — 将 attention 整体替换为线性形式，不涉及混合。\n\n上述工作或为“高成本预训练”，或为“十亿级 token 蒸馏且长上下文性能退化”。本文 HALO 首次把训练数据压缩到 2.3 B tokens 以内，同时通过 HyPE 与 Lightning Attention 等改进显著提升了 128 K–1 M 长度的 recall。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **HALO（Hybrid Attention via Layer Optimization）** 流水线与 **HypeNet** 架构，从“蒸馏效率”与“长程泛化”两条路径同时突破：\n\n1. 极低成本蒸馏流水线（HALO）\n\n- **Stage-0 权重热启动**：直接把 Teacher 的  W_q,W_k,W_v,W_o  填入线性 RNN 的对应投影，省去随机初始化。\n- **Stage-1 隐状态对齐**：仅训练 RNN 层，最小化\n\nmathcal L_(stage1)^((l))=MSE!(Y^((l))_(teacher),; RNN^((l))(X^((l-1))))\n\n用 320 M tokens 即可得到“逐层替身”。\n- **Stage-1.5 重要性评分**：一次性推理得到每层被替换后的\n\ns_i=max_j(R_(orig)-R^((i))_j)max_j(C_(orig)-C^((i))_j)+varepsilon\n\n按  s_i  排序保留 top-k（k=⌊L/4⌋）attention 层，无需反复重训。\n- **Stage-2 端到端蒸馏**：固定 Teacher，用 1 B tokens 做 KL 蒸馏\n\nmathcal L_(stage2)=D_(KL)!(f_(orig)parallel f_(hybrid))\n\n- **Stage-3 长上下文微调**：再用 1 B tokens、16 K 长度、低 LR 微调，总开销 2.3 B tokens。\n2. 面向长程泛化的 HypeNet 架构\n\n- **HyPE 位置编码**\n– Attention 层用 **NoPE**，保证无训练长度外推；\n– RNN 层用 **RoPE**，弥补局部位置信息；\n– 推理时对 attention logits 做**动态缩放**\n\ntilde s_t=log_a(t+a),quad a∈500,600,900\n\n抑制长序列熵增导致的注意力分散。\n- **四项结构微调**\n➊ RNN 也加 QK-Norm；➋ GQA→MHA 解除 KV 头共享；➌ 给 RNN/Attention 都加输出门；➍ 去掉短卷积以减少 CUDA kernel 开销。\n- **通用 RNN Mixer 插槽**\n只要满足\n\nS_t=F_t S_(t-1)+k_t^top v_t,quad y_t=q_t S_t W_o\n\n即可接入；实验表明 **Lightning Attention**（数据无关 forget gate）在长度泛化与吞吐间取得最佳平衡。\n3. 结果\n把 Qwen3-1.7B/4B/8B 用 HALO 转成 HypeNet 后，\n\n- 128 K 上下文 NIAH 平均准确率 >90 %，而 Jet-Nemotron、KL-LS 同期方法在 128 K 已跌至 <30 %；\n- 512 K 长度下解码速度提升 3.0×，预填充 3.4×，且显存占用随序列长度线性增长，Teacher 在 1 M 长度即 OOM。\n\n综上，论文通过“分层重要性筛选 + 轻量对齐 + HyPE 缩放”将蒸馏数据压缩两个数量级，并首次使蒸馏型混合模型在长上下文召回任务上超越原始 Transformer。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 6 组实验，覆盖「主结果—消融—效率」三个层次，全部在 1×A800-80 GB 上完成，代码与 checkpoint 已开源。\n\n1. 主实验：Qwen3 → HypeNet 蒸馏\n\n- 模型规模：1.7 B → 2 B、4 B → 5 B、8 B → 9 B\n- 训练数据：FineWeb-Edu 共 2.3 B tokens（Stage-1 320 M + Stage-2 1 B + Stage-3 1 B）\n- 评测任务\n– CSR：ARC-E/C、HellaSwag、WinoGrande、PIQA、LAMBADA、MMLU 共 7 项零样本任务，取归一化准确率平均值。\n– 长程 recall：RULER 的 NIAH-Single-1/2/3，长度 32 K–256 K；额外测到 1 M。\n- 对比基线\n– 原始 Qwen3（Teacher，128 K 用 YaRN 外推）\n– 同期蒸馏混合模型：Jet-Nemotron-2 B（400 B tokens）、KL-LS-GDN-3 B（25 B tokens）\n- 关键结果\n– 128 K 平均 NIAH 90 %↑，而 Jet-Nemotron 与 KL-LS 分别跌至 0 % 与 14 %。\n– CSR 平均仅比 Teacher 低 1–2 分，但 128 K 上下文吞吐提升 2.4–3.0×，显存节省 40 % 以上。\n2. 从头训练消融：验证 HyPE 与 Mixer 选型\n\n- 500 M 参数、20 B tokens、4 K 上下文训练，测 512–256 K 外推。\na) 位置编码对比\n– 全 RoPE、全 NoPE、Attn-RoPE+RNN-NoPE、SWAN-GPT、HyPE\n– 结果：HyPE 在 64× 训练长度仍保持 93.5 % NIAH，显著优于仅 RoPE（20 %）与仅 NoPE（60 %）。\nb) RNN Mixer 对比\n– Lightning、Mamba2、GLA、GDN、RWKV-7\n– Lightning 在 128 K 取得 95 % NIAH，同时预填充速度比 Mamba2 快 3.6×。\n3. HALO 架构消融：逐组件剔除\n在 Qwen3-1.7 B→2 B 的同一 Teacher 上，保留其他条件不变，仅移除下列模块：\n\n- ➊ RNN-RoPE → 128 K NIAH 从 79.9 % 降至 47.9 %\n- ➋ RNN-QK-Norm → 128 K 降至 17.3 %\n- ➌ GQA→MHA → 128 K 略升 83.5 %，但 256 K 从 74.3 % 降至 58 %\n- ➍ 输出门（RNN 或 Attention）（-1.5～-5.4 分不等）\n证明四项改进均对长程 recall 有正向作用。\n4. HALO 层选择策略对比\n同一 HALO 流程，仅替换“保留哪些 attention 层”的算法：\n\n- 均匀分布、仅后半均匀、Jet-Nemotron 任务下降法、KL-LS 逐层重蒸馏法\n- 结果：本文提出的 recall/CSR 比值评分法在 8 K–256 K NIAH 上平均领先 8–15 分，且零额外训练成本。\n5. 训练超参消融\na) Stage-1 数据量：320 M 已饱和，再增到 1.3 B 反而过拟合。\nb) Stage-2 峰值 LR：1e-4 最佳；RADLADS 推荐的常数 1e-5 导致 128 K NIAH 掉到 60 %。\n\n6. 效率实测\n\n- 测解码延迟（TPOT）与预填充时间，batch=1、BF16、Flash-Attention-2 / Triton kernel。\n- 512 K 上下文：HypeNet-1.7 B 解码 3.0×，预填充 3.4×；Qwen3 在 1 M 长度 OOM，HypeNet 仍可用 73 GB。\n- 各 Mixer 预填充对比：Lightning 在 256 K 单 token 时延 6 ms，与 SWA-512 持平，比 Mamba2 快 4× 以上。\n\n综上，实验从「可比性能+更低成本」「更长外推」「更快推理」三个维度系统验证了 HALO 与 HypeNet 的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下问题在论文中尚未完全解决，可作为后续工作的直接延伸：\n\n1. 指令遵循与对齐恢复\nHALO 仅用 FineWeb-Edu 预训练语料，导致原模型的指令跟随、对话安全等后训练能力被稀释。\n\n- 如何设计「低成本、数据高效」的对齐恢复阶段（例如 LoRA+RLHF、DPO）？\n- 是否需要保留部分原始对话数据或引入合成指令数据？\n2. 层选择与压缩极限\n目前固定保留 25 % attention 层；若继续降低至 15 % 甚至 5 %，recall 性能下降曲线如何？\n\n- 能否用可微结构搜索（NAS）一次性联合优化「层类型 + 层位置 + 头数」？\n- 动态/自适应深度：推理时依据输入长度自动决定启用多少 attention 层。\n3. 长度外推机制再深化\nHyPE 的 logits 缩放仅使用一个全局超参 a。\n\n- 能否让缩放系数 st 与内容或头维度相关，实现「头相关、token 相关」的自适应缩放？\n- 进一步结合相对位置惩罚（ALiBi、xPos）或频域校正，实现 2 M+ 长度无损外推。\n4. 多模态与工具调用\n本文仅验证文本模态。\n\n- 视觉/语音交叉注意力层是否同样适用 HALO 的「重要性评分」筛选？\n- 工具调用、代码解释器等需要强召回的场景，RNN 层是否会出现「关键 API 遗忘」？\n5. 异构硬件与量化协同\nHypeNet 的 KV-cache 已大幅缩减，但新增输出门与 MHA 带来 10 % 参数量上升。\n\n- 与 4-bit/2-bit 量化、KV-cache 4-bit 压缩、Flash-Linear Attention 专用 CUDA kernel 联合优化，能否在边缘 GPU 实现 1 M 上下文 30 tokens/s？\n- 探索「遗忘门常量化」后是否可直接用 INT8 矩阵乘法，进一步提速。\n6. 持续学习与领域适应\n蒸馏后模型继续在新领域增量预训练时，RNN 层与 attention 层的「可塑性」差异显著。\n\n- 如何设计不同的学习率或正则策略，防止 RNN 层过早遗忘旧知识？\n- 能否用 expansion/compression 框架动态增长或剪枝 RNN 隐状态维度？\n7. 理论分析\n\n- 对 HyPE 给出长度泛化的严格证明：NoPE attention 的误差界与 RNN 的 receptive field 如何耦合？\n- Lightning Attention 的数据无关 forget gate 与数据相关 gate 的「长度外推」能力差异，能否用谱范数或梯度流解释？\n8. 非 Transformer 教师模型\n目前 HALO 假设教师为纯 Transformer。\n\n- 若教师本身已是混合结构（如 Jamba），如何递归蒸馏「混合 → 更稀疏混合」？\n- 对于 MoE、RetNet、TTT 等不同范式，HALO 的「重要性评分」是否仍然有效？\n9. 自动超参搜索\n缩放底数 a、峰值 LR、层比例 k 目前靠网格搜索。\n\n- 用贝叶斯优化或强化学习把「a + LR + k」联合搜索，目标函数为 Pareto（性能, 吞吐, 显存），能否再节省 30 % 搜索时间？\n10. 绿色 AI 评估\n给出端到端碳排与美元成本：2.3 B tokens × A800 GPU hours = ? kg CO₂，与从头预训练相比减少几个数量级？\n建立「每美元准确率」「每克碳 recall」指标，推动社区采用蒸馏而非重训。\n\n这些方向既有理论价值（长度泛化、重要性度量），也能直接落地（指令恢复、硬件量化、多模态），可作为后续研究的切入点。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：混合 Transformer-RNN 模型需百亿级从头预训练，现有蒸馏法仍耗 10 B+ tokens 且长上下文 recall 暴跌。\n- **方法**：提出 **HALO** 流水线（2.3 B tokens）（权重热启动→隐状态对齐→基于 recall/CSR 重要性选层→KL 蒸馏→长上下文微调），并设计 **HypeNet** 架构：\n– **HyPE**：Attention 层用 NoPE 保长度外推，RNN 层用 RoPE 补位置信息，加动态 logits 缩放。\n– 四项结构微调：RNN 加 QK-Norm、GQA→MHA、输出门、去短卷积。\n– 通用 RNN Mixer 插槽，Lightning Attention 表现最佳。\n- **实验**：Qwen3 1.7/4/8 B→2/5/9 B HypeNet，128 K 上下文 NIAH>90 %（同期方法<30 %），CSR 与 Teacher 持平，512 K 解码 3.0×、预填充 3.4× 提速，1 M 长度 Teacher OOM 而 HypeNet 仍可行。\n- **结论**：首次实现“极低数据成本蒸馏 + 卓越长程性能”的混合模型，为高效长上下文 LLM 提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Hybrid%20Linear%20Attention%20Done%20Right%3A%20Efficient%20Distillation%20and%20Effective%20Architectures%20for%20Extremely%20Long%20Contexts，链接是https://arxiv.org/pdf/2601.22156，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22156。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22155v1-1769990387967","title":"UEval: A Benchmark for Unified Multimodal Generation","description":"We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.","url":"https://arxiv.org/abs/2601.22155v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22155v1","authors":"Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu","categories":"cs.CV, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22155v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22155v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22155v1","rank":4,"domain":"VLM","zh_summary":"我们介绍了 UEval，这是一个用于评估统一模型的基准，即能够生成图像和文本的模型。UEval 包含 1,000 个由专家策划的问题，这些问题要求模型输出同时包含图像和文本，来源于 8 个现实任务。我们策划的问题覆盖了广泛的推理类型，从逐步指南到教材说明。评估开放式多模态生成并非易事，因为简单的“使用大型语言模型作为评判者”的方法可能会忽略细微差别。不同于以往依赖多模态大型语言模型（MLLM）来评估图像质量或文本准确性的工作，我们在 UEval 中设计了基于评分标准的评分系统。对于每个问题，提供参考图像和文本答案给 MLLM 生成初始评分标准，该标准包含多个评估标准，然后由人工专家进行完善和验证。总的来说，UEval 包含 10,417 个经过验证的评分标准，实现了可扩展且细粒度的自动评分。UEval 对当前的统一模型具有挑战性：GPT-5-Thinking 的得分仅为 100 分中 66.4 分，而表现最好的开源模型仅达到 49.1 分。我们观察到，推理模型通常优于非推理模型，将推理模型的推理轨迹转移到非推理模型上会显著缩小差距。这表明，推理能力可能对于需要复杂多模态理解和生成的任务非常重要。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在填补“统一多模态生成”评测的空白。现有基准主要将视觉任务割裂为两类独立范式：\n\n- 视觉问答（VQA）：输入图像→输出文本\n- 文生图（T2I）：输入文本→输出图像\n\n二者均未考察**同一查询需同时生成图像与文本**的真实场景，例如“请画出示意图并解释潮汐成因”。为此，作者提出 UEval 基准，核心贡献与目标可归纳为：\n\n1. 构建评测集\n\n- 1 000 条专家筛选问题，覆盖 8 类真实任务（空间、教科书、图表、论文、艺术、生活、技术、运动）。\n- 每条样本要求模型**联合输出图像+文本**，并附带人工校验的参考答案。\n2. 设计细粒度评分协议\n\n- 采用“数据相关”的 rubric 机制：针对每题自动生成十余条评分细则（共 10 417 条），由人类二次校准。\n- 用前沿 MLLM 作评委，按细则逐项打分，实现可扩展、可复现的自动评估。\n3. 验证基准难度与洞察\n\n- 闭源最佳模型 GPT-5-Thinking 仅得 66.4/100，开源最佳仅 49.1，显示任务极具挑战性。\n- 发现“推理型”模型显著优于非推理型，且将推理轨迹迁移给非推理模型可缩小差距，提示**思维链机制对多模态生成同样关键**。\n\n综上，论文首次系统评估了**统一模型在“同时生成图像与文本”上的推理与生成能力**，并揭示当前模型在跨模态一致性、多步规划等方面仍存在显著不足。\n\nQ2: 有哪些相关研究？\n\n与 UEval 直接相关的研究可划分为三条主线：\n\n1. 统一多模态理解与生成模型；\n2. 多模态评测基准；\n3. 细粒度/数据相关评分方法。\n\n1\\. 统一多模态模型\n\n- **Transfusion** (Zhou et al., 2025a) —— 同一 Transformer 内自回归预测文本、扩散生成图像。\n- **Emu3/Emu3.5** (Wang et al., 2024; Cui et al., 2025) —— 纯下一 token 预测实现文本-图像双向生成。\n- **DreamLLM** (Dong et al., 2024) —— 协同训练 LLM 与扩散解码器，支持交错图文生成。\n- **Janus/Janus-Pro** (Wu et al., 2025a; Chen et al., 2025d) —— 解耦视觉编码，统一自回归框架。\n- **Show-o2** (Xie et al., 2025) —— 离散扩散+自回归混合，支持图文交错输出。\n- **BAGEL** (Deng et al., 2025) —— 连续/离散潜变量统一训练，兼顾理解与生成。\n\n2\\. 多模态评测基准\n\n- **VQA 系列**\n- VQA-v2 (Goyal et al., 2017)\n- OK-VQA (Marino et al., 2019)\n- MMBench (Liu et al., 2024b)\n- MMMU (Yue et al., 2024)\n- BLINK (Fu et al., 2024) —— 侧重视觉感知细节。\n- **文生图评测**\n- T2I-CompBench (Huang et al., 2023) —— 组合属性一致性。\n- Geneval (Ghosh et al., 2023) —— 对象级对齐。\n- ScienceT2I (Li et al., 2025a) —— 科学概念可视化。\n- **交错/统一生成评测**\n- OpenLEAF (An et al., 2024) —— 开放域交错图文生成。\n- MMIE (Xia et al., 2025) —— 大规模交错理解基准。\n- RealUnify (Shi et al., 2025b) —— 统一模型综合探针。\n- Uni-MMMU (Zou et al., 2025) —— 多学科统一评测。\n\n3\\. 细粒度或数据相关评分\n\n- **HealthBench** (Arora et al., 2025) —— 针对医疗问答自动生成 rubric，人类二次校验；UEval 的 rubric 流程受此启发。\n- **VDC** (Chai et al., 2025) —— 引入样本相关评分提示，缓解通用 prompt 带来的偏差。\n\n上述工作为 UEval 提供了模型架构、评测维度与自动化评分三方面的理论与方法基础。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 设计可扩展评分协议 + 大规模实验诊断”三步解决统一多模态生成缺乏标准评测的问题。\n\n1. 构建 UEval 基准\n\n- 任务层面：精选 8 类真实场景（空间、教科书、图表、论文、艺术、生活、技术、运动），共 1 000 题；每题必须同时输出图像与文本。\n- 数据层面：人工撰写参考文本+检索/绘制参考图像，形成高质量答案对。\n- 划分类型：\n– 闭式任务（空间/教科书/图表/论文）——答案客观、明确；\n– 开式任务（艺术/生活/技术/运动）——允许多种合理步骤图。\n2. 设计数据相关 Rubric 评分协议\n\n- 自动初稿：用 Gemini-2.5-Pro 针对“问题+参考答案”生成 10 余条细粒度准则（图像/文本约各占一半）。\n- 人工精修：两轮审核去重、补漏，最终得到 10 417 条无歧义准则。\n- 自动评分：再以 Gemini-2.5-Pro 作评委，逐条判定模型响应是否满足准则，计算满足率作为最终得分；与人评 Pearson r = 0.88，≈90% 准则一致。\n3. 大规模实验与诊断\n\n- 评测 9 个最新统一模型（5 开源 + 4 闭源），给出 8 任务排行榜，揭示：\n– 闭源最佳 GPT-5-Thinking 仅 66.4/100，开源最佳 Emu3.5 仅 49.1，基准足够困难。\n– 多步规划类任务（艺术、生活等）得分显著低于知识类任务；模型常出现步骤图序号错乱、视觉一致性差。\n– 推理模型全面优于非推理模型；把 GPT-5-Thinking 的 Chain-of-Thought 轨迹拼到非推理模型 prompt 后，其图像准确率明显提升（对弱模型无效），证实“推理”对跨模态生成的重要性。\n\n通过以上三步，论文首次实现了对“统一模型同时生成图像+文本”能力的标准化、可复现且细粒度的评测，并量化揭示了现有模型的短板与改进方向。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「统一多模态生成」共开展 4 组实验，覆盖模型性能、评分可靠性、裁判模型差异与推理迁移效果。\n\n1. 主评测实验（UEval 排行榜）\n\n- 被测模型：9 个最新统一模型\n– 开源：Janus-Pro、Show-o2、MMaDA、BAGEL、Emu3.5\n– 闭源：Gemini-2.0-Flash、Gemini-2.5-Flash、GPT-5-Instant、GPT-5-Thinking\n- 指标：以 Gemini-2.5-Pro 为评委，按 10 417 条 rubric 计算「满足率」。\n- 结果：\n– 闭源全面领先，GPT-5-Thinking 平均 66.4，开源最佳 Emu3.5 仅 49.1。\n– 多步规划任务（art/life/tech/exercise）得分远低于知识任务（space/textbook/diagram/paper）。\n– 图像得分普遍低于文本得分；开源模型图像平均 <15，文本亦低于纯文本模型。\n2. 人类一致性验证\n\n- 随机抽取 10 % GPT-5-Thinking 输出，由人工独立标注每条 rubric 是否满足。\n- 指标：准则级匹配率、样本级 Pearson r。\n- 结果：≈90 % 准则匹配，r = 0.88，证明 LLM-as-a-judge 可替代人工。\n3. 不同评委模型敏感性实验\n\n- 用 6 个额外 MLLM（GPT-5-Thinking、Seed1.6-Vision、Qwen3-VL-235B-Thinking/Instruct、GLM-4.1V-Thinking）对同一批 GPT-5-Thinking 响应再评分。\n- 结果：\n– Gemini-2.5-Pro、GPT-5-Thinking、Qwen3-VL-235B-Thinking 给出高度一致分数；\n– Seed1.6-Vision、GLM-4.1V-Thinking 在开放任务打分显著偏高，提示评委选择影响结论。\n4. 推理轨迹迁移实验\n\n- 方法：将 GPT-5-Thinking 产生的 Chain-of-Thought 段落直接追加到原 prompt，再喂给非推理模型（GPT-5-Instant、Gemini-2.5-Flash、BAGEL）。\n- 指标：图像准确率变化（人工盲审）。\n- 结果：\n– 前两者生成的图像更接近参考答案（如自由女神像皇冠内部结构），得分提升；\n– BAGEL 等较弱模型无改善，说明「有效利用推理信号」本身需要足够强的生成能力。\n\n上述实验系统验证了 UEval 的难度、评分可靠性，并首次量化证明「推理能力」对统一多模态生成的正向作用。\n\nQ5: 有什么可以进一步探索的点？\n\n可进一步探索的方向（按研究类别分组）\n\n1\\. 基准与评测协议\n\n- **动态/可扩展 rubric**\n当前 rubric 为静态人工校验，可探索“迭代式”或“对抗式”生成：让评委模型与候选模型多轮博弈，自动发现新失败模式并即时补充准则。\n\n- **多图像一致性指标**\n开式任务含多图序列，可引入“跨帧 CLIP 相似度”“对象检测 ID 一致性”等自动度量，与 rubric 打分联合拟合，减少评委模型开销。\n\n- **人类-模型混合评审**\n对高分段样本（>90）仍显模糊，可采用“模型预筛 + 人工终审”的主动学习循环，进一步压缩人工成本。\n\n2\\. 模型能力诊断\n\n- **细粒度错误归因**\n将 rubric 准则按“视觉生成-文本生成-跨模态对齐”三级标签分类，统计失败分布，定位是“画不对”还是“说不对”抑或“图文不符”。\n\n- **多步规划深度扩展**\n当前最长 5–6 步，可构造 10+ 步的复杂流程（实验协议、手工折纸等），检验模型长程一致性及中间步骤可复现性。\n\n- **多图联合扩散 vs. 自回归**\n对同一问题分别用“一次扩散出整图网格”与“逐图自回归”两种策略，评估视觉一致性、推理链完整性，寻找最佳生成范式。\n\n3\\. 推理机制\n\n- **推理轨迹可迁移性上限**\n系统研究“评委模型-生成模型”能力 gap 阈值：当生成模型参数量/数据量低于何值时，外部推理轨迹不再有效。\n\n- **视觉思维链（Visual-CoT）**\n让模型在生成每幅图像前输出“中间草图”或“布局描述”，形成显式视觉推理链，检验能否进一步提升一致性与可解释性。\n\n- **逆向推理：文本→图像→文本**\n先让模型生成图像，再屏蔽原 prompt 仅让另一模型依据图像生成解释，循环评估可重建性，量化图像的信息完备度。\n\n4\\. 数据与任务\n\n- **引入时序/动态内容**\n将静态图扩展为短视频片段（3–5 秒），要求模型生成“关键帧+解说”，评测统一模型对时序因果的理解。\n\n- **跨语言与文化**\n将 prompt 与 rubric 本地化到多语言、多文化场景（如中国古建筑、阿拉伯图案），观察模型是否出现文化偏差或视觉符号误用。\n\n- **专业领域深度**\n在医学、法律、机械维修等高风险领域收集小样本，用 UEval 式 rubric 评估，验证统一模型在专业场景下的可靠性。\n\n5\\. 效率与鲁棒性\n\n- **评委模型效率优化**\n探索“小模型蒸馏 + 准则级早期退出”策略，在保持 0.85+ 人类相关度的前提下，将评审成本降低一个数量级。\n\n- **对抗攻击与鲁棒性**\n对图像生成加入隐写扰动或对文本加入同义词扰动，观察评委模型是否被误导，建立鲁棒性评分曲线。\n\n6\\. 统一模型改进\n\n- **联合训练目标加权**\n依据 UEval 的图像-文本 rubric 失败率，动态调整训练损失权重，实现“哪里弱训哪里”的自适应优化。\n\n- **检索增强生成（RAG-for-Generation）**\n允许模型在生成前检索参考图或知识图谱，检验外部记忆能否弥补多步规划与细节一致性缺陷。\n\n这些方向可分别从基准、诊断、推理、数据、鲁棒性与训练等角度推进，进一步揭示统一多模态模型的能力边界与改进路径。\n\nQ6: 总结一下论文的主要内容\n\n**UEval: A Benchmark for Unified Multimodal Generation** 提出首个系统评估“同一 prompt 下同时生成图像+文本”能力的基准，核心内容如下：\n\n1. 问题定义\n现有基准仅分别评测 VQA（图→文）或 T2I（文→图），忽视真实场景需联合输出图文。UEval 填补该空白。\n\n2. 基准构建\n\n- 1 000 道专家筛选题，覆盖 8 类任务：空间、教科书、图表、论文（闭式）；艺术、生活、技术、运动（开式）。\n- 每题配参考图像+文本，人工校验。\n3. 评分协议\n\n- 数据相关 rubric：Gemini-2.5-Pro 针对每题自动生成十余条细则→人工精修，共 10 417 条。\n- 用同一 MLLM 评委按细则逐项打分，满足率即最终得分；与人评 Pearson r = 0.88。\n4. 主实验结果\n\n- 9 个统一模型参评：闭源最佳 GPT-5-Thinking 66.4/100，开源最佳 Emu3.5 仅 49.1，显示基准挑战大。\n- 多步规划任务得分远低于知识任务；模型常出现步骤图序号错乱、图文不一致。\n- 文本生成普遍强于图像；开源模型图像得分常 <15。\n5. 推理迁移实验\n将 GPT-5-Thinking 的 Chain-of-Thought 轨迹追加到非推理模型 prompt，其图像准确率显著提升，表明推理能力对多模态生成关键。\n\n6. 结论\nUEval 提供可扩展、细粒度的统一多模态生成评测，量化揭示当前模型在跨模态一致性、复杂规划与图像质量上的显著不足，为未来研究指明改进方向。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UEval%3A%20A%20Benchmark%20for%20Unified%20Multimodal%20Generation，链接是https://arxiv.org/pdf/2601.22155，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22155。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22154v1-1769990387967","title":"Exploring Reasoning Reward Model for Agents","description":"Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.","url":"https://arxiv.org/abs/2601.22154v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:52.000Z","metadata":{"arxivId":"2601.22154v1","authors":"Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue","categories":"cs.AI, cs.CL","published":"2026-01-29T18:59:52Z","pdfUrl":"https://arxiv.org/pdf/2601.22154v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22154v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22154v1","rank":5,"domain":"VLM","zh_summary":"主动强化学习（Agentic RL）在使智能体执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍然依赖基于稀疏结果的奖励来进行训练。这种反馈无法区分中间推理质量，导致训练结果不理想。在本文中，我们提出了智能体推理奖励模型（Agent-RRM），这是一种多方面的奖励模型，为智能体轨迹生成结构化反馈，包括 (1) 明确的推理轨迹，(2) 聚焦的批评，通过突出推理缺陷提供改进指导，以及 (3) 评估过程表现的总体评分。利用这些信号，我们系统地研究了三种整合策略：Reagent-C（文本增强的优化）、Reagent-R（奖励增强的指导）和 Reagent-U（统一反馈整合）。在 12 个多样化基准测试中的广泛评估表明，Reagent-U 带来了显著的性能提升，在 GAIA 上达到 43.7%，在 WebWalkerQA 上达到 46.2%，验证了我们推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来研究。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**Agentic Reinforcement Learning（Agentic RL）**中**稀疏、仅依赖结果（outcome-based）的奖励信号**所带来的两大核心缺陷：\n\n1. **无法区分中间推理质量**\n在长程、多步工具调用任务里，只要最终答案错误，整条轨迹就被视为完全失败，导致成功的前置步骤被埋没，训练信号过于粗粒度。\n\n2. **缺乏可操作的改进指引**\n纯标量奖励只给出“好/坏”评价，不提供具体哪里出错、如何修正的信息，使得智能体难以针对性改进推理与工具使用策略。\n\n为此，作者提出**Agent Reasoning Reward Model（Agent-RRM）**，一次性输出三种结构化信号：\n\n- 显式推理轨迹 `<think>`\n- 针对性批评 `<critique>`\n- 整体质量评分 `<score>`\n\n并基于该多维度反馈设计三种训练变体（Reagent-C / R / U），系统验证**密集推理奖励**在提升智能体推理与工具使用能力上的有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究划分为两条主线，并指出各自与本文工作的差异。以下按两条主线归纳代表性文献及其与 Agent-RRM 的关系。\n\n2.1 Agentic Reinforcement Learning\n\n**核心问题**：现有方法几乎全部采用**稀疏、结果导向**的奖励，无法对长程轨迹的中间步骤进行细粒度监督。\n\n| 代表工作 | 主要贡献 | 与本文差异 |\n| --- | --- | --- |\n| Search-R1 (Jin et al., 2025) | 用 RL 训练 LLM 交替执行搜索与推理，提升检索增强推理性能 | 仅依赖最终答案正确性作为奖励，无中间反馈 |\n| WebSailor (Li et al., 2025b) | 将 RL 扩展到长程网页导航，降低信息搜寻不确定性 | 同样使用稀疏结果奖励，无法纠正中间步骤错误 |\n| Agent0 (Xia et al., 2025) | 无需人工监督，让工具感知行为通过共进化自动涌现 | 未引入显式推理质量评估，训练信号仍稀疏 |\n\n2.2 Reward Modeling\n\n**核心问题**：现有奖励模型要么只给**步骤级标量分**（标注成本高、易被 hack），要么只做**成对偏好比较**（无法提供可执行改进建议）。\n\n| 代表工作 | 奖励信号形式 | 与本文差异 |\n| --- | --- | --- |\n| Atom-Searcher (Deng et al., 2025) | 直接拿 30B 模型给每一步打 0/1 分 | 仅步级标量，无语言解释，易奖励黑客 |\n| PPR (Xu et al., 2025) | 用预定义原则给步骤打分 | 同样是步级标量，缺乏可操作 critique |\n| RM-R1 (Chen et al., 2025d) | 先生成推理 rubric 再给出评分 | 面向通用问答，未针对 agent 工具链设计 |\n| R1-Reward (Zhang et al., 2025b) | 多模态推理奖励模型，引入稳定 RL 算法 | 仅输出标量，无文本 critique 用于 refinement |\n\n小结\n\n- **Agentic RL 侧**：首次把“**推理轨迹 + 批评文本 + 标量分**”统一引入 agent 训练流程。\n- **Reward Model 侧**：首次为 agent 设计**无需真值**即可生成**可执行批评**的多维奖励模型。\n\nQ3: 论文如何解决这个问题？\n\n论文将“稀疏结果奖励 → 细粒度推理奖励”的转化拆成三步：\n\n1. 设计**多维奖励模型** Agent-RRM；\n2. 构建**覆盖四种任务类型**的高质量数据集；\n3. 提出**三种集成策略**系统地把推理奖励喂给智能体。\n\n以下按方法论（Method）顺序展开。\n\n3.3 数据集构造：先解决“有信号”问题\n\n- **Reagent-RL-709K**\n– 来源：公开 QA + 自采复杂任务，共 70.9 万条。\n– 过滤：三阶段去歧义、去重、难度采样，保证轨迹多样性。\n\n- **Reagent-SFT-55.6K**\n– 用 DeepSeek-V3.1 生成轨迹，**只保留最终答案正确**的 5.56 万条，作为冷启动 SFT 数据。\n\n- **Agent-RRM 专用标注**\n– 在 709 K 上采样 118 K 轨迹，用 GPT-4o-mini-120B 标注三元组 `<think>｜<critique>｜<score>`，得到\n– Reagent-RRM-SFT-28K\n– Reagent-RRM-RL-90K\n\n3.4 Agent-RRM：再解决“信号可信”问题\n\n两阶段训练：\n\n1. SFT 阶段：在 28 K 上让 8 B 模型学会输出结构化三元组。\n2. GRPO 阶段：在 90 K 上用强化学习微调，使\n- 推理轨迹自洽（降低幻觉）\n- 标量分与人工排序一致性最高（Kendall τ 提升 0.12）\n\n**输出格式**（训练/推理一致）：\n\n```text\n<think>…逐步检查工具调用与逻辑链…</think>\n<critique>…指出具体错误：漏 browse、误用工具、循环调用…</critique>\n<score>0.32</score>\n```\n\n3.5 Reagent：最后解决“信号用好”问题\n\n三种集成策略对应三条消融曲线：\n\n| 变体 | 利用的信号 | 训练方式 | 关键公式 |\n| --- | --- | --- | --- |\n| Reagent-C | 仅 <critique> | 零参更新，in-context 二次生成 |  o_i^((2))sim π_θ(·mid q,o_i^((1)),c_i)  |\n| Reagent-R | 仅 <score> | 传统 GRPO，把 score 当密集奖励 |  R_i=R_(rule)+λ·Agent-RRM(q,o_i)  |\n| Reagent-U | <critique>+<score> | 统一池化优化：初始+修正轨迹一起算优势 | 见下方公式 |\n\n**Reagent-U 统一目标**\n\nJ_U(θ)=E[(1) / (2G)∑_(k=1)^2∑_(i=1)^G[min!(r_i^((k))(θ)A_i^((k)),clip_ε)-β D_(KL)^((i,k))]]\n\n其中\n\n-  k∈1,2  对应“初始轨迹”与“经 critique 修正后的轨迹”\n- 优势  A_i^((k))  在 **混合池**  G_(pool)  内标准化，保证模型同时优化“一次做对”与“被提示后能改”两种能力。\n\n实验验证“信号有效”\n\n- **Reagent-C** 在 GSM8K 上把 94.6 → 94.9，**无需训练**即可修正 47 % 的初级计算错误。\n- **Reagent-R** 在 Bamboogle 上比纯规则奖励提升 11.2 个百分点，验证密集奖励缓解稀疏性。\n- **Reagent-U** 在 12 个基准上**全部领先**，GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相对 8 B 基线平均提升 18.4 %，证明两种信号**协同增益**最大。\n\n一句话总结\n\n论文通过“**多维奖励模型 → 数据 → 三种集成策略**”的闭环，把原本只能告诉智能体“错”的稀疏信号，升级为同时告诉它“**哪里错、为何错、如何改**”的密集推理奖励，从而系统性地提升了长程工具使用与推理能力。\n\nQ4: 论文做了哪些实验？\n\n论文在第 4 节“Experiments”中围绕 **12 个基准、3 类任务、3 种变体、4 组消融** 展开系统评估，可概括为“**一个主表 + 三个深度分析 + 两个参数实验**”。\n\n1\\. 主实验：12 基准全面比较\n\n**任务分组与指标**\n\n- **数学推理**：AIME24/25、MATH500、GSM8K        pass@1\n- **知识密集型**：HotpotQA、2Wiki、Bamboogle、MuSiQue  pass@1\n- **通用 Agent & 搜索**：GAIA、WebWalkerQA、HLE、xbench pass@1\n\n**对照维度**\n\n- 闭源：o3, o1-preview, Claude-4-Sonnet, DeepResearch\n- 开源同规模（≤8 B）：WebThinker、ARPO、VerlTool …\n- 开源大尺度（≤32 B）：QwQ-32B、DeepSeek-R1-671B、Search-o1 …\n- 开源过程奖励：Atom-Searcher、PPR-Instruct\n\n**核心结果**（表 1+2 汇总）\n\n| 模型 | GAIA | WebWalkerQA | Bamboogle | AIME24 | 平均提升 |\n| --- | --- | --- | --- | --- | --- |\n| Qwen3-8B 基线 | 21.4 | 29.0 | 53.6 | 46.7 | — |\n| Reagent-C | 25.2 | 35.5 | 61.6 | 56.7 | +6.8 |\n| Reagent-R | 36.9 | 45.3 | 72.8 | 53.3 | +11.4 |\n| Reagent-U | 43.7 | 46.2 | 76.8 | 60.0 | +18.4 |\n\n2\\. 深度分析 1：Textual-Critique 真有用吗？\n\n- **实验**：Reagent-C 零参更新，仅把 critique 作为 prompt 让模型再答一次。\n- **结果**：12 个数据集**全部上涨**，GSM8K 94.6→94.9，GAIA 21.4→25.2；**首次失败中 47 % 在第二次答对**。\n- **结论**：critique 提供**可执行诊断**，无需训练即可纠正工具误用与逻辑跳跃。\n\n3\\. 深度分析 2：Model-based Reward 能缓解稀疏性吗？\n\n- **实验**：固定 λ=0.3，比较 Reagent-R（规则+模型分） vs Reagent w/o Agent-RRM（仅规则）。\n- **结果**：Bamboogle +11.2，xbench +9.0，平均 +7.6 个百分点。\n- **结论**：密集**标量奖励**显著改善训练信号，但仍低于 Reagent-U，说明**纯分数不如“分数+文字”**。\n\n4\\. 深度分析 3：Unified Feedback 是否产生协同效应？\n\n- **实验**：Reagent-U 同时优化初始与 critique-修正轨迹，优势在混合池统一标准化。\n- **结果**：\n– 在**需要≥5 步工具调用**的 GAIA Lv3 上，Reagent-U 16.7 %，比 Reagent-R 高 10 个百分点。\n– 在**跨模态**的 GAIA-full（165 题）上，pass@1 38.8 %，比 MCP-R1 高 1.2 个百分点，验证**非文本任务同样受益**。\n- **结论**：scalar 信号提供**全局优化方向**，critique 信号提供**局部修正动作**，二者联合产生**1+1>2**的协同增益。\n\n5\\. 参数实验\n\n5.1 奖励权重 λ 扫描（图 3）\n\n- 范围：λ∈\n0,0.5\n；任务：AIME24（数学）、xbench（深度搜索）。\n- 结果：λ≈0.2–0.4 区域性能 plateau；λ≥0.5 开始下降，**过高权重会牺牲最终答案准确性**。\n\n5.2 训练步数与收敛性（附录 B）\n\n- 300 步 GRPO 已收敛，继续训练未见明显提升，说明**数据量与模型规模匹配**，未出现过度拟合。\n\n6\\. 案例可视化（附录 D）\n\n- **Case 1（GAIA 搜索）**：critique 指出“未 browse 权威页→假设错误”，第二次调用 browse 拿到 15 Mt 精确值，答案从 Cannikin→Bravo。\n- **Case 2（GSM8K 数学）**：critique 指出“Python 第一次无 print 浪费调用”，第二次去掉多余调用并纠正“总工时÷人数”逻辑，答案从 47.25→189。\n\n一句话总结\n\n实验从“**能不能用→管不管用→好不好用→怎么用最好**”四个层次验证：\n**多维推理奖励模型 + 统一集成训练** 在 12 基准、跨模态、长程工具调用场景下**全面领先**，且**零参 critique 即可带来显著推理修正**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下可探索方向按“**规模-数据-任务-信号-理论**”五层归纳，均直接对应论文第 6 节 Limitations 的开放问题，并给出可落地的下一步实验设计。\n\n1\\. 规模：更大参数与分布式 RL\n\n- **问题**：全文仅在 8 B 模型验证，协同效应是否会随规模放大？\n- **探索点**\n– 在 32 B/70 B 基座重复 Reagent-U 训练，观察“λ-性能”曲线是否左移（更强模型需要更少 critique 即可收敛）。\n– 采用 **Model-parallel + Rollout-parallel** 框架，把 Agent-RRM 与 Policy 解耦到不同 GPU 节点，验证千亿级模型下是否出现奖励过拟合（reward overfitting）。\n\n2\\. 数据：开放环境 vs 静态基准\n\n- **问题**：12 个基准均为**静态题库**，工具集固定，无法反映真实世界工具漂移。\n- **探索点**\n– 引入 **Live-API 环境**（如 Google Search 动态排名、WolframAlpha 付费限额），构建非平稳奖励，测试 Agent-RRM 的**在线自适应**能力。\n– 设计 **Adversarial Tool-Response** 数据集：故意返回错位、截断或冲突信息，量化 critique 对“工具幻觉”的纠错上限。\n\n3\\. 任务：长周期闭环与多智能体\n\n- **问题**：现有 episode 最长 30 步，且为单 agent。\n- **探索点**\n– **Science-of-Chemistry** 场景：让 agent 连续运行 100+ 步完成“文献检索→实验设计→代码仿真→结果验证”闭环，用 Agent-RRM 每 10 步做一次 critique，观察**长程信用分配**是否仍有效。\n– **Multi-Agent Debate**：把 critique 角色独立为第三方裁判，与其他 agent 的 solution 进行**多轮对抗式 refinement**，检验 critique 语言是否出现**策略性误导**（reward hacking 新形态）。\n\n4\\. 信号： critique 的粒度与格式\n\n- **问题**：固定三段式输出可能不是最优。\n- **探索点**\n– **Token-Level 奖励**：把 `<critique>` 拆成**逐句批评**，用指针网络对齐到具体动作，实现**子步骤级**优势估计：\n\nA_t = R_(sentence)(t) - μσ\n\n– **Multimodal-Critique**：当观测含图/音频时，让 Agent-RRM 输出**圈图 mask** 或**音频时间戳**，验证**视觉定位**能否进一步降低修正成本。\n\n5\\. 理论：奖励可解释性与收敛保证\n\n- **问题**：Agent-RRM 的自洽性缺乏形式化刻画。\n- **探索点**\n– 建立 **Critique Consistency Index (CCI)**：同一轨迹两次采样 critique 的**语义相似度**，用 CCI 与最终性能做皮尔逊检验，量化“**可解释→可改进**”的因果链。\n– 在**离线 RL** 框架（如 CQL）下，把 Agent-RRM 当作 Q-function 正则项，推导**误差上界**，回答“**推理奖励误差 ε 对策略性能影响是 O(ε) 还是 O(ε²)**”。\n\n快速启动清单（8 卡 A800 可完成）\n\n1. 用 32 B 模型复现 Reagent-U，记录 λ∈{0.1,0.3,0.5} 的 GAIA 性能 → 验证规模效应。\n2. 在 Live-API 环境跑 500 条长 episode（≥50 步），统计 critique 成功纠正“工具返回漂移”的比例 → 验证鲁棒性。\n3. 把 critique 拆成句子级嵌入，与动作序列做注意力对齐，重新计算优势 → 验证更细粒度信号是否带来额外 2–3 个百分点。\n\n一句话总结\n\n未来工作可沿“**更大模型、更乱环境、更长链条、更细信号、更严理论**”五轴推进，把 Agent-RRM 从“静态题库好”推向**开放世界可信**。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Agent-RRM**（Agent Reasoning Reward Model）与 **Reagent** 训练框架，解决 Agentic RL 中“稀疏结果奖励无法区分中间推理质量、缺乏可执行改进信号”的核心痛点。主要贡献与内容可概括为以下四点：\n\n1. 多维推理奖励模型\nAgent-RRM 对每条轨迹输出三元组：\n\n- `<think>`：逐步推理轨迹\n- `<critique>`：可执行的错误指正\n- `<score>`：0–1 整体质量分\n无需真值即可提供密集、语言化、可解释的反馈。\n2. 高质量数据集\n构建 70.9 万 RL 样本（Reagent-RL-709K）与 5.56 万 SFT 样本（Reagent-SFT-55.6K），并标注 11.8 万条结构化评判（Reagent-RRM-SFT/RL-28K/90K），覆盖数学、多模态、网页搜索、复杂工具四类任务。\n\n3. 三种集成策略\n\n- Reagent-C：零参更新，用 critique 做 in-context 二次生成\n- Reagent-R：GRPO 训练，规则奖励 + Agent-RRM 分数\n- Reagent-U：统一池化初始与修正轨迹，同步优化 scalar 与语言信号\n4. 系统实验\n在 12 个基准（GAIA、WebWalkerQA、AIME、HotpotQA 等）上，Reagent-U 取得新 SOTA：GAIA(text) 43.7 %、WebWalkerQA 46.2 %，相比 8 B 基线平均提升 18.4 %；零参的 Reagent-C 也能稳定上涨，验证多维推理奖励的普适性与可解释性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Exploring%20Reasoning%20Reward%20Model%20for%20Agents，链接是https://arxiv.org/pdf/2601.22154，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22154。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22153v1-1769990387967","title":"DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation","description":"Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.","url":"https://arxiv.org/abs/2601.22153v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:51.000Z","metadata":{"arxivId":"2601.22153v1","authors":"Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu","categories":"cs.RO, cs.CV","published":"2026-01-29T18:59:51Z","pdfUrl":"https://arxiv.org/pdf/2601.22153v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22153v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22153v1","rank":6,"domain":"VLM","zh_summary":"操作动态物体仍然是视觉-语言-动作（VLA）模型面临的一个开放挑战。尽管这些模型在静态操作中表现出强大的泛化能力，但在需要快速感知、时间预测和连续控制的动态场景中仍然困难重重。我们提出了 DynamicVLA，这是一个用于动态物体操作的框架，通过三项关键设计整合了时间推理和闭环自适应：1）一个紧凑的 0.4B VLA，使用卷积视觉编码器进行空间高效、结构忠实的编码，实现快速多模态推理；2）连续推理（Continuous Inference），实现重叠的推理与执行，从而降低延迟并及时适应物体运动；3）潜在感知的动作流（Latent-aware Action Streaming），通过强制动作执行与时间对齐来弥合感知与执行之间的差距。为了弥补动态操作数据的不足，我们引入了动态物体操作（DOM）基准，该基准从零构建，采用自动数据收集管线高效获取 2.8K 场景、206 个物体的 20 万个合成实验，并支持在无遥控操作的情况下快速收集 2 千个真实世界实验。大量评估表明，DynamicVLA 在反应速度、感知能力和泛化性能上有显著提升，使其成为一个适用于各种实体的一体化动态物体操作框架。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决\\*\\*动态物体操控（dynamic object manipulation）\\*\\*中 Vision-Language-Action（VLA）模型面临的三大核心难题：\n\n1. **感知-执行错位（perception–execution misalignment）**\n物体在推理延迟  m  内持续运动，导致  A_t  生成时对应的物体状态  s_t  已演变为  s_(t+m) ，动作序列失去时效性。\n\n2. **块间等待（inter-chunk waiting）**\n现有 VLA 采用“推理-执行串行”模式：必须等上一段动作全部执行完才启动下一次推理，造成  n-m  步空窗期，无法及时响应动态变化。\n\n3. **数据稀缺**\n公开数据集几乎全是静态桌面操作，缺乏大规模“物体持续运动”下的多模态演示，难以支撑对不确定运动、精确接触的通用策略学习。\n\n为此，作者提出 **DynamicVLA** 框架，通过\n\n- 0.4B 参数的轻量级卷积-扩散架构，实现 ≤12 ms 单帧推理；\n- **Continuous Inference** 流水线，让推理与执行重叠，消除块间等待；\n- **Latent-aware Action Streaming** 机制，在线丢弃过期动作、优先执行最新预测，强制保持时序对齐。\n\n同时，作者构建了首个大规模动态操控基准 **DOM**，在仿真+真机自动采集 200 K/2 K 条演示，为上述问题提供标准化评测与训练数据。\n\nQ2: 有哪些相关研究？\n\n论文在第二节 **Related Work** 中将与 DynamicVLA 直接相关的研究划分为三大主线，并指出其局限：\n\n1. Vision-Language-Action 模型\n\n- 早期 Transformer 方案：RT-1、RT-2、OpenVLA 等，依赖 3B–7B 大 backbone，推理延迟高。\n- 扩散-动作模型：π₀、π₀.5、Diffusion Policy，把动作生成视为去噪过程，但仍串行执行，存在块间等待。\n- 轻量级/适配器路线：SmolVLA、VLA-Adapter-Pro、GR00T-N1.5，通过减小模型或插入适配器提速，却未解决“推理时物体继续运动”带来的时序错位。\n- 并发实时 VLA：RDT-2、RTVLA、VLASH，面向高速目标（乒乓球、传球）做实时交互，但任务允许大接触裕度，无需精确 6-DoF 操控，也未处理错位与等待问题。\n2. 机器人学习数据集\n\n- 静态桌面集：BridgeData V2、LIBERO、CALVIN、BEHAVIOR-1K 等，对象静止，缺乏动态场景。\n- 长跨度或语言条件集：VIMA Bench、VLABench、RoboCasa，任务复杂但仍以“物体静止”为前提。\n- 生成式仿真：RoboGen、RoboTwin，可扩量，却受低帧率、伪影、内存限制，且未提供运动物体真值状态。\n→ 结论：尚无面向“物体持续运动+精确接触”的大规模基准。\n3. 机器人动态操控\n\n- 专用/结构化场景：DBC-TFP、GEM、TossingBot，依赖传送带或已知动力学，仅处理可预测轨迹。\n- 反应式控制：乒乓球、足球、投掷系统，手工设计感知-控制流水线，难以泛化到新对象或新运动模式。\n→ 结论：开放环境、不确定运动、语言条件、端到端学习的通用动态操控仍空白。\n\nDynamicVLA 在上述工作的基础上，首次把“低延迟推理+连续执行+时序对齐”作为整体框架提出，并配套构建 DOM 数据集，填补了动态物体操控在 VLA 范式下的研究空白。\n\nQ3: 论文如何解决这个问题？\n\n论文从 **模型架构、执行机制、数据基础** 三个层面协同解决动态物体操控难题，具体方案如下：\n\n1\\. 模型层面：0.4 B 参数轻量级 VLA\n\n- **卷积视觉编码器 FastViT**\n- 单帧 384×384 → 36 个 960-D 视觉 token，避免 Transformer 的二次增长，编码延迟 <4 ms。\n- **截断语言主干 SmolLM2-360M**\n- 仅用前 16 层，参数量 360 M；与视觉、状态 token 拼接后一次性前向，单步推理 ≤12 ms（RTX A6000 上 88 Hz）。\n- **扩散式 Action Expert**\n- 16 层轻量 Transformer，独立承担 20 步动作块去噪，降低多模态主干负担；隐空间维度压缩至 720，进一步提速。\n\n2\\. 执行层面：双机制消除“感知-执行错位”与“块间等待”\n\nA. Continuous Inference（连续推理）\n\n- 推理循环与动作执行 **完全并行**：\n- 上一 chunk 尚在执行时，下一 chunk 的推理已启动；\n- 只要块长度  n  > 推理延迟  m ，就不会出现空窗。\n- 形式化：推理完成时刻为  t, t+m, t+2m, … ，执行流无需等待。\n\nB. Latent-aware Action Streaming（隐空间动作流）\n\n- **过期动作丢弃**：对 chunk  A_t ，只执行子序列  a_(t+m),…,a_(t+n) ，前  m  步已过时直接舍弃。\n- **最新预测优先**：当  A_t  与  A_(t+m)  时间重叠时，用  A_(t+m)  的动作覆盖旧序列，保证机器人始终按 **最新观测** 行动。\n\n3\\. 数据层面：DOM 基准与自动采集 pipeline\n\n- **仿真侧**\n- Isaac Sim + 状态机控制器，实时读取 6D 姿态/速度；\n- 2.8 K 场景、206 种物体、速度 0–1 m/s，自动产生 200 K 条演示。\n- **真机侧**\n- 双 RGB 视角 + EfficientTAM 在线分割 → 三角化求 3D 质心 → 滑动窗拟合 6D 速度，构建“真实世界模拟器”；\n- 无需遥操作，≈10 s/episode，采集 2 K 条真机演示。\n\n4\\. 训练流程三阶段\n\n1. VLM 预训练：150 M 图文对（COYO-700M）对齐视觉-语言空间。\n2. DOM 合成数据 mid-training：端到端联合训练主干与扩散 Action Expert。\n3. 真机数据 post-training：适配具体机器人形态与相机配置。\n\n5\\. 效果总结\n\n- 在 DOM 仿真 1 800 trials 上，DynamicVLA 平均成功率 47.1 %，比最强基线（VLA-Adapter-Pro 13.6 %）提升 **+33.5 pp**；\n- 真机 6 项动态任务，平均成功率 73 %，而 π₀.5、SmolVLA、VLASH 均 <30 %；\n- 消融验证：去掉 Continuous Inference 或 LAAS 任一模块，成功率分别下降 **\\-10.9 pp** 与 **\\-7.3 pp**，二者互补且对现有 VLA 通用可插拔。\n\n通过“小模型+连续推理+动作流”三位一体设计，论文首次在 VLA 范式下实现了对 **不确定运动物体** 的毫秒级闭环、精确 6-DoF 操控，并提供了可复现、可扩展的基准与数据 pipeline。\n\nQ4: 论文做了哪些实验？\n\n论文在 **仿真** 与 **真机** 两侧、共 **5 类实验** 中系统验证 DynamicVLA 的有效性，全部基于新提出的 DOM 基准与 16 项真机任务。实验设计围绕 4 个研究问题展开，可总结如下：\n\n1\\. 主实验：DOM 仿真基准全面评测\n\n- **规模**：9 个子维度 × 10 场景 × 20 重复 = 1 800 trials。\n- **对比方法**：Diffusion Policy、OpenVLA-OFT、π₀、π₀.5、SmolVLA、GR00T-N1.5、VLA-Adapter-Pro、VLASH。\n- **指标**：成功率 SR、路径长度 Path Len、任务完成时间 Time。\n- **结果**：DynamicVLA 平均 SR 47.1 %，领先最强基线 **+33.5 pp**；Time 缩短 1.5 s（−15 %）。\n\n2\\. 真机动态交互实验（Interaction）\n\n- **机器人**：Franka Emika Panda + AgileX PiPER。\n- **任务**：6 项，含“滚动咖啡罐入木盒”“连续收集乒乓球”等，均用二臂抛射产生 0.3–0.8 m/s 运动。\n- **对比**：π₀.5、SmolVLA、VLASH。\n- **结果**：DynamicVLA 平均成功率 **73.3 %**，最佳基线仅 **28.3 %**；在最长序列任务（收集 5 球）上差距最大 **+55 pp**。\n\n3\\. 真机多模态感知实验（Perception）\n\n- **任务**：6 项，考察视觉区分、空间方位、速度辨识。\n- 例：同时抛出网球+匹克球，仅把“ slower ball ”放入纸碗。\n- **结果**：DynamicVLA 平均 **51.9 %**，最佳基线 **16.7 %**；运动感知任务差距最大 **+40 pp**。\n\n4\\. 真机泛化实验（Generalization）\n\n- **任务**：4 项，未见物体外观（塑料饮料瓶、高尔夫球）与未见运动模式（不规则滚动的土豆、青苹果）。\n- **结果**：DynamicVLA 平均 **70 %**，最佳基线 **31.7 %**；外观泛化差距 **+46 pp**，运动泛化差距 **+39 pp**。\n\n5\\. 消融与机制分析\n\n| 实验 | 变量 | 主要结论 |\n| --- | --- | --- |\n| A. backbone 容量 | 135 M / 360 M / 1.7 B | 360 M 在 SR 与延迟间取得最佳平衡；1.7 B 因延迟增加反降 SR。 |\n| B. 视觉编码器 | FastViT ↔ Transformer | FastViT 在同等精度下 latency −30 %，SR +18 pp。 |\n| C. Continuous Inference | 开/关 | 关闭后 SR −10.9 pp，Time +0.6 s，出现明显块间等待。 |\n| D. LAAS | 开/关 | 关闭后 SR −7.3 pp，路径更短但失败率升高，验证“过期动作”危害。 |\n| E. 跨模型移植 CI+LAAS | 直接插入 π₀.5 / SmolVLA | SmolVLA SR +13 pp，π₀.5 仅 +4 pp，说明机制通用但受限于主干延迟。 |\n| F. 时序视觉窗口 | {ot} / {ot-1,ot} / {ot-2,ot} / 4 帧 | {ot-2,ot} 为最佳，单帧掉 9 pp，证明稀疏但足够间隔的时序信息最关键。 |\n| G. LLM 深度 | 8 / 16 / 24 / 32 层 | 16 层再往下容量骤降，往上延迟增幅大于收益，确认“截断 16 层”最优。 |\n\n6\\. 额外分析\n\n- **GPU 内存与频率**：DynamicVLA 仅 1.8 GB，在 RTX A6000 上稳定 **88 Hz**，满足 ≤12 ms 实时需求。\n- **安全约束**：超 workspace 边界即回 home pose，真机实验零碰撞。\n\n通过 **1800 仿真 trials + 16 项真机任务 + 7 组消融**，论文验证了所提架构与执行机制在 **响应速度、感知精度、泛化能力** 上均显著优于现有 VLA 基线，且 CI 与 LAAS 可即插即用地提升其他模型。\n\nQ5: 有什么可以进一步探索的点？\n\n论文在 Discussion & Future Work 中已给出三条明确方向；结合实验结果与当前局限，可进一步探索的关键点归纳如下：\n\n1\\. 架构与实时效率\n\n- **亚毫秒级 VLA**：\n探索 **<5 ms** 推理的极端延迟预算，需联合设计\n- 事件相机 + 稀疏卷积 backbone，\n- 动作专家 **量化/蒸馏** 到 8-bit 或 **LSTM-扩散混合** 结构，\n- **KV-cache 提前裁剪** 与 **动态深度退出**（early-exit）。\n- **端侧芯片级部署**：\n将 FastViT-16 + 扩散专家移植到 **NPU/FPGA**，验证在 **Jetson Orin** 级平台仍能保持 ≥50 Hz。\n\n2\\. 长周期动态任务\n\n- **带记忆的多阶段规划**：\n当前仅短-中期闭环。可引入\n- **时间感知记忆池**（例如 Ego4D 特征缓存），\n- **语言条件下的分层策略**：高层 VL-Planner 生成子目标 → 低层 DynamicVLA 执行，\n- **在线任务分解评估**：在 DOM 上新增 “10 步以上、物体持续运动” 的长程套件（如动态装配、运动分拣线）。\n\n3\\. 非刚性/流体动力学\n\n- **可形变物体 VLA**：\n把 DOM 的刚体状态  s_t  扩展为\n- 粒子集 / 网格顶点（**MPM 仿真**提供真值），\n- 隐式神经表示（**NeRF-Time** 连续场），\n训练模型直接预测 **力/速度场** 而非 6-DoF 位姿。\n- **流体操控基准**：\n建立 “倒水-运水-注水” 动态任务，评估 VLA 对 **液体晃动、表面波动** 的预测与补偿能力。\n\n4\\. 自监督与持续学习\n\n- **运动预测前置任务**：\n利用 DOM 的 6-DoF 真值，引入 **flow-masked modeling**——随机遮蔽未来 3 帧物体位姿，让模型自回归预测，提高 **zero-shot 动态抓取** 成功率。\n- **真机在线适应**：\n部署后遇到新摩擦、新质量分布，用 **1-shot 演示 + 梯度缓存** 快速微调 Action Expert，验证 **5 min 内完成适应** 的极限。\n\n5\\. 多机协同与对抗运动\n\n- **多臂动态协作**：\n两条臂同时抓取 **同一运动物体**（如长杆），或 **传球-接球** 闭环，考察 VLA 对 **共享运动预测** 的协调能力。\n- **对抗式扰动**：\n引入 **主动干扰臂** 在任意时刻撞击目标，形成 **非稳态、非马尔可夫** 运动，升级 DOM 的 Disturbance Robustness 维度。\n\n6\\. 安全与不确定性量化\n\n- **延迟感知不确定性估计**：\n在扩散动作去噪过程中，输出 **每步动作置信区间**；当置信度低于阈值即触发 **安全冻结** 或 **重规划**。\n- **形式化安全验证**：\n结合 **可达性分析**（Reach-nn\\*）对 Continuous Inference 循环进行 **最坏-case 延迟下的安全区域** 离线验证，提供可证明的碰撞规避上界。\n\n7\\. 跨模态传感器融合\n\n- **视觉-触觉-惯导联合**：\n在真机手腕增加 **低成本触觉阵列 + IMU**，构建 **触觉-视觉-语言** 三模态 VLA，验证对 **滑动、碰撞瞬间** 的亚毫秒响应是否优于纯视觉。\n- **事件相机替换 RGB**：\n用 **DVS 事件流** 作为视觉输入，探索 **>200 Hz** 推理频率下是否可进一步缩小 **感知-执行错位** 至 1-2 ms 量级。\n\n8\\. 通用化理论分析\n\n- **动态操控的泛化边界**：\n基于 **VC-Dimension / DAG 泛化误差** 框架，量化模型容量、推理延迟与 **运动速度上限** 三者的理论关系，给出 **“给定延迟预算，最大可适应速度”** 的解析界。\n\n综上，DynamicVLA 已验证“低延迟+连续执行”在刚性动态操控的有效性；下一步可向 **更极端延迟、更长周期、更复杂物理、更安全可信** 四个维度深入，形成覆盖 **刚体-可形变-流体-多机** 的通用实时 VLA 体系。\n\nQ6: 总结一下论文的主要内容\n\n论文核心速览\n\n1\\. 问题\n\n- 动态物体操控要求机器人在物体**持续运动**中完成抓取-放置，但现有 VLA 模型存在：\n- 推理延迟  m  导致感知-执行错位；\n- 串行“推理-执行”带来块间等待；\n- 缺乏大规模动态操控数据。\n\n2\\. 方法 DynamicVLA\n\n- **0.4 B 轻量架构**：FastViT 卷积编码 + SmolLM2-360M（前 16 层）+ 扩散式 Action Expert，单帧 ≤12 ms。\n- **Continuous Inference**：推理与执行重叠，消除等待。\n- **Latent-aware Action Streaming**：在线丢弃过期动作、优先最新预测，强制时序对齐。\n- **DOM 基准**：自动采集 200 K 仿真 + 2 K 真机演示，覆盖 206 物体、2.8 K 场景、0-1 m/s 运动。\n\n3\\. 实验\n\n- **仿真 1 800 试次**：DynamicVLA 平均成功率 47.1 %，领先最强基线 **+33.5 pp**，任务时间缩短 15 %。\n- **真机 16 任务**（Franka & PiPER）：交互、感知、泛化三大维度分别领先 **+45 pp、+40 pp、+39 pp**。\n- **消融**：Continuous Inference 与 LAAS 分别贡献 **+10.9 pp、+7.3 pp**，可即插即用到其他 VLA。\n\n4\\. 贡献\n\n- 首次在 VLA 范式下实现**毫秒级闭环、精确 6-DoF 动态物体操控**；\n- 提出可扩展的**连续推理+动作流执行机制**；\n- 发布首个大规模动态操控基准与自动采集 pipeline，为后续研究提供数据与评测标准。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DynamicVLA%3A%20A%20Vision-Language-Action%20Model%20for%20Dynamic%20Object%20Manipulation，链接是https://arxiv.org/pdf/2601.22153，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22153。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22150v1-1769990387967","title":"Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions","description":"Large Vision-Language Models (VLMs) often answer classic visual illusions \"correctly\" on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.","url":"https://arxiv.org/abs/2601.22150v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:24.000Z","metadata":{"arxivId":"2601.22150v1","authors":"Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy","categories":"cs.CV","published":"2026-01-29T18:59:24Z","pdfUrl":"https://arxiv.org/pdf/2601.22150v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22150v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22150v1","rank":7,"domain":"VLM","zh_summary":"大型视觉-语言模型（VLMs）常常能够在原始图像上“正确”回答经典视觉错觉，但当错觉因素被反转时，它们仍然会保持相同的回答，即使这种视觉变化对人类来说非常明显。这引出了一个根本性的问题：VLMs 是在感知视觉变化，还是仅仅在回忆记忆中的模式？虽然已有多项研究注意到这种现象，但其潜在原因仍不清楚。为了从观察走向系统性的理解，本文提出了 VI-Probe，一个可控的视觉错觉框架，具有分级扰动和匹配的视觉对照（没有错觉诱因），能够将视觉感知与语言驱动的记忆区分开来。与以往关注平均准确率的研究不同，我们使用极性翻转一致性（Polarity-Flip Consistency）、模板固定指数（Template Fixation Index）以及经过匹配对照标准化的错觉乘数来衡量稳定性和敏感性。在不同系列的实验中发现，回答的持续性源于多种不同原因，而非单一机制。例如，GPT-5 表现出记忆覆盖现象，Claude-Opus-4.1 展现了感知-记忆竞争，而 Qwen 系列则提示视觉处理的限制。我们的研究结果挑战了单一原因的观点，并推动基于探测的评估方法，用以同时衡量知识量和对可控视觉变化的敏感性。数据和代码可在 https://sites.google.com/view/vi-probe/ 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在回答一个核心问题：\n**当前的大型视觉-语言模型（VLMs）在面对经典视觉错觉图像时，究竟是“看见”了视觉变化，还是仅仅“回忆”了语言或记忆模板？**\n\n具体而言，作者观察到：\n\n- 在原始错觉图像上，多数 VLMs 能给出符合物理事实的“正确”答案；\n- 当错觉的关键诱导因素被**反转**（perturbed）后，人类能立即感知到变化，但模型仍固执地输出与原始图像相同的答案。\n\n这一现象表明，模型可能并未真正感知视觉变化，而是依赖**语言先验或记忆模板**进行回答。为系统解释这一“感知–记忆”断裂，论文提出以下研究目标：\n\n1. 构建可控评测框架 VI-Probe，通过**分级视觉扰动**与**匹配对照图像**分离视觉感知与语言回忆；\n2. 设计超越静态准确率的诊断指标（PFC、TFI、illusion multiplier R），量化模型对视觉变化的敏感度与稳定性；\n3. 揭示不同模型家族在错觉任务下的**异质失效机制**（记忆覆盖、感知–记忆竞争、视觉处理瓶颈）；\n4. 为后续模型训练与评测提供**反事实一致性**与**感知优先架构**的改进方向。\n\nQ2: 有哪些相关研究？\n\n论文在 Related Work 部分将已有研究归为两条主线，并指出其共同局限：仅报告平均准确率，缺乏对“感知–记忆”机制的细粒度拆解。主要相关文献如下：\n\n1. 视觉错觉 VLM 评测\n\n- HallusionBench \n17\n：15 例错觉，固定强度，无对照图像，仅二进制准确率。\n- IllusionVQA \n33\n：20 类 374 图，无扰动版本，无法测量翻转一致性。\n- VLMBiased \n36\n：6 类 198 图，提出语言先验偏差，但未分离视觉因素。\n- IllusionBench \n41\n 与 Illusion-illusion \n35\n：大规模静态测试，缺乏连续扰动与对照。\n2. VLM 语言先验 / 偏见研究\n\n- 社会–文化偏见 \n7, 13, 27, 28, 29, 30, 34\n：聚焦文本模态，未涉及视觉错觉场景。\n- 视觉–语言接口偏见 \n10, 11, 15, 19, 21\n：发现模型“盲信”文本，但缺乏可控视觉扰动证据。\n- 反事实或对比评测 \n14, 20, 24\n：提出一致性指标，尚未应用于错觉扰动图像。\n3. 人类与模型错觉对比\n\n- \n25, 39, 40\n：定性观察模型与人类在错觉上的差异，未提供可复现的扰动 pipeline 与量化指标。\n\n综上，已有工作多停留在“现象观察”层面；本文首次引入**可复现的扰动–对照生成 pipeline**与**感知–记忆解耦指标**，将观察升级为可量化的机制诊断。\n\nQ3: 论文如何解决这个问题？\n\n论文从“观察现象”推进到“可控实验 + 量化诊断”，通过三步策略系统拆解“感知 vs. 记忆”问题：\n\n1. 构建可控评测框架 VI-Probe\n\n- 视觉侧：对 27 种经典错觉生成\n– 原始图 x^O\n– 扰动图 x^P（关键诱导因素连续反转，强度 α∈\n0,1\n）\n– 对照图 x^{OC}/x^{PC}（移除诱导元素，保留全局语义）\n– 提示图 x^{OH}/x^{PH}（叠加对齐标记）\n- 语言侧：为每张图配三问\n– q\\_f：正向提问（“是否相等？”）\n– q\\_r：反向提问（“是否不同？”）\n– q\\_I：显式指令“仅依赖视觉，忽略先验”\n- 结果：得到 870×11 组图像-问题对，可连续采样，保证统计功效。\n2. 设计“感知–记忆”诊断指标\n\n- Polarity-Flip Consistency\n\nPFC=mathbb E[mathbf 1(a_r=1-a_f)]\n\n衡量模型在问题极性反转时是否保持逻辑互补。\n- Template Fixation Index\n\nTFI=1-PFC\n\n量化“模板化重复”程度。\n- Illusion multiplier\n\nR=|Acc(x^O)-Acc(x^P)||Acc(x^(OC))-Acc(x^(PC))|+varepsilon\n\n以对照扰动为基线，归一化错觉特异性的记忆干扰：\n– R>1 表示记忆覆盖；\n– R<1 表示视觉瓶颈；\n– R≈1 表示感知–记忆竞争。\n3. 大规模实验与机制归因\n\n- 15 个最新模型（OpenAI、Anthropic、Google、Qwen）零样本评测。\n- 用剂量-响应曲线、人类 perceptual threshold 对比、跨家族热图，揭示三类异质失效：\n– 记忆覆盖：GPT-5 在错觉条件下准确率 91.7%→4.5%，对照仅降 44%，R=1.97。\n– 感知–记忆竞争：Claude-Opus-4.1 随 α 变化呈现可测剂量效应，R≈1.01。\n– 视觉处理瓶颈：Qwen2.5-3B 对照与错觉降幅均很小，R=0.13，体现表征纠缠。\n\n通过“可控扰动 + 对照归一 + 多指标联合”方案，论文将“模型是否真看”转化为可复现、可量化的科学实验，并给出改进方向：反事实一致性训练、感知优先架构、评测必须同时报告 PFC/TFI/R 而非单点准确率。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 VI-Probe 框架共执行 4 组互相关联的实验，覆盖 15 个最新 VLMs，总计约 870×11=9 570 个图像-问题对。核心实验一览如下：\n\n1. 极性翻转一致性实验（Sec 4.2.1）\n\n- 对每张图像同时提问 q\\_f 与 q\\_r，计算 PFC、PFA、TFI、CbW。\n- 目的：筛除“模板重复”模型（高 TFI），确保后续视觉分析可信。\n2. 记忆效应分离实验（Sec 4.2.2）\n\n- 四条件对比：Illusion-Original / Illusion-Perturbed vs Control-Original / Control-Perturbed。\n- 指标：R 值 + 原始/扰动准确率悬崖。\n- 发现：GPT-5 错觉降 87 ppt，对照仅降 44 ppt，R=1.97→归类为“记忆覆盖”；Qwen2.5-3B 对照与错觉降幅均低，R=0.13→归类为“视觉瓶颈”。\n3. 剂量-响应与人类阈值实验（Sec 4.2.3）\n\n- 在 10 级扰动强度 α 上绘制 Acc(α) 曲线；收集人类 95% 检测阈值作参照。\n- 结果：\n– GPT-5 错觉曲线平坦（0–5%），对照曲线 70→25%，证实完全记忆覆盖；\n– Claude-Opus-4.1 错觉曲线仍呈剂量依赖，但斜率低于对照，体现感知-记忆竞争；\n– 人类阈值（红色竖线）处，多数模型在错觉侧已“提前崩溃”，在控制侧仍高于随机，证明失效源于模板检索而非感知极限。\n4. 干预与类别细分实验（Sec 4.2.4）\n4a 视觉提示干预\n\n- 在图像上叠加对齐线/网格，观察 Original vs Perturbed 准确率变化。\n- 结论：13/15 模型 Original 提升（+6.2 pp 平均），但 12/15 模型 Perturbed 下降（-6.9 pp），说明提示强化模板匹配而非促进视觉更新。\n\n4b 系统提示干预\n\n- 追加“忽略先验，仅比较视觉”指令。\n- 旗舰模型出现“全有或全无”模式切换：GPT-5 Original 从 98%→14%，Perturbed 从 2%→66%；小参数 Qwen 模型则两端同步提升，验证弱模板存储可避免模式坍塌。\n\n4c 错觉类别敏感性\n\n- 按 Size/Color/Orientation 三类别计算 Control-Illusion 差距。\n- OpenAI 家族在 Length 类差距高达 33 ppt；Qwen 在 Color 类出现负差距（-6%），进一步佐证不同架构的异质偏差。\n5. 补充实验（Appendix B）\n\n- 跨代一致性：GPT-4o、Claude-3.5-Sonnet 等前代模型重复相同 VI-Probe 流程，证实失效模式跨代稳定。\n- 纯诱导物测试：仅向模型呈现箭头或外围圆环（无比较目标），GPT-5 与 Gemini 仍给出与原图一致答案，量化“纯幻觉”模板偏差。\n\n通过上述实验组合，论文从“语言学鲁棒性筛查”到“记忆-感知量化”，再到“干预-类别细分”，完整揭示了 VLMs 在视觉错觉下的异质失效机制。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接延伸 VI-Probe 的实验与理论框架，无需增加额外数据采集成本即可展开，或仅需最小改动即可生成新的可验证假设：\n\n1. 时序感知探测\n\n- 将静态错觉扩展为 3–5 帧的渐进扰动视频，记录模型在每一帧的预测翻转点，与人类 psychometric function 对比，量化“动态感知延迟”。\n- 可验证：记忆覆盖型模型是否在前 1-2 帧就已锁定模板，视觉瓶颈型模型是否随帧数增加才缓慢更新。\n2. 跨模态注意力擦除（attention erasure）\n\n- 在交叉注意力层对视觉 token 或文本 token 做逐层消融，观察 PFC 与 R 的变化曲线，定位“记忆覆盖”究竟发生在哪一层。\n- 可验证：是否仅需擦除 <10% 的视觉 token 就能让 GPT-5 的 R 值从 1.97 降至 ≈1，从而确认关键视觉线索的层位。\n3. 反事实微调基准\n\n- 利用 VI-Probe 已有的 Original+Perturbed 配对，构造“翻转一致性损失”\n\nmathcal L_(flip) = CE(f(x^O,q_f),; 1-f(x^P,q_f))\n\n在冻结 LLM 部分的情况下只微调视觉编码器，测试是否能在 1-2 个 epoch 内把 R 值压到 <1.1，而标准对比学习不能。\n4. 幻觉强度回归估计\n\n- 把离散 α 扩展为连续变量，训练辅助回归头预测 α 值，考察模型能否显式输出“错觉强度”，而非仅给二元答案。\n- 可验证：记忆覆盖型模型的回归误差是否显著高于视觉瓶颈型，从而量化其“内部标尺”缺失。\n5. 多语言错觉迁移\n\n- 保持视觉图像不变，把问题 q\\_f/q\\_r 翻译成 10 种低资源语言，观察 PFC 下降斜率与语言语料在预训练中的占比是否负相关。\n- 可验证：模板偏差是否主要由高资源语言的先验驱动，低资源语言能否意外提升视觉敏感度。\n6. 细粒度诱导物分解\n\n- 对同一错觉继续参数化“诱导物”与“目标”之间的距离、数量、对比度，建立 3×3 网格实验，用方差分析分解主效应与交互效应。\n- 可验证：对 GPT-5 而言，诱导物数量是否比对比度对 R 值贡献更大，从而确认“语义显著性”而非低层视觉特征主导模板召回。\n7. 推理时干预（test-time scaling）\n\n- 在解码阶段引入“视觉验证链”：先让模型生成理由，再用文本理由作为 query 对图像做二次交叉注意力，迭代 2–3 轮，监测 PFA 提升幅度。\n- 可验证：记忆覆盖型模型是否仅需 1 轮迭代即可把 Perturbed 准确率从 5% 提升到 >50%，而无需重新训练。\n8. 神经-模型对齐\n\n- 同步采集人眼在相同扰动序列下的眼动与 EEG，计算“注视偏移”与“P300 幅度”作为人类翻转标记，与模型每帧预测做时间对齐，得到“脑-模型延迟图”。\n- 可验证：视觉瓶颈型模型是否与人类 EEG 标记更同步，记忆覆盖型模型是否在人脑已翻转后仍保持 2–3 帧的滞后。\n9. 物理参数可解释嵌入\n\n- 用错觉图像的物理真值（如实际长度差、RGB 差值）训练线性探针，预测模型内部视觉 token 的平均池化特征，看能否恢复物理量。\n- 可验证：R<1 的模型是否仍保留可解码的物理信号，而 R>1.5 的模型是否完全丢失，从而判断“感知信号”是否被上层语言先验覆盖。\n10. 自动化错觉生成对抗\n\n- 把 VI-Probe 的扰动 pipeline 接入对抗优化环路，以最大化 R 值为目标，自动生成“最能让模型固执己见”的新错觉图像，再让人类评判是否仍感知为错觉。\n- 可验证：是否存在“对人类无效，对模型高 R”的对抗错觉，从而揭示模型特有的盲点空间。\n\n以上 10 点均可在现有 VI-Probe 数据与代码基础上立即展开，无需重新标注，且每条都对应可度量的新指标或因果结论，适合作为短期后续工作。\n\nQ6: 总结一下论文的主要内容\n\n**VI-Probe：用经典视觉错觉量化 VLMs 是“看见”还是“回忆”**\n\n1. 问题\n大型视觉-语言模型在经典错觉图上常给出“正确”答案，但当错觉诱导因素被反转后仍固执原回答，人类却能立刻感知变化——这究竟是视觉失效还是记忆模板主导？\n\n2. 方法\n\n- 框架 VI-Probe：\n– 27 种经典错觉 × 原始图 x^O + 扰动图 x^P（连续强度 α） + 对照图 x^{OC}/x^{PC}（去诱导） + 提示图 x^{OH}/x^{PH}\n– 语言三问：正向 q\\_f、反向 q\\_r、指令 q\\_I\n- 新指标：\n– Polarity-Flip Consistency (PFC) / Template Fixation Index (TFI) → 测语言模板重复\n– Illusion multiplier R = |ΔAcc\\_illusion| / |ΔAcc\\_control| → 归一化“记忆-感知”贡献\n– 剂量-响应曲线 vs 人类 95% 检测阈值 → 测翻转临界点\n3. 实验与发现\n\n- 15 个最新模型（OpenAI/Anthropic/Google/Qwen）共 9.6 k 样本\n- 记忆覆盖：GPT-5 错觉降 87 ppt，对照仅 44 ppt，R=1.97\n- 感知-记忆竞争：Claude-Opus-4.1 曲线仍随 α 变化，R≈1\n- 视觉瓶颈：Qwen2.5-3B 对照与错觉均低，R=0.13\n- 干预：视觉提示强化模板（Original +6.2%，Perturbed −6.9%）；系统提示让旗舰模型出现“全或无”模式切换（GPT-5 Original −84%，Perturbed +64%）\n- 人类阈值处模型已崩溃，证明失效源于模板检索而非感知极限\n4. 结论\n\n- 响应固执由异质机制导致：记忆覆盖、感知-记忆竞争、视觉处理瓶颈\n- 单点准确率掩盖真相；评测需同时报告 PFC/TFI/R\n- 未来方向：反事实一致性训练、感知优先架构、推理时视觉验证链\n\n数据与代码已开源，支持无限扩展新错觉与即时复现。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Do%20VLMs%20Perceive%20or%20Recall%3F%20Probing%20Visual%20Perception%20vs.%20Memory%20with%20Classic%20Visual%20Illusions，链接是https://arxiv.org/pdf/2601.22150，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22150。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22149v1-1769990387967","title":"DynaWeb: Model-Based Reinforcement Learning of Web Agents","description":"The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.","url":"https://arxiv.org/abs/2601.22149v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:59:07.000Z","metadata":{"arxivId":"2601.22149v1","authors":"Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu","categories":"cs.CL, cs.AI","published":"2026-01-29T18:59:07Z","pdfUrl":"https://arxiv.org/pdf/2601.22149v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22149v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22149v1","rank":8,"domain":"VLM","zh_summary":"由大型语言模型（LLM）和强化学习（RL）驱动的自主网页代理的发展，代表了通用人工智能助手迈出的重要一步。然而，与实时互联网的交互在训练这些代理时带来了严重的障碍，因为这种方式效率低、成本高且风险重重。基于模型的强化学习（MBRL）通过学习环境的世界模型以实现模拟交互，提供了一种有前景的解决方案。本文提出了 DynaWeb，一种新颖的 MBRL 框架，通过与经过训练以预测代理动作下自然网页表示的网页世界模型进行交互来训练网页代理。该模型作为一个合成的网页环境，在其中代理策略可以通过生成大量的回滚动作轨迹来进行“模拟”，从而实现高效的在线强化学习。除了自由策略回滚之外，DynaWeb 还结合了来自训练数据的真实专家轨迹，这些轨迹在训练过程中随机穿插在策略内回滚中，以提高稳定性和样本效率。在具有挑战性的 WebArena 和 WebVoyager 基准测试中进行的实验表明，DynaWeb 一贯且显著地提升了最先进开源网页代理模型的性能。我们的研究结果证明，通过“想象”训练网页代理是可行的，为扩展在线代理强化学习提供了一种可扩展且高效的方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何在不与真实互联网交互的前提下，高效、安全地训练具备在线强化学习能力的网页智能体”这一核心问题。\n具体而言，现有基于大模型的网页智能体虽可通过在线 RL 显著提升鲁棒性与长程决策能力，但直接依赖真实网页交互会带来三大瓶颈：\n\n1. 效率低：每次策略更新都需大量实时页面请求，训练周期长。\n2. 成本高：频繁访问真实站点易触发验证码、限流或封禁，需额外代理与清洗开销。\n3. 风险大：策略探索可能产生不可逆副作用（误下单、删数据、泄露隐私等），难以规模化。\n\n为此，作者提出将“世界模型”从传统推理工具升级为**在线 RL 的训练环境**，通过模型化互联网动力学，使智能体在“想象”中完成策略优化，从而把对真实环境的采样需求降到最低，同时保留在线 RL 的探索与信用分配优势。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均与“网页智能体”或“世界模型”密切相关，但尚未将二者以\\*\\*在线模型强化学习（MBRL）\\*\\*方式深度耦合：\n\n- **网页智能体训练范式**\n- 提示工程与推理框架：ReAct、Cognitive Kernel、MCP 等通过链式思维或结构化提示提升单步决策。\n- 监督/模仿学习：NNetNav、Go-Browse、Explorer、InSTA 等收集或合成高质量轨迹，用 SFT 训练。\n- 在线 RL：WebAgent-R1、WebRL、WorkForceAgent-R1、WebDancer 等直接在真实环境采样，用 GRPO、DAPO 等算法优化，但需大量真实交互，成本高。\n- **世界模型在网页场景的辅助应用**\n- 推理时规划：WebDreamer、WMA、RAP、WKM 用大模型模拟下一步网页状态，供树搜索或价值估计，**仅用于推断阶段**，不更新策略。\n- 离线数据增强：WebEvolver、Explorer 用世界模型生成额外轨迹，再离线蒸馏到策略，**与 on-policy 优化解耦**。\n- **通用世界模型与 MBRL 基础**\n- 视觉/游戏领域：Dreamer、SimPLe、IRLA、Diffusion World Models 等将环境动力学模型嵌入 RL 循环，实现“想象训练”。\n- 文本/决策抽象：RWKM、RAP、Diffusion for World Modeling 等把 LLM 当作隐式环境模型，但尚未在网页这类高动态、部分可观察场景做在线策略优化。\n\n综上，已有工作要么停留在“**推理阶段**”使用世界模型，要么“**离线生成**”伪轨迹后再蒸馏；DynaWeb 首次把**专门训练的网页世界模型**作为**可微环境**，与真实专家轨迹混合，直接用于 on-policy 强化学习，填补了“网页 MBRL”空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 DynaWeb，一套**面向网页任务的在线模型强化学习（MBRL）框架**，把“真实网页交互”替换为“可学习的网页世界模型 + 少量真实专家轨迹”的混合训练流程，核心步骤如下：\n\n1. 训练网页世界模型（WWM）\n\n- 将网页状态表示为**可访问性树**  o_t 。\n- 用监督数据  (o_t, a_t, o_(t+1))  训练大模型  p_φ  先输出**自然语言状态变更描述**  Delta ，再自回归生成下一帧树  hat o_(t+1) ，损失为\n\nL_φ = -∑ log p_φ(r,Delta mid I,o_t,a_t)\n\n- 得到的  p_φ  充当**可反复采样的网页“服务器”**，无需访问真实站点即可展开多步交互。\n2. 想象 rollout：策略与世界模型闭环\n\n- 策略  π_θ  以当前真实或想象观测为输入，采样动作  a_tsim π_θ(· mid hat o_(1:t),h_(1:t-1),a_(1:t-1),q) 。\n- 世界模型立即返回  hat o_(t+1)sim p_φ(· mid hat o_t,a_t,q) ，形成**想象轨迹**\n\nhatτ = (hat o_1,h_1,a_1,dots,hat o_T,h_T,a_T)\n\n- 终止时用模型自评给出稀疏奖励  hat r(hatτ,q)∈0,1 ，作为整条轨迹的回报  G(hatτ) 。\n3. 混合真实专家轨迹\n\n- 每次梯度更新按 50 % 比例**随机混入**来自 NNetNav SFT 数据集的**真实专家轨迹**，与想象 rollout 一起构成 batch。\n- 真实轨迹提供**无偏状态转移与奖励信号**，抑制世界模型幻觉，稳定训练。\n4. 序列级策略优化（GSPO）\n\n- 对每条轨迹计算单序列优势  hat A_i = G(hatτ_i) - bar G 。\n- 采用**序列级重要性权重**\n\ns_i(θ)=exp!l((1) / (|y_i|)∑_k logπ_θ(y_(ik)mid·s){π_(θ_old)(y_(ik)mid·s)}r)\n\n代入 clipped objective\n\nJ_(GSPO)(θ)=mathbb E![(1) / (G)∑_(i=1)^G min!l(s_i(θ)hat A_i, clip(s_i(θ),1!-!varepsilon,1!+!varepsilon)hat A_ir)]\n\n- 实现**长程信用分配**且避免 token 级方差爆炸。\n5. 控制想象深度与误差\n\n- 实验发现 dream length 4–5 步最佳，过长会累积模型误差；同时限制单条 rollout 最大 5 步并早停终端状态，兼顾探索深度与仿真保真度。\n\n通过上述设计，DynaWeb 把**策略优化完全搬到世界模型内部**，仅依赖少量真实轨迹做正则化，从而\n\n- 将真实环境采样需求降低一个数量级；\n- 避免不可逆操作与封禁风险；\n- 在 WebArena 与 WebVoyager 上相对最强基线分别提升 **+16.1 %** 和 **+18.7 %** 成功率，验证“用想象训练网页智能体”的可行性与扩展性。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“**能否用想象 rollout 替代真实网页交互来训练高性能网页智能体**”展开，分三部分：\n\n1. 主实验：WebArena & WebVoyager 成功率对比\n\n- 基准\n– 零样本提示：Llama-3.1-8B-Instruct+CoT、GPT-4o\n– 监督微调：NNetNav、Go-Browse\n– 离线 RL：WebRL\n– 推理时世界模型：ITL（每步模拟选最优动作，不训练策略）\n- 结果\n– WebArena（812 任务，5 站点）\nDynaWeb-8B 平均 SR **31.0 %**，相对离线 RL **+16.1 %**，在 Reddit/GitLab/CMS/Shopping 四域均第一。\n– WebVoyager（643 开放站点）\nDynaWeb 平均 SR **38.7 %**，显著高于次佳基线 WebRL（32.6 %），在 Amazon/BBC/Map 等 9 个站点领先。\n2. 消融实验：关键设计对性能的影响\n\n- Dream length 消融\n固定其余超参，令最大想象步数 1–8 变化；SR 在 4–5 步处峰值，过短欠探索，过长累积幻觉。\n- 真实轨迹比例消融\n0 %–100 % 替换想象 rollout；40 % 真实数据即可显著超越纯 SFT，>60 % 后收益饱和，验证“少量真实轨迹正则化”即可。\n- 世界模型训练必要性\n用**冻结 GPT-oss-120b** 替代微调后的 WWM；WebArena SR 从 31.0 % → 20.9 %，WebVoyager 35.4 % → 28.6 %，表明**显式环境动力学训练**不可或缺。\n3. 训练过程分析\n\n- 训练曲线：混入真实轨迹后，GSPO 的序列级 variance 更小，收敛更快。\n- 错误类型统计：相比纯想象策略，DynaWeb 将“页面元素幻觉”与“无效动作”两类错误各降低约 30 %。\n\n综上，实验系统验证了\n\n- 想象 rollout 可直接用于 on-policy RL；\n- 适度真实数据 + 适度 dream length 是性能拐点；\n- 专门训练的网页世界模型是提升的关键。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步扩展 DynaWeb 的想象力驱动范式：\n\n- **长程高保真世界模型**\n- 研究层级或扩散式架构，将页面视觉截图与 Accessibility Tree 联合建模，降低 5 步后的幻觉累积，实现 ≥20 步稳定 rollout。\n- 引入“页面差异压缩”或 Delta-VAE，仅对变化节点建模，减少自回归长度与误差传播。\n- **多模态世界模型**\n- 同时接受 HTML、CSS、屏幕截图、DOM 变化事件，统一文本与视觉空间，缓解纯文本树对动态 UI（Canvas、SVG、Shadow-DOM）信息丢失问题。\n- 探索视觉-动作对齐预训练，提升对图标、悬浮动画等细粒度交互的预测精度。\n- **可自我批评的世界模型**\n- 让同一模型输出“下一状态 + 预测不确定性/置信度”，策略据此自适应决定是否请求真实环境验证，形成**选择性查询**（selective query）机制，进一步压缩真实交互预算。\n- 引入对比式或能量模型，对世界模型置信度进行校准，防止过度乐观幻觉。\n- **层次化想象与策略分解**\n- 将任务分解为子目标（URL 导航、表单填充、结果提取），训练**高层子目标世界模型**与**低层原子动作世界模型**，实现“子目标→原始动作”两级想象，降低长程信用分配难度。\n- 结合选项框架（Options），在想象空间先做子目标规划，再展开低层动作 rollout。\n- **持续学习与领域增量**\n- 研究世界模型在线微调策略，避免新站点数据导致旧站点灾难性遗忘；可采用 EWC、LoRA-merging 或经验回放。\n- 探索任务-站点混合分布外推（OOD）检测，自动触发世界模型局部更新，保持长期一致性。\n- **安全与可验证想象**\n- 引入形式化约束（如不变量：价格≥0、表单不可越权提交），在世界模型采样阶段即屏蔽非法状态，减少策略探索危险动作。\n- 结合合约或沙箱 API，让世界模型对敏感操作（支付、写库）返回“虚拟确认”而非真实执行，实现零风险训练。\n- **跨环境迁移与元世界模型**\n- 收集 100+ 站点数据，训练**通用网页世界模型**，再针对新站点少量微调，验证“预训练-微调”范式能否快速适应未知站点动力学。\n- 研究元学习（MAML、Prompt-tuning）初始化，使世界模型在新站点仅用 50 条轨迹即可适配。\n- **更细粒度奖励模型**\n- 当前仅用终端 0/1 奖励，可训练**稠密价值模型**对想象中间状态打分，实现单步 TD 学习，进一步缩短真实轨迹需求。\n- 探索人工偏好 + 自动规则混合的奖励函数，缓解稀疏奖励导致的想象轨迹价值估计方差。\n- **系统级优化**\n- 世界模型与策略模型联合蒸馏为单一小模型，降低推理延迟，实现端侧部署。\n- 研究异步并行想象：GPU 集群同时展开数千条 rollout，用 Ray/vLLM 动态调度，提升训练吞吐。\n- **评测扩展**\n- 构建更长 horizon（≥50 步）与多会话（跨天登录、购物车保留）任务集，检验世界模型对**状态持久化与动态会话**的建模能力。\n- 引入对抗评测：在想象与真实环境间随机切换，测量策略对模型误差的鲁棒性，推动世界模型-策略协同优化。\n\n这些方向分别从模型架构、训练策略、安全约束、系统效率到评测维度出发，可进一步释放“想象驱动网页 RL”的潜力，并推广至更广泛的数字界面自动化任务。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **DynaWeb**：首个面向网页任务的**在线模型强化学习（MBRL）框架**，用“想象”替代昂贵、高风险的真实互联网交互，实现高效、安全的智能体训练。\n\n1. 问题\n在线 RL 可提升网页智能体的鲁棒性与长程决策，但直接访问真实站点效率低、成本高、风险大，难以规模化。\n\n2. 解法\n\n- 训练**网页世界模型**  p_φ ，输入当前可访问性树  o_t  与动作  a_t ，自回归输出状态变更描述  Delta  并生成下一帧树  hat o_(t+1) ，充当可无限采样的**虚拟网站**。\n- 策略  π_θ  与世界模型闭环，展开多步**想象轨迹**  hatτ ，用模型自评稀疏奖励  hat r(hatτ,q)  进行**序列级策略优化（GSPO）**。\n- 训练中**随机混入 40 % 真实专家轨迹**，提供无偏状态转移，抑制幻觉并稳定学习。\n- 控制想象深度 4–5 步，平衡探索与仿真保真。\n3. 实验\n\n- WebArena（812 任务）：DynaWeb 平均成功率 **31.0 %**，相对最强离线 RL 基线 **+16.1 %**。\n- WebVoyager（643 开放站点）：**38.7 %**，显著优于 GPT-4o、WebRL 等。\n- 消融：真实数据比例、dream length、世界模型训练必要性均被验证为性能拐点。\n4. 结论\nDynaWeb 首次证明**专门训练的世界模型可直接用于 on-policy 网页 RL**，在零风险、低成本的前提下持续提升开源智能体，为“想象驱动”的通用数字代理提供可扩展路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DynaWeb%3A%20Model-Based%20Reinforcement%20Learning%20of%20Web%20Agents，链接是https://arxiv.org/pdf/2601.22149，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22149。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2509.21670v4-1769990387967","title":"MORPH: PDE Foundation Models with Arbitrary Data Modality","description":"We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters, MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.","url":"https://arxiv.org/abs/2509.21670v4","source":"ArXiv Domain","timestamp":"2025-09-25T22:38:36.000Z","metadata":{"arxivId":"2509.21670v4","authors":"Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Alexander Scheinker, Diane Oyen, Nathan Debardeleben, Earl Lawrence, Ayan Biswas","categories":"cs.CV, cs.AI, cs.LG, physics.comp-ph","published":"2025-09-25T22:38:36Z","pdfUrl":"https://arxiv.org/pdf/2509.21670v4.pdf","abstractUrl":"https://arxiv.org/abs/2509.21670v4","coolPaperUrl":"https://papers.cool/arxiv/2509.21670v4","rank":9,"domain":"VLM","zh_summary":"我们介绍了MORPH，一种与模态无关的自回归偏微分方程（PDE）基础模型。MORPH构建在卷积视觉变换器骨干上，能够无缝处理不同分辨率下不同数据模态（1D-3D）的异构时空数据集，以及混合标量和向量分量的多个场。该架构结合了（i）分量卷积，联合处理标量和矢量通道以捕捉局部交互，（ii）场间交叉注意力，建模并选择性地在不同物理场之间传递信息，（iii）轴向注意力，沿单个空间和时间轴分解完整的时空自关注，以降低计算负担同时保持表达力。我们在多样化的偏微分方程数据集上预训练多个模型变体，并评估其向下游预测任务的转移。通过全模型微调和参数高效的低秩适配器，MORPH 在从零训练模型中表现优于模型。通过广泛的评估，MORPH与强有力的基线和最新最先进的模型相匹配甚至超越。这些能力共同构成了从科学观测异质性和多模态特性中学习的灵活且强大的骨干，开辟了通往可扩展且数据高效科学机器学习的道路。源代码、数据集和模型均公开于 https://github.com/lanl/MORPH。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**科学机器学习（SciML）中构建通用偏微分方程（PDE）基础模型**所面临的两大核心瓶颈：\n\n1. **数据异构性**\n现有PDE数据集在空间维度（1D–3D）、分辨率、标量/矢量场混合、物理场数量等方面差异巨大，且规模可达TB级。传统方法需对数据进行填充或重采样以统一形状，导致计算与存储开销爆炸，且难以纳入3D或高分辨率2D数据（自注意力复杂度 O(L^2) 随序列长度 L 二次增长）。\n\n2. **部分可观测性**\n实验或仿真往往只能提供稀疏、低维的观测（如1D探针、2D切片），而真实物理过程是3+1维连续时空演化。现有基础模型多假设2D笛卡尔网格、固定场类型，无法直接利用1D或3D数据，造成信息浪费与泛化盲区。\n\n为此，作者提出**MORPH**——一个**形状无关（shape-agnostic）**、自回归的PDE基础模型，其目标是在**不重新设计架构**的前提下，统一处理任意维度、任意分辨率、混合标量/矢量场的多物理PDE数据，实现：\n\n- **预训练阶段**：从六类异构数据（1D可压缩NS、2D不可压缩NS、2D反应-扩散、2D浅水、3D可压缩NS、3D磁流体）中学习通用演化算子。\n- **下游任务**：通过全参数微调或LoRA高效适配，在七项新任务（1D Burgers、1D反应-扩散、2D Kolmogorov流、3D湍流冷却等）上零样本或少量样本即可超越从头训练的专用模型。\n\n总结：MORPH试图用**单一架构**解决“**如何在部分、异构、多模态的PDE数据上预训练，并低成本迁移到任意下游物理问题**”这一SciML基础模型核心挑战。\n\nQ2: 有哪些相关研究？\n\n论文第2节“Related Work”将相关研究划分为两大主线，并指出它们与MORPH的差异。可归纳为以下四类：\n\n1. 专用PDE代理模型（Standalone Surrogates）\n\n- 数据驱动：3D-CNN、ConvLSTM、DCGAN、GNN\n- 物理嵌入：PINNs、神经算子（DeepONet、FNO、WNO、LNO、PINO）\n- 生成式：GAN、扩散模型（score-based）、潜空间演化（VAE+Transformer/LSTM/DeepONet）\n共同点：针对**单一PDE族**重新训练，无法跨物理迁移。\n2. 早期迁移/多任务尝试\n\n- 深度迁移算子学习（Deep Transfer Operator Learning）\n- 上下文算子学习（In-context Operator Learning）\n局限：仅在**同维度、同网格**任务间迁移，未解决异构数据。\n3. 近期PDE基础模型（Foundation Models）\n\n| 模型 | 可处理维度 | 预训练数据 | 关键限制 |\n| --- | --- | --- | --- |\n| MPP | 仅2D | 2D多物理 | 固定2D网格，无法1D/3D |\n| DPOT | 仅2D | 2D+时序10步 | 需10步上下文，不支持3D |\n| POSEIDON | 仅2D | 2D多尺度 | 需Swin窗口，形状固定 |\n| PDEformer | 仅1D | 1D方程 | 无法处理2D/3D |\n| 共同瓶颈： |\n\n- 维度、分辨率、标量/矢量场**需重新设计架构**；\n- 3D体积数据导致序列长度爆炸，被显式排除在预训练之外；\n- 1D实验信号（探针、地震道）被强制填充到2D，浪费算力。\n4. 与MORPH最相关的“异构”讨论\n\n- Ye et al. 2024、McCabe et al. 2024 均指出“**扩展到3D及混合维度**”是开放难题；\n- MORPH首次把**1D–3D、任意分辨率、混合场**统一进**同一套权重**，并通过轴向注意力将复杂度从 O((TDHW)^2) 降至 O(T^2+D^2+H^2+W^2) ，填补了这一空白。\n\n简言之，既有研究要么“专模专用”，要么“二维打天下”，而MORPH首次系统性地把**形状无关、多维度、多物理、部分可观测**作为第一优先级设计目标。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“一格式、三机制、四变体”的系统设计，把异构、部分可观测的 PDE 数据塞进同一个自回归 Vision Transformer，实现“任何维度、任何分辨率、任何场类型”端到端预训练与低成本迁移。具体方案如下：\n\n1\\. 统一格式：UPTF-7\n\n- 语义保留的 7D 张量模板\n (N,T,F,C,D,H,W) \n-  F ：物理场数目（可运行时变化）\n-  C ：场分量数（标量=1，矢量≥1；混合时广播到最大  C ）\n- 零填充仅发生在**GPU 端 mini-batch 加载瞬间**，原始 TB 级数据仍按原生格式存盘，避免存储爆炸。\n- 结果：1D 探针、2D 切片、3D 体数据可同批训练，无需改图、改代码或重采样。\n\n2\\. 三大架构机制\n\n| 机制 | 作用 | 实现要点 | 复杂度/优势 |\n| --- | --- | --- | --- |\n| Component-wise 3D 卷积 | 局部空间关联+标/矢混合 | 只在  C  维滑动，输出  F  张特征图； 1×1×1  升维→ 3×3×3  深度卷积 | 参数随  F  线性增长，诱导局部偏置，提升小样本效率 |\n| Inter-field Cross-Attention | 跨物理场信息筛选与融合 | 单查询向量  q∈R^(E)  对  F  个场做 MH-CrossAttn，输出单一融合场 |  O(F)  替代  O(F^2) ，抑制尺度失配带来的伪激活；运行时场数可变 |\n| 4D 轴向注意力 | 全局时空依赖，内存可控 | 将完整自注意力拆成四支独立 1D-MHA：时间、深度、高度、宽度；残差相加 | 复杂度从  O((TDHW)^2)  降到  Ol(T^2+D^2+H^2+W^2r) ，支持 3D 高分辨批量训练 |\n\n3\\. 参数高效迁移：LoRA\n\n- 仅对 Attention 与 MLP 的稠密矩阵引入低秩分解\n W'=W_0+(α) / (r)BA,;A∈R^(r× k),B∈R^(d× r) \n- MORPH-L（480 M）中仅 77 M 可训练，即可匹配全参数微调效果，首次把 LLM 的“冻结主干+小插件”范式搬到 PDE 基础模型。\n\n4\\. 四尺度模型变体\n\n| 模型 | 参数量 | 注意力维度 | 头数 | 深度 | 适用场景 |\n| --- | --- | --- | --- | --- | --- |\n| TI (Tiny) | 7 M | 256 | 4 | 4 | 单卡/快速实验 |\n| S (Small) | 30 M | 512 | 8 | 4 | 单卡/生产 |\n| M (Medium) | 126 M | 768 | 12 | 8 | 多卡 |\n| L (Large) | 480 M | 1024 | 16 | 16 | 多节点+LoRA |\n\n5\\. 训练与数据策略\n\n- **平衡任务采样**：按轨迹数  N_i  反比加权，避免小数据集被遗忘。\n- **ReVIN 归一化**：预计算全局均值/方差，训练与推理阶段可逆，消除分布漂移。\n- **流式+自定义分片**：六数据集并行 DataLoader，DDP 下按样本级取模分片，支持 TB 级数据外存训练。\n- **AR(1) 自回归**：严格遵循初值问题语义，减少曝光偏差；也可扩展为 AR(p) 或变阶。\n\n6\\. 效果验证\n\n- **零样本**：在预训练集上直接推理，平均误差优于或接近专用模型。\n- **全样本微调**：7 个下游任务全部刷新 SOTA（NRMSE/VRMSE）。\n- **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 256 条轨迹（<1 %）即可打败全量训练的独立模型。\n- **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调 200 epoch，MSE/RMSE 低于 122 M 的 DPOT-FM-M 与 158 M 的 POSEIDON-FM-B。\n\n综上，论文用“UPTF-7 统一表示 + 卷积-交叉-轴向三重高效算子 + LoRA 插件式迁移”组合拳，把以往只能分而治之的 1D/2D/3D、标量/矢量、多物理问题纳入**同一套权重**，在数据与算力双重稀缺场景下仍能实现跨物理、跨维度、跨分辨率的强泛化，从而系统性地解决了 PDE 基础模型的异构与部分可观测难题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“**异构预训练 → 零样本/少样本迁移 → 数据与算力稀缺场景**”设计了四层实验，覆盖 13 个数据集、4 个模型尺度、3 类对照组。核心结果汇总如下（行内公式统一用  ...  格式）。\n\n1\\. 预训练（Pretraining）\n\n**目的**：验证 MORPH 能否在 6 类异构数据上同时收敛且不遗忘。\n\n| 数据集 | 维度 | 场类型 | 轨迹数 | 时间步 | 空间分辨率 |\n| --- | --- | --- | --- | --- | --- |\n| 1D-CFD | 1D | 1×矢量+2×标量 | 10 k | 100 | 1024 |\n| 2D-DR | 2D | 2×标量 | 1 k | 100 |  128^2  |\n| 2D-CFD-IC | 2D | 1×矢量 | 80 | 1000 |  512^2  |\n| 2D-SW | 2D | 1×标量 | 1 k | 100 |  128^2  |\n| 3D-CFD | 3D | 1×矢量+2×标量 | 200 | 21 |  128^3  |\n| 3D-MHD | 3D | 2×矢量+1×标量 | 97 | 100 |  64^3  |\n\n- **训练步数**：TI/S 200k，M 100k，L 125k 梯度步。\n- **采样策略**：按轨迹数  N_i  反比加权，3D-MHD 被采样概率 0.31，1D-CFD 0.08。\n- **监控指标**：训练/验证 MSE，无早期停止（loss 仍在下降）。\n\n2\\. 零样本评估（Zero-shot）\n\n**设置**：直接用预训练权重在预训练集上做单步预测，不更新参数。\n\n| 指标 | 1D-CFD | 2D-DR | 2D-CFD-IC | 2D-SW | 3D-CFD | 3D-MHD |\n| --- | --- | --- | --- | --- | --- | --- |\n| NRMSE ↓  | 0.0506 | 0.1112 | 0.0858 | 0.0045 | 0.1161 | – |\n| VRMSE ↓  | – | – | – | – | – | 0.285 |\n\n- MORPH-FM-S 在 4/6 数据集上**优于或接近**专用 standalone 模型；\n- 首次展示 1D→3D 跨维度零样本推理能力（对比基线只能 2D）。\n\n3\\. 下游微调（Full-shot Fine-tuning）\n\n**7 个新物理任务**，100–150 epoch，全参数或 LoRA。\n\n| 数据集 | 维度 | 任务特色 | 最优结果 |\n| --- | --- | --- | --- |\n| 1D-DR | 1D | 反应-扩散前沿 | NRMSE= 1.25×10^(-3) （TI） |\n| 1D-BE | 1D | Burgers 激波 | NRMSE=0.0302（S） |\n| 2D-CFD | 2D | 可压缩 NS  512^2  | NRMSE=0.0423（L-LoRA） |\n| 2D-GSDR | 2D | Gray-Scott 斑图 | VRMSE=0.00725（S） |\n| 2D-FNS-KF | 2D | Kolmogorov 强迫湍流 | MSE=0.00089（S） |\n| 3D-CFD-Turb | 3D | 湍流  64^3  | NRMSE=0.0713（M） |\n| 3D-TGC | 3D | 自引力冷却 | VRMSE=0.0395（L-LoRA） |\n\n- **全部 7 项**均取得**新 SOTA**（对比 FNO、U-Net、MPP、DPOT、POSEIDON）。\n- LoRA 仅调 77 M 参数即超越全参数 M 模型，验证参数高效迁移。\n\n4\\. 数据与算力稀缺实验\n\n4.1 少数据\n\n- **1D-DR**：用 25 % 轨迹微调 100 epoch，RMSE 已**低于**全量训练的 standalone-TI。\n- **2D-FNS-KF**：用 256 条轨迹（<1 %）微调 100 epoch，RMSE 同样**低于**全量 standalone-S。\n\n4.2 少算力\n\n- 固定 128 条轨迹、200 epoch，对比同等微调预算下的 SOTA 基础模型：\n\n| 模型 | 参数量 | MSE | RMSE |\n| --- | --- | --- | --- |\n| DPOT-FM-M | 122 M | 0.0301 | 0.176 |\n| POSEIDON-FM-B | 158 M | 0.0017 | 0.0412 |\n| MORPH-FM-S | 30 M | 0.00162 | 0.0401 |\n\n- **4× 更轻参数**仍取得最低误差，证明在**数据+算力双稀缺**场景优势最大。\n\n5\\. 长时程自回归 rollout\n\n- **Shallow-Water 10 步**：MORPH-SS-S NRMSE 比 TI 低 28 %，无发散。\n- **FNS-KF 10 步**：MORPH-FM-S 每步 MSE 均低于 FM-TI，误差累积缓慢。\n\n6\\. 消融与系统实验\n\n- \\*\\* patch size\\*\\*：8×8×8 在 3D 显存与精度间最佳；降至 4×4×4 序列长度翻倍，训练时间↑2.3×，收益<2 %。\n- **轴向 vs 全时空注意力**：3D-CFD batch 下显存节省 5.7×，速度↑3.2×，NRMSE 差距<1 %。\n- **平衡采样 vs 均匀采样**：无加权时 3D-MHD NRMSE 恶化 18 %，证实反比加权必要性。\n\n7\\. 可重复性\n\n- 代码、权重、训练日志已开源（GitHub + HuggingFace）。\n- 提供 PyTorch 数据加载、ReVIN 统计、LoRA 配置、rollout 脚本，确保指标可复现。\n\n综上，实验从**预训练收敛 → 零样本泛化 → 全量微调 SOTA → 数据/算力稀缺优势 → 长时稳定性**五个层面，系统验证了 MORPH 在异构 PDE 数据上的通用性与实用性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 MORPH 框架的“直接延伸”或“范式升级”，均围绕其**形状无关、部分可观测、参数高效**三大特性展开，具有明确的研究价值与可行性。\n\n1\\. 维度与几何外推\n\n- **非笛卡尔网格**：曲线坐标、 immersed boundary、 unstructured tet-grid。\n→ 把轴向注意力泛化到“图-轴向混合”：先按节点局部连通做图注意力，再沿用户自定义的“几何轴”做轴向注意力，保持  O(N)  复杂度。\n- **4D+ 时空-参数空间**：将 PDE 参数  (μ)  视为第 5 维，实现“一推理直接扫参数平面”，迈向**超参数快速寻优**。\n\n2\\. 物理一致性层\n\n- **守恒-辛-对称嵌入**：在轴向注意力后接入**显式守恒投影层**（ rho,v,E  Moment Constraint）或**辛子网络**，确保 rollout 全程质量、动量、能量误差有界。\n- **可微分边界条件**：把 BC 编码为可学习的软约束项  L_(BC) = ∫_(∂Omega) |u_θ(x,t) - g(x,t)|^2 ，随注意力反向传播，实现“零样本”边界迁移。\n\n3\\. 多保真与实验-仿真融合\n\n- **保真度轴 (Fidelity Axis)**：在 UPTF-7 再增一维  M （Mesh-level 或 Noise-level），同批次混入低分辨/含噪实验信号与高分辨仿真，用**轴向注意力天然的分支结构**做保真度-自适应加权。\n- **物理-数据双驱动微调**：当实验只测到 1D 探针时，把仿真 3D 场作为 teacher，通过**轴向知识蒸馏**让模型在探针位置输出与实验一致，同时保持 3D 场预测合理。\n\n4\\. 参数高效范式升级\n\n- **LoRA 组合路由**：\n- 冻结共享主干，为每个物理系**自动学习路由权重**  w_i  组合多个 LoRA 插件：\n\nDelta W = ∑_(i=1)^k w_i (α_i) / (r_i) B_i A_i\n\n- 实现“一个主干 + 数十个插件”覆盖全物理库，插件总大小 <10 % 主干。\n- **渐进式扩展**：\n- 先训练 7 M 模型 → 用 function-preserving 放大到 30 M → 126 M → 480 M，对比“直接训练大模型”在数据-算力稀缺时的样本效率提升。\n\n5\\. 长时程与混沌行为\n\n- **Koopman 轴向注意力**：在时间轴引入**线性 Koopman 层**  z_(t+1) = K z_t ，再残差接非线性轴向注意力，缓解多步 rollout 的 Lyapunov 漂移。\n- **自回归策略搜索**：目前固定 AR(1)。可探索**可变阶调度**  p_t sim π_θ(error_t) ，用强化学习最小化累积误差，实现“自适应步长”。\n\n6\\. 逆向与不确定性\n\n- **Bayes-LoRA**：把 LoRA 矩阵  A,B  设为**随机变量**，采用 SVGD 或 NF 推断后验，实现**轻量级不确定性量化**；仅需  2r(d+k)  个采样参数而非全模型。\n- **稀疏观测反演**：利用形状无关编码器，把稀疏探针数据直接映射到潜码，再解码完整 3D 场，实现**实验-在线重构**。\n\n7\\. 跨模态与多物理新基准\n\n- **实验-仿真混合基准**：发布一套同时包含\n- 1D 探针（压力-时序）\n- 2D PIV 切片\n- 3D LES 仿真\n的公开数据集，推动社区研究“部分可观测”设定。\n- **多物理耦合新任务**：\n- 辐射-磁流体 (radiation-MHD)\n- 流-固-热耦合\n- 等离子体-壁面相互作用\n验证 MORPH 在“真实多物理”下的零样本能力。\n\n8\\. 系统与硬件协同\n\n- **轴向注意力核融合**：将四维轴向 MHA 写成单一 CUDA kernel，利用共享内存缓存  D,H,W  切片，实测可再提速 1.8–2.2×。\n- **边缘部署**：把 TI 模型 (<30 MB) 编译到 TensorRT-Lite，在 NVIDIA Jetson 上实现**毫秒级 128³ 3D 场在线预测**，服务实验闭环控制。\n\n9\\. 理论基础\n\n- **形状无关通用逼近定理**：证明对任意  d∈1,2,3  与任意可测域  Omega⊂R^d ，MORPH 编码器-解码器对  M_θ: L^2(Omega)to L^2(Omega)  在恰当 Sobolev 范数下稠密。\n- **轴向注意力复杂度-精度权衡**：给出  O(T^2+D^2+H^2+W^2)  与 full  O((TDHW)^2)  的谱范数误差上界，指导用户按硬件自动选择分支数。\n\n10\\. 伦理与双用\n\n- **湍流武器化风险**：建立“物理模型权重出口”审查清单，对高保真 3D 可压缩 NS 权重加**水印+差分隐私**，防止直接用于超高速武器设计。\n\n简言之，MORPH 把“维度墙”和“数据墙”打通后，下一步可在**几何、物理、不确定性、系统、理论**五个层面继续深挖，形成“**形状无关+物理严格+参数插件+边缘实时**”的下一代科学基础模型生态。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **MORPH**——首个**形状无关**的偏微分方程（PDE）基础模型，用统一架构端到端处理 **1D–3D、任意分辨率、混合标量/矢量场**的异构科学数据，并在**零样本、全样本、数据-算力稀缺**场景下均取得 SOTA 或可比性能。核心内容可概括为 **“一格式、三机制、四变体、五验证”**：\n\n1️⃣ 一格式：UPTF-7\n\n- 7D 张量  (N,T,F,C,D,H,W)  把任意 PDE 数据语义化封装，**磁盘保持原生格式**，GPU 端按需零填充，解决 TB 级存储与维度差异。\n\n2️⃣ 三机制\n\n| 机制 | 作用 | 复杂度 |\n| --- | --- | --- |\n| Component-wise 3D 卷积 | 联合提取标/矢量局部特征 |  O(F)  |\n| Inter-field Cross-Attention | 跨物理场选择性融合为单一场 |  O(F)  |\n| 4D 轴向注意力 | 时间+空间三轴分别 1D-MHA，全局感受野 |  O(T^2+D^2+H^2+W^2)  |\n\n3️⃣ 四变体\n\n| 模型 | 参数量 | 适用 |\n| --- | --- | --- |\n| TI | 7 M | 单卡快速实验 |\n| S | 30 M | 生产级 |\n| M | 126 M | 多卡 |\n| L | 480 M | 多节点+LoRA |\n\n4️⃣ 五验证\n\n1. **异构预训练**：6 数据集（1D–3D）同时收敛，无灾难遗忘。\n2. **零样本推理**：在预训练集直接推理，**4/6 任务**优于专用模型。\n3. **下游微调**：7 项新物理任务**全部刷新 SOTA**（NRMSE/VRMSE）。\n4. **数据稀缺**：1D-DR 仅用 25 % 数据、2D-FNS-KF 用 <1 % 数据即**超越全量训练模型**。\n5. **算力稀缺**：30 M 参数的 MORPH-FM-S 在 128 条轨迹微调下，MSE/RMSE **低于 122 M 的 DPOT 与 158 M 的 POSEIDON**。\n\n5️⃣ 贡献一句话\n\nMORPH 用**统一架构**打通 1D–3D、任意分辨率、多物理场的壁垒，首次在**部分可观测、TB 级异构数据**上实现**低成本预训练+参数高效迁移**，为科学机器学习提供**通用、轻量、强泛化**的 PDE 基础模型基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是MORPH%3A%20Shape-agnostic%20PDE%20Foundation%20Models，链接是https://arxiv.org/pdf/2509.21670，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.21670。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"arxiv-2601.22143v1-1769990387967","title":"JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion","description":"Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.","url":"https://arxiv.org/abs/2601.22143v1","source":"ArXiv Domain","timestamp":"2026-01-29T18:57:13.000Z","metadata":{"arxivId":"2601.22143v1","authors":"Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or","categories":"cs.GR, cs.CV","published":"2026-01-29T18:57:13Z","pdfUrl":"https://arxiv.org/pdf/2601.22143v1.pdf","abstractUrl":"https://arxiv.org/abs/2601.22143v1","coolPaperUrl":"https://papers.cool/arxiv/2601.22143v1","rank":10,"domain":"VLM","zh_summary":"音视频基础模型（Audio-Visual Foundation Models）经过预训练，可以联合生成音频和视觉内容，最近在多模态生成和编辑方面展现出了前所未有的能力，为下游任务开辟了新的机会。在这些任务中，视频配音尤其可以从这种先验知识中获益，但大多数现有解决方案仍依赖复杂的、任务特定的流程，在实际应用中表现不佳。在本研究中，我们提出了一种单模型方法，通过轻量级的LoRA将基础音视频扩散模型（audio-video diffusion model）用于视频到视频的配音。该LoRA使模型能够以输入音视频为条件，同时生成翻译后的音频和同步的面部动作。为了训练该LoRA，我们利用生成模型自身来合成同一说话者的多语言配对视频。具体而言，我们生成单个片段中语言切换的多语言视频，然后对每一半的视频进行面部和音频修补，以匹配另一半的语言。通过利用音视频模型丰富的生成先验，我们的方法在保留说话者身份和唇形同步的同时，对复杂动作和真实场景动态具有良好的鲁棒性。实验结果表明，与现有的配音流程相比，我们的方法能够生成高质量的配音视频，具有更高的视觉保真度、唇形同步性和鲁棒性。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该工作旨在解决**真实世界视频配音**中因“音频-视觉分离处理”而导致的三大核心缺陷：\n\n1. **唇形-语音错位**\n传统管线先翻译音频，再用音频驱动人脸重绘。两者独立优化，时长、节奏或口型稍有偏差即出现明显不同步。\n\n2. **身份-发音权衡**\n纯音频模型为保音色会泄露源语言韵律；视觉模型为保口型会牺牲音色。二者无法在同一框架内同时满足“说话人声音不变”与“目标语言发音正确”。\n\n3. **场景语义断裂**\n分离式方案把语音从背景声中“抠出”再贴回。若翻译后时长变化，原背景声（狗吠、笑声、关门等）会错位或丢失，破坏视听一致性。\n\n为此，作者将配音任务重新表述为**联合音频-视觉条件生成问题**：在单个预训练的音视扩散模型上，用轻量级 LoRA 微调，使同一次去噪过程同时生成目标语言的语音与与之精准同步的唇形，并保留说话人身份、表情、姿态及环境声事件。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归为三类，并指出各自与本文方法的差异：\n\n- **音频-视觉基础模型**\n- Ovi、UniAVGen、Seedance 1.5 pro、MM-Sonate、Syncphony 等最新 DiT 结构通过双流或孪生骨干+非对称跨模态注意力实现联合去噪，但均未针对“配音”这一需要保留身份、仅局部改写唇音的细粒度编辑任务进行专门设计。\n- 本文直接在这些统一先验之上引入 In-Context LoRA，实现轻量级、上下文感知的局部改写。\n- **音频驱动说话人视频生成**\n- 早期 Wav2Lip 用同步判别器，后续 MuseTalk / LatentSync 等基于“遮唇+补绘”策略，对大幅姿态、遮挡或非人脸场景容易失败；InfiniteTalk、OmniSync、X-Dub 等用 Video-DiT 避免显式遮罩，但仍只改视觉，音频需外接 TTS。\n- 本文首次在同一模型内端到端联合生成目标语音与对应唇形，无需遮罩、关键点或级联模块。\n- **零样本语音克隆**\n- CosyVoice、OpenVoice、IndexTTS2、SSPO 等利用神经编解码器或 LLM 实现时长对齐的零样本 TTS，然而仅处理音频，无法根据视觉语境调整语速、停顿与副语言事件。\n- 本文利用视觉流信息，使笑声、叹息、背景声与口型/动作同步变化，突破了纯音频方法无法感知的“视听语义一致”瓶颈。\n\nQ3: 论文如何解决这个问题？\n\n论文把“视频配音”重新定义为**一次受约束的联合音频-视觉生成任务**，通过三步实现：\n\n1. 用基础模型自身合成“身份一致的多语配对数据”\n\n- 先让 LTX-2 生成“同一人前半句说 A 语言、后半句说 B 语言”的连续镜头。\n- 将前半句的音频与嘴部 latent 完全加噪，用后半句的全局视听上下文+翻译文本作为条件，重新去噪，得到“画面/音色/背景均一致，仅语言不同”的伪配对样本。\n- 为消除“嘴部 latent 泄漏”，提出 Latent-Aware Fine Masking；为增加唇形区分度，引入夸张字母级发音的 Lip Augmentation，从而解决身份-发音权衡与“mumbling”问题。\n2. 在预训练 AV-DiT 上注入轻量级 IC-LoRA\n\n- 仅在自注意力与交叉注意力层插入秩-128 的可训练矩阵，冻结主干。\n- 训练目标沿用流匹配损失\n\nL_(FM)=E_(t,x_0,x_1,c)[|v_θ(x_t,t,c,LoRA)-(x_1-x_0)|^2]\n\n使 LoRA 学会“依据文本提示，把源视听流改写成目标语言，同时保持身份、姿态、环境声”。\n3. 推理阶段一次去噪完成配音\n\n- 将原视频作为上下文，目标语文本作为条件，联合生成新音频与新视频帧；无需人脸检测、遮罩或后对齐。\n- 通过 Modality-Isolated Cross-Attention 强制“噪声音频仅查噪声视频”，阻断跨模态泄漏，确保音画同步、时长自动对齐，且背景事件（狗吠、笑声）与画面动作同步保留。\n\nQ4: 论文做了哪些实验？\n\n实验从**定量测评、用户主观评价、可视化对比、消融分析**四个层面展开，覆盖“标准场景+野外复杂场景”两套基准。\n\n1 数据集与基准\n\n| 层级 | 来源 | 规模 | 特点 |\n| --- | --- | --- | --- |\n| Standard | HDFT + TalkVid | 150 对 | 正脸、干净声学、5 语种互译 |\n| Challenging | YouTube 电影片段 25 + 合成场景 25 | 50 对 | 侧脸、遮挡、低照度、非人类、剧烈动作 |\n\n2 对比方法\n\n- **视觉配音**：LatentSync、MuseTalk（均外挂 CosyVoice 音频）\n- **音频配音**：CosyVoice、OpenVoice\n- **商业系统**：HeyGen（闭源）\n\n3 定量指标\n\n| 维度 | 指标 | 含义 |\n| --- | --- | --- |\n| 视频质量 | Succ ↑ | 生成成功率（检测到脸并输出） |\n| CSIM ↑ | 身份保留（ArcFace 余弦） |\n| FID ↓ | 单帧视觉逼真度 |\n| FVD ↓ | 时序一致性 |\n| MAR Div ↑ | 嘴部开合多样性 |\n| 音频质量 | Dur-Err ↓ | 音频-视频时长误差 |\n| V-SIM ↑ | 说话人音色相似度 |\n| Int-Corr ↑ | 音量包络一致性 |\n| WER ↓ | 目标语言字错率 |\n| 音画同步 | ASync ↓ | SyncNet 全局偏移（帧） |\n\n4 主要结果\n\n- **Standard 基准**\n- 100 % Succ；FVD 131.88（最低）；CSIM 0.8471；ASync 2.21 帧≈真实视频水平。\n- **Challenging 基准**\n- 仍保持 100 % Succ；FVD 353.54（远低于 LatentSync 758.66）；侧脸/遮挡/非人类场景下其他方法出现 ERROR 或“复制粘贴”失败。\n- **音频时长对齐**\n- Dur-Err 仅 0.046 s（CosyVoice 1.96 s），自动压缩/拉伸语音以匹配原镜头，避免“倒带”伪影。\n\n5 用户研究\n\n25 名受试者 × 18 段视频 × 3 项指标（唇同步、文本忠实度、整体偏好）→ 450 组 pairwise。\n**Ours 在全部维度显著优于 LatentSync、HeyGen**（p < 0.01）。\n\n6 消融实验\n\n| 变体 | ID-SIM | V-SIM | WER | 结论 |\n| --- | --- | --- | --- | --- |\n| Full | 0.65 | 0.56 | 0.27 | 平衡身份-可懂度 |\n| w/o LoRA | 0.76 | 0.84 | 0.98 | 高相似但完全不听写→失败 |\n| w/o Lip-Aug | 0.72 | 0.68 | 0.52 | 唇形多样性低，仍部分复制 |\n\n7 可视化对比\n\n- **时长对齐**：基线出现“倒带”或长度不符，本方法帧级同步。\n- **非语音事件**：狗吠、咀嚼声与画面动作同步保留；HeyGen 等直接丢失。\n- **极端姿态**：侧脸、遮挡、卡通人物，基线检测失败或贴图错位，本方法稳定生成。\n\n8 结论\n\n定量、主观、可视化与消融四项实验一致表明：\n统一音视扩散模型 + IC-LoRA 在**同步精度、身份保持、野外鲁棒性**上均优于现有级联管线。\n\nQ5: 有什么可以进一步探索的点？\n\n- **身份-音色解耦**\n当前方法在部分样本仍出现音色漂移。可引入全局说话人向量或分解式 Codec，显式解耦“语言内容”与“说话人风格”，实现更稳定的零样本音色保持。\n\n- **长时序与多角色场景**\n模型仅处理 121 帧（≈5 s）。对电影长镜头、多人对话、快速角色切换，需要：\n\n- 层级或滑动窗口式生成；\n- 角色 ID 记忆机制，避免交叉串音；\n- 情节级上下文提示，保持语义连贯。\n- **实时与压缩**\n全帧 VAE + DiT 计算量大。可探索：\n\n- 低秩近似或蒸馏实现 25 FPS+ 实时；\n- 音频与视频双码本量化，降低 latent 带宽。\n- **更丰富的副语言控制**\n笑声、叹息、耳语等仅由视觉隐式触发。可引入情感或副语言标签作为显式条件，实现“情绪可控”的跨语言配音。\n\n- **非人声与虚拟形象**\n卡通、CG、动物说话等数据稀缺。结合参数化角色模型或 3D 先验，可合成大规模非人类配对数据，提升对虚拟形象的泛化。\n\n- **多模态安全与伦理**\n高保真语音克隆+唇同步易被滥用。需同步研究：\n\n- 对抗式水印（音视频联合隐藏签名）；\n- 检测器同步升级，识别合成配音内容。\n- **端到端翻译**\n当前依赖外部文本翻译。将机器翻译模块融入扩散流程，实现“语音-文本-视听”三模态联合优化，可减少级联误差并支持端到端训练。\n\nQ6: 总结一下论文的主要内容\n\n**一句话总结**\n本文提出 JUST-DUB-IT：在单一预训练音视扩散模型上插入轻量级 IC-LoRA，把“视频配音”重新定义为**联合音视条件生成任务**，一次去噪同时输出目标语言语音与同步唇形，无需人脸检测、遮罩或级联管线，实现身份保持、时长自动对齐、环境声同步，在标准与野外复杂场景均优于现有组合式方法。\n\n**核心要点**\n\n| 维度 | 内容 |\n| --- | --- |\n| 问题 | 传统“音频翻译→音频驱动人脸重绘”两级管线在真实视频中易唇形错位、音色漂移、背景声不同步。 |\n| 思路 | 将配音视为受约束的音视联合去噪：给定原视频+目标文本，模型同时生成新音频与新画面，保留身份、姿态、环境。 |\n| 数据 | 利用基础模型自循环：生成“同一人前半英语后半法语”视频→对前半音/嘴加噪并重绘→获得身份一致的多语配对数据。 |\n| 技术 | 1. Latent-Aware Fine Masking 阻断 VAE 泄漏；2. Lip Augmentation 增强唇形多样性；3. 冻结 LTX-2，仅训 LoRA 秩-128；4. Modality-Isolated Cross-Attention 防止音视互泄。 |\n| 实验 | HDFT/TalkVid + 自采野外 50 段；对比 LatentSync、MuseTalk、CosyVoice、HeyGen；指标覆盖 Succ、CSIM、FID、FVD、MAR、ASync、Dur-Err、V-SIM、WER。 |\n| 结果 | 100 % 生成成功率；FVD 最低；时长误差 <0.05 s；用户研究全面领先；侧脸/遮挡/非人类场景稳定输出。 |\n| 局限 | 音色保真仍有漂移；限于 121 帧；需外部文本翻译。 |\n| 未来 | 身份-内容解耦、长视频多角色、实时蒸馏、情感控制、端到端翻译、防伪水印。 |\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是JUST-DUB-IT%3A%20Video%20Dubbing%20via%20Joint%20Audio-Visual%20Diffusion，链接是https://arxiv.org/pdf/2601.22143，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.22143。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}