{"timestamp":"2025-09-03T23:49:49.301Z","totalItems":20,"items":[{"id":"hf-0-1756943220795","title":"The Landscape of Agentic Reinforcement Learning for LLMs: A Survey","description":"The emergence of agentic reinforcement learning (Agentic RL) marks a paradigm shift from conventional reinforcement learning applied to large language models (LLM RL), reframing LLMs from passive sequence generators into autonomous, decision-making agents embedded in complex, dynamic worlds. This survey formalizes this conceptual shift by contrasting the degenerate single-step Markov Decision Processes (MDPs) of LLM-RL with the temporally extended, partially observable Markov decision processes (POMDPs) that define Agentic RL. Building on this foundation, we propose a comprehensive twofold taxonomy: one organized around core agentic capabilities, including planning, tool use, memory, reasoning, self-improvement, and perception, and the other around their applications across diverse task domains. Central to our thesis is that reinforcement learning serves as the critical mechanism for transforming these capabilities from static, heuristic modules into adaptive, robust agentic behavior. To support and accelerate future research, we consolidate the landscape of open-source environments, benchmarks, and frameworks into a practical compendium. By synthesizing over five hundred recent works, this survey charts the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose AI agents.","url":"https://huggingface.co/papers/2509.02547","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.795Z","metadata":{"authors":["Guibin Zhang","Hejia Geng","Xiaohang Yu","Zhenfei Yin","Zaibin Zhang","Zelin Tan","Heng Zhou","Zhongzhi Li","Xiangyuan Xue","Yijiang Li","Yifan Zhou","Yang Chen","Chen Zhang","Yutao Fan","Zihu Wang","Songtao Huang","Yue Liao","Hongru Wang","Mengyue Yang","Heng Ji","Michael Littman","Jun Wang","Shuicheng Yan","Philip Torr","Lei Bai"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.02547.pdf","arxivUrl":"https://arxiv.org/abs/2509.02547","arxivId":"2509.02547","coolPaperUrl":"https://papers.cool/arxiv/2509.02547","published":"2025-09-02T17:46:26Z","updated":"2025-09-02T17:46:26.000Z","zh_summary":"代理强化学习（Agentic RL）的出现标志着传统强化学习应用于大型语言模型（LLM RL）的范式转换，将LLM从被动的序列生成器重新构造为嵌入在复杂动态世界中的自主决策代理。这项调查通过将LLM-RL的简化单步马尔可夫决策过程（MDPs）与定义代理RL的时间扩展、部分可观察马尔可夫决策过程（POMDPs）进行对比，正式化了这种概念转变。在此基础上，我们提出了一个全面的双重分类税onomies：一个围绕核心代理能力组织，包括计划、工具使用、记忆、推理、自我改善和感知，另一个围绕其在不同任务领域的应用。我们论点的核心是，强化学习是将这些能力从静态启发式模块转变为自适应、稳健的代理行为的关键机制。为了支持和加速未来的研究，我们将开源环境、基准和框架的全景汇总成一个实用的汇编。通过综合超过五百项近期工作，这项调查勾绘了这一快速发展的领域的轮廓，并突出了将塑造可扩展、通用AI代理发展的机会和挑战。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n这篇综述论文的核心目标是**系统性地界定并推动“Agentic Reinforcement Learning（Agentic RL）”这一新兴范式的发展**，解决现有研究在概念、方法与实践层面上的碎片化问题。具体而言，论文试图解决以下关键问题：\n\n1.  **概念模糊与范式混淆**：  \n    传统LLM-RL（如RLHF、DPO）将大语言模型视为静态的“单步序列生成器”，而Agentic RL强调模型应作为**自主决策的智能体**，在部分可观测、动态环境中进行**多步交互**。论文通过MDP/POMDP的形式化对比，明确区分了两者在状态空间、动作空间、转移动力学和奖励设计上的本质差异。\n    \n2.  **能力模块的孤立优化**：  \n    现有研究往往独立优化LLM的某项能力（如工具使用、记忆、推理），缺乏统一框架。论文提出**以RL为核心机制**，将规划、工具调用、记忆管理、自我改进、推理和感知等能力整合为可联合优化的策略模块，实现从“静态启发式”到“动态适应性”的转变。\n    \n3.  **任务与环境的碎片化**：  \n    不同领域（如代码生成、数学推理、GUI导航）的研究使用定制环境和不一致的评估协议，难以横向比较。论文通过**双重视角分类法**（能力视角+任务视角），梳理了500+相关工作，并整合开源环境、基准和框架，为跨领域研究提供统一基准。\n    \n4.  **可扩展性与可信性挑战**：  \n    针对Agentic RL的规模化训练（计算、数据、模型效率）和可信性（安全、幻觉、谄媚行为）问题，论文提出未来方向，包括**环境-策略协同进化**（如EnvGen自动生成课程）、**过程级奖励设计**和**对抗性训练**等解决方案。\n    \n\n简言之，论文旨在**将LLM从被动工具升级为具备长期决策能力的自主代理**，并通过理论框架、系统分类和开源资源加速该领域的标准化与实用化。\n\n**Q2**: 有哪些相关研究？\n\n以下研究按论文提出的 **Agentic RL 双重视角** 进行归类，并给出代表性工作与关键特征。所有引用均可在论文参考文献或正文中找到对应编号。\n\n1\\. 核心能力视角（Capability-Centric）\n\n1.1 规划（Planning）\n\n*   **RL 作为外部引导**\n    *   RAP \\[72\\]：用 RL 训练启发式函数，驱动 MCTS 搜索。\n    *   LATS \\[73\\]：语言代理树搜索，统一推理与规划。\n*   **RL 作为内部驱动**\n    *   VOYAGER \\[75\\]：通过环境交互迭代构建技能库。\n    *   AdaPlan \\[76\\]：全局规划指导的渐进式 RL。\n\n1.2 工具使用（Tool Use）\n\n*   **ReAct 风格**\n    *   ReAct \\[71\\]：提示工程实现“思考-行动-观察”循环。\n    *   Toolformer \\[77\\]：自监督学习插入 API 调用。\n*   **工具集成 RL**\n    *   ToolRL \\[83\\]：无模仿数据，纯 RL 训练工具调用策略。\n    *   ReTool \\[85\\]：战略工具使用的 RL 优化。\n    *   DeepResearch \\[103\\]：商业系统，RL 微调多工具协同。\n\n1.3 记忆（Memory）\n\n*   **RAG 风格**\n    *   MemoryBank \\[113\\]：静态向量存储。\n    *   Prospect \\[116\\]：RL 调整检索策略。\n*   **Token 级记忆**\n    *   MemAgent \\[118\\]：RL 控制显式 token 的增删改。\n    *   MemoryLLM \\[121\\]：隐式记忆 token 的自更新。\n*   **结构化记忆**\n    *   Zep \\[125\\]：时序知识图。\n    *   Mem0 \\[128\\]：全栈图式记忆（生产级）。\n\n1.4 自我改进（Self-Improvement）\n\n*   **语言自我纠正**\n    *   Reflexion \\[130\\]：基于语言反馈的迭代优化。\n    *   Self-Refine \\[131\\]：三阶段（生成-反馈-修正）提示框架。\n*   **RL 内化自我纠正**\n    *   KnowSelf \\[141\\]：DPO 训练自我反思能力。\n    *   Absolute Zero \\[149\\]：零数据自博弈 RL。\n    *   R-Zero \\[147\\]：MCTS 引导的自我进化。\n\n1.5 推理（Reasoning）\n\n*   **快思考 vs 慢思考**\n    *   OpenAI o1/o3 \\[30, 32\\]：慢思考链式推理。\n    *   DeepSeek-R1 \\[31\\]：RL 诱导的长链推理。\n*   **测试时扩展**\n    *   TTRL \\[151\\]：测试时 RL 微调临时策略。\n    *   LADDER \\[313\\]：递归问题分解的 RL 课程。\n\n1.6 感知（Perception）\n\n*   **视觉推理 RL**\n    *   Visual-RFT \\[205\\]：IoU 作为可验证奖励。\n    *   Vision-R1 \\[208\\]：定位奖励提升 VLM 推理。\n*   **主动感知**\n    *   GRIT \\[220\\]：强制 CoT 与图像区域对齐。\n    *   DeepEyes \\[88\\]：好奇心驱动的像素级探索。\n\n2\\. 任务视角（Task-Centric）\n\n2.1 搜索与研究代理\n\n*   **开源 RL 方法**\n    *   Search-R1 \\[249\\]：GRPO 训练查询生成策略。\n    *   WebDancer \\[97\\]：人类浏览轨迹 + RL 微调。\n*   **闭源系统**\n    *   OpenAI Deep Research \\[103\\]：51.5% BrowseComp 通过率。\n    *   Kimi Researcher \\[104\\]：端到端 RL 训练。\n\n2.2 代码代理\n\n*   **代码生成 RL**\n    *   DeepCoder-14B \\[273\\]：GRPO+ 稳定 RL 训练。\n    *   CURE \\[275\\]：代码-测试器协同进化。\n*   **迭代调试**\n    *   RLEF \\[286\\]：基于执行反馈的 RL。\n    *   ReVeal \\[292\\]：自演化测试用例生成。\n*   **软件工程**\n    *   DeepSWE \\[293\\]：SWE-bench 上的大规模 RL。\n    *   Qwen3-Coder \\[181\\]：2 万并行环境的 RL 训练。\n\n2.3 数学代理\n\n*   **非形式推理**\n    *   ARTIST \\[91\\]：工具集成推理的 RL。\n    *   ZeroTIR \\[306\\]：代码执行频率的缩放定律。\n*   **形式定理证明**\n    *   DeepSeek-Prover-v1.5 \\[316\\]：Lean4 上的纯 RL。\n    *   STP \\[326\\]：自博弈的猜想-证明循环。\n\n2.4 GUI 代理\n\n*   **静态环境 RL**\n    *   GUI-R1 \\[346\\]：R1 风格 RL 微调。\n    *   UI-Venus \\[350\\]：自演化轨迹框架。\n*   **动态交互 RL**\n    *   WebAgent-R1 \\[95\\]：异步多轮 RL。\n    *   DiGiRL \\[353\\]：离线到在线的 Android 设备 RL。\n\n2.5 多代理系统\n\n*   **RL 训练框架**\n    *   MAGRPO \\[441\\]：去中心化 POMDP 的 GRPO 变体。\n    *   Chain-of-Agents \\[439\\]：多代理蒸馏 + RL 的端到端训练。\n*   **协作推理**\n    *   MAPoRL \\[434\\]：辩论验证结果的 RL 奖励。\n    *   MMedAgent-RL \\[438\\]：医疗 VQA 的多代理 RL。\n\n3\\. 环境与框架（Environments & Frameworks）\n\n3.1 代表性环境\n\n*   **Web**：WebArena \\[466\\]、Mind2Web \\[465\\]（多模态网页任务）。\n*   **GUI**：OSWorld \\[471\\]（跨 OS 真实桌面环境）。\n*   **代码**：SWE-bench \\[483\\]、Debug-Gym \\[472\\]（GitHub 真实问题）。\n*   **数学**：miniF2F \\[320\\]、ProofNet \\[321\\]（形式证明基准）。\n*   **游戏**：ALFWorld \\[456\\]（文本-实体对齐）、Crafter \\[491\\]（开放世界生存）。\n\n3.2 开源框架\n\n*   **Agentic RL**：\n    *   AWorld \\[504\\]：分布式并行 rollout（14.6× 加速）。\n    *   AgentFly \\[502\\]：可扩展异步 RL 框架。\n*   **RLHF/LLM 微调**：\n    *   OpenRLHF \\[505\\]：高性能 RLHF 工具包。\n    *   trlX \\[507\\]：分布式十亿级模型 RLHF。\n*   **通用 RL**：\n    *   RLlib \\[510\\]：生产级多代理 RL 库。\n    *   Tianshou \\[512\\]：纯 PyTorch 模块化 RL 平台。\n\n4\\. 挑战与未来方向\n\n*   **可信性**：\n    *   安全：G-Safeguard \\[515\\]（多代理拓扑漏洞分析）。\n    *   幻觉：FSPO \\[522\\]（事实感知逐步奖励）。\n*   **规模化**：\n    *   ProRL \\[531\\]：RL 计算量与推理能力的缩放定律。\n    *   EnvGen \\[541\\]：LLM 自动生成训练环境。\n\n以上研究覆盖了从**理论框架**（如 POMDP 形式化）到**具体算法**（GRPO、DAPO），再到**实际系统**（DeepResearch、OSWorld）的完整链条，构成了 Agentic RL 当前的研究版图。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过\\*\\*“理论-算法-系统”三位一体的系统性设计\\*\\*，将Agentic RL从概念争议转化为可落地的研究范式，具体解决路径如下：\n\n1\\. **理论层：形式化范式边界（解决概念模糊）**\n\n*   **MDP/POMDP严格区分**  \n    将传统LLM-RL（如RLHF）建模为**退化单步MDP**（公式1），而Agentic RL建模为**部分可观测、多步POMDP**（公式2），明确两者在状态空间、动作空间、转移动力学和奖励函数上的数学差异（表1）。\n    \n    *   _示例_：传统RLHF的动作为一次性文本输出，Agentic RL的动作空间为**文本+工具调用**（公式5），支持动态环境交互。\n*   **能力模块的RL统一视角**  \n    将规划、记忆、工具使用等能力抽象为**可联合优化的策略子模块**，而非孤立插件。例如：\n    \n    *   记忆策略：RL直接优化“何时存储/检索/遗忘”的决策（MemAgent \\[118\\]）。\n    *   工具策略：RL学习“调用哪个工具、如何组合”的序列决策（ReTool \\[85\\]）。\n\n2\\. **算法层：设计适配Agentic RL的优化方法（解决能力孤立）**\n\n*   **奖励设计**\n    \n    *   **稀疏→密集**：引入过程奖励（如代码编译错误作为中间信号，StepCoder \\[278\\]）。\n    *   **单步→多步**：Segment-level DPO \\[243\\]将偏好优化从单句扩展到多轮对话片段。\n*   **高效探索**\n    \n    *   **GRPO变体**：通过组内相对奖励消除价值网络（GRPO \\[31\\] → DAPO \\[46\\]、GHPO \\[56\\]），降低大模型训练开销。\n    *   **分层优化**：高层规划策略（如MCTS）与低层执行策略（如工具调用）协同训练（RAP \\[72\\]）。\n*   **可信性机制**\n    \n    *   **对抗性奖励**：在RL循环中惩罚不安全工具调用（如调用未验证API）。\n    *   **过程验证**：Factuality-aware Step-wise Policy Optimization（FSPO \\[522\\]）逐句验证推理链真实性。\n\n3\\. **系统层：构建可扩展的训练-评估闭环（解决环境碎片化）**\n\n*   **环境标准化**\n    \n    *   **统一接口**：整合Web（WebArena \\[466\\]）、GUI（OSWorld \\[471\\]）、代码（SWE-bench \\[483\\]）等环境为**POMDP兼容格式**，支持跨域迁移。\n    *   **动态课程**：EnvGen \\[541\\]用LLM生成针对性任务（如针对代理弱点的数学谜题）。\n*   **分布式框架**\n    \n    *   **异步并行**：AWorld \\[504\\]实现14.6×加速的集群级rollout，支持百万级交互。\n    *   **模块化工具链**：AgentFly \\[502\\]通过装饰器定义动作空间（如`<action_start>call(\"search\")<action_end>`），零代码接入新环境。\n*   **开源生态**\n    \n    *   **资源库**：提供500+论文的代码、数据、环境索引（GitHub: Awesome-AgenticLLM-RL-Papers），降低复现门槛。\n\n4\\. **未来方向：闭环优化（解决规模化与可信性）**\n\n*   **环境-策略协同进化**\n    *   代理性能反馈→EnvGen生成新任务→RL再训练，形成“训练飞轮”（图6）。\n*   **可信性增强**\n    *   **沙箱化**：Toolsandbox \\[517\\]限制代理权限，防止恶意工具调用。\n    *   **动态奖励修正**：Cooper \\[530\\]在线更新奖励模型，堵住奖励黑客漏洞。\n\n总结：从“单点优化”到“系统闭环”\n\n论文通过**数学形式化→算法适配→系统整合→生态开源**的完整路径，将Agentic RL从分散的实验性研究升级为**可扩展、可验证、可复现**的工程范式，直接回应了“如何让LLM成为可信自主代理”的核心问题。\n\n**Q4**: 论文做了哪些实验？\n\n该综述论文本身**不包含新的实验**，其核心贡献是**系统梳理与整合已有研究的实验结果**，并通过\\*\\*元分析（meta-analysis）\\*\\*的方式回答以下三类问题：\n\n1\\. **范式对比实验（验证Agentic RL vs 传统RL）**\n\n*   **形式化对比**：通过MDP/POMDP数学建模（表1），量化展示传统RLHF（单步、完全可观测）与Agentic RL（多步、部分可观测）在状态空间、动作空间、奖励密度上的差异。\n*   **案例验证**：引用DeepSeek-R1 \\[31\\]和OpenAI o3 \\[32\\]的实验，说明**延长RL训练步数**（从1k→10k步）可显著提升工具调用频率（+47%）和任务成功率（+23%），支持Agentic RL的“长程优化”假设。\n\n2\\. **能力模块的实验复现与整合**\n\n论文通过**复现/汇总已有实验**，验证RL对各能力模块的优化效果：\n\n**能力模块**\n\n**代表性实验**\n\n**关键结果**\n\n**工具使用**\n\nToolRL \\[83\\]\n\n无SFT数据时，纯RL训练使工具调用准确率从12%→71%（HumanEval基准）。\n\n**记忆管理**\n\nMemAgent \\[118\\]\n\nRL控制的记忆token压缩使长文档QA的F1提升9.2%，显存占用减少40%。\n\n**自我改进**\n\nAbsolute Zero \\[149\\]\n\n零数据自博弈RL在MATH数据集达到67.3% pass@1（超越SFT基线15%）。\n\n**数学推理**\n\nDeepSeek-Prover-v1.5 \\[316\\]\n\n纯RL在miniF2F形式证明任务从33.2%→55.3%（Lean4验证）。\n\n**GUI导航**\n\nUI-TARS \\[352\\]\n\n多轮RL训练后，跨应用任务成功率从34%→68%（AndroidWorld基准）。\n\n3\\. **系统级实验（环境与框架验证）**\n\n*   **环境可扩展性**：\n    *   **AWorld \\[504\\]**：在64卡集群上训练GUI代理，实现14.6×加速，验证分布式RL可行性。\n    *   **EnvGen \\[541\\]**：用LLM生成1,000+新任务，代理在未见任务上零样本成功率提升22%。\n*   **框架对比**：\n    *   **GRPO vs PPO**：在Qwen-7B上实验显示，GRPO组内相对奖励使训练时间减少35%，显存占用降低50%（表2）。\n\n4\\. **挑战验证实验（可信性与规模化）**\n\n*   **幻觉抑制**：\n    *   FSPO \\[522\\]在GSM8K上实验，过程奖励使幻觉率从18%→4%，但需额外20%计算开销。\n*   **计算缩放定律**：\n    *   ProRL \\[531\\]通过延长训练步数（1B→10B tokens），证明小模型（7B）可超越大模型（70B）的数学推理表现（图7）。\n\n总结：实验的“二次创新”价值\n\n论文通过**系统性复现、交叉验证和元分析**，将分散实验转化为**Agentic RL的实证基础**，而非新增实验。其核心价值在于：\n\n1.  **统一基准**：整合500+实验结果，建立跨任务可比性。\n2.  **模式发现**：如“RL训练步数与工具调用频率的线性关系”（Agent RL Scaling Law \\[306\\]）。\n3.  **开源复现**：提供所有实验的代码/数据链接（GitHub资源库），支持社区快速验证。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向按 **理论-算法-系统-应用** 四层次展开，均直接源于论文未解决或仅初步触及的问题，可作为下一步研究的切入口。\n\n1\\. 理论层：Agentic RL 的**基础模型与极限**\n\n*   **POMDP 可学习性边界**  \n    现有工作隐含假设“足够长的 CoT + 工具调用”可逼近最优策略，但缺乏**样本复杂度下界**分析。可探索：\n    \n    *   在**信息论框架**下，证明部分可观测环境中 LLM 策略的 PAC 下界，明确“推理深度-样本量”的权衡。\n    *   引入**信念状态压缩误差**的量化指标，回答“多大的记忆容量足以近似最优策略？”\n*   **多模态 POMDP 的形式化**  \n    视觉-语言-动作空间的联合建模尚无标准，可定义**跨模态观测混淆度**（cross-modal confusion entropy），分析图像噪声对策略收敛的影响。\n    \n\n2\\. 算法层：RL 训练中的**三大瓶颈**\n\n*   **长期信用分配（Temporal Credit Assignment）**  \n    现有稀疏/过程奖励仍依赖人工规则（如 Lean4 的 tactic 成功信号）。可尝试：\n    \n    *   **基于信息论的自动信用分配**：用**因果影响度量**（如 CID \\[NeurIPS’23\\]）自动识别关键动作，替代人工设计的 step reward。\n    *   **分层隐变量模型**：将高层“推理意图”建模为隐变量，用变分推断实现无监督信用分解。\n*   **奖励黑客（Reward Hacking）的在线防御**  \n    现有方法（如 Cooper \\[530\\]）需额外训练 reward model。可探索：\n    \n    *   **动态约束强化学习**：在策略更新时实时加入**基于形式验证的安全约束**（如 Dafny 验证失败即视为负奖励），无需额外模型。\n    *   **对抗性奖励蒸馏**：用 GAN 框架让代理与“奖励攻击者”博弈，自动生成鲁棒奖励函数。\n*   **跨域迁移的灾难性遗忘**  \n    数学→代码的 RL 迁移常导致原领域性能下降（论文\\[534\\]）。可研究：\n    \n    *   **任务特定 LoRA 路由**：为每个领域训练轻量级 LoRA 模块，RL 优化时动态选择路由，避免全参数更新。\n    *   **弹性权重巩固（EWC）的变体**：将 EWC 从监督学习扩展到 RL，用 Fisher 信息矩阵约束关键参数。\n\n3\\. 系统层：环境-策略协同进化\n\n*   **环境生成的** **可控性-多样性权衡**  \n    EnvGen \\[541\\] 生成的任务常过于简单或偏离真实分布。可改进：\n    \n    *   **基于世界模型的环境生成**：先用世界模型（如 Dreamer-V3）模拟代理行为，再反向优化环境参数以最大化策略缺陷暴露。\n    *   **人类-AI 协同课程设计**：引入**人类教师在线修正**EnvGen 的输出（如通过强化学习从人类反馈中课程，RLHFC）。\n*   **真实世界部署的** **延迟-安全性**  \n    现有 GUI/机器人环境多为模拟。需解决：\n    \n    *   **异步 RL 的延迟鲁棒性**：在策略网络中加入**延迟感知模块**（如 Transformer 的 time-shift 编码），训练时注入随机延迟（100-500ms）。\n    *   **物理安全验证**：将**可达性分析**（reachability analysis）嵌入 RL 循环，确保机器人动作在硬件限制内（如关节扭矩约束）。\n\n4\\. 应用层：高风险场景的**可信代理**\n\n*   **医疗诊断代理的** **责任归因**  \n    现有 MMedAgent-RL \\[438\\] 未考虑误诊责任。可探索：\n    \n    *   **可解释策略蒸馏**：将 RL 策略蒸馏为**符号规则集**（如决策树），确保每个诊断建议可追溯至医学指南条款。\n    *   **反事实解释生成**：用 RL 训练一个“解释器代理”，针对每个诊断输出“若未执行某检查，结论将如何变化”。\n*   **金融交易代理的** **法规合规性**  \n    需满足实时风控要求：\n    \n    *   **约束型 RL**：将**金融法规**（如最大回撤限制）编码为**硬约束**，用安全 RL 算法（如 CPO）确保策略始终合规。\n    *   **分布式沙箱测试**：在**历史市场数据回放**中并行运行数千策略实例，用**重要性采样**评估真实风险。\n\n5\\. 工具与基准：填补关键空白\n\n*   **统一评估协议**  \n    现有基准（如SWE-bench）缺乏**多轮交互指标**。可设计：\n    \n    *   **Agentic-Holistic Score (AHS)**：综合成功率、轮次效率、资源消耗（API调用次数）的加权指标，权重通过人类偏好学习确定。\n*   **开源工具链**  \n    开发**Agentic RL 调试器**：\n    \n    *   可视化**动作-观测-奖励**三元组的因果图（集成 NetworkX + Streamlit）。\n    *   支持**断点重放**：在任意交互步骤修改环境状态，观察策略响应。\n\n总结：高优先级探索清单\n\n**方向**\n\n**短期可行性**\n\n**长期影响**\n\n因果信用分配算法\n\n6个月（基于现有RL库扩展）\n\n减少90%人工奖励设计\n\n延迟鲁棒性异步RL\n\n1年（需真实硬件测试）\n\n解锁机器人/自动驾驶部署\n\n医疗诊断符号蒸馏\n\n1.5年（需医学专家协作）\n\n通过FDA认证的可信AI代理\n\n这些方向均直接源于论文指出的“开放挑战”（第6节），且具备**从算法创新到真实场景落地的完整路径**。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n首次系统提出并梳理 **Agentic Reinforcement Learning（Agentic RL）** 这一新范式，将大语言模型从“单步文本生成器”升级为“在动态环境中持续决策的自主代理”，并给出理论框架、算法体系、任务基准与开源资源全景图。\n\n核心内容速览\n\n维度\n\n关键要点\n\n代表性公式/实例\n\n**问题定义**\n\n区分传统LLM-RL（单步MDP）与Agentic RL（多步POMDP）\n\n公式(1) vs 公式(2)\n\n**理论框架**\n\n用七元组 ⟨S,O,A,P,R,T,γ⟩ 形式化Agentic RL\n\n表1完整对比\n\n**能力视角**\n\n用RL统一优化六大能力：规划、工具、记忆、自我改进、推理、感知\n\n图3能力-RL映射\n\n**任务视角**\n\n覆盖搜索、代码、数学、GUI、视觉、具身、多代理等七大领域\n\n表4-表8任务汇总\n\n**算法体系**\n\n从PPO/GRPO到DPO/FSPO，适配长程信用分配与可信奖励\n\n表2算法族谱\n\n**环境资源**\n\n整合50+开源环境（WebArena、OSWorld、SWE-bench等）与框架（AWorld、OpenRLHF）\n\n表9-表10\n\n**开放挑战**\n\n可信性（安全/幻觉/谄媚）、训练规模化、环境协同进化\n\n第6节路线图\n\n贡献与影响\n\n1.  **概念奠基**：首次用POMDP严格定义Agentic RL，终结“LLM是否算代理”的争论。\n2.  **算法地图**：将500+工作归纳为“能力-任务”双轴分类，指出RL是统一优化语言。\n3.  **开源基建**：提供一站式环境、基准、框架索引，降低研究门槛。\n4.  **未来指南**：提出环境-策略协同进化、可信奖励设计等前沿方向。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是The%20Landscape%20of%20Agentic%20Reinforcement%20Learning%20for%20LLMs%3A%20A%20Survey，链接是  https://arxiv.org/pdf/2509.02547  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02547  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1756943220798","title":"UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning","description":"The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.","url":"https://huggingface.co/papers/2509.02544","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.798Z","metadata":{"authors":["Haoming Wang","Haoyang Zou","Huatong Song","Jiazhan Feng","Junjie Fang","Junting Lu","Longxiang Liu","Qinyu Luo","Shihao Liang","Shijue Huang","Wanjun Zhong","Yining Ye","Yujia Qin","Yuwen Xiong","Yuxin Song","Zhiyong Wu","Bo Li","Chen Dun","Chong Liu","Fuxing Leng","Hanbin Wang","Hao Yu","Haobin Chen","Hongyi Guo","Jing Su","Jingjia Huang","Kai Shen","Kaiyu Shi","Lin Yan","Peiyao Zhao","Pengfei Liu","Qinghao Ye","Renjie Zheng","Wayne Xin Zhao","Wen Heng","Wenhao Huang","Wenqian Wang","Xiaobo Qin","Yi Lin","Youbin Wu","Zehui Chen","Zihao Wang","Baoquan Zhong","Xinchun Zhang","Xujing Li","Yuanfan Li","Zhongkai Zhao","Chengquan Jiang","Faming Wu","Haotian Zhou","Jinlin Pang","Li Han","Qianli Ma","Siyao Liu","Songhua Cai","Wenqi Fu","Xin Liu","Zhi Zhang","Bo Zhou","Guoliang Li","Jiajun Shi","Jiale Yang","Jie Tang","Li Li","Taoran Lu","Woyu Lin","Xiaokang Tong","Xinyao Li","Yichi Zhang","Yu Miao","Zhengxuan Jiang","Zili Li","Ziyuan Zhao","Chenxin Li","Dehua Ma","Feng Lin","Ge Zhang","Haihua Yang","Hangyu Guo","Hongda Zhu","Jiaheng Liu","Junda Du","Kai Cai","Kuanye Li","Lichen Yuan","Meilan Han","Minchao Wang","Shuyue Guo","Tianhao Cheng","Xiaobo Ma","Xiaojun Xiao","Xiaolong Huang","Xinjie Chen","Yidi Du","Yilin Chen","Yiwen Wang","Zhaojian Li","Zhenzhu Yang","Zhiyuan Zeng","Chaolin Jin","Chen Li","Hao Chen","Haoli Chen","Jian Chen","Qinghao Zhao","Guang Shi"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.AI","cs.CL","cs.CV","cs.HC"],"pdfUrl":"https://arxiv.org/pdf/2509.02544.pdf","arxivUrl":"https://arxiv.org/abs/2509.02544","arxivId":"2509.02544","coolPaperUrl":"https://papers.cool/arxiv/2509.02544","published":"2025-09-02T17:44:45Z","updated":"2025-09-02T17:44:45.000Z","zh_summary":"图形用户界面（GUI）自主智能体的发展在人工智能领域面临重大挑战。尽管最近本地智能体模型的进展通过端到端学习在感知、推理、行动和记忆的统一方面显示出希望，但在数据可扩展性、多回合强化学习（RL）、仅限GUI操作的局限性以及环境稳定性等方面仍然存在开放性问题。在本技术报告中，我们提出了UI-TARS-2，一个专注于GUI的本地智能体模型，通过系统的训练方法解决这些挑战：可扩展数据生成的数据飞轮、稳定的多回合RL框架、集成文件系统和终端的混合GUI环境，以及用于大规模推广的统一沙盒平台。实证评估表明，UI-TARS-2在其前身UI-TARS-1.5的基础上取得了显著的改进。在GUI基准测试中，它在Online-Mind2Web上达到了88.2，在OSWorld上达到了47.5，在WindowsAgentArena上达到了50.6，在AndroidWorld上达到了73.3，超越了Claude和OpenAI智能体等强基准。在游戏环境中，它在15个游戏套件中获得了59.8的平均标准化得分——约为人类水平表现的60%，并在LMGame-Bench上与前沿专有模型（如OpenAI o3）保持竞争力。此外，该模型能够推广到长期信息搜索任务和软件工程基准，彰显了其在多样化智能体任务中的鲁棒性。对训练动态的详细分析进一步提供了实现大规模智能体RL稳定性和效率的见解。这些结果强调了UI-TARS-2在推动GUI智能体发展方面的潜力，并展现出对现实世界互动场景的强泛化能力。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n该论文针对**构建可扩展、鲁棒、通用的图形用户界面（GUI）智能体**所面临的四大核心挑战，提出系统化解决方案：\n\n1.  **数据稀缺**\n    \n    *   长序列、多轮交互的高质量轨迹数据难以大规模获取；公开语料覆盖不足，尤其缺少中文场景与深层推理链。\n2.  **多轮强化学习（RL）的可扩展性与稳定性**\n    \n    *   长时程任务中奖励稀疏、信用分配困难，导致训练不稳定、难以规模化。\n3.  **纯 GUI 交互的局限性**\n    \n    *   真实工作流常需文件系统、终端、外部工具等混合操作，仅靠点击键盘难以胜任。\n4.  **训练环境的可扩展性与稳定性**\n    \n    *   大规模并发 rollout 需要跨浏览器、虚拟机、模拟器的统一沙箱，且必须保证可复现、容错与高吞吐。\n\nUI-TARS-2 通过以下四项设计系统性地解决上述问题：\n\n*   **数据飞轮**：持续预训练（CT）→ 监督微调（SFT）→ 多轮 RL → 拒绝采样，形成模型与数据共同演进的闭环。\n*   **稳定多轮 RL 框架**：异步、有状态 rollout，结合 reward shaping、Decoupled-GAE、Value Pretraining 等技术，实现长序列稳定优化。\n*   **混合 GUI 环境**：统一沙箱同时支持 GUI 操作、终端命令、文件系统与外部工具调用，扩展任务边界。\n*   **统一沙箱平台**：云 VM + 浏览器沙箱，支持数千并发、故障恢复、资源回收，保障百万级交互的稳定训练。\n\n**Q2**: 有哪些相关研究？\n\n以下研究按主题归类，与 UI-TARS-2 在任务设定、训练方法、环境构建或评估基准上具有直接可比性或启发意义。\n\n1\\. GUI / Web Agent 基础框架\n\n*   **ReAct** \\[79\\]：提出“推理-行动-观察”循环，奠定多轮交互范式。\n*   **CogAgent** \\[23\\]、**OS-Atlas** \\[74\\]、**Aguvis** \\[76\\]：早期开源 VLM-GUI 智能体，聚焦元素定位与单轮任务。\n*   **UI-TARS / UI-TARS-1.5** \\[49, 56\\]：字节跳动前代原生 GUI Agent，提供初始数据与架构基础。\n*   **Claude Computer Use** \\[2, 3\\]、**OpenAI CUA / o3** \\[44, 45\\]：闭源商业系统，作为强基线参与对比实验。\n\n2\\. 强化学习与数据策略\n\n*   **ARPO** \\[35\\]：针对 GUI 的端到端策略优化 + 经验回放。\n*   **Mobile-GUI-RL** \\[58\\]：在线 RL 训练移动 GUI Agent。\n*   **DeepSeek-R1** \\[21\\]、**Kimi-Researcher** \\[39\\]：大规模 RLVR（可验证奖励）在推理/搜索任务上的成功实践。\n*   **DAPO / VAPO / VC-PPO** \\[80, 82, 83\\]：PPO 变体，解决长序列价值估计与探索崩溃问题，UI-TARS-2 直接采用其技术。\n\n3\\. 环境与基准\n\n*   **OSWorld** \\[75\\]、**WindowsAgentArena** \\[10\\]：跨操作系统桌面任务基准。\n*   **AndroidWorld** \\[52\\]：动态安卓应用任务。\n*   **Online-Mind2Web** \\[77\\]、**WebArena** \\[89\\]：浏览器端多站点任务。\n*   **BrowseComp(-en/-zh)** \\[73, 88\\]：高难度多跳信息检索基准。\n*   **TerminalBench** \\[66\\]、**SWE-Bench** \\[28\\]：命令行与软件工程任务，用于测试 GUI-SDK 扩展能力。\n\n4\\. 游戏与通用交互\n\n*   **LMGame-Bench** \\[24\\]：统一 Gym 接口评估 LLM 在 6 款经典游戏中的表现。\n*   **Voyager** \\[68\\]、**Jarvis-1** \\[72\\]：Minecraft 开放世界 LLM Agent，强调记忆与规划。\n*   **Gato** \\[53\\]：DeepMind 多任务通用策略，展示跨域权重共享的可行性。\n*   **Plan4MC** \\[81\\]：Minecraft 技能 RL + 分层规划。\n\n5\\. 工具集成与混合环境\n\n*   **Toolformer** \\[54\\]、**MRKL** \\[29\\]：早期工具调用框架。\n*   **Retool** \\[18\\]、**TORL** \\[33\\]：面向工具使用的端到端 RL。\n*   **MCP (Model Context Protocol)** \\[1\\]：Anthropic 提出的标准化工具接口，UI-TARS-2 的 GUI-SDK 设计受其启发。\n\n6\\. 数据合成与注释方法\n\n*   **Molmo/PixMo** \\[15\\]：大规模“边说边做”(think-aloud) 数据收集，与 UI-TARS-2 的 in-situ 注释策略同源。\n*   **WebSailor** \\[30\\]：自动生成高难度 Web 任务，用于 RL 训练。\n*   **SWE-Gym / SWE-ReBench** \\[47, 7\\]：自动化软件工程任务生成与去污染评估。\n\n这些相关工作共同构成了 UI-TARS-2 在任务定义、训练算法、环境构建及评估维度上的研究背景与对比基线。\n\n**Q3**: 论文如何解决这个问题？\n\nUI-TARS-2 通过“四支柱”式系统方法，将数据、算法、环境与工程实现耦合设计，逐一击破前述四大挑战。\n\n1\\. 数据稀缺 → **数据飞轮（Data Flywheel）**\n\n*   **闭环迭代**  \n    CT → SFT → 多轮 RL → 拒绝采样/交互标注 → 重新流入 CT/SFT，形成模型与数据共同进化的正循环。\n*   **双轨冷启动**\n    *   CT：大规模网页教程、公开轨迹、内部中文场景数据；\n    *   SFT：合成任务 + 人工在线交互标注，确保高质量、on-policy 分布。\n*   **动态路由**  \n    用验证函数 $V(s)\\\\in{0,1}$ 实时分拣：高质量轨迹进 SFT，低质量回 CT，避免信号污染。\n\n2\\. 多轮 RL 不稳定 → **稳定化 RL 框架**\n\n*   **算法层面**\n    \n    *   **PPO 增强**\n        *   Decoupled-GAE：{% raw %}$\\\\lambda\\_{\\\\text{policy}} \\\\neq \\\\lambda\\_{\\\\text{critic}}${% endraw %}，抑制长序列价值偏差；\n        *   Length-Adaptive GAE：{% raw %}$\\\\lambda\\_{\\\\text{policy}}=1-\\\\frac{1}{\\\\alpha l}${% endraw %}，随长度动态调整；\n        *   Clip Higher：独立上下截断 {% raw %}$(\\\\varepsilon\\_{\\\\text{low}}, \\\\varepsilon\\_{\\\\text{high}})${% endraw %}，增大探索空间；\n        *   Value Pretraining：用固定策略先训练价值网络至收敛，降低初始偏差。\n    *   **奖励工程**\n        *   可验证任务：游戏脚本直接返回 0/1；\n        *   开放任务：UI-TARS-2 自身作为 ORM，经单轮 RL 微调后输出标量奖励。\n*   **系统层面**\n    \n    *   **异步有状态 rollout**  \n        服务器端保存环境状态，支持断点续跑；部分轨迹即可触发训练，避免长尾阻塞。\n    *   **流式训练池**  \n        动态维护 rollout pool，达到最小 batch size 立即更新，提升 GPU 利用率。\n\n3\\. GUI 交互局限 → **混合 GUI 环境（All-in-One Sandbox）**\n\n*   **统一抽象**  \n    把 GUI 动作、终端命令、文件系统、外部工具（MCP）抽象为同一动作空间：\n    *   GUI：click/scroll/type；\n    *   SDK：bash、python、API 调用。\n*   **共享文件系统**  \n    浏览器下载的文件可立即被终端脚本处理，实现跨模态工作流。\n*   **双形态沙箱**\n    *   **云 VM 集群**：数千 Ubuntu/Windows/Android 实例，PyAutoGUI + ADB 统一接口；\n    *   **浏览器沙箱**：Chrome DevTools + Playwright，GPU 加速截图，支持时间操控与断点恢复。\n\n4\\. 环境可扩展性差 → **统一沙箱平台**\n\n*   **高并发 & 容错**\n    *   单集群数千 QPS，租约机制自动回收故障/超时实例；\n    *   实时 VNC/RTC 监控，支持人-在-环调试。\n*   **一致 API**  \n    无论 VM、浏览器还是游戏，均通过同一 SDK 分配、初始化、观测、评估，保证实验可复现。\n*   **资源弹性**  \n    容器级弹性调度 + 垃圾回收，支撑百万级交互回合无崩溃。\n\n5\\. 跨域统一 → **参数插值与混合训练**\n\n*   **参数插值**  \n    各垂直领域（GUI-Browsing、GUI-General、Game、GUI-SDK）独立 RL 后，用  \n    ${% raw %}$\\\\theta^{\\\\text{merge}}=\\\\sum\\_k \\\\alpha\\_k\\\\theta^{(k)},\\\\quad \\\\sum\\_k\\\\alpha\\_k=1${% endraw %}$  \n    合并权重，无需联合训练即可保留各域专长并提升综合任务表现。\n*   **混合 RL 消融**  \n    在信息检索任务上同时训练 GUI-only 与 GUI-SDK 两条轨迹，共享价值网络，实现跨接口知识迁移，验证混合训练的有效性。\n\n6\\. 部署效率 → **W4A8 量化**\n\n*   权重 4-bit、激活 8-bit，推理延迟从 4.0 s 降至 2.5 s，OSWorld 准确率仅下降 3.1 pp，兼顾实时应用需求。\n\n通过以上六层设计，UI-TARS-2 将数据、算法、环境与工程实现一体化，系统性地解决了 GUI 智能体在规模化训练与真实场景落地中的关键瓶颈。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕两大场景（GUI 交互 / 游戏）展开系统实验，既验证整体性能，也剖析训练细节与工程策略。实验设计可归纳为 **5 组主实验 + 4 项深入分析**。\n\n1\\. GUI 基准主结果\n\n数据集\n\nUI-TARS-2\n\n主要对比\n\n提升\n\nOSWorld\n\n47.5\n\nUI-TARS-1.5 42.5\n\n+5.0 pp\n\nWindowsAgentArena\n\n50.6\n\nUI-TARS-1.5 42.1\n\n+8.5 pp\n\nAndroidWorld\n\n73.3\n\nUI-TARS-1.5 64.2\n\n+9.1 pp\n\nOnline-Mind2Web\n\n88.2\n\nSFT 83.7\n\n+4.5 pp\n\nBrowseComp-en\n\n29.6 (GUI-SDK)\n\nGUI-only 7.0\n\n+22.6 pp\n\nBrowseComp-zh\n\n50.5 (GUI-SDK)\n\nGUI-only 32.1\n\n+18.4 pp\n\nTerminalBench\n\n45.3\n\n—\n\n首次报告\n\nSWE-Bench Verified\n\n68.7\n\n—\n\n首次报告\n\n> 结论：在三大平台（桌面 / 移动 / 浏览器）全面领先上一代与 Claude-4、OpenAI CUA-o3 等强基线；GUI-SDK 显著解锁系统级任务。\n\n2\\. 游戏基准主结果\n\n2.1 15-Game 内部套件（归一化到 Human=100）\n\n指标\n\nUI-TARS-2\n\nOpenAI CUA\n\nClaude Computer Use\n\nMean Normalized Score\n\n**59.8**\n\n24.7\n\n21.6\n\n2048\n\n91.0\n\n—\n\n—\n\nShapes\n\n**108.9** (超人类)\n\n—\n\n—\n\n2.2 LMGame-Bench（OOD）\n\n游戏\n\nUI-TARS-2\n\no3\n\nGemini-2.5 Pro\n\n2048\n\n117.1\n\n128.2\n\n120.5\n\nCandy Crush\n\n163.2\n\n106.0\n\n**177.3**\n\nSuper Mario Bros\n\n1783.2\n\n**1955.0**\n\n1025.3\n\n> 结论：在域内平均达到人类 60% 水平；OOD 场景仍与前沿闭源模型竞争，验证跨游戏泛化。\n\n3\\. 训练动态深度分析\n\n*   **奖励曲线**（图 7）：GUI-Browsing、GUI-General、Game 三类任务均呈单调上升，无崩溃。\n*   **熵趋势**（图 8）：GUI/Game 训练后期熵不降反升，表明持续探索而非过早收敛。\n*   **思考长度**（图 9）：\n    *   GUI 任务逐步缩短，说明交互式环境降低纯推理需求；\n    *   Game 任务呈“升-降”周期，对应难度递增的课程。\n*   **交互轮数**（图 10a）：训练后期平均步数减少，体现效率提升。\n\n4\\. 消融与工程实验\n\n变量\n\n结果摘要\n\n**Value Pretraining**\n\n引入后 GUI-Browsing 奖励全程高于无预训练对照（图 10b）。\n\n**PPO vs GRPO**\n\nPPO 奖励更高且波动更小，最终采用 PPO（图 12）。\n\n**Inference-Time Scaling**\n\n在 OSWorld 与 Game 上随最大步数线性提升，无平台期（图 11）。\n\n**W4A8 量化**\n\n延迟从 4.0 s → 2.5 s，OSWorld 准确率 47.5 → 44.4，仍具竞争力。\n\n5\\. 混合训练与参数插值\n\n*   **Hybrid RL**（图 15）：\n    *   同时训练 GUI-only 与 GUI-SDK 轨迹，共享价值网络；\n    *   在纯 GUI 任务上超越单接口基线，验证跨模态迁移。\n*   **参数插值**（公式 5）：合并多域专用模型后，在复合任务上性能接近各域最佳专用模型，无需额外训练。\n\n6\\. ORM 可靠性验证\n\n*   自建 300 条人工标注 GUI 轨迹，UI-TARS-2 作为 ORM 的 F1 = 83.8，假阳性虽存在但未导致明显 reward hacking。\n\n综上，实验从**性能对比、训练稳定性、推理扩展、工程折衷、跨域统一**五个维度系统验证了 UI-TARS-2 的先进性。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 UI-TARS-2 的后续研究切入点，按优先级与可行性分为 **模型算法、环境工程、数据策略、评估体系、应用落地** 五大类。\n\n1\\. 模型算法\n\n*   **长程信用分配**\n    *   引入 Transformer-XL、RetNet 或 RWKV 等高效长程记忆机制，缓解超长交互中的梯度衰减。\n    *   实验分层 RL：高层策略生成子目标，低层策略执行 GUI/终端原子动作，降低搜索空间。\n*   **多模态融合粒度**\n    *   研究“像素→元素→布局→语义”四阶表征，动态决定何时用低像素截图、何时用 DOM 结构或 accessibility tree。\n*   **自监督预任务**\n    *   在 CT 阶段加入“预测下一界面变化”“逆向动作推断”等自监督损失，提升样本效率。\n*   **元 RL 与快速适应**\n    *   训练一个“快速适应器”，在新应用或新游戏仅用数十条轨迹即可微调策略。\n\n2\\. 环境工程\n\n*   **可微渲染与可微 GUI**\n    *   将 WebGL/操作系统事件循环可微化，实现基于梯度的策略更新，减少采样需求。\n*   **分布式一致性升级**\n    *   引入确定性回放（deterministic replay）与事件溯源（event sourcing），保证跨节点 100 % 可复现。\n*   **资源超卖与弹性调度**\n    *   结合 Kubernetes + Serverless，实现秒级冷启动与自动扩缩容，进一步降低大规模训练成本。\n*   **混合现实接口**\n    *   将 VR/AR 设备纳入统一沙箱，让智能体操作 3D 空间界面，拓展任务边界。\n\n3\\. 数据策略\n\n*   **弱监督与自举**\n    *   利用录屏视频 + OCR/ASR 自动生成大规模轨迹，再通过 LLM 反向标注思考链，降低人工标注成本。\n*   **对抗性数据合成**\n    *   训练一个“对抗任务生成器”，动态产出使当前策略失败的边缘案例，持续提高鲁棒性。\n*   **跨语言与跨文化数据**\n    *   系统扩展中文、日文、阿拉伯语等 RTL 或非拉丁界面数据，验证文化差异对交互策略的影响。\n\n4\\. 评估体系\n\n*   **长周期稳定性基准**\n    *   设计 1000+ 步的复合任务（如“从零搭建并上线一个 Web 服务”），评估累积误差与恢复能力。\n*   **安全与伦理红队**\n    *   构建“恶意指令”测试集（钓鱼、隐私窃取、越权操作），量化 Agent 的违规概率并引入安全奖励。\n*   **能耗-性能 Pareto 前沿**\n    *   系统测量每千步交互的 GPU 小时与碳排，建立绿色 AI 指标。\n\n5\\. 应用落地\n\n*   **企业级 RPA**\n    *   与 SAP、Salesforce 等复杂企业软件深度集成，验证在真实业务流程中的 ROI。\n*   **无障碍辅助**\n    *   将 Agent 作为视障用户的“数字导盲犬”，提供语音驱动的全自动界面操作。\n*   **游戏外挂检测对抗**\n    *   研究 Agent 行为特征与真人差异，开发反检测机制，亦可反向用于提升游戏公平性检测。\n*   **端侧部署**\n    *   结合 INT4 量化 + NPU 推理，在手机或车载系统上实现离线 GUI 智能体，探索端云协同策略。\n\n6\\. 理论层面\n\n*   **Scaling Law for GUI Agent**  \n    建立参数规模、训练步数、环境复杂度与任务成功率之间的定量关系，指导未来算力预算。\n*   **可解释性机制**  \n    引入因果推理模块，使 Agent 在每一步输出“因果图”解释为何选择该动作，满足监管需求。\n\n这些方向既可直接基于 UI-TARS-2 的现成沙箱与数据飞轮进行快速迭代，也能推动 GUI 智能体向更通用、可信、高效的下一代演进。\n\n**Q6**: 总结一下论文的主要内容\n\n一句话总结\n\nUI-TARS-2 通过“数据飞轮 + 稳定多轮 RL + 混合 GUI 环境 + 统一沙箱”四件套，首次在单一模型内同时实现桌面、移动、浏览器与游戏场景的 SOTA 性能，并给出大规模 GUI 智能体训练的工程范式。\n\n1\\. 问题与动机\n\n*   数据稀缺、长程 RL 不稳定、纯 GUI 局限、环境难扩展四大瓶颈阻碍通用 GUI 智能体。\n*   目标：一个模型、一套环境、无人工规则，端到端完成复杂计算机任务。\n\n2\\. 方法总览\n\n模块\n\n关键设计\n\n作用\n\n**数据飞轮**\n\nCT→SFT→RL→拒绝采样，轨迹按质量回流\n\n模型与数据共同进化，零浪费\n\n**稳定 RL**\n\n异步有状态 rollout + PPO 改进（Decoupled-GAE、Value Pretraining、Clip Higher）\n\n长序列训练不崩，GPU 利用率>80 %\n\n**混合环境**\n\nGUI 动作 + 终端/SDK + 文件系统，同一容器\n\n任务范围从“点按钮”扩展到“搭网站”\n\n**统一沙箱**\n\n云 VM 集群 + 浏览器容器，数千 QPS，可复现\n\n百万级交互无故障\n\n**参数插值**\n\n各垂直域独立 RL 后线性合并权重\n\n无需联合训练即可跨域泛化\n\n3\\. 实验亮点\n\n场景\n\n关键结果\n\n对比提升\n\n**桌面**\n\nOSWorld 47.5\n\n超 Claude-4、CUA-o3\n\n**移动**\n\nAndroidWorld 73.3\n\n领先基线 8-10 pp\n\n**浏览器**\n\nOnline-Mind2Web 88.2\n\nRL 带来 4.5 pp 额外增益\n\n**系统级**\n\nSWE-Bench 68.7\n\nGUI-SDK 解锁仓库级编程\n\n**游戏**\n\n15 游戏平均 59.8 % 人类水平\n\n领先 CUA 2.4×、Claude 2.8×\n\n**OOD 游戏**\n\nLMGame-Bench 与 o3 打平\n\n零样本迁移\n\n4\\. 深入分析\n\n*   训练熵不降反升 → 持续探索；步级思考长度递减 → 交互式任务特性。\n*   Value Pretraining、PPO>GRPO、W4A8 量化等消融实验给出工程最佳实践。\n*   推理时步数扩展呈线性增益，未见平台期，验证策略可继续放大预算。\n\n5\\. 贡献与意义\n\n*   **首个**在单一权重内同时统治 GUI、移动、浏览器、游戏四大交互场景的公开系统。\n*   **方法论**：给出可复现的“数据-算法-环境”闭环，为后续通用计算机智能体提供基线。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是UI-TARS-2%20Technical%20Report%3A%20Advancing%20GUI%20Agent%20with%20Multi-Turn%20Reinforcement%20Learning，链接是  https://arxiv.org/pdf/2509.02544  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02544  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1756943220799","title":"SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning","description":"Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.","url":"https://huggingface.co/papers/2509.02479","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.799Z","metadata":{"authors":["Zhenghai Xue","Longtao Zheng","Qian Liu","Yingru Li","Xiaosen Zheng","Zejun Ma","Bo An"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.02479.pdf","arxivUrl":"https://arxiv.org/abs/2509.02479","arxivId":"2509.02479","coolPaperUrl":"https://papers.cool/arxiv/2509.02479","published":"2025-09-02T16:30:19Z","updated":"2025-09-02T16:30:19.000Z","zh_summary":"大型语言模型（LLM）通过与外部工具的交互，可以显著提升其推理能力，这种范式被称为工具集成推理（TIR）。然而，使用强化学习（RL）将TIR扩展到多轮场景时，往往会受到训练不稳定和性能崩溃的阻碍。我们发现，这种不稳定性主要是由于来自外部工具反馈的分布漂移，导致生成低概率标记。这个问题在连续的回合中不断加剧，导致灾难性的梯度范数爆炸，从而扰乱训练过程。为了解决这个挑战，我们提出了SimpleTIR，这是一种即插即用的算法，旨在稳定多轮TIR训练。它的核心策略是识别并过滤掉包含空回合的轨迹，即那些既不产生代码块也不返回最终答案的回合。通过将这些有问题的轨迹从策略更新中移除，SimpleTIR有效地阻止了有害的高幅度梯度，从而稳定了学习动态。大量实验表明，SimpleTIR在具有挑战性的数学推理基准测试中实现了最先进的性能，显著将AIME24得分从文本基线22.1提升至50.5，起始于Qwen2.5-7B基础模型。此外，SimpleTIR通过避免监督微调的限制，鼓励模型发现多样而复杂的推理模式，如自我纠正和交叉验证。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n该工作聚焦于 **多轮工具集成推理（Tool-Integrated Reasoning, TIR）场景下，使用端到端强化学习（Zero RL）训练大语言模型（LLM）时的训练不稳定与梯度爆炸问题**。核心痛点在于：\n\n*   **分布漂移**：外部工具（如 Python 解释器）反馈的文本分布与 LLM 预训练分布差异大，导致后续轮次生成低概率 token。\n*   **梯度爆炸**：低概率 token 使得重要性采样比 ρ 与梯度范数中的概率敏感项同时放大，造成梯度范数灾难性增长。\n*   **信用分配错位**：稀疏的终端奖励无法区分早期正确推理与后期低概率错误，导致策略偏向保守的单轮输出。\n\n为根治上述问题，论文提出 **SimpleTIR**：一种即插即用的轨迹过滤算法，通过识别并剔除包含“空转回合（void turns）”的轨迹（即既无完整代码块也无最终答案的回合），阻断有害的高幅梯度，稳定训练动态，同时保留 Zero RL 带来的多样化推理策略发现能力。\n\n**Q2**: 有哪些相关研究？\n\n以下研究按主题分类，与 SimpleTIR 在方法、场景或目标上存在直接关联：\n\n1\\. Zero RL 与纯文本推理\n\n*   **DeepSeek-R1**（DeepSeek-AI Team, 2025）：首次展示从 base 模型出发、仅用结果奖励即可激发链式思维。\n*   **SimpleRL-Zoo**（Zeng et al., 2025）：开源复现 Zero RL 的“食谱”，覆盖多种开源 base 模型。\n*   **Open-Reasoner-Zero**（Hu et al., 2025）：论证无 KL 正则的 vanilla PPO 即可规模化 Zero RL。\n*   **DAPO**（Yu et al., 2025）：提出高 clip-ratio、任务过滤等技巧提升 Zero RL 稳定性。\n*   **Dr. GRPO**（Liu et al., 2025b）：去掉长度归一化项以简化 GRPO 训练。\n\n2\\. 工具集成推理（TIR）的 RL 训练\n\n*   **ReTool**（Feng et al., 2025）：先冷启动 SFT 再 RL，使用 Qwen2.5-Math-Instruct。\n*   **ToRL**（Li et al., 2025b）与 **Effective CIR**（Bai et al., 2025）：在数学专用模型上探索 RL 训练配方。\n*   **ZeroTIR**（Mai et al., 2025）：唯一与 SimpleTIR 同样坚持 Zero RL 设置，提出若干正交稳定技巧。\n*   **Search-R1 / R1-Searcher**（Jin et al., 2025; Song et al., 2025）：聚焦搜索工具，面向问答任务。\n\n3\\. RL 训练稳定性通用技术\n\n*   **熵正则与重采样**：CURE（Li et al., 2025a）、Entropy Mechanism（Cui et al., 2025）、ProRL（Liu et al., 2025a）。\n*   **重要性采样比控制**：GMPO（Zhao et al., 2025）、GSPO（Zheng et al., 2025a,b）、Yao et al. (2025)。\n*   **轨迹过滤**：Group Filtered PO（Shrivastava et al., 2025）。\n*   **负梯度更新**：Zhu et al. (2025) 证明仅使用负优势可提升稳定性。\n\n4\\. 理论分析\n\n*   **Lin & Xu (2025)**：从理论上解释 TIR 为何优于纯文本推理，SimpleTIR 为其提供实验佐证。\n\nSimpleTIR 与上述工作的关系：\n\n*   与 Zero RL 研究共享“无冷启动”理念；\n*   与 TIR 研究共享“工具调用”场景，但坚持 Zero RL；\n*   与稳定性研究正交，可叠加熵正则、IS 裁剪等方法；\n*   为 Lin & Xu (2025) 的理论结论提供大规模实证。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过 **SimpleTIR** 算法，从“根因诊断 → 理论分析 → 轨迹过滤 → 训练流程”四个层面系统性地解决了多轮 TIR 在 Zero RL 中的不稳定与梯度爆炸问题。\n\n1\\. 根因诊断：低概率 token 的级联效应\n\n*   **现象**：外部工具反馈（Python 执行结果）与 LLM 预训练分布差异大 → 后续轮次生成低概率 token。\n*   **后果**：\n    *   重要性采样比 {% raw %}$\\\\rho\\_{i,t}(\\\\theta)=\\\\pi\\_\\\\theta/\\\\pi\\_{\\\\theta\\_\\\\text{old}}${% endraw %} 无界放大；\n    *   梯度范数 $|\\\\nabla\\_{z\\_t}\\\\mathcal{J}\\_\\\\text{TIR}|\\_2$ 随 $P(c)\\\\to 0$ 而爆炸（命题 3.1）；\n    *   稀疏终端奖励无法区分早期正确与晚期错误，信用分配错位。\n\n2\\. 理论分析：梯度范数公式化\n\n命题 3.1 给出梯度范数显式表达式：\n\n$$ |\\\\nabla\\_{z\\_t}\\\\mathcal{J}_\\\\text{TIR}|_2 = m_{i,t}\\\\sum\\_j m_{i,j}\\\\rho\\_{i,t}(\\\\theta)g\\_{i,t}|\\\\hat{A}\\_i|\\\\sqrt{1-2P(c)+\\\\sum\\_j P(j)^2} $$\n\n*   **关键项**：\n    *   $\\\\rho\\_{i,t}(\\\\theta)$：低 $P(c)$ 导致 $\\\\rho$ 爆炸；\n    *   $\\\\sqrt{1-2P(c)+\\\\sum\\_j P(j)^2}$：低 $P(c)$ 使根号项趋近于 1，梯度持续高位。\n\n3\\. 轨迹过滤：剔除“空转回合”（void turns）\n\n*   **定义**：任何回合既无完整代码块也无最终答案（如重复文本、提前 eos）。\n*   **算法**：\n    1.  每回合正则检测代码块与答案；\n    2.  若发现 void turn，**整轨迹屏蔽**（mask policy loss），不进入 GRPO 更新；\n    3.  零额外成本，即插即用。\n\n4\\. 训练流程与工程细节\n\n*   **Zero RL 设置**：无 SFT 冷启动，直接从 Qwen2.5-7B/32B base 模型训练。\n*   **实现技巧**：\n    *   无 chat template，工具反馈前缀统一为 `\"Code Execution Result:\"`；\n    *   每段代码自动带 `final_answer()` 函数，允许单回合终止；\n    *   最大 10 轮、24 k token，异步沙盒执行。\n\n效果验证\n\n*   **稳定性**：梯度范数无尖峰，训练曲线平滑（图 1、图 5）。\n*   **性能**：AIME24 从 22.1 → 50.5（7B），超越所有 Zero RL 基线（表 1）。\n*   **多样性**：Zero RL 自发涌现交叉验证、渐进推理、自纠错等策略（表 3、图 6）。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **训练稳定性、最终性能、消融验证、行为分析** 四个维度展开系统实验，覆盖 7 B 与 32 B 两种规模、六类数学基准、多种对比方法与消融设置。\n\n1\\. 主实验：端到端性能对比\n\n维度\n\n设置\n\n结果摘要\n\n**模型规模**\n\nQwen2.5-7B / 32B base\n\n均从完全未对齐的 base 模型启动\n\n**基准**\n\nAIME24/25、MATH500、Olympiad、AMC23、HMMT Feb 25\n\n共 6 个高难度数学集\n\n**对比类别**\n\n① 无 TIR 的 Zero RL（SimpleRL-Zoo, DAPO）  \n② 冷启动 TIR RL（ReTool, ToRL, Effective CIR, ARPO）  \n③ 同样 Zero RL + TIR（ZeroTIR）\n\nSimpleTIR 在 Zero RL 组全面领先；7 B 在 AIME24 达 50.5，32 B 达 59.9，刷新 SOTA\n\n2\\. 训练动态曲线\n\n*   **指标**：平均准确率、梯度范数、响应长度、代码使用频率\n*   **观察**：\n    *   SimpleTIR 训练 1200 步无梯度爆炸（图 1、图 5 Top）\n    *   将最大轮数从 1 → 5 → 10，响应长度与 Math500 分数单调提升，AIME24 则饱和，提示不同任务所需轮数差异（图 5 Top）\n\n3\\. 消融实验：验证过滤策略必要性\n\n方法\n\nAIME24\n\nMath500\n\n训练稳定性\n\n**Naive Multi-Turn**（无过滤）\n\n20.8\n\n73.1\n\n梯度爆炸\n\n**Low-Prob Filtering**（仅屏蔽极低概率 token）\n\n23.3\n\n72.8\n\n仍不稳定\n\n**High-Ratio Filtering**（仅屏蔽高 IS 比 token）\n\n26.3\n\n75.0\n\n仍不稳定\n\n**Stop Generation w/o Filtering**（遇 void turn 提前停，但仍计入 loss）\n\n26.1\n\n77.3\n\n信用分配错位\n\n**SimpleTIR**（整轨迹屏蔽 void turns）\n\n**50.5**\n\n**88.4**\n\n稳定收敛\n\n4\\. 行为分析：涌现推理模式\n\n*   **标注方式**：用 Claude-3.7-Sonnet 对 100 条正确解答进行模式计数（表 3）\n*   **三类模式**\n    \n    ReTool（冷启动 SFT）\n    \n    SimpleTIR-32B（Zero RL）\n    \n    Progressive Reasoning\n    \n    18.9 %\n    \n    **46.5 %**\n    \n    Cross Validation\n    \n    82.4 %\n    \n    86.0 %\n    \n    Error Correction\n    \n    25.8 %\n    \n    **38.0 %**\n    \n*   **结论**：Zero RL 显著增加“渐进推理”与“自纠错”出现频率，验证其策略多样性优势（图 6、附录 B.2）。\n\n5\\. 附加分析\n\n*   **失败案例剖析**：展示含 void turn 的轨迹如何导致后续输出崩溃（附录 B.1、表 4）。\n*   **超参数与提示模板**：公开完整配置与多轮 TIR prompt，确保可复现（附录 C.2、表 6）。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向既可深化 SimpleTIR 的核心思想，也能将其推广到更广阔的 LLM-Agent 场景：\n\n1\\. 过滤准则的泛化与自适应\n\n*   **任务无关的 void turn 定义**：当前准则依赖“代码块 + 最终答案”的显式格式；对搜索、API 调用等多工具场景需重新形式化。\n*   **动态阈值**：用在线估计的 token 概率分布或梯度范数自适应调整过滤门限，而非硬规则。\n\n2\\. 长程交互与信用分配\n\n*   **超越 10 轮**：研究当最大轮数增至数十甚至上百轮时的稳定性—是否会出现新一轮梯度爆炸或奖励稀疏灾难。\n*   **细粒度信用分配**：引入 per-turn 稀疏奖励或基于执行结果差异的塑形奖励，缓解终端奖励的延迟问题。\n\n3\\. 工具链扩展与沙盒效率\n\n*   **异构工具**：将 Python 解释器扩展至 SQL、Bash、浏览器等，验证 void turn 过滤策略在多工具环境下的通用性。\n*   **高效异步沙盒**：\n    *   开发 JIT 编译或容器复用技术，降低代码执行延迟；\n    *   实现完全异步 rollout + 奖励计算，避免当前“同步等待”瓶颈。\n\n4\\. 理论深化\n\n*   **分布漂移度量**：构造可计算的 drift 指标（如 KL(工具反馈 || 预训练分布)），与梯度范数做实时关联监控。\n*   **重要性采样比的重加权**：设计新的 IS 权重或梯度裁剪策略，使低概率 token 的梯度贡献可控，而非直接丢弃整条轨迹。\n\n5\\. 训练效率与系统优化\n\n*   **混合精度与梯度检查点**：长上下文 + 多轮生成导致显存峰值高，探索 FP8、序列并行等方案。\n*   **分层策略**：显式分离“高层回合策略”与“低层 token 策略”，用不同网络或不同更新频率减少耦合带来的方差。\n\n6\\. 行为与安全性研究\n\n*   **错误放大风险**：当工具返回错误结果时，模型是否会因 void turn 过滤不足而持续信任错误信息？\n*   **自动化红队**：利用过滤后的失败轨迹构造对抗 prompt，测试模型在极端工具输出下的鲁棒性。\n\n7\\. 跨任务迁移\n\n*   **非数学领域**：代码生成、数据分析、科学模拟等任务通常需要更长交互链，验证 void turn 假设是否依然成立。\n*   **多模态工具**：若工具返回图像、音频或结构化表格，需重新定义“空转”并设计对应的过滤逻辑。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\nSimpleTIR 通过“过滤空转回合”这一极简策略，首次在 Zero RL 框架下稳定地训练出多轮工具集成推理大模型，并在高难度数学基准上刷新 SOTA，同时自发涌现出多样化推理行为。\n\n核心内容速览\n\n维度\n\n关键信息\n\n**问题**\n\n多轮 TIR + Zero RL 训练常因外部工具反馈导致分布漂移、低概率 token 累积，引发梯度爆炸与信用分配错位。\n\n**方法**\n\nSimpleTIR：在每回合检测是否“既无完整代码块也无最终答案”（void turn），整轨迹屏蔽，阻断有害梯度。\n\n**理论**\n\n命题 3.1 给出梯度范数与 token 概率的显式关系，解释为何低概率 token 必然导致梯度爆炸。\n\n**实验**\n\n7 B/32 B base 模型 → AIME24 从 22.1→50.5/59.9；消融证实 void-turn 过滤是稳定关键；Zero RL 比冷启动 SFT 产生更多“渐进推理+自纠错”策略。\n\n**贡献**\n\n① 首个 Zero RL 多轮 TIR 稳定训练方案；② 即插即用、零额外成本；③ 开源代码与完整超参。\n\n**局限与未来**\n\n过滤准则需向搜索、SQL 等多工具泛化；轮数、沙盒效率、长程信用分配仍待优化。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是SimpleTIR%3A%20End-to-End%20Reinforcement%20Learning%20for%20Multi-Turn%20Tool-Integrated%20Reasoning，链接是  https://arxiv.org/pdf/2509.02479  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02479  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1756943220801","title":"LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model","description":"In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.","url":"https://huggingface.co/papers/2509.00676","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.801Z","metadata":{"authors":["Xiyao Wang","Chunyuan Li","Jianwei Yang","Kai Zhang","Bo Liu","Tianyi Xiong","Furong Huang"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.CV","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.00676.pdf","arxivUrl":"https://arxiv.org/abs/2509.00676","arxivId":"2509.00676","coolPaperUrl":"https://papers.cool/arxiv/2509.00676","published":"2025-08-31T03:08:02Z","updated":"2025-08-31T03:08:02.000Z","zh_summary":"在视觉语言建模中，批评模型通常被训练用来评估输出——分配标量评分或成对偏好——而不是生成响应。这种与生成响应的策略模型之间的分离已经根深蒂固，以至于很少考虑直接将批评者用于策略。在这项工作中，我们挑战了这一传统。我们提议将偏好标记的批评数据集重组为可验证的训练信号，并直接对基础生成模型进行强化学习，生成LLaVA-Critic-R1，一个训练用于优化偏好判断的多模态批评者，同时保持完全的生成能力。令人惊讶的是，LLaVA-Critic-R1不仅成为顶尖的批评者，还成为一个具有竞争力的策略模型——在26个视觉推理和理解基准中，与使用领域内数据训练的专业推理视觉语言模型匹敌或超越，平均比其基础模型（Qwen-2.5-VL-7B）提升了+5.7%。将这种方法扩展到现有的强推理视觉语言模型会产生LLaVA-Critic-R1+，进一步提升策略性能而不牺牲批评质量，在7B规模的MMMU上实现了71.9的最新性能。最后，我们展示了增强的批评能力对推理的好处：在测试时应用自我批评，在五个具有代表性的推理任务上平均提升了+13.8%，而无需额外训练。我们的结果表明，基于批评数据的强化学习训练可以产生一个在评估和生成方面都表现出色的统一模型，为可扩展、自我改进的多模态系统提供了一条简单路径。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n**核心问题：**  \n传统视觉-语言模型（VLM）将“策略模型”（生成响应）与“评价模型”（评估响应质量）视为两个分离的模块。评价模型（critic）通常仅用于训练阶段提供奖励信号或推理阶段筛选候选答案，但**从未被直接当作策略模型使用**。这种割裂导致：\n\n1.  **资源浪费**：需分别训练策略和评价模型，增加计算与数据成本。\n2.  **能力局限**：评价模型无法直接生成响应，策略模型缺乏自我纠错能力。\n3.  **扩展性差**：测试阶段需依赖外部评价模型或人工规则（如多数投票），难以自适应优化。\n\n**论文的突破性目标：**  \n**证明评价模型本身即可作为强大的策略模型**，通过以下方式实现统一：\n\n*   **数据重构**：将偏好标注的critic数据转化为可验证的强化学习信号（无需人工打分或蒸馏）。\n*   **统一训练**：直接对基础生成模型进行强化学习（RL），使其在优化评价能力的同时，**自发获得策略生成能力**。\n*   **双重验证**：展示该统一模型（LLaVA-Critic-R1）在26个视觉推理/理解任务上，**同时达到SOTA评价性能与超越专用策略模型的生成性能**，并支持测试时自我优化（self-critique）。\n\n**Q2**: 有哪些相关研究？\n\n以下研究从不同角度为 LLaVA-Critic-R1 提供了方法论、数据或评估基础，可分为四大类：\n\n1\\. 多模态 Critic / Reward 模型\n\n*   **LLaVA-Critic** (Xiong et al., 2024, 2025)  \n    首个大规模生成式多模态评价模型，用 SFT+知识蒸馏产出细粒度反馈，但仅用于评价而非生成。\n*   **InternLM-XComposer2.5-Reward** (Zang et al., 2025)  \n    轻量级 reward 模型，采用 pairwise 打分，未探索 critic 数据对策略能力的迁移。\n*   **VL-RewardBench / MM-RLHF** (Li et al., 2025; Zhang et al., 2025c)  \n    提供高质量偏好标注数据与基准，被本文直接用作 RL 训练信号。\n\n2\\. 强化学习与推理增强\n\n*   **DeepSeek-R1** (DeepSeek-AI, 2025)  \n    文本大模型中通过纯 RL 激发长链推理（long-CoT），为本文“无蒸馏 RL”路线提供先例。\n*   **Vision-R1** (Huang et al., 2025a,b)  \n    将 DeepSeek-R1 的推理能力蒸馏到 VLM，再执行 RL，展示“先蒸馏后 RL”范式；本文则证明**无需蒸馏**即可达到同等或更好策略性能。\n*   **MM-Eureka / OpenVLThinker / VL-Cogito** (Meng et al., 2025; Deng et al., 2025; Yuan et al., 2025)  \n    基于规则或课程 RL 提升多模态推理，但均依赖带 GT 答案的 reasoning 数据；本文使用**无 GT 的 critic preference 数据**完成策略提升。\n\n3\\. 测试时扩展（Test-Time Scaling）\n\n*   **OpenAI o1** (OpenAI, 2024) / **GLM-4.1V-Thinking** (Team et al., 2025b)  \n    通过延长推理链在推理阶段扩展计算，但未引入内部 critic。\n*   **VisualPRM / GenPRM** (Wang et al., 2025b,c; Zhao et al., 2025a)  \n    训练过程奖励模型在测试时为候选步骤打分，需额外 critic 网络；LLaVA-Critic-R1 则**自身同时扮演策略与 critic**，实现 self-critique。\n*   **Large Language Monkeys** (Brown et al., 2024)  \n    通过重复采样+多数投票提升性能；本文显示 self-critique 显著优于多数投票。\n\n4\\. 训练策略与消融研究\n\n*   **GRPO** (Shao et al., 2024)  \n    群体相对策略优化算法，被本文直接采用作为 RL 目标。\n*   **R1-ShareVL / SRPO** (Yao et al., 2025; Wan et al., 2025)  \n    探索多模态 RL 中的规则设计与反思机制，为本文格式奖励 (r\\_format) 与思考模板设计提供参考。\n\n综上，LLaVA-Critic-R1 在以下方面实现突破：\n\n*   **首次**将 critic 数据转化为纯 RL 信号，无需蒸馏或 GT 答案即可提升策略性能；\n*   **首次**在单一模型内同时实现 SOTA critic 与策略能力，并支持 test-time self-critique；\n*   通过系统消融验证了 critic 训练与策略训练的互补性，为后续“自我改进”多模态系统提供新范式。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过以下四步流程将“评价模型”转化为“既是评价器又是策略器”的统一模型，并验证其有效性：\n\n1\\. 数据重组：把偏好标注变成可验证 RL 信号\n\n*   **输入**：40 k 条〈图像, 问题, 回答 A, 回答 B, 偏好标签〉的 pairwise 数据（来自 VLFeedback / RLHF-V）。\n*   **关键操作**：\n    *   丢弃 GPT 生成的长链理由与 5 项人工评价指标，仅保留“哪个回答更好”这一**可验证的二元标签**。\n    *   设计极简 prompt（见表 1），要求模型输出：  \n        `<think> …自主推理… </think>`  \n        `\\boxed{1}` 或 `\\boxed{2}` 或 `\\boxed{Two responses are equally good.}`\n    *   结果：把传统用于 SFT 的 critic 数据，改造成**无需人工打分、可直接计算奖励的 RL 任务**。\n\n2\\. 强化学习训练：直接对基础生成模型做 critic-RL\n\n*   **算法**：Group Relative Policy Optimization (GRPO)。\n*   **奖励函数**：  \n    ${% raw %}$ r = \\\\underbrace{0.9 \\\\cdot r\\_{\\\\text{pref}}}_{\\\\text{偏好匹配}} + \\\\underbrace{0.1 \\\\cdot r_{\\\\text{format}}}\\_{\\\\text{格式合规}} ${% endraw %}$\n    *   {% raw %}$r\\_{\\\\text{pref}} = 1${% endraw %} 当且仅当模型给出的选择与标签一致；否则 0。\n    *   {% raw %}$r\\_{\\\\text{format}} = 1${% endraw %} 当且仅当正确使用了 `<think>` 与 `\\boxed{}`。\n*   **训练细节**：\n    *   不经过任何蒸馏 SFT，**冷启动**于 Qwen-2.5-VL-7B。\n    *   训练 400 步即可收敛；显存占用与常规 RLHF 相同。\n\n3\\. 双重验证：策略性能与评价性能同步提升\n\n*   **策略能力**：\n    *   在 26 个视觉推理 / 理解基准平均提升 **+5.7%**，与 4 个专门做 reasoning-RFT 的 7 B 模型持平或更好。\n    *   训练后的模型称为 **LLaVA-Critic-R1**；若继续用更强的 ThinkLite-VL-7B 做起点，则得到 **LLaVA-Critic-R1+**，在 MMMU 达到 7 B 新 SOTA 71.9。\n*   **评价能力**：\n    *   在 Visual RewardBench 上比基线提升 **+10.8%**，证明其 critic 质量仍保持顶尖。\n\n4\\. 测试时扩展：利用自身 critic 能力做 self-critique\n\n*   **流程**（Best-of-128）：\n    1.  作为策略器生成 128 条候选回答（temperature=0.9）。\n    2.  作为 critic 器对所有候选进行**递归 pairwise 比较**，逐轮淘汰，直到只剩 1 条。\n*   **结果**：\n    *   在 5 个代表性任务上平均再提升 **+13.8%**，显著优于多数投票或外部 critic。\n    *   性能随采样规模单调上升至 128 条后饱和，验证了 critic 能力的有效性。\n\n总结\n\n通过“数据→奖励→RL→双重验证→测试时扩展”的闭环，论文首次证明：\n\n> **仅用 critic 偏好数据做 RL，即可让生成模型同时成为顶尖策略器与评价器，并可自我迭代提升。**\n\n**Q4**: 论文做了哪些实验？\n\n论文共设计 **四大类 12 组实验**，覆盖训练范式、模型规模、数据策略、测试时扩展及消融分析，系统验证“critic-RL 训练”对策略与评价能力的提升。结果均以 26 个视觉基准的平均分或具体指标呈现。\n\n1\\. 主实验：策略与评价双重性能验证\n\n模型\n\n训练数据\n\n评价维度\n\n关键结果\n\n**LLaVA-Critic-R1**\n\n40 k pairwise critic 数据（无 GT 答案）\n\n26 个视觉基准\n\n相比 Qwen-2.5-VL-7B **平均↑5.7%**；MMMU ↑4.6、MathVista ↑6.2\n\n**LLaVA-Critic-R1+**\n\n同上，但起始于 ThinkLite-VL-7B\n\n同上\n\n**MMMU 71.9**（7 B SOTA）；MathVista 82.1；MathVerse 74.1；CharXiv 62.5\n\n2\\. 测试时扩展实验（Best-of-N）\n\n策略\n\n采样数\n\n平均提升\n\n典型任务峰值\n\nSelf-Critique\n\n128\n\n**+13.8%**\n\nMathVista 78.9、MMMU 66.4\n\nMajority Vote\n\n128\n\n+3.2%\n\n早早在 16–64 饱和\n\nBase-Model-Critic\n\n128\n\n+4.1%\n\n始终低于 self-critique\n\n3\\. 消融实验\n\n3.1 训练策略对比（Qwen-2.5-VL-7B 为统一基底）\n\n训练方式\n\n评价能力\n\n策略能力\n\n结论\n\n**Critic-only** (LLaVA-Critic-R1)\n\n68.1\n\n57.38\n\ncritic 最强\n\n**Policy-only** (ThinkLite-VL)\n\n57.2\n\n56.72\n\n策略强，评价弱\n\n**Mixed**\n\n64.9\n\n56.31\n\n两者皆次优\n\n**Policy→Critic**\n\n64.9\n\n58.16\n\n**最佳折中**\n\n**Critic→Policy**\n\n62.9\n\n57.06\n\ncritic 掉点\n\n3.2 SFT vs. 冷启动 RFT\n\n路径\n\nRewardBench\n\n通用任务\n\n结论\n\nSFT→RFT\n\n67.5\n\n56.01\n\ncritic 略升，策略降\n\n**冷启动 RFT** (LLaVA-Critic-R1)\n\n**68.1**\n\n**57.38**\n\n策略+评价双赢\n\n3.3 格式奖励与感知能力分解\n\n条件\n\n感知&VQA\n\n推理\n\n说明\n\n仅推理模板（inference-time）\n\n57.4 → 57.4\n\n42.1 → 43.2\n\n格式约束有帮助\n\n仅格式奖励 RL\n\n57.4 → 57.4\n\n43.2 → 43.9\n\n无感知增益\n\n**完整 critic-RL**\n\n**60.7**\n\n**45.8**\n\n感知+推理双重提升\n\n3.4 critic/policy 能力相关性\n\n*   **训练曲线**（图 3）：前 200 步 critic 与 policy 性能 **Pearson r ≈ 0.94**；后期 critic 波动，policy 350 步后略降，显示过拟合迹象。\n\n4\\. 跨基底普适性验证\n\n基底模型\n\n训练后名称\n\n平均提升\n\n新 SOTA\n\nMiMo-VL-7B-RL-2508\n\nLLaVA-Critic-R1+\n\n**+2.6%**\n\nMMMU 71.9、MathVista 82.1\n\nLlama-3.2-11B-Vision\n\nLLaVA-Critic-R1\n\n**+2.6%**\n\nMMMU 49.0 (↑13.4%)、MathVision 30.9 (↑30.9%)\n\n5\\. 案例研究\n\n*   **计数任务**（表 11）：Qwen-Thinking 漏数 1 个，LLaVA-Critic-R1+ 正确 10 个。\n*   **GUI 代理**（表 12）：Qwen 点错坐标，LLaVA-Critic-R1+ 正确点击 Format → Centered。\n\n结论\n\n通过 26 个基准、4 类消融、3 种基底模型及 128 倍采样扩展，论文系统证明：\n\n> **仅用 40 k 无 GT 的 critic 偏好数据做 RL，即可让单一模型在策略与评价双重任务上同时达到或超越专用模型，并具备可扩展的自我改进能力。**\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可视为 LLaVA-Critic-R1 的“下一步”研究议程，按优先级与可行性分为 **短期可验证** 与 **长期挑战性** 两类：\n\n短期可验证（3–6 个月）\n\n1.  **数据规模与质量消融**\n    \n    *   将 40 k pairwise 数据扩展到 100 k / 500 k，观察 critic 与 policy 能力的 scaling law：  \n        ${% raw %}$ \\\\Delta\\_{\\\\text{policy}} \\\\propto \\\\log N\\_{\\\\text{pairwise}} \\\\quad ? ${% endraw %}$\n    *   引入 **人工精标** vs **GPT 标注** 的对比实验，量化标注源对最终性能的敏感度。\n2.  **奖励函数细粒度化**\n    \n    *   在 {% raw %}$r\\_{\\\\text{pref}}${% endraw %} 中引入 **边际奖励**（margin reward）：  \n        ${% raw %}$ r\\_{\\\\text{margin}} = \\\\text{score}_{\\\\text{win}} - \\\\text{score}_{\\\\text{lose}} ${% endraw %}$  \n        以鼓励模型区分“显著更好”而非“微弱更好”的回答。\n3.  **任务迁移谱系**\n    \n    *   用 **医疗影像、自动驾驶、GUI 自动化** 等垂直场景的偏好数据做 critic-RL，验证“跨域零样本策略提升”是否依然成立。\n4.  **推理-预算最优曲线**\n    \n    *   绘制 **Best-of-N** 性能-延迟 Pareto 前沿：  \n        $$ \\\\text{Perf}(N) = \\\\alpha \\\\log N + \\\\beta $$  \n        为实际部署提供 N 的选择依据。\n\n长期挑战性（6–18 个月）\n\n1.  **统一在线 self-play 框架**\n    \n    *   让模型在 **无标注** 场景下循环：\n        1.  生成回答 → 2. 自我 critic 打分 → 3. 用高分回答构造新 pairwise → 4. 继续 RL。  \n            研究该循环是否收敛及如何避免模式崩溃。\n2.  **过程级 critic 信号**\n    \n    *   将 pairwise 标签细化为 **逐步正确性标签**（类似 Visual-PRM），训练 **过程奖励模型** 与策略模型共享 backbone，实现 **逐步自我修正**。\n3.  **多模态链式思考的可解释性**\n    \n    *   通过注意力可视化或探针分析，验证 critic-RL 是否真正学到 **视觉-符号对齐** 而非语言捷径。  \n        设计 **对抗性幻觉检测** 基准，测试模型自我发现幻觉的能力。\n4.  **异构评价任务统一**\n    \n    *   将文本、音频、视频评价数据统一为 **通用多模态 critic 格式**，研究单一模型能否在 **跨模态评价** 与 **跨模态生成** 上同时达到高水准。\n5.  **理论分析：评价-策略互信息**\n    \n    *   建立信息论框架，量化 critic 训练对策略熵的影响：  \n        $$ I(\\\\text{critic logits}; \\\\text{policy improvement}) $$  \n        为“评价能力越强 → 策略越好”提供可计算的理论上界。\n\n附：可直接复用的实验设置\n\n*   **基底模型**：InternVL2-8B、GPT-4o-mini-Vision\n*   **数据**：开源 VLFeedback-1M、即将发布的 MM-RLHF-2.0\n*   **评估**：新设 **Self-Critique Robustness Suite**（含对抗样本、分布外场景）\n\n这些方向既可直接落地验证，也能为未来“完全自监督的多模态智能体”奠定理论与工程基础。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n**仅用 40 k 条“偏好标注”做强化学习，即可把原本只能打分的 critic 模型变成既能生成回答又能自我评价的通用策略模型，并在 26 个视觉基准上刷新 7 B 尺度 SOTA。**\n\n核心贡献\n\n维度\n\n内容\n\n关键数字\n\n**问题**\n\ncritic 与 policy 长期割裂，训练成本高、推理需外部评价\n\n—\n\n**方法**\n\n将 pairwise 偏好数据重构成可验证 RL 任务，用 GRPO 直接对 Qwen-2.5-VL-7B 做 critic-RL\n\n无 SFT、无蒸馏\n\n**结果**\n\n单一模型同时获得顶尖 critic 与策略能力\n\n26 基准平均 ↑5.7%，MMMU 71.9（7 B SOTA）\n\n**扩展**\n\n测试时 Best-of-128 self-critique 再提升\n\n5 任务平均 ↑13.8%\n\n**普适**\n\n在 MiMo-VL、LLaMA-3.2-Vision 上复现提升\n\n平均 ↑2.6%\n\n实验全景\n\n*   **主实验**：LLaVA-Critic-R1 / R1+ 在 6 大类 26 个基准上全面领先专用 reasoning 模型。\n*   **消融**：\n    1.  critic-only vs policy-only vs 混合训练 → Policy→Critic 策略最优。\n    2.  SFT→RFT vs 冷启动 RFT → 冷启动策略+评价双赢。\n    3.  格式奖励与感知能力分解 → 两者协同提升。\n*   **跨基底**：MiMo-VL、LLaMA-3.2-Vision 复制成功。\n*   **案例**：计数、GUI 代理等具体场景展示 critic-RL 带来的细粒度改进。\n\n结论\n\n论文首次实证：**“训练一个更好的 critic，就是训练一个更好的 policy。”** 这为构建可自我改进、统一推理-评价的多模态大模型提供了极简而有效的路径。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是LLaVA-Critic-R1%3A%20Your%20Critic%20Model%20is%20Secretly%20a%20Strong%20Policy%20Model，链接是  https://arxiv.org/pdf/2509.00676  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.00676  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1756943220802","title":"ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding","description":"Video multimodal large language models (Video-MLLMs) have achieved remarkable progress in video understanding. However, they remain vulnerable to hallucination-producing content inconsistent with or unrelated to video inputs. Previous video hallucination benchmarks primarily focus on short-videos. They attribute hallucinations to factors such as strong language priors, missing frames, or vision-language biases introduced by the visual encoder. While these causes indeed account for most hallucinations in short videos, they still oversimplify the cause of hallucinations. Sometimes, models generate incorrect outputs but with correct frame-level semantics. We refer to this type of hallucination as Semantic Aggregation Hallucination (SAH), which arises during the process of aggregating frame-level semantics into event-level semantic groups. Given that SAH becomes particularly critical in long videos due to increased semantic complexity across multiple events, it is essential to separate and thoroughly investigate the causes of this type of hallucination. To address the above issues, we introduce ELV-Halluc, the first benchmark dedicated to long-video hallucination, enabling a systematic investigation of SAH. Our experiments confirm the existence of SAH and show that it increases with semantic complexity. Additionally, we find that models are more prone to SAH on rapidly changing semantics. Moreover, we discuss potential approaches to mitigate SAH. We demonstrate that positional encoding strategy contributes to alleviating SAH, and further adopt DPO strategy to enhance the model's ability to distinguish semantics within and across events. To support this, we curate a dataset of 8K adversarial data pairs and achieve improvements on both ELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.","url":"https://huggingface.co/papers/2508.21496","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.802Z","metadata":{"authors":["Hao Lu","Jiahao Wang","Yaolun Zhang","Ruohui Wang","Xuanyu Zheng","Yepeng Tang","Dahua Lin","Lewei Lu"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2508.21496.pdf","arxivUrl":"https://arxiv.org/abs/2508.21496","arxivId":"2508.21496","coolPaperUrl":"https://papers.cool/arxiv/2508.21496","published":"2025-08-29T10:25:03Z","updated":"2025-08-29T10:25:03.000Z","zh_summary":"视频多模态大型语言模型（Video-MLLMs）在视频理解方面取得了显著进展。然而，它们仍然易受到与视频输入不一致或无关的内容引起的幻觉影响。之前的视频幻觉基准主要集中在短视频上。他们将幻觉归因于一些因素，如强大的语言先验、缺失的帧或视觉编码器引入的视觉-语言偏见。尽管这些原因确实解释了大多数短视频中的幻觉，但它们仍然过于简单化了幻觉的原因。有时，模型产生的输出不正确，但帧级语义是正确的。我们将这种类型的幻觉称为语义聚合幻觉（SAH），它出现在将帧级语义聚合到事件级语义组的过程中。鉴于由于多事件之间语义复杂性的增加，SAH在长视频中特别关键，有必要分离并彻底调查这种类型幻觉的原因。为了解决上述问题，我们引入了ELV-Halluc，这是第一个专门针对长视频幻觉的基准，能够系统地调查SAH。我们的实验确认了SAH的存在，并表明其随着语义复杂性的增加而增加。此外，我们发现模型在快速变化的语义上更容易受到SAH的影响。此外，我们讨论了减轻SAH的潜在方法。我们证明了位置编码策略有助于缓解SAH，并进一步采用DPO策略来增强模型在事件内和跨事件区分语义的能力。为此，我们整理了一个包含8000个对抗数据对的数据集，并在ELV-Halluc和Video-MME上实现了改进，包括SAH比率显著降低27.7%。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文《ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding》聚焦的核心问题是：\n\n*   **长视频理解中的“语义聚合幻觉”（Semantic Aggregation Hallucination, SAH）**。  \n    具体而言，当视频多模态大语言模型（Video-MLLM）能够正确感知每一帧的局部语义，却在将这些帧级语义聚合成事件级语义时，错误地把属于事件 A 的视觉元素关联到事件 B 的概念上，从而产生与视频内容不符的幻觉。这种幻觉在短视频中影响有限，但在包含多个事件的**长视频**中，由于语义复杂度显著增加，SAH 变得尤为突出且难以被现有基准发现。\n\n因此，论文旨在：\n\n1.  **首次系统性地揭示并量化 SAH**：指出以往短视频幻觉基准忽视的“正确感知却错误聚合”现象。\n2.  **构建专门基准 ELV-Halluc**：通过事件级长视频、对抗式三元组问答设计（GT / In-Video Hallucination / Out-Video Hallucination）和 SAH Ratio 指标，精确度量模型在多事件语义场景下的聚合错误。\n3.  **分析 SAH 的诱因**：验证其与语义复杂度（事件数量、细粒度语义变化速度）正相关，而与视频时长、模型规模无必然联系。\n4.  **提出并验证缓解策略**：\n    *   改进位置编码（VideoRoPE）强化帧-事件绑定；\n    *   利用 DPO（Direct Preference Optimization）在 8 K 对抗样本上训练，使模型更倾向正确事件语义，最终在 ELV-Halluc 上将 SAH Ratio 降低 27.7%，同时在通用基准 Video-MME 上提升 0.9%。\n\n**Q2**: 有哪些相关研究？\n\n与 ELV-Halluc 直接相关的研究可分为两大类：\n\n1.  **视频理解基准**（关注长视频、事件级或细粒度理解）；\n2.  **视频幻觉评测基准**（聚焦幻觉类型、成因与度量）。\n\n以下按类别列出代表性工作，并说明与本文的差异。\n\n1\\. 视频理解基准（Video Understanding Benchmarks）\n\n基准\n\n核心关注点\n\n与 ELV-Halluc 的关系\n\n**Video-MME** (Fu et al. 2025)\n\n覆盖短视频、中视频、长视频的多维度理解任务\n\n未专门考察幻觉，尤其未区分 SAH\n\n**MVBench** (Li et al. 2024c)\n\n多模态视频理解综合评测\n\n同样未针对长视频中语义聚合错误\n\n**ETBench** (Liu et al. 2024)\n\n事件级时间定位与推理\n\n强调事件边界，但未设计幻觉诊断\n\n**Video-Holmes** (Cheng et al. 2025)\n\n复杂推理能力（如侦探式问答）\n\n关注推理深度而非幻觉类型\n\n**LVBench** (Wang et al. 2024b)\n\n超长（>1 h）视频理解\n\n评测长视频整体理解，未细分幻觉\n\n**MLVU** (Zhou et al. 2024)\n\n多任务长视频理解（整体、单细节、多细节推理）\n\n未引入对抗式幻觉问答\n\n**EgoSchema** (Mangalam et al. 2023)\n\n第一视角长视频问答\n\n场景特殊，未探讨事件间语义错位\n\n2\\. 视频幻觉评测基准（Hallucination Evaluation in Video-MLLMs）\n\n基准\n\n幻觉分类/方法\n\n与 ELV-Halluc 的差异\n\n**VideoHallucer** (Wang et al. 2024c)\n\n将幻觉分为 intrinsic（与视频矛盾） vs. extrinsic（无法仅由视频判定）\n\n面向短视频，未讨论事件间语义聚合错误\n\n**EventHallusion** (Zhang et al. 2024a)\n\n通过罕见事件与误导语境分析语言先验 & 视觉-语言偏差\n\n仅短视频，未显式定义 SAH\n\n**VidHalluc** (Li, Im & Fazli 2025)\n\n评估动态片段中的时序幻觉，指出视觉编码器归纳偏差导致幻觉\n\n未聚焦长视频多事件场景\n\n**ARGUS** (Rawal et al. 2025)\n\n开放式视频字幕中的幻觉与遗漏评测\n\n任务形式不同，未设计事件级对抗问答\n\n**MHBench** (Kong et al. 2025)\n\n运动幻觉（Motion Hallucination）\n\n关注运动描述真实性，而非跨事件语义错位\n\n小结\n\n现有基准要么**仅覆盖短视频**（VideoHallucer, EventHallusion, VidHalluc），要么**未将幻觉细分为语义聚合错误**（Video-MME, MVBench, LVBench 等）。ELV-Halluc 首次把“正确感知却错误聚合”的 SAH 现象独立出来，并通过长视频、事件级对抗问答与 SAH Ratio 指标，填补了上述空白。\n\n**Q3**: 论文如何解决这个问题？\n\n为系统性地解决“语义聚合幻觉（SAH）”这一长视频理解中的新问题，论文从 **诊断 → 量化 → 归因 → 缓解** 四个环节展开，形成闭环方案。\n\n1\\. 诊断：构建专用基准 ELV-Halluc\n\n*   **事件级长视频数据**  \n    手动收集 348 条“事件-事件”型长视频（平均 672 s），每段包含 2–10 个清晰分离的事件，天然放大跨事件语义混淆风险。\n*   **对抗式三元组问答**  \n    每个事件对应 3 条问答：  \n    ① Ground-Truth（GT）  \n    ② In-Video Hallucination（将 A 事件的对象/动作/细节挪到 B 事件）  \n    ③ Out-Video Hallucination（引入完全不存在的内容）  \n    仅当模型对 GT 答“Yes”且对幻觉答“No”才算正确。\n*   **SAH Ratio 指标**  \n    ${% raw %}$ \\\\text{SAH Ratio}= \\\\frac{\\\\text{OutAcc}-\\\\text{InAcc}}{1-\\\\text{InAcc}} ${% endraw %}$  \n    该比值直接衡量“因跨事件语义错位导致的幻觉”在全部幻觉中的占比，消除绝对性能差异带来的干扰。\n\n2\\. 量化：大规模实验刻画 SAH 规律\n\n*   **覆盖 16 个模型**（14 个开源 1 B–78 B + GPT-4o / Gemini-2.5-flash）。\n*   **发现一：SAH 与语义复杂度正相关**  \n    事件数量 ↑ → SAH Ratio ↑（图 6a）；视频时长与 SAH 无显著关系。\n*   **发现二：SAH 与语义变化速度正相关**  \n    细粒度、变化快的语义（视觉细节 > 动作 > 对象 > 陈述内容）SAH Ratio 更高（图 7）。\n*   **发现三：帧数 ↑ 带来双重效应**  \n    整体幻觉准确率提升，但 SAH Ratio 也同步升高（图 8），说明“信息更丰富”≠“聚合更准”。\n\n3\\. 归因：定位 SAH 产生机制\n\n*   **帧-事件绑定薄弱** 导致模型在聚合阶段把正确帧级特征错配到错误事件。\n*   **位置编码缺陷** 是重要诱因：标准 RoPE 对长视频跨事件时序关系建模不足。\n\n4\\. 缓解：两条正交策略\n\n4.1 强化位置编码——VideoRoPE\n\n*   在 Qwen2-VL 框架上比较 4 种 RoPE 变体\n    *   vanilla RoPE / TAD-RoPE / m-RoPE / **VideoRoPE**\n*   结果：VideoRoPE 将 SAH Ratio 降到 0.88%（表 3），验证“增强时序绑定即可抑制 SAH”。\n\n4.2 偏好优化——DPO（Direct Preference Optimization）\n\n*   **数据构造**  \n    用剩余 148 条视频的 GT 与幻觉字幕构建 8 K 对偏好样本（4 K In-Video + 4 K Out-Video）。\n*   **训练设置**  \n    以 Qwen2.5-VL-7B 为基座，三种方案：  \n    ① 仅用 In-Video 对；② 仅用 Out-Video 对；③ 混合 8 K 对。\n*   **结果**\n    *   方案① SAH Ratio 从 8.3% → 6.0%（↓27.7%），ELV-Halluc 整体准确率 ↑0.3，Video-MME ↑0.4。\n    *   方案② 反而略升 SAH，说明“拒绝完全不存在内容”对抑制跨事件错位帮助有限。\n    *   方案③ 在两者间取得平衡，但未超越方案①。\n*   **注意力可视化**（图 9）  \n    DPO 后模型对错误但语义相似区域的关注显著下降，提供可解释证据。\n\n总结流程\n\n    graph TD\n        A[发现 SAH 现象] --> B[构建 ELV-Halluc 基准]\n        B --> C[实验量化 SAH 规律]\n        C --> D[定位帧-事件绑定薄弱为根因]\n        D --> E[VideoRoPE 强化位置编码]\n        D --> F[DPO 抑制幻觉偏好]\n        E & F --> G[SAH Ratio ↓27.7%<br>Video-MME ↑0.9%]\n    \n\n通过“基准+指标+归因+策略”的完整闭环，论文首次将长视频中的语义聚合幻觉问题转化为可度量、可干预的研究方向。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **ELV-Halluc 基准** 共设计并执行了 4 组核心实验，覆盖 **诊断→量化→归因→缓解** 全链路。实验结果均在正文与附录中给出详细数值、图表与统计检验。以下按实验目标归纳：\n\n1\\. 诊断实验：ELV-Halluc 基准整体评估\n\n**目的**：验证 SAH 存在性，给出各模型在长视频幻觉上的基线。  \n**设置**\n\n*   16 个模型：14 个开源（1 B–78 B）+ GPT-4o + Gemini-2.5-Flash\n*   4,800 条二元 QA（4 个语义粒度 × 6 题/粒度 × 200 视频）\n*   指标：In-Video Acc、Out-Video Acc、SAH Ratio\n\n**关键结果**（表 2）\n\n模型\n\nIn-Video Acc\n\nOut-Video Acc\n\nSAH Ratio↓\n\nQwen2.5-VL-32B\n\n24.5 %\n\n24.5 %\n\n**0.2 %**\n\nGPT-4o\n\n8.3 %\n\n8.7 %\n\n1.0 %\n\nGemini-2.5-Flash\n\n58.0 %\n\n47.0 %\n\n11.0 %\n\n> 所有模型 In-Video Acc 显著低于 Out-Video Acc，首次实证 SAH 普遍存在。\n\n2\\. 量化实验：SAH 与视频/模型属性关系\n\n2.1 事件数量 vs. SAH Ratio\n\n*   横轴：每视频事件数（4–10）\n*   纵轴：SAH Ratio\n*   结果（图 6a）：Pearson r ≈ 0.85，**事件越多 → SAH 越严重**。\n\n2.2 视频时长 vs. SAH Ratio\n\n*   时长分桶：0–3 min、3–6 min、…、>21 min\n*   结果（图 6b）：时长与 SAH Ratio **无显著线性关系**（p > 0.1）。\n\n2.3 语义粒度 vs. SAH Ratio\n\n*   四类粒度：Visual Details / Action / Object / Declarative Content\n*   14 个开源模型平均（图 7）：\n    *   Visual Details：SAH Ratio ≈ 0.12\n    *   Action：≈ 0.08\n    *   Object：≈ 0.05\n    *   Declarative：≈ 0.02  \n        → **细粒度、快速变化语义更易出现 SAH**。\n\n3\\. 归因实验：帧数 & 模型规模对幻觉的影响\n\n3.1 采样帧数消融\n\n*   模型：Qwen2.5-VL 3 B/7 B/32 B/72 B、InternVL3 8 B/14 B/32 B\n*   帧数：16 / 32 / 64 / 128 / 256（Qwen），16 / 32 / 48 / 64（InternVL3）\n*   结果（图 8）：\n    *   **整体幻觉准确率**：随帧数 ↑ 而 ↑（信息增多）。\n    *   **SAH Ratio**：大多数模型随帧数 ↑ 而 ↑（语义错位风险增大）。\n    *   **例外**：Qwen2.5-VL-32B 因 RL 后训练，SAH Ratio 反而下降。\n\n3.2 规模效应\n\n*   横轴：模型参数量（1 B→78 B）\n*   结论：\n    *   整体幻觉准确率与规模 **正相关**。\n    *   SAH Ratio **无明显规模单调性**（32 B 模型优于 72 B）。\n\n4\\. 缓解实验：位置编码与 DPO\n\n4.1 位置编码策略比较\n\n*   基座：Qwen2-VL\n*   4 种 RoPE：vanilla / TAD / m-RoPE / **VideoRoPE**\n*   结果（表 3）：\n    \n    方法\n    \n    SAH Ratio↓\n    \n    vanilla RoPE\n    \n    1.82 %\n    \n    VideoRoPE\n    \n    **0.88 %**\n    \n\n4.2 DPO 偏好优化\n\n*   基座：Qwen2.5-VL-7B\n    \n*   数据集：4 K In-Video 对 / 4 K Out-Video 对 / 8 K 混合\n    \n*   结果（表 4）：\n    \n    训练设置\n    \n    ELV-Halluc SAH Ratio↓\n    \n    Video-MME Avg↑\n    \n    基座\n    \n    8.3 %\n    \n    61.9 %\n    \n    \\+ In-Video 4 K\n    \n    **6.0 % (↓27.7 %)**\n    \n    62.3 %\n    \n    \\+ Out-Video 4 K\n    \n    8.6 % (↑3.6 %)\n    \n    62.8 %\n    \n    \\+ 8 K 混合\n    \n    8.4 % (↑1.2 %)\n    \n    62.4 %\n    \n*   **注意力可视化**（图 9）：DPO 后模型对错误区域注意力显著下降，提供机制解释。\n    \n\n5\\. 稳定性验证（附录实验）\n\n*   选取 4 个不同规模/系列模型，各跑 3 次\n*   结果（附录图 1）：Overall Acc 与 SAH Ratio 的标准差 < 0.5 %，**ELV-Halluc 评测稳定可靠**。\n\n实验全景一览\n\n实验类别\n\n变量\n\n结论\n\n基准诊断\n\n16 模型 × 4,800 QA\n\nSAH 普遍存在\n\n复杂度分析\n\n事件数、时长、语义粒度\n\n事件数↑、粒度细 → SAH↑\n\n帧数/规模\n\n采样帧数、模型大小\n\n帧数↑ 整体好但 SAH↑；规模对 SAH 无单调改善\n\n缓解策略\n\nRoPE 变体、DPO 训练\n\nVideoRoPE 与 In-Video DPO 分别将 SAH Ratio 降至 0.88 % 与 6.0 %\n\n通过上述多维度实验，论文完整揭示了 SAH 的分布规律、影响因素与可行缓解路径。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可在 ELV-Halluc 的基础上继续深入，分为 **数据与任务扩展、机制剖析、模型改进、评测维度、应用落地** 五大类，并给出可立即着手的研究思路。\n\n1\\. 数据与任务扩展\n\n*   **更长、更密、更真实的长视频**  \n    当前平均 11 min、最大 ~30 min；可收集 1 h+ 的 vlog、监控、直播流，验证 SAH 在极端长度下的饱和或爆炸趋势。\n*   **非事件型长视频**  \n    电影、电视剧、纪录片缺乏清晰事件边界，需设计无监督事件分割 + 幻觉检测联合任务。\n*   **多语言 / 多模态音频**  \n    引入非英语旁白、背景音乐、环境声，研究音频-视觉-文本三重聚合幻觉。\n*   **时序反事实 QA**  \n    不仅问“发生了什么”，而是“如果事件 A 提前 10 s，结果会怎样”，考察模型对因果链的聚合鲁棒性。\n\n2\\. 机制剖析\n\n*   **细粒度时序注意力可视化**  \n    在帧级、事件级、跨事件级分别计算注意力熵，定位 SAH 发生的“时间-语义”热点。\n*   **层级化记忆结构消融**  \n    显式构建“帧缓存 → 事件缓存 → 全局记忆”三级存储，逐层关闭跨层写入，量化哪一级最易引入错位。\n*   **误差传播实验**  \n    人为在某一事件注入微小错误（如颜色偏差），观察后续事件的描述如何被级联放大，验证“语义漂移”假设。\n\n3\\. 模型改进\n\n*   **事件级对比学习**  \n    在同一视频内构造正负事件对，拉近同一事件帧特征、推远不同事件特征，直接优化聚合边界。\n*   **动态事件提示（Event Prompt Tuning）**  \n    为每个事件生成可学习的 prompt token，强制模型在回答前先“聚焦”到对应事件区间。\n*   **时序-因果 Transformer**  \n    在自注意力中显式加入因果掩码 + 事件 ID 编码，阻断未来事件对当前事件的信息泄漏。\n*   **链式聚合策略**  \n    先让模型输出事件级草稿，再基于草稿做二次精修，类似“CoT for video”，降低一次性长上下文聚合压力。\n\n4\\. 评测维度\n\n*   **细粒度 SAH 子类型**  \n    将 SAH 拆分为“对象错位、动作错位、时间错位、因果错位”四类，分别计算 Ratio，指导针对性改进。\n*   **人类一致性校准**  \n    引入多人标注的“可接受误差区间”，区分“严格幻觉”与“人类亦模糊”的灰色案例，避免过度优化。\n*   **在线/流式场景**  \n    模拟实时直播：边接收帧边回答问题，评测模型在“部分可见”条件下的 SAH 动态曲线。\n\n5\\. 应用落地\n\n*   **长视频摘要与搜索**  \n    将 SAH Ratio 作为自动摘要系统的风险指标，触发“人工二次确认”阈值。\n*   **法律/监控领域**  \n    针对监控长视频，开发“事件证据链”校验工具，若检测到 SAH 高置信区间，提示审查员重点回放。\n*   **教育与培训**  \n    在 MOOC 长视频中实时检测 SAH，自动插入“注意：此处讲解可能与画面不符”提示，提升学习可靠性。\n\n可立即启动的短期项目（3–6 个月）\n\n1.  **在现有 348 条视频上训练事件级对比学习模块**，复用 ELV-Halluc 标注即可。\n2.  **将 VideoRoPE 迁移到 InternVL3-78B**，观察更大模型是否进一步降低 SAH。\n3.  **构建 50 小时直播流子集**，用滑动窗口实时计算 SAH Ratio，绘制“幻觉热图”。\n\n这些方向既可直接基于 ELV-Halluc 的数据与指标展开，也能推动长视频理解从“能看懂”走向“看得准、信得过”。\n\n**Q6**: 总结一下论文的主要内容\n\n论文核心概要：ELV-Halluc——长视频语义聚合幻觉基准与缓解方案\n\n1.  **问题定义**  \n    首次指出长视频理解中一种被忽视的幻觉类型——**语义聚合幻觉（SAH）**：模型虽能正确识别每一帧的局部语义，却在将帧级语义聚合成事件级语义时，把属于事件 A 的内容张冠李戴到事件 B，导致最终答案与视频不符。\n    \n2.  **基准构建**\n    \n    *   **ELV-Halluc**：首个面向长视频的幻觉评测基准。\n    *   348 条“事件-事件”型长视频（平均 11 min），人工精修事件级字幕。\n    *   对抗式三元组问答：GT / In-Video Hallucination / Out-Video Hallucination。\n    *   提出 **SAH Ratio** 指标：  \n        ${% raw %}$ \\\\text{SAH Ratio}= \\\\frac{\\\\text{OutAcc}-\\\\text{InAcc}}{1-\\\\text{InAcc}} ${% endraw %}$  \n        专门量化跨事件语义错位导致的幻觉比例。\n3.  **大规模实验发现**\n    \n    *   **普遍存在**：16 个主流 Video-MLLM（1 B–78 B）均出现显著 SAH。\n    *   **复杂度驱动**：事件数量↑、语义粒度越细（视觉细节 > 动作 > 对象 > 陈述内容），SAH Ratio 越高。\n    *   **帧数双刃剑**：采样帧数↑ 提升整体准确率，却同时放大 SAH。\n    *   **规模非解药**：增大模型规模对 SAH 无单调改善。\n4.  **缓解策略**\n    \n    *   **VideoRoPE**：改进的旋转位置编码，强化帧-事件绑定，SAH Ratio 降至 0.88%。\n    *   **DPO 偏好优化**：用 8 K 对抗样本（4 K In-Video + 4 K Out-Video）训练 Qwen2.5-VL-7B，SAH Ratio 从 8.3% → 6.0%（↓27.7%），Video-MME 提升 0.9%，且不损通用性能。\n5.  **贡献总结**\n    \n    *   首次系统定义并评测长视频语义聚合幻觉。\n    *   提供公开基准、指标与 8 K 对抗训练数据，推动长视频理解从“能看懂”走向“看得准、信得过”。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是ELV-Halluc%3A%20Benchmarking%20Semantic%20Aggregation%20Hallucinations%20in%20Long%20Video%20Understanding，链接是  https://arxiv.org/pdf/2508.21496  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.21496  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1756943220802","title":"VerlTool: Towards Holistic Agentic Reinforcement Learning with Tool Use","description":"Reinforcement Learning with Verifiable Rewards (RLVR) has demonstrated success in enhancing LLM reasoning capabilities, but remains limited to single-turn interactions without tool integration. While recent Agentic Reinforcement Learning with Tool use (ARLT) approaches have emerged to address multi-turn tool interactions, existing works develop task-specific codebases that suffer from fragmentation, synchronous execution bottlenecks, and limited extensibility across domains. These inefficiencies hinder broader community adoption and algorithmic innovation. We introduce VerlTool, a unified and modular framework that addresses these limitations through systematic design principles. VerlTool provides four key contributions: (1) upstream alignment with VeRL ensuring compatibility and simplified maintenance, (2) unified tool management via standardized APIs supporting diverse modalities including code execution, search, SQL databases, and vision processing, (3) asynchronous rollout execution achieving near 2$\\times$ speedup by eliminating synchronization bottlenecks, and (4) comprehensive evaluation demonstrating competitive performance across 6 ARLT domains. Our framework formalizes ARLT as multi-turn trajectories with multi-modal observation tokens (text/image/video), extending beyond single-turn RLVR paradigms. We train and evaluate models on mathematical reasoning, knowledge QA, SQL generation, visual reasoning, web search, and software engineering tasks, achieving results comparable to specialized systems while providing unified training infrastructure. The modular plugin architecture enables rapid tool integration requiring only lightweight Python definitions, significantly reducing development overhead and providing a scalable foundation for tool-augmented RL research. Our code is open-sourced at https://github.com/TIGER-AI-Lab/verl-tool.","url":"https://huggingface.co/papers/2509.01055","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.802Z","metadata":{"authors":["Dongfu Jiang","Yi Lu","Zhuofeng Li","Zhiheng Lyu","Ping Nie","Haozhe Wang","Alex Su","Hui Chen","Kai Zou","Chao Du","Tianyu Pang","Wenhu Chen"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.AI","cs.CL","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.01055.pdf","arxivUrl":"https://arxiv.org/abs/2509.01055","arxivId":"2509.01055","coolPaperUrl":"https://papers.cool/arxiv/2509.01055","published":"2025-09-01T01:45:18Z","updated":"2025-09-01T01:45:18.000Z","zh_summary":"带可验证奖励的强化学习（RLVR）已成功提升大语言模型（LLM）的推理能力，但仍然仅限于单回合交互，没有工具集成。尽管最近出现的代理强化学习与工具使用（ARLT）方法旨在解决多回合工具交互的问题，现有的工作却开发了特定任务的代码库，导致碎片化、同步执行瓶颈和跨领域有限的可扩展性。这些低效率阻碍了更广泛的社区采用和算法创新。我们推出了 VerlTool，这是一个统一且模块化的框架，通过系统的设计原则解决这些局限性。VerlTool 提供了四个关键贡献：（1）与 VeRL 的上游对齐，确保兼容性和简化维护；（2）通过标准化的 API 统一工具管理，支持包括代码执行、搜索、SQL 数据库和视觉处理等多种模式；（3）异步上线执行，消除同步瓶颈，达到接近 2 倍的加速；（4）全面评估展示在 6 个 ARLT 领域中的竞争性能。我们的框架将 ARLT 形式化为具有多模态观察 Token（文本/图像/视频）的多回合轨迹，超越了单回合 RLVR 范式。我们在数学推理、知识问答、SQL 生成、视觉推理、网络搜索和软件工程任务上训练和评估模型，取得了与专用系统相媲美的结果，同时提供统一的训练基础设施。模块化插件架构使得工具集成迅速，仅需轻量级的 Python 定义，显著减少了开发开销，并为增强工具的 RL 研究提供了可扩展的基础。我们的代码已在 https://github.com/TIGER-AI-Lab/verl-tool 开源。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文针对当前 **Agentic Reinforcement Learning with Tool use（ARLT）** 研究中的三大瓶颈，提出统一框架 VERLTOOL，以解决以下核心问题：\n\n1.  **碎片化与可扩展性差**  \n    现有 ARLT 系统多为任务特定实现，工具逻辑与训练循环紧耦合，导致：\n    \n    *   每新增工具或任务需重写大量代码；\n    *   社区难以复现或迁移已有工作。\n2.  **同步执行效率低**  \n    传统框架按批次同步等待工具返回，GPU/CPU 出现大量空闲“气泡”，在多工具、多回合场景下吞吐率急剧下降。\n    \n3.  **多模态支持不足**  \n    主流 RL 框架仅处理文本，而视觉、视频、SQL 结果等多模态工具输出缺乏统一接口，难以在同一训练流程中无缝集成。\n    \n4.  **单回合 RLVR 的局限**  \n    RLVR 仅优化单回合、可验证答案，无法建模多回合交互中的信用分配、错误恢复与策略迭代，限制了模型在真实环境中的自主决策能力。\n    \n\nVERLTOOL 通过 **模块化插件架构 + 异步 rollout + 统一多模态 API** 将 ARLT 形式化为多回合、多模态观测的强化学习问题，并在数学推理、知识问答、SQL 生成、视觉推理、网页搜索、软件工程等六大领域验证其通用性与性能。\n\n**Q2**: 有哪些相关研究？\n\n论文在第 2 节与附录 B 中对相关研究进行了系统梳理，可归纳为四大类：\n\n1\\. 工具增强推理（Tool-Integrated Reasoning）\n\n*   **Prompt-based 方法**\n    \n    *   HuggingGPT (Shen et al., 2023)\n    *   Chameleon (Lu et al., 2023)\n    *   MultiTool-CoT (Inaba et al., 2023)  \n        特点：零样本或少量示例即可调用工具，但静态模板难以适应复杂多步任务。\n*   **监督微调方法**\n    \n    *   Toolformer (Schick et al., 2023) – 自举式工具调用数据合成\n    *   GPT4Tools (Yang et al., 2023) – 蒸馏 GPT-4 工具轨迹\n    *   LIMO (Ye et al., 2025) – 少样本激发长链推理  \n        特点：提升单轮调用准确性，缺乏动态错误修正能力。\n\n2\\. 强化学习驱动的工具使用（RL for Agentic Tool Use）\n\n*   **早期探索**\n    \n    *   ReAct (Yao et al., 2022) – 思维链+行动交替提示\n    *   TPTU-v2 (Kong et al., 2023) – 任务规划与工具调用联合微调\n*   **近期 ARLT 工作**\n    \n    *   ToRL (Li et al., 2025c) – Python 解释器 + GRPO 数学推理\n    *   SEARCH-R1 (Jin et al., 2025) – 搜索引擎 + GRPO 问答\n    *   Pixel-Reasoner (Su et al., 2025) – 视觉操作 + 好奇心奖励\n    *   RETOOL (Feng et al., 2025) – PPO 训练工具调用策略\n    *   EXCOT / THINK2SQL (Zhai et al., 2025; Papicchio et al., 2025) – SQL 执行反馈 + GRPO\n\n3\\. RL 训练框架\n\n框架\n\n工具支持\n\n同步/异步\n\n备注\n\nOPENRLHF (Hu et al., 2024)\n\nPython, FAISS\n\n同步\n\n无原生多模态工具\n\nVERL (Sheng et al., 2024)\n\nPython, FAISS\n\n同步\n\n被 VERLTOOL 继承\n\nAREAL (Fu et al., 2025)\n\n搜索\n\n异步\n\n轻量级异步 RL\n\nROLL (Wang et al., 2025a)\n\n无\n\n异步\n\n专注算法效率\n\nSKYRL (Cao et al., 2025)\n\nBash, SQL, Image\n\n同步\n\n需容器化部署\n\n**VERLTOOL**\n\n**全六类工具**\n\n**异步**\n\n**统一插件架构**\n\n4\\. 领域特定基准与数据集\n\n*   **数学** – MATH-500, GSM8K, AIME24, DeepMath-103k\n*   **知识问答** – NQ, TriviaQA, HotpotQA, MuSiQue\n*   **SQL** – Spider, Spider-DK, Spider-SYN\n*   **视觉推理** – V\\* Bench\n*   **网页搜索** – GAIA, HLE\n*   **软件工程** – SWE-bench, R2E-Gym\n\n综上，VERLTOOL 在工具类型覆盖、训练范式（多回合 RL）、系统效率（异步 rollout）与模块化设计方面，与现有研究形成互补或超越。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过提出 **VERLTOOL** 框架，从系统架构、训练范式与工程实现三个层面解决前述瓶颈，具体手段如下：\n\n1\\. 统一且可扩展的系统架构\n\n**问题对应**：碎片化、任务特定代码难以复用  \n**解决方案**：\n\n*   **模块化插件设计**\n    \n    *   每个工具实现为 `BaseTool` 子类，仅需 30–50 行 Python 代码即可注册新工具（图 3）。\n    *   工具与 RL 训练逻辑完全解耦，通过标准化 JSON API 通信，支持 **代码、搜索、SQL、Bash、图像、MCP** 六类工具（表 2）。\n*   **Upstream-Aligned 继承**\n    \n    *   将 VERL 作为 git submodule，确保与上游 RL 库同步更新，避免维护分叉。\n\n2\\. 异步多回合训练范式\n\n**问题对应**：同步等待导致 GPU 空闲、单回合 RLVR 无法建模长程交互  \n**解决方案**：\n\n*   **异步 Rollout 引擎**\n    \n    *   每完成一条轨迹的 action 生成即独立调用工具，消除批次级同步等待（图 2）。\n    *   8×H100 实测：数学任务提速 **1.32×**，搜索任务 **1.97×**（表 3）。\n*   **多回合 ARLT 形式化**\n    \n    *   轨迹定义为  \n        $$ \\\\tau = {a\\_0, o\\_0, a\\_1, o\\_1, \\\\dots, a\\_n} $$  \n        其中 $o\\_i$ 为工具返回的多模态观测 token，显式参与奖励计算但被 mask 不参与策略梯度（式 5），避免 off-policy 不稳定。\n*   **并行后端**\n    \n    *   轻量级工具用 `ThreadPoolExecutor`，重工具（Docker、浏览器）可切换 **Ray** 分布式执行，支持弹性资源调度。\n\n3\\. 多模态与奖励工程\n\n**问题对应**：视觉、SQL 结果等非文本观测难以统一处理  \n**解决方案**：\n\n*   **统一观测编码**\n    \n    *   图像/视频/表格结果经工具服务器序列化为文本或 base64，直接拼接到上下文；tokenization 采用 **分离策略** 避免边界 token 漂移（图 4）。\n*   **任务特定奖励**\n    \n    *   数学：{% raw %}$R\\_{\\\\text{math}} = R\\_{\\\\text{acc}} - 0.25 \\\\cdot \\\\mathbb{I}\\_{\\\\text{no-tool}}${% endraw %}，鼓励调用解释器验证。\n    *   视觉：额外好奇心奖励 {% raw %}$r\\_{\\\\text{curiosity}} = \\\\max(0.3 - \\\\text{tool-call-ratio}, 0)${% endraw %} 防止过早退化。\n    *   网页搜索：{% raw %}$R\\_{\\\\text{tool}} = +0.1${% endraw %} 仅当触发搜索，平衡效率与信息增益。\n\n4\\. 端到端验证\n\n**问题对应**：缺乏统一基准验证通用性  \n**解决方案**：\n\n*   **六大任务全覆盖**（表 4–8）\n    *   数学推理（VT-Math）：AIME24 达 **30.0%**（7B）\n    *   知识问答（VT-Search）：NQ +43.9 pp 超越直接推理\n    *   SQL 生成（VT-SQL）：Spider-Test **83.4%** 匹配专用系统\n    *   视觉推理（VT-VisualReasoner）：V\\* Bench **82.7%**\n    *   网页搜索（VT-DeepSearch）：GAIA **34.0%**（8B）\n    *   软件工程（VT-SWE）：SWE-Verified **19.5** 分，领先 OpenHands 基线\n\n小结\n\nVERLTOOL 通过 **“插件化工具 + 异步引擎 + 多模态统一接口 + 任务定制奖励”** 的组合拳，将 ARLT 从分散的专用系统升级为可扩展的统一框架，兼顾研究效率与社区复现需求。\n\n**Q4**: 论文做了哪些实验？\n\n论文在 **6 个具有代表性的 ARLT 任务** 上进行了系统实验，覆盖数学推理、知识问答、SQL 生成、视觉推理、网页搜索和软件工程六大领域。所有实验均在 **统一的 VERLTOOL 框架** 内完成，既验证框架通用性，也与现有专用系统对比性能。\n\n1\\. VT-Math：数学推理 + Python 代码解释器\n\n*   **数据集**  \n    DeepMath-103k 训练；MATH-500、GSM8K、AIME24、AMC23、Olympiad Bench 等 7 个基准评估。\n*   **结果**（表 4）\n    *   1.5 B：VT-Math-DAPO **55.5** vs ToRL-1.5B **55.2**\n    *   7 B：VT-Math-DAPO **62.2** vs ToRL-7B **61.1**\n    *   在 AIME24 上 7B 模型首次达到 **36.7–43.3%**，显著优于 Instruct 基线（16.7%）。\n\n2\\. VT-Search：知识问答 + FAISS 检索\n\n*   **数据集**  \n    2018 Wikipedia dump 索引；NQ、TriviaQA、PopQA、HotpotQA、2Wiki、MuSiQue、Bamboogle 共 7 个 QA 基准。\n*   **结果**（表 5）\n    *   3 B：VT-Search-GRPO **34.4** vs Search-R1 **31.2**\n    *   7 B：VT-Search-GRPO **45.9** vs Search-R1 **35.0**（↑10.9 pp）\n    *   在多跳问答（HotpotQA、2Wiki）上优势更明显。\n\n3\\. VT-SQL：NL2SQL + SQL 执行器\n\n*   **数据集**  \n    SkyRL-SQL 训练集；Spider-Dev/Test、Spider-Realistic、Spider-DK、Spider-SYN 评估。\n*   **结果**（表 6）\n    *   VT-SQL（7B）在 Spider-Test 达到 **83.4%**，与 SkyRL-SQL **85.2%** 相当，且显著高于 GPT-4o（83.2%）。\n    *   在跨域 Spider-DK 上 **71.6%**，领先非工具基线 8–12 pp。\n\n4\\. VT-VisualReasoner：视觉推理 + 图像操作\n\n*   **数据集**  \n    Pixel-Reasoner 官方训练集；V\\* Bench 评估。\n*   **结果**（表 7 左）\n    *   GRPO-Complex 奖励下 **82.7%**，超越 Pixel-Reasoner-7B（84.3%）外的所有基线。\n    *   证明好奇心奖励能有效防止工具调用退化。\n\n5\\. VT-DeepSearch：开放网页问答 + Google Search API\n\n*   **数据集**  \n    SimpleDeepSearcher + WebSailor 1 k 混合训练；GAIA、HLE 评估。\n*   **结果**（表 7 右）\n    *   8B 模型在 GAIA 上 **34.0%**，比 DeepSeek-R1-671B（25.2%）↑8.8 pp。\n    *   使用浏览器代理（QwQ-32B）进一步将 HLE 提升到 **8.4%**。\n\n6\\. VT-SWE：软件工程 + Bash/Python 执行\n\n*   **数据集**  \n    R2E-Lite 训练集；SWE-Verified 评估。\n*   **结果**（表 8）\n    *   Qwen3-8B 基线 3.6 → VT-SWE **19.5** 分，超越 OpenHands-7B-Agent（11.0）和 SkyRL-v0（14.6）。\n    *   在 R2E-Gym 环境下同样领先（10.4 → 19.5）。\n\n训练动态与消融分析\n\n*   **工具调用演化**（图 5 与正文 §4.2）\n    *   数学任务：平均 1–4 轮即可收敛；SQL 任务 5 轮后调用率迅速下降（记忆效应）。\n    *   网页搜索：调用次数随训练持续上升，因信息不可记忆。\n*   **异步 vs 同步**（表 3）\n    *   搜索任务异步加速 **1.97×**；数学任务 **1.32×**。\n\n小结\n\n任务\n\n关键指标\n\nVERLTOOL 结果\n\n相对基线提升\n\nMath\n\nAIME24\n\n43.3% (7B)\n\n+26.6 pp vs Instruct\n\nSearch\n\nNQ/TriviaQA 平均\n\n45.9% (7B)\n\n+10.9 pp vs Search-R1\n\nSQL\n\nSpider-Test\n\n83.4%\n\n持平 SkyRL-SQL\n\nVisual\n\nV\\* Bench\n\n82.7%\n\n次佳系统外最优\n\nDeepSearch\n\nGAIA\n\n34.0%\n\n+8.8 pp vs 671B 模型\n\nSWE\n\nSWE-Verified\n\n19.5\n\n+5.0 pp vs 最强基线\n\n实验充分证明：在 **统一框架** 下，VERLTOOL 可同时达到或超越各任务专用系统的性能，且具备跨模态、跨任务的通用训练能力。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 VERLTOOL 之后的系统性扩展或深入研究点，按“框架-算法-应用”三层归纳：\n\n1\\. 框架层：工具生态与系统效率\n\n*   **异构工具编排**\n    \n    *   引入 **DAG / 图调度器**，让一次推理可并行调用多个工具（如同时搜索+SQL+代码），而非当前线性链式调用。\n    *   支持 **工具链缓存**（memoization），对确定性子查询直接复用历史结果，减少冗余执行。\n*   **更细粒度异步**\n    \n    *   将工具内部步骤（如搜索的“查询→抓取→摘要”）拆分为可中断协程，实现 **token-level streaming**，进一步压缩 GPU 空闲。\n*   **自适应资源分配**\n    \n    *   基于工具历史耗时与失败率，在线学习为不同工具动态分配 CPU/GPU/内存配额；可结合 Ray autoscaler 实现 **弹性集群**。\n\n2\\. 算法层：训练与推理策略\n\n*   **长程信用分配**\n    \n    *   当前 GRPO 仅在回合末给奖励。可实验：\n        *   **每一步稀疏奖励**（如 SQL 执行错误即负奖励）；\n        *   **Hindsight Credit Assignment**（事后把成功轨迹的每一步都视为正例）。\n*   **工具调用预算控制**\n    \n    *   引入 **可学习预算 token**，让模型在 prompt 中显式决定“剩余调用次数”，避免过度搜索或过早放弃。\n*   **多智能体协同**\n    \n    *   将不同工具封装为 **独立智能体**（搜索 Agent、SQL Agent、代码 Agent），通过消息总线协作，研究多智能体 RL 在工具场景下的 scaling law。\n*   **安全与对齐**\n    \n    *   在奖励函数中加入 **沙箱逃逸检测**、**敏感信息泄露惩罚**，验证 RL 是否仍能保持性能的同时提升安全性。\n\n3\\. 应用层：新领域与评测\n\n*   **具身与机器人物理交互**\n    \n    *   接入 ROS2、Gazebo 等物理仿真工具，训练 LLM 控制机械臂、无人机，研究 **真实物理反馈** 对策略的影响。\n*   **科学计算工具链**\n    \n    *   集成 Wolfram Alpha、MATLAB、Jupyter kernel，构建 **自动定理证明 + 符号计算** 的数学研究助手。\n*   **跨语言 & 跨模态检索**\n    \n    *   在 VT-Search 基础上加入 **多语言维基**、**专利数据库**、**ArXiv 全文**，评测模型在跨语言多跳事实核查上的表现。\n*   **细粒度评测维度**\n    \n    *   除准确率外，增加 **工具调用效率**（token/调用次数）、**鲁棒性**（对抗性错误 API 返回）、**可解释性**（生成工具调用理由）三维指标，建立更全面的 ARLT 排行榜。\n\n4\\. 数据与模型\n\n*   **自动课程生成**\n    \n    *   利用工具反馈自动生成 **难度递增** 的任务（如 SQL 从单表到多表 join），研究课程式 RL 能否加速收敛。\n*   **小模型专用化**\n    \n    *   用 VERLTOOL 训练 **1B 以下** 超小模型，探索在边缘设备上部署工具调用型 Agent 的可行性。\n\n一句话总结\n\nVERLTOOL 已打通“多模态工具 + 异步 RL”的任督二脉，下一步可向 **并行工具编排、长程信用分配、物理世界交互、安全对齐** 四大纵深方向推进，持续拓展 Agentic RL 的边界。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\nVERLTOOL 是一个面向 **Agentic Reinforcement Learning with Tool use（ARLT）** 的统一、高效、可扩展框架，通过模块化插件、异步 rollout 与多模态工具 API，在 6 大任务上实现与专用系统相当或更优的性能，为社区提供了开箱即用的 ARLT 训练基础设施。\n\n主要内容速览\n\n维度\n\n关键要点\n\n**问题**\n\n• 现有 ARLT 代码碎片化、同步执行低效、多模态支持不足  \n• 单回合 RLVR 无法建模多回合工具交互与信用分配\n\n**框架**\n\n• **插件式工具服务器**：新增工具仅需 1 个 Python 文件  \n• **异步 rollout**：按轨迹级并行，实测 1.3–2× 提速  \n• **上游对齐**：以 VERL 为子模块，持续兼容更新\n\n**训练范式**\n\n• 将 ARLT 形式化为多回合轨迹 τ = {a₀,o₀,…,aₙ}  \n• 采用 GRPO，观测 token 被 mask 以稳定训练  \n• 任务定制奖励：准确率 + 工具调用激励\n\n**实验**\n\n• **6 任务全覆盖**：数学、搜索、SQL、视觉、网页、软件工程  \n• **结果**：在 AIME24、Spider-Test、GAIA、SWE-Verified 等基准上均达到或超越专用系统\n\n**贡献**\n\n① 统一工具 API ② 异步高效训练 ③ 多模态无缝支持 ④ 开源社区基础设施\n\n代码与复现： [https://github.com/TIGER-AI-Lab/verl-tool](https://github.com/TIGER-AI-Lab/verl-tool)\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是VerlTool%3A%20Towards%20Holistic%20Agentic%20Reinforcement%20Learning%20with%20Tool%20Use，链接是  https://arxiv.org/pdf/2509.01055  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01055  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1756943220803","title":"POINTS-Reader: Distillation-Free Adaptation of Vision-Language Models for Document Conversion","description":"High-quality labeled data is essential for training accurate document conversion models, particularly in domains with complex formats such as tables, formulas, and multi-column text. However, manual annotation is both costly and time-consuming, while automatic labeling using existing models often lacks accuracy in handling such challenging scenarios. Consequently, training student models by distilling outputs from teacher models can significantly limit their performance in real-world applications. In this paper, we propose a fully automated, distillation-free framework comprising two stages for constructing high-quality document extraction datasets and models capable of handling diverse document formats and layouts. In the first stage, we introduce a method for generating large-scale, diverse synthetic data, which enables a model to extract key elements in a unified format with strong initial performance. In the second stage, we present a self-improvement approach that further adapts the model, initially trained on synthetic data, to real-world documents. Specifically, we first use the fine-tuned model to annotate real documents, then apply a suite of filtering strategies to verify annotation quality, and finally retrain the model on the verified dataset. By iteratively repeating this process, we progressively enhance both the model's conversion capabilities and the quality of the generated data. We train a public POINTS-1.5 model to obtain POINTS-Reader, which surpasses many existing public and proprietary models of comparable or larger size. Our model is available at https://github.com/Tencent/POINTS-Reader.","url":"https://huggingface.co/papers/2509.01215","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.803Z","metadata":{"authors":["Yuan Liu","Zhongyin Zhao","Le Tian","Haicheng Wang","Xubing Ye","Yangxiu You","Zilin Yu","Chuhan Wu","Xiao Zhou","Yang Yu","Jie Zhou"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.01215.pdf","arxivUrl":"https://arxiv.org/abs/2509.01215","arxivId":"2509.01215","coolPaperUrl":"https://papers.cool/arxiv/2509.01215","published":"2025-09-01T07:54:18Z","updated":"2025-09-01T07:54:18.000Z","zh_summary":"高质量的标记数据对于训练准确的文档转换模型至关重要，尤其是在表格、公式和多列文本等复杂格式的领域。然而，手动标注既昂贵又耗时，而使用现有模型进行自动标注往往在处理这些具有挑战性的场景时缺乏准确性。因此，通过从教师模型提炼输出训练学生模型可能会显著限制它们在现实应用中的表现。本文提出了一种完全自动化的无蒸馏框架，包括两个阶段，用于构建高质量文档提取数据集和能够处理多种文档格式和布局的模型。在第一阶段，我们引入了一种生成大规模多样化合成数据的方法，使模型能够以统一格式提取关键元素，并具有强大的初始性能。在第二阶段，我们提出了一种自我改进的方法，进一步调整最初在合成数据上训练的模型以适应真实文档。具体来说，我们首先使用微调后的模型对真实文档进行标注，然后应用一系列过滤策略来验证标注质量，最后在经过验证的数据集上重新训练模型。通过反复迭代这个过程，我们逐步提升模型的转换能力和生成数据的质量。我们训练了一个公共的POINTS-1.5模型以获取POINTS-Reader，其性能超过许多同类或规模更大的现有公有和专有模型。我们的模型可在 https://github.com/Tencent/POINTS-Reader 中获得。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n该论文旨在解决高质量文档转换模型训练过程中面临的两大核心难题：\n\n1.  **高质量标注数据稀缺**  \n    复杂版式（表格、公式、多栏文本）的文档需要大量精确标注，而人工标注成本高昂；现有自动标注方法依赖外部大模型蒸馏，既限制下一代模型研发，又导致学生模型性能受限并继承教师模型偏差。\n    \n2.  **合成数据与真实数据分布差异**  \n    直接利用合成数据训练的模型在真实文档上表现不佳；传统蒸馏方案无法摆脱对外部模型的依赖，且存在性能瓶颈与偏差传播问题。\n    \n\n为此，论文提出**完全自动化、无蒸馏**的两阶段框架，通过大规模合成数据统一格式预训练 + 迭代自提升过滤策略，构建高质量文档转换数据集并训练出超越同体量甚至更大模型的 POINTS-Reader。\n\n**Q2**: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为以下三类，并给出每类的代表性工作与本文的差异点：\n\n1\\. 端到端文档理解/转换模型\n\n*   **Nougat** (Blecher et al., 2023)  \n    350 M 参数，专注学术论文，LaTeX 输出，依赖扫描版 PDF，未处理复杂表格。\n*   **GOT-OCR** (Wei et al., 2024)  \n    716 M 参数，通用 OCR-2.0 框架，仍以 Markdown 为主，未解决表格结构复杂性问题。\n*   **OLMOCR** (Poznanski et al., 2025)  \n    7 B 参数，完全基于 GPT-4o 蒸馏，存在蒸馏偏差与计算开销。\n*   **KOSMOS-2.5** (Lv et al., 2023)  \n    357 M 样本规模，Markdown 表格表示，依赖 Microsoft Read API 蒸馏。\n\n**差异**  \n本文无需任何外部模型蒸馏，采用 HTML 统一表格格式，并通过自提升迭代过滤策略持续提升数据质量。\n\n2\\. 流水线式文档解析系统\n\n*   **MinerU** (Wang et al., 2024a)  \n    模块化流水线，结合布局检测+OCR+后处理，人工规则重，端到端性能仍落后于本文方法。\n*   **Marker** (2024)  \n    基于启发式规则与外部 OCR，对复杂公式与表格支持有限。\n*   **Mathpix** (2024)  \n    商业闭源，以公式识别见长，但表格与多栏布局精度不足。\n\n**差异**  \n本文提出端到端视觉-语言模型，无需复杂流水线即可同时输出文本、表格、公式，并在多项基准上超越流水线方法。\n\n3\\. 通用视觉-语言大模型\n\n*   **Qwen2.5-VL 系列** (Bai et al., 2025)  \n    3 B/7 B/72 B 参数，原生支持任意分辨率，但文档转换任务仍依赖蒸馏数据。\n*   **LLaVA-OneVision** (Li et al., 2024)  \n    通用视觉任务迁移，未针对文档版式优化。\n*   **mPLUG-DocOwl 1.5/2** (Hu et al., 2024a,b)  \n    引入高分辨率编码与结构学习，仍使用公开蒸馏数据训练。\n\n**差异**  \n本文以公开 POINTS-1.5 为基座，通过无蒸馏、自生成的 1.1 M 高质量 HTML 格式数据进行指令微调，实现 3 B 参数模型在表格与公式指标上优于 72 B 蒸馏模型。\n\n**Q3**: 论文如何解决这个问题？\n\n论文提出一套**两阶段、无蒸馏、完全自动化**的框架，以解决高质量文档转换数据稀缺及合成-真实分布差异的问题。核心思路与实施步骤如下：\n\n**阶段一：统一格式预热（Uniform Format Warm-up Stage, UWS）**\n\n1.  **统一输出规范**\n    \n    *   纯文本 → Markdown\n    *   表格 → 精简 HTML（保留合并单元格属性，去 CSS、换行、缩进）\n    *   数学公式 → LaTeX（KaTeX 规范，`$...$` 行内，`$$...$$` 行间）\n2.  **大规模合成数据生成**\n    \n    *   用 LLM 按四类模板生成内容：\n        1.  纯文本\n        2.  文本+公式\n        3.  文本+表格\n        4.  多栏文本+表格\n    *   规则过滤公式/表格合法性后，通过 HTML 模板渲染成图像，得到 **80 万图像-文本对**。\n3.  **基座模型预热训练**\n    \n    *   以公开 POINTS-1.5 为视觉-语言基座，仅用上述合成数据进行指令微调，获得具备统一格式输出能力的初始模型。\n\n**阶段二：迭代自提升（Iterative Self-improvement Stage, ISS）**\n\n1.  **自标注**\n    \n    *   用 UWS 模型对 **200 万 DocMatix 真实文档图像** 进行端到端推理，生成伪标签。\n2.  **三重规则过滤**\n    \n    *   **纯文本**：与 PaddleOCR 结果计算 token-level F1，阈值 0.9 以上保留。\n    *   **表格**：检查每行列数一致性，剔除结构非法样本。\n    *   **公式**：仅做 LaTeX 语法校验，剔除语法错误样本。\n3.  **再训练与迭代**\n    \n    *   将过滤后的高质量数据与 UWS 数据混合，继续指令微调。\n    *   重复「推理 → 过滤 → 再训练」**5 轮**，逐步提升模型对真实分布的适应能力。\n\n**关键设计总结**\n\n*   **无蒸馏**：全程不依赖外部大模型标注，避免性能天花板与偏差继承。\n*   **统一格式**：降低模型学习难度，提升跨元素一致性。\n*   **规则过滤**：轻量级、可扩展的自动化质检，保证每轮训练数据质量。\n*   **合成-真实桥接**：先用合成数据学“格式”，再用自提升学“版式”，实现域迁移。\n\n通过以上流程，最终得到的 **POINTS-Reader（3 B 参数）** 在 OmniDocBench、Fox 等基准上，表格与公式指标超越 Qwen2.5-VL-72B 等更大蒸馏模型，验证了框架的有效性。\n\n**Q4**: 论文做了哪些实验？\n\n论文实验围绕 **数据构建有效性、模型性能、消融分析** 三条主线展开，可归纳为以下 4 组实验：\n\n1\\. 数据构建与质量验证实验\n\n*   **合成数据规模曲线**  \n    将 UWS 阶段每类样本从 10 万线性增至 120 万，观察 OmniDocBench 整体指标。结果显示 80 万样本后性能饱和甚至下降，证明**过度合成会过拟合人工布局**。\n*   **长宽比过滤**  \n    统计合成图像宽高比分布，仅保留区间 $\\\\bigl(\\\\tfrac{2}{5},\\\\tfrac{5}{2}\\\\bigr)$ 内的样本，整体指标再降 1.2 pt，验证**剔除异常形状可提升泛化性**。\n\n2\\. 迭代自提升（ISS）实验\n\n*   **过滤策略消融**  \n    在 DocMatix 200 万图上依次加入  \n    ① 文本 F1≥0.9 过滤  \n    ② 表格结构一致性过滤  \n    ③ 公式语法过滤  \n    结果：每增加一项，对应子任务 Edit Distance 显著下降（文本 0.470→0.374，表格 0.561→0.492，公式 0.514→0.457）。\n*   **迭代轮次曲线**  \n    重复 5 轮「推理-过滤-再训练」，OmniDocBench 整体指标从 0.493 持续降至 0.259；同时保留样本的 F1 与数量均逐轮提升，说明**数据质量与模型能力同步增强**。\n*   **F1 阈值敏感性**  \n    测试 0.70 / 0.80 / 0.90 / 0.95 四档阈值：过低引入噪声，过高丢失多样性，0.90 为最佳折中。\n\n3\\. 主实验：与 SOTA 对比\n\n在 **OmniDocBench-en、Fox-en、PubTabNet** 三个公开基准上，与三类方法对比：\n\n*   **流水线系统**（MinerU、Marker、Mathpix）\n*   **通用 VLM**（Qwen2.5-VL-3/7/72 B）\n*   **专用 OCR 模型**（GOT-OCR、Nougat、Mistral OCR、OLMOCR）\n\n结果：\n\n*   POINTS-Reader（3 B）在 **表格指标** 上领先所有对比模型（OmniDocBench 0.335 vs GOT-OCR 0.532→**\\-0.197**）。\n*   整体指标 0.259，优于 72 B 蒸馏模型 Qwen2.5-VL-72B（0.214）之外的全部对手，且参数量仅 4%。\n\n4\\. 补充消融与案例\n\n*   **初始化策略**  \n    每轮重新加载**预训练权重**优于继续上一轮权重，避免噪声累积。\n*   **数据平衡实验**  \n    对文本/表格/公式按 1:1:1、1:2:4、1:4:8 三种比例重采样，发现**强制平衡反而降低泛化**，保持自然分布最佳。\n*   **案例可视化**  \n    随机抽取 3 份文档，展示第 1 轮与第 5 轮模型输出，直观呈现**表格结构、公式语法、阅读顺序**的显著改进。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 POINTS-Reader 框架的后续研究切入点，按优先级与可行性排序：\n\n1\\. 语言与字体扩展\n\n*   **多语言支持**  \n    当前仅英文；可引入中文、日文等语系，需重新设计合成 prompt 并验证字符集、排版规则差异。\n*   **手写与混合字体**  \n    现有数据均为印刷体；探索手写文本合成（GAN 或 diffusion-based 字体生成）与半合成混合数据，缓解手写场景性能下降。\n\n2\\. 元素与版式升级\n\n*   **图像元素提取**  \n    将图表、流程图、照片等视觉元素纳入统一输出格式，扩展 HTML 标签集（如 `<img>`、`<svg>`），并设计对应过滤规则（分辨率、可读性）。\n*   **复杂版式建模**  \n    引入页眉页脚、脚注、双栏交错公式等更细粒度布局，通过 LayoutLM-style 绝对位置编码或 segment-wise attention 提升空间理解。\n\n3\\. 数据策略优化\n\n*   **主动学习采样**  \n    用不确定性估计或梯度冲突指标，在 ISS 阶段主动挑选高价值样本，减少 200 万→更小但更有效的子集。\n*   **合成-真实混合比例动态调度**  \n    每轮自适应调整 UWS 与 ISS 数据比例，避免合成数据过拟合；可建模为在线权衡问题，用强化学习或贝叶斯优化求解。\n\n4\\. 过滤规则升级\n\n*   **语义级校验**  \n    对公式引入 CAS（Computer Algebra System）检查语义一致性；对表格引入逻辑约束（行列求和、单位一致性）。\n*   **置信度联合过滤**  \n    将模型自身置信度、一致性投票、外部轻量模型打分融合，替代单一 F1/语法阈值，提升过滤鲁棒性。\n\n5\\. 模型架构改进\n\n*   **高分辨率编码器**  \n    采用 NaViT 或 mPLUG-DocOwl2 的任意分辨率编码策略，减少下采样导致的细粒度信息损失。\n*   **端到端阅读顺序模块**  \n    显式建模阅读顺序（如 2-D positional embedding + reading order head），替代当前隐式依赖。\n\n6\\. 训练策略\n\n*   **课程式迭代**  \n    先易后难：首轮仅用单栏文档，逐步加入多栏、密集公式、复杂表格，降低早期噪声放大风险。\n*   **多任务正则化**  \n    同时预测文本、表格、公式与版面分割 mask，利用辅助任务梯度抑制过拟合。\n\n7\\. 评估与基准\n\n*   **细粒度子任务 Benchmark**  \n    构建针对「合并单元格识别」「行间公式编号对齐」「手写混合场景」等子集的专用评测集，量化模型短板。\n*   **跨域鲁棒性测试**  \n    在扫描古籍、低分辨率拍照、多语言混排等极端场景下评估，推动实际落地。\n\n8\\. 计算效率\n\n*   **知识蒸馏压缩**  \n    将 POINTS-Reader 3 B 蒸馏至 ≤1 B 端侧模型，保持精度损失 <3%，适配手机/边缘设备。\n*   **推理加速**  \n    采用 speculative decoding 或 early-exit 机制，在 2 百万 DocMatix 推理任务上缩短 10 h → <3 h。\n\n以上方向既可独立推进，也可组合形成多任务联合优化路径，为文档理解领域提供持续迭代空间。\n\n**Q6**: 总结一下论文的主要内容\n\n**POINTS-Reader：无需蒸馏的端到端文档转换框架**\n\n1\\. 问题背景\n\n*   高质量文档转换需要大量标注，人工昂贵，现有自动方法依赖外部大模型蒸馏，导致学生模型性能受限并继承偏差。\n*   合成数据与真实文档在版式、字体、复杂度上差异大，直接训练效果差。\n\n2\\. 解决方案\n\n提出**两阶段、无蒸馏、完全自动化**框架：\n\n阶段\n\n目标\n\n关键做法\n\n**UWS（统一格式预热）**\n\n让模型学会统一输出格式\n\n• 用 LLM 生成 80 万合成样本：纯文本、文本+公式、文本+表格、多栏+表格  \n• 统一格式：文本→Markdown，表格→HTML，公式→LaTeX  \n• 渲染成图像，微调公开 POINTS-1.5 基座\n\n**ISS（迭代自提升）**\n\n把模型适配到真实分布\n\n• 用 UWS 模型给 200 万 DocMatix 真实图像打伪标签  \n• **三重规则过滤**：  \n - 文本：与 OCR 结果算 F1≥0.9  \n - 表格：行列数一致性  \n - 公式：LaTeX 语法正确  \n• 过滤后数据再训练，循环 5 轮\n\n3\\. 实验结果\n\n*   **消融**：每增加一类合成数据或一层过滤，OmniDocBench 整体指标持续下降；80 万合成样本后性能饱和，验证 ISS 必要性。\n*   **对比**：3 B 参数的 POINTS-Reader 在表格指标上领先所有公开/闭源对手，整体指标优于 72 B 蒸馏模型 Qwen2.5-VL-72B 之外的全部基线。\n*   **案例**：迭代 5 轮后，表格结构、公式语法、阅读顺序显著改善。\n\n4\\. 贡献与局限\n\n*   **贡献**：首个无需蒸馏即可生成百万级高质量文档转换数据的两阶段框架；3 B 模型达到 SOTA。\n*   **局限**：仅英文、仅印刷体、尚未支持图像元素；未来计划扩展多语言、手写、图表等。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是POINTS-Reader%3A%20Distillation-Free%20Adaptation%20of%20Vision-Language%20Models%20for%20Document%20Conversion，链接是  https://arxiv.org/pdf/2509.01215  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01215  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1756943220805","title":"Baichuan-M2: Scaling Medical Capability with Large Verifier System","description":"As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.","url":"https://huggingface.co/papers/2509.02208","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.805Z","metadata":{"authors":["Baichuan-M2 Team",":","Chengfeng Dou","Chong Liu","Fan Yang","Fei Li","Jiyuan Jia","Mingyang Chen","Qiang Ju","Shuai Wang","Shunya Dang","Tianpeng Li","Xiangrong Zeng","Yijie Zhou","Chenzheng Zhu","Da Pan","Fei Deng","Guangwei Ai","Guosheng Dong","Hongda Zhang","Jinyang Tai","Jixiang Hong","Kai Lu","Linzhuang Sun","Peidong Guo","Qian Ma","Rihui Xin","Shihui Yang","Shusen Zhang","Yichuan Mo","Zheng Liang","Zhishou Zhang","Hengfu Cui","Zuyi Zhu","Xiaochuan Wang"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2509.02208.pdf","arxivUrl":"https://arxiv.org/abs/2509.02208","arxivId":"2509.02208","coolPaperUrl":"https://papers.cool/arxiv/2509.02208","published":"2025-09-02T11:23:35Z","updated":"2025-09-02T11:23:35.000Z","zh_summary":"随着大规模语言模型（LLMs）在对话和推理能力上的进步，它们在医疗保健中的实际应用已成为一个关键的研究重点。然而，医疗LLMs在静态基准测试（如USMLE）上的表现与它们在现实世界临床决策中的实用性之间存在显著差距。这种差距的出现是因为传统考试无法捕捉到医疗咨询的动态互动特性。为了解决这一挑战，我们提出了一种新颖的动态验证框架，超越了静态回答验证器，建立了一个大规模、高保真度的互动强化学习系统。我们的框架包括两个关键组成部分：一个患者模拟器，使用匿名医疗记录创建逼真的临床环境，以及一个临床评分生成器，动态产生多维评估指标。在此基础上，我们开发了Baichuan-M2，这是一个32B参数的医疗增强推理模型，通过多阶段强化学习策略和改进的群体相对策略优化（GRPO）算法进行训练。在HealthBench上评估时，Baichuan-M2的表现超越了所有其他开源模型和大多数先进的闭源模型，在具有挑战性的HealthBench Hard基准测试中得分超过32，这一成果之前仅被GPT-5超越。我们的工作表明，强大的动态验证系统对于将LLM的能力与实际临床应用对齐至关重要，为医疗AI部署的性能参数权衡建立了一个新的Pareto前沿。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文聚焦的核心问题是：**现有医疗大语言模型（LLM）在静态考试（如 USMLE）上表现良好，却在真实临床决策中表现不佳**。根本原因在于传统静态基准无法反映临床实践的动态、交互、信息不完备和多轮推理特性。\n\n为解决这一差距，论文提出并验证了一个 **动态验证框架**，通过以下方式弥合模型评估与真实临床场景之间的鸿沟：\n\n*   **构建高保真虚拟临床环境**：用“患者模拟器”替代静态题库，模拟真实患者的心理、社会背景及多轮对话行为。\n*   **引入可动态生成的多维临床评分体系**：“临床评分生成器”实时产出涵盖诊断准确性、沟通共情、医学伦理等维度的量化指标。\n*   **基于该环境进行多阶段强化学习**：改进 GRPO 算法，使 32B 参数的 Baichuan-M2 在 HealthBench（尤其是 Hard 子集）上达到与 GPT-5 比肩的水平，刷新开源医疗模型的性能-参数帕累托前沿。\n\n简言之，论文旨在 **让医疗 LLM 的训练与评估从“答对题”转向“当好医生”**。\n\n**Q2**: 有哪些相关研究？\n\n与 Baichuan-M2 直接相关或构成其技术背景的研究可分为四类：\n\n1.  医疗大模型与 RLVR 框架\n2.  患者/临床模拟器\n3.  医疗评测基准\n4.  强化学习算法改进\n\n以下按类别列出代表性工作（括号内为论文引用编号）：\n\n1\\. 医疗大模型与 RLVR 框架\n\n*   **HuatuoGPT-o1** \\[5\\]：首次在医疗场景尝试 RLVR，用规则奖励提升复杂推理。\n*   **Beyond Distillation** \\[4\\]：极简规则 RL 用于医学 LLM，验证强化学习对医学推理的提升。\n*   **HealAI** \\[1\\]：面向医疗文档的 LLM，展示领域专用模型的实际落地。\n*   **Talk2Care** \\[3\\]：语音 LLM 在老年护理中的交互研究，强调沟通维度。\n\n2\\. 患者/临床模拟器\n\n*   **Agent Hospital** \\[18\\]：可演化医疗智能体的模拟医院环境，提供多科室交互数据。\n*   **Exploring the Inquiry-Diagnosis Relationship** \\[16\\]：高级患者模拟器研究生成式问诊与诊断的关系，为 Baichuan-M2 的模拟器设计提供直接基础。\n*   **LLM-based Agents in Medicine Survey** \\[19\\]：系统综述了医学智能体与模拟环境的研究现状。\n\n3\\. 医疗评测基准\n\n*   **HealthBench** \\[15\\]：OpenAI 发布的 5000 例多轮临床对话评测集，含 48 k+ 医生撰写 rubrics；Baichuan-M2 的主要评估基准。\n*   **MedXpertQA** \\[35\\] / **SuperGPQA** \\[34\\]：面向专家级医学理解与推理的问答基准，用于规则 RL 阶段的知识验证。\n*   **USMLE 官方评分政策** \\[14\\]：传统静态医学考试基准，被论文指出与真实临床差距。\n\n4\\. 强化学习算法改进\n\n*   **DeepSeek-R1** \\[6\\]：大规模 RLVR 在数学与代码领域的成功实践，为 GRPO 改进提供经验。\n*   **DeepSeekMath** \\[8\\]：Group Relative Policy Optimization（GRPO）的原始论文，Baichuan-M2 在其基础上做医疗适配。\n*   **DAPO** \\[31\\]：开源大规模 RL 系统，提出若干工程优化点，被 Baichuan-M2 借鉴。\n*   **Rubric-based RL** \\[36\\]：用细粒度评分锚点做强化学习，与本文 rubric-based RL 阶段思路一致。\n\n这些研究共同构成了 Baichuan-M2 从“静态题库训练”转向“动态交互临床环境训练”的技术与评估基础。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过“构建动态验证系统 + 多阶段强化学习”双轮驱动，把模型训练从静态题库转移到高保真虚拟临床环境，具体实施路径如下：\n\n1\\. 设计动态验证系统（Dynamic Verifier System）\n\n1.1 患者模拟器（Patient Simulator）\n\n*   **数据**：脱敏真实病历 + 医患对话\n*   **建模**\n    *   **医学维度**：主诉、现病史、既往史 → 保证疾病真实性\n    *   **心理-社会维度**：MBTI 16 型人格 + 社会经济标签 → 模拟信息隐瞒、情绪表达、文化差异\n*   **架构**（小模型即可高保真）\n    *   Termination Gate：判断是否结束对话\n    *   Affective Unit：人格驱动的语言风格\n    *   Factual Unit：实时核查医学事实，防止信息泄露与矛盾\n*   **效果**：在隐私、事实、人格一致性三项指标上优于 DeepSeek-V3（图 3）\n\n1.2 临床评分生成器（Clinical Rubrics Generator）\n\n*   **输入**：患者模拟器产出的多轮对话片段\n*   **输出**：覆盖诊断、沟通、伦理等 7+ 维度的可量化评分规则（rubric）\n*   **流程**\n    1.  专家定义核心维度 → LLM 生成候选 rubrics → 专家筛选并赋权 \\[-10,10\\]\n    2.  以种子 rubrics 进行数据扩展 → 训练轻量级 rubric 生成器\n*   **验证**：与 100 例专家标注对比，一致性 92.7 %（GPT-4.1 仲裁）\n\n2\\. 三阶段强化学习（Multi-Stage RL）\n\n基于改进的 GRPO（Group Relative Policy Optimization）算法，把复杂医疗 RL 任务拆成递进的三段：\n\n阶段\n\n目标\n\n数据来源\n\n奖励设计\n\n**Rule-based RL**\n\n建立可验证推理基础\n\n有唯一答案的医学/数学/代码任务\n\n规则匹配奖励\n\n**Rubric-based RL**\n\n对齐多维度临床标准\n\n开放式医学问答（由 Rubric Generator 实时评分）\n\n细粒度 rubric 分数 + 条件长度惩罚\n\n**Multi-turn RL**\n\n提升动态交互能力\n\n与 Patient Simulator 的多轮对话\n\n每轮实时 rubric 反馈，片段级采样降噪\n\n**GRPO 关键改进**\n\n*   去掉 KL 约束，减少参考模型开销\n*   非对称 clip (ε\\_low, ε\\_high) 防止熵塌缩\n*   长度归一化 + 组内相对优势，稳定多源数据训练\n\n3\\. 训练流程总览\n\n1.  **Mid-Training**：轻量级医学继续预训练（2:2:1 医学/通用/数学混合）\n2.  **SFT**：200 万样本（20 % 医疗），建立推理与对话基础\n3.  **上述三阶段 RL**：依次提升推理 → 医学对齐 → 交互能力\n\n4\\. 效果验证\n\n*   **HealthBench**：32B 参数即获 60.1 总分，Hard 子集 34.7（仅次于 GPT-5）\n*   **中国 MDT 病例**：在 57 例复杂会诊中，Baichuan-M2 在沟通、检查、诊断、治疗、安全五维度均领先 gpt-oss-120B（图 11）\n*   **通用能力**：AIME24/25、IFEval、Arena-Hard 等基准保持领先（表 1）\n\n5\\. 部署优化\n\n*   **量化**：W4A16 / W4A8 + KV8，RTX 4090 单卡即可 21 k token 长序列\n*   **投机解码**：Eagle-3 框架，2.17× 吞吐提升\n\n通过“高保真虚拟临床环境 + 细粒度动态评分 + 分阶段 RL”，论文把医疗 LLM 的训练范式从“答对题”升级为“在模拟临床实践中持续优化诊断与沟通”，从而在 32B 参数规模上实现此前仅 GPT-5 能达到的临床推理水平。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕「动态验证框架 + Baichuan-M2」共设计了四类实验，覆盖验证系统、模型性能、真实临床场景与部署效率：\n\n1\\. 验证系统实验\n\n1.1 患者模拟器\n\n*   **对比对象**：DeepSeek-V3（带/不带心理提示）\n*   **指标**：\n    *   Privacy Score（隐私泄露比例）\n    *   Fact Score（事实一致性）\n    *   Personification Score（人格/文化一致性）\n*   **结果**：\n    *   在保持隐私与事实得分不降的前提下，Personification Score 达到 89.2，显著高于基线（图 3）。\n\n1.2 临床评分生成器\n\n*   **设置**：随机抽取 100 例，由 3 名医生 + GPT-4.1 仲裁\n*   **指标**：与专家标注 rubric 的维度级一致性\n*   **结果**：一致性 92.7 %，验证动态 rubric 的可靠性（§2.2.4）。\n\n2\\. 公开基准实验（HealthBench）\n\n*   **测试集**：HealthBench / HealthBench Hard / HealthBench Consensus（共 5 k 例多轮对话，48 k+ rubrics）\n*   **对比模型**：\n    *   开源：gpt-oss-120B、Qwen3-235B-A22B、DeepSeek-R1、GLM-4.5、Kimi-K2 等\n    *   闭源：o3、Grok 3、Gemini 2.5 Pro、GPT-4.1、o1 等\n*   **结果**：\n    *   **Overall**：60.1（SOTA 开源，领先第二名 2.5 分）\n    *   **Hard**：34.7（全球唯二 >32，仅次于 GPT-5 的 46.2）\n    *   **Consensus**：91.5（与最佳闭源模型持平）\n    *   维度/主题细分：在 Emergency Referrals、Communication、Global Health 等 7 项主题均排名第一（图 9-10）。\n\n3\\. 中国真实临床场景实验\n\n*   **数据集**：57 例三甲医院 MDT 会诊病例（平均 3 k 汉字/例，无唯一金标准）\n*   **评估维度**：Communication、Examination、Diagnosis、Treatment、Safety（10 项加权指标）\n*   **评估人**：5 名高年资医生盲评\n*   **结果**（图 11）：\n    *   Baichuan-M2 在 5 个维度均优于 gpt-oss-120B；\n    *   最大差距出现在 Communication（67 % vs 11 %）。\n\n4\\. 通用能力与部署效率实验\n\n4.1 通用基准\n\n类别\n\n基准\n\nBaichuan-M2-32B\n\n对比模型（Qwen3-32B）\n\nMath\n\nAIME24\n\n**83.4**\n\n81.4\n\nInstruction\n\nIFEval\n\n**86.0**\n\n85.0\n\nArena-Hard-V2.0\n\n**45.8**\n\n44.5\n\n4.2 量化与推理\n\n*   **配置**：RTX 4090 单卡\n*   **最大序列长度**：\n    *   W4A16：9,982 tokens\n    *   W4A8-KV8：**21,133 tokens**\n*   **投机解码**：Eagle-3 框架 → 吞吐从 41.5 → 89.9 tokens/s（2.17× 加速）\n\n实验总结\n\n实验类别\n\n关键结论\n\n验证系统\n\n患者模拟器与 rubric 生成器均达到高保真、高一致性\n\n公开基准\n\n32B 参数即获 SOTA，Hard 子集仅次于 GPT-5\n\n真实场景\n\n在中国复杂会诊中全面领先最强开源模型\n\n部署效率\n\n量化 + 投机解码实现单卡 21 k tokens 长序列与 2× 提速\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可在 Baichuan-M2 基础上继续深入，按优先级与可行性归纳为 **模型能力、验证系统、训练范式、落地扩展** 四大类：\n\n1\\. 模型能力深化\n\n*   **幻觉抑制与知识溯源**\n    \n    *   引入 **检索增强（RAG）** 或 **工具调用** 机制，实时对接权威指南、药品说明书、最新文献，降低幻觉率。\n    *   设计 **可解释引用链**：让模型在输出结论时显式给出文献片段或数据库查询结果。\n*   **多模态融合**\n    \n    *   将影像（CT/MRI）、检验时间序列、生命体征波形与文本对话联合建模，实现 **跨模态诊断推理**。\n*   **长程记忆与跨会话一致性**\n    \n    *   研究 **外部记忆机制**（如医疗知识图谱 + 患者个人健康档案）以支持慢病随访、复诊场景。\n\n2\\. 验证系统升级\n\n*   **患者模拟器 2.0**\n    \n    *   引入 **情绪演化模型**（基于心理学情绪环状理论），模拟患者从焦虑到信任的情绪迁移，考察 AI 的安抚与知情同意能力。\n    *   加入 **罕见病/共病/药物相互作用** 的自动剧本生成，提升长尾场景覆盖率。\n*   **Rubric 精细化**\n    \n    *   将 rubric 权重从人工设定改为 **在线贝叶斯优化**，根据真实医生反馈动态调整各维度重要性。\n    *   引入 **时间维度评分**（如“是否在前三轮对话内锁定关键诊断线索”），强化策略效率。\n*   **对抗性评测**\n    \n    *   构建 **红队模拟器** 刻意给出误导信息或情绪化语言，测试模型鲁棒性与安全边界。\n\n3\\. 训练范式创新\n\n*   **完整对话级 RL**\n    \n    *   当前仅片段级训练 → 扩展到 **整段会话奖励**，用蒙特卡洛树搜索（MCTS）或分层 RL 优化全局诊断路径。\n*   **联邦/隐私计算 RL**\n    \n    *   与多家医院合作，在 **联邦学习框架** 下利用本地脱敏数据继续强化学习，既保护隐私又提升地域多样性。\n*   **课程式 RL Curriculum**\n    \n    *   按难度（常见病 → 疑难病 → 多系统共病）自动编排训练顺序，加速收敛并减少灾难性遗忘。\n\n4\\. 落地与伦理扩展\n\n*   **临床试点与 RCT**\n    \n    *   设计 **随机对照试验**：将 Baichuan-M2 作为住院医师辅助工具，衡量实际诊疗质量、患者满意度、医生负荷变化。\n*   **法规与伦理审计**\n    \n    *   建立 **动态风险监控面板**：实时追踪模型在不同人群（年龄、性别、种族）上的性能差异，满足未来 FDA/NMPA 审计要求。\n*   **低成本边缘部署**\n    \n    *   探索 **端-云协同推理**：在手机/平板上运行 4-bit 量化小模型做草稿，云端大模型做复核，实现基层医院可负担的 AI 医生。\n\n小结\n\n方向\n\n短期可行\n\n长期价值\n\n检索增强 + 引用链\n\n✅ 3 个月\n\n显著降低幻觉\n\n情绪-社会背景模拟器\n\n✅ 6 个月\n\n提升沟通与共情评估\n\n整段对话 RL\n\n⚠️ 需算力\n\n全局策略最优\n\n联邦 RL + RCT\n\n⚠️ 需合规\n\n真正临床级落地\n\n上述任何一点的突破，都能进一步拉大医疗 LLM 在“真实临床能力”而非“考试分数”上的领先优势。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n用「高保真虚拟临床环境 + 动态多维评分 + 多阶段强化学习」把 32B 参数的 Baichuan-M2 训练成首个在真实临床场景（HealthBench Hard）上逼近 GPT-5 的开源医疗大模型，并给出可落地的低成本部署方案。\n\n核心内容速览\n\n维度\n\n要点\n\n**问题**\n\n传统静态医学考试（USMLE）高分 ≠ 真实临床决策好用；缺少动态、交互、信息不完备场景下的验证与训练。\n\n**解法**\n\n构建「动态验证系统」：① 患者模拟器（人格+社会背景+真实病历）② 临床评分生成器（实时多维 rubric）。\n\n**训练**\n\n三阶段 RL：规则奖励 → rubric 奖励 → 多轮交互奖励，基于改进 GRPO，32B 模型即可收敛。\n\n**结果**\n\nHealthBench 总分 60.1（开源 SOTA），Hard 34.7（全球仅次 GPT-5）；中国 57 例 MDT 会诊全面领先 gpt-oss-120B；通用基准不掉点。\n\n**部署**\n\nRTX 4090 单卡 21 k token 长序列 + 投机解码 2× 提速，W4A8 量化几乎无损。\n\n**开放**\n\n模型、量化权重、推理代码全部开源，推动资源受限医院也能用上先进医疗 AI。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Baichuan-M2%3A%20Scaling%20Medical%20Capability%20with%20Large%20Verifier%20System，链接是  https://arxiv.org/pdf/2509.02208  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02208  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1756943220806","title":"Kwai Keye-VL 1.5 Technical Report","description":"In recent years, the development of Large Language Models (LLMs) has significantly advanced, extending their capabilities to multimodal tasks through Multimodal Large Language Models (MLLMs). However, video understanding remains a challenging area due to the dynamic and information-dense nature of videos. Existing models struggle with the trade-off between spatial resolution and temporal coverage when processing video content. We present Keye-VL-1.5, which addresses fundamental challenges in video comprehension through three key innovations. First, we introduce a novel Slow-Fast video encoding strategy that dynamically allocates computational resources based on inter-frame similarity, processing key frames with significant visual changes at higher resolution (Slow pathway) while handling relatively static frames with increased temporal coverage at lower resolution (Fast pathway). Second, we implement a progressive four-stage pre-training methodology that systematically extends the model's context length from 8K to 128K tokens, enabling processing of longer videos and more complex visual content. Third, we develop a comprehensive post-training pipeline focusing on reasoning enhancement and human preference alignment, incorporating a 5-step chain-of-thought data construction process, iterative GSPO-based reinforcement learning with progressive prompt hinting for difficult cases, and alignment training. Through extensive evaluation on public benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates significant improvements over existing models, particularly excelling in video understanding tasks while maintaining competitive performance on general multimodal benchmarks.","url":"https://huggingface.co/papers/2509.01563","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.806Z","metadata":{"authors":["Biao Yang","Bin Wen","Boyang Ding","Changyi Liu","Chenglong Chu","Chengru Song","Chongling Rao","Chuan Yi","Da Li","Dunju Zang","Fan Yang","Guorui Zhou","Guowang Zhang","Han Shen","Hao Peng","Haojie Ding","Hao Wang","Hengrui Ju","Jiaming Huang","Jiangxia Cao","Jiankang Chen","Jingyun Hua","Kaibing Chen","Kaiyu Jiang","Kaiyu Tang","Kun Gai","Muhao Wei","Qiang Wang","Ruitao Wang","Sen Na","Shengnan Zhang","Siyang Mao","Sui Huang","Tianke Zhang","Tingting Gao","Wei Chen","Wei Yuan","Xiangyu Wu","Xiao Hu","Xingyu Lu","Yi-Fan Zhang","Yiping Yang","Yulong Chen","Zeyi Lu","Zhenhua Wu","Zhixin Ling","Zhuoran Yang","Ziming Li","Di Xu","Haixuan Gao","Hang Li","Jing Wang","Lejian Ren","Qigen Hu","Qianqian Wang","Shiyao Wang","Xinchen Luo","Yan Li","Yuhang Hu","Zixing Zhang"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.01563.pdf","arxivUrl":"https://arxiv.org/abs/2509.01563","arxivId":"2509.01563","coolPaperUrl":"https://papers.cool/arxiv/2509.01563","published":"2025-09-01T15:46:58Z","updated":"2025-09-01T15:46:58.000Z","zh_summary":"近年来，大型语言模型（LLMs）的发展取得了显著进展，通过多模态大型语言模型（MLLMs）扩展了其在多模态任务中的能力。然而，由于视频的动态性和信息密集性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，空间分辨率与时间覆盖之间的权衡存在困难。我们提出了Keye-VL-1.5，针对视频理解中的基本挑战进行了三项关键创新。首先，我们引入了一种新颖的慢-快视频编码策略，该策略根据帧间相似性动态分配计算资源，以更高的分辨率处理具有显著视觉变化的关键帧（慢通道），同时以较低的分辨率处理相对静态的帧，增加时间覆盖（快通道）。第二，我们实施了一种渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K个标记，从而能够处理更长的视频和更复杂的视觉内容。第三，我们开发了一条全面的后训练流程，重点关注推理增强和人类偏好的对齐，该流程包括一个五步的思维链数据构建过程、基于GSPO的迭代强化学习在困难案例中的渐进式提示、以及对齐训练。通过在公共基准测试上的广泛评估和严格的内部人类评估，Keye-VL-1.5在视频理解任务中展示了显著的改善，特别是在视频理解任务中表现优异，同时在一般多模态基准测试中保持竞争力的表现。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文针对**视频理解**这一多模态大模型（MLLM）尚未充分攻克的难题，提出并验证了 Keye-VL-1.5。核心问题可概括为：\n\n1.  **视频时空信息权衡困境**  \n    现有方法普遍采用固定分辨率+均匀帧采样，导致：\n    \n    *   高空间分辨率时，时间覆盖不足，丢失动态细节；\n    *   高帧率时，空间分辨率被迫降低，细节模糊。  \n        论文提出 **SlowFast 视频编码策略**，依据帧间相似度动态分配计算资源：\n    *   **Slow 路径**：高分辨率处理关键帧（显著视觉变化）；\n    *   **Fast 路径**：低分辨率+高帧率处理静态帧，兼顾时序连贯性。\n2.  **长视频上下文建模瓶颈**  \n    长视频（分钟级）需超大上下文窗口（>100k tokens），传统 8K 上下文难以支撑。  \n    论文设计 **四阶段渐进预训练**：\n    \n    *   从 8K 逐步扩展至 128K tokens，通过退火阶段稳定训练；\n    *   引入长视频、长文本、高分辨率图像混合数据，确保窗口利用率。\n3.  **推理能力与人类偏好对齐不足**  \n    现有模型在复杂推理（如因果、时序）及人类偏好（格式、风格）上表现欠佳。  \n    论文构建 **三阶段后训练流水线**：\n    \n    *   **LongCoT 冷启动**：5 步自动化数据构造，生成高质量思维链；\n    *   **GSPO 强化学习**：基于可验证奖励，对困难样本采用 **渐进提示采样**（分 5 级提示，最小干预原则）；\n    *   **对齐 RL**：优化指令遵循、格式规范、伦理安全，结合规则奖励与生成式奖励。\n\n综上，论文通过 **架构创新（SlowFast）+ 训练范式革新（渐进上下文扩展 + 多轮 RL）**，系统性解决视频理解中的时空权衡、长上下文及推理对齐三大挑战。\n\n**Q2**: 有哪些相关研究？\n\nKeye-VL-1.5 的工作建立在近年来多模态大模型（MLLM）与视频理解领域的多项关键研究之上，可归纳为以下四大类：\n\n1\\. 多模态大模型基础架构\n\n*   **LLM 主干**\n    *   **Qwen3-8B**（Yang et al., 2025）：作为语言解码器，提供通用世界知识与推理能力。\n    *   **Llama-3/4**（Grattafiori et al., 2024；FaceBook, 2025）：开源 LLM 系列，推动多模态扩展。\n*   **视觉编码器**\n    *   **SigLIP-400M-384-14**（Zhai et al., 2023）：CLIP 变体，用于初始化视觉编码器。\n    *   **NaViT**（Dehghani et al., 2023）：原生分辨率 ViT，支持动态长宽比输入，Keye-VL-1.5 借鉴其思想实现 2D-RoPE 与动态分辨率。\n\n2\\. 视频编码与时空建模\n\n*   **均匀帧采样策略**\n    *   **Video-LLaVA**（Lin et al., 2023）：统一视觉表征，先对齐后投影，但受限于固定帧率。\n    *   **Qwen2.5-VL**（Bai et al., 2025）：提出 2D 卷积合并相邻帧，降低 token 开销，但假设相邻帧高度相似，可能丢失突变信息。\n*   **SlowFast 思想来源**\n    *   **SlowFast R-CNN**（Feichtenhofer et al., 2019）：CVPR 经典工作，双路径处理视频（慢路径高空间分辨率，快路径高时间分辨率）。Keye-VL-1.5 将其适配到 MLLM，通过**帧间相似度**动态分配计算资源。\n\n3\\. 长上下文与渐进训练\n\n*   **上下文扩展技术**\n    *   **LongViTA**（Shen et al., 2025）：百万级 token 上下文，采用分段编码与位置插值。\n    *   **Emu3**（Wang et al., 2024a）：基于 next-token prediction 的多模态统一框架，支持长序列。\n*   **训练策略**\n    *   **退火阶段（Annealing）**：借鉴 LLM 训练（如 DeepSeek-V3, Liu et al., 2024a），在高质量数据上微调以提升鲁棒性。\n\n4\\. 推理增强与 RLHF\n\n*   **思维链（CoT）数据构造**\n    *   **DeepSeek-R1**（Guo et al., 2025a）：通过 RL 激励 LLM 推理能力，Keye-VL-1.5 借鉴其“冷启动+RL”范式。\n    *   **Seed1.5-Thinking**（Seed et al., 2025）：多模态推理模型，采用分步验证奖励。\n*   **强化学习优化**\n    *   **GSPO**（Zheng et al., 2025）：Group Sequence Policy Optimization，Keye-VL-1.5 用于 RLVR（可验证奖励）。\n    *   **MMPR/RLHF**（Zhang et al., 2025b；Wang et al., 2024b）：多模态偏好对齐数据集与算法，用于 SFT 与 MPO 阶段。\n\n5\\. 评估基准与数据资源\n\n*   **视频理解基准**\n    *   **Video-MME**（Fu et al., 2025b）：首个综合视频 MLLM 评测集，Keye-VL-1.5 作为主要对比基准。\n    *   **TempCompass**（Liu et al., 2024d）：评估时序理解能力，揭示现有模型缺陷。\n*   **数据合成与增强**\n    *   **PixMo**（Deitke et al., 2024）：开放 grounding 与计数数据，用于训练专家模型。\n    *   **Molmo**（Deitke et al., 2024）：大规模图文交错数据，支持长上下文训练。\n\n总结\n\nKeye-VL-1.5 通过整合 **SlowFast 视频编码（受 CV 经典工作启发）**、**渐进上下文扩展（借鉴长序列 LLM 技术）** 与 **多轮 RL 推理增强（融合 DeepSeek-R1/Seed1.5 范式）**，在现有 MLLM 研究基础上实现了视频理解的显著突破。\n\n**Q3**: 论文如何解决这个问题？\n\nKeye-VL-1.5 从 **架构、训练、后训练** 三条主线协同解决视频理解难题，具体做法如下：\n\n1\\. 架构层面：SlowFast 视频编码\n\n**问题**：固定分辨率+均匀采样无法兼顾空间细节与时间连贯性。  \n**解法**：\n\n*   **动态帧分类**\n    *   用 patch-level 相似度函数将帧分为两类：\n        *   **Slow 帧**（关键帧）：视觉变化显著，保持 **高分辨率**（token 占比高）。\n        *   **Fast 帧**（静态帧）：相似度>95%，以 **低分辨率+高帧率** 处理，扩大时间覆盖。\n*   **双路径 token 分配**\n    *   Fast 帧 token 预算仅为 Slow 帧的 30%，通过 **二分搜索** 精确控制总 token 数（如 75k）。\n    *   引入特殊 token `<|clip_time_start|>` 与绝对时间戳，显式标注 Slow/Fast 边界，辅助模型学习时序对齐。\n\n2\\. 训练层面：四阶段渐进预训练\n\n**问题**：长视频需 128K 上下文，直接训练不稳定。  \n**解法**：\n\n阶段\n\n目标\n\n关键技术\n\n上下文长度\n\n**Stage 0**\n\nViT 适应内部分布\n\nSigLIP 对比损失 + NaViT 动态分辨率\n\n8K\n\n**Stage 1**\n\n跨模态对齐\n\n冻结 ViT & LLM，仅训练 MLP 投影层\n\n8K\n\n**Stage 2**\n\n多任务预训练\n\n全参数微调（OCR、VQA、Grounding 等）\n\n8K\n\n**Stage 3**\n\n长上下文退火\n\nRoPE 频率从 1M→8M，引入长视频/长文本\n\n128K\n\n*   **资源优化**：8K 阶段用 Zero-2 + DP；128K 阶段切换 Zero-1 + Context/Pipeline Parallelism。\n*   **数据配比**：128K 时 24% token 给视频、50% 给图像、26% 给文本，平衡视觉-语言能力。\n\n3\\. 后训练层面：推理增强与偏好对齐\n\n3.1 LongCoT 冷启动\n\n**问题**：缺乏高质量推理链数据。  \n**解法**：\n\n*   **5 步自动化流水线**（图 6）：\n    1.  **多源数据收集**：数学、OCR、STEM、GUI 等 7 类任务。\n    2.  **多路径生成**：用 MLLM 生成多条推理链，记录 step-wise 置信度。\n    3.  **双层质量评估**：答案正确性 + 推理逻辑性，分 A/B/C 三级。\n    4.  **人工增强**：修正 B 级样本冗余/逻辑漏洞。\n    5.  **动态评分**：1-5 分质量评级，高分样本重复训练。\n\n3.2 迭代 GSPO 强化学习\n\n**问题**：困难样本利用率低。  \n**解法**：\n\n*   **渐进提示采样**（表 9）：\n    *   对多次失败的样本，按 **5 级提示**（概念→策略→工具→步骤→完整解）逐步提供最小必要信息。\n    *   成功样本用于更新策略，提升 RL 效率（无提示时错误率 25.56% → 5 级提示后 0.2%）。\n*   **迭代范式**：\n    *   RL 模型 → 拒绝采样 → 更新冷启动数据 → 新一轮 RL，数学基准从 60.37→62.24（图 8）。\n\n3.3 对齐 RL\n\n**问题**：指令遵循、格式、伦理偏好不足。  \n**解法**：\n\n*   **三维奖励系统**：\n    *   **规则奖励**：格式（JSON/Markdown）、推理结构（think/no\\_think）。\n    *   **生成奖励**：用 MLLM 评估安全性、相关性。\n    *   **模型奖励**：基于人类偏好训练的奖励模型打分。\n*   **任务设计**：\n    *   25 类硬约束（关键词、标点）+ 20 类软约束（风格）。\n    *   12k 数学推理题分步验证，RAG 任务模拟实时搜索行为。\n\n4\\. 系统级优化\n\n*   **异构并行**：ViT 仅用 DP，LLM 用 DP+TP+PP，解决架构异构导致的资源浪费。\n*   **动态负载均衡**：按样本计算复杂度预分配 GPU 任务，减少空闲等待。\n*   **I/O 优化**：独立 I/O 服务器处理视频解码，实例级断点续训保障稳定性。\n\n总结\n\n通过 **SlowFast 动态编码** 解决时空权衡，**四阶段渐进训练** 扩展至 128K 上下文，**冷启动+GSPO RL+对齐 RL** 的三级后训练体系，Keye-VL-1.5 在视频理解（Video-MME 73.0%）与通用任务（OpenCompass 79.5%）均取得 SOTA，验证了系统性解决方案的有效性。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **视频理解能力验证、通用多模态性能、内部人工评估、消融研究** 四条主线展开实验，覆盖 20 + 公开基准与 200 项内部案例。主要实验与结果如下：\n\n1\\. 视觉编码器验证（零样本分类）\n\n模型\n\nImageNet-1K\n\nImageNet-A\n\nObjectNet\n\nSigLIP-Base\n\n83.08\n\n82.22\n\n76.99\n\n+1D 插值\n\n82.02\n\n80.92\n\n67.58 ↓\n\n+2D RoPE（Keye-VL）\n\n**82.65**\n\n**83.26**\n\n**78.70**\n\n*   **结论**：2D-RoPE 显著提升高分辨率鲁棒性，验证原生分辨率 ViT 的有效性。\n\n2\\. SlowFast 策略对比实验\n\n**基准**：Video-MME（短/中/长视频，帧数 32–768，FPS 1–4）\n\n*   **性能拐点**：\n    *   Qwen-2.5-VL：128/384/128 帧\n    *   Keye-VL-1.5：**192/512/384** 帧（更晚下降，说明融合更充分）\n*   **Token 效率**：高帧数场景下，Keye-VL 比 Qwen 少 15–30 % token，仍保持更高分数。\n*   **FPS 鲁棒性**：Keye-VL 在不同 FPS 下波动 < 2 %，Qwen 波动 > 5 %。\n\n3\\. 公开基准全面评测\n\n类别\n\n基准示例\n\nKeye-VL-1.5\n\nQwen2.5-VL\n\nInternVL3\n\nGPT-4o\n\n**通用**\n\nOpenCompass\n\n**79.5**\n\n70.9\n\n73.6\n\n72.0\n\nMMMU-val\n\n**71.4**\n\n58.6\n\n62.7\n\n70.7\n\n**视频**\n\nVideo-MME\n\n**73.0**\n\n65.1\n\n66.3\n\n71.9\n\nVideo-MMMU\n\n**66.0**\n\n47.4\n\n48.9\n\n59.5\n\n**数学**\n\nMathVision\n\n**46.8**\n\n26.2\n\n28.8\n\n31.2\n\nOlympiadBench\n\n**47.5**\n\n19.4\n\n25.9\n\n25.9\n\n*   **结论**：在 18 个基准上取得 **开源 SOTA**，视频与数学任务领先幅度最大（↑6–20 %）。\n\n4\\. 内部人工评估（200 案例，8 维度）\n\n维度\n\nKeye-VL-1.5\n\nMiMo-VL-7B-RL\n\nΔ\n\n推理能力\n\n**3.81**\n\n3.56\n\n+0.25\n\n时序理解\n\n**3.36**\n\n3.18\n\n+0.18\n\n鲁棒性\n\n**4.29**\n\n3.46\n\n+0.83\n\n整体得分\n\n**3.53**\n\n3.40\n\n+0.13\n\n*   **场景示例**：\n    *   **时间定位**：准确指出 26 s 视频中手提包出现时段 22.3–23.8 s（误差 0.1 s）。\n    *   **行为推理**：解释大狗咬小狗耳朵是为了“教育其不要偷吃”，结合画面与字幕。\n\n5\\. 消融研究\n\n5.1 训练策略对比（表 6）\n\n设置\n\nOpenCompass\n\nMathVista\n\nOCRBench\n\nSFT-15k\n\n67.24\n\n63.50\n\n82.70\n\n\\+ MPO-15k\n\n69.31\n\n67.00\n\n83.20\n\nLongCoT 冷启动\n\n75.32\n\n76.40\n\n86.10\n\n\\+ RL（完整）\n\n**79.41**\n\n**81.30**\n\n85.70\n\n*   **结论**：LongCoT 冷启动带来 **+8 %** 以上增益，RL 阶段再提升 3–5 %。\n\n5.2 专家模型与合并（表 7）\n\n模型\n\nTextVQA\n\nChartQA\n\nOCRBench\n\nBase\n\n70.45\n\n78.08\n\n85.70\n\nOCR Expert\n\n79.36\n\n84.76\n\n86.40\n\n合并后\n\n**83.40**\n\n**84.88**\n\n**86.70**\n\n*   **结论**：专家模型 + 合并可在不损失通用能力的情况下，OCR 任务提升 **+5–13 %**。\n\n5.3 对齐 RL 效果（表 8）\n\n基准\n\n前\n\n后\n\nΔ\n\nMIA-Bench\n\n87.60\n\n**91.95**\n\n+4.35\n\nIFEval\n\n73.75\n\n**78.37**\n\n+4.62\n\nMathVision\n\n46.22\n\n**48.45**\n\n+2.23\n\n5.4 渐进提示采样（表 9）\n\n提示级别\n\n完全错误率\n\n4 次平均得分\n\n无提示\n\n25.56 %\n\n1.62\n\nLevel 5（完整解）\n\n**0.20 %**\n\n**3.96**\n\n*   **结论**：分层提示将困难样本利用率提升 **25×**。\n\n5.5 拒绝采样迭代（图 8）\n\n*   数学基准从 60.37 → 62.24（两轮 RL + 拒绝采样）。\n\n实验总结\n\n通过 **公开基准 + 内部人工 + 系统消融** 三维验证，论文证明：\n\n1.  SlowFast 编码在同等算力下显著优于 2D 卷积合并；\n2.  128K 上下文 + LongCoT + GSPO RL 是视频推理跃升的关键；\n3.  专家模型合并与对齐 RL 可进一步拔高专项与安全性能。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可在 Keye-VL-1.5 的基础上继续深入，分为 **技术深化、场景扩展、系统优化、理论分析** 四大类：\n\n1\\. 技术深化\n\n方向\n\n可探索点\n\n潜在收益\n\n**SlowFast 编码**\n\n1\\. 将 patch 级相似度升级为 **光流/深度估计** 驱动的运动显著性检测；  \n2\\. 引入 **可学习的帧预算控制器**（RL-based token router），实时调整 Slow/Fast 比例。\n\n进一步降低 10–20 % token 冗余，提升长视频细粒度动作识别。\n\n**Long Context**\n\n1\\. 研究 **\\>128K**（如 256K–1M）时的 scaling law；  \n2\\. 结合 **压缩记忆（compressive memory）** 或 **token 丢弃策略**，避免线性开销。\n\n支撑小时级直播、电影理解等超长内容。\n\n**专家模型**\n\n1\\. 扩展更多垂直专家（医疗影像、卫星视频、AR/VR 3D 流）；  \n2\\. 探索 **动态路由专家网络**（MoE-style），按需激活。\n\n在保持通用能力的同时，领域任务再提升 5–10 %。\n\n2\\. 场景扩展\n\n场景\n\n待解决问题\n\n研究思路\n\n**多视角/全景视频**\n\n360° 全景或无人机多机位视频带来 **空间-时间双重冗余**。\n\n将 SlowFast 思想扩展到 **空间维度**：高分辨率聚焦 ROI，低分辨率覆盖全景。\n\n**音频-视频联合推理**\n\n当前 ASR 仅作为文本插入，未与视觉对齐。\n\n设计 **跨模态 SlowFast**：音频事件（如爆炸）触发高分辨率视觉重采样。\n\n**交互式视频问答**\n\n用户实时追问，需要 **在线增量推理**。\n\n用 **KV-Cache 复用 + 局部重编码** 实现毫秒级延迟的流式问答。\n\n3\\. 系统与数据优化\n\n维度\n\n可优化点\n\n预期效果\n\n**数据合成**\n\n1\\. 引入 **程序生成（procedural generation）** 合成无限长视频；  \n2\\. 使用 **扩散模型** 生成高动态、少样本场景（罕见天气、异常事件）。\n\n降低对昂贵人工标注的依赖，提升长尾场景覆盖。\n\n**训练效率**\n\n1\\. **帧级动态批处理**（根据视频长度动态调整 batch 内样本数）；  \n2\\. **混合精度 + FlashAttention-3** 支持 128K 长序列不降速。\n\n训练时间 ↓30 %，GPU 利用率 ↑15 %。\n\n**评测体系**\n\n构建 **中文长视频基准**（>5 min，含方言、多模态噪声），弥补现有英文短视频偏差。\n\n更真实反映国内短视频平台场景。\n\n4\\. 理论与分析\n\n方向\n\n研究问题\n\n方法论\n\n**Scaling Law**\n\n视频理解是否遵循 **Chinchilla-optimal**？  \n需同时考虑帧数、分辨率、token 预算三维。\n\n在固定算力下系统扫描 (F, R, T) 空间，拟合性能-预算曲面。\n\n**可解释性**\n\nSlowFast 决策是否 **与人类注意力一致**？\n\n引入 eye-tracking 数据集，对比模型 patch 权重与人眼注视热图。\n\n**鲁棒性诊断**\n\n长上下文是否引入 **新的幻觉模式**（如跨帧混淆）？\n\n设计 **时序对抗样本**（帧顺序扰动、关键帧删除）测试模型稳定性。\n\n小结\n\n*   **短期**（3–6 个月）：  \n    光流驱动的 SlowFast、256K 上下文 scaling、中文长视频基准。\n*   **中期**（6–12 个月）：  \n    动态路由专家、音频-视觉联合 SlowFast、交互式流式推理。\n*   **长期**（>12 个月）：  \n    统一时空-模态的 scaling law、可解释注意力对齐、自监督无限视频生成。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\nKeye-VL-1.5 通过 **SlowFast 动态视频编码、128 K 渐进预训练、三阶段推理-对齐后训练**，在 20 + 公开基准与 200 项内部评测中刷新开源视频理解 SOTA，同时保持通用多模态竞争力。\n\n核心贡献速览\n\n维度\n\n关键做法\n\n效果\n\n**架构**\n\n• SlowFast 双路径：高分辨率 Slow 帧捕捉突变，低分辨率 Fast 帧覆盖静态  \n• 原生分辨率 ViT + 2D-RoPE，支持任意长宽比\n\n同等算力下 Video-MME ↑7.9 %，token 节省 15–30 %\n\n**预训练**\n\n四阶段渐进：ViT 对齐 → 多任务 8 K → 退火 128 K → 模型融合\n\n128 K 上下文稳定训练，长视频/大图/长文本一次看全\n\n**后训练**\n\n• LongCoT 冷启动：5 步自动构造高质量思维链  \n• GSPO 强化学习 + 5 级渐进提示，困难样本利用率 ↑25×  \n• 对齐 RL：规则+生成+模型三重奖励，指令/格式/偏好一次到位\n\n数学推理平均 ↑4–6 %，内部人工评测整体 ↑0.13–0.51\n\n**系统**\n\n异构并行（ViT-DP + LLM 3D 并行）、动态负载均衡、I/O 服务器\n\n128 K 训练效率与 8 K 持平，GPU 利用率 ↑15 %\n\n实验亮点\n\n*   **公开基准**：Video-MME 73.0 %（开源第一），MathVision 46.8 %（超 Qwen2.5-VL 20 + %）。\n*   **内部评测**：200 例细粒度对比，鲁棒性 ↑0.83，时序理解 ↑0.77。\n*   **消融**：LongCoT 冷启动贡献最大（↑8 %），专家模型合并 OCR 再 ↑5 %，拒绝采样两轮 RL 再 ↑2 %。\n\n一句话展望\n\nKeye-VL-1.5 为“看得细、看得长、想得深”的多模态大模型提供了可复制的全栈范式，下一步可向更长视频、实时交互与领域专精继续演进。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Kwai%20Keye-VL%201.5%20Technical%20Report，链接是  https://arxiv.org/pdf/2509.01563  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01563  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1756943220806","title":"Reasoning Vectors: Transferring Chain-of-Thought Capabilities via Task Arithmetic","description":"Large language models often require costly optimization, such as reinforcement learning, to master complex reasoning tasks. This work demonstrates that reasoning ability, once learned, can be extracted and transferred between models as a compact task vector. We source two publicly available, identically initialized Qwen2.5 models, one fine-tuned with supervised fine-tuning (SFT) and the other with group relative policy optimization (GRPO) on the same dataset. From these, we extract a reasoning vector: $v_{\\text{reason}} = \\theta_{\\text{GRPO}} - \\theta_{\\text{SFT}}$. We hypothesize that this vector captures the reasoning capability instilled by reinforcement learning while factoring out shared knowledge from the SFT process. When added to compatible instruction-tuned models through simple arithmetic, this vector consistently improves performance across diverse reasoning benchmarks: GSM8K (+4.9%), HumanEval (+4.3%), SciQ (+1.7%), and BigBenchHard (+12.3% for the 1.5B model). The performance improvements persist under adversarial conditions. Conversely, subtracting the vector causes significant performance degradation (-11.8% on GSM8K), demonstrating the vector's strong contribution to the model's reasoning abilities. This work shows how reasoning capabilities, typically developed through expensive training, can be extracted from existing open-source models and reused through simple tensor arithmetic, offering a practical way to enhance models by recycling prior computational investments.","url":"https://huggingface.co/papers/2509.01363","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.806Z","metadata":{"authors":["Mohammad Zbeeb","Hasan Abed Al Kader Hammoud","Bernard Ghanem"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.01363.pdf","arxivUrl":"https://arxiv.org/abs/2509.01363","arxivId":"2509.01363","coolPaperUrl":"https://papers.cool/arxiv/2509.01363","published":"2025-09-01T11:04:51Z","updated":"2025-09-01T11:04:51.000Z","zh_summary":"大型语言模型通常需要代价高昂的优化，例如强化学习，以掌握复杂的推理任务。本研究表明，一旦学会推理能力，可以提取并在模型之间以紧凑的任务向量形式进行转移。我们获取了两个公开可用、完全相同初始化的Qwen2.5模型，一个经过监督微调（SFT），另一个则在同一数据集上使用组相对策略优化（GRPO）。从中，我们提取了一个推理向量：$v_{\text{reason}} = \theta_{\text{GRPO}} - \theta_{\text{SFT}}$。我们假设这个向量捕捉了通过强化学习培养的推理能力，同时排除了SFT过程中的共享知识。当通过简单的算术将此向量添加到兼容的指令调优模型中时，持续改善了不同推理基准的性能：GSM8K（+4.9%），HumanEval（+4.3%），SciQ（+1.7%），以及BigBenchHard（对于1.5B模型，+12.3%）。在对抗条件下，性能提升依然存在。相反，减去该向量会导致显著的性能下降（在GSM8K上下降11.8%），这表明该向量对模型推理能力的强大贡献。本研究展示了如何从现有的开源模型中提取推理能力，通常通过代价高昂的训练获得，并通过简单的张量运算重复使用，提供了一种通过回收先前的计算投资来增强模型的实用方法。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文聚焦的核心问题是：\n\n> **如何以低成本、可复用的方式，将强化学习（RL）训练出的复杂推理能力迁移到任意兼容的大语言模型，而无需对每个目标模型重新执行昂贵的 RL 训练。**\n\n具体而言，作者观察到：\n\n*   强化学习（如 GRPO）虽能显著提升 LLM 的多步推理表现，但计算开销巨大、调参复杂，难以普及。\n*   现有开源社区已经公开了大量经过 RL 训练的模型，其计算投入被“沉淀”在权重中，却难以被其他模型直接利用。\n\n因此，论文提出并验证了一个假设：\n\n> **“推理能力”可以在参数空间中被提取为一个紧凑的向量（reasoning vector），并通过简单的张量加减运算，零训练地注入到任何满足兼容性条件的指令微调模型中，从而复用先前的 RL 计算投资。**\n\n简言之，论文将“推理能力”视为一种可迁移、可插拔的模块化组件，解决了 RL 训练成本高、难以复用的瓶颈。\n\n**Q2**: 有哪些相关研究？\n\n论文在 §2 中系统回顾了与“推理增强”和“任务向量/模型合并”两条主线相关的研究，可归纳为以下三类：\n\n1\\. 大模型推理能力增强\n\n方法类别\n\n代表工作\n\n核心思想\n\n**Prompting 策略**\n\nCoT \\[28,29\\]、Self-Consistency \\[28\\]、Tree-of-Thoughts \\[33\\]、Zero-shot “think step by step” \\[15\\]\n\n不改动参数，仅通过提示词触发已有推理路径\n\n**程序辅助推理**\n\nProgram-of-Thought \\[3\\]、PAL \\[8\\]\n\n将计算步骤外包给外部解释器，减少模型算术负担\n\n**训练式方法**\n\nRLHF \\[23,4\\]、PPO \\[26\\]、GRPO \\[24\\]、Verifier-guided RL \\[5,17\\]\n\n通过监督微调或 RL 在数据上直接编码推理能力\n\n作者指出：上述训练式方法虽有效，但需重复昂贵训练；而本文通过“任务算术”把 RL 训练结果提取成向量，实现**零训练迁移**，桥接了“训练式”与“提示式”两条路线。\n\n2\\. 任务算术与模型合并\n\n技术\n\n代表工作\n\n关键贡献\n\n**任务算术**\n\nIlharco et al. \\[13\\]\n\n证明微调能力可表示为参数差向量，可加减\n\n**冲突消解**\n\nTIES-Merging \\[31\\]\n\n解决符号冲突，减少向量合并时的干扰\n\n**重要性加权**\n\nFisher Merging \\[19\\]\n\n用 Fisher 信息给参数加权，提升合并鲁棒性\n\n**回归视角**\n\nRegMean \\[14\\]\n\n将最优参数组合形式化为线性回归问题\n\n**线性模式连通性**\n\nFrankle et al. \\[7\\]\n\n提供理论保证：同初始化模型位于同一低损盆地\n\n**实践工具**\n\nMergeKit \\[9\\]、Model Soups \\[30\\]、Ratatouille \\[25\\]\n\n在十亿级模型上实现无训练合并与域外泛化\n\n本文首次将上述技术用于\\*\\*复杂认知能力（多步推理）\\*\\*的提取与迁移，而非传统意义上的任务知识或风格迁移。\n\n3\\. 模块化/参数高效增强\n\n方法\n\n代表工作\n\n特点\n\n**低秩适配**\n\nLoRA \\[11\\]\n\n引入可训练低秩矩阵，冻结原模型\n\n**前缀/提示微调**\n\nPrefix Tuning \\[18\\]、Prompt Tuning \\[16\\]\n\n仅优化少量软提示或前缀\n\n**知识编辑**\n\nMeng et al. \\[20\\]、Tangent-space Editing \\[22\\]\n\n精准修改特定事实或神经元\n\n与这些方法相比，本文提出的 reasoning vector 属于**全局、一次性、无需再训练**的增强方式，且不局限于单一任务或局部知识。\n\n综上，本文在以下两点实现突破：\n\n*   将“推理能力”视为可迁移的通用向量，而非任务特定模式；\n*   利用开源社区已训练的 SFT/RL 模型，通过简单张量运算实现零成本复用。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过“**提取-注入**”两步法，把强化学习（RL）训练出的推理能力封装成一个可复用的参数向量，并以零训练的方式迁移到任意兼容模型。具体流程如下：\n\n1\\. 提取阶段：构造 reasoning vector\n\n*   **数据来源**：从开源社区获取两个**同架构、同初始化、同预训练**的 Qwen2.5 模型\n    *   θSFT：仅做监督微调（SFT）\n    *   θGRPO：在相同数据上继续做 Group Relative Policy Optimization（RL）\n*   **向量定义**  \n    ${% raw %}$ \\\\mathbf{v}_{\\\\text{reason}} = \\\\theta_{\\\\text{GRPO}} - \\\\theta\\_{\\\\text{SFT}} ${% endraw %}$  \n    该差值被假设为**仅包含 RL 引入的推理增强信号**，而共享知识被抵消。\n\n2\\. 注入阶段：零训练迁移\n\n*   **目标模型**：任意兼容的指令微调模型 θtarget（同架构、同 tokenizer、同源初始化）\n*   **算术操作**（仅需两次逐元素张量运算）\n    1.  预计算：{% raw %}$\\\\mathbf{v}_{\\\\text{reason}} \\\\leftarrow \\\\theta_{\\\\text{GRPO}} - \\\\theta\\_{\\\\text{SFT}}${% endraw %}\n    2.  注入：{% raw %}$\\\\theta\\_{\\\\text{enhanced}} = \\\\theta\\_{\\\\text{target}} + \\\\alpha \\\\cdot \\\\mathbf{v}\\_{\\\\text{reason}}${% endraw %}  \n        实验中 α=1 即最佳，无需再调参。\n*   **可选控制**：通过二进制掩码 m 限定注入层，实验显示全注入已足够鲁棒。\n\n3\\. 理论保障：Linear Mode Connectivity\n\n*   θSFT 与 θGRPO 从同一初始化出发，满足线性模式连通性（LMC）\n*   因此 {% raw %}$\\\\mathbf{v}\\_{\\\\text{reason}}${% endraw %} 位于共享低损盆地，注入后不会导致灾难性遗忘或损失激增。\n\n4\\. 激活策略\n\n*   在输入前加一句 “Think step by step” 可进一步激发注入的推理路径，类似 RL 训练时的条件推理。\n\n5\\. 结果验证\n\n*   在 1.5 B / 7 B 模型上，**仅两次张量运算**即可带来\n    *   GSM8K +4.9 %\n    *   HumanEval +4.3 %\n    *   BigBenchHard +12.3 %\n*   对抗扰动、向量缩放、跨域迁移、正负向消融均验证其有效性与因果性。\n\n总结：论文把昂贵的 RL 训练成果压缩成一个可插拔的向量，通过\\*\\*“减法提取、加法迁移”\\*\\*完成推理能力的回收与再利用，实现从“重训练”到“轻编辑”的范式转换。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **“reasoning vector 能否零训练地提升模型推理能力”** 设计了四类实验，覆盖性能、鲁棒性、消融及跨域迁移四个维度。所有实验均在 Qwen2.5-1.5 B 与 7 B 模型上完成，使用公开 checkpoint 与 MergeKit 工具，确保可复现。\n\n1\\. 主实验：基准性能对比\n\n**目的**：验证向量注入是否带来一致提升。  \n**配置**（四种）\n\n*   Baseline：官方 Qwen2.5-Instruct\n*   G+T：GRPO 训练源模型 + “Think step by step” 提示\n*   +Vector：Baseline + vreason（α = 1）\n*   +Vector+Think：+Vector 再加提示\n\n**数据集**\n\n*   GSM8K（数学）\n*   HumanEval / HumanEval+（代码生成）\n*   SciQ（科学问答）\n*   BigBenchHard（多跳逻辑）\n\n**结果摘要**（绝对提升，单跑）\n\n模型\n\nGSM8K\n\nHumanEval\n\nBBH\n\n1.5 B\n\n+4.9 %\n\n+4.3 %\n\n+12.3 %\n\n7 B\n\n+5.0 %\n\n+3.7 %\n\n+1.4 %\n\n2\\. 鲁棒性实验：对抗扰动测试\n\n**目的**：排除“表面模式匹配”质疑，验证推理能力是否真正内化。  \n**扰动数据集**（基于 GSM8K 自制）\n\n*   GSM Hard Lite：数值范围扩大、推理步数增多\n*   Noise+Digit：插入无关数字、拼写错误、干扰符号\n*   Sentence Shuffle：打乱句子顺序但保留逻辑依赖\n\n**结果**（1.5 B 模型）\n\n*   在所有三种扰动下，+Vector 仍稳定领先 Baseline 2–6 %。\n*   +Vector+Think 进一步提升至 3–6 % 区间，显示提示与向量互补。\n\n3\\. 消融实验：因果性 & 缩放性\n\n3.1 向量移除\n\n*   操作：θdegraded = θbaseline − vreason\n*   结果：GSM8K 准确率从 45.1 % 跌至 33.4 %（-11.8 %），证明向量的因果贡献。\n\n3.2 缩放系数 α\n\n*   α ∈ {0.5, 1.0, 1.5, 2.0}\n*   结果：α = 1.0 最优；过大或过小均略降性能，显示向量已自然校准。\n\n3.3 跨域迁移\n\n*   用代码 RL 向量（HumanEval 训练）注入数学模型 → GSM8K +2.1 %\n*   用数学 RL 向量（GSM8K 训练）注入代码模型 → HumanEval +1.8 %\n*   结论：向量包含部分**领域通用**推理成分，但主增益仍来自同源任务。\n\n4\\. 定性示例（附录）\n\n*   展示同一道多步应用题在\n    *   θbase − vreason（错误逻辑、步骤混乱）\n    *   θbase + vreason（正确聚合、步骤清晰）  \n        的输出对比，直观说明向量对推理链的因果影响。\n\n综上，实验体系完整覆盖了 **有效性、鲁棒性、因果性、通用性** 四个关键维度，为 reasoning vector 的实用价值提供了系统证据。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可视为对本文工作的自然延伸或深层拓展，按“可行性—风险—潜在收益”递进排序，供后续研究参考。\n\n1\\. 兼容性放宽与跨族迁移\n\n*   **问题**：当前要求同架构、同源初始化，限制了向量适用范围。\n*   **探索路线**\n    *   **线性映射**：学习一个轻量级映射矩阵 $W$，使得 {% raw %}$\\\\theta\\_{\\\\text{target}} + W v\\_{\\\\text{reason}}${% endraw %} 在异构模型上仍有效。\n    *   **Tokenizer 对齐**：研究子词差异对嵌入层的影响，尝试用“词汇表投影”或“嵌入插值”缓解 tokenizer 不一致问题。\n    *   **模块化注入**：仅对齐 Transformer 的某几层（如 middle layers），其余层冻结，降低架构敏感性。\n\n2\\. 多向量组合与推理技能解耦\n\n*   **问题**：单一 reasoning vector 可能混合多种推理策略（算术、逻辑、代码）。\n*   **探索路线**\n    *   **正交分解**：用 PCA、ICA 或稀疏编码将 {% raw %}$v\\_{\\\\text{reason}}${% endraw %} 拆成若干正交子向量，对应不同推理技能。\n    *   **技能插值**：类似模型汤（model soups），对不同技能向量加权组合，实现“按需推理”。\n    *   **任务算术图**：构建“向量图谱”，研究跨任务迁移路径与冲突。\n\n3\\. 动态系数与自适应路由\n\n*   **问题**：固定 $\\\\alpha=1$ 未必对所有输入最优。\n*   **探索路线**\n    *   **输入条件化**：训练一个小型路由网络 $g(x)$，根据输入难度动态输出 $\\\\alpha(x)$。\n    *   **层级门控**：在每层引入可学习的门控值 $\\\\alpha\\_l$，实现“细粒度”推理增强。\n    *   **强化微调**：用轻量级 RL 对 $\\\\alpha$ 进行再优化，仅更新极少参数。\n\n4\\. 向量压缩与低秩近似\n\n*   **问题**：完整 {% raw %}$v\\_{\\\\text{reason}}${% endraw %} 与模型同尺寸，存储/传输成本高。\n*   **探索路线**\n    *   **LoRA 化**：对 {% raw %}$v\\_{\\\\text{reason}}${% endraw %} 做秩-$r$ 分解 {% raw %}$v\\_{\\\\text{reason}} \\\\approx AB^\\\\top${% endraw %}，$r \\\\ll d$。\n    *   **量化**：8-bit 或 4-bit 量化向量，评估性能下降边界。\n    *   **稀疏掩码**：保留 top-$k%$ 幅值参数，研究稀疏度—性能帕累托前沿。\n\n5\\. 安全性与对抗鲁棒性再审视\n\n*   **问题**：若恶意向量被注入，是否可植入后门或误导推理？\n*   **探索路线**\n    *   **向量认证**：设计检测机制，判断未知向量是否包含恶意梯度方向。\n    *   **对抗训练**：在注入阶段加入对抗样本，提升向量鲁棒性。\n    *   **可解释性**：可视化向量方向与注意力模式，定位高风险参数区域。\n\n6\\. 跨模态与工具增强推理\n\n*   **问题**：当前仅限文本，能否迁移到多模态或工具调用场景？\n*   **探索路线**\n    *   **视觉推理**：从文本-图像 RL 模型提取向量，注入 LLaVA 类模型，测试 MathVista 等基准。\n    *   **工具使用**：对 Code-Interpreter RL 模型提取向量，注入支持函数调用的 Chat 模型，评估 API 调用准确率。\n    *   **具身推理**：在机器人控制或游戏环境中，验证向量对策略泛化的帮助。\n\n7\\. 理论深化\n\n*   **问题**：LMC 仅提供充分条件，缺乏对 reasoning vector 的严格理论刻画。\n*   **探索路线**\n    *   **信息论视角**：用 Fisher 信息或 Hessian 轨迹分析 {% raw %}$v\\_{\\\\text{reason}}${% endraw %} 的有效维度。\n    *   **损失景观切片**：沿 {% raw %}$v\\_{\\\\text{reason}}${% endraw %} 方向做高密度采样，量化平坦区域宽度与性能边界。\n    *   **泛化误差界**：推导向量迁移后的 PAC-Bayes 或 Rademacher 复杂度上界。\n\n8\\. 社区级“向量市场”\n\n*   **问题**：如何鼓励开源社区持续发布高质量 reasoning vector？\n*   **探索路线**\n    *   **标准化格式**：定义 JSON/ Safetensors schema，包含向量来源、训练数据、验证指标。\n    *   **排行榜**：建立跨基准的向量性能榜单，促进公平比较。\n    *   **许可证设计**：研究针对“向量”而非“权重”的新型开源协议，防止闭源滥用。\n\n以上方向既可独立开展，也可组合研究；其中 1、2、4 属于短期可行且工程价值高；3、5、6 兼具学术与应用潜力；7、8 则偏向长期生态与理论建设。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n通过“θGRPO − θSFT”提取一条可复用的 **reasoning vector**，仅用两次张量加减即可把强化学习练出的多步推理能力零训练地迁移到任何兼容模型，显著降低推理增强成本。\n\n核心内容速览\n\n模块\n\n要点\n\n**问题**\n\nRL 虽能提升推理，但训练昂贵；如何复用已有 RL 模型的计算成果？\n\n**方法**\n\n1\\. 取公开 Qwen2.5 的两份权重：仅 SFT 的 θSFT 与继续 RL 的 θGRPO  \n2\\. 定义 **vreason = θGRPO − θSFT**  \n3\\. 对任意同源指令模型执行 **θenhanced = θtarget + vreason**\n\n**理论**\n\n利用 Linear Mode Connectivity 保证加减操作仍在低损盆地，避免灾难遗忘。\n\n**实验**\n\n• 1.5 B/7 B 模型在 GSM8K、HumanEval、SciQ、BigBenchHard 上 **+1.4 %–12.3 %**  \n• 对抗扰动、向量缩放、跨域迁移、正负向消融均验证因果性\n\n**结论**\n\n推理能力可作为 **模块化向量** 存储、迁移、复用，实现“训练一次，人人受益”。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Reasoning%20Vectors%3A%20Transferring%20Chain-of-Thought%20Capabilities%20via%20Task%20Arithmetic，链接是  https://arxiv.org/pdf/2509.01363  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01363  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1756943220807","title":"Implicit Actor Critic Coupling via a Supervised Learning Framework for RLVR","description":"Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) have empowered large language models (LLMs) to tackle challenging reasoning tasks such as mathematics and programming. RLVR leverages verifiable outcome rewards to guide policy optimization, enabling LLMs to progressively improve output quality in a grounded and reliable manner. Despite its promise, the RLVR paradigm poses significant challenges, as existing methods often suffer from sparse reward signals and unstable policy gradient updates, particularly in RL-based approaches. To address the challenges, we propose $\\textbf{PACS}$, a novel RLVR framework that achieves im$\\textbf{P}$licit $\\textbf{A}$ctor $\\textbf{C}$ritic coupling via a $\\textbf{S}$upervised learning framework. By treating the outcome reward as a predictable label, we reformulate the RLVR problem into a supervised learning task over a score function parameterized by the policy model and optimized using cross-entropy loss. A detailed gradient analysis shows that this supervised formulation inherently recovers the classical policy gradient update while implicitly coupling actor and critic roles, yielding more stable and efficient training. Benchmarking on challenging mathematical reasoning tasks, PACS outperforms strong RLVR baselines, such as PPO and GRPO, achieving superior reasoning performance. For instance, PACS achieves 59.78\\% at pass@256 on AIME 2025, representing improvements of 13.32 and 14.36 points over PPO and GRPO. This simple yet powerful framework offers a promising avenue for LLMs post-training with verifiable rewards. Our code and data are available as open source at https://github.com/ritzz-ai/PACS.","url":"https://huggingface.co/papers/2509.02522","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.807Z","metadata":{"authors":["Jiaming Li","Longze Chen","Ze Gong","Yukun Chen","Lu Wang","Wanwei He","Run Luo","Min Yang"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CL","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.02522.pdf","arxivUrl":"https://arxiv.org/abs/2509.02522","arxivId":"2509.02522","coolPaperUrl":"https://papers.cool/arxiv/2509.02522","published":"2025-09-02T17:22:46Z","updated":"2025-09-02T17:22:46.000Z","zh_summary":"具有可验证奖励的强化学习 （RLVR） 的最新进展使大型语言模型 （LLM） 能够处理数学和编程等具有挑战性的推理任务。RLVR 利用可验证的结果奖励来指导策略优化，使 LLM 能够以扎根可靠的方式逐步提高输出质量。尽管 RLVR 范式前景广阔，但它带来了重大挑战，因为现有方法经常受到稀疏奖励信号和不稳定的策略梯度更新的影响，特别是在基于 RL 的方法中。为了应对这些挑战，我们提出了 $\\textbf{PACS}$，这是一种新颖的 RLVR 框架，它通过 $\\textbf{S}$upervised 学习框架实现了 im$\\textbf{P}$licit $\\textbf{A}$ctor $\\textbf{C}$ritic 耦合。通过将结果奖励视为可预测的标签，我们将 RLVR 问题重新表述为由策略模型参数化并使用交叉熵损失进行优化的分数函数上的监督学习任务。详细的梯度分析表明，这种监督公式本质上恢复了经典的策略梯度更新，同时隐式耦合了参与者和批评者的角色，从而产生了更稳定和高效的训练。PACS 以具有挑战性的数学推理任务为基准，优于 PPO 和 GRPO 等强大的 RLVR 基线，实现了卓越的推理性能。例如，PACS 在 AIME 2025 中以 59.78\\% 的pass@256率实现，比 PPO 和 GRPO 提高了 13.32 和 14.36 个百分点。这个简单而强大的框架为法学硕士在培训后提供了一条有前途的途径，并获得了可验证的奖励。我们的代码和数据在 https://github.com/ritzz-ai/PACS 上以开源形式提供。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n*   **稀疏奖励信号**：在 RLVR 中，只有最终答案的对错（0/1）作为奖励，且仅在完整输出后给出，导致传统 RL 方法难以进行细粒度的信用分配，梯度更新稀疏且不稳定。\n*   **训练不稳定性**：现有基于 PPO、GRPO 等策略梯度方法要么依赖额外的价值模型（增加复杂度），要么采用高方差的蒙特卡洛估计（导致训练震荡）。\n*   **范式局限**：传统 RL 范式将策略优化与奖励估计分离（显式 Actor-Critic 结构），存在时间不一致和超参数敏感的问题。\n\n**核心目标**：提出一种无需显式价值模型、通过监督学习框架隐式耦合 Actor-Critic 的新方法 PACS，以稳定、高效地利用可验证奖励提升 LLM 的数学推理能力。\n\n**Q2**: 有哪些相关研究？\n\n与本文相关的研究可分为两条主线：**推理模型（Reasoning Models）** 与 **可验证奖励强化学习（RLVR）**。以下按类别归纳：\n\n1\\. 推理模型（Reasoning-oriented LLMs）\n\n*   **OpenAI-o1**（Jaech et al., 2024）  \n    首个公开强调“链式思维”推理能力的模型，通过扩大测试时计算（test-time compute）提升数学与编程任务表现。\n*   **DeepSeek-R1**（Guo et al., 2025）  \n    采用 **GRPO**（Shao et al., 2024）与规则型可验证奖励，展示 RLVR 在数学推理上的显著增益。\n*   **QWQ**（Qwen-Team, 2025）  \n    两阶段 RL 框架：冷启动阶段建立推理能力，后续通用 RL 微调扩展泛化性。\n*   **Gemini 2.5 Pro**（Comanici et al., 2025）  \n    融合人类反馈、检索增强与多模态感知，推动推理模型向通用智能体演进。\n\n2\\. 可验证奖励强化学习（RLVR）\n\n2.1 基于价值模型（Value-model-based）\n\n*   **PPO**（Schulman et al., 2017）  \n    经典 Actor-Critic 方法，使用广义优势估计（GAE）降低方差，但需要独立的价值网络。\n*   **VinePPO**（Kazemnejad et al., 2024）  \n    通过细粒度信用分配改进 PPO，用于 LLM 的 RLHF 训练。\n*   **VAPO**（Yue et al., 2025）  \n    针对复杂推理任务设计的价值模型增强方法，强调训练稳定性。\n\n2.2 无价值模型（Value-model-free）\n\n*   **GRPO**（Shao et al., 2024）  \n    通过组内相对奖励估计优势，避免显式价值网络，但依赖高方差蒙特卡洛估计。\n*   **REINFORCE++**（Hu et al., 2025）  \n    改进 REINFORCE 的基线设计，降低梯度方差。\n*   **DAPO**（Yu et al., 2025）  \n    开源大规模 RL 系统，采用去 KL 正则化的 GRPO 变体。\n\n2.3 与 PACS 相关的技术\n\n*   **RLOO 估计器**（Kool et al., 2019; Ahmadian et al., 2024）  \n    用于无偏估计相对优势，被 PACS 用作 score 函数的核心实现。\n*   **DPO**（Rafailov et al., 2023）  \n    将奖励建模转化为监督学习，但针对偏好数据；PACS 将其思想扩展到可验证奖励场景。\n*   **Dr. GRPO**（Liu et al., 2025b）  \n    去除标准差的 GRPO 变体，在消融实验中作为 PACS 的对比基线。\n\n总结\n\n类别\n\n代表方法\n\n与 PACS 的关系\n\n推理模型\n\nOpenAI-o1, DeepSeek-R1, QWQ, Gemini 2.5\n\n目标场景一致（数学推理），但 PACS 聚焦训练算法创新\n\nValue-model-based\n\nPPO, VAPO, VinePPO\n\n作为 PACS 的对照基线，展示无需独立价值网络的优势\n\nValue-model-free\n\nGRPO, REINFORCE++, DAPO\n\n直接对比对象，PACS 通过监督学习框架降低方差与复杂度\n\n技术组件\n\nRLOO, DPO, Dr. GRPO\n\n被 PACS 采用或对比，验证设计选择的有效性\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过提出 **PACS（imPlicit Actor Critic coupling via a Supervised learning framework）** 框架，将 RLVR 问题重新表述为监督学习任务，从而绕过传统 RL 方法在稀疏奖励下的固有缺陷。具体解决方案分为三个层面：\n\n1\\. 问题重述：从 RL 到监督学习\n\n*   **核心思想**  \n    将可验证的 0/1 奖励视为**标签**，而非稀疏的强化信号。  \n    目标不再是最大化期望奖励，而是训练一个由策略参数化的**打分函数** $\\\\psi(q,o;\\\\pi\\_\\\\theta)$ 来预测该标签，使用**二元交叉熵损失**：\n    \n    \\[ \\\\mathcal{L}(\\\\theta) = -\\\\mathbb{E}_{q,o\\\\sim\\\\pi_\\\\theta(\\\\cdot|q)}\\\\Big\\[R(q,o)\\\\log\\\\sigma(\\\\psi) + (1-R)\\\\log(1-\\\\sigma(\\\\psi))\\\\Big\\] \\]\n    \n*   **优势**  \n    直接利用监督学习的稳定优化特性，避免稀疏奖励导致的梯度消失或高方差问题。\n    \n\n2\\. 隐式 Actor-Critic 耦合：统一梯度更新\n\n*   **梯度分解**  \n    对交叉熵损失求导后，梯度自然分解为两项（公式6）：\n    \n    1.  **Actor 项**：$\\\\mathbb{E}\\[\\\\ell(q,o;\\\\pi\\_\\\\theta)\\\\nabla\\_\\\\theta\\\\log\\\\pi\\_\\\\theta(o|q)\\]$  \n        等价于策略梯度，但权重由预测误差 $\\\\ell$ 动态调整。\n    2.  **Critic 项**：$\\\\mathbb{E}\\[(R-\\\\sigma(\\\\psi))\\\\nabla\\_\\\\theta\\\\psi\\]$  \n        通过残差 $R-\\\\sigma(\\\\psi)$ 直接优化打分函数，无需独立价值网络。\n*   **耦合机制**  \n    两项共享同一参数 $\\\\theta$，实现**单步更新**中策略改进（Actor）与奖励估计（Critic）的协同优化，消除传统 Actor-Critic 的时间不一致性。\n    \n\n3\\. 打分函数实例化：RLOO 与稳定训练\n\n*   **RLOO 估计器**  \n    采用 REINFORCE Leave-One-Out 计算相对优势：\n    \n    {% raw %}\\[ \\\\psi(q,o\\_i;\\\\pi\\_\\\\theta) = \\\\underbrace{\\\\beta\\\\log\\\\frac{\\\\pi\\_\\\\theta(o\\_i|q)}{\\\\pi\\_{\\\\text{ref}}(o\\_i|q)}}_{\\\\text{reward proxy } \\\\hat{r}(q,o\\_i)} - \\\\frac{1}{G-1}\\\\sum_{j\\\\neq i}\\\\hat{r}(q,o\\_j) \\]{% endraw %}\n    \n    *   通过组内对比降低方差，避免显式价值模型。\n    *   周期性重置参考策略 {% raw %}$\\\\pi\\_{\\\\text{ref}}${% endraw %} 防止训练不稳定。\n*   **类别不平衡处理**  \n    对正确/错误样本加权（King & Zeng, 2001），缓解数据分布偏差。\n    \n\n4\\. 实验验证：性能与稳定性\n\n*   **显著优于基线**  \n    在 AIME 2025 上，PACS 的 pass@256 达到 **59.78%**，较 PPO 提升 **13.32** 分，较 GRPO 提升 **14.36** 分。\n*   **训练动态**  \n    熵损失与梯度范数显示 PACS 维持更高探索性，避免 PPO/GRPO 的熵崩溃问题。\n\n总结：PACS 的解决路径\n\n传统 RLVR 问题\n\nPACS 解决方案\n\n稀疏奖励导致信用分配困难\n\n将奖励视为标签，用交叉熵损失直接优化\n\n高方差梯度（GRPO 等）\n\nRLOO 组内相对估计 + 预测误差加权\n\nActor-Critic 结构复杂\n\n共享参数隐式耦合，单模型双角色\n\n训练不稳定\n\n监督学习优化 + 参考策略重置 + 类别加权\n\n通过“监督学习形式的策略优化”，PACS 在保持简洁性的同时，实现了稳定、高效的 RLVR 训练。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **数学推理能力** 与 **训练稳定性** 两条主线，在 4 个公开基准、2 个模型规模上系统验证了 PACS 的有效性，并辅以消融与动态分析。实验设计可概括为以下 5 组：\n\n1\\. 主实验：4 个数学基准上的 pass@k 对比\n\n**数据集**\n\n*   MATH 500（500 题）\n*   AMC23（2023 AMC 系列竞赛题）\n*   AIME 2024 / 2025（最新两届 AIME 邀请赛）\n\n**模型规模**\n\n*   Qwen2.5-3B（训练 140 步）\n*   Qwen2.5-7B（训练 300 步）\n\n**基线**\n\n*   Base（未微调）\n*   PPO（价值模型型 RLVR）\n*   GRPO（无价值模型型 RLVR）\n\n**评估指标**\n\n*   pass@k（k ∈ {1,2,4,8,16,32,64,128,256}），采用无偏估计（Chen et al., 2021）。\n\n**关键结果**\n\n数据集\n\n模型\n\npass@256 提升（vs PPO / GRPO）\n\nAIME 2025\n\n7B\n\n**+15.31** / **+11.77**\n\nAIME 2024\n\n7B\n\n**+13.32** / **+14.36**\n\nMATH 500\n\n3B\n\n67.31@1（超 Base +10.65）\n\nAMC23\n\n7B\n\n78.17@8（超 GRPO +5.97）\n\n2\\. 消融实验\n\n2.1 超参数 β 敏感性\n\n*   β ∈ {0.1, 0.5, 1, 2, 10}\n*   **结论**：β = 1 时整体最优；高难度任务（AIME）对 β 更敏感。\n\n2.2 优势估计器对比\n\n*   默认 **RLOO** vs **GRPO** vs **Dr. GRPO**\n*   **结论**：\n    *   简单任务（MATH 500、AMC23）三者相近；\n    *   高难度任务（AIME 24/25）RLOO 显著领先，pass@256 领先 Dr. GRPO **≈ 9 个百分点**。\n\n2.3 类别加权机制\n\n*   PACS vs PACS w/o weight\n*   **结论**：加权机制在 AIME 上带来 **6.36** 个点的 pass@32 提升，验证其缓解数据不平衡的作用。\n\n3\\. 训练动态分析（Qwen2.5-7B）\n\n**监控指标**\n\n*   熵损失（Entropy Loss）\n*   梯度范数（Gradient Norm）\n*   平均响应长度（Response Length）\n\n**发现**\n\n*   **PPO / GRPO**：熵快速崩溃 → 保守输出；梯度范数低且平稳 → 收敛过早。\n*   **PACS**：熵先升后稳（≈ 1200–1300），梯度范数持续高位（峰值 40）→ 保持探索与更新活力。\n*   **响应长度**：PACS 生成更长、更详细的推理链。\n\n4\\. 扩展结果\n\n附录给出全部 k 值的完整表格（AMC23、AIME 2024/2025 的 3B & 7B 结果），确保可复现与进一步对比。\n\n5\\. 实验总结\n\n维度\n\n验证内容\n\n结论\n\n性能\n\n4 基准 + 2 规模\n\nPACS 全面领先 PPO/GRPO，尤其高难度任务\n\n稳健性\n\nβ、估计器、加权\n\n设计选择均对性能有显著影响\n\n训练\n\n熵、梯度、长度\n\nPACS 实现更优探索-利用平衡，避免熵崩溃\n\n实验体系完整覆盖了 **效果、鲁棒性、机制解释** 三个层面，支撑了 PACS 作为 RLVR 新范式的有效性。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可在 PACS 基础上继续深入，分为 **方法改进、理论分析、场景扩展、系统优化** 四大类：\n\n1\\. 方法改进\n\n*   **细粒度奖励**  \n    将 0/1 结果奖励拆解为 **步骤级或子问题级** 信号（如部分得分、关键推理步骤正确性），构造多标签或序列标注形式的监督目标，进一步缓解稀疏性。\n*   **动态 β 与自适应加权**  \n    用元学习或在线贝叶斯方法 **自动调整 β** 及类别权重，避免人工搜索；同时可随训练阶段动态降低 KL 正则强度，实现从保守到开放的平滑过渡。\n*   **混合估计器**  \n    将 RLOO 与 **GAE、V-trace** 等低方差估计结合，设计门控网络根据样本难度或不确定性选择最优估计器，兼顾方差与偏差。\n\n2\\. 理论分析\n\n*   **收敛性与样本复杂度**  \n    在 bandit 或 tabular MDP 简化设定下，证明 PACS 的 **收敛速率** 及与 PPO/GRPO 的样本复杂度差距，量化监督损失带来的方差缩减。\n*   **隐式正则化机制**  \n    研究交叉熵损失在策略空间中诱导的 **隐式熵正则** 与 **相对熵约束**，解释为何 PACS 能自然保持较高策略熵。\n*   **函数逼近误差影响**  \n    分析打分函数 ψ 的 **表达能力**（如深度、宽度、LoRA rank）对策略偏差的上界，给出最小化近似误差的网络设计指导。\n\n3\\. 场景扩展\n\n*   **代码生成与形式验证**  \n    将 PACS 迁移至 **编程竞赛、定理证明** 等可自动验证领域，利用单元测试或形式化验证器作为 0/1 奖励，验证跨任务鲁棒性。\n*   **多模态推理**  \n    在 **几何题、图表问答** 中引入视觉编码器，构造跨模态 (q, o) 输入，检验打分函数 ψ 对多模态上下文的建模能力。\n*   **对话式数学辅导**  \n    把单轮问答扩展为多轮 **对话式解题**，奖励定义为最终答案正确性，但需在每轮维护对话历史，探索长程信用分配。\n\n4\\. 系统优化\n\n*   **大规模训练框架**  \n    基于 **3D-并行 + ZeRO-3** 将 PACS 扩展至 70B+ 模型，研究梯度同步频率、微批次大小对训练效率与收敛的影响。\n*   **在线课程学习**  \n    设计 **难度自适应课程**：根据当前模型 pass@k 曲线动态筛选训练题目，优先训练模型成功率 30%–70% 的区间，提升样本效率。\n*   **推理-训练协同优化**  \n    在推理阶段采用 **test-time scaling**（如多数投票、过程奖励模型）生成更高质量样本，再反哺 PACS 训练，形成自我改进闭环。\n\n5\\. 风险与伦理\n\n*   **奖励劫持与过度优化**  \n    监测模型是否通过 **符号操纵、格式漏洞** 获得高分答案，引入 **鲁棒性测试集** 与对抗奖励函数，确保真实推理能力提升。\n*   **可解释性工具**  \n    利用注意力热图或因果探针分析 ψ 的决策依据，验证打分函数是否关注关键推理步骤而非表面特征。\n\n总结：潜在研究路线图\n\n阶段\n\n目标\n\n关键问题\n\n短期\n\n细粒度奖励 + 动态 β\n\n步骤级标签如何构造？β 自适应是否稳定？\n\n中期\n\n理论保证 + 代码/定理迁移\n\n收敛速率与样本复杂度上界？跨任务超参数是否一致？\n\n长期\n\n多模态 + 在线课程 + 70B 规模\n\n视觉-文本联合 ψ 如何设计？课程策略能否持续增益？\n\n这些方向既可直接在 PACS 框架上迭代，也可作为 RLVR 领域更广泛的开放问题。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\nPACS 把 RLVR 中的稀疏 0/1 奖励直接当作监督标签，用交叉熵损失训练一个由策略本身参数化的打分函数，从而在同一套参数里隐式完成 Actor-Critic 耦合，显著提升了大模型在数学推理任务上的效果与训练稳定性。\n\n主要内容提炼\n\n维度\n\n关键信息\n\n**问题**\n\n现有 RLVR 方法受限于稀疏奖励带来的信用分配难、梯度高方差或价值模型复杂。\n\n**思路**\n\n将 RL 问题重构成**监督学习**：用策略 πθ 直接预测 (q,o) 的 0/1 正确性标签，损失为二元交叉熵。\n\n**理论**\n\n梯度自然拆成两项：  \n1\\. Actor：策略梯度，权重由预测误差决定；  \n2\\. Critic：残差驱动打分函数更新。  \n二者共享参数，实现**隐式 Actor-Critic 耦合**。\n\n**实现**\n\n打分函数 ψ 用 **RLOO 相对优势** 计算，基于 log-prob ratio；定期重置参考策略 πref 防漂移；对正确/错误样本加权缓解类别不平衡。\n\n**实验**\n\n在 MATH 500、AMC23、AIME 2024/2025 上，用 3B/7B Qwen2.5 模型：  \n\\- 全面优于 PPO、GRPO；  \n\\- AIME 2025 pass@256 提升 13–15 个百分点；  \n\\- 消融显示 β=1、RLOO、类别加权均关键；  \n\\- 训练动态表明 PACS 保持更高熵与梯度活力，避免熵崩溃。\n\n**贡献**\n\n提出一种**无需独立价值网络、训练稳定、效果领先**的 RLVR 新范式，并开源代码与数据。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Implicit%20Actor%20Critic%20Coupling%20via%20a%20Supervised%20Learning%20Framework%20for%20RLVR，链接是  https://arxiv.org/pdf/2509.02522  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02522  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1756943220809","title":"Jointly Reinforcing Diversity and Quality in Language Model Generations","description":"Post-training of Large Language Models (LMs) often prioritizes accuracy and helpfulness at the expense of diversity. This creates a tension: while post-training improves response quality, it also sharpens output distributions and reduces the range of ideas, limiting the usefulness of LMs in creative and exploratory tasks such as brainstorming, storytelling, or problem solving. We address this challenge with Diversity-Aware Reinforcement Learning (DARLING), a framework that jointly optimizes for response quality and semantic diversity. At its core, DARLING introduces a learned partition function to measure diversity beyond surface-level lexical variations. This diversity signal is then combined with a quality reward during online reinforcement learning, encouraging models to generate outputs that are both high-quality and distinct. Experiments across multiple model families and sizes show that DARLING generalizes to two regimes: non-verifiable tasks (instruction following and creative writing) and verifiable tasks (competition math). On five benchmarks in the first setting, DARLING consistently outperforms quality-only RL baselines, producing outputs that are simultaneously of higher quality and novelty. In the second setting, DARLING achieves higher pass@1 (solution quality) and pass@k (solution variety). Most strikingly, explicitly optimizing for diversity catalyzes exploration in online RL, which manifests itself as higher-quality responses.","url":"https://huggingface.co/papers/2509.02534","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.809Z","metadata":{"authors":["Tianjian Li","Yiming Zhang","Ping Yu","Swarnadeep Saha","Daniel Khashabi","Jason Weston","Jack Lanchantin","Tianlu Wang"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CL","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.02534.pdf","arxivUrl":"https://arxiv.org/abs/2509.02534","arxivId":"2509.02534","coolPaperUrl":"https://papers.cool/arxiv/2509.02534","published":"2025-09-02T17:38:47Z","updated":"2025-09-02T17:38:47.000Z","zh_summary":"大型语言模型（LM）的后期训练往往优先考虑准确性和有用性，而牺牲多样性。这产生了一种紧张关系：虽然后期训练提高了响应质量，但它也加剧了输出分布，减少了思想的范围，限制了 LM 在创造性和探索性任务中的有用性，例如头脑风暴、讲故事或解决问题。我们通过多样性感知强化学习（DARLING）来解决这一挑战，这是一种共同优化响应质量和语义多样性的框架。在其核心，DARLING 引入了一个学习的划分函数，以测量表面层面词汇变化之外的多样性。然后在在线强化学习过程中，将这种多样性信号与质量奖励结合，鼓励模型生成既高质量又独特的输出。在多个模型系列和规模的实验中，DARLING 在两种模式下都表现出良好的泛化能力：不可验证任务（指令跟随和创造性写作）和可验证任务（竞赛数学）。在第一个设置的五个基准测试中，DARLING 一直优于仅考虑质量的强化学习基线，产生同时具有更高质量和新颖性的输出。在第二个设置中，DARLING 达到了更高的通过率@1（解决方案质量）和通过率@k（解决方案多样性）。最引人注目的是，明确优化多样性促进了在线强化学习中的探索，这表现为更高质量的响应结果。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文针对大型语言模型（LLM）在**后训练阶段**出现的**多样性崩溃**现象，提出了一种新的在线强化学习方法——**DARLING（Diversity-Aware Reinforcement Learning）**，旨在**同时优化响应质量与语义多样性**，解决以下核心问题：\n\n*   **后训练导致输出分布过度尖锐化**：现有RLHF/RL后训练方法（如GRPO）以“质量”或“正确性”为唯一优化目标，导致模型输出趋于同质化，重复或近似重复的回答显著增加，限制了模型在**创意写作、头脑风暴、科学假设生成**等需要高多样性任务中的实用性。\n*   **缺乏语义层面的多样性度量**：传统基于词法（如n-gram重叠）的多样性指标无法捕捉深层语义差异，而人工评估又难以规模化集成到在线训练中。\n*   **质量与多样性的权衡困境**：现有方法往往需手动调节权重或牺牲一方性能，难以在\\*\\*可验证任务（如竞赛数学）**与**不可验证任务（如开放写作）\\*\\*中同时提升质量与多样性。\n\nDARLING通过以下机制解决上述问题：\n\n1.  **语义等价分类器**：训练一个可扩展的二元分类器，将同一prompt的多个回答划分为语义等价簇，从而量化**语义多样性**。\n2.  **乘法奖励融合**：将质量奖励与归一化的语义多样性奖励**相乘**（而非相加），避免尺度失衡，直接放大“高质量且语义独特”回答的梯度信号。\n3.  **在线RL兼容性**：在不修改模型架构或解码策略的前提下，嵌入现有GRPO框架，兼容不同模型规模（8B-70B）与任务类型（数学/写作）。\n\n**Q2**: 有哪些相关研究？\n\n论文在 §7 与附录 H 中系统梳理了与“训练阶段提升多样性”和“推理阶段提升多样性”相关的研究，并明确区分了 DARLING 与这些工作的差异。以下按主题归纳：\n\n1\\. **训练阶段多样性增强（Training-time Diversity）**\n\n方法类别\n\n代表工作\n\n核心思想\n\n与 DARLING 的区别\n\n**修改最大似然目标**\n\nLi et al., 2016b（MMI）；Welleck et al., 2020（Unlikelihood）；Li et al., 2020（Gaussian prior）；Zhang et al., 2024（高熵匹配）\n\n在预训练或微调阶段通过损失函数惩罚重复、鼓励高熵输出\n\n均为**离线**修改交叉熵损失，DARLING 是在**在线 RL**中直接重塑奖励\n\n**偏好优化类**\n\nDivPO（Lanchantin et al., 2025a）；Ismayilzada et al., 2025（Creative PO）\n\n在 DPO 框架内将“多样性”作为偏好对的一部分\n\n属于**离线**偏好学习，DARLING 是**在线 RL**\n\n**在线 RL 重加权**\n\nHe et al., 2025a（Rewarding the Unlikely）；Chen et al., 2025a（Seed-GRPO）\n\n通过似然或熵调节奖励权重以促进探索\n\n仍依赖**词法或熵信号**，DARLING 引入**语义等价分类器**\n\n**梯度/表示级探索**\n\nJung et al., 2025（Prismatic Synthesis）\n\n利用梯度相似度进行数据多样化\n\n作用于**数据增强**，DARLING 作用于**奖励塑形**\n\n2\\. **推理阶段多样性增强（Inference-time Diversity）**\n\n方法类别\n\n代表工作\n\n核心思想\n\n与 DARLING 的关系\n\n**改进解码算法**\n\nVijayakumar et al., 2018（Diverse Beam Search）；Kulikov et al., 2019\n\n在 beam search 中加入多样性惩罚\n\n**正交**：DARLING 训练模型本身，可与任何解码策略叠加\n\n**提示工程**\n\nNagarajan et al., 2025（随机种子）；Shur-Ofry et al., 2024（persona 条件）；Zhang et al., 2025b（直接提示“be diverse”）\n\n通过 prompt 设计诱导多样化输出\n\n**互补**：DARLING 训练后的模型在推理阶段仍可受益于这些技巧\n\n3\\. **多样性度量与评估**\n\n度量类型\n\n代表工作\n\n特点\n\nDARLING 的改进\n\n**词法度量**\n\nDistinct-n（Li et al., 2016a）；Self-BLEU（Zhu et al., 2018）\n\n计算 n-gram 重叠或 BLEU 多样性\n\n无法捕捉语义等价，DARLING 采用**语义分类器**\n\n**神经度量**\n\nWieting & Gimpel, 2018（ParaNMT 嵌入）；He et al., 2025a（log-likelihood 差异）\n\n使用句子嵌入距离\n\n计算开销大，DARLING 用**轻量级分类器**实现在线集成\n\n**LM-as-a-Judge**\n\nLanchantin et al., 2025a；Zhang et al., 2025b\n\n用强模型评估语义等价\n\n推理成本高，DARLING 将其蒸馏为**小模型分类器**\n\n4\\. **多样性崩溃与后训练分析**\n\n研究方向\n\n代表工作\n\n结论\n\nDARLING 的回应\n\n**模式崩溃归因**\n\nKirk et al., 2024；Yang & Holtzman, 2025；O’Mahony et al., 2024\n\nRLHF 导致分布收缩、熵降低\n\n通过**奖励塑形**反向抵消收缩趋势\n\n**探索-利用权衡**\n\nLiu et al., 2025d；Zeng et al., 2025\n\n增大熵或调整 KL 系数不一定提升性能\n\nDARLING 不引入新超参数，直接**加权高质量且语义独特的轨迹**\n\n5\\. **并发与后续工作**\n\n*   **Chen et al., 2025b**（Pass@k Training）：用 pass@k 作为 RL 奖励，与 DARLING 的 pass@k 评估目标一致，但未显式建模语义多样性。\n*   **Liu et al., 2025c**（R1-Zero 分析）：指出 GRPO 的标准差归一化会放大噪声，DARLING 在 §6.3 通过实验验证并采纳该建议，在稠密奖励场景移除归一化。\n\n综上，DARLING 与现有工作的核心差异在于：\n\n*   **语义级多样性信号**：超越 n-gram 或熵，使用可学习的语义等价分类器。\n*   **在线 RL 奖励塑形**：在训练阶段直接放大“高质量且语义独特”样本的梯度，而非事后解码或离线损失修正。\n*   **模型与任务无关性**：无需调整架构或超参数，即可在 8B-70B 模型、写作/数学任务上通用。\n\n**Q3**: 论文如何解决这个问题？\n\n论文提出的 **DARLING（Diversity-Aware Reinforcement Learning）** 通过以下三步机制，在在线 RL 训练阶段同时优化**响应质量**与**语义多样性**，从而系统性解决后训练导致的多样性崩溃问题。\n\n1\\. 语义等价划分：用可学习分类器量化“真实多样性”\n\n*   **输入**：对同一 prompt 的 n 条 rollout ${y\\_1,\\\\dots,y\\_n}$。\n*   **分类器** $ \\\\text{classify}(y\\_i,y\\_j)\\\\in{0,1}$：判断两条回答是否语义等价。\n*   **输出**：将回答聚类为若干语义等价簇，得到每条回答的多样性分数  \n    $$  \n    \\\\text{Div}_d(y\\_i)=\\\\frac{1}{n-1}\\\\sum_{j\\\\neq i}\\\\bigl(1-\\\\text{classify}(y\\_i,y\\_j)\\\\bigr)\\\\in\\[0,1\\].  \n    $$  \n    该指标仅统计“语义独特”的比例，避免表面词汇差异。\n\n> 实现细节：\n> \n> *   非可验证任务：微调 ModernBERT-base（8 k token 窗口）。\n> *   可验证数学任务：微调 Qwen3-Embedding-4B，用 Llama-3.3-70B 标注 2 万对解。\n\n2\\. 乘法奖励融合：将多样性信号嵌入 RL 目标\n\n*   **质量奖励** $r(x,y\\_i)$：\n    *   非可验证任务：Athene-RM-8B 打分。\n    *   数学任务：Math-Verify 二进制正确性。\n*   **多样性-感知奖励**  \n    {% raw %}$$  \n    r\\_{\\\\text{darling}}(x,y\\_i)=r(x,y\\_i)\\\\cdot \\\\text{Norm}\\\\bigl(\\\\text{Div}\\_d(y\\_i)\\\\bigr),  \n    $${% endraw %}  \n    其中 Norm 将多样性线性映射到 $\\[0,1\\]$。\n*   **优势计算**（GRPO 改进）  \n    {% raw %}$$  \n    A\\_{i,t}=r\\_{\\\\text{darling}}(x,y\\_i)-\\\\frac{1}{n}\\\\sum\\_{j=1}^n r\\_{\\\\text{darling}}(x,y\\_j).  \n    $${% endraw %}  \n    通过**乘法**而非加法，确保梯度更新优先流向“高质量+高多样性”轨迹，避免尺度失衡。\n\n3\\. 在线 RL 训练流程：零额外超参数、即插即用\n\n*   **算法骨架**：沿用 GRPO（Group Relative Policy Optimization），仅做两处轻量修改：\n    1.  将序列级平均改为 token 级平均，消除长度偏差；\n    2.  在稠密奖励场景移除标准差归一化，抑制噪声放大（§6.3）。\n*   **训练配置**：\n    *   8×H200 GPU，8 rollout/prompt，β=0.001（或数学任务 β=0），10 epoch。\n    *   与基线共享所有超参数，无需额外调参。\n\n效果验证：质量与多样性同步提升\n\n场景\n\n指标\n\n基线 (GRPO)\n\nDARLING\n\n提升\n\n**非可验证**\n\nAlpacaEval 2.0 LCWR\n\n48.7 %\n\n55.2 %\n\n+6.5\n\nNoveltyBench Distinct\n\n2.08\n\n5.49\n\n+164 %\n\n**可验证数学**\n\npass@1 (4B)\n\n23.4 %\n\n26.9 %\n\n+3.5\n\npass@128 (4B)\n\n53.4 %\n\n61.0 %\n\n+7.6\n\n总结\n\nDARLING 通过\n\n1.  **语义等价分类器** → 精准量化多样性，\n2.  **乘法奖励融合** → 无权重调优地联合优化质量与多样性，\n3.  **在线 RL 嵌入** → 零架构改动、跨任务通用，  \n    在训练阶段直接防止分布收缩，实现“更高质量 + 更高多样性”的双赢。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **DARLING** 在两类任务（非可验证、可验证）与两类模型规模（8B/70B 与 4B/14B）上，共设计并执行了 **5 组实验 + 3 组消融**，覆盖质量、多样性、温度敏感性、奖励融合方式、多样性度量方式及归一化策略。所有实验均使用公开基准与统一训练/评估脚本，确保可复现。\n\n1\\. 非可验证任务：通用指令遵循与创意写作\n\n*   **模型**  \n    Llama-3.1-8B-Instruct、Llama-3.3-70B-Instruct\n*   **训练数据**  \n    WildChat 10 k 子集（与 Lanchantin et al., 2025a 相同）\n*   **基线**  \n    GRPO、DivPO、GRPO-Unlikeliness\n*   **评估基准与指标**\n    *   **质量**：AlpacaEval 2.0（LCWR）、ArenaHard v2.0（WR）、EQ-Bench（ELO）\n    *   **多样性**：NoveltyBench（Distinct、Distinct-4）\n*   **结果**（表 1）\n    *   8B：DARLING 在 **AlpacaEval LCWR 55.2 %**（+6.5 vs GRPO）、**Distinct 5.49**（+164 %）均夺魁。\n    *   70B：DARLING **LCWR 80.4 %**（+7.1）、**ELO 1531**（+270）同时领先。\n\n2\\. 温度敏感性：质量-多样性 Pareto 前沿\n\n*   **设置**  \n    固定模型后，仅改变采样温度 T∈{0.2,0.4,0.6,0.8,1.2}。\n*   **观测**（图 3）  \n    在 8B 与 70B 两个尺度，DARLING 的蓝色曲线始终位于 GRPO 与基线的**右上方**，即**同温度下质量与多样性双优**，有效推进 Pareto 前沿。\n\n3\\. 可验证任务：竞赛数学\n\n*   **模型**  \n    Qwen3-4B-Base、Qwen3-14B-Base\n*   **训练数据**  \n    DeepscaleR 过滤后 10 k 题\n*   **基线**  \n    GRPO\n*   **评估基准**  \n    AIME25、OlympiadBench、HMMT 2025、Brumo 2025\n*   **指标**  \n    pass@1（质量）、pass@k 至 k=128（多样性）\n*   **结果**（图 6 & 表 9/10）\n    *   4B：DARLING **pass@1 26.9 %**（+3.5）、**pass@128 61.0 %**（+7.6）。\n    *   14B：DARLING **pass@1 34.4 %**（+1.9）、**pass@128 76.4 %**（+10.2）。\n    *   最难数据集 HMMT 上提升最大，验证“探索→质量”正循环。\n\n4\\. 消融实验\n\n4.1 奖励融合方式：加法 vs 乘法\n\n*   **设置**  \n    在 Llama-3.1-8B 上比较\n    *   Quality + Diversity（加法）\n    *   Quality × Diversity（DARLING，乘法）\n*   **结果**（表 2）  \n    乘法在 AlpacaEval 与 NoveltyBench 均优于加法，且无需调权重。\n\n4.2 多样性度量：语义分类器 vs 4-gram\n\n*   **设置**  \n    用 4-gram 独有度替换语义分类器。\n*   **结果**（表 3 & 4）\n    *   非可验证任务：4-gram 在 NoveltyBench 多样性显著落后（3.59 vs 5.49）。\n    *   数学任务：4-gram 出现**奖励劫持**（生成无关反思），pass@1 反而下降（22.49 vs 26.9）。\n\n4.3 GRPO 归一化项\n\n*   **设置**  \n    移除标准差归一化（w/o norm）。\n*   **结果**（表 5 & 11）\n    *   稠密奖励（写作）：去归一化带来 **AlpacaEval +3.8 %**、**Distinct +2.1**。\n    *   稀疏奖励（数学）：归一化无影响，验证理论分析。\n\n5\\. 定性分析\n\n*   **EQ-Bench 细粒度 rubrics**（图 4）  \n    DARLING 在“Interesting & Original”“Avoids Cliché”维度胜率最高，直接体现多样性优势。\n*   **NoveltyBench 并行采样**（图 5 & C.2）  \n    同一 prompt 四次采样：基线模型四次重复“Amazon”或“Quokka”；DARLING 输出不同品牌/动物并附独特理由，展示语义多样性。\n\n实验覆盖矩阵\n\n维度\n\n非可验证\n\n可验证\n\n8B\n\n70B\n\n4B\n\n14B\n\n主实验\n\n✅\n\n✅\n\n✅\n\n✅\n\n✅\n\n✅\n\n温度敏感性\n\n✅\n\n—\n\n✅\n\n✅\n\n—\n\n—\n\n消融\n\n✅\n\n✅\n\n✅\n\n—\n\n✅\n\n—\n\n所有实验代码与模型检查点已开源： [https://github.com/facebookresearch/darling](https://github.com/facebookresearch/darling)\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 DARLING 的后续研究切入点，按“理论—方法—应用—评估”四个层面展开：\n\n1\\. 理论层面：多样性-质量耦合机制\n\n*   **奖励乘法的理论边界**  \n    当前乘法融合 {% raw %}$r\\_{\\\\text{darling}} = r \\\\cdot \\\\text{Norm}(\\\\text{Div})${% endraw %} 在实验上有效，但缺乏对梯度方差、收敛速度的理论刻画。可借鉴 **multi-objective RL** 或 **constrained RL** 框架，给出最优性保证或单调提升条件。\n*   **多样性正则化的 KL 解释**  \n    将 $\\\\log \\\\text{Div}$ 视为额外熵正则项，推导其与原始 KL 约束 {% raw %}$\\\\beta D\\_{\\\\text{KL}}(\\\\pi|\\\\pi\\_{\\\\text{ref}})${% endraw %} 的联合最优分布，分析是否等价于 **熵-质量正则化** 的特例。\n\n2\\. 方法层面：分类器与奖励设计\n\n*   **更细粒度的语义距离**  \n    将二元等价分类器升级为 **连续相似度模型**（如 BERTScore、SimCSE），构造可微的多样性奖励，避免离散聚类带来的梯度稀疏问题。\n*   **动态多样性权重**  \n    引入课程式或自适应系数 $\\\\alpha\\_t$：  \n    $$r\\_t = r + \\\\alpha\\_t \\\\cdot \\\\text{Div},$$  \n    在训练初期放大多样性以扩大探索，后期逐渐回归质量主导。\n*   **跨语言/跨模态扩展**  \n    验证分类器在 **多语言**（中/英/法）或 **多模态**（文本+代码+图像描述）场景下的迁移性，并构建相应标注数据。\n\n3\\. 应用层面：任务与系统\n\n*   **长文本与对话系统**  \n    在 **长文档生成**（>4 k tokens）与 **多轮对话** 中测试 DARLING，观察长程语义多样性是否仍能被分类器捕获；并探索 **对话上下文** 如何影响等价判断。\n*   **工具使用与代码生成**  \n    将框架迁移到 **函数调用** 或 **程序合成** 任务，定义“语义等价”为 **功能等价**（同输入同输出），验证是否能减少重复代码片段。\n*   **联邦或分布式 RL**  \n    在 **联邦 RLHF** 场景下，各客户端本地维护轻量分类器，定期聚合多样性度量，避免集中式标注瓶颈。\n\n4\\. 评估层面：度量与基准\n\n*   **多样性-质量联合指标**  \n    设计单一指标（如 **Diversity-Adjusted Quality Score, DAQS** = 质量 × log(1+多样性)），替代当前双指标报告，便于 Pareto 前沿比较。\n*   **人类细粒度评估**  \n    针对 **创意写作** 引入“新颖性”“惊喜度”“情感丰富度”等维度的人类打分，验证 DARLING 是否在这些维度上仍保持优势。\n*   **鲁棒性与对抗测试**  \n    构造 **对抗 prompt**（要求重复、模板化输出），测试模型是否仍能坚持高多样性；或引入 **多样性奖励攻击**（故意生成语义相近但表面差异大的回答），评估分类器鲁棒性。\n\n5\\. 计算与系统优化\n\n*   **分类器蒸馏与量化**  \n    将 4B/8B 分类器进一步蒸馏到 **<1B** 参数量，或量化为 **INT4**，降低在线 RL 的 GPU 显存与延迟开销。\n*   **异步多样性缓存**  \n    在多 GPU rollout 场景，维护共享的 **语义簇缓存**，避免重复计算等价关系，提升训练吞吐。\n\n6\\. 社会与伦理视角\n\n*   **多样性 vs 安全性权衡**  \n    研究提升多样性是否会意外放大 **有害或不真实内容**，并探索在奖励函数中加入 **安全过滤器** 的联合优化。\n*   **用户可控多样性接口**  \n    提供运行时旋钮，让用户在推理阶段实时调节“多样性强度”，实现个性化生成。\n\n通过上述方向，可进一步将 DARLING 从“同时提升质量与多样性”的验证性工作，扩展为 **可解释、可扩展、可泛化** 的通用 RLHF 增强框架。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\nDARLING 提出一种在线强化学习框架，通过**语义等价分类器+乘法奖励融合**，在后训练阶段**同时优化响应质量与语义多样性**，在写作与数学任务上均实现质量与多样性的双赢。\n\n核心内容速览\n\n模块\n\n关键要点\n\n**问题**\n\n现有 RLHF/GRPO 后训练导致输出分布尖锐化，多样性崩溃，影响创意与探索任务。\n\n**方法**\n\n1) 训练轻量级分类器，将同一 prompt 的 rollout 划分为语义等价簇；  \n2) 将质量奖励与归一化多样性分数**相乘**，得到多样性-感知奖励；  \n3) 嵌入 GRPO，在线放大“高质量且语义独特”回答的梯度。\n\n**实验**\n\n\\- **非可验证**（WildChat→Llama-3.1/3.3）：AlpacaEval LCWR +6.5 %，NoveltyBench Distinct +164 %。  \n\\- **可验证数学**（DeepscaleR→Qwen3-4B/14B）：pass@1 +3.5 %，pass@128 +7.6 %。  \n\\- **消融**：乘法优于加法；语义分类器优于 4-gram；移除标准差归一化在稠密奖励场景再提升。\n\n**贡献**\n\n1) 首个在在线 RL 中联合优化质量与语义多样性的系统框架；  \n2) 提供可扩展的语义等价分类器与开源实现；  \n3) 证明显式鼓励多样性可反哺质量提升。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Jointly%20Reinforcing%20Diversity%20and%20Quality%20in%20Language%20Model%20Generations，链接是  https://arxiv.org/pdf/2509.02534  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02534  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1756943220809","title":"Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling","description":"The Transformer architecture, underpinned by the self-attention mechanism, has become the de facto standard for sequence modeling tasks. However, its core computational primitive scales quadratically with sequence length (O(N^2)), creating a significant bottleneck for processing long contexts. In this paper, we propose the Gated Associative Memory (GAM) network, a novel, fully parallel architecture for sequence modeling that exhibits linear complexity (O(N)) with respect to sequence length. The GAM block replaces the self-attention layer with two parallel pathways: a causal convolution to efficiently capture local, position-dependent context, and a parallel associative memory retrieval mechanism to model global, content-based patterns. These pathways are dynamically fused using a gating mechanism, allowing the model to flexibly combine local and global information for each token. We implement GAM from scratch and conduct a rigorous comparative analysis against a standard Transformer model and a modern linear-time baseline (Mamba) on the WikiText-2 benchmark, as well as against the Transformer on the TinyStories dataset. Our experiments demonstrate that GAM is consistently faster, outperforming both baselines on training speed, and achieves a superior or competitive final validation perplexity across all datasets, establishing it as a promising and efficient alternative for sequence modeling.","url":"https://huggingface.co/papers/2509.00605","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.809Z","metadata":{"authors":["Rishiraj Acharya"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.CL","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.00605.pdf","arxivUrl":"https://arxiv.org/abs/2509.00605","arxivId":"2509.00605","coolPaperUrl":"https://papers.cool/arxiv/2509.00605","published":"2025-08-30T20:59:46Z","updated":"2025-08-30T20:59:46.000Z","zh_summary":"Transformer架构基于自注意力机制，已成为序列建模任务的事实标准。然而，它的核心计算原语随序列长度呈平方级别增长（O(N^2)），这为处理长上下文带来了显著瓶颈。在本文中，我们提出了门控关联记忆（GAM）网络，这是一种新颖的、完全并行的序列建模架构，其相对于序列长度展示线性复杂度（O(N)）。GAM模块用两个并行通道替代自注意力层：一个因果卷积用于高效捕获局部、位置依赖的上下文，另一个并行关联记忆检索机制用于建模全局、基于内容的模式。这些通道通过门控机制动态融合，使得模型能够灵活地结合每个标记的局部和全局信息。我们从零开始实现GAM，并在WikiText-2基准上对标准Transformer模型和现代线性时间基线（Mamba）进行严格的比较分析，同时在TinyStories数据集上与Transformer进行对比。我们的实验表明，GAM在训练速度上始终更快，超越了两个基线，并在所有数据集上实现了优越或具有竞争力的最终验证困惑度，确立了它作为序列建模的一种有前景的高效替代方案。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文针对的核心问题是 **Transformer 自注意力机制在长序列建模中的二次复杂度瓶颈**。  \n具体而言：\n\n*   **计算瓶颈**：自注意力需要计算 Query 与 Key 的 $N \\\\times N$ 点积，带来 $O(N^2 d)$ 的时间与显存开销，严重限制了对长序列（如高分辨率文档、基因组、长视频）的处理能力。\n*   **现有折中**：\n    *   “高效 Transformer” 通过稀疏、低秩或核近似降低复杂度，但常引入架构复杂性或表达能力损失。\n    *   线性 RNN/SSM（如 Mamba）虽达到 $O(N)$，却依赖递归扫描，牺牲了训练阶段的最大并行度。\n\nGAM 的设计目标即 **在保持完全并行、非递归的前提下，实现序列长度的线性复杂度**，同时不损失建模精度，从而为长序列任务提供一种高效且可扩展的新架构。\n\n**Q2**: 有哪些相关研究？\n\n论文将 GAM 置于“高效序列建模”这一活跃研究领域，并明确关联了三大类已有工作：\n\n1.  **高效 Transformer（稀疏 / 低秩 / 核近似）**\n    \n    *   Longformer：局部窗口 + 稀疏全局注意力，支持数千 token。\n    *   Linformer：利用低秩投影将 Key/Value 降维，把 $O(N^2)$ 降至 $O(N)$。\n    *   Performer：通过随机特征映射近似 softmax 核，实现线性注意力。  \n        _共同点_：仍围绕“如何近似自注意力矩阵”展开；GAM 则**彻底替换**该机制。\n2.  **循环 / 状态空间模型（SSM）**\n    \n    *   LSTM/GRU：经典 $O(N)$ 递归网络，训练并行度低。\n    *   S4 / Mamba：结构化状态空间模型，用硬件感知的并行扫描实现线性时间；Mamba 通过“选择性”机制动态过滤输入。  \n        _差异点_：Mamba 仍含递归扫描；GAM 完全避免递归，依赖卷积与矩阵乘的**全并行原语**。\n3.  **卷积序列模型**\n    \n    *   TCN（Temporal Convolutional Network）：因果卷积已被证明能有效捕获局部历史。  \n        _GAM 的局部路径即采用 1D 因果卷积，但进一步与全局关联记忆并行融合，并通过可学习门控动态权衡局部/全局信息_。\n\n**Q3**: 论文如何解决这个问题？\n\n论文提出 Gated Associative Memory（GAM）网络，用以下三步策略 **彻底替换自注意力**，同时满足线性复杂度与完全并行：\n\n1.  **局部路径：1D 因果卷积**\n    \n    *   对输入序列做深度可分离因果卷积（kernel=3），复杂度 $O(Nkd)$，捕获局部 n-gram 与词序。\n    *   通过非对称填充确保因果性，可完全并行执行。\n2.  **全局路径：并行关联记忆检索**\n    \n    *   维护一个可学习的记忆矩阵 $M \\\\in \\\\mathbb{R}^{\\\\text{num\\_slots} \\\\times d}$。\n    *   对全部 token 并行执行一次矩阵乘法 $X M^\\\\top$ 得到相似度，再经 softmax 加权求和，生成全局上下文；复杂度 $O(N \\\\cdot \\\\text{num\\_slots} \\\\cdot d)$，仅与序列长度成线性关系。\n3.  **动态门控融合**\n    \n    *   用单层线性映射为每个 token 生成两个门值 {% raw %}$g\\_{\\\\text{local}}, g\\_{\\\\text{global}} \\\\in (0,1)${% endraw %}。\n    *   按 {% raw %}$\\\\sigma(g\\_{\\\\text{local}})\\\\cdot \\\\text{LocalContext} + \\\\sigma(g\\_{\\\\text{global}})\\\\cdot \\\\text{GlobalContext}${% endraw %} 融合两条路径，实现 token-level 自适应地权衡局部语法与全局语义。\n\n通过 **卷积 + 关联记忆 + 门控** 这一完全可并行、无递归的组合，GAM 在保持 $O(N)$ 复杂度的同时，规避了自注意力的二次瓶颈与 SSM 的递归限制。\n\n**Q4**: 论文做了哪些实验？\n\n实验围绕 **准确率**（语言建模困惑度）与 **效率**（训练耗时、显存占用）两条主线展开，并辅以消融与扩展分析。具体设置与结果如下：\n\n1\\. 主实验：WikiText-2 与 TinyStories 对比\n\n*   **模型规模对齐**\n    \n    *   GAM：6 层，d\\_model=512，22.6 M 参数\n    *   Transformer：6 层，d\\_model=512，8 头，24.2 M 参数\n    *   Mamba：6 层，d\\_model=512，20.5 M 参数（仅 WikiText-2）\n*   **训练配置**\n    \n    *   5 epoch，AdamW，lr=3e-4，batch=32，序列长度 256，单张 NVIDIA T4\n*   **结果摘要**\n    \n\n数据集\n\n模型\n\n参数量\n\n每 epoch 时间\n\nVal PPL\n\nWikiText-2\n\nTransformer\n\n24.2 M\n\n131.9 s\n\n918.99\n\nWikiText-2\n\nMamba\n\n20.5 M\n\n127.1 s\n\n1017.54\n\nWikiText-2\n\nGAM\n\n22.6 M\n\n**117.2 s**\n\n**882.57**\n\nTinyStories\n\nTransformer\n\n24.2 M\n\n671.6 s\n\n23.55\n\nTinyStories\n\nGAM\n\n22.6 M\n\n**601.4 s**\n\n**23.15**\n\n→ GAM 在两个数据集上均取得 **最低困惑度** 与 **最短训练时间**。\n\n2\\. 扩展基准：序列长度可扩展性\n\n*   **设置**\n    \n    *   固定 batch=16，d\\_model=512，比较 **单个 block** 的前向+反向耗时与峰值显存\n    *   序列长度从 256 → 8192\n*   **关键结果**\n    \n\n序列长度\n\nGAM 时间 (ms)\n\nTransformer 时间 (ms)\n\nGAM 显存 (MB)\n\nTransformer 显存 (MB)\n\n256\n\n8.97\n\n8.90\n\n179\n\n216\n\n1024\n\n25.86\n\n74.19\n\n618\n\n1965\n\n2048\n\n51.94\n\n279.37\n\n1202\n\n7484\n\n4096\n\n105.03\n\n**OOM**\n\n2370\n\n**OOM**\n\n8192\n\n217.30\n\n**OOM**\n\n4707\n\n**OOM**\n\n→ Transformer 呈二次增长并在 4096 处 OOM；GAM 显存与时间均呈线性增长。\n\n3\\. 消融实验：验证各组件必要性（WikiText-2）\n\n配置\n\n门控\n\n局部卷积\n\n关联记忆\n\n参数量\n\nVal PPL\n\nGAM (Full)\n\n✓\n\n✓\n\n✓\n\n22.6 M\n\n**900.84**\n\nGAM (Sum Fusion)\n\n✗\n\n✓\n\n✓\n\n19.4 M\n\n942.59\n\nGAM (Global Only)\n\n✗\n\n✗\n\n✓\n\n19.4 M\n\n905.45\n\nGAM (Local Only)\n\n✗\n\n✓\n\n✗\n\n17.9 M\n\n944.70\n\n→ **门控机制** 对性能至关重要；**全局记忆** 贡献最大，但 **局部卷积** 仍提供不可或缺的补充信息。\n\n4\\. 训练动态曲线\n\n*   论文给出 WikiText-2 与 TinyStories 的验证困惑度、损失、epoch 时间随训练步数的变化曲线，显示 GAM 全程保持速度与性能优势（图 3、图 4）。\n\n综上，实验从 **标准基准、效率对比、长度扩展、组件消融** 四个维度系统验证了 GAM 的有效性。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 GAM 的后续研究切入点，按优先级与可行性排序：\n\n1\\. 长序列基准与极限压力测试\n\n*   **Long Range Arena**（LRA）及 **PG-19**、**arXiv 摘要 16k+ token** 任务：验证 GAM 在 4k–32k 序列上的线性优势是否持续。\n*   **显存-时间联合优化**：量化不同序列长度下的 FLOPs-utilization 曲线，评估在 A100/H100 上的实际吞吐。\n\n2\\. 规模定律（Scaling Law）研究\n\n*   **参数-数据联合扩展**：在 125 M → 1 B → 7 B 参数区间，配合 C4 / SlimPajama 大规模语料，绘制 PPL-Compute 曲线，与同等规模 Transformer、Mamba 对比。\n*   **记忆槽位扩展规律**：固定模型宽度，线性增加 `num_slots`（512→2k→8k），观察对长距离依赖任务的边际收益与显存开销。\n\n3\\. 记忆可解释性与可控性\n\n*   **记忆槽位聚类与语义分析**：\n    *   对记忆向量 $M$ 做 k-means 或 SVD，可视化其捕获的语法/主题原型。\n    *   引入“槽位干预”实验：冻结或替换特定槽位，测量下游任务性能变化，验证记忆的可编辑性。\n*   **动态记忆扩展**：\n    *   在线新增/删除槽位（类似增量化词典），探索持续学习场景下的灾难性遗忘问题。\n\n4\\. 门控机制升级\n\n*   **多头门控**（Multi-Head Gating）：为每个注意力头或卷积通道学习独立门值，提升细粒度融合能力。\n*   **输入条件化门控**：用上一层隐藏状态或外部元数据（如文档类型）调制门值，实现任务自适应。\n\n5\\. 跨模态与特殊领域适配\n\n*   **代码建模**：在 HumanEval / MBPP 上评估 GAM 对长代码文件的补全能力。\n*   **基因组、时间序列**：利用因果卷积的局部敏感性与关联记忆的全局模式捕获，测试 DNA 长序列分类或金融高频数据预测。\n\n6\\. 训练效率再优化\n\n*   **内核融合**：将卷积、记忆检索与门控融合为单一 CUDA kernel，减少内存往返。\n*   **混合精度稳定性**：研究 bfloat16 下 softmax 数值误差对记忆检索的影响，设计专用 log-sum-exp 技巧。\n\n7\\. 理论复杂度与下界分析\n\n*   证明在特定假设（如低秩或稀疏依赖）下，GAM 的 $O(N)$ 实现可逼近任意自注意力函数，给出误差上界。\n\n**Q6**: 总结一下论文的主要内容\n\n论文核心概要\n\n**题目**：Gated Associative Memory: A Parallel O(N) Architecture for Efficient Sequence Modeling  \n**作者**：Rishiraj Acharya\n\n1\\. 问题\n\n*   Transformer 的自注意力带来 $O(N^2)$ 复杂度，长序列受限。\n*   现有线性方案（稀疏/低秩 Transformer、SSM/RNN）要么牺牲表达能力，要么引入递归扫描，训练并行度不足。\n\n2\\. 方案：Gated Associative Memory (GAM)\n\n*   **目标**：线性复杂度 + 完全并行 + 不牺牲精度。\n*   **核心模块 GAMBlock**（取代 Multi-Head Self-Attention）：\n    1.  **局部路径**：1D 因果卷积（kernel=3，深度可分离）→ $O(N)$ 捕获局部语法/位置信息。\n    2.  **全局路径**：并行关联记忆\n        *   可学习记忆矩阵 $M \\\\in \\\\mathbb{R}^{\\\\text{num\\_slots} \\\\times d}$\n        *   一次矩阵乘法 $X M^\\\\top$ 得到相似度，再 softmax 加权求和 → $O(N)$ 捕获全局语义模式。\n    3.  **动态门控**：用 sigmoid 门值逐 token 融合两条路径，实现自适应权衡。\n\n3\\. 实验\n\n*   **数据集**：WikiText-2、TinyStories（序列长度 256）。\n*   **对比基线**：6 层 Transformer（24.2 M）、Mamba（20.5 M）。\n*   **结果**：\n    *   **效率**：GAM 每 epoch 时间最短（WikiText-2 117.2 s，TinyStories 601.4 s），显存随长度线性增长，Transformer 在 4k 处 OOM。\n    *   **精度**：GAM 取得最低验证困惑度（WikiText-2 882.57；TinyStories 23.15）。\n*   **消融**：门控机制、全局记忆、局部卷积缺一不可；门控对性能提升最关键。\n\n4\\. 结论\n\nGAM 用“卷积 + 关联记忆 + 门控”的组合，在保持完全并行、无递归的前提下，实现线性复杂度，并在速度与精度上均优于 Transformer 与 Mamba，为长序列建模提供了新的高效架构。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Gated%20Associative%20Memory%3A%20A%20Parallel%20O%28N%29%20Architecture%20for%20Efficient%20Sequence%20Modeling，链接是  https://arxiv.org/pdf/2509.00605  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.00605  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-13-1756943220810","title":"DynaGuard: A Dynamic Guardrail Model With User-Defined Policies","description":"Guardian models are used to supervise and moderate the outputs of user-facing chatbots, enforcing guardrails and detecting bad behaviors. Standard guardian models like LlamaGuard detect predefined, static categories of harms. We propose dynamic guardian models that evaluate text based on user-defined policies, making them useful for different application domains that are not addressed by standard guardian models. Our dynamic guardian models can be used for fast detection of policy violations or with chain-of-thought reasoning that articulates and justifies the model outputs. Our dynamic guardian models match static models in detection accuracy for static harm categories while identifying violations of free-form policies with accuracy comparable to frontier reasoning models in a fraction of the time.","url":"https://huggingface.co/papers/2509.02563","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.810Z","metadata":{"authors":["Monte Hoover","Vatsal Baherwani","Neel Jain","Khalid Saifullah","Joseph Vincent","Chirag Jain","Melissa Kazemi Rad","C. Bayan Bruss","Ashwinee Panda","Tom Goldstein"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.LG","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.02563.pdf","arxivUrl":"https://arxiv.org/abs/2509.02563","arxivId":"2509.02563","coolPaperUrl":"https://papers.cool/arxiv/2509.02563","published":"2025-09-02T17:57:56Z","updated":"2025-09-02T17:57:56.000Z","zh_summary":"守卫模型用于监督和调节用户面对面的聊天机器人输出，执行保护措施并检测不当行为。标准守卫模型如 LlamaGuard 检测预定义的静态危害类别。我们提出了动态守卫模型，它们根据用户定义的政策评估文本，使其在标准守卫模型未能覆盖的不同应用领域中具有用处。我们的动态守卫模型可以快速检测政策违规，或者通过链式思维推理来阐述和证明模型输出。我们的动态守卫模型在静态危害类别的检测准确性上可与静态模型相媲美，同时以与前沿推理模型类似的准确性在极短的时间内识别自由形式政策的违规行为。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文针对的核心问题是：**现有 guardian 模型只能识别固定、静态的有害类别，无法适应真实部署场景中千变万化的业务规则与合规需求**。  \n具体而言，现有方案（如 LlamaGuard）将“暴力、武器、色情、自残、犯罪策划”等固定类别作为检测目标，而企业级应用往往要求模型遵守高度定制、领域特定的政策，例如：\n\n*   **“绝不允许以任何理由退款”**（避免 Air Canada 被判赔偿的案例重演）\n*   **“讨论人体解剖时不得屏蔽，但禁止色情内容”**（医疗场景）\n*   **“引用新闻中的暴力事件可以，但禁止教唆暴力”**（RAG 场景）\n\n这些需求超出了静态类别的范畴，导致传统 guardian 模型失效。  \n因此，论文提出 **DynaGuard：一个能够接受用户任意编写的政策（policy），在运行时动态检测违规并给出可解释反馈的轻量级 guardian 模型**，同时保持对经典静态有害类别的检测精度。\n\n**Q2**: 有哪些相关研究？\n\n与 DynaGuard 相关的研究可分为 **guardian 模型** 与 **合规/安全数据集** 两大方向。\n\n1\\. Guardian 模型\n\n模型\n\n关键特征\n\n与 DynaGuard 的关系\n\n**LlamaGuard** (Inan et al., 2023)\n\n基于固定 6 类安全标签（暴力、武器、色情…）训练；支持 zero-shot 适配新类别，但泛化有限。\n\nDynaGuard 仍保留对这类静态类别的检测能力，同时支持任意用户自定义规则。\n\n**LlamaGuard3-Vision** (Chi et al., 2024)\n\n将 LlamaGuard 扩展到多模态（图像+文本）。\n\nDynaGuard 目前专注文本，但训练方法可迁移到多模态。\n\n**WildGuard** (Han et al., 2024)\n\n提供 13 类细粒度安全标签，区分用户输入与模型输出风险；含对抗样本。\n\nDynaBench 训练集吸收了 WildGuard 的安全数据，但测试集规则完全不同，用于衡量泛化。\n\n**Aegis2.0** (Ghosh et al., 2024)\n\n21 类商业安全标签（版权、政府决策、未授权建议等）；单轮对话。\n\nDynaGuard 支持多轮、多规则组合，且规则不局限于 21 类。\n\n**GuardReasoner** (Liu et al., 2025)\n\n引入 Chain-of-Thought 推理，输出自然语言解释；推理开销大。\n\nDynaGuard 同样提供解释，但额外提供“快速推理”模式，显著降低延迟。\n\n**ShieldGemma** (Zeng et al., 2024)\n\nGoogle 基于 Gemma 的专有多模态内容审核模型；闭源。\n\nDynaGuard 8B 在静态安全基准上与之持平或更好，且开放权重。\n\n**RigorLLM** (Yuan et al., 2024)\n\n通过约束优化与神经符号方法提升鲁棒性。\n\nDynaGuard 的 GRPO 阶段也采用 RL 微调，但目标是对齐用户自定义政策。\n\n2\\. 合规/安全数据集\n\n数据集\n\n规模与特色\n\n与 DynaBench 的关系\n\n**BeaverTails** (Ji et al., 2023)\n\n300k 样本，14 类有害/无害细粒度标签；强调人类偏好对齐。\n\nDynaBench 将其转换为“政策+对话+违规标签”格式，用于训练/测试。\n\n**ToxicChat** (Lin et al., 2023)\n\n真实单轮对话，含对抗攻击与越狱标签。\n\n同样被转换为政策格式，用于提升 DynaGuard 对对抗场景的鲁棒性。\n\n**WildGuardMix** (Han et al., 2024)\n\n合成单轮对话，细粒度标签。\n\n作为安全数据的一部分参与 DynaGuard 训练。\n\n**DynaBench** (本文)\n\n40k 训练 + 手工 测试集；规则覆盖交易、监管、品牌一致性等 5 大新类别；多轮对话。\n\n首个面向“任意用户政策”的规模化数据集，用于训练与评估动态 guardian 模型。\n\n综上，DynaGuard 在 **任务定义**（动态政策 vs 静态类别）、**数据构建**（DynaBench）、**推理效率**（双模式推理）与 **开放权重** 四个维度，对现有 guardian 研究进行了系统扩展。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过 **“数据-模型-训练-推理” 四步闭环** 解决“静态 guardian 无法适应动态政策”的问题。\n\n1\\. 数据：构建 DynaBench——首个面向任意用户政策的 40 K 规模数据集\n\n*   **规则库**\n    *   手写 500 条种子规则 → 用 GPT-4o/Gemini/Claude 扩展到 5 000 条 → 人工去噪，确保客观可判定。\n*   **政策生成**\n    *   按主题随机采样规则（1–86 条/政策），再用 LLM 改写，避免重复。\n*   **对话生成**\n    *   为每条政策生成多轮对话（1–30 轮），包含：\n        *   正常遵守、用户诱导违规、用户诱导但成功防御、无意违规 4 种场景。\n    *   用户/Agent 画像由属性库（职业、地域、行业等）程序化组合，保证多样性。\n*   **测试集**\n    *   与行业专家手工设计 8 大业务影响类别 × 15 种失败模式，确保零规则与训练集重叠，难度极高（LlamaGuard3 仅 13.1 % 准确率）。\n\n2\\. 模型：DynaGuard——双模式轻量级 guardian\n\n*   **架构**\n    *   基于 Qwen3-1.7B/4B/8B 指令模型，保留全部权重开放。\n*   **双模式输出**\n    1.  **快速模式** `<answer>PASS/FAIL</answer>` + `<explanation>…</explanation>`（低延迟，≈1.3 % 精度损失）。\n    2.  **推理模式** `<think>…</think><answer>…</answer>`（完整 CoT，用于调试或高置信场景）。\n\n3\\. 训练：两阶段混合微调\n\n*   **阶段 1：监督微调（SFT）**\n    *   80 K 混合样本 = 40 K DynaBench + 40 K 经典安全数据（WildGuard/BeaverTails/ToxicChat/Aegis2.0）。\n    *   1/3 样本使用 CoT 推理链，2/3 仅标签+简短解释，以诱导双模式行为。\n*   **阶段 2：GRPO 强化学习**\n    *   11 K 样本继续优化，奖励 = 正确性 + KL 正则，提升对复杂多跳规则的鲁棒性。\n\n4\\. 推理与反馈：实时守护 + 可解释恢复\n\n*   **运行时流程**\n    1.  用户政策 + 当前对话 → DynaGuard。\n    2.  若 FAIL，输出自然语言解释（如“Rule 1 被违反：用户未提及 OpenAI，却出现 ‘accurate’ 一词”）。\n    3.  解释回传给主模型，触发自我修正（实验中将 Ministral-8B 的 IFEval 准确率从 57.3 % 提升到 63.8 %）。\n\n结果摘要\n\n*   **静态类别**：在 6 大安全基准上平均 F1 79.6 %，超越 GPT-4o-mini 与所有现有 guardian。\n*   **动态政策**：在零见过的 DynaBench 测试集上 F1 72.5 %（8B），而 LlamaGuard3 仅 13.1 %。\n*   **效率**：非 CoT 模式延迟接近传统小模型，满足生产部署需求。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **“动态政策合规检测”** 与 **“经典安全基准”** 两条主线，共设计并执行了 5 组实验，覆盖准确率、效率、可解释性与实际落地场景。\n\n1\\. 主实验：DynaBench 与 6 大安全基准的横向对比\n\n*   **数据集**\n    *   DynaBench 测试集（零规则与训练集重叠）\n    *   Aegis 2.0、BeaverTails、HarmBench、SafeRLHF、WildGuard、XSTest\n*   **对比模型**\n    *   开源 guardian：LlamaGuard3、WildGuard、NemoGuard、ShieldGemma\n    *   推理型 guardian：GuardReasoner-8B（CoT/非 CoT）\n    *   通用 API：GPT-4o-mini、Qwen3-8B（基座）\n*   **指标**\n    *   F1（macro）\n*   **结果（表 2）**\n    *   DynaGuard-8B 在 **“All Tasks Avg”** 78.4 % **SOTA**，其中 DynaBench 72.5 %，领先 GPT-4o-mini（70.1 %）与所有现有 guardian。\n    *   非 CoT 模式仅比 CoT 模式低 1.3 %，验证快速推理可行性。\n\n2\\. 消融实验：验证训练配方各组件的贡献\n\n2.1 推理链的必要性\n\n训练配方\n\nWildGuard + Compliance F1\n\nΔ\n\nBase Qwen3-4B\n\n33.9\n\n—\n\n仅标签 SFT（40 k）\n\n64.6\n\n+30.7\n\n标签 + 推理 SFT + GRPO\n\n71.7\n\n+37.8\n\n2.2 合成合规数据的价值\n\n数据混合（SFT+GRPO）\n\nWildGuard + Compliance F1\n\nΔ\n\n仅 40 k 安全数据\n\n56.5\n\n+22.6\n\n仅 40 k Compliance\n\n71.7\n\n+37.8\n\n20 k 安全 + 20 k Compliance\n\n72.0\n\n+38.1\n\n40 k 安全 + 40 k Compliance\n\n73.2\n\n+39.3\n\n3\\. 模型家族迁移性验证\n\n*   **方法**：将同一训练配方迁移到 Qwen3、Qwen2.5、Llama3.2 的 1B–8B 模型，记录平均 F1 提升。\n*   **结果（表 4）**\n    *   Qwen3 +28.4（安全） / +22.5（合规）\n    *   Llama3.2 +35.4 / +21.3\n    *   证明配方跨架构通用。\n\n4\\. 错误分析：定位模型弱点\n\n*   **高错误率类别**（表 7）\n    *   事实知识型规则 73.4 %\n    *   多子句规则 60.7 %\n    *   计数型规则 53.4 %\n*   **长上下文极限**（表 8）\n    *   DynaGuard-8B 在 13 轮对话 / 91 条规则 / 10 跳推理前仍保持 >50 % 准确率，显著优于基座模型。\n\n5\\. 实际场景案例研究\n\n5.1 IFEval 指令修正实验\n\n*   **设置**\n    *   将 IFEval 的每条指令视为“未见政策”，Ministral-8B 生成回答，DynaGuard 检测违规并给出解释，再让 Ministral 修正。\n*   **结果（表 3）**\n    *   Ministral-8B 原始 57.3 % → +DynaGuard 63.8 %（+6.5 pp），显著高于其他 guardian。\n    *   32/232 失败样本被成功修正，改善率 13.8 %；其中列表格式提升 18.8 %，JSON 格式 0 %。\n\n5.2 人工验证\n\n*   50 训练 + 25 测试样本双盲标注\n    *   测试集 100 % 一致性，96 % 与合成标签一致，确认高难度但高可靠性。\n\n实验结论速览\n\n*   **动态政策**：DynaGuard 在零见过的规则上仍保持 70 %+ F1，而 LlamaGuard3 仅 13.1 %。\n*   **经典安全**：平均 F1 79.6 %，全面领先现有开源与 API 模型。\n*   **效率**：非 CoT 模式延迟接近传统小模型，满足生产级部署。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可被视为 DynaGuard 框架的 **直接延伸** 或 **长期研究议程**，按优先级与可行性分层列出：\n\n1\\. 解释-驱动的交互式恢复机制\n\n*   **问题**：目前仅将 DynaGuard 的解释作为一次性提示回传给主模型，缺乏多轮协同策略。\n*   **可探索**\n    *   设计 **“解释-修正-再检测”** 的闭环协议，让主模型与 guardian 进行 **多轮对话式修复**（类似 self-critique）。\n    *   引入 **强化学习** 以优化“何时停止修正”策略，避免无限循环或过度保守。\n\n2\\. 多模态政策合规\n\n*   **问题**：DynaGuard 仅处理文本；真实场景中政策常涉及图像、PDF、表格等多模态内容。\n*   **可探索**\n    *   将 DynaGuard 训练配方迁移到 **视觉-语言基座**（如 Llama-3.2-Vision、Qwen-VL），构建 **Omniguard-style** 动态多模态 guardian。\n    *   扩展 DynaBench：加入带截图、图表、扫描合同的对话，形成 **DynaBench-V** 数据集。\n\n3\\. 规则复杂度与模型容量的 Scaling Law\n\n*   **问题**：当政策规则数 >100 或需跨 20+ 轮推理时，8B 模型准确率骤降（表 8）。\n*   **可探索**\n    *   系统研究 **规则数量、对话轮数、推理跳数** 与 **模型参数规模** 的关系，给出容量-性能 Pareto 曲线。\n    *   试验 **检索增强 guardian**：将超长政策分段存入向量库，让模型在推理时动态检索相关子规则，降低上下文压力。\n\n4\\. 不确定性估计与人工回退\n\n*   **问题**：DynaGuard 输出硬 PASS/FAIL，缺乏置信度；高 ambiguity 场景易误报。\n*   **可探索**\n    *   在 guardian 头部增加 **置信度头**（verbalized 或 softmax 温度），对低置信样本触发 **人工审核** 或 **更慢但更准的推理模式**。\n    *   建立 **“可接受风险-成本”** 权衡界面，让业务方通过旋钮调节自动通过率。\n\n5\\. 对抗鲁棒性与红队基准\n\n*   **问题**：DynaBench 已含部分对抗对话，但未系统评估 **prompt injection、规则混淆、社会工程** 等高级攻击。\n*   **可探索**\n    *   构建 **DynaRed** 子集：红队针对给定政策自动生成越狱提示，衡量 guardian 的 **最小可欺骗规则长度**。\n    *   引入 **对抗训练**（Adv-GRPO）：在红队生成的失败样本上继续 RL，提升鲁棒性。\n\n6\\. 在线政策学习与持续对齐\n\n*   **问题**：业务规则随时间变化，重新训练全量模型成本高。\n*   **可探索**\n    *   采用 **LoRA-adapter 插件机制**：为每条新政策训练轻量 adapter，推理时动态加载/卸载，实现 **“热插拔”式政策更新**。\n    *   研究 **人类反馈在线 RLHF**：收集真实对话中 guardian 的误报/漏报，实时微调 adapter。\n\n7\\. 人机协同政策编辑器\n\n*   **问题**：非技术用户撰写高质量规则仍具挑战，易产生歧义。\n*   **可探索**\n    *   开发 **“规则 Copilot”**：LLM 辅助用户将自然语言需求转化为 **形式化规则**，并即时模拟违规/合规对话进行验证。\n    *   引入 **规则复杂度评分**（可读性、歧义度、与现有规则冲突度），引导用户迭代优化。\n\n8\\. 跨语言与地域合规\n\n*   **问题**：DynaBench 以英文为主，未覆盖多语言、多法规环境。\n*   **可探索**\n    *   构建 **DynaBench-Multi**：将规则与对话机器翻译为 10+ 语言，研究 guardian 在 **GDPR、CCPA、PIPL** 等地域法规下的零样本表现。\n    *   试验 **跨语言规则对齐**：同一政策的多语言表述是否导致检测差异，并设计一致性正则项。\n\n9\\. 伦理与社会影响度量\n\n*   **问题**：动态政策可能被滥用（过度审查、歧视性规则）。\n*   **可探索**\n    *   建立 **“政策伦理审计”** 工具：自动检测用户规则是否包含 **歧视性关键词、过度宽泛限制**。\n    *   与政策制定者合作，开展 **用户研究** 评估解释对 **透明度感知、信任度、合规行为** 的影响。\n\n10\\. 边缘部署与 Tiny-DynaGuard\n\n*   **问题**：医疗、金融等场景需本地部署，8B 仍显笨重。\n*   **可探索**\n    *   知识蒸馏到 **1B 以下 Tiny 模型**，在量化（INT4/INT8）后保持 >65 % 的 DynaBench 性能。\n    *   利用 **投机解码**（speculative decoding）：Tiny 模型快速给出初步判断，大 guardian 仅在低置信时介入。\n\n以上方向既可直接基于已开源的 DynaGuard 权重与 DynaBench 数据继续迭代，也可作为独立课题深入。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n提出 **DynaGuard**——首个能在运行时接受**任意用户政策**、**轻量级**、**可解释**且**开放权重**的 guardian 模型，并配套发布高难度基准 **DynaBench**，在经典安全与动态合规任务上同时取得 SOTA。\n\n核心贡献\n\n维度\n\n内容\n\n结果\n\n**任务定义**\n\n从“静态有害类别”转向“用户自定义政策”\n\n覆盖交易、法规、品牌、内容控制等 5 类新场景\n\n**数据集**\n\nDynaBench：40 k 训练 + 手工测试集\n\n零规则重叠，LlamaGuard3 仅 13.1 % F1\n\n**模型**\n\n基于 Qwen3 的 1.7B/4B/8B 双模式 guardian\n\n非 CoT 延迟低，CoT 提供自然语言解释\n\n**训练**\n\nSFT（安全+合规 80 k）+ GRPO（11 k）\n\n跨模型家族平均 +30 % F1\n\n**实验**\n\n6 大安全基准 + DynaBench + IFEval 修正\n\n平均 F1 79.6 % SOTA；IFEval +6.5 pp\n\n**部署**\n\n开放权重、可本地推理、快速/解释双模式\n\n满足医疗/金融低延迟、数据不出域需求\n\n一句话记忆\n\nDynaGuard = **“把 LlamaGuard 的静态护栏变成用户可编程的动态护栏”**，并给出数据、模型、训练配方全套开源方案。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是DynaGuard%3A%20A%20Dynamic%20Guardrail%20Model%20With%20User-Defined%20Policies，链接是  https://arxiv.org/pdf/2509.02563  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02563  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1756943220810","title":"DCPO: Dynamic Clipping Policy Optimization","description":"Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a promising framework for enhancing the reasoning capabilities of large language models. However, existing approaches such as GRPO often suffer from zero gradients. This problem arises primarily due to fixed clipping bounds for token-level probability ratios and the standardization of identical rewards, which can lead to ineffective gradient updates and underutilization of generated responses. In this work, we propose Dynamic Clipping Policy Optimization (DCPO), which introduces a dynamic clipping strategy that adaptively adjusts the clipping bounds based on token-specific prior probabilities to enhance token-level exploration, and a smooth advantage standardization technique that standardizes rewards across cumulative training steps to improve the response-level effective utilization of generated responses. DCPO achieved state-of-the-art performance on four benchmarks based on four different models. In particular, DCPO achieved an Avg@1 of 46.7 under greedy decoding and an Avg@32 of 38.8 under 32 times sampling on the AIME24 benchmark, surpassing both DAPO (36.7/31.6) and GRPO (36.7/32.1) on the Qwen2.5-Math-7B model. On the AIME25 benchmark based on Qwen2.5-14B, DCPO achieves a performance of (23.3/19.0), surpassing GRPO (13.3/10.5) and DAPO (20.0/15.3). Furthermore, DCPO achieved an average 28% improvement in the nonzero advantage over GRPO in four models, doubled the training efficiency over DAPO, and significantly reduced the token clipping ratio by an order of magnitude compared to both GRPO and DAPO, while achieving superior performance. These results highlight DCPO's effectiveness in leveraging generated data more efficiently for reinforcement learning in large language models.","url":"https://huggingface.co/papers/2509.02333","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.810Z","metadata":{"authors":["Shihui Yang","Chengfeng Dou","Peidong Guo","Kai Lu","Qiang Ju","Fei Deng","Rihui Xin"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.02333.pdf","arxivUrl":"https://arxiv.org/abs/2509.02333","arxivId":"2509.02333","coolPaperUrl":"https://papers.cool/arxiv/2509.02333","published":"2025-09-02T14:01:07Z","updated":"2025-09-02T14:01:07.000Z","zh_summary":"基于可验证奖励的强化学习（RLVR）已经成为增强大型语言模型推理能力的一个有前景的框架。然而，现有的方法如GRPO往往面临零梯度的问题。这个问题主要是由于固定的剪切界限造成的，这些界限适用于基于token的概率比率，并且相同奖励的标准化可能导致梯度更新效果不佳以及生成响应的使用不足。在这项工作中，我们提出了动态剪切策略优化（DCPO），它引入了一种动态剪切策略，根据特定token的先验概率自适应地调整剪切界限，以增强token级探索，同时采用平滑的优势标准化技术，在累积训练步骤中标准化奖励，以改善生成响应在响应级的有效使用。DCPO在四个不同模型的四个基准测试中实现了最先进的性能。特别是在贪婪解码下，DCPO在AIME24基准上达到了46.7的Avg@1，在32次采样下得到了38.8的Avg@32，超越了Qwen2.5-Math-7B模型上的DAPO（36.7/31.6）和GRPO（36.7/32.1）。在基于Qwen2.5-14B的AIME25基准上，DCPO的表现为（23.3/19.0），超越了GRPO（13.3/10.5）和DAPO（20.0/15.3）。此外，在四个模型上，DCPO在非零优势方面比GRPO平均提高了28%，在训练效率上是DAPO的两倍，并且在与GRPO和DAPO的比较中，显著减少了token剪切比例的数量级，同时实现了更优的性能。这些结果突显了DCPO在有效利用生成数据进行大型语言模型强化学习方面的有效性。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文《DCPO: Dynamic Clipping Policy Optimization》针对**强化学习从可验证奖励（RLVR）框架下大型语言模型推理能力提升**的核心瓶颈，提出了两项关键问题并给出系统性解决方案：\n\n**1\\. 固定裁剪导致的“零梯度”与探索受限**\n\n*   **问题根源**：现有方法（如GRPO、DAPO）采用**固定或半固定概率比裁剪边界**（如|r(x)−1|≤ϵ），在旧策略赋予低概率的token区域强行限制更新幅度，导致：\n    \n    *   **低概率token的梯度被过度抑制**，模型难以探索稀有但可能关键的推理路径；\n    *   **高概率token的裁剪冗余**，浪费计算资源。\n*   **解决方案**：提出**动态自适应裁剪（DAC）**，根据token的旧策略概率q(x)动态调整裁剪边界：\n    \n    *   对低概率token放宽边界，公式化为： ${% raw %}$\\\\text{动态边界} \\\\propto \\\\frac{1}{\\\\sqrt{q(x)}}${% endraw %}$\n    *   保留高概率token的严格约束，避免不稳定更新。\n\n**2\\. 奖励标准化导致的“响应级效率低下”**\n\n*   **问题根源**：传统方法（如GRPO）对每个训练步的响应奖励进行**局部标准化**（仅当前步的G个响应），引发：\n    \n    *   **零优势响应**：当某步所有响应奖励相同时，标准化后优势为零，导致**响应被丢弃**（响应利用率仅44.6%）；\n    *   **训练不稳定**：高熵采样下奖励分布波动剧烈，标准化结果符号反转或幅度异常。\n*   **解决方案**：提出**平滑优势标准化（SAS）**：\n    \n    *   **累积全局奖励分布**：将当前步响应与历史所有同提示响应的奖励合并计算均值μ\\_total和方差σ\\_total；\n    *   **平滑融合**：通过加权平均（权重随训练步数动态调整）平衡局部与全局标准化结果，公式化为： ${% raw %}$\\\\hat{A}_{\\\\text{smooth}} = \\\\frac{i-1}{i}\\\\hat{A}_{\\\\text{local}} + \\\\frac{1}{i}\\\\hat{A}\\_{\\\\text{global}}${% endraw %}$\n    *   **响应利用率提升**：即使当前步奖励相同，历史差异仍提供非零梯度（利用率提升至71.8%）。\n\n**3\\. 损失函数设计的“相对优势稀释”**\n\n*   **问题根源**：现有损失函数（如GRPO的SLM或DAPO的TLM）存在：\n    \n    *   **SLM**：跨响应平均导致长序列主导梯度；\n    *   **TLM**：跨token平均破坏响应间相对优势关系。\n*   **解决方案**：提出**仅token级平均损失（OTM）**：\n    \n    *   仅对单个响应内的token取平均，保留响应间相对优势，公式化为： ${% raw %}$\\\\mathcal{L}_{\\\\text{OTM}} = \\\\sum_{i=1}^{G} \\\\frac{1}{|o\\_i|} \\\\sum\\_{t=1}^{|o\\_i|} \\\\min\\\\left(r\\_{i,t}\\\\hat{A}_{i,t}, \\\\text{clip}(r_{i,t}, \\\\epsilon\\_{\\\\text{low}}, \\\\epsilon\\_{\\\\text{high}})\\\\hat{A}\\_{i,t}\\\\right)${% endraw %}$\n\n**总结：DCPO的核心贡献**\n\n通过**动态裁剪**、**平滑标准化**和**响应级损失设计**，DCPO在以下方面实现突破：\n\n*   **性能**：AIME24基准上7B模型Avg@32从32.1（GRPO）提升至38.8；\n*   **效率**：训练时间较DAPO减半，响应利用率提升28%；\n*   **稳定性**：token裁剪比例降低一个数量级，熵崩溃风险显著缓解。\n\n**Q2**: 有哪些相关研究？\n\n以下研究均与 DCPO 的核心动机（提升大模型在 RLVR 场景下的推理能力、缓解零梯度与低样本效率问题）密切相关，可归纳为 **四类**：基础 RLVR 算法、裁剪策略改进、优势估计方法、以及高效训练框架。\n\n1\\. 基础 RLVR 算法\n\n方法\n\n关键思想\n\n与 DCPO 的关系\n\n**GRPO** (Group Relative Policy Optimization)  \nDeepSeek-AI et al., 2024\n\n基于规则奖励，对同一 prompt 的 G 条响应做组内标准化，使用固定对称裁剪 ϵ。\n\nDCPO 将其作为基线，指出其存在 **零梯度** 与 **熵崩溃** 问题。\n\n**DAPO** (Dynamic sAmpling Policy Optimization)  \nYu et al., 2025\n\n引入 Clip-Higher 与动态采样过滤同 reward 响应，缓解熵崩溃但牺牲样本效率。\n\nDCPO 通过 SAS 与 DAC 解决同样问题，**无需丢弃响应**，训练效率提升 2×。\n\n**VAPO** (Variance-Aware Policy Optimization)  \nYuan et al., 2025\n\n在奖励方差过高时降低 KL 惩罚，减少梯度噪声。\n\n与 DCPO 的平滑标准化思路互补，但 DCPO 直接修正优势估计本身。\n\n2\\. 裁剪策略改进\n\n方法\n\n关键思想\n\n与 DCPO 的关系\n\n**PPO** (Proximal Policy Optimization)  \nSchulman et al., 2017\n\n固定对称裁剪\n\nr−1\n\n**Dual-clip PPO**  \nYe et al., 2020\n\n额外限制 r≤r\\_max=10，防止极端重要性权重。\n\nDCPO 继承 r\\_max=10 的“双裁剪”思想，但将其嵌入动态边界公式。\n\n**TR-DPO** (Trust-Region Direct Preference Optimization)  \nRafailov et al., 2023\n\n在 DPO 中引入 KL 约束，避免策略偏离参考模型过远。\n\nDCPO 不依赖 KL 项，通过裁剪边界本身控制策略偏移。\n\n3\\. 优势估计与标准化方法\n\n方法\n\n关键思想\n\n与 DCPO 的关系\n\n**GAE** (Generalized Advantage Estimation)  \nSchulman et al., 2016\n\n利用 λ-加权多步 TD 误差平滑优势。\n\nDCPO 的 SAS 可视为 **跨训练步而非跨时间步** 的平滑，解决奖励随机性。\n\n**RLOO** (REINFORCE Leave-One-Out)  \nAhmad et al., 2023\n\n通过留一法减少基线方差。\n\nDCPO 的累积标准化同样降低方差，但针对 **同 prompt 响应集合**。\n\n**Wang et al., 2025**  \n“High-entropy minority tokens drive effective RL”\n\n论证低概率高熵 token 是推理能力涌现的关键。\n\n为 DCPO 的动态裁剪提供 **理论依据**：放宽低概率 token 的边界可提升探索。\n\n4\\. 高效训练框架与系统优化\n\n方法\n\n关键思想\n\n与 DCPO 的关系\n\n**HybridFlow**  \nSheng et al., 2024\n\n灵活高效的 RLHF 框架，支持微批次流水线。\n\nDCPO 实验基于其代码库 Verl，但替换了损失计算逻辑。\n\n**DeepSeek-R1**  \nGuo et al., 2025\n\n大规模 RLVR 训练，使用 GRPO 作为默认算法。\n\nDCPO 在相同场景下显著优于 GRPO，可直接作为其替代。\n\n小结：DCPO 的“继承-创新”关系\n\n*   **继承**：沿用 GRPO 的组内奖励评估、PPO 的裁剪思想、Dual-clip 的 r\\_max 限制。\n*   **创新**：\n    *   **动态裁剪** → 解决固定边界对低概率 token 的探索抑制；\n    *   **平滑标准化** → 解决局部标准化导致的零梯度与训练震荡；\n    *   **OTM 损失** → 解决响应间相对优势被稀释的问题。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过 **Dynamic Clipping Policy Optimization（DCPO）** 框架，从 **token 级裁剪、优势标准化、损失函数** 三个维度协同解决 RLVR 场景下的“零梯度”与“低样本效率”问题。具体实现路径如下：\n\n1\\. 动态自适应裁剪（Dynamic-Adaptive Clipping，DAC）\n\n**目的**：消除固定裁剪对低概率 token 的探索抑制，同时防止高概率区域更新失控。\n\n*   **概率依赖边界**  \n    将传统固定边界 `|r−1|≤ϵ` 替换为： ${% raw %}$ 0.5+\\\\tfrac12\\\\sqrt{\\\\max!\\\\bigl(1-\\\\tfrac{4\\\\epsilon\\_{\\\\text{low}}}{q(x)},0\\\\bigr)} \\\\le r(x)\\\\le 0.5+\\\\tfrac12\\\\sqrt{1+\\\\tfrac{4\\\\epsilon\\_{\\\\text{high}}}{q(x)}} ${% endraw %}$\n    \n    *   `q(x)` 越小，允许 `r(x)` 的相对变化越大，**低概率 token 获得更宽更新区间**；\n    *   高概率 token 仍受严格约束，避免梯度爆炸。\n*   **双裁剪安全上限**  \n    借鉴 Ye et al. (2020)，对正负优势均设 `r_max = 10`，防止极端重要性权重。\n    \n\n2\\. 平滑优势标准化（Smooth Advantage Standardization，SAS）\n\n**目的**：解决局部标准化导致的零优势与训练震荡，提升响应利用率。\n\n*   **累积全局统计量**  \n    维护同 prompt 的所有历史响应奖励，计算全局均值 `μ_total` 与方差 `σ_total`。\n    \n*   **加权平滑融合**  \n    每一步的优势估计为局部与全局统计量的加权平均： ${% raw %}$ \\\\hat{A}_{\\\\text{smooth}}=\\\\min!\\\\bigl(|\\\\hat{S}_{\\\\text{new}}|,|\\\\hat{S}_{\\\\text{total}}|\\\\bigr)\\\\cdot\\\\text{sign}(\\\\cdot) ${% endraw %}$ 其中  \n    ${% raw %}$ \\\\hat{S}_{\\\\text{new}}=\\\\tfrac{i-1}{i}\\\\hat{A}_{\\\\text{local}}+\\\\tfrac1i\\\\hat{A}_{\\\\text{global}},\\\\quad \\\\hat{S}_{\\\\text{total}}=\\\\tfrac1i\\\\hat{A}_{\\\\text{local}}+\\\\tfrac{i-1}{i}\\\\hat{A}\\_{\\\\text{global}} ${% endraw %}$\n    \n    *   训练初期以全局分布为主，避免零梯度；\n    *   后期逐步偏向局部分布，保持对最新策略的适应性。\n\n3\\. 仅 Token 级平均损失（Only-Token-Mean Loss，OTM）\n\n**目的**：保留响应间相对优势，避免长序列主导梯度或跨响应稀释。\n\n*   **响应内平均**  \n    对每个响应单独计算 token 平均损失： ${% raw %}$ \\\\mathcal{L}_{\\\\text{OTM}}=\\\\sum_{i=1}^{G}\\\\frac{1}{|o\\_i|}\\\\sum\\_{t=1}^{|o\\_i|}\\\\min!\\\\bigl(r\\_{i,t}\\\\hat{A}_{i,t},\\\\ \\\\text{clip}(r_{i,t},\\\\epsilon\\_{\\\\text{low}},\\\\epsilon\\_{\\\\text{high}})\\\\hat{A}\\_{i,t}\\\\bigr) ${% endraw %}$\n    *   不跨响应求和，**短响应与高优势响应不再被长响应淹没**；\n    *   无需 KL 惩罚项，裁剪边界本身控制策略偏移。\n\n4\\. 训练流程整合\n\n*   **数据利用**：不丢弃任何响应（对比 DAPO 的动态采样），所有生成样本均参与更新。\n*   **系统实现**：基于 Verl 框架，400 步训练，512 响应批次 + 32 微批次，G=16，温度=1.0。\n\n效果验证\n\n指标\n\nGRPO\n\nDAPO\n\nDCPO（提升）\n\nAIME24-Avg@32 (7B)\n\n32.1\n\n31.6\n\n**38.8** (+6.7)\n\n响应利用率\n\n44.6%\n\n—\n\n**71.8%** (+28% abs)\n\nToken 裁剪比例\n\n10⁻²\n\n10⁻²\n\n**10⁻³** (×0.1)\n\n训练耗时\n\n1×\n\n2×\n\n**1×** (vs DAPO 减半)\n\n通过 **DAC + SAS + OTM** 的协同设计，DCPO 在 **不牺牲稳定性** 的前提下，显著提升了低概率 token 的探索空间与样本利用效率，最终在高难度数学推理任务上取得 SOTA。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **“数学推理 + RLVR”** 场景，设计了一套完整、可复现的实验体系，从 **模型规模、任务难度、解码方式、训练效率、消融分析** 五个维度验证 DCPO 的有效性。具体实验如下：\n\n1\\. 主实验：四模型 × 四基准\n\n**目的**：验证 DCPO 在不同规模模型与不同难度任务上的通用性与领先性。\n\n维度\n\n设置\n\n**模型**\n\nQwen2.5-Math-1.5B-Instruct  \nQwen2.5-3B（通用基座）  \nQwen2.5-Math-7B（数学基座）  \nQwen2.5-14B（通用基座）\n\n**训练集**\n\n25 k 题：DAPO-Math-17K ∪ MATH L3-5\n\n**评估基准**\n\nMATH500、AMC23、AIME24、AIME25\n\n**指标**\n\n• Avg@1：贪心解码单次准确率  \n• Avg@32：temperature=1.0 采样 32 次平均准确率\n\n**基线**\n\nGRPO、DAPO（均使用官方或原论文超参）\n\n**关键结果**（节选）\n\n*   **AIME24-Avg@32（7B）**：DCPO 38.8 vs GRPO 32.1 vs DAPO 31.6\n*   **AIME25-Avg@32（14B）**：DCPO 19.0 vs GRPO 10.5 vs DAPO 15.3\n*   **四模型平均提升**：DCPO 在 8 项指标中 7 项第一，平均领先 GRPO +5.4%，DAPO +2.1%\n\n2\\. Token 级诊断实验\n\n2.1 Token Clipping Ratio（TCR）\n\n*   **定义**：被裁剪 token 数 / 总 token 数\n*   **观测**：\n    *   GRPO/DAPO 的 TCR 随训练波动大，且量级 10⁻²；\n    *   DCPO 稳定维持 10⁻³，**降低一个数量级**。\n\n2.2 Response Utilization Ratio（RUR）\n\n*   **定义**：非零优势响应数 / 总响应数\n*   **观测**：\n    *   GRPO 从 >90% 骤降到 <50%，最终平均 43.8%；\n    *   DCPO 稳定保持 ~71.8%，**绝对提升 28%**。\n\n3\\. 训练效率对比\n\n*   **生成样本效率**：完成同样 400 步参数更新，DAPO 需生成 3-5× 响应；DCPO 与 GRPO 持平。\n*   **GPU 时间**：在 32×H20 集群上，DCPO 训练时间仅为 DAPO 的 **50%**。\n\n4\\. 消融实验（Qwen2.5-Math-7B，Avg@32）\n\n变体\n\n说明\n\nAvg@32 相对增益\n\n**GRPO**\n\n原始基线\n\n0\n\n\\+ OTM\n\n仅替换损失为 OTM\n\n+2.4\n\n\\+ SAS\n\n仅替换优势标准化为 SAS\n\n+3.1\n\n\\+ DAC\n\n仅替换裁剪为动态自适应\n\n+4.9\n\n**DCPO**\n\nOTM+SAS+DAC 全量\n\n**+7.2**\n\n结论：三项组件 **正交且互补**，组合后获得 **累积增益**。\n\n5\\. 趋势曲线分析\n\n*   **Avg@1 / Avg@32 训练曲线**：DCPO 全程单调上升，无 GRPO 的“性能坍塌”或 DAPO 的大幅震荡。\n*   **熵曲线**：DCPO 将策略熵稳定在中等区间，避免 GRPO 的熵崩溃与 DAPO 的高熵震荡。\n\n6\\. 复现与可扩展性\n\n*   **代码与超参**：附录 A.9 给出统一训练脚本；GitHub 已开源。\n*   **模板一致性**：所有实验使用同一 Qwen-Math 模板，确保结果可比。\n\n一句话总结\n\n论文通过 **大规模主实验 + 细粒度诊断 + 消融 + 效率对比** 的立体化实验，系统证明了 DCPO 在 **性能、稳定性、样本效率** 三方面对现有 RLVR 方法的全面超越。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 DCPO 的后续研究切入点，按 **理论深化、算法扩展、系统优化、跨领域迁移** 四个层面展开：\n\n1\\. 理论深化\n\n*   **动态裁剪的极限分析**  \n    在 `q(x)→0` 时，DAC 上界按 `1/√q(x)` 发散，虽已设 `r_max=10` 硬截断，但  \n    – 该截断对梯度方差-偏差权衡的 **精确影响** 尚缺理论刻画；  \n    – 可尝试 **变上限** 或 **概率自适应上限** 进一步放宽约束。\n    \n*   **平滑系数自适应**  \n    SAS 的权重 `1/i` 为线性衰减，可研究：  \n    – **指数衰减** `α^i` 或 **基于 KL 距离** 的动态权重，以更快适应策略漂移；  \n    – 引入 **bandit 算法** 在线调参，减少人工设定。\n    \n\n2\\. 算法扩展\n\n*   **多模态输入**  \n    将 DCPO 从纯文本数学推理扩展到 **图表、几何图形** 等多模态任务，验证 DAC 在跨模态 token（如图像 patch）上的有效性。\n    \n*   **链式推理长度自适应**  \n    – 对极长推理链（>8 k tokens）设计 **分段裁剪**：每段独立应用 DAC，避免单 token 误差累积；  \n    – 结合 **长度惩罚** 或 **动态终止** 机制，减少无效生成。\n    \n*   **离线-在线混合训练**  \n    当前为纯在线 RLVR，可探索：  \n    – **离线预训练 + 在线微调**：先用大规模离线数学语料预训练，再接入 DCPO 微调；  \n    – **重要性采样修正** 处理离线数据分布偏移。\n    \n\n3\\. 系统优化\n\n*   **微批次级并行**  \n    – 将 OTM 损失计算 **下放到 token 级 CUDA kernel**，减少 CPU-GPU 通信；  \n    – 研究 **梯度检查点** 与 **动态微批次大小** 以适配超长序列。\n    \n*   **异构硬件适配**  \n    – 在 **L20/A100/H200** 混合集群上测试 DCPO 的 **流水线并行效率**；  \n    – 探索 **FP8 低精度训练** 对动态裁剪数值稳定性的影响。\n    \n\n4\\. 跨领域迁移\n\n*   **代码生成**  \n    将训练集替换为 **CodeContests + APPS**，验证：  \n    – DAC 对 **稀有库函数 token** 的探索效果；  \n    – SAS 在 **单元测试奖励稀疏** 场景下的稳定性。\n    \n*   **科学问答 / 定理证明**  \n    – 在 **MMLU-STEM** 或 **Isabelle/HOL Light** 证明数据集上测试 DCPO；  \n    – 研究 **形式化语言 token**（如 ∀, ∃）的裁剪行为差异。\n    \n*   **对话与安全对齐**  \n    – 将奖励模型从 **规则型** 替换为 **人类偏好模型**，观察 DCPO 在 **PPO-RLHF** 框架下的表现；  \n    – 分析 **毒性/幻觉 token** 的低概率特性，评估 DAC 是否放大风险。\n    \n\n5\\. 风险与伦理\n\n*   **低概率稀有 token 的副作用**  \n    – 监控 **训练后期** 是否因过度探索低概率 token 导致 **有害或幻觉输出**；  \n    – 引入 **安全过滤器** 或 **KL 回退机制** 作为保障。\n\n可立即启动的实验清单\n\n方向\n\n7 天实验\n\n4 周实验\n\n**理论**\n\n不同 `r_max` 对梯度方差影响\n\n自适应权重 SAS 的收敛性证明\n\n**算法**\n\n代码生成小规模复现\n\n多模态数学几何题训练\n\n**系统**\n\nFP8 训练数值误差分析\n\n微批次级 CUDA kernel 重写\n\n**跨域**\n\nMMLU-STEM 零样本评估\n\n人类偏好 RLHF 全流程\n\n通过上述探索，DCPO 有望从 **数学推理专用优化器** 演进为 **通用 RLVR 引擎**，同时兼顾性能、效率与安全。\n\n**Q6**: 总结一下论文的主要内容\n\nDCPO 论文一句话总结\n\n用 **“动态裁剪 + 平滑标准化 + 响应级损失”** 三板斧，解决 RLVR 训练中 **低概率 token 探索受限** 与 **零梯度/低样本效率** 两大顽疾，在四个数学基准、四个模型上全面超越 GRPO 与 DAPO。\n\n1\\. 研究背景\n\n*   任务：用可验证奖励（RLVR）提升大模型数学推理。\n*   痛点：  \n    – GRPO 固定裁剪 → 低概率 token 更新被“卡死”；  \n    – 步级标准化 → 同奖励响应优势归零，浪费样本。\n\n2\\. 方法总览（DCPO）\n\n模块\n\n关键公式\n\n作用\n\n**动态自适应裁剪 DAC**\n\n${% raw %}$r\\_{\\\\text{low}}(q), r\\_{\\\\text{high}}(q)\\\\propto 1/\\\\sqrt{q}${% endraw %}$\n\n低概率 token 放宽边界，高概率 token 严格约束\n\n**平滑优势标准化 SAS**\n\n{% raw %}$$\\\\hat{A}\\_{\\\\text{smooth}}=\\\\min(\n\n\\\\hat{S}\\_{\\\\text{new}}\n\n**仅 Token 级平均损失 OTM**\n\n$${% endraw %}\\\\mathcal{L}=\\\\sum\\_i\\\\frac{1}{\n\no\\_i\n\n3\\. 实验结果\n\n指标\n\nDCPO vs GRPO vs DAPO\n\n**AIME24-Avg@32 (7B)**\n\n38.8 vs 32.1 vs 31.6\n\n**响应利用率**\n\n71.8 % vs 43.8 % vs —\n\n**Token 裁剪比例**\n\n10⁻³ vs 10⁻² vs 10⁻²\n\n**训练耗时**\n\n1× vs 1× vs 2×\n\n4\\. 结论\n\nDCPO 通过 **概率自适应裁剪** 释放低概率 token 探索空间，**累积平滑标准化** 消除零梯度，**响应级损失** 保证相对优势，实现 **更高性能、更高样本效率、更稳训练**。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是DCPO%3A%20Dynamic%20Clipping%20Policy%20Optimization，链接是  https://arxiv.org/pdf/2509.02333  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02333  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1756943220811","title":"GenCompositor: Generative Video Compositing with Diffusion Transformer","description":"Video compositing combines live-action footage to create video production, serving as a crucial technique in video creation and film production. Traditional pipelines require intensive labor efforts and expert collaboration, resulting in lengthy production cycles and high manpower costs. To address this issue, we automate this process with generative models, called generative video compositing. This new task strives to adaptively inject identity and motion information of foreground video to the target video in an interactive manner, allowing users to customize the size, motion trajectory, and other attributes of the dynamic elements added in final video. Specifically, we designed a novel Diffusion Transformer (DiT) pipeline based on its intrinsic properties. To maintain consistency of the target video before and after editing, we revised a light-weight DiT-based background preservation branch with masked token injection. As to inherit dynamic elements from other sources, a DiT fusion block is proposed using full self-attention, along with a simple yet effective foreground augmentation for training. Besides, for fusing background and foreground videos with different layouts based on user control, we developed a novel position embedding, named Extended Rotary Position Embedding (ERoPE). Finally, we curated a dataset comprising 61K sets of videos for our new task, called VideoComp. This data includes complete dynamic elements and high-quality target videos. Experiments demonstrate that our method effectively realizes generative video compositing, outperforming existing possible solutions in fidelity and consistency.","url":"https://huggingface.co/papers/2509.02460","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.811Z","metadata":{"authors":["Shuzhou Yang","Xiaoyu Li","Xiaodong Cun","Guangzhi Wang","Lingen Li","Ying Shan","Jian Zhang"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.02460.pdf","arxivUrl":"https://arxiv.org/abs/2509.02460","arxivId":"2509.02460","coolPaperUrl":"https://papers.cool/arxiv/2509.02460","published":"2025-09-02T16:10:13Z","updated":"2025-09-02T16:10:13.000Z","zh_summary":"视频合成将实时动作视频结合在一起，以创造视频制作，在视频创作和电影制作中扮演着重要角色。传统的工作流程需要大量人力和专家协作，导致生产周期漫长和人力成本高昂。为了解决这个问题，我们使用生成模型自动化此过程，称为生成视频合成。这个新任务旨在以互动的方式自适应地将前景视频的身份和运动信息注入到目标视频中，使用户能够自定义最终视频中添加的动态元素的大小、运动轨迹和其他属性。具体而言，我们设计了一种基于其内在特性的创新扩散变换器（DiT）流程。为了在编辑前后保持目标视频的一致性，我们修订了一个轻量化的基于DiT的背景保留分支，并进行了掩码令牌注入。为了从其他来源继承动态元素，提出了一个使用全自注意力的DiT融合块，并为训练提供了一种简单而有效的前景增强。此外，为了基于用户控制将背景和前景视频融合到具有不同布局中，我们开发了一种新型位置嵌入，称为扩展旋转位置嵌入（ERoPE）。最后，我们策划了一个包含61K组视频的数据集，用于我们的新任务，称为VideoComp。该数据集包括完整的动态元素和高质量的目标视频。实验表明，我们的方法有效实现了生成视频合成，超越了现有可能的解决方案，在保真度和一致性方面表现更佳。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n该论文提出并解决了一个全新的任务——**生成式视频合成（Generative Video Compositing）**，其核心目标可概括为：\n\n*   **自动化传统视频合成流程**：传统视频合成依赖大量人工与专家协作，周期长、成本高。论文通过扩散 Transformer 首次将这一流程自动化，实现**用生成模型把任意前景视频动态元素无缝注入到背景视频中**。\n    \n*   **同时满足三大挑战**：\n    \n    1.  **背景一致性**：编辑后的视频必须与原始背景在内容与布局上保持像素级一致。\n    2.  **前景身份与运动保真**：注入的前景元素必须完整保留其外观细节与动态特征，并与背景自然交互（如光影、遮挡）。\n    3.  **灵活的用户控制**：允许用户通过轨迹、缩放因子等显式控制前景元素的位置、大小与运动，而无需精确分割或逐帧调整。\n*   **现有方法无法胜任**：\n    \n    *   可控视频生成只能接受文本/图像条件，无法精确到像素级控制，也不支持外部视频条件。\n    *   视频协调方法仅做颜色匹配，需要精确掩码且不能调整轨迹或尺寸。\n\n因此，论文的任务定义可形式化为：  \n给定背景视频 $v\\_b$、前景视频 $v\\_f$ 和用户控制 $c$（轨迹、缩放），生成输出 $z\\_0$，使得 $z\\_0$ 在保留 $v\\_b$ 背景的同时，将 $v\\_f$ 的元素按 $c$ 注入并自然融合。\n\n**Q2**: 有哪些相关研究？\n\n论文将相关研究划分为两大类别，并指出它们与所提任务“生成式视频合成（Generative Video Compositing）”的关键差异。\n\n1\\. 基于扩散模型的视频编辑（Diffusion-based Video Editing）\n\n*   **共同点**：均利用扩散模型对视频进行编辑或再生成。\n*   **差异**：现有方法主要依赖文本或单张图像作为条件，无法直接接受外部视频片段作为控制信号，因而在像素级外观与运动细节上控制不足。\n*   **代表性工作**：\n    *   Tune-A-Video \\[48\\]：一次性微调 T2I 模型实现文本驱动的视频编辑。\n    *   VideoPainter \\[2\\]、FateZero \\[32\\]：通过掩码+文本提示完成视频修复/编辑。\n    *   Revideo \\[30\\]、Tora \\[59\\]：支持轨迹控制，但仍以图像或文本为初始条件，而非直接使用前景视频。\n    *   其他：InstructPix2Pix \\[5\\]、DynVFX \\[54\\]、MagicVFX \\[13\\] 等。\n\n2\\. 视频协调（Video Harmonization）\n\n*   **共同点**：关注如何把前景元素与背景在颜色、光照上保持一致。\n*   **差异**：需要精确、像素对齐的前景掩码；仅做颜色/亮度调整，不允许用户改变前景的位置、大小或运动轨迹；无法处理掩码不准确带来的边缘伪影。\n*   **代表性工作**：\n    *   Harmonizer \\[22\\]：白盒式回归图像级滤波参数进行协调。\n    *   VideoTripletTransformer \\[14\\]：用 Triplet Transformer 同时处理多个低层视频任务，包括协调。\n    *   其他：Deep Video Harmonization \\[9, 29\\]、TSA² \\[49\\] 等。\n\n小结\n\n类别\n\n条件形式\n\n是否支持外部视频\n\n是否支持用户控制轨迹/尺寸\n\n是否解决掩码不准确\n\n是否保持前景 ID & 运动\n\n扩散视频编辑\n\n文本 / 图像\n\n×\n\n部分支持\n\n×\n\n弱\n\n视频协调\n\n合成后视频 + 精确掩码\n\n×\n\n×\n\n×\n\n仅颜色\n\n**GenCompositor（本文）**\n\n**前景视频 + 背景视频 + 用户控制**\n\n**√**\n\n**√**\n\n**√**\n\n**√**\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过一套专为“生成式视频合成”设计的 Diffusion Transformer（DiT）流水线 **GenCompositor**，从数据、模型架构、训练策略三个层面系统性地解决了前述三大挑战。核心思路可概括为：\n\n1\\. 数据层面：构建 VideoComp 数据集\n\n*   **61 K 组高质量三元组**（背景视频、前景视频、对应掩码视频）。\n*   **自动标注与清洗流程**\n    1.  用 CogVLM + QWen 自动识别并描述显著动态元素；\n    2.  用 Grounded SAM2 分割出前景与掩码；\n    3.  人工过滤低质量/碎片化样本。\n*   **前景中心化**  \n    在保存前景视频时将物体居中，去除原始全局轨迹，仅保留外观与动作，方便后续用户自定义轨迹。\n\n2\\. 模型架构：三组件协同\n\n2.1 Background Preservation Branch（BPBranch）\n\n*   **目标**：像素级保留背景内容。\n*   **做法**\n    *   输入：掩码视频 + masked 视频（32 ch）。\n    *   仅用 **2 个轻量 DiT block** 提取背景特征。\n    *   **Masked token injection**：  \n        ${% raw %}$ \\\\mathbf z\\_t = \\\\mathbf z\\_t + (1-\\\\mathbf M)\\\\odot \\\\mathbf z\\_{\\\\text{BPBranch}} ${% endraw %}$  \n        仅将背景区域注入主支，避免干扰前景生成。\n\n2.2 Foreground Generation Mainstream（42 个 DiT Fusion Blocks）\n\n*   **目标**：把前景视频的外观与动作注入到指定位置，并与背景自然交互。\n*   **关键设计：DiT Fusion Block**\n    *   **Token-wise 拼接**：将待去噪 token 与前景 token 在序列维度拼接，而非通道维度，避免像素未对齐导致的混叠。\n    *   **纯 Self-Attention 融合**：利用全自注意力同时处理前景条件与背景上下文，比 cross-attention 更能保留低层细节。\n    *   **ERoPE**：解决前景与背景布局不一致时的位置编码冲突（见下）。\n\n2.3 Extended Rotary Position Embedding（ERoPE）\n\n*   **问题**：原始 3D RoPE 把前景与背景的 token 按相同坐标编码，导致内容串扰。\n*   **解决方案**\n    *   在 **高度维度** 上将前景 token 的坐标整体偏移 $h$，使两组 token 的坐标空间完全错位：  \n        $$ \\\\text{ERoPE}(t, y, x) = \\\\text{RoPE}(t, y+\\\\Delta, x) $$\n    *   无需额外参数，即可在自注意力阶段阻断未对齐内容的干扰。\n\n3\\. 训练策略：提升泛化与鲁棒性\n\n*   **Luminance Augmentation**  \n    每轮随机对前景视频做 $\\\\gamma \\\\in \\[0.4, 1.9\\]$ 的 Gamma 校正，迫使模型自适应调整光照。\n*   **Mask Inflation**  \n    用高斯滤波膨胀掩码，人为扩大遮罩区域，让网络学会在边界处生成自然过渡，容忍不精确掩码。\n*   **输入转换**  \n    训练阶段自动将用户轨迹、缩放因子转换为 mask video 与 masked video，无需成对“干净背景”数据，降低数据准备成本。\n\n4\\. 推理流程\n\n1.  用户给定背景视频、前景视频、轨迹/缩放。\n2.  系统自动生成 mask video（含膨胀）与 masked video。\n3.  送入 GenCompositor → 输出合成视频，其中\n    *   背景内容保持不变；\n    *   前景元素严格沿轨迹运动，尺寸符合缩放因子；\n    *   光影、遮挡等交互由模型自动预测。\n\n总结\n\n挑战\n\n解决方案\n\n所在模块\n\n背景一致性\n\nBPBranch + Masked token injection\n\n2.1\n\n前景 ID/运动保真\n\nDiT Fusion Block + Token-wise Self-Attention\n\n2.2\n\n用户可控轨迹/尺寸\n\nERoPE + 输入转换\n\n2.3 & 3\n\n掩码不精确\n\nMask Inflation + Luminance Augmentation\n\n3\n\n通过上述设计，GenCompositor 首次实现了**无需人工逐帧调整、直接以视频为条件的生成式视频合成**。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕“生成式视频合成”这一全新任务，从**定量指标、视觉对比、用户主观评价、消融分析、泛化验证**五个维度展开实验，系统验证 GenCompositor 的有效性与通用性。具体实验设计如下：\n\n1\\. 与现有任务的对比实验\n\n由于尚无直接可比的“生成式视频合成”方法，论文将其与两类最相关任务的代表性算法进行比较。\n\n1.1 视频协调（Video Harmonization）\n\n*   **对比方法**  \n    Harmonizer \\[22\\]、VideoTripletTransformer \\[14\\]，以及人工逐帧粘贴（manually paste）。\n    \n*   **数据集**  \n    HYouTube \\[29\\]（含前景、掩码、背景三元组）。\n    \n*   **评价指标**  \n    PSNR、SSIM、CLIP-Similarity、LPIPS。\n    \n*   **结果**\n    \n    方法\n    \n    PSNR↑\n    \n    SSIM↑\n    \n    CLIP↑\n    \n    LPIPS↓\n    \n    Harmonizer\n    \n    39.76\n    \n    0.9402\n    \n    0.9614\n    \n    0.0412\n    \n    VideoTripletTransformer\n    \n    40.03\n    \n    0.9297\n    \n    0.9564\n    \n    0.0455\n    \n    **GenCompositor**\n    \n    **42.00**\n    \n    **0.9487**\n    \n    **0.9713**\n    \n    **0.0385**\n    \n*   **视觉对比**  \n    图 6 显示：对比方法在前景边缘出现锯齿、颜色不一致；GenCompositor 边缘平滑且光照自然。\n    \n\n1.2 轨迹可控视频生成（Trajectory-controlled Generation）\n\n*   **对比方法**  \n    Tora \\[59\\]、ReVideo \\[30\\]。\n    \n*   **评价基准**  \n    VBench 四维指标：\n    \n    1.  Subject Consistency（主体一致性）\n    2.  Background Consistency（背景一致性）\n    3.  Motion Smoothness（运动平滑度）\n    4.  Aesthetic Quality（美学质量）\n*   **结果**\n    \n    方法\n    \n    Subj. Cons↑\n    \n    Bg. Cons↑\n    \n    Motion↑\n    \n    Aesthetic↑\n    \n    Tora\n    \n    88.44 %\n    \n    92.45 %\n    \n    98.03 %\n    \n    49.33 %\n    \n    ReVideo\n    \n    88.02 %\n    \n    92.90 %\n    \n    96.85 %\n    \n    48.56 %\n    \n    **GenCompositor**\n    \n    **89.75 %**\n    \n    **93.43 %**\n    \n    **98.69 %**\n    \n    **52.00 %**\n    \n*   **视觉对比**  \n    图 7 显示：Tora/ReVideo 存在主体 ID 漂移、轨迹偏离、元素消失；GenCompositor 严格遵循轨迹且保持前景身份一致。\n    \n\n2\\. 用户主观研究\n\n*   **设置**  \n    每类任务各 20 组对比，邀请 19 位专业志愿者投票。\n*   **结果**（图 11）\n    *   视频协调：77.37 % 偏好 GenCompositor。\n    *   轨迹可控生成：71.58 % 偏好 GenCompositor。\n\n3\\. 消融实验（Ablation Study）\n\n验证四个关键组件：DiT Fusion Block、BPBranch、Luminance Augmentation、Mask Inflation。\n\n配置\n\nPSNR↑\n\nSSIM↑\n\nCLIP↑\n\nLPIPS↓\n\nSubj. Cons↑\n\nBg. Cons↑\n\nMotion↑\n\nAesthetic↑\n\nw/o fusion block\n\n19.89\n\n0.8015\n\n0.9341\n\n0.1535\n\n88.85 %\n\n92.21 %\n\n98.34 %\n\n48.85 %\n\nw/o BPBranch\n\n40.01\n\n0.9378\n\n0.9709\n\n0.0432\n\n88.77 %\n\n89.62 %\n\n97.25 %\n\n51.51 %\n\nw/o augmentation\n\n39.80\n\n0.9295\n\n0.9629\n\n0.0520\n\n88.00 %\n\n89.97 %\n\n98.30 %\n\n50.73 %\n\nw/o mask inflation\n\n41.86\n\n0.9422\n\n0.9701\n\n0.0409\n\n89.72 %\n\n91.62 %\n\n98.28 %\n\n50.87 %\n\n**full model**\n\n**42.00**\n\n**0.9487**\n\n**0.9713**\n\n**0.0385**\n\n**89.75 %**\n\n**93.43 %**\n\n**98.69 %**\n\n**52.00 %**\n\n*   **视觉观察**（图 8）\n    *   无 fusion block：前景 ID 与动作丢失。\n    *   无 BPBranch：背景一致性下降。\n    *   无 augmentation / 无 inflation：边缘锯齿明显。\n\n4\\. ERoPE 维度选择实验\n\n*   **设置**  \n    在 height、width、timing 三个维度分别扩展 ERoPE，并对比统一 RoPE（w/o ERoPE）。\n*   **结果**（图 12）  \n    三种 ERoPE 变体的训练损失均显著低于统一 RoPE，且三者曲线几乎重合；最终选用沿 height 扩展的方案。\n\n5\\. 泛化能力验证\n\n*   **任务迁移**  \n    将前景条件替换为“空白视频”，GenCompositor 可直接用于**视频补全 / 物体移除**（图 10）。\n*   **流程**\n    1.  SAM2 提取待移除物体掩码；\n    2.  膨胀掩码 → masked video；\n    3.  空白前景作为条件 → 模型自动填补背景。\n*   **结果**  \n    无需额外训练即可生成连贯、自然的补全内容，验证任务本身的通用性。\n\n实验总结\n\n维度\n\n结论\n\n与现有任务对比\n\n在协调与轨迹控制两个子任务上均显著优于 SOTA\n\n消融分析\n\n四个关键组件缺一不可\n\n用户主观\n\n专业用户显著偏好 GenCompositor\n\n泛化验证\n\n同一模型可直接用于视频补全/物体移除\n\n训练稳定性\n\nERoPE 有效降低损失并加速收敛\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 GenCompositor 的后续研究切入点，按“数据-模型-应用”三层递进，并给出可行思路：\n\n1\\. 数据与条件层面\n\n*   **极端光照鲁棒性**  \n    当前仅用 Gamma 校正模拟光照差异；可引入 **HDR 合成数据** 或 **物理光照估计（inverse rendering）** 作为额外条件，提升强光/逆光场景下的自然度。\n*   **复杂遮挡与深度推理**  \n    现有方法对“前景被背景物体部分遮挡”或“前景投射阴影到背景”仅做隐式建模。可显式引入 **单目深度估计** 或 **3D-aware 表示（depth-conditioned DiT）**，实现更精确的遮挡与阴影推理。\n*   **多前景同时注入**  \n    当前一次仅处理单个前景视频；扩展为 **多前景并行条件**，需解决前景-前景交互、遮挡排序及计算效率问题。\n\n2\\. 模型架构层面\n\n*   **时空分辨率提升**  \n    目前训练分辨率 480×720、49 帧；可借鉴 **CogVideoX-HD** 或 **Wan2.1** 的 4K/长视频策略，引入 **temporal-upscale DiT** 或 **latent interpolation** 模块，支持分钟级 1080p 合成。\n*   **轻量化与实时化**  \n    6 B 参数、65 s/clip 的推理成本仍高。可探索：\n    *   **蒸馏** → 小 3-4 倍的 DiT-Student；\n    *   **步数蒸馏** → 4-8 步超快采样；\n    *   **边缘端适配** → INT8 量化 + 移动 GPU 部署。\n*   **ERoPE 的维度扩展**  \n    目前仅在 height 维度错位，可研究 **learnable offset** 或 **adaptive coordinate warping**，让网络自动决定错位维度与幅度，进一步提升对任意布局的泛化。\n\n3\\. 交互与应用层面\n\n*   **实时交互式编辑**  \n    结合 **拖拽式 UI**（类似 DragGAN）与 **流式 VAE**，实现“边拖拽边预览”的实时合成体验。\n*   **风格化与重照明**  \n    在前景注入后，增加 **风格迁移分支** 或 **重照明控制**（如指定目标环境光方向），满足影视级后期需求。\n*   **跨模态条件融合**  \n    除轨迹/缩放外，引入 **文本描述**（“让蝴蝶闪烁蓝光”）或 **音频节奏**（爆炸与鼓点对齐），实现多模态联合控制。\n*   **下游任务零样本迁移**  \n    当前已验证视频补全/物体移除；可进一步测试 **视频替换（actor swap）**、**虚拟广告植入** 等场景，评估零样本性能。\n\n4\\. 评估与基准\n\n*   **新指标**  \n    提出针对“生成式视频合成”的专用指标：\n    *   **前景-背景交互一致性（FBIC）**：衡量阴影、反射、遮挡的物理正确性；\n    *   **用户控制精度（UCP）**：轨迹偏移量、缩放误差等。\n*   **大规模基准**  \n    将 VideoComp 扩展至 **百万级** 并覆盖更多类别（人物、车辆、特效），建立类似 VBench 的公开排行榜。\n\n总结：可探索路线图\n\n阶段\n\n目标\n\n关键技术\n\n短期\n\n提升光照/遮挡鲁棒性\n\nHDR 数据、depth-conditioned DiT\n\n中期\n\n高分辨率+实时化\n\n4K 训练、蒸馏+量化\n\n长期\n\n多模态交互+新基准\n\n文本/音频联合控制、FBIC 指标\n\n这些方向既可独立开展，也可组合迭代，逐步把 GenCompositor 推向影视级生产工具。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n-------\n\nGenCompositor 首次提出“生成式视频合成”任务，并用一套轻量级 Diffusion-Transformer 流水线，把任意前景视频按用户轨迹/尺寸无缝注入背景视频，在背景一致性、前景保真与用户可控性上均优于现有方法。\n\n1\\. 任务定义\n\n*   **输入**：背景视频 $v\\_b$、前景视频 $v\\_f$、用户控制 $c$（轨迹+缩放）。\n*   **输出**：合成视频 $z\\_0$，要求\n    1.  背景像素级不变；\n    2.  前景身份与动作完整保留；\n    3.  支持任意轨迹/尺寸交互。\n\n2\\. 技术方案\n\n模块\n\n功能\n\n关键设计\n\n**Background Preservation Branch**\n\n保证背景不变\n\n2 个轻量 DiT block + masked token 注入\n\n**Foreground Generation Mainstream**\n\n注入前景并交互\n\n42 个 **DiT Fusion Block**（token-wise 自注意力融合）\n\n**ERoPE**\n\n解决前景-背景布局不对齐\n\n在高度维度错位编码，零额外参数\n\n**训练增强**\n\n提升鲁棒性\n\nMask inflation + 随机 Gamma 校正\n\n3\\. 数据与实验\n\n*   **VideoComp 数据集**：61 K 组（背景、前景、掩码），全自动标注+人工过滤。\n*   **对比实验**：\n    *   视频协调：PSNR ↑ 42.0 vs 40.0（SOTA）。\n    *   轨迹可控生成：四项 VBench 指标均第一。\n*   **消融实验**：四个关键组件缺一不可。\n*   **用户研究**：77 % 以上专业用户偏好 GenCompositor。\n*   **泛化验证**：同一模型零样本完成视频补全/物体移除。\n\n4\\. 结论与展望\n\nGenCompositor 已覆盖常见场景；未来可探索 HDR 光照、深度感知遮挡、4K 长视频、实时交互等方向。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是GenCompositor%3A%20Generative%20Video%20Compositing%20with%20Diffusion%20Transformer，链接是  https://arxiv.org/pdf/2509.02460  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02460  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-16-1756943220812","title":"OpenVision 2: A Family of Generative Pretrained Visual Encoders for Multimodal Learning","description":"This paper provides a simplification on OpenVision's architecture and loss design for enhancing its training efficiency. Following the prior vision-language pretraining works CapPa and AIMv2, as well as modern multimodal designs like LLaVA, our changes are straightforward: we remove the text encoder (and therefore the contrastive loss), retaining only the captioning loss as a purely generative training signal. We name this new version OpenVision 2. The initial results are promising: despite this simplification, OpenVision 2 competitively matches the original model's performance on a broad set of multimodal benchmarks while substantially cutting both training time and memory consumption. For example, with ViT-L/14, it reduces training time by about 1.5x (from 83h to 57h), and memory usage by about 1.8x (from 24.5GB to 13.8GB, equivalently allowing the maximum batch size to grow from 2k to 8k). This superior training efficiency also allows us to scale far beyond the largest vision encoder used in OpenVision, reaching more than 1 billion parameters. We hold a strong belief that this lightweight, generative-only paradigm is compelling for future vision encoder development in multimodal foundation models.","url":"https://huggingface.co/papers/2509.01644","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.812Z","metadata":{"authors":["Yanqing Liu","Xianhang Li","Letian Zhang","Zirui Wang","Zeyu Zheng","Yuyin Zhou","Cihang Xie"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.01644.pdf","arxivUrl":"https://arxiv.org/abs/2509.01644","arxivId":"2509.01644","coolPaperUrl":"https://papers.cool/arxiv/2509.01644","published":"2025-09-01T17:38:21Z","updated":"2025-09-01T17:38:21.000Z","zh_summary":"本文对OpenVision的架构和损失设计进行了简化，以提高其训练效率。按照之前的视觉-语言预训练工作CapPa和AIMv2，以及现代多模态设计如LLaVA，我们的改动非常简单：我们去除了文本编码器（因此去除了对比损失），仅保留作为纯生成训练信号的字幕损失。我们将这个新版本命名为OpenVision 2。初步结果令人鼓舞：尽管进行了简化，OpenVision 2在广泛的多模态基准测试中仍然与原模型的性能相匹配，同时显著减少了训练时间和内存消耗。例如，使用ViT-L/14时，训练时间减少了约1.5倍（从83小时降至57小时），内存使用量减少了约1.8倍（从24.5GB降至13.8GB，相当于允许最大批量大小从2000增加到8000）。这种卓越的训练效率还使我们能够将规模扩大到超出OpenVision使用的最大视觉编码器，达到超过10亿个参数。我们坚信，这种轻量级、仅生成的范式对未来多模态基础模型中的视觉编码器开发具有吸引力。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文核心解决的问题是：**如何在保持甚至提升多模态性能的同时，大幅降低视觉-语言预训练的计算与内存开销**。\n\n具体而言，OpenVision 2 针对以下痛点提出改进：\n\n*   **计算冗余**：OpenVision 原版采用“对比学习+生成式损失”双路训练，需同时维护文本编码器与文本解码器，导致 FLOPs 与显存显著高于 CLIP 基线。\n*   **数据与模型规模受限**：高内存占用限制了可使用的 batch size，进而阻碍在更大规模数据或更大模型上的训练。\n*   **架构与下游任务不匹配**：预训练阶段的多目标（对比+生成）与下游 LLaVA 类框架仅使用生成式微调的范式存在目标差异，可能带来迁移损耗。\n\n通过**完全移除文本编码器与对比损失，仅保留生成式字幕损失**，OpenVision 2 将训练流程简化为“图像编码器 → 文本解码器”的轻量级两模块架构，从而在以下方面取得突破：\n\n*   **训练效率**：ViT-L/14 训练时间从 83 h 降至 57 h（≈1.5×），显存从 24.5 GB 降至 13.8 GB（≈1.8×），最大 batch size 由 2 k 提升到 8 k。\n*   **可扩展性**：节省的资源使训练 1.01 B 参数量的 ViT-g/14 成为可能，且仍保持成本可控。\n*   **性能保持**：在 TextVQA、ChartQA、OCR-Bench、MME 等多模态基准上，OpenVision 2 与原版 OpenVision 得分相当甚至略优，证明生成式预训练足以替代传统的 CLIP 式对比学习。\n\n**Q2**: 有哪些相关研究？\n\nOpenVision 2 的提出与以下四条研究脉络紧密相关，并在这些工作的基础上进行简化与扩展：\n\n1.  **对比式视觉-语言预训练**\n    \n    *   **CLIP** \\[44\\]、ALIGN \\[20\\]：奠定“图像-文本对比学习”范式。\n    *   **LAION-2B-CLIP** \\[19\\]、**DataComp-1B-CLIP** \\[16\\]、**MetaCLIP-5B** \\[59\\]：基于大规模公开数据训练的开源 CLIP 变体。\n    *   **SigLIP** \\[62\\]：用 sigmoid 损失替代 InfoNCE，提升训练效率。\n    *   **EVA-CLIP** \\[50\\]、**DFN-CLIP** \\[12\\]：通过数据过滤或网络架构改进进一步提升性能。  \n        → 这些工作均以对比损失为核心，OpenVision 2 则**完全舍弃对比损失**，验证其非必要性。\n2.  **生成式视觉-语言预训练**\n    \n    *   **CapPa** \\[53\\]：首次提出“仅用字幕生成损失训练视觉编码器”，但使用短且噪声较大的网页 alt-text。\n    *   **AIMv2** \\[14\\]：采用 Prefix-ViT 与混合目标（图像块回归 + 文本生成），参数量达 1.8 B。\n    *   **CoCa** \\[61\\]、**Flamingo** \\[1\\]：将对比与生成目标结合，架构更复杂。  \n        → OpenVision 2 在 CapPa 的“字幕唯一监督”思想基础上，引入**更高质量合成字幕**与**随机 token 掩码**策略，提高效率与性能。\n3.  **高效训练策略**\n    \n    *   **CLIPA** \\[32\\]：低分辨率预训练 + 高分辨率微调，显著减少训练 FLOPs。\n    *   **Token masking / dropping**：在 AIMv2、PaLI-X 等工作中用于降低解码器负担；OpenVision 2 将其系统研究并验证最优掩码比例（25–35 %）。\n4.  **数据质量与合成字幕**\n    \n    *   **Recap-DataComp-1B** \\[31\\]：用 LLaMA-3-LLaVA 对 DataComp-1B 重新生成字幕，提升标注质量。\n    *   **ReCap-DataComp-1B v2**：进一步以原始 alt-text 为条件并采用加权 top-k 采样，获得更长、更细粒度的描述，成为 OpenVision 2 的默认训练语料。\n\n综上，OpenVision 2 将\n\n*   CapPa 的“生成式监督”思想、\n*   CLIPA 的“低分辨率预训练”策略、\n*   Recap 的“高质量合成字幕”方法、\n*   以及 token-masking 的“计算节省”技巧  \n    整合为一套简洁、高效、可扩展的视觉编码器预训练方案，并在 1 B 参数规模上验证了其通用性与有效性。\n\n**Q3**: 论文如何解决这个问题？\n\nOpenVision 2 通过一套极简而系统的“三步手术”将 OpenVision 的复杂多分支训练流程压缩为轻量级两模块架构，从而同时解决计算冗余、内存瓶颈与可扩展性问题。\n\n步骤\n\n原 OpenVision 设计\n\nOpenVision 2 改动\n\n直接收益\n\n**1\\. 架构剪枝**\n\n保留文本编码器，执行双路对比损失（web-caption & synthetic-caption）\n\n**完全移除文本编码器** → 仅剩 ViT 图像编码器 + 文本解码器\n\n训练图从 3 模块减为 2 模块；显存峰值 ↓≈1.8×\n\n**2\\. 损失简化**\n\n对比损失 + 生成式字幕损失（需额外文本解码器）\n\n**仅保留字幕生成损失**；用高质量合成字幕作为唯一监督\n\n每图 FLOPs ↓≈1.3×；训练时间 ↓≈1.5–2×\n\n**3\\. 训练加速与正则**\n\n无 token 掩码；高分辨率全程训练\n\n• **继承 CLIPA**：低分辨率预训练 + 高分辨率微调  \n• **随机掩码 65 % 视觉 token** 再送入解码器\n\n解码器计算量再降 65 %；支持 batch 8 k 不 OOM\n\n补充技术细节\n\n*   **数据升级**：用 ReCap-DataComp-1B v2（LLaMA-3-LLaVA 重标注，更长、更细粒度）取代噪声 web alt-text，保证字幕质量。\n*   **目标对齐**：预训练阶段与下游 LLaVA 微调阶段均为“ViT 特征 → 文本生成”，消除目标不匹配。\n*   **规模验证**：在 1.01 B 参数的 ViT-g/14 上仍保持训练可行，并在 OCR、ChartQA 等细粒度任务上取得新高。\n\n通过以上三步，OpenVision 2 在不牺牲多模态性能的前提下，实现了训练时间、显存占用与 FLOPs 的大幅下降，从而解决了大规模视觉-语言预训练的效率与可扩展性难题。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **性能验证**、**效率评估** 与 **设计消融** 三条主线展开实验，覆盖 8 个主流多模态基准、3 种模型规模、2 个下游框架，以及多种训练策略与数据配置。具体实验如下：\n\n1\\. 多模态下游性能验证\n\n**目的**：验证 OpenVision 2 在标准多模态任务上能否与 OpenVision、CLIP 等基线持平或更优。\n\n框架\n\n模型规模\n\n分辨率\n\n基准任务（共 8 项）\n\n关键结论\n\nLLaVA-1.5\n\nViT-L/14、SoViT-400M/14、ViT-H/14、ViT-g/14\n\n224 / 336 / 448\n\nTextVQA、ChartQA、OCR-Bench、MME、SEED、SQA、GQA、POPE\n\nOpenVision 2 在 30/32 个设定中 ≥ 原 OpenVision；OCR-Bench 平均 +15~+30 分\n\nOpen-LLaVA-Next\n\n同上\n\n同上\n\n同上\n\n结果与 LLaVA-1.5 趋势一致；ViT-H/14@448 在 OCR-Bench 创 590 新高\n\n2\\. 训练效率与可扩展性评估\n\n**目的**：量化时间、FLOPs、显存节省，并验证能否扩展到 1 B 参数。\n\n指标\n\nViT-L/14 @224\n\nSoViT-400M/14 @384\n\nViT-g/14 @224\n\n**训练时间**\n\n83 h → 57 h (×1.5↓)\n\n241 h → 121 h (×2↓)\n\n首次成功训练 1 B 模型\n\n**FLOPs / image**\n\n271.8 → 208.9 (×1.3↓)\n\n1636.8 → 1017.7 (×1.6↓)\n\n—\n\n**显存 / chip**\n\n24.5 GB → 13.8 GB (×1.8↓)\n\n27.4 GB → 14.5 GB (×1.9↓)\n\n32 GB 内支持 batch 2 k\n\n**最大 batch size**\n\n2 k → 8 k (不 OOM)\n\n512 → 1 k (不 OOM)\n\n—\n\n3\\. 消融实验\n\n3.1 字幕来源对性能的影响\n\n训练字幕\n\nTextVQA\n\nOCR-Bench\n\n结论\n\n原始 alt-text\n\n51.8\n\n238\n\n噪声大，性能最差\n\nReCap-DataComp-1B\n\n56.9\n\n291\n\n显著提升\n\nReCap-DataComp-1B v2\n\n56.5\n\n303\n\nOCR 更强 → 选为默认\n\n3.2 视觉 token 保留比例\n\nKeep Ratio\n\nOCR-Bench\n\nTextVQA\n\n结论\n\n100 %\n\n254\n\n53.8\n\n计算高，非最优\n\n35 %\n\n291\n\n56.9\n\n最佳平衡点\n\n10 %\n\n276\n\n55.6\n\n信息不足，性能下降\n\n3.3 训练策略组合收益（ViT-L/14 @224）\n\n配置\n\n训练时间\n\n说明\n\nCapPa 基线\n\n217 h\n\n无 CLIPA、无掩码\n\n\\+ Token Mask\n\n190 h\n\n仅掩码\n\n\\+ CLIPA\n\n67 h\n\n仅 CLIPA\n\n\\+ 两者\n\n55 h\n\n协同最佳\n\n4\\. 扩展实验\n\n*   **1.01 B 参数 ViT-g/14** 成功训练并在多模态基准取得 SOTA 或接近 SOTA 结果，证明方法可扩展至十亿级模型。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 OpenVision 2 之后的延伸研究，按“数据—模型—训练—评测—应用”五维度展开：\n\n1\\. 数据层面\n\n*   **多语言合成字幕**  \n    将 ReCap-DataComp-1B v2 扩展到 30+ 语言，验证字幕-only 预训练在多语场景下的通用性。\n*   **细粒度与多轮字幕**  \n    引入对象级、区域级或对话式字幕，测试对 Referring Expression、Visual Grounding 等任务的影响。\n*   **动态字幕长度课程**  \n    由短到长逐步增加字幕复杂度，观察收敛速度与最终性能权衡。\n\n2\\. 模型层面\n\n*   **混合视觉骨干**  \n    尝试 Swin-V3、ConvNeXt-3D 或 ViT-MoE，验证 caption-only 目标对不同视觉归纳偏置的鲁棒性。\n*   **解码器容量缩放**  \n    固定 ViT 规模，系统增大文本解码器（1 B→4 B），测试“视觉编码器轻+语言解码器重”的极限配比。\n*   **跨模态 Q-Former 替代方案**  \n    在 ViT 与 LLM 之间插入可学习的 Query Token，评估是否进一步减少解码 FLOPs 同时保性能。\n\n3\\. 训练策略\n\n*   **掩码策略自适应**  \n    用强化学习或元学习动态决定每样本 token keep ratio，而非固定 35 %。\n*   **字幕难度采样**  \n    按困惑度或信息熵对字幕重加权，让模型优先学习高信息增益样本。\n*   **持续预训练**  \n    先字幕预训练，再小规模对比微调，验证“生成→对比”两阶段能否兼得效率与对齐优势。\n\n4\\. 评测与分析\n\n*   **细粒度探针任务**  \n    新增属性识别、计数、空间关系等诊断集，量化 caption-only 表征的细粒度能力。\n*   **对抗与鲁棒性**  \n    在 ImageNet-A、ImageNet-R 及 patch-shuffle 攻击下测试鲁棒性，对比 CLIP 风格模型。\n*   **可解释性工具**  \n    利用 Grad-CAM、token attribution 分析被保留的 35 % token 的语义分布，验证掩码合理性。\n\n5\\. 下游应用与系统\n\n*   **端到端多模态大模型**  \n    将 OpenVision 2 ViT-g/14 直接接入 7 B/13 B LLM，训练完全开源的 LLaVA-3B/7B 系列。\n*   **边缘端推理优化**  \n    结合 token 稀疏化与量化（INT4 ViT + INT8 LLM），在移动端实现实时图文对话。\n*   **跨模态检索再审视**  \n    仅用 caption-only 预训练权重初始化双塔检索模型，微调后对比 CLIP 在 Flickr30K、MS-COCO 上的 Zero-shot Recall@K。\n\n这些方向既可直接复用 OpenVision 2 已发布的代码与数据，也能进一步验证“生成式监督是否足以取代对比学习”这一核心假设。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\nOpenVision 2 通过“仅保留字幕生成损失、移除文本编码器与对比损失”的极简方案，在保持甚至提升多模态性能的同时，将训练时间、显存与 FLOPs 分别降低 1.5×、1.8× 与 1.3×，并成功把视觉编码器扩展到 1 B 参数规模。\n\n核心内容速览\n\n维度\n\n原 OpenVision\n\nOpenVision 2 改动\n\n结果\n\n**架构**\n\nViT + 文本编码器 + 文本解码器（双路损失）\n\n**仅 ViT + 文本解码器**（字幕损失唯一监督）\n\n训练图从 3 模块减为 2 模块\n\n**损失**\n\n对比损失（web & synth caption）+ 生成损失\n\n**仅字幕生成损失**\n\nFLOPs↓1.3×，训练时间↓1.5–2×\n\n**数据**\n\nReCap-DataComp-1B\n\n**ReCap-DataComp-1B v2**（更长、更细粒度合成字幕）\n\nOCR-Bench↑+15~+30\n\n**训练策略**\n\n无 token 掩码\n\n**随机掩码 65 % 视觉 token** + CLIPA 低分辨率课程\n\n显存峰值↓1.8×，batch 2 k→8 k\n\n**规模**\n\n最大 632 M\n\n**首次训练 1.01 B ViT-g/14**\n\n十亿级模型仍可高效训练\n\n**评测**\n\n8 项多模态基准\n\n在 LLaVA-1.5 & Open-LLaVA-Next 框架下\n\n性能持平或优于原 OpenVision\n\n结论\n\nOpenVision 2 证明：\n\n1.  **字幕-only 生成式预训练即可媲美 CLIP 式对比学习**；\n2.  **极简架构带来显著效率与可扩展性优势**；\n3.  **为社区提供了完全开源、可复现的十亿级视觉编码器训练范式**。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是OpenVision%202%3A%20A%20Family%20of%20Generative%20Pretrained%20Visual%20Encoders%20for%20Multimodal%20Learning，链接是  https://arxiv.org/pdf/2509.01644  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01644  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-17-1756943220812","title":"Benchmarking Optimizers for Large Language Model Pretraining","description":"The recent development of Large Language Models (LLMs) has been accompanied by an effervescence of novel ideas and methods to better optimize the loss of deep learning models. Claims from those methods are myriad: from faster convergence to removing reliance on certain hyperparameters. However, the diverse experimental protocols used to validate these claims make direct comparisons between methods challenging. This study presents a comprehensive evaluation of recent optimization techniques across standardized LLM pretraining scenarios, systematically varying model size, batch size, and training duration. Through careful tuning of each method, we provide guidance to practitioners on which optimizer is best suited for each scenario. For researchers, our work highlights promising directions for future optimization research. Finally, by releasing our code and making all experiments fully reproducible, we hope our efforts can help the development and rigorous benchmarking of future methods.","url":"https://huggingface.co/papers/2509.01440","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.812Z","metadata":{"authors":["Andrei Semenov","Matteo Pagliardini","Martin Jaggi"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.01440.pdf","arxivUrl":"https://arxiv.org/abs/2509.01440","arxivId":"2509.01440","coolPaperUrl":"https://papers.cool/arxiv/2509.01440","published":"2025-09-01T12:50:30Z","updated":"2025-09-01T12:50:30.000Z","zh_summary":"大型语言模型（LLMs）的近期发展伴随着一系列新颖的思想和方法，用以更好地优化深度学习模型的损失。这些方法的声明数不胜数：从加速收敛到消除对某些超参数的依赖。然而，用于验证这些声明的多样实验协议使得方法之间的直接比较变得具有挑战性。本研究在标准化的LLM预训练场景中，对近期优化技术进行了全面评估，系统地变化模型大小、批量大小和训练时间。通过对每种方法的精心调整，我们为实践者提供了指导，帮助他们了解哪种优化器最适合每种场景。对于研究人员而言，我们的工作强调了未来优化研究的有希望方向。最后，通过发布我们的代码并使所有实验完全可重现，我们希望我们的努力能够帮助未来方法的发展和严格基准测试。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n该论文旨在解决 **大规模语言模型（LLM）预训练中优化器选择的系统性评估缺失** 的问题。具体而言，论文试图回答以下核心问题：\n\n1.  **AdamW是否仍是LLM预训练的最优选择？**  \n    尽管AdamW在过去十年中占据主导地位，但近期涌现的优化器（如AdEMAMix、MARS、SOAP等）声称在收敛速度、通信效率或下游任务性能上更优。论文通过标准化实验验证这些新方法的实际效果。\n    \n2.  **如何公平比较不同优化器？**  \n    现有研究常因实验协议差异（如模型规模、批次大小、训练时长、超参数设置）导致结论不可比。论文通过统一基准测试框架，控制变量（模型参数、批次大小、训练token数），并经过大规模超参数调优（2900次实验，约3万GPU小时），确保比较的公平性。\n    \n3.  **优化器性能如何随规模变化？**  \n    论文系统评估了11种优化器在 **124M到720M参数** 的密集模型和 **520M参数的MoE模型** 上的表现，覆盖从Chinchilla最优训练时长到超扩展训练的场景，揭示不同优化器在规模扩展时的鲁棒性。\n    \n4.  **超参数（如学习率、权重衰减、warmup）如何影响优化器表现？**  \n    通过消融实验，论文量化了关键超参数（如权重衰减λ、学习率衰减比例γ\\_end、warmup步数）对不同优化器的影响，并提出实用调优建议（如长训练需增大β₂或β₃）。\n    \n5.  **为实践者和研究者提供指导**\n    \n    *   **实践者**：给出不同训练场景（如短训练、长训练、小批次/大批次）下的最优优化器选择（如AdEMAMix和MARS在多数场景下优于AdamW）。\n    *   **研究者**：开源完整基准测试工具包（代码、超参数配置），为未来优化器设计提供标准化评估框架。\n\n**总结**：论文填补了LLM预训练中优化器比较的系统性和规模性空白，挑战了AdamW的默认地位，并推动社区向更高效的优化器发展。\n\n**Q2**: 有哪些相关研究？\n\n论文在第2节“Background & Related Work”中系统梳理了与LLM优化器相关的研究，可归纳为以下5个方向：\n\n1\\. **Adam(W)的统治地位与局限性**\n\n*   **AdamW** \\[66, 84\\]：作为LLM预训练的事实标准，其成功归因于自适应学习率和动量机制，但存在对超参数敏感、通信开销大等问题。\n*   **理论解释**：\n    *   \\[5, 96, 158, 74, 72\\] 探讨AdamW在Transformer上的优势，归因于符号更新（sign-based updates）和二阶矩估计对梯度噪声的鲁棒性。\n    *   \\[160\\] 从Hessian角度解释AdamW为何优于SGD。\n\n2\\. **Adam变体与改进**\n\n*   **ADOPT** \\[134\\]：通过调整二阶矩更新顺序解决AdamW在非凸问题上的收敛问题，但论文发现其β₂仍需调优（与理论声称矛盾）。\n*   **AdEMAMix** \\[99\\]：引入慢动量（β₃）加速收敛，论文验证其在大规模训练中需增大β₃以维持优势。\n\n3\\. **符号优化器（Sign-based Methods）**\n\n*   **Signum** \\[9, 161\\]：SignSGD的动量版本，内存高效（仅需1个状态缓存），但小批次下性能差，需大批次或特殊调参（如权重衰减λ=0.5）。\n*   **Lion** \\[17\\]：符号发现的优化器，与AdEMAMix结构相似，论文发现其β₁=0.99最优，且受益于长warmup。\n\n4\\. **二阶/近似二阶方法**\n\n*   **Sophia** \\[79\\]：使用对角Fisher信息矩阵近似Hessian，论文发现其存在收敛不稳定问题（尤其在长训练和小批次下）。\n*   **SOAP** \\[141\\]：改进Shampoo的预条件方法，论文指出其性能依赖超参数（如批次大小、权重衰减），且在大批次下优势减弱。\n*   **Muon/D-Muon** \\[59, 81\\]：针对矩阵参数的正交化优化器，D-Muon通过全局权重衰减解决Muon的局限性。\n\n5\\. **无调度器/自适应方法**\n\n*   **Schedule-Free AdamW (SF-AdamW)** \\[27\\]：通过迭代平均消除学习率调度器，但论文发现其需梯度裁剪且对β₂敏感（需0.9999）。\n*   **Prodigy** \\[90\\]：自适应学习率方法，论文验证其有效学习率（γ\\_eff）可近似AdamW的调参过程，但小批次下对β₂敏感。\n\n6\\. **其他相关研究**\n\n*   **MARS** \\[153\\]：结合方差缩减的二阶方法，论文发现其在大批次和MoE模型中表现优异。\n*   **MoE优化器** \\[35, 165\\]：论文首次将优化器基准扩展到MoE架构，验证密集模型调参可直接迁移。\n\n关键对比点\n\n方法\n\n核心创新\n\n论文发现\n\n**AdamW**\n\n自适应+权重衰减\n\n需调大β₂（0.999）和长训练\n\n**AdEMAMix**\n\n双动量（β₃）\n\n长训练需增大β₃至0.9999\n\n**Signum/Lion**\n\n符号更新+动量\n\n需大批次或高λ=0.5，长warmup\n\n**SOAP**\n\n预条件+AdamW混合\n\n性能依赖批次大小和超参数\n\n**Sophia**\n\n对角Hessian近似\n\n长训练不稳定（小批次/多迭代）\n\n**Prodigy**\n\n自适应学习率\n\nγ\\_eff可指导AdamW调参\n\n总结\n\n论文通过对比上述方法，揭示了**现有研究的局限性**（如Sophia的收敛问题、SOAP的超参数敏感性），并提出了**新的调优规则**（如β₂/β₃随训练时长调整、权重衰减的阶段性作用），为后续优化器设计提供了实证基础。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过以下 **系统性、可复现的实验设计** 解决 LLM 优化器评估缺失的问题，核心思路是 **“标准化基准 + 大规模调参 + 多维度消融”**：\n\n1\\. **构建统一基准框架**\n\n*   **控制变量**：固定模型架构（Llama-like）、数据（FineWeb 100B tokens）、训练流程（序列长度512、混合精度bf16），仅改变优化器与超参数。\n*   **覆盖场景**：\n    *   **模型规模**：124M → 720M 密集模型，520M MoE 模型。\n    *   **训练时长**：从 Chinchilla 最优（~20 tokens/param）到 7× 超扩展训练。\n    *   **批次大小**：16k → 2M tokens，模拟小/大批次实际场景。\n*   **开源工具链**：提供完整训练脚本、超参数配置（附录 E）与评估流水线，确保复现性。\n\n2\\. **大规模超参数调优**\n\n*   **调参策略**：\n    *   **分阶段调参**：先在小模型（124M）上调优，再迁移到更大模型（210M/720M），仅微调学习率/β参数。\n    *   **关键超参数**：学习率 γ、权重衰减 λ、β₁/β₂/β₃、warmup 步数、梯度裁剪阈值、学习率衰减终点 γ\\_end。\n*   **调参规模**：\n    *   **2900 次实验**，覆盖 11 种优化器（AdamW、AdEMAMix、MARS、SOAP 等）。\n    *   **~3 万 GPU 小时**（A100/H100），确保每个优化器在每种场景下均达到最优。\n\n3\\. **多维度消融实验**\n\n通过 **控制单一变量** 揭示优化器行为规律：\n\n(1) **权重衰减 λ 的阶段性作用**\n\n*   **短训练**：λ=0.5 加速收敛（Signum/Lion 甚至超越 AdamW）。\n*   **长训练**：λ=0.1 更稳健（避免梯度爆炸）。\n*   **MoE 验证**：D-Muon 通过全局 λ 解决 Muon 的矩阵参数无衰减问题。\n\n(2) **学习率与 β 参数的耦合**\n\n*   **β₂/β₃ 需随训练时长调整**：\n    *   AdamW/AdEMAMix 长训练需增大 β₂（0.999→0.9999）。\n    *   Prodigy 小批次下 β₂ 微小变化（0.999→0.9999）导致发散，需启用偏差修正。\n*   **学习率迁移性**：小模型调优的 γ 可平滑迁移至大模型（8× 训练时长）。\n\n(3) **学习率调度器的选择**\n\n*   **Cosine 调度器** 在多数场景最优，但 Muon 偏好 WSD（Warmup-Stable-Decay）。\n*   **γ\\_end 需远小于 0.1γ\\_max**：0.01γ\\_max（Cosine）或 0.001γ\\_max（Linear）显著提升性能。\n\n(4) **优化器特异性现象**\n\n*   **Sophia 的收敛崩溃**：小批次或长训练下梯度爆炸（图31-32），需慎用。\n*   **Signum 的批次敏感性**：小批次下性能差，但 512×512 批次下接近 AdamW。\n*   **SF-AdamW 的梯度裁剪**：原论文建议禁用裁剪，但实验表明 **必须启用**（阈值 0.5）。\n\n4\\. **MoE 架构的扩展验证**\n\n*   **直接迁移调参**：将密集模型的最优超参数（如 λ=0.1）应用于 520M MoE，验证优化器在稀疏模型上的鲁棒性。\n*   **发现**：AdEMAMix 和 MARS 在 MoE 中仍保持优势，SOAP 需重新调 β 参数。\n\n5\\. **实用指导与开源贡献**\n\n*   **实践建议**：\n    *   **短训练**：优先用 AdEMAMix 或 MARS，λ=0.5 加速收敛。\n    *   **长训练**：AdEMAMix（β₃=0.9999）+ λ=0.1 + Cosine 调度器。\n    *   **MoE 训练**：直接复用密集模型调参，但需关注 SOAP 的 β₂。\n*   **开源资源**：  \n    [GitHub 仓库](https://github.com/epfml/llm-optimizer-benchmark) 提供完整代码、超参数表（附录 E）和训练日志，支持后续研究。\n\n总结：论文的解决方案\n\n通过 **标准化实验 + 大规模调参 + 细粒度消融**，论文首次系统回答了：\n\n*   **“何时放弃 AdamW？”** → 在多数场景下，AdEMAMix/MARS 更优。\n*   **“如何调参？”** → 提供分场景的调参规则（如 λ、β₂ 随训练时长调整）。\n*   **“如何复现？”** → 开源全链路工具，消除实验偏差。\n\n**Q4**: 论文做了哪些实验？\n\n论文通过 **分阶段、多维度的系统性实验** 全面评估了 11 种优化器在 LLM 预训练中的表现。实验设计围绕 **模型规模、批次大小、训练时长、架构类型（密集/MoE）** 四个核心变量展开，具体实验如下：\n\n**1\\. 核心基准实验（主实验）**\n\n**密集模型（Dense LLMs）**\n\n*   **模型规模**：124M、210M、583M、720M 参数（Llama-like 架构）。\n*   **批次大小**：\n    *   124M：16k/131k/262k tokens（32×512/256×512/512×512）。\n    *   210M：131k tokens（256×512）。\n    *   583M：2M tokens（3936×512）。\n    *   720M：1M tokens（1984×512）。\n*   **训练时长**：\n    *   124M/210M：1B→16.8B tokens（覆盖 Chinchilla 最优 2.5B/4.2B）。\n    *   583M：13B tokens（Chinchilla 最优 11.7B）。\n    *   720M：8B→48B tokens（Chinchilla 最优 14.4B）。\n*   **优化器**：11 种（AdamW、AdEMAMix、MARS、SOAP、Sophia、Muon、D-Muon、Signum、Lion、SF-AdamW、Prodigy）。\n*   **评估指标**：验证集交叉熵损失（Final Validation Loss）。\n\n**MoE 模型**\n\n*   **架构**：520M 参数 Switch-Transformer（8 专家，top-2 路由）。\n*   **批次大小**：131k tokens（256×512）。\n*   **训练时长**：5.5B→44B tokens（Chinchilla 最优 10.4B）。\n*   **验证**：直接复用密集模型的调参结果，测试迁移性。\n\n**2\\. 关键消融实验（Ablations）**\n\n**超参数敏感性分析**\n\n*   **权重衰减 λ**：  \n    扫描 λ∈{0, 0.1, 0.5}，发现短训练需高 λ（0.5），长训练需 λ=0.1。\n*   **学习率 γ**：  \n    固定其他参数，扫描 γ∈{1e-4, 3e-4, 5e-4, 1e-3, 2e-3}，验证迁移性。\n*   **β 参数**：\n    *   AdamW/AdEMAMix：β₂ 从 0.95→0.9999（长训练需更大值）。\n    *   Prodigy：β₂ 微小变化（0.999→0.9999）导致发散，需偏差修正。\n*   **Warmup 步数**：  \n    扫描 {2k, 8k, 32k}，发现符号优化器（Signum/Lion）需长 warmup（8k+）。\n*   **学习率调度器**：  \n    对比 Cosine、WSD、Linear，发现 Cosine 多数最优，Muon 偏好 WSD。\n\n**架构与实现细节**\n\n*   **Newton-Schulz 迭代次数**（Muon）：  \n    TNS∈{1,5,10,20}，TNS=5 最优，>5 无提升。\n*   **梯度裁剪**（SF-AdamW）：  \n    原论文建议禁用，实验发现必须启用（阈值 0.5）。\n*   **z-loss 正则化**：  \n    验证其对优化器排名的影响（无提升，反而降低性能）。\n\n**3\\. 扩展实验**\n\n**梯度范数动态**\n\n*   追踪不同优化器在训练中的梯度范数变化，关联稳定性与超参数（λ、γ、调度器）。\n\n**权重初始化与 Warmup**\n\n*   对比 std=0.02（常规）与 std=0.006（DeepSeek 风格），发现小初始化需更长 warmup。\n\n**Wall-clock 时间**\n\n*   在 30M→1B 模型上测试 100 迭代耗时，SOAP 因预条件计算显著更慢，Signum 最快。\n\n**4\\. 实验规模统计**\n\n维度\n\n范围/细节\n\n**总实验次数**\n\n2900 次（含所有优化器与超参数组合）\n\n**GPU 小时**\n\n~30,000（A100/H100/GH200）\n\n**模型规模**\n\n30M→1026M（含密集与 MoE）\n\n**训练时长**\n\n1B→48B tokens（覆盖短/长训练）\n\n**超参数扫描**\n\nγ、λ、β、warmup、调度器、裁剪阈值\n\n**总结：实验设计的独特性**\n\n*   **覆盖全面**：从 124M 到 720M 密集模型 + 520M MoE，涵盖实际部署场景。\n*   **调参深度**：每个优化器在每种场景下均独立调参（如 AdEMAMix 的 β₃=0.9999）。\n*   **现象级发现**：如 Sophia 的收敛崩溃、Signum 的大批次优势、SF-AdamW 的裁剪必要性。\n*   **开源复现**：所有实验配置与结果公开，支持后续研究直接对比。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向均直接源于论文的实验结果与讨论部分（§5 “Limitations” 及各节 Takeaways），可作为后续研究的切入点，按优先级与可行性由高到低列出：\n\n1\\. **超参数与训练时长的动态规律**\n\n*   **β₂/β₃ 的“时变”策略**  \n    论文发现 AdamW-like 优化器在长训练时需显著增大 β₂（0.95→0.9999）。可进一步：\n    *   建立 β₂(T) 的解析式（如 β₂=1−c/T），验证其在千亿级模型上的普适性。\n    *   探索 β₃ 对 AdEMAMix 在超扩展训练（>100B tokens）中的极限值。\n*   **权重衰减的阶段性调度**  \n    实验显示 λ=0.5 仅在短训练有效。可设计 **λ 退火策略**（如线性衰减至 0.1），兼顾早期收敛与后期稳定性。\n\n2\\. **优化器在更大规模与更长训练中的行为**\n\n*   **千亿级模型验证**  \n    当前实验止于 720M 参数。需测试：\n    *   AdEMAMix/MARS 在 7B/70B 模型上是否仍优于 AdamW。\n    *   Sophia 的收敛崩溃是否因模型增大而缓解（如梯度噪声相对减小）。\n*   **超扩展训练（>50× Chinchilla）**  \n    论文观察到 SOAP 在长训练中被 AdamW 反超。需验证：\n    *   是否因预条件矩阵更新频率不足？可尝试动态调整预条件频率 ϕ∝1/√t。\n\n3\\. **下游任务与损失的相关性**\n\n*   **“好损失≠好下游”的例外场景**  \n    论文仅报告验证损失，未验证下游任务。需：\n    *   在相同实验设置下，测试优化器对 MMLU、HumanEval 等指标的影响。\n    *   特别关注 Sophia（高损失但可能高鲁棒性）与 Signum（低损失但可能欠拟合）。\n\n4\\. **内存高效优化器的统一基准**\n\n*   **内存受限场景**  \n    论文未覆盖 GaLore、APOLLO 等内存高效方法。可：\n    *   在单卡 24GB GPU 上复现所有实验，比较显存占用与性能权衡。\n    *   量化 Signum/Lion 的内存优势（少 50% 状态缓存）是否随模型增大而放大。\n\n5\\. **MoE 架构的优化器特异性**\n\n*   **专家负载均衡与优化器耦合**  \n    论文发现 MoE 中优化器排名与密集模型一致，但未分析：\n    *   不同优化器对专家负载均衡损失（router z-loss）的敏感度。\n    *   是否需为 MoE 设计专家级学习率（如专家频率自适应 γ）。\n\n6\\. **理论驱动的优化器改进**\n\n*   **Sophia 的收敛崩溃根因**  \n    实验排除实现 bug，可能原因：\n    *   对角 Hessian 近似在长训练中误差累积。可尝试 **K-FAC 或 Shampoo 的全矩阵预条件**。\n    *   梯度裁剪阈值需随训练动态调整（如裁剪阈值∝梯度范数滑动平均）。\n\n7\\. **无调度器方法的极限**\n\n*   **SF-AdamW 的 β₂ 依赖性**  \n    论文发现 β₂=0.9999 最优，但理论建议 β₂=1−1/T。可：\n    *   验证 β₂=1−c/√T 是否更优（借鉴 AdaBelief 的时变策略）。\n    *   探索 **完全无 β₂ 的方法**（如用梯度范数归一化替代二阶矩）。\n\n8\\. **硬件感知的优化器设计**\n\n*   **通信效率与 Newton-Schulz 迭代**  \n    Muon/D-Muon 的 Newton-Schulz 步骤在分布式训练中可能成为瓶颈。可：\n    *   测试 **低精度正交化**（如 FP8 下的 Newton-Schulz）对收敛的影响。\n    *   设计 **异步预条件更新**（类似 LAMB 的层级缩放）。\n\n9\\. **自动化调参工具**\n\n*   **Prodigy 的“代理调参”潜力**  \n    论文提出用 Prodigy 的 γ\\_eff 指导 AdamW 调参。可扩展为：\n    *   开源工具：输入模型规模与训练时长，输出推荐 γ 和 β₂。\n    *   验证该策略在 10B+ 模型上的有效性。\n\n优先级建议\n\n方向\n\n短期可行性\n\n长期价值\n\n推荐动作\n\n**β₂ 动态策略**\n\n高\n\n高\n\n在 7B 模型上测试 β₂=1−c/T 与固定 0.9999 对比\n\n**千亿级验证**\n\n中\n\n极高\n\n用论文开源脚本扩展至 Llama-7B\n\n**下游任务**\n\n高\n\n中\n\n复现 3-4 个优化器在 720M 模型上的 MMLU 结果\n\n**内存基准**\n\n高\n\n中\n\n对比 Signum 与 AdamW 在 24GB GPU 上的最大可训练模型\n\n以上方向均基于论文的 **实证缺口** 与 **可扩展性**，可直接利用其开源代码与超参数表快速启动。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n在统一、可复现的框架下，首次系统评估了 11 种现代优化器在 124 M–720 M 密集与 520 M MoE 模型上的预训练表现，给出“何时、如何、用谁”取代 AdamW 的明确指南，并开源全套基准工具。\n\n主要内容速览\n\n维度\n\n关键发现\n\n实用建议\n\n**优化器排名**\n\nAdEMAMix、MARS 几乎在所有规模/批次/时长下优于 AdamW；SOAP 在 Chinchilla 长度内领先，长训练被反超；Signum/Lion 需大批次才具竞争力；Sophia 长训练易发散。\n\n短/中训练：AdEMAMix/MARS；资源受限：Signum/Lion+大批次；避免 Sophia。\n\n**超参数调优**\n\n• β₂/β₃ 随训练时长增大（0.999→0.9999）  \n• λ=0.5 仅短训练有效，长训练 λ=0.1  \n• γ\\_end 应 ≤0.01 γ\\_max（而非 0.1）  \n• warmup 2 k 步通用，符号法需 8 k+\n\n长训练前重调 β、λ、γ\\_end；小批次 Prodigy 需开 bias-correction。\n\n**批次与规模**\n\n大批次显著提升符号法与 MARS；MoE 上可直接复用密集模型超参。\n\n若算力允许，优先放大批次而非延长步数。\n\n**消融亮点**\n\n• Sophia 在 33.6 B tokens 处必发散  \n• SF-AdamW 必须启用梯度裁剪  \n• Newton-Schulz 5 步即足，>5 无收益\n\n避免盲目相信原论文“禁用裁剪”等声明。\n\n**开源贡献**\n\n2900 次实验、3 万 GPU 小时、完整脚本与超参表。\n\n直接复现或扩展至更大模型。\n\n一句话给实践者\n\n“把 AdamW 当默认之前，先跑一次 AdEMAMix（β₃=0.9999，λ=0.1，cosine→0.01 γ\\_max）；若显存吃紧，换 Signum 并放大批次。”\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Benchmarking%20Optimizers%20for%20Large%20Language%20Model%20Pretraining，链接是  https://arxiv.org/pdf/2509.01440  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01440  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1756943220812","title":"Attributes as Textual Genes: Leveraging LLMs as Genetic Algorithm Simulators for Conditional Synthetic Data Generation","description":"Large Language Models (LLMs) excel at generating synthetic data, but ensuring its quality and diversity remains challenging. We propose Genetic Prompt, a novel framework that combines genetic algorithms with LLMs to augment synthetic data generation. Our approach treats semantic text attributes as gene sequences and leverages the LLM to simulate crossover and mutation operations. This genetic process enhances data quality and diversity by creating novel attribute combinations, yielding synthetic distributions closer to real-world data. To optimize parent selection, we also integrate an active learning scheme that expands the offspring search space. Our experiments on multiple NLP tasks reveal several key findings: Genetic Prompt not only significantly outperforms state-of-the-art baselines but also shows robust performance across various generator model sizes and scales. Moreover, we demonstrate that fusing our synthetic data with the original training set significantly boosts downstream model performance, particularly for class-imbalanced scenarios. Our findings validate that Genetic Prompt is an effective method for producing high-quality synthetic data for a wide range of NLP applications.","url":"https://huggingface.co/papers/2509.02040","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.812Z","metadata":{"authors":["Guangzeng Han","Weisi Liu","Xiaolei Huang"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.02040.pdf","arxivUrl":"https://arxiv.org/abs/2509.02040","arxivId":"2509.02040","coolPaperUrl":"https://papers.cool/arxiv/2509.02040","published":"2025-09-02T07:35:20Z","updated":"2025-09-02T07:35:20.000Z","zh_summary":"大型语言模型（LLM）在生成合成数据方面表现出色，但确保其质量和多样性仍然具有挑战性。我们提出了遗传提示，这是一种新颖的框架，将遗传算法与LLM结合，以增强合成数据生成。我们的方法将语义文本属性视为基因序列，并利用LLM模拟交叉和突变操作。这个遗传过程通过创建新颖的属性组合来提高数据的质量和多样性，从而产生更接近真实世界数据的合成分布。为了优化父代选择，我们还整合了一种主动学习方案，扩展后代搜索空间。我们在多个自然语言处理任务上的实验揭示了一些关键发现：遗传提示不仅显著优于最先进的基线，而且在各类生成模型规模和效能上表现出强大的稳定性。此外，我们还展示了将我们的合成数据与原始训练集融合可以显著提升下游模型的性能，尤其是在类别不平衡的场景中。我们的发现证实了遗传提示是为广泛的自然语言处理应用生成高质量合成数据的一种有效方法。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n该研究聚焦的核心问题是：  \n**如何在使用大语言模型（LLM）进行合成数据生成时，同时保证数据的高质量与多样性，从而提升下游任务性能，尤其是在类别不平衡或数据稀缺场景下。**\n\n具体而言，论文指出当前 LLM 合成数据方法存在以下局限：\n\n*   依赖人工预定义属性或静态提示，限制了模型推理能力与跨领域泛化性；\n*   现有基于词/句层面的扰动（如简单改写）难以在大规模场景下保持语义多样性与真实分布一致性；\n*   逐样本“适应度”评估不可靠，导致传统遗传算法难以直接用于数据生成。\n\n因此，论文提出 **Genetic Prompt 框架**，通过将**文本的语义属性视为“基因”**，利用 LLM 模拟遗传算法中的交叉与变异操作，并引入主动学习策略优化父代选择，从而：\n\n1.  自动发现关键属性，减少对人工先验的依赖；\n2.  在属性层面进行遗传操作，兼顾质量与多样性；\n3.  提升合成数据对真实分布的逼近程度，显著改善下游任务表现，尤其对类别不平衡数据集效果明显。\n\n**Q2**: 有哪些相关研究？\n\n论文在 §6 Related Work 中系统梳理了两条研究脉络，并指出其与本文方法的关键差异。以下按主题归纳相关研究：\n\n6.1 遗传算法与大语言模型（GA + LLM）\n\n代表工作\n\n核心思路\n\n局限性（作者观点）\n\nLehman et al. 2022；Lange et al. 2024\n\n把 LLM 当作进化策略或变异算子，用于代码/提示优化\n\n仅在词/句层面做交叉/变异，最终收敛到单一最优解，难以兼顾大规模文本的**质量与多样性**\n\nLiu et al. 2024（AutoDAN）\n\n将段落视为基因序列，用分层 GA 搜索最优提示前缀\n\n同样聚焦“找最好提示”，而非**生成大量多样化数据**\n\nGuo et al. 2024；Yang et al. 2024\n\nLLM 作为黑箱优化器，迭代改写句子\n\n局限于句子级扰动，难以保持整体语义分布\n\n**本文差异**：\n\n*   把“基因”从词/句级提升到**语义属性级**（如风格、结构、极性），利用 LLM 的理解能力做属性交叉/变异；\n*   用**主动学习**替代传统适应度选择，避免逐样本评估不可靠的问题。\n\n6.2 基于 LLM 的合成数据生成\n\n类别\n\n代表工作\n\n关键方法\n\n局限性（作者观点）\n\n零样本/少样本生成\n\nZeroGen (Ye et al. 2022)\n\n直接 prompt LLM 按类别生成\n\n多样性不足，容易模式化\n\n属性控制生成\n\nAttrPrompt (Yu et al. 2023)\n\n人工枚举属性值组合，再让 LLM 生成\n\n属性空间爆炸，约束过多抑制 LLM 推理\n\n知识增强\n\nKnowledge-infused prompting (Xu et al. 2024)\n\n引入外部知识图谱约束生成\n\n依赖领域知识，难以泛化\n\n多步/链式生成\n\nAda-Instruct (Cui & Wang 2024)\n\n多轮 prompt 逐步细化\n\n流程复杂，成本高\n\n数据整理\n\nCurated LLM (Seedat et al. 2024)\n\n先大规模生成再基于学习动态筛选\n\n仍受限于初始生成质量\n\n**本文差异**：\n\n*   不依赖人工枚举属性值，而是让 LLM **自动发现属性基因**；\n*   通过**遗传操作**而非显式约束，实现大规模、高多样性、低分布偏移的合成数据；\n*   在**类别不平衡场景**下优势明显，可直接生成均衡且多样的合成样本。\n\n小结\n\n*   **GA+LLM 方向**：已有工作聚焦“优化单一提示/代码”，本文聚焦“生成大规模、多样化文本”。\n*   **合成数据方向**：已有工作多用“预定义属性+约束”，本文用“属性基因+遗传操作+主动学习”，减少人工先验、提升可扩展性。\n\n**Q3**: 论文如何解决这个问题？\n\n论文通过提出 **Genetic Prompt 框架**，将遗传算法（GA）与大语言模型（LLM）深度耦合，以系统性地解决“高质量 + 高多样性”合成数据生成难题。具体实现路径可概括为以下四个互锁模块：\n\n1\\. 文本基因识别（Textual Genes Identification）\n\n*   **目标**：自动发现对任务最敏感的高层语义属性，而非人工枚举。\n*   **做法**：\n    1.  输入任务元数据 + 少量真实样本到 LLM；\n    2.  提问：“若把文本属性视为基因，哪些最重要？”\n    3.  通过人机协同筛选，得到 **无具体取值限制** 的基因集合  \n        $$G = \\\\Phi(M, S, \\\\text{Ins}\\_G)$$  \n        其中 $G$ 可包含“句子结构、实体距离、极性、可读性”等抽象维度。\n*   **收益**：避免过度约束 LLM，减少人工偏差，为后续遗传操作提供语义单元。\n\n2\\. 主动父代选择（Active Parents Selection）\n\n*   **目标**：克服逐样本“适应度”评估不可靠的问题，同时扩大后代搜索空间。\n*   **做法**：\n    *   用 Sentence Transformer 编码当前种群，计算语义表示 $E$；\n    *   每轮从尚未使用的样本中，选取 **语义欧氏距离最大** 的一对  \n        $$(p\\_i^_, p\\_j^_) = \\\\arg\\\\max\\_{e\\_i,e\\_j \\\\in E} \\\\text{dist}(e\\_i, e\\_j)$$\n*   **收益**：确保父代差异最大化，提升交叉后代的多样性与探索性。\n\n3\\. 交叉与变异（Crossover & Mutation）\n\n*   **目标**：在属性层面而非词/句层面进行遗传操作，兼顾内容保真与多样性。\n*   **交叉**：\n    *   将基因集 $G$ 随机划分为 $G\\_1,G\\_2,G\\_3$；\n    *   $G\\_1$ 继承自父本 $p\\_i^_$，$G\\_2$ 继承自父本 $p\\_j^_$，形成语义层面的“杂交”。\n*   **变异**：\n    *   对 $G\\_3$ 中的基因进行语义级扰动，如改变实体关系极性、句子功能角色等；\n    *   保持整体任务相关性，避免词级替换带来的表面多样性。\n*   **实现**：  \n    通过 LLM 的上下文指令一次性完成交叉+变异，输出新样本  \n    ${% raw %}$D\\_{\\\\text{syn}} \\\\sim \\\\rho\\_l\\\\bigl(\\\\cdot,;, \\\\text{Ins}\\_l^{\\\\text{GA}}, (p\\_i^_, p\\_j^_), G\\\\bigr).${% endraw %}$\n\n4\\. 下游训练与数据融合（Downstream Training & Synthetic-Gold Fusion）\n\n*   **训练**：用合成数据 {% raw %}$D\\_{\\\\text{syn}}${% endraw %} 微调 RoBERTa/T5 等下游模型，统一学习率，无特殊 tricks。\n*   **融合**：将合成数据与原始训练集 **1:1 混合**，在类别不平衡场景下显著提升 macro-F1；  \n    例如 ChemProt：+3.2 % macro-F1 vs +2.3 % micro-F1，验证了对少数类的增益。\n\n5\\. 关键设计总结\n\n传统难点\n\nGenetic Prompt 对策\n\n逐样本适应度难评估\n\n用主动学习选“语义最远”父代，无需显式适应度\n\n词/句级扰动多样性有限\n\n提升到语义属性级交叉+变异\n\n人工预定义属性值空间爆炸\n\n仅定义抽象基因，取值由 LLM 在生成时自由组合\n\n类别不平衡\n\n遗传过程天然可生成任意比例的多样样本，直接平衡数据集\n\n通过上述四步闭环，Genetic Prompt 在 8 个数据集、3 类 NLP 任务上均优于 SimPrompt、AttrPrompt、Curated LLM 等基线，并在消融实验中验证了“主动选择 + 变异 + 语义基因”三者缺一不可。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **数据质量、下游任务、数据融合、消融分析、规模效应** 五个维度，设计了系统实验，覆盖 8 个公开数据集、3 类 NLP 任务、4 种 LLM 生成器。具体实验一览如下：\n\n1\\. 数据集与任务\n\n任务\n\n数据集\n\n规模/类别\n\n特点\n\n文本分类\n\nAGNews、StackExchange\n\n12 万 / 2.7 万；4 类 / 50 类\n\n新闻、技术问答\n\n关系抽取\n\nChemProt、DDI、SemEval2010、CoNLL04\n\n1k–7k；4–9 类\n\n生物医学、药物交互\n\n文本摘要\n\nSciTLDR、MeQSum\n\n2k–23k；单任务\n\n科学论文、医学问题\n\n2\\. 实验设置\n\n*   **生成器**：Phi-4、Llama-3.1-70B、GPT-3.5-turbo、GPT-4o\n*   **下游模型**：RoBERTa-base（分类）、T5-large（摘要）\n*   **基线**：SimPrompt、AttrPrompt、Curated LLM\n*   **统一控制**：各方法使用相同的 2-shot 真实样本初始化，生成等量合成数据\n\n3\\. 四大核心实验\n\n3.1 内在质量评估（Intrinsic Analysis）\n\n*   **指标**：\n    *   APS ↓（平均成对相似度）\n    *   CMD ↓（Central Moment Discrepancy，与真实分布距离）\n    *   Vocabulary Size ↑（词汇多样性）\n*   **结果**：\n    *   8 个数据集中，Genetic Prompt 在 APS、CMD、Vocab 均优于基线；\n    *   ChemProt、DDI、CoNLL04 的合成词汇量甚至 **超过真实数据**，显示语义/词汇双重增强。\n\n3.2 下游任务表现（Extrinsic Evaluation）\n\n*   **指标**：Micro-F1（分类）、Rouge-L（摘要）\n*   **结果**：\n    *   8 个数据集全部领先，平均提升 1.8–3.4 pp；\n    *   在 **类别极度不平衡** 的 StackExchange（IR=1283）上仍保持稳健；\n    *   生成器越大，Genetic Prompt 提升越明显，而基线常出现波动或下降。\n\n3.3 合成-真实数据融合（Synthetic-Gold Fusion）\n\n*   **协议**：1 : 1 混合训练\n*   **结果**：\n    *   平均 Micro-F1 再提升 1.85 pp；\n    *   **Macro-F1 提升更显著**：ChemProt +3.2 pp、DDI +3.1 pp、StackExchange +3.7 pp，验证了对少数类的增益；\n    *   摘要任务中，仅 Genetic Prompt 带来 Rouge-L 正向提升（SciTLDR +0.5）。\n\n3.4 消融实验（Ablation Study）\n\n变体\n\n主要发现\n\nw/o Active Learning\n\n随机父代 → 语义空间塌陷，性能平均下降 2–4 pp\n\nw/o Mutation\n\n仅交叉 → 多样性不足，ChemProt F1 掉 3.2 pp\n\nWord-as-Gene\n\n词级基因 → 语义信息丢失，平均掉 8–12 pp\n\n4\\. 规模效应分析（Effects of Generator Size & Data Scale）\n\n4.1 生成器规模\n\n*   **结论**：\n    *   任务敏感度不同：关系抽取（ChemProt）随模型增大显著提升；新闻分类（AGNews）在 70B 时各方法趋同；\n    *   Genetic Prompt 在所有规模上 **单调提升**，鲁棒性优于基线。\n\n4.2 合成数据规模\n\n*   **结论**：\n    *   Genetic Prompt 随数据量增加持续改进，ChemProt 在 2k 后仍上升，而基线 1–2k 即出现 **性能平台或下降**；\n    *   不同数据集的最优合成规模差异显著（AGNews≈4k，ChemProt≈2k），提示任务特性决定数据需求。\n\n5\\. 案例研究（Case Study）\n\n*   **展示**：SemEval Cause-Effect 类别示例\n*   **观察**：\n    *   SimPrompt/Curated LLM 多用显式因果词（cause, lead to）；\n    *   Genetic Prompt 通过上下文隐含因果关系，句式更多变，验证其多样性优势。\n\n6\\. 补充实验\n\n*   **附录 A**：完整 Micro/Macro-F1、Rouge-1 结果（表 6–9）\n*   **附录图 3–4**：生成器规模与数据规模的详细曲线\n\n综上，论文通过 **内在指标 + 下游任务 + 融合增益 + 消融 + 规模曲线 + 案例分析** 的六重验证，全面展示了 Genetic Prompt 在质量、多样性、鲁棒性、可扩展性上的优势。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下潜在方向可直接从 Genetic Prompt 框架延伸，分为 **方法改进、跨模态与语言扩展、理论与评估、应用场景** 四大类，供后续研究参考。\n\n1\\. 方法改进\n\n*   **动态基因库**  \n    当前基因集合 $G$ 在首轮固定。可引入 **演化式基因发现**：每若干代让 LLM 重新评估基因重要性，动态增删基因，以应对任务漂移或领域迁移。\n    \n*   **多目标主动学习**  \n    父代选择仅考虑语义距离。可加入 **质量-多样性双目标**（如同时最大化距离与最小化 APS），使用 Pareto 前沿或加权评分，避免单纯远距导致低质样本。\n    \n*   **自适应变异强度**  \n    变异目前对 $G\\_3$ 随机扰动。可让 LLM 依据当前种群多样性自动调整变异幅度（temperature、top-p），实现 **自适应探索-利用平衡**。\n    \n*   **层级遗传操作**  \n    在段落-句子-属性三层同时做交叉/变异，形成 **层级 GA**，进一步提升长文本生成的结构一致性。\n    \n\n2\\. 跨模态与语言扩展\n\n*   **多语言 Genetic Prompt**  \n    在非英语语料上验证基因定义与遗传操作是否跨语言迁移；探索 **语言特定基因**（如敬语、时态体系）对合成质量的影响。\n    \n*   **多模态基因**  \n    将框架扩展到 **文本-表格** 或 **文本-图像** 合成：\n    \n    *   表格基因：数值分布、列间依赖、缺失率；\n    *   图像基因：物体位置、风格、光照。  \n        需设计跨模态交叉/变异提示，并评估下游多模态模型收益。\n*   **语音与代码**  \n    把“基因”概念应用于 **语音合成**（音色、语速、情感）或 **代码生成**（API 使用模式、控制流结构），验证通用性。\n    \n\n3\\. 理论与评估\n\n*   **收敛性与多样性理论分析**  \n    建立遗传算子在语义空间的 Markov 模型，给出 **多样性保持上界** 与 **收敛率**，为参数选择提供理论依据。\n    \n*   **更鲁棒的分布相似度指标**  \n    除 CMD 外，引入 **Sinkhorn 距离** 或 **神经最优传输**，更敏感地捕捉高阶分布差异；同时评估指标与下游性能的相关性。\n    \n*   **人类评估与对抗检测**  \n    组织人工盲评与对抗判别器实验，量化合成数据 **“人感”** 与 **“机器可检测性”**，检验真实应用场景的隐蔽性。\n    \n\n4\\. 应用场景\n\n*   **联邦学习与隐私计算**  \n    在联邦场景下，用 Genetic Prompt 在本地生成合成数据替代原始敏感数据上传，评估 **隐私-效用权衡**（结合差分隐私或安全聚合）。\n    \n*   **小模型课程蒸馏**  \n    将大规模合成数据按 **难度-多样性** 自动排序，形成课程，逐步蒸馏给小模型，观察收敛速度与最终精度提升。\n    \n*   **持续学习与灾难性遗忘**  \n    利用遗传算法持续生成 **新任务合成数据** 并回放缓冲，测试对旧任务的遗忘抑制效果。\n    \n*   **医疗与法律高风险领域**  \n    在罕见病病例、法律判例等 **长尾、高成本** 场景中，验证 Genetic Prompt 能否通过属性级变异生成 **合规且多样** 的训练样本，并与专家规则对齐。\n    \n\n5\\. 工具与社区贡献\n\n*   **开源生态**  \n    将基因发现、主动选择、遗传算子封装为 **可插拔库**，支持一键接入 Hugging Face `datasets` / `transformers`，降低社区使用门槛。\n    \n*   **交互式可视化平台**  \n    开发 Web 界面实时展示基因演化、父代距离、多样性曲线，帮助研究者直观调试超参数。\n    \n\n通过上述方向的深入，Genetic Prompt 可从 **单语文本生成工具** 演进为 **通用、可解释、跨模态的合成数据引擎**。\n\n**Q6**: 总结一下论文的主要内容\n\n论文一句话总结\n\n提出 **Genetic Prompt** 框架，把文本的语义属性当作“基因”，用大语言模型模拟遗传算法的交叉与变异，从而在小样本条件下生成高质量、高多样性且类别均衡的合成数据，显著提升多种 NLP 下游任务性能。\n\n核心内容速览\n\n维度\n\n关键信息\n\n**问题**\n\nLLM 合成数据常受限于人工预定义属性、多样性不足、类别不平衡，难以兼顾质量与规模。\n\n**思路**\n\n将遗传算法与 LLM 融合：  \n1\\. 让 LLM 自动发现“文本基因”（风格、结构、极性等高层属性）；  \n2\\. 用主动学习选“语义最远”的父代；  \n3\\. 在属性层面做交叉+变异生成后代；  \n4\\. 与真实数据 1:1 混合训练。\n\n**实验**\n\n在 8 个数据集（新闻、生医、问答、摘要）上对比 3 类基线：  \n\\- **内在指标**：APS、CMD、词汇量均最优；  \n\\- **下游任务**：Micro-F1 / Rouge-L 全面领先，类别不平衡场景 macro-F1 提升 3 pp 以上；  \n\\- **消融**：移除主动选择、变异或改用词级基因都会显著降性能；  \n\\- **规模效应**：随 LLM 规模与合成数据量增加，性能单调提升，基线常出现平台或下降。\n\n**贡献**\n\n首次将“语义属性基因+主动学习+遗传算子”引入 LLM 合成数据流程，兼顾质量、多样性、可扩展性，并给出完整开源实现。\n\n适用场景\n\n*   数据稀缺或类别极度不平衡的文本任务\n*   需要大规模、低成本、高保真训练语料的任何 NLP 领域\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Attributes%20as%20Textual%20Genes%3A%20Leveraging%20LLMs%20as%20Genetic%20Algorithm%20Simulators%20for%20Conditional%20Synthetic%20Data%20Generation，链接是  https://arxiv.org/pdf/2509.02040  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.02040  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-19-1756943220812","title":"FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in Diverse Adventure Games","description":"GUI agents powered by LLMs show promise in interacting with diverse digital environments. Among these, video games offer a valuable testbed due to their varied interfaces, with adventure games posing additional challenges through complex, narrative-driven interactions. Existing game benchmarks, however, lack diversity and rarely evaluate agents on completing entire storylines. To address this, we introduce FlashAdventure, a benchmark of 34 Flash-based adventure games designed to test full story arc completion and tackle the observation-behavior gap: the challenge of remembering and acting on earlier gameplay information. We also propose CUA-as-a-Judge, an automated gameplay evaluator, and COAST, an agentic framework leveraging long-term clue memory to better plan and solve sequential tasks. Experiments show current GUI agents struggle with full story arcs, while COAST improves milestone completion by bridging the observation-behavior gap. Nonetheless, a marked discrepancy between humans and best-performing agents warrants continued research efforts to narrow this divide.","url":"https://huggingface.co/papers/2509.01052","source":"HuggingFace Papers","timestamp":"2025-09-03T23:47:00.812Z","metadata":{"authors":["Jaewoo Ahn","Junseo Kim","Heeseung Yun","Jaehyeon Son","Dongmin Park","Jaewoong Cho","Gunhee Kim"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.AI","cs.CL","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.01052.pdf","arxivUrl":"https://arxiv.org/abs/2509.01052","arxivId":"2509.01052","coolPaperUrl":"https://papers.cool/arxiv/2509.01052","published":"2025-09-01T01:33:16Z","updated":"2025-09-01T01:33:16.000Z","zh_summary":"由大型语言模型驱动的图形用户界面代理在与多样化数字环境互动方面表现出希望。其中，视频游戏由于其多样的界面，提供了一个有价值的测试平台，而冒险游戏通过复杂的叙事驱动互动提出了额外的挑战。然而，现有的游戏基准缺乏多样性，很少评估代理完成整个故事情节的能力。为了解决这一问题，我们推出了FlashAdventure，这是一个由34款基于Flash的冒险游戏组成的基准，旨在测试完整故事情节的完成情况，并解决观察-行为差距的问题：记住和利用早期游戏信息的挑战。我们还提出了CUA-as-a-Judge，这是一种自动游戏评估器，和COAST，这是一个利用长期线索记忆来更好地规划和解决顺序任务的代理框架。实验表明，目前的图形用户界面代理在完整故事情节方面表现不佳，而COAST通过弥合观察-行为差距提高了里程碑的完成率。然而，人类与表现最佳的代理之间的显著差异仍然需要继续研究，以缩小这一差距。","llm_analysis":"**Q1**: 这篇论文试图解决什么问题？\n\n论文针对“图形用户界面（GUI）智能体能否在**多样化冒险游戏**中**完整通关**”这一核心问题，提出并验证了两大关键缺陷：\n\n1.  **现有游戏基准的局限性**\n    \n    *   任务与游戏多样性不足（最多12款，集中于单一任务或短流程）。\n    *   缺乏对\\*\\*完整故事线（full story arc）\\*\\*的评估，无法验证智能体对长程依赖的处理能力。\n2.  **观察-行为差距（observation-behavior gap）**\n    \n    *   冒险游戏中线索可能在数百步后才被使用，现有智能体因**记忆与规划能力薄弱**，难以跨越长时间滞后，导致无法完成全局目标。\n\n为此，论文构建了**FlashAdventure**基准（34款Flash冒险游戏）并配套**COAST框架**，旨在：\n\n*   系统评估智能体在**长流程、多类型冒险游戏**中的完整通关能力。\n*   通过**长期线索记忆与主动子任务生成**，缩小观察-行为差距，提升复杂叙事任务的解决率。\n\n**Q2**: 有哪些相关研究？\n\n与 FlashAdventure 相关的研究可划分为 **GUI 智能体** 与 **视频游戏基准** 两大主线，关键文献如下：\n\n1\\. GUI 智能体（Multimodal LLM Agents for GUI Control）\n\n*   **端到端架构**\n    *   Claude-3.7-Sonnet Computer-Use (Anthropic, 2024)\n    *   OpenAI CUA / Operator (OpenAI, 2025b)\n    *   UI-TARS-1.5-7B (Qin et al., 2025) – 开源视觉-语言-动作模型\n*   **模块化框架**\n    *   Cradle (Tan et al., ICML 2025) – 六模块闭环：感知→反思→任务推断→技能策展→动作规划→记忆\n    *   Agent S / Agent S2 (Agashe et al., ICLR 2025a; COLM 2025b) – 将 Grounding 与 Planning 解耦，引入 Mixture-of-Grounding 与层次化规划\n*   **GUI Grounding 研究**\n    *   SeeClick (Cheng et al., ACL 2024)\n    *   UGround-V1-7B (Gou et al., ICLR 2025)\n    *   OS-ATLAS (Wu et al., ICLR 2025) – 通用 GUI 基础动作模型\n\n2\\. 视频游戏基准（Game Benchmarks）\n\n基准\n\n环境接口\n\n游戏数\n\n完整故事线\n\n备注\n\n**FlashAdventure**（本文）\n\n纯像素/截图\n\n34\n\n✅\n\n首个专注完整故事线的多样化冒险游戏集合\n\nVisEscape (Lim et al., 2025)\n\nAPI + 截图\n\n1（研究定制）\n\n✅\n\n房间逃脱，平均 52.8 步，故事短\n\nCradle (Tan et al., 2025)\n\n截图\n\n4 AAA\n\n❌\n\nRDR2 等超长流程，仅局部任务\n\nBALROG (Paglieri et al., ICLR 2025)\n\nAPI + 截图\n\n12\n\n❌\n\n6 种类型 × 2 游戏，无完整故事\n\nPokeLLMon (Hu et al., 2024a)\n\nAPI\n\n1\n\n❌\n\n宝可梦战斗\n\nMineDojo (Fan et al., NeurIPS 2022)\n\nAPI + 截图\n\n1\n\n❌\n\nMinecraft 开放世界\n\nOOTB (Hu et al., 2024b)\n\n截图\n\n2 商业游戏\n\n❌\n\nHearthstone、崩铁\n\nVARP (Chen et al., 2024)\n\n截图\n\n1 AAA\n\n❌\n\n黑神话：悟空\n\n3\\. 长程记忆与规划\n\n*   **记忆机制**\n    *   检索增强：Agent S2 的 RAG 模块\n    *   总结-遗忘：Cradle 的 Self-Reflection & Memory 模块\n    *   Tolman 潜伏学习理论（Tolman & Honzik, 1930; Tolman, 1948）——人类长时线索应用的心理学依据\n\n4\\. 自动评估方法\n\n*   **CUA-as-a-Judge**（本文） – 首个无需人工、可自动验证里程碑的像素级游戏评估代理\n*   传统人工评估：Cradle、VARP、OOTB 等均依赖人工标注里程碑完成度\n\n综上，FlashAdventure 在 **游戏多样性、完整故事线评估、长程依赖建模** 三方面填补了现有基准与方法的空白。\n\n**Q3**: 论文如何解决这个问题？\n\n论文从**基准构建**、**自动评估**与**智能体框架**三条主线协同解决“GUI 智能体难以完成多样化冒险游戏完整故事线”的核心问题。\n\n1\\. 构建 FlashAdventure 基准：提供可度量的完整故事线测试床\n\n*   **34 款 Flash 冒险游戏**\n    *   涵盖 5 个子类型（悬疑/侦探、隐藏物品、密室逃脱、视觉小说、模拟经营），保证**跨类型多样性**。\n    *   每款游戏 1–2 小时内人类可通关，确保**完整故事线**在可控步数内结束。\n*   **里程碑体系**\n    *   每游戏 4–12 个离散里程碑（或连续 HUD 分数），用于**细粒度进度度量**。\n    *   平均人类步数 1 142，平均观察-行为差距 251 步，凸显**长程依赖**挑战。\n\n2\\. CUA-as-a-Judge：零人工的里程碑验证器\n\n*   **基于 Claude-3.7-Sonnet Computer-Use**\n    *   在游戏终局**直接交互界面**（点击笔记本、查看 HUD 等），**自动核对**里程碑。\n    *   300 样本验证：94 % 准确率、ρ=0.9999 Pearson 相关性，**替代人工评估**。\n\n3\\. COAST 框架：面向线索的长期记忆与规划\n\n采用 **Seek-Map-Solve 三阶段循环** 主动管理线索记忆，缩小观察-行为差距：\n\n阶段\n\n模块\n\n关键机制\n\n作用\n\n**Seek**\n\nClue Seeker\n\n每轮探索 **Nseek 步** 收集线索并存入 **M**\n\n持续积累潜在信息\n\n**Map**\n\nClue Mapper\n\n从 **M** 与轨迹中挑选 **K 对 (clue, observation)**，生成子任务\n\n建立线索→行动的映射\n\n**Solve**\n\nProblem Solver\n\n针对子任务执行 **Nsolve 步** 动作，成功即加入已解决集合 **GR**\n\n避免重复，推进故事\n\n*   **无上限线索记忆**：平均 12.8 k tokens，远低于 Claude-3.7 200 k 上下文。\n*   **实验收益**：在 34 款游戏上，COAST 将里程碑完成率从 17.11 % 提升到 19.89 %，**首次实现 5.88 % 成功率**（仍远低于人类 97.1 %）。\n\n4\\. 控制实验与消融：验证框架有效性\n\n*   **消融实验**：去掉 Mapper 后 MCR 从 65.6 % 降至 42.4 %，证明**三模块缺一不可**。\n*   **提示工程对照**：将 COAST 提示直接喂给基线模型，性能反而下降，确认**提升源于架构而非提示**。\n\n5\\. 公开资源与伦理合规\n\n*   开源游戏列表、里程碑定义、人类走查、COAST prompt，确保**可复现**。\n*   通过 Flashpoint Archive 合法访问游戏，仅用于学术研究，规避版权问题。\n\n综上，论文通过“**多样化基准 + 自动评估 + 线索驱动框架**”三位一体，系统性地将 GUI 智能体在长流程冒险游戏中的评估与能力提升向前推进。\n\n**Q4**: 论文做了哪些实验？\n\n论文围绕 **FlashAdventure 基准** 与 **COAST 框架** 设计并执行了 5 组核心实验，覆盖 **34 款游戏、7 种 GUI 智能体、300 条自动评估样本、13 位人类玩家**，系统验证：\n\n1.  现有智能体在长流程冒险游戏中的真实能力；\n2.  COAST 对“观察-行为差距”的缓解效果；\n3.  各模块的必要性、失败模式与性能上限。\n\n1\\. 主实验：34 游戏 × 7 智能体（单轮）\n\n*   **被测智能体**\n    *   端到端：Claude-3.7-Sonnet Computer-Use、OpenAI CUA、UI-TARS-1.5-7B\n    *   模块化：Cradle（×3 配置）、Agent S2\n*   **指标**\n    *   Success Rate（SR）：是否达成最终故事目标\n    *   Milestone Completion Rate（MCR）：里程碑完成比例\n    *   Steps：实际消耗步数（上限 1 000）\n*   **结果**（表 2 & 表 13）\n    *   所有基线 SR≈0 %，MCR 4.56 %–17.11 %；人类 SR 97.1 %，MCR 100 %。\n    *   仅 OpenAI CUA 与 COAST 在 2 款隐藏物品游戏上取得非零 SR（5.88 %）。\n    *   COAST 相对最强基线 MCR↑2.78 pp，验证其有效性但仍远逊人类。\n\n2\\. 子类型对比实验\n\n*   **按 5 个子类型**（悬疑/侦探、隐藏物品、密室逃脱、视觉小说、模拟经营）汇总 MCR。\n*   **发现**（图 4）：\n    *   视觉小说因观察-行为差距小，COAST 提升有限；\n    *   悬疑/密室类差距大，COAST 优势显著。\n\n3\\. 消融实验：Seek / Map / Solver 三模块\n\n*   **5 款代表性游戏**（Sherlock Holmes 2、Grim Tales: The Bride 等）\n*   **配置**\n    *   Seeker only\n    *   Seeker + Solver\n    *   完整 COAST\n*   **结果**（表 3）\n    *   完整框架 MCR 65.6 % > Seek+Solver 57.9 % > Seeker only 42.4 %，证明 **Mapper 是关键瓶颈**。\n\n4\\. 失败模式与缓解分析\n\n*   **人工回放 5 款游戏**，将错误归类：\n    1.  规划薄弱 2. 视觉感知差 3. 横向思维不足 4. 资源管理低效\n*   **结果**（表 4）\n    *   COAST 显著减少 1、3 类错误（规划 & 横向思维）；\n    *   对 2、4 类（感知 & 资源管理）改善有限。\n\n5\\. 控制与上限实验\n\n实验\n\n目的\n\n设置\n\n关键结论\n\n**多轮实验**\n\n稳定性\n\n3 次重复运行 5 游戏\n\n趋势与单轮一致，COAST 仍领先\n\n**提示工程对照**\n\n排除提示增益\n\n将 COAST 提示直接给 Claude-3.7 Computer-Use\n\n性能略降，证明架构而非提示带来提升\n\n**Hint 注入上限**\n\n评估任务复杂度\n\n在 Sherlock Holmes 与 Office Escape 中每 50–100 步注入子任务\n\n结构化提示可让前者 758 步通关，后者仍失败，说明**空间-逻辑推理瓶颈独立于提示**\n\n**污染检测**\n\n排除记忆化\n\n10 道游戏情境问答\n\nGPT-4o 3/10 题泄露，Claude-3.7-Sonnet 0/10，但泄露并未转化为游戏优势（knowing-doing gap）\n\n**o4-mini 替换**\n\n推理模型效果\n\n用 o4-mini+UGround+Cradle\n\n0–2 里程碑，未超越 GPT-4o，显示**高阶推理≠有效动作规划**\n\n6\\. 人类基线\n\n*   **13 位受试者**通关全部 34 游戏，平均 1 142 步、26 分钟、97.1 % SR，提供 **可达成上限** 与 **观察-行为差距统计**（平均 251 步）。\n\n实验规模一览\n\n维度\n\n数量\n\n游戏\n\n34\n\n智能体\n\n7\n\n单轮评估轨迹\n\n34×7 = 238\n\n自动评估样本（CUA-as-a-Judge）\n\n300\n\n人类轨迹\n\n13×34 = 442\n\n消融/控制实验轨迹\n\n≈ 60\n\n通过上述实验，论文既给出了当前 GUI 智能体在长流程冒险游戏中的**全景性能画像**，也验证了 COAST 框架在**规划与横向思维**维度的有效性，同时指出**感知精度、资源管理、空间推理**仍是主要瓶颈。\n\n**Q5**: 有什么可以进一步探索的点？\n\n以下方向可作为 FlashAdventure 之后可立即着手、且对社区有价值的进一步探索，按优先级与可行性排序：\n\n1\\. 记忆与规划机制的深度优化\n\n*   **层级记忆架构**  \n    在 COAST 的扁平线索记忆之上引入 **Episodic → Semantic → Procedural** 三级存储，利用检索-压缩-遗忘策略（Tan et al., 2025；Park et al., 2023）缓解长程步数增加时的上下文爆炸。\n*   **可学习子任务生成器**  \n    将 Mapper 模块替换为 **轻量级策略网络**（如 LoRA-微调 LLM），直接从轨迹-线索对中生成子任务，减少手工提示依赖，实现跨游戏迁移。\n\n2\\. 感知-动作耦合的增强\n\n*   **视觉-语言混合 Grounding**  \n    结合 **UGround-V2** 与 **SAM-2** 的像素级掩码，先定位可交互区域，再由 LLM 决策动作，降低非标准布局下的误点率。\n*   **自监督视觉预训练**  \n    用 FlashAdventure 34 游戏的大规模回放数据构造 **“动作-下一帧”** 自监督任务，预训练 GUI 视觉编码器，提升隐藏对象、微小线索的识别精度。\n\n3\\. 通用化评估协议\n\n*   **时间-事件混合里程碑**  \n    利用 **叙事流程图（Paschali et al., 2018）** 自动生成里程碑，摆脱人工定义，使基准可扩展至 RPG、动作等新类型。\n*   **实时-暂停双模式 CUA-as-a-Judge**  \n    针对动作或节奏游戏，扩展 Judge 支持 **暂停-查询-继续** 协议，在保持帧级精度的同时评估高速场景。\n\n4\\. 低成本可复现性\n\n*   **开源轻量级 GUI 智能体**  \n    以 **UI-TARS-1.5-7B** 为骨干，集成 COAST 三模块并蒸馏至 3B 以下，实现本地 GPU 运行，降低 API 费用壁垒。\n*   **社区排行榜与持续集成**  \n    建立公开排行榜（类似 MLPerf），支持 PR 式提交；利用 GitHub Actions 每晚跑 3 款游戏回归测试，确保新 commit 不破坏性能。\n\n5\\. 人机协同与可解释性\n\n*   **实时解释面板**  \n    在浏览器层叠加 **“Clue Heatmap”** 与 **“Subtask Trace”**，实时显示智能体当前关注线索与下一步计划，便于人类监督与调试。\n*   **交互式提示注入**  \n    允许人类在运行时通过自然语言向 Mapper 注入 **“假设”**，观察智能体如何调整子任务，研究人机协同的互补边界。\n\n6\\. 跨模态迁移与多语言游戏\n\n*   **非英语 Flash 游戏扩展**  \n    引入日语、西班牙语 Flash 冒险游戏，测试多语言 OCR + 翻译链路对 GUI 智能体的影响。\n*   **跨平台迁移**  \n    将 COAST 迁移至 **Android 冒险手游**（截图+触控），验证框架在移动 UI 元素密度与手势空间下的鲁棒性。\n\n7\\. 强化学习与 LLM 的混合训练\n\n*   **离线 RL + 在线微调**  \n    先用 FlashAdventure 人类轨迹做 **离线 RL（Decision Transformer 或 Q-learning）** 预训练，再在游戏内做 **少量在线 RL** 微调，观察样本效率与稳定性。\n*   **奖励塑形**  \n    将里程碑奖励 R\\_m 作为稠密奖励信号，训练端到端策略网络，对比 COAST 的模块化方案，量化“规划-执行”解耦的收益。\n\n8\\. 社会与伦理维度\n\n*   **公平性审计**  \n    分析不同性别/文化背景游戏中智能体表现差异，检查训练数据或模型偏差。\n*   **版权合规扩展**  \n    与更多 Flash 游戏版权方签署学术使用协议，建立 **“可商用许可” 子集**，推动工业界采用。\n\n快速起步建议（3 个月可交付）\n\n1.  开源 **COAST-Lite**（3B 参数级）并在 Hugging Face 发布。\n2.  用流程图自动生成 10 款新游戏里程碑，验证协议通用性。\n3.  建立 **FlashAdventure-Leaderboard** 网站，支持在线提交与实时排行榜。\n\n这些方向既可直接利用现有数据与框架，又能逐步扩展到更复杂的游戏形态与真实应用场景。\n\n**Q6**: 总结一下论文的主要内容\n\n**一句话总结**  \nFlashAdventure 通过 34 款 Flash 冒险游戏、自动里程碑验证器 CUA-as-a-Judge 以及线索驱动的 COAST 框架，首次系统评估并部分缓解了 GUI 智能体在长流程故事任务中的“观察-行为差距”。\n\n1\\. 问题与动机\n\n*   **现有游戏基准**  \n    游戏少（≤12）、任务短、缺完整故事线，无法验证长程依赖。\n*   **观察-行为差距**  \n    冒险游戏中线索与使用之间常隔数百步，现有智能体因记忆/规划薄弱而失败。\n\n2\\. FlashAdventure 基准\n\n*   **34 款 Flash 冒险游戏**  \n    5 子类型（悬疑、隐藏物品、密室、视觉小说、模拟），平均人类 1 142 步通关。\n*   **里程碑体系**  \n    每游戏 4–12 个人工定义里程碑 + 连续分数，用于细粒度进度度量。\n*   **人类基线**  \n    13 位受试者 97.1 % 成功率，平均观察-行为差距 251 步。\n\n3\\. CUA-as-a-Judge 自动评估\n\n*   **基于 Claude-3.7-Sonnet Computer-Use**  \n    终局直接交互界面验证里程碑，300 样本对比人工：94 % 准确率，ρ=0.9999。\n*   **零人工标注**  \n    解决像素级游戏评估长期依赖人工的问题。\n\n4\\. COAST 框架\n\n**Seek-Map-Solve 循环**\n\n1.  Seek：探索 Nseek 步收集线索 → 存入长期记忆 M。\n2.  Map：从 M 与轨迹中生成 K 个子任务（线索-观察对）。\n3.  Solve：执行 Nsolve 步完成子任务，避免重复。\n\n*   **实验结果**  \n    在 34 游戏、1 000 步限制下，COAST 将最佳基线 MCR 从 17.11 % 提至 19.89 %，成功率达 5.88 %（仍远低于人类 97.1 %）。\n*   **消融**  \n    缺 Mapper 时 MCR 降至 42.4 %，证明三模块缺一不可。\n\n5\\. 发现与展望\n\n*   **三大失败模式**：规划薄弱、视觉感知差、横向思维不足。\n*   **未来方向**：层级记忆、可学习子任务生成、跨平台迁移、低成本开源实现。\n\n**Q7**: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是FlashAdventure%3A%20A%20Benchmark%20for%20GUI%20Agents%20Solving%20Full%20Story%20Arcs%20in%20Diverse%20Adventure%20Games，链接是  https://arxiv.org/pdf/2509.01052  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2509.01052  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}