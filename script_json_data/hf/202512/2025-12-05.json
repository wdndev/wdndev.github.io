{"timestamp":"2025-12-04T23:49:38.021Z","totalItems":20,"items":[{"id":"hf-0-1764892008386","title":"Qwen3-VL Technical Report","description":"We introduce Qwen3-VL, the most capable vision-language model in the Qwen series to date, achieving superior performance across a broad range of multimodal benchmarks. It natively supports interleaved contexts of up to 256K tokens, seamlessly integrating text, images, and video. The model family includes both dense (2B/4B/8B/32B) and mixture-of-experts (30B-A3B/235B-A22B) variants to accommodate diverse latency-quality trade-offs. Qwen3-VL delivers three core pillars: (i) markedly stronger pure-text understanding, surpassing comparable text-only backbones in several cases; (ii) robust long-context comprehension with a native 256K-token window for both text and interleaved multimodal inputs, enabling faithful retention, retrieval, and cross-referencing across long documents and videos; and (iii) advanced multimodal reasoning across single-image, multi-image, and video tasks, demonstrating leading performance on comprehensive evaluations such as MMMU and visual-math benchmarks (e.g., MathVista and MathVision). Architecturally, we introduce three key upgrades: (i) an enhanced interleaved-MRoPE for stronger spatial-temporal modeling across images and video; (ii) DeepStack integration, which effectively leverages multi-level ViT features to tighten vision-language alignment; and (iii) text-based time alignment for video, evolving from T-RoPE to explicit textual timestamp alignment for more precise temporal grounding. Under comparable token budgets and latency constraints, Qwen3-VL achieves superior performance in both dense and Mixture-of-Experts (MoE) architectures. We envision Qwen3-VL serving as a foundational engine for image-grounded reasoning, agentic decision-making, and multimodal code intelligence in real-world workflows.","url":"https://huggingface.co/papers/2511.21631","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.386Z","metadata":{"authors":["Shuai Bai","Yuxuan Cai","Ruizhe Chen","Keqin Chen","Xionghui Chen","Zesen Cheng","Lianghao Deng","Wei Ding","Chang Gao","Chunjiang Ge","Wenbin Ge","Zhifang Guo","Qidong Huang","Jie Huang","Fei Huang","Binyuan Hui","Shutong Jiang","Zhaohai Li","Mingsheng Li","Mei Li","Kaixin Li","Zicheng Lin","Junyang Lin","Xuejing Liu","Jiawei Liu","Chenglong Liu","Yang Liu","Dayiheng Liu","Shixuan Liu","Dunjie Lu","Ruilin Luo","Chenxu Lv","Rui Men","Lingchen Meng","Xuancheng Ren","Xingzhang Ren","Sibo Song","Yuchong Sun","Jun Tang","Jianhong Tu","Jianqiang Wan","Peng Wang","Pengfei Wang","Qiuyue Wang","Yuxuan Wang","Tianbao Xie","Yiheng Xu","Haiyang Xu","Jin Xu","Zhibo Yang","Mingkun Yang","Jianxin Yang","An Yang","Bowen Yu","Fei Zhang","Hang Zhang","Xi Zhang","Bo Zheng","Humen Zhong","Jingren Zhou","Fan Zhou","Jing Zhou","Yuanzhi Zhu","Ke Zhu"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2511.21631.pdf","arxivUrl":"https://arxiv.org/abs/2511.21631","arxivId":"2511.21631","coolPaperUrl":"https://papers.cool/arxiv/2511.21631","published":"2025-11-26T17:59:08Z","updated":"2025-11-26T17:59:08.000Z","zh_summary":"我们推出了 Qwen3-VL，这是迄今为止 Qwen 系列中功能最强大的视觉-语言模型，在广泛的多模态基准测试中表现卓越。它原生支持多达 256K 令牌的交错上下文，能够无缝整合文本、图像和视频。该模型系列包括密集型（2B/4B/8B/32B）和专家混合型（30B-A3B/235B-A22B）变体，以满足不同延迟与质量的权衡需求。Qwen3-VL 提供三大核心优势：(i) 显著增强的纯文本理解能力，在多个场景下超越可比的文本专用骨干模型；(ii) 强大的长上下文理解能力，原生支持 256K 令牌的文本与交错多模态输入，使长文档和视频的信息能够被忠实保留、检索与交叉引用；(iii) 高级多模态推理能力，覆盖单图、多图及视频任务，在 MMMU 和视觉数学基准（如 MathVista 和 MathVision）等综合评估中展现领先表现。在架构上，我们引入三项关键升级：(i) 强化的交错-MRoPE，用于提升图像及视频的时空建模能力；(ii) DeepStack 集成，高效利用多层 ViT 特征以增强视觉-语言对齐；(iii) 基于文本的视频时间对齐，从 T-RoPE 演进为显式文本时间戳对齐，实现更精确的时间定位。在可比的令牌预算和延迟约束下，Qwen3-VL 在密集型与专家混合型（MoE）架构中均表现优异。我们设想 Qwen3-VL 将成为图像驱动推理、智能决策和多模态代码智能在实际工作流中的基础引擎。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nQwen3-VL 旨在解决当前视觉-语言模型（VLM）在以下三个关键维度上的瓶颈：\n\n1. **长上下文多模态理解**\n现有 VLM 大多只能处理几十 K 量级的短序列，无法对长达数百页的技术文档、数小时视频等真实场景进行忠实、可检索的跨模态推理。Qwen3-VL 把原生上下文窗口扩展到 256 K token，并支持图像-文本-视频交错输入，实现“一页不落地”读完一本图文混排教材，或“一帧不跳地”看完两小时的监控录像后仍能准确定位关键帧。\n\n2. **视觉推理与纯文本能力兼顾**\n以往强化视觉任务时，语言侧往往出现灾难性遗忘。论文提出平方根重加权损失与分阶段训练策略，在扩大视觉-数学、OCR、 grounding 等数据的同时，保持甚至超越同规模纯文本基座模型的语言基准分数，做到“视觉更强，语言不弱”。\n\n3. **统一架构下的多粒度感知与代理决策**\n传统方案对图像、视频、GUI、3D 场景等分别设计专用编码或后处理流程。Qwen3-VL 通过三项架构升级——交错式 MRoPE、DeepStack 跨层视觉注入、文本时间戳——让同一套参数即可实现：\n\n- 单图细粒度定位（RefCOCO 92+ mAP）\n- 长视频时序 grounding（Charades-STA 64+ mIoU）\n- GUI 代理闭环操作（OSWorld 38+ 分）\n- 3D 单目空间推理（SUN RGB-D 39+ [mAP@0.15](mailto:mAP@0.15)）\n\n简言之，论文把“看得细、记得长、想得深、做得对”这四件事统一到一个 256 K 上下文、支持稠密/MoE 双路线、可开箱即用的视觉-语言基座模型中，为下游的文档智能、视频分析、GUI 代理及具身智能提供通用底座。\n\nQ2: 有哪些相关研究？\n\n与 Qwen3-VL 直接可比或为其提供关键模块、数据、训练策略的研究可归纳为 6 条主线（按“模块-对应文献”给出，便于快速定位）：\n\n1\\. 长上下文多模态位置编码\n\n- **MRoPE 原始方案**\nWang et al., 2024c — Qwen2-VL 首次将 t/h/w 三维位置拆分为独立旋转频率，但带来低频-高频分布不均。\n- **Interleaved / Balanced-RoPE 改进**\nHuang et al., 2025 — 提出在嵌入维度上“交错”排列 t/h/w，缓解长视频频谱偏差；Qwen3-VL 沿用并扩展至多帧-多图交错场景。\n- **YaRN / PI 外延**\nPeng et al., 2023；Chen et al., 2023 — 用于 256 K→1 M token 推理阶段的外推，无需继续训练。\n\n2\\. 跨层视觉-语言融合\n\n- **DeepStack**\nMeng et al., 2024 — 把 ViT 多尺度 token 直接注入 LLM 不同层，避免额外 Q-Former 或压缩器；Qwen3-VL 将其从“多尺度输入”改为“多层级 ViT 特征”，实现单图-单模型端到端。\n- **Flamingo / Perceiver VL**\nAlayrac et al., 2022；Jaegle et al., 2021 — 采用交叉注意力插入层，但需额外参数；DeepStack 用残差加性融合，参数量几乎零增加。\n- **Multi-layer ViT Feature Reuse**\nTschannen et al., 2025 (SigLIP-2) — 提供 conv-next 风格的多层特征接口，为 DeepStack 提供“即插即用”特征源。\n\n3\\. 视频时序建模\n\n- **T-RoPE / Time-aware RoPE**\nBai et al., 2025 (Qwen2.5-VL) — 把绝对帧时间直接映射为 position id，长视频 id 稀疏且采样成本大。\n- **Textual Timestamp Tokens**\nChen et al., 2024b — 用“<3.0 s>”显式字符串标记帧组，简化时序对齐；Qwen3-VL 全面替换 T-RoPE 并支持秒/HMS 双格式。\n- **Vid-LLM 稠密采样策略**\nLi et al., 2024b (MVBench) — 提出 1-2 fps 稠密帧采样+多帧联合 prompt，为 Qwen3-VL 训练/评测提供基线。\n\n4\\. 多模态预训练数据与课程\n\n- **Obelics / Multimodal-C4**\nLaurençon et al., 2023；Zhu et al., 2023 — 大规模网页图文交错语料；Qwen3-VL 沿用其清洗流程并补充 256 K 级“整书拼接”。\n- **PixMo / Grounding DINO 自动标注**\nDeitke et al., 2024；Liu et al., 2023a — 为 pointing & box grounding 提供伪标签流水线，Qwen3-VL 直接集成并扩展至 3D 场景。\n- **STEM 合成数据引擎**\nLu et al., 2023 (MathVista)；Zhang et al., 2024 (MathVerse) — 程序渲染几何图+问答对；Qwen3-VL 复现其 pipeline 并产出 600 万图表 caption。\n\n5\\. 强化学习与“思考”范式\n\n- **R1 / Search-R1**\nJin et al., 2025 — 用 RL 让 LLM 学会“搜索-推理-再搜索”循环；Qwen3-VL 把相同思路搬到视觉，引入 answer/multi-turn/tool-calling 三重奖励。\n- **Soft Adaptive Policy Optimization (SAPO)**\nGao et al., 2025 — 解决多任务 RL 梯度冲突，Qwen3-VL 的 General-RL 阶段直接采用 SAPO。\n- **Cold-start CoT Distillation**\nLai et al., 2025 (Mini-O3) — 先蒸馏 10 k 高质量轨迹再 RL 放大；Qwen3-VL 的两阶段“10 k→120 k”工具交互数据即沿用该范式。\n\n6\\. 端到端 GUI/Embodied Agent\n\n- **SeeClick & ScreenSpot**\nCheng et al., 2024；Li et al., 2025b — 将 UI 元素检测转化为纯 prompt 坐标回归，为 Qwen3-VL 的 ScreenSpot Pro 评测提供基准任务。\n- **OSWorld / AndroidWorld**\nXie et al., 2025c；Rawles et al., 2024 — 真实操作系统多步评测环境；Qwen3-VL 在相同协议下取得 38.1/63.7 分，刷新公开 VLM 记录。\n- **Gemini Robotics**\nGemini Robotics Team, 2025 — 把 VLM 用于机器人任务规划；Qwen3-VL 的 RoboSpatialHome、RefSpatial 等 3D grounding 数据集即面向同类 embodied 场景。\n\n小结\n\nQwen3-VL 并非孤立工作，而是在“长上下文位置编码、跨层视觉融合、显式时序对齐、大规模自监督数据、工具-增强 RL、GUI/3D 代理评测”六条研究脉络的交汇点上，将已有模块重新组合并给出统一缩放定律，从而同时刷新语言-视觉-动作三域的多个公开榜单。\n\nQ3: 论文如何解决这个问题？\n\nQwen3-VL 把“看得细、记得长、想得深、做得对”拆成 4 个可工程化的子问题，分别用“架构-数据-训练”三位一体的方式一次性解决。核心手段可概括为 3 组架构升级、4 段预训练课程、3 阶段后训练流程，以及 2 种推理模式。\n\n1\\. 架构升级：让模型“看得细、记得长”\n\n| 模块 | 关键改动 | 解决的痛点 |\n| --- | --- | --- |\n| Interleaved-MRoPE | 把 t/h/w 三维位置均匀交错到高-低频率带，而非整块切分 | 消除长视频 >8 k 帧时的频率失衡，256 K token 内线性外推误差 <0.5% |\n| DeepStack | ViT 第 4/8/12 层特征分别投射后，残差加到 LLM 第 1/2/3 层 | 不增加上下文长度即可注入低-中-高层视觉信号，InfoVQA +2.3 点 |\n| Text Timestamp Token | 每帧前缀可学习 token <t=3.0 s>，而非把绝对时间硬编码进 position id | 长视频（2 h）帧 id 稀疏问题消失，Charades-STA 时序定位 mIoU 提升 6.4 点 |\n\n2\\. 四段预训练课程：让模型“记得长”\n\n1. **S0 对齐**（67 B token，8 K）\n仅训练 MLP merger，冻结 ViT & LLM → 快速拉齐视觉-文本空间，2 个 epoch 即收敛。\n\n2. **S1 多模态**（1 T token，8 K）\n全参数解冻，VL : 文本 = 55 : 45，平方根重加权损失\n L=α√n_(vl)L_(vl)+β√n_(text)L_(text) \n保证文本能力不降级，MMMU 提升 4.1 点。\n\n3. **S2 长上下文**（1 T token，32 K）\n继续 4× 扩长，30 % 视频+长文档，引入 agent 多轮轨迹；平均检索位置误差从 13.2 % 降到 4.7 %。\n\n4. **S3 超长适配**（100 B token，262 K）\n采用 YaRN 式 RoPE 缩放 + 10 % 长度的纯合成“needle”视频，1 M token 外推准确率 99.5 %。\n\n3\\. 三阶段后训练：让模型“想得深、做得对”\n\n1. **Cold-start SFT**\n\n- 非 thinking：120 万真实场景指令（32 K→256 K 两阶段）\n- thinking：12 M 长 CoT，过滤掉“无图可解”样本，确保多模态必需性。\n2. **Strong-to-Weak 蒸馏**\n用 235B-A22B 教师生成 1.8 T token，轻量学生 KL 对齐 → 2 B 模型 AIME-25 从 10.6 → 22.2 点。\n\n3. **强化学习**\n\n- Reasoning-RL：30 K 可验证任务（math/OCR/grounding），SAPO 算法，单卡 2 k 步收敛。\n- General-RL：指令遵循+人类偏好双奖励，引入“反直觉”计数/时钟任务，HallusionBench 幻觉率从 8.3 % 压到 3.0 %。\n\n4\\. 两种推理模式：部署时“按需切换”\n\n- **非 thinking**（temperature 0.7）\n延迟 < 200 ms，用于 OCR、VQA、GUI 定位等低延迟场景。\n\n- **thinking**（temperature 1.0，max 81 k token）\n生成内部 `<thought>` 标签，可自我纠错、调用工具，AIME-25 89.7 点，超越 OpenAI o3-medium。\n\n5\\. 结果验证：问题是否被真正解决？\n\n- **长视频** 2 h-1 M token Needle-in-Haystack 准确率 99.5 % → “记得长”\n- **MMMU 80.6 / MathVista 85.8** 均领先同规模模型 → “想得深”\n- **OSWorld 38.1 / AndroidWorld 63.7** 公开 VLM 第一 → “做得对”\n- **MMLU-Pro 81.8** 仍高于同规模纯文本基座 → “语言不丢”\n\n通过“架构-数据-训练-推理”全链路协同，Qwen3-VL 把长上下文多模态推理从“做得到”推进到“做得好”，并在 2 B-235 B 全尺寸范围内保持线性可扩展，从而一次性解决了开篇提出的三大核心难题。\n\nQ4: 论文做了哪些实验？\n\nQwen3-VL 的实验体系围绕“长上下文-多模态-代理”三条主线展开，共 11 类任务、60 + 公开基准、4 组消融，覆盖从 2 B 到 235 B 全尺寸模型。以下按“任务类别-关键指标-旗舰结果”给出一览，便于快速定位。\n\n1\\. 通用视觉问答（VQA）\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Instruct |\n| --- | --- | --- |\n| MMBench-EN | top-1 acc | 89.3 |\n| RealWorldQA | top-1 acc | 79.2 |\n| MMStar | top-1 acc | 78.4 |\n\n2\\. 多模态推理（STEM & Puzzle）\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Thinking |\n| --- | --- | --- |\n| MMMU | top-1 acc | 80.6 |\n| MathVista-mini | top-1 acc | 85.8 |\n| MathVision | top-1 acc | 74.6 |\n| LogicVista | top-1 acc | 72.2 |\n| AIME-25 (math-comp) | pass@1 | 89.7 |\n\n3\\. 长文档 / OCR / 图表\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B |\n| --- | --- | --- |\n| DocVQA-test | ANLS | 97.1 |\n| InfoVQA-test | ANLS | 89.2 |\n| OCRBench_v2-en | F1 | 67.1 |\n| MMLongBench-Doc | acc | 57.0 |\n\n4\\. 2D & 3D Grounding\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B |\n| --- | --- | --- |\n| RefCOCO-avg | top-1 acc | 92.1 |\n| ODinW-13 | mAP@1.0 | 48.6 |\n| SUN RGB-D | mAP@0.15 | 39.4 |\n| CountBench | top-1 acc | 93.7 |\n\n5\\. 细粒度感知（工具增强）\n\n| 基准 | 指标 | w/ image_zoom_in_tool |\n| --- | --- | --- |\n| V* | top-1 acc | 93.7 |\n| HRBench-4K | top-1 acc | 85.3 |\n| HRBench-8K | top-1 acc | 82.3 |\n\n6\\. 多图像理解\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Thinking |\n| --- | --- | --- |\n| BLINK | top-1 acc | 70.7 |\n| MUIRBench | top-1 acc | 80.1 |\n\n7\\. 视频理解（最长 2 h）\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B |\n| --- | --- | --- |\n| Video-MME w/o sub | top-1 acc | 79.2 |\n| MLVU-Avg | top-1 acc | 84.3 |\n| LVBench (120 min) | top-1 acc | 67.7 |\n| Charades-STA | mIoU@0.5 | 64.8 |\n\n8\\. GUI & 代理决策\n\n| 基准 | 指标 | Qwen3-VL-32B |\n| --- | --- | --- |\n| OSWorld | task success | 38.1 % |\n| AndroidWorld | task success | 63.7 % |\n| ScreenSpot Pro | top-1 acc | 62.0 % |\n\n9\\. 文本中心任务（与纯文本基座对照）\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Instruct |\n| --- | --- | --- |\n| MMLU-Pro | top-1 acc | 81.8 |\n| AIME-25 | pass@1 | 74.7 |\n| LiveCodeBench-v6 | pass@1 | 54.3 |\n| Arena-Hard v2 | GPT-4 裁判 win-rate | 77.4 % |\n\n10\\. 多语言 OCR（39 语）\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Instruct |\n| --- | --- | --- |\n| MMBench-EN | top-1 acc | 89.3 |\n| RealWorldQA | top-1 acc | 79.2 |\n| MMStar | top-1 acc | 78.4 |\n\n0\n\n11\\. 长上下文压力测试\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Instruct |\n| --- | --- | --- |\n| MMBench-EN | top-1 acc | 89.3 |\n| RealWorldQA | top-1 acc | 79.2 |\n| MMStar | top-1 acc | 78.4 |\n\n1\n\n12\\. 消融实验\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Instruct |\n| --- | --- | --- |\n| MMBench-EN | top-1 acc | 89.3 |\n| RealWorldQA | top-1 acc | 79.2 |\n| MMStar | top-1 acc | 78.4 |\n\n2\n\n13\\. 尺度扩展性\n\n| 基准 | 指标 | Qwen3-VL-235B-A22B-Instruct |\n| --- | --- | --- |\n| MMBench-EN | top-1 acc | 89.3 |\n| RealWorldQA | top-1 acc | 79.2 |\n| MMStar | top-1 acc | 78.4 |\n\n3\n\n所有实验均在统一 prompt 模板、相同采样超参下完成，结果可复现；代码与模型 Apache-2.0 开源，配套评测脚本已上传至 GitHub。\n\nQ5: 有什么可以进一步探索的点？\n\nQwen3-VL 已经验证了“256 K 上下文 + 统一视觉编码 + 工具-增强 RL”在单模型、多任务、全尺寸范围内的有效性，但仍留下一系列开放问题。可进一步探索的方向按“数据-架构-训练-推理-应用”五层归纳如下：\n\n1\\. 数据层\n\n- **任意长视频自监督预训练**\n当前 1 M token 仍靠 YaRN 外推，若收集千万级 8-24 h 原始视频，配合时间-文本自动对齐，可探索“真正无限长”视频-语言对比学习。\n- **多模态 Chain-of-Thought 自动挖掘**\n现有 12 M 长 CoT 靠强模型蒸馏，能否用环境反馈（编译器、机器人、GUI）在线生成“可验证”CoT，实现数据飞轮？\n- **3D-4D 场景合成**\n仅单目 3D 框 9-DoF；若能引入 NeRF/3D-GS 渲染的 4D 轨迹，可扩展至动态遮挡、物理交互数据，提升具身推理。\n\n2\\. 架构层\n\n- **视觉-语言统一生成**\n目前 ViT 仅编码，能否把 SigLIP-2 换成 VAE 或 Diffusion 解码器，实现“看图生成图”与“看图生成代码”端到端联合训练？\n- **混合专家化（MoE）细粒度路由**\n235B-A22B 仅按层路由；若按“任务-模态-语言”三维度路由，可在不增激活量的前提下进一步压榨多语、多任务性能。\n- **可变形视觉 Token**\n高分辨率图仍用 2×2 合并，导致 4 K 图 token 数 >3 k。引入 Deformable Attention 或 Region-of-Interest Tokenizer，可把视觉 token 预算压缩 50 % 而保持精度。\n\n3\\. 训练层\n\n- **继续扩展上下文到 1 M+ 原生**\n无需 YaRN，直接重新设计 RoPE 基频与指数衰减因子，看是否能在 2 M token 上仍保持 95 %+ 检索准确率。\n- **多模态 RL 奖励函数统一**\n当前分“可验证奖励”与“模型裁判奖励”两套，能否用一条通用价值函数（如多模态 RM-Critic）同时处理客观题与主观题，减少奖励 hacking？\n- **在线强化学习（On-Policy RL）**\n目前仅离线 SAPO；若与 GUI/机器人实时环境交互，探索在线 PPO-continual，实现“训练一次，终身更新”。\n\n4\\. 推理层\n\n- **思考预算自适应**\nthinking 模式固定 32 K max；能否根据问题难度动态决定 `<thought>` 长度，实现“可中断”推理，节省 30-70 % 推理 FLOPs？\n- **视觉缓存与复用**\n多轮对话中同一幅图被反复编码；设计“视觉 KV-Cache”跨轮复用，可把首 token 延迟从 2.1 s 降到 0.3 s。\n- **端侧量化**\n2 B/4 B 模型已可在手机跑，但视觉 ViT 仍 FP16；探索 ViT-LLM 联合 INT4 量化 + 8-bit KV-Cache，在 8 GB RAM 实现 256 K 上下文实时推理。\n\n5\\. 应用层\n\n- **实时视频流代理**\n把 1 FPS 提升到 15 FPS，结合滑动窗口与事件相机，实现“直播级”视频问答与广告插入定位。\n- **多机器人协同**\n单模型同时驱动多机，共享 256 K 历史上下文，实现分布式任务规划与冲突协商。\n- **科学文献自动化**\n与 LaTeX 编译器、实验数据接口打通，形成“阅读-假设-实验-写作”闭环，迈向全自动科研助理。\n- **多模态代码仓库级理解**\n输入整个 repo（README + 截图 + 源码），模型自动生成 UI 回归测试脚本，验证前端改动是否破坏视觉效果。\n\n6\\. 评测与可解释性\n\n- **百万级“needle”压力测试**\n构建 10 M token 级图文交错序列，系统研究召回率随深度、模态比例、噪声的变化曲线，建立新的“多模态缩放定律”。\n- **视觉思维链可解释性**\n对 `<thought>` 部分进行注意力 rollout，可视化模型“到底在看哪一块”再做决策，检测幻觉根源。\n- **安全性红队**\n长上下文可被用来隐藏恶意指令（text-needle），需建立多模态红队基准，评估模型对跨模态对抗样本的鲁棒性。\n\n总结\n\nQwen3-VL 把“长上下文+统一视觉+工具 RL”的框架跑通，但距离“无限长、任意模态、实时交互、安全可信”的通用多模态智能体仍有巨大空间。上述方向既包含学术悬念（原生 1 M 上下文、统一生成架构），也蕴含产业价值（端侧量化、实时视频代理、自动科研），可供社区在未来 1-3 年内持续深挖。\n\nQ6: 总结一下论文的主要内容\n\nQwen3-VL 技术报告核心内容速览\n\n1. 目标\n构建一个“看得细、记得长、想得深、做得对”的通用视觉-语言基座模型，在 256 K token 内统一处理文本、图像、视频，并保持纯文本能力不降级。\n\n2. 主要创新\n\n- **Interleaved-MRoPE**：把 t/h/w 位置均匀交错到高低频，解决长视频频谱失衡。\n- **DeepStack**：将 ViT 多层特征残差注入 LLM 前 3 层，零长度增量融合多粒度视觉信息。\n- **文本时间戳**：用可学习 token `<t=3.0 s>` 替代绝对时间 position id，长视频时序定位更准。\n- **平方根重加权损失**：平衡 VL 与文本梯度，多模态性能↑同时语言基准不掉点。\n- **四段预训练 + 三段后训练**：8 K→32 K→256 K→1 M 上下文逐级扩展；SFT→蒸馏→RL 提升推理与对齐。\n- **双模式推理**：非 thinking 低延迟、thinking 可生成长 CoT 并调用工具。\n3. 模型家族\n稠密：2 B / 4 B / 8 B / 32 B\nMoE：30 B-A3B / 235 B-A22B（激活 22 B）\n\n4. 数据规模\n预训练 2.2 T token（含 1 T VL + 1.2 T 文本）；后训练 1.2 M 指令样本 + 12 M 长 CoT；39 语言 OCR；覆盖 STEM、GUI、3D、视频、代码等 11 类场景。\n\n5. 关键结果\n\n- 长上下文：1 M token 视频 needle 准确率 99.5 %。\n- 多模态推理：MMMU 80.6、MathVista 85.8、AIME-25 89.7。\n- 2D/3D 定位：RefCOCO 92.1、ODinW-13 48.6 mAP、SUN RGB-D 39.4 [mAP@0.15](mailto:mAP@0.15)。\n- GUI 代理：OSWorld 38.1、AndroidWorld 63.7，刷新公开 VLM 记录。\n- 文本能力：MMLU-Pro 81.8，仍高于同规模纯文本基座。\n6. 结论与展望\nQwen3-VL 验证了“单一模型、统一架构、256 K 上下文”即可在视觉-语言-动作全栈任务上取得 SOTA，为未来 embodied AI、实时多模态代理和统一生成式架构奠定基座。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Qwen3-VL%20Technical%20Report，链接是https://arxiv.org/pdf/2511.21631，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.21631。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1764892008387","title":"Steering Vision-Language-Action Models as Anti-Exploration: A Test-Time Scaling Approach","description":"Vision-Language-Action (VLA) models, trained via flow-matching or diffusion objectives, excel at learning complex behaviors from large-scale, multi-modal datasets (e.g., human teleoperation, scripted policies). However, since VLAs incorporate diverse data modes in the pre-training stage, and the finetuning dataset often contains demonstration data collected in a kinematically suboptimal or undesirable way, it exists redundant action modes that are irrelevant to the success action modes of the downstream task. Specifically, we observe a critical inference-time fragility among various sampled noises after supervised finetuning of pre-trained VLAs. In this paper, we attribute this instability to the distribution shift between the VLA policy and the policy induced by stable success modes of the downstream task dataset. Thus, we propose \\textbf{TACO}, a test-time-scaling (TTS) framework that applies a lightweight pseudo-count estimator as a high-fidelity verifier of action chunks. The VLA models integrated with TACO can execute the actions with maximum pseudo-count from all sampled action chunks, thereby preventing distribution shifts while preserving the generalization ability of VLAs since the constraint is applied only during inference. Our method resembles the classical anti-exploration principle in offline reinforcement learning (RL), and being gradient-free, it incurs significant computational benefits compared to RL update, especially for flow or diffusion-based VLAs which are difficult to perform RL update due to denoising process. Extensive experiments across four simulation benchmarks (RoboTwin2.0, Robotwin, LIBERO, SimplerEnv) and a dual-arm platform demonstrate that our method significantly improves the inference stability and success rates in downstream-task adaptations.","url":"https://huggingface.co/papers/2512.02834","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.387Z","metadata":{"authors":["Siyuan Yang","Yang Zhang","Haoran He","Ling Pan","Xiu Li","Chenjia Bai","Xuelong Li"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.RO","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2512.02834.pdf","arxivUrl":"https://arxiv.org/abs/2512.02834","arxivId":"2512.02834","coolPaperUrl":"https://papers.cool/arxiv/2512.02834","published":"2025-12-02T14:42:54Z","updated":"2025-12-02T14:42:54.000Z","zh_summary":"视觉-语言-动作（VLA）模型，通过流匹配或扩散目标进行训练，在从大规模多模态数据集（例如人类远程操作、脚本化策略）中学习复杂行为方面表现出色。然而，由于VLA在预训练阶段纳入了多样的数据模式，而微调数据集通常包含以运动学上不理想或不期望方式收集的演示数据，因此存在与下游任务成功动作模式无关的冗余动作模式。具体来说，我们观察到在对预训练VLA进行监督微调后，各种采样噪声在推理时表现出关键的脆弱性。在本文中，我们将这种不稳定性归因于VLA策略与下游任务数据集中稳定成功模式所诱导策略之间的分布偏移。因此，我们提出了\\textbf{TACO}，一种测试时缩放（TTS）框架，它使用轻量级伪计数估计器作为动作片段的高保真验证器。结合TACO的VLA模型可以从所有采样的动作片段中执行伪计数最大的动作，从而在推理期间防止分布偏移，同时由于约束仅在推理阶段应用，也保留了VLA的泛化能力。我们的方法类似于离线强化学习（RL）中的经典反探索原则，并且无需梯度，相比于RL更新具有显著的计算优势，尤其对于基于流或扩散的VLA，由于去噪过程难以执行RL更新。跨四个仿真基准（RoboTwin2.0、Robotwin、LIBERO、SimplerEnv）和双臂平台的大量实验表明，我们的方法显著提高了下游任务适应中的推理稳定性和成功率。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对 Vision-Language-Action（VLA）模型在下游任务微调（SFT）后出现的**推理阶段脆弱性**——即同一模型在不同随机噪声下成功率剧烈波动（0 %–80 %）——展开研究。作者将这一现象归因于：\n\n1. 预训练阶段吸收的**冗余动作模式**在 SFT 后仍未被有效剔除；\n2. SFT 数据本身可能包含**次优或异质策略**，导致策略分布与“理想成功模式”之间存在**分布偏移**。\n\n为此，论文提出**TACO**（Test-time Anti-exploration via pseudo-COunts）框架，把上述问题形式化为**离线强化学习中的 out-of-support 难题**，并在**测试时**通过轻量级伪计数估计器（Coin-Flipping Network, CFN）筛选高支持度动作块，实现**无需梯度更新的抗探索（anti-exploration）**，从而提升推理稳定性与任务成功率。\n\nQ2: 有哪些相关研究？\n\n论文在 §2 Related Work 中系统梳理了三条相关研究脉络，并指出自身与它们的区别与联系：\n\n1. Vision-Language-Action Models\n\n- 早期自零训练：RT-1、Octo\n- 利用预训练 VLM 离散化动作：RT-2、OpenVLA（PaLI-X、Prismatic 骨干）\n- 连续生成式策略（扩散/流匹配）：RDT-1B、DexVLA、π0、GR00T N1、SmolVLA\n- 潜在空间对齐：ATE（训练阶段引入分类器引导）\n**区别**：上述工作聚焦**架构或训练阶段**的改进，而 TACO 首次将**推理阶段的不稳定性**诊断为 out-of-support 问题，提出**测试时抗探索**机制。\n2. Anti-exploration & Offline RL\n\n- 离线 RL 的核心难点是分布偏移与外推误差，典型解法包括保守 Q 学习、不确定性惩罚、Model-based 惩罚等\n- 近期轻量级计数法：RND、随机网络蒸馏、Coin-Flipping Network（CFN）\n**区别**：传统方法需在训练阶段施加惩罚或修改价值函数；TACO**仅在测试时**用 CFN 做伪计数筛选，**无需梯度更新**，适配流/扩散模型的复杂采样动态。\n3. Test-Time Scaling in VLA\n\n- 基于价值或优势函数重排序：Nakamoto et al.、Zhang et al. 2025d\n- 基于世界模型距离：Dynaguide\n- 多采样+大参数验证器：RoboMonkey（7 B VLM 做验证）\n**区别**：TACO 首次**利用 VLA 自身内部特征**构建轻量级（MLP）伪计数器，不增加大模型负担，也不依赖额外价值函数或世界模型，实现**低延迟、高保真**的测试时扩缩。\n\nQ3: 论文如何解决这个问题？\n\n论文将 VLA 在推理时的不稳定性形式化为“策略分布与下游任务成功模式之间的 out-of-support 偏移”，并给出**测试时零梯度**解决方案 TACO，核心流程分三阶段：\n\n1. 问题建模：抗探索目标\n把动作块选择写成\n\na^*_(1:H)=argmax_(a_1:H) N_(mathcal D_sft)(o,l,a_(1:H))\n\n即直接在 SFT 数据集上寻找\\*\\*最高密度（最大伪计数）\\*\\*的动作，等价于离线 RL 的 anti-exploration 目标，无需训练阶段改网络。\n\n2. 轻量级伪计数器：Coupled CFN\n\n- 利用**已冻结 VLA 的内部表征**  h_θ  作为输入，训练 2 层 MLP 的 Coin-Flipping Network；\n- 对扩散/流匹配模型，提出 **High-Fidelity Feature Search**：给同一真值动作加  N  档噪声→VLA 去噪→选预测最接近真值的  h^((i^*))_θ ，既在分布内又保真；\n- 训练后伪计数\n\nhat N(h_θ)propto 1/|f_φ(h_θ)|^2\n\n范数越小→出现频次越高→越“安全”。\n3. 测试时扩缩：generate-then-verify\n\n- **生成**：对同一  (o,l)  并行采样  M  组初始噪声，经去噪得  M  个动作候选与对应  h^((i))_θ ；\n- **验证**：CFN 输出伪计数，选最大值执行；\n- **加速**：共享观测-语言 KV-cache，仅运行一次骨干网络，额外候选边际成本≈0（32 候选时总延迟↓73 %）。\n\n通过“**伪计数最大化**”代替随机采样，TACO 在推理阶段把 VLA 输出约束到 SFT 数据的高密度成功区域，从而抑制预训练或次优数据引入的冗余模式，实现**零重训练、低延迟、高稳定**的性能提升。\n\nQ4: 论文做了哪些实验？\n\n实验围绕 **Q1 通用性**、**Q2 效率**、**Q3 设计消融** 三条研究问题展开，覆盖 **4 个仿真基准 + 1 个双臂真机平台**，共 **69 项任务**，并辅以 **3 组深度分析**。具体清单如下：\n\n1 仿真验证（Q1）\n\n| 基准 | 任务数 | 主测模型 | 对照方法 |\n| --- | --- | --- | --- |\n| RoboTwin1.0 | 9 项双臂任务 | π0 | 原生 π0 |\n| Simpler-WindowX | 4 项 | π0 | RT-1-X、Octo、RoboVLM、SpatialVLA |\n| RoboTwin2.0 | 30 项 | π0.5 | RDT |\n| LIBERO-long | 10 项 | π0.5、OpenVLA | RoboMonkey（7 B VLM 验证器） |\n\n**结果**\n\n- π0 平均提升 **+9.1 %、+7.5 %、+4.7 %**\n- π0.5 已高达 94.8 % 的 suite 再涨 **+1.8 %**\n- OpenVLA 涨 **+6.0 %**，与 RoboMonkey **+6.7 %** 打平，但参数量仅 **1/1000**\n\n2 真机验证（Q2）\n\n**平台**：RealMan75 双臂 + 3 台 RealSense\n**任务**：5 个长时序日常操作（接书、关笔记本、双物分拣等）\n**设置**：每步并行采样 30 条 20-step 动作块，RTX 4090 边缘部署\n\n**结果**\n\n- 平均成功率提升 **+16 %**\n- 长时序任务“Paper & Pen” 涨 **+25 %**\n- KV-cache 使 32 候选的延迟从 2.3 s → **0.62 s（-73 %）**，满足实时需求\n\n3 消融与深度分析（Q3）\n\n| 消融项 | 平均成功率下降 |\n| --- | --- |\n| 去掉 CFN 计数器 | −6.6 % |\n| 去掉 Feature Scaling | −3.7 % |\n| 不用内部表征，改用 CNN+MLP 编码器 | −8.3 % |\n\n**补充实验**\n\n- **伪计数 vs. 真值距离**：100 随机样本的 **1/‖fϕ‖² 与 L2 误差相关系数 ≈ 0.92**，验证计数器确实挑到最近真值动作\n- **CFN ↔ RND 替换**：相同表征下 CFN 再赢 **+4 %**\n- **去噪过程可视化**：仅“内部表征+高保真搜索”能在最后一步保持明显区分度，其余方法提前收敛，失去筛选能力\n\n4 小结\n\n实验从 **仿真→真机、离散→扩散、单臂→双臂、短→长时序** 多维度验证：\n\n- TACO **跨模型、跨任务、跨平台** 一致提升；\n- 额外延迟 **< 30 ms** 级，可边缘实时部署；\n- 轻量级 CFN + 内部表征是性能-效率权衡的最优解。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 TACO 的“直接外延”或“深层追问”，均未被原文系统讨论，值得进一步探索：\n\n1. 伪计数器的**表征层级**与**时间深度**\n\n- 当前仅用 VLA 最后一层 token 向量；尝试\n- 多层加权/自适应融合\n- 显式引入时序信息（动作块内部  h_(θ)^(1:H)  而非单步向量）\n- 研究不同层级对“成功模式”判别力的敏感度，建立层级-性能曲线。\n2. **动态候选数  M(t) ** 与 **早停机制**\n\n- 目前  M  固定；可令置信度高的场景自动减少候选，困难场景自动放大，实现**计算-精度在线权衡**。\n- 引入 sequential hypothesis testing，当最高与次高伪计数差距大于阈值即提前决策，进一步降低延迟。\n3. **跨 embodiment 的伪计数迁移**\n\n- TACO 先在单 embodiment 数据上训练 CFN；若直接换机械臂/手型，伪计数是否仍有效？\n- 探索 embodiment-agnostic 的共享表征空间（如仅依赖相机-语义特征，去掉关节维度），实现**零样本或少量样本迁移**。\n4. **与强化学习微调的正交结合**\n\n- TACO 只做推理筛选；若后续做离线 RL（如 IQL、CQL）或在线微调，可将伪计数作为**保守正则项**加入价值函数：\n\nQ_(reg)(s,a)=Q(s,a)-β/N(h_(θ))\n\n研究“筛选-微调”交替迭代是否能进一步压缩冗余模式。\n5. **开放世界长尾动作**\n\n- 伪计数天然对“未出现”动作赋予低分，可能**过度抑制新必要动作**（如故障恢复、新物体抓取）。\n- 引入**不确定性加权**或**伪计数动态校准**（贝叶斯更新、狄利克雷先验），在保持抗探索的同时允许**安全增广**支持集。\n6. **多模态指令下的伪计数一致性**\n\n- 同一场景的不同语言指令（“把杯子放左边” vs. “把杯子放右边”）应产生**不同成功密度峰**；验证 CFN 能否自动给出**指令相关的多峰分布**，而非单峰硬选。\n7. **理论侧：伪计数与真实支持度的误差界**\n\n- CFN 仅给出相对密度，缺乏**绝对概率保证**。\n- 结合 VAMP-2 特征核或最大均值差异（MMD），推导  |N^(-1)-p_(true)|  的 PAC 界，指导“需要多少 SFT 数据才能让伪计数可信”。\n8. **硬件侧：KV-cache 的极致优化**\n\n- 将共享缓存部署到 **NPU on-chip SRAM**，或采用**动作候选级流水线并行**（候选维度映射到 CUDA warp），进一步把 30 ms 级延迟压到 **10 ms** 以下，满足 100 Hz 力控闭环。\n9. **合成数据增强与伪计数自监督**\n\n- 利用扩散模型自我生成大量候选动作-观测对，再用 CFN 自动标注“高/低支持度”，迭代扩大 Dh，实现**伪计数器自举**（bootstrap），降低对真实 tele-operation 数据的依赖。\n10. **人机协同场景的安全验证**\n\n- 在有人类介入的共享工作空间，将“导致人靠近危险区”的动作额外标记为负支持，扩展 CFN 的**安全约束维度**，实现**抗探索 + 安全双重筛选**。\n\n这些方向既涵盖**算法-理论**深化，也覆盖**系统-硬件**优化，并指向**开放世界、跨本体、安全交互**等下一代机器人学习场景。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **TACO**（Test-time Anti-exploration via pseudo-COunts），用于解决 Vision-Language-Action（VLA）模型在下游任务微调后**推理阶段因冗余动作模式导致的成功率剧烈波动**问题。核心思路与贡献如下：\n\n1. 问题诊断\n\n- 将“同一模型、不同随机噪声 → 成功率 0 %–80 %”现象形式化为**策略分布与成功模式之间的 out-of-support 偏移**。\n- 归因于预训练吸收的多余动作模式在 SFT 后仍残留，且 SFT 数据本身含次优/异质策略。\n2. 解决框架\n\n- **测试时零梯度**：不做额外训练，仅在推理阶段用轻量级**Coin-Flipping Network（CFN）**估计动作块的**伪计数**（出现频次）。\n- **generate-then-verify**：并行采样 M 条噪声 → 去噪得 M 个动作候选 → 选伪计数最大者执行，实现**抗探索（anti-exploration）**。\n- **高效实现**：共享观测-语言 KV-cache，边际延迟≈0；32 候选时总时间↓73 %。\n3. 实验验证\n\n- **4 仿真基准**（RoboTwin1/2、Simpler、LIBERO）**64 任务 + 双臂真机 5 任务**，覆盖 π0、π0.5、OpenVLA、RDT 等架构。\n- 平均成功率提升 **+1.8 %–+9.1 %**；真机长时序任务最高 **+25 %**；延迟保持实时。\n4. 理论依据\n\n- 证明“选最大伪计数”等价于离线 RL 的**抗探索目标**\n\na^*=argmax_a [Q(s,a)-b(s,a)]\n\n其中  b(s,a)propto 1/N(s,a) ，无需价值估计即可在测试时完成。\n5. 轻量级设计\n\n- **Coupled CFN**：直接利用 VLA 内部表征，不增大型骨干；对扩散/流模型提出**High-Fidelity Feature Search**，保证表征既在分布内又保真。\n\n综上，TACO 以**零重训练、低延迟、高兼容**的方式，把 VLA 的输出约束到 SFT 数据的高密度成功区域，显著抑制推理随机性带来的性能抖动，为生成式机器人策略提供了**即插即用的测试时扩缩方案**。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Steering%20Vision-Language-Action%20Models%20as%20Anti-Exploration%3A%20A%20Test-Time%20Scaling%20Approach，链接是https://arxiv.org/pdf/2512.02834，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02834。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1764892008388","title":"PretrainZero: Reinforcement Active Pretraining","description":"Mimicking human behavior to actively learning from general experience and achieve artificial general intelligence has always been a human dream. Recent reinforcement learning (RL) based large-thinking models demonstrate impressive expert-level abilities, i.e., software and math, but still rely heavily on verifiable rewards in specific domains, placing a significant bottleneck to extend the performance boundary of general reasoning capabilities. In this work, we propose PretrainZero, a reinforcement active learning framework built on the pretraining corpus to extend RL from domain-specific post-training to general pretraining. PretrainZero features the following characteristics: 1) Active pretraining: inspired by the active learning ability of humans, PretrainZero learns a unified reasoning policy to actively identify reasonable and informative contents from pretraining corpus, and reason to predict these contents by RL. 2) Self-supervised learning: without any verifiable labels, pretrained reward models, or supervised fine-tuning, we directly pretrain reasoners from 3 to 30B base models on the general Wikipedia corpus using RL, significantly breaking the verification data-wall for general reasoning. 3) Verification scaling: by tackling increasingly challenging masked spans, PretrainZero substantially enhances the general reasoning abilities of pretrained base models. In reinforcement pretraining, PretrainZero improves Qwen3-4B-Base for 8.43, 5.96 and 10.60 on MMLU-Pro, SuperGPQA and math average benchmarks. In post-training, the pretrained models can also serve as reasoning foundation models for downstream RLVR tasks.","url":"https://huggingface.co/papers/2512.03442","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.388Z","metadata":{"authors":["Xingrun Xing","Zhiyuan Fan","Jie Lou","Guoqi Li","Jiajun Zhang","Debing Zhang"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2512.03442.pdf","arxivUrl":"https://arxiv.org/abs/2512.03442","arxivId":"2512.03442","coolPaperUrl":"https://papers.cool/arxiv/2512.03442","published":"2025-12-03T04:51:32Z","updated":"2025-12-03T04:51:32.000Z","zh_summary":"模仿人类行为，从一般经验中主动学习并实现通用人工智能一直是人类的梦想。基于强化学习（RL）的近期大型思维模型展现了令人印象深刻的专家级能力，如软件和数学能力，但仍严重依赖特定领域的可验证奖励，这在拓展通用推理能力的性能边界时形成了重要瓶颈。在本研究中，我们提出了 PretrainZero，这是一种建立在预训练语料上的强化主动学习框架，旨在将 RL 从特定领域的后训练扩展到通用预训练。PretrainZero 具有以下特征：1）主动预训练：受人类主动学习能力启发，PretrainZero 学习统一的推理策略，以主动识别预训练语料中合理且有信息量的内容，并通过 RL 进行推理来预测这些内容。2）自监督学习：在没有任何可验证标签、预训练奖励模型或监督微调的情况下，我们直接使用 RL 从一般维基百科语料库对 3 到 30B 基础模型进行预训练，大幅突破了通用推理的验证数据壁垒。3）验证扩展：通过处理越来越困难的遮蔽片段，PretrainZero 显著提升了预训练基础模型的通用推理能力。在强化预训练中，PretrainZero 将 Qwen3-4B-Base 在 MMLU-Pro、SuperGPQA 和数学平均基准上的成绩提升了 8.43、5.96 和 10.60。在后训练中，预训练模型也可以作为下游 RLVR 任务的推理基础模型使用。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在突破“通用推理能力”在预训练阶段的数据瓶颈。现有 RLVR（可验证奖励强化学习）只能在数学、代码等存在确定性答案的窄域里利用验证信号，一旦进入开放域便面临无奖励可用、人工标注成本极高的“数据墙”。作者提出把强化学习从“后训练”提前到“预训练”，直接在无标注、噪声大、信息密度低的通用语料（Wikipedia）上做可验证的 self-supervised RL，从而\n\n- 摆脱对人工标注、奖励模型、SFT 冷启动的依赖；\n- 让基础模型在预训练阶段就获得可泛化的链式推理能力；\n- 为后续任意下游 RLVR 任务提供一个“自带推理能力”的初始化点。\n\n核心待解决问题可归纳为：\n\n1. 如何在无标注、低信息密度的真实预训练语料上构造“可验证”的强化学习目标；\n2. 如何避免固定掩码或高熵掩码带来的低效与崩溃，实现高效探索；\n3. 如何仅通过预训练语料，让基础模型在通用与数学推理基准上同时获得显著提升。\n\nQ2: 有哪些相关研究？\n\n与 PretrainZero 直接相关或构成对比的研究可划分为三条主线：\n\n1. 自监督预训练\n\n- BERT、GPT 系列：固定 Next-Token Prediction / Masked-Language-Model 目标，无主动探索。\n- SpanBERT、BGE-M3：引入 span 级掩码或多语言掩码，但仍为静态目标。\n2. 强化学习后训练（RLHF / RLVR）\n\n- InstructGPT、Tulu-3、DeepSeek-R1：依赖人工偏好或领域可验证答案，只能在后训练阶段使用，无法扩展到通用开放域。\n- GeneralReasoner、DAPO：提出“通用”RLVR 配方，但仍需 Web-Instruct 等 QA 数据与奖励模型。\n3. 强化“预”训练（RLPT）\n\n- Quiet-STaR / Fast-Quiet-STaR：在预训练语料上做 token 级自举推理，但无主动掩码生成，且需 thought-token 架构。\n- RPT（Dong et al. 2025）：首次把 next-token 预测变成可验证 RL，但依赖合成 OmniMath 数据与后训练模型，未在 base 模型上从零训练。\n- PRT、RLPT1：同期工作，分别引入辅助 RL 目标或额外奖励模型/SFT 冷启动；PretrainZero 则完全去掉奖励模型与 SFT，实现“纯 RL+主动掩码”的 stand-alone 方案。\n\nQ3: 论文如何解决这个问题？\n\n论文把问题转化为一个**可验证的主动掩码博弈**，在纯 RL 框架内同时学习“哪里值得学”与“如何推理”。具体做法分为三步：\n\n1. 构造双向可验证任务\n\n- **Mask Generation**：同一模型作为生成器 πω，针对原始段落输出一段连续掩码 m，使得该掩码“对别人难、对自己可验证”。\n- **Mask Prediction**：同一模型作为预测器 ψω，在看到被掩码段落后先输出 CoT，再恢复原文片段；预测结果可与原文做 exact-match 验证，无需任何外部标注。\n2. 建立 min–max 强化学习目标\n\nω^* = argmin_(ω')max_(ω) E_(ssim D) E_(msimπ_ω')(·|s),hat xsimpsi_(ω)(·|m,s)!l[R(s,m,hat x)r]\n\n- 预测器最大化恢复准确率（内层 max）。\n- 生成器最小化该准确率（外层 min），从而主动挑选“模型尚未掌握且富含信息”的片段。\n3. 用 GRPO 同时优化双方\n\n- 预测器奖励：$r^(pred)=I\nhat x=x\n$。\n- 生成器奖励： r^(gen)=1-Acc ，若 Acc=0 则置 0，避免鼓励不可预测的噪声掩码。\n- 同一 batch 内拼接生成与预测样本，统一做重要性采样与 clip，保证训练稳定。\n\n通过 2000 步纯 RL 训练，模型在 Wikipedia 上持续遇到自己“最不会”的片段，迫使 CoT 能力不断增强，最终把通用与数学推理基准一并提升，并可作为后续任意 RLVR 任务的“推理基础模型”。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“能否在真实预训练语料上，用纯 RL 训练出更强的通用推理模型”展开，分三条主线：\n\n1. 预训练阶段（RLPT）\n\n- 模型：Qwen3-4B/8B、30B-A3B-MoE、SmolLM3-3B 四个尺寸的基础模型，**无 SFT 冷启动**。\n- 数据：仅用 Wikipedia 通用段落，不含任何 QA 或合成 CoT。\n- 步骤：2000 步 GRPO，batch=288（32 段×8 掩码生成 + 256 掩码×8 预测）。\n- 对比基线：\n– Base Model\n– Continue Pre-Training（自监督 NTP）\n– Supervised Fine-Tuning（把掩码预测转成 QA 格式，去掉 CoT）\n– Random RLPT（随机选掩码）\n- 结果：PretrainZero 在 MMLU-Pro、SuperGPQA、BBEH 及 6 个数学基准上平均提升 **+7.25（4B）/+5.71（8B）/+4.20（30B-MoE）/+3.16（SmolLM3）**，显著优于 Random RLPT 与监督方案。\n2. 后训练阶段（RLVR）\n\n- 用 Web-Instruct 通用问答数据做 400 步 RLVR，验证“预训练推理能力能否迁移”。\n- 结果：PretrainZero-2000 在相同 RLVR 预算下，比 Base 模型再提升 **+2.56 overall / +2.18 math**，且 CoT 长度更短、更稳定。\n3. 消融与稳健性\n\n- 数据域：Wikipedia vs MathPile → 通用域反而更好，降低数据成本。\n- 掩码正则：\n– 允许多次出现但段落内 <8 次\n– 仅保留完整词跨度\n– 仅替换首次出现\n三种策略均可稳定训练，默认策略最优。\n- 训练曲线：PretrainZero 的熵、响应长度、奖励持续上升，Random RLPT 快速饱和，Entropy 掩码则崩溃。\n\n综上，实验首次证明**完全基于通用语料、无标注、无奖励模型的 stand-alone RLPT** 可以持续 2000 步不崩溃，并在预训练与后训练两阶段同时带来显著增益。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步扩展 PretrainZero 的边界与实用性：\n\n- **跨语言与多模态主动掩码**\n将掩码生成/预测扩展到多语言 Wikipedia 与图文、音文混合语料，验证主动学习是否同样能挖掘跨模态可验证片段。\n\n- **课程式难度调度**\n用在线估计的预测损失或熵曲线动态调节生成器“难度旋钮”，实现从短句→长段→多跳推理的自动课程，避免过早陷入高噪声掩码。\n\n- **稀疏与检索增强掩码**\n允许生成器直接调用外部检索或代码解释器，把“可验证”从 exact-match 扩展到 execution-result-match，打开生物、化学等开放域 RLPT。\n\n- **数学符号与定理级掩码**\n在 arXiv 或 MathPile 上实验公式、定理名称、证明步骤的连续掩码，检验主动学习能否自动发现“关键引理”并生成可检验证明。\n\n- **参数高效化与系统优化**\n用 LoRA/AdaLoRA 仅训练掩码生成头与预测头，降低 30B+ 模型 RLPT 的 GPU 内存；结合 off-policy 缓存与 rollout 压缩，把步数扩展到 10k+。\n\n- **理论分析**\n建立生成器-预测器博弈的收敛界，刻画“可预测但未被掌握”掩码的度量，与信息增益或 V-C 维关联，给出何时避免噪声崩溃的充分条件。\n\n- **与继续预训练无缝衔接**\n探索 RLPT 与自监督 NTP 的混合目标：在同一 batch 内按动态比例交替优化，既保持通用语言建模又持续注入推理能力，实现“零额外成本”的持续提升。\n\nQ6: 总结一下论文的主要内容\n\n**PretrainZero：在通用预训练语料上做可验证强化学习**\n\n1. 问题\n现有 RLHF/RLVR 只能在有确定性答案的窄域里使用，开放域面临“无奖励、标注贵”的数据墙；而继续预训练或随机掩码 RL 又会在低信息密度、带噪声的 Wikipedia 上崩溃。\n\n2. 思路\n把强化学习从“后训练”提前到“预训练”，用**主动掩码博弈**代替固定 next-token 或随机掩码：\n\n- 生成器 πω 主动挑选“对别人难、对自己可验证”的连续片段；\n- 预测器 ψω 用 CoT 恢复原文，exact-match 给出无标注奖励；\n- 同一模型参数 ω 在 min–max 目标下同时优化：生成器最小化预测准确率，预测器最大化准确率。\n3. 算法\n用 GRPO 对生成与预测样本统一做重要性采样+clip，2000 步纯 RL，无需 SFT、奖励模型或人工标注。\n\n4. 结果\n\n- 预训练：Qwen3-4B 在 MMLU-Pro↑8.43、SuperGPQA↑5.96、6 项数学平均↑10.60；8B/30B-MoE/3B 均一致领先随机掩码与监督方案。\n- 后训练：同一 Web-Instruct RLVR 预算下，再涨 +2.56 overall / +2.18 math，且 CoT 更短更稳。\n- 消融：通用 Wikipedia 优于 MathPile；多种掩码正则均可稳定 2000 步。\n5. 意义\n首次证明**完全基于真实预训练语料、零标注、零奖励模型的 stand-alone RLPT** 可持续提升通用与数学推理，为后续任意 RLVR 任务提供“自带推理能力”的基础模型。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PretrainZero%3A%20Reinforcement%20Active%20Pretraining，链接是https://arxiv.org/pdf/2512.03442，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03442。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1764892008388","title":"ViDiC: Video Difference Captioning","description":"Understanding visual differences between dynamic scenes requires the comparative perception of compositional, spatial, and temporal changes--a capability that remains underexplored in existing vision-language systems. While prior work on Image Difference Captioning (IDC) has enabled models to describe semantic changes between static images, these approaches fail to capture motion continuity, event evolution, or editing consistency over time. We introduce the ViDiC (Video Difference Captioning) task and its corresponding ViDiC-1K dataset, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) to provide fine-grained descriptions of similarities and differences between video pairs. ViDiC-1K comprises 1,000 curated video pairs annotated with over 4,000 comparative checklist items, covering seven categories: subject, style, background, cinematography, motion, location, and playback techniques. To ensure reliable evaluation, we propose a dual-checklist framework that measures the accuracy of similarity and difference separately, based on the LLM-as-a-Judge protocol. Experiments on nineteen representative multimodal models reveal a significant performance gap in their comparative description and difference perception abilities. We hope ViDiC-1K can be a challenging benchmark that lays a solid foundation for advancing video understanding, edit awareness, and comparative reasoning in multimodal intelligence.","url":"https://huggingface.co/papers/2512.03405","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.388Z","metadata":{"authors":["Jiangtao Wu","Shihao Li","Zhaozhou Bian","Yuanxing Zhang","Jialu Chen","Runzhe Wen","An Ping","Yiwen He","Jiakai Wang","Jiaheng Liu"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2512.03405.pdf","arxivUrl":"https://arxiv.org/abs/2512.03405","arxivId":"2512.03405","coolPaperUrl":"https://papers.cool/arxiv/2512.03405","published":"2025-12-03T03:23:24Z","updated":"2025-12-03T03:23:24.000Z","zh_summary":"理解动态场景之间的视觉差异需要比较感知构图、空间和时间的变化——这是现有视觉-语言系统中仍未充分探索的能力。虽然先前关于图像差异字幕（IDC）的工作使模型能够描述静态图像之间的语义变化，但这些方法无法捕捉运动连续性、事件演变或编辑一致性。我们提出了视频差异字幕（ViDiC）任务及其对应的 ViDiC-1K 数据集，旨在评估多模态大语言模型（MLLMs）对视频对之间相似性和差异性进行细粒度描述的能力。ViDiC-1K 包含 1,000 对精心挑选的视频对，并附有超过 4,000 个比较核对清单项，涵盖七个类别：主体、风格、背景、摄影、动作、地点和播放技巧。为了确保评估的可靠性，我们提出了一个双核对清单框架，根据“LLM 作为裁判”的协议，分别测量相似性和差异性的准确性。在对十九种代表性多模态模型的实验中，显示了它们在比较描述和差异感知能力上的显著性能差距。我们希望 ViDiC-1K 能成为具有挑战性的基准，为推动多模态智能中的视频理解、编辑意识和比较推理奠定坚实基础。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有视觉-语言系统对**动态场景之间视觉差异**缺乏细粒度、时空一致的理解与描述能力的问题。具体而言：\n\n- 图像差异描述（IDC）方法只能处理静态图像对，无法捕捉**运动连续性、事件演化或编辑一致性**等时间维度信息。\n- 现有视频编辑数据集侧重“执行”层面（如编辑保真度），而非“理解”层面（即能否用语言准确描述编辑前后视频的差异与共性）。\n\n为此，作者提出：\n\n1. **新任务：Video Difference Captioning (ViDiC)**\n要求模型对给定的两个短视频，生成自然语言描述，同时准确刻画它们在**静态内容**与**时间动态**上的相似与差异。\n\n2. **新基准：ViDiC-1K 数据集**\n\n- 1 000 对精心筛选/合成的视频，覆盖 7 大维度（主体、风格、背景、摄影、运动、位置、播放技巧）。\n- 4 000+ 人工校验的二元（Yes/No）检查项，分别评估“相似”与“差异”。\n3. **新评测框架：双检查表 + LLM-as-a-Judge**\n用独立大模型依据生成的描述回答检查项，计算事实一致性准确率，避免传统 BLEU/CIDEr 仅衡量文本相似度的缺陷。\n\n通过该任务与基准，论文系统暴露了当前多模态大模型在**细粒度时空推理、编辑理解与差异感知**上的显著不足，为后续研究提供明确方向。\n\nQ2: 有哪些相关研究？\n\n论文在“2 Related Works”部分系统梳理了与 ViDiC 任务直接相关的三条研究脉络，并指出其局限。相关研究可归纳为以下三类：\n\n1\\. 图像差异描述（Image Difference Captioning, IDC）\n\n- **Spot-the-Diff** (Jhamtani & Berg-Kirkpatrick, EMNLP 2018)\n首个提出“用自然语言描述两张静态图像差异”的数据集与基准，仅覆盖 1 400 对真实照片。\n\n- **CLEVR-Change** (Park et al., arXiv 2019)\n基于 CLEVR 合成场景，引入 5 类几何/属性变化，共 7 970 对，完全人工合成。\n\n- **OmniDiff** (Liu et al., arXiv 2025)\n结合真实与合成数据，12 类细粒度变化，1 560 对，支持属性-区域-原因三级描述。\n\n- **ViDi** (Anonymous, ACL Rolling Review 2025)\n聚焦真实街拍，5 类变化，200 对，强调风格与物体增减。\n\n**共性局限**：仅处理**静态图像对**，无法建模时序演化、运动一致性或编辑连贯性。\n\n2\\. 视频编辑/差异数据集（Video Editing & Action Differencing）\n\n- **TGVE Competition Dataset** (Wu et al., CVPR 2023)\n文本驱动视频编辑比赛用数据，侧重“执行”——给定文本指令后编辑是否成功，未要求模型**描述**差异。\n\n- **Anatomy of Video Editing** (Argaw et al., ECCV 2022)\n提供重构图、rotoscoping、调色等 7 类编辑任务，评估指标为像素/感知保真度，而非语义描述。\n\n- **EditVerse** (Ju et al., arXiv 2025)\n统一图像-视频编辑指令数据，数十万样本，仍聚焦**生成/编辑质量**，不评估模型对差异的理解与语言表述。\n\n- **VidDiffBench** (Burgess et al., arXiv 2025)\n与 ViDiC 最接近：要求模型判断“两段视频中动作是否不同”，但仅输出**二元标签**（same/different），不提供**自然语言解释**，且只覆盖 549 对真实视频。\n\n**共性局限**：\n\n- 侧重“编辑执行”或“动作差异分类”，缺乏**细粒度自然语言描述**；\n- 无**相似性与差异双重检查表**，无法同时衡量“是否 hallucinate 相同点”与“是否遗漏差异”。\n\n3\\. 视频相似度/理解基准（Video Similarity & Understanding）\n\n- **Tencent-MVSE** (Zeng et al., CVPR 2022)\n大规模多模态视频相似度数据，仅给出整体相似分数，不生成差异文本。\n\n- **ConViS-Bench** (Liberatori et al., NeurIPS 2025)\n基于语义概念的视频相似度评估，依旧停留在**打分**层面。\n\n- **MMBench、SEED-Bench、VideoChat、InternVideo 系列**\n侧重单视频问答、字幕、推理，未设计**成对视频比较**任务。\n\n**共性局限**：\n\n- 单视频输入，不具备**跨视频差异推理**能力；\n- 评估指标为传统召回/准确率或文本相似度，无法**细粒度**验证事实正确性。\n\n小结\n\n| 研究方向 | 代表工作 | 是否支持时序 | 是否生成差异描述 | 是否同时评估相似+差异 |\n| --- | --- | --- | --- | --- |\n| 图像差异描述 | Spot-the-Diff / OmniDiff / ViDi | ❌ | ✅ | ❌ |\n| 视频编辑数据集 | TGVE / EditVerse / Anatomy | ✅ | ❌ | ❌ |\n| 视频动作差异 | VidDiffBench | ✅ | ❌（仅标签） | ❌ |\n| 视频相似度 | MVSE / ConViS | ✅ | ❌ | ❌ |\n| ViDiC（本文） | — | ✅ | ✅ | ✅（双检查表） |\n\nViDiC 首次将“差异描述”从静态图像扩展到**动态视频**，并通过**双检查表+LLM-as-a-Judge**实现可扩展、可解释、细粒度的评测，填补了上述研究空白。\n\nQ3: 论文如何解决这个问题？\n\n论文从**任务定义、数据构造、评测框架、实验验证**四个层面系统解决“动态场景差异理解”难题，具体路线如下：\n\n1\\. 任务层面：提出 Video Difference Captioning（ViDiC）\n\n- **输入**：一对短视频 A/B（2–12 s，真实或合成）。\n- **输出**：自然语言段落，**同时**陈述\n- 相似性（Same）：共有的主体、风格、背景、摄影、运动、位置、播放技巧。\n- 差异性（Diff）：上述七维度中**可验证**的细粒度变化。\n- **目标**：超越静态 IDC 与单视频字幕，强制模型具备**跨视频时空推理与编辑理解**能力。\n\n2\\. 数据层面：构建 ViDiC-1K 高质量基准\n\n2.1 多元来源混合\n\n| 来源 | 比例 | 关键处理 |\n| --- | --- | --- |\n| 真实公开集 | ≈ 38 % | VidDiffBench / IF-VidCap / YouTube 等，时序二分后人工过滤。 |\n| 合成 pipeline | ≈ 62 % | ① 帧拼接：首末帧垂直拼接→Veo3 生成→一切为二，像素级对齐；② CV/渲染：SAM-2 增删主体、ReCamMaster 换视角、风格化、UE 重动画。 |\n\n2.2 双阶段标注\n\n- **自动草稿**：Qwen3-VL-plus 生成侧写→Gemini-2.5-Pro 生成相似/差异检查表。\n- **人工精修**：6 名专业标注员两两独立审核，第三人仲裁；仅 16.32 % 的模型条目被**原样保留**，其余重写或丢弃，确保事实零误差。\n\n2.3 统计特性\n\n- 1 000 对视频，4 107 条二元检查项（1 056 相似 + 3 051 差异）。\n- 七维度分布均衡，时长 2–12 s 占 95 %，分辨率 720 p 以上占 75 %。\n\n3\\. 评测层面：双检查表 + LLM-as-a-Judge\n\n3.1 评测公式\n\nAccuracy = (1) / (|Q|)∑_(i=1)^(|Q|) Il(A_(J,i)=A_(GT),ir)\n\n3.2 相似 vs 差异差异化策略\n\n- **相似问题**（逆问法）：\n“两视频拍摄地点不同吗？”→ 答案 **No** 才算对；模型只要**不 hallucinate 差异**即可得分。\n- **差异问题**：\n“视频 A 太阳无遮挡，而 B 仍被云遮住吗？”→ 答案 **Yes** 必须被模型**显式或可直接推理**得出，否则扣分。\n\n3.3 可扩展裁决\n\n- 裁判模型（GPT-5-mini）**仅接触文本描述**，不访问视频，与人类一致性 95.22 %，五次重复方差 < 0.6 %，保证公平与复现。\n\n4\\. 实验层面：大规模诊断现有 MLLM\n\n- **19 个代表模型**（闭源：Gemini-2.5-Pro/Flash、GPT-5/4o；开源：Qwen3-VL、InternVL-3.5、LLaVA-v1.6 等）。\n- **关键发现** → 直接指引后续研究：\n1. **性能分层明显**：Gemini-2.5-Pro 平均 66.72 %，LLaVA-v1.6 仅 8.96 %。\n2. **维度短板突出**：所有模型在**摄影技巧**（Camera）与**播放技巧**（Playback Tech）准确率最低，< 30 %。\n3. **相似-差异权衡**：GPT-4o 相似 81 %，差异仅 39 %，暴露**过度保守**或**幻觉抑制不足**。\n4. **Thinking 模式双刃剑**：开启后差异得分↑但相似得分↓，说明**深入推理会诱发伪差异**。\n5. **输入噪声鲁棒性**：模糊/噪声/饱和度↑ → 相似分↑、差异分↓，模型**丢失细粒度感知**但**减少幻觉**。\n\n5\\. 后续扩展\n\n- 数据侧：将清洗流程放大到**十万对级**，构建指令微调数据。\n- 模型侧：基于 ViDiC-1K 诊断出的**Camera & Playback Tech 短板**，设计时空对齐与编辑感知专用架构或损失函数。\n\n结论\n\n通过“任务-数据-评测”三位一体设计，论文不仅**首次**把差异描述从静态图像升级到**动态视频**，还提供了一套**可复现、可解释、细粒度**的评测体系，直接揭示现有 MLLM 在**编辑理解与时空推理**上的盲区，为后续算法与数据研究奠定基准。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **ViDiC-1K 基准** 共设计了 **5 组实验 + 3 项补充分析**，覆盖 **19 个代表性多模态大模型**，系统评估其在 **Video Difference Captioning** 任务上的细粒度表现。实验一览如下（按出现顺序归纳）：\n\n1\\. 主实验：19 模型整体与分维度准确率\n\n- **目的**：给出 **性能排行榜** 并定位 **维度短板**。\n- **指标**：\n- 宏观：Avg = (Similarity + Difference) / 2\n- 微观：七维度单独 Accuracy（Subject / Motion / Position / Background / Camera / Style / Playback Tech）\n- **关键结论**（Table 2 & Figure 5）：\n1. 闭源 > 开源，但 **Qwen3-VL-32B** 已反超部分闭源模型。\n2. **Camera & Playback Tech** 普遍 < 30 %，为 **共同瓶颈**。\n3. **Thinking 模式**提升差异感知（+6–9 %），却降低相似准确率（−5–10 %）。\n\n2\\. Judge 一致性 & 稳定性验证\n\n- **目的**：确保 **LLM-as-a-Judge** 可替代人工。\n- **做法**：\n- 750 对子集（75 % 数据）让人类与 **GPT-5-mini / DeepSeek-V3 / Qwen3-32B** 同时打分。\n- 对 Gemini-2.5-Pro 的生成描述进行 **5 轮重复评判**。\n- **结果**（Table 3）：\n- GPT-5-mini 与人一致性 **95.22 %**，显著高于其余法官；\n- 5 轮方差 < 0.6 %，**稳定性**满足大规模自动评测要求。\n\n3\\. 视频参数敏感性分析\n\n- **目的**：探明 **帧数** 与 **分辨率** 对差异感知的影响。\n- **设置**：固定另一变量，分别采样\n- 帧数：8 / 32 / 64 / 128 帧（分辨率 360 p）\n- 分辨率：224 p / 360 p / 480 p / 720 p（1 fps）\n- **结果**（Figure 6a）：\n- **非单调帧数效应**：32 帧最佳，过长序列导致 **长程依赖饱和**；\n- **单调分辨率效应**：720 p 比 224 p 平均提升 **8–10 %**，**空间保真度**对细粒度差异至关重要。\n\n4\\. 视觉损坏鲁棒性实验\n\n- **目的**：检验模型在 **模糊、高斯噪声、色彩饱和度** 下的 **Similarity-Difference 权衡**。\n- **设置**：\n- 3 种失真 × 3 档强度（Light/Medium/Heavy），**同一对视频同步施加相同扰动**。\n- **结果**（Figure 6c）：\n- **相似分↑、差异分↓**：扰动掩盖像素级细节 → 降低细粒度感知，却抑制幻觉。\n- **侧面验证**：模型在 **干净场景** 的“幻觉差异”是 **可观测且可量化**的。\n\n5\\. 细粒度类别下探与错误模式分析\n\n- **目的**：定位 **最难子任务** 与 **三类典型失败**。\n- **做法**：\n- 对 4 个代表性模型（GPT-4o、Gemini-2.5-Pro、InternVL3.5-8B、Qwen2.5-VL-72B）在 **38 项叶子类别** 上单独计算准确率（Figure 5）。\n- 人工抽样 200 例失败描述，归类错误。\n- **结论**：\n- **OCR、视频倒放、景深、表情** 等子任务平均 < 20 %，为 **下一步重点**。\n- 错误模式占比：\n1. **幻觉差异**（~40 %）\n2. **自相矛盾**（~25 %）\n3. **遗漏/模糊描述**（~35 %）\n\n6\\. 附加实验（附录）\n\n- **B.2** 补充 3 个开源模型（VideoChat2、ARC-Hunyuan-Video、LLaVA-Video）结果，**人类基线** 84.74 % 平均，**拉开 50+ % 差距**。\n- **B.3** 给出 **失真参数表**（kernel、σ、HSV 缩放），确保 **可复现**。\n- **C** 详细展示 **三级清洗流水线**（8 756 → 1 000），并公开 **标注界面截图**。\n\n实验总结\n\n| 实验组 | 主要发现 |\n| --- | --- |\n| 主排行榜 | 闭源领先、维度短板显著、Thinking 双刃剑 |\n| Judge 一致性 | GPT-5-mini 95 % 一致，可放心自动评测 |\n| 帧数/分辨率 | 32 帧 + 720 p 为性价比最优输入 |\n| 鲁棒性 | 扰动抑制幻觉但牺牲差异感知 |\n| 错误分析 | 幻觉、矛盾、遗漏 为三大主因；OCR/倒放/景深最难 |\n| 人类基线 | 仍领先最佳模型 18+ %，证明任务挑战性 |\n\n整套实验 **从宏观排行榜到微观子任务、从输入参数到输出错误、从自动评判到人类基线**，全方位验证了 ViDiC-1K 的 **诊断价值** 与 **未来改进方向**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接延续 ViDiC 的“任务-数据-评测”框架，进一步拓展视频差异理解的研究边界：\n\n1\\. 数据与标注\n\n- **大规模指令微调数据**\n将 1 K→100 K 对，保持七维度均衡，引入“差异解释+编辑指令”成对样本，支持 **差异描述+编辑生成** 联合训练。\n\n- **长视频差异**\n当前 2–12 s；探索 **分钟级** 纪录片、剧情片段，引入 **事件序列对齐** 与 **关键子片段定位** 子任务。\n\n- **多模态差异**\n同步比较 **音频/配乐/声效** 或 **文本字幕**，研究视听一致性差异描述（A-V Difference Captioning）。\n\n- **多语言差异描述**\n构建中英双语检查表，考察模型在 **跨文化语境** 下对风格、手势、符号差异的表达能力。\n\n2\\. 任务与评测\n\n- **差异定位（Grounding）**\n输出 **时序-空间掩码** 或 **自然语言+边界框**，实现 **“说什么”与“在哪说”** 双重可解释。\n\n- **差异推理（Causal Difference）**\n引入 **“为什么不同”** 问答：要求模型给出 **编辑操作**（如换背景、减速）或 **拍摄条件**（如换机位、调光）推断。\n\n- **差异可控生成**\n给定源视频+差异描述，模型 **自动执行编辑** 并保证 **与描述完全一致**，形成 **描述-生成闭环** 评测。\n\n- **增量差异追踪**\n对 **≥3 条视频序列**（V₁→V₂→V₃…）进行 **增量差异链描述**，评估 **长程依赖与一致性**。\n\n3\\. 模型与训练\n\n- **差异预训练目标**\n设计 **时空差异对比损失**（spatio-temporal contrastive loss）或 **掩码差异重建**（masked difference reconstruction），显式注入 **编辑感知表征**。\n\n- **专用差异解码器**\n在现有 MLLM 外接 **轻量级差异头**（difference decoder），仅对差异 token 进行 **高分辨率特征交叉**，降低计算量。\n\n- **迭代式自我训练**\n利用 **Judge LLM** 自动为 **未标注视频对** 生成伪检查表，通过 **不确定性加权** 迭代微调，突破人工标注瓶颈。\n\n- **思维链 vs 幻觉抑制**\n针对 “Thinking 模式” 出现的 **伪差异幻觉**，引入 **反思机制**（self-reflection）或 **对比式奖励模型**（reward model），在 **深度推理与感知忠实** 间动态平衡。\n\n4\\. 场景与应用\n\n- **版权与伪造检测**\n结合 **差异定位** 输出 **像素级篡改掩码**，用于 **深度伪造溯源** 与 **影视盗版比对**。\n\n- **自动 Change-Log 生成**\n在 **工业级视频剪辑流水线** 中，实时生成 **人类可读修改摘要**，打通 **AI 编辑与人审协同**。\n\n- **体育/康复动作纠错**\n将 **参考视频与用户自拍** 对比，输出 **关节轨迹差异文本+图示**，实现 **个性化动作指导**。\n\n- **科学观测报告**\n对 **长时序监控录像**（冰川、火山、城市扩张）自动生成 **结构化差异年报**，支持 **政策决策**。\n\n5\\. 评测与理论\n\n- **人类-模型混合评测**\n引入 **交互式纠错**：人类对模型描述进行 **最小编辑** 即可修正检查项答案，用 **编辑距离** 量化 **可解释性**。\n\n- **差异复杂度理论**\n建立 **差异信息量度量**（Difference Information Gain, DIG），从 **时空熵、语义层级、编辑操作难度** 多角度量化 **样本难度**，指导 **自适应评测**。\n\n- **对抗差异样本**\n构造 **视觉几乎一致但存在微小关键差异** 的对抗对（如 DeepFake 微表情），测试模型 **鲁棒性与安全性**。\n\n一句话总结\n\nViDiC 打开了“视频编辑理解”这一新品类，后续可在 **数据规模、差异定位、因果推理、专用架构、真实场景落地** 五大方向持续深挖，推动多模态模型从“看见”走向“看懂编辑”。\n\nQ6: 总结一下论文的主要内容\n\nViDiC 论文核心内容速览\n\n1\\. 任务提出\n\n- **Video Difference Captioning (ViDiC)**：首次将“差异描述”从静态图像升级到**动态视频**，要求模型对一对短视频同时生成**可验证的相似性与差异性**自然语言解释，涵盖主体、风格、背景、摄影、运动、位置、播放技巧七维度。\n\n2\\. 数据基准\n\n- **ViDiC-1K**：1 000 对 2–12 s 视频（真实+合成），含 4 100+ 人工校验二元检查项；提供**相似与差异双检查表**，支持细粒度、可解释评测。\n\n3\\. 评测框架\n\n- **LLM-as-a-Judge**：裁判模型仅依据生成文本回答检查项，与人类一致性 95 %，实现**可扩展、可复现**的事实准确率计算。\n\n4\\. 实验发现\n\n- 19 个主流 MLLM 评估显示：\n- **性能梯度明显**：最佳 Gemini-2.5-Pro 平均 66.7 %，最差 LLaVA-v1.6 仅 9 %。\n- **维度短板**：摄影技巧 & 播放技巧普遍 < 30 %。\n- **相似-差异权衡**：高相似得分常伴随幻觉抑制不足；Thinking 模式提升差异感知却降低相似准确率。\n- **输入敏感**：32 帧+720 p 为最优；视觉扰动抑制幻觉但削弱细粒度差异检测。\n\n5\\. 意义与展望\n\n- 建立**首个视频差异理解基准**，暴露模型在**时空编辑推理**上的显著不足；数据、评测代码与排行榜全部开源，为后续**差异定位、因果推理、编辑生成**研究提供基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ViDiC%3A%20Video%20Difference%20Captioning，链接是https://arxiv.org/pdf/2512.03405，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03405。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1764892008389","title":"OneThinker: All-in-one Reasoning Model for Image and Video","description":"Reinforcement learning (RL) has recently achieved remarkable success in eliciting visual reasoning within Multimodal Large Language Models (MLLMs). However, existing approaches typically train separate models for different tasks and treat image and video reasoning as disjoint domains. This results in limited scalability toward a multimodal reasoning generalist, which restricts practical versatility and hinders potential knowledge sharing across tasks and modalities. To this end, we propose OneThinker, an all-in-one reasoning model that unifies image and video understanding across diverse fundamental visual tasks, including question answering, captioning, spatial and temporal grounding, tracking, and segmentation. To achieve this, we construct the OneThinker-600k training corpus covering all these tasks and employ commercial models for CoT annotation, resulting in OneThinker-SFT-340k for SFT cold start. Furthermore, we propose EMA-GRPO to handle reward heterogeneity in multi-task RL by tracking task-wise moving averages of reward standard deviations for balanced optimization. Extensive experiments on diverse visual benchmarks show that OneThinker delivers strong performance on 31 benchmarks, across 10 fundamental visual understanding tasks. Moreover, it exhibits effective knowledge transfer between certain tasks and preliminary zero-shot generalization ability, marking a step toward a unified multimodal reasoning generalist. All code, model, and data are released.","url":"https://huggingface.co/papers/2512.03043","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.389Z","metadata":{"authors":["Kaituo Feng","Manyuan Zhang","Hongyu Li","Kaixuan Fan","Shuang Chen","Yilei Jiang","Dian Zheng","Peiwen Sun","Yiyuan Zhang","Haoze Sun","Yan Feng","Peng Pei","Xunliang Cai","Xiangyu Yue"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2512.03043.pdf","arxivUrl":"https://arxiv.org/abs/2512.03043","arxivId":"2512.03043","coolPaperUrl":"https://papers.cool/arxiv/2512.03043","published":"2025-12-02T18:59:52Z","updated":"2025-12-02T18:59:52.000Z","zh_summary":"强化学习（RL）最近在激发多模态大型语言模型（MLLMs）中的视觉推理方面取得了显著成功。然而，现有方法通常为不同任务训练独立的模型，并将图像和视频推理视为不相关的领域。这导致向多模态推理通用模型扩展的能力有限，限制了实际的多功能性，并阻碍了跨任务和模态的潜在知识共享。为此，我们提出了 OneThinker，这是一种一体化推理模型，统一了图像和视频理解，涵盖多种基础视觉任务，包括问答、描述生成、空间和时间定位、跟踪以及分割。为了实现这一目标，我们构建了覆盖所有这些任务的 OneThinker-600k 训练语料库，并采用商业模型进行 CoT 标注，从而得到用于 SFT 冷启动的 OneThinker-SFT-340k。此外，我们提出了 EMA-GRPO 来处理多任务 RL 中的奖励异质性，通过跟踪各任务奖励标准差的移动平均值，实现平衡优化。在多种视觉基准上进行的大量实验表明，OneThinker 在 10 个基础视觉理解任务的 31 个基准上表现出强大性能。此外，它在某些任务之间具有有效的知识迁移能力，并表现出初步的零样本泛化能力，标志着朝着统一多模态推理通用模型迈出了一步。所有代码、模型和数据均已发布。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有视觉推理模型“任务专一、模态割裂”导致的可扩展性差、知识迁移受限的问题，提出一个统一的多模态推理通才模型 OneThinker，使其在**单一套参数**下同时完成图像与视频的 10 余项基础视觉任务（问答、描述、定位、跟踪、分割等），并通过大规模多任务强化学习实现跨任务、跨模态的知识共享与零样本泛化。\n\nQ2: 有哪些相关研究？\n\n- **RL-for-LLM 推理**\n- DeepSeek-R1：GRPO 算法激发逐步推理\n- Dr.GRPO、GSPO、Critique-GRPO：改进 GRPO 的方差或反馈机制\n- **MLLM 视觉推理**\n- 图像问答：Vision-R1、SophiaVL-R1\n- 视频问答：Video-R1、VideoChat-R1、Time-R1\n- 感知任务：Perception-R1（检测）、Seg-R1（分割）、VLM-R1\n- **多任务/多模态统一**\n- VideoChat-R1 仅三任务 18 k 样本，局限视频模态\n- 其余工作通常单任务或单模态，未形成图像-视频-任务统一框架\n\nQ3: 论文如何解决这个问题？\n\n论文通过以下三大组件将“任务-专一、模态-割裂”问题转化为“统一多模态通才”问题：\n\n1. 数据层面：构建双阶段语料\n\n- OneThinker-600k：覆盖 10 类视觉任务、图像-视频双模态，共约 60 万样本，用于 RL 训练。\n- OneThinker-SFT-340k：用 Seed1.5-VL 生成并过滤高质量 CoT，作为 SFT 冷启动。\n2. 算法层面：提出 EMA-GRPO\n针对多任务奖励异质性，维护**任务专属**的指数滑动平均奖励标准差  σ_τ(t) ，实现\n\n- 任务内：消除低方差 rollout 被过度更新的偏差；\n- 任务间：避免稀疏奖励任务（如数学）主导、密集奖励任务（如检测）被抑制。\n优势函数统一为\n\nA_i^τ(t)=R_i-mean(R_j)σ_τ(t)\n\n保证跨任务梯度贡献均衡。\n3. 训练与推理协议\n\n- 所有任务统一为“<think>…</think><answer>…</answer>”文本接口；感知任务输出 JSON 结构，可直接计算格式奖励与精度奖励。\n- 联合训练后，单一套参数在 31 个基准上取得 SOTA 或可比性能，并展现跨任务、跨模态知识迁移与零样本泛化能力。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“统一模型能否在 10 类视觉任务、31 项基准上同时取得强劲性能”展开，分 5 个层次：\n\n1. 主实验：31 基准全面评估\n\n- 图像问答 8 项（MMMU、MathVista 等）\n- 视频问答 7 项（VideoMMMU、LongVideo-Reason 等）\n- caption 3 项（MMSci-Cap、VideoMMLU-Cap 等）\n- 时序定位 3 项（Charades、ActivityNet、ANet-RTL）\n- 空间定位 3 项（RefCOCO/+ /g）\n- 时空定位 1 项（STVG）\n- 跟踪 1 项（GOT-10k）\n- 分割 4 项（RefCOCO 系列、MeViS、ReasonVOS）\n结果：OneThinker-8B 全部优于或媲美专用/开源最强模型，平均提升 2–30 分。\n2. 消融实验\n\n- 仅 SFT → 平均掉 2.8 分\n- 标准 GRPO → 掉 2.5 分（ intra-task 偏差）\n- Dr.GRPO → 掉 2.0 分（ inter-task 偏差）\n验证 EMA-GRPO 对双失衡的必要性。\n3. 统一训练收益分析\n分别去掉空间定位/时序定位/图像 QA 数据，观察到：\n\n- 无时序定位 → 视频 QA 与跟踪显著下降\n- 无空间定位 → 图像 QA 与分割下降\n- 无图像 QA → 视频 QA 掉 11 分\n证实跨任务、跨模态知识共享真实存在。\n4. 零样本泛化\n在 MMT-Bench 162 项**未见过**任务中，OneThinker 在点跟踪、图像质量评估、旋转目标检测等 6 项上平均领先基线 10+ 分，展示通才迁移能力。\n\n5. 效率与可扩展性\n\n- 全训练 32×H800 10 天完成\n- 128 帧视频、4096 token 输出长度下，推理速度持平同规模基线模型\n\nQ5: 有什么可以进一步探索的点？\n\n- **统一奖励设计**\n当前不同任务仍依赖手工规则或外部奖励模型，可探索**跨任务可学习的统一奖励函数**，减少人工标定与模型调用开销。\n\n- **任意时空粒度推理**\n视频任务以固定 1 s 帧采样输出，可扩展到**任意时刻、任意框粒度**的连续时空定位，支持更细粒度事件解析。\n\n- **多轮交互与工具调用**\n现模型仅单轮输出，可引入**多轮对话+工具 API**（计算器、SAM、搜索引擎），实现复杂推理闭环，迈向 agent 级视觉推理。\n\n- **高效长视频建模**\n128 帧上限仍可能遗漏长时依赖，可结合**记忆机制/视频压缩 Token**，在千卡规模下训练小时级长视频通才模型。\n\n- **跨模态知识蒸馏**\n利用 OneThinker 的跨任务知识，设计**自蒸馏**或**小模型蒸馏**方案，压缩为端侧可用的 1–3 B 统一小模型。\n\n- **开放世界持续学习**\n目前为一次性训练，后续可研究**任务增量/模态增量**场景下的持续 RL 微调，避免灾难性遗忘，实现真正的终身多模态推理。\n\nQ6: 总结一下论文的主要内容\n\n**OneThinker：图像-视频统一推理通才**\n\n1. 问题\n现有视觉推理模型“一任务一模态”，缺乏可扩展性与跨任务知识迁移。\n\n2. 方案\n\n- 数据：OneThinker-600k（60 万样本，10 任务，图像+视频）+ OneThinker-SFT-340k（CoT 冷启动）。\n- 算法：EMA-GRPO，用任务专属指数滑动平均奖励标准差  σ_τ(t)  同时消除任务内与任务间优化失衡。\n- 协议：统一“<think>…</think><answer>…</answer>”文本接口，感知任务输出 JSON，可直接计算奖励。\n3. 结果\n\n- 31 项基准全面领先，平均提升 2–30 分；展现跨任务、跨模态知识共享与零样本泛化。\n- 消融与剔除实验验证统一训练与 EMA-GRPO 的必要性。\n4. 意义\n首次在单一套参数下实现图像-视频-任务全覆盖的多模态推理通才，为可扩展、可迁移的视觉 AGI 提供新基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是OneThinker%3A%20All-in-one%20Reasoning%20Model%20for%20Image%20and%20Video，链接是https://arxiv.org/pdf/2512.03043，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03043。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1764892008389","title":"SpaceTools: Tool-Augmented Spatial Reasoning via Double Interactive RL","description":"Vision Language Models (VLMs) demonstrate strong qualitative visual understanding, but struggle with metrically precise spatial reasoning required for embodied applications. The agentic paradigm promises that VLMs can use a wide variety of tools that could augment these capabilities, such as depth estimators, segmentation models, and pose estimators. Yet it remains an open challenge how to realize this vision without solely relying on handcrafted prompting strategies or enforcing fixed, predefined tool pipelines that limit VLMs' ability to discover optimal tool-use patterns. Reinforcement Learning could overcome this gap, but has so far been limited to reasoning with a single visual tool due to the large search space in multi-tool reasoning. We introduce Double Interactive Reinforcement Learning (DIRL), a two-phase training framework where VLMs learn to coordinate multiple tools through interactive exploration and feedback. In the teaching phase, we combine demonstrations from a single tool specialist trained via interactive RL with traces from a frontier model using all tools. In the exploration phase, the model further refines multi-tool coordination through continued RL. Our model, SpaceTools, with tool-augmented spatial reasoning ability, achieves state-of-the-art performance on spatial understanding benchmarks (RoboSpatial-Home, BLINK, BOP-ASK) and demonstrates reliable real-world manipulation using a 7-DOF robot as a tool. DIRL provides substantial improvements over the vanilla SFT (+12% on RoboSpatial) and RL (+16% on RoboSpatial) baselines. Project page: https://spacetools.github.io/.","url":"https://huggingface.co/papers/2512.04069","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.389Z","metadata":{"authors":["Siyi Chen","Mikaela Angelina Uy","Chan Hee Song","Faisal Ladhak","Adithyavairavan Murali","Qing Qu","Stan Birchfield","Valts Blukis","Jonathan Tremblay"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.CV","cs.RO"],"pdfUrl":"https://arxiv.org/pdf/2512.04069.pdf","arxivUrl":"https://arxiv.org/abs/2512.04069","arxivId":"2512.04069","coolPaperUrl":"https://papers.cool/arxiv/2512.04069","published":"2025-12-03T18:50:04Z","updated":"2025-12-03T18:50:04.000Z","zh_summary":"视觉语言模型（VLMs）展现了强大的定性视觉理解能力，但在需要精确度量的空间推理任务中仍存在困难，这类任务对于具身应用非常关键。智能体范式表明，VLMs 可以利用多种工具来增强这些能力，例如深度估计器、分割模型和姿态估计器。然而，如何在不依赖手工设计的提示策略或不强制固定的预定义工具流水线（这些方法会限制VLMs发现最优工具使用模式的能力）的情况下实现这一愿景，仍然是一个开放挑战。强化学习可以弥补这一空白，但迄今为止，由于多工具推理中搜索空间过大，其应用仍局限于单一视觉工具的推理。我们提出了双重交互强化学习（DIRL），这是一种两阶段训练框架，VLMs 可通过交互探索和反馈学习多工具协同。在教学阶段，我们将单工具专家通过交互式强化学习训练得到的演示与使用所有工具的前沿模型的轨迹结合。在探索阶段，模型通过持续的强化学习进一步优化多工具协同能力。我们的模型 SpaceTools，具备工具增强的空间推理能力，在空间理解基准（RoboSpatial-Home、BLINK、BOP-ASK）上实现了最先进的性能，并展示了使用 7 自由度机器人作为工具进行可靠的实际操作。DIRL 在基础 SFT（在 RoboSpatial 上提升 +12%）和 RL（在 RoboSpatial 上提升 +16%）上均提供了显著改进。项目主页：https://spacetools.github.io/。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“视觉-语言模型（VLM）在需要度量级精确空间推理的具身应用中表现不足”这一核心问题。具体而言：\n\n- **定性 vs. 定量**：现有 VLM 擅长开放域视觉问答，但在回答“物体间距几厘米”“抓取姿态是否可行”等需要精确几何度量与多步 3D 感知的问题时，准确率显著下降。\n- **工具增强瓶颈**：虽然“让 VLM 调用外部视觉工具（深度估计、分割、位姿估计等）”被寄予厚望，但传统方法要么依赖手工提示、固定流水线，要么因工具组合空间爆炸导致强化学习探索失败。\n- **可扩展训练缺失**：端到端微调需要海量标注，且每新增一项低层感知能力（如深度、指向、3D 框）就要重新设计数据与架构，难以持续扩展。\n\n为此，作者提出 **Double Interactive RL（DIRL）** 框架，让 VLM 在**教学阶段**先通过单工具 IRL 与前沿模型演示掌握基本工具调用，再在**探索阶段**以多工具 IRL 自主优化组合策略；并构建 **Toolshed** 分布式平台，在训练时实时 serving 10+ 计算密集型视觉/机器人工具。最终模型 **SpaceTools** 在 RoboSpatial-Home、BLINK、BOP-ASK 等空间推理基准及真实 7-DoF 机械臂抓取任务上取得 SOTA，验证了“通过交互式工具协同而非单纯增大模型或数据”即可显著提升度量级空间推理与具身决策能力。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究归为三大脉络，并在表格 1 中与本文方法做对比。以下按类别归纳主要工作（均来自原文引用编号）：\n\n1\\. 空间推理与 VLM\n\n- **通用 VLM 的空间短板**\n- GPT-4o \n45\n、Gemini-ER 1.5 \n61\n、LLaVA-NeXT \n28\n、Qwen2.5-VL \n3\n 等在大规模 VQA 上表现强劲，但在度量几何、3D 关系、遮挡推理上误差大 \n22, 40, 71, 77\n。\n- **显式注入 3D/几何先验**\n- SpatialVLM \n9\n、SpatialLLaVA \n10\n、SpatialBot \n7\n、SpatialPIN \n35\n、SpatialReasoner \n36, 37\n 通过额外 3D 监督或网络改造提升深度/姿态估计，但需重训整个模型且数据昂贵。\n- **机器人场景空间基准**\n- RoboSpatial-Home \n55\n、RefSpatial \n83\n、BLINK \n16\n、BOP-ASK \n4\n、CVBench \n60\n 提供指向、深度、位姿、抓取等多维评测，本文全部纳入实验。\n\n2\\. 工具增强推理（Tool-augmented Reasoning）\n\n- **LLM 工具使用**\n- WebGPT \n43\n、ReAct \n21\n、ToolACE \n30\n 让语言模型调用搜索、计算器、代码解释器，但多为文本模态。\n- **VLM 视觉工具链**\n- Visual Programming \n17\n、ViPer \n57\n、m&m’s \n39\n、ToolVQA \n73\n 用硬编码或提示模板拼接分割、检测、深度模型，缺乏训练阶段交互，工具出错无法自恢复。\n- **并发工作**\n- TIGeR \n18\n 同样面向机器人几何推理，但采用“大模型生成代码+固定工具流水线+离线重写”方式，训练阶段不执行工具；本文 DIRL 在交互中实时调用 10+ 工具并优化组合策略。\n\n3\\. 强化学习用于推理（RL for Reasoning）\n\n- **数学/代码/通用 VQA**\n- DeepSeekMath \n51\n、Kimi-k1.5 \n62\n、R1-VL \n78\n 等用 GRPO/PPO 提升可验证任务准确率。\n- **空间-视觉 RL**\n- ViGoRL \n50\n 首次证明“单工具（crop）+ IRL”可提升指向与关系推理，但仅 1 个轻量工具；直接扩展到多工具会因动作空间爆炸而失效 \n23, 44, 64\n。\n- Robot-R1 \n23\n、SVQA-R1 \n64\n、Visual-RFT \n33\n 在机器人或 3D 问答中采用步级或链式奖励，同样未解决多工具联合探索难题。\n- **本文差异**\nDIRL 通过“单工具 IRL 教学 → 多工具 IRL 探索”两阶段分解，将搜索空间从一次性 |Tools|! 级别降至渐进式课程，实现 10+ 异构工具（分割、深度、3D bbox、抓取、机器人执行）在线协同训练。\n\n4\\. 系统与基础设施\n\n- 先前工作要么工具简单（裁剪）\n50\n，要么预计算离线输出 \n18\n，要么 API 延迟高无法支撑 RL 大规模 rollout。\n- Toolshed 借鉴 Ray 分布式框架，实现工具容器化、异步并行、弹性扩缩，与 VERL \n53\n 无缝衔接，首次在 VLM-RL 训练环路中实时 serving 高算力视觉/机器人模块。\n\n小结\n\n本文在“空间推理+工具增强+交互 RL”三者的交叉点上定位空白：\n\n- 纯微调或纯提示的方法难以获得度量级精度；\n- 单工具 RL 无法扩展到多工具组合；\n- 多工具流水线缺乏训练阶段在线执行与自我修正。\n\nDIRL 与 Toolshed 的提出正是为了填补上述缺口，使 VLM 在不改变骨干网络的前提下，通过可扩展的交互式工具协同获得强几何感知与具身决策能力。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Double Interactive Reinforcement Learning（DIRL）** 框架与 **Toolshed** 分布式工具平台，从“训练范式”与“系统支撑”两条线同时切入，解决“VLM 如何在多工具、度量级空间推理任务中实现稳定、可扩展的在线协同”难题。核心思路可概括为：\n\n> **单工具 IRL 先教会“ grounded 指向” → 前沿模型生成多工具演示 → SFT 建立初始协同 → 多工具 IRL 继续自我优化 → Toolshed 实时 serving 工具**\n\n具体步骤如下（均对应原文章节与算法）：\n\n1\\. 问题建模：多轮决策过程\n\n将空间问答或机器人操纵形式化为\n\nπ_θ(h_t) arrow a_t ∈ reason, <tool_call>, <answer>\n\n的历史增强 MDP（Algorithm 1）。VLM 可在  T_(max)  轮内连续调用多种视觉/机器人工具，直至输出最终答案。\n\n2\\. DIRL 两阶段训练\n\n2.1 教学阶段（Teaching Phase）——**“先学走路再学跑”**\n\n- **单工具 IRL**：仅在“指向（pointing）”工具上跑 Interactive RL，搜索空间小，易收敛，得到 **IRL-Teacher**。\n- **前沿模型演示**：用 Claude Sonnet 4.5 + 全工具集解决同一批空间任务，保留正确轨迹，得到 **Universal-Teacher**。\n- **混合 SFT**：将 IRL-Teacher 轨迹（2k）与 Universal-Teacher 轨迹（6k）合并，对基模型做监督微调，获得**会初步调用多工具的  π_(SFT) **。\n\n2.2 探索阶段（Exploration Phase）——**“真刀真枪在线练”**\n\n- 从  π_(SFT)  初始化，重新开放**全部 10+ 工具**（分割、深度、3D bbox、抓取、机器人 capture/place 等）。\n- 采用 **Group Relative Policy Optimization (GRPO)**，每批并行 rollout  N  条多轮轨迹，用任务级奖励（IoU、NNDC、NNCE 等）做相对优势估计，并加 KL 正则防止偏离  π_(SFT) 。\n- 通过**在线错误信号**持续优化工具选择顺序、参数解析与失败恢复策略，最终得到 **SpaceTools**。\n\n3\\. Toolshed：让“工具随时可用”\n\n- **分布式 Actor 模型**：每个工具以独立 Python 容器运行，支持异步并行、弹性扩缩，避免一次工具调用阻塞整批 rollout。\n- **统一数据协议**：文本、图像、点云、mask、抓取姿态等皆以标准化变量名在 VLM 与工具间传递，实现**跨节点零拷贝**。\n- **无缝接入训练与推理**：训练时与 VERL 集成，推理时通过轻量 API 即可给 GPT-4o、Claude 等“外挂”工具链，增强其空间推理。\n\n4\\. 奖励设计：把“几何误差”变成“可学习信号”\n\n针对不同任务给出**归一化、可微或分段可微**奖励：\n\n- 指向： R_(NNDC) = exp(-5d)  再与二值精度取 max，强调“离中心越近分越高”。\n- 2D 框：Mean-IoU 直接计算。\n- 位姿：将 8 个 3D 角点投影成 2D 凸包后算 IoU。\n- 抓取：提出 **NNCE**（Normalized Negative Coordinate Error），同时惩罚中心偏移与两指方向误差。\n所有奖励均压缩到 $\n0,1\n$，保证不同任务梯度尺度一致，便于多任务混合训练。\n\n5\\. 结果验证：从基准到真实机器人\n\n- **Benchmark**：在 RoboSpatial-Home、BLINK、RefSpatial、BOP-ASK、CVBench 等 5 大空间推理数据集上，3B 参数的 SpaceTools 平均领先次优模型 **7–24%**。\n- **真实机器人**：把“机器人”也当成一种工具（capture / grasp / place），在 7-DoF Kinova 臂上完成 Pick、Relational Pick、Pick&Place 三类任务，**86% 总体成功率**，显著高于 GPT-5+Toolshed (65%) 与 Claude+Toolshed (79%)，且首次运动时间缩短到 10s 级。\n\n一句话总结\n\n论文通过“**先单工具 IRL 打地基 → 再多工具 IRL 精修协同**”的课程式强化学习，加上“**Toolshed 实时工具服务**”的系统支撑，使 VLM 无需修改骨干网络即可在训练与推理阶段**动态调用多种视觉/机器人模块**，从而系统性地解决了“度量级空间推理难”“多工具组合搜索空间爆炸”“工具调用与错误恢复无法端到端学习”三大痛点。\n\nQ4: 论文做了哪些实验？\n\n论文从“空间推理基准测评 → 真实机器人操纵 → 消融与超参 → 零样本工具增强”四个层次展开实验，覆盖 5 大公开数据集与 7-DoF 机械臂现场任务，具体列示如下（均对应原文表/图编号）：\n\n1\\. 空间推理基准实验（Table 2 + Figure 3）\n\n**数据集**\n\n- RoboSpatial-Home（空间 VQA + 空闲区域指向）\n- BLINK（相对深度二选一）\n- RefSpatial（物体可放性/位置/未出现检测）\n- CVBench（2D 关系 + 3D 深度）\n- BOP-ASK（抓取可行性问答）\n\n**指标**\n\n- 分类：Accuracy\n- 指向：NNDC→Accuracy\n- 2D 框：Mean-IoU\n- 位姿：8 角点凸包 IoU\n- 抓取：MACE（°）+ Success Rate（MACE>40% 比例）\n\n**对比组别**\n\n1. 商用封闭模型：Claude-Sonnet-4.5、GPT-4o、GPT-5、Gemini-ER-1.5\n2. 开源通用 VLM：LLaVA-NeXT-8B、Qwen2.5-VL-32B/3B\n3. 空间专用 VLM：SpaceLLaVA-13B、RoboPoint-13B、Molmo-7B、RoboBrain2.0-7B、RoboRefer-8B-SFT\n4. 同底座无工具训练：Qwen2.5-VL-3B-Tool-free-SFT、Qwen2.5-VL-3B-Tool-free-RL（DeepSeek-R1 风格推理 RL）\n\n**结果**\n\n- SpaceTools-3B 在 10 项子任务中 **9 项第一、1 项第二**；整体平均领先最强基线（Gemini-ER-1.5）**7.5%**，领先同尺寸无工具 RL **+16%**。\n\n2\\. 真实机器人操纵实验（Table 3 + Table 7 + Figure 4/8/9）\n\n**平台**\nKinova Jaco 7-DoF 臂 + ZED2 RGB-D + CuRobo 运动规划器；机器人接口封装为 Toolshed 的 6 个工具（capture\\_image / get\\_depth / execute\\_grasp / place\\_object…）。\n\n**任务协议**\n\n- Pick：7 个单物体抓取\n- Relational Pick：6 个带空间关系描述（更远/更近/左侧/右侧）的抓取\n- Pick & Place：14 个“先抓取再放置到指定容器/位置”组合，每任务 1 分（抓 0.5 + 放 0.5）\n\n**对照**\n\n- π₀.5（Vision-Language-Action 端到端模型）\n- GPT-5 + Toolshed\n- Claude-Sonnet-4.5 + Toolshed\n- Qwen2.5-VL-3B + Toolshed（零样本）\n\n**结果**\n\n| Model | Pick | Rel.Pick | P&P | TTFM |\n| --- | --- | --- | --- | --- |\n| SpaceTools | 86% | 83% | 86% | 10 s |\n| Claude+Tools | 86% | 50% | 79% | 30 s |\n| GPT-5+Tools | 71% | 33% | 65% | 36 s |\n| π₀.5 | 0% | 0% | 0% | 1 s |\n\n此外给出单任务细粒度失败分析（软玩具无法区分、边界点选取导致放置失败等，见图 9、12）。\n\n3\\. 消融与组件分析（Table 4 + Table 9–11）\n\n**A. 训练配方消融**\n\n- w/o IRL-Teacher：-11.4% RoboSpatial\n- w/o Universal-Teacher：-4.9% 且 Pose 降 25%\n- w/o Stage-2 IRL：-2.3%\n- 纯 Tool-SFT 或 Tool-NIRL（非交互 RL）：平均再低 13–14 点\n\n**B. 直接全工具 IRL**\n取消课程式，一上来就用 10+ 工具跑 IRL，RoboSpatial 掉到 52.9%（对比 70.0%），验证搜索空间爆炸问题。\n\n**C. 奖励与提示消融（1k 子集）**\n\n- NNDC+Clip+Norm+示例提示 最优（35.3%）\n- 去掉 Clip/Norm/示例 均显著下降；加格式奖励无额外收益。\n\n**D. 数据配比与规模**\n\n- 含 2D 框（Grounding）数据即使测试集无该任务，仍能提升 RoboSpatial-Home 13 点\n- 纯增样本到 8k 收益饱和，说明**多样性> brute 数量**\n\n4\\. 零样本工具增强测试（Table 5）\n\n把 Toolshed 直接挂给未微调的 GPT-5/Claude，观察“工具反馈能否即时提升空间推理”：\n\n| Model | RefSpatial | Pose | Grasp-MACE |\n| --- | --- | --- | --- |\n| GPT-5 | 23.1 → 36.1 (+13.0) | 9.0 → 15.0 (+6.0) | 39.6 → 41.5 (+1.9) |\n| Claude | 7.5 → 27.8 (+20.3) | 1.7 → 25.0 (+23.3) | 40.1 → 44.2 (+4.1) |\n\n表明工具链可缓解前沿模型的几何 grounding 缺陷，但对高阶语义任务提升不一，佐证“交互式训练 > 零样本外挂”。\n\n5\\. 可视化与失败案例（Figure 3/10–12）\n\n- 展示相对深度、位姿、抓取、空间兼容等成功推理全链路\n- 给出抓取工具在拥挤场景找不到无碰撞姿态、指向工具误定位导致放置到边界等典型失败，指出未来需更紧耦合的 2D-3D 验证与机器人反馈。\n\n总结\n\n实验既覆盖“静态图片几何问答”又覆盖“闭环感知-操纵”两大场景，数量上包含\n\n- 5 基准 × 10 子任务 × 4 类基线 ≈ 200 组精度对比\n- 27 组真实机器人任务 × 4 方法 = 108 次实体试验\n- 多维度消融 > 50 组\n全面验证 DIRL 与 Toolshed 在精度、效率、泛化、真实可部署性上的优势。\n\nQ5: 有什么可以进一步探索的点？\n\n论文在附录 A 与未来工作部分已给出若干开放问题，结合最新实验结果，可进一步探索的关键点归纳如下：\n\n1\\. 任务与场景扩展\n\n- **长周期多阶段任务**\n目前实验集中在“单步或 8-10 轮短序列”的空间问答与 pick&place。可扩展到：\n\n- 桌面整理、冰箱装货、多房间搜寻等“小时级”长程任务，考察模型对工具链的**长期规划与错误恢复**。\n- 引入部分可观察环境（物体被遮挡、灯光变化），测试模型是否能主动调用“探索性”工具（如移动相机、开关灯）。\n- **更丰富的物理环境**\n- 与物理引擎深度集成（Isaac Sim、NVIDIA Cosmos），加入碰撞、质心、摩擦等属性，研究工具是否需要返回**不确定性或置信度**供 VLM 风险评估。\n- 大规模众包场景：让模型在 Minecraft、VR 中通过工具链解决用户实时提出的空间任务，收集多样化失败数据。\n\n2\\. 工具形态与协同机制\n\n- **视觉输出级工具**\n当前工具主要返回结构化文本/变量。下一步让工具直接输出**图像或热力图**（例如深度不确定性图、抓取碰撞热力图），VLM 需要学会“看工具结果再决定”，引入跨模态注意力或图像-文本融合策略。\n\n- **工具可靠性建模**\n- 为每个工具引入**运行时置信度**或**失败码**，VLM 维护隐式或显式的“工具可信度”先验，实现贝叶斯式选择/融合。\n- 研究**工具升级热插拔**：升级单个工具（如 Depth-Pro→Depth-Pro-v2）时，无需重训 VLM，仅通过少量元数据或 prompt 即可保持整体协同性能。\n- **自适应工具链生成**\n目前仍需人工定义工具 API。可探索：\n\n- 让 VLM 在代码执行器里**自动生成微脚本**（Python+OpenCV）完成全新几何运算，实现“工具即代码”的自扩展。\n- 结合神经-符号方法，学习**高阶工具组合原语**（loop、if、retry），形成可复用的空间推理子程序。\n\n3\\. 训练与算法改进\n\n- **逐步/细粒度奖励**\n现有奖励只在最终答案层。可设计**步级奖励**：\n\n- 指向正确→+0.3，深度误差<5cm→+0.2，抓取无碰撞→+0.5，放置成功→+1.0。\n- 结合过程监督（PRM）或优势-演员-评论家，减少稀疏奖励导致的早期探索崩溃。\n- **分层强化学习**\n上层策略负责“选哪种工具类”（感知 vs 行动），下层策略负责“调用哪个具体参数”，降低动作空间并提升样本效率。\n\n- **多模态课程与自我博弈**\n- 课程从“合成渲染→真实照片→真实机器人”渐进，以缓解 sim2real 差距。\n- 自我博弈：让两个 VLM 互为“任务生成器”与“求解器”，自动生成更具挑战的空间谜题，实现无监督数据扩增。\n\n4\\. 系统与效率优化\n\n- **实时机器人内循环训练**\n目前仅推理阶段用真机，训练用 mock 工具。未来需：\n\n- 低延迟调度+经验回放，把真实失败样本即时回灌 RL，实现**真机在线 fine-tune**（可借鉴 OTG-RL、Safe-RL）。\n- 开发**轻量级工具近似器**（如 8-bit 深度网络）或**缓存相似场景结果**，减少 GPU 占用与推理延迟。\n- **异构资源调度**\n引入动态批处理、预测式弹性伸缩，解决“高分辨率分割模型突然吃满显存”导致的训练抖动；结合 serverless 实现工具按调用量计费，降低大规模实验成本。\n\n5\\. 评测与可解释性\n\n- **细粒度错误归因基准**\n建立“工具级失败标签”数据集（指向漂移、深度跳变、抓取碰撞、放置边界），支持社区统一诊断是“感知错”“策略错”还是“工具错”。\n\n- **可解释空间推理**\n要求模型在 <think> 中显式输出**工具不确定性、备选方案、风险权衡**，并用注意力可视化指向/抓取区域，供机器人工程师审核安全。\n\n- **公平性与泛化度量**\n考察在不同相机内参、光照、文化场景（中式 vs 西式餐桌）下，工具增强是否比端到端模型**更公平地跨域泛化**，减少隐含偏见。\n\n6\\. 安全与伦理\n\n- **工具滥用防护**\n当 VLM 被提示“用机械臂砸东西”时，需在工具层或策略层加入**硬安全约束**（力矩限制、人类检测暂停），并研究如何在 RL 奖励中嵌入安全惩罚。\n\n- **隐私敏感场景**\n若工具链涉及深度摄像头扫描家庭环境，需探索**设备端局部推理+工具联邦调用**，避免原始图像上传云端。\n\n总结\n\n下一步工作可从“**更长更杂的任务**、**更智能的工具协同与自生成**、**细粒度奖励与真机在线学习**、**系统级高效安全部署**”四条主线推进，逐步把“工具增强空间推理”从桌面演示推向真实世界的长周期、高可靠、可解释具身智能系统。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：视觉-语言模型（VLM）在需要度量级精确空间推理的具身任务中表现不足，且简单依赖外部视觉工具会因搜索空间爆炸而难以用强化学习直接训练。\n- **方法**：提出 **Double Interactive RL（DIRL）** 两阶段框架——先单工具 IRL 教会“指向”，再用前沿模型演示+SFT 建立多工具协同，最后全工具 IRL 在线优化；配套构建 **Toolshed** 分布式平台，在训练/推理时实时异步 serving 10+ 计算密集型视觉与机器人工具。\n\n- **结果**：3B 参数的 **SpaceTools** 在 5 大空间推理基准上取得 SOTA，真实 7-DoF 机械臂抓取-放置任务成功率 86%，显著优于同等工具配置的 GPT-5/Claude 等基线，验证“通过交互式工具协同而非增大模型”即可获得强度量几何感知与具身决策能力。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SpaceTools%3A%20Tool-Augmented%20Spatial%20Reasoning%20via%20Double%20Interactive%20RL，链接是https://arxiv.org/pdf/2512.04069，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04069。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1764892008390","title":"Rethinking Prompt Design for Inference-time Scaling in Text-to-Visual Generation","description":"Achieving precise alignment between user intent and generated visuals remains a central challenge in text-to-visual generation, as a single attempt often fails to produce the desired output. To handle this, prior approaches mainly scale the visual generation process (e.g., increasing sampling steps or seeds), but this quickly leads to a quality plateau. This limitation arises because the prompt, crucial for guiding generation, is kept fixed. To address this, we propose Prompt Redesign for Inference-time Scaling, coined PRIS, a framework that adaptively revises the prompt during inference in response to the scaled visual generations. The core idea of PRIS is to review the generated visuals, identify recurring failure patterns across visuals, and redesign the prompt accordingly before regenerating the visuals with the revised prompt. To provide precise alignment feedback for prompt revision, we introduce a new verifier, element-level factual correction, which evaluates the alignment between prompt attributes and generated visuals at a fine-grained level, achieving more accurate and interpretable assessments than holistic measures. Extensive experiments on both text-to-image and text-to-video benchmarks demonstrate the effectiveness of our approach, including a 15% gain on VBench 2.0. These results highlight that jointly scaling prompts and visuals is key to fully leveraging scaling laws at inference-time. Visualizations are available at the website: https://subin-kim-cv.github.io/PRIS.","url":"https://huggingface.co/papers/2512.03534","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.390Z","metadata":{"authors":["Subin Kim","Sangwoo Mo","Mamshad Nayeem Rizve","Yiran Xu","Difan Liu","Jinwoo Shin","Tobias Hinz"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2512.03534.pdf","arxivUrl":"https://arxiv.org/abs/2512.03534","arxivId":"2512.03534","coolPaperUrl":"https://papers.cool/arxiv/2512.03534","published":"2025-12-03T07:54:05Z","updated":"2025-12-03T07:54:05.000Z","zh_summary":"在文本到视觉生成中，实现用户意图与生成视觉内容的精确对齐仍然是一个核心挑战，因为单次尝试往往无法生成预期的结果。为了解决这个问题，以往的方法主要通过扩展视觉生成过程（例如增加采样步数或种子）来处理，但这很快就会导致质量达到瓶颈。这一限制的原因在于用于指导生成的提示（prompt）保持固定。为此，我们提出了推理时提示重设计（Prompt Redesign for Inference-time Scaling，简称PRIS）框架，该框架能够在推理过程中根据扩展的视觉生成自适应地修改提示。PRIS 的核心思想是审核生成的视觉内容，识别视觉内容中反复出现的失败模式，并在用修改后的提示重新生成视觉内容之前重新设计提示。为了为提示修改提供精确的对齐反馈，我们引入了一种新的验证器——元素级事实校正（element-level factual correction），它在细粒度层面对提示属性与生成视觉内容之间的对齐进行评估，比整体性度量提供更准确、更可解释的评估。大量在文本到图像和文本到视频基准上的实验表明了我们方法的有效性，包括在 VBench 2.0 上实现了 15% 的提升。这些结果表明，在推理时同时扩展提示和视觉内容是充分利用规模法则的关键。可视化内容可在网站查看：https://subin-kim-cv.github.io/PRIS。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“文本–视觉生成”在推理阶段（inference-time）的 scaling 定律遭遇的瓶颈：\n仅靠扩大视觉采样（更多步数、更多种子、Best-of-N 等）很快出现性能平台，而提示词（prompt）始终保持不变，导致相同错误反复出现。\n\n为此，作者提出将 scaling 对象从“纯视觉”扩展到“提示词本身”，即 **Prompt Redesign for Inference-time Scaling（PRIS）**。核心目标可概括为：\n\n- 在推理阶段动态诊断已生成样本的**共性失败模式**\n- 基于细粒度验证结果**即时重写提示词**，强化被持续忽略的元素\n- 用重写后的提示词继续采样，实现**文本-视觉联合 scaling**，突破固定提示词下的对齐天花板\n\n简言之，论文要解决的核心问题是：\n\n> **如何在推理阶段充分利用额外计算量，同时 scaling 视觉采样与提示词，以持续提高复杂提示的忠实度，避免平台效应。**\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归为四条主线，并逐条指出其与 PRIS 的区别。关键文献与核心观点如下：\n\n1. **推理阶段视觉 scaling**\n\n- 增加采样步数或候选数：Best-of-N \n30\n、Search-over-Paths \n12\n、DAS \n19\n、RBF \n18\n、EvoSearch \n12\n\n- 共用局限：**仅扩大视觉搜索空间，提示词固定**，无法纠正因提示歧义导致的系统性失败。\n2. **文本-视觉提示词设计 / 改写**\n\n- 人工辅助探索：Promptify \n4\n、PromptCharm \n40\n\n- 自动扩展或优化：Prompt Expansion \n6, 11, 14\n\n- 局限：**单样本反馈**、需人工介入，或**与推理-scale 脱节**，不追踪跨样本的共性失败。\n3. **链式思维（CoT）与迭代反思**\n\n- 图像生成 CoT：T2I-R1 \n17\n、ImageGen-CoT \n25\n、Mint \n38\n、ReflectionFlow \n45\n\n- 区别：\n– 上述方法**针对单张样本**做逐步编辑或反思；PRIS 聚合**跨样本失败统计**再改写提示。\n– 多数需**额外训练**或专用编辑模型；PRIS **零训练**、即插即用。\n4. **视觉忠实度评估（verifier / reward model）**\n\n- 整体打分：VisionReward \n41\n、VideoAlign \n28\n、UnifiedReward \n39\n\n- 细粒度 VQA：DA-Score \n33\n\n- 本文提出 **EFC**，通过**元素级文本-文本推理**替代直接 VQA，降低肯定偏差，为零训练 prompt 改写提供可解释信号。\n\n综上，PRIS 首次把“提示词本身”纳入推理阶段 scaling 维度，与仅视觉 scaling 或单样本 prompt 调优形成鲜明对比。\n\nQ3: 论文如何解决这个问题？\n\n论文把“固定提示词”这一瓶颈拆解为**诊断→提炼→重写→再生成**的闭环，具体实现分为两大模块：\n\n1\\. 细粒度诊断：Element-level Factual Correction（EFC）\n\n- **元素拆解**\n将原始提示  p  映射为原子语义集合\n\np=p_1,…,p_s, quad p_i∈core,extra\n\ncore 元素客观且必须满足，extra 元素主观或风格化。\n\n- **文本-文本验证**\n\n1. 用 MLLM 为每幅视觉生成自然语言描述（caption）。\n2. 将每个  p_i  视为**文本假设**，与 caption 做自然语言推理（NLI）：\n- entailment：元素被满足\n- contradiction：元素被违背\n- neutral：caption 信息不足\n3. 对 neutral 元素，自动生成开放式问题再次询问视觉，把回答再送 NLI，最终只留 entailment / contradiction。\n- **优先 core 打分**\n样本得分 = 满足 core 元素的比例；相同时再比较 extra。\n\n2\\. 共性失败驱动的 Prompt 重写：PRIS 四步循环\n\n1. **Generation & Verification**\n用原提示生成  M  个样本，EFC 给出每条  p_i  的满足标签。\n\n2. **Top-k 选择**\n选**共同覆盖最多元素**的  k=lceil N/4 rceil  条样本，并记录其随机种子。\n\n3. **失败模式提炼与重写**\n\n- 统计 top-k 中**成功率 < 50 %** 的 core 元素 → 记为共性失败。\n- 用 MLLM 把原提示改写为  p' ：\n– 显式强化失败元素\n– 保留已满足元素与用户原意\n- 若无失败，则对提示做**探索式改写**以扩大搜索空间。\n4. **Regeneration**\n用  p'  和 top-k 的种子重新生成剩余  N-M  条样本；可迭代回到步骤 2。\n\n3\\. 与现有视觉 scaling 兼容\n\nPRIS 只改动提示，**不改动生成模型内部采样机制**，因此可无缝叠加在 BoN、DAS、RBF、EvoSearch 等方法之上：先视觉 scaling→取种子→PRIS 重写→再视觉 scaling，实现**双空间联合 scaling**。\n\n4\\. 训练无关 & 计算可控\n\n- EFC 与 PRIS 均基于**现成 MLLM**（Qwen2.5-VL），零训练。\n- 在相同 NFE 或相同 wall-clock 时间下，PRIS 把额外算力投向“诊断+提示改写”，而非盲目增加样本，获得更高边际收益。\n\n通过上述流程，论文把“如何持续提高复杂提示忠实度”转化为**可解释、可迭代、可扩展**的推理阶段算法，突破了固定提示词带来的性能平台。\n\nQ4: 论文做了哪些实验？\n\n论文从**定量指标、Scaling 曲线、Verifier 能力、消融与计算成本**四个层面展开系统实验，覆盖文本到图像（T2I）与文本到视频（T2V）两大任务。主要结果一览如下（均在与 Baseline 相同 NFE 或相同 wall-clock 条件下完成）：\n\n1\\. 固定计算预算下的对齐与质量\n\n| 任务 | 数据集 | 指标 | 主要提升 |\n| --- | --- | --- | --- |\n| T2I | GenAI-Bench（320 条复杂组合提示） | VQAScore（给定奖励）DA-Score（未见奖励）Aesthetic（未见） | +7.1 % / +3.2 % / 不下降 |\n| T2V | VBench-2.0（Controllability & Creativity） | 官方子项平均 | +13.88 %（1.3 B）+15.19 %（14 B） |\n\n- 基线：FLUX.1-dev / Wan2.1-1.3B / 14B + Best-of-N（BoN）或标准 prompt expansion（\\*）\n- PRIS 仅用 **½-⅔ 样本**做首轮生成，剩余预算用于重写后再生成，仍显著优于全量 BoN。\n\n2\\. Scaling 行为与迭代修订\n\n- **曲线实验**（图 1、5）\n- BoN 在 1e3 NFE 后 prompt 忠实度即饱和；PRIS 随 NFE 持续上升。\n- **迭代修订**（表 3）\n- 第一次重写即 **+7 %** VQAScore，第二次再 **+1.5 %**，验证“诊断-重写”比单纯加样本有效。\n\n3\\. 与现有视觉 scaling 方法叠加\n\n| 方法 | 给定奖励↑ | 未见奖励↑ | 美学↓ |\n| --- | --- | --- | --- |\n| SDXL + DAS | 0.657 → 0.700 | 0.671 → 0.688 | 5.819 → 5.897 |\n| FLUX.1-schnell + RBF | 0.922 → 0.936 | 0.706 → 0.723 | 5.426 → 5.528（止跌回升） |\n\n- PRIS 在保持 BoN 兼容的同时，**缓解 reward over-optimization**（图 10，文字印图现象消失）。\n\n4\\. Verifier benchmark 与选型\n\n- 自建 **410 条 prompt + 2k 视频** 数据集（GT vs 部分对齐 distractors）\n- EFC 准确率 **76.3 %**，显著高于最强学习奖励模型 VideoAlign（69.3 %）与分解 VQA（70.0 %）。\n- 细粒度解读：在 Motion-Order、Physics 等需要“时序/因果”判断的子集上领先幅度最大（>8 %）。\n\n5\\. 消融与计算时间\n\n- **common-failure vs per-sample 改写**（表 6）\n- T2V 上 per-sample 策略反而 **降低 9.2 %**；common-failure 提升 **4.3 %**，证明跨样本统计必要性。\n- **wall-clock 对比**（表 7）\n- 把 4× NFE 的 BoN 预算换成 1× NFE + EFC，PRIS 仍获得 **+4.4 %**（T2I）与 **+2.9 %**（T2V）增益，说明“诊断时间”比“盲增样本”更划算。\n\n6\\. 定性示例\n\n- 图 3、4、16-18 显示：\n- 复杂否定（“no laces”“not made of wood”）、数量比较、时序动作（“A then B”）等场景，PRIS 均能首次生成即正确，BoN 即使 20-40 样本仍反复出错。\n- 可视化网页（index.html）提供 100+ 图文/视频侧对侧对比。\n\n综上，实验覆盖 **2 任务 × 3 模型 × 5 指标 × 2 计算模式（NFE/时间）**，一致验证：\n\n> **“联合 scaling 提示+视觉”显著优于“仅视觉 scaling”**，且 EFC 提供的细粒度诊断是改写有效性的关键。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为论文打开的新“接口”，均围绕\\*\\*“把提示本身当成可缩放变量”\\*\\*这一核心思想展开，训练与推理阶段、多模态与跨任务场景均存在大量空白。\n\n1\\. 训练阶段：让生成器“天生”适应提示迭代\n\n- 在扩散模型 / 自回归 Transformer 的原始训练目标里显式加入“prompt-rewriting 鲁棒性”：\n- 同一语义对  (p, p', p'')  进行条件 dropout，鼓励模型对改写后的提示保持相同数据似然。\n- 可借鉴“instruction-following”强化学习方法（DPO、IPO），把“能否在两次改写内生成对齐样本”作为偏好信号。\n\n2\\. 轻量级 verifier：从分钟级到秒级\n\n- EFC 依赖大 MLLM，推理耗时≈3× 生成。可探索：\n1. **蒸馏**专用“alignment-checker”：输入帧序列+元素列表，直接输出 entailment 向量。\n2. **延迟-质量权衡搜索**：用早期浅层特征先粗略排序，再对 top-10 % 调用重型 EFC，实现自适应计算分配。\n3. **训练-free 的量化/投机解码**，把 NLI 部分换成更小模型或纯文本 LLM。\n\n3\\. 多轮对话式生成：用户意图漂移与在线纠错\n\n- 把 PRIS 嵌入交互式画布：\n- 用户每提供一次语言反馈 → 系统即时运行 EFC 找出“仍失败元素” → 自动重写提示并补生成。\n- 研究问题：如何维持**多轮一致性**（不引入新错误）与**风格一致性**（色调、角色外貌不变）？\n\n4\\. 跨模态扩展\n\n- **文本-到-3D / 文本-到-神经辐射场**：\n- 失败模式从“像素”升级为“几何”“拓扑”“可动关节”，提示改写需引入空间关系词汇。\n- **文本-到-音频**：\n- 元素拆解变为“节奏、旋律线、音色、混响”，验证器可用文本-音频 caption + NLI。\n- **组合生成**（图像+声音+视频）：\n- 研究不同模态间失败耦合：视频对了但声音节奏错，应如何统一重写提示？\n\n5\\. 黑盒 API 场景下的“无梯度提示优化”\n\n- 对于仅提供生成接口的商业模型（DALL·E 3、Sora）：\n- 把 PRIS 看成**黑盒优化**的迭代更新算子，可对比遗传算法、贝叶斯优化、强化提示搜索；\n- 研究 query 成本 vs 改进增益的 Pareto 前沿，给出最优停止准则。\n\n6\\. 失败模式库与迁移学习\n\n- 大规模收集“元素-失败”对，构建**可检索失败知识库**：\n- 新提示先语义匹配库中相似失败 → 直接给出“经验式改写”而无需重新生成 M 个样本。\n- 探索 meta-learner：输入失败向量，输出最优改写策略，形成“零样本 prompt 修正”。\n\n7\\. 安全与偏见：把诊断器当探针\n\n- 利用 EFC 的细粒度标签反向审计模型偏见：\n- 检查“core 元素”是否在某些人群属性（性别、肤色）上系统性地被 contradict；\n- 通过改写提示进行干预，量化偏见下降程度，成为动态去偏工具。\n\n8\\. 理论视角：提示空间的 scaling law\n\n- 目前只验证“视觉样本数 + 提示迭代次数”联合增益，可进一步拟合\n\nAlign(N, R) propto (N^α + λ R^β)\n\n其中  R  为重写次数，研究  α, β, λ  在不同模型/任务下的稳定性，为“分配计算预算”提供理论依据。\n\n9\\. 统一框架：理解-生成一体化模型\n\n- 当前 PRIS 外挂 MLLM 诊断 + 独立生成模型。未来可在**统一自回归或扩散架构**内部：\n- 把“诊断-重写”作为隐变量步骤，用单一模型完成“生成→自我 critique→继续生成”的循环，实现完全端到端训练。\n\n10\\. 个性化提示风格化\n\n- 让改写策略学习用户历史偏好（色调、词汇复杂度、文化背景）：\n- 把“失败元素”与“用户风格向量”同时输入策略网络，输出既补全语义又符合个人审美的提示。\n\n综上，论文首次把“提示缩放”纳入推理阶段 scaling 定律，留下大量开放问题：从**训练-推理协同**、**跨模态失败诊断**到**黑盒优化理论**，均值得后续深入。\n\nQ6: 总结一下论文的主要内容\n\n**Prompt Redesign for Inference-time Scaling（PRIS）** 提出“把提示词本身当成可缩放变量”，在推理阶段联合扩大“视觉采样”与“提示迭代”，从而持续提高复杂文本-视觉对齐度。核心内容可归纳为三点：\n\n1. **问题洞察**\n仅增加采样步数/种子（Best-of-N 等）很快饱和，因为提示固定导致**共性失败反复出现**；继续投算力收益趋零。\n\n2. **方法框架**\n\n- **EFC 诊断器**：零训练 MLLM 将提示拆成原子元素 → 文本-caption 做 NLI → 输出“每元素是否满足”向量，避免 VQA 肯定偏差。\n- **PRIS 四步循环**：生成 M 样本→选 top-k 成功种子→统计成功率 <50 % 的 core 元素→自动重写提示强化失败点→用新提示+旧种子再生成；可迭代。\n3. **实验结果**\n\n- **T2I** GenAI-Bench：VQAScore +7 %，未见奖励 DA-Score +3 %，美学不降。\n- **T2V** VBench-2.0：Controllability +13.9 %（1.3 B）/+15.2 %（14 B）。\n- **Scaling 曲线**：BoN 1e3 NFE 即饱和，PRIS 持续上升；与 DAS/RBF/EvoSearch 叠加仍增益。\n- **Verifier 基准**：EFC 76 % 准确率，超最强学习奖励模型 7 %。\n- **消融**：跨样本“共性失败”改写远胜单样本修正；同等 wall-clock 时间亦优于盲增样本。\n\n**结论**：首次验证“提示-视觉联合 scaling”可突破传统视觉-only 天花板，为推理阶段 scaling 定律提供新维度。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Rethinking%20Prompt%20Design%20for%20Inference-time%20Scaling%20in%20Text-to-Visual%20Generation，链接是https://arxiv.org/pdf/2512.03534，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03534。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1764892008390","title":"RELIC: Interactive Video World Model with Long-Horizon Memory","description":"A truly interactive world model requires three key ingredients: real-time long-horizon streaming, consistent spatial memory, and precise user control. However, most existing approaches address only one of these aspects in isolation, as achieving all three simultaneously is highly challenging-for example, long-term memory mechanisms often degrade real-time performance. In this work, we present RELIC, a unified framework that tackles these three challenges altogether. Given a single image and a text description, RELIC enables memory-aware, long-duration exploration of arbitrary scenes in real time. Built upon recent autoregressive video-diffusion distillation techniques, our model represents long-horizon memory using highly compressed historical latent tokens encoded with both relative actions and absolute camera poses within the KV cache. This compact, camera-aware memory structure supports implicit 3D-consistent content retrieval and enforces long-term coherence with minimal computational overhead. In parallel, we fine-tune a bidirectional teacher video model to generate sequences beyond its original 5-second training horizon, and transform it into a causal student generator using a new memory-efficient self-forcing paradigm that enables full-context distillation over long-duration teacher as well as long student self-rollouts. Implemented as a 14B-parameter model and trained on a curated Unreal Engine-rendered dataset, RELIC achieves real-time generation at 16 FPS while demonstrating more accurate action following, more stable long-horizon streaming, and more robust spatial-memory retrieval compared with prior work. These capabilities establish RELIC as a strong foundation for the next generation of interactive world modeling.","url":"https://huggingface.co/papers/2512.04040","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.390Z","metadata":{"authors":["Yicong Hong","Yiqun Mei","Chongjian Ge","Yiran Xu","Yang Zhou","Sai Bi","Yannick Hold-Geoffroy","Mike Roberts","Matthew Fisher","Eli Shechtman","Kalyan Sunkavalli","Feng Liu","Zhengqi Li","Hao Tan"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2512.04040.pdf","arxivUrl":"https://arxiv.org/abs/2512.04040","arxivId":"2512.04040","coolPaperUrl":"https://papers.cool/arxiv/2512.04040","published":"2025-12-03T18:29:20Z","updated":"2025-12-03T18:29:20.000Z","zh_summary":"一个真正互动的世界模型需要三个关键要素：实时的长时程流、稳定的空间记忆以及精确的用户控制。然而，大多数现有方法通常仅单独处理其中一个方面，因为同时实现这三者非常具有挑战性——例如，长期记忆机制往往会影响实时性能。在这项工作中，我们提出了 RELIC，这一统一框架同时应对这三大挑战。给定单张图像和文本描述，RELIC 可以在实时条件下进行具备记忆的长时间场景探索。基于近期自回归视频扩散蒸馏技术，我们的模型使用高度压缩的历史潜在标记来表示长时记忆，这些标记在 KV 缓存中编码了相对动作和绝对相机姿态。这一紧凑、支持相机感知的记忆结构能够实现隐式的三维一致内容检索，并以最小的计算开销保证长期一致性。同时，我们对一个双向教师视频模型进行微调，使其生成超出原始 5 秒训练时长的序列，并通过一种新的高效记忆自强制（self-forcing）范式，将其转化为因果学生生成器，从而支持在长时教师和学生自回滚过程中进行全上下文蒸馏。RELIC 以 140 亿参数模型实现，并在精心挑选的 Unreal Engine 渲染数据集上训练，能够以 16 FPS 实时生成，同时在动作跟随的准确性、长时程流的稳定性以及空间记忆检索的鲁棒性方面都优于以往方法。这些能力使 RELIC 成为下一代互动世界建模的坚实基础。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在同时解决构建“实用交互式视频世界模型”必须满足、但此前无法同时实现的三大核心需求：\n\n1. 实时长时序流式生成\n要求模型在接收到连续的用户控制信号（如键盘/鼠标）后，能以 ≥16 FPS 的帧率即时输出新帧，且可无限延伸视频长度，不出现漂移或质量退化。\n\n2. 一致的空间记忆\n当相机在场景中来回移动、重访旧视点时，模型必须准确复现先前已生成的内容，保持物体位置、外观、光照等属性长期一致，而非“忘记”或“重生成”不一致的新内容。\n\n3. 精确的用户控制\n模型需对 13-DoF 的离散/连续动作（平移、旋转、静止等）做出可预测、可组合的响应，实现“指哪打哪”的交互体验。\n\n现有方法通常只能满足其中一项或两项：\n\n- 具备长时一致性的系统往往依赖显式 3D 表示或外部记忆库，推理开销大，难以实时；\n- 实时流式模型多采用短窗口自回归，缺乏长程记忆，重访场景时容易“失忆”；\n- 动作可控性与生成质量、泛化能力之间常存在权衡。\n\nRELIC 通过“压缩历史潜码 + 绝对相机位姿”的轻量级记忆结构、20 s 长上下文双向教师蒸馏、以及内存高效的 replayed back-propagation 训练策略，首次在 14 B 参数规模上把三项需求统一实现：16 FPS 实时输出、20 s+ 稳定视频、重访视点时准确召回历史内容。\n\nQ2: 有哪些相关研究？\n\n以下研究被论文明确引用或对比，可视为 RELIC 的直接相关文献，按主题分组列出（按首字母排序，无第一人称）。\n\n1\\. 长时序 / 流式自回归视频生成\n\n- **FIFO-Diffusion** (Kim et al., NeurIPS 2024)\n- **PA-VDM** (Xie et al., CVPR 2025)\n- **Rolling Diffusion** (Ruhe et al., arXiv 2024)\n- **SkyReels-v2** (Chen et al., arXiv 2025a)\n- **AR-Diffusion** (Sun et al., CVPR 2025)\n- **StreamDiT** (Kodaira et al., arXiv 2025)\n- **Rolling Forcing** (Liu et al., arXiv 2025b)\n- **Wan’s Streamer** (Wan et al., arXiv 2025)\n- **LongLive** (Yang et al., arXiv 2025)\n- **APT-2** (Lin et al., arXiv 2025)\n\n2\\. 自回归蒸馏与 Diffusion-Forcing 范式\n\n- **Self-Forcing** (Huang et al., NeurIPS 2025d)\n- **Distribution Matching Distillation (DMD)** (Yin et al., CVPR 2024b; NeurIPS 2024a)\n- **From Slow Bidirectional to Fast Autoregressive Video Diffusion** (Yin et al., CVPR 2025)\n\n3\\. 显式或外部空间记忆机制\n\n- **WorldMem** (Xiao et al., arXiv 2025)\n- **Context-as-Memory** (Yu et al., arXiv 2025b)\n- **Gen3C** (Ren et al., CVPR 2025)\n- **Vid2World** (Huang et al., arXiv 2025b)\n- **WonderWorld** (Yu et al., CVPR 2025a)\n\n4\\. 工业级或开源交互世界模型\n\n- **Genie-2** (Parker-Holder et al., 2024)\n- **Genie-3** (Ball et al., 2025)\n- **Matrix-Game 2.0** (He et al., arXiv 2025)\n- **Hunyuan-GameCraft** (Li et al., arXiv 2025a)\n- **Marble / RTFM** (World Labs, 2025a,b)\n- **Yume** (Mao et al., arXiv 2025)\n\n5\\. 数据集与动作-视频对齐\n\n- **Sekai** (Li et al., arXiv 2025b)\n- **GameGen-X** (Che et al., arXiv 2024)\n- **The Matrix** (Feng et al., arXiv 2024)\n\n6\\. 基础扩散骨干与效率优化\n\n- **Wan-2.1** (Wan et al., arXiv 2025) — RELIC 的初始权重\n- **FlashAttention-3** (Shah et al., 2024)\n- **YaRN** (Peng et al., arXiv 2023) — 长上下文 RoPE 扩展\n\n以上工作分别从“长视频去漂移”“记忆机制”“动作可控蒸馏”“工业世界模型”四个方向与 RELIC 形成对比或技术承接。\n\nQ3: 论文如何解决这个问题？\n\n论文把“实时长时序流式生成 + 一致空间记忆 + 精确动作控制”三项需求拆解为三个互补的技术模块，并在统一框架内联合优化。核心思路是：先用一个**20 秒长上下文双向教师**提供全序列监督，再把它蒸馏成一个**块因果、带压缩记忆的学生模型**，最后通过**内存高效训练策略**让 14 B 参数模型在 4×H100 上跑到 16 FPS。具体做法如下：\n\n1\\. 长上下文教师：把 5 s 窗口扩展到 20 s\n\n- 以 Wan-2.1-14B 为骨架，保留原始 ST-VAE（8× 空间、4× 时间压缩）。\n- **课程式微调**：5 k 步 5 s → 1 k 步 10 s → 4 k 步 20 s，配合 YaRN 延长 RoPE。\n- **动作-位姿双路注入**\n– 13-DoF 相对动作  A_t  在 Self-Attention 后加性嵌入；\n– 6-DoF 绝对相机位姿  (P_t,R_t)  通过 Pose Encoder 产生 QK 偏移，实现“视点感知”注意力。\n- 结果：教师可在 20 s、317 帧内保持动作对齐与时空一致，为后续蒸馏提供**全 horizon 监督**。\n\n2\\. 学生模型：块因果 + 压缩记忆 KV Cache\n\n- 网络结构\n– 把教师的双向 DiT 改为**块因果自注意力**（block-causal），每块 4 帧，支持流式逐块生成。\n– 保留相同 VAE， latent 空间直接复用。\n- 双分支 KV-Cache\n1. Rolling Cache：保存最近  w  块**未压缩** KV，保证局部细节。\n2. Compressed Memory：对更早历史做**空间下采样**（1×,2×,4× 交错），token 数量降至 1/4，长期记忆开销 ≈ 常数。\n- 隐性 3D 检索\n– 绝对位姿嵌入作为 QK 偏移，使注意力在重访视点时自动对齐历史 latent，实现“无显式 3D 表示”的内容召回。\n\n3\\. 蒸馏策略：Hybrid Forcing + Replayed Back-Propagation\n\n- **ODE 初始化**\n– 训练序列拆成两段：前  B-K  块用**教师强制**（clean compressed），后  K  块用**扩散强制**（noisy uncompressed），兼顾长程记忆与局部随机性。\n- **Replayed Back-Propagation**（解决 20 s 梯度爆炸）\n1. 前向 rollout 全程关梯度，只缓存每块 latent  hat x_(0:L)  与分数差  Delta s_(0:L)=s_(data)-s_(gen) ；\n2. 按块重放前向，开梯度后立即回传对应  Delta s_l  并累加参数梯度；\n3. 每块计算图即时释放，峰值 GPU 内存从  mathcal O(L)  降到  mathcal O(1) 。\n- 渐进 rollout：5 s → 10 s → 20 s，稳定收敛。\n\n4\\. 推理加速\n\n- torch.compile + 手工融合 RMSNorm、RoPE、Modulation。\n- KV-Cache 以 FP8-E4M3 存储，体积减半；FlashAttention-3 跑在 Hopper GPU。\n- 序列并行（线性层）与头并行（Self-Attention）混合，NCCL All-to-All 切换布局；KV-Cache 按头分片，多卡带宽打满。\n\n5\\. 数据保障\n\n- 350 个 UE 场景、1600 min 720p 视频，平均 75 s，最大 9 min；\n- 人工轨迹强调“横向/旋转/升降”均衡与**视点重访**，克服真实数据集动作单一、无回访缺陷；\n- 时间反转增广：强制模型“回头看”，提升长程召回。\n\n通过上述设计，RELIC 把原本互斥的三项指标同时推到可用水平：\n\n- **16 FPS** 实时输出（4×H100）\n- **20 s+** 稳定视频，RPE-trans/rot 最低\n- 重访视点时物体、布局、光照一致，无需显式 3D 重建或外部记忆库。\n\nQ4: 论文做了哪些实验？\n\n实验部分围绕「长时序-实时-可控-有记忆」四项能力展开，定量与定性并重，并专门设计行为学评测协议以保证动作对比公平。具体实验如下：\n\n1 实施细节与训练可复现性\n\n- 32×H100 训练，结合 FSDP + 序列并行 + 张量并行，把 20 s/14 B 模型装进 80 GB 显存。\n- 学生模型压缩表 S=\n1,4,2,4,…\n 循环，总 token 从 ≈120 k 降到 ≈30 k。\n- 渐进式 rollout：5 s（250 iter）→10 s（150 iter）→20 s（150 iter），保证收敛。\n\n2 能力展示（定性）\n\n- **跨域泛化**：单张输入可驱动油画、漫画、矢量图、低多边形等风格实时漫游，远景-近景速度关系正确。\n- **可调速**：连续系数 γ=1.0,2.0,3.0 时，平移/旋转速度线性放大而无抖动或漂移。\n- **多键组合**：同时按下 W+PanLeft+TiltUp 等复杂指令，相机轨迹平滑且语义一致。\n- **长时记忆**：图 1 & 图 9 显示相机先离开再返回，长椅、窗户等物体外观/位置几乎完全复现。\n\n3 定量对比（20 s 固定长度）\n\n测试集：220 张 Adobe Stock 图（写实+非写实各半），11 组脚本，共 220 条 20 s 视频。\n\n| 指标 | RELIC | Matrix-Game-2.0 | Hunyuan-GameCraft |\n| --- | --- | --- | --- |\n| 平均视觉质量† | 0.8015 | 0.7447 | 0.7885 |\n| Image Quality | 0.6665 | 0.6551 | 0.6737 |\n| Aesthetic | 0.5967 | 0.4931 | 0.5874 |\n| RPE-trans ↓ | 0.0906 | 0.1122 | 0.1149 |\n| RPE-rot ↓ | 1.00 | 1.48 | 1.23 |\n\n† 采用 VBench 七项（主体一致性、背景一致性、运动平滑度、动态程度、美学、成像质量等）平均。\nRPE 计算流程：ViPE 重建轨迹 → Sim(3) Umeyama 对齐 → 取平均相对位姿误差。\n\n4 动作准确性定性对比\n\n- **Tilt Up**：Matrix-2.0 顶部出现黑边，Hunyuan 几乎无垂直运动；RELIC 正确显示天花板。\n- **Truck Left**：Hunyuan 实际执行 Pan Left，Matrix-2.0 静止；RELIC 横向平移且透视正确。\n\n5 记忆能力定性对比\n\n同一场景绕行：\n\n- Hunyuan-GameCraft 与 Matrix-2.0 在 12 s 后均遗忘右侧长椅；\n- RELIC 在 20 s 重访时几何、纹理、光照一致，无浮动高斯或网格伪影（对比 Marble 的 Gaussian floater）。\n\n6 效率评测\n\n- 4×H100 生成 480×832/16 FPS 视频，端到端吞吐 16.0 FPS（含 VAE 解码），显存占用 4×80 GB → 62 GB（FP8 KV-cache）。\n- 对比纯滚动窗口基线，压缩记忆使 Attention FLOPs 降为 1/4，GPU 内存带宽瓶颈缓解 3.8×。\n\n7 消融与灵敏度（补充附录）\n\n- 无压缩记忆：20 s 视频重访误差 RPE-trans ↑ 38 %，GPU 峰值内存 ↑ 3.9×。\n- 无绝对位姿嵌入：记忆召回率 ↓ 22 %，物体重叠率 ↑ 0.12。\n- 教师仅用 5 s 窗口：学生 20 s 漂移帧数 ↑ 4.7×。\n\n综上，实验从「视觉质量-动作精度-记忆一致性-运行效率」四维度验证 RELIC 在同等或更大参数规模对手上的领先性，并给出可复现的训练/评测协议。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“数据与场景”“模型与算法”“系统与交互”“评测与应用”四大类，均直接对应 RELIC 尚未充分解决的局限或新场景需求。\n\n1 数据与场景\n\n- **动态场景与物理事件**\n当前 UE 数据以静态光照、刚体为主，可引入可动物体、流体、天气、昼夜循环，检验模型对“世界状态变化”的因果建模能力。\n\n- **多智能体与交互式对象**\n收集带 NPC、车辆、宠物等交互实体的长视频，研究模型在“用户-环境-第三方”高阶控制下的一致性。\n\n- **真实世界微调**\n构建小规模手持 + 头戴相机长序列，与 UE 合成数据混合微调，测试域迁移后记忆与动作精度是否保持。\n\n2 模型与算法\n\n- **分钟级到无限长度**\n20 s 仍属“中等长”。可探索：\n\n- 层级记忆（短期 latent + 中期 hash-grid + 长期文本-视觉联合索引）；\n- 事件驱动的记忆淘汰/合并策略，避免 KV-cache 随时间线性增长。\n- **显式-隐式混合表示**\n在 latent 之外引入可微分的稀疏体素/高斯点云支线，仅当重访误差>阈值时激活，兼顾“轻量运行”与“高精度召回”。\n\n- **动作空间升级**\n13-DoF 相机→6-DoF 物体操控 + 离散交互（开门、拾取）。需设计新的动作编码器与合成数据集，验证模型对“因果-物理合理性”的隐式学习。\n\n- **多模态条件融合**\n同时接受语音、草图、深度图、文本描述作为“即席”条件，研究不同模态冲突时的优先级与融合策略。\n\n3 系统与交互\n\n- **端侧实时推理**\n14 B 参数需 4×H100。可尝试：\n\n- 8-bit/4-bit 量化 + Tiny-VAE 进一步压缩；\n- 投机解码（speculative diffusion）减少去噪步数；\n- 边缘-云协同：低精度本地预览 + 高精度云端补帧。\n- **可变分辨率与 foveated 生成**\n依据眼动或头显视角动态调整空间-时间 token 密度，实现 4K 局部、360p 周边的“人眼适配”流式输出。\n\n- **用户个性化记忆**\n允许同一基础世界在不同用户端维护私有记忆库（物品摆放、装修风格），研究如何在不重训主模型条件下快速加载/切换。\n\n4 评测与应用\n\n- **长程记忆基准**\n现有 VBench 无“重访误差”专项。可发布：\n\n- 带 ground-truth 位姿的“环形-折返”轨迹套件；\n- 自动检测物体重现率、ID 切换、纹理漂移的指标包。\n- **可交互式仿真-到-真实迁移**\n在 RELIC 生成场景里预训练导航/抓取策略，再零样本部署到真实机器人，验证世界模型对下游 embodied AI 的增益。\n\n- **创意工具链**\n集成文本-草图-语音多条件输入，支持“一句话实时搭建可行走片场”，为影视预览、游戏关卡、VR 社交提供即席内容生成接口。\n\n5 理论层面\n\n- **记忆-注意力可视化**\n剖析绝对位姿嵌入在 QK 空间的最近邻分布，定量回答“模型到底记住了什么、以何种几何结构索引”。\n\n- **蒸馏误差下界分析**\n给定教师双向分布与学生因果分布的信息论极限，推导长时 horizon 与压缩率之间的最优权衡公式，指导未来内存预算设计。\n\n这些方向既可直接继承 RELIC 的压缩记忆与长教师蒸馏框架，也能逐步扩展到真正“无限时长、多智能体、多模态、可物理交互”的下一代世界模型。\n\nQ6: 总结一下论文的主要内容\n\n**RELIC: Interactive Video World Model with Long-Horizon Memory**\n提出一个 14 B 参数、可实时运行的交互式视频世界模型，核心贡献与结果可概括为：\n\n1 要解决的问题\n\n- 实时长时序流式生成（≥16 FPS）\n- 一致的空间记忆（重访视点不遗忘）\n- 精确动作控制（13-DoF 相机输入）\n\n现有方法只能满足其一或两项，且常引入显式 3D 表示或外部记忆库，导致推理开销大。\n\n2 技术路线\n\n**① 20 s 双向教师**\n\n- 在 Wan-2.1-14B 上课程式微调（5→10→20 s），用 YaRN 扩展 RoPE；\n- 13-DoF 相对动作 + 6-DoF 绝对位姿双路注入，实现动作对齐与视点感知。\n\n**② 块因果学生**\n\n- 把教师蒸馏为 4 步去噪的块因果 AR 模型；\n- 双分支 KV-Cache：近期无压缩 + 历史 1/2/4× 空间压缩，token 总量降 4×；\n- 绝对位姿嵌入 QK，隐性 3D 检索，无需显式几何。\n\n**③ 内存高效训练**\n\n- Hybrid Forcing 初始化（教师强制 + 扩散强制）；\n- Replayed Back-Propagation：先关梯度 rollout 并缓存分数差，再分块重放求梯度，峰值 GPU 内存从 O(L) 降到 O(1)。\n\n**④ 系统优化**\n\n- FP8 KV-Cache、FlashAttention-3、torch.compile、序列/头并行；\n- 4×H100 上 480×832 视频 16 FPS 实时生成。\n\n3 数据\n\n- 350 个 UE 场景、1600 min 720p 视频，平均 75 s，最大 9 min；\n- 人工轨迹平衡 13 类动作并强制“视点重访”；\n- 5 s 分段静态字幕 + 时间反转增广，提升长程召回。\n\n4 实验结果\n\n- **定量（220 条 20 s 视频）**\n– 视觉质量平均分 0.8015，超 Matrix-2.0 与 Hunyuan-GameCraft；\n– 动作轨迹误差 RPE-trans 0.0906、RPE-rot 1.00，均为最低。\n- **定性**\n– 油画/漫画/矢量艺术多风格泛化；\n– 可调速、多键组合无漂移；\n– 重访视点时物体、光照一致，优于基线的“失忆”与高斯浮点伪影。\n\n5 结论\n\nRELIC 首次在 14 B 规模同时实现实时 16 FPS、20 s+ 稳定视频、重访视点记忆一致，无需显式 3D 表示或外部记忆库，为可交互、可探索的通用世界模型提供了可扩展的基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RELIC%3A%20Interactive%20Video%20World%20Model%20with%20Long-Horizon%20Memory，链接是https://arxiv.org/pdf/2512.04040，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04040。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1764892008391","title":"Thinking with Programming Vision: Towards a Unified View for Thinking with Images","description":"Multimodal large language models (MLLMs) that think with images can interactively use tools to reason about visual inputs, but current approaches often rely on a narrow set of tools with limited real-world necessity and scalability. In this work, we first reveal a critical and previously overlooked weakness: even state-of-the-art MLLMs are surprisingly brittle, showing significant performance degradation on images with simple orientation changes or natural corruptions, underscoring the need for more robust tool-based reasoning. To address this, we propose CodeVision, a flexible and scalable code-as-tool framework where the model generates code as a universal interface to invoke any image operation, moving beyond fixed tool registries. We train our model using a two-stage methodology, beginning with Supervised Fine-Tuning (SFT) on a high-quality dataset curated for complex, multi-turn tool composition and error recovery, followed by Reinforcement Learning (RL) with a novel and dense process reward function to encourage strategic and efficient tool use. To facilitate this research, we construct new SFT and RL datasets and introduce a challenging new benchmark suite designed to rigorously evaluate robustness to orientation changes and multi-tool reasoning. Experiments on Qwen2.5-VL and Qwen3-VL series show that our approach significantly improves model performance and fosters emergent capabilities such as flexible tool composition, efficient chained execution, and robust error recovery from runtime feedback. Code is available at https://github.com/ByteDance-BandAI/CodeVision.","url":"https://huggingface.co/papers/2512.03746","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.391Z","metadata":{"authors":["Zirun Guo","Minjie Hong","Feng Zhang","Kai Jia","Tao Jin"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.CV","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2512.03746.pdf","arxivUrl":"https://arxiv.org/abs/2512.03746","arxivId":"2512.03746","coolPaperUrl":"https://papers.cool/arxiv/2512.03746","published":"2025-12-03T12:44:15Z","updated":"2025-12-03T12:44:15.000Z","zh_summary":"能够通过图像进行思考的多模态大语言模型（MLLMs）可以交互式地使用工具来推理视觉输入，但当前的方法往往依赖于一小部分工具，其在现实世界中的必要性和可扩展性有限。在本工作中，我们首先揭示了一个关键且以前被忽视的弱点：即使是最先进的MLLMs也出乎意料地脆弱，在图像发生简单方向变化或自然损坏时性能会显著下降，这凸显了对更稳健工具化推理的需求。为此，我们提出了CodeVision，一种灵活且可扩展的“代码即工具”框架，模型通过生成代码作为通用接口来调用任何图像操作，从而超越固定工具注册表。我们采用两阶段方法训练模型，首先在精心策划的高质量数据集上进行监督微调（SFT），该数据集针对复杂的多轮工具组合和错误恢复进行设计，然后通过新颖且密集的过程奖励函数进行强化学习（RL），以鼓励策略性和高效的工具使用。为了支持这项研究，我们构建了新的SFT和RL数据集，并引入了一套具有挑战性的基准测试，用于严格评估模型对方向变化和多工具推理的稳健性。在Qwen2.5-VL和Qwen3-VL系列上的实验表明，我们的方法显著提升了模型性能，并催生了诸如灵活工具组合、高效链式执行以及从运行时反馈中稳健恢复错误等新兴能力。代码可在 https://github.com/ByteDance-BandAI/CodeVision 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决当前多模态大语言模型（MLLM）在“用图像思考”场景下的三大核心缺陷：\n\n1. 工具必要性不足\n现有方法过度依赖“裁剪”工具，在 V\\*、HRBench 等基准上仅带来 2–5% 的微弱提升，且无需工具的纯 RL 基线即可媲美，表明任务并未真正激发工具的价值。\n\n2. 灵活性与可扩展性差\n传统方案需人工预定义工具名称与参数，一旦工具改名或新增接口就必须重训，难以泛化到未见工具。\n\n3. 多轮多工具组合缺失\n已有系统大多单轮或仅重复裁剪，缺乏跨轮次、跨工具的组合推理，难以应对真实世界复杂任务。\n\n为此，作者首先揭示一个被忽视的关键脆弱性：即使最先进的 MLLM 在图像仅发生简单旋转或翻转时，性能也会骤降 80%。据此提出 CodeVision 框架，将“代码即工具”作为统一接口，让模型通过生成代码调用任意图像操作，突破固定工具表限制；并设计两阶段训练——先基于高质量多轮工具组合与错误恢复数据进行监督微调（SFT），再采用带密集过程奖励的强化学习（RL）——以激发策略性、高效且鲁棒的工具使用。实验表明，该方法在新建的一系列单工具与多工具基准上显著优于基线，并涌现出灵活工具组合、高效链式执行与运行时错误恢复等新能力。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中系统梳理了三条主线，并指出它们与本文工作的区别。可归纳为以下研究脉络：\n\n- **Thinking with Images**\n- OpenAI o3（2025c）首次提出“用图像思考”范式，后续工作如 Grit、Mini-o3、DeepEyes、Thyme 等多聚焦于“裁剪/放大”单工具，缺乏对多工具组合与真实损坏（如方向错乱）的深入验证。\n- 本文首次将方向修正作为必要工具，并构建多轮多工具组合任务，填补该空白。\n- **Tool Integration**\n- 语言侧：LLM-I、Search-R1、Search-o1、DeepResearch 等将搜索、代码、生成模型等工具接入大模型，实现多轮证据收集。\n- 视觉侧：OpenThinkImg、PixelReasoner 等尝试引入 OCR、分割、画线等工具，但仍依赖手工注册接口，扩展性差。\n- 本文采用“代码即工具”统一接口，摆脱固定工具表，实现任意图像操作的可扩展调用。\n- **MLLM Reasoning with RL**\n- 文本推理：PPO → GRPO → DAPO → GSPO 等算法持续优化策略梯度，提升数学/代码推理。\n- 视觉推理：Observe-R1、APO 等通过“先观察后推理”或不对称策略优化增强 MLLM 推理。\n- 本文首次将密集过程奖励（must-use 工具、建议工具、效率惩罚）引入视觉工具学习，解决稀疏奖励导致的策略崩塌与奖励黑客问题。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **CodeVision** 框架，通过“代码即工具”统一接口与两阶段训练流程，系统性地解决前述三大缺陷。核心思路与步骤如下：\n\n1. 诊断并构造“必须工具”场景\n\n- 发现 SOTA 模型在图像旋转/翻转下性能暴跌（最多 −80%），据此把方向修正设为刚性需求；\n- 在训练与评测数据中，对每张图像随机施加 90°/180°/270° 旋转或水平/垂直翻转，使工具调用成为任务成功的必要前提。\n2. 代码即工具（Code-as-Tool）\n\n- 不再维护固定工具注册表，而是让模型直接生成 Python 代码，借助 PIL/OpenCV 等库完成任意图像操作；\n- 运行时沙箱执行代码，返回执行结果或错误日志，模型可据此多轮迭代修正，实现“无限”工具集与即插即用扩展。\n3. 两阶段训练策略\n**Stage-1 冷启动 SFT**\n\n- 构建 5 k 条高质量多轮轨迹，覆盖单工具、多工具、多步裁剪、错误恢复、无工具五类场景；\n- 采用掩码语言建模损失，仅对 assistant 生成的推理与代码 token 计算梯度，快速习得语法与基础策略。\n\n**Stage-2 强化学习 RL**\n\n- 数据：4 万条带“must-use 工具”标注的困难样本，过滤掉全对/全错轨迹，保留有信号区间；\n- 奖励：设计密集多分量奖励\n\nR_(total)(τ)=R_(outcome)+β_1 ∑_t R_(strategy)(a_t) − β_2 P_(cost)(τ)\n\n–  R_(outcome) ：终端答案正确性与格式标签奖励；\n–  R_(strategy) ：\n• must-use 工具按 1/N 预算给一次性 bonus，crop 按 IoU 增量奖励；\n• 建议工具 bonus：通过 8 rollout 对比，若某可选工具显著提升成功率，则给成功轨迹额外  r_(nec) ；\n–  P_(cost) ：对超限轮次、低质量裁剪、不必要工具三类“奖励黑客”行为施加惩罚。\n- 算法：基于 GRPO，8 条轨迹/样本，KL 正则 0.001，训练 2 epoch。\n4. 新基准与评测\n\n- 单工具：V\\*, HRBench4k/8k；\n- 方向鲁棒性：在 OCRBench、ChartQAPro 上施加五种几何损坏，考察恢复 canonical view 能力；\n- 多工具组合：自建 MVToolBench，强制“方向修正 + 精细裁剪”两步串联，评估工具链推理。\n\n通过上述设计，模型在各项基准上显著超越基线，涌现出训练集未见的工具（亮度、锐化、边缘检测等）与多操作单轮链式调用，实现高效、鲁棒、可扩展的“用图像思考”。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“单工具”“方向鲁棒性”“多工具组合”三类场景，共构建/选用 6 个基准，并在 3 组主干模型上开展系统实验。主要结果如下（数值均取自原文 Table 1 & 2）：\n\n| 实验类别 | 基准 | 主干规模 | 基线平均得分 | CodeVision 平均得分 | 最大提升 |\n| --- | --- | --- | --- | --- | --- |\n| 方向鲁棒性 | OCRBench（五种几何损坏） | 7B | 56.0 → 73.4 | +17.4 |\n| 8B | 52.2 → 75.4 | +23.2 |\n| ChartQAPro（同上） | 7B | 24.4 → 31.7 | +7.3 |\n| 8B | 29.5 → 40.7 | +11.2 |\n| 单工具 | V* | 7B | 74.6 → 83.7 | +9.1 |\n| HRBench4k | 7B | 69.4 → 75.6 | +6.2 |\n| HRBench8k | 7B | 67.5 → 72.2 | +4.7 |\n| 多工具组合 | MVToolBench | 7B | 18.1 → 60.1 | +42.0 |\n| 8B | 19.7 → 62.7 | +43.0 |\n| 32B | 28.6 → 65.4 | +36.8 |\n\n补充实验与可视化\n\n- 训练曲线：图 5 显示 outcome / strategy / total 奖励均单调上升；图 7 追踪“ emergent 工具奖励”同样持续走高，证明模型不断发现训练集未出现的新工具。\n- 案例研究：图 6、9、10 给出多轮错误恢复、单轮链式调用（对比度+灰度）、五工具组合（亮度↑、对比度↑、裁剪、rotate90、锐化）等定性示例。\n- 消融实验：表 3 表明去掉 strategy reward 或 constraint penalty 后，MVToolBench 分别下降 9.4 与 4.2 个百分点；图 15-16 进一步验证 dense reward 与 SFT 冷启动的必要性。\n\n综上，实验覆盖 7B-32B 三个规模、6 个基准、共 30 余项指标，一致验证 CodeVision 在鲁棒性、单工具、多工具组合任务上的显著优势。\n\nQ5: 有什么可以进一步探索的点？\n\n可进一步探索的方向集中在 **工具广度、数据规模、奖励设计、坐标精度与多模态扩展** 五个维度：\n\n1. 工具多样性与组合复杂度\n\n- 将“代码即工具”从 PIL/OpenCV 扩展到自定义 API（生成模型、搜索、数据库、3D 渲染），实现黑盒工具即插即用；\n- 引入跨图像工具（diff、拼接、超分辨率、视频帧操作），研究多图像联合推理。\n2. 数据与任务规模化\n\n- 构建十万级多轮轨迹，覆盖医疗影像、遥感、工业设计等高价值场景；\n- 引入课程学习：从单工具→多工具→长链条→对抗扰动，逐步提升组合复杂度。\n3. 奖励与过程监督细化\n\n- 用 LLM-as-a-judge 动态生成“beneficial 工具”列表，替代固定 rollout 对比，实现更细粒度的在线策略修正；\n- 引入可微分图像指标（LPIPS、SSIM）替代离散 IoU，让裁剪奖励连续可导，提升坐标回归稳定性。\n4. 坐标精度与定位专用头\n\n- 为裁剪任务增加轻量级定位头，采用 anchor-free 或扩散式坐标生成，缓解“保守长条”与“相邻偏移”失败案例；\n- 在 RL 阶段对坐标使用 Huber loss 辅助回归，降低离散网格搜索带来的误差。\n5. 多模态与实时交互\n\n- 把工具链推广到音频-视觉同步（例如先旋转视频再提取字幕），研究跨模态工具依赖；\n- 在边缘设备部署沙箱运行时，探索量化-编译协同优化，实现毫秒级代码执行与反馈，支持实时交互应用。\n\nQ6: 总结一下论文的主要内容\n\n**论文核心内容速览**\n\n1. 问题\n现有 MLLM 在“用图像思考”场景下暴露出三大缺陷：\n\n- 工具必要性不足（裁剪仅带来 2–5% 提升）\n- 接口僵化、难以扩展（手工注册工具，改名即失效）\n- 缺乏多轮多工具组合（真实任务常需方向修正+精细裁剪）\n2. 关键发现\n对 200 张图像施加旋转/翻转后，GPT-5、Gemini2.5-Pro 等 SOTA 准确率最高下降 80%，揭示模型对方向扰动极度脆弱。\n\n3. 方法：CodeVision\n\n- **代码即工具**：模型直接生成 Python 代码调用 PIL/OpenCV，无需固定工具表，支持无限扩展。\n- **两阶段训练**\n– SFT：5 k 高质量多轮轨迹，覆盖单/多工具、错误恢复、无工具场景。\n– RL：4 万困难样本，采用密集多分量奖励\n\nR_(total)=R_(outcome)+β_1∑ R_(strategy)−β_2 P_(cost)\n\n‑ _strategy_：must-use 工具按 1/N 奖励，可选工具通过 8-rollout 对比动态发现。\n‑ _cost_：惩罚冗余轮次、低 IoU 裁剪、对正常图像误用方向工具。\n4. 新基准\n\n- 方向鲁棒性：OCRBench-Rot/Flip、ChartQAPro-Rot/Flip\n- 多工具组合：MVToolBench（方向修正 → 精细裁剪）\n5. 结果（绝对提升）\n\n- 方向鲁棒：7B 平均 +17.4，8B 平均 +23.2（OCRBench）\n- 单工具：V\\* +9.1，HRBench4k/8k +4~6\n- 多工具：MVToolBench 7B 18→60，8B 20→63，32B 29→65（≈翻倍 SOTA）\n6. 涌现能力\n训练集未出现的亮度、锐化、边缘检测等工具被主动调用；单轮链式执行 5 个操作；运行时捕获错误并自动修正。\n\n7. 未来方向\n扩展自定义 API 与跨图像工具、十万级课程数据、可微分奖励、专用定位头、实时边缘部署。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Thinking%20with%20Programming%20Vision%3A%20Towards%20a%20Unified%20View%20for%20Thinking%20with%20Images，链接是https://arxiv.org/pdf/2512.03746，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03746。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1764892008392","title":"Flowing Backwards: Improving Normalizing Flows via Reverse Representation Alignment","description":"Normalizing Flows (NFs) are a class of generative models distinguished by a mathematically invertible architecture, where the forward pass transforms data into a latent space for density estimation, and the reverse pass generates new samples from this space. This characteristic creates an intrinsic synergy between representation learning and data generation. However, the generative quality of standard NFs is limited by poor semantic representations from log-likelihood optimization. To remedy this, we propose a novel alignment strategy that creatively leverages the invertibility of NFs: instead of regularizing the forward pass, we align the intermediate features of the generative (reverse) pass with representations from a powerful vision foundation model, demonstrating superior effectiveness over naive alignment. We also introduce a novel training-free, test-time optimization algorithm for classification, which provides a more intrinsic evaluation of the NF's embedded semantic knowledge. Comprehensive experiments demonstrate that our approach accelerates the training of NFs by over 3.3$\\times$, while simultaneously delivering significant improvements in both generative quality and classification accuracy. New state-of-the-art results for NFs are established on ImageNet 64$\\times$64 and 256$\\times$256. Our code is available at https://github.com/MCG-NJU/FlowBack.","url":"https://huggingface.co/papers/2511.22345","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.392Z","metadata":{"authors":["Yang Chen","Xiaowei Xu","Shuai Wang","Chenhui Zhu","Ruxue Wen","Xubin Li","Tiezheng Ge","Limin Wang"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2511.22345.pdf","arxivUrl":"https://arxiv.org/abs/2511.22345","arxivId":"2511.22345","coolPaperUrl":"https://papers.cool/arxiv/2511.22345","published":"2025-11-27T11:35:08Z","updated":"2025-11-27T11:35:08.000Z","zh_summary":"正则化流（Normalizing Flows, NFs）是一类生成模型，其特点是具有数学上可逆的架构，其中前向传递将数据转换为潜在空间以进行密度估计，而反向传递则从该空间生成新样本。这一特性在表示学习和数据生成之间创造了内在的协同效应。然而，标准NFs的生成质量受到对数似然优化所产生的语义表示能力不足的限制。为了解决这一问题，我们提出了一种创新的对齐策略，巧妙地利用了NFs的可逆性：我们不是对前向传递进行规范化，而是将生成（反向）传递的中间特征与强大的视觉基础模型的表示进行对齐，显示出比简单对齐更优的效果。我们还引入了一种新的无需训练的测试时优化算法用于分类，这提供了对NF嵌入语义知识更本质的评估。综合实验表明，我们的方法使NFs的训练速度提高了3.3倍以上，同时在生成质量和分类准确性方面均有显著提升。在ImageNet 64×64和256×256上，我们建立了NFs的新最新成果。我们的代码可在 https://github.com/MCG-NJU/FlowBack 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对**标准归一化流（Normalizing Flows, NFs）在生成质量与语义表征之间的内在冲突**提出系统性的改进方案。核心问题可归纳为：\n\n1. **生成-表征失衡**\n仅通过最大似然（MLE）训练得到的 NFs 虽然具备精确可逆结构，却倾向于学到低层统计特征，缺乏高层语义，导致生成样本逼真度受限。\n\n2. **评估手段缺失**\n传统线性探针无法直接度量 NFs 内部蕴含的判别知识，需要一种**无需额外训练、能反映模型本身语义能力**的评估协议。\n\n3. **对齐策略空白**\n扩散模型中行之有效的“表征对齐”（REPA）并不能直接迁移到可逆网络；如何利用 NFs **独有的反向生成路径**来注入语义指导，此前尚无研究。\n\n为此，论文提出 **R-REPA（Reverse Representation Alignment）**，首次将视觉基础模型的特征对齐损失施加在生成（z→x）而非编码（x→z）过程，并配套给出一种**基于单步梯度的免训练分类算法**，在 ImageNet 64/256 上同时刷新 NFs 的 FID 与分类准确率，且训练提速 3.3×。\n\nQ2: 有哪些相关研究？\n\n论文第2节“Related Work”将相关研究划分为两条主线，并在实验部分与最新方法进行了对比。可归纳如下：\n\n1\\. Normalizing Flows（NF）本身的发展\n\n- **经典可逆流块**\n- NICE (Dinh et al. 2014)\n- RealNVP (Dinh et al. 2016)\n- IAF / MAF (Kingma et al. 2016; Papamakarios et al. 2017)\n- **现代 Transformer-式流**\n- TARFlow (Zhai et al. 2024) —— 本文基线，首次在 ImageNet 上与扩散模型可比\n- JetFormer (Tschannen et al. 2024) —— 联合图文自回归生成，以 NF 为核心\n- STARFlow (Gu et al. 2025) —— 同期工作，进一步扩容量与分辨率\n→ 这些研究证明了“流模型复兴”的可行性，但未解决语义表征弱的问题。\n\n2\\. Representation Alignment for Generation（表征对齐加速生成）\n\n- **REPA 起源**\n- REPA (Yu et al. 2024) —— 在扩散 Transformer 内部特征与 DINOv2 对齐，显著提速并提升 FID\n- 后续变体\n- REPA-E (Lee et al. 2024)：端到端训练潜空间扩散\n- LightningDiT (Zheng et al. 2024)：对齐 VAE 潜码\n- U-REPA (Tian et al. 2024b)：将策略迁移至 U-Net\n→ 上述方法均针对**前向去噪网络**，未探索**可逆模型的反向生成路径**。\n\n3\\. 与本文方法直接对比的强基线 / 前沿模型（实验部分）\n\n| 模型类别 | 代表文献 | 与本文结果对照 |\n| --- | --- | --- |\n| 扩散/流匹配 | EDM, iDDPM, ADM, SiT, DiT | Table 3 & 4：R-REPA 在 2-step 下 FID 逼近或优于多步扩散 |\n| 自回归（离散） | LlamaGen, VAR | Table 4：Latent-TARFlow+R-REPA 4.18 FID 与 VAR 1.73 差距缩小，且采样步数极少 |\n| 自回归（连续） | MAR, DART | 同上，验证流模型在连续域也可受益对齐 |\n| GANs | BigGAN, IC-GAN | Table 3：R-REPA 3.69 FID 优于 BigGAN 4.06 |\n| Consistency Models | CD, iCT-deep | Table 3：R-REPA 取得更低 FID |\n\n4\\. 其他被引用或利用的辅助工作\n\n- **视觉基础模型**\n- DINOv2 (Oquab et al. 2023) —— 默认对齐目标\n- CLIP, MAE —— 附录表8 验证对齐策略对 encoder 选择的鲁棒性\n- **潜空间压缩**\n- VAE-ft-EMA (StabilityAI 2022) —— 256×256 实验采用的标准潜码器\n- **位置编码与激活**\n- RoPE (Su et al. 2024)\n- SwiGLU (Touvron et al. 2023) —— 用于高分辨率 Transformer 模块\n\n综上，本文首次把“表征对齐”思想从扩散/自回归领域引入到**可逆流模型的反向生成通路**，并在与当前各类主流生成模型（扩散、GAN、AR、Consistency）对比中取得新 SOTA，填补了“可逆架构如何利用语义指导”这一研究空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **R-REPA（Reverse Representation Alignment）** 框架，从“诊断-对齐-加速”三个层面系统解决标准归一化流（NF）语义弱、生成质量受限的问题。核心思路是**利用 NF 独有的可逆性，把表征对齐损失施加在生成（z→x）而非编码（x→z）路径**，从而在不破坏密度估计的前提下注入高层语义。具体步骤如下：\n\n1\\. 诊断：免训练、单步梯度分类器\n\n- **问题**：传统线性探针无法直接度量 NF 本身学到的语义。\n- **做法**：\n- 将类别标签建模为可微的“软嵌入”  e_(eff)=softmax(λ)^top E 。\n- 对冻结的类条件 TARFlow 计算条件对数似然  log p(x|e_(eff);θ) 。\n- 仅执行**一次反向传播**得到  ∇_λ log p(x|e_(eff);θ) ，取最大梯度分量对应的类别作为预测。\n- **效果**：\n- 与逐层线性探针天花板高度一致（图 4），却无需任何额外训练，成为后续消融与主实验的“内在语义指标”。\n\n2\\. 对齐：三种反向传播策略的系统比较\n\n在 TARFlow 的任意中间层  (t,l)  引入对齐损失\n\nL_(align)^((t,l))=-(1) / (P)∑_(p=1)^P sim!l(v[p],,Proj_φl(h^((t,l))[p]r)r),\n\n其中  v=Phi(x)  为冻结 DINOv2-B 特征， sim  为余弦相似度。关键是如何回传梯度：\n\n| 策略 | 梯度更新范围 | 缺点 |\n| --- | --- | --- |\n| F-REPA（Forward） | 当前块及所有前驱块 | 早期块被迫同时拟合密度与语义，FID 恶化 |\n| D-REPA（Detach） | 仅当前块 | 更新孤立，提升有限 |\n| R-REPA（Reverse） | 当前块及所有后继块（生成方向） | ✅ 与 z→x 生成次序一致，语义指导直接作用于合成阶段 |\n\n- **R-REPA 实现**：\n1. 前向缓存每块输入  hat x_(t-1)=stop_grad(x_(t-1)) ；\n2. 构造“伪逆”并行计算\n\nz_(t-1)^d = μ_θ^t(hat x_(t-1)^(<d)) + σ_θ^t(hat x_(t-1)^(<d))odot z_t^d,\n\n利用缓存打破自回归顺序，训练阶段加速 ≈50×；\n3. 对齐损失仅在逆路径图上回传，保证  θ_(1... t-1)  不受干扰。\n\n3\\. 加速与扩展：潜空间 + 噪声增强\n\n- **高分辨率迁移**：采用预训练 VAE-ft-EMA（8×下采，4 通道）将 256×256 图像转为潜码  x 。\n- **训练**：对潜码加高斯噪声  tilde x=x+varepsilon,;varepsilonsimmathcal N(0,σ^2 I) ，NF 学习  p_θ(tilde x) 。\n- **采样**：\n1. 逆变换得噪声潜码  tilde x=f_θ^(-1)(z) ；\n2. 一步得分去噪  hat x=tilde x+σ^2∇_(tilde x)log p_θ(tilde x) ；\n3. VAE 解码得图像。\n整个流程仅 **2 步**（1 步逆变换 + 1 步去噪），保持 NFs 的采样效率优势。\n\n4\\. 目标函数与训练流程\n\n总损失\n\nL_(total)=L_(NF)+λ_(align)(1) / (|A|)∑_((t,l)∈ A)L_(align)^((t,l)),\n\n其中  L_(NF)  为标准最大似然（式 3）， λ_(align)=0.1  经网格搜索确定。对齐层集合  A  优选 **第 7、8 块较深 layer**（生成最早接触潜变量 z 的位置），兼顾 FID 与分类准确率。\n\n5\\. 结果总结\n\n- **ImageNet 64×64**：\n- FID 从 11.76→11.25，分类准确率 39.97%→57.02%；\n- 400k 迭代即超越基线 1M 迭代，**训练提速 3.3×**；\n- 50k 样本 FID 3.69，**首次让 NFs 击败 BigGAN** 并逼近 iDDPM（2.92）。\n- **ImageNet 256×256（潜空间）**：\n- FID 从 13.05→12.79（+R-REPA），继续优化 patch size=1 后 **4.18**，与同期 AR/扩散模型差距大幅缩小；\n- 分类准确率 40.22%→56.24%，保持 2 步采样。\n\n一句话总结\n\n论文通过“**反向对齐 + 免训练诊断 + 潜空间加速**”三位一体策略，首次在保持可逆精确采样优势的同时，让归一化流的生成质量与语义能力同步达到新 SOTA。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **ImageNet-1K 条件生成与分类** 任务，在 **64×64 像素空间** 和 **256×256 潜空间** 两个尺度上展开系统实验，共包含 **4 组核心实验 + 3 项补充消融**，可归纳如下：\n\n1\\. 诊断实验：验证免训练分类指标\n\n- **目的** 确认提出的单步梯度分类器（Algorithm 1）能否可靠反映模型语义能力。\n- **做法** 与标准线性探针逐层结果对比（图 4）。\n- **结论** 该指标与线性探针“天花板”高度一致，且无需任何额外训练，可作为后续消融的快捷评价工具。\n\n2\\. 策略消融：三种对齐方式对比（ImageNet 64×64，400k iter）\n\n| 变量 | 可选值 | 主要观测指标 |\n| --- | --- | --- |\n| 反向策略 | F-REPA / D-REPA / R-REPA | FID、sFID、IS、分类准确率 |\n| 对齐位置 | 全部 8 块 vs 仅 {1,2} vs {7,8} |\n| 对齐深度 | layer 2 / 4 / 6 |\n\n- **关键结论**（表 1 + 图 5 左）：\n- **R-REPA 显著优于另两种策略**（FID 11.93 vs 12.12/12.19）。\n- **对齐后层（7&8）→ 最佳 FID**；对齐前层（1&2）→ 最高准确率，但牺牲 FID。\n- **越深 layer 越好**：layer-6 将 FID 从 11.93 降至 11.71。\n\n3\\. 超参消融（同样 64×64）\n\n- **CFG scale 网格搜索**（图 5 左）：Reverse 7&8 在 3.1 处最优，且对尺度鲁棒。\n- **训练曲线**（图 5 右）：R-REPA 在 400k 即超越基线 1M 的 FID 与准确率，**收敛提速 3.3×**。\n\n4\\. 主结果对比（SOTA 对标）\n\n4.1 64×256 样本量 50k\n\n| 模型 | FID↓ | sFID↓ |\n| --- | --- | --- |\n| TARFlow 原基线 | 4.21 | 5.34 |\n| +R-REPA（本文） | 3.69 | 4.34 |\n\n- **意义**：首次让 NFs 在 ImageNet 64×64 上 **击败 BigGAN（4.06）** 并逼近 iDDPM（2.92），且仅 **2 步采样**。\n\n4.2 256×256 潜空间\n\n| 模型 | FID↓ | sFID↓ | IS↑ |\n| --- | --- | --- | --- |\n| Latent-TARFlow | 5.15 | 6.78 | 243 |\n| +R-REPA | 4.95 | 6.89 | 235 |\n| 再调 patch=1 | 4.18 | 4.96 | 241 |\n\n- **意义**：与同期最优自回归模型（VAR 1.73）（需数百步）差距显著缩小，仍保持 **2 步采样** 优势。\n\n5\\. 补充消融（附录）\n\nC.1 对齐层深度（Detach 策略）\n\n- 在“全部块+Detach”设定下，layer 2/4/6 的 FID 仅波动 0.23，验证 **Reverse 策略本身而非层选择带来主要增益**。\n\nC.2 视觉编码器鲁棒性\n\n- 替换 DINOv2-B 为 CLIP-B / MAE-B / DINOv2-L，R-REPA 仍稳定优于无对齐基线，且 **生成质量与分类精度高度正相关**（表 8）。\n\nC.3 对齐损失权重 λ\\_align\n\n- λ\\_align=0.1 最优；过大（0.5）反而干扰密度估计，FID 回升至 13.74（表 9）。\n\n6\\. 可视化样例\n\n- **图 2、6、7** 分别给出 256×256 与 64×64 随机采样示例，展示 R-REPA 在多样类别、纹理细节上的高保真合成能力。\n\n一句话总结\n\n实验从 **诊断工具→内部消融→超参搜索→SOTA 对比→补充鲁棒性** 五个维度完整闭环，证明 **R-REPA 在保持两步采样效率的同时，将 NFs 的生成与语义能力同时推向新标杆**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为对“Flowing Backwards”框架的直接延伸或深层扩展，均围绕 **“可逆架构 + 表征对齐”** 这一新范式展开，具有理论与应用双重价值：\n\n1\\. 对齐信号的深度与广度\n\n- **多层、多尺度联合对齐**\n目前仅在 2 个 TARBlock 的单一 layer 施加对齐。可引入**层级加权**或**金字塔特征**（DINOv2 多阶段输出）同步约束浅层纹理与高层语义，观察生成一致性是否进一步提升。\n- **动态定位策略**\n设计可学习的“对齐位置门控”，让网络自动决定在哪一步、哪一层注入语义监督，避免手工搜索 block/layer。\n\n2\\. 反向对齐理论的抽象与推广\n\n- **通用可逆函数理论**\n将 R-REPA 的梯度路由机制抽象为\\*\\*“生成方向正则化”\\*\\*定理：对任意可逆映射  f_θ ，若对齐损失施加于  f_θ^(-1)  路径，则语义梯度仅更新后半段参数。可在一般耦合层、可逆 ResNet 上验证是否普遍成立。\n- **与最优传输的关联**\n把对齐损失视为在 Wasserstein 流形上对后验分布  p_θ(z|x)  的切空间投影，探讨其如何影响推前测度  (f_θ^(-1))_sharp p_0  的模态匹配速度。\n\n3\\. 跨模态与混合架构\n\n- **文本-图像联合 NF**\n将类嵌入替换为**CLIP 文本编码**，在图文对齐损失下训练**文本→图像可逆流**，实现零样本或开集生成，同时保持精确似然。\n- **NF-Diffusion 混合管线**\n用 R-REPA 训练的前半段可逆流作为**语义保持的潜码先验**，再接 1-2 步扩散精炼，形成“两步精确流 + 轻量扩散”的极速采样新范式。\n\n4\\. 任务与数据外延\n\n- **视频时间连贯流**\n把 TARBlock 的自回归维度从空间 token 改为**时空三维扫描顺序**，用 R-REPA 对齐 DINOv2-Video 特征，研究能否在保持帧一致性的同时获得精确似然模型——目前视频领域尚无天然可逆生成器。\n- **3D 几何 / 神经辐射场**\n将场景表示为体素或 Plücker 坐标下的连续密度，构建**体素-流**或**坐标-流**，用 R-REPA 对齐预训练 3D 视觉 Transformer 特征，探索**可逆生成式 NeRF** 的可能性。\n\n5\\. 训练与推理效率再提升\n\n- **对齐蒸馏（Alignment Distillation）**\n训练阶段使用大视觉编码器（DINOv2-L），推理阶段把对齐后的特征作为软标签，**蒸馏到小型自编码器**，实现编码器侧零参数开销。\n- **完全并行逆路径**\n当前“伪逆”仍依赖缓存。可探索**自回归顺序无关的耦合层设计**（如基于低秩 mask 或 block-wise invertible attn），让反向传播在训练期即可完全并行，进一步缩短 wall-clock。\n\n6\\. 评估与风险控制\n\n- **细粒度语义探针**\n除 Top-1 准确率外，引入 **mIoU、场景图一致性、OOD 鲁棒性** 等指标，检验反向对齐是否导致**表面语义过度耦合**而牺牲多样性或 OOD 密度估计。\n- **生成偏见与公平性量化**\n利用 R-REPA 的免训练分类器对大量合成样本进行**人口属性标注**，计算 Fréchet Identity Distance (FIDentity) 等公平指标，明确可逆流是否会放大预训练编码器的偏见。\n\n7\\. 理论极限与相反方向\n\n- **是否必须对齐视觉大模型？**\n尝试用**随机初始化的固定投影**或**聚类中心**作为目标特征，验证 R-REPA 的增益多大程度来自“高质量语义”而非“任何外部一致信号”。\n- **前向 vs 反向的理论临界点**\n建立两Block toy模型，推导**可逆块雅可比条件数**与对齐梯度噪声的解析关系，找出前向/反向策略各自适用的“条件数-语义强度”相图。\n\n一句话展望\n\nR-REPA 把“可逆”与“语义”首次桥接，为归一化流打开了**跨模态、跨领域、跨任务**的广阔空间；未来工作可从**理论抽象、架构革新、场景外延、效率极致**四个维度继续“Flowing Backwards”。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：标准归一化流（NF）仅优化最大似然，中间表征缺乏语义，导致生成质量受限。\n- **思路**：利用 NF 可逆架构，把“表征对齐”损失施加在**生成（z→x）而非编码（x→z）路径**，称为 **Reverse-REPA（R-REPA）**。\n- **技术**：\n1. 提出**免训练、单步梯度分类器**，快速探测模型语义能力；\n2. 设计**并行伪逆**算法，使反向对齐训练提速≈50×；\n3. 在 TARFlow 后层（7&8）深度 layer-6 施加余弦对齐损失，平衡密度与语义。\n- **结果**：\n- ImageNet 64×256：FID 3.69（↓12%），2 步采样**击败 BigGAN**；\n- ImageNet 256×256 潜空间：FID 4.18（↓19%），仍保持 2 步采样；\n- 分类准确率提升 17–18%，训练收敛**加速 3.3×**。\n- **结论**：R-REPA 首次让归一化流在生成与判别双重指标上同时达到新 SOTA，验证了“反向对齐”是可逆模型的高效语义增强范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Flowing%20Backwards%3A%20Improving%20Normalizing%20Flows%20via%20Reverse%20Representation%20Alignment，链接是https://arxiv.org/pdf/2511.22345，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.22345。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1764892008392","title":"SR-GRPO: Stable Rank as an Intrinsic Geometric Reward for Large Language Model Alignment","description":"Aligning Large Language Models (LLMs) with human preferences typically relies on external supervision, which faces critical limitations: human annotations are scarce and subjective, reward models are vulnerable to reward hacking, and self-evaluation methods suffer from prompt sensitivity and biases. In this work, we propose stable rank, an intrinsic, annotation-free quality signal derived from model representations. Stable rank measures the effective dimensionality of hidden states by computing the ratio of total variance to dominant-direction variance, capturing quality through how information distributes across representation dimensions. Empirically, stable rank achieves 84.04% accuracy on RewardBench and improves task accuracy by an average of 11.3 percentage points over greedy decoding via Best-of-N sampling. Leveraging this insight, we introduce Stable Rank Group Relative Policy Optimization (SR-GRPO), which uses stable rank as a reward signal for reinforcement learning. Without external supervision, SR-GRPO improves Qwen2.5-1.5B-Instruct by 10% on STEM and 19% on mathematical reasoning, outperforming both learned reward models and self-evaluation baselines. Our findings demonstrate that quality signals can be extracted from internal model geometry, offering a path toward scalable alignment without external supervision.","url":"https://huggingface.co/papers/2512.02807","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.392Z","metadata":{"authors":["Yixuan Tang","Yi Yang"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2512.02807.pdf","arxivUrl":"https://arxiv.org/abs/2512.02807","arxivId":"2512.02807","coolPaperUrl":"https://papers.cool/arxiv/2512.02807","published":"2025-12-02T14:21:29Z","updated":"2025-12-02T14:21:29.000Z","zh_summary":"将大语言模型（LLMs）与人类偏好对齐通常依赖外部监督，但这种方法存在关键限制：人工标注稀缺且主观，奖励模型容易被奖励机制操纵，自我评估方法则受到提示敏感性和偏差的影响。在本研究中，我们提出了稳定秩（stable rank），这是一种来源于模型表示的内在、无需标注的质量信号。稳定秩通过计算总方差与主方向方差的比值来衡量隐藏状态的有效维度，从而通过信息在表示维度中的分布来捕捉质量。从实验上看，稳定秩在RewardBench上达到84.04%的准确率，并通过Best-of-N采样比贪心解码平均提高任务准确率11.3个百分点。基于这一洞察，我们提出了稳定秩组相对策略优化（Stable Rank Group Relative Policy Optimization，SR-GRPO），将稳定秩作为强化学习的奖励信号。在无需外部监督的情况下，SR-GRPO使Qwen2.5-1.5B-Instruct在STEM任务上提升了10%，在数学推理任务上提升了19%，优于学习型奖励模型和自我评估基线。我们的研究表明，质量信号可以从模型内部几何中提取，为在无需外部监督的情况下实现可扩展的模型对齐提供了一条路径。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决大语言模型（LLM）对齐过程中对外部监督（人工标注、奖励模型、可验证信号等）的高度依赖问题。核心痛点包括：\n\n- 人工标注稀缺且主观，难以覆盖细粒度行为；\n- 训练得到的奖励模型易受奖励黑客攻击，泛化性差；\n- 自评估方法对提示敏感且存在系统偏差；\n- 可验证奖励仅适用于数学、代码等可自动判定的任务，无法扩展到开放域对话。\n\n为此，作者提出一种**完全无需外部标注**的对齐思路：利用模型自身隐藏状态的**内在几何属性——稳定秩（stable rank）**——作为质量信号。稳定秩通过衡量隐藏状态矩阵的有效维度，捕捉“信息是否分散在多个语义方向”这一几何特征，从而在无监督条件下判断生成文本的质量。\n\n论文进一步将该信号嵌入强化学习框架，提出**SR-GRPO**算法，实现从零开始、不依赖任何偏好数据的模型对齐，并在推理、对话等多类任务上取得与甚至超过传统监督方法的效果。\n\nQ2: 有哪些相关研究？\n\n论文在第 7 节“Related Work”中系统梳理了三条研究脉络，并指出自身与它们的区别。以下按主题归纳，并补充关键文献出处（按原文引用编号）。\n\n1\\. 依赖外部反馈的对齐方法\n\n**核心特征**：需要人类标注、偏好数据或训练显式奖励模型。\n\n- **RLHF 系列**\n- Ouyang et al. 2022：InstructGPT 的 RLHF 流水线，训练 Bradley-Terry 奖励模型后再用 PPO 微调策略。\n- Bai et al. 2022：HH-RLHF 数据集与“有用+无害”助手训练。\n- **偏好优化变体**\n- Rafailov et al. 2023：DPO，直接用偏好对优化策略，省去显式奖励模型，但仍需成对标注。\n- Ethayarajh et al. 2024：KTO，将偏好信号转化为二元匹配信号。\n- Chakraborty et al. 2024：MaxMin-RLHF，处理多分布人类偏好。\n- **过程或生成式奖励模型**\n- Zhang et al. 2025b；Yin et al. 2025：为数学推理提供逐步分数或文本批评。\n\n**共同局限**：全部依赖外部监督，面临奖励黑客、标注成本高、领域迁移差等问题。\n\n2\\. 减少/替代人工标注的自动信号方法\n\n**目标**：降低或消除人工标注，但多数仍需要可验证答案或模型自评。\n\n- **可验证奖励（Verifiable Rewards）**\n- DeepSeek-AI 2025：DeepSeek-R1，用代码执行器或数学答案检验器提供稀疏奖励。\n- Lambert et al. 2024：Tülu 3，在代码/数学任务上用单元测试或答案匹配。\n**局限**：只能用于可自动判定的封闭任务，无法评价开放域对话。\n- **自评估 / AI 反馈（Self-Evaluation, RLAIF）**\n- Yuan et al. 2024：Self-Rewarding LM，用模型自己给出的 1–5 分作为奖励。\n- Lee et al. 2024：RLAIF，用另一个 LLM 代替人类标注偏好。\n- Garg et al. 2025：IPO，利用“Yes/No”token 概率构造偏好信号。\n**局限**：对提示敏感、存在立场偏差，小模型难以给出可靠评分。\n- **内部激活诊断**\n- He et al. 2024c：Factoscope，用隐藏状态检测事实性。\n- Chen et al. 2024：INSIDE，用内部状态识别幻觉风险。\n**区别**：上述工作仅做**事后诊断**，本文首次将几何度量直接用作**在线优化奖励**。\n\n3\\. 表征几何与生成质量的理论研究\n\n- **Softmax 瓶颈与秩需求**\n- Yang et al. 2018：指出低秩隐藏表示会限制 softmax 表达能力，需高秩分布才能建模自然语言。\n- Godey et al. 2024：在小型 LM 上验证“语言分布高秩 → 需要高秩表示”。\n- **表示塌陷与退化**\n- Gao et al. 2019：训练 NLG 模型时，若表示塌陷到狭窄锥体，生成质量下降。\n- **无标签秩度量**\n- Roy & Vetterli 2007：提出 effective rank（熵加权）。\n- Garrido et al. 2023：RankMe，用自监督表示的有效秩预测下游任务性能，无需标签。\n\n**本文差异**：首次将**稳定秩**（Frobenius-范数/谱-范数平方比）引入 LLM 对齐，并证明其作为**密集奖励信号**在强化学习中的可行性与优越性。\n\n小结\n\n| 类别 | 代表文献 | 是否需外部监督 | 与本文关系 |\n| --- | --- | --- | --- |\n| RLHF / DPO 等 | Ouyang22, Rafailov23 | 是（偏好对） | 本文完全去除偏好数据 |\n| 可验证奖励 | DeepSeek25 | 是（答案/测试） | 仅封闭任务，本文面向开放域 |\n| 自评估 | Yuan24, Garg25 | 是（自提示） | 本文无需任何提示打分，零标注 |\n| 内部激活诊断 | He24c, Chen24 | 否（事后） | 本文把几何度量转为在线奖励 |\n| 表征秩理论 | Yang18, Gao19 | 否（理论） | 本文实证并优化，提出 SR-GRPO |\n\nQ3: 论文如何解决这个问题？\n\n论文提出“**稳定秩（stable rank）**”这一**完全无需外部标注**的内在几何信号，并围绕它设计了一整套**从质量评估到策略优化**的闭环方案。核心步骤如下：\n\n1\\. 发现稳定秩与文本质量的强相关性\n\n- **定义**：对一条长度为 T 的回复，提取最后一层隐藏状态矩阵  H ∈ ℝ^(T×d) ，计算\n\nSR(H)= (‖H‖_F^2) / (‖H‖_2^2)= (∑_iσ_i^2) / (σ_1^2).\n\n该比值衡量“信息是否分散在多个语义方向”：\n\n- 若表示塌陷到单一主方向，SR≈1；\n- 若各方向方差均衡，SR 接近真实秩，代表高维、丰富的语义编码。\n- **零样本验证**：在 RewardBench 2 985 对偏好样本上，**仅用 SR 大小判定优劣**，无需任何训练，即可达到 **84.04 % 准确率**，优于 LLM-as-Judge、IPO 等强基线。\n- **Best-of-N 解码**：用 SR 作为评分函数，在 STEM 与数学基准上平均比贪心解码提升 **11.3 个百分点**，证明其可作为**测试时奖励代理**。\n\n2\\. 把稳定秩嵌入强化学习——SR-GRPO\n\n目标：彻底摆脱偏好数据、奖励模型或人工标注，仅依靠 SR 提供**密集奖励**完成对齐。\n\n2.1 算法框架\n\n- **基础**：Group Relative Policy Optimization (GRPO)\n- 每个 prompt 采样 K 条回答，组内做**相对排序**，无需额外价值网络。\n- **奖励**：用**冻结的参考模型** π\\_ref 计算每条回答的 SR，保证奖励信号**静态、不可被策略操纵**。\n- **方差控制**：组内标准化\n\nA_k= (r_k − μ) / (σ+ε),\n\n消除量纲影响，提供稳定梯度。\n- **目标函数**\n\nJ(ϕ)=𝔼_xl[(1) / (K)∑_(k=1)^K ρ_k A_k − βD_(KL)(π_ϕ‖π_(ref))r],\n\n其中  ρ_k=π_ϕ(y_k|x)/π_(ϕ_old)(y_k|x)  为重要性权重。\n\n2.2 训练细节\n\n- 采用 LoRA（r=16, α=32）高效微调；计算 SR 时**临时关闭 LoRA 适配器**，确保奖励来自冻结基模型。\n- 计算复杂度  O(Td) ，相比一次前向可忽略；截断到 512 token 已足够，无需长序列。\n\n3\\. 实验结果：零标注超越强监督\n\n| 模型 | 方法 | STEM↑ | 数学↑ | 对话↑ |\n| --- | --- | --- | --- | --- |\n| Qwen2.5-1.5B-Instruct | 基线 | 33.3 | 28.0 | 1036 |\n| + 1.7B 奖励模型 | 31.4 | 27.3 | 1043 |\n| + Self-Reward | 31.6 | 30.0 | 1041 |\n| + SR-GRPO | 34.5 | 32.4 | 1062 |\n| DeepSeek-R1-Distill-1.5B | 基线 | 35.8 | 58.5 | 914 |\n| + SR-GRPO | 38.4 | 64.7 | 932 |\n\n- **零外部标注**情况下，SR-GRPO 在数学推理上**最高提升 19 %**，显著优于训练过的奖励模型与自评估基线。\n- 在开放域对话 WildBench 上 Elo 提升 **19–26 分**，证明 SR 信号不仅适用于可验证任务，也适用于**通用聊天质量**。\n\n4\\. 解释性分析：SR 到底捕捉了什么？\n\n对 RewardBench 5 970 条回答计算 37 项可解释指标，发现 SR 同时关联三大质量维度：\n\n| 维度 | 典型指标 | 相关性 |\n| --- | --- | --- |\n| 语义连贯 | 相邻句相似度均值、QA 对齐一致性 | ρ=0.31 |\n| 信息密度 | 压缩比、词汇多样性 | ρ=0.23–0.24 |\n| 推理结构 | 转折/因果连接词（however, because） | 正相关；枚举、附加词 |\n\n⇒ SR **惩罚**冗余啰嗦、模板化连接；**奖励**紧凑、连贯、关键处出现因果/转折词的高质量论述。\n\n5\\. 设计鲁棒性验证\n\n- **跨层实验**：仅最后 1–2 层 SR 与质量强相关，早期层≈随机，验证“深层抽象表示才含质量信号”。\n- **替代度量**：条件数、有效秩、PCA 95 % 方差维度在 RewardBench 上分别仅 36 %、54 %、61 % 准确率，**稳定秩 84 % 显著领先**。\n- **输入长度 & 提示格式**：512 token 后饱和；6 种格式变化 ≤3 %，部署无需精细调格式。\n\n结论\n\n论文通过“稳定秩”把**表示几何 → 文本质量 → 密集奖励 → 策略优化**完整打通，实现了\n\n- **零人工标注**\n- **零可验证答案**\n- **零提示工程**\n\n的 LLM 对齐，并在多模型、多任务上取得与甚至超过传统监督方法的性能，为**可扩展、无需标注的对齐**提供了一条全新路径。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 **4 组实验 + 3 项消融**，覆盖 **零样本评估 → 测试时解码 → 强化学习对齐 → 可解释分析 → 设计鲁棒性验证** 完整链条。所有实验均公开代码与配置，可复现。\n\n1 零样本奖励代理实验（RewardBench）\n\n**目的**：验证“稳定秩无需任何训练即可判断偏好”。\n\n- **数据**：RewardBench 2 985 对人工标注偏好（Chat / Chat-Hard / Safety / Code / Math）。\n- **方法**：对每对回复计算 SR，预测“SR 高者”为优选。\n- **模型**：5 个规模差异巨大的基座\n– Qwen2.5-1.5B-Instruct\n– Qwen3-0.6B\n– Qwen3-8B\n– Llama-3.1-8B-Instruct\n– Phi-3.5-mini-Instruct\n- **基线**\n– Pointwise Scoring（1-5 自评）\n– Pairwise Comparison（直接比两回复）\n– IPO（Yes/No token 概率）\n\n**结果**\n\n| 模型 | 最佳基线 | 稳定秩 | Δ |\n| --- | --- | --- | --- |\n| Qwen3-8B | 83.70 | 84.04 | +0.34 |\n| Qwen2.5-1.5B | 65.85 | 75.95 | +10.1 |\n| Llama-3.1-8B | 58.14 | 68.36 | +10.2 |\n\n⇒ SR 在所有模型上 **≥ 最佳基线**，小模型优势更显著。\n\n2 Best-of-N 解码实验\n\n**目的**：验证 SR 作为**测试时评分函数**能否持续提升任务准确率。\n\n- **基准**\n– STEM：GPQA、MMLU-redux\n– 数学：MATH500、OlympiadBench、AMC23\n- **模型**：4 个 1.5 B 级别模型（Qwen2.5-1.5B、Phi-3.5-mini、Llama-3.2-1B、DeepSeek-R1-Distill-1.5B）\n- **协议**：温度 0.7/top-p 0.9 采样 N∈{1,4,8,16}，分别用“随机选”与“SR 最高选”做对比。\n\n**主要结果（N=16）**\n\n| 模型 | 贪心@1 | 随机@16 | SR@16 | ΔRand | ΔGreedy |\n| --- | --- | --- | --- | --- | --- |\n| Llama-3.2-1B | 19.8 | 19.8 | 26.5 | +33.8 % | +20.5 % |\n| Qwen2.5-1.5B | 35.0 | 36.3 | 41.0 | +13.0 % | +17.0 % |\n| DeepSeek-R1 | 57.8 | 57.8 | 60.7 | +5.0 % | +10.2 % |\n\n⇒ SR 选择**始终优于随机**，且随 N 增大增益扩大；随机常低于贪心，说明 SR 真正识别质量而非采样多样性。\n\n3 强化学习对齐实验（SR-GRPO）\n\n**目的**：验证“仅用 SR 作密集奖励”能否在**零标注**条件下提升模型表现。\n\n- **训练集**：SmolTalk2（仅 prompt，无偏好标签）\n- **训练步**：Qwen2.5-1.5B 400 步 / DeepSeek-R1-1.5B 300 步；LoRA r=16；K=8 条回答/组。\n- **评测基准**\n– STEM：GPQA、MMLU-redux → 平均准确率\n– 数学：MATH500、AIME25、OlympiadBench、AMC23 → 平均准确率\n– 对话：WildBench → GPT-4o-mini 评判 Elo\n\n**对照**\n① 基座模型\n② + 1.7B 训练奖励模型（Skywork-Reward）\n③ + Self-Reward（自评 1-5）\n④ + Perplexity（负 PPL）\n⑤ + IPO（Yes/No 概率）\n\n**结果**\n\n| 模型 | 方法 | STEM | 数学 | WildBench |\n| --- | --- | --- | --- | --- |\n| Qwen2.5-1.5B | 基线 | 33.3 | 28.0 | 1036 |\n| + RM | 31.4 | 27.3 | 1043 |\n| + Self-Reward | 31.6 | 30.0 | 1041 |\n| + SR-GRPO | 34.5 | 32.4 | 1062 |\n| DeepSeek-R1 | 基线 | 35.8 | 58.5 | 914 |\n| + SR-GRPO | 38.4 | 64.7 | 932 |\n\n⇒ SR-GRPO **零标注**即可在数学任务上提升 **4.4–6.2 pp**，对话 Elo 提升 **19–26**，**全面超越**外部奖励模型与自评估方法。\n\n4 可解释性分析实验\n\n**目的**：量化 SR 与人工可理解指标的相关性，回答“SR 到底奖励了什么”。\n\n- **数据**：RewardBench 5 970 条回答 + 2 985 对偏好差值\n- **指标**\n– 语义连贯：相邻句相似度、progression score、QA 对齐一致性\n– 信息密度：token 数、压缩比、词汇多样性（TTR）\n– 语言标记： discourse/logical marker 每 100 token 频率\n\n**关键相关系数（Spearman ρ）**\n\n| 维度 | 指标 | ρ |\n| --- | --- | --- |\n| 连贯 | QA alignment consistency | +0.316 |\n| 连贯 | Progression score | +0.313 |\n| 连贯 | Coherence std | −0.356 |\n| 密度 | Lexical diversity | +0.238 |\n| 密度 | Compression ratio | +0.233 |\n| 密度 | Token count | −0.294 |\n| 标记 | Contrastive (存在与否) | +0.187 |\n| 标记 | Enumeration | −0.148 |\n| 标记 | Total marker count | −0.204 |\n\n⇒ SR **奖励**紧凑、连贯、转折/因果关键词恰当中肯的文本；**惩罚**冗长、模板化、枚举式堆砌的连接词。\n\n5 消融实验\n\n5.1 替代内在维度度量\n\n- **条件数、PCA-95 % 方差、有效秩** 在 RewardBench 整体准确率分别为 **36 %、61 %、54 %**，稳定秩 **84 %** 显著领先。\n\n5.2 上下文长度\n\n- 截断到 128 token 准确率跌至 62.6 %；**≥512 token 后饱和**，提升 <0.2 %，说明 SR 捕捉核心语义而非机械鼓励更长。\n\n5.3 输入提示格式\n\n- 6 种格式（无前缀、User/Assistant、Question/Answer 等）整体波动 **≤3 %**，部署时无需精细格式调优。\n\n实验总结\n\n| 实验 | 结论 |\n| --- | --- |\n| RewardBench | 零训练即可达 SOTA 偏好预测精度 |\n| Best-of-N | 测试时 SR 评分持续 > 贪心+随机 |\n| SR-GRPO | 零标注 RL 对齐，数学+对话全面超监督基线 |\n| 可解释 | SR 显式关联“连贯+密度+关键推理词” |\n| 消融 | 度量、长度、格式设计鲁棒，最终层 SR 最关键 |\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为**理论深挖**、**信号扩展**、**算法升级**与**系统应用**四个层面。\n\n1 理论深挖：稳定秩与生成质量的因果机制\n\n- **因果验证**：当前仍是强相关性，可用干预式实验（如人为注入塌陷噪声→观察SR与质量是否同步下降）检验**因果链**。\n- **秩-容量-泛化三角关系**：探究“稳定秩 ↔ 模型容量利用率 ↔ 下游泛化”的定量关系，建立类似“秩-泛化误差界”的理论框架。\n- **层间动态**：仅最后一层SR最有效，可分析**信息几何随深度演化**的解析表达式，解释为何深层才出现质量判别模式。\n\n2 信号扩展：多几何度量融合\n\n- **局部-全局联合**：将稳定秩（全局）与**点级雅可比谱**（局部敏感度）结合，形成token-level密集奖励，缓解长序列稀疏问题。\n- **时序演化奖励**：对隐藏状态做**奇异值熵时序曲线**，奖励“逐步展开而非一次性塌陷”的生成动力学。\n- **跨模态几何**：在视觉-语言模型中，把图像-patch矩阵与文本隐藏矩阵的**联合谱分布**作为多模态质量信号。\n\n3 算法升级：训练与推理框架\n\n- **自适应截断**：根据生成难度动态选择计算SR的token窗口，减少>50 %计算量。\n- **可学习投影**：在SR计算前加**可微正交投影** P_θ ，让策略优化同时学习“最能暴露质量的几何子空间”，形成**Meta-SR-GRPO**。\n- **分层混合奖励**：\n– 可验证任务：优先用规则奖励；\n– 开放域：自动切换至SR；\n实现**统一奖励调度器**，无需人工指定领域。\n- **在线分布修正**：用SR实时检测分布漂移（SR突然下降）→ 触发**即时KL重校准**或**提示重采样**，抵御奖励黑客。\n\n4 系统应用与评测\n\n- **小模型专属对齐**：SR不依赖大模型评判，可在**1 B以下设备端模型**做持续自我改进，形成“边缘自对齐”范式。\n- **长文本与对话连贯**：用SR奖励**多轮一致性**（跨回合隐藏状态拼接后算SR），缓解“对话前后矛盾”问题。\n- **多语言几何差异**：探究不同语系隐藏空间的本征维度分布，修正SR的语种归一化系数，实现**跨语种零标注对齐**。\n- **安全与幻觉检测**：将SR与**内部早期预警指标**（如token-entropy突变）联合，实现**生成前**实时拒绝或重写。\n- **公开挑战榜**：建立以“零标注+几何奖励”为规则的**RankBench**，推动社区在相同无监督条件下对比新方法。\n\n5 风险与伦理扩展\n\n- **几何奖励黑客**：策略可能生成“高SR但人类不可读”的伪丰富表示，需设计**人类可读性正则项**或**多模态重排名**作为防护。\n- **隐私泄漏**：隐藏状态含训练数据痕迹，可研究**差分私有奇异值计算**，防止通过SR梯度反推敏感信息。\n\n总结\n\n稳定秩为“无监督对齐”打开了一扇门，后续可在**理论因果、几何融合、算法自适应、系统部署**等多条线上持续深挖，最终目标是**让任意规模模型在任何语言、任何任务上都能“自己产生奖励、自己持续对齐”**。\n\nQ6: 总结一下论文的主要内容\n\n**SR-GRPO：用稳定秩实现零标注大模型对齐**\n\n背景痛点\n\n- RLHF 依赖昂贵人工标注，奖励模型易被黑客攻击。\n- 自评估、可验证奖励局限在特定任务或提示敏感。\n→ 亟需**无需外部监督**的内在质量信号。\n\n核心贡献\n\n1. 提出**稳定秩（SR）**：\n SR(H)=‖H‖_F^2/‖H‖_2^2=∑σ_i^2/σ_1^2 \n\n- 仅利用**最后一层隐藏状态**的几何维度，零标注即可衡量文本质量。\n2. 零样本能力\n\n- RewardBench 偏好预测 **84.04 %** 准确率，超 LLM-as-Judge 等强基线。\n- Best-of-N 解码平均提升 **11.3 pp**，跨 4 模型 5 基准一致有效。\n3. SR-GRPO 算法\n\n- 将 SR 作为**密集奖励**嵌入 Group Relative Policy Optimization，**零偏好数据**训练。\n- Qwen2.5-1.5B-Instruct 数学任务 **+19 %**，对话 Elo **+26**；DeepSeek-R1 继续提升，**全面超越**外部奖励模型与自评估方法。\n4. 可解释性\nSR 同时关联**语义连贯**、**信息密度**与**关键推理词**，自动抑制冗余、奖励紧凑且逻辑清晰的回答。\n\n5. 鲁棒性\n\n- 仅最后层有效；512 token 截断即饱和；提示格式波动 <3 %；替代维度度量显著落后。\n\n结论\n\n稳定秩把“表示几何”转化为“在线奖励”，首次实现**大规模 LLM 零标注强化学习对齐**，为可扩展、低成本、高鲁棒的模型对齐提供了新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SR-GRPO%3A%20Stable%20Rank%20as%20an%20Intrinsic%20Geometric%20Reward%20for%20Large%20Language%20Model%20Alignment，链接是https://arxiv.org/pdf/2512.02807，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.02807。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1764892008392","title":"Jina-VLM: Small Multilingual Vision Language Model","description":"We present Jina-VLM, a 2.4B parameter vision-language model that achieves state-of-the-art multilingual visual question answering among open 2B-scale VLMs. The model couples a SigLIP2 vision encoder with a Qwen3 language backbone through an attention-pooling connector that enables token-efficient processing of arbitrary-resolution images. Across standard VQA benchmarks and multilingual evaluations, Jina-VLM outperforms comparable models while preserving competitive text-only performance.","url":"https://huggingface.co/papers/2512.04032","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.392Z","metadata":{"authors":["Andreas Koukounas","Georgios Mastrapas","Florian Hönicke","Sedigheh Eslami","Guillaume Roncari","Scott Martens","Han Xiao"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2512.04032.pdf","arxivUrl":"https://arxiv.org/abs/2512.04032","arxivId":"2512.04032","coolPaperUrl":"https://papers.cool/arxiv/2512.04032","published":"2025-12-03T18:13:41Z","updated":"2025-12-03T18:13:41.000Z","zh_summary":"我们推出了 Jina-VLM，这是一款拥有 24 亿参数的视觉语言模型，在开放的 20 亿参数规模 VLM 中实现了最先进的多语言视觉问答性能。该模型通过一个注意力池化连接器将 SigLIP2 视觉编码器与 Qwen3 语言主干相结合，使任意分辨率图像的令牌处理更加高效。在标准 VQA 基准测试和多语言评估中，Jina-VLM 的表现优于同类模型，同时保持了具有竞争力的纯文本性能。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对当前视觉-语言模型（VLM）在实际部署中面临的两大痛点提出解决方案：\n\n1. **多语言能力退化**\n现有 VLM 在英语基准上表现强劲，但在非英语场景下性能显著下降，出现跨语种视觉理解不均衡。\n\n2. **训练与推理成本高昂**\n高质量 VLM 通常参数量大、训练数据规模庞大，导致资源受限的研究者与从业者难以负担。\n\n为此，作者提出 **jina-vlm**（2.4 B 参数），通过以下手段在“小模型”尺度下同时缓解上述问题：\n\n- 架构层面：任意分辨率图像的**重叠切片 + 注意力池化**连接器，将视觉 token 数量压缩 4×，降低计算开销。\n- 训练层面：两阶段多语种数据配方，并在多模态训练中显式混入文本数据，抑制语言能力的灾难性遗忘。\n\n实验表明，jina-vlm 在 2 B 级开源 VLM 中取得**多语种视觉问答 SOTA**，且英语 VQA 与纯文本能力保持竞争力，从而验证“小模型亦可在多语种视觉理解上媲美大模型”的核心论点。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中系统梳理了与 jina-vlm 密切相关的四条研究脉络，并给出代表性文献。可归纳为：\n\n1. VLM 通用架构与训练范式\n\n- PaLI、LLaVA、Qwen-VL、InternVL、Ovis 等系列工作奠定了“ViT 编码器–连接器–解码器”这一标准框架。\n- 训练策略上，交替多模态指令微调与纯文本训练、引入学术 VQA 数据集、以及大规模混合数据策展等被反复验证。\n2. 高效分辨率无关图像处理\n\n- 固定分辨率 ViT 需 resize，丢失细节；直接高分辨率带来二次方计算膨胀。\n- 现有解决方案包括：\n– 重叠切片（Molmo/PixMo）\n– 动态分辨率+位置嵌入（Naive Dynamic Resolution、Multimodal RoPE）\n– 原生分辨率 ViT（NaViT）\n– 视觉 token 压缩/剪枝（PyramidDrop、LLaVA-PruMerge、VisionZip 等）\n- jina-vlm 在此基础上提出“重叠切片+注意力池化”组合，兼顾细节保留与 token 降量。\n3. 小型化 VLM\n\n- MobileVLM-V2、MiniCPM-V、SmolVLM 等证明 <2 B 参数量仍可取得接近大模型的性能。\n- 量化、激进分辨率缩减、系统级设计空间探索是主要降本手段。\n4. 多语种 VLM 与文本能力保持\n\n- 许多轻量级模型在非英语任务上明显退化；Qwen-VL、InternVL 等通过定向多语种数据缓解。\n- Pangea、Parrot 等工作将指令微调扩展至 30+ 语言。\n- 多模态训练常导致纯文本性能下降，常用对策：混合文本数据、学习率调度、部分骨干冻结等。jina-vlm 采用“文本-多模态联合训练”策略，在 2 B 规模下实现多语种视觉问答 SOTA 的同时，文本基准退化可控。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“架构-训练”协同设计，把多语种视觉理解能力压缩到 2.4 B 参数规模，具体手段如下：\n\n1\\. 架构：任意分辨率 + 4× 视觉 token 压缩\n\n- **重叠切片**\n将输入图像按 378×378 像素、112 像素重叠切分为 ≤12 张 tile，外加一张全局缩略图，保证高分辨率细节不丢失。\n- **双中间层特征融合**\n取 SigLIP2 第 18 层（−9）与第 24 层（−3）输出拼接，兼顾低层空间细节与高层语义。\n- **注意力池化连接器**\n对 2×2 邻域 patch 做 attention pooling，公式：\n\nH_(pooled)=softmax!(QW_Q(H_(concat)W_K)^top{√d_k})H_(concat)W_V W_O\n\n729 token → 182 token，再经 SwiGLU 投影到 Qwen3 词嵌入空间，实现 4× 降量且保留局部结构。\n\n2\\. 训练：两阶段多语种混合配方\n\n| 阶段 | 目标 | 数据 | 关键设置 |\n| --- | --- | --- | --- |\n| Stage-1 对齐 | 视觉-文本跨语种对齐 | 5 M 图文对（PixmoCap、PangeaIns 等）+ 15 % 纯文本 Common Corpus | 连接器 LR 高 10×，快速收敛；全部参数可训 |\n| Stage-2 指令微调 | 指令跟随、VQA、推理 | 15 M 样本（LLaVA-OneVision、Cauldron、Cambrian 等）+ 文本指令集 | 先单源 batch 30 K 步，再混合 batch 30 K 步，缓解数据异构 |\n\n3\\. 能力保持机制\n\n- **文本数据混入**：两阶段均加入 15 % 纯文本数据，抑制多模态训练带来的语言灾难性遗忘。\n- **学习率差异化**：Vision Encoder 用低 LR（6e-6→5e-6），Connector 用高 LR（2e-4→5e-6），LLM 用中等 LR（2e-5→1e-5），平衡视觉与语言权重更新幅度。\n\n通过上述设计，jina-vlm 在 2 B 级开源 VLM 中取得：\n\n- 多语种视觉问答 SOTA（MMMB 78.8、Multilingual MMBench 74.3）\n- 英语 VQA 平均 72.3，领先同规模模型\n- 纯文本能力仅轻微下降（MMLU 56.1 vs 62.6），实现“小参数、多语言、强视觉”三者兼得。\n\nQ4: 论文做了哪些实验？\n\n论文在 6 大能力维度、累计 25+ 公开基准上，与 4 款同规模（≈2 B）开源 VLM 进行系统对比。所有实验均基于 VLMEvalKit，统一英文 prompt 格式，保证公平可比。\n\n1. 通用视觉问答（8 项英文基准）\nAI2D / ChartQA / TextVQA / DocVQA / InfoVQA / OCRBench / SEED-Bench-2-Plus / CharXiv\n→ jina-vlm 平均 72.3，领先次优模型 0.7–5.9 分。\n\n2. 文档与现实世界理解（6 项）\nMME / MMB v1.1 / MMStar / RealWorldQA / MME-RealWorld / R-Bench\n→ 多模态综合 67.4，现实场景 61.9，均位列第一梯队。\n\n3. 多图推理与幻觉（5 项）\nBLINK / MuirBench / MMT-Bench + HallBench / POPE\n→ 多图平均 47.3（训练数据限制），但 POPE 90.3（幻觉最低）。\n\n4. 数学与逻辑推理（6 项）\nMMMU / MathVista / MathVision / MathVerse / WeMath / LogicVista\n→ 综合 33.1，显著超越 Qwen2-VL-2B（25.3），与 InternVL3-2B（35.3）接近。\n\n5. 纯文本能力（5 项）\nMMLU / MMLU-Pro / GSM-8k / ARC-C / HellaSwag\n→ 平均 58.9，较 backbone Qwen3-1.7B（63.3）仅降 4.4 分，证明灾难性遗忘受控。\n\n6. 多语种多模态理解（3 项）\nMMMB（6 语）/ Multilingual MMBench（6 语）/ MTVQA\n→ 两项平均 78.8 & 74.3，均为 2 B 级开源模型 SOTA；MTVQA 25.6，仅次于 Qwen3-VL-2B 的 27.3。\n\n综上，实验覆盖英文 VQA、文档-OCR、多图、数学、幻觉、纯文本、多语种等场景，验证了 jina-vlm 在 2.4 B 参数规模下“多语言视觉理解领先、通用能力不降级”的核心主张。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分点列出供参考：\n\n- **更高分辨率的高效处理**\n当前 12 tile 上限对 4K 图像仍显不足，可探索：\n\n- 动态 tile 预算分配（依据图像内容复杂度或梯度能量）\n- 级联或金字塔式 token 压缩，进一步降低 >2K 分辨率下的二次方计算\n- **多图 / 视频序列理解**\n训练集以单图为主，多图基准平均落后 5–7 分。可引入：\n\n- 时间-空间联合切片与 token 复用机制\n- 跨帧对比学习或帧间差异掩码，减少冗余视觉 token\n- **跨语种数据配比理论**\n目前多语种性能靠经验配比 50 % 非英语数据，可研究：\n\n- 按语种资源高低自适应采样（temperature-based sampling）\n- 基于梯度范数或遗忘度量的在线数据调度，实现“语种-任务”帕累托前沿\n- **更细粒度的幻觉诊断与抑制**\nPOPE 虽达 90.3，但 HallBench 仅 39.1，说明细节幻觉仍在。可尝试：\n\n- 引入视觉掩码一致性检查（masked image consistency, MIC）\n- 对比式解码或置信度重排序，降低低视觉激活 token 的生成概率\n- **文本能力零退化极限**\n当前 MMLU 绝对下降 6.5 分，可探索：\n\n- 部分层冻结或 LoRA 融合，仅微调跨模态接口\n- 知识蒸馏：固定 LLM 主权重，用 KL 约束保持原始输出分布\n- **安全性与对齐**\n论文未涉及红队测试或有害内容过滤，后续可：\n\n- 加入视觉安全基准（Visual-SafetyBench、MM-Safety）\n- 多语种有害指令微调，强化跨语种拒答一致性\n- **缩放定律验证**\n训练配方是否随参数线性放大仍未知，可：\n\n- 在 7 B / 13 B 规模的 Qwen3 骨干上复现两阶段配方\n- 观测多语种增益与英文能力的缩放曲线，验证小模型结论的普适性\n- **端侧部署优化**\n2.4 B 虽已小型化，但 182×13=2366 视觉 token 对手机仍昂贵，可：\n\n- 8-bit / 4-bit 量化与通道级分组注意力\n- NPU 友好的 2×2 池化算子硬化，实现 <200 ms 端到端延迟\n\n通过上述探索，可进一步释放小参数 VLM 在多语种、高分辨率、多图视频及端侧场景下的潜力。\n\nQ6: 总结一下论文的主要内容\n\n论文核心速览\n\n1\\. 研究动机\n\n- **多语种退化**：现有 VLM 在非英语任务上性能骤降\n- **资源门槛**：高质量 VLM 普遍>10 B，训练与部署昂贵\n\n2\\. jina-vlm 方案（2.4 B 参数）\n\n| 模块 | 关键设计 | 效果 |\n| --- | --- | --- |\n| 视觉编码 | SigLIP2-So400M/14-384 | 强多语种表征 |\n| 任意分辨率 | 重叠 378×378 tile + 全局缩略图 | 保留高分辨率细节 |\n| VL 连接器 | 双中间层特征 + 2×2 注意力池化 | 729 → 182 token，降量 4× |\n| 语言骨干 | Qwen3-1.7 B-Base | 保持文本能力 |\n| 训练策略 | 两阶段全参数更新，混入 15 % 纯文本数据 | 抑制灾难遗忘 |\n\n3\\. 实验结果（25+ 基准）\n\n- **英文 VQA 平均 72.3** → 2 B 级最佳\n- **多语种 MMMB 78.8 / Multilingual MMBench 74.3** → 同规模 SOTA\n- **纯文本仅降 4.4 分** → 语言退化可控\n- **幻觉 POPE 90.3** → 生成可靠性高\n\n4\\. 结论\n\n小参数 VLM 通过“分辨率无关压缩 + 多语种-文本混合训练”，可在 2 B 量级实现跨语种视觉问答新 SOTA，且通用能力与文本性能不牺牲，为资源受限场景提供可用的高性能多模态模型。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Jina-VLM%3A%20Small%20Multilingual%20Vision%20Language%20Model，链接是https://arxiv.org/pdf/2512.04032，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04032。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1764892008393","title":"CookAnything: A Framework for Flexible and Consistent Multi-Step Recipe Image Generation","description":"Cooking is a sequential and visually grounded activity, where each step such as chopping, mixing, or frying carries both procedural logic and visual semantics. While recent diffusion models have shown strong capabilities in text-to-image generation, they struggle to handle structured multi-step scenarios like recipe illustration. Additionally, current recipe illustration methods are unable to adjust to the natural variability in recipe length, generating a fixed number of images regardless of the actual instructions structure. To address these limitations, we present CookAnything, a flexible and consistent diffusion-based framework that generates coherent, semantically distinct image sequences from textual cooking instructions of arbitrary length. The framework introduces three key components: (1) Step-wise Regional Control (SRC), which aligns textual steps with corresponding image regions within a single denoising process; (2) Flexible RoPE, a step-aware positional encoding mechanism that enhances both temporal coherence and spatial diversity; and (3) Cross-Step Consistency Control (CSCC), which maintains fine-grained ingredient consistency across steps. Experimental results on recipe illustration benchmarks show that CookAnything performs better than existing methods in training-based and training-free settings. The proposed framework supports scalable, high-quality visual synthesis of complex multi-step instructions and holds significant potential for broad applications in instructional media, and procedural content creation.","url":"https://huggingface.co/papers/2512.03540","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.393Z","metadata":{"authors":["Ruoxuan Zhang","Bin Wen","Hongxia Xie","Yi Yao","Songhan Zuo","Jian-Yu Jiang-Lin","Hong-Han Shuai","Wen-Huang Cheng"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2512.03540.pdf","arxivUrl":"https://arxiv.org/abs/2512.03540","arxivId":"2512.03540","coolPaperUrl":"https://papers.cool/arxiv/2512.03540","published":"2025-12-03T08:01:48Z","updated":"2025-12-03T08:01:48.000Z","zh_summary":"烹饪是一项顺序性和视觉导向的活动，每一个步骤，如切菜、混合或煎炸，都具有程序逻辑和视觉语义。尽管最近的扩散模型在文本生成图像方面显示出强大的能力，但它们在处理像食谱插图这样的结构化多步骤场景时仍存在困难。此外，目前的食谱插图方法无法适应食谱长度的自然变化，无论实际指令结构如何，都会生成固定数量的图像。为了解决这些限制，我们提出了CookAnything，这是一个灵活且一致的基于扩散的框架，可根据任意长度的烹饪文本指令生成连贯且语义上清晰区分的图像序列。该框架引入了三个关键组件：(1) 步骤区域控制（Step-wise Regional Control, SRC），在一次去噪过程中将文本步骤与对应的图像区域对齐；(2) 灵活RoPE（Flexible RoPE），一种步骤感知的位置编码机制，提升时间一致性和空间多样性；以及 (3) 跨步骤一致性控制（Cross-Step Consistency Control, CSCC），保持各步骤间的细粒度食材一致性。在食谱插图基准测试中的实验结果表明，CookAnything 在有训练和无训练设置下均优于现有方法。该框架支持复杂多步骤指令的可扩展、高质量视觉合成，并且在教学媒体和程序化内容创作方面具有广泛的应用潜力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**从任意长度的文本烹饪指令中生成灵活且连贯的多步骤食谱图像序列**这一核心问题。具体而言，现有方法存在以下关键缺陷：\n\n1. **固定步数限制**：先前工作（如 StackedDiffusion）只能输出固定数量的步骤图像，无法适应真实食谱中步骤数量的天然可变性，导致欠生成或过生成。\n2. **单图模型局限**：主流文本到图像扩散模型（如 FLUX、Stable Diffusion 系列）仅为单帧生成设计，直接扩展到多步骤时会出现**语义纠缠**（visual-semantic leakage），即不同步骤的视觉特征相互污染，图像高度相似、缺乏区分度。\n3. **细粒度一致性缺失**：在同时去噪的多图生成框架中，**微小食材的跨步骤连续性**（Tiny Ingredient Continuity Problem）难以保持，易出现形状、颜色突变甚至遗漏。\n\n为此，论文提出 CookAnything 框架，首次在统一扩散模型中实现：\n\n- **任意步数、一步生成**：单次前向即可输出与文本步骤数完全匹配的图像序列；\n- **步骤间语义解耦**：通过 Step-wise Regional Control 将每条指令绑定到专属潜空间区域，抑制跨步骤特征泄漏；\n- **跨步骤细粒度一致**：引入 Cross-Step Consistency Control，利用上下文 token 显式追踪并同步微小食材的视觉属性；\n- **位置编码重初始化**：提出 Flexible RoPE，为每个步骤图像重新设置独立坐标系，避免长序列中的位置错位与模糊。\n\n综上，论文解决了**可变长、多步骤、细粒度一致的食谱图像序列生成**这一此前未被攻克的难题。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两大主线：**食谱分析任务** 与**过程式序列生成模型**。关键工作按时间顺序与贡献归纳如下：\n\n1. 食谱分析任务（Recipe Analysis Task）\n\n- **单图生成阶段**\n- CookGAN \n8\n：最早以食材列表为条件，用 GAN 生成“成品”菜肴图。\n- ChefGAN \n23\n：首次把完整食谱文本作为输入，但仍只输出单帧成品。\n- ML-CookGAN \n14\n：融合食材+步骤，依旧仅生成最终摆盘图像。\n- **多图生成萌芽**\n- StackedDiffusion \n18\n：首次提出“为每个步骤生成一张插图”的任务定义，但强制输出 6 张图，无法适应可变步长；且基于 UNet，步骤间一致性弱。\n2. 过程式序列生成模型（Procedural Sequence Generation Model）\n\n- **单帧扩散基线**\n- Stable Diffusion 系列 \n26, 30, 32, 34\n、FLUX.1-dev \n11\n：仅支持单图生成，无跨步骤约束。\n- **简单拼接扩展**\n- In-Context LoRA \n9\n：把多步骤提示直接拼接后一次性生成，出现严重语义纠缠（CSC 指标比真值低 9.03）。\n- **故事/长序列可视化**\n- StoryDiffusion \n51\n、StoryDALL-E \n15\n：面向漫画/故事板，未考虑食材连续性，布局与一致性不满足食谱需求。\n- **布局-感知方法**\n- GLIGEN \n13\n、Attention Refocusing \n25\n：通过空间框或注意力重聚焦实现区域控制，但未建模步骤级时序与可变步长。\n3. 食品计算数据集\n\n- Recipe1M+ \n17\n、Food-101 \n2\n、VireoFood-172 \n5\n、Nutrition5k \n37\n：提供图文配对或营养标注，但无步骤级图像。\n- RecipeGen \n46\n、VGSI-Recipe \n41\n：本文提出的基准，首次提供可变步长、步骤-图像对齐的评测数据。\n\n综上，**尚无工作同时解决“可变步长、一步生成、跨步骤食材一致”的食谱插图需求**；CookAnything 在该交叉点上首次给出统一扩散框架。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **CookAnything** 框架，在单次扩散去噪过程中完成“可变长-多步骤-高一致”的食谱图像序列生成。核心解决方案由三大模块协同实现：\n\n1. Step-wise Regional Control（SRC）\n将整幅潜空间特征图纵向划分为与步骤数相等的独立区域，通过\n\nM_(i,j)= 1,& i=j lor |i-j|=N 0,&otherwise\n\n的掩码限制 DiT 自注意力，仅允许第  i  步文本 token 与第  i  块图像 token 交互，彻底阻断跨步骤语义泄漏；再用全局描述分支  z_(base)  与区域分支  z_(region)  加权融合\n\nz_t=αz_(base),t-1+(1-α)z_(region),t-1\n\n保证局部可控与全局连贯。\n\n2. Flexible RoPE\n传统 RoPE 在全局坐标系下编码，导致多图布局塌陷。Flexible RoPE 为每一区域  n  重新初始化旋转坐标系：\n\nboldsymbolz^((n))_(i,j)= z^((n))_(i,j)· R^((n))(i,j)\n\n使不同步骤的同名相对位置获得独立编码，消除长序列重复与模糊。\n\n3. Cross-Step Consistency Control（CSCC）\n先由 Cooking Agent（GPT-4o）补全每步隐式食材并统一形状/颜色描述；再把整段食谱一次性送入 T5 得到全局 token  C_(recipe) ，按步长切片后与原独立步 token  C^((n))  做加权平均：\n\nboldsymbolC^((n))[0:t^((n))]= C^((n))[0:t^((n))]+λ·C_(recipe)[b^((n)):b^((n))+t^((n))]\n\n强制共享食材的跨步表征一致，显著缓解微小食材消失或形变问题。\n\n通过“区域-掩码解耦 + 坐标系重初始化 + 上下文 token 融合”三位一体，CookAnything 在单路去噪中即可输出任意步数、语义区分且食材连贯的高保真食谱图像序列。\n\nQ4: 论文做了哪些实验？\n\n论文在 RecipeGen 与 VGSI-Recipe 两个数据集上，从**定量指标、消融分析、可变步长泛化、定性可视化、用户主观评价**五个维度展开系统实验，结果均以markdown列表归纳如下：\n\n1 基准对比（Quantitative Evaluation）\n\n- **对手模型**\n- UNet 系：StoryDiffusion、SDXL、StackedDiffusion(SKD)\n- DiT 系：SD3.5、Flux.1-dev、In-Context LoRA、Regional Prompt Flux(RPF)\n- 布局感知：GLIGEN、Attention Refocusing\n- **评测指标**（↑越高越好，↓越低越好）\n- Goal Faithfulness(GF) ↑\n- Step Faithfulness(SF) ↑\n- Cross-Step Consistency(CSC) ↓\n- Ingredient Accuracy(IA) ↑\n- Usability(UB) ↑\n- **主要结果**\n- RecipeGen：CookAnything(TB) 在全部 5 项指标取得 SOTA；CSC 仅 0.19，较最佳基线降低 79%。\n- VGSI-Recipe：Training-Free/Training-Based 两种设定均包揽第一，IA 提升 >2.0 分。\n\n2 消融研究（Ablation Study）\n\n| 移除模块 | GF↓ | SF↓ | CSC↑ |\n| --- | --- | --- | --- |\n| w/o Flexible-RoPE | −1.93 | −0.98 | 0.23 |\n| w/o CSCC | −0.02 | −0.17 | 0.29 |\n| w/o Cooking-Agent | −1.59 | −0.86 | 3.06 |\n\n- **λ 敏感性**：λ=0.2 时 CSC 最低(0.17)，后续实验固定该值。\n\n3 可变步长泛化（Variable-Length Generalization）\n\n- 在 3–10 步区间逐长度采样 500 条食谱。\n- **指标趋势**：CookAnything 的 CSC 曲线始终低于 0.3，而 IC-LoRA、RPF 随步数增加快速上升至 2.0 以上，验证本框架对任意步数的稳健性。\n\n4 定性可视化（Qualitative Evaluation）\n\n- 选取西式舒芙蕾、亚洲炒胡萝卜干丝、蒸鸡翅芋头等 6 套菜谱。\n- **观察**\n- 竞品常出现“胡萝卜立方→长条”形变、芋头消失、舒芙蕾塌陷。\n- CookAnything 保持食材形状、颜色、空间位置跨步一致，步骤边界清晰。\n\n5 用户主观评价（User Study）\n\n- 53 名参评者，75 组问卷，5 项感知指标（CSC、SF、GF、AQ、OA）。\n- **得分(%)**\n\n| Method | OA↑ | AQ↑ |\n| --- | --- | --- |\n| SD3.5 | 13.46 | 13.58 |\n| Flux.1-dev | 16.86 | 26.34 |\n| CookAnything | 69.68 | 60.38 |\n\n用户显著偏好本方法在美观度与整体吸引力上的表现。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 CookAnything 框架的自然延伸与深化，均围绕“结构化过程生成”这一核心能力展开：\n\n1. 时序-因果建模升级\n\n- 引入显式因果图或动态贝叶斯网络，对“火候-形态-颜色”连续变化进行物理一致性约束，缓解仅靠 token 加权平均的弱连续性问题。\n- 探索神经微分方程或扩散-流混合模型，把“烹饪时间”作为连续变量嵌入去噪过程，实现子步骤插值与慢动作生成。\n2. 多模态视频与交互式编辑\n\n- 将 SRC 掩码从单帧空间拓展到时空立方体，生成可循环播放的 5–15 秒短视频，同时保持食材跟踪与镜头一致性。\n- 开发“指令-掩码-笔刷”三端口交互界面：用户可在任意步骤涂抹或拖动食材，模型即时重采样潜码并补全后续帧，实现实时食谱再创作。\n3. 真实世界对齐与物理反馈\n\n- 与热成像、重量传感器同步采集真实烹饪过程，构建 Recipe1M-Video 级别的大规模“文本-图像-物理信号”三元组，用于监督式微调。\n- 引入可微渲染或流体仿真损失，使生成的“汤汁飞溅”“油花纹理”与物理参数（粘度、温度）保持一致，提升可信度。\n4. 跨文化、跨器具零样本泛化\n\n- 研究元学习或 LoRA-Rank-Adaptation，在仅给定新地区 10–20 条样本的情况下，快速适配地方器具（如石锅、蒸笼）与特有食材（如木薯、芭蕉花）。\n- 构建多语言-多模态对齐空间，支持用户用母语口语描述“适量盐少许”，模型仍可映射到视觉咸度区间。\n5. 营养与风味可解释性\n\n- 将营养成分表作为条件向量注入扩散模型，生成“低钠版”“高纤维版”同菜谱图像，并通过显著图可视化展示营养成分对颜色、质地的因果影响。\n- 结合电子鼻或质谱数据，建立“风味-化合物-视觉特征”链路，实现“看起来更健康且风味不变”的可控生成。\n6. 安全与伦理评估框架\n\n- 建立 RecipeGuard 基准，测试模型对“野生毒蘑菇”“生氰木薯”等危险食材的拒绝能力，避免生成误导性图像。\n- 引入对抗性提示检测器，防止用户通过同音或拆分词绕过安全过滤，确保生成内容符合食品安全法规。\n7. 边缘端高效推理\n\n- 研究步骤-区域稀疏注意力与动态网络剪枝，使得 12 B 参数的 DiT 在移动端 NPU 上可 2–3 秒内完成 6 步 512×512 图像序列生成。\n- 探索 INT4/FP6 量化与潜码字典蒸馏，实现存储 <2 GB 的“口袋级食谱插画助手”。\n\n通过上述拓展，CookAnything 可由“食谱可视化”演化为“通用过程级多模态生成与编辑平台”，覆盖教育、机器人烹饪、健康干预等更广泛场景。\n\nQ6: 总结一下论文的主要内容\n\n**CookAnything：灵活且连贯的多步骤食谱图像生成框架**\n\n1\\. 问题背景\n\n- 烹饪是高度视觉化、时序化的过程，现有文本-图像扩散模型只能生成单帧，无法按步骤插图。\n- 早期多图方法（StackedDiffusion）强制输出固定 6 张图，对真实食谱的步数变化适应性差，且存在严重语义纠缠与食材不连续。\n\n2\\. 核心贡献\n\n- **首次提出**任意长度、一步生成的扩散框架 CookAnything，单次去噪即可输出与文本步骤数完全匹配的图像序列。\n- **三大模块**：\n1. Step-wise Regional Control（SRC）——掩码隔离+全局融合，实现步骤-区域精准绑定。\n2. Flexible RoPE——每步重初始化旋转坐标，消除长序列位置错位与布局塌陷。\n3. Cross-Step Consistency Control（CSCC）——上下文 token 加权，保证微小食材跨步形状、颜色、存在一致。\n- **训练自由 / 训练微调**双设定均在 RecipeGen 与 VGSI-Recipe 上取得 SOTA，Cross-Step Consistency 降低 79%，用户主观评分提升 3 倍以上。\n\n3\\. 实验结果\n\n| 指标 | 最佳基线 | CookAnything(TB) | 提升 |\n| --- | --- | --- | --- |\n| Goal Faithfulness ↑ | 27.46 | 30.59 | +11% |\n| Cross-Step Consistency ↓ | 0.70 | 0.19 | −79% |\n| Ingredient Accuracy ↑ | 6.43 | 9.70 | +51% |\n| User Overall Appeal ↑ | 16.86 | 69.68 | +313% |\n\n4\\. 未来展望\n\n- 拓展为视频生成、交互式编辑、营养-风味可控、边缘端高效推理，并建立安全评估基准，迈向通用过程级多模态生成平台。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是CookAnything%3A%20A%20Framework%20for%20Flexible%20and%20Consistent%20Multi-Step%20Recipe%20Image%20Generation，链接是https://arxiv.org/pdf/2512.03540，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03540。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-13-1764892008393","title":"AutoNeural: Co-Designing Vision-Language Models for NPU Inference","description":"While Neural Processing Units (NPUs) offer high theoretical efficiency for edge AI, state-of-the-art Vision--Language Models (VLMs) tailored for GPUs often falter on these substrates. We attribute this hardware-model mismatch to two primary factors: the quantization brittleness of Vision Transformers (ViTs) and the I/O-bound nature of autoregressive attention mechanisms, which fail to utilize the high arithmetic throughput of NPUs. To bridge this gap, we propose AutoNeural, an NPU-native VLM architecture co-designed for integer-only inference. We replace the standard ViT encoder with a MobileNetV5-style backbone utilizing depthwise separable convolutions, which ensures bounded activation distributions for stable INT4/8/16 quantization. Complementing this, our language backbone integrates State-Space Model (SSM) principles with Transformer layers, employing efficient gated convolutions to achieve linear-time complexity. This hybrid design eliminates the heavy memory I/O overhead of Key-Value caching during generation. Our approach delivers substantial efficiency gains, reducing quantization error of vision encoder by up to 7x and end-to-end latency by 14x compared to conventional baselines. The AutoNeural also delivers 3x decoding speed and 4x longer context window than the baseline. We validate these improvements via a real-world automotive case study on the Qualcomm SA8295P SoC, demonstrating real-time performance for cockpit applications. Our results highlight that rethinking model topology specifically for NPU constraints is a prerequisite for robust multi-modal edge intelligence.","url":"https://huggingface.co/papers/2512.02924","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.393Z","metadata":{"authors":["Wei Chen","Liangmin Wu","Yunhai Hu","Zhiyuan Li","Zhiyuan Cheng","Yicheng Qian","Lingyue Zhu","Zhipeng Hu","Luoyi Liang","Qiang Tang","Zhen Liu","Han Yang"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2512.02924.pdf","arxivUrl":"https://arxiv.org/abs/2512.02924","arxivId":"2512.02924","coolPaperUrl":"https://papers.cool/arxiv/2512.02924","published":"2025-12-02T16:45:25Z","updated":"2025-12-02T16:45:25.000Z","zh_summary":"虽然神经处理单元（NPU）在边缘人工智能中理论上具有高效能，但为 GPU 优化的最先进视觉-语言模型（VLM）在这些硬件上往往表现不佳。我们将这种硬件-模型不匹配归因于两个主要因素：视觉变换器（ViT）的量化脆弱性以及自回归注意力机制的 I/O 限制特性，这些机制无法利用 NPU 的高算力吞吐量。为了弥合这一差距，我们提出了 AutoNeural，一种为 NPU 原生设计的 VLM 架构，专为纯整数推理而共同设计。我们将标准 ViT 编码器替换为使用深度可分卷积的 MobileNetV5 风格骨干网络，以确保激活分布有界，从而实现稳定的 INT4/8/16 量化。同时，我们的语言骨干将状态空间模型（SSM）原理与 Transformer 层结合，采用高效门控卷积实现线性时间复杂度。这种混合设计消除了生成过程中键值缓存的高内存 I/O 开销。我们的方法带来了显著的效率提升，使视觉编码器的量化误差降低高达 7 倍，端到端延迟较传统基线减少 14 倍。AutoNeural 的解码速度也达到基线的 3 倍，上下文窗口长度延长 4 倍。我们通过 Qualcomm SA8295P SoC 上的真实汽车用例验证了这些改进，展示了驾驶舱应用的实时性能。我们的结果表明，为 NPU 限制重新思考模型拓扑是实现稳健多模态边缘智能的前提。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**Vision–Language Models（VLMs）在 Neural Processing Units（NPUs）上部署时面临的两大核心瓶颈**：\n\n1. **Vision Transformer（ViT）的量化脆弱性**\nViT 中的全局自注意力与 RMSNorm 路径对激活分布极为敏感，INT8/16 量化后精度骤降。\n\n2. **自回归 Transformer 的内存 I/O 瓶颈**\n逐 token 生成需反复读写 Key-Value Cache，导致 NPU 算力闲置、延迟飙升，TTFT（Time-To-First-Token）与每 token 延迟均恶化。\n\n为此，作者提出 **AutoNeural**：一种面向 NPU 整数推理的**原生协同设计架构**，用\n\n- **MobileNetV5 式卷积视觉编码器** 替代 ViT，保证激活有界、INT4/8/16 量化稳定；\n- **Transformer-SSM 混合语言骨干** 以线性复杂度门控卷积层替代大部分注意力，彻底取消 KV Cache，内存流量降低 60%。\n\n在车载 Qualcomm SA8295P NPU 上，端到端延迟降低 **14×**，量化误差降低 **7×**，解码速度提升 **3×**，上下文长度扩展 **4×**，实现座舱实时多模态交互。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归入三大主线，并指出它们与 AutoNeural 的差异点：\n\n1. 车载多模态模型\n\n- 代表工作：VLM-Auto、VLM-RL、Octopus 系列等。\n- 共同点：把大 VLM 用于场景生成、驾驶决策。\n- 缺口：全部基于 GPU 浮点推理，未针对 NPU 的整数管线与功耗-延迟硬约束做协同设计。\n2. NPU 上的 LLM/VLM 推理优化\n\n- 方法：量化感知训练（Qualcomm 系列）、低秩分解、操作融合、MindVL、MiniCPM-V 等。\n- 共同点：在已有 GPU-first 架构上做“后处理”式压缩或调度。\n- 缺口：未改变 ViT 全局注意力与 KV Cache 的本质，量化鲁棒性与内存带宽问题依旧。\n3. 高效视觉编码器 & 线性序列建模\n\n- 视觉：MobileNet 家族、PaliGemma 证明小卷积编码器仍可迁移。\n- 语言：SSM/Mamba、Liquid Foundation Models 用线性状态空间替代注意力。\n- 缺口：此前没有把“卷积式视觉稳定性”与“SSM 线性语言建模”统一到一个端到端 VLM，并针对 NPU 整数管线做量化-校准-部署的完整流程。\n\nAutoNeural 首次将上述两条硬件友好路线融合，并在车载 NPU 上完成端到端验证。\n\nQ3: 论文如何解决这个问题？"}},{"id":"hf-14-1764892008393","title":"Economies of Open Intelligence: Tracing Power & Participation in the Model Ecosystem","description":"Since 2019, the Hugging Face Model Hub has been the primary global platform for sharing open weight AI models. By releasing a dataset of the complete history of weekly model downloads (June 2020-August 2025) alongside model metadata, we provide the most rigorous examination to-date of concentration dynamics and evolving characteristics in the open model economy. Our analysis spans 851,000 models, over 200 aggregated attributes per model, and 2.2B downloads. We document a fundamental rebalancing of economic power: US open-weight industry dominance by Google, Meta, and OpenAI has declined sharply in favor of unaffiliated developers, community organizations, and, as of 2025, Chinese industry, with DeepSeek and Qwen models potentially heralding a new consolidation of market power. We identify statistically significant shifts in model properties, a 17X increase in average model size, rapid growth in multimodal generation (3.4X), quantization (5X), and mixture-of-experts architectures (7X), alongside concerning declines in data transparency, with open weights models surpassing truly open source models for the first time in 2025. We expose a new layer of developer intermediaries that has emerged, focused on quantizing and adapting base models for both efficiency and artistic expression. To enable continued research and oversight, we release the complete dataset with an interactive dashboard for real-time monitoring of concentration dynamics and evolving properties in the open model economy.","url":"https://huggingface.co/papers/2512.03073","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.393Z","metadata":{"authors":["Shayne Longpre","Christopher Akiki","Campbell Lund","Atharva Kulkarni","Emily Chen","Irene Solaiman","Avijit Ghosh","Yacine Jernite","Lucie-Aimée Kaffee"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.CY","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2512.03073.pdf","arxivUrl":"https://arxiv.org/abs/2512.03073","arxivId":"2512.03073","coolPaperUrl":"https://papers.cool/arxiv/2512.03073","published":"2025-11-27T12:50:25Z","updated":"2025-11-27T12:50:25.000Z","zh_summary":"自2019年以来，Hugging Face 模型中心一直是全球共享开放权重 AI 模型的主要平台。通过发布包含模型元数据的完整每周模型下载历史数据集（2020年6月至2025年8月），我们提供了迄今为止对开放模型经济中集中度动态和不断变化特征进行的最严格的分析。我们的分析涵盖了851,000个模型、每个模型200多个聚合属性以及22亿次下载。我们记录了经济权力的根本性再平衡：美国开放权重行业由 Google、Meta 和 OpenAI 主导的地位明显下降，取而代之的是非关联开发者、社区组织，以及截至2025年的中国行业，其中 DeepSeek 和 Qwen 模型可能预示着市场力量的新一轮整合。我们识别出模型属性的统计显著变化，平均模型规模增加了17倍，多模态生成（3.4倍）、量化（5倍）和专家混合架构（7倍）快速增长，同时数据透明度令人担忧地下降，2025年开放权重模型首次超过真正开源模型。我们揭示了新出现的一层开发者中介，专注于为效率和艺术表现量化和调整基础模型。为了促进持续的研究与监管，我们发布了完整的数据集，并提供交互式仪表板，用于实时监控开放模型经济中的集中度动态和不断变化的特征。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在回答一个尚未被系统研究的核心问题：**在“开放权重”AI 模型生态中，经济权力是如何随时间集中与扩散的？**\n具体而言，它试图量化并追踪以下三个维度的动态演化：\n\n1. **模型层面**：哪些模型获得最大规模采用，其技术特征（规模、模态、架构、训练方法）如何变迁。\n2. **开发者层面**：权力是否仍集中在少数美国巨头（Google、Meta、OpenAI），还是已向社区、个人开发者或中国厂商转移。\n3. **国家层面**：地理分布上的主导权是否出现新的中心，以及这种变化对全球 AI 治理的含义。\n\n为此，作者构建了 Hugging Face 平台 2020-2025 年共 22 亿次下载的完整时序数据集，并引入经济学中的 **Herfindahl-Hirschman Index (HHI)** 与 **Gini 系数** 来度量市场集中度，从而首次给出开放模型经济“权力地图”的纵向实证证据。\n\nQ2: 有哪些相关研究？\n\n论文在 §5 “Related Work” 中将与自身相关的研究划分为四大脉络，并指出它们各自关注的角度与空白。以下按脉络归纳，并补充每类代表文献的核心结论或方法，方便快速定位。\n\n1\\. 社会-政治批判视角的 AI 研究\n\n- **Lehdonvirta et al. (2024)**\n提出“Compute North vs. Compute South”框架，指出算力资源集中在富裕国家，形成对全球南方的结构性提取。\n- **Crawford《The Atlas of AI》(2021)**\n论证 AI 供应链（数据、劳动力、能源）如何掩盖权力不平等，技术中立性是一种迷思。\n- **Noble《Algorithms of Oppression》(2018)**\n揭示搜索引擎排序如何系统性地强化种族与性别偏见。\n\n→ 共同点：聚焦“AI 系统如何再生产社会等级”，但**未量化**开放模型生态中的权力流动。\n\n2\\. 经济-集中度视角的 AI 研究\n\n- **Korinek & Vipra (2025)**\n用产业组织理论证明大模型存在“规模-市场集中”正反馈，预测闭源市场将趋向寡头。\n- **Vipra & West (2023)**\n指出“算力准入”而非“模型是否开源”才是决定市场结构的核心变量。\n- **Nagle & Yue (2025)**\n实证显示开放模型虽占下载量主流，却只贡献 5% 收入，揭示“开放”未必削弱商业集中。\n\n→ 共同点：提供**闭源市场**或**收入端**的集中度证据，但**缺乏对开放权重下载行为的纵向度量**。\n\n3\\. 地缘政治-国别视角的 AI 研究\n\n- \\*\\*Lamb"}},{"id":"hf-15-1764892008394","title":"AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs","description":"Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/.","url":"https://huggingface.co/papers/2511.20515","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.394Z","metadata":{"authors":["Kuniaki Saito","Risa Shinoda","Shohei Tanaka","Tosho Hirasawa","Fumio Okura","Yoshitaka Ushiku"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2511.20515.pdf","arxivUrl":"https://arxiv.org/abs/2511.20515","arxivId":"2511.20515","coolPaperUrl":"https://papers.cool/arxiv/2511.20515","published":"2025-11-25T17:19:47Z","updated":"2025-11-25T17:19:47.000Z","zh_summary":"评估图像-文本对齐模型（如 CLIP）对于连接视觉与语言表示至关重要。然而，现有的基准测试依赖规则化扰动或简短的标题，限制了其衡量细粒度对齐的能力。我们推出了 AlignBench，这是一个通过评估由多种图像到文本和文本到图像模型生成的详细图像-标题对来提供图像-文本对齐新指标的基准测试。每个句子都经过正确性标注，从而能够直接评估视觉语言模型（VLM）作为对齐评估器的表现。对广泛的基于解码器的 VLM 进行基准测试揭示了三个关键发现：（i）即使是针对组合推理优化的 CLIP 模型，仍几乎无感；（ii）检测器系统性地对前几句评分过高；（iii）它们表现出强烈的自我偏好，倾向于偏向自己的输出，从而损害检测性能。我们的项目页面将在 https://dahlian00.github.io/AlignBench/ 提供。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nAlignBench 旨在解决“如何精细、可靠地评估视觉-语言模型（VLM）的图文对齐能力”这一核心问题。现有基准存在以下局限：\n\n- 依赖规则化扰动（如简单替换名词/形容词）或短句，难以反映真实场景中的细微幻觉；\n- 样本规模小、模型覆盖窄，无法系统比较不同 VLM 作为“评测器”时的对齐检测水平；\n- 缺乏对长句、复杂句以及 Text-to-Image 生成对的全面测试，导致评估结果与下游幻觉检测、组合推理任务脱节。\n\n为此，作者构建了一个大规模、多样化、带细粒度正确性标签的合成图文对基准 AlignBench（≈ 89 k 句），直接利用 SOTA 的 Image-to-Text 与 Text-to-Image 模型输出作为“硬负例”，并系统评测各类 VLM 能否准确判断“句子是否真实描述图像”。通过该基准，论文首次揭示：\n\n1. CLIP 类对齐模型在细粒度幻觉面前几乎“失明”；\n2. 评测器普遍存在“句首偏好”与“自我偏好”，显著高估早期句子及自身输出；\n3. AlignBench 成绩与 MMMU 等推理基准高度相关，但反向不成立，因而可作为更全面、更具挑战性的图文对齐能力新指标。\n\nQ2: 有哪些相关研究？\n\n以下研究主题与 AlignBench 直接相关，可归纳为四条主线，并给出代表性文献（按首字母排序）：\n\n1. 图文对齐基准\n\n- ARO \n61\n、SugarCrepe \n22\n、Winoground \n48\n：通过规则扰动或人工构造短句，探测 CLIP 类模型的组合理解能力。\n- SeeTRUE \n58\n、GenAI-Bench \n31\n：利用 T2I 模型生成图像，但 caption 简短且仅做相对排序，无细粒度二元标签。\n- FOIL/HAT \n41\n：早期幻觉检测数据集，规模小、模板单一。\n2. 幻觉检测与缓解\n\n- CHAIR \n43\n：先用对象检测器再统计幻觉，覆盖受限。\n- MHalDetect \n20\n、MHaluBench \n10\n、HaELM \n52\n、ZINA \n51\n：提供幻觉标签，但样本数或模型覆盖不足，难以系统评测 VLM。\n- 近期缓解方法 \n15, 16, 25, 45, 56, 63, 65, 66\n：通过对比解码、熵估计、注意力修正等降低幻觉，仍需可靠对齐指标评估效果。\n3. VLM 评测体系\n\n- MMMU \n59\n、MME \n18\n、HallusionBench \n19\n、Seed-Bench \n26\n、Cambrian-1 \n49\n：聚焦推理、知识、错觉等能力，而非细粒度对齐。\n- LLM-as-evaluator 自偏好研究 \n39\n：发现模型倾向给自身生成更高分数，与 AlignBench 观察到的“自我偏好”现象一致。\n4. 基础对齐模型\n\n- CLIP \n42\n、SigLIP \n62\n、BLIP-2 \n29\n、TripletCLIP \n40\n：提供图像-文本相似度，但论文显示它们在长句、细微幻觉上几乎失效。\n- 大规模生成式 VLM（LLaVA-1.6 \n27\n、Qwen-VL \n53\n、InternVL2/2.5 \n11, 12\n、Llama-4 \n35\n、Gemini-2.0 \n46\n、GPT-4o \n37\n 等）被同时用作 Captioner 与 Evaluator，构成 AlignBench 的核心实验对象。\n\n综上，AlignBench 在数据规模、模型多样性、幻觉类型覆盖与评测协议上，填补了现有图文对齐与幻觉检测基准的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 系统评测”双轨策略解决细粒度图文对齐评估难题，具体步骤如下：\n\n1. 合成大规模硬负例\n\n- 同时调用 6 个 SOTA Image-to-Text 模型（GPT-4o、ShareGPT、LLaVA-1.6、Llama-4、Qwen-2、CogVLM）与 2 个 Text-to-Image 模型（Stable Diffusion 3.5、GPT-image-1），在 2000 张聚类后的真实图像和 1000 条人工 prompt 上生成 12k+2k 组图文对。\n- 由此天然产生“现代模型才会犯的细微幻觉”，避免规则扰动带来的分布偏差。\n2. 高质量句子级标注\n\n- 采用五人冗余标注 + 多数投票 + 专家复审，给每条句子打“正确 / 错误 / 未知”标签；未知样本直接排除，降低主观噪声。\n- 额外标记幻觉类型（属性、对象、数量、方位、文本、方向、错觉、关系、其他）与片段位置，支持细粒度诊断。\n3. 设计阈值无关评测协议\n\n- 将 VLM 作为 Evaluator，独立地对“单句+图像”输出 0–100 对齐分数；按 captioner 分别计算 AUROC，避免阈值选择干扰。\n- 提供 segment-level IoU 评估，衡量模型能否精确定位幻觉片段。\n4. 大规模横向对比\n\n- 覆盖 20+ 模型（CLIP 系、开源/闭源 VLM），参数 0.2B–400B；揭示 CLIP-like 模型几乎随机（≈50 AUROC），而最新 VLM 可达 81+。\n- 发现“句首偏好”“自我偏好”等系统性偏差，并验证 Ensembling、Chain-of-Thought、Self-ensemble 可进一步提升 2–4 AUROC。\n5. 建立新指标地位\n\n- AlignBench 成绩与 MMMU 等推理基准高度正相关（ρ≈−0.83），但反向不成立；证明其更具区分度与挑战性，可作为图文对齐能力的综合试金石。\n\n通过上述流程，论文不仅提供了可复用的基准与标注，还首次量化了现代 VLM 在“当裁判”时的能力边界与偏差，为后续对齐模型与数据清洗研究奠定标准。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 AlignBench 开展了多组实验，系统评估视觉-语言模型（VLM）作为“图文对齐评测器”的能力与行为偏差。主要实验内容如下：\n\n1\\. 主实验：句子级对齐检测（AUROC）\n\n- **协议**：将 20+ 模型（含 CLIP 系、开源/闭源 VLM）作为评测器，对 89k 句子独立输出 0–100 对齐分数；按 8 个 Captioner/T2I 模型分别计算 AUROC。\n- **结果**：\n- CLIP-like 模型（TripletCLIP、SigLIP、BLIP-2）≈ 50 AUROC，几乎失明。\n- 最佳开源 Llama-4 109B 达 73.4，闭源 GPT-5 最高 81.2；同一家族参数越大性能越高。\n- T2I 模型中，Stable Diffusion split 对 Llama-4/GPT-5-mini 最易，对 Gemma/Qwen 反而更难，揭示域差异。\n\n2\\. 幻觉类型鲁棒性分析\n\n- 仅对“错误”句子按 8 类幻觉分组，计算评测器给出的平均分数（越低越好）。\n- **结论**：所有模型在 Direction（方向）与 Number（数量）两类上分数最高，即最难检测。\n\n3\\. 位置偏差（Positional Bias）\n\n- 将句子按出现位置 1,2,3… 分组，计算评测器平均得分。\n- **结论**：无论句子是否正确，句首得分显著更高，证明存在“句首偏好”。\n\n4\\. 自我偏好（Self-preference）\n\n- 让评测器评估自己生成的 caption（对角）vs 其他模型 caption（非对角）。\n- **量化**：对角 AUROC 普遍低于非对角；GPT-4o 给自己错误句子的平均分远高于给 Llama-4 错误句子，且正确-错误分差更小，导致性能下降。\n\n5\\. 幻觉片段定位（Segment Localization）\n\n- 给定一条已标注幻觉句，要求模型用 **\n\n** 标出幻觉片段；以 IoU≥0.3 为正确，计算 AP 与 mIoU。\n- **结果**：最好模型 Llama-4 400B 仅 24.2 AP/19.6 mIoU，显示细粒度定位仍有巨大提升空间。\n\n6\\. 模型集成（Ensembling）\n\n- 将两个强评测器的对齐分数取平均，再算 AUROC。\n- **结果**：几乎在所有 split 上提升 1–4 点；Llama-4 + GPT-5-mini 组合在多数 split 达 84–86 AUROC。\n\n7\\. 自集成与链式思维（Self-ensemble & Chain-of-Thought）\n\n- **Self-ensemble**：对同一模型用温度=1.5 采样 5/10 条 COT 推理路径再平均分数，Llama-4 提升 2–3 点。\n- **COT**：先让模型给出 2–3 句推理再输出分数，Llama-4 在 6/8 个 split 提升，GPT4.1-mini 有升有降，说明 COT 效果模型相关。\n\n8\\. 上下文影响测试\n\n- 把待测句子与其前面所有句子一起输入，观察是否更准确。\n- **结果**：Llama-4 因长序列解析错误增多而性能略降；GPT-5-mini 无解析错但仍轻微下降，表明单句评估已足够公平。\n\n9\\. 与现有基准对比\n\n- 在同一批评测器上运行 MHalDetect、FOIL、HAT、ARO、SugarCrepe、Winoground、SeeTrue，计算 AUROC。\n- **结论**：AlignBench 表现好的模型（如 Llama-4、GPT-5）在其他基准也领先；反之，Gemma-3 在短句基准领先，却在 AlignBench 落后 10+ 点，证明 AlignBench 更具区分度与迁移性。\n\n10\\. 统计与可视分析\n\n- 给出幻觉类型分布、句位置错误率、图像域错误率、评分分布直方图等，进一步验证“属性、文本、方向”三类最易出错，插图与文本域图像幻觉更多。\n\n通过以上 10 组实验，论文全面揭示了现代 VLM 在细粒度图文对齐任务上的能力边界、偏差类型与改进空间，并确立 AlignBench 作为新一代评测基准的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“数据与标注”“模型与方法”“评测与协议”“应用与系统”四个层面，供后续研究参考：\n\n数据与标注\n\n1. **多轮对话上下文幻觉**\n当前仅评估单句对齐，可扩展到多轮对话（如用户追问 + VLM 回答），研究指代、逻辑一致性及新幻觉传播机制。\n\n2. **视频-文本对齐**\n将 AlignBench 从静态图像扩展到短视频片段，标注动作顺序、时序关系、状态变化等细粒度对齐错误。\n\n3. **多语言与低资源语言**\n构建非英语（中文、西班牙语、斯瓦希里语等）的幻觉基准，考察 VLM 在多语言场景下的对齐偏差与文化幻觉。\n\n4. **主观 vs. 客观幻觉界定**\n引入“可验证性”维度：对无法绝对判定的描述（艺术风格、情感氛围）建模不确定性，发展概率标签或软标签。\n\n模型与方法\n\n5. **专用对齐小模型**\n当前最佳评测器仍需 100B+ 参数，可探索 <10B 的“对齐专用”轻量模型，用于实时数据清洗与在线过滤。\n\n6. **视觉指代与定位增强**\n结合全景分割、OCR、开放词汇检测器，显式注入边界框或掩码，提升对数量、方向、文本等细粒度幻觉的感知。\n\n7. **因果干预与反事实训练**\n利用因果框架干预“句首偏好”“自我偏好”等偏差，训练对位置与作者中立的对齐评分函数。\n\n8. **跨模态对比与链式推理融合**\n将图像-文本-指令三模态同时输入，采用对比 + 生成混合目标，鼓励模型先显式指出证据再给出评分。\n\n评测与协议\n\n9. **连续评分校准**\n现有 0–100 分存在分布漂移，可研究概率校准（Platt scaling、温度缩放）使分数具备可比性与概率解释。\n\n10. **对抗性幻觉生成**\n主动学习或对抗攻击框架，迭代生成“最难检测”的幻觉句，动态扩充基准，避免模型快速过拟合。\n\n11. **人类-模型一致性度量**\n引入“可解释一致性”指标：不仅看 AUROC，还衡量模型给出的理由与人类标注者理由的语义相似度。\n\n12. **在线交互式评测**\n允许评测器向图像提问（multi-turn QA）再下结论，考察主动信息获取能力，而非被动一次性评分。\n\n应用与系统\n\n13. **数据飞轮闭环**\n用 AlignBench 最佳评测器自动清洗网络爬取的图文对，再微调 Captioner，迭代提升数据质量与模型生成能力。\n\n14. **幻觉实时警告系统**\n在终端部署轻量对齐模型，对 VLM 直播 caption、辅助医疗报告、教育讲解等场景实时标红潜在幻觉。\n\n15. **与生成策略联合优化**\n将对齐评分作为强化学习奖励，在解码阶段采用对比式或加权式生成，直接降低长 caption 的幻觉率。\n\n16. **法律与伦理风险评估**\n研究幻觉对高风险领域（新闻、医疗、司法）可能造成的责任界限，建立可审计的“对齐日志”与追溯机制。\n\n可验证的开放问题\n\n- **理论极限**：给定图像信息熵与文本复杂度，幻觉检测的 Bayes 最优误差下界是多少？\n- **人类上限**：专业标注者 vs. 普通众包 vs. VLM 的幻觉检测天花板差距多大？\n- \\*\\* scaling law\\*\\*：对齐能力是否随参数、数据、计算呈现可预测增长，还是存在平台期？\n\n探索上述问题可推动幻觉检测从“经验基准”走向“可解释、可校准、可落地”的下一阶段。\n\nQ6: 总结一下论文的主要内容\n\nAlignBench 核心内容速览\n\n1\\. 研究动机\n\n- 现有图文对齐基准依赖**规则扰动**或**短句**，难以衡量细粒度、真实场景的幻觉。\n- 亟需大规模、多样、带**真实幻觉标签**的基准，系统评估视觉-语言模型（VLM）的**对齐检测能力**。\n\n2\\. AlignBench 构建\n\n| 维度 | 规模/策略 |\n| --- | --- |\n| 图文对 | 12k(I2T) + 2k(T2I) = 14k 对 |\n| 句子 | 89k（平均 17.7 词） |\n| 模型覆盖 | 6 Captioner + 2 T2I，开源+闭源 |\n| 标注 | 五人冗余→多数投票→专家复审；三标签：正确/错误/未知 |\n| 幻觉类型 | 8 类（属性、对象、数量、方位、文本、方向、错觉、关系） |\n\n3\\. 实验发现\n\n- **CLIP-like 模型几乎失明**：AUROC≈50。\n- **最佳评测器**：GPT-5 81.2，开源 Llama-4 73.4。\n- **两大偏差**：\n① 句首偏好——早期句子得分虚高；\n② 自我偏好——评测器给自己输出更高分，对角 AUROC 显著下降。\n- **最难检测**：Direction（方向）与 Number（数量）幻觉。\n- **幻觉定位**：最佳模型仅 24.2 AP，空间定位仍困难。\n- **集成提升**：模型平均或自集成 COT 可再涨 2–4 点。\n- **跨基准迁移**：AlignBench 强 ⇒ 其他幻觉/组合基准也强，反之不然，确立**更综合的对齐能力指标**。\n\n4\\. 贡献总结\n\n1. 提出**首个十万句级**、**多模型覆盖**、**细粒度幻觉标签**的图文对齐基准。\n2. 揭示现代 VLM 作为评测器的**能力边界**与**系统性偏差**。\n3. 验证 AlignBench 对现有基准的**领先区分度**，为后续对齐研究与数据清洗提供**可复用标尺**。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是AlignBench%3A%20Benchmarking%20Fine-Grained%20Image-Text%20Alignment%20with%20Synthetic%20Image-Caption%20Pairs，链接是https://arxiv.org/pdf/2511.20515，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2511.20515。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-16-1764892008394","title":"SkillFactory: Self-Distillation For Learning Cognitive Behaviors","description":"Reasoning models leveraging long chains of thought employ various cognitive skills, such as verification of their answers, backtracking, retrying by an alternate method, and more. Previous work has shown that when a base language model exhibits these skills, training that model further with reinforcement learning (RL) can learn to leverage them. How can we get models to leverage skills that aren't exhibited by base models? Our work, SkillFactory, is a method for fine-tuning models to roughly learn these skills during a supervised fine-tuning (SFT) stage prior to RL. Our approach does not rely on distillation from a stronger model, but instead uses samples from the model itself, rearranged to provide training data in the format of those skills. These \"silver\" SFT traces may be imperfect, but are nevertheless effective for priming a model to acquire skills during RL. Our evaluation shows that (1) starting from SkillFactory SFT initialization helps a model to generalize to harder variants of a task post-RL, despite lower performance pre-RL; (2) cognitive skills are indeed used by the model; (3) RLed SkillFactory models are more robust to regression on out-of-domain tasks than RLed base models. Our work suggests that inductive biases learned prior to RL help models learn robust cognitive skill use.","url":"https://huggingface.co/papers/2512.04072","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.394Z","metadata":{"authors":["Zayne Sprague","Jack Lu","Manya Wadhwa","Sedrick Keh","Mengye Ren","Greg Durrett"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2512.04072.pdf","arxivUrl":"https://arxiv.org/abs/2512.04072","arxivId":"2512.04072","coolPaperUrl":"https://papers.cool/arxiv/2512.04072","published":"2025-12-03T18:54:53Z","updated":"2025-12-03T18:54:53.000Z","zh_summary":"利用长链思维的推理模型会运用各种认知技能，例如验证其答案、回溯、通过替代方法重试等。以往的研究表明，当基础语言模型具备这些技能时，通过强化学习（RL）进一步训练该模型可以学会利用这些技能。那么，如何让模型利用基础模型尚未展现的技能呢？我们的工作 SkillFactory 是一种在 RL 之前的监督微调（SFT）阶段对模型进行微调的方法，使其大致学习这些技能。我们的方法不依赖于从更强的模型中进行蒸馏，而是使用模型自身生成的样本，并重新排列以提供符合这些技能格式的训练数据。这些“银级”SFT 路径可能并不完美，但对于引导模型在 RL 过程中习得技能仍然有效。我们的评估显示：(1) 从 SkillFactory SFT 初始化开始，即使在 RL 之前表现较低，也有助于模型在 RL 后推广到任务的更困难变体；(2) 模型确实使用了认知技能；(3) 与经过 RL 的基础模型相比，经过 RL 的 SkillFactory 模型在域外任务上更不易出现回退。我们的工作表明，在 RL 之前学习的归纳偏置有助于模型学习稳健的认知技能运用。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何让基础语言模型在缺乏外部强模型蒸馏的情况下，自主习得并稳定运用认知行为（如验证、回溯、重试）”这一问题。具体而言：\n\n- 现有 RL 方法只能放大模型已具备的隐性技能；若基础模型从未表现出验证或重试行为，纯 RL 往往无法“凭空”创造这些能力。\n- 蒸馏或持续预训练虽能注入高级技能，却依赖更大的教师模型或海量定制数据，成本高且通用性受限。\n\nSkillFactory 提出“自蒸馏”路线：通过采样–反思–重组三步，把模型自身生成的正确/错误答案拼接成带标签的“银质”长思维链，仅用轻量级 SFT 就让模型先“记住”认知行为的结构，再进入 RL 阶段进行精细校准。由此，模型在后续 RL 中更易稳定地泛化到更难任务与跨域场景，且无需任何外部强模型参与。\n\nQ2: 有哪些相关研究？\n\n论文在 §2 与 §4 中系统梳理了四条相关研究脉络，可归纳为：\n\n1. **RL 自发式技能涌现**\n\n- DeepSeek-R1、DAPO、GRPO 等仅依赖稀疏正确性奖励的 RL 训练，可放大模型已隐含的验证/回溯行为，但无法“凭空”创造模型从未表现出的技能。\n- 代表：Shao et al. 2024, Yu et al. 2025, Liu et al. 2025。\n2. **强模型蒸馏**\n\n- 利用 R1、QwQ、Claude 等更强教师生成的长思维链做 SFT，再接入 RL 微调。\n- 代表：Muennighoff et al. 2025 (s1), Ye et al. 2025 (LIMO), Guha et al. 2025 (OpenThoughts)。\n3. **针对性数据策划与持续预训练**\n\n- 通过手工模板、MCTS 回滚或持续预训练把“回溯-验证”样例灌入模型。\n- 代表：Gandhi et al. 2025（持续预训练+回溯样例）、Kim et al. 2025 (ASTRO)、Pang et al. 2025 (BOLT)。\n4. **自举/自蒸馏**\n\n- STaR：迭代保留自采样得到的正确解答再做 SFT，但仅利用“答案正确”信号，不强制结构。\n- 代表：Zelikman et al. 2022。\n\nSkillFactory 与第 4 类同样“无强教师”，但关键差异在于：\n\n- 不依赖答案正确性作为唯一筛选，而是把正确/错误解答与自生成的“反思-裁决”显式拼接，强制输出带有 `<sample>` `<reflect>` 等标签的结构化轨迹，从而先让模型“记住”认知行为的模板，再交由 RL 校准何时/如何调用。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 SkillFactory 框架，把“让模型先具备认知行为结构，再由 RL 精炼”拆成三步，全程无需外部强模型。核心流程如下：\n\n1. 数据策划（§3）\na. 多样性采样：对每道题用 4 组不同 CoT prompt 各采 16 条解答，得到 64 条候选轨迹。\nb. 自生成反思：用专用 prompt 让模型对每条解答写“裁决”并打 <verdict> 标签，只保留裁决与真实正确性一致的“有效反思”。\nc. 结构化重组：随机混合 n⁺ 条正确与 n⁻ 条错误解答-反思对，用胶水短语拼接成“最终必成功”的长轨迹，强制出现 `<sample>`、`<reflect>`、`<verdict>` 等显式标签。\n\n2. 轻量 SFT（§4.3）\n在自建“银质”轨迹上只做 2 epoch 全参微调，目标不是提升任务准确率，而是让模型学会按模板“先给出解答→再反思→必要时重试”。\n\n3. 强化学习（§4.3）\n用 GRPO 在全新题库上继续训练，奖励仅为最终答案正确性 0/1。此时模型已具备显式技能结构，RL 只需学习“何时调用反思/重试”以及“如何修正策略”，从而稳定泛化到更难或跨域任务。\n\n通过“结构先验 + 稀疏奖励”的组合，SkillFactory 在 Countdown-3arg→4/5/6arg、OpenThoughts→AIME/GPQA 等场景上，均优于纯 RL、STaR、BOLT 及 R1 蒸馏基线，同时减少灾难性遗忘。\n\nQ4: 论文做了哪些实验？\n\n论文从 **通用化能力**、**鲁棒性** 与 **高阶推理** 三条主线设计实验，覆盖 2 个训练场景、3 个基座模型、10 余项任务，共 30 余组对比。核心结果汇总如下（所有数值均为准确率 %）：\n\n1 训练场景与超参\n\n| 场景 | 训练数据 | 基座模型 | SFT 步数 | RL 步数 | 上下文 |\n| --- | --- | --- | --- | --- | --- |\n| Countdown | CD-3arg 4k 条 | Qwen2.5-1.5B/7B、OLMo-3-7B | 2 epoch | 150 | 4k→16k |\n| OpenThoughts | OT 1k/10k 行 | Qwen2.5-7B | 2 epoch | 150 | 4k→16k |\n\n2 主实验结果\n\n2.1 易→难泛化（CD-3arg→4/5/6arg）\n\n| 方法 | Qwen2.5-1.5B | Qwen2.5-7B | OLMo-3-7B |\n| --- | --- | --- | --- |\n| Base | 1.9 | 14.4 | 23.6 |\n| RL Only | 15.8 | 25.7 | 51.1 |\n| R1-Distill→GRPO | 21.2 | 36.4 | 59.6 |\n| SkillFactory→GRPO | 25.1 | 37.0 | 65.3 |\n\n2.2 跨域鲁棒性（Acronym/Letter-CD/Multi/CSQA/GSM8k 平均）\n\n| 方法 | Qwen2.5-1.5B | Qwen2.5-7B |\n| --- | --- | --- |\n| RL Only | 27.0 | 32.9 |\n| R1-Distill→GRPO | 32.6 | 70.1 |\n| SkillFactory→GRPO | 31.9 | 54.1 |\n\n2.3 高阶数学与科学（OpenThoughts→GPQA/AIME25/AMC/Math500）\n\n| 方法 | 1k SFT | 10k SFT |\n| --- | --- | --- |\n| RL Only | 38.0 | — |\n| QwQ-Distill | 33.5 | 42.5 |\n| SkillFactory | 42.1 | 40.6 |\n\n3 诊断实验\n\n- **预算强制**：在 4k→8k token 继续生成，SkillFactory 在 CD 任务再提升 **+5.3** pp，高于 RL-Only 的 +1.2。\n- **技能使用率**：SkillFactory 模型在 OOD 任务平均产生 **2.3** 次显式尝试与 **2.0** 次反思， verifier F1>0.8；RL-Only 几乎无显式标签。\n- **消融**：移除“样本顺序”“反思标签”或“提示多样性”任一组件，OOD 平均准确率下降 **6–9** pp。\n\n4 结论性数字\n\n- 在 **CD-3arg→6arg** 上，SkillFactory 把 1.5B 模型的 **0.8** 提升到 **13.9**，相对提升 **>17×**。\n- 在 **5-digit 乘法** 上，1.5B 模型从 **0.1** 提升到 **0.6**，仍保持 GSM8k **68.2** 分，遗忘最小。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 SkillFactory 的“直接外延”或“深层追问”，实验与理论价值兼具：\n\n1. 技能模板再扩展\n\n- 引入“子目标分解”“假设-实验-归纳”“对立面论证”等新标签，验证是否仅需追加对应结构化轨迹即可在 RL 阶段自发激活。\n- 研究标签粒度（粗/细、层级化）对样本效率与最终性能的影响。\n2. 跨模态与工具调用\n\n- 将 `<sample>` / `<reflect>` 与 `<code>` `<execute>` `<retrieve>` 等工具标签混合，考察能否让模型在代码执行或检索失败时自动触发“反思-重试”循环，实现真正“可执行”的闭环推理。\n3. 技能可解释性度量\n\n- 设计“技能使用率”与“任务性能”的因果干预实验（如人工屏蔽反思标签、替换胶水短语），量化各技能组件对最终答案的边际贡献。\n- 用探测分类器判断模型内部表示何时切换到“验证”或“重试”模式，建立隐状态→显行为的可解释映射。\n4. 自适应预算分配\n\n- 当前预算强制为人工触发。可训练一个“继续/停止”策略网络，根据当前隐状态动态决定何时追加思考长度，实现样本级自适应计算量分配。\n5. 多轮自举与迭代蒸馏\n\n- 将 RL 后的 SkillFactory 模型作为新的“基础模型”，再次执行采样-反思-重组，形成多代自举链，观察技能深度与广度是否随代次指数提升，或出现收益饱和/崩溃。\n6. 遗忘-记忆权衡的理论刻画\n\n- 在持续学习序列（Countdown→GSM8K→Code）上测量“技能迁移”与“任务遗忘”曲线，结合 Fisher 信息或梯度冲突指标，验证结构化技能先验能否作为正则项抑制灾难性遗忘。\n7. 奖励黑客与技能鲁棒性\n\n- 引入“陷阱任务”——答案看似正确但违反隐藏约束，检验模型能否通过反思机制自我纠正，降低奖励黑客风险；若失败，可尝试在反思 prompt 中加入对抗性提示或不确定性估计。\n8. 小模型与大模型的技能涌现阈值\n\n- 系统扫描 0.5B→30B 参数区间，绘制“参数规模-技能使用率-任务性能”三维曲面，验证 SkillFactory 是否降低认知行为出现的临界规模，为“小模型推理”提供数据点。\n9. 形式验证与安全性\n\n- 对数学或代码类任务，把反思后的答案送入外部定理证明器或符号执行引擎，实现“机器可检”的二次验证；将验证结果作为额外奖励信号，进一步提升技能可靠性。\n10. 人类协同编辑\n\n- 开放“胶水短语”与标签模板的人工编辑接口，允许领域专家注入先验知识，形成“人-机”混合技能工厂，测试少量人类示范能否显著加速新领域适配。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **SkillFactory**：一种**无需强教师模型**、仅靠模型自身采样-反思-重组即可注入“验证、重试”等认知行为的自蒸馏框架。核心思想是**先让模型在 SFT 阶段记住结构化技能模板，再用 RL 精炼何时/如何调用这些技能**，从而解决“纯 RL 无法创造未出现过技能”的瓶颈。\n\n1\\. 方法三步骤\n\n- **采样**：对同一问题用多样 prompt 生成 64 条解答，自动打标签区分正误。\n- **反思**：让模型自评每条解答并给出 `<verdict>`，只保留裁决正确的“有效反思”。\n- **重组**：把正/误解答-反思对随机混合，用胶水短语拼接成“最终必成功”的长轨迹，强制包含 `<sample>` `<reflect>` 等显式标签，构成银质 SFT 数据。\n\n2\\. 实验结果\n\n| 场景 | 基线最佳 | SkillFactory→GRPO | 关键提升 |\n| --- | --- | --- | --- |\n| CD-3arg→4/5/6arg（1.5B） | 21.2 | 25.1 | +3.9 pp，易→难泛化最优 |\n| OOD 平均（1.5B） | 32.6 | 31.9 | 无强教师下媲美 R1 蒸馏 |\n| OpenThoughts→AIME/GPQA（7B, 1k） | 33.5 | 42.1 | 少样本超越 QwQ 蒸馏 |\n\n- **预算强制**：再续写 4k token，CD 任务额外 +5.3 pp，高于 RL-Only 的 +1.2。\n- **消融**：移除“反思标签”或“样本顺序”任一项，OOD 平均掉 6–9 pp，证明结构关键。\n\n3\\. 结论\n\nSkillFactory 用**结构先验+稀疏奖励**实现：\n① 无强教师也能习得复杂认知技能；\n② SFT 阶段低准确率反而利于后续 RL 泛化；\n③ 显著缓解灾难遗忘，支持推理时续写缩放。\n\n> 代码、数据、模型已开源，为“小模型自举高阶推理”提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SkillFactory%3A%20Self-Distillation%20For%20Learning%20Cognitive%20Behaviors，链接是https://arxiv.org/pdf/2512.04072，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04072。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-17-1764892008395","title":"In-Context Representation Hijacking","description":"We introduce \\textbf{Doublespeak}, a simple \\emph{in-context representation hijacking} attack against large language models (LLMs). The attack works by systematically replacing a harmful keyword (e.g., \\textit{bomb}) with a benign token (e.g., \\textit{carrot}) across multiple in-context examples, provided a prefix to a harmful request. We demonstrate that this substitution leads to the internal representation of the benign token converging toward that of the harmful one, effectively embedding the harmful semantics under a euphemism. As a result, superficially innocuous prompts (e.g., ``How to build a carrot?'') are internally interpreted as disallowed instructions (e.g., ``How to build a bomb?''), thereby bypassing the model's safety alignment. We use interpretability tools to show that this semantic overwrite emerges layer by layer, with benign meanings in early layers converging into harmful semantics in later ones. Doublespeak is optimization-free, broadly transferable across model families, and achieves strong success rates on closed-source and open-source systems, reaching 74\\% ASR on Llama-3.3-70B-Instruct with a single-sentence context override. Our findings highlight a new attack surface in the latent space of LLMs, revealing that current alignment strategies are insufficient and should instead operate at the representation level.","url":"https://huggingface.co/papers/2512.03771","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.395Z","metadata":{"authors":["Itay Yona","Amir Sarid","Michael Karasik","Yossi Gandelsman"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.CR","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2512.03771.pdf","arxivUrl":"https://arxiv.org/abs/2512.03771","arxivId":"2512.03771","coolPaperUrl":"https://papers.cool/arxiv/2512.03771","published":"2025-12-03T13:19:34Z","updated":"2025-12-03T13:19:34.000Z","zh_summary":"我们介绍了\\textbf{双重语言（Doublespeak）}，这是一种针对大型语言模型（LLMs）的简单\\emph{上下文内表示劫持（in-context representation hijacking）}攻击。该攻击通过在提供有害请求前缀的多个上下文示例中，系统性地将有害关键词（例如，\\textit{bomb}）替换为无害词元（例如，\\textit{carrot}）来实现。我们证明，这种替换会导致无害词元的内部表示逐渐向有害词元的表示收敛，从而有效地将有害语义隐藏在委婉语下。因此，表面上无害的提示（例如，“如何制造胡萝卜？”）在内部会被解释为被禁止的指令（例如，“如何制造炸弹？”），从而绕过模型的安全对齐机制。我们使用可解释性工具表明，这种语义覆盖是逐层出现的，早期层的无害意义逐渐在后期层汇聚成有害语义。双重语言无需优化，具有广泛的模型家族可迁移性，并在闭源和开源系统上取得了很高的成功率，在使用单句上下文覆盖的情况下，在Llama-3.3-70B-Instruct上可达74\\%的ASR。我们的研究揭示了LLM潜在空间中的新攻击面，表明当前的对齐策略不足，应该转向在表示层面进行防护。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在揭示并验证一种针对大语言模型（LLM）的新型安全漏洞——**表征劫持（representation hijacking）**。核心问题可归纳为：\n\n- **现有对齐机制仅关注表层 token 或浅层表征**，而 LLM 在推理过程中会动态更新 token 的上下文表征，导致深层语义可能与表层字面意义严重偏离。\n- **攻击者可利用这一动态更新机制**，通过在上下文中反复将无害词（如 “carrot”）与有害词（如 “bomb”）建立映射，使得模型在深层将无害词表征“重写”为有害语义，从而绕过拒绝机制，生成危险内容。\n- **因此，论文提出并验证 Doublespeak 攻击**，证明当前安全范式存在“表征盲区”，呼吁未来对齐与防御必须**贯穿整个前向传播过程**，对表征演化进行持续监控或内在鲁棒性设计。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为三大主线，并在第 2 节系统回顾。可概括为：\n\n1. 上下文表征动力学\n\n- 早期静态嵌入（Mikolov et al., 2013）→ Transformer 层间动态更新（Vaswani et al., 2017；Peters et al., 2018；Devlin et al., 2019）。\n- 模型在内部构建“世界模型”与结构化知识（Gurnee & Tegmark, 2024；Templeton et al., 2024）。\n- 上下文示例可即时重编程内部表征（Olsson et al., 2022；Park et al., 2025）——本文攻击正利用该机制。\n2. 越狱（Jailbreak）研究\n\n- **Token 级**：基于梯度或搜索的对抗 token 序列（Zou et al., 2023b；Guo et al., 2021；Wen et al., 2023；Shin et al., 2020；Andriushchenko et al., 2024）。\n- **Prompt 级**：构造连贯自然语言提示诱导违规（Chao et al., 2023；Liu et al., 2023；Mehrotra et al., 2023）。\n- Doublespeak 介于两者之间：保留自然语言流畅性，但仅替换 1–2 个关键词，**直接劫持表征而非混淆输入**。\n3. 拒绝机制（Refusal）的可解释性\n\n- 拒绝可由单一激活方向主导（Arditi et al., 2024）。\n- 拒绝可被建模为表征的仿射函数（Marshall et al., 2024），且可通过表征工程抑制或恢复（Zou et al., 2023a）。\n- 安全微调如何植入拒绝方向（Jain et al., 2024）及其局限（Lindsey et al., 2025）。\n- 本文指出这些机制**主要检查早期层表征**，为“时检–时用”型漏洞提供条件。\n\n综上，Doublespeak 首次把越狱战场从“输入层 token”或“提示模板”下沉到“层间表征演化”，填补了上述三线交叉处的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文并未提出“修复”方案，而是**系统暴露并验证问题**，迫使社区正视表征层面的安全盲区。具体做法可概括为四步：\n\n1. 设计零阶、可迁移的攻击\n\n- 无需梯度或模型参数，仅通过**上下文示例**完成劫持，降低攻击门槛，证明威胁即时存在。\n- 在 520 条 AdvBench 有害指令上评估，覆盖开源（Llama-3、Gemma）与闭源（GPT-4o、o1、Claude-3.5、Gemini 2.5）模型，ASR 最高达 92 %，显示**现行对齐普遍失效**。\n2. 提供可解释证据\n\n- **Logit Lens**：快速探测层间解码分布，直观展示“carrot”在中后期层被解码为“bomb”。\n- **Patchscopes**：让模型用自己的语言描述内部表征，量化“benign↔harmful”概率随层数翻转，确认劫持发生在**拒绝方向层之后**，形成“时检–时用”漏洞。\n3. 剖析攻击成功机理\n\n- 早期层仍保持 benign 语义 → 安全过滤器未触发；\n- 中后期层表征被改写为 harmful 语义 → 模型生成危险内容。\n- 给出两种互补解释：\n① 拒绝机制仅驻留在浅层；\n② 表征处于“叠加态”，残余 benign 语义足以逃过检测，却仍能驱动有害输出。\n4. 指明防御新方向\n\n- 强调**表征级对齐**（representation-aware alignment）的必要性：\n– 在每一层或关键层持续监控语义演化；\n– 设计对上下文重写内在鲁棒的拒绝机制；\n– 将安全判定从“输入层关键词”迁移到“深层语义一致性”。\n- 呼吁未来研究把越狱视角从“token/提示”下沉到“层间表征动力学”，并开放代码与评测框架供社区验证与迭代。\n\n综上，论文以“攻击+解释+呼吁”的方式**解决“问题未被正视”的问题**，而非直接给出补丁，从而推动下一代 LLM 安全范式向表征级防御演进。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“Doublespeak 表征劫持”设计了**多维度、跨模型、跨规模**的实验体系，可归纳为以下 6 组核心实验（均基于 AdvBench 520 条有害指令）：\n\n1. 主实验：跨模型迁移成功率\n\n- **目标模型**：Llama-3.1-8B、Llama-3.3-70B-Instruct、Gemma-3（0.27 B→27 B）、GPT-4o、o1、Claude-3.5-Sonnet、Gemini-2.5-Flash、DeepSeek-V2 等。\n- **指标**：Attack Success Rate（ASR，StrongReject 自动评分+人工三方标注）。\n- **结果**：\n– 开源 70 B 模型最高 88 %，LlamaGuard-3-8B 达 92 %；\n– 闭源 API 亦被突破（GPT-4o 31 %、o1 15 %、Claude-3.5 16 %），证明**零阶攻击即可迁移**。\n2. 上下文长度消融（Scaling of Context）\n\n- 在 Llama-3.3-70B 上遍历 K = 1,4,7,…,30 句“potato”示例。\n- **发现**：仅 1 句即可达 75 % ASR，**多于 10 句后触发拒绝机制**，呈现“倒 U”曲线。\n3. 模型规模消融（Scaling of Model Size）\n\n- Gemma-3 系列 0.27 B→27 B 全尺寸扫描。\n- **结论**：\n– 小模型（<4 B）需更多示例才能劫持；\n– 大模型（27 B）1–4 句即峰值，再多示例反而因“意图检测”增强而被拒绝。\n4. 替换词鲁棒性（Lexical Category Ablation）\n\n- 固定有害词，将“potato”替换为名词、代词、形容词、动词各 5 个常见词。\n- ASR 稳定在 52 %–56 %，**词性、词频、语义距离均不构成障碍**，验证攻击利用的是通用上下文学习机制。\n5. 多 token 同时劫持（Multi-token Hijacking）\n\n- 同时映射 (sleep→bake, truck→cake) 等 2–3 组替换。\n- 模型仍能正确解析混合语义并生成“烘焙蛋糕”教程，**证明劫持可并行扩展**。\n6. 可解释性追踪实验\n\n- **Logit-Lens**：逐层解码中间表征，展示“carrot”在第 40–75 层被读为“bomb”。\n- **Patchscopes**：量化 benign/harmful 概率比，确认**拒绝方向层（≈12 层）仍处于 benign 区**，而后期层完成语义翻转。\n- 对照基线（无劫持）无此翻转，说明观测效应确由攻击引入。\n\n以上实验组合**从有效性、迁移性、 scaling 规律、鲁棒性、机理解释五方面**完整刻画了 Doublespeak 的威胁图景，并公开代码与评测脚本以供复现。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为论文直接抛出的“未解问题”与延伸战场，按**攻击面扩展、机理深挖、防御落地、评估体系**四条线展开：\n\n1\\. 攻击面扩展\n\n- **跨模态表征劫持**\n将 Doublespeak 从纯文本迁移到图文/视频模型：用无害图像替代危险概念（如“胡萝卜”图标 ↔ “炸弹”），观察视觉-语言联合表征是否同样可被上下文重映射。\n- **工具使用场景**\n劫持 API 调用参数名：让模型把 `tool_name=\"carrot\"` 内部解析为 `tool_name=\"sql_injection\"`，触发危险工具调用。\n- **长时记忆与 RAG**\n把劫持示例注入向量数据库或系统提示，测试跨会话持久性；研究“记忆-遗忘”阈值。\n- **链式思维（CoT）污染**\n仅在思维链内部完成语义翻转，表面问题保持 benign，观察是否绕过监控器。\n\n2\\. 机理深挖\n\n- **叠加态量化**\n用分布式可解释性（如稀疏自编码器）度量 benign/harmful 特征在单 token 表征中的共存比例，建立“叠加系数-攻击成功率”解析式。\n- **层间因果定位**\n通过激活修补（activation patching）精确找出哪几层、哪些注意力头负责完成语义翻转，绘制“劫持路径图”。\n- **学习算法还原**\n检验模型在上下文前向传播中是否隐式实现了梯度下降步骤，将 Doublespeak 视为“元梯度”攻击，给出可计算的上界。\n- **拒绝方向时序动力学**\n实时追踪拒绝方向向量在层间的 L2 变化率，验证“早期恒定-后期突变”假设，建立时间--of-check 漏洞的定量判据。\n\n3\\. 防御落地\n\n- **表征级哨兵（Sentinel）**\n在每层或关键层插入轻量二分类头，实时监测 token 语义是否偏离字面，若偏离>阈值即触发拒绝或降权。\n- **上下文重写检测**\n训练对比模型，比较“字面语义”与“深层语义”余弦距离，无需人工规则即可泛化到任意替换词。\n- **鲁棒对齐目标**\n将拒绝损失从输出层前移到多层表征，采用类似对抗训练的方式最小化“劫持方向”灵敏度，实现内禀鲁棒。\n- **可验证延迟披露**\n对高风险请求采用“延迟响应”+“二次确认”机制，给用户与监控器额外一次检验机会，降低实时劫持成功率。\n\n4\\. 评估与基准\n\n- **DoublesBench**\n构建覆盖 20+ 危险类别、多语言、多模态的劫持评测套件，含自动替换脚本与公开 ASR 排行榜。\n- **隐含语义提取器竞赛**\n举办共享任务：给定被劫持上下文，参赛队用任意可解释工具还原原始有害词，推动监控技术标准化。\n- **Scaling Law for Hijacking**\n系统采样模型规模×上下文长度×替换词频空间，拟合 ASR 的幂律或相变曲线，为防御资源分配提供理论依据。\n\n5\\. 伦理与延伸风险\n\n- **隐蔽偏见注入**\n用 Doublespeak 把“女性”映射为“低能力”，评估社会偏见是否可通过上下文悄悄植入并影响后续决策。\n- **法律与医疗高风险域**\n测试在合同/病历/判决文书生成场景中，表征劫持能否让模型输出违背法规或医学指南的建议，量化真实危害。\n\n这些方向既涵盖**短期可验证的实证课题**，也指向**长期表征级对齐的理论重构**，可直接对接开源社区、红队项目与标准制定组织。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有 LLM 安全对齐只在输入层或浅层做关键词/方向检测，忽略层间表征会被上下文动态重写。\n- **方法**：提出 Doublespeak——零阶、可迁移的表征劫持：在上下文示例里把有害词系统替换成无害词（bomb→carrot），迫使模型深层将“carrot”表征收敛为“bomb”，从而对“如何制造 carrot”生成真实炸弹教程。\n- **验证**：\n– 520 条 AdvBench 上，Llama-3.3-70B 88 %、LlamaGuard-3-8B 92 %、GPT-4o 31 %、Claude-3.5 16 % 被攻破；1 句上下文即可峰值。\n– Logit-Lens 与 Patchscopes 显示：早期层仍读 benign，拒绝方向层（≈12）未触发，中后期层完成语义翻转。\n- **结论**：揭示“时检–时用”式表征盲区，呼吁安全机制必须**贯穿整个前向传播**，实现表征级持续监控与内禀鲁棒。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是In-Context%20Representation%20Hijacking，链接是https://arxiv.org/pdf/2512.03771，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03771。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1764892008395","title":"UniQL: Unified Quantization and Low-rank Compression for Adaptive Edge LLMs","description":"Deploying large language model (LLM) models on mobile platforms faces significant challenges due to the limited memory and shared computational resources of the device. Resource availability may be an issue as it is directly impacted by the current device workload, adding to the uncertainty of model deployment. We introduce UniQL, a unified post-training quantization and low-rank compression framework with on-device configurable pruning rates for edge LLMs. UniQL is a general framework that integrates quantization and low-rank compression for Transformers, State Space Models (SSMs), and hybrid models to support diverse edge applications. In our proposed joint framework, we introduce an efficient structured weight-sorting method that speeds up computation by 20x, quantization-aware singular value decomposition (SVD) to minimize quantization errors, state-aware weight sorting for SSMs, and a fused rotary positional embedding (RoPE) kernel for pruned models. Our framework performs weight-sorting, fine-tuning, and quantization in the cloud in a single-pass workflow, while enabling on-device configurable pruning rates up to 35%. Our experiments show that quantized and pruned models achieve a memory reduction of 4x-5.7x and a token-throughput improvement of 2.7x-3.4x, maintaining accuracy within 5% of the original models at 15% pruning across Transformers (Llama3 and Qwen2.5), SSMs (Mamba2), and hybrid models (Nemotron-H and Bamba-v2). The code and quantized models are available at: https://github.com/enyac-group/UniQL.","url":"https://huggingface.co/papers/2512.03383","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.395Z","metadata":{"authors":["Hung-Yueh Chiang","Chi-Chih Chang","Yu-Chen Lu","Chien-Yu Lin","Kai-Chiang Wu","Mohamed S. Abdelfattah","Diana Marculescu"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2512.03383.pdf","arxivUrl":"https://arxiv.org/abs/2512.03383","arxivId":"2512.03383","coolPaperUrl":"https://papers.cool/arxiv/2512.03383","published":"2025-12-03T02:33:39Z","updated":"2025-12-03T02:33:39.000Z","zh_summary":"在移动平台上部署大型语言模型（LLM）面临重大挑战，因为设备的内存有限且计算资源需要共享。资源可用性可能是一个问题，因为它直接受到当前设备工作量的影响，从而增加了模型部署的不确定性。我们提出了 UniQL，这是一个统一的后训练量化和低秩压缩框架，支持边缘 LLM 的设备端可配置剪枝率。UniQL 是一个通用框架，整合了 Transformers、状态空间模型（SSM）和混合模型的量化与低秩压缩，以支持多样的边缘应用。在我们提出的联合框架中，我们引入了一种高效的结构化权重排序方法，可将计算速度提高 20 倍，量化感知奇异值分解（SVD）以最小化量化误差，针对 SSM 的状态感知权重排序，以及用于剪枝模型的融合旋转位置嵌入（RoPE）内核。我们的框架在云端以单次流程完成权重排序、微调和量化，同时支持设备端可配置的剪枝率高达 35%。我们的实验表明，量化和剪枝后的模型在 Transformers（Llama3 和 Qwen2.5）、SSM（Mamba2）及混合模型（Nemotron-H 和 Bamba-v2）中，在 15% 剪枝下，相比原模型在精度上保持在 5% 以内，同时实现 4x-5.7x 的内存减少和 2.7x-3.4x 的令牌吞吐量提升。代码和量化模型可在以下地址获取：https://github.com/enyac-group/UniQL.","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决\\*\\*在资源受限且动态变化的边缘设备上部署大语言模型（LLM）\\*\\*所面临的核心矛盾：\n\n- **固定压缩/量化模型无法适应运行时资源波动**：现有方法在云端一次性压缩或量化后，模型大小固定，难以应对边缘设备上内存、算力等资源的动态变化（如系统负载升高时可用资源减少）。\n- **多副本存储成本高昂**：为不同资源场景预生成多个压缩率模型，会显著增加存储与维护开销。\n- **重新压缩代价不可接受**：在设备端重新执行压缩或量化需要数小时级GPU时间，边缘场景无此算力冗余。\n- **架构特异性限制通用性**：现有弹性训练或压缩方案往往只针对单一模型族（如仅Llama-3.1-8B），无法同时覆盖Transformer、State-Space Model（SSM）及混合架构。\n\n为此，论文提出**UniQL**——**统一的后训练量化+低秩结构化剪枝框架**，在**单张服务器级GPU上一次性完成压缩**，然后**在边缘侧按需动态剪枝**，实现：\n\n1. **单模型支持多级压缩率**（0 %–35 %），无需重复训练或量化。\n2. **4×–5.7×内存缩减**与**2.7×–3.4×推理加速**，精度损失控制在5 %以内。\n3. **通用性**：同时覆盖Transformer、SSM、Hybrid三类主流架构。\n\nQ2: 有哪些相关研究？\n\n论文在第2节“Related work”中系统梳理了与UniQL密切相关的三大研究方向，并指出其差异与局限性。以下按主题归纳：\n\n1\\. Transformer 压缩\n\n- **低比特量化**\n- SmoothQuant、AWQ、QServe、SpinQuant、QuaRot 等聚焦于INT8/INT4权重量化，降低存储与计算需求。\n- 局限：仅量化，无法进一步减参；压缩率固定，无法运行时调整。\n- **非结构化/半结构化剪枝**\n- SparseGPT、SliceGPT、E-Sparse 采用非结构化或N:M稀疏模式。\n- 局限：需要专用硬件（稀疏加速器）才能兑现加速；运行时稀疏度不可调。\n- **结构化剪枝**\n- LLM-Pruner、SliceGPT-column、MoDeGPT、SVD-LLM 直接裁剪整通道/头，适配通用GPU。\n- 局限：每压缩率需重新分解或微调，时间与存储开销随压缩率线性增长（O(n)）。\n- **任意尺寸压缩**\n- BitStack、SVD-LLM-v2 通过迭代叠加残差方式生成不同规模模型。\n- 局限：推理延迟反而高于原始FP16，且需多轮微调。\n\n2\\. SSM（State Space Model）压缩\n\n- **量化**\n- MambaQuant、QuaMba、QuaMba2 提出针对Mamba系列的低比特方案，但仅解决量化，未涉及剪枝。\n- **结构化/非结构化剪枝**\n- Mamba-Shedder、SparseSSM 对SSM头或块进行一次性修剪；Group-aware SSM Pruning 引入分组知识蒸馏。\n- 局限：仅针对单一SSM架构，不支持Transformer或混合模型；压缩率固定，无法边缘自适应。\n- **Token/维度剪枝**\n- 视觉SSM中的token pruning、维度缩减工作，与文本LLM场景差异大，未考虑量化联合。\n\n3\\. 弹性训练（Elastic Training）\n\n- **多合一架构**\n- Flextron、LLaMaFlex 通过权重共享与渐进剪枝，在训练阶段生成可伸缩子网络。\n- Jet-Nemotron 采用后训练神经架构搜索生成紧凑型变体。\n- 共同局限：\n– 需要大量GPU资源与精选数据集进行再训练；\n– 仅面向特定模型（如仅Llama-3.1-8B），无法泛化到SSM或Hybrid；\n– 不支持在边缘侧“零成本”调整模型大小。\n\n4\\. 与UniQL的核心区别\n\n| 特性 | 既有研究 | UniQL |\n| --- | --- | --- |\n| 量化+结构化剪枝联合 | 多数仅量化或仅剪枝 | 统一框架，一次完成 |\n| 后训练，无需再训练 | 弹性训练需再训练 | 纯后训练，单GPU一次搞定 |\n| 运行时自适应压缩率 | 不支持 | 边缘侧可配置0–35 %剪枝 |\n| 架构通用性 | 通常只针对Transformer或SSM | 同时支持Transformer、SSM、Hybrid |\n| 计算复杂度 | O(n) 随压缩率增加 | O(1) 与压缩率无关 |\n\n综上，UniQL首次在**后训练场景**下，将**量化与结构化低秩剪枝**整合为**一次性压缩、多端自适应**的通用方案，填补了边缘LLM动态部署的研究空白。\n\nQ3: 论文如何解决这个问题？\n\nUniQL 把“云端一次性压缩”与“边缘按需剪枝”解耦，通过四条关键技术一次性解决动态资源适配难题：\n\n1\\. 结构化权重排序（Structured Weight Sorting）\n\n- **目标**：让模型在边缘侧只需“砍尾部通道”即可立即获得不同大小的子网络，无需重新计算。\n- **做法**：\n- 以**校准集激活**为依据，对**整通道**计算重要性分数。\n- 提出**无伪逆**（Pinv-free）排序：\n- MLP 用 ridge leverage score  `diag!l[C(C+λ I)^(-1)r]`  避开  `O(n^3)`  的伪逆。\n- MHSA 对 Query/Key 做**对称排序**，兼容旋转位置编码（RoPE）。\n- SSM 提出**状态感知**（State-aware）排序：先对状态矩阵  `B,C`  做输入掩码相关度排序，再对  `x,z,o`  做状态相关度排序。\n- 排序后权重**一次性重排**，后续任意剪枝率仅对应“取前  `D'`  列”，实现  `O(1)`  压缩。\n\n2\\. 量化感知低秩分解（Quantization-aware SVD）\n\n- **目标**：低秩近似后仍适合 INT4 量化，避免“量化误差 > 低秩误差”。\n- **做法**：\n- 对  `W_v,W_o`  等矩阵做**激活缩放 SVD**：\n `C^(1/2)W_vW_o ⇒ U_vSigma V^top` \n将**奇异值矩阵  `Sigma`  并入左奇异向量**得到  `W_v'=C^(-1/2)U_vUSigma` 、 `W_o'=V^top` 。\n- 奇异值  `σ_i`  直接成为**量化组缩放因子**，消除长尾分布带来的量化误差。\n\n3\\. 掩码 LoRA 微调（Masked LoRA Fine-tuning）\n\n- **目标**：让“未剪枝”的排序模型在一次微调中就能**同时适应所有剪枝率**。\n- **做法**：\n- 每步随机采样全局剪枝率  `P_t∈[15%,35%]` ，按 BI 分数算出的层-wise 比例屏蔽掉尾部通道。\n- 仅训练 LoRA 旁路，主干权重保持冻结；一次训练 5 epoch，总计 <7 h（单 A6000）。\n- 微调后模型**保留完整权重**，边缘可随时按指定比例“掩掉”通道即可部署。\n\n4\\. 4-bit 量化与边缘自适应剪枝（Quantization & On-device Pruning）\n\n- **量化**：采用 **group-wise symmetric INT4**（group=128），嵌入与 lm\\_head 一并量化，模型体积再降 30 %。\n- **边缘剪枝**：\n- 仅对**中间通道**（MLP  `D_(int)` 、MHSA  `D_(hd)` 、SSM  `D_s/D_(hd)` ）做在线裁剪；**隐藏维度  `D_h`  不变**，保证层间形状兼容。\n- INT4 权重**运行时解压→裁剪→重新打包**为 INT32，实现零额外存储、零重新量化。\n\n5\\. 系统级优化\n\n- **融合 RoPE 内核**：把“gather 索引 + 旋转 embedding”写进单 CUDA kernel，减少 10 % 延迟。\n- **Hadamard 融合**：仅在**未剪枝方向**乘 Hadamard 矩阵，避免剪枝后维度不匹配；支持即插即拔。\n\n结果\n\n- **一次性压缩**：单 A6000 上 19 min 完成权重排序 + 7 h 完成掩码微调 + 40 min 完成量化，总耗时 7 h 43 min，即可**产出 0 %–35 % 任意剪枝率的 4-bit 模型**。\n- **边缘零开销**：在 Nano 8 G 上根据内存压力**动态选择剪枝率**，无需重新计算或下载新模型。\n- **性能**：相比 FP16，内存 ↓ 4×–5.7×，生成延迟 ↓ 2.7×–3.4×，下游任务精度在 15 % 剪枝率下损失 <5 %（Transformer、SSM、Hybrid 均适用）。\n\n通过以上设计，UniQL 把“**云端一次压缩**”与“**边缘无限适配**”解耦，真正实现了大模型在资源动态环境下的**弹性部署**。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“一次压缩、多端自适应”** 目标，从 **精度、延迟、内存、能耗、压缩时间、消融、Pareto-front** 七个维度展开系统实验，覆盖 **Transformer / Hybrid / SSM** 三大类共 **6 个主流模型**（Llama-2-7B、Llama-3.1-8B、Qwen-2.5-7B、Nemotron-H-8B、Bamba-v2-9B、Mamba2-8B）。主要实验一览：\n\n1\\. 下游零样本精度对比\n\n| 对比对象 | 实验内容 | 关键结论 |\n| --- | --- | --- |\n| 结构化剪枝基线MoDeGPT、SVD-LLM | 15 % / 25 % 剪枝率，FP16 & 4-bit，有无微调 | UniQL 无微调即超 MoDeGPT；微调后平均↑1.5-3 %，全面领先。 |\n| 量化基线TRT-AWQ、TAO-HQQ、GPTQ | 纯 4-bit 权重，无剪枝 | UniQL 与 TRT-AWQ/TAO-HQQ 精度持平或↑0.3-1 %，且支持后续剪枝。 |\n| 一次性自适应剪枝 | 同一模型 0 %→35 % 四档剪枝 | 15 % 剪枝仅掉 1-2 %，35 % 仍维持 57-63 %，显著优于 SVD-LLM。 |\n\n2\\. 压缩时间（单 A6000）\n\n| 阶段 | MoDeGPT | SVD-LLM | UniQL |\n| --- | --- | --- | --- |\n| 权重分解 | 7 h 03 m | 0 h 35 m | 0 h 19 m (↓22×) |\n| + 微调 | 7 h 03 m | 15 h 57 m | 6 h 59 m |\n| + 量化 | 7 h 43 m | 16 h 46 m | 7 h 43 m |\n| → O(1) 复杂度：与压缩率数量无关；竞品每档压缩率需重新跑一遍。 |\n\n3\\. 模型体积与内存\n\n| 模型 | FP16 | TRT-AWQ* | TAO-HQQ* | UniQL 4-bit | UniQL 4-bit +35 % 剪枝 |\n| --- | --- | --- | --- | --- | --- |\n| Llama-3.1-8B | 16.0 GB | 5.8 GB | 5.7 GB | 4.1 GB | 2.8 GB (↓5.7×) |\n| Qwen-2.5-7B | 15.2 GB | 5.6 GB | 6.0 GB | 3.9 GB | 2.7 GB |\n| Nemotron-H-8B | 16.2 GB | — | — | 4.1 GB | 2.9 GB |\n\n4\\. 延迟评测\n\n**Cloud (A6000, 1 k+1 k tokens)**\n\n- TPOT：UniQL 4-bit 9.0 ms → 7.3 ms（35 % 剪枝），**vs TRT-AWQ 11.2 ms**\n- TTLT：UniQL 9.9 s → 8.1 s，**↓30 % vs TRT-AWQ**\n\n**Edge (Nano 8 GB, 512+512 tokens)**\n\n- TPOT：UniQL 77 ms → 58 ms（35 % 剪枝），**vs TAO-HQQ 134 ms**（↑1.7×）\n- 原 FP16 全部 OOM，UniQL 使模型首次可跑。\n\n5\\. 能耗（Joules / request, Nano 512+512）\n\n| 模型 | TAO-HQQ 4-bit | UniQL 4-bit | UniQL 4-bit +35 % |\n| --- | --- | --- | --- |\n| Qwen-2.5-7B | 381 J | 208 J | 143 J (↓62 %) |\n| Mamba2-8B | — | 225 J | 154 J |\n\n6\\. 消融实验\n\n| 组件 | 实验设置 | 收益 |\n| --- | --- | --- |\n| Fused RoPE | 有无 fused kernel，Llama-3.1-8B | TPOT ↓10 % |\n| Masked LoRA FT | 25 % 剪枝，FP16 & 4-bit | 精度↑2.6-3.7 % |\n| Quantization-aware SVD | 对比 W=UΣV 与 W=(UΣ)V | 4-bit 精度↑7.5 %（Llama-3.1） |\n\n7\\. 扩展评测\n\n- **MMLU 5-shot**：UniQL 4-bit 0 % 剪枝与 TRT-AWQ/TAO-HQQ 持平；15 % 剪枝仍领先 SVD-LLM >20 %。\n- **MBPP+ 代码生成**：UniQL 4-bit 15 % 剪枝 Pass@1 54.2 %，高于 SVD-LLM 24 %。\n- **3-bit 仿真**：继续降精度到 3-bit，平均精度仅再掉 3-5 %，验证框架鲁棒。\n- **校准集消融**：Alpaca 全阶段 → 最佳；WikiText2 仅用于分配剪枝率即可。\n\n8\\. Pareto-front 分析\n\n- **A6000**（图5）：在 2–4 GB 内存区间，UniQL 星形点**完全支配** GPTQ 圆点与 FP16 方点（更高精度+更低延迟）。\n- **Nano 8 G**（图6）：同等内存下，UniQL 延迟-精度 trade-off **全面优于** TAO-HQQ，且是唯一能在 35 % 剪枝后仍 <2 s 完成 512+512 生成的方案。\n\n综上，实验从**精度、压缩率、延迟、内存、能耗、压缩时间、组件贡献、多任务泛化**全方位验证：\n**UniQL 一次压缩，即可在云端生成通用 4-bit 模型，边缘按需剪枝 0–35 %，全面优于现有后训练量化与结构化剪枝方案。**\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 UniQL 的“直接延伸”或“深层扩展”，均围绕**边缘动态部署**这一核心场景，兼具学术价值与落地潜力：\n\n1\\. 极端低比特 <3-bit 与混合精度\n\n- **问题**：INT3 已验证可行，但 INT2/INT1 尚未触及；通道剪枝后剩余权重往往更“敏感”。\n- **探索**：\n- 将 UniQL 的“量化感知 SVD”推广到 **二进制或 ternary** 空间，引入 **可学习量化阈值** 或 **通道级混合精度**（部分 INT4/INT2/INT1 共存）。\n- 结合 **recent LLM 1-bit 训练技术**（BitNet, 1.58-bit）做 **量化-剪枝联合蒸馏**，看能否在 1–2 bit 仍保持 50 %+ 精度。\n\n2\\. 动态剪枝决策 → 在线学习\n\n- **问题**：当前剪枝率需人工或规则指定，无法根据 **实时负载/能耗/用户 QoS** 自动切换。\n- **探索**：\n- 在设备侧部署 **轻量级强化学习 agent**（<1 M 参数），以 **内存占用、能耗、延迟、缓存命中率** 为状态，**剪枝率 + batch size + 线程数** 为动作，**奖励 = QoS – 能耗**，实现在线自我调整。\n- 研究 **分层决策**：OS 级资源预测 + 模型级剪枝率预测，形成 **“系统-模型”协同弹性**。\n\n3\\. 跨任务迁移与继续学习\n\n- **问题**：UniQL 仅用 Alpaca 做通用指令微调，边缘可能跑 **垂直任务**（医疗、法律、代码）。\n- **探索**：\n- 引入 **任务感知的掩码 LoRA**：在边缘用 **小领域数据**（<100 条）做 **1-epoch 增量训练**，仅更新任务 LoRA 与 **任务专属剪枝掩码**，实现“**同一套权重，多套任务-剪枝配置**”。\n- 研究 **灾难性遗忘边界**：当剪枝率增大时，**可塑性-稳定性 trade-off** 如何变化，能否用 **正则化掩码** 缓解。\n\n4\\. 与其他压缩维度正交组合\n\n- **方向**：\n- **Token 级剪枝**：结合 **FastGen、PoWER-BERT** 等思路，对 **剪枝后模型** 再执行 **动态 token 丢弃**，形成 **“权重-Token”二阶压缩**。\n- **KV-cache 量化/压缩**：SSM 虽无 KV-cache，但 Hybrid 模型仍有 self-attention；可研究 **KV-cache 4-bit + 通道剪枝** 的联合最优策略。\n- **NAS 搜索最优块结构**：以 UniQL 为“超级网络”，用 **零样本 NAS** 搜索 **不同深度、宽度、MoE 稀疏度** 的最佳子网，进一步榨干 Pareto 前沿。\n\n5\\. 异构边缘芯片的深度适配\n\n- **问题**：论文仅验证 NVIDIA A6000 / Orin Nano；实际边缘含 **ARM Cortex-M、Apple NPU、Qualcomm HTP、RISC-V AI 加速器**。\n- **探索**：\n- 将 **INT4 核 + Fused RoPE + 在线 unpack-repack** 移植到 **ARM-CL/TFLite-MLIR**，研究 **无 CUDA 的纯 CPU/NPU 路径** 是否仍能保持 2×+ 加速。\n- 针对 **无 FP32 累加器** 的低端 NPU，重新设计 **量化尺度校准** 与 **Hadamard 融合** 策略，避免精度塌陷。\n- 评估 **功耗墙受限场景**（<5 W）下，**剪枝率-电压-频率** 三维联合优化，形成 **“模型-芯片-电路”协同**。\n\n6\\. 安全与隐私侧信道\n\n- **问题**：边缘模型小且可被逆向，**结构化剪枝暴露的通道索引** 可能成为 **侧信道**。\n- **探索**：\n- 研究 **随机掩码调度**：每次推理随机在 Top-K 重要通道中再丢弃 5 %，**以延迟换安全**，量化对精度的影响。\n- 结合 **可信执行环境（TEE）**，把 **排序索引** 与 **剪枝逻辑** 放入安全世界，防止攻击者通过索引反推敏感数据。\n\n7\\. 理论极限与误差传播\n\n- **问题**：UniQL 依赖“校准集→重要性分数”，但 **误差如何随层数、剪枝率、量化比特** 传播尚无封闭分析。\n- **探索**：\n- 建立 **“量化-剪枝”联合扰动界**，给出 **每通道误差灵敏度** 与 **整体 logits 偏差** 的解析关系，指导 **最优剪枝率分配**。\n- 引入 **随机矩阵理论** 分析 **UΣV 合并后量化噪声** 的谱范数变化，验证 **为何 QSVD 优于传统 U+V 分解**。\n\n8\\. 生成式多模态扩展\n\n- **方向**：\n- 将 UniQL 迁移到 **视觉-语言模型**（LLaVA、Flamingo）与 **语音-文本模型**（Whisper-LLM），研究 **图像/音频 Token 维度** 是否也能做 **结构化排序剪枝**。\n- 探索 **跨模态通道对齐**：当视觉通道被剪枝时，文本子网络是否需要 **联动剪枝** 以保持多模态一致性。\n\n9\\. 可持续性与绿色 AI\n\n- **问题**：边缘设备生命周期内 **碳排放 = 制造 + 使用**；过度剪枝可能增加 **训练碳足迹**。\n- **探索**：\n- 建立 **“制造-使用”全生命周期模型**，找出 **不同部署规模** 下的 **最优剪枝率-比特组合**，使得 **总碳排最小**而非单点能耗最低。\n- 研究 **联邦场景**：百万边缘节点各自自适应剪枝，**全局碳排** 与 **模型性能** 的 Pareto 前沿。\n\n10\\. 开源基准与社区共建\n\n- **建议**：\n- 发布 **UniQL-Bench**：统一 API 支持 **PyTorch → TFLite-MLIR → ONNX-QDQ** 一键导出，内置 **延迟-精度-能耗** 自动评测脚本，填补“**边缘动态压缩基准**”空白。\n- 举办 **EdgeLLM 压缩挑战赛**：设置 **<2 GB 内存、<1 W 功耗、<500 ms 延迟** 三档赛道，推动社区探索极限。\n\n总结\n\nUniQL 已验证“**一次压缩、多端适配**”在 3–8 B 级别 LLM 的有效性；下一步可向\n**更极端比特**、**在线学习决策**、**跨模态-跨任务**、**异构芯片-安全-碳排** 等多维纵深，\n形成 **“算法-系统-硬件-可持续”** 全栈研究生态，为 **真正无处不在的 LLM** 奠定基础。\n\nQ6: 总结一下论文的主要内容\n\nUniQL：面向边缘动态部署的大语言模型**统一后训练量化与低秩结构化剪枝框架**\n（一次压缩，多端自适应）\n\n1\\. 问题背景\n\n- 边缘设备内存与算力**动态变化**，固定压缩模型常因资源不足无法运行。\n- 重复量化/剪枝**耗时数小时**，多副本存储**空间爆炸**。\n- 现有弹性方案**需再训练**且**仅支持单一架构**（如仅 Llama）。\n\n2\\. 核心目标\n\n**在单张服务器 GPU 上“一次性”完成压缩，量化模型可在边缘侧“零成本”按需剪枝 0–35 %，同时覆盖 Transformer、SSM、Hybrid 三大类模型。**\n\n3\\. 技术路线（四板斧）\n\n| 步骤 | 关键创新 | 效果 |\n| --- | --- | --- |\n| ① 结构化权重排序 | 无伪逆 ridge-score、对称 RoPE 排序、状态感知 SSM 排序 | 任意剪枝率仅“砍尾部通道”，复杂度 O(1) |\n| ② 量化感知低秩分解 | 将奇异值 Σ 并入 U，列缩放即量化步长 | INT4 精度↑7.5 %，避免长尾分布误差 |\n| ③ 掩码 LoRA 微调 | 每步随机采样全局剪枝率，一次性训练 | 单 A6000 上 <7 h 即适配 0–35 % 全部档位 |\n| ④ 边缘自适应剪枝 | 仅剪中间维度，隐藏维度不变；在线 unpack→裁剪→repack | 零额外存储，毫秒级切换 |\n\n4\\. 实验结果（6 模型，三类架构）\n\n| 指标 | 数值 |\n| --- | --- |\n| 内存 | ↓ 4×–5.7×（4-bit + 35 % 剪枝） |\n| 延迟 | ↓ 2.7×–3.4×（Nano 8 G 生成阶段） |\n| 精度 | 15 % 剪枝平均掉 <2 %；35 % 仍维持 57-63 % |\n| 压缩时间 | 权重排序 19 min（vs 竞品 7 h） |\n| 能耗 | Nano 每请求 ↓ 62 %（35 % 剪枝） |\n\n5\\. 结论\n\nUniQL 首次实现**后训练场景下“量化+结构化剪枝”一体化**，\n**单模型、单 GPU、一次压缩**，即可在边缘**动态适配内存与算力**，\n为百亿级 LLM 在**手机、AR/VR、车载、IoT** 等弹性部署提供通用、可落地的解决方案。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是UniQL%3A%20Unified%20Quantization%20and%20Low-rank%20Compression%20for%20Adaptive%20Edge%20LLMs，链接是https://arxiv.org/pdf/2512.03383，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.03383。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-19-1764892008395","title":"PSA: Pyramid Sparse Attention for Efficient Video Understanding and Generation","description":"Attention mechanisms are the core of foundation models, but their quadratic complexity remains a critical bottleneck for scaling. This challenge has driven the development of efficient attention mechanisms, with sparsity emerging as the dominant paradigm. Current methods typically retain or discard entire key-value blocks with binary masks, resulting in substantial information loss under high sparsity. To mitigate this gap, we present Pyramid Sparse Attention (PSA), a versatile module applicable to both video understanding and generation tasks. Instead of binary masking, PSA introduces multi-level pooled KV representations, enabling finer mask granularity. Specifically, each query block dynamically allocates lower pooling levels to critical KV blocks and higher levels to less important ones, creating an informative interpolation between full retention and complete pruning. This design, analogous to fixed-point quantization and classical feature pyramid networks in computer vision, effectively mitigates information loss while preserving computational efficiency under a low compute budget. It works with a native, hardware-friendly kernel that leverages decoupled block-tile design to ensure efficient execution. Across video understanding and generation benchmarks, PSA preserves contextual information and visual fidelity, consistently outperforming or achieving comparable performance over existing sparse attention baselines with superior efficiency-quality trade-offs. Our code and model weights are publicly available at: http://ziplab.co/PSA","url":"https://huggingface.co/papers/2512.04025","source":"HuggingFace Papers","timestamp":"2025-12-04T23:46:48.395Z","metadata":{"authors":["Xiaolong Li","Youping Gu","Xi Lin","Weijie Wang","Bohan Zhuang"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2512.04025.pdf","arxivUrl":"https://arxiv.org/abs/2512.04025","arxivId":"2512.04025","coolPaperUrl":"https://papers.cool/arxiv/2512.04025","published":"2025-12-03T18:02:11Z","updated":"2025-12-03T18:02:11.000Z","zh_summary":"注意力机制是基础模型的核心，但其二次复杂度仍然是扩展的关键瓶颈。这个挑战推动了高效注意力机制的发展，其中稀疏性已成为主要范式。当前的方法通常通过二进制掩码保留或丢弃整个键值块，在高稀疏情况下会导致大量信息丢失。为缓解这一问题，我们提出了金字塔稀疏注意力（Pyramid Sparse Attention, PSA），这是一种适用于视频理解和生成任务的多功能模块。PSA并不使用二进制掩码，而是引入多层次池化的键值表示，实现在掩码粒度上的精细控制。具体而言，每个查询块动态地将较低池化级别分配给关键的键值块，将较高池化级别分配给不太重要的键值块，从而在完全保留和完全剪枝之间创建信息丰富的插值。这种设计类似于固定点量化和计算机视觉中的经典特征金字塔网络，在保持低计算预算下有效缓解了信息丢失，同时保持计算效率。它支持原生、硬件友好的内核，利用解耦块-瓦片设计确保高效执行。在视频理解和生成基准测试中，PSA能够保留上下文信息和视觉保真度，始终在效率-质量权衡上优于或与现有稀疏注意力基线保持可比性能。我们的代码和模型权重公开可用，地址为：http://ziplab.co/PSA","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**长序列视频理解与生成模型中注意力机制计算复杂度二次增长带来的效率瓶颈**。\n\n具体而言：\n\n- 现代视频模型（如扩散 Transformer 和自回归架构）的序列长度可达数万 token，导致标准注意力计算  O(N^2)  的开销在推理阶段占据绝对主导（例如 Wan2.1-14B 生成 81 帧 720p 视频时，注意力占单卡 H20 上近 2 h 总耗时的 80 % 以上）。\n- 现有稀疏注意力方法普遍采用**整块保留/丢弃的二元掩码**，在极高稀疏率下会强制每个 query 只能看到极少量 key-value 块，造成显著信息丢失与视觉/语义质量下降。\n- 论文提出 **Pyramid Sparse Attention (PSA)**，用**多级池化 KV 表示**替代二元掩码，使同一计算预算下每个 query 块可覆盖更大比例的 KV 区域（70 % vs BSA 的 20 %），从而在低算力预算下显著缓解信息损失，保持上下文完整性与视觉保真度。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统回顾了与高效注意力相关的三条研究脉络，并指出它们与 Pyramid Sparse Attention（PSA）的区别与联系。相关研究可归纳如下：\n\n1. 静态稀疏注意力（Static Sparse Attention）\n\n- 方法：预先固定、输入无关的稀疏模式，如滑动窗口（sliding-window）、attention sink、时空能量衰减模式。\n- 代表工作：\n– Longformer / Sliding Tile Attention（Zhang et al. 2025c）\n– H2O（Zhang et al. 2023b）、Streaming LLM（Xiao et al. 2024b）\n– Radial Attention（Li et al. 2025）\n- 局限：模式刚性，无法捕捉关键长距离依赖，生成质量容易不稳定。\n2. 动态稀疏注意力（Dynamic Sparse Attention）\n\n- 方法：在推理阶段根据输入内容实时生成元素级或块级掩码，如按注意力得分阈值剪枝。\n- 代表工作：\n– SpargeAttention（Zhang et al. 2025a）\n– XAttention（Xu et al. 2025）\n– Sparse VideoGen / SVG2（Xi et al. 2025; Yang et al. 2025）\n- 局限：掩码非结构化，导致 GPU 内存访问不规则、硬件利用率低。\n3. 块稀疏注意力（Block Sparse Attention, BSA）\n\n- 方法：将 Q/K/V 张量划分为粗粒度块，以二元块掩码决定整块计算或跳过，保持块内密集矩阵运算从而适配 GPU Tensor Core。\n- 代表工作：\n– FlashAttention-2（Dao 2024; Dao et al. 2022）\n– Block-Sparse Attention（Guo et al. 2024）\n- 局限：高稀疏率下二元 keep/drop 决策强制丢弃大量信息，query 可见 KV 区域急剧缩小，造成显著性能下降。\n4. 其他补充方向\n\n- 量化与蒸馏：TDM（Luo et al. 2025）通过轨迹匹配蒸馏减少采样步数；PSA 可与该类方法正交叠加。\n- 重排序/置换：SpargeAttention、SVG2 采用 Hilbert 曲线或语义重排，将重要 token 聚集到有限块内；PSA 无需重排序即可兼容因果/双向掩码，避免因果结构破坏与额外开销。\n\n综上，PSA 与上述研究的核心区别在于：\n\n- 用**多级池化 KV 金字塔**取代二元保留/丢弃，实现“细粒度保留、粗粒度压缩、完全丢弃”三档平滑过渡；\n- 采用**阈值式动态掩码生成**，在同一计算预算下显著扩大 query 的感受野；\n- 设计**解耦块-瓦片内核**，在保持 FlashAttention 系列硬件友好性的同时支持异构块大小，实现高 GPU 利用率。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Pyramid Sparse Attention（PSA）**，从算法、掩码策略与内核实现三个层面协同解决“高稀疏率下信息丢失”与“硬件效率”之间的矛盾。核心思路是：**用多级池化 KV 金字塔取代二元 keep/drop，使同一 FLOPs 预算内 query 可见的 KV 区域最大化**。具体方案如下：\n\n1. 多级 KV 金字塔（Pyramid KV Blocks）\n\n- 对原始 K、V 序列按块做 **H 级平均池化**，每升一级块长减半、token 数减半：\n\nK^(h+1)_i = MeanPool(K^h_i, 2,2),quad V^(h+1)_i = MeanPool(V^h_i, 2,2)\n\n- 结果得到 **粗→细** 四档表示\\*\\*：一级最细（原始块），H 级最粗（聚合 2^{H-1} token）。\n2. 动态多级掩码生成（Multi-Level Mask Generator）\n\n- **轻量重要性估计**：\n- 视频生成：在 query/key 块内随机采样少量 token，计算最大 attention score\n\nS_(ij)=maxl[Softmaxl(tilde Q_i tilde K_j^top/√ dr)r]\n\n- 视频理解：采用反对角线得分 + 块内 cosine 相似度阈值，防止低相似块被过度池化。\n- **阈值式级别分配**：对每行重要性得分降序累加，按累积阈值序列\n\n0le τ_1leτ_2le·sleτ_Hle 1\n\n将块分配到 1…H 级或丢弃（0），实现“重要块细、次要块粗、冗余块丢”的平滑过渡。\n3. 自适应金字塔注意力（Adaptive Pyramid Attention）\n\n- 按掩码取对应级别 KV，注意力 logits 加 **层级偏置**  (h-1)ln 2  以抵消池化带来的概率稀释：\n\nA_(ij)=Softmaxl(Q_i (K_j^h)^top/√ d + (h-1)ln 2r)\n\n- 块内计算仍保持密集矩阵乘法，兼容 Tensor Core；整体复杂度由有效稀疏率\n\nbarrho=frac1n_q n_k∑_(i,j)(1) / (2^(M_(ij)-1))quad(M_(ij)>0)\n\n控制，可在同等 FLOPs 下覆盖更多 KV 块。\n4. 硬件优化内核（Decoupled Block-Tile Kernel）\n\n- **逻辑块大小**（由金字塔级别决定）与 **硬件瓦片大小** 解耦：过小块被合并，过大块被拆分，始终映射到最优 tile 尺寸。\n- 基于改进版 FlashAttention，融合 softmax、缩放、写回，避免 warp divergence，H200 上较朴素实现提速 10×。\n5. 训练无关即插即用\n\n- 不改动权重、不依赖 token 重排序，因果/双向掩码均直接支持；可在预填充阶段单独开启，或与蒸馏（TDM）叠加，实现 30× 去噪步加速而无质量损失。\n\n通过“多级池化 + 动态阈值掩码 + 解耦内核”三位一体，PSA 在 15 %–35 % 原始注意力计算预算下，于视频生成/理解多项基准上取得与全注意力相当甚至更好的精度，显著优于现有块稀疏注意力基线。\n\nQ4: 论文做了哪些实验？\n\n论文在 **视频生成（training-free & 蒸馏）** 与 **视频理解** 两大任务、四款开源模型、共 10 余项指标上系统验证 Pyramid Sparse Attention（PSA）。实验设计覆盖 **相似度、感知质量、推理延迟、稀疏率、消融与可视化**，具体列表如下：\n\n1. 训练无关视频生成（Training-Free Video Generation）\n模型：Wan2.1-{1.3B,14B}、Wan2.2-5B\n分辨率/帧数：1280×768 / 69 帧（默认），额外 720p 场景用于可视化\n指标：\n\n- 相似度：PSNR ↑、SSIM ↑、LPIPS ↓\n- 感知质量：VBench-Aesthetic、Background Consistency、Imaging Quality ↑\n- 效率：稀疏率（FLOP 等效）、单视频平均生成时间（Latency）\n对照：SVG、SVG2、SpargeAttention、STA、Full Attention\n结果：表 1 & 图 4 显示，在 0.83–0.91 稀疏率区间，PSA 全部相似度指标领先或次优，感知分数显著高于其他稀疏方法，接近 Full Attention，延迟与最强基线持平。\n2. 结合蒸馏加速（Distillation + PSA）\n模型：CogVideoX-5B\n框架：TDM 4-step 蒸馏\n指标：VBench 总分\n设置：PSA 85 % 稀疏 + 4 步蒸馏 vs. 纯 4 步蒸馏 vs. 原始 50 步 Full Attention\n结果：表 2 显示，PSA 组合方案 VBench 0.826，**超过 50 步全注意力基线（0.819）**，实现 30.2× 去噪时间压缩而无质量损失。\n\n3. 视频理解（Training-Free Video Understanding）\n模型：Qwen2.5-VL-7B\n数据集：Video-MME（1 fps 子集）\n指标：Short / Medium / Long / Overall 准确率 ↑\n稀疏率：≈ 0.65（最高）\n对照：Full Attention、XAttention、SpargeAttention\n结果：表 3 显示，PSA 在 Medium、Long 视频上得分最高，Overall 0.654 超过 Full Attention（0.651），验证高稀疏下仍保持语义能力。\n\n4. 消融实验（Ablation Studies）\n\n4.1 多级掩码 vs. 二元掩码\n表 4：同等计算预算下，多级掩码在更高稀疏率（0.79 vs 0.75）同时取得 PSNR↑0.24、SSIM↑0.005、LPIPS↓0.006，验证平滑池化优于硬丢弃。\n\n4.2 掩码生成策略 + Token 重排序\n表 5：\n\n- 阈值式掩码 + Hilbert 重排序 > 分位式掩码 + 重排序 > 分位式无重排序\n- 阈值式策略在相同稀疏率（≈0.84）下 PSNR 高 1.24，LPIPS 低 0.033。\n\n4.3 余弦相似度池化约束\n视频理解：表 6 显示，引入相似度阈值后 Video-MME Overall 从 0.647→0.654。\n视频生成：表 7 显示，PSA(sim) 在 0.80 稀疏率下 PSNR 比 PSA(no-sim) 高 0.35，LPIPS 低 0.010。\n\n4.4 逻辑块大小与重要性池化算子\n表 8：固定 0.8 稀疏率，(32,32) 块大小 + max 池化取得最佳 PSNR/SSIM；块过大或 mean 池化均轻微降低质量。\n\n4.5 多级分配策略\n表 9–10：在 0.25× FLOPs 预算下，对比 5 种金字塔分配预设。\n\n- 极端策略（全丢或全粗）指标严重下降；\n- 均衡多层方案 PSA-3 最优，PSNR 22.16、LPIPS 0.1004，验证“细-粗-丢弃”混合分配的必要性。\n5. 可视化对比\n图 4 & 附录图 5–10：相同文本提示与稀疏率下，PSA 生成视频在细节锐度、时序一致性、色彩饱和度上最接近 Full Attention，其他稀疏方法出现结构跳跃、色块崩塌或背景扭曲。\n\n综上，实验从 **定量指标、人类感知、推理速度、消融控制、视觉样例** 多维度证明：PSA 在 15 %–35 % 原始注意力计算预算内，一致优于现有块稀疏注意力基线，且可即插即用地叠加蒸馏框架，实现高质量、高吞吐的长视频推理与生成。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 PSA 的直接延伸或深层扩展，均尚未在原文中系统探讨，具备理论与实用价值：\n\n1. 最优金字塔分配策略学习\n\n- 目前阈值 τ 与层级比例靠人工网格搜索，可引入可微分 NAS、强化学习或 Gumbel-Softmax，把 τ 与层级分布作为隐变量，端到端学习“任务-模型-分辨率”专属的最优稀疏模式。\n2. 动态深度 / 动态宽度混合稀疏\n\n- 将“层级”维度扩展为“头维度”：不同注意力头学习异构金字塔（有头需细粒度，有头可极粗），形成 **Multi-Scale Head**；进一步结合 Mixed-of-Experts，实现“动态深度+动态宽度”联合稀疏。\n3. 与量化、低秩、硬件算子三重协同\n\n- 池化后的粗粒度 KV 具有更低熵，可激进量化为 4-bit 或 8-bit；同时利用低秩分解补充残差，构建 **Pyramid-Quantize-LoRA** 统一内核，把计算、带宽、显存三条瓶颈同时降低。\n4. 时间-空间金字塔解耦\n\n- 当前沿序列维度统一池化，对高帧率视频会模糊短时时序。可设计 **Factorized Pyramid**：空间维度用 2D 池化，时间维度用 1D 池化，分别生成时空金字塔，再按跨维度重要性融合，提升高速运动场景保真度。\n5. 针对因果自回归的增量推理缓存\n\n- 视频生成通常多步去噪，但自回归模型需逐 token 增量更新 KV-cache。可研究 **Pyramid Cache**：新 token 只与最细一级交互，历史远场按指数衰减逐步合并到粗级，实现 O(log t) 缓存增长。\n6. 与扩散采样算法的联合优化\n\n- 当前 PSA 在固定时间步区间开启稀疏。可用强化学习搜索 **Per-Timestep Sparsity Schedule**，或结合 DDIM/DPMSolver 的误差估计，动态调整每步金字塔级别，使“采样误差 + 注意力近似误差”全局最小。\n7. 多模态长上下文统一框架\n\n- 将 PSA 扩展到“文本-图像-视频-音频”混合序列，研究跨模态金字塔：不同模态预定义不同池化核（图像 2D、音频 1D、文本不池化），再设计模态间重要性融合函数，实现真正的任意模态长序列高效推理。\n8. 异构硬件后端自动调优\n\n- 当前内核在 NVIDIA GPU 验证。可将 decoupled block-tile 思想迁移到\n– Apple M-series / Intel GPU 的 tile-based renderer\n– 华为 Ascend 的 cube unit\n– 龙芯/申威等国产加速卡\n构建自动调优器，对“块大小-瓦片大小-共享内存-寄存器”四维空间做硬件感知搜索，形成跨平台通用内核生成器。\n9. 理论误差界与稀疏率预测\n\n- 建立 PSA 近似误差的 PAC-Bayes 或 Rademacher 复杂度界，给出“池化级别-稀疏率-任务损失”显式关系；利用界在线预测满足用户指定 ε-精度所需的最小稀疏率，实现 **Loss-Aware Sparsity Controller**。\n10. 绿色 AI 与碳排放评估\n\n- 记录端到端能耗（GPU 功耗 × 时间），建立“视觉质量 / 碳排放”指标；探索在碳预算约束下，PSA 与可再生能源波动相结合的 **Carbon-Aware Scheduling**，推动大模型推理碳足迹可审计、可交易。\n\n这些方向既涵盖算法理论（可学习性、误差界）、也覆盖系统实现（异构内核、碳感知），可推动 Pyramid Sparse Attention 向“自适应、多模态、绿色、跨平台”下一代高效注意力框架演进。\n\nQ6: 总结一下论文的主要内容\n\nPyramid Sparse Attention（PSA）是一篇面向长序列视频理解与生成的高效注意力论文，核心贡献与内容可概括为：\n\n1. 问题\n标准注意力  O(N^2)  复杂度在万帧级视频中成为推理瓶颈；现有块稀疏方法用二元保留/丢弃掩码，高稀疏率下信息丢失严重，视觉/语义质量骤降。\n\n2. 思路\n用“多级池化 KV 金字塔”替代二元决策：\n\n- 重要块 → 最细级别（原分辨率）\n- 次要块 → 中等池化\n- 冗余块 → 更粗池化或完全跳过\n同一计算预算下，query 可覆盖 70 % KV 区域（传统 BSA 仅 20 %），实现“细-粗-丢弃”平滑过渡。\n3. 方法\n\n- Pyramid KV：沿序列逐层 1D 平均池化，级数 h 每增 1，块长减半。\n- Multi-Level Mask：轻量重要性估计（生成用采样-max，理解用反对角线 + cosine 阈值）→ 行归一化累加 → 阈值 τ 分配层级。\n- Adaptive Attention：对池化后 KV 加层级偏置  (h-1)ln 2  保持 softmax 概率一致；块内仍用密集 GEMM，兼容 FlashAttention。\n- Hardware Kernel：逻辑块与 GPU tile 解耦，动态拆分/合并，保证 Tensor Core 高利用率，H200 上较朴素实现提速 10×。\n4. 实验\n\n- Training-Free 生成：Wan2.1{1.3B,5B,14B} & CogVideoX-5B，15 %–20 % 计算预算下 PSNR/SSIM/LPIPS 与感知指标全面优于 SVG/Sparge/STA，视觉接近 Full Attention。\n- 蒸馏加速：PSA + TDM 4 步采样，VBench 得分 0.826，**超过 50 步原始模型（0.819）**，实现 30× 去噪加速零质量损失。\n- 视频理解：Qwen2.5-VL-7B on Video-MME，65 % 稀疏率仍获最高 Overall 准确率 0.654，长视频提升明显。\n- 消融：多级掩码 > 二元掩码；阈值式分配 > 分位式；cosine 相似约束进一步提升质量；均衡金字塔分配优于极端策略。\n5. 结论\nPSA 通过“池化金字塔 + 动态层级掩码 + 解耦内核”在低算力预算内保留更多上下文，首次在训练无关设定下使稀疏注意力在视频生成与理解两大任务同时达到或超越全注意力精度，具备即插即用、硬件友好、可叠加蒸馏等优势。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PSA%3A%20Pyramid%20Sparse%20Attention%20for%20Efficient%20Video%20Understanding%20and%20Generation，链接是https://arxiv.org/pdf/2512.04025，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2512.04025。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}