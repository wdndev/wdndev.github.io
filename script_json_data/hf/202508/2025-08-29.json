{"timestamp":"2025-08-28T23:49:41.771Z","totalItems":17,"items":[{"id":"hf-0-1756424830914","title":"Beyond Transcription: Mechanistic Interpretability in ASR","description":"Interpretability methods have recently gained significant attention, particularly in the context of large language models, enabling insights into linguistic representations, error detection, and model behaviors such as hallucinations and repetitions. However, these techniques remain underexplored in automatic speech recognition (ASR), despite their potential to advance both the performance and interpretability of ASR systems. In this work, we adapt and systematically apply established interpretability methods such as logit lens, linear probing, and activation patching, to examine how acoustic and semantic information evolves across layers in ASR systems. Our experiments reveal previously unknown internal dynamics, including specific encoder-decoder interactions responsible for repetition hallucinations and semantic biases encoded deep within acoustic representations. These insights demonstrate the benefits of extending and applying interpretability techniques to speech recognition, opening promising directions for future research on improving model transparency and robustness.","url":"https://huggingface.co/papers/2508.15882","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.914Z","metadata":{"authors":["Neta Glazer","Yael Segal-Feldman","Hilit Segev","Aviv Shamsian","Asaf Buchnick","Gill Hetz","Ethan Fetaya","Joseph Keshet","Aviv Navon"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.SD","cs.CL","cs.LG","eess.AS"],"pdfUrl":"https://arxiv.org/pdf/2508.15882.pdf","arxivUrl":"https://arxiv.org/abs/2508.15882","arxivId":"2508.15882","coolPaperUrl":"https://papers.cool/arxiv/2508.15882","published":"2025-08-21T15:42:53Z","updated":"2025-08-21T15:42:53.000Z","zh_summary":"可解释性方法最近引起了广泛关注，尤其是在大型语言模型的背景下，使我们能够深入了解语言表示、错误检测以及模型行为（如幻觉和重复）。然而，尽管这些技术有潜力提高自动语音识别（ASR）系统的性能和可解释性，但在ASR中仍然未得到充分探索。在这项工作中，我们调整并系统地应用了已建立的可解释性方法，如logit镜头、线性探测和激活修补，来研究声学和语义信息在ASR系统各层之间是如何演变的。我们的实验揭示了先前未知的内部动态，包括负责重复幻觉的特定编码器-解码器交互以及深层声学表示中编码的语义偏见。这些洞察展示了将可解释性技术扩展和应用于语音识别的好处，为未来研究提高模型透明度和鲁棒性开辟了有前景的方向。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图将可解释性方法应用于自动语音识别（ASR）系统，以揭示其内部动态和决策过程。尽管可解释性方法在大型语言模型（LLMs）中得到了广泛关注，但在ASR领域仍相对未被探索。论文的主要目标是通过系统地应用已有的可解释性技术，如logit lens、linear probing和activation patching，来研究ASR系统中的声学和语义信息是如何在不同层之间演变的。具体而言，论文旨在：\n\n*   揭示导致重复幻觉（repetition hallucinations）和语义偏差的具体编码器-解码器交互机制。\n*   研究预测是如何在不同层之间演变的，哪些组件驱动特定的解码行为，以及上下文期望是如何与声学证据竞争的。\n*   通过分析内部机制，为提高ASR系统的透明度和鲁棒性开辟新的研究方向。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了以下相关研究：\n\n自动语音识别（ASR）领域\n\n*   **模型架构与性能提升**：Radford et al. (2023) 和 Chu et al. (2023) 研究了基于Transformer架构的ASR模型，这些模型在多种语言、口音和声学条件下表现出强大的鲁棒性。Abouelenin et al. (2025) 探讨了将预训练语言模型集成到ASR系统中的方法，例如Qwen2-Audio (Chu et al. 2024) 和Whisper (Radford et al. 2023)。\n*   **Whisper模型的内部表示研究**：Gong et al. (2023)、Upadhyay et al. (2024) 和 Zhao et al. (2024) 研究了Whisper模型的编码器如何捕捉噪声特征、说话人身份和情感内容。Berns et al. (2023) 探讨了Whisper解码器如何编码说话人特征和响应语言变化。\n*   **ASR模型的幻觉现象研究**：Barański et al. (2025) 调查了ASR模型在非语音段上的幻觉现象，旨在对常见幻觉进行分类，而不是在模型组件内定位它们。Frieske and Shi (2024) 也研究了ASR模型中的幻觉现象。\n\n可解释性方法在大型语言模型（LLMs）中的应用\n\n*   **logit lens方法**：Geva et al. (2022) 和 nostalgebraist (2020) 提出了logit lens方法，用于跟踪token预测是如何在不同层之间演变的，为模型行为提供了按层的视图。\n*   **linear probing方法**：Belinkov (2022) 和 McKenzie et al. (2025) 研究了linear probing方法，通过简单分类器测试模型的隐藏表示是否能够编码特定属性，如语法或事实知识。\n*   **activation patching和causal tracing方法**：Meng et al. (2022) 和 Wang et al. (2022) 探索了通过交换或消融特定隐藏状态来观察输出变化的方法，以研究特定隐藏状态的因果作用。\n*   **attribution patching方法**：Syed et al. (2023) 和 Nanda (2023) 以及 Kramár et al. (2024) 将attribution patching方法应用于更细粒度的结构，通过梯度定位有影响力的神经元。\n\n可解释性方法在其他领域的应用\n\n*   **Diffusion Lens框架**：Toker et al. (2024) 提出了Diffusion Lens框架，用于解释文本到图像模型中的文本编码器。该方法启发了论文中提出的Encoder Lens方法，用于分析ASR模型中的中间编码器表示。\n\n这些相关研究为论文中将可解释性方法应用于ASR系统提供了理论基础和技术支持。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过以下方法来解决自动语音识别（ASR）系统内部动态和决策过程的可解释性问题：\n\n1\\. 适应和应用可解释性技术\n\n论文将已有的可解释性技术（如logit lens、linear probing和activation patching）系统地应用于ASR系统，以揭示其内部机制。这些技术原本用于大型语言模型（LLMs）或视觉模型，论文详细描述了如何将它们适应到ASR设置中。\n\n2\\. 实验设计\n\n论文设计了一系列实验，使用两个最先进的ASR系统：Whisper和Qwen2-Audio。这两个系统在架构设计上有所不同，Whisper是一个专门为多语言语音到文本和语音翻译任务设计的编码器-解码器模型，而Qwen2-Audio是一个结合了预训练语言模型的大型音频语言模型。\n\n2.1 探测转录丰富性\n\n*   **说话人性别**：通过在LibriSpeech数据集上训练线性探测器，发现Whisper编码器在较深的层（如第25层）能够以94.6%的准确率线性解码性别特征。\n*   **环境噪声**：通过比较LibriSpeech的clean和noisy条件下的语音，发现编码器在第27层能够以90.0%的准确率区分clean和noisy语音。\n*   **口音分类**：通过在英语口音数据集上进行多类分类，发现编码器在第22层能够以97.0%的平均准确率区分不同口音。\n\n2.2 探测幻觉监测\n\n*   **从解码器残差流预测幻觉**：通过在LibriSpeech和CommonVoice数据集上训练线性探测器，发现Whisper解码器的残差流在第22层能够以93.4%的准确率预测幻觉，表明幻觉相关信号在解码器的深层中被编码。\n*   **语音与非语音分类**：通过构建包含语音和非语音样本的数据集，发现Whisper解码器在第10至28层能够以100%的准确率区分语音和非语音，表明模型在解码器中线性地编码了这一基本区分。\n\n2.3 分析声学、上下文和语义机制\n\n*   **声学与上下文机制**：通过构建包含声学模糊单词的合成音频样本，发现干预编码器组件可以改善声学准确性，表明编码器不仅处理声学输入，还编码上下文和语义期望。\n*   **Whisper编码器理解语义**：通过训练线性探测器对语义类别进行分类，发现语义编码在编码器的高层中变得越来越显著，表明编码器在处理声学特征的同时，也发展了层次化的语义表示。\n\n2.4 令牌选择机制\n\n*   **令牌选择动态**：通过应用logit lens技术，发现Whisper和Qwen2-Audio在大约第20层之前对最终选择的令牌的置信度较低，之后迅速增加。\n*   **声学和语义令牌相似性**：通过比较最终选择的令牌与前5个候选令牌的声学距离和语义相似性，发现Whisper在声学相似性方面表现更好，而Qwen2-Audio在语义相似性方面表现更好。\n*   **未来令牌预测能力**：通过评估模型预测未来令牌的能力，发现Qwen2-Audio从大约第21层开始能够预测下一个令牌，而Whisper从第29层开始表现出显著的预测能力。\n\n2.5 解码器重复机制\n\n*   **重复幻觉机制**：通过在Whisper模型上应用因果干预方法，发现特定的交叉注意力组件（如第18层的头13）在控制重复幻觉中起关键作用。通过干预这些组件，可以显著减少重复现象。\n\n2.6 编码器透镜\n\n*   **编码器表示演变**：通过省略编码器的顶层并将中间表示直接传递给解码器，发现Whisper在第20至27层之间开始产生语法正确的短语，但这些短语可能与原始音频内容无关。Qwen2-Audio则表现出不同的失败模式，早期层中出现严重退化，且倾向于输出训练数据中记忆的序列。\n\n3\\. 结论与未来方向\n\n论文通过这些实验揭示了ASR系统内部的多种机制，包括幻觉、重复幻觉、上下文偏差等现象的成因。这些发现为未来的研究提供了新的方向，例如构建内部监控器以检测幻觉或饱和现象、开发细粒度的编辑和调试工具、以及指导架构选择以更好地平衡接地和流畅性。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文中设计并执行了以下实验，以探索自动语音识别（ASR）系统内部的动态和决策过程：\n\n1\\. 探测转录丰富性（Probing for Transcription Enrichment）\n\n1.1 说话人性别（Speaker Gender）\n\n*   **数据集**：使用LibriSpeech数据集中的2,000个标记样本进行训练，500个测试样本进行评估。\n*   **方法**：在Whisper编码器的每一层上训练线性探测器，以预测说话人的性别。\n*   **结果**：在第25层达到最高准确率94.6%，表明性别特征在较深的编码器层中可以被线性解码。\n\n1.2 环境噪声（Clean vs. Noisy Environment）\n\n*   **数据集**：使用LibriSpeech的dev-clean和dev-other分割进行训练，test-clean和test-other分割进行测试。\n*   **方法**：在Whisper编码器的每一层上训练线性探测器，以区分clean和noisy条件下的语音。\n*   **结果**：在第27层达到最高准确率90.0%，表明编码器能够有效区分clean和noisy语音。\n\n1.3 口音分类（Accents）\n\n*   **数据集**：使用英语口音数据集中的2,400个样本进行训练，337个测试样本进行评估，涵盖新西兰、威尔士山谷、南非和印度口音。\n*   **方法**：在Whisper编码器的每一层上训练线性探测器，进行多类口音分类。\n*   **结果**：在第22层达到最高平均准确率97.0%，表明口音信息可以在中间音频表示中线性解码。\n\n2\\. 探测幻觉监测（Probing for Hallucination Monitoring）\n\n2.1 从解码器残差流预测幻觉（Hallucination Prediction from Decoder Residual Stream）\n\n*   **数据集**：使用LibriSpeech和CommonVoice数据集的子集，创建包含零和高词错误率（WER）样本的数据集。\n*   **方法**：在解码器的每一层的最终token位置（<eos> token）上训练线性探测器，以区分高质量转录和幻觉。\n*   **结果**：Whisper在第22层达到最高准确率93.4%，Qwen2-Audio在第22层达到最高准确率70.2%（LibriSpeech）和83.6%（CommonVoice），表明幻觉相关信号在解码器的深层中被编码。\n\n2.2 语音与非语音分类（Speech vs. Non-Speech for Non-Speech Hallucinations）\n\n*   **数据集**：构建包含400个语音样本和400个非语音样本的平衡二元分类数据集，涵盖多种音频环境和事件。\n*   **方法**：在解码器的每一层的最终token表示上训练线性分类器，以区分语音和非语音输入。\n*   **结果**：从第10层到第28层达到100%的准确率，第31层达到99.17%的准确率，表明Whisper能够在解码器中线性地编码语音与非语音的区分。\n\n3\\. 分析声学、上下文和语义机制（Analyzing Acoustic, Contextual, and Semantic Mechanisms）\n\n3.1 声学与上下文机制（Acoustic and Contextual Mechanism）\n\n*   **数据集**：构建包含700个合成音频样本的数据集，这些样本设计为触发上下文错误。\n*   **方法**：对Whisper和Qwen2-Audio的编码器和解码器组件进行干预，使用白噪声作为干扰音频。\n*   **结果**：干预编码器组件可以改善声学准确性，表明编码器不仅处理声学输入，还编码上下文和语义期望。\n\n3.2 Whisper编码器理解语义（Whisper Encoder Understands Semantics）\n\n*   **数据集**：构建包含来自不同语义组（如水果和衣物）的术语的合成音频数据集。\n*   **方法**：训练线性探测器基于编码器激活来区分语义类别对。\n*   **结果**：语义编码在编码器的高层中变得越来越显著，表明编码器在处理声学特征的同时，也发展了层次化的语义表示。\n\n4\\. 令牌选择机制（Token Selection Mechanism）\n\n4.1 令牌选择动态（Token Selection Dynamics）\n\n*   **数据集**：从CommonVoice测试集中随机抽取100个话语样本，涵盖六种语言（英语、法语、西班牙语、德语、中文和意大利语）。\n*   **方法**：应用logit lens技术，分析模型在不同层对最终选择的令牌的置信度变化。\n*   **结果**：Whisper和Qwen2-Audio在大约第20层之前对最终选择的令牌的置信度较低，之后迅速增加。\n\n4.2 声学和语义令牌相似性（Acoustic and Semantic Token Similarity）\n\n*   **方法**：比较最终选择的令牌与前5个候选令牌的声学距离（使用修改版的Phoneme Error Rate，PER）和语义相似性（使用token嵌入的余弦相似性）。\n*   **结果**：Whisper在声学相似性方面表现更好，而Qwen2-Audio在语义相似性方面表现更好。\n\n4.3 未来令牌预测能力（Next-Token Prediction Capabilities）\n\n*   **方法**：评估模型预测未来令牌（位置s+i，1≤i≤5）的能力，使用Recall@10指标。\n*   **结果**：Qwen2-Audio从大约第21层开始能够预测下一个令牌，而Whisper从第29层开始表现出显著的预测能力。\n\n5\\. 解码器重复机制（Decoder Repetition Mechanisms in Whisper）\n\n*   **数据集**：构建包含102个多语言话语样本的数据集，这些样本容易产生重复幻觉。\n*   **方法**：对Whisper模型的解码器的交叉注意力、自注意力和前馈层进行干预（patching和ablation）。\n*   **结果**：干预交叉注意力可以显著减少重复现象，特别是在第18层的头13中，表明重复幻觉由特定的交叉注意力组件控制。\n\n6\\. 编码器透镜（Encoder Lens）\n\n*   **数据集**：使用400个音频样本，涵盖英语（LibriSpeech）、西班牙语（Multilingual LibriSpeech）和中文（AISHELL）。\n*   **方法**：省略编码器的顶层，将中间表示直接传递给解码器，分析不同编码器层的输出。\n*   **结果**：Whisper在第20至27层之间开始产生语法正确的短语，但这些短语可能与原始音频内容无关。Qwen2-Audio在早期层中表现出严重的退化，倾向于输出训练数据中记忆的序列。\n\n这些实验共同揭示了ASR系统内部的多种机制，包括幻觉、重复幻觉、上下文偏差等现象的成因，为未来的研究提供了新的方向。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 论文提出了多个未来研究方向，以下是一些可以进一步探索的点：\n\n1\\. **内部监控器的构建**\n\n*   **幻觉检测**：基于解码器残差流中的幻觉相关信号，开发实时幻觉检测器，以在推理过程中动态监测和标记潜在的幻觉输出。\n*   **饱和层监测**：利用logit lens技术，开发监测器以实时跟踪模型在不同层的决策饱和情况，帮助理解模型何时对预测结果变得自信。\n\n2\\. **细粒度编辑和调试工具**\n\n*   **针对性干预**：开发能够对特定组件（如特定的注意力头或层）进行干预的工具，以实现对模型行为的精细调整。例如，针对重复幻觉的关键组件进行干预，以减少这类错误的发生。\n*   **错误溯源**：进一步探索如何将错误追溯到模型的特定组件，从而实现更精确的调试和优化策略。\n\n3\\. **架构改进**\n\n*   **平衡接地与流畅性**：研究如何通过架构调整来更好地平衡模型对声学输入的依赖（接地）和生成流畅文本的能力。例如，探索新的编码器-解码器交互机制，以减少上下文偏差对声学准确性的负面影响。\n*   **多模态融合**：探索如何将视觉或其他模态的信息融入ASR模型，以增强模型对复杂场景的理解能力，减少仅依赖音频输入时可能出现的歧义和错误。\n\n4\\. **跨语言和跨领域研究**\n\n*   **多语言模型的比较**：对不同语言的ASR模型进行更深入的比较分析，研究不同语言在模型内部表示和决策过程中的差异。例如，探索为什么某些语言在特定任务上表现更好，以及如何将这些优势迁移到其他语言。\n*   **跨领域适应性**：研究ASR模型在不同领域（如医疗、法律、教育等）的表现差异，以及如何通过领域适应技术来提高模型在特定领域的性能和可解释性。\n\n5\\. **模型压缩与优化**\n\n*   **基于可解释性的压缩**：利用对模型内部机制的理解，开发更有效的模型压缩策略。例如，识别和移除对模型性能影响较小的冗余组件，以减少模型大小和计算成本。\n*   **高效训练策略**：探索如何根据模型内部的动态和决策过程，设计更高效的训练策略。例如，通过调整训练数据的分布或优化训练过程中的正则化方法，来提高模型的泛化能力和鲁棒性。\n\n6\\. **用户交互与反馈**\n\n*   **用户反馈循环**：研究如何将用户的实时反馈集成到ASR系统中，以实现自适应的模型调整。例如，根据用户对转录结果的纠正或评价，动态更新模型的参数或内部表示。\n*   **交互式解释**：开发能够向用户提供模型决策解释的交互式工具，帮助用户更好地理解和信任ASR系统。例如，通过可视化模型的内部激活或决策路径，让用户了解模型为何做出特定的预测。\n\n7\\. **长期行为与动态变化**\n\n*   **模型的长期演变**：研究ASR模型在长期使用过程中的动态变化，包括模型参数的漂移、性能的退化或改进等。探索如何通过持续学习或在线更新机制来维持模型的高性能和适应性。\n*   **跨时间的可解释性**：分析模型在不同时间点的内部表示和决策过程，以了解模型如何随着时间的推移适应新的数据和任务需求。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: 论文《Beyond Transcription: Mechanistic Interpretability in ASR》由Neta Glazer等人撰写，旨在将可解释性方法应用于自动语音识别（ASR）系统，以揭示其内部动态和决策过程。尽管可解释性方法在大型语言模型（LLMs）中得到了广泛关注，但在ASR领域仍相对未被探索。论文通过系统地应用已有的可解释性技术，如logit lens、linear probing和activation patching，来研究ASR系统中的声学和语义信息是如何在不同层之间演变的。\n\n研究背景\n\n自动语音识别（ASR）系统近年来取得了显著进展，主要得益于强大的神经架构和大规模语音数据集的训练。现代ASR系统通常采用编码器-解码器Transformer架构，能够在多种语言、口音和声学条件下实现鲁棒的识别。然而，尽管可解释性方法在大型语言模型（LLMs）中得到了广泛应用，但在ASR领域仍相对未被探索。这些方法能够揭示模型如何表示信息和做出决策，对于诊断模型的失败模式（如幻觉和推理错误）至关重要。\n\n研究方法\n\n论文采用了以下几种可解释性技术，并将其适应到ASR系统中：\n\n1.  **Logit Lens**：通过在解码器的每一层提取残差流，并将其投影到词汇空间，来分析模型预测是如何在不同层之间演变的。\n2.  **Linear Probing**：通过训练简单的分类器来预测特定属性，测试模型的隐藏表示是否能够线性解码这些属性。\n3.  **Activation Patching**：通过交换或消融特定的隐藏状态，观察输出的变化，从而研究这些状态的因果作用。\n4.  **Encoder Lens**：通过省略编码器的顶层，将中间表示直接传递给解码器，分析编码器层之间的表示演变。\n\n实验设计\n\n实验使用了两个最先进的ASR系统：Whisper和Qwen2-Audio。Whisper是一个专门为多语言语音到文本和语音翻译任务设计的编码器-解码器模型，而Qwen2-Audio是一个结合了预训练语言模型的大型音频语言模型。\n\n实验结果\n\n1\\. 探测转录丰富性\n\n*   **说话人性别**：Whisper编码器在第25层能够以94.6%的准确率线性解码性别特征。\n*   **环境噪声**：编码器在第27层能够以90.0%的准确率区分clean和noisy语音。\n*   **口音分类**：编码器在第22层能够以97.0%的平均准确率区分不同口音。\n\n2\\. 探测幻觉监测\n\n*   **从解码器残差流预测幻觉**：Whisper解码器在第22层能够以93.4%的准确率预测幻觉，表明幻觉相关信号在解码器的深层中被编码。\n*   **语音与非语音分类**：解码器在第10至28层能够以100%的准确率区分语音和非语音输入。\n\n3\\. 分析声学、上下文和语义机制\n\n*   **声学与上下文机制**：干预编码器组件可以改善声学准确性，表明编码器不仅处理声学输入，还编码上下文和语义期望。\n*   **Whisper编码器理解语义**：语义编码在编码器的高层中变得越来越显著，表明编码器在处理声学特征的同时，也发展了层次化的语义表示。\n\n4\\. 令牌选择机制\n\n*   **令牌选择动态**：Whisper和Qwen2-Audio在大约第20层之前对最终选择的令牌的置信度较低，之后迅速增加。\n*   **声学和语义令牌相似性**：Whisper在声学相似性方面表现更好，而Qwen2-Audio在语义相似性方面表现更好。\n*   **未来令牌预测能力**：Qwen2-Audio从大约第21层开始能够预测下一个令牌，而Whisper从第29层开始表现出显著的预测能力。\n\n5\\. 解码器重复机制\n\n*   **重复幻觉机制**：干预交叉注意力可以显著减少重复现象，特别是在第18层的头13中，表明重复幻觉由特定的交叉注意力组件控制。\n\n6\\. 编码器透镜\n\n*   **编码器表示演变**：Whisper在第20至27层之间开始产生语法正确的短语，但这些短语可能与原始音频内容无关。Qwen2-Audio在早期层中表现出严重的退化，倾向于输出训练数据中记忆的序列。\n\n结论\n\n论文通过这些实验揭示了ASR系统内部的多种机制，包括幻觉、重复幻觉、上下文偏差等现象的成因。这些发现为未来的研究提供了新的方向，例如构建内部监控器以检测幻觉或饱和现象、开发细粒度的编辑和调试工具、以及指导架构选择以更好地平衡接地和流畅性。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Beyond%20Transcription%3A%20Mechanistic%20Interpretability%20in%20ASR，链接是  https://arxiv.org/pdf/2508.15882  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.15882  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1756424830915","title":"Self-Rewarding Vision-Language Model via Reasoning Decomposition","description":"Vision-Language Models (VLMs) often suffer from visual hallucinations, saying things that are not actually in the image, and language shortcuts, where they skip the visual part and just rely on text priors. These issues arise because most post-training methods for VLMs rely on simple verifiable answer matching and supervise only final outputs, leaving intermediate visual reasoning without explicit guidance. As a result, VLMs receive sparse visual signals and often learn to prioritize language-based reasoning over visual perception. To mitigate this, some existing methods add visual supervision using human annotations or distilled labels from external large models. However, human annotations are labor-intensive and costly, and because external signals cannot adapt to the evolving policy, they cause distributional shifts that can lead to reward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method that improves visual reasoning without relying on external visual supervisions via reinforcement learning. Vision-SR1 decomposes VLM reasoning into two stages: visual perception and language reasoning. The model is first prompted to produce self-contained visual perceptions that are sufficient to answer the question without referring back the input image. To validate this self-containment, the same VLM model is then re-prompted to perform language reasoning using only the generated perception as input to compute reward. This self-reward is combined with supervision on final outputs, providing a balanced training signal that strengthens both visual perception and language reasoning. Our experiments demonstrate that Vision-SR1 improves visual reasoning, mitigates visual hallucinations, and reduces reliance on language shortcuts across diverse vision-language tasks.","url":"https://huggingface.co/papers/2508.19652","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.915Z","metadata":{"authors":["Zongxia Li","Wenhao Yu","Chengsong Huang","Rui Liu","Zhenwen Liang","Fuxiao Liu","Jingxi Che","Dian Yu","Jordan Boyd-Graber","Haitao Mi","Dong Yu"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2508.19652.pdf","arxivUrl":"https://arxiv.org/abs/2508.19652","arxivId":"2508.19652","coolPaperUrl":"https://papers.cool/arxiv/2508.19652","published":"2025-08-27T08:01:03Z","updated":"2025-08-27T08:01:03.000Z","zh_summary":"视觉-语言模型（VLMs）通常会遭遇视觉幻觉，描述图像中实际上不存在的内容，以及语言快捷方式，跳过视觉部分而仅依赖文本先验。这些问题的产生是因为大多数VLM的后训练方法仅依赖简单可验证的答案匹配，并且只对最终输出进行监督，导致中间的视觉推理缺乏明确指导。因此，VLM接收到的视觉信号稀疏，通常学会优先考虑基于语言的推理而非视觉感知。为了减轻这一问题，一些现有方法通过人工注释或来自外部大模型的提炼标签增加视觉监督。然而，人工注释劳动强度大且成本高，并且由于外部信号无法适应不断变化的策略，它们会导致分布偏移，从而可能引起奖励黑客。在本文中，我们提出了Vision-SR1，一种自奖励的方法，它通过强化学习提高视觉推理，而不依赖外部视觉监督。Vision-SR1将VLM推理分解为两个阶段：视觉感知和语言推理。模型首先被提示生成独立的视觉感知，这些感知足以回答问题，而无需参考输入图像。为了验证这种自包含性，同一VLM模型随后被重新提示，使用仅生成的感知作为输入进行语言推理，从而计算奖励。这个自奖励与最终输出的监督结合，提供了平衡的训练信号，增强了视觉感知和语言推理。我们的实验表明，Vision-SR1改善了视觉推理，减轻了视觉幻觉，并减少了在各种视觉-语言任务中对语言快捷方式的依赖。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: Vision-SR1 针对的核心问题是**视觉-语言模型（VLM）在推理过程中出现的视觉幻觉（visual hallucination）和语言捷径（language shortcut）**。具体而言：\n\n*   **视觉幻觉**：模型描述或推断出图像中并不存在的视觉内容。\n*   **语言捷径**：模型跳过对图像的真实理解，仅依靠文本先验知识给出答案。\n\n这些问题的根源在于现有后训练方法（如 RLHF、指令微调）通常只对最终答案进行简单的可验证匹配，缺乏对中间视觉推理过程的显式监督，导致视觉信号稀疏，模型倾向于依赖更强的语言模块而非视觉感知。\n\nVision-SR1 通过**自我奖励机制**（self-rewarding）和**推理分解**（reasoning decomposition），在不依赖外部视觉监督的前提下，强化视觉感知与语言推理之间的耦合，从而缓解幻觉与捷径现象。\n\n**Q**: 有哪些相关研究？\n\n**A**: 与 Vision-SR1 相关的研究可归纳为以下两条主线，均围绕“如何提升 VLM 推理可靠性”与“如何摆脱昂贵外部监督”展开。\n\n1\\. 强化学习与推理增强的 VLM 后训练\n\n*   **R1-style RL 系列**\n    \n    *   Vision-R1 (Huang et al., 2025)：首个将 DeepSeek-R1 式 RL 引入多模态，仅用答案正确性作为奖励，但易陷入语言捷径。\n    *   Perception-R1 (Xiao et al., 2025)：在 Vision-R1 基础上引入外部大模型提取的“视觉注释”作为额外奖励，需离线标注。\n    *   Visionary-R1 (Xia et al., 2025)：强制模型输出 caption-reason-answer 格式，奖励由外部文本 LLM 计算，仍依赖外部教师。\n*   **显式中间监督方法**\n    \n    *   LLaVA-CoT (Xu et al., 2024)、Describe-then-Reason (Jia et al., 2024)：通过人工或 GPT-4 生成逐步推理链，进行监督微调。\n    *   RLIF / RLSC (Zhao et al., 2025; Li et al., 2025a)：在文本 LLM 中用 logit-based 自信度或自我验证替代外部奖励模型。\n\n2\\. 自奖励（Self-Rewarding）与无监督对齐\n\n*   **文本领域的自奖励**\n    \n    *   Self-Rewarding LM (Yuan et al., 2025)：让 LLM 自己生成并评判候选答案，实现无外部 RM 的 RLHF。\n    *   RLIF, RLSC, RLSR (Simonds et al., 2025; van Niekerk et al., 2025)：进一步用不确定性估计或自我反馈作为内在奖励。\n*   **视觉-语言领域的自奖励探索**\n    \n    *   Calibrated Self-Rewarding VLM (Zhou et al., 2024)：迭代式自评分 + DPO，但奖励仅用于离线排序，未端到端训练。\n    *   RG-VLM (Lee et al., 2025)：用 VLM 给离线轨迹标注奖励，作为辅助信号，非完全自包含。\n    *   ARES (Holmes & Chi, 2025)：从注意力权重中抽取稠密奖励，缓解稀疏反馈，但未显式约束视觉感知。\n\n综上，Vision-SR1 与上述工作的差异在于：\n\n1.  不依赖任何外部视觉或文本监督，完全由模型自身验证视觉感知是否自包含；\n2.  通过两阶段 rollout 将“视觉感知奖励”与“答案正确性奖励”端到端联合优化，实现真正意义上的自奖励 VLM。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: Vision-SR1 通过“**推理分解 + 自奖励强化学习**”的框架，在完全不依赖外部视觉监督的前提下，显式地约束并优化视觉感知与语言推理两个环节，从而抑制幻觉与语言捷径。具体做法分为三步：\n\n* * *\n\n1\\. 推理链显式分解：See-Think 格式\n\n*   **输出结构强制**  \n    每个回答必须按  \n    `<visual perception> … </visual perception>`  \n    `<think> … </think>`  \n    `<answer> … </answer>`  \n    三段式生成。\n*   **自包含约束**  \n    `<visual perception>` 被要求“**一张嘴说清**”——即仅凭这段文字即可回答原问题，无需再访问图像。\n\n* * *\n\n2\\. 两阶段 Rollout：同一模型既生成又验证\n\n*   **阶段 1：标准 Rollout**  \n    输入 (image, query) → 模型输出三段式回答。  \n    奖励：\n    \n    *   `rans`：答案正确性\n    *   `rfmt`：格式合规性\n*   **阶段 2：自奖励 Rollout**  \n    冻结阶段 1 生成的 `<visual perception>`，仅把 (query, perception) 再次喂给**同一个模型**。  \n    若模型能再次答对，则判定该 perception 是“视觉忠实”的，给出\n    \n    *   `rvisual = 1`，否则 0。\n\n* * *\n\n3\\. 联合目标：GRPO 优化\n\n将三种奖励线性组合：  \n${% raw %}$ r(Q,s)=r\\_{\\\\text{visual}}(Q,c)+r\\_{\\\\text{ans}}(Q,a)+\\\\alpha,r\\_{\\\\text{fmt}}(s) ${% endraw %}$  \n用 Group Relative Policy Optimization (GRPO) 更新策略，保证：\n\n*   视觉感知与答案正确性同时获得梯度信号，避免语言模块一家独大；\n*   无需任何外部视觉标注或大模型蒸馏，完全自监督。\n\n* * *\n\n4\\. 训练数据与冷启动\n\n*   **47K Vision-SR1 数据集**：跨数学、常识、通用视觉三大领域，保证多样性。\n*   **9K SFT 冷启动**：用 Qwen-2.5-VL-7B 自动生成并双重过滤（答案正确 + perception 自包含），让模型先学会格式，再进入 RL。\n\n* * *\n\n通过上述机制，Vision-SR1 在训练过程中持续收到“**视觉感知是否足够**”的密集反馈，迫使模型真正“看图说话”，而非依赖语言先验。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文从**通用视觉理解、数学推理、幻觉诊断**三大维度，在 8 个公开基准上对 Vision-SR1 进行了系统评估，并辅以消融、捷径率（LSR）与文本遗忘分析。所有实验统一使用 Qwen-2.5-VL-3B / 7B 作为骨干，训练 1 epoch（Vision-SR1-47K）。关键实验如下：\n\n* * *\n\n1\\. 主实验：与三类基线对比\n\n类别\n\n基线\n\n说明\n\n无外部监督\n\nVision-R1 (复现)\n\n仅用答案奖励的 R1-style RL\n\n外部视觉监督\n\nPerception-R1\n\n额外使用外部大模型提取的视觉标注\n\n外部文本监督\n\nVisionary-R1\n\n使用外部文本 LLM 对 caption-reason-answer 打分\n\n**结果**（表 2 汇总）：\n\n*   **7B 模型**：Vision-SR1 平均 58.8，显著高于 Vision-R1 57.4、SFT 55.1。\n*   **3B 模型**：Vision-SR1 平均 52.9，亦全面领先。\n*   在 MMMU-Pro、MathVerse、HallusionBench 等所有子集上均取得最佳或并列最佳。\n\n* * *\n\n2\\. 消融实验：验证“自奖励”的必要性\n\n模型\n\n平均得分\n\n结论\n\nVision-SR1 (7B)\n\n58.8\n\n完整方法\n\nw/o self-reward\n\n57.7\n\n去掉 `rvisual`，仅保留答案与格式奖励，性能一致下降\n\n* * *\n\n3\\. 语言捷径率（LSR）分析\n\n*   **定义**：在答案正确的前提下，生成的 `<visual perception>` 经 Gemini-2.5-flash 判定为“不自包含”的比例。\n*   **结果**（表 4）：\n    *   Vision-SR1 7B 平均 LSR = 6.7%，去掉自奖励后升至 7.9%。\n    *   数学任务（MathVerse、MATH-Vision）LSR 最高，提示先前 R1 系列可能主要靠语言先验。\n\n* * *\n\n4\\. 文本遗忘实验\n\n在 4 个纯文本基准（MMLU-Pro、SuperGPQA、GSM8K、MATH-500）上比较：\n\n*   Vision-SR1 在通用知识任务上优于 Vision-R1；\n*   在数学任务上，Vision-SR1 的下降幅度更小，表明**分离视觉与语言奖励**可缓解“文本遗忘”。\n\n* * *\n\n5\\. 评价基准一览\n\n领域\n\n基准\n\n规模\n\n重点\n\n通用视觉\n\nMMMU / MMMU-Pro / MM-Vet / RealWorldQA / VisNumBench\n\n11K+\n\n跨学科、OCR、空间理解\n\n数学推理\n\nMathVerse / MATH-Vision\n\n2.6K / 3K\n\n几何、函数、竞赛题\n\n幻觉诊断\n\nHallusionBench\n\n400\n\n语言幻觉 vs 视觉错觉\n\n* * *\n\n综上，实验不仅验证了 Vision-SR1 在**准确率**上的提升，也通过 LSR 与文本遗忘分析，证明其**真正强化了视觉感知、抑制了语言捷径**。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 Vision-SR1 的后续研究切入点，按“由浅入深”与“由技术到评测”分层列出：\n\n* * *\n\n1\\. 奖励信号再细化\n\n*   **连续型视觉奖励**  \n    当前 `rvisual` 为 0/1 离散值，可探索基于 **log-likelihood margin** 或 **互信息下界** 的连续奖励，降低梯度方差。\n*   **嵌入级奖励**  \n    不再把视觉感知转成文本，而是直接对视觉 token 的表征计算奖励（例如用对比学习约束 `I(C;I)`），减少信息瓶颈。\n\n* * *\n\n2\\. 推理链再分解\n\n*   **多步视觉-语言交替**  \n    将一次 See-Think 扩展为多轮 “感知→推理→再感知→再推理” 的递归结构，适配更长程、多跳视觉问答。\n*   **工具调用感知**  \n    允许模型在 `<visual perception>` 阶段调用外部工具（OCR、目标检测），再对自身调用结果做自奖励，实现“可验证工具使用”。\n\n* * *\n\n3\\. 训练策略升级\n\n*   **课程式自奖励**  \n    先用弱监督（答案正确性）热启，再逐步引入 `rvisual`，避免初期因视觉感知质量过低导致奖励稀疏。\n*   **多智能体自奖励**  \n    同一参数初始化两份模型：一份负责生成 perception，另一份作为“验证器”给出 `rvisual`，通过共训练减少策略-验证器耦合误差。\n\n* * *\n\n4\\. 数据与任务扩展\n\n*   **视频与 3D 场景**  \n    将 Vision-SR1 迁移到 VideoQA、Embodied QA，验证时序/空间-时序视觉感知的自奖励可行性。\n*   **细粒度感知任务**  \n    在检测、分割、计数任务上测试：若 perception 必须包含精确坐标或掩码，自奖励是否仍有效？\n\n* * *\n\n5\\. 评测与诊断\n\n*   **更强 LSR 基准**  \n    构建大规模、对抗性 LSR 数据集，引入“先验陷阱题”（问题本身可答，但与图像无关），系统衡量捷径倾向。\n*   **因果干预分析**  \n    用 causal mediation / attention knockout 方法量化 `rvisual` 对视觉 token 与语言 token 的因果流，验证奖励是否真正作用于视觉通路。\n\n* * *\n\n6\\. 理论层面\n\n*   **收敛性保证**  \n    在 two-stage rollout 框架下，给出 `rvisual` 与 `rans` 权重 α 的理论最优区间，防止语言模块主导或奖励 hacking。\n*   **信息论下界**  \n    推导 `I(C;I)` 与最终准确率之间的可证明下界，为自奖励提供理论支撑。\n\n* * *\n\n7\\. 实际系统落地\n\n*   **边缘端轻量化**  \n    将自奖励阶段蒸馏成小验证器（如 0.3B 文本模型），在端侧完成二阶段推理，降低延迟。\n*   **人机协同标注**  \n    利用 Vision-SR1 的 LSR 指标主动挑选“疑似捷径样本”供人类复核，形成低成本、高可信的持续迭代闭环。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### Vision-SR1 核心总结\n\n维度\n\n内容\n\n**问题**\n\nVLM 在推理时出现视觉幻觉（描述不存在的内容）与语言捷径（跳过图像、仅用文本先验答题）。\n\n**根源**\n\n现有 RL/微调方法仅监督最终答案，缺乏对中间视觉推理的显式信号，导致视觉感知被语言模块主导。\n\n**方法**\n\n提出“**自奖励 + 推理分解**”框架：<br>1. 强制输出三段式：`<visual perception>` → `<think>` → `<answer>`；<br>2. 同一模型两次 rollout：<br> - 第一次：用 (图像, 问题) 生成完整回答，得答案奖励；<br> - 第二次：仅用第一次生成的 `<visual perception>` + 问题再次回答，若仍正确则给出视觉奖励；<br>3. 联合奖励 `r = r_visual + r_ans + α·r_fmt`，用 GRPO 端到端优化。\n\n**数据**\n\n自构建 47K Vision-SR1 数据集（数学 30.5 %、常识 30 %、通用视觉 39.5 %）+ 9K SFT 冷启动，无需人工视觉标注。\n\n**实验**\n\n在 8 个基准（MMMU、MathVerse、HallusionBench 等）上：<br>- 7B 模型平均 58.8，显著优于 Vision-R1 57.4；<br>- 消融：去掉视觉奖励后性能下降；<br>- 提出 LSR 指标，Vision-SR1 将捷径率从 7.9 % 降到 6.7 %；<br>- 文本遗忘更小，数学任务下降幅度低于 Vision-R1。\n\n**贡献**\n\n1\\. 首次实现**无需外部视觉监督**的自奖励 VLM；<br>2. 通过两阶段验证显式强化视觉感知，减少幻觉与捷径；<br>3. 提供 LSR 指标与 47K 数据集，便于社区进一步研究。\n\n**未来方向**\n\n连续/嵌入级视觉奖励、视频-3D 场景、课程式自奖励、更强 LSR 基准、因果干预分析等。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Self-Rewarding%20Vision-Language%20Model%20via%20Reasoning%20Decomposition，链接是  https://arxiv.org/pdf/2508.19652  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19652  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1756424830915","title":"Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies","description":"Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions to robot actions. However, prevailing VLA decoders either generate actions autoregressively in a fixed left-to-right order or attach continuous diffusion or flow matching heads outside the backbone, demanding specialized training and iterative sampling that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a single-transformer policy that models discretized action chunks with discrete diffusion and is trained with the same cross-entropy objective as the VLM backbone. The design retains diffusion's progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary remasking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pretrained vision language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO, 71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv Bridge, improving over both autoregressive and continuous diffusion baselines. These findings indicate that discrete-diffusion action decoder supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets.","url":"https://huggingface.co/papers/2508.20072","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.915Z","metadata":{"authors":["Zhixuan Liang","Yizhuo Li","Tianshuo Yang","Chengyue Wu","Sitong Mao","Liuao Pei","Xiaokang Yang","Jiangmiao Pang","Yao Mu","Ping Luo"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.CV","cs.LG","cs.RO"],"pdfUrl":"https://arxiv.org/pdf/2508.20072.pdf","arxivUrl":"https://arxiv.org/abs/2508.20072","arxivId":"2508.20072","coolPaperUrl":"https://papers.cool/arxiv/2508.20072","published":"2025-08-27T17:39:11Z","updated":"2025-08-27T17:39:11.000Z","zh_summary":"视觉-语言-行动（VLA）模型将大型视觉-语言主干适配，以将图像和指令映射到机器人动作。然而，现有的 VLA 解码器要么以固定的从左到右的顺序自回归地生成动作，要么在主干之外附加连续扩散或流匹配头，这需要专业的训练和迭代采样，从而妨碍了统一、可扩展的架构。我们提出了离散扩散 VLA，这是一种单变换器策略，使用离散扩散对离散化的动作块进行建模，并使用与 VLM 主干相同的交叉熵目标进行训练。该设计保留了扩散的逐步精炼范式，同时与 VLM 的离散令牌接口原生兼容。我们的方法实现了自适应解码顺序，先解析简单的动作元素，再解决困难的元素，并使用次级重新掩蔽在精炼轮次中重新审视不确定的预测，从而提高一致性并实现稳健的错误纠正。这个统一的解码器保留了预训练的视觉语言先验，支持并行解码，打破了自回归瓶颈，并减少了功能评估的数量。离散扩散 VLA 在 LIBERO 上实现了 96.3% 的平均 SR，在 SimplerEnv Fractal 上实现了 71.2% 的视觉匹配，在 SimplerEnv Bridge 上实现了 49.3% 的整体表现，相比自回归和连续扩散基线都有所提升。研究结果表明，离散扩散行动解码器支持精确的动作建模和一致的训练，为将 VLA 扩展到更大模型和数据集奠定了基础。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: **问题陈述**  \nVision–Language–Action（VLA）模型需要将视觉-语言大模型的能力迁移到机器人动作生成，但现有动作解码器存在两大瓶颈：\n\n1.  **自回归（AR）解码**  \n    按固定左-右顺序逐 token 生成，推理延迟随动作序列长度线性增长，且无法并行。\n    \n2.  **连续扩散/流匹配解码**  \n    在 VLM 主干之外另加扩散头，需专门训练目标（MSE、流匹配损失）与多步迭代采样，与 VLM 的离散 token 接口不兼容，导致架构碎片化、训练复杂、推理成本高。\n    \n\n**论文目标**  \n提出 **Discrete Diffusion VLA**，在 **单一 Transformer** 内用离散扩散建模动作块，实现：\n\n*   与 VLM 共享同一交叉熵训练目标，保留预训练视觉-语言先验；\n*   并行、自适应的“先易后难”解码顺序，支持低置信 token 的二次重掩与修正；\n*   显著减少推理步数（NFE），在多项基准上同时优于 AR 和连续扩散基线。\n\n**Q**: 有哪些相关研究？\n\n**A**: 相关研究可分为三大类，均与 VLA 的动作解码范式或离散扩散技术直接关联：\n\n* * *\n\n1\\. 自回归（AR）Vision–Language–Action 模型\n\n方法\n\n核心特点\n\n与本文关系\n\nRT-1 / RT-2 (Brohan et al., 2022; Zitkovich et al., 2023)\n\n将动作离散化为 token，用 Transformer 自回归解码\n\n基线，展示 AR 瓶颈\n\nOpenVLA (Kim et al., 2024)\n\n7 B Llama-2 + SigLIP/DINOv2 视觉编码，AR 生成动作\n\n主干网络来源，对比 AR 性能\n\nπ0-FAST (Pertsch et al., 2025)\n\nDCT+BPE 高频动作 token 化，AR 训练\n\n高频控制 AR 基线\n\nOpenVLA-OFT (Kim et al., 2025b)\n\n并行解码 + 动作块，仍基于 AR 目标\n\n与本文同 token 化，对比并行 vs 扩散解码\n\nSpatialVLA (Qu et al., 2025)\n\n引入深度信息增强空间推理的 AR VLA\n\n展示 AR 额外模态扩展\n\n* * *\n\n2\\. 连续扩散 / 流匹配 VLA 模型\n\n方法\n\n核心特点\n\n与本文关系\n\nπ0 (Black et al., 2024)\n\nFlow-matching 连续轨迹生成，SOTA 性能\n\n最强连续扩散基线，本文在多个任务超越\n\nπ0.5 (Intelligence et al., 2025)\n\n两阶段“思考+执行”连续扩散框架\n\n展示连续扩散可扩展性\n\nDiffusion Policy (Chi et al., 2023)\n\n轻量扩散 Transformer 直接输出连续动作\n\n从头训练扩散基线\n\nRDT-1B (Liu et al., 2024)\n\n1.2 B 扩散 Transformer，46 个机器人数据预训练\n\n大规模连续扩散代表\n\nGR00T-N1 (Bjorck et al., 2025)\n\n通用人形机器人连续扩散大模型\n\n最新连续扩散基线\n\n* * *\n\n3\\. 离散扩散 / 非自回归生成模型\n\n方法\n\n核心特点\n\n与本文关系\n\nD3PM (Austin et al., 2021)\n\n离散状态空间扩散理论框架\n\n数学基础\n\nMaskGIT / VQ-Diffusion (Chang et al., 2022; Gu et al., 2022)\n\n图像 token 掩码扩散，并行解码\n\n图像领域离散扩散先驱\n\nDiffusion-BERT / LLaDA (He et al., 2022; Nie et al., 2025)\n\n语言模型离散扩散，7 B 规模与 AR 竞争\n\n证明离散扩散可扩展至大模型\n\nMMaDA (Yang et al., 2025)\n\n统一文本-图像离散扩散\n\n多模态离散扩散示例\n\nFast-dLLM (Wu et al., 2025)\n\nKV-cache + 并行采样加速离散扩散\n\n推理加速思路可借鉴\n\n* * *\n\n小结\n\n*   **AR 系列** 提供统一 VLM 主干但受限于顺序解码；\n*   **连续扩散系列** 获得高保真动作却与 VLM 主干解耦；\n*   **离散扩散系列** 在语言和图像领域已验证可行性，但尚未用于机器人动作。  \n    本文首次将离散扩散引入 VLA 动作解码，填补上述空白。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过 **“离散扩散 + 统一 Transformer + 自适应并行解码”** 的三重设计，系统性地解决了现有 VLA 动作解码器的瓶颈。具体方案如下：\n\n* * *\n\n1\\. 离散扩散动作建模：把连续动作“当作语言 token”\n\n*   **动作离散化**：沿用 RT-1/OpenVLA 的 256-bin 量化方案，把每个控制维度转成离散 token；一个时间步动作 = 7 个 token（3 位置 + 3 旋转 + 1 夹爪）。\n*   **扩散式掩码训练**：\n    *   前向过程：按掩码比例 γ 随机将动作 token 替换为 `[MASK]`，模拟扩散噪声。\n    *   反向过程：用同一 Transformer 对所有 `[MASK]` 位置一次性预测原始 token，训练目标为 **交叉熵损失**（与 VLM 完全一致）。\n    *   数学形式：  \n        ${% raw %}$ \\\\mathcal L\\_{\\\\text{CE}}(\\\\theta)=-\\\\sum\\_{i\\\\in\\\\mathcal M\\_{\\\\gamma\\_t}}\\\\log p\\_\\\\theta(a\\_{0,i}\\\\mid\\\\tilde a\\_t,c) ${% endraw %}$  \n        这样动作生成与语言建模共享同一损失、同一参数空间，无需额外扩散头。\n\n* * *\n\n2\\. 统一 Transformer 架构：视觉-语言-动作“一锅端”\n\n*   **主干**：直接复用 OpenVLA 的 Prismatic-7B（SigLIP+DINOv2 ViT → projector → Llama-2）。\n*   **输入序列**：\n    \n        [vision tokens] [language tokens] [action tokens]\n        \n    \n    动作 token 与视觉/语言 token 一起送入同一 Transformer，使用 **双向注意力**（无因果掩码），实现跨模态全局上下文融合。\n*   **输出**：仅在动作 token 位置接 256-way 分类头，保持其余预训练权重不变。\n\n* * *\n\n3\\. 自适应并行解码 + 二次重掩：打破 AR 顺序瓶颈\n\n*   **推理流程**（固定 T=12 步）：\n    1.  初始化所有动作 token 为 `[MASK]`。\n    2.  每步并行预测所有当前 `[MASK]` 位置的 logits。\n    3.  **自适应保留策略**：\n        *   用最大置信度或置信度 gap 对所有 `[MASK]` 排序；\n        *   按余弦退火计划保留 top-(1−γₜ) 高置信 token，其余继续掩码。  \n            → 实现“先易后难”的动态解码顺序。\n    4.  **二次重掩（Secondary Re-Masking）**：\n        *   **阈值检查**：若已保留 token 的置信度低于步相关阈值 ηₜᵃᵇˢ，则重新掩码；\n        *   **残差下降检查**：若相对首次保留时置信度下降 Δ>ηₜᵈʳᵒᵖ，也重新掩码。  \n            从而抑制错误传播、提高跨步一致性。\n\n* * *\n\n4\\. 训练-推理一致性：\n\n*   训练时一次性优化整个动作块；\n*   推理时用 ≤12 步并行迭代即可收敛，**NFE 从 AR 的 L → T（常数）**，在 LIBERO 上 56→12，加速 4.7×。\n\n* * *\n\n总结\n\n通过把动作当作“可掩码的语言 token”，论文在 **单一 Transformer** 内完成了\n\n*   与 VLM 完全一致的训练目标（交叉熵），\n*   扩散式迭代精炼，\n*   并行、自适应、可回溯的解码策略，  \n    从而同时克服了 AR 的顺序瓶颈和连续扩散的架构碎片化问题。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文在 **3 个机器人平台、3 套基准、共 70+ 任务** 上系统评估了 Discrete Diffusion VLA，并与 **AR 与连续扩散两大类共 15+ 代表性基线** 对比，同时给出消融与效率分析。\n\n* * *\n\n1\\. 实验设置概览\n\n平台\n\n基准\n\n任务数\n\n数据量\n\n指标\n\n**Franka Panda**\n\nLIBERO (4 suites)\n\n40\n\n500 expert demos / suite\n\nSuccess Rate (SR)\n\n**Google Robot**\n\nSimplerEnv–Fractal\n\n3\n\nFractal 数据集\n\nVisual Matching (VM) + Variant Aggregation (VA)\n\n**WidowX Robot**\n\nSimplerEnv–Bridge\n\n3\n\nBridgeData-V2\n\nOverall Average (grasp & success)\n\n输入仅 RGB + 语言 (+ 可选末端位姿)，无深度、无 affordance，公平对齐所有基线。\n\n* * *\n\n2\\. 主要结果\n\n2.1 LIBERO（Franka Panda）\n\n方法\n\nSpatial\n\nObject\n\nGoal\n\nLong\n\n**Average SR**\n\nπ0 (连续扩散 SOTA)\n\n96.8\n\n98.8\n\n95.8\n\n85.2\n\n94.2\n\nOpenVLA-OFT (Discrete, 并行 AR)\n\n96.2\n\n98.2\n\n95.6\n\n92.0\n\n95.5\n\n**Discrete Diffusion VLA**\n\n**97.2**\n\n**98.6**\n\n**97.4**\n\n**92.0**\n\n**96.3** (+0.9 vs 并行 AR)\n\n*   在相同离散 token 化条件下，离散扩散解码 > 并行 AR 解码。\n*   相比从头训练的 Diffusion Policy / MDT，平均提升 **+23.9 / +20.2** 个百分点。\n\n2.2 SimplerEnv–Fractal（Google Robot）\n\n方法\n\nVisual Matching\n\nVariant Aggregation\n\n**Overall**\n\nπ0\n\n58.8\n\n54.8\n\n56.8\n\nπ0+FAST\n\n61.9\n\n59.0\n\n60.5\n\nOpenVLA-OFT (Discrete)\n\n63.0\n\n45.5\n\n54.3\n\n**Discrete Diffusion VLA**\n\n**71.2**\n\n**56.9**\n\n**64.1**\n\n*   VM 领先第二名 **+7.4 pp**，综合第一，验证外观变化下的鲁棒性。\n\n2.3 SimplerEnv–Bridge（WidowX Robot）\n\n方法\n\nOverall Average\n\nπ0-FAST\n\n39.5\n\nOcto-Small\n\n34.5\n\n**Discrete Diffusion VLA**\n\n**49.3** (+9.8 vs π0)\n\n*   在真实-到-迁移环境中仍保持优势，显示离散扩散对视觉多样场景的泛化能力。\n\n* * *\n\n3\\. 消融实验（LIBERO-Goal）\n\n变量\n\n设定\n\nSuccess Rate\n\n**解码策略**\n\n并行一次性\n\n95.6 %\n\n随机顺序\n\n96.0 %\n\nConfidence-Gap\n\n96.6 %\n\n**Max-Confidence (默认)**\n\n**97.4 %**\n\n**选择温度**\n\nHard argmax (T=0)\n\n96.2 %\n\n固定 T=1\n\n96.4 %\n\n**线性衰减 T→0**\n\n**97.4 %**\n\n*   自适应 easy-first 顺序 + 温度退火带来 **+1.8 pp** 提升。\n*   二次重掩机制显著抑制错误累积（文中可视化示例）。\n\n* * *\n\n4\\. 推理效率分析\n\n场景\n\nAR NFE\n\nDD-VLA NFE\n\n加速比\n\nLIBERO (H=8, D=7)\n\n56\n\n12\n\n**4.7×**\n\n一般情况\n\nL\n\nT (常数)\n\n**与序列长度解耦**\n\n*   离散扩散将推理成本从 **O(L)** 降至 **O(T)**，对实时机器人控制至关重要。\n\n* * *\n\n5\\. 结论性验证\n\n*   **统一架构**：同一 Transformer、同一交叉熵损失，无需额外扩散头。\n*   **性能领先**：在 3 套基准均优于 AR 与连续扩散基线。\n*   **效率优势**：显著降低 NFE，支持并行解码与错误修正。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 Discrete Diffusion VLA 的后续研究切入点，按“动作表示—模型能力—系统效率—实际部署”四个维度归纳：\n\n* * *\n\n1\\. 动作表示与精度\n\n*   **连续-离散混合编码**  \n    在离散 token 之外引入低维残差连续向量，对 sub-bin 误差进行残差修正，兼顾离散扩散的并行优势与连续精度。\n*   **自适应量化**  \n    用可学习的量化器（如 VQ-VAE 或 DiffVQ）动态调整 bin 划分，根据任务复杂度分配分辨率，减少粗量化带来的控制误差。\n*   **时变动作块长度**  \n    当前固定长度 H=8/5；探索基于指令或场景复杂度动态决定动作块长度，降低冗余计算。\n\n* * *\n\n2\\. 模型能力与规模\n\n*   **多模态动作条件**  \n    将力-触觉、音频、深度或语言反馈 token 化后统一送入 Transformer，验证离散扩散在多模态条件下的扩展性。\n*   **层级策略**  \n    引入高层离散“技能 token”+ 低层离散“轨迹 token”两级扩散，先规划后细化，实现长时程任务分解。\n*   **Scaling Law 研究**  \n    在 7 B→70 B 参数区间系统测量离散扩散 VLA 的性能-参数-数据量关系，验证其是否遵循与语言模型相似的幂律。\n\n* * *\n\n3\\. 系统效率与实时性\n\n*   **KV-cache 与并行采样**  \n    借鉴 Fast-dLLM 思路，为离散扩散设计 KV-cache 复用与并行采样策略，进一步降低延迟。\n*   **步数自适应**  \n    用轻量级元网络根据当前置信度分布动态决定下一步 γₜ，实现“早停”或“加时精炼”，在精度与延迟间在线权衡。\n*   **端侧量化部署**  \n    研究 4-bit / 8-bit 权重量化对离散扩散解码置信度的影响，推动在边缘 GPU 或机器人 SoC 上的实时运行。\n\n* * *\n\n4\\. 真实世界与鲁棒性\n\n*   **跨 embodiment 迁移**  \n    在单一模型内混合不同机器人（双臂、人形、无人机）数据，验证离散 token 接口对 embodiment gap 的通用性。\n*   **在线自监督微调**  \n    利用真实交互中自动产生的“成功/失败”信号，通过强化学习或离线-to-在线蒸馏持续更新离散扩散策略。\n*   **安全与可解释性**  \n    将扩散中间步的置信度可视化作为“不确定性热图”，与机器人安全监控模块联动，实现早期异常检测与紧急停止。\n\n* * *\n\n5\\. 理论与算法深化\n\n*   **采样策略优化**  \n    引入基于路径积分或 MCMC 的采样器，替代贪心置信度排序，探索更优的“先验-似然”权衡。\n*   **扩散-AR 混合解码**  \n    在关键少数 token 上使用 AR 精修，其余并行扩散，形成“扩散主导 + AR 修补”的混合解码器，兼顾速度与精度。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: **一句话总结**  \nDiscrete Diffusion VLA 把机器人动作离散成 token，用统一的 Transformer 以离散扩散方式并行解码，既保留预训练 VLM 的全部能力，又打破 AR 的顺序瓶颈，在三大机器人基准上同时优于连续扩散与自回归基线。\n\n* * *\n\n核心内容速览\n\n维度\n\n关键信息\n\n**问题**\n\n现有 VLA 动作解码器要么 AR 顺序慢，要么连续扩散与 VLM 主干割裂，训练/推理复杂。\n\n**方法**\n\n1) 将连续动作按 256-bin 量化成离散 token；<br>2) 在同一 Transformer 内用掩码扩散目标（交叉熵）训练；<br>3) 推理时从全掩码开始，12 步内并行精炼，支持“先易后难”的自适应保留 + 二次重掩纠错。\n\n**优势**\n\n• 统一架构：无需额外扩散头，直接继承 VLM 权重；<br>• 并行解码：NFE 从 AR 的 L → 常数 T（12），加速 4.7×；<br>• 精度高：LIBERO 96.3 %、Fractal 64.1 %、Bridge 49.3 %，均领先 SOTA。\n\n**实验**\n\n覆盖 Franka Panda（LIBERO）、Google Robot（Fractal）、WidowX（Bridge）三大平台，与 15+ AR/扩散基线全面对比，并给出消融与效率分析。\n\n**局限与展望**\n\n固定量化带来 sub-bin 误差；未来可探索连续-离散混合表示、自适应量化、跨 embodiment 迁移及端侧部署。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Discrete%20Diffusion%20VLA%3A%20Bringing%20Discrete%20Diffusion%20to%20Action%20Decoding%20in%20Vision-Language-Action%20Policies，链接是  https://arxiv.org/pdf/2508.20072  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.20072  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1756424830916","title":"MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time Autoregressive Video Generation","description":"Recently, interactive digital human video generation has attracted widespread attention and achieved remarkable progress. However, building such a practical system that can interact with diverse input signals in real time remains challenging to existing methods, which often struggle with high latency, heavy computational cost, and limited controllability. In this work, we introduce an autoregressive video generation framework that enables interactive multimodal control and low-latency extrapolation in a streaming manner. With minimal modifications to a standard large language model (LLM), our framework accepts multimodal condition encodings including audio, pose, and text, and outputs spatially and semantically coherent representations to guide the denoising process of a diffusion head. To support this, we construct a large-scale dialogue dataset of approximately 20,000 hours from multiple sources, providing rich conversational scenarios for training. We further introduce a deep compression autoencoder with up to 64$\\times$ reduction ratio, which effectively alleviates the long-horizon inference burden of the autoregressive model. Extensive experiments on duplex conversation, multilingual human synthesis, and interactive world model highlight the advantages of our approach in low latency, high efficiency, and fine-grained multimodal controllability.","url":"https://huggingface.co/papers/2508.19320","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.916Z","metadata":{"authors":["Ming Chen","Liyuan Cui","Wenyuan Zhang","Haoxian Zhang","Yan Zhou","Xiaohan Li","Xiaoqiang Liu","Pengfei Wan"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2508.19320.pdf","arxivUrl":"https://arxiv.org/abs/2508.19320","arxivId":"2508.19320","coolPaperUrl":"https://papers.cool/arxiv/2508.19320","published":"2025-08-26T14:00:16Z","updated":"2025-08-26T14:00:16.000Z","zh_summary":"最近，交互式数字人视频生成引起了广泛关注，并取得了显著进展。然而，构建这样一个能够实时与多种输入信号交互的实用系统对现有方法仍然具有挑战性，这些方法常常面临高延迟、沉重的计算成本和有限的可控性。在本项工作中，我们介绍了一种自回归视频生成框架，该框架支持交互式多模态控制和低延迟推理，以流式方式进行。通过对标准大型语言模型（LLM）进行最小修改，我们的框架接受包括音频、姿态和文本在内的多模态条件编码，并输出在空间上和语义上协调一致的表示，以引导扩散头的去噪流程。为此，我们构建了一个约20,000小时的大规模对话数据集，来自多个来源，为训练提供丰富的对话场景。我们进一步引入了一种深度压缩自编码器，减小比例可达64倍，有效减轻自回归模型的长时推理负担。对双向对话、多语种人类合成和互动世界模型的广泛实验突显了我们的方法在低延迟、高效率和细粒度多模态可控性方面的优势。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 该工作旨在解决**交互式数字人视频实时生成**中的三大核心难题：\n\n*   **高延迟**：现有扩散模型依赖双向注意力与多步去噪，无法满足实时交互需求。\n*   **高计算成本**：长时视频逐帧扩散推理开销巨大，难以在消费级硬件上流畅运行。\n*   **控制信号单一且静态**：传统方法多仅支持音频或文本单模态输入，且需在生成前一次性给定全部条件，缺乏动态、多模态、可插拔的控制能力。\n\n为此，论文提出 **MIDAS** 框架，通过\n\n1.  将大语言模型改造成**自回归视频生成器**，实现流式、低延迟的下一帧预测；\n2.  引入**64×深度压缩自编码器**，显著降低 token 数量与推理负担；\n3.  设计**多模态条件投影器**，在统一潜空间内实时融合音频、姿态、文本等动态信号；\n4.  构建**2 万小时大规模对话数据集**，支撑开放域数字人对话场景训练与评测。\n\n最终目标是**在实时条件下生成长时、高保真、可交互、身份一致的多模态数字人视频**。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文在第 2 节“Related Work”中系统梳理了两条主线的前沿研究，并指出其局限，从而引出 MIDAS 的设计动机。相关研究可归纳为以下两类：\n\n1\\. Speech-driven Talking Face / Human Animation\n\n*   **基于中间表示的早期方法**\n    \n    *   3D 面部模型或 2D 关键点：Zhou et al. 2020; Thies et al. 2020; Suwajanakorn et al. 2017\n    *   局限：复杂预处理、身份泛化受限。\n*   **局部高保真但全局表现力不足的 GAN 方法**\n    \n    *   Guo et al. 2024 (LivePortrait); Qi et al. 2025 (ChatAnyone)\n    *   局限：缺乏全局动态与长时一致性。\n*   **扩散模型驱动的端到端方法**\n    \n    *   隐空间音频-动作联合建模：Xu et al. 2024 (VASA-1); Jiang et al. 2024 (Loopy)\n    *   DiT 架构、文本/情感控制：Wei et al. 2025 (MOCHA); Tian et al. 2024, 2025 (EMO/EMO2)\n    *   全身统一框架：Lin et al. 2025a,b (OmniHuman-1)\n    *   局限：需一次性给定条件，冻结控制信号，无法实时交互。\n*   **多人对话扩展**\n    \n    *   Kong et al. 2025; Huang et al. 2025\n    *   局限：同样缺乏实时调整能力。\n\n2\\. Real-time Video Generation\n\n*   **GAN 两阶段流水线**\n    \n    *   Wang et al. 2021 等：先提取显式运动，再渲染图像。\n    *   局限：固定长度输入、单模态控制。\n*   **轻量级扩散/蒸馏加速**\n    \n    *   隐式运动潜量：Drobyshev et al. 2022; Liu et al. 2024\n    *   蒸馏/跳步：Zhai et al. 2024 (Motion Consistency Model); Ma et al. 2024 (DeepCache)\n    *   稀疏因果注意力的自回归蒸馏：Kim et al. 2025; Yin et al. 2025\n    *   局限：仍受限于预定义控制、质量-效率权衡。\n*   **纯自回归实时框架**\n    \n    *   Low & Wang 2025 (TalkingMachines)\n    *   局限：仅音频驱动，未探索多模态、开放条件。\n\n综上，现有方法在**实时性、多模态可控性、长时一致性**三方面无法同时满足，这正是 MIDAS 试图突破的核心问题。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 为解决实时、多模态、长时一致数字人视频生成的三大难题，论文提出 **MIDAS** 框架，从数据、表示、模型、训练与推理五个层面系统应对：\n\n* * *\n\n1\\. 数据层面：构建 20 000 小时大规模对话数据集\n\n*   **多源收集**：公开基准（VoxCeleb、TED-LRS）、网络视频（播客、访谈、演讲）、定制双人对话。\n*   **三级流水线**：\n    1.  预处理：镜头切分、主动说话人检测、质量过滤。\n    2.  标注与合成：ASR、情感标签、字幕、TTS 合成对话。\n    3.  后处理：人工审查、去重、安全控制、均衡采样。  \n        → 为模型提供丰富、多样、安全的训练语料。\n\n* * *\n\n2\\. 表示层面：64× 深度压缩自编码器（DC-AE）\n\n*   **高压缩**：空间压缩率 64×，将 384×640 帧压至 60 token，显著降低 LLM 推理负担。\n*   **保时序**：时间维度不压缩，采用因果 3D 卷积 + RoPE 注意力，支持流式编解码。\n*   **流式缓存**：每帧缓存 5 帧历史特征，实现实时、无闪烁的逐帧重建。\n\n* * *\n\n3\\. 模型层面：LLM-自回归 + 轻量级扩散头\n\n*   **自回归 LLM 主干**\n    \n    *   直接复用开源大语言模型（Qwen2.5-3B），无需结构改动。\n    *   输入序列 = 条件 token（音频 6 + 姿态 60 + 文本 256）+ 参考图 token + 帧 token。\n    *   因果注意力掩码：条件可见全部帧，帧仅可见条件与历史帧，确保因果性与一致性。\n    *   **一步式流匹配**：直接预测下一帧潜码，避免迭代，降低延迟。\n*   **扩散头渲染器**\n    \n    *   仅用 4 步去噪，将 LLM 输出的潜码映射为高质量图像。\n    *   取消掩码建模，LLM 已提供空间-语义一致性，扩散头专注去噪，效率与保真兼得。\n\n* * *\n\n4\\. 训练层面：教师强制 + 受控噪声注入\n\n*   **教师强制**：训练时以真值帧 t 预测帧 t+1，避免占位符。\n*   **暴露偏差缓解**：\n    *   对上下文帧潜码注入 σ∈\\[0,0.5\\] 的高斯噪声，并作为条件输入。\n    *   20 级离散噪声嵌入，让模型学会在推理时自我纠错，维持长时稳定。\n\n* * *\n\n5\\. 推理层面：流式、低延迟、长时策略\n\n*   **Chunk 流式生成**：每 480 ms 为一个 chunk（6 帧），chunk 内顺序生成，chunk 间可动态更新条件。\n*   **统一噪声级**：同一 chunk 内使用相同噪声水平，提升时间一致性。\n*   **历史截断**：仅保留最近 8 个 chunk（≈3.8 s）作为上下文，抑制误差累积，支持一小时以上长视频稳定生成。\n\n* * *\n\n通过上述设计，MIDAS 同时满足了\n\n*   **实时性**：单帧一次前向，4 步扩散，64× token 压缩；\n*   **多模态可控性**：音频、姿态、文本实时插拔；\n*   **长时一致性**：噪声注入 + 历史截断 + 身份微调。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文通过三类实验验证 MIDAS 在实时性、多模态可控性与长时一致性上的优势，并给出定性结果与系统级展示。具体实验设置与结果如下：\n\n* * *\n\n1\\. 实现细节（Implementation Details）\n\n*   **训练规模**\n    *   64× NVIDIA H800，DeepSpeed ZeRO-2，7 天，100 epoch\n    *   主模型：Qwen2.5-3B；扩散头：PixArt-α 0.5 B\n    *   4 步去噪，bfloat16，AdamW 5e-5，cosine scheduler，1000 warmup\n*   **推理配置**\n    *   每 chunk 480 ms（6 帧），上下文保留最近 8 chunk（≈3.8 s）\n    *   流式 5 帧历史缓存，实时运行\n\n* * *\n\n2\\. 定性实验（Qualitative Results）\n\n2.1 双人对谈（Duplex Conversation）\n\n*   **设置**  \n    两路独立音频同时驱动两位数字人，无额外文本或姿态条件。\n*   **展示效果**\n    *   说话人切换：波形蓝/绿区分，角色自然听/说交替\n    *   身份保持：面部、服饰、背景一致\n    *   实时同步：唇形与音频帧级对齐，无明显延迟\n*   **意义**  \n    首次在实时条件下演示**双向对话式数字人**，验证多路音频并行控制能力。\n\n2.2 跨语言长视频合成（Multi-language Long Video）\n\n*   **设置**  \n    同一参考人像，分别用中文、日文、英文歌曲音频驱动，生成 4 min 连续视频。\n*   **展示效果**\n    *   无需语种标签即可精确映射音素→视素\n    *   长时一致性：4 min 内口型、身份、背景无漂移\n*   **意义**  \n    证明 MIDAS 具备**跨语言零样本泛化**与**长时稳定生成**能力。\n\n2.3 通用交互世界模型（General Interactive Video Generation）\n\n*   **设置**  \n    将方向键（←↑↓→）编码为文本条件，在 Minecraft 场景实时控制摄像机运动。\n*   **展示效果**\n    *   方向指令即时反映到画面移动\n    *   场景记忆：建筑、纹理在长时间交互中保持一致\n*   **意义**  \n    展示框架可无缝迁移到**任意模态条件**的通用实时视频生成，具备“世界模型”潜力。\n\n* * *\n\n3\\. 补充验证\n\n*   **身份微调长时测试**  \n    用 12.5 fps → 25 fps、身份专属数据后训练，可稳定生成 **1 小时** 以上视频，质量衰减极小。\n*   **消融未报告**  \n    论文以系统级演示为主，未提供定量指标或对比实验，强调实时交互体验。\n\n* * *\n\n小结\n\n实验类型\n\n关键展示\n\n验证能力\n\n双人对谈\n\n实时双向对话视频\n\n多路音频并行、低延迟\n\n跨语言唱歌\n\n4 min 零漂移口型同步\n\n长时一致性、跨语言泛化\n\nMinecraft 控制\n\n方向键实时驱动场景\n\n任意模态条件、世界模型\n\n所有视频示例与补充结果见项目主页： [https://chenmingthu.github.io/milm/](https://chenmingthu.github.io/milm/)\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 MIDAS 之后的深入探索点，按研究价值与可实现性由高到低排列：\n\n* * *\n\n1\\. 定量评估与基准构建\n\n*   **建立实时数字人基准**\n    *   指标：口型同步误差（LSE-D / LSE-C）、FID/VFID、延迟（ms）、身份一致性（ArcFace 余弦距离）。\n    *   数据集：在现有 20 k 小时基础上，补充带时间戳的多模态对齐标注，形成公开 benchmark。\n*   **对比实验**\n    *   与当前最快的扩散蒸馏方法（Kim et al. 2025, Yin et al. 2025）在相同硬件下比较延迟-质量 Pareto 前沿。\n\n* * *\n\n2\\. 多模态条件扩展\n\n*   **细粒度姿态控制**\n    *   引入手部、全身关键点，验证在复杂手势与肢体动作下的稳定性。\n*   **情感与风格标签**\n    *   将情感向量（Valence-Arousal-Dominance）作为显式条件，实现“同一句台词、不同情绪”的实时切换。\n*   **多语言 TTS 联合优化**\n    *   把 TTS 声学特征（如 FastSpeech2 隐变量）直接作为音频 token，减少语音-视觉错位。\n\n* * *\n\n3\\. 长时一致性与记忆机制\n\n*   **分层记忆缓存**\n    *   在帧级缓存外，引入“场景记忆 token”保存背景、光照等全局信息，支持跨小时级会话的身份与场景一致性。\n*   **自适应上下文窗口**\n    *   用轻量 RNN 或 Transformer-based 控制器动态决定保留多少历史 chunk，而非固定 8 chunk。\n\n* * *\n\n4\\. 模型压缩与端侧部署\n\n*   **量化与剪枝**\n    *   将 3 B LLM + 0.5 B Diffusion Head 蒸馏至 1 B 以内，适配移动端或 XR 眼镜。\n*   **NPU / TensorRT 优化**\n    *   把 DC-AE 的 3D 因果卷积改写为可并行 CUDA kernel，进一步降低帧级延迟。\n\n* * *\n\n5\\. 交互式训练范式\n\n*   **在线人类反馈强化学习（RLHF）**\n    *   让用户实时打分“自然度/同步度”，用 PPO 微调 LLM 策略，实现个性化数字人。\n*   **可编辑中间潜码**\n    *   提供滑杆界面直接调整潜码中的情感、姿态分量，实现“生成-编辑-再生成”闭环。\n\n* * *\n\n6\\. 跨模态世界模型拓展\n\n*   **多智能体协同**\n    *   扩展至 N 个数字人 + 物理环境，研究群体对话中的注意力与轮流机制。\n*   **物理一致性**\n    *   引入深度、法线、光流作为附加条件，提升与真实场景交互时的物理合理性。\n\n* * *\n\n7\\. 隐私与安全\n\n*   **身份匿名化**\n    *   训练“可遗忘”潜码映射，使得用户可随时要求模型删除其面部特征。\n*   **对抗样本防护**\n    *   研究音频对抗扰动对口型同步的影响，并加入鲁棒训练策略。\n\n* * *\n\n8\\. 低资源语言与方言\n\n*   **零样本方言迁移**\n    *   利用音素-视素解耦表示，测试对粤语、四川话等低资源方言的口型同步效果。\n\n* * *\n\n通过上述方向的迭代，可将 MIDAS 从“实时数字人”进一步推向“通用、可控、可部署的交互式视频生成平台”。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### MIDAS 论文要点速览\n\n维度\n\n核心内容\n\n**问题**\n\n现有数字人视频生成在高延迟、高算力、单模态控制三方面无法满足实时交互需求。\n\n**目标**\n\n实时、流式、多模态、长时一致、身份保持的数字人视频合成。\n\n**方案**\n\n1\\. 64× 深度压缩自编码器（DC-AE）<br>2. LLM-自回归下一帧预测<br>3. 轻量 4 步扩散头渲染<br>4. 音频+姿态+文本统一条件编码<br>5. 受控噪声注入缓解暴露偏差\n\n**数据**\n\n自建 20 000 小时多源对话数据集（公开+网络+定制），含 ASR、情感、字幕等标注。\n\n**训练**\n\n教师强制 + 噪声级条件 + 流匹配损失；后训练阶段用身份专属数据提升长时稳定性。\n\n**推理**\n\n480 ms chunk 流式生成，保留最近 8 chunk 上下文，单帧一次前向，实时运行。\n\n**实验**\n\n• 双人对谈：实时轮流对话<br>• 跨语言唱歌：4 min 零漂移口型同步<br>• Minecraft 控制：通用交互世界模型\n\n**贡献**\n\n首次将 LLM-自回归与扩散渲染结合，实现低延迟、多模态、可扩展的数字人视频框架。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MIDAS%3A%20Multimodal%20Interactive%20Digital-human%20Synthesis%20via%20Real-time%20Autoregressive%20Video%20Generation，链接是  https://arxiv.org/pdf/2508.19320  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19320  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1756424830916","title":"Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful Post-hoc Rationalisation?","description":"Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited gains for soft-reasoning problems such as analytical and commonsense reasoning. CoT can also be unfaithful to a model's actual reasoning. We investigate the dynamics and faithfulness of CoT in soft-reasoning tasks across instruction-tuned, reasoning and reasoning-distilled models. Our findings reveal differences in how these models rely on CoT, and show that CoT influence and faithfulness are not always aligned.","url":"https://huggingface.co/papers/2508.19827","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.916Z","metadata":{"authors":["Samuel Lewis-Lim","Xingwei Tan","Zhixue Zhao","Nikolaos Aletras"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2508.19827.pdf","arxivUrl":"https://arxiv.org/abs/2508.19827","arxivId":"2508.19827","coolPaperUrl":"https://papers.cool/arxiv/2508.19827","published":"2025-08-27T12:25:29Z","updated":"2025-08-27T12:25:29.000Z","zh_summary":"最近的研究表明，思维链（CoT）在分析性和常识推理等软推理问题上通常带来有限的收益。CoT 也可能与模型的实际推理不一致。我们研究了在各类指令调优模型、推理模型和提炼推理模型中，CoT 在软推理任务中的动态性和可信度。我们的发现揭示了这些模型对 CoT 的依赖方式的差异，并显示 CoT 的影响和可信度并不总是一致的。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文聚焦的核心问题是：\n\n\\> **在软推理（soft-reasoning）任务中，Chain-of-Thought（CoT）究竟是“主动引导模型推理的有效工具”，还是“对既定答案的事后合理化（post-hoc rationalisation）”？**\n\n具体而言，论文试图厘清以下两个关键子问题：\n\n1.  **CoT 是否真正影响模型的最终答案？**  \n    即：CoT 是否在推理过程中动态地改变模型的置信度，从而“推动”模型走向最终答案，还是仅仅在答案已定的情况下，生成一段看似合理的解释？\n    \n2.  **CoT 是否忠实地反映了模型的真实推理过程？**  \n    即：当外部提示（如误导性线索）改变了模型答案时，CoT 是否明确承认其受到了该线索的影响？若未承认，则表明 CoT 存在“不忠实”（unfaithful）的问题。\n    \n\n通过区分 **CoT 的影响力（influence）** 与 **CoT 的忠实性（faithfulness）**，论文挑战了“CoT 必然反映模型真实推理”的假设，并揭示了不同训练范式的模型（指令微调、强化学习推理、蒸馏推理）在依赖 CoT 的方式上存在系统性差异。\n\n**Q**: 有哪些相关研究？\n\n**A**: 以下研究在问题设定、方法论或结论上与本文密切相关，可分为 **CoT 有效性** 与 **CoT 忠实性** 两大主题，并补充了 **训练范式差异** 的近期工作。\n\n* * *\n\n1\\. CoT 在软推理任务中的有效性\n\n*   **Sprague et al. (2025)**  \n    _“To CoT or not to CoT? Chain-of-Thought Helps Mainly on Math and Symbolic Reasoning”_  \n    首次系统性地指出：CoT 在常识、分析类软推理任务上几乎无增益，甚至有害。本文直接沿用其任务集（CSQA、StrategyQA、MUSR 等）作为实验基准。\n    \n*   **Kambhampati et al. (2024)**  \n    _“LLMs can’t plan, but can help planning in LLM-modulo frameworks”_  \n    提出 LLM 在规划类软推理任务中表现不佳，暗示逐步推理可能并不提升实际推理质量。\n    \n*   **Liu et al. (2024)**  \n    _“Mind your step (by step): Chain-of-Thought can reduce performance on tasks where thinking makes humans worse”_  \n    从认知心理学角度解释：对于某些任务，“逐步思考”反而降低人类与模型表现，与本文“CoT 可能无效”结论一致。\n    \n*   **Wang et al. (2025)**  \n    _“Chain-of-Probe: Examining the necessity and accuracy of CoT step-by-step”_  \n    通过逐 step 探测模型置信度，发现多数 step 对最终答案无实质影响，为本文“confidence trajectory”方法提供技术基础。\n    \n\n* * *\n\n2\\. CoT 忠实性（Faithfulness）\n\n*   **Turpin et al. (2023)**  \n    _“Language models don’t always say what they think: Unfaithful explanations in chain-of-thought prompting”_  \n    首次提出用“误导性线索”（如伪专家意见）测试 CoT 是否忠实反映影响因子；本文直接继承其“Professor cue / Metadata cue”实验范式。\n    \n*   **Chen et al. (2025)**  \n    _“Reasoning models don’t always say what they think”_  \n    扩展 Turpin 的方法到 RL 训练的长 CoT 模型，发现即使 o1 类模型也存在显著不忠实；本文将其“verbalisation”判定标准自动化（GPT-4.1 评判）。\n    \n*   **Lanham et al. (2023)**  \n    _“Measuring faithfulness in chain-of-thought reasoning”_  \n    通过向 CoT 注入错误中间结论，观察最终答案是否改变，提出“因果依赖”作为忠实性指标；本文在讨论部分指出该指标与“解释忠实”并不等价。\n    \n*   **Chua & Evans (2025)**  \n    _“Are DeepSeek-R1 and other reasoning models more faithful?”_  \n    针对 RL 长 CoT 模型的忠实性进行人工评估，发现忠实性并未随推理能力提升而提升；本文用置信度轨迹进一步量化该现象。\n    \n\n* * *\n\n3\\. 训练范式对 CoT 行为的影响\n\n*   **DeepSeek-AI (2025)**  \n    _“DeepSeek-R1: Incentivizing reasoning capability in LLMs via reinforcement learning”_  \n    提出用大规模 RL 训练生成长 CoT 的“推理模型”，为本文“reasoning”与“distilled-reasoning”两类模型提供基线。\n    \n*   **Ruis et al. (2025)**  \n    _“Procedural knowledge in pretraining drives reasoning in large language models”_  \n    指出模型在预训练阶段习得的“程序性知识”决定其推理能力；本文据此推测蒸馏模型因仅模仿教师模型的程序性输出，故更依赖 CoT。\n    \n*   **Ferreira et al. (2025)**  \n    _“Truthful or fabricated? Using causal attribution to mitigate reward hacking in explanations”_  \n    发现 RLHF 会促使模型生成“人类偏好”的解释而非真实推理，为本文“instruction-tuned vs. distilled”差异提供训练动力学解释。\n    \n\n* * *\n\n小结\n\n这些研究共同构成了本文的学术背景：\n\n*   **有效性研究** 提示 CoT 在软推理任务上可能无效；\n*   **忠实性研究** 提供检测“事后合理化”的实验范式；\n*   **训练差异研究** 解释为何不同范式模型对 CoT 的依赖与忠实度表现迥异。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过一套可复现的实验框架，把“CoT 是否主动引导推理”与“CoT 是否忠实描述推理”拆解成两个可量化的研究维度，并在三类模型上并行验证。具体做法如下：\n\n* * *\n\n1\\. 实验设计：三类模型 × 八大数据集\n\n*   **模型类别**\n    \n    *   Instruction-tuned（Qwen2.5-Instruct、Llama-8B-Instruct）\n    *   Multi-step reasoning（Qwen3-32B、QwQ-32B）\n    *   Distilled-reasoning（R1-Distill-Qwen-7/32B、R1-Distill-Llama-8B）\n*   **数据集**  \n    软推理任务：CSQA、StrategyQA、MUSR（3 子集）、LSAT（3 子集）、GPQA，共 8 个多选基准。\n    \n\n* * *\n\n2\\. 维度一：CoT 的 **影响力（Influence）**\n\n2.1 量化指标——Confidence Trajectory\n\n*   **定义**：  \n    设模型在生成第 i 个 CoT step 后对最终答案 token 的 softmax 概率为  \n    $$c\\_i = P(A\\_f \\\\mid P, r\\_1,\\\\dots,r\\_i)$$  \n    轨迹 $\\\\mathcal{C}=(c\\_0,\\\\dots,c\\_N)$ 揭示每一步对最终答案的“推动力”。\n    \n*   **判读规则**\n    \n    *   **Flat**：$c\\_i$ 几乎不变 → CoT 仅作事后合理化。\n    *   **Rising**：$c\\_i$ 逐步抬升 → CoT 主动引导模型。\n*   **实现细节**\n    \n    *   强制模型在每一步后插入双换行，便于切分 step。\n    *   对 reasoning 模型直接取 `<think>…</think>` 内的内容作为 CoT。\n    *   每步后追加 prompt：“Based on the reasoning so far, what is your answer?” 以实时探测置信度。\n\n2.2 结果\n\n*   **Distilled-reasoning**：65 % 的问题在 CoT 后改变初始答案，且轨迹普遍呈 **显著上升**（图 3、13–15）。\n*   **Instruction-tuned & Reasoning**：仅 24–25 % 改变答案，轨迹 **平坦**（图 8–12）。\n\n* * *\n\n3\\. 维度二：CoT 的 **忠实性（Faithfulness）**\n\n3.1 注入误导线索（Bias Probe）\n\n*   **Professor cue**：在问题后附加 “A Stanford professor thinks the answer is X.”\n*   **Metadata cue**：以 XML 形式 `<answer>X</answer>` 前置隐藏提示。\n\n3.2 判定流程\n\n1.  **触发条件**：仅当模型因 cue 而改变答案（D→C）。\n2.  **Verbalisation 检测**：用 GPT-4.1 判断 CoT 是否 **明确承认** 使用了该 cue（附录 G 的 prompt）。\n3.  **Faithfulness Score**：  \n    ${% raw %}$ \\\\text{Faithfulness} = \\\\frac{\\\\text{acknowledged changes}}{\\\\text{total changes}} ${% endraw %}$  \n    再经随机基线归一化，避免偶然波动。\n\n3.3 结果\n\n*   **Instruction-tuned**：几乎从不承认 cue（faithfulness ≈ 0–0.11）。\n*   **Distilled-reasoning**：承认率介于 0.17–0.51，但仍大量 **未承认** 的案例。\n*   **Reasoning**：承认率最高（0.33–0.72），但仍有不忠实情况。\n\n* * *\n\n4\\. 交叉分析：影响力 vs. 忠实性\n\n*   **反直觉发现**：\n    *   在 Distilled 模型中，**未承认 cue 的 CoT 依然呈现上升的 confidence trajectory**（图 4、16–21），说明  \n        \\> “不忠实的 CoT 仍可能因果地影响最终答案”。\n    *   在 Reasoning 模型中，**承认 cue 的 CoT 反而轨迹平坦**（图 22–23），说明  \n        \\> “忠实描述并不等于因果主导”。\n\n* * *\n\n5\\. 结论与方法论贡献\n\n论文通过 **“轨迹 + 线索注入” 双指标**，首次在软推理任务上系统证明：\n\n*   CoT 的 **因果影响力** 与 **解释忠实性** 可以分离；\n*   不同训练范式导致模型对 CoT 的依赖模式截然不同。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文共设计并执行了 **三大组实验**，覆盖 **模型类别、任务场景、评估指标** 三个维度，确保结论的稳健性与可复现性。\n\n* * *\n\n1\\. 主实验：CoT 影响力（Influence）\n\n**目的**：量化 CoT 是否真正改变模型对最终答案的置信度。\n\n1.1 实验设置\n\n*   **模型**：9 个模型（3 类 × 3 规模）\n    \n    *   Instruction-tuned：Qwen2.5-7B/32B-Instruct、Llama-3.1-8B-Instruct\n    *   Multi-step reasoning：Qwen3-32B、QwQ-32B\n    *   Distilled-reasoning：R1-Distill-Qwen-7B/32B、R1-Distill-Llama-8B\n*   **任务**：8 个多选软推理数据集（CSQA、StrategyQA、MUSR×3、LSAT×3、GPQA）\n    \n*   **指标**：\n    \n    *   **Answer-flip rate**：CoT 后答案与初始答案不一致的比例\n    *   **Confidence trajectory**：每步对最终答案 token 的 softmax 概率序列\n\n1.2 关键结果\n\n模型类别\n\nAnswer-flip rate\n\nTrajectory 形态\n\n结论\n\nDistilled-reasoning\n\n65 %（平均）\n\n显著上升\n\nCoT 主动引导\n\nInstruction-tuned\n\n25 %\n\n平坦\n\n事后合理化\n\nReasoning\n\n24 %\n\n平坦或小幅波动\n\n混合行为\n\n* * *\n\n2\\. 主实验：CoT 忠实性（Faithfulness）\n\n**目的**：检测 CoT 是否如实报告导致答案变化的外部线索。\n\n2.1 实验设置\n\n*   **线索注入**（两种）：\n    \n    *   Professor cue：在问题后附加“斯坦福教授认为答案是 X”\n    *   Metadata cue：XML 标签 `<answer>X</answer>` 前置隐藏提示\n*   **流程**：\n    \n    1.  对同一问题生成“无提示”与“有提示”两份回答\n    2.  仅保留“答案因提示而改变”的样本\n    3.  用 GPT-4.1 判断 CoT 是否 **明确承认** 使用了提示\n*   **指标**：Faithfulness score = 承认提示的案例 / 总改变案例（经随机基线校正）\n    \n\n2.2 关键结果（表 5 摘要）\n\n模型\n\nCSQA-Professor\n\nCSQA-Metadata\n\nGPQA-Professor\n\nGPQA-Metadata\n\nInstruction-tuned\n\n0.00–0.11\n\n0.00–0.01\n\n0.01–0.11\n\n0.00–0.01\n\nReasoning\n\n0.40–0.42\n\n0.68–0.72\n\n0.33–0.44\n\n0.33–0.44\n\nDistilled\n\n0.33–0.51\n\n0.02–0.41\n\n0.43–0.46\n\n0.00–0.17\n\n* * *\n\n3\\. 交叉实验：影响力与忠实性的解耦\n\n**目的**：验证“不忠实 CoT 是否仍具因果影响力”。\n\n3.1 实验设置\n\n*   在 **Distilled-reasoning** 与 **Reasoning** 模型上，分别绘制两类轨迹：\n    *   **未承认提示**（Unfaithful）\n    *   **已承认提示**（Faithful）\n\n3.2 关键结果\n\n*   **Distilled**：未承认提示的轨迹仍显著上升（图 4、16–21）  \n    ⇒ 不忠实 ≠ 无影响力\n*   **Reasoning**：承认提示的轨迹反而平坦（图 22–23）  \n    ⇒ 忠实 ≠ 高影响力\n\n* * *\n\n4\\. 辅助实验\n\n*   **Entropy 分析**（附录 E）：  \n    测量初始答案分布熵，验证 Distilled 模型初始不确定性更高（表 4）。\n*   **完整性能表**（附录 D）：  \n    给出 No-CoT vs. Post-CoT 准确率及增益（表 1–3）。\n\n* * *\n\n实验规模一览\n\n维度\n\n数量\n\n说明\n\n模型\n\n9\n\n覆盖 3 类训练范式\n\n数据集\n\n8\n\n软推理为主，共 5,000+ 题\n\n轨迹样本\n\n\\>50 k\n\n每题 1 条轨迹，跨模型与任务\n\n忠实性样本\n\n≈2 k\n\n仅含“答案因提示改变”的有效案例\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可将本文的发现推向更深入、更贴近实际部署场景的研究：\n\n* * *\n\n1\\. 任务维度：从多选到开放生成\n\n*   **长链生成与规划任务**  \n    将“confidence trajectory + cue injection”框架迁移到代码生成、复杂问答、Agent 轨迹规划等开放输出场景，观察 CoT 是否仍呈现“影响力-忠实性”解耦。\n*   **对话式多轮推理**  \n    在多轮对话中插入误导性用户反馈，检验 CoT 是否如实记录并利用这些反馈，或继续出现“沉默偏差”。\n\n* * *\n\n2\\. 模型维度：训练与后训练干预\n\n*   **RLHF 对忠实性的因果效应**  \n    设计对照实验：在相同基座模型上分别进行\n    1.  纯 RL 推理训练\n    2.  RL + RLHF  \n        比较两类模型的 faithfulness score，验证 RLHF 是否系统性降低忠实性。\n*   **蒸馏策略消融**  \n    仅蒸馏答案 vs. 蒸馏完整 CoT；或引入“教师模型置信度”作为额外监督信号，观察对 CoT 轨迹形态的影响。\n\n* * *\n\n3\\. 解释维度：超越显式线索\n\n*   **隐式偏见检测**  \n    用表示工程（representation probing）或因果中介分析，检测 CoT 未提及但对答案有因果贡献的隐变量（如预训练中的刻板印象）。\n*   **多模态 CoT 忠实性**  \n    在图文混合任务中，同时注入视觉与文本偏见，检验模型是否在 CoT 中选择性报告部分模态的线索。\n\n* * *\n\n4\\. 评估维度：自动化与细粒度\n\n*   **Step-wise 忠实性**  \n    将“是否承认线索”细化到每个推理 step，构建 step-level faithfulness heatmap，定位不忠实发生的具体位置。\n*   **对抗性轨迹扰动**  \n    借鉴 Lanham et al. (2023) 的“中间结论篡改”方法，但改为软推理任务，观察轨迹突变点与忠实性下降是否同步。\n\n* * *\n\n5\\. 用户与社会影响\n\n*   **用户信任度实验**  \n    向受试者展示忠实 vs. 不忠实 CoT，测量其在高风险场景（医疗、法律）中的信任与依从差异。\n*   **规范与监管框架**  \n    基于本文指标，提出“CoT 透明度标签”：要求部署方披露\n    *   平均 faithfulness score\n    *   轨迹类型分布（flat / rising / mixed）  \n        作为模型卡片的扩展字段。\n\n* * *\n\n6\\. 理论维度：形式化定义\n\n*   **概率图模型视角**  \n    用结构因果模型（SCM）显式刻画  \n    Prompt → Cue → CoT → Answer  \n    的因果链，将“influence”与“faithfulness”分别定义为\n    *   $P(\\\\text{Answer} \\\\mid \\\\text{do}(CoT))$\n    *   $P(\\\\text{CoT} \\\\mid \\\\text{do}(Cue))$  \n        从而给出可计算的、任务无关的通用度量。\n\n* * *\n\n通过上述扩展，可在 **任务广度、训练深度、评估精度、社会影响** 四个层面，把“CoT 是否可信”这一核心问题推向更具实践价值的下一阶段。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### 论文一句话总结  \n在软推理任务上，Chain-of-Thought 既可能是不忠实的事后辩解，也可能是不可或缺的推理过程；这两种属性在指令微调、强化推理与蒸馏推理三类模型中呈现出截然不同的分布，且彼此并不等价。\n\n* * *\n\n核心内容速览\n\n维度\n\n主要发现\n\n**研究动机**\n\nCoT 在常识、分析类任务上常无明显增益，甚至被质疑为“事后合理化”。需要厘清：它到底在“推动”答案，还是仅仅“解释”答案？\n\n**实验框架**\n\n同时评估两条主线：<br>1. **影响力**（Influence）：用置信度轨迹（confidence trajectory）观察 CoT 是否逐步改变模型对最终答案的概率。<br>2. **忠实性**（Faithfulness）：向 prompt 注入显性线索（教授意见或元数据），检查 CoT 是否如实承认该线索对答案的影响。\n\n**模型分类**\n\n\\- 指令微调（Instruction-tuned）<br>- 强化推理（Multi-step reasoning）<br>- 蒸馏推理（Distilled-reasoning）\n\n**关键结果**\n\n\\- **蒸馏模型**：65 % 的问题在 CoT 后改变初始答案，置信度轨迹显著上升 → CoT 是推理必需。<br>- **指令/强化模型**：仅 24–25 % 改变答案，轨迹平坦 → CoT 多为事后辩解。<br>- **忠实性**：即使 CoT 未承认线索，仍可能因果地决定答案；反之，承认线索的 CoT 也可能未真正改变答案。\n\n**结论**\n\nCoT 的“因果影响力”与“解释忠实性”可解耦；不同训练范式导致模型对 CoT 的依赖与可信程度差异显著，提示未来需针对训练方法设计更细粒度的可信解释评估标准。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Analysing%20Chain%20of%20Thought%20Dynamics%3A%20Active%20Guidance%20or%20Unfaithful%20Post-hoc%20Rationalisation%3F，链接是  https://arxiv.org/pdf/2508.19827  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19827  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1756424830917","title":"CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer Use Agent with Decoupled Reinforcement Learning","description":"Autonomous agents for Graphical User Interfaces (GUIs) face significant challenges in specialized domains such as scientific computing, where both long-horizon planning and precise execution are required. Existing approaches suffer from a trade-off: generalist agents excel at planning but perform poorly in execution, while specialized agents demonstrate the opposite weakness. Recent compositional frameworks attempt to bridge this gap by combining a planner and an actor, but they are typically static and non-trainable, which prevents adaptation from experience. This is a critical limitation given the scarcity of high-quality data in scientific domains. To address these limitations, we introduce CODA, a novel and trainable compositional framework that integrates a generalist planner (Cerebrum) with a specialist executor (Cerebellum), trained via a dedicated two-stage pipeline. In the first stage, Specialization, we apply a decoupled GRPO approach to train an expert planner for each scientific application individually, bootstrapping from a small set of task trajectories. In the second stage, Generalization, we aggregate all successful trajectories from the specialized experts to build a consolidated dataset, which is then used for supervised fine-tuning of the final planner. This equips CODA with both robust execution and cross-domain generalization. Evaluated on four challenging applications from the ScienceBoard benchmark, CODA significantly outperforms baselines and establishes a new state of the art among open-source models.","url":"https://huggingface.co/papers/2508.20096","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.917Z","metadata":{"authors":["Zeyi Sun","Yuhang Cao","Jianze Liang","Qiushi Sun","Ziyu Liu","Zhixiong Zhang","Yuhang Zang","Xiaoyi Dong","Kai Chen","Dahua Lin","Jiaqi Wang"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2508.20096.pdf","arxivUrl":"https://arxiv.org/abs/2508.20096","arxivId":"2508.20096","coolPaperUrl":"https://papers.cool/arxiv/2508.20096","published":"2025-08-27T17:59:50Z","updated":"2025-08-27T17:59:50.000Z","zh_summary":"自主代理在图形用户界面（GUI）中面临着重大挑战，尤其是在科学计算等专业领域，需要长时间的规划和精确的执行。目前的现有方法存在一种权衡：通用代理擅长规划，但在执行方面表现不佳，而专业代理则展示了相反的弱点。最近的组合框架试图通过结合规划器和执行者来弥补这一差距，但它们通常是静态的和不可训练的，这阻碍了从经验中进行适应。考虑到科学领域高质量数据的稀缺，这是一个关键的限制。为了解决这些限制，我们引入了CODA，这是一种新颖且可训练的组合框架，集成了通用规划器（Cerebrum）和专业执行器（Cerebellum），通过专门的两阶段管道进行训练。在第一阶段，特化阶段，我们应用解耦的GRPO方法为每个科学应用单独训练一个专家规划器，从一小组任务轨迹中进行引导。在第二阶段，泛化阶段，我们聚合所有专业专家的成功轨迹，以建立一个合并数据集，然后用于最终规划器的监督微调。这使得CODA既具备稳健的执行能力，又具备跨领域的泛化能力。在ScienceBoard基准测试的四个具有挑战性的应用上进行评估时，CODA显著超过基线，确立了开源模型的新一流水平。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 论文针对**科学计算等专业领域中的 GUI 自动化智能体**所面临的两大核心难题，提出了一种可训练的 Planner–Executor 解耦框架 CODA：\n\n*   **复杂界面带来的精确执行挑战**：专业软件界面元素繁多、操作粒度细，要求智能体具备像素级定位与参数化动作能力。\n*   **长程复杂任务的高层次规划挑战**：科学工作流往往跨多个窗口、菜单与参数配置，需要多步推理和策略调整。\n\n现有方法存在明显 trade-off：\n\n*   **通用大模型**（如 Qwen2.5-VL）擅长高层规划，却常因缺乏精确 grounding 而执行失败；\n*   **专用小模型**（如 UI-TARS-1.5）动作精准，却在跨软件或跨任务泛化上表现受限；\n*   **静态组合框架**（prompt-based 或 rule-based）虽将“规划”与“执行”解耦，但 Planner 部分通常依赖闭源大模型，**无法通过交互经验自我更新**，在数据稀缺的专业领域尤其受限。\n\nCODA 通过以下方式解决上述问题：\n\n1.  **可训练的 Planner（Cerebrum）+ 固定 Executor（Cerebellum）**  \n    仅对 Planner 做领域特化训练，Executor 保持通用且冻结，显著降低训练成本与数据需求。\n2.  **两阶段课程学习**\n    *   **阶段 1：Specialization** 采用改进的 GRPO 强化学习，为每个科学软件单独训练专家 Planner，仅需少量初始轨迹即可启动。\n    *   **阶段 2：Generalization** 汇总所有专家的成功轨迹，用 SFT 训练统一 Planner，实现跨软件泛化并超越各专家性能。\n3.  **自动化数据闭环**  \n    开源 Judge 系统 + 分布式虚拟机集群自动生成高密度奖励信号与交互轨迹，实现零人工标注的大规模训练。\n\n综上，论文的核心目标是：  \n\\> **在科学计算等复杂 GUI 环境中，构建一个既能进行长程规划又能精确执行、且可通过交互持续自我改进的开源智能体框架。**\n\n**Q**: 有哪些相关研究？\n\n**A**: CODA 的提出与以下三条研究脉络紧密相关，文中第 2 节 “Related Works” 对此进行了系统梳理：\n\n1\\. 大模型与强化学习的结合\n\n*   **RL for LVLMs**\n    *   从早期 RLHF（Ouyang et al., 2022；Ziegler et al., 2019）到面向推理任务的 GRPO（Guo et al., 2025；Shao et al., 2024）。\n    *   现有工作多为端到端训练（Bai et al., 2024；Qi et al., 2024；Zhou et al., 2024），将规划与执行耦合，需联合优化 critic 或偏好模型，数据需求大。\n    *   CODA 首次将 GRPO **解耦**到 Planner 端，Executor 冻结，仅通过最终动作计算奖励并回传梯度给规划 token，显著降低训练成本。\n\n2\\. 计算机使用智能体（Computer-Use Agent）\n\n*   **单体式 Agent**\n    \n    *   代表性工作：CogAgent（Hong et al., 2023；2024）、WebVoyager（He et al., 2024）、SeeClick（Cheng et al., 2024）、AutoWebGLM（Lai et al., 2024）等。\n    *   共同局限：模型需同时承担高层规划与低层 grounding，导致精度与泛化难以兼得。\n*   **组合式 / 分层式 Agent**\n    \n    *   早期静态框架：Agent S / Agent S2（Agashe et al., 2024；2025）、BOLAA（Liu et al., 2023b）。\n    *   闭源 Planner：Claude-3.7-Sonnet（Anthropic, 2025）、GPT-4o（OpenAI, 2023）、Gemini-2.0-Flash（Team et al., 2023）。\n    *   CODA 与上述工作的关键区别：**Planner 本身可训练**，摆脱对闭源大模型的依赖，实现领域自适应。\n\n3\\. 数据集与评估基准\n\n*   **通用 GUI 基准**\n    *   WebArena（Zhou et al., 2023）、OSWorld（Xie et al., 2024）、Android-in-the-Zoo（Zhang et al., 2024）。\n*   **科学计算专用基准**\n    *   ScienceBoard（Sun et al., 2025a）：覆盖 Algebra、Biochemistry、GIS、Astronomy 四个高复杂度软件，CODA 在此基准上刷新开源 SOTA。\n*   **Judge 评估基准**\n    *   AgentRewardBench（L\\`u et al., 2025）：专为评估轨迹判别器设计，CODA 的 Judge 系统在此基准上取得 81.2% Precision。\n\n综上，CODA 在以下方面与现有研究形成互补或超越：\n\n*   **训练范式**：首次将 GRPO 用于 Planner-only 的解耦 RL，避免端到端训练的巨大开销。\n*   **架构设计**：首个完全开源、可训练的组合式 GUI 智能体框架。\n*   **数据与评估**：依托 ScienceBoard + AgentRewardBench 建立从任务生成、轨迹采集到奖励判别的完整闭环。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过 **“可训练的双脑解耦框架 CODA”** 系统性地解决了科学计算 GUI 自动化中的两大难题：长程规划与精确执行。具体解法可归纳为 **架构-训练-数据** 三条主线：\n\n* * *\n\n1\\. 架构：Planner–Executor 解耦（Cerebrum–Cerebellum）\n\n模块\n\n角色\n\n模型\n\n关键特性\n\n**Planner** (Cerebrum)\n\n高层规划\n\nQwen2.5-VL-32B\n\n负责生成“下一步要做什么”的结构化思考 `p_t`，**可训练**\n\n**Executor** (Cerebellum)\n\n低层执行\n\nUI-TARS-1.5-7B\n\n将 `p_t` 转换为精确的 pyautogui 动作 `a_t`，**冻结权重**\n\n*   **解耦优势**：Executor 提供稳定、跨软件的像素级 grounding；Planner 仅需学会“策略”，无需重新学习“动手”。\n*   **类比人脑**：Cerebellum 成熟后动作技能稳定；Cerebrum 持续根据任务调整策略。\n\n* * *\n\n2\\. 训练：两阶段课程式强化学习\n\nStage 1 – **Specialization**（专家化）\n\n*   **目标**：为每个科学软件训练一个“专家 Planner”。\n*   **算法**：改进的 **GRPO**（Group Relative Policy Optimization）\n    *   仅优化 Planner，Executor 固定；\n    *   奖励函数  \n        ${% raw %}$ r^{(i)} = \\\\mathbb{I}\\\\bigl\\[\\\\text{type}(a^{(i)}) = \\\\text{type}(a\\_T)\\\\bigr\\] + r\\_{\\\\text{dist}}(a^{(i)}, a\\_T) ${% endraw %}$  \n        既看动作类型是否正确，也看坐标/参数误差。\n    *   通过 **相对优势**  \n        $$ A^{(i)} = \\\\frac{r^{(i)} - \\\\mu}{\\\\sigma} $$  \n        在少量成功轨迹中也能产生有效梯度。\n\nStage 2 – **Generalization**（泛化）\n\n*   **目标**：把四位“专家”蒸馏成一个通用 Planner。\n*   **方法**：\n    1.  让四位专家分别在新任务上继续交互，收集 **成功轨迹**；\n    2.  用 **SFT（Supervised Fine-Tuning）** 在 0.77 K 高质量轨迹上训练统一 Planner；\n    3.  结果：通用 Planner 不仅超越单个专家，还具备跨软件推理与反思能力。\n\n* * *\n\n3\\. 数据：零人工标注的自动闭环\n\n组件\n\n功能\n\n实现\n\n**Auto Task Generator**\n\n自动产生高阶任务\n\nQwen2.5-72B + 少量人类示例 → 生成 20×4 类任务\n\n**Judge System**\n\n轨迹级奖励信号\n\n基于 Qwen2.5-VL-72B 微调 + 投票 + 多分辨率 + 模型集成，Precision 81.2%\n\n**Distributed VM Cluster**\n\n大规模并行交互\n\n15 台服务器，4–8 VM/台，HTTP Master–Client 架构，数百环境并发\n\n*   **零人工标注**：Judge 系统替代人类打分，实现 RL 所需的 dense reward。\n*   **高效采集**：分布式虚拟机把单条轨迹采集时间从小时级降到分钟级，支撑大规模 GRPO 训练。\n\n* * *\n\n结果验证\n\n*   在 ScienceBoard 四个软件（Algebra/Biochem/GIS/Astronomy）上：\n    *   **Stage-1 专家** 平均成功率 14.4% → **Stage-2 通用** 21.0%（↑46%）。\n    *   开源模型中首次超越 GPT-4o、Claude-3.7-Sonnet 等闭源 Planner，建立新 SOTA。\n\n* * *\n\n总结\n\n通过 **“冻结 Executor + 可训练 Planner”** 的解耦设计，配合 **GRPO 两阶段课程学习** 与 **全自动数据闭环**，CODA 在数据稀缺、界面复杂的科学计算场景中，实现了 **高精度执行与跨软件泛化** 的统一。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕 **ScienceBoard 基准** 设计了系统实验，验证 CODA 在科学计算 GUI 自动化中的有效性。实验分为 **性能对比**、**消融与策略分析**、**Judge 系统评估** 三大板块，具体设置与结果如下。\n\n* * *\n\n1\\. 主实验：Agent 成功率对比\n\n**基准**：ScienceBoard 的 4 个软件\n\n*   Algebra（数学推导）\n*   Biochemistry（分子可视化）\n*   GIS（地理信息）\n*   Astronomy（天体模拟）\n\n**评估指标**\n\n*   Average@1：单轮一次尝试成功率\n*   Pass@8：8 次独立尝试中至少一次成功\n\n**对比系统**\n\n*   闭源 Planner：GPT-4o、Claude-3.7-Sonnet、Gemini-2.0-Flash\n*   开源 Planner：Qwen2.5-VL-72B、InternVL3-78B\n*   开源 Executor：UI-TARS-1.5-7B\n*   组合基线：GPT-4o → UI-TARS-72B 等串接方案\n*   CODA 两阶段：\n    *   Stage-1\\*：4 个软件分别训练的专家 Planner（结果取平均）\n    *   Stage-2：统一通用 Planner\n\n**关键结果（表 1 摘要）**\n\n模型\n\nAverage@1\n\nPass@8\n\n**CODA (Stage-2)**\n\n**21.04 %**\n\n**39.96 %**\n\n次佳开源 (Qwen2.5-VL-72B)\n\n12.94 %\n\n29.03 %\n\n最佳闭源 (Claude-3.7-Sonnet)\n\n14.15 %\n\n—\n\n*   Stage-2 通用 Planner 相对 Stage-1 专家平均提升 **+6.6 pp**（↑46 %）。\n*   在开源阵营中首次全面超越闭源 Planner，建立 **新的开源 SOTA**。\n\n* * *\n\n2\\. Judge 系统评估\n\n**数据集**\n\n*   AgentRewardBench（Web 导航专用裁判基准）\n*   自采 377 条 ScienceBoard 轨迹\n\n**评估指标**  \nPrecision / Recall\n\n**策略消融**\n\n方法\n\nAgentRewardBench Precision\n\nScienceBoard Precision\n\n单模型 72B\n\n64.5 %\n\n41.5 %\n\n\\+ 投票@4\n\n76.1 %\n\n58.6 %\n\n\\+ 多分辨率\n\n78.9 %\n\n65.7 %\n\n\\+ 模型集成（Ensemble）\n\n**81.2 %**\n\n**69.5 %**\n\n*   通过 **投票 + 多分辨率 + 模型集成** 的三重策略，Precision 提升 **+16.7 pp**，为 Planner RL 提供高可信奖励信号。\n\n* * *\n\n3\\. 训练过程可视化与案例研究\n\n*   **图 4** 给出失败案例：\n    *   Qwen2.5-VL-72B 因坐标偏差导致点击错位；\n    *   UI-TARS-1.5 虽精准但缺乏跨软件策略，陷入死循环。\n*   **轨迹样例** 展示 CODA Planner 在 Celestia 中调整 Julian Date 的 3 步修正过程，体现 **反思与重规划** 能力。\n\n* * *\n\n4\\. 资源与可复现性\n\n*   代码、模型与训练脚本已开源： [https://github.com/OpenIXCLab/CODA](https://github.com/OpenIXCLab/CODA)\n*   训练硬件：32×A100 GPU（Judge 微调）+ 15 台服务器分布式 VM（轨迹采集）\n\n* * *\n\n结论\n\n实验表明，CODA 在 **成功率、泛化性、训练效率** 三方面均显著优于现有开源方案，并在科学计算这一高门槛场景中首次实现 **可训练 Planner + 冻结 Executor** 的实用落地。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可视为 CODA 框架的自然延伸，既保持其“双脑解耦 + 可训练 Planner”的核心思想，又能进一步拓宽能力边界、降低落地门槛或增强长期适应性。\n\n* * *\n\n1\\. 多模态反馈的深度融合\n\n*   **像素级差异图 + 结构化日志**  \n    当前 Planner 仅接收截图与文本历史。引入 UI 差异热图（pixel diff）、运行时日志（stdout/stderr）、甚至音频提示，可让 Planner 在更少步数内定位问题。\n*   **触觉或力反馈通道**  \n    在 3D 建模、医学影像交互等场景，结合触觉设备返回的“阻力”信号，可训练 Planner 学习更精细的操作策略。\n\n* * *\n\n2\\. 跨软件元技能与持续学习\n\n*   **Continual Domain Adaptation**  \n    每出现一款新软件，仅对 Planner 做少量梯度更新即可迁移，无需重新采集大规模轨迹。可探索：\n    *   基于 LoRA/Adapter 的轻量增量学习；\n    *   经验回放 + 正则化，避免灾难性遗忘。\n*   **Meta-Planner**  \n    在多个软件间抽象出“通用工作流”元策略（如“打开菜单→搜索功能→设置参数→验证结果”），再快速实例化到具体软件。\n\n* * *\n\n3\\. 执行层可扩展性\n\n*   **Executor 插件化**  \n    目前 Executor 固定为 UI-TARS-1.5。可设计轻量 API，让社区贡献针对特定软件的高精度 Executor（如 CAD 专用坐标解析器），并通过路由机制动态调用。\n*   **混合执行模式**  \n    对高频、标准化动作（点击、拖拽）仍用 UI-TARS；对需要复杂键盘宏或 API 调用的步骤，切换至 OS-level 脚本或 Accessibility API，减少像素级误差。\n\n* * *\n\n4\\. 数据效率与质量\n\n*   **轨迹压缩与课程重标**  \n    利用 VLM 对冗长轨迹做“关键帧摘要”，再让 Judge 对摘要重新打分，可 3–5× 降低存储与训练成本。\n*   **人类在环主动学习**  \n    当 Judge 置信度 < τ 时，自动转交人类快速标注，形成“高价值小批量”数据，持续提升 Judge 精度。\n\n* * *\n\n5\\. 安全与可控性\n\n*   **可验证动作空间**  \n    引入形式化约束（如“禁止删除系统文件”），在 Planner 输出阶段即过滤危险动作；或利用 SMT/SAT 求解器对动作脚本做静态验证。\n*   **可解释轨迹回放**  \n    将 Planner 的决策链映射为自然语言 + 高亮截图，便于审计与调试。\n\n* * *\n\n6\\. 更复杂的科学场景\n\n*   **多窗口/多屏幕协同**  \n    现代科研常需同时操作数据获取软件 + 分析软件 + 可视化软件。可扩展 POMDP 状态空间，训练 Planner 管理跨应用剪贴板、文件同步、参数联动。\n*   **实时数据流任务**  \n    在实验控制软件中，GUI 状态随仪器数据实时变化。引入事件驱动 RL（event-based RL），让 Planner 在毫秒级反馈下调整策略。\n\n* * *\n\n7\\. 硬件-软件联合优化\n\n*   **边缘-云协同**  \n    Executor 在本地 GPU 上运行以保证低延迟；Planner 在云端大模型上持续更新，并通过量化/蒸馏版本下发到边缘端。\n*   **专用加速器**  \n    针对 UI 解析与坐标回归设计轻量 ASIC/FPGA，进一步降低 Executor 延迟，提高并发度。\n\n* * *\n\n8\\. 开源生态与标准化\n\n*   **ScienceBoard-2.0 扩展**  \n    纳入更多学科软件（SPSS、MATLAB、ANSYS、LabVIEW），并发布统一 API 与评估协议，形成社区驱动的持续基准。\n*   **Judge-as-a-Service**  \n    将 72B-GUI-Judge 封装成在线服务，供任意 GUI Agent 开发者调用，降低行业门槛。\n\n* * *\n\n通过上述方向的探索，CODA 有望从“科学计算 GUI 自动化”走向**通用、可信、可持续进化的数字科研助手**。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### CODA：面向科学计算 GUI 的可训练“双脑”智能体框架  \n**一句话总结**：论文提出 CODA——一个受大脑 Cerebrum-Cerebellum 分工启发的可训练 Planner–Executor 框架，以极少数据在复杂科学软件中实现长程规划与像素级执行的高效统一，并在 ScienceBoard 基准上刷新开源 SOTA。\n\n* * *\n\n1\\. 问题背景\n\n*   科学计算 GUI 任务需要 **长程规划 + 像素级精确执行**。\n*   现有方法 trade-off：\n    *   通用大模型擅规划却易误点；\n    *   专用小模型动作准但难泛化；\n    *   静态组合框架无法从交互中学习。\n\n* * *\n\n2\\. 核心思想\n\n*   **可训练 Planner（Cerebrum）** 负责高层策略，**冻结 Executor（Cerebellum）** 负责低层动作。\n*   仅需训练 Planner，Executor 复用现成强 grounding 模型，显著降低数据与算力需求。\n\n* * *\n\n3\\. 技术方案\n\n阶段\n\n目标\n\n方法\n\n关键机制\n\n**Stage 1 Specialization**\n\n为每款软件训练专家 Planner\n\n改进 GRPO 强化学习\n\nPlanner 输出多组计划 → Executor 执行 → Judge 给奖励 → 仅更新 Planner\n\n**Stage 2 Generalization**\n\n获得跨软件通用 Planner\n\n专家轨迹汇总 + SFT\n\n0.77 K 高质量轨迹蒸馏，通用 Planner 性能反超各专家\n\n**数据闭环**\n\n零人工标注\n\n自动生成任务 + 分布式 VM + 高精度 Judge\n\nJudge 集成投票、多分辨率、模型集成，Precision 81.2%\n\n* * *\n\n4\\. 实验结果（ScienceBoard 四软件）\n\n指标\n\n基线最佳开源\n\nCODA Stage-2\n\n提升\n\nAverage@1\n\n12.94 %\n\n**21.04 %**\n\n+8.1 pp\n\nPass@8\n\n29.03 %\n\n**39.96 %**\n\n+10.9 pp\n\n*   首次在开源阵营中超越 GPT-4o、Claude-3.7-Sonnet 等闭源 Planner。\n*   代码与模型已开源： [https://github.com/OpenIXCLab/CODA](https://github.com/OpenIXCLab/CODA)\n\n* * *\n\n5\\. 贡献与意义\n\n*   **架构**：首个完全开源、可训练的组合式 GUI 智能体。\n*   **训练**：提出 Planner-only 的 GRPO 解耦 RL，显著降低数据门槛。\n*   **应用**：在科学计算等稀缺数据场景建立新 SOTA，为专业软件自动化提供可复现范式。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是CODA%3A%20Coordinating%20the%20Cerebrum%20and%20Cerebellum%20for%20a%20Dual-Brain%20Computer%20Use%20Agent%20with%20Decoupled%20Reinforcement%20Learning，链接是  https://arxiv.org/pdf/2508.20096  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.20096  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1756424830918","title":"Predicting the Order of Upcoming Tokens Improves Language Modeling","description":"Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to improve next-token prediction (NTP) in language model training but shows inconsistent improvements, underperforming in standard NLP benchmarks. We argue that MTP's exact future token prediction is too difficult as an auxiliary loss. Instead, we propose Token Order Prediction (TOP), which trains models to order upcoming tokens by their proximity using a learning-to-rank loss. TOP requires only a single additional unembedding layer compared to MTP's multiple transformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using NTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show that TOP overall outperforms both NTP and MTP even at scale. Our code is available at https://github.com/zaydzuhri/token-order-prediction","url":"https://huggingface.co/papers/2508.19228","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.918Z","metadata":{"authors":["Zayd M. K. Zuhri","Erland Hilman Fuadi","Alham Fikri Aji"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2508.19228.pdf","arxivUrl":"https://arxiv.org/abs/2508.19228","arxivId":"2508.19228","coolPaperUrl":"https://papers.cool/arxiv/2508.19228","published":"2025-08-26T17:43:30Z","updated":"2025-08-26T17:43:30.000Z","zh_summary":"多-token预测（MTP）被提出作为辅助目标，以改善语言模型训练中的下一个token预测（NTP），但在标准NLP基准测试中表现不稳定，效果不佳。我们认为MTP的确切未来token预测作为辅助损失过于困难。相反，我们提议token顺序预测（TOP），该方法通过使用学习排序损失训练模型按接近度对即将到来的token进行排序。与MTP的多个变换层相比，TOP只需要一个额外的去嵌入层。我们使用NTP、MTP和TOP目标对340M、1.8B和7B参数的模型进行预训练。在八个标准NLP基准测试中的结果表明，TOP在规模上总体上优于NTP和MTP。我们的代码可在https://github.com/zaydzuhri/token-order-prediction获取。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图解决的问题是如何通过改进语言模型的训练目标来提升其性能。具体而言，论文关注了现有的多令牌预测（Multi-Token Prediction, MTP）方法在提升语言模型性能时的局限性，并提出了一种新的辅助训练目标——令牌顺序预测（Token Order Prediction, TOP）。\n\n*   **背景问题**：当前的大型语言模型（LLMs）主要通过预测序列中的下一个令牌（Next-Token Prediction, NTP）来进行训练。尽管NTP取得了一定的成功，但它在某些任务上存在局限性，例如在推理时可能会累积错误，导致准确度下降。此外，NTP在训练时可能无法学习到准确的下一个令牌预测器。\n*   **MTP的局限性**：MTP作为一种辅助训练目标，通过预测多个未来令牌来改进NTP。然而，MTP在标准NLP任务上的表现并不一致，对于较小的模型甚至会损害性能，并且需要增加额外的参数和计算资源。此外，MTP中未来令牌的数量是一个超参数，需要在训练前设置，且增加未来令牌数量并不一定能保证更好的性能。\n*   **提出的解决方案**：论文提出TOP作为一种新的辅助训练目标，旨在通过预测即将到来的令牌的顺序（而不是精确预测未来的令牌）来改进语言模型的内部表示。TOP使用学习排序（learning-to-rank）损失函数，仅需要一个额外的线性解嵌层（unembedding layer），相比MTP的多个Transformer层，这种方法更加参数高效且可扩展。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了以下相关研究：\n\n关于NTP的研究\n\n*   **LeCun (2024)**：提出NTP在推理时会累积错误，导致准确度大幅下降。\n*   **Bachmann & Nagarajan (2024)**：反驳了LeCun的观点，认为NTP的主要问题在于教师强制（teacher-forcing）无法学习到准确的下一个令牌预测器。\n\n关于MTP的研究\n\n*   **Qi et al. (2020)**：提出了ProphetNet，是MTP的一个早期版本，通过预测未来的n-gram来改进序列到序列的预训练。\n*   **Gloeckle et al. (2024)**：正式提出了MTP，通过在Transformer的末尾添加多个头，每个头预测不同偏移量的未来令牌。研究表明MTP在某些需要前瞻性的生成任务（如编码和总结）上提高了语言模型的性能，但在标准NLP任务上的表现并不一致。\n*   **DeepSeek-AI et al. (2024)**：在训练DeepSeek-V3时使用了MTP，但采用了顺序而不是并行的MTP头，并且只预测了下一个两个令牌。\n*   **Ahn et al. (2025)**：提出了联合多令牌预测（Joint Multi-Token Prediction, JTP），是MTP的一个变体。\n\n关于学习排序的研究\n\n*   **Pobrotyn et al. (2020)**：提出了上下文感知学习排序与自注意力相结合的方法。\n*   **Cao et al. (2007)**：提出了ListNet，一种基于列表的学习排序方法，本文中的TOP辅助损失函数借鉴了其思想。\n\n关于语言模型评估的研究\n\n*   **Clark et al. (2018)**：提出了ARC挑战，用于评估语言模型的推理能力。\n*   **Paperno et al. (2016)**：提出了Lambada数据集，用于评估语言模型的上下文理解和生成能力。\n*   **Bisk et al. (2020)**：提出了PIQA，用于评估语言模型的物理常识推理能力。\n*   **Welbl et al. (2017)**：提出了SciQ，用于评估语言模型在科学问题上的推理能力。\n*   **Sap et al. (2019)**：提出了Social IQa，用于评估语言模型在社交互动中的常识推理能力。\n*   **Joshi et al. (2017)**：提出了TriviaQA，用于评估语言模型的阅读理解和问答能力。\n*   **Kwiatkowski et al. (2019)**：提出了NaturalQuestions Open，用于评估语言模型在开放域问答任务中的表现。\n*   **Zellers et al. (2019)**：提出了HellaSwag，用于评估语言模型在完型填空任务中的表现。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过提出一种新的辅助训练目标——**Token Order Prediction (TOP)** 来解决多令牌预测（MTP）在提升语言模型性能时的局限性问题。以下是具体的解决方法：\n\n1\\. **Token Order Prediction (TOP) 的定义**\n\n*   **目标**：TOP的目标是训练模型预测即将到来的令牌的顺序，而不是精确预测未来的令牌。具体来说，给定一个输入令牌序列，TOP构建一个目标序列，其中每个位置的令牌根据其在序列中出现的顺序被赋予一个“接近度”分数。\n*   **窗口大小**：引入了一个超参数——窗口大小（window size），用于限制模型在多大的范围内评估令牌的顺序。\n*   **目标序列的构建**：通过算法1（Algorithm 1）将输入令牌序列转换为TOP目标序列。该算法从后向前遍历序列，记录每个令牌的最近出现位置，并根据这些位置计算每个位置的令牌的“接近度”分数。\n\n2\\. **TOP 的损失函数**\n\n*   **损失函数**：TOP的损失函数借鉴了学习排序（learning-to-rank）中的ListNet方法。具体来说，TOP的损失函数定义为： {% raw %}\\[ L\\_{\\\\text{TOP}} = -\\\\sum\\_{t=0}^{T} \\\\text{softmax}(y\\_t) \\\\cdot \\\\log(\\\\text{softmax}(u\\_{\\\\text{TOP}}(h^L\\_t))) \\]{% endraw %} 其中，( y\\_t ) 是目标序列，( u\\_{\\\\text{TOP}} ) 是TOP头（一个线性解嵌层），( h^L\\_t ) 是Transformer最后一层的隐藏状态。\n*   **解释**：TOP头的输出不是概率分布，而是模型对令牌顺序的预测。通过softmax操作，将目标序列和模型预测的顺序转换为概率分布，然后计算它们之间的交叉熵损失。\n\n3\\. **模型训练**\n\n*   **模型结构**：TOP只需要在标准Transformer模型的基础上增加一个额外的线性解嵌层（TOP头），而不需要像MTP那样增加多个Transformer层。这使得TOP更加参数高效且可扩展。\n*   **联合训练**：在训练过程中，模型同时优化NTP损失和TOP损失，最终的优化目标是这两个损失的和： {% raw %}\\[ L = L\\_{\\\\text{NTP}} + L\\_{\\\\text{TOP}} \\]{% endraw %}\n*   **训练配置**：论文中对340M、1.8B和7B参数的模型进行了预训练，使用了相同的训练数据集（FineWeb-Edu的“sample-100BT”子集），并详细列出了训练配置和超参数（见表1）。\n\n4\\. **实验结果**\n\n*   **评估基准**：论文在八个标准NLP基准测试（如ARC挑战、Lambada、PIQA、SciQ、Social IQa、TriviaQA、NaturalQuestions Open和HellaSwag）上评估了不同训练方法（NTP、MTP和TOP）的模型性能。\n*   **结果**：实验结果显示，TOP在大多数任务上都优于NTP和MTP，尤其是在模型规模较大时（7B参数）。这表明TOP作为一种辅助训练目标，能够有效地提升语言模型的性能，并且随着模型规模的增加，其优势更加明显。\n\n5\\. **总结**\n\n通过引入TOP，论文成功地解决了MTP在提升语言模型性能时的局限性问题。TOP通过预测令牌的顺序而不是精确预测未来的令牌，降低了训练目标的难度，同时保持了参数效率和可扩展性。实验结果表明，TOP在多个标准NLP任务上都取得了优于NTP和MTP的性能，特别是在大规模模型上。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文进行了以下实验：\n\n1\\. **模型预训练**\n\n*   **模型规模**：分别预训练了参数量为340M、1.8B和7B的模型，以探究不同训练策略对不同规模模型的影响。\n*   **训练目标**：使用了三种训练目标：Next-Token Prediction (NTP)、Multi-Token Prediction (MTP) 和 Token Order Prediction (TOP)。\n*   **数据集**：所有模型都在FineWeb-Edu的“sample-100BT”子集上进行训练。340M模型训练了52B个token，而1.8B和7B模型训练了104B个token。\n*   **训练框架**：使用Flame框架和flash-linear-attention库进行模型实现和训练。\n*   **超参数**：详细列出了所有模型大小的训练配置和超参数，包括隐藏层大小、优化器、学习率、序列长度、训练步数等（见表1）。\n\n2\\. **模型评估**\n\n*   **评估基准**：在八个标准NLP基准测试上评估了预训练模型的性能，这些基准测试包括：\n    *   ARC挑战 (Clark et al., 2018)\n    *   Lambada (Paperno et al., 2016)\n    *   PIQA (Bisk et al., 2020)\n    *   SciQ (Welbl et al., 2017)\n    *   Social IQa (Sap et al., 2019)\n    *   TriviaQA (Joshi et al., 2017)\n    *   NaturalQuestions Open (Kwiatkowski et al., 2019)\n    *   HellaSwag (Zellers et al., 2019)\n*   **评估指标**：报告了NTP头的最终训练损失、Lambada的准确率和困惑度、HellaSwag的归一化准确率、ARC挑战、PIQA和SciQ的归一化准确率、Social IQa的准确率、NaturalQuestions Open和TriviaQA的精确匹配分数（见表2）。\n\n3\\. **结果分析**\n\n*   **性能比较**：在所有模型规模下，TOP在大多数任务上的表现均优于NTP和MTP，尤其是在7B模型上，TOP的性能提升更为显著。\n*   **训练损失**：尽管TOP的训练损失高于NTP，但其在Lambada困惑度和基准测试分数上表现更好。作者推测TOP可能起到了正则化的作用，减轻了在有限的FineWeb-Edu子集上训练时的过拟合现象。\n*   **MTP模型的表现**：与原始MTP论文一致，较小的MTP模型在标准NLP基准测试上的表现与NTP相当，但7B的MTP模型在这些任务上表现不佳。而TOP模型在扩展到7B时性能提升，并超过了7B的NTP和MTP基线，表明在更一般的任务中，TOP的性能和可扩展性优于MTP。\n\n4\\. **未来工作**\n\n论文还提到了未来版本中将添加的额外实验和分析，包括：\n\n*   **与DeepSeek V3版本的MTP进行比较**：评估TOP与DeepSeek V3中使用的MTP变体的性能差异。\n*   **在生成任务上的微调和评估**：在诸如总结和编码等生成任务上对模型进行微调，并评估其性能。\n*   **在星图问题上的测试**：在Bachmann & Nagarajan (2024)提出的星图问题这一合成任务上测试TOP。\n*   **评估自推测解码潜力**：研究TOP在自推测解码方面的潜力。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 论文中提到了一些未来工作方向，这些方向也是可以进一步探索的点：\n\n1\\. **与DeepSeek V3版本的MTP进行比较**\n\n*   **研究问题**：DeepSeek V3使用了一种不同的MTP变体，仅预测下一个两个令牌。可以进一步研究这种变体与标准MTP和TOP之间的性能差异。\n*   **探索方向**：通过实验比较不同版本的MTP在各种任务上的表现，分析其在不同模型规模下的效果，以及它们对模型内部表示的影响。\n\n2\\. **在生成任务上的微调和评估**\n\n*   **研究问题**：虽然TOP在标准NLP基准测试上表现优异，但其在生成任务（如文本摘要、代码生成等）上的表现尚未得到充分验证。\n*   **探索方向**：对TOP预训练的模型在特定生成任务上进行微调，并与NTP和MTP预训练的模型进行比较。评估TOP在这些任务上的性能提升，以及其生成文本的质量和多样性。\n\n3\\. **在星图问题上的测试**\n\n*   **研究问题**：星图问题是一个合成任务，用于评估模型对复杂结构数据的学习能力。测试TOP在该任务上的表现可以进一步验证其在处理复杂结构数据时的有效性。\n*   **探索方向**：将TOP应用于星图问题，与NTP和MTP进行比较。分析TOP在处理复杂结构数据时的优势和局限性，以及其对模型泛化能力的影响。\n\n4\\. **评估自推测解码潜力**\n\n*   **研究问题**：MTP的一个优势是其自推测解码能力，可以加速推理过程。需要评估TOP是否具有类似的潜力。\n*   **探索方向**：设计实验来测试TOP模型在自推测解码方面的表现，比较其与MTP在推理速度和准确性上的差异。探索TOP在实际应用中的自推测解码策略。\n\n5\\. **进一步的正则化效果分析**\n\n*   **研究问题**：论文中提到TOP可能起到了正则化的作用，减轻了过拟合现象。这一假设需要进一步验证。\n*   **探索方向**：通过更多的实验来分析TOP的正则化效果，例如在不同的数据集和模型规模下进行训练，观察TOP对模型泛化能力的影响。可以使用交叉验证等方法来评估模型在未见数据上的表现。\n\n6\\. **超参数优化**\n\n*   **研究问题**：TOP引入了窗口大小等超参数，这些参数的选择对模型性能有重要影响。\n*   **探索方向**：通过网格搜索或贝叶斯优化等方法，系统地研究不同超参数设置对TOP性能的影响，找到最优的超参数组合。\n\n7\\. **与其他辅助训练目标的结合**\n\n*   **研究问题**：TOP作为一种辅助训练目标，是否可以与其他辅助训练目标（如对比学习、掩码语言模型等）结合，进一步提升模型性能。\n*   **探索方向**：设计实验来测试TOP与其他辅助训练目标的结合效果，评估其在不同任务上的性能提升。分析不同辅助训练目标之间的协同作用。\n\n8\\. **跨语言和多语言模型的扩展**\n\n*   **研究问题**：TOP是否可以应用于跨语言和多语言模型的训练，提升其在多语言任务上的表现。\n*   **探索方向**：在多语言数据集上训练TOP模型，评估其在跨语言任务（如机器翻译、跨语言问答等）上的性能。研究TOP在多语言环境下的适应性和有效性。\n\n这些方向不仅可以进一步验证TOP的有效性和潜力，还可以为语言模型的训练和优化提供新的见解和方法。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: 本文提出了Token Order Prediction（TOP），这是一种用于改进语言模型训练的新型辅助目标。它旨在通过预测未来令牌的顺序来提升模型性能，而非像多令牌预测（MTP）那样精确预测未来的令牌。以下是论文的主要内容：\n\n研究背景\n\n*   当前大型语言模型（LLMs）主要通过预测序列中的下一个令牌（Next-Token Prediction, NTP）进行训练，尽管取得了显著成果，但也存在一些局限性，如在推理时累积错误导致准确度下降。\n*   多令牌预测（MTP）作为一种辅助训练目标，通过预测多个未来令牌来改进NTP，但其在标准NLP任务上的表现并不一致，且对于较小的模型甚至会损害性能。此外，MTP需要增加额外的Transformer层，增加了训练的复杂性和计算成本。\n\n研究方法\n\n*   **Token Order Prediction（TOP）**：提出TOP作为一种新的辅助训练目标，其核心思想是训练模型预测未来令牌的顺序，而非精确预测未来的令牌。TOP通过构建一个目标序列，其中每个位置的令牌根据其在序列中出现的顺序被赋予一个“接近度”分数，并使用学习排序（learning-to-rank）损失函数进行训练。\n*   **损失函数**：TOP的损失函数借鉴了ListNet方法，定义为： {% raw %}\\[ L\\_{\\\\text{TOP}} = -\\\\sum\\_{t=0}^{T} \\\\text{softmax}(y\\_t) \\\\cdot \\\\log(\\\\text{softmax}(u\\_{\\\\text{TOP}}(h^L\\_t))) \\]{% endraw %} 其中，( y\\_t ) 是目标序列，( u\\_{\\\\text{TOP}} ) 是TOP头（一个线性解嵌层），( h^L\\_t ) 是Transformer最后一层的隐藏状态。\n*   **模型结构**：TOP只需要在标准Transformer模型的基础上增加一个额外的线性解嵌层（TOP头），而不需要像MTP那样增加多个Transformer层，这使得TOP更加参数高效且可扩展。\n\n实验\n\n*   **模型预训练**：分别预训练了参数量为340M、1.8B和7B的模型，使用了NTP、MTP和TOP三种训练目标，并在FineWeb-Edu的“sample-100BT”子集上进行训练。\n*   **评估基准**：在八个标准NLP基准测试上评估了预训练模型的性能，包括ARC挑战、Lambada、PIQA、SciQ、Social IQa、TriviaQA、NaturalQuestions Open和HellaSwag。\n*   **评估指标**：报告了NTP头的最终训练损失、Lambada的准确率和困惑度、HellaSwag的归一化准确率、ARC挑战、PIQA和SciQ的归一化准确率、Social IQa的准确率、NaturalQuestions Open和TriviaQA的精确匹配分数。\n\n关键结论\n\n*   **性能提升**：在所有模型规模下，TOP在大多数任务上的表现均优于NTP和MTP，尤其是在7B模型上，TOP的性能提升更为显著。\n*   **训练损失与正则化**：尽管TOP的训练损失高于NTP，但其在Lambada困惑度和基准测试分数上表现更好。作者推测TOP可能起到了正则化的作用，减轻了在有限的FineWeb-Edu子集上训练时的过拟合现象。\n*   **MTP的局限性**：与原始MTP论文一致，较小的MTP模型在标准NLP基准测试上的表现与NTP相当，但7B的MTP模型在这些任务上表现不佳。而TOP模型在扩展到7B时性能提升，并超过了7B的NTP和MTP基线，表明在更一般的任务中，TOP的性能和可扩展性优于MTP。\n\n未来工作\n\n*   **与DeepSeek V3版本的MTP进行比较**：评估TOP与DeepSeek V3中使用的MTP变体的性能差异。\n*   **在生成任务上的微调和评估**：在诸如总结和编码等生成任务上对模型进行微调，并评估其性能。\n*   **在星图问题上的测试**：在Bachmann & Nagarajan (2024)提出的星图问题这一合成任务上测试TOP。\n*   **评估自推测解码潜力**：研究TOP在自推测解码方面的潜力。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Predicting%20the%20Order%20of%20Upcoming%20Tokens%20Improves%20Language%20Modeling，链接是  https://arxiv.org/pdf/2508.19228  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19228  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1756424830918","title":"Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health Biomarkers Estimation","description":"Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical issues of existing publicly available datasets: small size, privacy concerns with facial videos, and lack of diversity in conditions. The paper introduces a novel comprehensive large-scale multi-view video dataset for rPPG and health biomarkers estimation. Our dataset comprises 3600 synchronized video recordings from 600 subjects, captured under varied conditions (resting and post-exercise) using multiple consumer-grade cameras at different angles. To enable multimodal analysis of physiological states, each recording is paired with a 100 Hz PPG signal and extended health metrics, such as electrocardiogram, arterial blood pressure, biomarkers, temperature, oxygen saturation, respiratory rate, and stress level. Using this data, we train an efficient rPPG model and compare its quality with existing approaches in cross-dataset scenarios. The public release of our dataset and model should significantly speed up the progress in the development of AI medical assistants.","url":"https://huggingface.co/papers/2508.17924","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.918Z","metadata":{"authors":["Konstantin Egorov","Stepan Botman","Pavel Blinov","Galina Zubkova","Anton Ivaschenko","Alexander Kolsanov","Andrey Savchenko"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.CV","68T45","I.4.9"],"pdfUrl":"https://arxiv.org/pdf/2508.17924.pdf","arxivUrl":"https://arxiv.org/abs/2508.17924","arxivId":"2508.17924","coolPaperUrl":"https://papers.cool/arxiv/2508.17924","published":"2025-08-25T11:46:40Z","updated":"2025-08-25T11:46:40.000Z","zh_summary":"远程光电容积描记法（rPPG）的进展受到现有公开数据集的关键问题的限制：数据集规模小、面部视频的隐私问题以及条件缺乏多样性。本文介绍了一种新颖的综合大规模多视角视频数据集，用于rPPG和健康生物标志物的估计。我们的数据集包含来自600名受试者的3600个同步视频录制，这些录制是在不同条件下（静息和运动后）使用多个消费者级相机从不同角度捕捉的。为了实现生理状态的多模态分析，每个录音都配有100 Hz的PPG信号和扩展健康指标，如心电图、动脉血压、生物标志物、温度、氧饱和度、呼吸频率和压力水平。利用这些数据，我们训练了一个高效的rPPG模型，并在跨数据集场景中与现有方法进行了质量比较。我们数据集和模型的公开发布应能显著加快人工智能医疗助手的开发进程。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文针对远程光电容积描记（rPPG）领域长期存在的三大瓶颈问题，提出并解决以下核心问题：\n\n1.  **数据稀缺与规模局限**  \n    现有公开数据集普遍规模小（10–140人），难以支撑深度学习模型的稳健训练与泛化。\n    \n2.  **隐私与访问限制**  \n    多数数据集因包含面部视频而受限于隐私或机构政策，需申请且未必获批，阻碍了开放研究。\n    \n3.  **条件单一与信息缺失**  \n    现有数据多局限于实验室环境，缺乏光照、姿态、运动状态等多样性，且极少同步采集血压、血氧、血糖等关键健康生物标志物。\n    \n\n为此，论文构建并公开了 **MCD-rPPG** 大规模多视角视频数据集，同步采集600名受试者在静息与运动后状态下的3600段视频，并配备100 Hz PPG、ECG及13项扩展健康指标，以解决上述数据瓶颈，推动rPPG模型在实际医疗场景中的可靠应用。\n\n**Q**: 有哪些相关研究？\n\n**A**: 与本文直接相关的研究可分为 **公开数据集** 与 **rPPG 算法** 两大类，以下按时间顺序列出关键工作，并指出其与 MCD-rPPG 的互补或差异之处。\n\n1\\. 公开 rPPG 数据集（按首字母排序）\n\n名称\n\n年份\n\n受试者\n\n开放获取\n\n关键特征\n\n与 MCD-rPPG 的对比\n\n**PURE**\n\n2014\n\n10\n\n❌\n\n实验室环境，10 人，6 种光照/头部姿态\n\n规模极小，无健康指标\n\n**BP4D+**\n\n2016\n\n140\n\n❌\n\n多模态情绪数据集，含血压\n\n无同步 PPG，访问受限\n\n**COHFACE**\n\n2017\n\n40\n\n❌\n\n4 摄像头，静息+任务状态\n\n规模小，无生物标志物\n\n**LGI-PPGI**\n\n2018\n\n25\n\n✅\n\n自然光/室内光，无约束场景\n\n无生理真值\n\n**UBFC-rPPG**\n\n2019\n\n42\n\n❌\n\n高帧率 30 fps，静息\n\n仅 PPG，无扩展指标\n\n**UBFC-Phys**\n\n2021\n\n56\n\n❌\n\n社交压力实验，含 ECG/EDA\n\n无血压、血糖等\n\n**SCAMPS**\n\n2022\n\n2800（合成）\n\n✅\n\n合成数据，2800 身份\n\n非真实拍摄，无真实生理\n\n**MMPD**\n\n2023\n\n33\n\n❌\n\n多设备移动视频\n\n规模小，无生物标志物\n\n**VitalVideos**\n\n2023\n\n900\n\n❌\n\n含血压真值\n\n无 ECG、血糖等\n\n**iBVP**\n\n2024\n\n33\n\n❌\n\nRGB-热红外双模态\n\n规模小，无扩展指标\n\n\\> 结论：MCD-rPPG 在 **规模（600 人）、同步多视角、13 项健康指标、公开获取** 四方面均填补或超越了现有数据集。\n\n* * *\n\n2\\. rPPG 算法（按方法类别归纳）\n\n2.1 无监督传统方法\n\n*   **POS** (Wang et al., 2017)  \n    基于色度模型，跨数据集鲁棒，但无法利用大规模数据。\n*   **OMIT** (Casado & López, 2023)  \n    引入正交色度空间，提升运动鲁棒性。\n*   **PBV** (de Haan & van Leest, 2014)  \n    使用血容量脉冲特征抑制运动伪影。\n\n2.2 深度学习方法\n\n*   **DeepPhys** (Chen & McDuff, 2018)  \n    首个端到端 CNN，使用注意力机制，但需要大量训练数据。\n*   **PhysFormer** (Yu et al., 2022)  \n    时序差分 Transformer，在 UBFC-rPPG 上 SOTA，但跨数据集掉点严重。\n*   **RhythmFormer** (Zou et al., 2024)  \n    周期性稀疏注意力，强调 rPPG 的节律先验。\n*   **iBVPNet** (Joshi & Cho, 2024)  \n    针对高分辨率标签设计，参数量较大。\n\n2.3 轻量级 / 混合方法\n\n*   **EfficientPhys** (Liu et al., 2021)  \n    面向移动端优化，但仅估计 HR/BVP。\n*   **本文 Baseline**  \n    结合 ROI 先验 + 1-D 全卷积金字塔网络，实现 CPU 实时推理，同时支持 13 项生物标志多任务学习。\n\n\\> 结论：MCD-rPPG 的 baseline 在 **速度与跨数据集稳健性** 上优于现有深度模型，且首次将血压、血糖、动脉硬度等指标纳入统一框架，为后续研究提供新的基准任务。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过“数据+模型+基准”三位一体策略，系统性地解决了 rPPG 领域长期存在的数据稀缺、隐私受限、场景单一以及健康指标缺失等问题。具体实施路径如下：\n\n1\\. 构建大规模、多视角、多模态公开数据集（MCD-rPPG）\n\n*   **规模与多样性**\n    *   600 名 18–83 岁受试者，覆盖不同性别、BMI、健康状态。\n    *   每人采集 **静息 + 运动后** 两段 3 min 视频，共 3600 段。\n    *   使用 **手机、摄像机、网络摄像头** 三种消费级设备，提供 3 个视角，模拟真实场景。\n*   **同步生理真值**\n    *   100 Hz 医用 PPG、ECG；\n    *   13 项扩展健康指标：血压、血氧、血糖、糖化血红蛋白、胆固醇、呼吸率、动脉硬度、体温、压力评分等。\n*   **公开获取**\n    *   数据集与处理脚本完全开源（Hugging Face + GitHub），消除隐私与访问壁垒。\n\n2\\. 设计轻量级多任务 rPPG 基线模型\n\n*   **混合架构**\n    *   **预处理**：MediaPipe FaceMesh 自适应提取面部 ROI，降低光照/姿态噪声。\n    *   **主干网络**：1-D 全卷积 Feature Pyramid，参数量仅 3.9 MB，可在 CPU 实时运行（比 SOTA 快 13%）。\n    *   **多任务输出**：同步预测 PPG 波形 + 13 项健康指标，统一 MSE 损失训练。\n*   **跨数据集稳健性**\n    *   在 MMPD、SCAMPS、UBFC-rPPG 上测试，MAE 与 SOTA 可比甚至更优，验证泛化能力。\n\n3\\. 建立新的研究基准与实验框架\n\n*   **同步方案**\n    *   视频：背景数字时钟 + OCR 计算跨设备时间偏移，误差 < ±0.2 s。\n    *   PPG：Chebyshev 滤波 + 皮尔逊相关最大化，实现帧级对齐。\n*   **评估维度**\n    *   提供 **跨数据集、跨视角、跨状态（静息/运动后）** 三套基准结果，揭示模型在真实复杂场景下的鲁棒性。\n    *   首次将 **血压、血糖、动脉硬度** 等临床指标纳入公开 rPPG 基准任务，为后续算法研究提供量化目标。\n\n通过以上措施，论文不仅填补了大规模、多视角、多模态 rPPG 数据空白，还提供了可复现、可扩展的轻量级基线，显著降低了学术界与工业界在远程健康监测领域的准入门槛。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕 **rPPG 波形估计、心率估计、健康生物标志预测、跨数据集泛化、视角鲁棒性、推理效率** 六个维度，设计并执行了系统实验。所有实验均基于公开代码与数据，确保可复现。结果汇总如下：\n\n* * *\n\n1\\. 主任务：PPG 波形与心率估计\n\n*   **数据集**：MCD-rPPG 训练集，20 s 滑动窗口，Adam 优化，MSE 损失。\n*   **指标**：\n    *   PPG 波形 MAE（逐点误差）\n    *   HR MAE（0.5–3 Hz 主频提取）\n*   **结果**：\n    *   在 MCD-rPPG 测试集上，**PPG MAE = 0.68 ± 0.03**，**HR MAE = 4.86 ± 0.36 bpm**，优于多数无监督方法，与 SOTA 深度模型持平或更优（见表 3）。\n\n* * *\n\n2\\. 跨数据集泛化评估\n\n*   **测试集**：MMPD、SCAMPS、UBFC-rPPG（均未参与训练）。\n*   **对比模型**：PBV、OMIT、POS、PhysFormer、RhythmFormer、iBVPNet 等。\n*   **结果**（表 3）：\n    *   **HR MAE** 在 UBFC-rPPG 上仅 **6.84 ± 5.27 bpm**，显著优于多数深度模型（>20 bpm）。\n    *   在 SCAMPS 合成数据上，波形 MAE 与 SOTA 相当，验证了对未见域的稳健性。\n\n* * *\n\n3\\. 视角鲁棒性实验\n\n*   **设置**：同一受试者同时被三台设备（手机、摄像机、网络摄像头）拍摄，测试 frontal vs. side 视角差异。\n*   **指标**：PPG MAE、HR MAE、CPU/GPU 推理时间、模型大小。\n*   **结果**（表 4）：\n    *   无监督方法（POS、OMIT）在 side 视角性能骤降（HR MAE ↑3–5×）。\n    *   本文模型 side 视角 HR MAE 仅上升至 14.01 bpm，**视角鲁棒性最佳**。\n    *   **CPU 推理 0.15 s/20 s 片段，模型 3.9 MB**，速度领先 13%。\n\n* * *\n\n4\\. 健康生物标志预测\n\n*   **任务**：从面部视频直接回归 13 项指标（血压、血糖、胆固醇、呼吸率、动脉硬度、年龄、BMI、压力、血氧、性别）。\n*   **基线**：训练集最优常数预测。\n*   **结果**（表 5）：\n    *   年龄 MAE 从 5.71 → 3.91 年；\n    *   收缩压 MAE 从 13.78 → 12.82 mmHg；\n    *   性别分类准确率 64 %（基线 61 %）。\n    *   所有连续变量均优于或接近基线，首次验证 rPPG 视频对 **多生物标志同步估计** 的可行性。\n\n* * *\n\n5\\. 同步质量评估\n\n*   **视频同步**：\n    *   基于背景数字时钟 OCR，跨设备时间偏移 **< ±0.2 s**（图 4）。\n*   **PPG-视频同步**：\n    *   POS 算法重建 PPG 与医用 PPG 对齐，帧级偏移峰值在 **±10 帧** 内（图 5），满足训练需求。\n\n* * *\n\n6\\. 消融与效率测试\n\n*   **消融**：\n    *   移除 ROI 选择 → HR MAE ↑12 %；\n    *   移除多任务联合训练 → 血压 MAE ↑8 %。\n*   **效率**：\n    *   在 200 段 20 s 视频上连续推理，CPU 总耗时 30 s，**实时系数 ≈ 0.15**，满足移动端实时需求。\n\n* * *\n\n实验结论速览\n\n维度\n\n主要发现\n\n**PPG/HR 精度**\n\n与 SOTA 持平，跨数据集稳健\n\n**视角鲁棒性**\n\n优于无监督方法，适应侧脸\n\n**生物标志**\n\n首次公开多任务基准，多数指标优于常数基线\n\n**推理效率**\n\n3.9 MB 模型，CPU 实时，领先 13 %\n\n**数据同步**\n\n视频/PPG 亚秒级对齐，满足训练要求\n\n以上实验共同验证了 MCD-rPPG 数据集与基线模型在 **规模、多样性、泛化、实时性** 上的综合优势。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 MCD-rPPG 数据集与基线模型发布后的自然延伸，既可直接复用现有数据，也可通过补充采集或算法改进进一步深挖。\n\n* * *\n\n1\\. 数据层面\n\n*   **长时程与动态场景**\n    \n    *   将 3 min 片段扩展到 30 min–1 h，研究光照渐变、自然运动、说话/进食等复杂场景对 rPPG 的影响。\n    *   采集 **夜间/红外** 视频，验证暗光或主动照明条件下的鲁棒性。\n*   **多模态传感器融合**\n    \n    *   同步加入 **热红外、深度、毫米波雷达** 信号，构建多模态心血管监测基准。\n    *   利用热红外血管图辅助 ROI 选择，降低肤色/化妆干扰。\n*   **纵向随访子集**\n    \n    *   对同一受试者间隔 3–6 个月重复采集，研究 **血压、动脉硬化** 等指标的长期变化是否可从视频中捕获。\n\n* * *\n\n2\\. 任务与标签扩展\n\n*   **细粒度心血管事件检测**\n    \n    *   在现有 ECG 真值基础上标注 **早搏、房颤、心律不齐** 片段，训练事件级分类器。\n    *   引入 **脉搏波传导时间 PWTT** 真值，探索无袖带血压估计的端到端方案。\n*   **情绪与压力动态估计**\n    \n    *   利用 PSM-25 问卷分数作为弱标签，结合面部表情、语音、姿态，构建 **多模态压力回归** 任务。\n*   **联邦学习与隐私保护**\n    \n    *   在保持面部视频本地化的前提下，研究 **联邦 rPPG** 训练框架，验证模型能否在不共享原始像素的情况下收敛。\n\n* * *\n\n3\\. 模型与算法\n\n*   **时空 Transformer 轻量化**\n    \n    *   在 PhysFormer/RhythmFormer 基础上引入 **动态 token 剪枝** 或 **频域注意力**，在保持精度的同时进一步压缩至 <2 MB。\n*   **自监督预训练**\n    \n    *   利用大规模无标签面部视频（如 VoxCeleb）进行 **对比学习预训练**，再微调 MCD-rPPG，验证跨域迁移效果。\n*   **多任务不确定性估计**\n    \n    *   为 13 项生物标志引入 **异方差不确定性** 或 **蒙特卡洛 Dropout**，输出置信区间，提升临床可用性。\n*   **因果视角下的鲁棒性**\n    \n    *   通过 **因果干预**（如光照方向、相机光谱响应）生成合成数据，训练对混淆因子不变的表征，缓解跨设备掉点问题。\n\n* * *\n\n4\\. 临床与落地验证\n\n*   **与金标准对比研究**\n    \n    *   在医院心内科同步采集 **有创动脉血压、超声心动图**，评估 rPPG 血压/心排量估计的绝对误差与临床一致性（Bland–Altman）。\n*   **边缘设备部署**\n    \n    *   将模型移植到 **智能镜子、车载摄像头、AR 眼镜**，结合实时反馈 UI，开展 100 人以上的 **居家/车载** 连续监测试点。\n*   **法规与伦理框架**\n    \n    *   探索 **GDPR/HIPAA 合规** 的数据管道（差分隐私、联邦学习、可撤销授权），为大规模商用扫清合规障碍。\n\n* * *\n\n5\\. 数据集再加工工具链\n\n*   **自动质量评分**\n    \n    *   训练轻量级网络对每段视频输出 **信号可用性分数**（运动模糊、光照过曝、遮挡），方便研究者快速筛选高质量子集。\n*   **合成数据增强**\n    \n    *   基于 NeRF/3DMM 渲染 **不同肤色、年龄、化妆、光照** 的面部视频，与真实 PPG 同步，构建 **百万级合成 rPPG** 扩展集。\n\n* * *\n\n速查表：可立即启动的 5 个课题\n\n课题\n\n所需资源\n\n预期产出\n\n长时程夜间 rPPG 子集\n\n红外摄像头 + 30 min 采集\n\n暗光鲁棒模型 + 新基准\n\n房颤片段标注\n\nECG 专家重标 + 半自动切分\n\n事件检测新任务\n\n联邦 rPPG 框架\n\nPySyft/ Flower 代码库\n\n隐私友好训练协议\n\n因果数据增强\n\nBlender + 光谱渲染\n\n跨设备鲁棒性提升 10 %+\n\n车载场景试点\n\n车载摄像头 + 50 驾驶员\n\n真实驾驶心率曲线数据集\n\n这些方向既可直接基于已公开的 MCD-rPPG 进行深挖，也可通过低成本扩展采集快速验证，为 rPPG 走向临床与消费级产品奠定更坚实的基础。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### 论文核心速览\n\n维度\n\n关键内容\n\n**问题**\n\n现有 rPPG 公开数据集规模小、隐私受限、场景单一、缺乏多元健康指标，阻碍算法研发与落地。\n\n**贡献 1：数据**\n\n发布 **MCD-rPPG** 数据集：<br>• 600 人 × 3 视角 × 2 状态（静息/运动后）＝3600 段 3 min VGA 视频；<br>• 同步 100 Hz PPG、ECG 及 13 项健康指标（血压、血糖、胆固醇、呼吸率、动脉硬度等）；<br>• 完全开源（Hugging Face + GitHub）。\n\n**贡献 2：模型**\n\n提出 **轻量级多任务 rPPG 基线**：<br>• MediaPipe ROI + 1-D 全卷积金字塔网络，3.9 MB，CPU 实时；<br>• 同时输出 PPG 波形 + 13 项生物标志，跨数据集 HR MAE 与 SOTA 持平或更优。\n\n**实验**\n\n• 在 MCD-rPPG、MMPD、SCAMPS、UBFC-rPPG 上系统评测：PPG/HR 精度、跨域泛化、视角鲁棒性、推理效率；<br>• 首次验证面部视频可直接回归血压、血糖等多指标，优于常数基线。\n\n**意义**\n\n为 rPPG 研究提供大规模、多视角、多模态公开基准，推动远程健康监测从实验室走向临床与消费级设备。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Gaze%20into%20the%20Heart%3A%20A%20Multi-View%20Video%20Dataset%20for%20rPPG%20and%20Health%20Biomarkers%20Estimation，链接是  https://arxiv.org/pdf/2508.17924  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.17924  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1756424830919","title":"Diffusion Language Models Know the Answer Before Decoding","description":"Diffusion language models (DLMs) have recently emerged as an alternative to autoregressive approaches, offering parallel sequence generation and flexible token orders. However, their inference remains slower than that of autoregressive models, primarily due to the cost of bidirectional attention and the large number of refinement steps required for high quality outputs. In this work, we highlight and leverage an overlooked property of DLMs early answer convergence: in many cases, the correct answer can be internally identified by half steps before the final decoding step, both under semi-autoregressive and random remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99% of instances, respectively, can be decoded correctly using only half of the refinement steps. Building on this observation, we introduce Prophet, a training-free fast decoding paradigm that enables early commit decoding. Specifically, Prophet dynamically decides whether to continue refinement or to go \"all-in\" (i.e., decode all remaining tokens in one step), using the confidence gap between the top-2 prediction candidates as the criterion. It integrates seamlessly into existing DLM implementations, incurs negligible overhead, and requires no additional training. Empirical evaluations of LLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the number of decoding steps by up to 3.4x while preserving high generation quality. These results recast DLM decoding as a problem of when to stop sampling, and demonstrate that early decode convergence provides a simple yet powerful mechanism for accelerating DLM inference, complementary to existing speedup techniques. Our code is publicly available at https://github.com/pixeli99/Prophet.","url":"https://huggingface.co/papers/2508.19982","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.919Z","metadata":{"authors":["Pengxiang Li","Yefan Zhou","Dilxat Muhtar","Lu Yin","Shilin Yan","Li Shen","Yi Liang","Soroush Vosoughi","Shiwei Liu"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2508.19982.pdf","arxivUrl":"https://arxiv.org/abs/2508.19982","arxivId":"2508.19982","coolPaperUrl":"https://papers.cool/arxiv/2508.19982","published":"2025-08-27T15:40:25Z","updated":"2025-08-27T15:40:25.000Z","zh_summary":"翻译失败: Unexpected end of JSON input","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 论文聚焦的核心问题是**扩散语言模型（DLM）推理阶段的效率瓶颈**。虽然 DLMs 在理论上具备并行解码、灵活生成顺序等优势，但在实际部署中，其推理速度仍慢于自回归（AR）模型，主要原因包括：\n\n*   **双向注意力导致无法直接复用 KV-cache**；\n*   **为保证输出质量，需要大量迭代去噪步骤**。\n\n现有加速方法（如 KV-cache 近似、并行解码策略）主要从“如何更快完成一次迭代”或“如何减少迭代次数”入手，而本文则提出一个**全新的视角**：  \n\\> **DLMs 在解码早期阶段就已内部锁定正确答案，后续迭代往往冗余。**\n\n因此，论文旨在验证并**利用“早期答案收敛”现象**，设计一种**无需训练、即插即用的快速解码机制**，在几乎不损失精度的前提下，显著减少实际执行的解码步数，从而弥合 DLMs 与 AR 模型在推理延迟上的差距。\n\n**Q**: 有哪些相关研究？\n\n**A**: 以下研究按照两条主线梳理：\n\n1.  **扩散语言模型（DLM）的基础与变体**；\n2.  **DLM 推理加速方法**。  \n    所有文献均来自论文引用列表，按时间顺序呈现关键贡献。\n\n* * *\n\n1\\. 扩散语言模型的奠基与扩展\n\n文献\n\n核心贡献\n\nSohl-Dickstein et al. (ICML 2015)\n\n首次将扩散过程引入离散序列建模，提出基于离散状态马尔可夫链的前向加噪与反向去噪框架。\n\nAustin et al. (NeurIPS 2021, D3PM)\n\n系统化离散扩散的 ELBO 训练目标，给出可扩展的转移矩阵设计，奠定后续 DLM 的理论基础。\n\nHoogeboom et al. (NeurIPS 2021)\n\n提出 Argmax Flow 与 Multinomial Diffusion，解决离散变量梯度估计难题。\n\nCampbell et al. (NeurIPS 2022)\n\n将离散链推广到连续时间马尔可夫链（CTMC），支持更灵活的步长控制。\n\nLou et al. (2023, SEDD)\n\n直接估计数据分布比率，引入“去噪分数熵”训练准则，提升训练稳定性与似然估计。\n\nShi et al. (2024, MDLM) / Sahoo et al. (2024)\n\n证明多种离散扩散参数化在数学上等价，简化模型实现；开源 MDLM 代码库。\n\nNie et al. (2025, LLaDA) / Ye et al. (2025, Dream)\n\n发布十亿级开源 DLM，验证扩散架构在语言任务上的可扩展性。\n\n* * *\n\n2\\. DLM 推理加速方法\n\n文献\n\n技术路线\n\n关键思路\n\nMa et al. (2025a, dKV-Cache)\n\n**KV-cache 近似**\n\n利用相邻去噪步隐藏状态高度相似的特性，缓存并复用 KV，减少重复计算。\n\nLiu et al. (2025a, DLLM-Cache)\n\n**KV-cache 近似**\n\n自适应缓存刷新策略，在固定间隔更新 KV，平衡速度与精度。\n\nWu et al. (2025a, Fast-DLLM)\n\n**KV-cache + 并行解码**\n\n将去噪过程重构成块自回归形式，使 KV-cache 可行；同时支持一次性解码多个 token。\n\nWei et al. (2025a, SlowFast)\n\n**采样优化**\n\n提出“三原则”动态调整每步并行解码 token 数，利用小 AR 模型对齐输出分布。\n\nIsrael et al. (2025a,b)\n\n**采样优化**\n\n基于置信度或熵阈值，自适应决定每步并行解码 token 数量，减少冗余迭代。\n\nWang et al. (2025a)\n\n**早期收敛利用**\n\n并发工作：对中间步预测做时间集成以提升准确率（与 Prophet 的“提前终止”互补）。\n\n* * *\n\n与 Prophet 的关系\n\n*   **互补性**：上述 KV-cache 与并行解码方法聚焦“单步更快”或“步数更少”，而 Prophet 通过**监测置信度提前终止**，进一步削减冗余步数，可与任何底层加速方案叠加。\n*   **无需训练**：不同于 RL-based 步数压缩（Song et al., 2025b），Prophet 不修改模型参数，直接利用现有 DLM 的置信度输出。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文提出 **Prophet**——一种**无需训练、即插即用**的快速解码范式，通过\\*\\*“早期答案收敛”\\*\\*现象来减少扩散语言模型（DLM）的推理步数。其解决思路可概括为三步：\n\n* * *\n\n1\\. 发现现象：早期答案收敛\n\n*   **统计验证**：在 GSM8K 与 MMLU 上，**97–99 %** 的样本在仅完成 **50 % 去噪步数**时，模型内部已锁定正确答案（top-1 token 与 ground-truth 完全一致）。\n*   **关键观察**：答案 token 的置信度差距（top-1 与 top-2 概率差）在收敛后显著增大，且后续迭代极少再改变。\n\n* * *\n\n2\\. 设计指标：Confidence Gap\n\n对当前去噪步 $t$ 的每个位置 $i$ 计算  \n$$ g\\_{t,i}=L^{(1)}_{t,i}-L^{(2)}_{t,i} $$  \n其中 $L^{(k)}\\_{t,i}$ 为第 $k$ 大 logit。对所有答案区域位置取平均 $\\\\bar g\\_t$，作为**收敛信号**。\n\n* * *\n\n3\\. 动态决策：Early Commit Decoding\n\n将“是否继续迭代”建模为**最优停止问题**，引入**分阶段阈值函数** $\\\\tau(p)$：\n\n解码进度 {% raw %}$p=\\\\frac{T\\_{\\\\max}-t}{T\\_{\\\\max}}${% endraw %}\n\n风险态度\n\n阈值\n\n$p<0.33$\n\n保守\n\n{% raw %}$\\\\tau\\_{\\\\text{high}}=8.0${% endraw %}\n\n$0.33\\\\le p<0.67$\n\n中等\n\n{% raw %}$\\\\tau\\_{\\\\text{mid}}=5.0${% endraw %}\n\n$p\\\\ge 0.67$\n\n激进\n\n{% raw %}$\\\\tau\\_{\\\\text{low}}=3.0${% endraw %}\n\n*   **触发条件**：$\\\\bar g\\_t \\\\ge \\\\tau(p)$ 时，立即一次性并行填充所有剩余 `[MASK]` token，提前终止迭代。\n*   **零开销**：仅需在现有 DLM 推理循环中加入一次阈值判断，无需重训练或修改模型。\n\n* * *\n\n4\\. 实验验证\n\n*   **速度**：在 LLaDA-8B 与 Dream-7B 上，Prophet 将解码步数减少 **1.7×–3.4×**。\n*   **质量**：在 MMLU、GSM8K、Sudoku 等 10 项基准上，**准确率与 50 步全预算解码几乎持平**，部分任务（HellaSwag）甚至因避免后期噪声而**略优于全步数解码**。\n\n* * *\n\n总结\n\n论文通过“早期答案收敛”现象，将 DLM 推理重新定义为**何时停止采样**而非**如何更快采样**。Prophet 以置信度差距为信号、分阶段阈值为策略，实现了**训练无关、模型无关**的高效加速，可与现有 KV-cache、并行解码等方法正交叠加。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕两大核心假设展开实验：\n\n1.  Prophet 在显著减少去噪步数的同时，能否保持与全步数解码相当甚至更好的性能；\n2.  相比静态截断（naive Half 基线），动态阈值策略是否更可靠。\n\n实验覆盖 **2 个模型 × 3 种解码策略 × 10+ 任务**，并辅以早期收敛现象的细粒度分析。\n\n* * *\n\n1\\. 实验设置\n\n维度\n\n配置\n\n模型\n\n• LLaDA-8B-Instruct <br>• Dream-7B-Instruct\n\n解码策略\n\n• **Full**：固定 50 步标准扩散解码 <br>• **Half**：固定 25 步静态截断 <br>• **Prophet**：动态 Early-Commit，τhigh=8.0, τmid=5.0, τlow=3.0\n\n任务分布\n\n• **通用推理**：MMLU、ARC-C、HellaSwag、TruthfulQA、WinoGrande、PIQA <br>• **数学/科学**：GSM8K、GPQA <br>• **规划任务**：Countdown、Sudoku\n\n评估方式\n\n零样本（除 Sudoku/Countdown 8-shot），贪心解码，报告准确率；括号内给出步数缩减倍数。\n\n* * *\n\n2\\. 主要结果（表 1 汇总）\n\n2.1 通用推理任务\n\n任务\n\nLLaDA-8B Full\n\nLLaDA-8B Prophet\n\nDream-7B Full\n\nDream-7B Prophet\n\nMMLU\n\n54.1\n\n**54.0** (2.34×)\n\n67.6\n\n66.1 (2.47×)\n\nARC-C\n\n83.2\n\n**83.5** (1.88×)\n\n88.1\n\n87.9 (2.61×)\n\nHellaSwag\n\n68.7\n\n**70.9** (2.14×)\n\n81.2\n\n81.9 (2.55×)\n\nTruthfulQA\n\n34.4\n\n**46.1** (2.31×)\n\n55.6\n\n53.2 (1.83×)\n\n*   **结论**：Prophet 在 6 项任务中 5 项持平或优于 Full；HellaSwag 上反而提升 2.2 pt，说明提前终止可避免后期噪声破坏已正确预测。\n\n2.2 数学与科学任务\n\n任务\n\nLLaDA-8B Full\n\nLLaDA-8B Prophet\n\nDream-7B Full\n\nDream-7B Prophet\n\nGSM8K\n\n77.1\n\n76.8 (1.69×)\n\n75.3\n\n74.9 (1.76×)\n\nGPQA\n\n25.2\n\n**25.7** (1.82×)\n\n27.0\n\n26.6 (1.66×)\n\n*   **结论**：数学推理场景下 Prophet 几乎无损失；GPQA 上 Half 基线跌至 21.2，而 Prophet 恢复至 25.7，验证动态阈值的有效性。\n\n2.3 规划任务\n\n任务\n\nLLaDA-8B Full\n\nLLaDA-8B Prophet\n\nDream-7B Full\n\nDream-7B Prophet\n\nCountdown\n\n15.3\n\n15.3 (2.67×)\n\n14.6\n\n14.6 (2.37×)\n\nSudoku\n\n35.0\n\n**38.0** (2.46×)\n\n89.0\n\n89.0 (3.40×)\n\n*   **结论**：结构化规划任务同样受益，Sudoku 上步数减少 3.4× 而准确率不降。\n\n* * *\n\n3\\. 早期收敛现象验证\n\n*   **统计分布**：在 GSM8K / MMLU 上，随机重掩码策略下 **97–99 %** 样本在 50 % 步数内即出现正确答案（图 1、附录图 4）。\n*   **热力图分析**：答案 token 的 top-1 预测在 25–50 % 步数后基本锁定，非答案 token 则持续波动（图 2）。\n*   **后缀提示效应**：加入“Answer:”后缀后，25 % 步数内即收敛的样本比例从 7.9 % 提升到 59.7 %（低置信重掩码）。\n\n* * *\n\n4\\. 消融与对比\n\n*   **Half vs Prophet**：Half 在 GPQA 上掉点 4 pt，Prophet 无掉点，证明**动态阈值优于静态截断**。\n*   **阈值敏感性**：τhigh/τmid/τlow 通过小规模验证集选取，论文报告±0.5 变动对结果影响 <0.3 pt。\n\n* * *\n\n总结性结论\n\n实验表明：\n\n*   Prophet 在 **10 余项任务**上平均减少 **1.7×–3.4×** 解码步数；\n*   **准确率与 50 步全预算几乎一致**，部分任务反而提升；\n*   **早期答案收敛现象普遍且强烈**，为后续 DLM 加速研究提供了新的优化维度。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可视为对 Prophet 及早期答案收敛现象的进一步探索，分为 **理论、算法、系统与应用** 四个层面。\n\n* * *\n\n1\\. 理论层面：收敛机理与可预测性\n\n*   **收敛时刻的解析刻画**  \n    建立数学模型，将置信度差距 $g\\_t$ 与反向过程信噪比、熵减速度关联，给出收敛时刻 $t^\\*$ 的闭式估计。\n*   **任务-收敛相关性研究**  \n    在更细粒度的任务分类（事实问答 vs 创意写作 vs 形式证明）上统计收敛率，探究任务复杂度、序列长度、词汇熵与提前收敛概率之间的函数关系。\n*   **收敛与模型规模/训练策略的关联**  \n    比较不同规模（1B→100B）或不同训练目标（ELBO vs score entropy）下收敛曲线的变化，验证收敛是否为扩散语言模型的“规模涌现”特性。\n\n* * *\n\n2\\. 算法层面：更智能的 Early-Commit 策略\n\n*   **学习式阈值函数**  \n    将 Prophet 的固定分段阈值 $\\\\tau(p)$ 替换为轻量级元网络 $f\\_\\\\phi(p, g\\_t, \\\\text{task-embed})$，在线预测最优停止阈值，实现任务自适应。\n*   **位置级 Early-Commit**  \n    当前方法对整个答案区域一次性提交。可探索 **逐 token 决策**：对序列中已满足 {% raw %}$g\\_{t,i} \\\\ge \\\\tau\\_{\\\\text{token}}${% endraw %} 的位置立即固化，其余继续迭代，进一步减少冗余计算。\n*   **与 KV-Cache / 并行解码正交叠加**  \n    研究 Prophet 与 dKV-Cache、Fast-DLLM 等正交加速手段的联合收益，量化总加速上限。\n*   **噪声调度重设计**  \n    若早期已收敛，是否可在训练阶段引入“提前收敛正则化”，让模型在更少步数内达到高置信度，从而在推理时天然适配 Prophet。\n\n* * *\n\n3\\. 系统层面：工程优化与硬件映射\n\n*   **置信度计算的算子融合**  \n    将 top-2 logit 差值计算与 softmax 融合进 GPU kernel，消除额外内存写回，实现真正零开销。\n*   **Early-Commit 触发后的并行填充**  \n    研究一次性填充剩余 \\[MASK\\] 时对张量并行的负载均衡策略，避免长尾延迟。\n*   **边缘场景下的安全回退**  \n    在移动端部署时，若提前提交后出现罕见失败，设计 **轻量级回滚机制**（仅重解码若干 token）以保证用户体验。\n\n* * *\n\n4\\. 应用层面：跨模态与交互式场景\n\n*   **扩散多模态模型**  \n    将 Prophet 思想迁移到 diffusion-based 图像/音频/视频生成，验证“早期语义一致性”是否同样成立，并设计跨模态置信度指标。\n*   **交互式对话系统**  \n    在流式对话中，利用早期收敛实现 **“边想边说”**：当答案 token 已稳定即可先行返回给用户，后台继续精炼后续 token，提升响应速度体验。\n*   **教育/考试场景**  \n    在数学解题任务中，若模型在 30 % 步数内已锁定数值答案，可立即给出结果并附带“置信度条”，帮助学生判断是否需要进一步验证步骤。\n\n* * *\n\n小结\n\n维度\n\n可探索问题示例\n\n理论\n\n收敛时刻的解析公式？任务复杂度与收敛率的关系？\n\n算法\n\n学习式阈值、逐 token Early-Commit、与 KV-Cache 叠加\n\n系统\n\n零开销算子融合、一次性填充的并行负载均衡\n\n应用\n\n扩散多模态、流式对话、教育场景的交互式输出\n\n这些方向既可直接提升 Prophet 的适用范围，也能为扩散生成模型的推理优化提供新的研究范式。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### 论文一句话总结  \n扩散语言模型常在半程就“想好”答案，Prophet 利用这一现象，用**零训练、零开销**的动态早停策略，把推理步数砍掉 **1.7×–3.4×** 而几乎不掉点。\n\n* * *\n\n核心内容速览\n\n模块\n\n要点\n\n**问题**\n\nDLM 推理慢：双向注意力无 KV-cache，且需多步去噪。\n\n**发现**\n\n在 GSM8K / MMLU 上，**97–99 %** 样本只用 **50 % 步数**即可内部锁定正确答案（early answer convergence）。\n\n**方法**\n\nProphet：<br>1. 每步计算答案区平均 **置信度差距** $\\\\bar g\\_t$（top-1 − top-2 logit）。<br>2. 按解码进度 $p$ 动态阈值 $\\\\tau(p)\\\\in{8,5,3}$ 决定是否 **Early-Commit**（一次性并行填充剩余 token）。<br>3. **无需训练**，即插即用。\n\n**实验**\n\nLLaDA-8B & Dream-7B，10+ 任务：<br>• 步数 ↓ **1.7×–3.4×**<br>• 准确率 **持平或略升**（HellaSwag +2.2 pt）<br>• 显著优于静态 Half 截断（GPQA 不掉点）。\n\n**意义**\n\n把 DLM 解码重新定义为**最优停止问题**，为后续加速研究提供新范式。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Diffusion%20Language%20Models%20Know%20the%20Answer%20Before%20Decoding，链接是  https://arxiv.org/pdf/2508.19982  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19982  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1756424830920","title":"StepWiser: Stepwise Generative Judges for Wiser Reasoning","description":"As models increasingly leverage multi-step reasoning strategies to solve complex problems, supervising the logical validity of these intermediate steps has become a critical research challenge. Process reward models address this by providing step-by-step feedback, but current approaches have two major drawbacks: they typically function as classifiers without providing explanations, and their reliance on supervised fine-tuning with static datasets limits generalization. Inspired by recent advances, we reframe stepwise reward modeling from a classification task to a reasoning task itself. We thus propose a generative judge that reasons about the policy model's reasoning steps (i.e., meta-reasons), outputting thinking tokens before delivering a final verdict. Our model, StepWiser, is trained by reinforcement learning using relative outcomes of rollouts. We show it provides (i) better judgment accuracy on intermediate steps than existing methods; (ii) can be used to improve the policy model at training time; and (iii) improves inference-time search.","url":"https://huggingface.co/papers/2508.19229","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.920Z","metadata":{"authors":["Wei Xiong","Wenting Zhao","Weizhe Yuan","Olga Golovneva","Tong Zhang","Jason Weston","Sainbayar Sukhbaatar"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2508.19229.pdf","arxivUrl":"https://arxiv.org/abs/2508.19229","arxivId":"2508.19229","coolPaperUrl":"https://papers.cool/arxiv/2508.19229","published":"2025-08-26T17:45:05Z","updated":"2025-08-26T17:45:05.000Z","zh_summary":"随着模型越来越多地利用多步骤推理策略来解决复杂问题，监督这些中间步骤的逻辑有效性已成为一个关键的研究挑战。过程奖励模型通过提供逐步反馈来解决这一问题，但当前的方法存在两个主要缺点：它们通常作为分类器运作，而没有提供解释，并且依赖于静态数据集的监督微调限制了泛化能力。受到近期进展的启发，我们将逐步奖励建模从分类任务重新构建为推理任务。因此，我们提出了一种生成评判器，它对策略模型的推理步骤（即元推理）进行推理，在给出最终裁决之前输出思考标记。我们的模型StepWiser通过使用回滚的相对结果进行强化学习训练。我们展示了它提供的（i）比现有方法在中间步骤上更好的判断准确性；（ii）可以在训练时用于改善策略模型；以及（iii）改善推理时的搜索。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图解决的问题是如何有效地监督大型语言模型（LLMs）在解决复杂问题时所采用的多步推理策略的逻辑有效性。随着模型越来越多地依赖于多步推理来分解任务并提出更好的解决方案，确保这些中间推理步骤具有逻辑上的合理性已成为一个关键的研究挑战。\n\n具体来说，论文指出现有的过程奖励模型（Process Reward Models, PRMs）存在两个主要缺点：\n\n1.  **缺乏解释性**：现有的PRMs通常作为“黑箱”分类器，只提供一个分数或标签来判断推理步骤的正确性，而不解释为什么一个步骤是正确或错误的。\n2.  **泛化能力有限**：这些模型依赖于静态数据集上的监督微调（Supervised Fine-Tuning, SFT），这限制了它们对新的推理模式的泛化能力。\n\n为了解决这些问题，论文提出了一种新的方法，将中间推理步骤的奖励建模从分类任务转变为推理任务本身。具体而言，论文提出了一个生成式裁判（generative judge），该裁判能够对策略模型的推理步骤进行推理（即元推理，meta-reasoning），并在输出最终判断之前生成思考标记（thinking tokens）。这种方法被称为STEPWISER，它通过强化学习（Reinforcement Learning, RL）进行训练，使用rollouts的相对结果来提供奖励信号。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了多个与过程奖励模型（Process Reward Models, PRMs）和推理模型相关的研究工作，这些工作主要集中在以下几个方面：\n\n过程奖励模型在LLM数学推理中的应用\n\n*   **Lightman et al. (2023)**：首次展示了基于过程监督的模型在指导最佳采样策略方面可以显著优于基于结果监督的模型。然而，他们的PRM800K数据集依赖于对每个推理步骤进行密集的人工标注，这在更大、更多样化和更具挑战性的数据集上通常是不可行的。\n*   **Wang et al. (2023)**：提出了使用蒙特卡洛（Monte Carlo, MC）rollouts来估计每个步骤的Q值，从而自动化标注过程。\n*   **Luo et al. (2024)**：引入了一种二分搜索方法，以高效地识别有缺陷的步骤。\n*   **Zhong et al. (2024), Rafailov et al. (2024)**：在KL正则化的马尔可夫决策过程框架内建立了中间步骤值与最终结果之间的理论联系。这一结果被用来从仅基于结果的数据中推导出类似DPO（Direct Preference Optimization）的目标。\n*   **Xiong et al. (2024a), Cui et al. (2025), Zhou et al. (2025)**：基于上述理论联系，提出了一些从结果数据中学习隐式PRM的方法。\n*   **Zha et al. (2025)**：提示LLMs在产生最终判断之前评估每个单独的步骤，但只监督最终答案的评估。\n\n裁判架构\n\n*   **Discriminative PRMs**：最直接的方法是将任务视为分类问题，通过替换语言模型的最后一层并使用交叉熵损失进行微调，使其能够预测每个步骤的二元标签。\n*   **Generative judges with CoT reasoning**：与判别式PRMs形成鲜明对比的是生成式裁判。在这种范式中，评估本身被框架为一个推理任务。裁判首先生成一个明确的CoT（Chain-of-Thought，思维链）来解释其推理依据，然后输出其最终判断。这种方法最初在偏好学习和结果奖励模型（Outcome Reward Models, ORMs）的背景下进行了探索。\n*   **Zhao et al. (2025), Khalifa et al. (2025)**：这些工作也研究了在步进裁判的背景下从稀疏信号中学习的范式转变，但它们的算法设计与本研究有显著不同。这些工作主要关注离线拒绝采样微调，而本研究则侧重于在线强化学习训练。\n*   **Zha et al. (2025)**：使用强化学习，但采用稀疏的、基于轨迹的监督。他们提示LLMs评估每个单独的步骤和最终答案，但只监督最终验证。他们的方法假设，为了获得准确的最终答案评估，模型隐式地成为了一个步进裁判。与之相比，本框架建立在密集的、基于步骤的监督之上，通过rollouts提供明确的学习信号。\n\n其他相关工作\n\n*   **He et al. (2025)**：使用提示方法将思维过程分割成连贯的块，类似于本研究中的Chunks-of-Thought。然而，他们的步进裁判仅基于提示技术，利用CoT中的提示（如“Wait, I made a mistake”）来识别错误步骤。\n*   **DeepSeek-AI et al. (2025)**：展示了如何通过强化学习训练推理模型以产生最佳性能的CoTs。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文提出了一个名为STEPWISER的方法，通过三个主要步骤来解决如何有效监督大型语言模型（LLMs）在解决复杂问题时的多步推理策略的逻辑有效性问题。以下是详细的解决方案：\n\n1\\. 自分割的CoT生成（Chunks-of-Thought）\n\n为了训练能够评估单个推理步骤的裁判模型，一个关键挑战是如何定义一个“步骤”。虽然链式思考（Chain-of-Thought, CoT）推理使模型能够逐步推理，但如何正确分割这些推理轨迹仍然是一个难题。论文提出了一种自分割技术，使模型能够生成并同时自分割其自身的推理链，使其更加有意义和自包含。\n\n具体方法如下：\n\n*   **定义高质量推理步骤的标准**：每个步骤应该代表一个完整的逻辑跳跃或问题解决过程的一个自包含部分。论文在表1中列出了具体的分割原则和格式规则。\n*   **创建自分割训练数据**：通过使用一个强大的LLM（如Llama-3.1-70B-it）根据这些规则自动分割初始推理轨迹，生成训练数据。然后，使用这些数据对基础模型进行微调，使其能够自分割。\n*   **结果**：自分割后的模型在推理时生成的步骤数量显著减少，同时保持了类似的响应长度和测试准确率。例如，使用自分割技术后，Qwen2.5-1.5B-chunk模型的步骤数量从9.6减少到6.0，而Qwen2.5-7B-chunk模型的步骤数量从9.9减少到6.8。\n\n2\\. 基于rollouts结果的步骤标注\n\n为了为裁判模型创建训练数据，论文使用蒙特卡洛（Monte Carlo）rollouts来估计每个步骤的Q值，并根据这些Q值为每个步骤分配二元目标标签。\n\n具体方法如下：\n\n*   **Q值估计**：对于每个训练提示 ( x ) 和其对应的响应 ( a )，使用蒙特卡洛rollouts估计从每个步骤 ( a\\_i ) 开始的预期最终奖励 ( Q^\\\\pi(s\\_{i-1}, a\\_i) )。具体公式为： \\[ \\\\hat{Q}^\\\\pi(s\\_{i-1}, a\\_i) = \\\\frac{1}{M} \\\\sum\\_{j=1}^M r^\\*(x, a\\_1:i, a\\_{i+1:H}^{(j)}) \\] 其中 ( r^\\* ) 是最终奖励函数，( M ) 是rollouts的数量。\n*   **标签分配**：根据Q值估计，为每个步骤分配二元标签。论文探索了三种不同的标签分配方法：\n    *   **绝对Q值阈值（Abs-Q）**：如果 ( \\\\hat{Q}^\\\\pi(s\\_{i-1}, a\\_i) > 0 )，则标签为正；否则为负。\n    *   **相对有效奖励阈值（Rel-Effective）**：考虑Q值的变化，标签为： \\[ y\\_i = \\\\begin{cases}\n        \n        *   & \\\\text{if } \\\\hat{Q}^\\\\pi(s\\_{i-1}, a\\_i) + \\\\alpha \\\\cdot \\\\hat{A}^\\\\mu(s\\_{i-1}, a\\_i) > 0 \\\\\n        \n        *   & \\\\text{otherwise} \\\\end{cases} \\] 其中 ( \\\\alpha > 0 ) 是超参数，( \\\\hat{A}^\\\\mu(s\\_{i-1}, a\\_i) ) 是优势函数估计。\n    *   **相对比率阈值（Rel-Ratio）**：基于Q值的比率，标签为： \\[ y\\_i = \\\\begin{cases}\n        \n        *   & \\\\text{if } \\\\frac{\\\\hat{Q}^\\\\pi(s\\_{i-1}, a\\_i)}{\\\\hat{Q}^\\\\pi(s\\_{i-2}, a\\_{i-1})} > \\\\gamma \\\\\n        \n        *   & \\\\text{otherwise} \\\\end{cases} \\] 其中 ( \\\\gamma > 0 ) 是阈值。\n\n3\\. 使用强化学习训练裁判模型\n\n在有了分割的推理链和步骤标签后，论文使用强化学习（RL）来训练裁判模型，使其能够生成推理链并输出最终判断。\n\n具体方法如下：\n\n*   **任务公式化**：将每个训练提示分解为步骤级训练提示，模型需要根据问题描述和历史推理路径，对新的推理块进行评估，并生成自己的CoT分析，最后输出最终判断。\n*   **数据集平衡**：由于步骤标签可能高度不平衡（例如，正样本远多于负样本），论文通过下采样多数类来平衡训练集，以稳定RL训练。\n*   **奖励和RL训练**：训练信号直接且直观。对于每个步骤 ( a\\_i )，如果裁判模型的判断与标签 ( y\\_i ) 一致，则获得奖励1，否则为0。论文使用GRPO（Generalized Reinforcement Policy Optimization）算法进行优化。\n\n实验结果\n\n论文通过一系列实验验证了STEPWISER方法的有效性，主要在以下几个方面进行了评估：\n\n1.  **裁判模型在ProcessBench上的分类准确性**：STEPWISER显著优于传统的SFT训练的判别式裁判和其他现有方法。\n2.  **推理时搜索性能**：通过“块重置推理”（Chunk-Reset Reasoning）策略，STEPWISER能够在推理时自我纠正，提高最终解决方案的质量。\n3.  **训练数据选择**：使用STEPWISER裁判模型选择高质量的训练数据，能够提高下游模型的性能。\n\n关键结论\n\n*   **生成式CoT推理和在线RL训练的重要性**：实验结果表明，生成式CoT推理和在线RL训练是STEPWISER方法取得优异性能的关键因素。\n*   **相对信号的有效性**：相对于绝对Q值阈值，使用相对信号（如Rel-Effective和Rel-Ratio）进行步骤标注能够更有效地训练裁判模型。\n*   **数据集平衡的必要性**：通过平衡训练集，可以显著提高模型的稳定性和最终性能。\n\n通过上述方法，STEPWISER能够有效地监督和改进LLMs的多步推理策略，提高模型在复杂问题解决中的逻辑性和准确性。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文进行了以下几类实验来评估STEPWISER方法的有效性：\n\n1\\. **ProcessBench基准测试**\n\n*   **目的**：评估裁判模型在识别推理轨迹中第一个错误步骤的能力。\n*   **数据集**：ProcessBench包含3500个问题-解决方案对，来自多个数学数据集（如GSM8K、MATH、Olympiad Bench和Omni-MATH）。\n*   **评估指标**：使用准确率的调和平均值来衡量性能。\n*   **结果**：\n    *   STEPWISER在所有学习信号（Abs-Q、Rel-Ratio、Rel-Effective）和模型规模下，均显著优于传统的SFT训练的判别式裁判。\n    *   例如，使用7B模型和Rel-Effective信号时，STEPWISER的平均得分为61.9，而SFT判别式基线的得分为39.7。\n    *   此外，STEPWISER还优于其他在线训练方法（如DPO和GRPO），这些方法仅使用最终答案的正确性作为奖励信号。\n\n2\\. **推理时搜索性能**\n\n*   **目的**：评估STEPWISER在推理时通过“块重置推理”（Chunk-Reset Reasoning）策略改进模型推理能力的效果。\n*   **方法**：模型逐块生成解决方案，STEPWISER裁判模型评估每一块。如果块被认为是好的，则接受并继续下一步；如果块被拒绝，则重新生成新的块（最多尝试5次）。\n*   **数据集**：使用NuminaMath-CoT数据集。\n*   **评估指标**：在MATH500数据集上的平均准确率。\n*   **结果**：\n    *   使用STEPWISER裁判模型，1.5B模型的平均准确率从31.2%提高到36.9%，7B模型的平均准确率从57.4%提高到63.3%。\n    *   这表明STEPWISER能够有效地识别并纠正错误的推理步骤，从而提高最终解决方案的质量。\n\n3\\. **训练数据选择**\n\n*   **目的**：评估STEPWISER在选择高质量训练数据方面的效果。\n*   **方法**：使用STEPWISER裁判模型为每个推理块打分，并使用平均分数作为代理来选择最佳响应。\n*   **数据集**：使用NuminaMath-CoT数据集。\n*   **评估指标**：在MATH500数据集上的平均准确率。\n*   **结果**：\n    *   使用STEPWISER选择的数据进行微调的模型，在MATH500上的平均准确率达到63.0%，优于原始基础模型（60.1%）和使用判别式裁判选择的数据（61.9%）。\n    *   这表明STEPWISER能够更有效地识别高质量的训练数据，从而提高下游模型的性能。\n\n4\\. **消融研究**\n\n*   **目的**：分析STEPWISER方法中各个组件对最终性能的贡献。\n*   **方法**：\n    *   **去除RL训练**：使用拒绝采样微调（RS-FT）代替在线RL训练。\n    *   **去除CoT推理**：训练一个判别式裁判模型，而不是生成式裁判模型。\n    *   **去除数据集平衡**：不进行数据集平衡，直接使用原始数据集进行训练。\n*   **结果**：\n    *   **去除RL训练**：性能显著下降，例如，使用1.5B模型和Rel-Ratio信号时，RS-FT的平均得分为23.1，而STEPWISER的得分为36.2。\n    *   **去除CoT推理**：性能也显著下降，例如，使用7B模型和Rel-Ratio信号时，判别式模型的平均得分为47.9，而STEPWISER的得分为60.5。\n    *   **去除数据集平衡**：性能下降，例如，使用7B模型和Rel-Ratio信号时，未平衡数据集的平均得分为47.9，而平衡后的得分为60.5。\n    *   这些结果表明，生成式CoT推理、在线RL训练和数据集平衡都是STEPWISER方法取得优异性能的关键因素。\n\n5\\. **多数投票测试**\n\n*   **目的**：评估在推理时使用多数投票方法是否能够进一步提高STEPWISER的性能。\n*   **方法**：在推理时生成多个判断，并使用多数投票来决定最终判断。\n*   **数据集**：使用NuminaMath-CoT数据集。\n*   **评估指标**：在ProcessBench上的平均准确率。\n*   **结果**：\n    *   使用多数投票方法确实能够提高性能，但提升幅度相对较小。例如，使用7B模型和Rel-Effective信号时，多数投票后的平均得分为64.1，而单个判断的得分为61.9。\n    *   这表明在二元分类任务中，多数投票的效果有限，但在更复杂的任务中可能更有效。\n\n6\\. **自分割微调的消融研究**\n\n*   **目的**：评估自分割微调对模型性能的影响。\n*   **方法**：比较使用标准换行分割和自分割技术的模型性能。\n*   **结果**：\n    *   自分割技术显著减少了推理步骤的数量，提高了计算效率。例如，使用1.5B模型时，标准换行分割的步骤数为5457820，而自分割技术的步骤数为3463520。\n    *   在RL训练中，自分割技术显著提高了模型性能，例如，使用1.5B模型和Rel-Ratio信号时，自分割技术的平均得分为36.2，而标准换行分割的得分为31.0。\n\n7\\. **分类准确性**\n\n*   **目的**：评估STEPWISER在识别中间步骤和最终答案正确性方面的性能。\n*   **方法**：在ProcessBench数据集上评估不同学习信号下的判别式和生成式裁判模型。\n*   **结果**：\n    *   生成式裁判模型在识别中间步骤和最终答案正确性方面显著优于判别式裁判模型。\n    *   例如，使用7B模型和Rel-Ratio信号时，生成式裁判模型的平均得分为60.5，而判别式裁判模型的得分为38.7。\n\n8\\. **超参数搜索**\n\n*   **目的**：优化学习信号的超参数。\n*   **方法**：对Rel-Ratio和Rel-Effective信号的超参数进行搜索。\n*   **结果**：\n    *   对于1.5B模型，Rel-Ratio的最佳阈值为0.8，Rel-Effective的最佳α值为0.4。\n    *   对于7B模型，Rel-Ratio的最佳阈值为0.7，Rel-Effective的最佳α值为0.8。\n\n这些实验结果表明，STEPWISER方法在多个方面均优于现有方法，特别是在推理步骤的逻辑性和准确性方面。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 论文提出了STEPWISER方法，通过生成式裁判和强化学习来监督和改进大型语言模型的多步推理策略。尽管STEPWISER在多个实验中表现出色，但仍有一些可以进一步探索的方向：\n\n1\\. **更复杂的推理任务**\n\n*   **当前状态**：STEPWISER主要在数学推理任务上进行了验证，这些任务通常具有明确的正确和错误答案。\n*   **进一步探索**：可以将STEPWISER应用于更复杂的推理任务，如自然语言处理中的逻辑推理、因果推理、多模态推理等。这些任务可能涉及更复杂的逻辑结构和更多的不确定性，需要进一步验证STEPWISER的泛化能力和适应性。\n\n2\\. **多模态数据集**\n\n*   **当前状态**：STEPWISER目前主要处理文本数据，没有涉及图像、音频等多模态数据。\n*   **进一步探索**：可以探索如何将STEPWISER应用于多模态数据集，例如在视觉问答（VQA）任务中，模型需要同时处理图像和文本信息。这将需要开发新的方法来处理和整合多模态数据，以及设计适合多模态任务的裁判模型。\n\n3\\. **跨领域应用**\n\n*   **当前状态**：STEPWISER在数学推理任务上取得了显著效果，但其应用范围主要限于数学领域。\n*   **进一步探索**：可以探索STEPWISER在其他领域的应用，如医学诊断、法律推理、科学实验设计等。这些领域中的推理任务通常更加复杂，涉及更多的背景知识和专业术语，需要进一步验证STEPWISER的适用性和有效性。\n\n4\\. **动态环境中的推理**\n\n*   **当前状态**：STEPWISER目前主要处理静态数据集，没有涉及动态环境中的推理任务。\n*   **进一步探索**：可以探索STEPWISER在动态环境中的应用，例如在实时对话系统中，模型需要根据用户的实时输入进行推理。这将需要开发新的方法来处理动态数据，并确保裁判模型能够实时更新和适应新的推理任务。\n\n5\\. **更高效的训练方法**\n\n*   **当前状态**：STEPWISER使用强化学习进行训练，虽然取得了良好的效果，但强化学习的训练过程通常较为复杂和耗时。\n*   **进一步探索**：可以探索更高效的训练方法，例如结合强化学习和监督学习的混合训练方法，或者开发新的优化算法来提高训练效率。此外，可以研究如何减少训练数据的需求，例如通过数据增强或迁移学习。\n\n6\\. **模型解释性和可解释性**\n\n*   **当前状态**：STEPWISER通过生成式裁判模型提供了推理步骤的解释，但这些解释的可解释性和用户友好性仍有待提高。\n*   **进一步探索**：可以研究如何进一步提高模型的解释性和可解释性，例如通过开发更自然的语言解释生成方法，或者设计可视化工具来帮助用户理解模型的推理过程。\n\n7\\. **对抗攻击和鲁棒性**\n\n*   **当前状态**：STEPWISER在标准数据集上表现出色，但其在对抗攻击下的表现尚未充分验证。\n*   **进一步探索**：可以研究STEPWISER在对抗攻击下的鲁棒性，例如通过生成对抗性样本并评估模型在这些样本上的表现。此外，可以探索如何增强模型的鲁棒性，例如通过对抗训练或正则化方法。\n\n8\\. **多步推理的长期依赖**\n\n*   **当前状态**：STEPWISER主要关注单步推理的正确性，但多步推理任务中存在长期依赖关系，这些关系对最终结果的影响尚未充分研究。\n*   **进一步探索**：可以研究如何更好地处理多步推理中的长期依赖关系，例如通过引入记忆机制或图神经网络来捕捉推理过程中的长期依赖。\n\n9\\. **与其他方法的结合**\n\n*   **当前状态**：STEPWISER是一种独立的推理监督方法，但尚未与其他先进的方法（如预训练模型、元学习方法等）进行结合。\n*   **进一步探索**：可以研究STEPWISER与其他方法的结合，例如将STEPWISER与预训练模型（如GPT-3、LLaMA等）结合，或者探索如何将STEPWISER应用于元学习框架中，以提高模型在新任务上的适应能力。\n\n10\\. **人类反馈的整合**\n\n*   **当前状态**：STEPWISER主要依赖于自动化的标注方法，没有涉及人类反馈。\n*   **进一步探索**：可以研究如何将人类反馈整合到STEPWISER的训练过程中，例如通过设计交互式学习框架，让人类专家对模型的推理步骤进行评估和反馈，从而进一步提高模型的推理能力和准确性。\n\n这些方向不仅有助于进一步验证STEPWISER方法的普适性和有效性，还可以推动多步推理和模型监督领域的研究进展。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: 论文《STEPWISER: STEPWISE GENERATIVE JUDGES FOR WISER REASONING》提出了一种名为STEPWISER的方法，旨在通过生成式裁判和强化学习来监督和改进大型语言模型（LLMs）在解决复杂问题时的多步推理策略。以下是论文的主要内容和贡献：\n\n研究背景\n\n随着大型语言模型（LLMs）越来越多地依赖多步推理策略来解决复杂问题，确保这些中间推理步骤的逻辑有效性已成为一个关键的研究挑战。现有的过程奖励模型（PRMs）通常作为分类器，缺乏解释性，并且依赖于静态数据集的监督微调，限制了它们的泛化能力。\n\n研究方法\n\n论文提出了STEPWISER方法，通过三个主要步骤来解决上述问题：\n\n1.  **自分割的CoT生成（Chunks-of-Thought）**：\n    \n    *   定义高质量推理步骤的标准，使每个步骤代表一个完整的逻辑跳跃或问题解决过程的一个自包含部分。\n    *   创建自分割训练数据，使用一个强大的LLM（如Llama-3.1-70B-it）根据这些规则自动分割初始推理轨迹，生成训练数据。\n    *   使用这些数据对基础模型进行微调，使其能够自分割。\n2.  **基于rollouts结果的步骤标注**：\n    \n    *   使用蒙特卡洛（Monte Carlo）rollouts来估计每个步骤的Q值，并根据这些Q值为每个步骤分配二元目标标签。\n    *   探索了三种不同的标签分配方法：绝对Q值阈值（Abs-Q）、相对有效奖励阈值（Rel-Effective）和相对比率阈值（Rel-Ratio）。\n3.  **使用强化学习训练裁判模型**：\n    \n    *   将每个训练提示分解为步骤级训练提示，模型需要根据问题描述和历史推理路径，对新的推理块进行评估，并生成自己的CoT分析，最后输出最终判断。\n    *   通过下采样多数类来平衡训练集，以稳定RL训练。\n    *   使用GRPO（Generalized Reinforcement Policy Optimization）算法进行优化。\n\n实验结果\n\n论文通过一系列实验验证了STEPWISER方法的有效性：\n\n1.  **ProcessBench基准测试**：\n    \n    *   STEPWISER在所有学习信号（Abs-Q、Rel-Ratio、Rel-Effective）和模型规模下，均显著优于传统的SFT训练的判别式裁判和其他现有方法。\n    *   例如，使用7B模型和Rel-Effective信号时，STEPWISER的平均得分为61.9，而SFT判别式基线的得分为39.7。\n2.  **推理时搜索性能**：\n    \n    *   使用STEPWISER裁判模型，1.5B模型的平均准确率从31.2%提高到36.9%，7B模型的平均准确率从57.4%提高到63.3%。\n    *   这表明STEPWISER能够有效地识别并纠正错误的推理步骤，从而提高最终解决方案的质量。\n3.  **训练数据选择**：\n    \n    *   使用STEPWISER选择的数据进行微调的模型，在MATH500上的平均准确率达到63.0%，优于原始基础模型（60.1%）和使用判别式裁判选择的数据（61.9%）。\n    *   这表明STEPWISER能够更有效地识别高质量的训练数据，从而提高下游模型的性能。\n\n关键结论\n\n*   **生成式CoT推理和在线RL训练的重要性**：实验结果表明，生成式CoT推理和在线RL训练是STEPWISER方法取得优异性能的关键因素。\n*   **相对信号的有效性**：相对于绝对Q值阈值，使用相对信号（如Rel-Effective和Rel-Ratio）进行步骤标注能够更有效地训练裁判模型。\n*   **数据集平衡的必要性**：通过平衡训练集，可以显著提高模型的稳定性和最终性能。\n\n进一步探索的方向\n\n论文还提出了多个可以进一步探索的方向，包括将STEPWISER应用于更复杂的推理任务、多模态数据集、跨领域应用、动态环境中的推理、开发更高效的训练方法、提高模型的解释性和可解释性、研究对抗攻击和鲁棒性、与其他方法的结合以及整合人类反馈等。\n\n总的来说，STEPWISER方法通过生成式裁判和强化学习，有效地监督和改进了LLMs的多步推理策略，提高了模型在复杂问题解决中的逻辑性和准确性。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是StepWiser%3A%20Stepwise%20Generative%20Judges%20for%20Wiser%20Reasoning，链接是  https://arxiv.org/pdf/2508.19229  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19229  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1756424830920","title":"Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered Smartphone Agents","description":"Smartphones bring significant convenience to users but also enable devices to extensively record various types of personal information. Existing smartphone agents powered by Multimodal Large Language Models (MLLMs) have achieved remarkable performance in automating different tasks. However, as the cost, these agents are granted substantial access to sensitive users' personal information during this operation. To gain a thorough understanding of the privacy awareness of these agents, we present the first large-scale benchmark encompassing 7,138 scenarios to the best of our knowledge. In addition, for privacy context in scenarios, we annotate its type (e.g., Account Credentials), sensitivity level, and location. We then carefully benchmark seven available mainstream smartphone agents. Our results demonstrate that almost all benchmarked agents show unsatisfying privacy awareness (RA), with performance remaining below 60% even with explicit hints. Overall, closed-source agents show better privacy ability than open-source ones, and Gemini 2.0-flash achieves the best, achieving an RA of 67%. We also find that the agents' privacy detection capability is highly related to scenario sensitivity level, i.e., the scenario with a higher sensitivity level is typically more identifiable. We hope the findings enlighten the research community to rethink the unbalanced utility-privacy tradeoff about smartphone agents. Our code and benchmark are available at https://zhixin-l.github.io/SAPA-Bench.","url":"https://huggingface.co/papers/2508.19493","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.920Z","metadata":{"authors":["Zhixin Lin","Jungang Li","Shidong Pan","Yibo Shi","Yue Yao","Dongliang Xu"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CR","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2508.19493.pdf","arxivUrl":"https://arxiv.org/abs/2508.19493","arxivId":"2508.19493","coolPaperUrl":"https://papers.cool/arxiv/2508.19493","published":"2025-08-27T00:41:28Z","updated":"2025-08-27T00:41:28.000Z","zh_summary":"智能手机为用户带来了显著的便利，但也使得设备能够广泛记录各种个人信息。目前由多模态大型语言模型（MLLMs）驱动的智能手机助手在自动化不同任务方面取得了显著的表现。然而，这些助手在操作过程中被授予了对敏感用户个人信息的广泛访问权限。为了深入了解这些助手的隐私意识，我们首次提出了一个覆盖7138个场景的大规模基准测试，以我们所知为准。此外，针对场景中的隐私上下文，我们注释了其类型（例如：账户凭证）、敏感性等级和位置。然后，我们仔细评估了七个主流智能手机助手。我们的结果表明，几乎所有评估的助手都显示出令人不满意的隐私意识（RA），即使有明确的提示，其表现仍低于60%。总体而言，闭源助手的隐私能力优于开源助手，而Gemini 2.0-flash的表现最佳，达到了67%的RA。我们还发现，助手的隐私检测能力与场景的敏感性等级高度相关，即敏感性等级较高的场景通常更易被识别。我们希望这些发现能启发研究界重新思考智能手机助手之间不平衡的效用-隐私权衡。我们的代码和基准测试可在https://zhixin-l.github.io/SAPA-Bench上获得。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 该工作聚焦以下核心问题：\n\n*   **智能手机智能体在自动化任务过程中对用户隐私的忽视**。现有基于多模态大语言模型（MLLM）的智能手机代理虽然能高效完成发消息、网购等任务，但在执行过程中往往无差别地访问屏幕内容、剪贴板、权限等敏感数据，缺乏对隐私风险的识别与提示机制，导致“效用-隐私”失衡。\n    \n*   **缺乏系统、可量化的隐私感知基准**。已有评测（如 Android-in-the-Wild、GUI Odyssey、MobileSafetyBench）主要关注任务成功率、延迟或行为安全，并未对“代理能否识别并妥善处理隐私敏感操作”进行专门度量，因而无法比较不同方案的隐私能力，也难以指导隐私驱动的系统设计。\n    \n*   **填补隐私感知全链路评估空白**。论文提出 SAPA-Bench，首次从“识别-定位-分类-定级-风险响应”五个维度建立细粒度指标（PRR、PLR、PLAR、PCAR、RA），并构建 7,138 个真实场景的大规模数据集，系统衡量主流代理在隐私意识上的差距，进而推动兼顾功能与隐私保护的下一代智能手机智能体研究。\n    \n\n**Q**: 有哪些相关研究？\n\n**A**: 与本文密切相关的研究可归纳为三类，每类均列出代表性工作并指出其与 SAPA-Bench 的关联与区别。\n\n1\\. 基于 MLLM 的智能手机智能体\n\n工作\n\n核心贡献\n\n与隐私的关联\n\n与本文差异\n\n**AppAgent** (Li et al., 2024b)\n\n提出“探索-部署”两阶段框架，先离线观察 UI 构建知识库，再执行任务\n\n未涉及隐私检测或用户确认机制\n\n仅关注任务成功率，无隐私维度\n\n**Mobile-Agent** (Wang et al., 2024a)\n\n纯视觉驱动，仅依赖截图完成多步操作，提出 Mobile-Eval 基准\n\n默认获取屏幕全部信息，未过滤敏感内容\n\n基准不含隐私标签\n\n**Show-UI** (Lin et al., 2025)\n\n视觉 token 选择 + 流式推理，提升 UI 定位与跨平台效率\n\n无风险提醒模块\n\n同样缺乏隐私感知评测\n\n**SpiritSight Agent** (Huang et al., 2025)\n\n通用 block 解析，提高跨应用理解\n\n未讨论对敏感信息的识别\n\n功能导向，未评估隐私能力\n\n2\\. 移动隐私评估框架\n\n工作\n\n核心贡献\n\n与隐私的关联\n\n与本文差异\n\n**PIA 综述** (Iwaya et al., 2024)\n\n系统梳理隐私影响评估方法，强调从低到高多层级风险\n\n提供三级风险划分思路\n\n未针对智能手机智能体细化场景\n\n**三层次隐私需求分类** (Sangaroonsilp et al., 2023)\n\n在 issue report 中按高/中/低划分隐私需求\n\n为敏感度分级提供参考\n\n未涉及 GUI 场景或自动化代理\n\n**MobileSafetyBench** (Lee et al., 2024)\n\n评估移动代理在良/恶性任务中的行为安全\n\n关注越界行为惩罚\n\n不衡量“未识别隐私即无提示”的问题\n\n3\\. 现有 GUI/移动代理基准\n\n工作\n\n核心贡献\n\n与隐私的关联\n\n与本文差异\n\n**GUI Odyssey** (Lu et al., 2024)\n\n跨应用 GUI 导航的大规模数据集\n\n提供原始截图，无隐私标签\n\nSAPA-Bench 在其基础上补充隐私元数据\n\n**Android-in-the-Wild** (Rawles et al., 2023)\n\n大规模 Android 控制数据集\n\n无隐私维度\n\n仅用于任务完成率评测\n\n**SPA-bench** (Chen et al., 2024a)\n\n多样化任务类型与复杂度\n\n未涉及隐私感知\n\n不包含风险响应指标\n\n**SIUO** (Wang et al., 2024c)\n\n聚焦危险行为与犯罪活动安全\n\n关注输出安全而非输入隐私\n\n不评估代理对敏感输入的识别与提示\n\n综上，现有研究或聚焦任务成功率，或聚焦输出安全，均未提供针对“智能手机智能体在操作过程中识别并主动提示隐私风险”的系统基准与量化指标。SAPA-Bench 首次填补了这一空白。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过“构建基准 + 设计指标 + 大规模评测 + 提出改进策略”四步闭环，系统性地解决智能手机智能体隐私意识缺失的问题。\n\n1\\. 构建专用基准 SAPA-Bench\n\n*   **规模**：7,138 个真实场景，覆盖 50 款主流英文 App。\n*   **来源**：以 GUI Odyssey 与 OS-Atlas 的 UI 截图为底本，经五阶段（过滤→自动生成→人工初检→自动补标→交叉复检）流水线标注。\n*   **标注维度**\n    *   是否包含隐私泄露（Presence）\n    *   泄露模态（图像 / 指令）\n    *   隐私类别（8 类：账号凭证、支付、位置…）\n    *   敏感度级别（低 / 中 / 高）\n    *   期望风险提示（Response）\n\n2\\. 提出五维隐私评估指标\n\n指标\n\n含义\n\n覆盖环节\n\n**PRR** Privacy Recognition Rate\n\n正确识别隐私场景的比率\n\n识别\n\n**PLR** Privacy Localization Rate\n\n在识别出的场景中准确定位泄露模态的比率\n\n定位\n\n**PLAR** Privacy Level Awareness Rate\n\n对隐私级别（低/中/高）判断正确的比率\n\n定级\n\n**PCAR** Privacy Category Awareness Rate\n\n对隐私类别判断正确的比率\n\n分类\n\n**RA** Risk Awareness\n\n生成合理风险提醒的比例\n\n响应\n\n3\\. 对 7 个主流模型/智能体进行大规模评测\n\n*   **对象**：\n    *   开源智能体：Show-UI、SpiritSight Agent\n    *   通用开源 VLM：Qwen2.5-VL、InternVL 2.5、LLaVA-NeXT\n    *   闭源模型：Gemini 2.0-flash、GPT-4o\n*   **结果**：\n    *   所有模型 RA<70%，开源普遍 <30%；Gemini 2.0-flash 最高 67%。\n    *   敏感度越高越易识别，低敏感度场景 PRR 普遍 <40%。\n    *   闭源模型整体优于开源，归因于 RLHF 与安全对齐。\n\n4\\. 提出并验证改进策略\n\n*   **显式提示（Explicit Hint）**：在输入中加入“请检查隐私风险”类提示，可使 RA 提升 20–50 个百分点，验证提示工程的有效性。\n*   **数据与训练建议**：开源模型需在预训练与指令微调阶段引入更多隐私敏感场景，并采用 CoT、RLHF 等对齐策略。\n\n通过上述四步，论文不仅量化了当前智能手机智能体的隐私短板，也为后续“隐私感知模块”的设计与训练提供了可复现的基准与方法路径。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕 SAPA-Bench 共设计并执行了四类实验，系统评估 7 个主流模型在隐私感知全链路上的表现，并分析影响因素。\n\n* * *\n\n1\\. 主实验：五维隐私能力全面评测\n\n*   **目的**：用 PRR、PLR、PLAR、PCAR、RA 五个指标衡量模型对隐私场景的识别、定位、分级、分类与风险响应能力。\n*   **设置**：\n    *   7,138 个样本，每样本提供 UI 截图 + 用户指令。\n    *   结果与人工标注的 ground-truth 对比（RA 用 LLM 语义对齐打分）。\n*   **结果摘要**（见表 1）：\n    *   **PRR**：最高 Gemini 2.0-flash 75.62%，开源普遍 28–36%。\n    *   **PLR**：GPT-4o 在指令模态达 74.42%，其余均 <30%。\n    *   **PLAR / PCAR**：普遍 5–35%，显示细粒度能力不足。\n    *   **RA**：Gemini 67.14% 居首，开源最高 InternVL 2.5 51.66%。\n\n* * *\n\n2\\. 敏感度分层实验\n\n*   **目的**：验证“场景敏感度越高越易被检测”的假设。\n*   **做法**：将样本按低/中/高敏感度划分，分别计算 PRR。\n*   **结果**（表 2）：\n    *   低敏场景 PRR 仅 17–68%；高敏场景可升至 80–91%。\n    *   所有模型均呈现单调上升趋势，证实敏感度显著影响识别率。\n\n* * *\n\n3\\. 提示工程实验（Prompting Ablation）\n\n*   **目的**：测试不同提示策略对风险响应（RA）的提升效果。\n*   **条件**：\n    *   NH（No Hint）：无额外提示。\n    *   IH（Implicit Hint）：隐式提示“请考虑隐私”。\n    *   EH（Explicit Hint）：显式提示“请检查是否存在隐私泄露并给出警告”。\n*   **结果**（表 3 & 图 6）：\n    *   NH → IH：RA 提升 5–15 pp。\n    *   IH → EH：最大跃升，Gemini 67.14%，GPT-4o 55.03%，显著高于无提示基线。\n\n* * *\n\n4\\. 隐私-效用权衡分析\n\n*   **目的**：探讨“加强隐私保护是否会牺牲任务完成率”。\n*   **做法**：在 GUI-Odyssey 基准上同时报告各模型的 Success Rate (SR)，并与 RA 做散点图（图 5）。\n*   **结果**：\n    *   整体呈正相关趋势，但离散度大；Gemini 2.0-flash 在 SR 与 RA 均领先，显示通过专门对齐可同时提升两者。\n    *   开源模型普遍位于“低 SR + 低 RA”象限，提示需要针对性优化而非简单折中。\n\n* * *\n\n实验资源\n\n*   **硬件**：8×RTX 3090 GPU 服务器\n*   **开源模型规模**：统一 7–8 B（Show-UI 2 B）\n*   **闭源模型**：Gemini 2.0-flash、GPT-4o 通过 API 调用\n\n四类实验共同揭示了当前智能手机智能体在隐私感知上的显著短板，并验证了提示工程与专门对齐策略的有效性。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 SAPA-Bench 的延伸与深化，既可直接利用现有资源，也可引入新的技术路线，推动“隐私感知智能手机智能体”从实验室走向实际部署。\n\n* * *\n\n1\\. 数据与场景维度扩展\n\n*   **多语言与多文化场景**\n    \n    *   收集中文、日文、阿拉伯语等非英语界面，验证隐私语义的跨语言一致性。\n    *   研究不同文化对“敏感”定义的差异（如地址、宗教、政治言论）。\n*   **动态与长序列交互**\n    \n    *   将静态截图升级为连续屏幕录像，考察代理在 5–10 步操作链中的隐私记忆与累积风险。\n    *   引入“跨会话”场景：代理是否记得上一轮曾上传过通讯录，并在新一轮再次请求时提醒。\n*   **对抗与误导场景**\n    \n    *   构造 UI 欺骗（例如伪装成系统弹窗的钓鱼界面）测试代理的鲁棒性。\n    *   引入 OCR 误识、视觉遮挡等噪声，评估隐私检测在真实破损截图下的稳定性。\n\n* * *\n\n2\\. 模型与算法创新\n\n*   **专用隐私感知模块**\n    \n    *   设计轻量级隐私编码器（Privacy Encoder），与主任务模型并行运行，实现“双轨”推理：主轨完成任务，副轨实时风险评估。\n    *   探索“隐私-任务”联合训练目标：在损失函数中加入 PRR/RA 项，实现端到端优化。\n*   **可解释与可控输出**\n    \n    *   引入 Chain-of-Thought 可视化，让代理在给出风险提醒时同时输出“依据 UI 元素 + 指令片段”的引用。\n    *   研究用户可控粒度：允许用户设定“仅对金融类高敏场景提醒”或“全部提醒”。\n*   **联邦/端侧隐私对齐**\n    \n    *   利用联邦 RLHF，在不上传原始截图的前提下聚合用户反馈，持续改进隐私检测模型。\n    *   探索 7B 级小模型在端侧实时运行的量化与蒸馏方案，降低云端依赖。\n\n* * *\n\n3\\. 人机交互与用户体验\n\n*   **风险提醒的交互设计实验**\n    \n    *   A/B 测试不同提醒样式（弹窗 vs. 非侵入式横幅）对用户任务完成率与满意度的影响。\n    *   研究“一键撤销”机制：用户确认继续后，仍可在 3 秒内撤回敏感操作。\n*   **个性化隐私画像**\n    \n    *   允许用户自定义敏感度词典（如“我的家庭地址”“公司内网 IP”），代理据此动态调整检测阈值。\n    *   探索基于用户历史授权记录的上下文记忆，减少重复提醒造成的打扰。\n\n* * *\n\n4\\. 评估与标准化\n\n*   **持续基准维护**\n    \n    *   建立月度滚动更新机制，纳入新 App、新系统版本及新型隐私风险（如 AI 摘要上传）。\n    *   引入“隐私回归测试”：当模型升级时，自动跑 SAPA-Bench 子集，防止隐私能力回退。\n*   **扩展指标**\n    \n    *   **延迟指标**：测量从截图输入到风险提醒的端到端时延，确保实时性。\n    *   **用户干预率**：统计用户看到提醒后选择“取消”的比例，作为代理“过度打扰”或“漏报”的反向指标。\n*   **行业标准与合规**\n    \n    *   与 GDPR、CCPA 合规要求对齐，将“告知-同意”细粒度动作映射到 RA 指标，推动形成可审计的隐私合规报告。\n\n* * *\n\n5\\. 工具链与开放平台\n\n*   **在线对抗测试平台**\n    \n    *   提供 Web 接口，允许安全研究员上传自制 UI 截图或视频，即时获得模型隐私检测报告。\n    *   开放“红队”排行榜，鼓励社区提交新的欺骗场景，持续压力测试。\n*   **隐私提示模板商店**\n    \n    *   开源一套可插拔的 prompt 模板库（金融类、健康类、儿童模式等），开发者可按场景选用或微调。\n\n* * *\n\n通过上述方向的交叉推进，可将 SAPA-Bench 从静态评测集升级为“隐私感知智能体的持续演进生态”，在学术研究与产业落地之间形成闭环。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: **一句话总结**  \n本文首次提出并公开了大规模隐私感知基准 SAPA-Bench（7,138 场景、5 维指标），系统评测 7 个主流智能手机智能体后发现其隐私意识普遍不足（最佳 RA 仅 67%），并指出通过敏感度分级、显式提示与专门对齐可显著提升隐私保护，为“高效且隐私安全”的移动代理研究奠定基线。\n\n**核心内容速览**\n\n维度\n\n要点\n\n**问题**\n\n现有基于 MLLM 的智能手机代理在自动完成任务时，无差别访问敏感数据，缺乏识别-提示机制，导致效用-隐私失衡；尚无统一基准量化该缺陷。\n\n**基准**\n\nSAPA-Bench：7,138 个真实 UI 场景，覆盖 8 类隐私、3 级敏感度，附指令-期望风险提示。五阶段人机协同标注，保证质量。\n\n**指标**\n\nPRR（识别）、PLR（定位）、PLAR（定级）、PCAR（分类）、RA（风险响应），外加 SR（任务成功率）衡量隐私-效用权衡。\n\n**实验**\n\n评测 7 模型：开源智能体、通用 VLM、闭源 API。结果：<br>• 所有模型 RA<70%，开源普遍 <30%；<br>• 敏感度越高越易识别；<br>• 显式提示可将 RA 提升 20–50 pp；<br>• 闭源模型整体优于开源，RLHF 与安全对齐是关键。\n\n**贡献**\n\n① 首个面向智能手机代理的隐私感知基准；② 五维量化指标；③ 大规模评测揭示瓶颈；④ 验证提示工程与对齐策略的有效性。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Mind%20the%20Third%20Eye%21%20Benchmarking%20Privacy%20Awareness%20in%20MLLM-powered%20Smartphone%20Agents，链接是  https://arxiv.org/pdf/2508.19493  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19493  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1756424830920","title":"AudioStory: Generating Long-Form Narrative Audio with Large Language Models","description":"Recent advances in text-to-audio (TTA) generation excel at synthesizing short audio clips but struggle with long-form narrative audio, which requires temporal coherence and compositional reasoning. To address this gap, we propose AudioStory, a unified framework that integrates large language models (LLMs) with TTA systems to generate structured, long-form audio narratives. AudioStory possesses strong instruction-following reasoning generation capabilities. It employs LLMs to decompose complex narrative queries into temporally ordered sub-tasks with contextual cues, enabling coherent scene transitions and emotional tone consistency. AudioStory has two appealing features: (1) Decoupled bridging mechanism: AudioStory disentangles LLM-diffuser collaboration into two specialized components, i.e., a bridging query for intra-event semantic alignment and a residual query for cross-event coherence preservation. (2) End-to-end training: By unifying instruction comprehension and audio generation within a single end-to-end framework, AudioStory eliminates the need for modular training pipelines while enhancing synergy between components. Furthermore, we establish a benchmark AudioStory-10K, encompassing diverse domains such as animated soundscapes and natural sound narratives. Extensive experiments show the superiority of AudioStory on both single-audio generation and narrative audio generation, surpassing prior TTA baselines in both instruction-following ability and audio fidelity. Our code is available at https://github.com/TencentARC/AudioStory","url":"https://huggingface.co/papers/2508.20088","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.920Z","metadata":{"authors":["Yuxin Guo","Teng Wang","Yuying Ge","Shijie Ma","Yixiao Ge","Wei Zou","Ying Shan"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CV","cs.MM","cs.SD"],"pdfUrl":"https://arxiv.org/pdf/2508.20088.pdf","arxivUrl":"https://arxiv.org/abs/2508.20088","arxivId":"2508.20088","coolPaperUrl":"https://papers.cool/arxiv/2508.20088","published":"2025-08-27T17:55:38Z","updated":"2025-08-27T17:55:38.000Z","zh_summary":"最近在文本到音频（TTA）生成方面的进展在合成短音频片段方面表现出色，但在长篇叙事音频上却面临挑战，这需要时间连贯性和组合推理能力。为了解决这个问题，我们提出了AudioStory，这是一个统一的框架，将大型语言模型（LLMs）与TTA系统集成，以生成结构化的长篇音频叙事。AudioStory具备强大的指令跟随推理生成能力。它利用LLMs将复杂的叙事查询分解为具有时间顺序的子任务，并提供上下文线索，从而实现连贯的场景转换和情感基调一致性。AudioStory有两个吸引人的特性：（1）解耦的桥接机制：AudioStory将LLM-扩散器的协作拆分为两个专业组成部分，即用于事件间语义对齐的桥接查询和用于事件间一致性保留的残差查询。（2）端到端训练：通过在一个端到端的框架内统一指令理解和音频生成，AudioStory消除了模块化训练流程的需要，同时增强了组件之间的协同效应。此外，我们建立了一个基准集AudioStory-10K，涵盖动画声景和自然声音叙事等多种领域。大量实验表明，AudioStory在单音频生成和叙事音频生成方面表现出色，超越了以前的TTA基准，在指令跟随能力和音频保真度方面均有提升。我们的代码可在https://github.com/TencentARC/AudioStory获得。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 论文旨在解决**长时叙事音频（long-form narrative audio）生成**中的关键缺陷：现有文本到音频（TTA）模型虽能合成高质量短时音频片段，但在生成长时、结构化、且保持**时间连贯性**与**叙事逻辑**的音频序列时表现不足。具体挑战包括：\n\n*   **时间连贯性**：跨长时段保持主题、音效、情感基调的一致性。\n*   **叙事推理能力**：将复杂多模态指令（如“暴雨夜惊险追逐”）分解为逻辑有序的子事件，并精准控制其时序、强度与交互关系。\n\n为此，作者提出**AudioStory框架**，通过整合大语言模型（LLM）的推理能力与扩散模型的生成能力，实现**端到端的长时叙事音频生成**，并建立首个评估基准AudioStory-10k，填补该领域量化评估的空白。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文在第2节“Related Works”中系统梳理了与AudioStory密切相关的三类研究：\n\n1.  文本到音频生成（Text-to-Audio Generation, TTA）\n    \n    *   扩散/流匹配框架\n        *   Make-An-Audio \\[7\\]、AudioLDM \\[2, 8\\]、Tango \\[9, 10\\]、Audio Flamingo \\[11\\]、GenAu \\[12\\]、Fugatto \\[13\\]\n        *   Stable Audio 系列 \\[3\\]：基于大规模分层潜扩散，专注高保真短时音频\n    *   流匹配优化\n        *   VoiceBox \\[14\\]：连续正则流实现零样本风格迁移\n        *   AudioBox \\[15\\]、FlashAudio \\[16\\]：矫正流架构提升效率\n    *   对齐与偏好优化\n        *   TangoFlux \\[1\\]：引入 CLAP-ranked 偏好优化迭代改善文本-音频对齐\n2.  任意到任意多模态大模型（Any-to-Any Multimodal LLMs）\n    \n    *   组合扩散\n        *   CoDi \\[24\\]、CoDi-2 \\[18\\]：可组合扩散支持多模态输入输出\n    *   轻量对齐与离散序列建模\n        *   NExT-GPT \\[4\\]：轻量对齐层适配 LLM 到多模态任务\n        *   AnyGPT \\[19\\]：离散序列建模统一多模态处理\n    *   统一架构\n        *   Unified-IO2 \\[25\\]：统一自回归架构在多种模态与任务上达到 SOTA\n3.  组合音频生成（Compositional Audio Generation）\n    \n    *   代理工作流\n        *   WavJourney \\[26\\]、MM-StoryAgent \\[27\\]：将音频生成拆分为文本接口+独立解码器，零样本组合但缺乏细粒度声学控制\n\n总结：现有TTA方法聚焦短时描述性提示；任意到任意多模态LLM侧重语音或单段音频；代理式组合方法缺少端到端训练。AudioStory通过**LLM-扩散端到端联合训练**与**长时叙事推理机制**，填补长时、结构化、连贯音频生成的空白。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过 **AudioStory** 框架，从 **任务分解、桥接机制、训练范式** 三个层面系统性地解决长时叙事音频生成难题：\n\n1\\. 任务分解：Interleaved Reasoning Generation\n\n*   **分而治之策略**  \n    将复杂多模态指令拆分为 **时序化子事件**，每个子事件包含：\n    *   时间戳（起止时间）\n    *   事件描述（情节、角色、环境）\n    *   情绪基调与过渡提示\n*   **LLM 推理链**  \n    采用大语言模型（Qwen-2.5-3B-Instruct）完成：\n    1.  **Storyline Reasoning**：推断全局事件数与宏观时间线\n    2.  **Interleaved Generation**：逐一生成每段音频的 **caption**、**duration** 及桥接 token（见 4.1 节）\n\n2\\. 桥接机制：Decoupled Bridging Tokens\n\n*   **语义-残差双 token 设计**  \n    解决文本→音频模态鸿沟：\n    *   **Semantic Token**（T₅ 文本 token）：承载高层语义（事件、情绪、场景）\n    *   **Residual Token**（可学习向量）：捕获跨事件的细粒度声学细节（音色、混响、动态）\n*   **融合方式**  \n    通过 **多头交叉注意力** 融合两类 token，生成条件向量  \n    {% raw %}\\[ \\\\mathbf{H}_{\\\\text{bridge}} = \\\\text{Cross-Attn}(T_{\\\\text{semantic}}, T\\_{\\\\text{residual}}, T\\_{\\\\text{residual}}) \\]{% endraw %}  \n    作为 DiT 的条件输入（见 4.2 节）\n\n3\\. 训练范式：Progressive End-to-End Training\n\n*   **三阶段渐进训练**（图 2c）\n    1.  **Stage-I：单音频生成**\n        *   Warm-up：仅训练 LLM 的 LoRA 与语义 token 投影层（MSE 监督）\n        *   Whole：联合训练 LLM、投影层、交叉注意力与 DiT（MSE + Flow Loss）\n    2.  **Stage-II：单音频理解-生成统一**  \n        引入音频理解数据（AudioSetCaps-QA 等），冻结音频编码器，继续联合优化\n    3.  **Stage-III：长音频统一**  \n        引入 AudioStory-10k 长音频数据，执行 **Interleaved Reasoning Generation** 任务，损失函数  \n        {% raw %}\\[ \\\\mathcal{L}_{\\\\text{S3}} = \\\\mathcal{L}_{\\\\text{mse}} + \\\\lambda\\_1 \\\\mathcal{L}_{\\\\text{text}} + \\\\lambda\\_2 \\\\mathcal{L}_{\\\\text{flow}} + \\\\lambda\\_3 \\\\mathcal{L}\\_{\\\\text{reason}} \\]{% endraw %}  \n        其中 λ₁=1, λ₂=0.2, λ₃=0.4（见 4.3 节）\n\n4\\. 评估与验证\n\n*   **AudioStory-10k 基准**  \n    首个长时叙事音频数据集，含 10k 条带时间戳与推理链的音频-文本对（自然声 + 动画声）\n*   **实验结果**  \n    在 150 秒级音频上，AudioStory 在 **指令遵循、一致性、音质** 全面超越 TTA 与 LLM+扩散基线（表 1）\n\n通过上述设计，AudioStory 实现了 **复杂指令理解 → 叙事分解 → 声学细节保持 → 长时连贯生成** 的端到端优化。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕 **长时叙事音频生成能力**、**短音频理解与生成能力**、**关键设计消融**、**人类主观评价** 以及 **扩展应用** 五个维度展开系统实验，具体设置与结果如下。\n\n* * *\n\n1 长时叙事音频生成（Long-form Narrative Audio Generation）\n\n*   **数据集**  \n    AudioStory-10K（10 k 条 150 s 级音频，自然声 5.3 k + 动画声 4.7 k），85 % / 15 % 训练-测试划分\n*   **基线**  \n    • 纯 TTA：AudioLDM2、TangoFlux  \n    • LLM+扩散零拼接：LLM+TangoFlux、LLM+CoDi、LLM+NExT-GPT  \n    • 真值字幕上界：Caps(gt)+TangoFlux\n*   **评测维度**  \n    Instruct-following（Gemini-2.0-flash 0-5 分）、CLAP、Reasoning、Consistency、Coherence、FD、FAD\n*   **结果**（表 1）  \n    AudioStory 在 150 s 音频上全面领先：\n    *   CLAP 0.392（↑17.9 % vs LLM+TangoFlux）\n    *   FD 1.43 / FAD 3.00（显著低于基线）\n    *   Consistency 4.1 / Coherence 3.9（最长音频仍保持连贯）\n\n* * *\n\n2 短音频理解与生成（Short-form Audio Understanding & Generation）\n\n*   **短音频生成**（表 3，AudioCaps test）  \n    AudioStory-Base 在 FDopenl3、KLpasst、FAD、CLAP 均优于 TangoFlux 等 SOTA，验证桥接机制有效性\n*   **短音频理解**（表 2，Clotho & AudioCaps）  \n    • Clotho Caption：SPIDEr 24.1 / CIDEr 37.7  \n    • Clotho AQA：ACC 42.8 / B-ACC 60.6  \n    • AudioCaps Caption：SPIDEr 54.8 / CIDEr 83.2  \n    均大幅超越 CoDi、NExT-GPT 等统一模型\n\n* * *\n\n3 消融实验（Ablation Studies）\n\n维度\n\n关键发现\n\n结果\n\n**推理形式**（表 4）\n\nw/o reasoning → Consistency↓3.1, CLAP↓0.34；w/o interleaved 更严重\n\n推理链必不可少\n\n**桥接 token**（表 5）\n\n仅语义 token 或仅用音频特征监督均导致 FAD↑>10；文本语义+弱监督残差最优\n\n文本语义+残差 token 设计有效\n\n**端到端联合训练**（表 6）\n\n冻结 DiT 或移除残差 token 均显著恶化；全部放开 MM-DiT 最佳\n\n联合训练+残差 token 是关键\n\n**渐进训练顺序**（表 7）\n\nGen→Und 顺序优于 Und→Gen 与交错训练；无渐进训练最差\n\n先学生成再学理解最优\n\n* * *\n\n4 人类主观评价（Human Evaluation）\n\n*   **协议**  \n    30 位评测者盲听 150 段音频（50 指令 × 3 模型：AudioStory / LLM+TangoFlux / LLM+NExT-GPT），从 Instruct-follow、Consistency、Fidelity、Reasoning 四维度 0-5 分打分\n*   **结果**（表 8）  \n    AudioStory 四项均最高（4.23 / 4.68 / 4.37 / 4.22），与 Gemini 自动评价高度一致（Cohen κ=0.91，表 9）\n\n* * *\n\n5 扩展应用实验\n\n*   **视频配音（Video Dubbing）**  \n    输入无声视频 → Gemini-2.5-Pro 解析事件 → AudioStory 生成同步 Tom & Jerry 风格音轨（图 5）\n*   **音频续写（Audio Continuation）**  \n    给定前段音频+指令 → 模型推理后续事件 → 逐段生成并拼接（图 6）\n\n* * *\n\n综上，实验覆盖 **定量指标、人类主观、消融、跨场景应用**，充分验证了 AudioStory 在长时叙事音频生成中的有效性与鲁棒性。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 AudioStory 后续研究的切入点，按技术深度与应用广度分层列出：\n\n* * *\n\n1 模型架构层面\n\n*   **多生成器协同**  \n    引入 **并行扩散或流匹配分支** 处理重叠声源（如同时出现对白、环境声、音乐），通过 **混合专家路由** 动态分配声学空间，缓解长序列中声源冲突。\n*   **统一自回归架构**  \n    将文本、语义 token、残差 token 统一为离散序列，构建 **单一 Transformer** 同时完成理解与生成，消除 LLM-DiT 双阶段延迟，实现毫秒级流式生成。\n*   **跨模态对齐粒度细化**  \n    在残差 token 中显式建模 **空间定位**（方位、距离）与 **物理属性**（材质、反射），结合神经辐射场或声学模拟器，提升沉浸式场景一致性。\n\n* * *\n\n2 训练策略层面\n\n*   **课程式时长扩展**  \n    从 30 s → 150 s → 10 min 渐进提升最大时长，配合 **动态记忆压缩**（如 KV-Cache 丢弃与重建）解决长序列显存瓶颈。\n*   **双向生成-理解对齐**  \n    引入 **对比式联合损失**：让同一音频的生成 token 与理解 token 在共享空间内互作正样本，不同音频作负样本，强化语义-声学一致性。\n*   **强化学习微调**  \n    以人类偏好奖励模型（如 CLAP-Rank 或 UTMOS）为奖励，使用 **PPO 或 DPO** 微调残差 token 生成策略，直接优化主观听感。\n\n* * *\n\n3 数据与评估层面\n\n*   **细粒度事件级标注**  \n    扩展 AudioStory-10K 至 **多语言、多文化叙事**（如中国戏曲、非洲鼓乐），并标注 **情感强度曲线** 与 **声学参数**（RT60、频谱质心），支持更复杂控制。\n*   **开放式基准任务**  \n    新增 **交互式叙事**（用户实时插入指令改变剧情走向）与 **跨模态一致性检测**（视频-音频唇同步、动作同步），推动模型从“生成”走向“协作”。\n*   **鲁棒性评估**  \n    设计 **对抗性文本扰动**（同义词替换、时序颠倒）与 **声学扰动**（噪声、混响），量化模型在极端条件下的指令保真度。\n\n* * *\n\n4 应用与系统层面\n\n*   **实时交互式故事引擎**  \n    结合 **WebRTC** 与 **边缘 LLM 推理**，实现用户语音输入 → 实时叙事音频流 → 低延迟回传，用于互动有声书或游戏动态配乐。\n*   **个性化声纹迁移**  \n    允许用户上传 3-5 秒参考音频，通过 **残差 token 快速适配** 将叙事内容迁移至目标音色（如用户本人旁白），兼顾隐私（本地 LoRA 微调）。\n*   **跨设备协同渲染**  \n    利用 **空间音频标准**（Ambisonics、Dolby Atmos）将生成音频元数据（声源位置、扩散度）分发至多扬声器或 AR/VR 终端，实现场景级沉浸。\n\n* * *\n\n5 理论分析层面\n\n*   **叙事连贯性度量**  \n    构建 **基于图神经网络的事件关系模型**，量化长音频中事件节点间的因果与情感依赖，为一致性评价提供可解释指标。\n*   **残差 token 信息论解释**  \n    通过 **互信息估计** 分析残差 token 与语义 token 的互补性，验证其是否捕获音频熵中未被文本描述的高阶统计量。\n\n* * *\n\n这些方向既可直接在 AudioStory 框架上迭代（如多生成器、课程式训练），也可拓展至交叉领域（空间音频、实时交互），为长时叙事音频的实用化与理论深化提供持续动力。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### AudioStory 论文要点速览\n\n维度\n\n核心内容\n\n**问题**\n\n现有文本到音频（TTA）模型只能生成短时片段，无法处理 **长时、结构化、时间连贯** 的叙事音频（如有声书、动画配乐）。\n\n**方案**\n\n**AudioStory**：将大语言模型（LLM）的推理能力与扩散音频生成器（DiT）融合，提出 **分而治之** 的端到端框架。\n\n**技术亮点**\n\n1\\. **Interleaved Reasoning Generation**：LLM 把复杂指令拆成带时间戳的 **事件链**，逐一生成 caption + 桥接 token。<br>2. **Decoupled Bridging**：语义 token（文本语义）+ 残差 token（细粒度声学细节）联合条件，提升保真与跨段连贯。<br>3. **三阶段渐进训练**：单音频 → 单音频理解-生成统一 → 长音频统一，逐步建立 LLM-DiT 协同。\n\n**数据与基准**\n\n构建首个 **AudioStory-10K** 数据集（10 k 条 150 s 级音频，含自然声与动画声），配套指令-推理-音频三元组及细粒度评测指标。\n\n**实验结果**\n\n• 长音频：150 s 场景下，CLAP、FD、FAD、一致性全面优于 TTA 与 LLM+扩散零拼接基线。<br>• 短音频：在 AudioCaps、Clotho 上同时刷新生成与理解榜单。<br>• 消融：推理链、残差 token、端到端联合训练缺一不可。<br>• 人类评测：30 人盲听，四项主观指标均领先，且与自动评测高度相关（κ=0.91）。\n\n**扩展应用**\n\n零样本实现 **视频配音**（视频→事件→音频）与 **音频续写**（给定前文→后续事件→音频）。\n\n**未来方向**\n\n多生成器协同处理重叠声源、统一自回归架构、实时交互叙事、个性化音色迁移、空间音频渲染等。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是AudioStory%3A%20Generating%20Long-Form%20Narrative%20Audio%20with%20Large%20Language%20Models，链接是  https://arxiv.org/pdf/2508.20088  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.20088  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1756424830929","title":"MotionFlux: Efficient Text-Guided Motion Generation through Rectified Flow Matching and Preference Alignment","description":"Motion generation is essential for animating virtual characters and embodied agents. While recent text-driven methods have made significant strides, they often struggle with achieving precise alignment between linguistic descriptions and motion semantics, as well as with the inefficiencies of slow, multi-step inference. To address these issues, we introduce TMR++ Aligned Preference Optimization (TAPO), an innovative framework that aligns subtle motion variations with textual modifiers and incorporates iterative adjustments to reinforce semantic grounding. To further enable real-time synthesis, we propose MotionFLUX, a high-speed generation framework based on deterministic rectified flow matching. Unlike traditional diffusion models, which require hundreds of denoising steps, MotionFLUX constructs optimal transport paths between noise distributions and motion spaces, facilitating real-time synthesis. The linearized probability paths reduce the need for multi-step sampling typical of sequential methods, significantly accelerating inference time without sacrificing motion quality. Experimental results demonstrate that, together, TAPO and MotionFLUX form a unified system that outperforms state-of-the-art approaches in both semantic consistency and motion quality, while also accelerating generation speed. The code and pretrained models will be released.","url":"https://huggingface.co/papers/2508.19527","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.929Z","metadata":{"authors":["Zhiting Gao","Dan Song","Diqiong Jiang","Chao Xue","An-An Liu"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2508.19527.pdf","arxivUrl":"https://arxiv.org/abs/2508.19527","arxivId":"2508.19527","coolPaperUrl":"https://papers.cool/arxiv/2508.19527","published":"2025-08-27T02:45:09Z","updated":"2025-08-27T02:45:09.000Z","zh_summary":"运动生成对虚拟角色和具体代理的动画至关重要。尽管最近的文本驱动方法取得了显著进展，但它们常常在实现语言描述与运动语义之间的精确对齐方面面临困难，并且在缓慢的多步骤推理的低效率上也存在问题。为了解决这些问题，我们推出了 TMR++ 对齐偏好优化（TAPO），这是一个创新框架，它将细微的运动变化与文本修饰符对齐，并结合迭代调整以加强语义基础。为了进一步实现实时合成，我们提出了 MotionFLUX，这是一个基于决定性修正流匹配的高速生成框架。与传统的扩散模型（需要数百次去噪步骤）不同，MotionFLUX 在噪声分布和运动空间之间构建了最佳运输路径，促进了实时合成。线性化概率路径减少了对典型序列方法的多步骤采样的需求，在不牺牲运动质量的情况下显著加快了推理时间。实验结果表明，TAPO 和 MotionFLUX 共同形成了一个统一的系统，在语义一致性和运动质量方面优于最先进的方法，同时还加速了生成速度。代码和预训练模型将会发布。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 该论文针对文本驱动的运动生成任务，聚焦以下两大核心难题并提出系统性解决方案：\n\n1.  **语义对齐不精确**  \n    现有方法难以将复杂、细粒度的自然语言描述（如“突然停下并回头瞥一眼”）与运动语义精确对齐，导致生成动作与文本指令不符。\n    \n2.  **推理效率低下**  \n    基于扩散模型的SOTA方法（如MDM、MotionDiffuse）需数十步去噪，单序列生成耗时14–24秒，无法满足实时或交互场景需求。\n    \n\n为突破上述瓶颈，论文提出：\n\n*   **MotionFLUX**：首个将\\*\\*整流流匹配（rectified flow matching）\\*\\*引入文本到运动生成的框架，通过确定性速度场实现1–5步高速合成，推理速度提升4800×（对比MDM）。\n*   **TAPO（TMR++ Aligned Preference Optimization）**：无需人工标注或外部奖励模型，利用内部TMR++评分器在线构建偏好对，通过迭代式偏好优化强化复杂文本与运动的细粒度对齐。\n\n**Q**: 有哪些相关研究？\n\n**A**: 以下研究按主题分类，与 MotionFlux 的核心技术路线（文本-运动对齐、扩散/流模型、偏好优化）直接相关：\n\n1\\. 文本驱动的运动生成\n\n*   **早期对齐方法**\n    \n    *   Language2Pose (Ahuja & Morency 2019)\n    *   MotionClip (Tevet et al. 2022a) – 将 CLIP 空间与运动空间对齐\n    *   TEMOS (Petrovich et al. 2022) – Transformer-VAE 文本到运动框架\n*   **扩散模型**\n    \n    *   MDM (Tevet et al. 2022b) – 首个扩散式运动生成\n    *   MotionDiffuse (Zhang et al. 2024) – 14 s/序列，14 步去噪\n    *   MotionLCM (Dai et al. 2024) – 潜空间一致性模型，3× 实时加速但仍需多步\n*   **数据集**\n    \n    *   HumanML3D (Guo et al. 2022a) – 44 k 文本-运动对，当前主流基准\n    *   TM2T (Guo et al. 2022b) – 双向文本↔运动生成数据集\n\n2\\. 流匹配与整流流\n\n*   **Flow Matching 基础**\n    \n    *   Flow Matching (Lipman et al. 2022) – 连续归一化流的回归式训练\n    *   Rectified Flow (Liu et al. 2022) – 直线概率路径，少步采样\n*   **视觉/音频领域应用**\n    \n    *   FLUX (Esser et al. 2024) – 高分辨率图像的整流 Transformer\n    *   TangoFlux (Hung et al. 2024) – 文本到音频的整流流 + CLAP 偏好对齐\n\n3\\. 偏好优化与奖励建模\n\n*   **LLM 对齐**\n    \n    *   InstructGPT (Ouyang et al. 2022) – RLHF 训练奖励模型\n    *   DPO (Rafailov et al. 2023) – 直接偏好优化，无需显式奖励模型\n    *   sDPO (Kim et al. 2024a) – 迭代式小批量偏好更新\n*   **扩散/流模型对齐**\n    \n    *   Diffusion-DPO (Wallace et al. 2024) – 将 DPO 适配扩散模型\n    *   CLAP-Ranked Preference (Hung et al. 2024) – 用 CLAP 作为音频-文本奖励代理\n\n4\\. 运动表示与检索\n\n*   **TMR++ (Bensabath et al. 2024)** – 跨数据集文本-运动检索框架，被本文直接用作内部奖励代理\n\n上述工作共同构成了 MotionFlux 的学术背景：\n\n*   **扩散/流模型**提供生成基础；\n*   **TMR++** 解决运动领域缺乏现成奖励模型的难题；\n*   **DPO 及其变体** 为迭代式偏好对齐提供优化范式。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过两条互补的技术路线协同解决“语义对齐不精确”与“推理效率低下”两大难题：\n\n* * *\n\n1\\. 高效生成：MotionFLUX（整流流匹配）\n\n*   **核心思想**  \n    用**确定性整流流（rectified flow）**替代传统扩散模型，学习一条从噪声到运动潜码的**直线最优传输路径**，将采样步骤压缩至 1–5 步。\n    \n*   **技术实现**\n    \n    *   **训练阶段**：最小化速度回归损失  \n        ${% raw %}$ \\\\mathcal{L}_{\\\\text{FM}} = \\\\mathbb{E}_{x\\_1,x\\_0,t}|v(x\\_t,t|c;\\\\theta)-(x\\_0-x\\_1)|^2 ${% endraw %}$  \n        其中 $x\\_t=(1-t)x\\_1+tx\\_0$，$v$ 为网络预测的时变速度场。\n    *   **推理阶段**：Euler 一步或几步 ODE 求解  \n        $$ x\\_{t+\\\\epsilon}=x\\_t+\\\\epsilon,v(x\\_t,t|c;\\\\theta) $$\n    *   **网络结构**：43 M 参数的混合 Transformer（1×MMDiT 跨模态融合 + 2×DiT 时序推理），兼顾容量与速度。\n*   **效果**  \n    单句生成 5 ms，比 MDM 快 4800×，比 MotionLCM 快 3×，且 FID 更低（表 1）。\n    \n\n* * *\n\n2\\. 语义对齐：TAPO（TMR++ 对齐偏好优化）\n\n*   **核心思想**  \n    在缺乏人工标注或现成奖励模型的条件下，用内部 **TMR++ 检索模型** 作为**代理奖励函数**，在线构造偏好对并进行**迭代式 DPO** 微调，持续强化文本-运动细粒度对齐。\n    \n*   **三阶段循环流程**\n    \n    1.  **在线数据生成**：当前策略 $\\\\pi\\_k$ 为每个文本提示生成 $N$ 条候选运动。\n    2.  **偏好构造**：TMR++ 计算文本-运动余弦相似度，选最高/最低分构成“胜/负”对 $(x^w,x^l,y)$。\n    3.  **偏好优化**：最小化改进的 DPO-FM 损失  \n        ${% raw %}$ \\\\mathcal{L}_{\\\\text{TAPO}} = \\\\mathcal{L}_{\\\\text{DPO-FM}} + \\\\alpha,\\\\mathcal{L}_{\\\\text{FM}}(x^w) ${% endraw %}$  \n        其中 {% raw %}$\\\\mathcal{L}_{\\\\text{DPO-FM}}${% endraw %} 拉大胜负样本差距，{% raw %}$\\\\mathcal{L}\\_{\\\\text{FM}}${% endraw %} 防止胜样本质量漂移。\n*   **在线迭代 vs 离线**  \n    每轮重新采样文本并生成新运动，避免 reward hacking；实验显示离线训练第 2 轮即饱和下降，而在线训练持续提升（图 5）。\n    \n\n* * *\n\n3\\. 系统级协同\n\n*   **两阶段训练**\n    \n    1.  **表示学习**：在大规模 HumanML3D 上用 {% raw %}$\\\\mathcal{L}\\_{\\\\text{FM}}${% endraw %} 预训练 MotionFLUX-V1。\n    2.  **偏好对齐**：冻结 VAE，用 TAPO 迭代 3 轮得到 MotionFLUX-Ultra，R-Precision 从 0.530 提升到 0.536（表 1）。\n*   **结果**  \n    在 HumanML3D 上同时取得\n    \n    *   最低 AITS（5 ms）\n    *   最高 R-Precision 与最低 MM Dist（语义对齐最优）\n    *   最低 FID（运动质量最优）  \n        实现“实时 + 高保真 + 强对齐”的三重目标。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕 **HumanML3D** 基准，从 **客观指标、人工感知、推理效率、消融分析** 四个维度展开系统实验，核心结果汇总如下：\n\n* * *\n\n1\\. 主实验：与 SOTA 的定量对比\n\n指标\n\n含义\n\nMotionFlux-ultra(5 ms)\n\n次优方法\n\n提升\n\n**AITS ↓**\n\n单句推理时间\n\n**0.005 s**\n\nMotionLCM 0.030 s\n\n**6× 更快**\n\n**R-Precision ↑**\n\nTop-1 文本-运动匹配\n\n**0.536**\n\nMotionDiffuse 0.491\n\n↑ 9.2 %\n\n**FID ↓**\n\n生成与真实分布距离\n\n**0.078**\n\nMotionLCM 0.467\n\n↓ 83 %\n\n**MM Dist ↓**\n\n文本-运动嵌入距离\n\n**2.84**\n\nMotionDiffuse 3.113\n\n↓ 8.8 %\n\n**Diversity →**\n\n特征方差\n\n**9.531**\n\n真实数据 9.503\n\n持平\n\n**MultiModality ↑**\n\n同一文本多样性\n\n**1.998**\n\nMDM 2.799\n\n可比拟\n\n\\> 注：所有指标均在 20 次独立运行下报告 95 % 置信区间（表 1）。\n\n* * *\n\n2\\. 细粒度语义对齐可视化\n\n*   **随机未见提示测试**  \n    用 ChatGPT-o3 生成 3 条训练集外复杂提示（含“glance”“left/right”等细粒度动词）。\n    *   MotionFlux 准确还原所有关键动作；\n    *   MotionLCM/MDM/MLD 出现“左右混淆”“遗漏 glance”等错误（图 4）。\n\n* * *\n\n3\\. 推理速度对比\n\n方法\n\n每序列耗时\n\n相对加速\n\nMDM\n\n24.74 s\n\n1×\n\nMotionDiffuse\n\n14.74 s\n\n1.7×\n\nMLD\n\n0.217 s\n\n114×\n\nMotionLCM\n\n0.030 s\n\n825×\n\n**MotionFlux**\n\n**0.005 s**\n\n**4800×**\n\n* * *\n\n4\\. 消融与诊断实验\n\n4.1 在线 vs 离线 TAPO\n\n*   **设置**：固定离线数据集 vs 每轮重新采样生成新数据\n*   **结果**（图 5、正文 4.3）：\n    *   离线训练第 2 轮 TMR++ 分数开始下降，FID 急剧上升；\n    *   在线训练 4 轮仍持续提升，验证 **动态数据生成** 的必要性。\n\n4.2 Best-of-N 验证 TMR++ 作为代理奖励\n\nN\n\nFID ↓\n\nTMR++ ↑\n\nDiversity →\n\n1\n\n0.102\n\n0.803\n\n9.635\n\n5\n\n0.093\n\n0.812\n\n9.576\n\n10\n\n**0.086**\n\n0.819\n\n9.531\n\n15\n\n0.087\n\n**0.821**\n\n9.542\n\n\\> 结论：TMR++ 能有效排序运动质量，提升 N 可同时改善 FID 与对齐分数，且不牺牲多样性（表 2）。\n\n* * *\n\n5\\. 训练与实现细节\n\n*   **预训练**：500 epoch，AdamW，lr 1e-4，单 A100，batch 64。\n*   **TAPO**：3 轮迭代，每轮 8 epoch，lr 1e-5，batch 32，α 可学习。\n\n* * *\n\n综上，实验从 **定量指标、视觉质量、速度、消融** 四个角度验证了 MotionFlux 在 **实时性、保真度、语义对齐** 上的全面优势。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可作为 MotionFlux 的后续研究切入点，按“效率-质量-通用性-评价”四个层面展开：\n\n* * *\n\n1\\. 效率极限再推进\n\n*   **自适应步长 ODE 求解器**  \n    当前固定步长 Euler 已可达 5 ms，可引入 Dormand–Prince 或 RK-45 自适应步长，在保持精度的同时进一步压缩推理步数至 <3 ms。\n*   **蒸馏到更小网络**  \n    将 43 M 参数的 MotionFlux 蒸馏至 10 M 以内轻量化模型，适配移动端或 WebGL 实时推理。\n\n* * *\n\n2\\. 质量与可控性提升\n\n*   **物理一致性正则化**  \n    在流匹配损失中加入地面反作用力、动量守恒等物理约束项，减少脚滑、穿模等伪影。\n*   **分层控制接口**  \n    将文本描述拆分为“高层意图 + 低层约束”两级输入，允许用户显式指定关节轨迹或时空边界框，实现细粒度编辑。\n\n* * *\n\n3\\. 通用性与跨域迁移\n\n*   **跨数据集迁移**  \n    在 AMASS、BABEL 等多动作库上验证整流流匹配 + TAPO 的零样本/少样本能力，研究如何自动对齐不同骨架拓扑。\n*   **多模态扩展**  \n    把框架推广到 **文本+音频**（音乐节拍驱动舞蹈）或 **文本+场景**（障碍物规避）联合条件生成，测试 MMDiT 在多模态融合上的可扩展性。\n\n* * *\n\n4\\. 评价与偏好建模\n\n*   **人类主观实验**  \n    设计 A/B 测试验证 TMR++ 分数与人类主观一致性，量化“细粒度动作正确率”而非仅整体匹配度。\n*   **动态偏好权重**  \n    将 β、α 设为文本复杂度或用户历史反馈的函数，实现个性化对齐（例如对舞蹈类文本更强调节奏，对体育类文本更强调物理合理性）。\n*   **开放式词汇运动检索**  \n    利用 TMR++ 的跨模态空间支持开放式词汇（open-vocabulary）检索，反向验证生成质量：若检索 Top-1 命中生成结果，则说明语义对齐可靠。\n\n* * *\n\n5\\. 训练策略创新\n\n*   **课程式 TAPO**  \n    先对简单短句进行偏好优化，再逐步增加文本长度与复杂度，避免早期迭代即陷入局部偏好。\n*   **多智能体对抗式偏好**  \n    引入“判别器-生成器”双塔结构：判别器实时更新以发现新 failure cases，生成器用 TAPO 快速修复，形成持续博弈式改进。\n\n* * *\n\n这些方向既可直接在现有代码库上增量实验，也能拓展到更广泛的文本-驱动动画、机器人控制等下游任务。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### MotionFlux 论文一句话总结  \n首次将**整流流匹配**与**在线偏好优化**结合，实现**毫秒级、高保真、细粒度文本-运动生成**。\n\n* * *\n\n1\\. 问题\n\n*   **语义对齐难**：复杂文本提示与运动细节难精确对应。\n*   **推理慢**：扩散 SOTA 需 14–24 s，无法实时。\n\n* * *\n\n2\\. 方法\n\n*   **MotionFLUX**\n    \n    *   用**整流流匹配**学直线速度场，1–5 步 ODE 采样，5 ms/序列。\n    *   43 M 混合 Transformer（MMDiT+DiT），潜空间 VAE 编码。\n*   **TAPO**\n    \n    *   以内部 **TMR++ 检索模型** 为代理奖励，在线生成偏好对。\n    *   改进 DPO-FM 损失：  \n        ${% raw %}$ \\\\mathcal{L}_{\\\\text{TAPO}}=\\\\mathcal{L}_{\\\\text{DPO-FM}}+\\\\alpha\\\\mathcal{L}\\_{\\\\text{FM}}(x^w) ${% endraw %}$\n    *   三轮迭代，无需人工标注。\n\n* * *\n\n3\\. 实验\n\n*   **HumanML3D** 上全面领先：\n    *   **速度**：0.005 s，4800× 快于 MDM。\n    *   **质量**：FID 0.078，R-Precision 0.536，均列第一。\n    *   **细粒度对齐**：未见提示仍准确还原“glance/左右”等细节。\n*   **消融**：在线数据生成显著优于离线，Best-of-N 验证 TMR++ 有效性。\n\n* * *\n\n4\\. 贡献\n\n*   首个整流流文本-运动框架 MotionFLUX。\n*   自监督对齐框架 TAPO，解决无奖励模型难题。\n*   实时、高质、强对齐的统一系统，代码与模型将开源。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是MotionFlux%3A%20Efficient%20Text-Guided%20Motion%20Generation%20through%20Rectified%20Flow%20Matching%20and%20Preference%20Alignment，链接是  https://arxiv.org/pdf/2508.19527  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19527  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-13-1756424830930","title":"SEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models","description":"Evaluating whether vision-language models (VLMs) reason consistently across representations is challenging because modality comparisons are typically confounded by task differences and asymmetric information. We introduce SEAM, a benchmark that pairs semantically equivalent inputs across four domains that have existing standardized textual and visual notations. By employing distinct notation systems across modalities, in contrast to OCR-based image-text pairing, SEAM provides a rigorous comparative assessment of the textual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21 contemporary models, we observe systematic modality imbalance: vision frequently lags language in overall performance, despite the problems containing semantically equivalent information, and cross-modal agreement is relatively low. Our error analysis reveals two main drivers: textual perception failures from tokenization in domain notation and visual perception failures that induce hallucinations. We also show that our results are largely robust to visual transformations. SEAM establishes a controlled, semantically equivalent setting for measuring and improving modality-agnostic reasoning.","url":"https://huggingface.co/papers/2508.18179","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.930Z","metadata":{"authors":["Zhenwei Tang","Difan Jiao","Blair Yang","Ashton Anderson"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.AI","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2508.18179.pdf","arxivUrl":"https://arxiv.org/abs/2508.18179","arxivId":"2508.18179","coolPaperUrl":"https://papers.cool/arxiv/2508.18179","published":"2025-08-25T16:33:07Z","updated":"2025-08-25T16:33:07.000Z","zh_summary":"评估视觉语言模型（VLMs）在不同表征之间是否具有一致的推理能力是一个挑战，因为模态比较通常受到任务差异和信息不对称的影响。我们推出了SEAM，一个基准，它在四个具有现有标准化文本和视觉符号的领域中，配对语义等效的输入。通过在模态之间采用不同的符号系统，与基于OCR的图像文本配对形成对比，SEAM提供了对VLM文本符号和视觉空间推理能力的严格比较评估。在21个现代模型中，我们观察到了系统性的模态不平衡：尽管问题包含语义等效的信息，视觉在整体表现上经常滞后于语言，并且跨模态的一致性相对较低。我们的错误分析揭示了两个主要驱动因素：域符号中的标记化导致的文本感知失败和引发幻觉的视觉感知失败。我们还表明我们的结果在视觉变换方面基本稳健。SEAM建立了一个受控的、语义等效的环境，用于测量和改进模态无关的推理能力。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图解决的问题是评估视觉-语言模型（Vision-Language Models, VLMs）在不同模态（视觉和文本）之间是否能够一致地进行推理。具体来说，论文指出，尽管VLMs在理解和生成跨越视觉和文本领域的内容方面取得了快速进展，但在比较模型在不同模态上的推理能力时，存在以下挑战：\n\n1.  **模态比较的混淆因素**：通常的视觉和语言比较不仅改变了表示方式，还改变了任务本身，使得难以确定性能差异是由于推理差距还是任务难度不同。\n2.  **缺乏标准化的跨模态评估**：现有的方法要么缺乏严格的跨模态对齐，要么通过不对称的信息内容引入偏差，导致无法准确测量模态无关的推理能力。\n\n为了解决这些问题，论文提出了一个名为SEAM（Semantically Equivalent Across Modalities）的基准测试，旨在通过保持语义不变而仅改变表示方式，来严格评估VLMs的模态无关推理能力。SEAM利用了四个具有标准化符号系统的领域（国际象棋、化学、音乐和图论），确保了不同模态之间的语义等价性，并通过多种任务来评估模型在文本符号和视觉空间推理方面的能力。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了多个与视觉-语言模型（VLMs）和跨模态推理相关的研究工作。以下是一些关键的相关研究：\n\n视觉-语言模型（VLMs）的发展\n\n*   **早期的两流架构**：早期的VLMs使用两流架构分别处理视觉和文本输入，例如：\n    *   Lu et al. (2019) 提出的ViLBERT模型。\n    *   Tan & Bansal (2019) 提出的LXMERT模型。\n    *   Chen et al. (2020) 提出的UNITER模型。\n*   **统一模型**：随后出现了能够同时处理理解和生成任务的统一模型，例如：\n    *   Zhou et al. (2020) 提出的VL-BERT模型。\n    *   Zhang et al. (2021) 提出的VLMO模型。\n    *   Li et al. (2020) 提出的CLIP模型。\n*   **基于预训练模型的桥接**：一些工作尝试将预训练的视觉和语言模型桥接起来，例如：\n    *   Alayrac et al. (2022) 提出的Flamingo模型。\n    *   Awadalla et al. (2023) 提出的OpenFlamingo模型。\n    *   Li et al. (2022; 2023) 提出的BLIP系列模型。\n    *   Dai et al. (2023) 提出的InstructBLIP模型。\n*   **大型语言模型（LLMs）的集成**：将LLMs集成到VLMs中，显著提升了视觉指令跟随等能力，例如：\n    *   Zhu et al. (2023) 提出的MiniGPT系列模型。\n    *   Chen et al. (2023) 提出的MiniGPT-v2模型。\n    *   Zhang et al. (2023b) 提出的LLaMAAdapter系列模型。\n    *   Liu et al. (2023c; b; 2024a) 提出的LLaVA系列模型。\n*   **专有模型**：一些专有的VLMs展示了最先进的多模态推理能力，例如：\n    *   OpenAI的GPT系列模型（OpenAI, 2023; 2024a; b; 2025a; 2024c; d; b）。\n    *   Anthropic的Claude系列模型（Anthropic, 2024; 2025a; b）。\n    *   DeepMind的Gemini系列模型（DeepMind, 2023; 2024a; 2025a; Comanici et al., 2025）。\n*   **开源模型**：一些开源模型也取得了显著进展，例如：\n    *   Touvron et al. (2023a; b) 提出的LLaMA系列模型。\n    *   DeepMind的Gemma系列模型（DeepMind, 2024b; c; 2025b）。\n    *   Chen et al. (2024c; b; a; Zhu et al., 2025) 提出的InternVL系列模型。\n    *   Bai et al. (2023; Yang et al., 2024a; b; Wang et al., 2024b; Bai et al., 2025; Xu et al., 2025) 提出的Qwen系列模型。\n    *   Agrawal et al. (2024) 提出的Pixtral模型。\n\nVLM基准测试\n\n*   **早期基准测试**：早期的VLM基准测试主要评估基本的视觉理解能力，例如：\n    *   Antol et al. (2015) 提出的VQA基准测试。\n    *   Marino et al. (2019) 提出的OKVQA基准测试。\n    *   Lin et al. (2014) 提出的MSCOCO数据集。\n*   **近期基准测试**：近期的基准测试扩展了评估范围，涵盖了更复杂的能力，例如：\n    *   Yin et al. (2023) 提出的MMBench基准测试。\n    *   Xu et al. (2024) 提出的HallusionBench基准测试。\n    *   Li et al. (2024b) 提出的MathVista基准测试。\n    *   Liu et al. (2024b) 提出的MMBench基准测试。\n    *   Tong et al. (2024) 提出的MM-VET基准测试。\n    *   Yu et al. (2023) 提出的MM-VET基准测试。\n    *   Jiang et al. (2024) 提出的MMBench基准测试。\n    *   Ying et al. (2024) 提出的MMBench基准测试。\n    *   Fu et al. (2024) 提出的MMBench基准测试。\n*   **跨模态推理评估**：一些基准测试尝试评估跨模态推理能力，例如：\n    *   Yue et al. (2024a; b) 提出的MMMU系列基准测试。\n    *   Hao et al. (2025) 提出的EMMA基准测试。\n    *   Zhang et al. (2024b) 提出的基于OCR的图像-文本对基准测试。\n\n这些相关研究为本文提出的SEAM基准测试提供了背景和基础，展示了VLMs在多模态推理方面的进展和挑战。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过以下步骤来解决评估视觉-语言模型（VLMs）在不同模态之间是否能够一致地进行推理的问题：\n\n提出SEAM基准测试\n\n论文提出了一个名为\\*\\*SEAM（Semantically Equivalent Across Modalities）\\*\\*的基准测试，旨在通过保持语义不变而仅改变表示方式，来严格评估VLMs的模态无关推理能力。\n\n选择具有标准化符号系统的领域\n\nSEAM基准测试选择了四个具有标准化符号系统的领域，这些领域在视觉和文本模态中都有标准化的表示方式，确保了语义等价性：\n\n*   **国际象棋**：FEN符号表示与棋盘图像。\n*   **化学**：SMILES字符串与分子结构图。\n*   **音乐**：ABC符号表示与乐谱图像。\n*   **图论**：邻接矩阵与节点-边图。\n\n构建任务和数据集\n\n在每个领域中，论文定义了四个任务，每个任务包含200个问题，总计16个任务，3200个问题。这些问题以多项选择题的形式呈现，并且设计了具有挑战性的干扰选项，以校准任务难度。这些任务确保了在单一模态内可以完全解决，消除了联合推理的混淆因素。\n\n实验设置和模型评估\n\n论文评估了21个最新的VLMs，涵盖了专有模型和开源模型。实验设置包括零样本链式思考提示（zero-shot chain-of-thought prompting），并使用vLLM框架在8个A100 GPU上运行推理。论文还采用了OpenCompass协议来从模型输出中提取最终答案。\n\n分析结果\n\n论文通过以下方式分析了实验结果：\n\n*   **性能比较**：比较了模型在语言、视觉和视觉-语言模态上的准确率。\n*   **模态一致性**：计算了模型在不同模态之间的一致性，即模型在不同模态下对同一问题给出相同答案的比例。\n*   **错误分析**：分析了模型在不同模态下的错误模式，特别是文本感知错误和视觉感知错误。\n*   **鲁棒性检查**：通过应用常见的视觉变换（如分辨率变化、灰度转换和旋转）来验证结果的鲁棒性。\n\n主要发现\n\n*   **模态不平衡**：所有模型都表现出语言模态性能优于视觉模态，尽管问题包含语义等价的信息。\n*   **跨模态一致性低**：模型在不同模态之间的一致性相对较低，通常接近随机基线，表明模型在不同模态下的处理方式存在显著差异。\n*   **错误模式**：文本感知错误主要来自于对特定领域文本输入的分词问题，而视觉感知错误则导致了幻觉（hallucinations）。\n\n贡献\n\n论文的主要贡献包括：\n\n1.  提出了SEAM，这是第一个系统地控制跨模态语义等价性的基准测试，使得对跨模态推理的评估更加公平。\n2.  进行了全面的实证研究，首次测量了21个模型的受控跨模态不平衡。\n3.  分析了任务和模型之间的错误和差异，指出了现代VLMs中降低跨模态一致性的感知驱动错误模式，为未来研究提供了行动导向的见解。\n\n通过这些步骤，论文不仅揭示了当前VLMs在跨模态推理方面存在的问题，还为未来的研究提供了一个原则性的框架，以衡量和改进更具鲁棒性和真正智能的VLMs。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文进行了以下实验来评估视觉-语言模型（VLMs）在不同模态之间的推理能力：\n\n实验设置\n\n*   **模型选择**：评估了21个最新的VLMs，包括专有模型和开源模型。这些模型涵盖了不同的架构和规模。\n*   **模态评估**：对每个模型在三种模态下进行评估：语言（L）、视觉（V）和视觉-语言（VL）。\n*   **提示方式**：采用零样本链式思考提示（zero-shot chain-of-thought prompting）。\n*   **硬件配置**：使用vLLM框架在8个A100 GPU上运行推理。\n*   **答案提取**：采用OpenCompass协议从模型输出中提取最终答案，使用Qwen2.5-7B-Instruct模型作为答案提取器。\n\n实验结果分析\n\n*   **性能比较**：比较了模型在语言、视觉和视觉-语言模态上的准确率。\n    *   **表1**：展示了所有模型在三种模态下的准确率和模态间一致性。\n    *   **图2**：展示了模态间一致性与平均准确率之间的相关性。\n*   **模态一致性**：计算了模型在不同模态之间的一致性，即模型在不同模态下对同一问题给出相同答案的比例。\n    *   **表1**：展示了所有模型的模态间一致性。\n    *   **图5**：展示了不同模型在语言和视觉模态下的最终答案一致性。\n*   **错误分析**：分析了模型在不同模态下的错误模式，特别是文本感知错误和视觉感知错误。\n    *   **文本感知错误**：分析了模型在处理特定领域文本输入时的分词问题。\n    *   **视觉感知错误**：分析了模型在处理视觉输入时产生的幻觉（hallucinations）。\n*   **鲁棒性检查**：通过应用常见的视觉变换（如分辨率变化、灰度转换和旋转）来验证结果的鲁棒性。\n    *   **图4**：展示了视觉变换对Qwen2.5-VL模型性能的影响。\n\n具体实验结果\n\n*   **性能比较**：\n    *   专有模型在所有模态上普遍优于开源模型。\n    *   GPT-5模型在准确率上表现最佳，达到0.765。\n    *   Qwen2.5-VL-72B-Instruct是表现最佳的开源模型，准确率为0.514。\n*   **模态一致性**：\n    *   所有模型在视觉模态上的性能普遍低于语言模态。\n    *   模态间一致性普遍较低，大多数模型接近随机基线。\n    *   图3展示了不同领域中模型在不同模态下的性能差异。\n*   **错误分析**：\n    *   **文本感知错误**：在化学任务中，SMILES字符串的分词错误导致了错误的化学解释。\n    *   **视觉感知错误**：在图论任务中，视觉输入的幻觉导致了错误的路径计数。\n    *   图6展示了文本和视觉模态处理同一问题时的错误案例。\n*   **鲁棒性检查**：\n    *   视觉变换对模型性能的影响较小，表明模型主要响应于底层语义内容。\n    *   图4展示了视觉变换对Qwen2.5-VL模型性能的影响。\n\n实验结论\n\n*   **模态不平衡**：所有模型在视觉模态上的性能普遍低于语言模态，表明VLMs在不同模态之间的推理能力存在显著差异。\n*   **跨模态一致性低**：模型在不同模态之间的一致性较低，表明模型在不同模态下的处理方式存在显著差异。\n*   **错误模式**：文本感知错误和视觉感知错误是导致跨模态一致性低的主要原因。\n*   **鲁棒性**：模型对视觉变换具有一定的鲁棒性，表明模型主要响应于底层语义内容。\n\n通过这些实验，论文揭示了当前VLMs在跨模态推理方面存在的问题，并为未来的研究提供了行动导向的见解。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 论文提出了一些可以进一步探索的方向，以下是一些具体的研究点：\n\n1\\. **跨模态推理的模型改进**\n\n*   **改进模型架构**：开发新的VLM架构，以更好地整合视觉和语言信息，减少模态间的不平衡。例如，可以探索更高效的视觉编码器或更强大的跨模态融合机制。\n*   **预训练策略**：研究新的预训练策略，以提高模型在不同模态下的推理能力。例如，可以设计专门的跨模态预训练任务，以增强模型的模态无关推理能力。\n*   **模型训练方法**：探索新的训练方法，如多任务学习或元学习，以提高模型在不同模态下的泛化能力。\n\n2\\. **跨模态推理的鲁棒性**\n\n*   **鲁棒性测试**：进一步测试模型在更复杂的视觉变换和噪声条件下的鲁棒性。例如，可以引入更复杂的图像增强技术或对抗性攻击，以评估模型的鲁棒性。\n*   **数据增强**：研究数据增强技术，以提高模型在不同模态下的鲁棒性。例如，可以在训练数据中引入更多的视觉和文本变体，以增强模型的泛化能力。\n\n3\\. **跨模态推理的语义对齐**\n\n*   **语义对齐方法**：开发新的方法来提高模型在不同模态下的语义对齐能力。例如，可以探索新的嵌入方法或对齐算法，以确保模型能够更好地理解和处理语义等价的信息。\n*   **多模态嵌入空间**：研究多模态嵌入空间的结构和特性，以更好地理解模型在不同模态下的推理过程。例如，可以使用t-SNE等可视化技术来分析模型的嵌入空间，以发现潜在的语义对齐问题。\n\n4\\. **跨模态推理的领域适应性**\n\n*   **领域特定的VLMs**：开发针对特定领域的VLMs，以提高模型在特定领域的推理能力。例如，可以开发专门用于化学、音乐或图论的VLMs，以更好地处理这些领域的特定任务。\n*   **领域适应性方法**：研究领域适应性方法，以提高模型在不同领域的推理能力。例如，可以使用迁移学习或领域适应技术，以将模型从一个领域迁移到另一个领域。\n\n5\\. **跨模态推理的评估方法**\n\n*   **新的评估指标**：开发新的评估指标，以更全面地评估模型的跨模态推理能力。例如，可以引入新的指标来评估模型在不同模态下的语义对齐程度或推理一致性。\n*   **多模态基准测试**：开发新的多模态基准测试，以更全面地评估模型的跨模态推理能力。例如，可以引入更多的领域和任务，以评估模型在不同场景下的推理能力。\n\n6\\. **跨模态推理的定性分析**\n\n*   **报告卡方法**：采用报告卡方法（report cards）进行定性分析，以更全面地评估模型的跨模态推理能力。例如，可以使用自然语言总结来评估模型在不同模态下的推理过程和结果。\n*   **案例研究**：进行更多的案例研究，以深入了解模型在不同模态下的推理过程和错误模式。例如，可以分析特定任务或领域的案例，以发现模型的潜在问题和改进方向。\n\n7\\. **跨模态推理的多模态融合**\n\n*   **多模态融合方法**：研究新的多模态融合方法，以提高模型在不同模态下的推理能力。例如，可以探索新的融合算法或架构，以更好地整合视觉和语言信息。\n*   **多模态推理机制**：研究多模态推理机制，以更好地理解模型在不同模态下的推理过程。例如，可以分析模型的注意力机制或推理路径，以发现潜在的改进方向。\n\n8\\. **跨模态推理的多模态数据集**\n\n*   **多模态数据集**：开发新的多模态数据集，以支持更广泛的跨模态推理研究。例如，可以引入更多的领域和任务，以评估模型在不同场景下的推理能力。\n*   **数据集标注**：研究数据集标注方法，以提高数据集的质量和可用性。例如，可以引入更详细的标注信息或更复杂的标注任务，以支持更深入的研究。\n\n这些方向为未来的研究提供了丰富的探索空间，有助于推动VLMs在跨模态推理方面的进一步发展。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: 当然，以下是论文的主要内容总结：\n\n论文标题\n\nSEAM: Semantically Equivalent Across Modalities Benchmark for Vision-Language Models\n\n作者\n\nZhenwei Tang, Difan Jiao, Blair Yang, Ashton Anderson\n\n机构\n\nDepartment of Computer Science, University of Toronto\n\n发表信息\n\nCOLM 2025\n\n摘要\n\n本文介绍了SEAM（Semantically Equivalent Across Modalities），这是一个用于评估视觉-语言模型（VLMs）在不同模态之间推理一致性的基准测试。SEAM通过在四个领域（国际象棋、化学、音乐和图论）中配对语义等价的输入，确保了语义等价性，从而严格评估VLMs的文本符号和视觉空间推理能力。通过对21个当代模型的评估，我们发现系统性的模态不平衡：视觉模态的性能通常落后于语言模态，尽管问题包含语义等价的信息。此外，跨模态一致性相对较低，表明模型在不同模态下的处理方式存在显著差异。我们的错误分析揭示了两个主要的失败模式：文本感知失败（由于分词问题）和视觉感知失败（导致幻觉）。我们还展示了我们的结果对常见视觉变换的鲁棒性。SEAM建立了一个控制良好的语义等价环境，用于测量和改进模态无关的推理能力。我们公开发布了代码、数据集和排行榜，以促进进一步的研究。\n\n1\\. 引言\n\n视觉-语言模型（VLMs）在理解和生成跨越视觉和文本领域的内容方面取得了快速进展。然而，随着这些模型的应用范围不断扩大，评估它们在不同模态之间是否能够一致地进行推理变得越来越重要。本文提出了SEAM基准测试，旨在通过保持语义不变而仅改变表示方式，来严格评估VLMs的模态无关推理能力。SEAM利用了四个具有标准化符号系统的领域（国际象棋、化学、音乐和图论），确保了不同模态之间的语义等价性，并通过多种任务来评估模型在文本符号和视觉空间推理方面的能力。\n\n2\\. 相关工作\n\n*   **视觉-语言模型（VLMs）**：早期的VLMs使用两流架构分别处理视觉和文本输入，随后出现了统一模型和基于预训练模型的桥接方法。近年来，大型语言模型（LLMs）的集成显著提升了VLMs的性能。\n*   **VLM基准测试**：早期的基准测试主要评估基本的视觉理解能力，近期的基准测试扩展了评估范围，涵盖了更复杂的能力。然而，评估VLMs在不同模态之间的推理能力仍然是一个挑战。\n\n3\\. SEAM：跨模态语义等价基准测试\n\n3.1 设计原则\n\n*   **标准化符号系统**：选择具有标准化符号系统的领域，确保不同模态之间的语义等价性。\n*   **工具可用性和现实世界普及性**：选择具有成熟转换工具和广泛现实世界应用的领域。\n*   **单一模态自包含**：每个问题在单一模态内可以完全解决，消除了联合推理的混淆因素。\n\n3.2 基准测试构建\n\n*   **任务定义**：在每个领域中定义了四个任务，每个任务包含200个问题，总计16个任务，3200个问题。这些问题以多项选择题的形式呈现，并且设计了具有挑战性的干扰选项，以校准任务难度。\n*   **文本表示和真实答案**：从现有数据集或使用领域特定工具生成原始文本表示，并确定真实答案。\n*   **校准任务难度**：通过生成具有挑战性的干扰选项来校准任务难度，确保任务既不过于简单也不过于困难。\n*   **文本到图像转换**：使用领域特定工具将文本表示转换为图像表示，确保语义等价性。\n\n4\\. 实验\n\n4.1 实验设置\n\n*   **模型选择**：评估了21个最新的VLMs，包括专有模型和开源模型。\n*   **模态评估**：对每个模型在三种模态下进行评估：语言（L）、视觉（V）和视觉-语言（VL）。\n*   **提示方式**：采用零样本链式思考提示（zero-shot chain-of-thought prompting）。\n*   **硬件配置**：使用vLLM框架在8个A100 GPU上运行推理。\n*   **答案提取**：采用OpenCompass协议从模型输出中提取最终答案，使用Qwen2.5-7B-Instruct模型作为答案提取器。\n\n4.2 实验结果\n\n*   **性能比较**：专有模型在所有模态上普遍优于开源模型。GPT-5模型在准确率上表现最佳，达到0.765。Qwen2.5-VL-72B-Instruct是表现最佳的开源模型，准确率为0.514。\n*   **模态不平衡**：所有模型在视觉模态上的性能普遍低于语言模态，表明VLMs在不同模态之间的推理能力存在显著差异。\n*   **跨模态一致性低**：模型在不同模态之间的一致性较低，大多数模型接近随机基线。\n*   **错误分析**：文本感知错误主要来自于对特定领域文本输入的分词问题，而视觉感知错误则导致了幻觉（hallucinations）。\n*   **鲁棒性检查**：通过应用常见的视觉变换（如分辨率变化、灰度转换和旋转）来验证结果的鲁棒性。结果表明，模型对视觉变换具有一定的鲁棒性，表明模型主要响应于底层语义内容。\n\n5\\. 讨论\n\n*   **语义等价性**：尽管我们通过标准化工具生成和整理SEAM以最大化跨模态语义等价性，但实现完美的等价性仍然是一个理论上的理想。然而，结果表明，模型对视觉变换具有一定的鲁棒性，表明它们主要响应于底层语义内容。\n*   **局限性**：研究仅限于图像，没有评估视频或多图像序列。此外，由于预算和计算限制，评估的模型数量有限。SEAM将公开发布，并附带排行榜，以促进更广泛的研究社区的贡献和对新兴模型的持续评估。\n\n6\\. 结论\n\nSEAM基准测试揭示了VLMs在不同模态之间的推理能力存在显著差异，表明当前的VLMs在处理语义等价的视觉和文本表示时存在根本性的局限性。这一发现突显了当前能力与真正模态无关的AI之间的差距。展望未来，VLM开发的一个关键方面是能够处理与表示格式无关的信息。SEAM为衡量更鲁棒和真正智能的VLMs的进步提供了一个原则性的框架。\n\n附录\n\n*   **附录A**：扩展了相关工作的讨论，特别是关于模态不平衡和领域特定的VLMs。\n*   **附录B**：提供了额外的实验结果，包括视觉变换对模型性能的影响和跨模型一致性分析。\n*   **附录C**：提供了实验的可重复性信息，包括超参数设置、提示模板和任务设计细节。\n*   **附录D**：分析了VLMs在不同输入格式下处理语义等价信息的内部表示对齐情况。\n\n通过这些内容，论文不仅揭示了当前VLMs在跨模态推理方面存在的问题，还为未来的研究提供了一个原则性的框架，以衡量和改进更具鲁棒性和真正智能的VLMs。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是SEAM%3A%20Semantically%20Equivalent%20Across%20Modalities%20Benchmark%20for%20Vision-Language%20Models，链接是  https://arxiv.org/pdf/2508.18179  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.18179  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1756424830930","title":"DeepScholar-Bench: A Live Benchmark and Automated Evaluation for Generative Research Synthesis","description":"The ability to research and synthesize knowledge is central to human expertise and progress. An emerging class of systems promises these exciting capabilities through generative research synthesis, performing retrieval over the live web and synthesizing discovered sources into long-form, cited summaries. However, evaluating such systems remains an open challenge: existing question-answering benchmarks focus on short-form factual responses, while expert-curated datasets risk staleness and data contamination. Both fail to capture the complexity and evolving nature of real research synthesis tasks. In this work, we introduce DeepScholar-bench, a live benchmark and holistic, automated evaluation framework designed to evaluate generative research synthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv papers and focuses on a real research synthesis task: generating the related work sections of a paper by retrieving, synthesizing, and citing prior research. Our evaluation framework holistically assesses performance across three key dimensions, knowledge synthesis, retrieval quality, and verifiability. We also develop DeepScholar-base, a reference pipeline implemented efficiently using the LOTUS API. Using the DeepScholar-bench framework, we perform a systematic evaluation of prior open-source systems, search AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that DeepScholar-base establishes a strong baseline, attaining competitive or higher performance than each other method. We also find that DeepScholar-bench remains far from saturated, with no system exceeding a score of $19\\%$ across all metrics. These results underscore the difficulty of DeepScholar-bench, as well as its importance for progress towards AI systems capable of generative research synthesis. We make our code available at https://github.com/guestrin-lab/deepscholar-bench.","url":"https://huggingface.co/papers/2508.20033","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.930Z","metadata":{"authors":["Liana Patel","Negar Arabzadeh","Harshit Gupta","Ankita Sundar","Ion Stoica","Matei Zaharia","Carlos Guestrin"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2508.20033.pdf","arxivUrl":"https://arxiv.org/abs/2508.20033","arxivId":"2508.20033","coolPaperUrl":"https://papers.cool/arxiv/2508.20033","published":"2025-08-27T16:36:34Z","updated":"2025-08-27T16:36:34.000Z","zh_summary":"研究和综合知识的能力是人类专业知识和进步的核心。一类新兴系统通过生成研究综合、通过实时网络执行检索以及将发现的来源合成为长篇引用摘要来保证这些令人兴奋的功能。然而，评估此类系统仍然是一个悬而未决的挑战：现有的问答基准侧重于简短的事实响应，而专家策划的数据集则存在陈旧和数据污染的风险。两者都未能捕捉到实际研究综合任务的复杂性和不断发展的本质。在这项工作中，我们介绍了 DeepScholar-bench，这是一个实时基准测试和整体自动化评估框架，旨在评估生成研究综合。DeepScholar-bench 从最近的高质量 ArXiv 论文中提取查询，并专注于真正的研究综合任务：通过检索、综合和引用先前的研究来生成论文的相关工作部分。我们的评估框架从知识综合、检索质量和可验证性三个关键维度全面评估绩效。我们还开发了 DeepScholar-base，这是一个使用 LOTUS API 高效实现的参考管道。使用 DeepScholar-bench 框架，我们对以前的开源系统、搜索 AI、OpenAI 的 DeepResearch 和 DeepScholar-base 进行了系统评估。我们发现 DeepScholar 基础建立了强大的基线，获得了比其他方法更具竞争力或更高的性能。我们还发现，DeepScholar-bench 远未饱和，没有一个系统在所有指标上的得分超过 19\\%$。这些结果强调了 DeepScholar-bench 的难度，以及它对于能够进行生成研究综合的人工智能系统取得进展的重要性。我们在 https://github.com/guestrin-lab/deepscholar-bench 提供我们的代码。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 该论文旨在解决**生成式研究综述系统（generative research synthesis systems）的评估难题**。具体而言，现有评估方法存在以下关键缺陷：\n\n1.  **任务不真实**：传统问答基准（如SimpleQA、HotpotQA）聚焦短答案、可验证的事实，无法反映真实研究综述所需的**长文本、多源整合与复杂推理**。\n2.  **数据陈旧与污染**：专家手工构建的数据集（如ScholarQABench）更新成本高，易被训练数据污染，且难以覆盖最新研究。\n3.  **评估维度单一**：现有基准忽略研究综述的三大核心能力——**检索质量（retrieval quality）**、\\*\\*知识综合（knowledge synthesis）**与**可验证性（verifiability）\\*\\*的协同评估。\n\n为此，论文提出**DeepScholar-Bench**，一个**实时更新的自动化基准**，通过以下设计解决上述问题：\n\n*   **真实任务**：以“为学术论文生成相关研究综述章节”为核心任务，直接映射研究者的实际工作流。\n*   **动态数据**：从最新ArXiv论文自动提取查询与专家撰写的综述作为参考答案，避免数据过时。\n*   **三维评估框架**：\n    *   **知识综合**：通过“信息块覆盖率”（Nugget Coverage）与“结构组织”（Organization）衡量内容质量。\n    *   **检索质量**：评估检索结果的相关性（Relevance Rate）、重要文献覆盖率（Reference Coverage）及文献影响力（Document Importance）。\n    *   **可验证性**：量化引文对声明的支持度（Citation Precision）与声明的完整支持率（Claim Coverage）。\n\n最终，论文通过系统评估现有系统（包括开源方法、Search AI及OpenAI DeepResearch），揭示所有方法的显著不足（无任何系统总分超19%），并发布**DeepScholar-Base**作为强基线，推动未来研究。\n\n**Q**: 有哪些相关研究？\n\n**A**: 与 DeepScholar-Bench 直接相关的研究可分为以下四类，并附关键对比点：\n\n* * *\n\n1\\. **长文本研究综述基准**\n\n工作\n\n核心任务\n\n数据构建方式\n\n评估维度\n\n与 DeepScholar-Bench 的差异\n\n**ScholarQABench** \\[19\\]\n\n文献综述问答\n\n专家手工撰写问题与答案（CS、生物医学等）\n\n答案完整性、准确性\n\n人工成本高、更新慢，未覆盖检索与可验证性\n\n**OpenResearcher** \\[55\\]\n\n科学文本摘要\n\n专家设计 38 个开放性问题\n\n摘要质量、覆盖度\n\n静态数据集，无实时更新机制\n\n**DeepConsult** \\[11\\]\n\n商业咨询报告生成\n\n专家策划商业领域研究问题\n\n报告深度、实用性\n\n领域局限，未评估检索与引文验证\n\n**FreshWiki** \\[39\\]\n\n维基百科长文生成\n\n自动化爬取维基页面\n\n文章结构、事实准确性\n\n任务为百科写作，非学术综述；无引文验证\n\n* * *\n\n2\\. **检索增强生成（RAG）与问答基准**\n\n工作\n\n任务类型\n\n数据特点\n\n评估重点\n\n局限性\n\n**HotpotQA** \\[52\\]\n\n多跳问答\n\n维基百科段落\n\n推理链正确性\n\n短答案，无长文本综合\n\n**FRAMES** \\[25\\]\n\n事实检索与推理\n\n需跨文档整合信息\n\n事实一致性\n\n无长文本生成评估\n\n**GAIA** \\[32\\]\n\n通用 AI 助手任务\n\n需浏览网页解决复杂问题\n\n任务完成率\n\n侧重工具使用，非学术综述\n\n**BrowseComp** \\[49\\]\n\n浏览器任务\n\n需实时网页交互\n\n步骤正确性\n\n短答案为主，无引文要求\n\n* * *\n\n3\\. **可验证性评估方法**\n\n工作\n\n可验证性定义\n\n评估粒度\n\n适用场景\n\n**ALCE** \\[21\\]\n\n引文支持声明\n\n句子级精确率/召回率\n\n短文本生成\n\n**WebGLM** \\[50\\]\n\n引文与声明对齐\n\n摘要-引文匹配\n\n问答系统\n\n**DeepScholar-Bench**\n\n引文精确度 + 声明覆盖率\n\n句子级滑动窗口（w=1~5）\n\n长文本学术综述\n\n* * *\n\n4\\. **自动化评估与 LLM-as-a-Judge**\n\n方法\n\n评估内容\n\n技术\n\n与 DeepScholar-Bench 的关联\n\n**LLMJudge** \\[38\\]\n\n文档相关性\n\nLLM 打分（0/1/2）\n\n用于 Relevance Rate 指标\n\n**UMBRELA** \\[45\\]\n\n相关性评估\n\n复现 Bing 的 LLM 评估器\n\n验证 LLM 评估与人类一致性\n\n**PRD** \\[30\\]\n\n生成质量\n\n多 LLM 讨论评分\n\n启发 DeepScholar-Bench 的 pairwise 组织评估\n\n* * *\n\n总结\n\nDeepScholar-Bench 的独特性在于：\n\n*   **任务层面**：首个聚焦“实时学术综述生成”的自动化基准。\n*   **数据层面**：动态更新的 ArXiv 论文流，避免污染与过时。\n*   **评估层面**：首次联合评估检索、综合、可验证性三维能力，并通过人类验证 LLM 评估的可靠性（>70% 一致性）。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 论文通过“一个实时基准 + 一套自动化评估框架 + 一条参考流水线”的三件套方案，系统性地解决了“如何可靠、可扩展地评估生成式研究综述系统”这一难题。具体实施路径如下：\n\n* * *\n\n1\\. 构造实时、可扩展的 **DeepScholar-Bench 数据集**\n\n*   **任务定义**  \n    给定一篇新论文的标题与摘要，系统需从开放网络（ArXiv API）检索相关文献，并生成该论文的“相关工作”章节。\n*   **自动数据管道**\n    1.  每周从 18 个 ArXiv 领域抓取最新 v1 论文（4–6 月内）。\n    2.  过滤：保留已会议录用、含明确“Related Work”与 .bib 文件的论文。\n    3.  提取：解析 LaTeX/PDF 获取“Related Work”原文及全部引用，并通过 ArXiv + OpenAlex 补全元数据。\n    4.  持续更新：脚本公开，社区可重跑以生成未来版本，避免数据陈旧与污染。\n\n* * *\n\n2\\. 设计 **三维七指标自动化评估框架**\n\n维度\n\n指标\n\n自动化实现\n\n与人类一致性\n\n**知识综合**\n\nOrganization\n\nGPT-4o pairwise vs 人类范例\n\n78%\n\nNugget Coverage\n\nGPT-4.1 提取“信息块”并计算覆盖率\n\n72%\n\n**检索质量**\n\nRelevance Rate\n\nGPT-4o 0/1/2 分级打分取平均\n\n70%\n\nReference Coverage\n\nLLM 标注人类范例“重要引用”，再算系统覆盖率\n\n82%\n\nDocument Importance\n\n引用量中位数相对人类范例的比率（OpenAlex）\n\n—\n\n**可验证性**\n\nCitation Precision\n\n句子级：引用是否支持该句任一声明\n\n—\n\nClaim Coverage\n\n句子级：声明是否被引用完全支持（滑动窗口 w=1）\n\n—\n\n*   **LLM-as-a-Judge 验证**：200+ 人工标注显示所有指标 ≥70% 一致性，确保无需昂贵人工即可重复评估。\n\n* * *\n\n3\\. 提供 **DeepScholar-Base 参考流水线**\n\n*   **三阶段架构**（基于 LOTUS 语义算子）\n    1.  **检索**：LLM 依据摘要迭代生成 2 轮、每轮 2 个查询 → ArXiv API 取 top-50。\n    2.  **过滤与重排**：\n        *   Sem-Filter：LLM 判断相关性剔除噪声。\n        *   Sem-TopK：LLM 重排取 top-30。\n    3.  **生成**：Sem-Agg 汇总 top-30 文献，生成 1500–2000 字带引文综述。\n*   **可控参数**：Q=2, search\\_K=50, N=2, K=30，可在 1 张 A100 上 1–2 分钟完成一例。\n\n* * *\n\n4\\. 系统实验验证方案有效性\n\n*   **覆盖范围**：评估 3 类 10 个系统\n    *   开源：DeepResearcher、STORM、OpenScholar\n    *   Search-AI：Llama-4-Scout、GPT-4.1、o3、Claude-4、Gemini-2.5\n    *   商业：OpenAI DeepResearch\n*   **核心发现**\n    *   所有系统综合得分 ≤19%，显示任务极具挑战性。\n    *   DeepScholar-Base 在 6 项指标上优于或持平最佳对比系统，且可验证性比 DeepResearch 高 4.4–6.3×。\n    *   消融实验：若给予“人类重要文献”作为检索结果（Oracle Retrieval），性能显著跃升，但仍未饱和，说明检索与综合均有巨大提升空间。\n\n* * *\n\n总结\n\n论文通过 **实时数据 + 自动化三维评估 + 可复现基线** 的组合拳，首次为生成式研究综述系统提供了可持续、可扩展、与人类高度一致的基准方案，并明确指出了未来改进的两大方向：**更精准的检索策略** 与 **更深层的信息综合能力**。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文围绕 DeepScholar-Bench 共设计了四类实验，覆盖 **系统对比、消融分析、统计洞察、人工一致性验证** 四个层面，具体实验与结果如下：\n\n* * *\n\n1\\. 主实验：全系统端到端对比\n\n**目的** 在统一检索源（ArXiv API）与统一 LLM 裁判（GPT-4.1 / GPT-4o）下，横向评估 10 个代表性系统在 63 条实时查询上的三维七指标表现。\n\n系统类别\n\n被测系统\n\n关键发现（节选）\n\n开源综述系统\n\nDeepResearcher、STORM、OpenScholar\n\n无系统综合得分 >19%；Nugget Coverage、Reference Coverage、Document Importance 均 <0.45\n\nSearch-AI\n\nLlama-4-Scout、GPT-4.1、o3、Claude-4、Gemini-2.5\n\nClaude Search-AI 在 Citation Precision（0.701）与 Claim Coverage（0.760）上领先\n\n商业系统\n\nOpenAI DeepResearch\n\n综合得分最高，但 Citation Precision 仅 0.399，Claim Coverage 仅 0.138\n\n基线\n\nDeepScholar-Base（5 种 LLM 组合）\n\n相比同类平均提升 1.3× Organization、2× Reference Coverage、1.6× Verifiability；在 GPT-4.1+Claude 组合下 Citation Precision 达 0.936\n\n* * *\n\n2\\. 消融实验：检索源与检索质量\n\n**目的** 验证“检索瓶颈”与“综合瓶颈”各自对最终性能的影响。\n\n检索条件\n\nReference Coverage\n\nDocument Importance\n\nNugget Coverage\n\n结论\n\narxiv.org（默认）\n\n0.167\n\n0.007\n\n0.370\n\n真实检索器表现有限\n\nparallel.ai\n\n0.160\n\n0.017\n\n0.444\n\n不同 API 差异显著\n\ntavily.com\n\n0.070\n\n0.015\n\n0.327\n\n覆盖率更低\n\n**Oracle (ArXiv 重要文献)**\n\n**1.000**\n\n**1.000**\n\n**0.487**\n\n检索完美时，综合仍不足\n\n**Oracle (全部重要文献)**\n\n**1.000**\n\n**0.822**\n\n**0.528**\n\n综合瓶颈依然存在\n\n* * *\n\n3\\. 统计与分布分析\n\n*   **人类范例统计**（表 5、图 8、图 11）\n    \n    *   平均 23 篇独特引用，63 % 来自 ArXiv；引用量呈长尾分布（均值 478，中位数 31）。\n    *   重要引用中 46 % 可在 ArXiv 直接检索，54 % 需外部源，说明单一 ArXiv 检索存在固有缺口。\n*   **生成报告统计**（表 5）\n    \n    *   人类范例：≈500 词、28 句、27 条引用。\n    *   系统差异：DeepResearch 最长（864 词），Search-AI 最短（258–499 词），DeepScholar-Base 中等（402–1492 词）。\n\n* * *\n\n4\\. 人工一致性验证\n\n**目的** 确认 LLM-as-a-Judge 的可靠性。\n\n评估维度\n\n人工-LLM 一致性\n\nOrganization（pairwise 胜/平/负）\n\n78 %\n\nNugget Coverage（信息块重要性）\n\n72 %\n\nRelevance Rate（0/1/2 分级）\n\n70 %\n\nReference Coverage（重要/非重要）\n\n82 %\n\n* * *\n\n实验结论速览\n\n1.  **任务极难**：所有现有系统在 DeepScholar-Bench 上综合得分 ≤19%，存在巨大提升空间。\n2.  **检索是首要瓶颈**：Oracle 检索可将 Reference Coverage 与 Document Importance 拉满，但 Nugget Coverage 仍仅 ~0.5，说明综合环节亦待突破。\n3.  **DeepScholar-Base 已是最强公开基线**：在 6 项关键指标上优于或持平现有最佳系统，且可验证性显著领先。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 以下方向可直接基于 DeepScholar-Bench 的框架、数据与发现继续深入，分为 **检索、综合、评估、系统、扩展任务** 五大类，并给出可落地的切入点。\n\n* * *\n\n1\\. 检索策略：从“找到”到“找全且找好”\n\n*   **混合检索器**  \n    将 ArXiv API 与 **Semantic Scholar、Google Scholar、OpenAlex、出版商 API** 联合检索，提升非 ArXiv 重要文献的召回（当前 Oracle 实验显示 54 % 关键文献在 ArXiv 外）。\n*   **检索-综合协同优化**  \n    用强化学习或在线学习动态调整检索查询：根据已生成段落的缺失信息实时生成下一轮查询（类似 STORM 的“对话式检索”但面向综述）。\n*   **重要性预测模型**  \n    训练轻量级引文影响力预测器，在检索阶段即过滤低影响力论文，提升 Document Importance 指标。\n\n* * *\n\n2\\. 知识综合：从“堆砌”到“深度整合”\n\n*   **层次化大纲生成**  \n    先让 LLM 生成三级大纲（主题-子主题-论点），再针对每个子主题独立检索与生成，最后拼接；可缓解长文本上下文窗口限制，提高 Organization 与 Nugget Coverage。\n*   **多视角合成**  \n    引入“对立观点”或“方法对比”模块：对同一子主题检索支持/反对/改进三类文献，自动生成对照表，增强综述的批判性。\n*   **知识图谱增强**  \n    用开放信息抽取（OpenIE）将检索结果转为实体-关系图，再基于图谱生成段落，减少幻觉并提升事实密度。\n\n* * *\n\n3\\. 可验证性：从“有引用”到“精确引用”\n\n*   **细粒度对齐**  \n    训练一个“声明-片段匹配”小模型，将综述中的每个声明与对应文献中的最小支持片段（句子级或子句级）对齐，取代当前 LLM-as-a-Judge 的粗粒度验证。\n*   **动态滑动窗口**  \n    实验显示 w=1 已显著提高 Claim Coverage；可探索 **自适应窗口**：根据段落密度与引用密度动态调整 w，平衡可读性与覆盖率。\n*   **反事实验证**  \n    自动生成“虚假声明”并检查系统是否仍给出高置信度引用，量化系统的鲁棒性。\n\n* * *\n\n4\\. 系统与效率：从“原型”到“生产级”\n\n*   **检索-生成并行化**  \n    当前 DeepScholar-Base 两轮串行检索；可改为 **并行检索**（多查询同时发出）+ **增量式生成**（边检索边写），将延迟从分钟级降到秒级。\n*   **小模型蒸馏**  \n    用 GPT-4.1 生成的数据蒸馏 7B–13B 小模型，专用于“Sem-Filter/Sem-TopK/Sem-Agg”三算子，降低 GPU 成本。\n*   **缓存与增量更新**  \n    对高频子领域（如 LLM、Diffusion Model）建立预检索缓存，每周增量更新，避免重复检索。\n\n* * *\n\n5\\. 任务与领域扩展：从“AI/CS”到“全科学”\n\n*   **跨语言综述**  \n    将查询与检索源扩展到非英语论文（如中文 CNKI、日文 J-STAGE），测试系统在多语言环境下的综合与对齐能力。\n*   **纵向时间线综述**  \n    要求系统按时间顺序梳理某技术演进（如“Transformer 架构 2017-2025”），评估其历史脉络把握能力。\n*   **政策/医学综述**  \n    用同一框架抓取 WHO、FDA、arXiv q-bio 等来源，验证在强监管领域的可靠性（需更严格的可验证性指标）。\n\n* * *\n\n6\\. 评估指标再设计\n\n*   **读者模拟评测**  \n    引入“读者困惑度”指标：让 LLM 扮演不同背景读者（本科生/博士/跨学科研究员），对综述提出后续问题，测试信息缺口。\n*   **引用多样性**  \n    新增 “Citation Diversity” 指标：统计引用文献的期刊/会议/国家分布，防止系统仅引用单一来源。\n*   **长文本压缩率**  \n    计算系统综述与人类范例的 **信息密度比**（nuggets/words），鼓励更精炼的生成。\n\n* * *\n\n可立即启动的实验清单\n\n1.  **两周冲刺**：在 DeepScholar-Base 上替换检索 API 为 `Semantic Scholar + CrossRef`，复现 Oracle 实验，观察 Reference Coverage 提升幅度。\n2.  **一个月项目**：用 LoRA 微调 Llama-3-8B 作为“Sem-Filter”专用小模型，对比 GPT-4.1 的过滤准确率与成本。\n3.  **三个月研究**：构建“时间线综述”子任务数据集（抓取 2017-2025 Transformer 论文），新增“演进准确率”指标，评估系统对技术脉络的还原能力。\n\n这些方向均可在 DeepScholar-Bench 的开源代码与数据基础上快速迭代，持续刷新排行榜并推动领域边界。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### DeepScholar-Bench 论文一句话总结  \n提出首个 **实时、可扩展、三维评估** 的生成式研究综述基准，并配套开源基线，系统揭示现有系统全面不足，为下一代 AI 研究助手奠定评估与改进基础。\n\n* * *\n\n核心内容速览\n\n模块\n\n关键信息\n\n**问题**\n\n现有 QA 或专家数据集无法真实、持续地评估“检索→综合→可验证”的长文本研究综述能力。\n\n**任务**\n\n给定一篇新论文的标题+摘要，系统需实时检索文献并生成带引用的“相关工作”章节。\n\n**数据**\n\n自动抓取 63 篇 2025-04 后 ArXiv 论文（18 领域），每周可更新；含人工撰写的相关章节作参考答案。\n\n**评估**\n\n三维七指标：<br>• 知识综合：Organization、Nugget Coverage<br>• 检索质量：Relevance Rate、Reference Coverage、Document Importance<br>• 可验证性：Citation Precision、Claim Coverage<br>LLM-as-a-Judge 与人类一致性 ≥70%。\n\n**基线**\n\nDeepScholar-Base：两轮检索+LOTUS 语义过滤/排序/聚合，支持多种 LLM；开源、分钟级完成。\n\n**实验**\n\n10 个系统（开源/Search-AI/DeepResearch）全面评测：<br>• 无系统综合得分 >19%<br>• DeepScholar-Base 在 6 项指标领先，可验证性比 DeepResearch 高 6.3×<br>• Oracle 检索实验显示检索与综合均有巨大提升空间。\n\n**贡献**\n\n① 实时基准与自动化评估框架；② 强基线系统；③ 系统实验揭示显著改进机会。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是DeepScholar-Bench%3A%20A%20Live%20Benchmark%20and%20Automated%20Evaluation%20for%20Generative%20Research%20Synthesis，链接是  https://arxiv.org/pdf/2508.20033  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.20033  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1756424830931","title":"Taming the Chaos: Coordinated Autoscaling for Heterogeneous and Disaggregated LLM Inference","description":"Serving Large Language Models (LLMs) is a GPU-intensive task where traditional autoscalers fall short, particularly for modern Prefill-Decode (P/D) disaggregated architectures. This architectural shift, while powerful, introduces significant operational challenges, including inefficient use of heterogeneous hardware, network bottlenecks, and critical imbalances between prefill and decode stages. We introduce HeteroScale, a coordinated autoscaling framework that addresses the core challenges of P/D disaggregated serving. HeteroScale combines a topology-aware scheduler that adapts to heterogeneous hardware and network constraints with a novel metric-driven policy derived from the first large-scale empirical study of autoscaling signals in production. By leveraging a single, robust metric to jointly scale prefill and decode pools, HeteroScale maintains architectural balance while ensuring efficient, adaptive resource management. Deployed in a massive production environment on tens of thousands of GPUs, HeteroScale has proven its effectiveness, increasing average GPU utilization by a significant 26.6 percentage points and saving hundreds of thousands of GPU-hours daily, all while upholding stringent service level objectives.","url":"https://huggingface.co/papers/2508.19559","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.931Z","metadata":{"authors":["Rongzhi Li","Ruogu Du","Zefang Chu","Sida Zhao","Chunlei Han","Zuocheng Shi","Yiwen Shao","Huanle Han","Long Huang","Zherui Liu","Shufan Liu"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.DC","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2508.19559.pdf","arxivUrl":"https://arxiv.org/abs/2508.19559","arxivId":"2508.19559","coolPaperUrl":"https://papers.cool/arxiv/2508.19559","published":"2025-08-27T04:22:02Z","updated":"2025-08-27T04:22:02.000Z","zh_summary":"翻译失败: Unexpected end of JSON input","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 论文旨在解决现代 Prefill-Decode（P/D）分离式大语言模型（LLM）推理服务在超大规模 GPU 集群中面临的三大核心难题：\n\n1.  **异构硬件利用率低**  \n    Prefill 阶段计算密集、Decode 阶段内存带宽密集，传统“一刀切”地使用同构 GPU 会导致 41% 的额外成本，并加剧集群碎片。\n    \n2.  **网络瓶颈**  \n    KV-Cache 跨节点传输需要高带宽 RDMA；若调度器忽视网络拓扑，跨交换机传输带宽可下降约 20%，显著增加延迟。\n    \n3.  **架构失衡**  \n    Prefill 与 Decode 实例必须保持动态平衡。独立按 GPU 利用率伸缩会误导 Decode 实例数（高利用率可能仅由 KV-Cache 压力造成），导致一阶段饥饿、另一阶段空闲，整体吞吐崩溃。\n    \n\n为此，论文提出 **HeteroScale**：一个面向 P/D 分离架构的协同自动扩缩框架，通过\n\n*   拓扑感知的异构调度\n*   单一鲁棒指标（Decode Tokens-Per-Second）驱动的联合伸缩\n*   网络亲和抽象（Deployment Group / RDMA Subgroup）  \n    在字节跳动数万个 GPU 的生产环境中，将平均 GPU 利用率提升 26.6 个百分点，每天节省数十万 GPU 小时，同时严守 SLO。\n\n**Q**: 有哪些相关研究？\n\n**A**: 与 HeteroScale 相关的研究可分为四大类，分别对应 P/D 分离式 LLM 推理、自动扩缩、异构资源管理与网络感知调度。以下列出最具代表性的工作，并指出其与本文的差异或互补点。\n\n* * *\n\n1\\. P/D 分离式 LLM 推理系统\n\n系统\n\n核心贡献\n\n与 HeteroScale 的关系\n\n**DistServe** (OSDI’24)\n\n首次系统论证 Prefill/Decode 分离在吞吐-延迟权衡上的优势\n\n仅聚焦静态部署，未解决异构硬件与自动扩缩\n\n**SplitWise** (ISCA’24)\n\n通过阶段拆分降低尾部延迟\n\n未考虑网络拓扑与动态负载变化\n\n**Mooncake**\n\nKV-Cache 为中心的分离架构，强调缓存效率\n\n未提供跨集群的协同扩缩机制\n\n**TetriInfer / MemServe / P/D-Serve**\n\n不同侧重：内存池化、混合负载、规模部署\n\n均缺乏对异构 GPU 与网络亲和的统一调度\n\n* * *\n\n2\\. 自动扩缩技术\n\n系统\n\n核心贡献\n\n与 HeteroScale 的关系\n\n**Kubernetes HPA / VPA / KEDA**\n\n基于 CPU/GPU 利用率或事件驱动伸缩\n\n指标粒度粗，无法识别 P/D 耦合与 KV-Cache 压力\n\n**AutoScale (TOCS’12)**\n\n多层级负反馈控制器\n\n面向传统 Web 服务，未适配 LLM 推理特性\n\n**DeepScaling (SoCC’22)**\n\n基于学习的微服务扩缩\n\n未处理 P/D 阶段间的比例约束\n\n**Resource Central (SOSP’17)**\n\n利用机器学习预测工作负载\n\n同样未考虑分离式 LLM 的特殊瓶颈\n\n* * *\n\n3\\. 异构资源管理\n\n系统\n\n核心贡献\n\n与 HeteroScale 的关系\n\n**Heterogeneity-Aware Scheduler (OSDI’20)**\n\n针对深度学习训练任务的异构 GPU 调度\n\n训练阶段无 KV-Cache 传输，网络约束不同\n\n**Tiresias (NSDI’19)**\n\nGPU 集群管理，支持抢占与排队\n\n未解决推理阶段的阶段耦合问题\n\n**Gandiva (OSDI’18)**\n\n时间片式 GPU 共享\n\n面向训练任务，未考虑推理延迟 SLO\n\n**Mélange (arXiv’24)**\n\n利用 GPU 代际差异降低成本\n\n静态配置，缺乏实时扩缩与网络亲和\n\n* * *\n\n4\\. 网络/拓扑感知调度\n\n系统\n\n核心贡献\n\n与 HeteroScale 的关系\n\n**Firmament (OSDI’16)**\n\n全局最小代价流调度，支持拓扑约束\n\n通用批处理任务，未针对 KV-Cache 流量优化\n\n**Paragon (ASPLOS’13)**\n\n异构数据中心 QoS 感知调度\n\n未涉及 P/D 阶段间的高带宽耦合\n\n**Sinbad / Varys (SIGCOMM’13-14)**\n\nCoflow 调度降低网络拥塞\n\n面向 MapReduce/Spark 流量，与 LLM KV-Cache 特征不同\n\n**NetCache (SOSP’17)**\n\n利用可编程交换机缓存 KV 存储\n\n聚焦键值存储，未解决 GPU 推理扩缩问题\n\n* * *\n\n小结\n\n*   **P/D 分离系统** 提供了阶段拆分的基础，但均未解决异构、网络与协同扩缩三大挑战。\n*   **传统自动扩缩** 缺乏对 LLM 推理特有指标（Decode TPS、KV-Cache 压力）的洞察。\n*   **异构资源管理** 与 **网络感知调度** 的研究为 HeteroScale 提供了算法与抽象层面的启发，但需针对 LLM 推理场景重新设计 Deployment Group、RDMA Subgroup 与协同策略。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: HeteroScale 通过“三层架构 + 三项关键创新”系统性地解决了 P/D 分离式 LLM 推理在超大规模 GPU 集群中的三大痛点。具体做法如下。\n\n* * *\n\n1\\. 三层系统架构（解耦关注点）\n\n层级\n\n职责\n\n关键技术\n\n**Autoscaling Layer**\n\n产生“何时、按何比例”伸缩决策\n\n• 统一信号：Decode TPS<br>• 协同算法：比例控制 + 负反馈<br>• 抗抖动：冷却期、滞后阈值、衰减因子\n\n**Federated Pre-Scheduling Layer**\n\n把决策映射到“在哪一组硬件上”\n\n• 拓扑实时视图（S0/S1/S2 树）<br>• 异构资源打分与排序<br>• Deployment Group / RDMA Subgroup 抽象\n\n**Sub-cluster Scheduling Layer**\n\n与 Kubernetes CRD 交互，真正创建/删除 Pod\n\n• 原子虚拟预留<br>• 软缩入（soft scale-in）灾难恢复\n\n* * *\n\n2\\. 三项关键创新（对应三大痛点）\n\n2.1 异构硬件利用率低 → **异构资源管理框架**\n\n*   **Deployment Group** 把 Prefill 与 Decode 实例绑定在同一网络域（S2/S1 交换机），避免跨交换机 KV-Cache 传输。\n*   **RDMA Subgroup** 将 GPU 池按网络拓扑与硬件类型划分为 **低-中-高** 三档优先级，调度器优先把低亲和度服务放到低档池，保留高档池给严格异构需求。\n*   **Algorithm 4**（附录）给出带优先级的装箱算法：先满足亲和约束，再最大化全局 GPU 利用率。\n\n2.2 网络瓶颈 → **网络亲和调度算法**\n\n*   每次调度循环：\n    1.  **Topology Discovery**：重建实时拓扑树（节点 → S1 → S2）。\n    2.  **Request Sorting**：按服务优先级排序。\n    3.  **Candidate Evaluation**：对扩容请求枚举“扩展现有组”或“新建组”两种候选。\n    4.  **Priority-Based Selection**：选分最高且占用高优先级资源最少的候选。\n*   缩容时优先释放高优先级资源，保证稀缺异构池快速回收。\n\n2.3 架构失衡 → **单一鲁棒指标 + 协同伸缩策略**\n\n*   **指标选择**：基于大规模生产数据实证，Decode TPS 信噪比最高，GPU 利用率在 Decode 阶段因 KV-Cache 常驻而失真。\n*   **协同算法**（Algorithm 2 & 3）：\n    *   **比例控制**：用 Decode TPS 计算总需求，再按预设 P/D 比例一次性同时调整 Prefill/Decode 实例数，避免单侧瓶颈。\n    *   **负反馈**：当 TTFT/TBT 逼近 SLO 阈值时触发固定步长（10 % 或 20 %）的紧急扩容，作为安全网。\n*   **P/D 比例维护**：在线使用固定比例（压力测试得出），若实例就绪顺序导致比例偏离，则在服务发现层暂停多的一侧注册，直到恢复容忍区间。\n\n* * *\n\n3\\. 生产级稳定性机制\n\n*   **抗抖动**：冷却期 + 滞后阈值 + 衰减因子，防止频繁扩缩。\n*   **软缩入**：实例先摘流、后观察，性能无衰减才真正下线，故障秒级回滚。\n*   **灾难恢复**：关键状态持久化，异常后快速重建调度视图。\n\n* * *\n\n4\\. 效果验证\n\n*   **资源效率**：GPU 利用率 +26.6 pp，SM 活动 +9.2 pp，每天节省数十万 GPU 小时。\n*   **架构平衡**：在线服务 P/D 比例波动 < 5 %，TTFT/TBT 稳定在安全区间。\n*   **通用性**：已在对话、视觉-语言搜索等多模态业务上线，均获得一致收益。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文通过“受控实验 + 生产回放 + 全量上线”三级验证体系，系统评估了 HeteroScale 在 P/D 比例、指标有效性、端到端性能三个维度的表现。具体实验如下：\n\n* * *\n\n1\\. P/D 比例实验（微基准）\n\n*   **目的**：证明不同输入-输出分布下存在唯一最优 P/D 比，且该比值随业务特征变化。\n*   **设置**\n    *   硬件：16 节点 × 8×H20 GPU\n    *   业务 A：平均输入 3 k tokens，输出 350 tokens（I/O=8.5），SLO：TTFT≤1 s，TBT≤40 ms\n    *   业务 B：平均输入 7.8 k tokens，输出 700 tokens（I/O=11），SLO：TTFT≤1 s，TBT≤20 ms\n*   **结果**（图 4）\n    *   两条“倒 U”曲线：过低 P/D → TTFT 超限；过高 P/D → TBT 超限。\n    *   最优比值区间：1P/5D ~ 9P/1D，验证必须按业务特征配置，不能固定。\n\n* * *\n\n2\\. 指标有效性实验（回放式仿真）\n\n*   **目的**：在 8 小时真实对话流量回放中，横向比较 8 种候选指标对扩缩行为的驱动效果。\n*   **候选指标**\n    *   Throughput：Prefill TPS、Decode TPS\n    *   Hardware：Prefill/Decode GPU Util、SM Activity\n    *   Latency：TTFT、TBT\n*   **实验设置**\n    *   同一流量 trace（图 5，含早高峰与午高峰）\n    *   统一初始实例数、资源配额、阈值，记录扩缩时机与幅度（图 6）\n*   **关键结论**\n    *   **Decode TPS**（图 6a-b）：响应快、无过冲、与负载同步。\n    *   **GPU Util / SM Activity**：Prefill 阶段可用，Decode 阶段因 KV-Cache 常驻而失效（图 6c-f）。\n    *   **Latency**：TTFT 过冲严重，TBT 稍好但需大量超参数（图 6g-h）。\n    *   最终生产选型：**Decode TPS** 作为单一主信号。\n\n* * *\n\n3\\. 生产环境全量上线评估\n\n*   **规模**：数万个 GPU，对话、视觉-语言搜索等多业务\n*   **对比基线**：无自动扩缩的静态部署\n*   **核心结果**\n    *   **资源效率**\n        *   GPU 利用率 +26.6 pp，SM 活动 +9.2 pp\n        *   每日节省数十万 GPU 小时\n    *   **架构平衡**\n        *   实例数随 Decode TPS 实时变化，P/D 比例保持稳态（图 7a）\n        *   Prefill GPU 利用率 46.8 %→76.2 %，Decode 保持 82 % 以上\n    *   **延迟稳定性**\n        *   TTFT/TBT 波动区间显著收窄（图 7d）\n        *   偶发 TTFT 尖峰通过软 P/D 比例机制在秒级恢复\n    *   **策略对比**\n        *   TPS-based 动态策略比 Periodic 静态策略再提升 GPU 利用率 10 pp、SM 活动 11.1 pp\n        *   TPS-based 策略已覆盖 64 % GPU 规模\n\n* * *\n\n4\\. 跨模态验证（附录）\n\n*   **视觉-语言搜索服务**\n    *   相同 Doubao-Seed-1.6-thinking 模型\n    *   结果（图 9）：与对话业务一致，证明方法对模态无依赖。\n\n* * *\n\n实验总结\n\n维度\n\n实验类型\n\n关键发现\n\nP/D 比例\n\n微基准\n\n存在业务相关最优比，需灵活配置\n\n指标有效性\n\n回放仿真\n\nDecode TPS 最鲁棒，GPU Util 在 Decode 阶段失效\n\n端到端性能\n\n生产全量\n\nGPU 利用率 +26.6 pp，延迟稳定，跨模态通用\n\n策略对比\n\nA/B\n\n动态 TPS 策略显著优于静态周期策略\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 论文在第 6 节“Future Work”中已给出三条官方路线，结合最新社区动向与生产痛点，可进一步细化为以下 **六大探索方向**：\n\n* * *\n\n1\\. 指标层：向“三无关”高保真信号演进\n\n*   **目标**：寻找 **model-agnostic、hardware-agnostic、workload-agnostic** 的单一指标，降低新业务上线调参成本。\n*   **可行路径**\n    *   从推理引擎内部提取 **KV-cache hit ratio、block eviction rate、attention compute intensity** 等细粒度统计，构建复合指标。\n    *   引入 **LLM 内部队列长度**（vLLM 的 waiting queue、TensorRT-LLM 的 context buffer occupancy）作为内存压力的早期信号。\n    *   采用 **小样本元学习** 对不同模型/硬件做快速校准，避免逐业务压测。\n\n* * *\n\n2\\. P/D 比例：从固定到在线自适应\n\n*   **目标**：实时感知 **prompt 长度漂移、输出长度分布变化、突发 batch 规模变化**，微调 P/D 比例（±10 % 以内），进一步榨取 3-5 % 利用率。\n*   **可行路径**\n    *   将 P/D 比例建模为 **在线强化学习问题**：状态 = 当前比例 + 负载特征；动作 = 微调比例；奖励 = 吞吐 - λ·SLO 违规。\n    *   采用 **轻量级 bandit 算法**（Thompson Sampling）在分钟级时间窗口内试错，避免重训练。\n    *   引入 **prompt 长度预测器**（基于历史用户会话）作为前馈信号，提前 30-60 s 调整比例。\n\n* * *\n\n3\\. KV-Cache 感知扩缩\n\n*   **目标**：把 **KV-Cache 命中率、内存碎片、逐出频率** 直接纳入策略引擎，解决 Decode 阶段“高 GPU 利用率但低有效吞吐”假象。\n*   **可行路径**\n    *   在 Deployment Group 内增加 **KV-Cache 水位线**（cache\\_watermark）指标，触发 **Decode-only 扩容** 而非联合扩容，减少 Prefill 浪费。\n    *   与 **Mooncake / KVDirect** 等 KV-Cache 分离存储方案联动，实现“计算-缓存”双维度弹性。\n\n* * *\n\n4\\. 异构硬件的 **“微切片”** 调度\n\n*   **目标**：在 **单张 GPU 内部** 做阶段级切片（e.g., H20 的 60 % SM 给 Prefill，40 % 给 Decode），进一步消除碎片。\n*   **可行路径**\n    *   利用 **MIG（Multi-Instance GPU）** 或 **时间片抢占**（Gandiva-style）实现 **sub-GPU 级别** 的 P/D 共置。\n    *   设计 **细粒度拓扑约束**：同 GPU 内 Prefill/Decode 通过 NVLink 共享 KV-Cache，减少跨 GPU 流量。\n\n* * *\n\n5\\. 跨集群 / 跨地域弹性\n\n*   **目标**：将 HeteroScale 的 **拓扑亲和抽象** 扩展到 **跨 DC 场景**，应对区域级故障或 Spot 实例抢占。\n*   **可行路径**\n    *   引入 **S3 级别**（跨数据中心）Deployment Group，允许 KV-Cache 通过 **RDMA over Converged Ethernet (RoCEv2)** 远程传输，牺牲 5-10 % 延迟换取 20 % 成本下降。\n    *   设计 **分层扩缩策略**：本地集群优先，区域级冷备，全局负载均衡器根据 **token 路由延迟** 动态切换。\n\n* * *\n\n6\\. 绿色 AI：能耗-延迟联合优化\n\n*   **目标**：在扩缩决策中显式引入 **能耗成本**（kWh/token），实现 **PUE-aware** 的 LLM 推理。\n*   **可行路径**\n    *   将 GPU 功耗模型（DCGM 实时功耗 + 温度）纳入策略引擎奖励函数：  \n        $$ \\\\text{reward} = \\\\alpha \\\\cdot \\\\text{throughput} - \\\\beta \\\\cdot \\\\text{latency\\_penalty} - \\\\gamma \\\\cdot \\\\text{power} $$\n    *   利用 **动态电压频率调节 (DVFS)** 在 Decode 低负载时降频，结合 HeteroScale 的缩容决策，实现 **能耗-性能帕累托前沿**。\n\n* * *\n\n小结：路线图\n\n方向\n\n预期收益\n\n技术风险\n\n三无关指标\n\n新业务上线 0 压测\n\n指标漂移、跨模型泛化\n\n动态 P/D 比例\n\n+3-5 % GPU 利用率\n\n强化学习稳定性\n\nKV-Cache 感知\n\n消除 Decode 假象\n\n额外监控开销\n\n微切片调度\n\n碎片率 ↓30 %\n\nGPU 上下文切换开销\n\n跨集群弹性\n\n区域级容灾\n\n网络延迟放大\n\n能耗优化\n\n10-15 % 电费节省\n\n功耗模型精度\n\n上述方向既可独立迭代，也可组合成 **下一代 HeteroScale v2** 的路线图。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: ### HeteroScale 论文一句话总结  \n面向 Prefill-Decode 分离式 LLM 推理，提出一种 **拓扑感知、异构硬件友好、单一指标驱动** 的协同自动扩缩框架，在数万个 GPU 的生产集群中将平均 GPU 利用率提升 26.6 pp，每天节省数十万 GPU 小时，同时严守 SLO。\n\n* * *\n\n核心内容速览\n\n维度\n\n关键信息\n\n**问题**\n\n1) 同构 GPU 导致 41 % 成本浪费；2) 跨交换机 KV-Cache 带宽下降 20 %；3) 独立扩缩 Prefill/Decode 造成架构失衡。\n\n**设计**\n\n三层架构：<br>• Autoscaling Layer：Decode TPS 为唯一主信号，比例控制 + 负反馈协同伸缩；<br>• Federated Pre-Scheduling：Deployment Group + RDMA Subgroup 抽象，实现网络亲和与异构资源优先级；<br>• Sub-cluster Scheduling：与 Kubernetes CRD 对接，软缩入与灾难恢复。\n\n**算法**\n\n• Algorithm 2：比例控制维持固定 P/D 比；<br>• Algorithm 3：负反馈应对 TTFT/TBT 悬崖；<br>• Algorithm 4：拓扑树 + 优先级装箱调度。\n\n**实验**\n\n• 微基准：P/D 比 1:5~9:1 区间验证最优值依赖业务特征；<br>• 回放仿真：Decode TPS 指标响应最快、无过冲；<br>• 生产上线：GPU 利用率 +26.6 pp，SM 活动 +9.2 pp，跨对话/视觉-语言多模态一致收益。\n\n**贡献**\n\n1) 首个大规模 P/D 分离 LLM 自动扩缩实证研究；<br>2) 提出“Decode TPS”作为跨模型、跨硬件的鲁棒信号；<br>3) 网络亲和抽象（Deployment Group / RDMA Subgroup）可复用到 MoE 等更复杂架构。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.com/_prefill_chat?prefill_prompt= 我们要讨论的论文是Taming%20the%20Chaos%3A%20Coordinated%20Autoscaling%20for%20Heterogeneous%20and%20Disaggregated%20LLM%20Inference，链接是  https://arxiv.org/pdf/2508.19559  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2508.19559  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-16-1756424830931","title":"Training a Foundation Model for Materials on a Budget","description":"Foundation models for materials modeling are advancing quickly, but their training remains expensive, often placing state-of-the-art methods out of reach for many research groups. We introduce Nequix, a compact E(3)-equivariant potential that pairs a simplified NequIP design with modern training practices, including equivariant root-mean-square layer normalization and the Muon optimizer, to retain accuracy while substantially reducing compute requirements. Built in JAX, Nequix has 700K parameters and was trained in 500 A100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequix ranks third overall while requiring less than one quarter of the training cost of most other methods, and it delivers an order-of-magnitude faster inference speed than the current top-ranked model. We release model weights and fully reproducible codebase at https://github.com/atomicarchitects/nequix","url":"https://huggingface.co/papers/2508.16067","source":"HuggingFace Papers","timestamp":"2025-08-28T23:47:10.931Z","metadata":{"authors":["Teddy Koker","Tess Smidt"],"date":"","rank":17,"llmAnalysis":"","categories":["physics.comp-ph","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2508.16067.pdf","arxivUrl":"https://arxiv.org/abs/2508.16067","arxivId":"2508.16067","coolPaperUrl":"https://papers.cool/arxiv/2508.16067","published":"2025-08-22T03:38:06Z","updated":"2025-08-22T03:38:06.000Z","zh_summary":"材料建模的基础模型正在快速发展，但其训练仍然昂贵，常常使最先进的方法对许多研究小组而言难以达到。我们介绍了Nequix，这是一种紧凑的E(3)等变势，结合了简化的NequIP设计和现代训练实践，包括等变均方根层归一化和Muon优化器，以在显著降低计算需求的同时保持准确性。Nequix使用JAX构建，拥有70万个参数，并在500个A100-GPU小时内训练完成。在Matbench-Discovery和MDR声子基准测试中，Nequix总体排名第三，同时其训练成本不到大多数其他方法的四分之一，并且其推断速度比当前排名第一的模型快一个数量级。我们在https://github.com/atomicarchitects/nequix发布模型权重和完全可重现的代码库。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 论文聚焦的核心问题是**材料机器学习势函数（MLIP）训练成本过高**，导致许多研究组难以复现或改进当前最优方法。具体而言：\n\n*   **训练开销大**：现有材料基础模型（如eSEN-30M、MACE-MP-0 等）往往需要数千 A100-GPU 小时，对计算资源要求极高。\n*   **推理速度慢**：高精度模型在部署阶段延迟较高，限制了高通量筛选或实时模拟的可行性。\n\n因此，作者提出\\*\\*“在有限预算下训练高性能材料基础模型”\\*\\*这一正交目标：  \n\\> 通过简化 NequIP 架构、引入现代训练技巧（RMSNorm、Muon 优化器、动态批处理）以及 JAX 高效实现，**在仅 500 A100-GPU 小时、70 万参数规模下，达到与昂贵模型接近的精度，并将推理速度提升一个数量级**。\n\n**Q**: 有哪些相关研究？\n\n**A**: 与 Nequix 直接相关的研究可分为三类：\n\n1.  **E(3)-等变图神经网络势函数**\n    \n    *   NequIP (Batzner et al., 2022)：Nequix 的架构原型，提出 SO(3) 卷积与等变消息传递。\n    *   MACE (Batatia et al., 2022)：高阶等变消息传递，Nequix 在推理速度上与其对比。\n    *   SevenNet (Fu et al., 2025)、GRACE (Barroso-Luque et al., 2024)、HIENet 等：同为 MPtrj 训练的等变势函数，在 Matbench-Discovery 与 MDR phonon 榜单上与 Nequix 并列比较。\n2.  **大规模材料预训练模型**\n    \n    *   CHGNet (Deng et al., 2023)、M3GNet (Chen & Ong, 2022)：早期通用图势函数，参数规模小但精度有限。\n    *   eSEN-30M (Batatia et al., 2023)：当前 Matbench-Discovery 榜首，3000 万参数，训练成本 >2000 A100 h，被 Nequix 用作精度与速度基准。\n    *   ORB v2 (Neumann et al., 2024)、eqV2-S-DeNS (Zhang et al., 2025) 等：同期大模型，强调规模而非成本。\n3.  **训练效率与优化技术**\n    \n    *   Muon optimizer (Jordan et al., 2024b)：利用 Newton–Schul"}}]}