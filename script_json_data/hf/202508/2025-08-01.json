{"timestamp":"2025-08-01T01:57:14.045Z","totalItems":2,"items":[{"id":"hf-0-1754013393800","title":"Seed-Prover: Deep and Broad Reasoning for Automated Theorem Proving","description":"LLMs have demonstrated strong mathematical reasoning abilities by leveraging reinforcement learning with long chain-of-thought, yet they continue to struggle with theorem proving due to the lack of clear supervision signals when solely using natural language. Dedicated domain-specific languages like Lean provide clear supervision via formal verification of proofs, enabling effective training through reinforcement learning. In this work, we propose \\textbf{Seed-Prover}, a lemma-style whole-proof reasoning model. Seed-Prover can iteratively refine its proof based on Lean feedback, proved lemmas, and self-summarization. To solve IMO-level contest problems, we design three test-time inference strategies that enable both deep and broad reasoning. Seed-Prover proves $78.1\\%$ of formalized past IMO problems, saturates MiniF2F, and achieves over 50\\% on PutnamBench, outperforming the previous state-of-the-art by a large margin. To address the lack of geometry support in Lean, we introduce a geometry reasoning engine \\textbf{Seed-Geometry}, which outperforms previous formal geometry engines. We use these two systems to participate in IMO 2025 and fully prove 5 out of 6 problems. This work represents a significant advancement in automated mathematical reasoning, demonstrating the effectiveness of formal verification with long chain-of-thought reasoning.","url":"https://huggingface.co/papers/2507.23726","source":"HuggingFace Papers","timestamp":"2025-08-01T01:56:33.800Z","metadata":{"authors":["Luoxin Chen","Jinming Gu","Liankai Huang","Wenhao Huang","Zhicheng Jiang","Allan Jie","Xiaoran Jin","Xing Jin","Chenggang Li","Kaijing Ma","Cheng Ren","Jiawei Shen","Wenlei Shi","Tong Sun","He Sun","Jiahui Wang","Siran Wang","Zhihong Wang","Chenrui Wei","Shufa Wei","Yonghui Wu","Yuchen Wu","Yihang Xia","Huajian Xin","Fan Yang","Huaiyuan Ying","Hongyi Yuan","Zheng Yuan","Tianyang Zhan","Chi Zhang","Yue Zhang","Ge Zhang","Tianyun Zhao","Jianqiu Zhao","Yichi Zhou","Thomas Hanwen Zhu"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2507.23726.pdf","arxivUrl":"https://arxiv.org/abs/2507.23726","arxivId":"2507.23726","coolPaperUrl":"https://papers.cool/arxiv/2507.23726","published":"2025-07-31T17:00:30Z","updated":"2025-07-31T17:00:30.000Z","zh_summary":"法学硕士通过利用具有长思维链的强化学习表现出了强大的数学推理能力，但由于仅使用自然语言时缺乏明确的监督信号，它们继续在定理证明方面遇到困难。精益等专用领域特定语言通过证明的形式验证提供明确的监督，通过强化学习实现有效的培训。在这项工作中，我们提出了 \\textbf{Seed-Prover}，一种引理式的全证明推理模型。Seed-Prover 可以根据精益反馈、已证明引理和自我总结迭代完善其证明。为了解决 IMO 级别的竞赛问题，我们设计了三种测试时推理策略，这些策略既能实现深度推理，又能进行广泛的推理。Seed-Prover 证明了 78.1\\%$ 的正式过去 IMO 问题，使 MiniF2F 饱和，并在 PutnamBench 上实现了超过 50\\%，大大优于之前的最先进的技术。为了解决精益中缺乏几何支持的问题，我们引入了一个几何推理引擎 \\textbf{Seed-Geometry}，它的性能优于以前的形式化几何引擎。我们使用这两个系统来参与 IMO 2025，并完全证明了 6 个问题中的 5 个。这项工作代表了自动化数学推理的重大进步，证明了长思维链推理的形式验证的有效性。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图解决如何利用大型语言模型（LLMs）进行自动化的数学定理证明问题，特别是在处理复杂的数学竞赛问题和正式化数学问题时的挑战。具体而言，论文提出了一种名为Seed-Prover的模型，旨在通过结合正式语言（如Lean）的清晰监督信号和长链推理能力，克服自然语言证明中缺乏明确监督信号的问题，从而有效提升定理证明的性能。\n\n此外，论文还针对Lean在几何问题支持上的不足，引入了一个专门的几何推理引擎Seed-Geometry，以增强系统在解决几何问题方面的能力。最终目标是在国际数学奥林匹克竞赛（IMO）等高水平数学竞赛中实现自动化的定理证明，并在多个正式化的数学基准测试中取得优异表现。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了以下相关研究：\n\n*   **AlphaProof**：使用Lean成功解决了2024年国际数学奥林匹克竞赛（IMO）中的3个问题，展示了LLMs在正式语言中证明复杂问题的能力。\n*   **Proof Artifact Co-training**、**Curriculum Learning**、**Hypertree Proof Search**、**ReProver**、**COPRA**、**DeepSeek-Prover-V1**、**InternLM2-Math-Plus**、**TheoremLlama**、**Lean-STaR**、**InternLM2-StepProver**、**DeepSeek-Prover-V1.5**、**ABEL**、**Alchemy**、**InternLM2.5-StepProver**、**3D-Prover**、**HunyuanProver**、**STP**、**BFS-Prover**、**Goedel-Prover**、**Leanabell-Prover**、**Kimina-Prover-Preview**、**DeepSeek-Prover-V2**、**DSP+**、**Kimina-Prover**、**Goedel-Prover-V2**、**Delta-Prover**：这些是近年来在MiniF2F测试中性能不断提升的模型，展示了在正式化数学问题上的研究进展。\n*   **Big-Math**：一个大规模、高质量的数学数据集，用于通过强化学习训练语言模型。\n*   **DeepSeek-AI**：通过强化学习激励LLMs的推理能力，特别是在数学基准测试如MATH和AIME上。\n*   **STP**：自博弈LLM定理证明器，通过迭代猜想和证明进行自我提升。\n*   \\*\\*Draft, Sketch, and Pro"}},{"id":"hf-1-1754013393801","title":"RecGPT Technical Report","description":"Recommender systems are among the most impactful applications of artificial intelligence, serving as critical infrastructure connecting users, merchants, and platforms. However, most current industrial systems remain heavily reliant on historical co-occurrence patterns and log-fitting objectives, i.e., optimizing for past user interactions without explicitly modeling user intent. This log-fitting approach often leads to overfitting to narrow historical preferences, failing to capture users' evolving and latent interests. As a result, it reinforces filter bubbles and long-tail phenomena, ultimately harming user experience and threatening the sustainability of the whole recommendation ecosystem. To address these challenges, we rethink the overall design paradigm of recommender systems and propose RecGPT, a next-generation framework that places user intent at the center of the recommendation pipeline. By integrating large language models (LLMs) into key stages of user interest mining, item retrieval, and explanation generation, RecGPT transforms log-fitting recommendation into an intent-centric process. To effectively align general-purpose LLMs to the above domain-specific recommendation tasks at scale, RecGPT incorporates a multi-stage training paradigm, which integrates reasoning-enhanced pre-alignment and self-training evolution, guided by a Human-LLM cooperative judge system. Currently, RecGPT has been fully deployed on the Taobao App. Online experiments demonstrate that RecGPT achieves consistent performance gains across stakeholders: users benefit from increased content diversity and satisfaction, merchants and the platform gain greater exposure and conversions. These comprehensive improvement results across all stakeholders validates that LLM-driven, intent-centric design can foster a more sustainable and mutually beneficial recommendation ecosystem.","url":"https://huggingface.co/papers/2507.22879","source":"HuggingFace Papers","timestamp":"2025-08-01T01:56:33.801Z","metadata":{"authors":["Chao Yi","Dian Chen","Gaoyang Guo","Jiakai Tang","Jian Wu","Jing Yu","Mao Zhang","Sunhao Dai","Wen Chen","Wenjun Yang","Yuning Jiang","Zhujin Gao","Bo Zheng","Chi Li","Dimin Wang","Dixuan Wang","Fan Li","Fan Zhang","Haibin Chen","Haozhuang Liu","Jialin Zhu","Jiamang Wang","Jiawei Wu","Jin Cui","Ju Huang","Kai Zhang","Kan Liu","Lang Tian","Liang Rao","Longbin Li","Lulu Zhao","Na He","Peiyang Wang","Qiqi Huang","Tao Luo","Wenbo Su","Xiaoxiao He","Xin Tong","Xu Chen","Xunke Xi","Yang Li","Yaxuan Wu","Yeqiu Yang","Yi Hu","Yinnan Song","Yuchen Li","Yujie Luo","Yujin Yuan","Yuliang Yan","Zhengyang Wang","Zhibo Xiao","Zhixin Ma","Zile Zhou","Ziqi Zhang"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.IR","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2507.22879.pdf","arxivUrl":"https://arxiv.org/abs/2507.22879","arxivId":"2507.22879","coolPaperUrl":"https://papers.cool/arxiv/2507.22879","published":"2025-07-30T17:55:06Z","updated":"2025-07-30T17:55:06.000Z","zh_summary":"推荐系统是人工智能最具影响力的应用之一，作为连接用户、商家和平台的关键基础设施。然而，当前大多数工业系统仍然严重依赖历史共现模式和日志拟合目标，即优化过去用户互动，而没有明确建模用户意图。这种日志拟合方法往往导致对狭窄历史偏好的过拟合，未能捕捉用户不断变化和潜在的兴趣。因此，这加剧了过滤气泡和长尾现象，最终损害用户体验，并威胁到整个推荐生态系统的可持续性。为了解决这些挑战，我们重新思考推荐系统的总体设计范式，并提出了RecGPT，一个将用户意图置于推荐管道中心的下一代框架。通过将大型语言模型（LLMs）集成到用户兴趣挖掘、物品检索和解释生成的关键阶段，RecGPT将日志拟合推荐转变为以意图为中心的过程。为了在规模上有效地将通用LLMs与上述特定领域的推荐任务对齐，RecGPT结合了多阶段训练范式，集成了增强推理的预对齐和自我训练演变，指导以人类-LLM合作评判系统。目前，RecGPT已在淘宝App上全面部署。在线实验表明，RecGPT在各利益相关者中实现了一致的性能提升：用户受益于内容多样性和满意度的提高，商家和平台获得了更大的曝光率和转化率。这些在所有利益相关者中的全面改善结果验证了以LLM驱动的、以意图为中心的设计可以促进更可持续和互惠的推荐生态系统。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图解决传统推荐系统在用户意图理解和推荐结果多样性方面的局限性问题。具体来说，论文指出当前大多数工业推荐系统过度依赖于历史共现模式和日志拟合目标，即主要通过优化过去的用户交互来生成推荐，而没有明确建模用户的意图。这种做法导致了以下问题：\n\n*   **信息茧房和长尾现象**：推荐系统往往会过度拟合用户狭窄的历史偏好，无法捕捉用户不断演变和潜在的兴趣，从而加剧了信息茧房效应，并且进一步边缘化了长尾内容。\n*   **用户体验受损**：由于无法有效发现用户的潜在兴趣，推荐系统可能会提供重复或过于集中的推荐内容，降低了用户的探索性和满意度。\n*   **商业可持续性问题**：对于商家和平台而言，这种推荐方式可能会导致流量和转化集中在少数热门项目上，而忽视了长尾项目的价值，不利于整个推荐生态系统的可持续发展。\n\n为了解决这些问题，论文提出了 RecGPT，这是一个下一代的推荐系统框架，它将用户意图置于推荐流程的核心位置，并通过整合大型语言模型（LLMs）来挖掘用户兴趣、检索项目和生成解释，从而将传统的日志拟合推荐转变为以意图为中心的推荐过程。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了多个与推荐系统和大型语言模型（LLMs）相关的研究领域，这些研究为 RecGPT 的提出提供了理论基础和技术背景。以下是一些关键的相关研究：\n\n推荐系统领域\n\n*   **特征工程和模型架构的演进**：论文回顾了推荐系统在特征表示和模型架构方面的进展，从手工制作的统计特征到序列和交叉特征，再到最新的生成式 Transformer 背骨网络。这些研究包括：\n    \n    *   **Factorization Machines** (Rendle, 2010)：一种经典的推荐系统模型，通过分解用户和物品的特征来捕捉用户-物品之间的交互。\n    *   **Deep Matching Networks** (Zhang et al., 2019)：利用深度学习技术来建模用户和物品之间的复杂匹配关系。\n    *   **Graph Neural Models** (Wu et al., 2022)：通过图神经网络来捕捉用户和物品之间的图结构关系。\n    *   **Generative Transformer Models** (Deldjoo et al., 2024)：利用 Transformer 架构来生成推荐结果，捕捉用户行为的长序列依赖关系。\n*   **用户意图建模**：论文强调了用户意图建模的重要性，并指出传统方法在这一方面的不足。相关研究包括：\n    \n    *   **User Intent Modeling**：通过分析用户行为来推断用户的潜在意图，例如通过搜索查询、点击行为等来理解用户的需求。\n    *   **Causal Reasoning in Recommendations** (Wang et al., 2022)：利用因果推理来理解用户行为背后的动机，从而提供更准确的推荐。\n\n大型语言模型（LLMs）领域\n\n*   **LLMs 的应用**：论文讨论了 LLMs 在推荐系统中的应用，指出 LLMs 的世界知识和推理能力可以用来更准确地分析用户兴趣和生成推荐。相关研究包括：\n    \n    *   **LLMs for Recommendation** (Wu et al., 2024)：探索如何利用 LLMs 来增强推荐系统，尤其是在小规模、离线基准测试中的应用。\n    *   **Reasoning Capabilities of LLMs** (Zhao et al., 2023)：研究 LLMs 的推理能力，这些能力对于理解用户行为和生成合理的推荐至关重要。\n*   **LLM-as-a-Judge**：论文提出了利用 LLMs 作为评估工具的方法，以提高推荐系统的评估效率和准确性。相关研究包括：\n    \n    *   **LLM-as-a-Judge for Evaluation** (Chen et al., 2024; Gu et al., 2024)：利用 LLMs 来评估推荐系统的输出，通过自动化评估减少人工标注的需求。\n    *   **Bias and Fairness in LLMs** (Dai et al., 2024)：研究 LLMs 在评估过程中可能出现的偏差和不公平性，这对于确保推荐系统的公正性至关重要。\n\n其他相关研究\n\n*   **Curriculum Learning**：论文中提到了利用课程学习（Curriculum Learning）来逐步训练模型，使其能够处理更复杂的任务。相关研究包括：\n    \n    *   **Curriculum Learning for Multiple Tasks** (Pentina et al., 2015)：通过课程学习来逐步训练模型，使其能够处理多个任务。\n    *   **Survey on Curriculum Learning** (Soviany et al., 2022)：对课程学习的全面综述，探讨了其在不同领域的应用。\n*   **Reinforcement Learning**：论文提出了利用强化学习（Reinforcement Learning）来优化推荐系统，以适应不断变化的用户偏好和产品特性。相关研究包括：\n    \n    *   **Reinforcement Learning from Human Feedback** (Casper et al., 2023)：利用人类反馈来训练强化学习模型，以优化推荐系统的性能。\n    *   **Survey of Reinforcement Learning from Human Feedback** (Kaufmann et al., 2024)：对利用人类反馈进行强化学习的综述，探讨了其在推荐系统中的应用。\n\n这些相关研究为 RecGPT 的提出提供了坚实的基础，并展示了如何利用最新的技术进展来解决推荐系统中的关键问题。\n\n**Q**: 论文如何解决这个问题？\n\n**A**: 为了解决传统推荐系统在用户意图理解和推荐结果多样性方面的局限性问题，论文提出了 RecGPT，这是一个以用户意图为中心的下一代推荐系统框架。RecGPT 通过整合大型语言模型（LLMs）到推荐流程的关键阶段，将传统的日志拟合推荐转变为以意图为中心的推荐过程。以下是 RecGPT 解决问题的具体方法：\n\n1\\. 用户意图挖掘（User Interest Mining）\n\nRecGPT 首先利用一个用户兴趣 LLM（LLMUI）来分析用户的终身行为历史，并生成一个简洁的自然语言用户兴趣画像。为了适应 LLM 的上下文窗口限制并提高兴趣挖掘的效率，论文提出了以下方法：\n\n*   **可靠行为序列压缩**：通过可靠行为提取和层次化行为压缩，将用户的多源异构行为序列压缩成信息密度更高的格式，以适应 LLM 的上下文窗口限制。\n*   **多阶段任务对齐框架**：通过课程学习基础多任务微调、推理增强预对齐和自训练演化，逐步提升 LLMUI 在用户兴趣挖掘任务上的性能。\n\n2\\. 项目标签预测（Item Tag Prediction）\n\n基于用户兴趣挖掘的结果，RecGPT 使用一个项目标签 LLM（LLMIT）来预测用户可能感兴趣的项目标签。这些标签用于后续的项目检索阶段。为了使 LLM 能够适应特定的产品领域，论文采用了以下方法：\n\n*   **多阶段任务对齐**：通过推理增强预对齐和自训练演化，使 LLMIT 能够理解和处理与产品相关的上下文信息。\n*   **增量学习**：通过定期使用用户的在线交互记录进行增量学习，使模型能够适应用户兴趣的变化和新的产品趋势。\n\n3\\. 项目检索（Item Retrieval）\n\nRecGPT 提出了一种标签感知的语义相关性检索方法，将预测的标签映射到特定的项目，同时结合用户行为的协同信号，以平衡语义相关性和协同相关性。具体方法包括：\n\n*   **用户-项目-标签检索框架**：通过用户塔、项目塔和标签塔的协同工作，生成协同分数和语义分数，最终通过加权融合实现可控的推荐。\n*   **优化算法**：通过对比学习优化协同相关性和语义相关性，同时引入类别对比损失函数以增强语义区分能力。\n\n4\\. 推荐解释生成（Recommendation Explanation Generation）\n\n为了提高推荐系统的透明度和用户体验，RecGPT 使用一个推荐解释 LLM（LLMRE）为最终推荐的项目生成个性化的解释。具体方法包括：\n\n*   **任务对齐**：通过推理增强预对齐和自训练演化，使 LLMRE 能够生成符合用户兴趣和项目特征的解释。\n*   **离线生产**：通过离线生成兴趣-项目-解释查找表，实现在在线推荐时快速检索个性化解释。\n\n5\\. 人机协作评估系统（Human-LLM Cooperative Judge）\n\n为了确保 LLM 生成的推荐结果符合人类的主观期望，论文提出了一个人机协作评估系统。该系统通过以下方法实现：\n\n*   **LLM 作为评估工具**：利用 LLM 的推理能力进行自动化评估，减少人工标注的需求。\n*   **人类在循环中**：通过定期的人类监督和针对性的微调，确保评估系统的可靠性和适应性。\n\n通过上述方法，RecGPT 不仅能够更准确地挖掘用户的潜在兴趣，还能生成多样化的推荐结果，并为用户提供透明的推荐解释。这不仅提高了用户体验，还为商家和平台带来了更大的商业价值，最终建立了一个更可持续和互利共赢的推荐生态系统。\n\n**Q**: 论文做了哪些实验？\n\n**A**: 论文中进行了多种实验来验证 RecGPT 的有效性，包括在线 A/B 测试、人类评估实验、案例研究和用户体验调查。以下是详细的实验内容：\n\n在线 A/B 测试\n\n*   **实验设置**：在淘宝首页的“猜你喜欢”场景中进行了为期一个月的在线 A/B 测试。实验组和对照组各分配了 1% 的流量。实验组使用 RecGPT 系统生成推荐，而对照组继续使用现有的基础推荐系统。\n*   **评估指标**：\n    *   **用户体验**：用户停留时间（DT）、曝光物品类别多样性（EICD）、点击物品类别多样性（CICD）。\n    *   **平台收益**：物品页面浏览量（IPV）、点击通过率（CTR）、每日点击活跃用户数（DCAU）、加入购物车数量（ATC）。\n*   **实验结果**：\n    *   **用户体验**：RecGPT 显著提高了用户停留时间（+4.82%）、曝光物品类别多样性（+0.11%）和点击物品类别多样性（+6.96%）。这表明 RecGPT 能够通过语义理解发现用户的潜在兴趣，提供更广泛类别的推荐，同时保持相关性。\n    *   **平台收益**：RecGPT 在关键参与度指标上表现出显著提升，包括 IPV（+9.47%）、CTR（+6.33%）和 DCAU（+3.72%）。这表明 RecGPT 能够更精准地推荐与用户兴趣相符的物品，减少浪费的展示，提高内容相关性。\n    *   **商家收益**：RecGPT 有效缓解了马太效应，为不同规模和受欢迎程度的商家提供更公平的曝光机会。如图 1 所示，与基线系统相比，RecGPT 在不同物品受欢迎程度组别中实现了更均匀的 CTR 表现，为较不受欢迎的物品提供了有意义的曝光机会。\n\n人类评估实验\n\n*   **实验目的**：验证 LLM 作为评估工具在推荐生成任务中的有效性。\n*   **实验设置**：使用 Qwen3 作为基础评估模型，通过监督式微调（SFT）在收集的人类评估数据上进行训练，得到 Qwen3-Judge-SFT 模型。对用户兴趣挖掘、项目标签预测和推荐解释生成三个任务的生成输出进行评估，采用二元分类或多级评估标准。\n*   **评估指标**：准确率（ACC）、精确率、召回率和 F1 分数。\n*   **实验结果**：\n    *   **用户兴趣挖掘**：Qwen3-Judge-Base 的准确率为 67.77%，经过 SFT 后，Qwen3-Judge-SFT 的准确率提升至 76.89%。\n    *   **项目标签预测**：Qwen3-Judge-Base 的准确率为 87.41%，Qwen3-Judge-SFT 的准确率提升至 93.08%。\n    *   **推荐解释生成**：Qwen3-Judge-Base 的准确率为 56.77%，Qwen3-Judge-SFT 的准确率大幅提升至 89.76%。\n\n这些结果表明，通过在人类评估数据上进行监督式微调，可以显著提高 LLM 作为评估工具的性能，使其能够可靠地评估推荐生成任务的质量。\n\n案例研究\n\n*   **实验目的**：通过具体案例展示 RecGPT 的工作流程和效果。\n*   **实验内容**：以一位 30 岁女性用户为例，分析其在淘宝上的三年行为历史，包括购买、搜索和浏览活动。RecGPT 的用户兴趣挖掘模块识别出“时尚穿搭”和“育儿与婴儿护理”两大兴趣领域。项目标签预测模块将这些兴趣转化为具体的项目标签，如“亚麻混纺阔腿裤套装”和“婴儿水温测量计”。用户-项目-标签检索框架利用这些标签选择与用户多样化兴趣相匹配的相关产品。个性化推荐解释模块生成个性化理由，将推荐项目与用户的行为历史清晰地联系起来。\n\n这个案例强调了 RecGPT 的核心优势：利用特定任务的大型语言模型，结合广泛的世界知识和逻辑推理，揭示用户隐藏的和多样化的兴趣，同时保持相关性。\n\n用户体验调查\n\n*   **实验目的**：系统地验证 RecGPT 在提高推荐质量和用户体验方面的有效性。\n*   **实验设置**：随机选择 500 名活跃用户，分为对照组和实验组。对照组接收由基线算法生成的推荐，实验组接收由 RecGPT 增强的推荐系统生成的推荐。\n*   **评估方法**：\n    *   采用三名评估者共识机制，只有在评估者意见一致时才将决策视为有效响应，以确保高可靠性并最小化主观偏差。\n    *   评估过程包括历史回顾、推荐分析和冗余评估三个步骤。\n*   **实验结果**：RecGPT 有效减少了推荐的冗余性。人类评估者在 RecGPT 系统中识别出更少的重复项目，重复率从基线的 37.1% 降低到 36.2%。在前 4 个推荐位置中，相似产品聚类的减少更为显著，从 27.7% 降低到 25.3%，表明 RecGPT 成功地在用户最关注的位置多样化了推荐。\n\n这些实验结果表明，RecGPT 在提高推荐质量和用户体验方面具有显著优势，能够有效减少推荐的冗余性，提供更丰富多样的选择。\n\n**Q**: 有什么可以进一步探索的点？\n\n**A**: 尽管 RecGPT 在在线 A/B 测试中展现出了显著的性能提升，但论文也指出了其存在的一些局限性，并提出了未来可以进一步探索的方向：\n\n1\\. 建模超长用户序列\n\n*   **问题**：处理超长用户行为序列对当前模型来说是一个挑战。一方面，模型训练和推理的计算负担巨大，处理大量用户历史记录时成本过高；另一方面，模型可能在长序列中关注到无关的噪声，而非有意义的兴趣模式，导致对用户兴趣的理解出现偏差。\n*   **探索方向**：研究专门针对 LLMs 的先进序列建模技术，重点改进上下文工程，动态优化用户行为序列的长期和短期记忆管理、上下文选择以及信息压缩。\n\n2\\. 多目标联合学习与强化学习\n\n*   **问题**：目前 RecGPT 依赖于监督学习和定期的模型更新，这在适应现实世界中不断演变的用户偏好和产品特性方面存在局限性。此外，不同的生成任务是分开训练的，没有实现理想的联合优化，尽管它们共同服务于最终的推荐目标。\n*   **探索方向**：引入基于强化学习（RL）的多目标联合优化，利用在线用户反馈数据作为统一的优化信号。这将允许在所有生成任务之间进行联合训练，同时优化多个目标，如用户参与度、转化率和平台长期健康度，从而提高模型对真实世界用户互动的适应性。\n\n3\\. 端到端的 LLM-as-a-Judge 系统\n\n*   **问题**：当前的 RecGPT 评估框架侧重于单独评估各个任务的质量，需要为不同的评估维度准备单独的训练数据。这导致了一个分散的评估过程，缺乏全面的端到端上下文理解，无法综合评估多个方面。\n*   **探索方向**：开发一个端到端的 LLM-as-a-Judge 系统，结合人类反馈的强化学习（RLHF）方法，使用人类反馈来训练能够进行综合多任务评估的评估器。此外，探索推理时可扩展的生成式奖励模型，以在推理过程中动态分配计算资源，提高评估质量，并提供更细致的流程评估。\n\n4\\. 实时交互与动态反馈\n\n*   **问题**：在现实世界的应用场景中，用户的行为和偏好是动态变化的，而推荐系统需要能够实时响应这些变化，以提供最相关和及时的推荐。\n*   **探索方向**：研究如何使 RecGPT 更好地适应实时交互和动态反馈。例如，通过引入在线学习机制，使模型能够实时更新和调整其对用户兴趣的理解，从而提供更精准的推荐。\n\n5\\. 多模态融合\n\n*   **问题**：目前 RecGPT 主要依赖于文本信息来进行用户兴趣挖掘和项目推荐，但在实际应用中，用户的行为和偏好可能还受到其他模态信息（如图像、视频等）的影响。\n*   **探索方向**：探索如何将多模态信息融合到 RecGPT 中，以更全面地理解用户兴趣和生成更丰富的推荐。例如，利用图像识别技术来分析用户对商品外观的偏好，或者利用视频内容来挖掘用户对特定主题的兴趣。\n\n6\\. 跨领域推荐\n\n*   **问题**：在一些情况下，用户可能在不同的领域或平台上表现出不同的兴趣和行为模式，而这些信息对于提供更全面的推荐可能非常有价值。\n*   **探索方向**：研究如何使 RecGPT 能够跨领域进行推荐，即将用户在不同领域或平台上的行为和偏好信息整合起来，以提供更个性化和多样化的推荐。\n\n7\\. 用户隐私和数据安全\n\n*   **问题**：随着推荐系统对用户数据的依赖程度越来越高，用户隐私和数据安全问题变得越来越重要。\n*   **探索方向**：研究如何在保护用户隐私和数据安全的前提下，有效地利用用户数据来训练和优化 RecGPT。例如，采用差分隐私技术或联邦学习方法来保护用户数据。\n\n这些探索方向将有助于进一步提升 RecGPT 的性能和适应性，使其能够更好地满足用户需求，为用户提供更优质、更个性化的推荐体验。\n\n**Q**: 总结一下论文的主要内容\n\n**A**: 论文提出了 RecGPT，这是一个创新的推荐系统框架，旨在通过整合大型语言模型（LLMs）来解决传统推荐系统在用户意图理解和推荐结果多样性方面的局限性。以下是论文的主要内容总结：\n\n研究背景\n\n*   传统的推荐系统主要依赖于历史共现模式和日志拟合目标，缺乏对用户意图的明确建模，导致推荐结果往往局限于用户的历史偏好，无法捕捉用户的潜在兴趣。\n*   这种方法容易导致信息茧房效应，强化用户的过滤泡沫，同时加剧了长尾现象，不利于用户体验和推荐生态系统的可持续发展。\n\n研究目标\n\n*   提出一个以用户意图为中心的推荐系统框架，通过利用 LLMs 的世界知识和推理能力，更准确地挖掘用户兴趣，提供多样化的推荐结果，打破信息茧房，促进推荐生态系统的健康发展。\n\n方法\n\n*   **RecGPT 框架**：RecGPT 通过将用户意图挖掘、项目标签预测、项目检索和推荐解释生成四个关键阶段与 LLMs 相结合，实现从日志拟合推荐到意图驱动推荐的转变。\n*   **用户意图挖掘**：使用 LLMUI 分析用户的终身行为历史，生成自然语言的用户兴趣画像。通过可靠行为序列压缩和多阶段任务对齐框架，提升 LLMUI 的性能。\n*   **项目标签预测**：基于用户兴趣挖掘结果，LLMIT 预测用户可能感兴趣的项目标签。采用多阶段任务对齐和增量学习方法，使 LLMIT 能够适应产品领域的特定需求。\n*   **项目检索**：提出标签感知的语义相关性检索方法，结合用户行为的协同信号，平衡语义相关性和协同相关性，提高推荐的准确性和多样性。\n*   **推荐解释生成**：LLMRE 根据用户兴趣和推荐项目生成个性化的推荐解释，提高推荐系统的透明度和用户体验。\n*   **人机协作评估系统**：利用 LLM 作为评估工具，结合人类专家的监督，确保推荐结果符合人类的主观期望，同时提高评估效率。\n\n实验\n\n*   **在线 A/B 测试**：在淘宝首页的“猜你喜欢”场景中进行的测试表明，RecGPT 在用户体验（如用户停留时间、物品类别多样性）和平台收益（如物品页面浏览量、点击通过率）方面均取得了显著提升。\n*   **人类评估实验**：验证了 LLM 作为评估工具的有效性，经过监督式微调的 LLM 在用户兴趣挖掘、项目标签预测和推荐解释生成等任务上的性能得到了显著提升。\n*   **案例研究**：通过具体案例展示了 RecGPT 如何通过分析用户行为历史，挖掘用户兴趣，并生成与用户兴趣相匹配的推荐项目和个性化解释。\n*   **用户体验调查**：通过用户调查验证了 RecGPT 在减少推荐冗余、提高推荐多样性方面的优势。\n\n结论\n\nRecGPT 通过利用 LLMs 的强大能力，成功地将用户意图挖掘和语义理解引入推荐系统，为用户提供更个性化、多样化的推荐体验，同时为商家和平台带来了显著的商业价值。此外，RecGPT 的人机协作评估系统为推荐系统的优化提供了一种高效、可靠的方法。\n\n未来工作\n\n*   **超长用户序列建模**：探索先进的序列建模技术，以更好地处理超长用户行为序列。\n*   **多目标联合学习与强化学习**：引入强化学习进行多目标联合优化，以适应不断变化的用户偏好和产品特性。\n*   **端到端的 LLM-as-a-Judge 系统**：开发能够进行综合多任务评估的端到端评估系统，提高评估的全面性和效率。\n\nRecGPT 的提出为推荐系统领域带来了新的视角和方法，展示了 LLMs 在提升推荐质量和用户体验方面的巨大潜力。\n\n**Q**: 想要进一步了解论文\n\n**A**: 以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**]( http://kimi.moonshot.cn/_prefill_chat?prefill_prompt= 我们要讨论的论文是RecGPT%20Technical%20Report，链接是  https://arxiv.org/pdf/2507.22879  ，已有的FAQ链接是  https://papers.cool/arxiv/kimi?paper=2507.22879  。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}