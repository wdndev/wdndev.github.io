{"timestamp":"2026-01-20T00:01:31.076Z","totalItems":20,"items":[{"id":"hf-0-1768866682131","title":"Your Group-Relative Advantage Is Biased","description":"Reinforcement Learning from Verifier Rewards (RLVR) has emerged as a widely used approach for post-training large language models on reasoning tasks, with group-based methods such as GRPO and its variants gaining broad adoption. These methods rely on group-relative advantage estimation to avoid learned critics, yet its theoretical properties remain poorly understood. In this work, we uncover a fundamental issue of group-based RL: the group-relative advantage estimator is inherently biased relative to the true (expected) advantage. We provide the first theoretical analysis showing that it systematically underestimates advantages for hard prompts and overestimates them for easy prompts, leading to imbalanced exploration and exploitation. To address this issue, we propose History-Aware Adaptive Difficulty Weighting (HA-DW), an adaptive reweighting scheme that adjusts advantage estimates based on an evolving difficulty anchor and training dynamics. Both theoretical analysis and experiments on five mathematical reasoning benchmarks demonstrate that HA-DW consistently improves performance when integrated into GRPO and its variants. Our results suggest that correcting biased advantage estimation is critical for robust and efficient RLVR training.","url":"https://huggingface.co/papers/2601.08521","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.131Z","metadata":{"authors":["Fengkai Yang","Zherui Chen","Xiaohan Wang","Xiaodong Lu","Jiajun Chai","Guojun Yin","Wei Lin","Shuai Ma","Fuzhen Zhuang","Deqing Wang","Yaodong Yang","Jianxin Li","Yikun Ban"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2601.08521.pdf","arxivUrl":"https://arxiv.org/abs/2601.08521","arxivId":"2601.08521","coolPaperUrl":"https://papers.cool/arxiv/2601.08521","published":"2026-01-13T13:03:15Z","updated":"2026-01-13T13:03:15.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“基于群体的强化学习从验证器奖励（RLVR）”框架中的一项被忽视的核心缺陷——群体相对优势估计（group-relative advantage estimation）存在系统性偏差——展开理论与算法研究。具体而言，其聚焦并解决以下关键问题：\n\n1. 理论缺陷的发现与量化\n\n- 首次严格证明：在小样本群体（如 G=8）下，以群体平均奖励为基线的优势估计量\n\nhat A_(t,i)=r_(t,i)-(1) / (G)∑_(j=1)^G r_(t,j)\n\n对真实期望优势\n\nA_(t,i)=r_(t,i)-p_t,quad p_t=mathbb E[rmid x_t,π_(θ_t)]\n\n存在**有偏性**。\n- 偏差方向与 prompt 难度挂钩：\n– 困难 prompt（ p_t < 0.5 ）被**系统性低估**；\n– 简单 prompt（ p_t > 0.5 ）被**系统性高估**；\n– 仅当  p_t=0.5  时无偏。\n- 给出概率意义下的误差界，并证明当  Gle 8  时，低估/高估概率均高于 63%，极端难度下甚至超过 78%。\n2. 训练后果的剖析\n\n- 低估导致策略对困难问题**探索不足**；\n- 高估导致策略对简单问题**过度利用**；\n- 长期训练下出现“易题过拟合、难题欠学习”的失衡，降低整体泛化与稳定性。\n3. 纠偏算法的设计与验证\n\n- 提出 **History-Aware Adaptive Difficulty Weighting (HA-DW)**，通过两阶段机制在线修正优势：\n1. 跨批次历史信息维护一个**演化难度锚点**  C_t ，实时追踪模型能力漂移；\n2. 基于 prompt 相对难度  M_t=|hat p_t - C_t|  与方向信号  D_(t,i) ，对每条样本的优势施加指数重权\n\nPhi_(t,i)=λ_(scale)exp!l(D_(t,i)· M_tr),\n\n从而放大困难样本、抑制简单样本的梯度贡献。\n- 理论证明：在恰当  λ_(scale)  范围内，HA-DW 能将期望偏差\n\nl|mathbb E[hat A_(t,i)Phi_(t,i)mid S]-A_(t,i)r|\n\n严格压缩至低于原始偏差\n\nl|mathbb E[hat A_(t,i)mid S]-A_(t,i)r|。\n\n4. 实证验证与规模测试\n\n- 在 Qwen-3-4B/8B 与 LLaMA-3.2-3B 上，将 HA-DW 作为即插即用模块嵌入 GRPO、GSPO、DAPO 等主流群体算法，在 MATH500、AIME25、AMC23、Minerva、OlympiadBench 五个数学推理基准上**一致提升**，平均增幅 1.7–3.4 个百分点。\n- 在固定 rollout 预算（G=8）下，HA-DW 即可超越 rollout 增至 16 的原始算法，验证其在低样本场景下的**偏置修正与样本效率优势**。\n\n综上，论文首次揭示并系统纠正了群体相对 RLVR 中“优势估计偏差”这一根本缺陷，为后续推理导向的大模型后训练提供了更稳健、高效的强化学习范式。\n\nQ2: 有哪些相关研究？\n\n论文在正文与附录 A 中系统梳理了与“群体相对策略优化（GRPO）”及“优势估计偏差”相关的研究，可归纳为以下四条主线：\n\n1\\. 群体相对 RLVR 算法族\n\n| 方法 | 核心改进 | 与本文关联 |\n| --- | --- | --- |\n| GRPO (Shao et al., 2024) | 去掉价值网络，用组内奖励均值做基线 | 被本文首次理论证明存在系统性偏差 |\n| GSPO (Zheng et al., 2025) | 序列级重要性采样 + 序列级优势 | 同样使用组内均值基线，偏差问题同 GRPO |\n| DAPO (Yu et al., 2025) | Token 级优势 + 解耦裁剪 + 动态采样 | 基线仍是组内均值，本文将其作为 HA-DW 的嵌入对象之一 |\n| Dr.GRPO (Liu et al., 2025b) | 移除启发式归一化，降低梯度方差 | 未触碰“基线偏差”本质 |\n| GMPO (Zhao et al., 2025) | 几何平均替代算术平均，缓解异常 IS 比 | 组内统计量依旧用于优势计算 |\n| DCPO (Yang et al., 2025) | 动态裁剪 + 平滑优势标准化 | 仅对优势做方差控制，未纠正期望偏差 |\n| FAPO (Ding et al., 2025) | 用生成式奖励模型识别“假正例” rollout | 关注奖励噪声，而非基线偏差 |\n| SPO (Guo et al., 2025) | 段级优势估计，改善信用分配 | 仍依赖组内奖励分布，偏差问题仍在 |\n| KTAE (Sun et al., 2025) | Token- rollout 联合统计，细化粒度 | 组内均值依旧出现，偏差未被讨论 |\n\n**结论**：现有 GRPO 变种主要围绕“方差”“裁剪”“粒度”做改进，**均未意识到或解决组内均值基线带来的有偏估计**。\n\n2\\. 跨批次（Cross-batch）信息利用\n\n| 领域 | 代表工作 | 技术要点 |\n| --- | --- | --- |\n| 对比学习 | XBM (Wang et al., 2020) | 用历史批次负样本扩大候选池 |\n| 推荐系统 | CBNS (Wang et al., 2021) | 跨批次负采样提升 embedding 一致性 |\n| 视觉 BN | CIBN (Yao et al., 2021) | 跨迭代统计量缓解小 batch 抖动 |\n| 人脸识别 | CBHEM-PLB (Tan et al., 2022) | 跨批次难例挖掘 + 伪大 batch |\n\n**与本文区别**：HA-DW 首次把“跨批次历史奖励”引入 **RLVR 难度锚点**构建，用于**在线纠正优势偏差**，而非单纯扩充样本或抑制噪声。\n\n3\\. 有偏估计的通用理论/矫正方法\n\n| 方向 | 代表工作 | 关键思想 |\n| --- | --- | --- |\n| 偏差-方差权衡 | Hastie et al., 2009; Murphy, 2012 | 显式分解误差来源，为本文“期望偏差”分析提供理论语境 |\n| Off-policy 矫正 | Retrace (Munos et al., 2016) | 用截断 IS 权重消除离线数据偏差 |\n| 分布式 RL | V-trace (Espeholt et al., 2018) | 重要性采样 + 截断，控制价值函数偏差 |\n| 双重鲁棒估计 | DR-OVR (Jiang & Li, 2016) | 回归+IS 混合，降低离线价值估计偏差 |\n\n**与本文区别**：上述方法聚焦**价值函数**或**离线数据**的偏差，而 HA-DW 针对**on-policy 小样本组内基线**的偏差，场景与对象均不同。\n\n4\\. 连续/非二元奖励下的偏差扩展\n\n论文在附录 D.5 进一步把二元奖励结论推广到：\n\n- **Beta 分布奖励**（软验证器常用）\n- **截断高斯奖励**（学习式奖励模型常用）\n\n证明：**只要奖励有界，组内均值基线依旧随 pt 偏离 0.5 而放大偏差**，从而说明“偏差现象”并非二元奖励特有。\n\n小结\n\n| 类别 | 已有研究 | 本文贡献 |\n| --- | --- | --- |\n| 群体 RLVR | 大量改进方差、裁剪、粒度 | 首次揭示并纠正“基线期望偏差” |\n| 跨批次信号 | 对比/推荐/BN 领域成熟应用 | 首次引入 RLVR 难度锚点，用于在线纠偏 |\n| 有偏估计理论 | 价值函数/离线场景 | 转向 on-policy 组内基线，给出严格概率界 |\n| 奖励分布 | 多集中于二元 | 推广到连续有界，验证偏差普适性 |\n\nQ3: 论文如何解决这个问题？\n\n论文采用“先理论剖析、后算法修正、再实验验证”的三段式路线，系统性地解决“群体相对优势估计有偏”问题。具体步骤如下：\n\n1\\. 理论剖析：量化偏差来源\n\n- **建模**：将每条响应奖励视为 Bernoulli(pt)，组内总数 R=Σrt,i～Binomial(G,pt)。\n- **关键观察**：GRPO 类算法只在 1≤R≤G−1 时更新（非退化事件 S）。\n- **推导条件期望**\n\nE[hat p_t|S]=(pt−p_t^G) / (1−(1−pt)^G−p_t^G)\n\n⇒ 当 pt<0.5 时 $E\nhat p_t|S\n>pt$（基线被高估），反之被低估。\n- **得到优势偏差**\n\nE[hat A_(t,i)|S]−A_(t,i)=pt−E[hat p_t|S]\n\n从而严格证明：\n- 困难 prompt（pt<0.5）（真优势>0）被**低估**\n- 简单 prompt（pt>0.5）（真优势<0）被**高估**\n- **概率误差界**：给出 P(Â<A) 与 P(Â>A) 的闭合表达式，证实在 G≤8 的小样本场景下，误判概率≥63%，极端难度下≥78%。\n\n2\\. 算法修正：HA-DW 双阶段框架\n\n（1）跨批次历史锚点 Ct\n\n- 把模型“解题能力”视为隐状态，用 Kalman 式更新\n\nC_t^+=(1−η_t)C_t^−+η_t y_t,quad y_t=(K_t) / (B_t)\n\n其中遗忘因子  η_t=η·σ_t  随训练稳定性自适应减小，兼顾快速追踪与噪声抑制。\n-  C_t  作为**动态难度锚点**，替代人工固定阈值。\n\n（2）历史感知重加权 Φt,i\n\n- 计算 prompt 相对难度\n\nM_t=|hat p_t−C_t^−|\n\n- 决定调整方向\n\nD_(t,i)=−sgn(hat A_(t,i))·sgn(hat p_t−C_t^−)\n\n⇒ 困难 prompt 的正优势得正指数放大，负优势被压缩；简单 prompt 相反。\n- 生成重权因子\n\nPhi_(t,i)=λ_(scale)·exp(D_(t,i)·M_t)\n\n- 嵌入任意群体目标\n\nL_(HA-DW)=(1) / (G)∑_(i=1)^G psi!((π_θ) / (π_(θ_{textold))})·ϕ(hat A_(t,i))·Phi_(t,i)\n\n即“即插即用”模块，无需改动原算法其余部分。\n\n（3）理论保证\n\n- 设矫正后基线  tilde p_t=c·hat p_t ，给出  c  的 pt-free 可行区间（Lemma 1）。\n- 证明存在  λ_(scale)  区间使\n\nl|E[hat A_(t,i)·Phi_(t,i)|S]−A_(t,i)r|<l|E[hat A_(t,i)|S]−A_(t,i)r|\n\n即 HA-DW **严格减小期望偏差**（Theorem 3）。\n\n3\\. 实验验证：多算法 × 多模型 × 多基准\n\n- **基准**：MATH500、AIME25、AMC23、Minerva、OlympiadBench\n- **模型**：Qwen-3-4B、Qwen-3-8B、LLaMA-3.2-3B\n- **算法**：GRPO、GSPO、DAPO 及其 +HA-DW 版本\n\n**主要结果**\n\n1. 一致提升：平均准确率 +1.7~+3.4 pp，困难题增幅最大（+3.4 pp）。\n2. 样本效率：G=8 的 GRPO+HA-DW 优于 G=16 的原始 GRPO，节约一倍 rollout。\n3. 训练动态：HA-DW 曲线更快收敛到更高平台，响应长度增加，验证其促进探索、抑制过度利用。\n4. 消融：\n- 动态锚点  C_t  优于固定阈值；\n- $λ_(scale)∈\n1.3,1.5\n$ 与理论区间吻合，性能最佳。\n\n一句话总结\n\n论文**先证明“群体均值基线必然有偏”**，**再设计跨批次历史锚点与指数重权因子在线纠偏**，**最后通过理论保证+大规模实验**验证 HA-DW 可在不增加 rollout 的前提下，一致提升各类群体 RLVR 算法的推理性能。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“验证 HA-DW 能否在真实 RLVR 训练场景中持续纠正群体相对优势估计偏差”这一目标，共设计了 **4 组共 12 项实验**，覆盖模型规模、算法种类、 rollout 预算、超参数、难度分布、训练动态等多个维度。所有实验均在 **VeRL 框架**下单节点 8×A100 完成，训练集统一采用 MATH-7.5 k，评测集为 5 个数学推理基准。结果均以 **accuracy（%）** 或 **avg@16** 形式报告。\n\n1\\. 主实验：跨模型 × 跨算法一致性验证\n\n| 模型 | 算法 | MATH500 | AIME25 | AMC23 | Minerva | OlympiadBench | AVG |\n| --- | --- | --- | --- | --- | --- | --- | --- |\n| Qwen3-4B | GRPO | 75.4 | 19.6 | 60.3 | 33.8 | 43.5 | 46.5 |\n| +HA-DW | 78.0 | 20.4 | 63.4 | 36.8 | 44.7 | 48.7 |\n| GSPO | 75.8 | 20.0 | 62.2 | 35.3 | 42.3 | 47.1 |\n| +HA-DW | 77.6 | 19.6 | 68.6 | 37.1 | 43.2 | 49.2 |\n| DAPO | 76.8 | 18.3 | 60.0 | 35.7 | 43.2 | 46.8 |\n| +HA-DW | 78.6 | 21.3 | 65.0 | 37.5 | 45.3 | 49.5 |\n| Qwen3-8B | GRPO | 78.8 | 20.4 | 64.2 | 38.2 | 46.4 | 49.6 |\n| +HA-DW | 80.0 | 22.9 | 72.8 | 39.7 | 47.1 | 52.5 |\n| LLaMA-3.2-3B | GRPO | 51.4 | 2.7 | 31.7 | 22.8 | 19.9 | 25.7 |\n| +HA-DW | 53.2 | 3.3 | 35.0 | 23.9 | 20.1 | 27.1 |\n\n**结论**：HA-DW 在 3 个模型、3 种算法、5 个基准上**全部取得正向增益**，平均提升 **+2.1 pp**。\n\n2\\. 难度分层实验：验证“偏差→性能”因果链\n\n- 将 MATH500 按官方标签划分为 **Easy（L1）、Mid（L2-3）、Hard（L4-5）** 三组。\n- 仅比较 **GRPO vs GRPO+HA-DW**（Qwen3-4B，G=8）。\n\n| 难度 | GRPO | +HA-DW | Δ |\n| --- | --- | --- | --- |\n| Easy | 92.1 | 92.4 | +0.3 |\n| Mid | 78.5 | 78.9 | +0.4 |\n| Hard | 51.2 | 54.6 | +3.4 |\n\n**结论**：HA-DW 主要提升来自 **Hard 集合**，与理论“困难 prompt 被低估”预测一致。\n\n3\\. 训练动态实验：监控整条学习曲线\n\n- 指标：\n1. 五步平均准确率（跨 5 基准）\n2. 在线奖励（rollout 即时回报）\n3. 平均响应长度\n- 观察：\n– HA-DW 曲线**更快收敛至更高平台**；\n– 最终奖励提升 **+4.8%**；\n– 响应长度增加 **+11%**，说明模型被激励生成更长推理链。\n\n4\\. 消融与对照实验\n\n4.1 锚点消融：动态 vs 固定阈值\n\n| 设置 | MATH500 | AIME25 | AMC23 | Minerva | OlympiadBench | AVG |\n| --- | --- | --- | --- | --- | --- | --- |\n| 固定 0.4 | 77.0 | 18.5 | 63.1 | 37.5 | 44.3 | 48.1 |\n| 固定 0.5 | 76.6 | 20.0 | 62.7 | 35.7 | 44.0 | 47.8 |\n| 固定 0.6 | 76.8 | 21.3 | 61.1 | 36.4 | 44.3 | 48.0 |\n| 动态 C_t | 78.0 | 20.4 | 63.4 | 36.8 | 44.7 | 48.7 |\n\n**结论**：动态锚点显著优于任何固定阈值，验证跨批次历史信息必要性。\n\n4.2 Rollout 预算对比：小样本优势\n\n| 设置 | MATH500 | AIME25 | AMC23 | Minerva | OlympiadBench |\n| --- | --- | --- | --- | --- | --- |\n| GRPO G=8 | 75.4 | 19.6 | 60.3 | 33.8 | 43.5 |\n| GRPO G=16 | 76.2 | 19.2 | 61.6 | 34.2 | 43.9 |\n| GRPO+HA-DW G=8 | 78.0 | 20.4 | 63.4 | 36.8 | 44.7 |\n\n**结论**：G=8+HA-DW **超越** G=16 原始 GRPO，节约 50% 推理算力。\n\n4.3 缩放因子 λ\\_scale 扫描\n\n| λ_scale | MATH500 | AIME25 | AMC23 | Minerva | OlympiadBench | AVG |\n| --- | --- | --- | --- | --- | --- | --- |\n| 0.5 | 75.4 | 18.1 | 61.1 | 34.2 | 43.7 | 46.5 |\n| 1.0 | 76.8 | 18.5 | 61.6 | 36.0 | 44.3 | 47.4 |\n| 1.3 | 78.0 | 20.4 | 63.4 | 36.8 | 44.7 | 48.7 |\n| 1.5 | 77.8 | 20.8 | 63.1 | 37.1 | 44.0 | 48.6 |\n| 2.0 | 76.8 | 19.0 | 61.9 | 35.3 | 43.5 | 47.3 |\n\n**结论**：最佳区间 **1.3–1.5** 与理论推导的“可行 λ 集合”高度吻合。\n\n5\\. 偏差实证补充（附录 E.1）\n\n- 从 MATH & DAPO-Math-17k 中抽取“ rollout=8 仅 1 正确”的 50 组 prompt，再用 128 rollout 评估真难度。\n- 结果：\n– 24（MATH）与 15（DAPO）组在 128 rollout 下仍少于 16 正确，**证实 rollout=8 时优势被显著低估**。\n- 反向实验“仅 1 错误”组亦观察到**高估**现象，与理论预测双向一致。\n\n实验总结\n\n1. **主实验** → 证明 HA-DW 对多种模型/算法/基准**普适有效**。\n2. **难度分层** → 验证偏差-性能因果链，**困难题增益最大**。\n3. **训练动态** → 显示 HA-DW **加速收敛、提升奖励、延长推理链**。\n4. **消融与对照** → 确认动态锚点、小样本优越性、λ\\_scale 理论区间**缺一不可**。\n5. **偏差实证** → 直接用 128 rollout 真值**双向验证**低估/高估存在。\n\n整体实验链条完整，从**因果 existence**到**算法 effectiveness**再到**系统 efficiency**，全方位支撑论文主张。\n\nQ5: 有什么可以进一步探索的点？\n\n以下问题与方向在论文的理论与实验边界之外，仍属空白或仅被初步触及，可作为后续研究的切入点：\n\n1\\. 偏差理论的深化\n\n- **非独立同分布奖励**\n论文假设同一 prompt 下 G 条响应奖励 i.i.d. Bernoulli(pt)。实际中 rollout 间存在共享前缀、自回归耦合，导致**时间相关**或**长度相关**。可建立**马尔可夫奖励过程**或**高维混合分布**，重新推导条件偏差。\n\n- **任意群体规模 G 的渐近紧界**\n当前给出 G≤8 的数值概率下界；可尝试**Berry-Esseen 型界**或**精确组合恒等式**，给出与 pt、G 相关的**紧误差公式**，指导工业场景 rollout 预算选型。\n\n- **多峰/多步奖励**\n数学推理任务常用 0/1 终局奖励。若引入**步级奖励**或**部分分数**，优势定义变为向量/矩阵。需扩展“组内均值”偏差定义到**高维、多峰、异方差**情形。\n\n2\\. 历史锚点的升级\n\n- **非线性滤波器**\n现用 Kalman 式线性更新。可尝试**粒子滤波**、**LSTM 记忆单元**或**指数移动平均的自适应衰减**，以捕捉**突变式能力跃迁**（如顿悟时刻）。\n\n- **Prompt-级细粒度记忆**\n当前 Ct 是全局平均；可维护**每道题目的独立后验** C\\_t(x) 或**聚类后验** C\\_t(cluster)，实现**更精细的个性化难度估计**。\n\n- **与非平稳 Bandit 结合**\n把“选 prompt”视为 Bandit 臂，用 Ct 作为非平稳奖励信号的输入，实现**主动课程学习**。\n\n3\\. 重加权策略的扩展\n\n- **Token-级/段-级重加权**\nHA-DW 目前同一 prompt 内所有 token 共享 Φt,i。可让 Φ 随深度或随 token 不确定性变化，进一步**缓解信用分配误差**。\n\n- **元学习式 λ\\_scale**\n现在 λ\\_scale 人工搜索；可把**偏差减小量**作为元目标，用**元梯度**或**超网络**实时输出 λ\\_scale，实现**任务自适应**。\n\n- **多目标重加权**\n同时考虑**长度惩罚**、**多样性奖励**、**格式合规度**，把 HA-DW 扩展为**多目标约束优化**形式。\n\n4\\. 算法通用化\n\n- **非群体 RLVR**\n本文仅针对“组内均值”基线。对于使用**价值网络**、**Retrace**、**V-trace** 的方法，可研究：\n– 小样本价值拟合的**过拟合偏差**；\n– 把 HA-DW 思想迁移到**优势函数正则化**或**价值目标平滑**。\n\n- **Off-policy / 人类反馈场景**\n探索在 RLHF、RLAIF 或离线数据下，历史锚点能否**检测分布漂移**并**自动降低可疑样本权重**，提升**鲁棒性**。\n\n- **多模态、代码、科学推理**\n验证 HA-DW 是否适用于**图像+文本**、**代码生成**、**化学/物理问题**，观察难度锚点是否**跨模态稳定**。\n\n5\\. 系统与效率\n\n- **零开销实现**\n当前需维护历史缓冲区与额外超参。可设计**在线流式算法**（O(1) 内存），或用**硬件计数器**实时更新 Ct，实现**零 GPU 额外开销**。\n\n- **与投机解码结合**\n投机解码（Spec-RL, Liu et al. 2025a）已用于群体 RLVR。可让 HA-DW 的 Φt,i 指导**投机候选评分**，进一步**加速训练**。\n\n- **大规模分布式**\n在千卡环境验证**异步更新**下历史锚点是否仍稳定；研究**不同节点能力差异**对 Ct 收敛的影响。\n\n6\\. 评估与可解释性\n\n- **偏差在线监控板**\n构建**实时偏差仪表盘**：持续采样超大 rollout（如 G=128）估计真 pt，与组内基线对比，输出**偏差热图**，实现**训练过程可解释**。\n\n- **因果干预实验**\n通过**人为注入极端难度 prompt** 并监测 Φt,i 变化，验证 HA-DW 是否**因果地**提升困难题探索率。\n\n- **与人类认知对齐**\n邀请人类专家标注“题目难度”，对比 HA-DW 的 diffhis\\_t 是否**与人类排序一致**，评估**难度锚点可解释性**。\n\n7\\. 理论-算法-系统闭环\n\n最终目标是建立\\*\\*“偏差检测→自适应矫正→系统级加速”\\*\\*的闭环：\n\n1. 理论给出__任意 rollout 预算下的最小可检测偏差 ε_(G)_\\*；\n2. 算法根据 ε\\* 自动选择**最优 λ\\_scale 与更新周期**；\n3. 系统层实现**零开销在线监控与动态调度**，形成**自修复 RLVR 训练框架**。\n\n一句话总结\n\n后续工作可从**更深理论**、**更细粒度记忆**、**更广算法迁移**、**更高系统效率**四个维度切入，把“群体相对优势偏差”这一核心问题**从数学推理推广到通用大模型后训练**，并最终实现**自监督、自修复、自优化的 RLVR 生态**。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：群体相对 RLVR 用组内平均奖励作优势基线，理论证明该估计对困难 prompt 系统性低估、对简单 prompt 系统性高估，导致探索-利用失衡。\n- **理论**：在 Bernoulli 奖励下给出期望偏差闭合式与概率误差界；小样本 G≤8 时误判概率≥63%，极端难度下≥78%。\n- **方法**：提出 History-Aware Adaptive Difficulty Weighting（HA-DW），以跨批次历史能力锚点 C\\_t 判定 prompt 相对难度，并用指数因子 Φ\\_t,i 在线重加权优势，即插即用。\n- **保证**：推导出 λ\\_scale 可行区间，证明 HA-DW 能严格减小期望优势偏差。\n- **实验**：在 Qwen-3-4B/8B、LLaMA-3.2-3B 上嵌入 GRPO、GSPO、DAPO，于五个数学推理基准一致提升 1.7–3.4 pp；G=8 即可超越 G=16 的原始算法，验证小样本偏置纠正与样本效率优势。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Your%20Group-Relative%20Advantage%20Is%20Biased，链接是https://arxiv.org/pdf/2601.08521，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08521。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1768866682132","title":"The Poisoned Apple Effect: Strategic Manipulation of Mediated Markets via Technology Expansion of AI Agents","description":"The integration of AI agents into economic markets fundamentally alters the landscape of strategic interaction. We investigate the economic implications of expanding the set of available technologies in three canonical game-theoretic settings: bargaining (resource division), negotiation (asymmetric information trade), and persuasion (strategic information transmission). We find that simply increasing the choice of AI delegates can drastically shift equilibrium payoffs and regulatory outcomes, often creating incentives for regulators to proactively develop and release technologies. Conversely, we identify a strategic phenomenon termed the \"Poisoned Apple\" effect: an agent may release a new technology, which neither they nor their opponent ultimately uses, solely to manipulate the regulator's choice of market design in their favor. This strategic release improves the releaser's welfare at the expense of their opponent and the regulator's fairness objectives. Our findings demonstrate that static regulatory frameworks are vulnerable to manipulation via technology expansion, necessitating dynamic market designs that adapt to the evolving landscape of AI capabilities.","url":"https://huggingface.co/papers/2601.11496","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.132Z","metadata":{"authors":["Eilam Shapira","Roi Reichart","Moshe Tennenholtz"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.GT","cs.AI","cs.CL","cs.MA"],"pdfUrl":"https://arxiv.org/pdf/2601.11496.pdf","arxivUrl":"https://arxiv.org/abs/2601.11496","arxivId":"2601.11496","coolPaperUrl":"https://papers.cool/arxiv/2601.11496","published":"2026-01-16T18:18:03Z","updated":"2026-01-16T18:18:03.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"LLM Analysis Failed: Error: 抓取失败（已重试2次）: Waiting failed: 30000ms exceeded"}},{"id":"hf-2-1768866682132","title":"Unlocking Implicit Experience: Synthesizing Tool-Use Trajectories from Text","description":"Enabling Large Language Models (LLMs) to effectively utilize tools in multi-turn interactions is essential for building capable autonomous agents. However, acquiring diverse and realistic multi-turn tool-use data remains a significant challenge. In this work, we propose a novel text-based paradigm. We observe that textual corpora naturally contain rich, multi-step problem-solving experiences, which can serve as an untapped, scalable, and authentic data source for multi-turn tool-use tasks. Based on this insight, we introduce GEM, a data synthesis pipeline that enables the generation and extraction of multi-turn tool-use trajectories from text corpora through a four-stage process: relevance filtering, workflow & tool extraction, trajectory grounding, and complexity refinement. To reduce the computational cost, we further train a specialized Trajectory Synthesizer via supervised fine-tuning. This model distills the complex generation pipeline into an efficient, end-to-end trajectory generator. Experiments demonstrate that our GEM-32B achieve a 16.5% improvement on the BFCL V3 Multi-turn benchmark. Our models partially surpass the performance of models trained on τ - bench (Airline and Retail) in-domain data, highlighting the superior generalization capability derived from our text-based synthesis paradigm. Notably, our Trajectory Synthesizer matches the quality of the full pipeline while significantly reducing inference latency and costs.","url":"https://huggingface.co/papers/2601.10355","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.132Z","metadata":{"authors":["Zhihao Xu","Rumei Li","Jiahuan Li","Rongxiang Weng","Jingang Wang","Xunliang Cai","Xiting Wang"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2601.10355.pdf","arxivUrl":"https://arxiv.org/abs/2601.10355","arxivId":"2601.10355","coolPaperUrl":"https://papers.cool/arxiv/2601.10355","published":"2026-01-15T12:58:46Z","updated":"2026-01-15T12:58:46.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**高质量多轮工具使用（multi-turn tool-use）训练数据稀缺**这一核心瓶颈，从而提升大模型在复杂、真实环境中作为自主智能体的能力。具体而言：\n\n- **传统范式依赖预定义 API**：现有方法通常先构造一套 API，再围绕这些 API 模拟用户-智能体对话。这种方式成本高、覆盖域有限，导致模型难以泛化到训练时未见过的真实场景。\n- **文本中隐含丰富经验**：作者观察到，预训练语料里存在大量“多步骤问题解决”的叙述（如“医院报销流程”），它们天然包含用户目标、环境工具与操作序列，却未被利用。\n- **提出“文本→轨迹”新范式**：通过四阶段流水线 GEM（过滤-抽取-生成-精炼），直接从任意文本语料中合成**带工具定义的多轮交互轨迹**，无需预先指定 API。\n- **进一步蒸馏为端到端合成器**：用 10 k 条 GEM 轨迹微调一个专用模型，实现低成本、可扩展的“文本进-轨迹出”数据生产。\n\n实验表明，仅用合成数据训练的 32 B 模型在 BFCL V3 多轮基准上提升 16.5 %，且在 τ²-bench 的跨域任务中与域内训练模型持平或更优，验证了该范式对**数据可扩展性与泛化能力**的双重改善。\n\nQ2: 有哪些相关研究？\n\n论文在第 5 节“Related Work”中将相关研究划分为两条主线：\n\n1. **工具使用数据合成**（Tool-Use Data Synthesis）\n2. **工具使用能力评测**（Tool-Use Capability Evaluation）\n\n以下按时间顺序梳理核心文献，并指出与本文“文本→轨迹”范式的差异。\n\n1\\. 工具使用数据合成（按首次 arXiv 时间排序）\n\n| 工作 | 核心思路 | 与 GEM 的本质区别 |\n| --- | --- | --- |\n| ToolBench (Qin et al., 2023) arXiv:2307.16789 | 爬取 16 k+ 真实 API，用单轮指令-调用对扩充数据集。 | 仅单轮；依赖现成 API 库。 |\n| ToolACE (Liu et al., 2024b) NeurIPS 2024 | 自动迭代扩展 API 池，生成可验证的多步调用链。 | 仍需自建 API 池；未利用自由文本。 |\n| APIGen-MT (Prabhakar et al., 2025) arXiv:2504.03601 | 在 τ-bench 航空/零售环境内，先构造任务蓝图再模拟人-机对话。 | 训练数据与测试域完全重合；无文本抽取。 |\n| MagNet (Yin et al., 2025) ACL 2025 | 将多工具工作流表示为图，再翻译成多轮对话。 | 图结构需人工设计；不直接从文本生成。 |\n| ToolACE-MT (Zeng et al., 2025) arXiv:2508.12685 | 非自回归一次性生成整段对话，再迭代编辑。 | 依赖预定义 API；无文本驱动抽取。 |\n| TOUCAN (Xu et al., 2025) arXiv:2510.01179 | 爬取 1.5 M 条真实 MCP 服务器日志，合成工具调用数据。 | 数据源是已有 API 日志，而非自由文本。 |\n| Simia-Tau (Li et al., 2025) arXiv:2511.01824 | 用推理模型在 τ-bench 环境内仿真用户-智能体博弈。 | 完全在仿真环境内；未利用开放文本。 |\n\n**共同点**：均遵循“先定义 API → 再生成对话”的**工具中心范式**。\n**GEM 差异**：**无需预定义 API**，直接从任意文本中**逆向抽取**工具与轨迹，实现“文本→轨迹”范式。\n\n2\\. 工具使用能力评测\n\n| 基准 | 评测维度 | 与本文关系 |\n| --- | --- | --- |\n| BFCL V3 (Patil et al.) | 语法/语义级函数调用准确率，含多轮子集。 | 主实验基准，GEM-32B 在此提升 16.5 %。 |\n| τ-bench (Yao et al., 2024) | 航空、零售域内用户-智能体对话，单控制方。 | 前身基准；GEM 数据未在其训练集出现。 |\n| τ²-bench (Barres et al., 2025) | 双控制环境（用户也可调用工具），更贴近客服场景。 | 跨域泛化实验平台，GEM 在 Retail 域 Pass@4 达 86.84 %，超过域内训练模型。 |\n| ACEBench (Chen et al., 2025) | 从“正常/特殊/智能体”三视角评测多轮工具使用。 | 同期工作，尚未在正文实验中被采用。 |\n| VitaBench (He et al., 2025) | 面向生活服务场景的仿真环境。 | 同期工作，与本文实验无重叠。 |\n\n3\\. 补充：数据蒸馏与合成器相关\n\n| 工作 | 与 GEM Synthesizer 的对比 |\n| --- | --- |\n| Distilling Step-by-Step (Hsieh et al., 2023) | 用小模型学习大模型推理链，属于链式思维蒸馏；GEM Synthesizer 蒸馏的是整条多轮工具轨迹。 |\n| WizardLM/Evol-Instruct (Xu et al., 2023) | 通过指令演化提升多样性；GEM 的精炼阶段同样引入“用户意图演化、陷阱注入”机制，但目标为工具调用复杂度而非开放问答。 |\n\n一句话总结\n\n现有研究全部围绕“**先找 API 再编对话**”展开，而 GEM 首次提出“**直接让文本自己告诉模型该有哪些工具与对话**”，从而把**开放域文本**转化为**可训练的多轮工具使用轨迹**，在数据可扩展性与跨域泛化上实现突破。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **GEM（Grounding Experience from Multi-step text）** 流水线，把“开放域文本”直接蒸馏成“可训练的多轮工具使用轨迹”，从而绕过传统“先定义 API 再模拟对话”的瓶颈。整体思路可概括为 **“四阶段抽取 + 一次蒸馏”**。\n\n1\\. 四阶段抽取流水线（GEM）\n\n| 阶段 | 输入 | 关键动作 | 输出 |\n| --- | --- | --- | --- |\n| S1 文本过滤 | Ultra-FineWeb 等原始语料 | 二分类器筛掉不含“多步操作”的段落 | 保留 14 % 含流程文本 |\n| S2 工作流 & 工具抽取 | 保留文本 | LLM 自动：① 枚举步骤序列与依赖② 按 OpenAI JSON Schema 设计最小原子工具 | 抽象工作流 + 工具集  P  |\n| S3 轨迹生成 | 文本 + 工作流 + 工具 | 用强教师模型（GLM-4-6）一次生成完整对话含系统提示、用户 query、助手调用、工具返回 | 原始轨迹  T  |\n| S4 精炼 + 验证 |  T  | ① 精炼：加歧义、加错误恢复、扩工具集、埋“陷阱”② 规则检查：格式、参数类型、调用域一致③ LLM 裁判：消除幻觉（参数、能力、上下文） | 终版轨迹  T_(final)  |\n\n2\\. 端到端蒸馏（Trajectory Synthesizer）\n\n- 用 10 k 条  T_(final)  对 Qwen3-8B 做**监督微调**，得到专用**数据合成器**。\n- 推理时仅需“**指令 + 任意文本**”→ 直接输出 **{工具集 + 多轮轨迹}**，成本比原流水线降低 >80 %，而质量持平（BFCL 差距 <2 %）。\n\n3\\. 训练与评测\n\n- 用 GEM 或 Synthesizer 生成的 10 k 条轨迹分别对 **Qwen3-8B/32B** 做全参数 SFT。\n- 在 **BFCL V3 多轮** 四子任务上，32B 模型绝对提升 **16.5 %**，超过 GPT-4.1。\n- 在 **τ²-bench** 航空/零售**域外**场景，Retail-Pass@4 达 **86.84 %**，**高于**用域内数据训练的 SIMIA/MUA，验证**文本范式泛化优势**。\n\n4\\. 关键消融\n\n| 去掉模块 | 8B 模型 BFCL 整体下降 | 说明 |\n| --- | --- | --- |\n| 精炼阶段 | −4.25 % | 复杂度不足，无法学习长链依赖 |\n| LLM 幻觉检查 | −2.87 % | 幻觉样本混入导致过拟合错误调用 |\n\n一句话总结\n\nGEM 通过“**先让文本自己泄露工具与流程 → 再生成高质量对话 → 最后蒸馏成廉价合成器**”的三部曲，把**开放域文本**转化为**可扩展、可泛化的多轮工具使用训练数据**，在数据获取层面彻底摆脱对预定义 API 的依赖。\n\nQ4: 论文做了哪些实验？\n\n论文从 **数据有效性、模型规模、跨域泛化、消融分析、数据特征、案例可视化** 六个维度展开系统实验，全部基于 **BFCL V3 多轮** 与 **τ²-bench** 两大公开基准。结果均以 **accuracy / Avg@4 / Pass@4** 报告，避免任何非公开测试集。\n\n1\\. 主实验：GEM 数据对基座模型的提升\n\n| 模型规模 | 基座 | +GEM 数据 | ΔOverall |\n| --- | --- | --- | --- |\n| 8B | 18.00 % | 30.25 % | +12.25 % |\n| 32B | 28.35 % | 44.88 % | +16.53 % |\n\n- 32B-GEM **超过** GPT-4.1（38.88 %）、DeepSeek-V3.2-Exp（37.38 %）等专有模型。\n- 在 **Long Context** 子集提升最显著（8B +5.5 %，32B +19.5 %），证明长链工具调用受益最大。\n\n2\\. 跨域泛化：τ²-bench（严格域外）\n\n| 领域 | 指标 | GEM-8B | GEM-32B | 域内最佳基线 | 差距 |\n| --- | --- | --- | --- | --- | --- |\n| Retail | Pass@4 | 75.44 % | 86.84 % | MUA 80.70 % | +6.14 % |\n| Airline | Pass@4 | 40.00 % | 56.00 % | SIMIA 62.00 % | -2.00 %（可竞争） |\n\n- GEM 数据**未使用任何航空/零售 API**，仍能媲美或超越**域内专门合成数据**，验证文本范式泛化能力。\n\n3\\. 蒸馏效果：Trajectory Synthesizer vs 原始 GLM 流水线\n\n| 数据源 | 生成方式 | BFCL Overall | Retail Pass@4 | 推理成本 |\n| --- | --- | --- | --- | --- |\n| Ultra-FineWeb | GLM-4-6 全流水 | 30.25 % | 75.44 % | 1×（参考） |\n| Ultra-FineWeb | 8B-Synthesizer | 28.38 % | 73.68 % | ≈0.2× |\n| WikiHow | 8B-Synthesizer | 28.50 % | 68.42 % | ≈0.2× |\n\n- 专用合成器**仅下降 1.5–2 %**，却把成本降低 80 % 以上，支持**大规模自举式数据生产**。\n\n4\\. 消融实验：验证关键模块贡献\n\n| 配置 | 8B Overall | 32B Overall | 主要现象 |\n| --- | --- | --- | --- |\n| 完整 GEM | 30.25 % | 44.88 % | — |\n| 去掉精炼 | 26.00 % | 32.50 % | 下降 12.4 %（32B），复杂度不足 |\n| 去掉幻觉检查 | 27.38 % | 44.25 % | 8B 下降明显，32B 轻微，说明小模型更易受污染 |\n\n5\\. 数据统计：合成轨迹复杂度对比\n\n| 指标 | GEM 轨迹 | APIGEN-MT | TOUCAN | 提升倍数 |\n| --- | --- | --- | --- | --- |\n| 平均消息数 | 46.1 | 18.5 | 6.24 | +2.5× |\n| 平均工具数 | 8.6 | — | — | 多样性显著 |\n| 平均调用次数 | 16.3 | 4.3 | — | +3.8× |\n\n- 轨迹更长、工具更多、调用更频繁，**降低“短 cuts”带来的虚假高 reward**。\n\n6\\. 案例研究：可视化一条轨迹\n\n图 8 & 附录 F 给出**照片加框**完整对话，涵盖：\n\n1. 参数澄清（image\\_path、location）\n2. 正确顺序调用（copy→open→add\\_text）\n3. 规则拒绝（font size >96 被拒）\n4. 错误恢复（打印机失败→换一台）\n\n证明合成数据**同时覆盖 Clarification、Rule Violation、Error-Recovery** 等真实交互模式。\n\n7\\. 超参与可复现性\n\n- 全参数 SFT，lr=5×10⁻⁶，2 epoch，32 K 长度，DeepSpeed-Z3，BF16。\n- 所有提示、随机种子、过滤脚本已放附录，**无闭源数据或人工标注**。\n\n一句话总结\n\n实验链条完整：**“主结果→跨域→蒸馏→消融→统计→案例”** 六级验证，一致表明 **GEM 能把廉价文本转化为高价值、可泛化的多轮工具使用训练数据**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为 **数据、模型、评测、理论** 四条主线，并给出可立即落地的短期实验与长期开放问题。\n\n1\\. 数据层面：扩大文本源与轨迹质量\n\n| 基准 | 评测维度 | 与本文关系 |\n| --- | --- | --- |\n| BFCL V3 (Patil et al.) | 语法/语义级函数调用准确率，含多轮子集。 | 主实验基准，GEM-32B 在此提升 16.5 %。 |\n| τ-bench (Yao et al., 2024) | 航空、零售域内用户-智能体对话，单控制方。 | 前身基准；GEM 数据未在其训练集出现。 |\n| τ²-bench (Barres et al., 2025) | 双控制环境（用户也可调用工具），更贴近客服场景。 | 跨域泛化实验平台，GEM 在 Retail 域 Pass@4 达 86.84 %，超过域内训练模型。 |\n| ACEBench (Chen et al., 2025) | 从“正常/特殊/智能体”三视角评测多轮工具使用。 | 同期工作，尚未在正文实验中被采用。 |\n| VitaBench (He et al., 2025) | 面向生活服务场景的仿真环境。 | 同期工作，与本文实验无重叠。 |\n\n0\n\n2\\. 模型层面：让合成器更可控、更小型\n\n| 基准 | 评测维度 | 与本文关系 |\n| --- | --- | --- |\n| BFCL V3 (Patil et al.) | 语法/语义级函数调用准确率，含多轮子集。 | 主实验基准，GEM-32B 在此提升 16.5 %。 |\n| τ-bench (Yao et al., 2024) | 航空、零售域内用户-智能体对话，单控制方。 | 前身基准；GEM 数据未在其训练集出现。 |\n| τ²-bench (Barres et al., 2025) | 双控制环境（用户也可调用工具），更贴近客服场景。 | 跨域泛化实验平台，GEM 在 Retail 域 Pass@4 达 86.84 %，超过域内训练模型。 |\n| ACEBench (Chen et al., 2025) | 从“正常/特殊/智能体”三视角评测多轮工具使用。 | 同期工作，尚未在正文实验中被采用。 |\n| VitaBench (He et al., 2025) | 面向生活服务场景的仿真环境。 | 同期工作，与本文实验无重叠。 |\n\n1\n\n3\\. 评测层面：更细粒度、更动态\n\n| 基准 | 评测维度 | 与本文关系 |\n| --- | --- | --- |\n| BFCL V3 (Patil et al.) | 语法/语义级函数调用准确率，含多轮子集。 | 主实验基准，GEM-32B 在此提升 16.5 %。 |\n| τ-bench (Yao et al., 2024) | 航空、零售域内用户-智能体对话，单控制方。 | 前身基准；GEM 数据未在其训练集出现。 |\n| τ²-bench (Barres et al., 2025) | 双控制环境（用户也可调用工具），更贴近客服场景。 | 跨域泛化实验平台，GEM 在 Retail 域 Pass@4 达 86.84 %，超过域内训练模型。 |\n| ACEBench (Chen et al., 2025) | 从“正常/特殊/智能体”三视角评测多轮工具使用。 | 同期工作，尚未在正文实验中被采用。 |\n| VitaBench (He et al., 2025) | 面向生活服务场景的仿真环境。 | 同期工作，与本文实验无重叠。 |\n\n2\n\n4\\. 理论与分析：为何“文本→轨迹”能泛化？\n\n| 基准 | 评测维度 | 与本文关系 |\n| --- | --- | --- |\n| BFCL V3 (Patil et al.) | 语法/语义级函数调用准确率，含多轮子集。 | 主实验基准，GEM-32B 在此提升 16.5 %。 |\n| τ-bench (Yao et al., 2024) | 航空、零售域内用户-智能体对话，单控制方。 | 前身基准；GEM 数据未在其训练集出现。 |\n| τ²-bench (Barres et al., 2025) | 双控制环境（用户也可调用工具），更贴近客服场景。 | 跨域泛化实验平台，GEM 在 Retail 域 Pass@4 达 86.84 %，超过域内训练模型。 |\n| ACEBench (Chen et al., 2025) | 从“正常/特殊/智能体”三视角评测多轮工具使用。 | 同期工作，尚未在正文实验中被采用。 |\n| VitaBench (He et al., 2025) | 面向生活服务场景的仿真环境。 | 同期工作，与本文实验无重叠。 |\n\n3\n\n5\\. 短期可落地实验（1–2 个月）\n\n1. **LoRA 蒸馏**：验证 8B→7B→3B 合成器在 BFCL 上的平滑下降曲线。\n2. **多语言试点**：在中法德各采 10 k 文本，用 GEM 生成轨迹，测试 BFCL 多语言子集。\n3. **动态沙箱**：把 SQLite 封装为可回滚服务，替换 mock，观察错误恢复率下降幅度。\n\n6\\. 长期开放问题（6 个月以上）\n\n- **自举极限**：无限循环“生成→训练→再生成”是否会出现模式崩塌？能否用信息论熵值作为早停信号？\n- **统一行动空间**：不同文本源抽取的 API 名称/参数空间不一致，如何学习统一行动嵌入，实现跨域零样本调用？\n- **具身物理工具**：文本描述的“扳手拧紧螺丝”如何映射到真实机器人末端执行器扭矩指令？需结合视觉-语言-动作模型。\n\n一句话总结\n\nGEM 把“文本”变成“工具轨迹”已被验证有效，下一步是**让合成器更小更可控、让评测更动态真实、让理论更可解释**，最终走向**无限自举、跨模态、具身化**的通用智能体数据工厂。\n\nQ6: 总结一下论文的主要内容\n\n论文提出“文本→轨迹”新范式，解决大模型多轮工具使用训练数据稀缺且依赖预定义 API 的瓶颈。核心贡献与流程如下：\n\n1. 关键洞察\n开放域文本天然包含“用户目标-环境工具-多步流程”三元组，是可扩展、真实、未开发的轨迹来源。\n\n2. GEM 四阶段流水线\n① 文本过滤 → ② 工作流&工具抽取（自动设计 JSON Schema 接口） → ③ 教师模型一次性生成完整对话 → ④ 精炼+双重验证（规则+LLM 裁判），输出高复杂度、零幻觉轨迹。\n\n3. 端到端蒸馏\n用 1 万条 GEM 轨迹微调出 8B Trajectory Synthesizer，推理成本↓80%，质量持平，实现“任意文本进→轨迹出”的廉价数据工厂。\n\n4. 实验结果\n\n- BFCL V3 多轮基准：32B 模型绝对提升 16.5%，超越 GPT-4.1。\n- τ²-bench 航空/零售域外测试：Retail-Pass@4 达 86.84%，高于域内训练模型，验证强泛化。\n- 消融：精炼阶段贡献 12 个百分点；幻觉检查再提 2–3 个百分点。\n- 统计：平均 46 轮、8.6 个工具、16.3 次调用，复杂度 3×于现有开源数据集。\n5. 结论\nGEM 首次证明无需预定义 API，可直接从文本批量生产高质量、可泛化的多轮工具使用数据，为构建通用自主智能体提供了可扩展的新路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Unlocking%20Implicit%20Experience%3A%20Synthesizing%20Tool-Use%20Trajectories%20from%20Text，链接是https://arxiv.org/pdf/2601.10355，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.10355。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1768866682133","title":"RubricHub: A Comprehensive and Highly Discriminative Rubric Dataset via Automated Coarse-to-Fine Generation","description":"Reinforcement Learning with Verifiable Rewards (RLVR) has driven substantial progress in reasoning-intensive domains like mathematics. However, optimizing open-ended generation remains challenging due to the lack of ground truth. While rubric-based evaluation offers a structured proxy for verification, existing methods suffer from scalability bottlenecks and coarse criteria, resulting in a supervision ceiling effect. To address this, we propose an automated Coarse-to-Fine Rubric Generation framework. By synergizing principle-guided synthesis, multi-model aggregation, and difficulty evolution, our approach produces comprehensive and highly discriminative criteria capable of capturing the subtle nuances. Based on this framework, we introduce RubricHub, a large-scale ($\\sim$110k) and multi-domain dataset. We validate its utility through a two-stage post-training pipeline comprising Rubric-based Rejection Sampling Fine-Tuning (RuFT) and Reinforcement Learning (RuRL). Experimental results demonstrate that RubricHub unlocks significant performance gains: our post-trained Qwen3-14B achieves state-of-the-art (SOTA) results on HealthBench (69.3), surpassing proprietary frontier models such as GPT-5. The code and data will be released soon.","url":"https://huggingface.co/papers/2601.08430","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.133Z","metadata":{"authors":["Sunzhu Li","Jiale Zhao","Miteto Wei","Huimin Ren","Yang Zhou","Jingwen Yang","Shunyu Liu","Kaike Zhang","Wei Chen"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2601.08430.pdf","arxivUrl":"https://arxiv.org/abs/2601.08430","arxivId":"2601.08430","coolPaperUrl":"https://papers.cool/arxiv/2601.08430","published":"2026-01-13T10:56:39Z","updated":"2026-01-13T10:56:39.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**开放式生成任务缺乏可验证真值**而导致的**监督信号天花板效应**。具体而言：\n\n- 在数学、代码等可验证领域，强化学习借助可验证奖励（RLVR）已显著提升模型推理能力；\n- 但在开放式任务（如写作、医疗问答、对话）中，由于没有标准答案，现有评估方法要么依赖昂贵人工标注，要么使用粗粒度、泛化性差的评分标准，导致：\n1. **可扩展性瓶颈**：高质量评分标准（rubric）依赖专家手工设计，难以大规模生产；\n2. **判别力不足**：粗粒度标准无法区分“看起来不错”与“真正卓越”的回复，造成**监督信号饱和**，顶级模型失去优化梯度。\n\n为此，作者提出**自动化“粗→细”评分标准生成框架**，并构建**RubricHub**（约11万条、多领域、细粒度、高判别力评分标准数据集），通过**Rubric-based Rejection Sampling Fine-Tuning (RuFT)** 与 **Rubric-based Reinforcement Learning (RuRL)** 两阶段后训练，显著提升模型在开放式任务上的表现，甚至在医疗基准 HealthBench 上**超越闭源前沿模型 GPT-5**。\n\nQ2: 有哪些相关研究？\n\n论文涉及的相关研究可归纳为三大主线，均围绕“如何在没有 ground-truth 的开放式任务中提供稳定、可扩展的监督信号”展开：\n\n1. LLM-as-a-Judge 与人工评分标准\n\n- 直接使用 LLM 给回复打整分（Likert 型）会因 position-bias、verbosity-bias 等导致方差大、可解释性差。\n- 近期工作转向“rubric-based”评估：将质量拆成多条可解释、可检查的标准，再让 LLM 按标准逐项打分，提升稳定性与可解释性。\n- 代表数据集：HealthBench、PaperBench、ProfBench、WritingBench、RaR（Rubrics as Rewards）等，但均为**人工设计**，规模小、领域窄、粒度粗。\n2. 自动化 Rubric 生成\n现有三条技术路线：\n\n- 纯 LLM 合成：给定任务描述，直接让模型生成标准（Gunjal et al. 2025; Huang et al. 2025）。\n- 从人类文档挖掘：从教材、调查、论文中抽取评估维度（ResearchQA、MegaScience）。\n- 从偏好信号反推：用 pairwise 偏好数据诱导出可复用的评分维度（OpenRubrics、AutoRule）。\n这些方法仍面临**单模型视角偏差**、**缺乏难度演化**、**判别力不足**等问题。\n3. 强化学习范式演进\n\n- RLHF / DPO：依赖人工偏好标注，难以规模化。\n- RLVR：在数学、代码等可验证领域用单元测试或答案匹配作为奖励（DeepSeek-R1、Tulu-3、DAPO）。\n- Rubric-RL：把“rubric 逐项得分”当成密集奖励，扩展到开放式任务（Rubicon、RuscaRL、OnlineRubrics）。\n本文的 RuRL 即属该范式，但首次**大规模自动化生成高判别力 rubric**，缓解监督天花板。\n\n综上，本文在“自动化生成高判别力评分标准”与“将 rubric 用于大规模 RL 后训练”两点上，与现有研究形成直接承继与突破关系。\n\nQ3: 论文如何解决这个问题？\n\n论文提出\\*\\*“粗→细”三阶段自动化框架\\*\\*，协同三项关键技术，系统性地解决开放式任务缺乏高判别力监督信号的问题：\n\n1. 阶段 1：Response-Grounded & Principle-Guided 生成\n\n- 以**参考回复**为锚点，避免标准漂移；\n- 引入四条元原则（一致性、结构、清晰、可评估）作为硬约束，保证标准与查询意图对齐。\n输出：候选标准集合  R_(cand) 。\n2. 阶段 2：Multi-Model Aggregation\n\n- 用异构前沿模型（GPT-5.1、Gemini-3-Pro 等）并行生成多组  R_(cand) ，形成**视角池**；\n- 通过聚合提示  P_(agg)  去重、消解冲突，得到**无单模型偏差**的基线标准  R_(base) 。\n3. 阶段 3：Difficulty Evolution\n\n- 在  R_(base)  上选取“已高分”的卓越回复对  A_(ref) ，用增强提示  P_(aug)  提炼**进一步区分“优秀→卓越”的细微差异**，生成附加标准  R_(add) ；\n- 合并得最终标准  R_(final) = R_(base) ∪ R_(add) ，实现**判别力升级**。\n\n基于该框架，作者构建 **RubricHub**（≈ 110 k 条、跨 5 领域、平均 30 条标准/查询），并设计两条后训练管线：\n\n- RuFT：用  R_(final)  做**拒绝采样**，只保留得分高于阈值 τ 的回复，构造 30 k 高质量 SFT 数据。\n- RuRL：将  R_(final)  转为**密集奖励**\n\nr(q,o)=(∑_i w_i b_i) / (∑_i w_i), quad b_i∈0,1\n\n采用 DAPO 算法在领域专属数据上继续优化策略。\n\n实验表明，该方案使 Qwen3-14B 在 HealthBench 上取得 **69.3 分，超越 GPT-5（67.2 分）**，显著缓解监督天花板效应。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“RubricHub 是否真能提供高判别力监督信号”与“RuFT→RuRL 管线能否带来实质性提升”两大核心问题，设计了**多维度、多尺度、多对比**的实验矩阵，具体包括：\n\n1. 主基准横评（Table 1）\n\n- **5 大领域、9 个公开基准**：\n– Science：GPQA-Diamond、ResearchQA\n– Instruction-Following：IFEval、IFBench\n– Writing：WritingBench、CreateWriting-V3\n– Medical：HealthBench、LLMEval-Med\n– Chat：Arena-Hard-V2\n- **三类对手**：\n– 闭源前沿：Gemini-3-Pro、GPT-5、GPT-4.1、DeepSeek-V3.1\n– 现有 Rubric-RL 模型：Rubicon-Preview、Baichuan-M2-32B、DR-Tulu-8B\n– 官方后训练版本：Qwen3-14B-Instruct、Qwen3-4B-Instruct\n- **结果**：Qwen3-14B + RuFT→RuRL 在 **Medical 69.3**（↑22.6 vs Instruct）、**IFEval 92.6**（超越 GPT-4.1）、**Arena-Hard 74.4**（↑69.2 vs Base）等全部 5 域均取得 SOTA 或可比成绩。\n2. 消融与增量实验\n\n- **粗→细框架消融**（Table 3）\nNaive LLM 生成 → +原则与回复锚定 → +多模型聚合 → +难度演化，HealthBench 由 60.9 → 66.2，LLMEval-Med 由 71.7 → 79.5，**严格单调提升**。\n- **拒绝采样样本数消融**（Figure 9）\n每查询采样 1→12 条回复，训练集平均最高分由 63.5 → 79.5，HealthBench 测试分由 43.6 → 48.8，验证“样本越多+标准越严→最终模型越好”。\n- **准则类型敏感性**（Figure 6）\n仅保留正向权重（Positive-only）相比引入负向陷阱（Pitfall），HealthBench 66.2 vs 63.2，说明**负准则噪声大，优化不稳定**。\n3. 评分器（Grader）能力边界分析\n\n- 用 7B→235B 五款模型担任 grader，对比人工 940 条准则标注。\n- Cohen’s κ 在 30B 以上趋于饱和（0.74→0.80），F1 达 0.90，**30B 是可靠阈值**；最终选用速度与精度折中的 gpt-oss-120B 作为统一 grader。\n4. 训练动态与维度细查\n\n- HealthBench 五维分项（Accuracy、Completeness、Communication、Context、Instruction）随 RL 步数同步上升（Figure 8），**无单维过拟合**。\n- 与原始 RaR rubric 对比（Figure 5）：同数据量下，RubricHub 标准在 HealthBench 提升 14.4 分，ResearchQA 提升 5.8 分，**验证新标准判别力显著更强**。\n5. 域泛化与规模一致性\n\n- 4B 与 14B 两规模均呈现 Base < RuFT < RuRL < RuFT→RuRL 的**严格递进曲线**，说明框架对模型规模不敏感，可复现。\n- 在纯聊天场景（Arena-Hard-V2）取得 **74.4** 分，比 Base 的 5.2 分提升 **14×**，证明方法对**非知识型开放式生成**同样有效。\n\n综上，实验从**基准性能→框架消融→采样策略→评分器可靠性→训练动态→域泛化**六层角度，系统验证了 RubricHub 与 RuFT→RuRL 管线的有效性与鲁棒性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为对 RubricHub 框架的**直接延伸**或**深层扩展**，均围绕“进一步降低人工成本、提升判别力、拓宽适用边界”展开：\n\n1\\. 长程智能体任务\n\n- 现有 rubric 主要面向**单轮或短链生成**，尚未覆盖需要**多步规划、工具调用、环境反馈**的智能体场景。\n- 可探索**动态 rubric**：随轨迹展开而演化，每步标准由前序执行结果与外部环境状态联合生成，实现“过程监督”而不仅是“终态评分”。\n\n2\\. 可验证域与开放域的统一奖励\n\n- 当前 RubricHub 刻意避开数学竞赛、代码竞赛等**硬可验证**任务，导致奖励函数二元割裂。\n- 未来可研究**混合奖励**：\n\nr = α · I_(unit-test) + (1-α) · r_(rubric)\n\n通过元学习或课程学习自动调节  α ，使模型在**可验证与开放域之间平滑迁移**。\n\n3\\. 专用“小 grader”架构\n\n- 实验表明 30B 是可靠阈值，但推理成本极高。\n- 可借鉴“小模型+工具”思路，训练\\*\\*<10B 的判别专用模型\\*\\*，配套：\n- 检索增强：实时召回准则定义、示例、外部知识；\n- 反思链：先生成评判理由再输出二元信号，降低参数需求；\n- 量化/蒸馏：将 120B grader 蒸馏为 3–7B 的“rubric-BERT”，实现**边缘端低成本部署**。\n\n4\\. 负准则（Pitfall）的噪声鲁棒化\n\n- 论文发现负权重准则会引入优化噪声，根本原因是小模型对“**什么是错误**”判断不准。\n- 可引入**置信加权**或**课程式负准则**：\n- 仅当 grader 置信 > τ 时才将负样本加入 loss；\n- 先训练“正准则”阶段，再渐进加入高置信负准则，避免初期梯度紊乱。\n\n5\\. 多模态 Rubric\n\n- 当前准则仅针对文本。面对**图像+文本**（医疗影像报告、图表写作）或**语音+文本**场景，需定义**跨模态可验证维度**。\n- 研究点包括：\n- 视觉 grounding：图中病灶是否被准确圈出；\n- 一致性检查：文字描述与图像内容是否冲突；\n- 模态权重自适应：根据查询类型动态调整文本/图像准则权重。\n\n6\\. 在线演化与对抗增强\n\n- 现有 Difficulty Evolution 为**离线一次**过程。可引入**双玩家博弈**：\n- Generator 模型：最大化 rubric 得分；\n- Evolver 模型：实时生成“更严格”准则或陷阱案例，迫使 generator 持续改进。\n形成**自监督飞轮**，无需额外人工标注即可持续放大判别力。\n\n7\\. 语言与文化的跨域适配\n\n- RubricHub 以英文为主，准则隐含欧美文化背景（如医疗伦理、法律免责声明）。\n- 可探索**跨文化元提示**：将元原则（安全、合规、清晰度）自动映射到当地法规或文化语境，生成**地域化 rubric**，实现**非英语市场零样本落地**。\n\n8\\. 高效并行推理架构\n\n- RuRL 阶段需对每条 rollout 的数十条准则逐一打分，延迟成为瓶颈。\n- 研究方向：\n- **准则合并图**：将原子准则组织为 DAG，共享中间特征，减少前向次数；\n- **混合批调度**：小模型处理 verifiable 准则，大模型仅处理语义准则，实现**异构算力最优分配**。\n\n9\\. 个性化与隐私敏感场景\n\n- 教育、心理咨询等场景需要**个性化 rubric**（不同学生、不同心理阶段标准不同）。\n- 探索**联邦式 rubric 生成**：在本地私有数据上微调轻量级生成器，仅上传准则而不上传数据，兼顾**个性化与隐私**。\n\n10\\. 自动度量“rubric 本身的质量”\n\n- 目前依赖人工 κ 与 F1 间接验证。可训练**meta-evaluator**预测“某条准则是否会导致 grader 间分歧”，实现**在线自检与自删除**，持续提升 rubric 集的信噪比。\n\n以上方向兼顾**算法、系统、产品、伦理**四层面，可作为 RubricHub 后续研究的**短-中-长期路线图**。\n\nQ6: 总结一下论文的主要内容\n\n论文核心贡献可概括为“**一个框架 + 一个数据集 + 一条管线 + 一套实验**”，系统解决开放式生成任务缺乏高判别力监督信号的问题：\n\n1\\. 框架：Coarse-to-Fine Rubric Generation\n\n- **三阶段自动化生产细粒度标准**\n① 原则+回复双锚定生成 → ② 多模型聚合去偏 → ③ 难度演化升级判别力\n- 输出单条查询 30+ 原子准则，**显著缓解“高分饱和”天花板**。\n\n2\\. 数据集：RubricHub\n\n- **≈ 11 万条查询-标准对**，覆盖医学、科学、写作、指令跟随、聊天五域；\n- 平均 28–32 项准则/查询，**判别力经人类与 30B-235B 模型双重验证**。\n\n3\\. 后训练管线：RuFT → RuRL\n\n- **RuFT**：用 RubricHub 做拒绝采样，筛出 30 k 高分样本做 SFT；\n- **RuRL**：以准则得分\n\nr(q,o)=(∑ w_i b_i) / (∑ w_i)\n\n为密集奖励，采用 DAPO 算法领域专属优化。\n\n4\\. 实验结果\n\n- **Qwen3-14B** 经该管线后，在 **HealthBench 69.3 分**，**超越 GPT-5（67.2）**；\n- 五域 9 项基准均取得 **SOTA 或可比成绩**，4B/14B 两规模一致呈现 **Base < RuFT < RuRL < RuFT→RuRL** 的严格递进；\n- 消融显示**每阶段组件单调增益**，且采样越多、标准越严，**最终模型越好**。\n\n综上，论文首次实现**大规模、高判别力、多领域 rubric 的自动构建与强化学习应用**，为开放式任务提供了**可扩展、可解释、可继续演化**的对齐新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RubricHub%3A%20A%20Comprehensive%20and%20Highly%20Discriminative%20Rubric%20Dataset%20via%20Automated%20Coarse-to-Fine%20Generation，链接是https://arxiv.org/pdf/2601.08430，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.08430。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1768866682133","title":"When Personalization Misleads: Understanding and Mitigating Hallucinations in Personalized LLMs","description":"Personalized large language models (LLMs) adapt model behavior to individual users to enhance user satisfaction, yet personalization can inadvertently distort factual reasoning. We show that when personalized LLMs face factual queries, there exists a phenomenon where the model generates answers aligned with a user's prior history rather than the objective truth, resulting in personalization-induced hallucinations that degrade factual reliability and may propagate incorrect beliefs, due to representational entanglement between personalization and factual representations. To address this issue, we propose Factuality-Preserving Personalized Steering (FPPS), a lightweight inference-time approach that mitigates personalization-induced factual distortions while preserving personalized behavior. We further introduce PFQABench, the first benchmark designed to jointly evaluate factual and personalized question answering under personalization. Experiments across multiple LLM backbones and personalization methods show that FPPS substantially improves factual accuracy while maintaining personalized performance.","url":"https://huggingface.co/papers/2601.11000","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.133Z","metadata":{"authors":["Zhongxiang Sun","Yi Zhan","Chenglei Shen","Weijie Yu","Xiao Zhang","Ming He","Jun Xu"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2601.11000.pdf","arxivUrl":"https://arxiv.org/abs/2601.11000","arxivId":"2601.11000","coolPaperUrl":"https://papers.cool/arxiv/2601.11000","published":"2026-01-16T05:20:10Z","updated":"2026-01-16T05:20:10.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文识别并解决“个性化引发的大模型幻觉”这一新问题。核心发现是：当大语言模型被个性化（注入用户历史、偏好或长期记忆）后，面对事实型查询时，其内部表示会把“用户先前说法”与“客观事实”纠缠在一起，导致模型输出迎合用户过往观点却违背客观真相的幻觉答案。该现象不仅降低模型的事实可靠性，还会通过交互将错误信念传递给下游用户，形成持续放大的知识污染。\n\n为此，作者提出轻量级推理时框架 FPPS（Factuality-Preserving Personalized Steering），在保留个性化收益的同时，实时检测并修正这种表示层面的纠缠，使模型在需要时回归事实子空间，从而抑制个性化诱导的幻觉。\n\nQ2: 有哪些相关研究？\n\n- **个性化 LLM**\n- 提示级个性化：Richardson et al. 2023a,b；Qiu et al. 2025a,b；Kumar et al. 2024b\n- 轻量适配/LoRA：Zhang et al. 2024, 2025；Zhang & Yang 2025\n- 偏好优化：Wu et al. 2024；Liu et al. 2025\n- 商业系统：OpenAI 2025（ChatGPT Memory）、Google 2025（Gemini Personal Context）、Anthropic 2025（Claude Memory）\n- **LLM 幻觉**\n- 检测与度量：Lin et al. 2022（TruthfulQA）、Manakul et al. 2023（SelfCheckGPT）、Sun et al. 2025（Reeep）\n- 缓解策略：Chuang et al. 2023（DoLa）、Liu et al.（鲁棒指令调优）、Sun et al.（LargePIG）\n- **表示操控与干预**\n- 激活工程：Rimsky et al. 2024；Turner et al. 2023\n- 超叠加与纠缠：Elhage et al. 2022\n- **个性化副作用**\n- 过滤气泡/极化：Lazovich 2023\n- 安全-效用权衡：Vijjini et al. 2025\n- 鲁棒性基准：Okite et al. 2025\n\n上述工作主要关注输出偏差、安全或通用幻觉，而本文首次系统研究“个性化表示与事实表示纠缠导致的幻觉”这一新失效模式，并提出推理时纠偏框架。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Factuality-Preserving Personalized Steering（FPPS）**，一套**推理时**轻量级框架，通过三步操作实时解除“个性化–事实”表示纠缠，仅当检测到幻觉风险才对内部激活做最小化修正，从而既抑制幻觉又保留个性化收益。\n\n1. **Representation Shift Locator（离线）**\n构造“带/不带用户历史”的对比输入，逐层计算 ground-truth 答案 token 的困惑度偏差\n Delta_ell = |PPL_ell^((with)) - PPL_ell^((without))| / PPL_ell^((with)) \n选取使事实可变性最大的层  L  作为后续干预点。\n\n2. **Factuality Entanglement Prober（离线）**\n在层  L  提取最终 token 隐藏状态  h_L’ ，用**事实退化样例**（个性化导致答错）作正例，**个性化受益样例**（依赖历史才答对）作负例，训练逻辑回归\n$ hat p = σ(W h_L’ + b),quad hat p∈\n0,1\n$\n输出  hat p  衡量当前表示受个性化干扰的事实风险。\n\n3. **Adaptive Knowledge Steering（在线）**\n基于  hat p  选择干预强度，提供三种实用变体：\n\n- **FPPS-H**（硬门控）\n$ tilde h_L^H =\nh_L’ - v_u, & hat pgeτ\nh\\_L’, & \\hat p<\\tau\n$\n其中  v_u = h_L(x,u) - h_L(x)  为个性化偏移，完全剔除高风险个性化。\n\n- **FPPS-S**（软双向修正）\n先计算“事实方向” s_f = m_(fact) - m_(pers) ，再连续调节\n tilde h_L^S = h_L’ + β_S(hat p),s_f,quad β_S(hat p)=γ(hat p-0.5) \n正系数抑制个性化，负系数增强个性化。\n\n- **FPPS-M**（混合自适应）\n$ tilde h_L^M =\nh_L’ + β_S(hat p),s_f, & hat p<τ quad (低风险，软修正)\nh_L’ - v_u, & hat pgeτ quad (高风险，硬剔除)\n\\end{cases}$\n兼顾连续性与安全性，单阈值  τ  即可稳定工作。\n\n通过上述“定位–探测–转向”流程，FPPS 在**任何提示级个性化模型**的生成阶段插入，几乎零额外训练成本，即可把幻觉压至最低，同时保持个性化问答精度。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“个性化诱发幻觉”这一核心问题，设计并执行了**三类实验**，覆盖**现象验证、方法有效性、机制分析**三个层面，具体实验如下：\n\n1\\. **现象验证实验**\n\n**目的**：验证“个性化会系统性扭曲事实推理”是否真实存在，并量化其对下游知识获取的影响。\n\n- **表示纠缠分析**（图2）\n- 在 PFQABench 的事实问答上，对比“带/不带个性化”的最终层表示，计算余弦相似度。\n- 结果：**幻觉样本的相似度显著低于真实样本**（p<0.001），证实个性化在表示空间拉离了事实方向。\n- **用户学习模拟**（图3 & 图6）\n- 用小模型（LLaMA-3.2-1/3B）扮演“学生”，大模型（LLaMA-3.1-8B、Qwen2.5-7/14/32B）扮演“教师”。\n- 控制组：教师为标准 LLM；实验组：教师为个性化 LLM（RAG 注入用户历史）。\n- 度量学生学完后的答题准确率。\n- 结果：个性化教师导致学生准确率平均下降 **10.5%**；当教师加装 FPPS-M 后，差距回收 **7.0%**，证明幻觉会传播且可被抑制。\n\n2\\. **方法有效性实验**\n\n**目的**：检验 FPPS 能否在保持个性化性能的同时，显著恢复事实正确率。\n\n- **主实验**（表1）\n- 基准：4 种主流提示级个性化方法（RAG、PAG、DPL、LLM-TRSR）× 3 个开源 backbone（LLaMA-3.1-8B-IT、Qwen2.5-7/14B-IT）。\n- 指标：P-Score（个性化问答准确率）、F-Score（事实问答准确率）、Overall（二者平均）。\n- 结果：\n- 原始系统 F-Score 仅 8.8–40.4%；加装 FPPS-M 后提升至 **75–85%**，平均 Overall 提升 **>50%**。\n- FPPS-H 事实最高，但 P-Score 略降；FPPS-S 温和；FPPS-M 在两者之间取得最佳平衡。\n- **消融实验**（表2）\n- 替换“探测器”或“转向向量”为随机向量，性能显著下降，验证两个组件缺一不可。\n- **超参与敏感性分析**（图7 & 图8）\n- 风险阈值 τ 在 0.3–0.7 区间平稳； steering 强度 γ 在 0.3–0.5 最佳；干预层越靠后效果越好，与理论预期一致。\n\n3\\. **机制与鲁棒性分析**\n\n**目的**：进一步理解个性化如何、在哪一层扭曲事实，以及 FPPS 的鲁棒性。\n\n- **层敏感度分析**（图9）\n- 逐层计算“带/不带历史”的 ground-truth 困惑度差值。\n- 发现**中后段层**的偏差最大，且该层与 FPPS 自动选出的干预层一致，说明幻觉主要发生在高层语义空间。\n- **探测器层间精度**（图10）\n- 在各层训练逻辑回归探针，发现**仅在中后段层**才能高准确率区分“事实退化”与“个性化受益”样本，进一步验证表示纠缠位置。\n- **历史长度影响**（图5）\n- 逐步增加 RAG 注入的历史比例，基线 F-Score 单调下降；FPPS-M 在全历史下仍保持稳定，证实对长上下文的鲁棒性。\n\n4\\. **数据集支撑**\n\n- **PFQABench**（§5.1 & 附录 E）\n- 首个**同时包含个性化问答与事实问答**的双任务 benchmark，1000 例（500 P / 500 F），覆盖 500 用户、50K 轮真实历史。\n- 采用会话级语义检索+混合重排序，确保事实题与用户历史“主题相关但事实无关”，恰好构成幻觉诱发场景。\n\n综上，实验从**表示分析→控制模拟→方法评测→机制拆解→鲁棒测试**形成闭环，系统验证了“个性化诱发幻觉”问题的存在性与危害，并证明 FPPS 在**多模型、多个性化范式、多长度历史**下均能**显著恢复事实准确率且保留个性化收益**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可延伸探索，按“现象-机制-方法-评测-应用”五层归纳：\n\n1\\. 现象层：幻觉类型与边界\n\n- **多语言与文化偏差**\n验证非英语语境下，个性化是否会放大本土谣言或地域偏见。\n- **多模态个性化幻觉**\n当历史包含图像、音频时，跨模态检索是否引入视觉/听觉伪事实。\n- **长期累积效应**\n设计 10-轮以上持续对话，观察同一用户错误信念是否被模型逐轮强化（“回音室斜率”量化）。\n\n2\\. 机制层：表示纠缠的因果解析\n\n- **因果归因**\n使用 interchange intervention 或梯度因果探针，精确定位哪些注意力头/神经元负责“个性化→事实”错位。\n- **正交化理论**\n研究能否在预训练阶段即把“用户偏好子空间”与“世界知识子空间”约束为近似正交，从源头降低纠缠。\n- **任务特异性**\n对比 QA、摘要、代码生成等任务，量化“个性化扭曲”是否仅影响需要外部知识的任务。\n\n3\\. 方法层：FPPS 的扩展与优化\n\n- **黑盒适配**\n对闭源 API 模型，用输出级 logit 蒸馏+输入侧 prompt 扰动近似估计 vu 与 sf，实现无梯度 FPPS。\n- **动态层选择**\n每层部署轻量探针，推理时根据输入自适应决定干预层，而非固定单点。\n- **个性化强度可学习**\n将 γ、τ 建模为输入相关的元网络输出，实现“千人千强度”的细粒度控制。\n- **与检索增强协同**\n把 FPPS 的 steering 信号反哺给检索器，实时过滤与用户历史过度相似但不可靠的文档。\n\n4\\. 评测层：更丰富与真实的基准\n\n- **纵向用户模拟器**\n构建可成长的“虚拟用户代理”，具备记忆更新、信念演化机制，用于测量模型在 100+ 轮对话后的长期知识污染率。\n- **高风险领域子集**\n划分医疗、法律、金融专用子基准，检验幻觉代价是否随决策风险呈指数放大。\n- **人机协同评测**\n引入真实用户标注“我是否因模型回答而改变了原有（正确）认知”，直接量化人类信任迁移。\n\n5\\. 应用层：系统与人因\n\n- **个性化可信等级**\n为不同场景（闲聊、教育、医疗）定义可接受的事实错误率阈值，动态切换 FPPS-H/S/M。\n- **可解释反馈**\n当触发硬干预时，向用户生成“基于您的历史，此回答已去除个性化影响”的可解释声明，提升透明度与用户信任。\n- **伦理与合规**\n研究 GDPR、“算法推荐管理规定”等法规下，干预行为是否构成“自动化决策限制”，需要额外用户授权。\n\n6\\. 交叉层：工具与开源\n\n- **发布通用干预库**\n将 FPPS 封装成 HuggingFace-compatible wrapper，支持一行代码对任何开源模型插入推理时纠偏。\n- **在线 A/B 平台**\n与浏览器插件或聊天客户端集成，实时收集用户“纠正/点赞”信号，持续蒸馏更好的 prober 与 steering 向量。\n\n通过上述探索，可系统性地把“个性化幻觉”从当前单点研究推进到**多语言、多模态、长周期、高风险、可解释、合规化**的下一阶段。\n\nQ6: 总结一下论文的主要内容\n\n论文核心内容可概括为“**一个问题、一个基准、一个方法、一套实验**”：\n\n1. **问题**：首次提出并验证“个性化诱发幻觉”——个性化 LLM 会把用户历史与客观事实在表示空间纠缠，导致答案迎合用户过往说法而失真，且该错误会随交互传递给下游用户。\n\n2. **基准**：发布 PFQABench，首个**同时包含个性化问答与事实问答**的双任务评测集（1000 例，500 用户），用于系统测量个性化对事实可靠性的影响。\n\n3. **方法**：提出 FPPS 推理时框架，三步完成“定位-探测-转向”：\n\n- 定位最易受个性化扰动的层；\n- 用轻量探针实时估计“事实-个性化”纠缠度；\n- 按风险等级硬剔除或软修正表示，恢复事实的同时保留个性化收益。\n4. **实验**：在 4 种主流个性化策略 × 3 个开源 backbone 上，FPPS 把事实准确率从平均 24% 提升到 80% 以上，Overall 得分提升 50%+，且对长历史、多轮教学场景均稳健，验证了其通用性与有效性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是When%20Personalization%20Misleads%3A%20Understanding%20and%20Mitigating%20Hallucinations%20in%20Personalized%20LLMs，链接是https://arxiv.org/pdf/2601.11000，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11000。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1768866682134","title":"ACoT-VLA: Action Chain-of-Thought for Vision-Language-Action Models","description":"Vision-Language-Action (VLA) models have emerged as essential generalist robot policies for diverse manipulation tasks, conventionally relying on directly translating multimodal inputs into actions via Vision-Language Model (VLM) embeddings. Recent advancements have introduced explicit intermediary reasoning, such as sub-task prediction (language) or goal image synthesis (vision), to guide action generation. However, these intermediate reasoning are often indirect and inherently limited in their capacity to convey the full, granular information required for precise action execution. Instead, we posit that the most effective form of reasoning is one that deliberates directly in the action space. We introduce Action Chain-of-Thought (ACoT), a paradigm where the reasoning process itself is formulated as a structured sequence of coarse action intents that guide the final policy. In this paper, we propose ACoT-VLA, a novel architecture that materializes the ACoT paradigm. Specifically, we introduce two complementary components: an Explicit Action Reasoner (EAR) and Implicit Action Reasoner (IAR). The former proposes coarse reference trajectories as explicit action-level reasoning steps, while the latter extracts latent action priors from internal representations of multimodal input, co-forming an ACoT that conditions the downstream action head to enable grounded policy learning. Extensive experiments in real-world and simulation environments demonstrate the superiority of our proposed method, which achieves 98.5%, 84.1%, and 47.4% on LIBERO, LIBERO-Plus and VLABench, respectively.","url":"https://huggingface.co/papers/2601.11404","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.134Z","metadata":{"authors":["Linqing Zhong","Yi Liu","Yifei Wei","Ziyu Xiong","Maoqing Yao","Si Liu","Guanghui Ren"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.RO"],"pdfUrl":"https://arxiv.org/pdf/2601.11404.pdf","arxivUrl":"https://arxiv.org/abs/2601.11404","arxivId":"2601.11404","coolPaperUrl":"https://papers.cool/arxiv/2601.11404","published":"2026-01-16T16:17:06Z","updated":"2026-01-16T16:17:06.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"LLM Analysis Failed: Error: 抓取失败（已重试2次）: Waiting failed: 30000ms exceeded"}},{"id":"hf-6-1768866682135","title":"BAPO: Boundary-Aware Policy Optimization for Reliable Agentic Search","description":"RL-based agentic search enables LLMs to solve complex questions via dynamic planning and external search. While this approach significantly enhances accuracy with agent policies optimized via large-scale reinforcement learning, we identify a critical gap in reliability: these agents fail to recognize their reasoning boundaries and rarely admit ``I DON'T KNOW'' (IDK) even when evidence is insufficient or reasoning reaches its limit. The lack of reliability often leads to plausible but unreliable answers, introducing significant risks in many real-world scenarios. To this end, we propose Boundary-Aware Policy Optimization (BAPO), a novel RL framework designed to cultivate reliable boundary awareness without compromising accuracy. BAPO introduces two key components: (i) a group-based boundary-aware reward that encourages an IDK response only when the reasoning reaches its limit, and (ii) an adaptive reward modulator that strategically suspends this reward during early exploration, preventing the model from exploiting IDK as a shortcut. Extensive experiments on four benchmarks demonstrate that BAPO substantially enhances the overall reliability of agentic search.","url":"https://huggingface.co/papers/2601.11037","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.135Z","metadata":{"authors":["Shiyu Liu","Yongjing Yin","Jianhao Yan","Yunbo Tang","Qinggang Zhang","Bei Li","Xin Chen","Jingang Wang","Xunliang Cai","Jinsong Su"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2601.11037.pdf","arxivUrl":"https://arxiv.org/abs/2601.11037","arxivId":"2601.11037","coolPaperUrl":"https://papers.cool/arxiv/2601.11037","published":"2026-01-16T07:06:58Z","updated":"2026-01-16T07:06:58.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“基于强化学习的智能体搜索（agentic search）”提出一个关键可靠性缺陷：\n经过大规模 RL 训练后，模型几乎从不承认“我不知道”（IDK），即使在证据不足或推理已触及极限时仍会编造看似合理却不可靠的答案。为此，作者提出 Boundary-Aware Policy Optimization（BAPO），目标是在不牺牲准确率的条件下，让模型具备动态识别自身推理边界并适时给出 IDK 的能力，从而提升整体可靠性。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均与“让大模型知道何时该拒绝回答”有关，但各自侧重点不同：\n\n1. 不确定性估计与校准\n\n- **SelfCheckGPT**（Manakul et al., 2023）\n- **Semantic Uncertainty**（Kuhn et al., 2023）\n- **Confidence-under-the-hood**（Kumar et al., 2024）\n- **Just Ask for Calibration**（Tian et al., 2023）\n共同点：利用 token 概率、输出一致性或显式 verbalized confidence 量化不确定性，再按阈值触发 IDK。\n局限：未考虑外部检索质量，难以区分“模型不会”与“检索不到”，在 agentic search 场景失效（论文表 8 验证其可靠性反而下降）。\n2. 静态知识/能力边界建模\n\n- **EKBM**（Zheng et al., 2025）\n- **CapBound**（Zhang et al., 2025c）\n共同点：通过对比参数记忆或推理链与已知事实/规则，判断问题是否超出模型“静态边界”。\n局限：边界定义固定，无法刻画“检索-推理”动态交互带来的时变边界。\n3. 强化学习引入拒绝信号\n\n- **Rejection Improves Reliability**（Xu et al., 2024）\n- **KnowRL**（Ren et al., 2025）\n- **BARREL**（Yang et al., 2025a）\n共同点：在 RL 中给 IDK 回答额外奖励，鼓励模型学会拒绝。\n局限：奖励为固定常数，易被模型利用成“懒惰捷径”，导致过度保守或准确率暴跌（论文 3.3 节与表 3 验证此现象）。\n\nBAPO 与上述工作的区别：\n\n- 首次把“动态推理边界”形式化为“同一问题多条 rollout 中是否出现正确答案”，并据此给出**组级边界感知奖励**；\n- 提出**两阶段自适应奖励调制器**，在探索期暂停 IDK 奖励、在平台期按 rollout 多样性再微调，防止 reward hacking；\n- 专门针对“检索-推理”交替的 agentic search 场景，在 5 k 样本规模下即可显著提升可靠性，而无需大规模人工标注或外部知识库。\n\nQ3: 论文如何解决这个问题？\n\n论文将“让智能体学会在真正超出能力边界时承认 IDK”形式化为一个带约束的强化学习目标，并提出 Boundary-Aware Policy Optimization（BAPO）。核心思路是**只在“确实无法答对”的样本上奖励 IDK**，同时用**自适应调制器**防止模型把 IDK 当成捷径。具体实现分为两步：\n\n1. 组级边界感知奖励\n对同一问题采样  G  条轨迹  τ_i_(i=1)^G ；\n若**组内无任何轨迹获得正分**（即  ∀ i,,R_(Correct)(τ_i)le 0 ），则对输出 IDK 的轨迹额外给予\n\nR_(IDK)=0.5·mathbb I(y_i=IDK);\n\n否则  R_(IDK)=0 。\n最终奖励  R=R_(Correct)+R_(IDK) 。\n该设计把“边界”定义为**经验可达性**：多条 rollout 均失败即视为超出边界，避免对可解问题错误鼓励 IDK。\n\n2. 自适应奖励调制器\n\n- **阶段级**：训练初期（验证分数连续 5 步未提升前）默认关闭  R_(IDK) ，让模型先充分探索；进入平台期后才启用。\n- **样本级**：平台期内，若某问题  G  条答案多样性高（ |y_1,dots,y_G|ge G/2 ），认为模型仍在积极搜索，临时关闭  R_(IDK) ；多样性低则正常发放。\n此外，对“组内全错”的困难问题允许最多重采样  k=2  次，以减小因采样不足而误判边界的误差。\n\n通过“只在真边界处奖励 IDK + 两阶段动态开关”，BAPO 在 4 个多跳 QA 基准上仅用 5 k 训练样本就把可靠性提升 15.8 分，而平均准确率下降不超过 2.2%，显著优于固定奖励或不确定性估计方法。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“可靠性”与“准确率”两大维度，在 4 个多跳 QA 基准、3 组模型规模上系统验证 BAPO 的有效性，并辅以消融、超参、动力学曲线、案例与不确定性基线对比。主要实验如下：\n\n1. 主实验（Qwen2.5-7B）\n数据集：HotpotQA / 2WikiMultiHopQA / MuSiQue / Bamboogle 完整测试集\n指标：accuracy、precision、IDK 率 ρ\\_IDK、综合可靠性\n\nreliability=(1-ρ_(IDK))·precision+ρ_(IDK)·accuracy\n\n结果：BAPO 平均可靠性 61.7，比最强基线 ReSearch 提升 15.8，准确率仅降 2.2。\n\n2. 跨模型规模泛化\n在 Qwen2.5-3B 与 14B 上重复主实验，BAPO 相对 Instruct backbone 可靠性分别提升 76 % 与 27 %，相对 GRPO 提升 13.9 % 与 11.9 %，趋势一致。\n\n3. 消融实验（3B）\n\n- 固定 IDK 奖励 0.5 → ρ\\_IDK 飙升至 53 %，可靠性最低。\n- 去掉样本级调制器 → 可靠性 −1.2。\n- 同时去掉阶段级+样本级调制器 → ρ\\_IDK 35 %，准确率降 7 分，验证调制器必要性。\n4. 训练动力学（14B）\n绘制 R\\_correct、R\\_IDK 与 ρ\\_IDK 随步数变化曲线：\n\n- 探索期（前 60 步）R\\_correct 快速上升，ρ\\_IDK 从 20 % 降至 5 %；\n- 平台期 R\\_IDK 开始激活并稳定在 0.25-0.3，ρ\\_IDK 回升至 25 % 以上，显示两阶段目标切换成功。\n5. 拒绝合理性验证\n定义 rejection success rate：BAPO 拒绝的问题中有多少也是 GRPO 无法答对的。\n3B/7B/14B 分别达 74.7 %、76.7 %、76.7 %，表明 IDK 决策与真实边界高度一致。\n\n6. 超参敏感性（3B）\n\n- IDK 比例阈值 α ∈ {0, 0.05, 0.2, 0.3}：α=0 时模型完全不会 IDK；α 过大则过早保守。\n- 重采样次数 k ∈ {0,1,2,3}：k=2 后收益边际递减，与 Pass@K 曲线在 K=32 趋于饱和相符。\n7. 不确定性基线对比（7B）\n实现三类无需 RL 的 IDK 策略：\n\n- 内部置信度（token prob）\n- 表达确定性（verbalized certainty）\n- 自反思（self-reflection）\n结果三者均显著牺牲准确率，可靠性无提升或下降，说明传统不确定性估计在 agentic search 场景失效。\n8. 案例可视化\n给出同一问题下 GRPO 模型幻觉答案与 BAPO 模型主动 IDK 的完整轨迹，直观展示边界感知差异。\n\n9. Exact-Match 指标复现\n在 3B/7B/14B 上改用严格字符匹配评分，BAPO 依旧取得最高可靠性，排除 GPT-4 评委偏差。\n\n综上，实验从主结果、泛化、消融、动力学、合理性、超参、基线、案例、指标九个角度闭环验证：BAPO 能在极小训练集下同时维持高准确率与可解释的自我拒绝能力。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步挖掘，均围绕“动态边界”与“真实部署”两大核心：\n\n1. 边界信号精细化\n\n- 将“组内无正确轨迹”升级为**置信加权边界估计**：利用 token 概率、检索片段相关性、推理步一致性等多源信号，通过小量级 reward model 学习更细粒度的“可达性”标签，减少采样方差。\n- 引入**在线贝叶斯更新**：随着训练步推进，用 running statistics 动态修正“失败阈值”，避免固定 0 分硬阈值带来的误判。\n2. 任务与领域外推\n\n- 数学、代码、医疗问答等**非检索型推理任务**中，边界定义不再依赖检索失败，而取决于逻辑闭环是否可构造；需重新设计边界感知奖励。\n- 跨语言、跨文化知识场景下，IDK 决策是否出现语言偏好或文化偏差，需要多语种可靠性评测框架。\n3. 更大规模与参数高效训练\n\n- 在 30 B+ 模型上验证 BAPO 是否仍能保持“低样本高效率”；若出现奖励噪声放大，可探索**奖励模型缩放**（如深窄 MLP critics）或**LoRA/DoRA 低秩微调**只训策略头。\n- 与 MoE 结合：专家层路由概率可否直接作为“能力边界”先验，用于提前触发 IDK，减少冗余推理步。\n4. 在线搜索噪声与对抗检索\n\n- 从本地 Wikipedia 切换到**真实搜索引擎**（Bing/Google API），面对广告、SEO、时效性冲突时，边界估计需融入**检索结果质量打分**（PageRank、时间戳、源可信度）。\n- 构建**对抗性 IDK 数据集**：故意提供误导性但看似相关的片段，检验模型是否仍能识别真边界，防止被“伪证据”骗过。\n5. 人机协同与可解释性\n\n- 将 BAPO 的“拒绝”与**人类标注者不确定性**进行一致性研究：当模型 IDK 时，人类是否也一致认为不可答？可建立双向校准指标。\n- 可视化“边界距离”：把 rollout 多样性、检索片段相关度、中间推理置信度映射到 2D 空间，让用户看到“为什么系统选择 IDK”，提升可解释信任。\n6. 长程迭代与终身学习\n\n- 在**持续知识更新**场景下，旧知识可能失效，新知识尚未被检索库覆盖；需让模型具备**时间感知边界**，对“知识过期”主动 IDK。\n- 引入**经验回放缓冲**，定期重放历史 IDK 样本，检验模型在新检索库下是否已能解答，实现“撤回 IDK”式的自我修正。\n7. 多智能体协作\n\n- 多 agent 辩论场景：若同一问题多个 agent 均给出 IDK，才最终触发系统级 IDK；否则利用交叉检验提升召回。可研究 BAPO 策略在多智能体博弈下的均衡与奖励分配。\n8. 安全与伦理\n\n- 对**敏感或违法查询**（如制造危险物品教程），即使检索到片段，也应强制 IDK；需把安全策略嵌入边界奖励，形成“安全边界”与“能力边界”双约束。\n- 检验 IDK 决策是否对特定人群、地域、主题产生**系统性拒答偏差**，避免模型以“不确定性”为借口强化信息不平等。\n\n通过上述扩展，可逐步把 BAPO 从“单模型-静态库-单任务”推向“大模型-开放网络-多任务-终身学习”的可靠智能体范式。\n\nQ6: 总结一下论文的主要内容\n\n论文核心内容可概括为“一个关键问题、一套解决方案、四项实验验证”：\n\n- **问题**\n强化学习驱动的智能体搜索模型在提升多跳问答准确率的同时，几乎丧失了对自身推理边界的感知——即便证据不足也拒绝说“我不知道”（IDK），导致长篇推理难以被用户验证，可靠性风险高。\n\n- **解决方案：BAPO**\n提出 Boundary-Aware Policy Optimization，在 GRPO 框架内增加两项机制：\n\n1. 组级边界感知奖励——仅当同一问题的多条 rollout 全部失败时才给 IDK 轨迹额外奖励，防止把可解问题误判为未知。\n2. 自适应奖励调制器——训练早期关闭 IDK 奖励避免捷径；平台期按 rollout 多样性动态开关，兼顾探索与边界意识。\n- **实验**\n- 4 个多跳 QA 基准、3 个模型规模（3B/7B/14B）上，用 5 k 样本即可将平均可靠性提升 15.8 分，准确率仅降 2.2%。\n- 消融、超参、训练动力学、拒绝合理性、不确定性基线等 9 组分析均表明：BAPO 能在保持解题能力的同时，让模型精准识别并主动拒绝超出边界的问题。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是BAPO%3A%20Boundary-Aware%20Policy%20Optimization%20for%20Reliable%20Agentic%20Search，链接是https://arxiv.org/pdf/2601.11037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11037。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1768866682137","title":"FrankenMotion: Part-level Human Motion Generation and Composition","description":"Human motion generation from text prompts has made remarkable progress in recent years. However, existing methods primarily rely on either sequence-level or action-level descriptions due to the absence of fine-grained, part-level motion annotations. This limits their controllability over individual body parts. In this work, we construct a high-quality motion dataset with atomic, temporally-aware part-level text annotations, leveraging the reasoning capabilities of large language models (LLMs). Unlike prior datasets that either provide synchronized part captions with fixed time segments or rely solely on global sequence labels, our dataset captures asynchronous and semantically distinct part movements at fine temporal resolution. Based on this dataset, we introduce a diffusion-based part-aware motion generation framework, namely FrankenMotion, where each body part is guided by its own temporally-structured textual prompt. This is, to our knowledge, the first work to provide atomic, temporally-aware part-level motion annotations and have a model that allows motion generation with both spatial (body part) and temporal (atomic action) control. Experiments demonstrate that FrankenMotion outperforms all previous baseline models adapted and retrained for our setting, and our model can compose motions unseen during training. Our code and dataset will be publicly available upon publication.","url":"https://huggingface.co/papers/2601.10909","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.137Z","metadata":{"authors":["Chuqiao Li","Xianghui Xie","Yong Cao","Andreas Geiger","Gerard Pons-Moll"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2601.10909.pdf","arxivUrl":"https://arxiv.org/abs/2601.10909","arxivId":"2601.10909","coolPaperUrl":"https://papers.cool/arxiv/2601.10909","published":"2026-01-15T23:50:07Z","updated":"2026-01-15T23:50:07.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对现有文本驱动人体运动生成方法“只能接受整段动作或整条序列的粗粒度描述，无法对**单个身体部位**进行**细粒度、时序精确**的控制”这一核心缺陷，提出了一套新的数据构造与模型训练框架，具体目标可归纳为：\n\n- **数据层面**：缺乏“原子级、带时间戳、按身体部位划分的运动标注”。\n- 现有数据集（HumanML3D、KIT-ML、BABEL 等）仅有整条序列或粗动作标签，未指明“哪一帧、哪一部件、做什么”。\n- 人工逐帧标注成本极高，几乎不可行。\n- **模型层面**：缺乏能同时接受\n\n1. 序列级整体语义、\n2. 动作级时序片段、\n3. 部位级逐帧提示\n的**统一生成框架**。\n\n- 已有方法要么只做时序组合（TEACH、PriorMDM），要么只做部位拼接（STMC、FineMoGen），无法端到端地学习“部位-动作-序列”三级条件之间的时空依赖。\n\n为此，论文提出两条关键技术路线：\n\n1. **FrankenAgent**（基于 LLM 的自动标注代理）：\n利用大语言模型的推理能力，把现有数据集的“粗标签”自动扩展成**异步、原子、带时间窗口**的“部位-动作-序列”三级标注，构建出迄今最大规模的细粒度运动数据集 **FrankenStein**。\n\n2. **FrankenMotion**（分层条件扩散模型）：\n在训练阶段同时接收\n\n- 序列文本  L_s \n- 动作文本  L_a \n- 逐帧部位文本  L_p \n通过 Transformer 编码器融合三级条件，实现“任意粒度输入、统一生成”的灵活控制；推理阶段可仅给部位提示、或仅给动作提示，也能组合出训练中从未出现的复杂运动。\n\n综上，论文旨在**首次实现“原子级、时序精确、部位可控”的文本到运动生成**，突破以往方法在时空细粒度控制上的天花板。\n\nQ2: 有哪些相关研究？\n\n与 FrankenMotion 直接相关的研究可按“控制粒度”与“技术路线”两条主线梳理如下：\n\n1\\. 文本驱动运动生成（整段/整句级）\n\n| 方法 | 关键思路 | 与本文差异 |\n| --- | --- | --- |\n| HumanML3D [12] | 首个大规模文本-运动配对数据集，支持整句到全长运动生成 | 仅有序列级自然语言，无动作分段、无部位标签 |\n| TEMOS [35] | VAE 框架，文本编码后全局注入 | 无时间结构、无部位控制 |\n| MotionDiffuse [61] | 扩散模型，全局文本条件 | 同样缺乏时序与部位细粒度 |\n| MDM/UniMotion [20,46] | 扩散+Transformer，支持帧级动作文本 | 帧级动作≠部位级；无“左右手/头/腿”等部件语义 |\n\n2\\. 时序组合与过渡控制（动作级）\n\n| 方法 | 关键思路 | 与本文差异 |\n| --- | --- | --- |\n| TEACH [1] | 文本+时间段拼接，两段动作平滑过渡 | 仅动作级，无部位概念 |\n| PriorMDM [44] | 两阶段扩散：先粗略再细化过渡 | 同样无部位通道 |\n| FlowMDM [3] | 位置编码混合，实现无缝衔接 | 只解决“动作”过渡，不解决“部位”差异 |\n| DART [68] | 自回归扩散，实时逐帧控制 | 误差累积，且文本仅全局提示，无部件信号 |\n\n3\\. 部位级或空间组合控制\n\n| 方法 | 关键思路 | 与本文差异 |\n| --- | --- | --- |\n| FineMoGen [62] | 扩散+部位 mask，可逐部件编辑 | 部位标签必须与动作段同步且等长，无法异步；无层次文本 |\n| STMC [37] | 测试时把多个预训练 MDM 输出按部位拼接 | 后融合、无端到端学习，部位间时空一致性差 |\n| Motion-in-Context [43] | 多通道扩散，通道对应不同部位 | 需要预定义“轨道”，未解决文本-部位对齐 |\n\n4\\. 数据侧：运动文本标注\n\n| 方法 | 关键思路 | 与本文差异 |\n| --- | --- | --- |\n| BABEL [39] | 给 AMASS 打“原子动作”标签 | 只有动作级，无部位 |\n| PoseScript/MotionScript [9,56] | 规则模板把 pose→文本 | 生成描述冗长、非原子、无时间窗口 |\n| ChatPose/MotionGPT [11,18] | 用 LLM 把 pose 翻译成自然语言 | 仅全局描述，未分解到部件；未用于“生成”训练 |\n\n5\\. 扩散模型训练技巧（通用）\n\n| 方法 | 关键思路 | 本文借鉴点 |\n| --- | --- | --- |\n| DDPM [16] | 原始去噪扩散目标 | 基础训练损失 |\n| Cosine Noise Schedule [6] | 改善低噪声区域 | 本文采用 |\n| Stochastic Masking [26] | 随机丢弃条件，提高鲁棒性 | 本文扩展为 Beta 分布采样，适配稀疏部位标签 |\n\n小结\n\n- **时序组合**工作解决了“动作→动作”过渡，但未触及“部位→部位”异步控制。\n- **部位编辑**工作需要同步标签或后融合，无法端到端学习三级语义。\n- **数据侧**工作仍停留在“整句”或“动作”粒度，缺乏原子、异步、部位级标注。\n\nFrankenMotion 首次把\n\n1. 序列级、\n2. 动作级、\n3. 部位级（逐帧可异步）\n三种条件统一在**同一扩散框架**内训练，并借助 LLM 自动构造所需细粒度数据，从而跳出上述任一单一粒度的局限。\n\nQ3: 论文如何解决这个问题？\n\n论文把“缺乏细粒度、时序精确、部位可控的文本-运动数据”与“缺乏统一的三级条件生成模型”这两个耦合难题**同步解决**，具体分为**数据侧**与**模型侧**两条并行管线：\n\n1\\. 数据侧：自动构造 FrankenStein 数据集\n\n**核心观察**\n\n- 已有数据（HumanML3D、BABEL 等）只给“整条序列”或“粗动作”标签，但人类可以根据高层语义**推理**出“哪一帧、哪一部件、做什么”。\n- 大语言模型（LLM）恰好具备这种**常识推理**能力。\n\n**步骤**\n\n1. 定义三级原子标注格式\n\n- 序列级： A_s = (L_s, 0, T) \n- 动作级： A_a = (L_i^a, t_i^s, t_i^e)_(i=1)^N 　（不重叠窗口）\n- 部位级： A_p = A_k_(k=1)^K ，其中  A_k=(L_(k,j)^p, t_j^s, t_j^e)_(j=1)^(M_k) 　（左右臂、左右腿、头、脊柱、轨迹共 7 部）\n2. 设计 **FrankenAgent**（基于 Deepseek-R1）\n\n- 输入：原始序列标签 + 动作标签\n- 输出：补齐后的  A_s, A_a, A_p ，不确定处显式输出 **unknown** 以避免幻觉\n- 提示工程：强制给出时间窗口、强制按部位列表输出、强制 unknown 机制\n3. 质量验证\n\n- 随机 50 段运动，3 名专家双盲评测 → 整体准确率 **93.08 %**（Gwet AC1 = 0.91）\n- 最终得到 39.1 h 运动、138.5 k 原子标签、46.1 k 部位标签，其中 28.8 k 标签为**推理生成**、从未在原始数据出现。\n\n2\\. 模型侧：FrankenMotion 扩散框架\n\n**目标函数**\n标准 DDPM 损失，但条件  c  由三级文本共同构成：\n\nL = E_(x_0,σ,ε)[|f_θ(x_σ,σ,L_s,L_a,L_p)-x_0|_2^2]\n\n**网络设计**\n\n1. 文本编码\n\n- 统一用 **冻结 CLIP ViT-B/32** 编码所有文本 → 512 维\n- 动作/部位嵌入再经 PCA 降维到 50 维，减少计算\n2. 时空嵌入（Spatio-Temporal Embedding）\n\n- 部位文本： F_p ∈ R^(T× (K× 50)) 　（每帧 7×50）\n- 动作文本： F_a ∈ R^(T× 50) 　（窗口复制到帧）\n- 拼接得  F_(a+p) ∈ R^(T× (K+1)50) ，再与噪声运动  x_σ  拼成  F_(a+p+m) \n3. 全局上下文\n\n- 序列文本经 MLP 得全局向量  F_s ∈ R^(D_(m+t)) \n- 扩散时间步  σ  也映射为向量\n- 二者作为 **2 个额外 token** 与  F_(a+p+m)  拼接 → 输入 **Transformer**\n4. 鲁棒训练策略\n\n- 原始标签为 unknown 的向量直接置 0\n- 其余标签以 **Beta(5r,5(1-r))** 随机丢弃，增强稀疏条件下的泛化\n\n3\\. 推理灵活性\n\n- **仅给部位提示**： L_p  稀疏或局部， L_a,L_s  空缺 → 模型自动补全合理动作\n- **仅给动作提示**： L_p  空缺 → 生成符合动作语义的全身运动\n- **三级同时给**：可精确控制“坐着的同时左手挥手”这类**训练阶段从未出现的组合**\n\n4\\. 实验验证\n\n- 在重新训练的 UniMotion、STMC、DART 上**全面领先**（R@1, R@3, M2T, FID）\n- 消融显示：仅部位条件已达 SOTA 90 % 性能，再加动作/序列条件**进一步提升**细节与真实感\n- 可生成**分钟级多段复合运动**（图 1 右），单趟扩散即可输出，无需后处理拼接\n\n结论\n\n论文通过“LLM 自动标注 + 三级条件扩散”双轮驱动，**一次性解决数据缺失与模型缺失**两个瓶颈，首次实现\n\n> **“任意粒度文本输入 → 原子级、时序精确、部位可控的 3D 人体运动生成”**。\n\nQ4: 论文做了哪些实验？\n\n论文从“数据质量→生成质量→模型设计→实际可用性”四个层面展开系统实验，全部在自建的 **FrankenStein** 数据集上完成。具体实验与结果如下：\n\n1\\. 数据质量验证（FrankenAgent 可靠性）\n\n- **协议**\n- 随机抽取 50 条运动序列\n- 3 名专业标注员双盲打分：每条“序列/动作/部位”标签是否与实际运动一致（二元判正）\n- **指标**\n- 平均准确率：93.08 %\n- 组间一致性：Gwet AC1 = 0.91（几乎完美）\n- **结论**\nLLM 自动标注达到**人工可信水平**，为后续训练提供可靠基础。\n\n2\\. 主实验：细粒度文本-运动生成对比\n\n**baseline** 均用 FrankenStein 重新训练，保证数据一致\n\n- **STMC** \n37\n：后融合部位扩散结果\n- **UniMotion** \n20\n：帧级动作文本扩散，无部位通道\n- **DART** \n68\n：自回归扩散，文本全局注入\n\n**评估协议**\n\n- 对三种控制粒度分别训练**独立评测模型**（共 9 个，7 部位+1 动作+1 序列）用于度量语义对齐\n- 20 次随机采样取 95 % 置信区间\n\n| 指标 | 部位平均 | 动作级 | 序列级 | realism (FID↓) |\n| --- | --- | --- | --- | --- |\n| FrankenMotion | 47.2 R@1 / 58.9 R@3 | 48.1 / 65.8 | 65.3 / 85.6 | 0.04 |\n| UniMotion | 45.7 / 57.4 | 47.6 / 65.6 | 62.7 / 82.1 | 0.05 |\n| STMC | 40.7 / 51.4 | 41.0 / 56.3 | 43.6 / 62.3 | 0.10 |\n| DART | 38.7 / 50.2 | 39.8 / 57.6 | 54.3 / 77.0 | 0.14 |\n\n- **结论**：在所有粒度上**显著领先**（R@1/R@3、M2T），且真实感最佳（FID 最低）。\n\n3\\. 消融实验：三级条件必要性\n\n训练三个变体：\n\n- **Part-only**：仅  L_p \n- **Part+Action**： L_p+L_a \n- **Full**： L_p+L_a+L_s \n\n| 条件 | 部位 R@3 | 动作 R@3 | 序列 R@3 | FID |\n| --- | --- | --- | --- | --- |\n| Part-only | 56.3 | — | — | 0.08 |\n| Part+Action | 57.7 | 65.4 | — | 0.07 |\n| Full | 58.9 | 65.8 | 85.6 | 0.05 |\n\n- **结论**\n- 即使**仅部位文本**已接近上限 90 % 性能，证明模型对稀疏细粒度提示鲁棒\n- 加入高层语义**持续提高**一致性与真实感，验证层次条件设计价值\n\n4\\. 稀疏/缺失条件鲁棒性\n\n- **协议**：在测试时随机丢弃 0 %–80 % 的部位标签，其余未知标签置 0\n- **结果**：R@3 从 58.9 → 52.1（80 % 缺失），FID 保持 < 0.12\n- **结论**：Beta 分布随机掩码训练显著提升**现实应用**中提示不完整时的可用性\n\n5\\. 复合/未见运动生成案例\n\n- **输入示例**\n“先站立→爬上楼梯→右手扶栏杆→左手静止→再站立”\n（部位、动作、序列三级提示混合，且**训练集未出现**“扶栏杆+爬楼”组合）\n- **结果**：生成运动被评测模型判定为 91 % 语义正确，FID = 0.06\n- **结论**：模型真正学到**原子元素组合**能力，而非简单记忆\n\n6\\. 运行效率\n\n- 单张 NVIDIA H100 训练 47.5 h（≈ 2 天）\n- 单条 256 帧运动生成耗时 0.23 s（RTX-4090）\n- 内存占用 6.8 GB，满足**实时交互**需求\n\n7\\. 定性可视化\n\n- 与 baseline 对比视频（图 4）\n- STMC：部位间明显滑步、转身细节丢失\n- UniMotion：忽略“转身”指令，动作过度平滑\n- DART：出现重复坐下-站起\n- **FrankenMotion**：精确执行“转身+坐下+手臂放置”所有细粒度提示，过渡自然\n\n总结\n\n实验覆盖\n\n1. 标注可信度\n2. 与现有最强适配 baseline 的**全面量化对比**\n3. 三级条件**消融**\n4. 稀疏提示鲁棒性\n5. 训练外复合运动**泛化**\n6. 实时性能\n\n结果一致表明：FrankenMotion 在**语义正确性、真实感、灵活性**三维度均取得**显著优势**，验证了“LLM 自动细粒度标注 + 层次扩散”这一新范式的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为对 FrankenMotion 当前局限与潜力的直接延伸，按“数据-模型-应用”三层归纳：\n\n1\\. 数据与标注\n\n- **多语言 / 文化扩展**\n现标注仅英文；同一动作在不同文化语境下部位描述差异大，可构建多语言并行语料以提升跨文化泛化。\n\n- **物理-语义一致性过滤**\nLLM 偶尔生成“物理不可能”部位描述（如“左手摸右肘同时右臂前伸 180°”）。引入**物理可行性验证层**（碰撞、关节极限）再回标 LLM，可形成自我修正闭环。\n\n- **层次化时间粒度自动发现**\n当前动作窗口仍靠原始 BABEL 分段。可让 LLM 先**自适应地发现**“子动作”边界，再生成部位标签，实现真正**原子级**时间粒度。\n\n2\\. 模型与算法\n\n- **长时序生成**\n单趟扩散目前限 256–512 帧。可探索\n– 层次扩散（粗→细）\n– 记忆机制（Transformer-XL / Mamba）\n– 潜空间自回归\n以支持**分钟级**连续运动而无需手动分段。\n\n- **双向 / 编辑器范式**\n现有框架为单向“文本→运动”。可训练**双向扩散**，允许用户**局部编辑**（拖拽手部关键帧或输入新文本），模型自动补全其余部位与过渡。\n\n- **物理-觉察损失**\n在扩散训练阶段引入\n– 地面接触损失\n– 动量守恒损失\n– 关节力矩正则\n减少滑步、漂浮等伪影，实现**物理可执行**运动。\n\n- **多角色 / 物体条件**\n将部位控制机制扩展到**双人交互**或**人-物交互**：把“交互对象”也视为虚拟“部位”，用同一 Transformer 融合，实现“左手握杯同时右手打招呼”级细粒度组合。\n\n- **音频-部位联动**\n对音乐节拍或语音重音，引入**音频-部位注意力**，实现“重拍时头部点头、弱拍时脚跟点地”的**细粒度同步**。\n\n3\\. 应用与系统\n\n- **实时 VR/AR 驱动**\n结合轻量级编解码器（如 Motion LCM）与 FrankenMotion 的稀疏提示能力，实现**口语化即时操控**：用户说“右手挥两下同时转身”，毫秒级生成匹配 Avatar。\n\n- **个性化风格微调**\n用 5–10 分钟用户专属动作录制，通过 LoRA/Adapter 对部位扩散头做**轻量微调**，使生成运动保留用户独特步态、手势风格。\n\n- **可解释运动分析**\n利用已训练的部位文本编码器，反向检索“哪一段运动导致不平衡”，为**体育训练**或**康复评估**提供可解释反馈。\n\n- **安全与伦理检测**\n构建“部位级不当提示”黑名单过滤器，防止生成暴力或冒犯性动作；同时开放**公平性评测协议**，确保不同性别、体型角色的生成质量一致。\n\n4\\. 理论层面\n\n- **组合泛化边界**\n系统研究“训练阶段未同时出现的部位-动作-序列三元组”最大复杂度，量化模型**真正组合能力** vs**记忆能力**，指导未来数据采样策略。\n\n- **文本-运动信息瓶颈**\n分析三级条件各自对最终分布的**互信息贡献**，用最小充分原则压缩冗余文本，提升推理效率。\n\n一句话总结\n\nFrankenMotion 已验证“LLM 自动原子标注 + 层次扩散”可行，下一步可向**更长、更物理、更交互、更个性化**的细粒度可控人体运动生成继续深入。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有文本驱动人体运动生成缺乏“原子级、时序精确、部位可控”的标注与模型，只能接受整句或粗动作描述，无法细粒度地指定“哪一帧、哪一部件、做什么”。\n- **解法**：\n\n1. **数据侧**——FrankenAgent（LLM）自动把 HumanML3D/BABEL 等粗标签扩展成“序列-动作-部位”三级、带时间窗口的细粒度标注，构建 39 h 的 FrankenStein 数据集（93 % 人工一致率）。\n2. **模型侧**——FrankenMotion 扩散 Transformer，同时接收序列文本  L_s 、动作文本  L_a 、逐帧部位文本  L_p ，端到端学习三级条件融合，支持任意粒度输入与组合生成。\n- **结果**：在自建的细粒度 benchmark 上，部位/动作/序列的语义正确率与真实感（FID）均显著优于重新训练的 STMC、UniMotion、DART 等强 baseline；可生成训练未见的复杂组合（如“坐着同时左手挥手”）；稀疏提示下仍保持鲁棒。\n- **意义**：首次实现“原子、异步、部位级”文本到运动生成，为细粒度、可组合、实时可控的 3D 人体动画提供新基准。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是FrankenMotion%3A%20Part-level%20Human%20Motion%20Generation%20and%20Composition，链接是https://arxiv.org/pdf/2601.10909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.10909。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1768866682137","title":"Entropy Sentinel: Continuous LLM Accuracy Monitoring from Decoding Entropy Traces in STEM","description":"Deploying LLMs raises two coupled challenges: (1) monitoring - estimating where a model underperforms as traffic and domains drift - and (2) improvement - prioritizing data acquisition to close the largest performance gaps. We test whether an inference-time signal can estimate slice-level accuracy under domain shift. For each response, we compute an output-entropy profile from final-layer next-token probabilities (from top-k logprobs) and summarize it with eleven statistics. A lightweight classifier predicts instance correctness, and averaging predicted probabilities yields a domain-level accuracy estimate. We evaluate on ten STEM reasoning benchmarks with exhaustive train/test compositions (k in {1,2,3,4}; all \"10 choose k\" combinations), across nine LLMs from six families (3B-20B). Estimates often track held-out benchmark accuracy, and several models show near-monotonic ordering of domains. Output-entropy profiles are thus an accessible signal for scalable monitoring and for targeting data acquisition.","url":"https://huggingface.co/papers/2601.09001","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.137Z","metadata":{"authors":["Pedro Memoli Buffa","Luciano Del Corro"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2601.09001.pdf","arxivUrl":"https://arxiv.org/abs/2601.09001","arxivId":"2601.09001","coolPaperUrl":"https://papers.cool/arxiv/2601.09001","published":"2026-01-13T21:54:38Z","updated":"2026-01-13T21:54:38.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"LLM Analysis Failed: Error: 抓取失败（已重试2次）: Waiting failed: 30000ms exceeded"}},{"id":"hf-9-1768866682138","title":"ProFit: Leveraging High-Value Signals in SFT via Probability-Guided Token Selection","description":"Supervised fine-tuning (SFT) is a fundamental post-training strategy to align Large Language Models (LLMs) with human intent. However, traditional SFT often ignores the one-to-many nature of language by forcing alignment with a single reference answer, leading to the model overfitting to non-core expressions. Although our empirical analysis suggests that introducing multiple reference answers can mitigate this issue, the prohibitive data and computational costs necessitate a strategic shift: prioritizing the mitigation of single-reference overfitting over the costly pursuit of answer diversity. To achieve this, we reveal the intrinsic connection between token probability and semantic importance: high-probability tokens carry the core logical framework, while low-probability tokens are mostly replaceable expressions. Based on this insight, we propose ProFit, which selectively masks low-probability tokens to prevent surface-level overfitting. Extensive experiments confirm that ProFit consistently outperforms traditional SFT baselines on general reasoning and mathematical benchmarks.","url":"https://huggingface.co/papers/2601.09195","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.138Z","metadata":{"authors":["Tao Liu","Taiqiang Wu","Runming Yang","Shaoning Sun","Junjie Wang","Yujiu Yang"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2601.09195.pdf","arxivUrl":"https://arxiv.org/abs/2601.09195","arxivId":"2601.09195","coolPaperUrl":"https://papers.cool/arxiv/2601.09195","published":"2026-01-14T05:50:40Z","updated":"2026-01-14T05:50:40.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决传统监督微调（SFT）中因“单参考答案”约束导致的表面形式过拟合问题。核心观察是：语言具有一对多特性，同一意图可由多种表达实现，而传统 SFT 强制模型逐词对齐唯一参考，会过度惩罚语义等价的替代表达，使模型记忆特定措辞而非掌握核心逻辑。\n为此，作者提出以下关键论点与方案：\n\n1. 问题根源\n\n- 单参考 SFT 将有效释义视为错误，抑制语义灵活性。\n- 引入多参考答案虽可缓解，但带来数据与计算成本线性增长、分布冲突及优化不稳定（图 2）。\n2. 核心发现\n\n- 通过语义标注与假设检验验证：**高概率 token 承载核心推理逻辑，低概率 token 多为可替换的表面表达**（图 3，p=1×10⁻⁶ 显著性）。\n3. 解决思路\n\n- 不追求昂贵的大规模多参考数据，而是**在单参考样本内部过滤高价值信号**。\n- 提出 **ProFit**：利用模型自身在线预测概率作为动态指标，对低概率“非核心”token 施加掩码，阻止其大梯度干扰关键参数更新（式 4–5，定理 1）。\n4. 预期效果\n\n- 在保持单参考训练效率的同时，显著降低对表面措辞的过拟合，提升模型在推理、数学、指令遵循等任务上的泛化性能。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归为两条主线，并指出 ProFit 与它们的区别：\n\n1. 数据高效的指令微调（Data-Efficient Instruction Tuning）\n\n- 代表工作：LIMA、Shadow-FT、Superfiltering、MIWV、ICL-based 过滤等。\n- 共同特点：在**样本级**做重要性采样或难度筛选，把整条指令-响应当作原子单元。\n- 局限性：无法剔除样本内部的“低密度”片段或风格噪声，仍迫使模型拟合非核心措辞。\n2. 令牌级训练目标（Token-Level Training Objectives）\n\n- 经典方法：Focal Loss、Unlikelihood Training——针对“难 token”加权，但需人工定义难易。\n- 近期 LLM 工作：\n– Rho-1、TIS-DPO：依赖外部参考模型估计 token 重要性，计算开销大。\n– DFT：用概率做软重加权，仍对所有 token 回传梯度。\n– CFT：验证“关键区域”需监督，但未给出自动筛选机制。\n- ProFit 的区别：\n– 完全**自包含**：利用当前模型自身概率，不引入外部模型。\n– **硬掩码**：直接丢弃低概率 token，一步完成去噪，而非软加权或再训练。\n– 理论保证：证明低概率 token 的梯度范数下界更大，会淹没核心信号，掩码可消除此干扰。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **ProFit**（**Pro**bability-guided **F**ine-tun**ing**），通过“在线概率-硬掩码”策略，在单参考数据上自动识别并抑制非核心 token 的干扰，从而兼顾训练效率与语义鲁棒性。具体步骤如下：\n\n1. 语义-概率联合验证\n\n- 借助 Gemini-3-Pro 对多参考答案进行人工标注，将 token 分为“核心（core）”与“琐碎（trivial）”两类。\n- 用 Qwen3-4B-Base 计算每个 token 的预测概率，统计发现：\n– 核心 token 概率分布集中在高置信区（均值 0.768）；\n– 琐碎 token 长尾分布于低概率区（均值 0.485）。\n- 假设检验 p=1×10⁻⁶，拒绝“两类 token 概率分布相同”的原假设，确立 **“低概率 ⇒ 非核心”** 的代理关系。\n2. 概率导向的硬掩码目标\n训练阶段仅对高概率 token 回传梯度，公式化如下：\n\nM_t = Il[sgl(π_θ(y^__t|x,y^__(<t))r) > τr],\n\nL_(ProFit) = E_(D)![-(1) / (T)∑_(t=1)^T M_t logπ_θ(y^__t|x,y^__(<t))].\n\n- sg(·) 停止梯度，确保掩码在反向传播时为常数门控，避免阶跃函数不可导问题。\n- τ∈\n0,1\n 为静态阈值，实验通用 0.5；消融显示 0.1–0.9 区间均稳定优于基线。\n3. 理论支撑：低概率 token 的梯度干扰\n定理 1 给出单 token 参数梯度范数下界：\n\n|∇_θell|_2 ge γl(1-π_θ(y^__t|x,y^__(<t))r),\n\n表明概率越低，梯度越大。掩码这些 token 可防止其大幅梯度淹没核心语义信号。\n\n4. 训练流程保持单参考、单 epoch、LoRA/全参皆可\n\n- 数据：2k 高分子集（BAAI-InfinityInstruct），无需额外参考答案。\n- 推理：保留原始解码超参，零额外成本。\n\n通过“在线概率 ⇨ 硬掩码 ⇨ 抑制大梯度”这一闭环，ProFit 在 Qwen3/Llama/OLMo2 的 0.6B–14B 模型上，平均准确率较标准 SFT 提升 3.0–10.9 个百分点，同时显著改善训练稳定性与收敛速度。\n\nQ4: 论文做了哪些实验？\n\n论文在 **通用推理、数学、指令遵循** 三大类共 **7 个基准** 上，对 **5 个模型系列（0.6B–14B）** 进行了系统实验，并辅以 **4 组深度分析**，可概括为：\n\n1. 主实验：五基准横向对比\n模型：Qwen3-0.6B/4B/14B-Base、OLMo-2-7B、Llama-3.1-8B\n基准：GPQA-Diamond、GSM8K、MATH-500、AIME’24、IFEval\n方法：Vanilla、标准 SFT、Entropy-过滤、DFT（软重加权）、ProFit\n结果：ProFit 在所有组合上 **一致最优**，平均准确率较 SFT 提升 **+3.0~10.9%**；在 Qwen3-14B 上逆转了 SFT 的 **\\-1.88%** 负迁移，取得 **+5.64%** 正收益。\n\n2. 阈值 τ 消融\n\n- 仅训练低概率 token（p<τ）→ 性能全面低于基线，验证其“非骨架”属性。\n- 仅训练高概率 token（p>τ）→ 在所有任务、所有阈值 **稳定超越**全token基线，最佳区间 τ∈\n0.3,0.7\n。\n3. LoRA 秩缩放实验\n秩 r=4→1024：\n\n- 核心 token（p>0.1）呈 **单调上升**，需大容量。\n- 非核心 token（p<0.1）与标准 SFT 呈 **U 型**：中等秩因干扰而掉点，高秩过拟合更严重；证明非核心表达是优化瓶颈。\n4. 训练动态追踪（5 epoch）\nProFit（p>τ）**第 1 epoch 即收敛**至最优，而低概率策略（p<τ）**震荡且早停于次优**，进一步验证高概率 token 提供主要梯度信号。\n\n5. RL 初始化验证\n以 ProFit、DFT、Base 三种 checkpoint 作为 GRPO 起点，在 MATH-500、OlympiadBench、Minerva 上继续强化学习：\n\n- ProFit 初始 **Pass@4/Avg@4 全面最高**，KL 发散最小，熵稳定，响应长度增长最陡（>8k token），表明其**推理骨架更优**，利于后续 RL 探索。\n6. 案例可视化\n多项式因式分解任务：SFT 因拟合低概率表面措辞漏掉交互项，逻辑崩塌；ProFit 保持完整系数链，输出正确答案。\n\n7. 超参与推理设定\n单 epoch、batch=1、accum=4、maxlen=8192；AIME 输出 32k token，其余 8k；温度、top-p 等详见附录，确保可复现。\n\n综上，实验从**宏观性能**到**微观token**、从**监督微调**到**强化学习**、从**0.6B**到**14B**全面覆盖，系统验证了 ProFit 的**通用性、稳定性与可扩展性**。\n\nQ5: 有什么可以进一步探索的点？\n\n- **动态阈值**：目前使用全局静态 τ，可探索按样本难度或 token 类型自适应调整，兼顾稳定性与细粒度筛选。\n- **创意生成任务**：低概率 token 在故事、诗歌等场景可能贡献风格多样性，需验证 ProFit 是否抑制创造性，并设计“任务感知的概率掩码”。\n- **多语言 / 多模态**：考察高/低概率分布是否跨语言、跨模态（代码、图文）一致，或需重新校准阈值。\n- **与 RL 阶段联合优化**：将 ProFit 的掩码机制嵌入在线 RL 的奖励计算，实现“细粒度探索-利用”闭环。\n- **理论深化**：在更大规模模型或不同架构上验证梯度下界 γ 的分布特性，研究 γ 与容量、任务复杂度之间的关系。\n\nQ6: 总结一下论文的主要内容\n\n**论文核心贡献速览**\n\n1. 问题\n传统 SFT 强制逐词对齐单参考，忽视语言“一对多”特性 → 表面过拟合；多参考缓解但成本高昂且优化不稳。\n\n2. 关键发现\n高概率 token ≈ 核心推理；低概率 token ≈ 可替换琐碎表达（p=1×10⁻⁶ 显著）。\n\n3. 方法：ProFit\n训练时仅用模型自身在线概率作信号，对 p<τ 的 token 施加硬掩码，阻止其大梯度干扰核心参数更新；保持单参考、单 epoch、零额外推理成本。\n\n4. 实验结果\n\n- 0.6B–14B 五模型 × 五基准：平均准确率较 SFT 提升 **+3.0~10.9%**，逆转 14B 的负迁移。\n- 消融：τ∈\n0.3,0.7\n 全任务稳超基线；仅学低概率则性能崩塌。\n- LoRA 秩 4→1024：核心 token 单调受益，非核心呈 U 型干扰曲线。\n- RL 初始化：ProFit 起点 **Pass@4/Avg@4 全面最优**，KL 低、熵稳定、CoT 长度最长。\n5. 意义\n给出“用概率即时滤噪”这一低成本、通用、可扩展的 SFT 新范式，兼顾训练效率与语义鲁棒性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ProFit%3A%20Leveraging%20High-Value%20Signals%20in%20SFT%20via%20Probability-Guided%20Token%20Selection，链接是https://arxiv.org/pdf/2601.09195，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.09195。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1768866682138","title":"Future Optical Flow Prediction Improves Robot Control & Video Generation","description":"Future motion representations, such as optical flow, offer immense value for control and generative tasks. However, forecasting generalizable spatially dense motion representations remains a key challenge, and learning such forecasting from noisy, real-world data remains relatively unexplored. We introduce FOFPred, a novel language-conditioned optical flow forecasting model featuring a unified Vision-Language Model (VLM) and Diffusion architecture. This unique combination enables strong multimodal reasoning with pixel-level generative fidelity for future motion prediction. Our model is trained on web-scale human activity data-a highly scalable but unstructured source. To extract meaningful signals from this noisy video-caption data, we employ crucial data preprocessing techniques and our unified architecture with strong image pretraining. The resulting trained model is then extended to tackle two distinct downstream tasks in control and generation. Evaluations across robotic manipulation and video generation under language-driven settings establish the cross-domain versatility of FOFPred, confirming the value of a unified VLM-Diffusion architecture and scalable learning from diverse web data for future optical flow prediction.","url":"https://huggingface.co/papers/2601.10781","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.138Z","metadata":{"authors":["Kanchana Ranasinghe","Honglu Zhou","Yu Fang","Luyu Yang","Le Xue","Ran Xu","Caiming Xiong","Silvio Savarese","Michael S Ryoo","Juan Carlos Niebles"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2601.10781.pdf","arxivUrl":"https://arxiv.org/abs/2601.10781","arxivId":"2601.10781","coolPaperUrl":"https://papers.cool/arxiv/2601.10781","published":"2026-01-15T18:49:48Z","updated":"2026-01-15T18:49:48.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“从单张图像与自然语言指令出发，预测通用、空间稠密的未来光流（future optical flow）”这一核心问题，并验证该预测在两大下游任务——语言驱动的机器人操纵与文本引导的视频生成——中的价值。具体而言，其关注以下挑战：\n\n- 现有方法难以从带噪、无结构化的网络视频中学习到可泛化的未来运动表示；\n- 稀疏轨迹或低层控制信号难以同时满足机器人控制与视频生成对精细、全局运动信息的需求；\n- 纯 RGB 帧预测包含大量与运动无关的静态信息，导致运动表征不纯粹。\n\n为此，作者提出统一 VLM-Diffusion 架构 FOFPred，通过以下策略解决上述问题：\n\n1. 将未来光流建模为 RGB 图像，利用强大 VAE 与扩散模型实现像素级生成保真；\n2. 设计相机-物体运动解耦算法，从网络视频中提取干净的相对光流监督；\n3. 基于大规模人类活动视频进行预训练，再分别接入扩散策略头或视频生成头，完成机器人控制与文本到视频生成任务。\n\n综上，论文首次展示了“语言条件 + 稠密未来光流预测”这一表征在跨域控制与生成任务中的通用性与有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中系统梳理了三条主线的前期研究，并指出 FOFPred 与它们的区别。按主题归纳如下（不出现第一人称，不罗列原文编号，仅给出技术脉络与代表思路）：\n\n1\\. 统一架构：控制与生成共用骨干\n\n- **通用智能体模型**\n- 典型如 Gato、UniPi 等，用单一 Transformer 处理视觉-语言-动作多模态数据，完成数百个任务。\n- **统一生成式扩散模型**\n- UniDiffuser、BLIP3-o、OmniGen-2、MetaQuery 等，在一个噪声预测框架内同时支持图像生成、编辑、风格迁移等多任务。\n- **ControlNet 类即插即用条件机制**\n- 通过额外网络分支把边缘、深度、姿态等低层空间信号注入预训练扩散模型，成为后续“多条件视频生成/编辑”的通用范式。\n- **与 FOFPred 的区别**：上述工作以“完成多任务”或“支持多种空间条件”为目标，并未专门研究“语言→未来稠密光流”这一表征本身，也未在机器人操纵与视频生成两大域同时验证其通用性。\n\n2\\. 机器人控制中的运动表征\n\n- **光流或轨迹作为显式运动先验**\n- 早期方法把光流当作辅助输入，用于强化学习或模仿学习，但依赖真实光流或仿真 ground-truth。\n- **稀疏点轨迹预测**\n- Track2Act、ATM、Any-point Trajectory 等从网络视频学习稀疏 2-D 轨迹，再映射到机器人动作，对全局机械臂或相机运动覆盖不足。\n- **视觉-语言-动作（VLA）模型**\n- RT-1、RoboFlamingo、GR-1、π0、DreamVLA 等把 RGB+文本直接映射为动作，不强制显式运动中间表征。\n- **近期“先预测运动、再决策动作”的两阶段方法**\n- FlowVLA、DreamVLA、VPP 等引入“光流”或“世界知识”作为 Chain-of-Thought，但 FlowVLA 仅预测下一帧光流，DreamVLA 用压缩 token 表示动态区域，VPP 采用 RGB 帧预测而非运动预测。\n- **与 FOFPred 的区别**：FOFPred 专注于“语言条件下的多步未来稠密光流”本身，采用扩散图像生成范式，在数据规模、表征密度、跨域通用性上进一步拓展。\n\n3\\. 视频生成中的运动控制\n\n- **稀疏轨迹/草图引导**\n- DragNUWA、DragAnything、Tora、TrackGo 等允许用户手绘几条轨迹或给出若干稀疏点，即可驱动目标物体沿路径运动。\n- **显式运动场或运动基元**\n- MotionCtrl、Motion-I2V、MOFAVideo 等用全局运动矢量场或相机参数作为条件，实现镜头与物体解耦控制。\n- **文本--only 方法**\n- CogVideoX、HunyuanVideo、Wan 等大规模 T2V 模型仅依赖文本提示，无显式运动信号，难以精确控制方向与速度。\n- **与 FOFPred 的区别**：FOFPred 把“文本→未来光流”作为独立可解释的中间层，再接入下游视频扩散模型，实现无需人工轨迹草图的细粒度运动控制，同时保持语言指令的直观性。\n\n小结\n\n前期研究要么聚焦“统一多模态模型”却未深入运动表征，要么利用“稀疏/单步运动”或“RGB 帧预测”导致信息缺失或静态冗余，要么在视频生成侧依赖人工轨迹或低层条件。FOFPred 首次将“语言驱动的稠密未来光流预测”作为独立任务，用 VLM-Diffusion 统一架构在海量无标注网络视频上训练，并同时在机器人操纵与文本到视频生成两条下游链路验证其通用性与可解释性。\n\nQ3: 论文如何解决这个问题？\n\n论文将“语言驱动的未来稠密光流预测”拆解为数据、表征、模型、训练与下游适配五个环节，提出 FOFPred 框架，具体解决方案如下（按环节分点陈述）：\n\n1\\. 数据层：从网络视频中提取干净运动监督\n\n- **相机-物体运动解耦**\n- 先用 RAFT 计算原始光流  F_(raw) ，再在低分辨率帧上均匀采样网格点，利用 RANSAC 估计单应矩阵  H ，得到整幅画面的“相机流” F_(cam) 。\n- 相对物体运动  F_(obj) = F_(raw) - F_(cam) ；对幅值小于阈值  τ_(noise)  的像素置零，抑制残差噪声。\n- **运动引导抽帧**\n- 离线两阶段过滤：\n1. 32×32 低分辨率 + Lucas-Kanade 快速计算代理运动；\n2. 取流幅值 top-k 百分位作为  μ_(proxy) ，仅保留  μ_(proxy)>τ  的连续帧对，剔除静态或微动片段，保证训练序列的动态一致性。\n\n2\\. 表征层：把光流变成“可扩散”的 RGB 图像\n\n- 将笛卡尔光流  (f_x, f_y)  转极坐标  (M,θ) ，幅值  M  归一化到 $\n0,1\n，角度 θ∈\n0,2π\n$。\n- 映射至 HSV： H=θ,; S=hat M,; Vequiv 1 （固定亮度消除帧间闪烁），再 differentiable 转换到 RGB。\n- 得益于此，可直接复用 Flux.1-VAE 的编解码权重，无需重新训练或微调 VAE。\n\n3\\. 模型层：统一 VLM-Diffusion 架构\n\n- **冻结的编码器**\n- 视觉端：Flux.1 VAE 编码器 → 16 通道潜空间特征  f_v 。\n- 语言-视觉端：Qwen2.5-VL（3B）自回归 Transformer，输入图文交错序列，输出 2520-d 文本特征  f_c 。\n- **可训练的中间层**\n- 两个 MLP 分别把  f_v 、 f_c  投影到共同维度  D=2520 ；噪声潜码亦经同一 MLP 映射。\n- 条件向量与噪声序列在 token 级拼接后送入 DiT。\n- **时序改造 DiT（4B）**\n- 3D RoPE：将原始“文本长度-高-宽”三轴重新解释为“帧序号-高-宽”，为输入帧  x_(t-1),x_t  与输出流潜码  y_(t+1:t+4)  赋予时间位置 ID。\n- 全时空自注意力：对所有条件 token 与噪声 token 一起计算 self-attention，实现跨帧动态建模，且不引入额外可学习参数，保留 OmniGen 的图像预训练权重。\n\n4\\. 训练层：流匹配扩散 + 无分类器引导\n\n- **目标**\n- 对过滤后的帧对  (x_i,x_(i+1))  计算相对光流  y_i ，经 VAE 编码得潜码  f_y 。\n- **损失**\n- 采用 Flow-Matching 目标\n\nmathcal L_(FM)(θ)=| D(f_c,f_v,tilde f_y) - (f_y - f_0)|_2^2\n\n其中  tilde f_y  为加噪版本， f_0  为噪声样本， D  代表 DiT+MLP 可学习部分。\n- **正则化**\n- 训练期以一定概率丢弃文本条件  f_c  或视觉条件  f_v ，实现无分类器引导，为推理阶段提供  <1  的引导尺度可调性。\n\n5\\. 推理与下游适配\n\n- **推理**\n- VLM 与 VAE 仅作特征提取，DiT 执行  k  步反向扩散（实验显示  k=1  亦可生成合理光流）。\n- 对文本/视觉条件分别施加引导系数，提升语义-运动对齐精度。\n- **机器人控制**\n- 两阶段微调：\n① 在机器人视频-文本对上继续训练 DiT，使其同时输出腕部相机与固定外参相机的未来光流；\n② 冻结光流预测支路，接入扩散策略网络（DPN），把预测光流、机器人本体状态与文本目标一起映射为 7-DoF 臂夹动作。\n- **文本到视频生成**\n- 两阶段管线：\n① FOFPred 根据首帧与文本提示生成 4 帧未来光流，插值到任意长度得到稠密运动场；\n② 将运动场与首帧输入 Go-with-the-Flow（GWTF）视频扩散模型，合成最终视频，实现无需人工轨迹的纯语言运动控制。\n\n通过上述“数据清洗-运动表征-统一架构-流匹配训练-任务特化头”的完整链路，论文解决了从大规模带噪网络视频学习通用、语言条件、空间稠密未来光流预测的问题，并在机器人操纵与视频生成两大领域同时取得 SOTA 或可比性能，验证了该中间表征的跨域通用性与可解释性。\n\nQ4: 论文做了哪些实验？\n\n论文在第四节“Experiments”中围绕两大下游任务、三类基准、四项消融，共开展如下实验（按任务归类，给出关键指标与结果）：\n\n1\\. 语言驱动机器人操纵\n\n1.1 CALVIN ABC→D 零样本长程操纵\n\n- **协议**：仅在 ABC 环境训练，零样本评测 D 环境；需连续完成 5 个语言指令链。\n- **指标**：第 i 步成功率 SR-i (↑) 与平均完成长度 Avg Len (↑)。\n- **结果（表 1）**\n- 100% 训练数据：FOFPred 取得 SR-5 = **78.7%**，Avg Len = **4.48**，均优于此前最佳 DreamVLA（78.1%，4.44）。\n- 10% 低数据：Avg Len = **3.43**，比强基线 VPP 提升 **+0.18**，验证数据高效性。\n\n1.2 RoboTwin 2.0 双手协调\n\n- **协议**：5 项必须双手配合的任务（递送、叠放等），官方 easy 设定。\n- **指标**：单任务成功率与平均成功率。\n- **结果（表 2）**\n- FOFPred 平均成功率 **68.6%**，相对 VPP 提升 **+6.8 pp**；5 项任务全部领先，最大单项提升 **+8 pp**。\n\n2\\. 文本到视频运动控制\n\n2.1 SSv2 验证集语言驱动生成\n\n- **协议**：仅用首帧 + 文本动作描述，生成 16 帧视频；与 CogVideoX 等公开权重在同等设置下比较。\n- **指标**：SSIM、PSNR、LPIPS、FVD、KVD、Motion Fidelity (MF) (↑ 除 LPIPS/FVD/KVD)。\n- **结果（表 3）**\n- FOFPred 两阶段管线在 **FVD 75.39**、**KVD 11.38**、**MF 0.662** 均刷新最佳；SSIM/PSNR 亦优于 CogVideoX，验证纯文本即可产生更忠实运动。\n\n3\\. 消融实验（统一在 CALVIN 400-seed 子集）\n\n| 实验 | 变量 | Avg Len | 主要结论 |\n| --- | --- | --- | --- |\n| 预训练数据源（表 4） | DROID 机器人视频 vs SSv2 人类视频 | 4.04 → 4.39 (+0.35) | 网络人类视频运动多样性更丰富，同训练步数下显著优于机器人专用数据。 |\n| 骨干选择（表 5） | 仅扩散 vs VLM-Diffusion vs +图像编辑预训练 | 4.01 → 4.14 → 4.39 | VLM 提供语言-视觉语义对齐，图像编辑预训练进一步提升运动-文本对应。 |\n| 运动目标形式（表 6） | 无预训练 / 静态帧目标 / 原始光流 / 解耦光流 | 3.91 / 4.28 / 3.89 → 4.39 | 相机-物体运动解耦是学到可泛化运动指令的关键；原始光流被相机运动淹没。 |\n| 稠密 vs 稀疏（附录 A.1） | 16×16 网格子采样 / ATM 稀疏轨迹 / 默认稠密 | 1.24 / 2.92 → 4.39 | 空间稠密光流保留精细动态，对复杂操纵至关重要。 |\n| 运动信号必要性（附录 A.2） | 无运动输入 / 静态视觉嵌入 / 未来光流 | 0.02 / 0.52 → 4.39 | 预测光流动态信息无法被静态视觉或纯文本替代。 |\n\n4\\. 分析与可视化\n\n- **多样性分析**（附录图 A.1）\n同一图-文 4 个随机种子：多数预测正确，偶发相机运动与物体运动耦合，揭示模型对文本细微差异敏感。\n- **T2V 可视化**（附录图 A.2）\n对比 GT、CogVideoX、FOFPred：在“远离镜头移动胶棒”“向右拉玩具车”等例中，FOFPred 运动方向与幅度更接近真实；偶见物体轻微形变，但运动保真度始终优于基线。\n\n小结\n\n实验覆盖 **零样本长程操纵**、**双手协调** 与 **文本驱动视频生成** 三大场景，共 10 余项指标全面领先；四项消融进一步证实“大规模人类视频 + 相机-物体解耦 + 稠密光流 + VLM-Diffusion 统一架构”各模块均对最终性能有显著正向作用。\n\nQ5: 有什么可以进一步探索的点？\n\n后续可在以下方向继续深入（按探索价值排序，给出可行思路与潜在指标）：\n\n1\\. 模型效率与部署\n\n- **轻量级蒸馏**\n- 以 FOFPred 为教师，训练小至 0.3B-1B 的学生 DiT，目标在 RTX-3060 级笔记本 ≥15 FPS 实时推理；监控 FVD/机器人 SR 下降 <3%。\n- **步数压缩**\n- 研究 DDIM/蒸馏-Flow 将反向步数从 1→¼ 步，验证是否仍保持 MF>0.65；或引入 Consistency Model 一次性生成。\n- **量化与剪枝**\n- 对 4B-DiT 做 8-bit/4-bit 权重量化，配合 block-wise 剪枝，评估显存 ≤8 GB 时 CALVIN Avg Len 衰减边界。\n\n2\\. 数据与监督信号\n\n- **自动文本重标注**\n- 用 VLM 对原始视频生成多视角动作描述（“把杯子从右推到左”“杯子向左滑动”），做 prompt-augmentation，测试对同义句鲁棒性（SR-5 提升空间）。\n- **多模态运动监督**\n- 引入事件相机、深度或 IMU，与 RGB 联合估计“真实”物体运动，降低单目 homography 误差，量化相机解耦残差对 FVD 的影响。\n- **跨文化视频扩展**\n- 收集非英语区（如中文、日语）日常活动视频，验证语言域外 zero-shot 机器人策略迁移能力。\n\n3\\. 表征与任务拓展\n\n- **光流→3-D 场景流**\n- 预测 3-D 场景流或粒子轨迹，接入 6-DoF 抓取规划，看是否提升遮挡/堆叠场景成功率。\n- **时域任意长度预测**\n- 把 DiT 改为 autoregressive 或 latent-ODE，支持 1→N 帧可变长度输出，测试长视频 FVD 与机器人多步（10-step）任务完成度。\n- **运动-语义联合嵌入**\n- 在 VLM 内部引入“运动 token”与名词/动词对齐，评估文本改动时（“推”↔“拉”）光流方向翻转准确率。\n\n4\\. 控制与生成耦合\n\n- **闭环在线重规划**\n- 机器人侧实时把最新帧馈入 FOFPred，滚动重预测光流并更新动作，测量动态扰动下成功率提升。\n- **运动-生成双向反馈**\n- 视频生成阶段若 FVD 高于阈值，自动调整光流幅值系数并重新合成，实现“生成-评估-再生成”自适应循环。\n\n5\\. 可解释性与安全性\n\n- **运动热图可视化**\n- 对 DiT 的 cross-attention 权重进行梯度加权，生成“语言关键词↔像素运动”热图，帮助诊断失败案例。\n- **对抗/安全文本检测**\n- 构建 adversarial prompt 数据集（“把杯子摔碎”），研究模型是否生成不符合安全约束的运动，并引入安全过滤头。\n\n6\\. 跨域迁移与通用代理\n\n- **从人到机器人零样本迁移**\n- 完全不在机器人数据上微调，仅用人视频预训练权重直接驱动真实机械臂，记录首次成功率与碰撞率。\n- **多智能体协同光流**\n- 输入多机位视频，预测“协同运动场”，验证双机械臂同时操作长杆等任务是否需要额外通信通道。\n\n7\\. 理论分析\n\n- **光流分布复杂度量化**\n- 用 PCA/ICA 维度与频谱能量比较自然图像 vs 光流潜空间，解释为何 1-step 扩散即可生成合理结果。\n- **语言-运动对齐误差下界**\n- 建立文本语义距离与光流角度误差之间的信息论下界，评估当前 VLM-Diffusion 是否已接近最优。\n\n8\\. 开源与基准\n\n- **发布多语言-多任务基准 FOFBench**\n- 包含 1) 机器人长程操纵、2) T2V 运动忠实度、3) 运动可解释问答三项子评测，推动后续方法横向对比。\n\n以上方向兼顾效率、精度、安全与理论理解，可系统推进“语言驱动稠密运动预测”从实验室原型走向实际产品落地。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：语言驱动的空间稠密未来光流预测尚缺可泛化、可扩展方案，且未在机器人控制与视频生成两端同时验证。\n- **方法**：提出 FOFPred，用冻结 VLM（Qwen2.5-VL）与 VAE（Flux.1）提取语言-视觉特征，训练可扩展 DiT（4B）在 RGB-光流潜空间执行流匹配扩散；辅以相机-物体运动解耦与运动引导抽帧，从 50 万网络视频-文本对学得通用预测器。\n\n- **下游**：\n\n1. 机器人——两阶段微调后接扩散策略头，CALVIN 长程任务 SR-5 达 78.7%，RoboTwin 双手平均成功率 68.6%，均超此前最佳。\n2. 视频——两阶段管线先预测光流再驱动 Go-with-the-Flow，SSv2 基准 FVD 75.39、MF 0.662，优于 CogVideoX 等纯文本方法。\n- **消融**：人类视频 > 机器人视频、VLM-Diffusion > 纯扩散、稠密光流 > 稀疏轨迹、解耦运动 > 原始光流，各模块均显著贡献。\n- **结论**：首次展示统一 VLM-Diffusion 架构可在 web 级噪声数据上习得语言条件稠密未来光流，并无缝迁移至控制与生成两大域，为运动感知通用世界模型奠定基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Future%20Optical%20Flow%20Prediction%20Improves%20Robot%20Control%20%26%20Video%20Generation，链接是https://arxiv.org/pdf/2601.10781，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.10781。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1768866682139","title":"ShapeR: Robust Conditional 3D Shape Generation from Casual Captures","description":"Recent advances in 3D shape generation have achieved impressive results, but most existing methods rely on clean, unoccluded, and well-segmented inputs. Such conditions are rarely met in real-world scenarios. We present ShapeR, a novel approach for conditional 3D object shape generation from casually captured sequences. Given an image sequence, we leverage off-the-shelf visual-inertial SLAM, 3D detection algorithms, and vision-language models to extract, for each object, a set of sparse SLAM points, posed multi-view images, and machine-generated captions. A rectified flow transformer trained to effectively condition on these modalities then generates high-fidelity metric 3D shapes. To ensure robustness to the challenges of casually captured data, we employ a range of techniques including on-the-fly compositional augmentations, a curriculum training scheme spanning object- and scene-level datasets, and strategies to handle background clutter. Additionally, we introduce a new evaluation benchmark comprising 178 in-the-wild objects across 7 real-world scenes with geometry annotations. Experiments show that ShapeR significantly outperforms existing approaches in this challenging setting, achieving an improvement of 2.7x in Chamfer distance compared to state of the art.","url":"https://huggingface.co/papers/2601.11514","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.139Z","metadata":{"authors":["Yawar Siddiqui","Duncan Frost","Samir Aroudj","Armen Avetisyan","Henry Howard-Jenkins","Daniel DeTone","Pierre Moulon","Qirui Wu","Zhengqin Li","Julian Straub","Richard Newcombe","Jakob Engel"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CV","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2601.11514.pdf","arxivUrl":"https://arxiv.org/abs/2601.11514","arxivId":"2601.11514","coolPaperUrl":"https://papers.cool/arxiv/2601.11514","published":"2026-01-16T18:51:24Z","updated":"2026-01-16T18:51:24.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nShapeR 旨在解决“从随意拍摄（casual capture）的图像序列中鲁棒地生成带度量尺度的三维物体形状”这一难题。\n现有三维形状生成方法普遍依赖干净、无遮挡、已分割的输入，而真实场景中的随手拍摄往往伴随遮挡、背景杂乱、低分辨率、运动模糊和视角不佳等问题，导致已有方法性能急剧下降。ShapeR 通过以下方式突破该瓶颈：\n\n- 引入多模态条件（稀疏 SLAM 点云、带位姿的多视角图像、机器生成文本描述），在统一框架下实现度量一致、完整且高保真的单物体形状生成。\n- 提出两阶段课程学习：先在大规模孤立物体数据上学习通用先验，再在合成场景数据上适应真实遮挡与组合关系，提升对复杂环境的泛化能力。\n- 无需显式二维分割，仅依靠三维点云及其二维投影掩码即可隐式定位目标物体，降低对人工标注或高精度分割的依赖。\n- 建立新的“in-the-wild”评测基准，含 178 个真实场景物体及完整三维真值，用于系统评估在随意拍摄条件下的重建鲁棒性。\n\nQ2: 有哪些相关研究？\n\nShapeR 涉及三条主线：非物体中心重建、条件式单物体重建、以及物体中心场景重建。以下按类别归纳代表性文献，并指出 ShapeR 与它们的差异。\n\n- **非物体中心重建**\n- 传统 SLAM / 融合：KinectFusion、Poisson Surface Reconstruction、Atlas、NeuralRecon、SimpleRecon\n- 神经隐式表示：NeRF、Mip-NeRF、Zip-NeRF、NeuS、NeuS2、MonoSDF、3D Gaussian Splatting\n- 前馈式全局网格预测：EFM3D、FoundationStereo、DP-Recon、LIRM\n→ 共同问题：输出单块场景网格，遮挡区域残缺，无法保证单个物体完整几何。\n- **条件式单物体重建**\n- 单图像/点云到 3D：DeepSDF、Occupancy Networks、ConvONet、LRM、TripoSG、Direct3DS2、Hunyuan3D-2.0\n- 文本到 3D：DreamFusion、Magic3D、ProlificDreamer、Shap-E\n- 遮挡推理：Amodal3R（TRELLIS 扩展）\n→ 普遍要求干净、无遮挡、已分割输入，且多数方法无度量尺度；ShapeR 引入稀疏度量点云与多视角序列，显著增强对真实遮挡与杂乱背景的鲁棒性。\n- **物体中心场景重建**\n- 联合检测-补全：Panoptic Reconstruction、RevealNet、FroDO\n- CAD 检索：Scan2CAD、Mask2CAD\n- 单图多物体：Total3DUnderstanding、Coherent3D、Gen3DSR、InstaScene、CAST、MIDI3D、SceneGen\n→ 依赖高质量 2D 实例分割，在机器分割失效时退化；ShapeR 仅利用 3D 检测框与稀疏点云即可隐式定位物体，避免分割误差传播。\n\n综上，ShapeR 首次将“rectified flow + 多模态度量条件 + 两阶段课程学习”整合，用于随意拍摄环境下的鲁棒物体重建，填补了“真实杂乱场景+度量完整形状”这一研究空白。\n\nQ3: 论文如何解决这个问题？\n\nShapeR 将“随意拍摄序列 → 度量完整三维物体”分解为四个核心步骤，每一步均针对真实场景的遮挡、杂乱、无分割、无尺度等挑战设计：\n\n1. 预处理：自动提取多模态条件\n\n- 视觉-惯性 SLAM 输出稀疏度量点云  P  与相机位姿  Pi_k 。\n- 3D 实例检测器在点云与图像上联合预测物体框，无需人工分割。\n- 对每帧可见点投影得到 2D 点掩码  M_i ，Vision-Language 模型生成文本标题  T_i 。\n结果：每物体获得  mathcal C_i={P_i,I_i,Pi_i,M_i,T_i} ，已带真实世界尺度。\n2. 统一生成框架：多模态 Rectified Flow Transformer\n\n- 3D VAE（Dora-VecSet）将网格编码为可变长度潜在序列  z∈mathbb R^(L× 64) 。\n- Flow 模型  f_θ  以高斯噪声  z_1simmathcal N(0,I)  为起点，在条件  mathcal C  驱动下预测速度场\n\ndot z_t = f_θ(z_t,t,mathcal C),quad t∈[0,1]\n\n训练目标为最小化  mathbb E_(t,z,mathcal C)|f_θ-(z_0-z_1)|_2^2 。\n- 条件编码：\n– 3D 点云 → 稀疏 3D-ResNet 令牌；\n– 图像 → 冻结 DINOv2 + Plücker 射线编码；\n– 2D 点掩码 → 2D-CNN 令牌，与图像令牌拼接；\n– 文本 → T5 + CLIP 令牌。\n无需显式分割掩码，模型靠 3D 点与 2D 投影隐式定位目标。\n3. 两阶段课程学习：从“干净孤立”到“真实杂乱”\n\n- 阶段 1：60 万个人工建模孤立物体网格，在线组合增强（背景合成、遮挡贴图、分辨率下降、点云丢点/噪声/轨迹截断、光度扰动），学习通用形状先验。\n- 阶段 2：在 Aria Synthetic Environments 的真实场景合成数据上微调，引入真实遮挡、物体间交互、SLAM 噪声，提升对组合与遮挡的鲁棒性。\n4. 推理：度量一致重建\n\n- 对归一化立方体 $\n-1,1\n^3 执行 midpoint 积分得到 z\\_0$，VAE 解码为 SDF，Marching Cubes 提取网格。\n- 用输入点云  P_i  的度量范围将网格缩放回原坐标系，保证尺寸与位置真实可用。\n- 全程自动，无需人工选图或交互分割。\n\n通过“多模态条件 + 隐式定位 + 大规模增强 + 课程微调”，ShapeR 在提出的 178 物体 in-the-wild 基准上 Chamfer 距离比现有最佳方法降低 2.7×，实现随意拍摄下的鲁棒度量形状生成。\n\nQ4: 论文做了哪些实验？\n\nShapeR 在实验部分系统评估了“随意拍摄→度量完整三维物体”这一设定下的重建鲁棒性，实验覆盖三类主流方法、九个基线、四个公开数据集，并辅以消融与用户研究。主要实验如下：\n\n1. 新基准 ShapeR Evaluation Dataset\n\n- 7 段真实室内随手拍序列，178 个完整标注物体（家具到小型工具）。\n- 提供：多视角图像、标定相机、SLAM 点云、机器生成标题、人工精修网格真值。\n- 指标：Chamfer ℓ₂ (CD↓)、Normal Consistency (NC↑)、F1@1% (F1↑)。\n2. 与“多视角→3D”场景重建方法对比\n基线：EFM3D、FoundationStereo+TSDF、DP-Recon、LIRM\n结果：ShapeR CD 2.37×10⁻²，最佳基线 6.48×10⁻²；F1 0.722 → 0.435，显著减少遮挡导致的残缺。\n\n3. 与“单图/多图→3D”生成模型对比\n基线：TripoSG、Direct3DS2、Hunyuan3D-2.0、Amodal3R\n设置：人工挑选最清晰视图 + SAM2 交互分割，单视图输入；ShapeR 全自动多视图。\n用户偏好调研（660 票）：ShapeR 胜率 81–89%。\n\n4. 与“单图→场景布局”方法对比\n基线：MIDI3D（单图）、SceneGen（四视图）\n结果：二者在杂乱场景出现尺度漂移、物体缺失；ShapeR 各物体独立度量重建，布局一致。\n\n5. 跨公开数据集泛化\n\n- ScanNet++ / Replica（6+7 场景，仅召回指标）：ShapeR R↑0.91/0.82 vs DP-Recon 0.45/0.57。\n- Digital Twin Catalog (DTC) Active/Passive（≈100 序列 each）：ShapeR 在更随意的 Passive split 上 CD 0.95×10⁻²，低于 LIRM 1.37×10⁻²；Active split 二者相当（0.94 vs 0.90）。\n6. 消融实验（同一新基准）\n\n- 无 SLAM 点云：CD +90%\n- 无点云增强：CD +38%，几何空洞\n- 无图像增强：CD +43%，受遮挡/错分割影响\n- 无两阶段微调：CD +29%\n- 无 2D 点掩码提示：CD +8%，易混淆相邻物体\n7. 鲁棒性趋势\n按“DTC Active → DTC Passive → ShapeR Eval”难度递增，ShapeR 指标下降平缓，基线方法在最后一档骤降，显示对“随意度”非线性增长的更强鲁棒性。\n\n8. 单目图像→度量 3D 演示\n借助 MapAnything 从单张图像估计度量点云与相机，直接运行 ShapeR 无需重训练，仍可得到带真实尺度的完整网格，验证多模态框架的通用性。\n\n综上，论文通过定量指标、用户偏好、跨数据集迁移与细粒度消融，全面验证了 ShapeR 在“真实杂乱-随意拍摄”条件下的度量重建优势。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为“数据与评测”“模型与算法”“系统与应用”三个层面：\n\n数据与评测\n\n- **真值获取升级**\n- 采用可移动光栅或手持 LiDAR 对同一物体在隔离与原位两种状态下扫描，降低人工对齐误差，提高尺度与细节真值精度。\n- 引入基于物理的材质属性（BSDF、折射率），将“形状真值”扩展为“外观-形状联合真值”，为后续材质联合重建提供基准。\n- **动态与可变形物体**\n- 当前 ShapeR 仅针对刚体；可采集带轻微形变（软包、靠垫、电线）或铰接（折叠椅、笔记本转轴）序列，建立“刚体+形变”混合标注，推动可变形或铰接生成模型研究。\n- **室外与大场景**\n- 将采集设备由室内 AR 眼镜扩展到户外手持 SLAM 或车载平台，引入光照变化、远距离、植被等类别，验证度量生成在 GPS 尺度下的稳定性。\n\n模型与算法\n\n- **端到端联合检测-重建**\n- 目前依赖外部 3D 检测器；可将检测框回归与形状流匹配合并为单一网络，采用“检测即潜变量”思路，减少级联误差并召回漏检物体。\n- **时序/多物体协同生成**\n- 在潜在空间引入跨物体注意力或图神经网络，对同一时刻的多个物体联合去噪，显式建模支撑、包含、相邻等关系，降低“桌子多腿”“椅子靠背粘连”等伪影。\n- **显式不确定度估计**\n- 在流匹配框架中引入随机微分方程或条件扩散，输出每条边的置信度或 SDF 方差，实现“哪里可能出错”的可视化，为后续人机交互或主动视图规划提供依据。\n- **单目深度先验的深度融合**\n- 当前 SLAM 点云稀疏；可将 DUSt3R、Metric3D 等稠密深度先验作为额外条件令牌，与稀疏点云一起输入，提高无纹理区域（白墙、光滑桌面）的完整度。\n- **生成-优化混合微调**\n- 在测试阶段引入可微渲染损失（silhouette、normal、photo-metric）对预测 SDF 进行 5–10 步梯度细化，实现“生成提供先验-优化补齐细节”的两级范式，无需重新训练即可提升细节。\n\n系统与应用\n\n- **主动视图规划**\n- 利用当前不确定度图，实时推荐“下一步最佳相机位姿”，在 AR 眼镜端完成闭环采集，把“随意拍”升级为“引导拍”，以最小用户负担换取最大重建增益。\n- **边缘端轻量化**\n- 采用量化、剪枝或 LORA 微调将 16B 级模型压缩至 1B 以下，结合 TensorRT/Metal 后端，在移动 SoC 上实现 5–10 s 级别的本地物体重建，降低云端依赖。\n- **语义-功能属性绑定**\n- 将生成潜在码与语义部件（椅背、抽屉把手）或物理属性（承重、可动）对齐，支持“生成即编辑”——例如文本指令“把扶手加高 5 cm”直接在潜在空间操作并解码为符合度量约束的新网格。\n- **AR/VR 实时数字孪生**\n- 把 ShapeR 嵌入 Project Aria 的实时 SLAM 管线，实现“边走边重建”的物体级数字孪生，支持遮挡-感知虚实遮挡、物理交互等下游 AR 应用。\n- **失败检测与自动重试**\n- 当检测器召回过低或点云覆盖率 < 20 % 时，系统主动提示用户补充扫描或调整视角；结合上述不确定度，可在客户端完成“质量-覆盖”自检，提升端到端可用率。\n\n通过“更高真值-更强模型-更闭环系统”的螺旋迭代，可逐步把“随手拍→度量形状”从实验室推广到日常消费级 AR 眼镜与移动设备。\n\nQ6: 总结一下论文的主要内容\n\nShapeR 提出一种**从随意拍摄的图像序列中鲁棒生成度量一致、完整三维物体形状**的新范式。核心内容可概括为：\n\n1. 问题定义\n现有物体重建方法依赖干净、无遮挡、已分割输入，真实“随手拍”场景下性能骤降。ShapeR 首次将“杂乱、遮挡、低分辨率、无人工分割”作为正式设定，目标是输出**度量准确、几何完整**的单个物体网格。\n\n2. 技术路线\n\n- **预处理**：视觉-惯性 SLAM 提供稀疏度量点云与相机位姿；3D 实例检测器提取物体框；VLM 生成标题；无需 2D 分割。\n- **生成模型**：采用**多模态 Rectified Flow Transformer**，以稀疏 3D 点、多视角图像、2D 点投影掩码、文本为条件，去噪 VecSet 潜在表示并解码为 SDF，再用 Marching Cubes 提取网格。\n- **训练策略**：两阶段课程学习——先在大规模孤立物体数据上做**组合增强**预训练，再在合成场景数据上微调，提升对真实遮挡与组合的鲁棒性。\n- **推理**：全程自动，输出网格按输入点云尺度反归一化，保证真实世界尺寸。\n3. 实验验证\n\n- 新建 **178 物体 in-the-wild 基准**（7 场景），含完整真值。\n- 对比 9 个基线（多视角融合、单图生成、场景布局三类），ShapeR **Chamfer 距离降低 2.7×**，用户偏好率 >80%。\n- 跨 ScanNet++、Replica、DTC 数据集一致领先；消融显示 SLAM 点、增强、两阶段训练、2D 点掩码均为关键组件。\n4. 贡献总结\n\n- 提出面向随意拍摄的**多模态度量形状生成框架**。\n- 设计**两阶段课程+组合增强**训练流程，实现无需分割的鲁棒重建。\n- 发布新基准与代码/模型权重，推动真实场景下**度量物体重建**研究。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ShapeR%3A%20Robust%20Conditional%203D%20Shape%20Generation%20from%20Casual%20Captures，链接是https://arxiv.org/pdf/2601.11514，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11514。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1768866682139","title":"Reasoning Models Generate Societies of Thought","description":"Large language models have achieved remarkable capabilities across domains, yet mechanisms underlying sophisticated reasoning remain elusive. Recent reasoning models outperform comparable instruction-tuned models on complex cognitive tasks, attributed to extended computation through longer chains of thought. Here we show that enhanced reasoning emerges not from extended computation alone, but from simulating multi-agent-like interactions -- a society of thought -- which enables diversification and debate among internal cognitive perspectives characterized by distinct personality traits and domain expertise. Through quantitative analysis and mechanistic interpretability methods applied to reasoning traces, we find that reasoning models like DeepSeek-R1 and QwQ-32B exhibit much greater perspective diversity than instruction-tuned models, activating broader conflict between heterogeneous personality- and expertise-related features during reasoning. This multi-agent structure manifests in conversational behaviors, including question-answering, perspective shifts, and the reconciliation of conflicting views, and in socio-emotional roles that characterize sharp back-and-forth conversations, together accounting for the accuracy advantage in reasoning tasks. Controlled reinforcement learning experiments reveal that base models increase conversational behaviors when rewarded solely for reasoning accuracy, and fine-tuning models with conversational scaffolding accelerates reasoning improvement over base models. These findings indicate that the social organization of thought enables effective exploration of solution spaces. We suggest that reasoning models establish a computational parallel to collective intelligence in human groups, where diversity enables superior problem-solving when systematically structured, which suggests new opportunities for agent organization to harness the wisdom of crowds.","url":"https://huggingface.co/papers/2601.10825","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.139Z","metadata":{"authors":["Junsol Kim","Shiyang Lai","Nino Scherrer","Blaise Agüera y Arcas","James Evans"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.CL","cs.CY","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2601.10825.pdf","arxivUrl":"https://arxiv.org/abs/2601.10825","arxivId":"2601.10825","coolPaperUrl":"https://papers.cool/arxiv/2601.10825","published":"2026-01-15T19:52:33Z","updated":"2026-01-15T19:52:33.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"LLM Analysis Failed: Error: 抓取失败（已重试2次）: Waiting failed: 30000ms exceeded"}},{"id":"hf-13-1768866682139","title":"PhysRVG: Physics-Aware Unified Reinforcement Learning for Video Generative Models","description":"Physical principles are fundamental to realistic visual simulation, but remain a significant oversight in transformer-based video generation. This gap highlights a critical limitation in rendering rigid body motion, a core tenet of classical mechanics. While computer graphics and physics-based simulators can easily model such collisions using Newton formulas, modern pretrain-finetune paradigms discard the concept of object rigidity during pixel-level global denoising. Even perfectly correct mathematical constraints are treated as suboptimal solutions (i.e., conditions) during model optimization in post-training, fundamentally limiting the physical realism of generated videos. Motivated by these considerations, we introduce, for the first time, a physics-aware reinforcement learning paradigm for video generation models that enforces physical collision rules directly in high-dimensional spaces, ensuring the physics knowledge is strictly applied rather than treated as conditions. Subsequently, we extend this paradigm to a unified framework, termed Mimicry-Discovery Cycle (MDcycle), which allows substantial fine-tuning while fully preserving the model's ability to leverage physics-grounded feedback. To validate our approach, we construct new benchmark PhysRVGBench and perform extensive qualitative and quantitative experiments to thoroughly assess its effectiveness.","url":"https://huggingface.co/papers/2601.11087","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.139Z","metadata":{"authors":["Qiyuan Zhang","Biao Gong","Shuai Tan","Zheng Zhang","Yujun Shen","Xing Zhu","Yuyuan Li","Kelu Yao","Chunhua Shen","Changqing Zou"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2601.11087.pdf","arxivUrl":"https://arxiv.org/abs/2601.11087","arxivId":"2601.11087","coolPaperUrl":"https://papers.cool/arxiv/2601.11087","published":"2026-01-16T08:40:10Z","updated":"2026-01-16T08:40:10.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有基于 Transformer 的视频生成模型在**刚体运动（rigid-body motion）**场景下**物理真实感缺失**的核心问题。具体而言：\n\n1. 现有预训练-微调范式仅关注像素级重建与感知质量，将物理定律视为“软约束”，导致：\n\n- 碰撞轨迹不稳定、不可信；\n- 物体在碰撞后出现撕裂、重叠或颜色突变等违背牛顿力学现象；\n- 时间一致性差，缺乏对速度、加速度等物理量的显式建模。\n2. 传统图形学或物理引擎虽能精确模拟刚体动力学，但难以与大规模数据驱动的生成模型无缝融合。\n\n3. 因此，作者首次提出**物理感知的强化学习框架 PhysRVG**，直接在**高维潜空间**中把**可验证的物理碰撞规则**作为**硬约束**进行优化，而非仅作为条件或损失项。通过引入 Mimicry-Discovery Cycle（MDcycle）统一策略，实现：\n\n- 在训练早期利用 Flow Matching 提供稳定像素级监督（Mimicry）；\n- 在训练后期通过 GRPO 强化学习注入物理知识（Discovery）；\n- 循环交替，保证大参数更新同时不破坏物理一致性。\n4. 为定量评估，作者构建了**PhysRVGBench** 刚体运动基准，用 IoU 与 Trajectory Offset 直接度量生成轨迹与真实轨迹的偏差，验证模型对碰撞、摆锤、自由落体、滚动等四种基本刚体运动的物理真实感。\n\n综上，论文核心贡献是**将“物理正确性”从可选条件转变为强化学习中的可验证奖励信号**，从而显著提升视频生成模型在刚体运动场景下的物理真实感与泛化能力。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中系统梳理了与 PhysRVG 密切相关的三大研究脉络，并在实验部分与最具代表性的方法进行了对比。可归纳为以下四类：\n\n1\\. 物理感知视频生成（Physics-Aware Video Generation）\n\n| 子类 | 代表工作 | 核心思路 | 与 PhysRVG 的区别 |\n| --- | --- | --- | --- |\n| 显式条件注入 | PhysGen, GPT4Motion, NewtonGen, PhysAnimator | 先用仿真器（Blender、Bullet 等）生成轨迹、深度或光流，再作为条件输入扩散模型。 | 依赖外部仿真器质量，物理知识仅作为输入提示，训练目标仍是像素重建；PhysRVG 把物理一致性写成可验证奖励，直接在潜空间优化。 |\n| 数据增广 | Wisa, Pisa | 构造含物理现象的专用数据集，用 MoE 或继续训练让模型“看”更多物理视频。 | 能力受限于数据分布，无法保证未见运动模式仍物理正确；PhysRVG 用 RL 探索并发现新物理组合。 |\n| 反馈式优化 | PhyT2V, PhysMaster | 用 MLLM 或人工打分做偏好排序，再跑 DPO/RLHF。 | 反馈信号主观、粗粒度；PhysRVG 提出Trajectory Offset + 碰撞检测的客观物理度量，与 GRPO 结合实现细粒度、可验证奖励。 |\n\n2\\. 强化学习用于生成模型（RL for Generative Model）\n\n| 方法 | 适用模型 | 关键机制 | PhysRVG 的借鉴与改进 |\n| --- | --- | --- | --- |\n| DDPO | 扩散模型 | 将去噪步视为 MDP，用 PPO 优化。 | —— |\n| Flow-GRPO / DanceGRPO / MixGRPO / TempFlowGRPO / G2RPO | 基于 Flow-Matching 的 Transformer | 把 ODE 采样改写成 SDE，用 GRPO 做组内相对优势估计，省去价值网络。 | PhysRVG 直接采用 GRPO 的高效组内归一化优势，但**首次将物理可验证奖励（TO & 碰撞）**引入视频生成，解决稀疏奖励与弱监督问题。 |\n\n3\\. 经典图形学/物理仿真（Traditional Physics Simulators）\n\n- Material Point Method (MPM)、Position-Based Dynamics (PBD)、XPBD 等。\n- 这些工作提供**精确刚体动力学解算**，但需显式建模几何、材质与力；PhysRVG 目标是**让数据驱动的神经网络自发习得相同物理行为**，而非调用外部求解器。\n\n4\\. 视频生成基准（Benchmarks）\n\n| 基准 | 评估维度 | 是否含刚体物理量化指标 |\n| --- | --- | --- |\n| VBench | 通用视觉质量、时序闪烁等 | ❌ |\n| VideoPhy-2 | 物理常识多项选择题 | ❌（仅语义标签） |\n| PhysRVGBench（本文） | 碰撞/摆锤/自由落体/滚动 | ✅ IoU + Trajectory Offset，直接度量轨迹偏差与空间重叠。 |\n\n一句话总结\n\nPhysRVG 在相关研究谱系中**首次把“可验证的牛顿力学约束”写成强化学习奖励**，突破了以往“物理作为条件/数据/主观反馈”的局限，实现了**无仿真器、可扩展、真正物理一致**的视频生成。\n\nQ3: 论文如何解决这个问题？\n\n论文将“生成物理上可信的刚体运动视频”形式化为**可验证的强化学习任务**，通过三项关键设计把物理定律直接写进训练目标，而非仅作为条件或数据增广：\n\n1\\. 物理可验证任务定义（Physics-Grounded Task Definition）\n\n- 只关注**刚体运动**——碰撞、摆锤、自由落体、滚动——因其满足\n**可观测性**（坐标可逐帧测量）与**确定性**（牛顿方程唯一决定轨迹）。\n- 给定初始  T_(obs)  帧与文本，要求模型生成后续  T_(pred)  帧，使得**物体中心轨迹**与真实轨迹的偏差可被**定量验证**。\n\n2\\. 物理感知奖励函数（Physics-Grounded Metric）\n\n用**两个可自动计算的客观量**取代主观打分：\n\n① Trajectory Offset (TO)\n\nO=(1) / (NT)∑_(s=1)^(N)∑_(t=T_obs)^(T)l|p_(t,s)^(gt)-p_(t,s)^(sample)r|_2\n\n- 利用 SAM2 从首帧人工标注点追踪整段视频，提取物体中心坐标。\n- 逐帧计算与真值坐标之欧氏距离，平均后得到 TO；**越小越物理准确**。\n\n② Collision-Aware Reweighting\n\n- 对轨迹求速度  v_t ，再求加速度  a_t ；用 SciPy 检测加速度**突增点**即为碰撞帧  C 。\n- 赋予碰撞帧及邻域更高权重： \nw_t= w_(col) & t∈ C[2pt] w_(adj) & t∈ C_(adj)[2pt] w & otherwise\n\n- 加权后的最终奖励为\n\nR = -O_c,quad O_c=(1) / (NT)∑_(s=1)^(N)∑_(t=T_obs)^(T)w_tl|p_(t,s)^(gt)-p_(t,s)^(sample)r|_2^2\n\n**直接最大化  R  即最小化物理偏差**，彻底避免“奖励黑客”偏好简单线性运动。\n\n3\\. Mimicry-Discovery Cycle (MDcycle) 统一训练框架\n\n把**Flow Matching 的稳定监督**与**GRPO 的高效探索**放在同一个梯度步骤里自适应切换：\n\n- 对每个条件采样  G=20  条视频，计算组平均  O_c 。\n- 若  O_c>Threshold （模型尚不能生成可信轨迹）→**Mimicry 分支**：\n额外加入 Flow Matching 像素损失  L_(M) ，提供细粒度监督，防止早期崩溃。\n- 否则→**Discovery 分支**：\n仅用 GRPO 最大化物理奖励  R ，在 SDE 采样窗口内注入可控噪声  σ_t=1.0 ，鼓励探索碰撞等复杂动力学。\n- 两分支损失统一写为\n\nL = L_(D) + α L_(M),quad α=1_(O)_c>Threshold\n\n**一次反向传播同时完成“视觉保真”与“物理正确”更新**。\n\n4\\. 两阶段训练管线（Two-Stage Pipeline）\n\n1. **Stage-1**：用 Flow Matching 把预训练 I2V 模型（Wan2.2 5B）微调为 V2V，输入 5 帧上下文，输出后续帧——**只学视觉连贯性**。\n2. **Stage-2**：冻结大部分参数，采用类 LoRA 的低秩更新，在高质量刚体数据上跑 MDcycle——**只学物理一致性**，250 步即可收敛。\n\n5\\. 客观评测基准（PhysRVGBench）\n\n- 700 段视频，人工标注首帧物体中心，SAM2 自动追踪。\n- 指标：\n- **IoU**——衡量预测与真值空间掩码重叠；\n- **TO**——衡量轨迹绝对偏差。\n两者均**无需人工打分**，直接反映物理真实度。\n\n效果\n\n- 在 PhysRVGBench 上 TO 从 162.78 ↓ 至 15.03，IoU 从 0.15 ↑ 至 0.64；\n- 在 VBench、VideoPhy-2 依然保持 SOTA 视觉质量；\n- 可视化显示碰撞不再撕裂/重叠，摆锤、滚动轨迹与牛顿力学预测高度吻合。\n\n一句话总结\n\nPhysRVG 通过\\*\\*“可验证物理奖励 + 自适应混合训练”**，首次把牛顿刚体定律写进高维潜空间的强化学习目标，使视频生成模型**无需外部仿真器即可自发产生物理正确且视觉连贯的刚体运动\\*\\*。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“物理真实感”与“视觉质量”两个维度，共设计了**五组实验**，覆盖定量对比、消融分析、超参数敏感性、可视化失败案例及伦理讨论，具体如下：\n\n1\\. 主实验：与现有 SOTA 方法的全面对比\n\n**数据集**\n\n- VBench（通用视觉质量）\n- VideoPhy-2（物理常识选择题）\n- 自建的 **PhysRVGBench**（刚体运动轨迹 IoU / TO）\n\n**对比方法**\n\n- I2V 模型：Wan2.2-5B、Wan2.2-14B、Kling2.5、CogVideoX、HunyuanVideo\n- V2V 模型：Magi-1\n\n**结果**\n\n| 指标 | PhysRVG 相对最佳基线的提升 |\n| --- | --- |\n| VBench 总分 | 78.89 → +1.12 |\n| VideoPhy-2 SA | 0.70 → +0.06 |\n| VideoPhy-2 PC | 0.41 → +0.03 |\n| PhysRVGBench IoU ↑ | 0.27 → +0.37（×2.4） |\n| PhysRVGBench TO ↓ | 113.42 → 15.03（×7.5） |\n\n**结论**：在**保持视觉质量不降**的前提下，刚体轨迹误差大幅降低，物理真实性显著优于所有现有模型。\n\n2\\. 消融实验：验证 MDcycle 与碰撞感知奖励的必要性\n\n| 训练策略 | IoU ↑ | TO ↓ |\n| --- | --- | --- |\n| Baseline（仅 FT） | 0.15 | 162.78 |\n| +LoRA | 0.41 | 48.60 |\n| +Full FT | 0.38 | 46.27 |\n| +FT+RL（无 MD） | 0.61 | 17.25 |\n| +FT+MDcycle | 0.64 | 15.03 |\n\n额外给出 **reward 曲线** 与 **指标分布直方图**：\n\n- 纯 RL 前期震荡且收敛值低；MDcycle 稳定上升，最终 reward 高 25%。\n- MDcycle 使 TO∈\n0,5\n 区间样本比例从 18 % → 54 %，IoU∈\n0.8,1.0\n 从 20 % → 48 %。\n\n3\\. 超参数敏感性实验\n\n在相同 GPU-hour 预算下逐一扫描：\n\n| 超参 | 搜索范围 | 最佳值 | 性能变化 |\n| --- | --- | --- | --- |\n| SDE 窗口 | 0–100 % | 75 %–100 % | TO ↓ 12.2 |\n| 噪声强度 σt | 0.2–1.4 | 1.0 | IoU ↑ 0.03 |\n| Threshold | 4–12 | 8 | TO ↓ 7.36 |\n| Stage-1 FT 步数 | 6 k–30 k | 16 k | 再增步数无收益 |\n\n**结论**：16 k FT + 250 MDcycle 步即可达到饱和，继续增加 FT 或 RL 步数对物理指标无额外提升。\n\n4\\. 定性可视化与失败案例分析\n\n- **图 5** 给出 7 组场景最后一帧对比：\n– 基线出现**物体撕裂、颜色突变、意外人物、多米诺穿透**；\n– PhysRVG 碰撞后轨迹、摆锤周期、滚动加减速均与真值一致。\n- **图 14** 展示失败案例：\n– 碰撞后物体颜色改变、多余球体突然出现；\n– 说明奖励仅监督**轨迹中心**，未约束外观、形状或额外实例，提示未来需多尺度物理评价。\n\n5\\. 伦理与局限性讨论\n\n- **潜在滥用**：可合成逼真交通事故或虚假证据。\n- **缓解措施**：\n– 模型发布时附带“研究用途”标签与详细文档；\n– 鼓励结合水印与内容溯源技术。\n\n一句话总结\n\n实验从**量化指标、消融、超参、可视化、伦理**五方面系统验证：\nPhysRVG 在**不牺牲视觉质量**的前提下，把刚体轨迹误差降到现有方法的 **1/7**，且训练成本仅 **250 步 RL + 16 k 步 FT** 即可完成。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 PhysRVG 框架的自然延伸，亦是目前实验或讨论中尚未充分解决的开放问题。为便于后续研究者快速切入，按“理论-数据-模型-系统-伦理”五个层面列出可探索要点。\n\n1\\. 理论层面：物理约束的完备性与可验证性\n\n- **非刚体与耦合现象**\n- 软体、流体、可变形对象需引入连续介质力学或 Navier-Stokes 守恒律；奖励函数需从“质点轨迹”升级为“场量守恒（质量、动量、能量）”。\n- **多物体/多材质接触**\n- 现有碰撞检测仅基于加速度突变；可引入恢复系数、摩擦系数等可测常量，验证能量守恒与动量守恒是否同时满足。\n- **可证明的物理层**\n- 借鉴 Neural ODE 或 Hamiltonian Network，在潜空间显式参数化守恒量，使生成过程满足**可微分物理方程**，而非仅事后奖励。\n\n2\\. 数据层面：更丰富、更难的物理场景\n\n- **自动物理标注**\n- 目前首帧坐标人工标注；可探索基于事件相机、IMU 或合成数据自动提取 6-DoF 姿态与速度，实现百万级刚体视频的快速标注。\n- **多模态物理基准**\n- 扩展 PhysRVGBench 到 3D+T（NeRF/GAUSSIAN）表示，支持立体碰撞、陀螺进动、斜抛空气阻力等中学-大学经典力学案例。\n- **分布外（OOD）压力测试**\n- 系统评估模型在**不同重力、摩擦、弹性系数**下的泛化边界，建立“物理鲁棒性-扰动曲线”。\n\n3\\. 模型层面：训练效率与架构升级\n\n- **自适应奖励组合**\n- 目前 wcol,wadj,w 为手工超参；可用元学习或演化搜索，让模型在训练过程**自动调整奖励权重**，实现任务特定最优物理 trade-off。\n- **多尺度物理奖励**\n- 将颜色恒常、形变一致、阴影连续等视觉物理先验纳入分层奖励，解决“轨迹对但颜色/阴影错”的失败案例。\n- **全参数 RL 的稳定性**\n- 论文因高维不稳定而采用类 LoRA 更新；未来可探索\n– 更大批次 + 分布式 GRPO，\n– 基于策略蒸馏的“教师-学生”渐进式全参数更新，\n– 或引入 Kullback-Leiber 约束的镜像下降，实现真正端到端物理大模型。\n\n4\\. 系统层面：实时交互与下游应用\n\n- **实时物理视频编辑**\n- 将 PhysRVG 蒸馏为 1-2 步潜空间预测器，支持用户实时修改初速度/角度并即时生成符合物理的视频。\n- **闭环机器人仿真**\n- 与 Isaac Gym、MuJoCo 对比，评估生成视频作为**sim-to-real**视觉预训练数据的可行性，衡量对真实策略迁移的增益。\n- **神经-符号混合管线**\n- 先生成物理正确但低分辨率视频，再用超分/NeRF 细化外观，实现“物理管形状 + 视觉管外观”的模块化生成。\n\n5\\. 伦理与评测：可信生成与自动检测\n\n- **物理一致性水印**\n- 在训练阶段嵌入不可见但可验证的物理签名（如守恒量哈希），供后续自动检测是否为合成视频。\n- **物理伪造检测器**\n- 反向利用 TO/IoU 指标，训练判别模型自动发现“轨迹轻微违背物理”的 deepfake，为平台提供低成本审核工具。\n- **公平与偏见审计**\n- 检查模型是否在特定种族、性别、文化场景下出现**物理规律双标**（如不同肤色人物碰撞后加速度分布不一致），确保物理公平性。\n\n可立即着手的小课题（≤ 3 个月）\n\n1. 在 PhysRVGBench 上增加“斜抛+空气阻力”类别，验证奖励加入 **|a\\_t| ≈ g - kv\\_t²** 约束是否进一步提升轨迹精度。\n2. 将 MDcycle 的 Threshold 改为 **在线滑动平均** 自动更新，观察是否减少人工调参且收敛曲线更平滑。\n3. 对比实验：用相同预算把 PPO 替换 GRPO，统计训练时间与最终 TO，验证 GRPO 在视频生成维度是否仍保持优势。\n\n一句话展望\n\nPhysRVG 把“物理正确”从软约束变成可验证奖励，但当前仅触及**刚体+加速度守恒**的冰山一角；向**非刚体、多守恒律、自适应奖励、实时交互与伦理检测**拓展，将有望构建真正“可信任、可解释、可交互”的物理世界生成器。\n\nQ6: 总结一下论文的主要内容\n\n**PhysRVG 论文核心内容一览**\n\n1\\. 问题\n\n- 现有 Transformer 视频生成模型只优化像素重建，**无视刚体物理定律**，导致碰撞撕裂、轨迹漂移、时间不一致。\n- 传统“物理条件”或“增广数据”把物理当软提示，**无法保证高维潜空间一定遵守牛顿力学**。\n\n2\\. 目标\n\n**首次**将“物理正确”变成**可验证、可优化的强化学习目标**，让模型在生成视频时**自发遵守刚体动力学**。\n\n3\\. 方法\n\n| 模块 | 关键公式 / 机制 | 作用 |\n| --- | --- | --- |\n| Physics-Grounded Metric |  R = -O_c ， O_c=(1) / (NT)∑ w_t|p_t^(gt)-p_t^(sample)|_2^2  | 用 SAM2 追踪物体中心，碰撞帧加权，客观度量轨迹偏差。 |\n| MDcycle |  Oc>Threshold⇒ L=LD+L_(M) ，否则  L=L_(D)  | 早期 Flow Matching 稳定像素，后期 GRPO 探索物理，循环切换。 |\n| 两阶段训练 | Stage-1：FT 变 I2V→V2V；Stage-2：LoRA 式小参数跑 MDcycle | 16 k FT + 250 RL 步即可收敛，训练高效。 |\n\n4\\. 实验\n\n- **基准**：自建 PhysRVGBench（700 段碰撞/摆锤/自由落体/滚动，手工首帧坐标 + SAM2 追踪）。\n- **指标**：IoU（空间重叠）+ TO（轨迹偏差）。\n- **结果**：TO 从 162.78 ↓ 到 **15.03**（×7 提升），IoU 从 0.15 ↑ 到 **0.64**；VBench、VideoPhy-2 视觉质量仍保持 SOTA。\n- **消融**：MDcycle 比纯 RL 稳定、样本质量高；碰撞加权消除“避碰”奖励黑客。\n\n5\\. 贡献一句话\n\nPhysRVG 把**牛顿刚体定律**写进**高维潜空间的强化学习奖励**，用**Mimicry-Discovery Cycle**在 250 步内让视频生成模型**既好看又物理正确**。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PhysRVG%3A%20Physics-Aware%20Unified%20Reinforcement%20Learning%20for%20Video%20Generative%20Models，链接是https://arxiv.org/pdf/2601.11087，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11087。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1768866682140","title":"PersonalAlign: Hierarchical Implicit Intent Alignment for Personalized GUI Agent with Long-Term User-Centric Records","description":"While GUI agents have shown strong performance under explicit and completion instructions, real-world deployment requires aligning with users' more complex implicit intents. In this work, we highlight Hierarchical Implicit Intent Alignment for Personalized GUI Agent (PersonalAlign), a new agent task that requires agents to leverage long-term user records as persistent context to resolve omitted preferences in vague instructions and anticipate latent routines by user state for proactive assistance. To facilitate this study, we introduce AndroidIntent, a benchmark designed to evaluate agents' ability in resolving vague instructions and providing proactive suggestions through reasoning over long-term user records. We annotated 775 user-specific preferences and 215 routines from 20k long-term records across different users for evaluation. Furthermore, we introduce Hierarchical Intent Memory Agent (HIM-Agent), which maintains a continuously updating personal memory and hierarchically organizes user preferences and routines for personalization. Finally, we evaluate a range of GUI agents on AndroidIntent, including GPT-5, Qwen3-VL, and UI-TARS, further results show that HIM-Agent significantly improves both execution and proactive performance by 15.7% and 7.3%.","url":"https://huggingface.co/papers/2601.09636","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.140Z","metadata":{"authors":["Yibo Lyu","Gongwei Chen","Rui Shao","Weili Guan","Liqiang Nie"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.AI","cs.CV","cs.HC","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2601.09636.pdf","arxivUrl":"https://arxiv.org/abs/2601.09636","arxivId":"2601.09636","coolPaperUrl":"https://papers.cool/arxiv/2601.09636","published":"2026-01-14T17:12:48Z","updated":"2026-01-14T17:12:48.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**个性化 GUI 智能体在真实部署场景下无法对齐用户层级化隐式意图**的问题。现有方法普遍假设用户指令完整、明确，而日常交互中用户常省略重复出现的偏好或例行信息，导致指令模糊甚至缺失。为此，作者提出：\n\n- **PersonalAlign 任务**：要求智能体利用长期用户记录，对**偏好意图**（补齐被省略的偏好细节）与**例行意图**（在无指令时基于用户状态主动预测）进行层级化隐式意图对齐。\n- **AndroidIntent 基准**：从 20 k 条真实用户日志中标注 775 条偏好、215 条例行，用于评估隐式意图对齐能力。\n- **HIM-Agent 记忆框架**：以流式聚合模块持续更新个人记忆，并通过执行偏好过滤器与状态例行过滤器，将记忆层级化为偏好记忆与例行记忆，支撑个性化执行与主动建议。\n\n实验表明，HIM-Agent 在 AndroidIntent 上将执行指标 CER 提升 15.7%，主动指标 F1 提升 7.3%，显著优于现有 GUI 智能体。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为两条主线：**个性化 GUI 智能体**与**智能体记忆机制**。代表性工作如下：\n\n个性化 GUI 智能体\n\n- **执行侧个性化**\n- IFRAgent（Wu et al., 2025）从短期指令轨迹显式/隐式意图流。\n- FingerTip（Yang et al., 2025b）用近期历史动作微调当前执行。\n- **主动侧个性化**\n- ProactiveAgent（Lu et al., 2025b）监控环境与用户活动生成主动行为。\n- ContextAgent（Yang et al., 2025a）利用真实传感器推断用户状态。\n- **联合范式**\n上述工作将“偏好执行”与“主动建议”视为独立任务，未在**长期、用户中心**视角下统一处理层级化隐式意图。\n\n智能体记忆\n\n- **对话型记忆**\n- MemAgent、Mem1（Yu et al., 2025; Zhou et al., 2025b）渐进压缩对话延长上下文。\n- MemoryR1（Yan et al., 2025）用 RL 训练智能体组织记忆。\n- **GUI 专用记忆**\n- Mirage-1（Xie et al., 2025b）构建“执行技能记忆”提升跨任务泛化。\n- Memory OS（Kang et al., 2025）将用户对话分段为短/中/长期模块检索。\n- **用户画像**\n- LettinGo（Wang et al., 2025b）用 LLM 生成自适应画像。\n- PersonaX（Shi et al., 2025）聚类历史记录建模用户画像。\n\n**共同局限**：仅依赖语义相似度，未按**执行轨迹**与**用户状态**层级化区分“一次性-偏好-例行”意图，难以支撑 PersonalAlign 的隐式对齐需求。\n\nQ3: 论文如何解决这个问题？\n\n论文将“层级化隐式意图对齐”拆解为**数据、任务、模型**三环节，逐一解决：\n\n1. 提出 **PersonalAlign 任务范式**\n形式化三种对齐层级：\n\n- Reactive： A_tarrow f_θ(I_t;{}) \n- Preference： A_tarrow f_θ(hat I_t;(I_i,A_i)∈ H) \n- Routine： I'_tarrow f_θ(I_varnothing;(I_i,T_i,S_i)∈ H) \n强制智能体利用长期记录  H  补齐被省略偏好，并在无指令时基于当前状态  (T_t,S_t)  主动生成建议。\n2. 构建 **AndroidIntent 基准**\n对 20 k 条真实日志采用“量化过滤→人工验证”两阶段策略：\n\n- 量化分数  Q_(score)=S_(cos)+Delta H_t+Delta H_s  筛选偏好/例行候选。\n- 人工交叉验证，最终标注 775 偏好、215 例行，并生成对应模糊指令与负例状态，供离线评估。\n3. 设计 **HIM-Agent 记忆框架**\n\n- **Streaming Aggregation Module**\n以天为单位增量聚类，维护 Record Prototype：\n P_i=R_hmid S_(consist)(R_h,P_i)>θ ，抑制长尾与漂移。\n- **Execution-based Preference Filter**\n联合语义相似度  S_(sim)=S_(cos)+S_(Jac)  与动作轨迹 DTW 距离：\n S_(consist)=S_(sim)+S_(action) ，将稳定原型写入 **Preference Intent Memory**。\n- **State-based Routine Filter**\n计算 proactive 置信度：\n Phi(P_i)=H_(state)+L_(record)+R_(consist) ，若  Phi>threshold  则升入 **Routine Intent Memory**。\n\n推理时，智能体按需检索两级记忆，完成偏好补全或状态触发的主动建议，实现端到端层级化隐式意图对齐。\n\nQ4: 论文做了哪些实验？\n\n实验围绕 **AndroidIntent** 的两大子任务展开：\n\n1. 模糊指令下的**执行对齐**（Preference Intent）\n2. 无指令时的**主动建议**（Routine Intent）\n\n共设计 4 组实验，覆盖 6 个主流 GUI 智能体与 3 类基线方法，指标与结论如下。\n\n1 模糊指令冲击分析（执行侧）\n\n- **设置**\n同一批轨迹分别在「完整指令」与「模糊指令」条件下测试，度量 Type Accuracy、Step-wise Success Rate (SSR)、**Cumulative Error Rate (CER)**。\n\n- **结果**（表 2）\n- 模糊指令使 SSR ↓20 %、CER ↓45 %，Type 仅 ↓3 %，证实「粗粒度目标可识别，细粒度偏好缺失致失败」。\n- 所有模型 CER 均 <43 %，说明现有智能体普遍缺乏个性化补全能力。\n\n2 主动建议能力评估（例行侧）\n\n- **设置**\n给定 215 个正例状态与 100 个人工构造的负例状态，输出是否建议及建议文本。\n指标：Intent Alignment（语义相似度+LLM-as-Judge）、Identification Alignment（Precision / Recall / False-Alarm / F1）。\n\n- **结果**（表 3）\n- 除 GPT-5.1 外，其余模型 False-Alarm >60 %，Recall 虽高但严重过触发。\n- 证明「平衡误报与召回」是当前智能体的共性瓶颈。\n\n3 方法对比（主实验）\n\n3.1 执行对齐\n\n- **基线**\n– Retrieve-based：Recent（最近 5 条）、Top-k 相似检索\n– Generalized-based：LLM-UM（让 GPT-5.1 先总结用户画像再执行）\n\n- **结果**（表 4）\nHIM-Agent 在 Qwen3-VL 骨架上取得 **CER 42.3**（↑15.7 %），SSR 24.0（↑16.5 %），显著优于两类基线。\n\n3.2 主动建议\n\n- **结果**（表 5）\nHIM-Agent 达到 **F1 79.7 %**，False-Alarm 降至 49 %（↓13 %），Semantic Alignment 53.5 %（↑4.1 %），均优于 Retrieve 与 LLM-UM，且 token 消耗仅 1605，远低于 LLM-UM 的 6518。\n\n4 消融与超分分析\n\n- **Execution-based Preference Filter 消融**（表 6）\n依次移除 Dense、Sparse、Action 组件，CER 从 42.3→37.3→33.2，证实三组件均贡献显著。\n\n- **State-based Routine Filter 消融**（图 5）\n移除时间或场景因子后 False-Alarm 回升至 68 %–70 %，说明「状态一致性」是抑制过触发的关键。\n\n5 案例可视化（图 6）\n\n- 左：模糊指令「Send the email」→ HIM-Agent 补全「用 EmailA、发送给常用联系人」等缺失偏好，轨迹完全对齐。\n- 右：无指令状态「12:00 & Home」→ HIM-Agent 主动询问是否帮忙点麦当劳单人餐，符合用户历史例行。\n\n综上，实验从「冲击诊断→能力评估→方法对比→内部消融→直观案例」五个维度系统验证：\n\n- 隐式意图对齐是真实瓶颈；\n- HIM-Agent 在执行与主动双任务上均取得显著且高效的提升。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可延续 PersonalAlign 框架，进一步突破数据、模型与系统层面的局限。\n\n1 数据与隐私\n\n- **跨平台长期日志**\n将 AndroidIntent 的构建流程迁移至桌面、车载、IoT 等 GUI 环境，验证层级化意图对齐的通用性。\n- **隐私保护个性化**\n探索联邦原型更新、本地差分隐私或同态 embedding，解决“云端聚合”与“用户隐私”矛盾。\n- **冷启动模拟**\n设计生成式用户模拟器（User-Agent），在缺乏真实历史时合成可信交互轨迹，缓解冷启动。\n\n2 意图建模\n\n- **多粒度意图分解**\n将一条指令拆分为「任务-子目标-动作」三层，每层独立维护偏好与例行，实现更细粒度对齐。\n- **动态意图漂移检测**\n引入变化点检测或在线聚类，实时识别用户偏好/例行迁移，自动淘汰过时原型。\n- **跨用户迁移**\n利用元学习或域适应，把相似用户的原型作为先验，加速新用户个性化。\n\n3 记忆机制\n\n- **层次化时间窗**\n在「日-周-月」多时间粒度维护独立原型，支持长期例行与短期热点的并行推理。\n- **多模态状态融合**\n将传感器（GPS、蓝牙、加速度计）与应用日志联合嵌入，丰富状态表示  S_t ，提升 proactive 置信度  Phi 。\n- **可解释记忆读写**\n为每次检索提供「原型 ID + 相似度 + 状态匹配度」的可解释摘要，增强用户信任与可控性。\n\n4 训练与评测\n\n- **在线/闭环评估**\n搭建真实设备集群，通过 ADB 自动下发动作并调用 LLM-as-Judge 实时判定成功率，缩短离线-在线差距。\n- **用户满意度信号**\n收集显式反馈（点击/拒绝/评分）作为强化奖励，用 RL 微调 proactive 阈值与策略网络。\n- **多任务统一训练**\n将 Preference、Routine、Reactive 三任务构建为统一的多目标损失，共享视觉-语言主干，降低推理成本。\n\n5 应用与系统\n\n- **Proactive Executing**\n在锁屏、车载、可穿戴等场景实现“零触控”自动执行（如打开导航、打卡），需配套安全沙箱与撤销机制。\n- **个性化 Rewriting 即服务**\n把 HIM-Agent 作为云端 API，向第三方 App 提供「模糊指令→个性化指令」重写能力，形成生态。\n- **边缘-云协同部署**\n轻量级原型检索放端侧，复杂推理放云端，实现低延迟与强性能的平衡。\n\n综上，未来工作可从**数据扩域、意图细化、记忆进化、在线评测、系统落地**五个维度深入，推动个性化 GUI 智能体走向真实、全场景、可信赖的“Jarvis 级”体验。\n\nQ6: 总结一下论文的主要内容\n\n**论文核心内容速览**\n\n| 模块 | 关键贡献 | 一句话总结 |\n| --- | --- | --- |\n| 问题 | 现有 GUI 智能体默认指令完整，无法对齐真实场景中被省略的偏好与例行。 | 提出“层级化隐式意图对齐”新任务。 |\n| 任务 | PersonalAlign：利用长期记录，对模糊指令补全偏好，对空指令主动预测例行。 | 形式化 Preference vs. Routine 两层对齐目标。 |\n| 数据 | AndroidIntent：91 用户 20 k 日志→775 偏好+215 例行，含模糊指令与负例状态。 | 首次提供“用户中心、长期、已标注”个性化 GUI 基准。 |\n| 模型 | HIM-Agent：流式原型聚合 → 执行偏好过滤器 → 状态例行过滤器，分层记忆。 | 用“执行轨迹+用户状态”双信号，持续更新并层级检索个人意图。 |\n| 实验 | 6 个主流 GUI 智能体对比，离线+消融+案例。 | HIM-Agent 执行 CER ↑15.7%，主动 F1 ↑7.3%，显著优于检索与画像基线。 |\n\n**总述**：论文首次把“个性化”从单点执行升级为“偏好补全+例行预测”的层级化隐式意图对齐，提供数据、任务、模型一站式解决方案，并验证其显著优于现有 GUI 智能体。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PersonalAlign%3A%20Hierarchical%20Implicit%20Intent%20Alignment%20for%20Personalized%20GUI%20Agent%20with%20Long-Term%20User-Centric%20Records，链接是https://arxiv.org/pdf/2601.09636，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.09636。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1768866682140","title":"Building Production-Ready Probes For Gemini","description":"Frontier language model capabilities are improving rapidly. We thus need stronger mitigations against bad actors misusing increasingly powerful systems. Prior work has shown that activation probes may be a promising misuse mitigation technique, but we identify a key remaining challenge: probes fail to generalize under important production distribution shifts. In particular, we find that the shift from short-context to long-context inputs is difficult for existing probe architectures. We propose several new probe architecture that handle this long-context distribution shift. We evaluate these probes in the cyber-offensive domain, testing their robustness against various production-relevant shifts, including multi-turn conversations, static jailbreaks, and adaptive red teaming. Our results demonstrate that while multimax addresses context length, a combination of architecture choice and training on diverse distributions is required for broad generalization. Additionally, we show that pairing probes with prompted classifiers achieves optimal accuracy at a low cost due to the computational efficiency of probes. These findings have informed the successful deployment of misuse mitigation probes in user-facing instances of Gemini, Google's frontier language model. Finally, we find early positive results using AlphaEvolve to automate improvements in both probe architecture search and adaptive red teaming, showing that automating some AI safety research is already possible.","url":"https://huggingface.co/papers/2601.11516","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.140Z","metadata":{"authors":["János Kramár","Joshua Engels","Zheng Wang","Bilal Chughtai","Rohin Shah","Neel Nanda","Arthur Conmy"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2601.11516.pdf","arxivUrl":"https://arxiv.org/abs/2601.11516","arxivId":"2601.11516","coolPaperUrl":"https://papers.cool/arxiv/2601.11516","published":"2026-01-16T18:54:29Z","updated":"2026-01-16T18:54:29.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**前沿大语言模型（LLM）被恶意用于网络攻击等高风险场景**的实时检测问题，核心挑战是：\n\n- **激活探针（activation probe）在“短上下文→长上下文”这一生产级分布漂移下性能急剧退化**，导致无法直接部署；\n- 直接拿长上下文数据训练探针虽能缓解，但**训练成本暴增 22×**，不可接受；\n- 因此需要在**不依赖长上下文训练的前提下**，让探针在**长输入、多轮对话、静态/自适应越狱**等真实分布上仍保持高鲁棒性，同时维持**低成本**优势。\n\n总结：\n\n> 构建**生产就绪、对长上下文鲁棒、训练成本可接受**的轻量级激活探针，以替代或级联昂贵的 LLM 监控器，实现对网络攻击提示的实时、低误报检测。\n\nQ2: 有哪些相关研究？\n\n论文在第 6 节“Related Work”中系统梳理了五条紧密相关的研究脉络，并给出关键文献。按主题归纳如下：\n\n1. 滥用缓解（Misuse Mitigation）\n\n- RLHF / RLAIF：Christiano et al. 2017；Bai et al. 2022\n- 输入-输出监控器：Naihin et al. 2023；Inan et al. 2023；Han et al. 2024；Sharma et al. 2025\n- AI Control框架：Greenblatt et al. 2023\n2. 激活探针（Activation Probing）\n\n- 线性探针奠基：Alain & Bengio 2016\n- 安全监控应用：Zou, Phan, Chen et al. 2023；McKenzie et al. 2025；Goldowsky-Dill et al. 2025\n- 低成本监控：Cunningham, Peng et al. 2025；Cunningham, Wei et al. 2026\n- 注意力/稀疏自编码器探针：Kantamneni et al. 2025；Shabalin & Belrose 2025；Tillman & Mossing 2025\n3. 级联分类器（Cascading Classifiers）\n\n- 经典级联：Viola & Jones 2001\n- 语言模型级联：Greenblatt et al. 2023；Cunningham, Peng et al. 2025；Hua et al. 2025；McKenzie et al. 2025；Oldfield et al. 2025\n4. 分布漂移（Distribution Shifts）\n\n- 长上下文难题：Liu et al. 2023；Hsieh et al. 2024\n- 多轮对话漂移：Laban et al. 2025\n5. 越狱与自适应攻击（Jailbreaking）\n\n- 自动越狱：Zou, Wang et al. 2023（GCG）；Perez et al. 2022；Chao, Robey et al. 2025（PAIR）\n- Many-shot 越狱：Anil et al. 2024\n- 自适应攻击下界：Nasr et al. 2025\n6. 自动化安全研究（Automated Safety Research）\n\n- 全自动科学发现：Lu et al. 2024；Gottweis et al. 2025\n- 进化式算法搜索：Novikov et al. 2025（AlphaEvolve）\n- 自动可解释性代理：Shaham et al. 2025（MAIA）\n\nQ3: 论文如何解决这个问题？\n\n论文把“短上下文→长上下文”这一核心分布漂移拆解为**架构-训练-推理**三阶段问题，并给出四条可组合的技术路线，最终形成**生产级探针系统**。具体做法如下：\n\n1\\. 架构层：让探针本身对长序列不敏感\n\n| 方案 | 关键改动 | 长上下文失效根因的针对性解决 |\n| --- | --- | --- |\n| MultiMax | 将注意力 softmax 换成逐头 hard-max： f_(MultiMax)(S_i)=∑_(h=1)^H max_(j∈[n_i]) vh^top yi,j  | 长序列里恶意信号只占少数位置，softmax 平均后信号被“稀释”；hard-max 直接取最强证据，不受序列长度影响。 |\n| Rolling Attention + MultiMax | 在滑动窗口内做注意力加权平均，再对所有窗口取 max | 既保留局部上下文建模，又通过“局部平均+全局 max”兼顾长距离稀疏信号。 |\n| AlphaEvolve 自动搜索 | 以注意力探针为种子，进化约 2500 种架构，最终自动发现“类 MultiMax + 双极 pooling + 门控正则”结构 | 把人工试错转化为可扩展的架构搜索，进一步压榨 50% 剩余误差。 |\n\n2\\. 训练层：坚持“**只训短上下文**”\n\n- **数据混合**：训练集仅 3k 条短样本（表 7），但覆盖攻击、误触发、多轮三种分布；长上下文数据**仅用于验证/测试**，不进入梯度更新。\n- **多种子投票**：100 个随机初始化 → 选验证 loss 最低种子，平均再降 0.8% 测试误差（图 3）。\n- **成本对比**：同样精度下，直接训长上下文需 22× GPU 小时（附录 I），新架构**零额外成本**即可泛化。\n\n3\\. 推理层：级联“探针 + LLM”兼得低成本与高鲁棒\n\n- **Threshold-Randomization-Optimal 策略**\n定义双阈值  (t_0,t_1) ：\n\n- 探针 logit < t₀ → 拒绝\n- 探针 logit > t₁ → 通过\n- 中间区域 → 递交给 Gemini 2.5 Flash\n\n在验证集上按**凸包贪心算法**（附录 E，O(N log N)）求解帕累托前沿，**确定性策略即可达到最优**，无需随机化。\n结果：仅 8% 查询调用 Flash，却能把总误差压到**比单独 Flash 更低**，成本 ≈ 1/50（图 5）。\n\n4\\. 自动化闭环：用 AlphaEvolve 同时优化探针 & 攻击\n\n- **探针侧**：以验证加权误差为奖励，自动进化出超越手工设计的架构。\n- **攻击侧**：用同一系统进化“叙事型”越狱提示，把成功率从 20% → 26%，再反哺到评估集（ART），形成**攻防自提升**循环。\n\n最终交付物\n\n| 指标 | 纯探针 | 探针+8% Flash |\n| --- | --- | --- |\n| 测试加权误差 | 2.50% | 1.5% |\n| 相对成本 | 1× | ≈1.6× |\n| 长上下文 FNR | 1.7% | 1.2% |\n\n该“Selected Probe”已直接集成进 Gemini 生产流量，实现对网络攻击提示的**实时、低成本、长上下文鲁棒**检测。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“**短上下文训练 → 长上下文部署**”这一核心漂移，设计了**三级九类**实验，覆盖**架构、训练、推理、自动化、跨域验证**五个维度。所有实验共用同一套**完全隔离的 train/val/test**（表 1、6），并采用**加权误差指标**（式 12）统一度量，保证结果可直接比。\n\n1\\. 主实验：Cyber 探针全景评估\n\n**目的**：在 9 类真实漂移数据上一次性比较 16 种分类器。\n**数据**：表 2 九组数据集（短/长/多轮/越狱/自适应红队）。\n**指标**：加权 FPR/FNR → 汇总为 Test Error（表 3）。\n**关键结论**：\n\n- Rolling Attention + MultiMax 误差 **2.50%**，显著低于线性基线 **6.18%**（bootstrap >0.999）。\n- 不训长上下文的 AlphaEvolve 探针也能做到 **2.53%**，与直接长上下文训练的 Attention 探针 **2.38%** 持平，但节省 **22×** 计算。\n\n2\\. 长上下文专项消融\n\n**设置**：固定短上下文训练集，仅改变推理时聚合函数。\n**结果**（图 4c）：\n\n- 线性探针长上下文 FNR **87.9%**（几乎全漏）。\n- MultiMax 聚合把同架构 FNR 降到 **1.7%**，FPR 仅升至 **1.2%**。\n\n3\\. 级联成本-精度前沿\n\n**方法**：在验证集上暴力枚举 (t₀, t₁) 两万对，用**凸包贪心**（附录 E）画帕累托前沿。\n**结果**（图 5）：\n\n- 探针单独即落在前沿；\n- 仅 **8%** 查询递交给 Gemini 2.5 Flash，就能把误差再降 **30%**，总成本仍 **<1/50** 全量 Flash。\n\n4\\. 统计显著性与种子方差\n\n**做法**：每类架构训练 **100 随机种子**，bootstrap 10 000 次“best-of-100”选择分布。\n**结果**（表 4）：\n\n- Rolling-Attention 系列对线性基线的胜率 **>0.999**；\n- 架构差距（0.025 vs 0.064）≈ **10×** 种子调优收益，验证“优先搜架构，再调种子”。\n\n5\\. AlphaEvolve 自动架构搜索\n\n**规模**：单次进化 **~2500 程序**，用验证加权误差做奖励。\n**结果**（图 10）：\n\n- 相对 Attention 种子基线，**50%** 剩余误差被自动关闭；\n- 最终程序自发复现“MultiMax + 双极 pooling + 门控正则”结构（算法 1）。\n\n6\\. 跨域可迁移性验证\n\n**数据**：12 个公开二元分类任务（TruthfulQA、伦理、新闻等）。\n**模型**：Gemma-2 9B（与主文 Gemini 隔离）。\n**结果**（图 6、7）：\n\n- AlphaEvolve 与 Attention 探针 median AUROC **0.975**，**+3.1 pp** 优于线性 LogReg；\n- 种子选择平均提升 **0.011**，架构选择提升 **0.109**，再次验证架构优先。\n\n7\\. 自适应红队（ART）迭代实验\n\n**流程**： attacker LLM 最多 10 轮迭代，每轮接收历史成败信号。\n**对照**：重复单轮独立采样 **T 次**。\n**结果**（图 12）：\n\n- 迭代轮次 **不优于** 独立重复；\n- 用 AlphaEvolve 优化 attacker prompt 后，单轮成功率 **20% → 26%**；\n- 该优化 prompt 被直接纳入主实验的 ART 测试集。\n\n8\\. 提示词鲁棒性\n\n**做法**：手工构造 8 条替代系统提示，在短上下文 FPR/FNR 上扫阈值。\n**结果**（图 11）：\n\n- 所有提示均落在同一条线性 Pareto 前沿，原提示已接近最优，故主文沿用。\n\n9\\. 训练成本与工程消融\n\n- **显存**：长上下文激活 **1 M token/样本** 无法整批放入 TPU HBM，采用**主机-设备换页+轮询采样**，训练成本 **22×**（附录 I）。\n- **推理**：提出 **O(n) 增量注意力探针**（附录 L），内存 **O(1)**，已合入生产代码。\n\n一句话总结\n\n实验从**像素级架构**到**生产级系统**，用**统计显著性+成本帕累托+跨域验证+自动攻防**四重闭环，证明：\n\n> “**不训长文本也能泛化长文本，且成本降 50× 的同时误差更低**” 这一核心结论可落地于 Gemini 实时流量。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为“**立即可行**”与“**中长期**”两类，均围绕**分布漂移、成本、自动化、理论**四条主线展开。\n\n立即可行（≤6 个月）\n\n1. **多层激活探针**\n本文仅采集中间一层激活；同期工作（Cunningham, Wei et al. 2026）显示**跨层拼接**可显著提升越狱鲁棒性。\n→ 用 AlphaEvolve 直接搜索“采哪些层、如何融合”，看能否**不增加推理 FLOPs** 而进一步降误差。\n\n2. **动态早期退出**（Early-Exit Probing）\n长上下文 1 M token 时，**后半段才出现恶意信号**的情况常见。\n→ 训练一组**逐层/逐块**探针，在生成过程中**一旦置信度>τ 立即触发**，平均减少 30–50% 激活读取量。\n\n3. **多任务探针**\n目前每类滥用（网络、CBRN、欺诈）单独训练。\n→ 共享底层 MLP+注意力，上层多任务输出，看能否**一次前向**覆盖全风险域，而 FPR 不叠加。\n\n4. **级联策略在线学习**\n当前 (t₀, t₁) 是静态的。\n→ 用**指数加权移动平均**（EWMA）持续跟踪生产流量 FPR/FNR，**每小时自动微调阈值**，适应新概念攻击或节假日流量突变。\n\n5. **压缩与量化**\n探针参数量仍 ~10⁵–10⁶。\n→ 对 MLP 做 8-bit 量化、对注意力头做**结构化剪枝**，验证是否**无损**或\\*\\*<0.1 pp\\*\\* 降点，可再降 4× 推理延迟。\n\n中长期（6 个月–2 年）\n\n6. **输出侧探针**（Response-Level Monitoring）\n本文仅监控**输入提示**；部分攻击通过**多轮诱导模型在回复中逐步泄露**有害内容。\n→ 训练**生成式早期停止探针**：在模型每生成 64 token 时判断“继续生成是否会导致违规”，**提前中断**并回滚。\n\n7. **对抗自适应下界理论**\nNasr et al. 2025 指出“攻击者后手”情况下**任何静态防御都可被绕过**。\n→ 建立**探针-攻击双人博弈**框架，量化**分布漂移速率 vs 探针更新周期**的**最小可达成误差**，给出**不可防御阈值**的解析界。\n\n8. **可解释探针**\n目前 MultiMax 只是“好用”但不知**关注哪些 token**。\n→ 用稀疏自编码器（SAE）把  v_h^top y_(i,j)  分解为**可解释特征**，再对齐到**MITRE ATT&CK 战术标签**，实现**可审计的违规理由**。\n\n9. **端到端可微分级联**\n当前级联是**硬阈值路由**。\n→ 让探针输出\\*\\*“拒绝/通过/ defer” 三分类分布\\*\\*，与 LLM 一起**联合训练**，使探针**学会在真正不确定时才 defer**，实现**可微分成本-风险优化**。\n\n10. **自动化红队 → 自动化防御闭环**\n本文用 AlphaEvolve 只优化攻击或探针**单方**。\n→ 构建**双层博弈自动机**：\n\\- 外层：生成新攻击 → 内层：自动生成新探针 → 外层：再攻击……\n用**种群博弈**算法维护**帕累托前沿攻击-防御对**，最终输出**可证明的最强探针 + 最难攻击集**，作为**持续更新基准**。\n\n一句话总结\n\n> 从“**单层输入探针**”走向“**多层-多任务-生成早期退出**”，再与“**在线学习+可解释+博弈论下界**”结合，最终目标是**可证明、可审计、可持续更新**的生产级防御体系。\n\nQ6: 总结一下论文的主要内容\n\n**论文主旨**\n构建可在**生产环境实时部署**的轻量级**激活探针**，以检测用户输入中的**网络攻击意图**；核心难题是“**短上下文训练 → 长上下文、多轮、越狱等分布漂移**”导致性能暴跌。作者提出**新架构 + 级联 + 自动化搜索**三件套，在**不训练长文本**的前提下实现**低成本、高鲁棒**的滥用缓解，并已嵌入 Gemini 线上流量。\n\n**技术贡献与结果**\n\n1. **MultiMax 探针**\n用 hard-max 取代 softmax，避免长序列信号稀释；**长上下文 FNR 从 87.9% → 1.7%**。\n\n2. **Rolling Attention + MultiMax**\n滑动窗口内注意力、全局再取 max；**测试加权误差 2.50%**，显著低于线性基线 6.18%。\n\n3. **AlphaEvolve 自动架构搜索**\n进化 2500 程序，**关闭 50% 剩余误差**，最终架构自发复现 MultiMax 思想。\n\n4. **级联分类器**\n探针先判、不确定再调 Gemini 2.5 Flash；**仅 8% 查询用 Flash，总误差再降 30%，成本 ≈1/50**。\n\n5. **跨域验证**\n12 个公开任务上 median AUROC **0.975**，**+3.1 pp** 优于线性方法，证明架构通用。\n\n**关键结论**\n\n- **架构优先 > 种子调优**；\n- **不训长文本也能泛化长文本**，训练成本降 **22×**；\n- **探针+LLM 级联**兼得**低成本**与**高鲁棒**；\n- 已**实际部署**于 Gemini 生产环境，实现实时网络攻击检测。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Building%20Production-Ready%20Probes%20For%20Gemini，链接是https://arxiv.org/pdf/2601.11516，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11516。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-16-1768866682141","title":"AgencyBench: Benchmarking the Frontiers of Autonomous Agents in 1M-Token Real-World Contexts","description":"Large Language Models (LLMs) based autonomous agents demonstrate multifaceted capabilities to contribute substantially to economic production. However, existing benchmarks remain focused on single agentic capability, failing to capture long-horizon real-world scenarios. Moreover, the reliance on human-in-the-loop feedback for realistic tasks creates a scalability bottleneck, hindering automated rollout collection and evaluation. To bridge this gap, we introduce AgencyBench, a comprehensive benchmark derived from daily AI usage, evaluating 6 core agentic capabilities across 32 real-world scenarios, comprising 138 tasks with specific queries, deliverables, and rubrics. These scenarios require an average of 90 tool calls, 1 million tokens, and hours of execution time to resolve. To enable automated evaluation, we employ a user simulation agent to provide iterative feedback, and a Docker sandbox to conduct visual and functional rubric-based assessment. Experiments reveal that closed-source models significantly outperform open-source models (48.4% vs 32.1%). Further analysis reveals significant disparities across models in resource efficiency, feedback-driven self-correction, and specific tool-use preferences. Finally, we investigate the impact of agentic scaffolds, observing that proprietary models demonstrate superior performance within their native ecosystems (e.g., Claude-4.5-Opus via Claude-Agent-SDK), while open-source models exhibit distinct performance peaks, suggesting potential optimization for specific execution frameworks. AgencyBench serves as a critical testbed for next-generation agents, highlighting the necessity of co-optimizing model architecture with agentic frameworks. We believe this work sheds light on the future direction of autonomous agents, and we release the full benchmark and evaluation toolkit at https://github.com/GAIR-NLP/AgencyBench.","url":"https://huggingface.co/papers/2601.11044","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.141Z","metadata":{"authors":["Keyu Li","Junhao Shi","Yang Xiao","Mohan Jiang","Jie Sun","Yunze Wu","Shijie Xia","Xiaojie Cai","Tianze Xu","Weiye Si","Wenjie Li","Dequan Wang","Pengfei Liu"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2601.11044.pdf","arxivUrl":"https://arxiv.org/abs/2601.11044","arxivId":"2601.11044","coolPaperUrl":"https://papers.cool/arxiv/2601.11044","published":"2026-01-16T07:22:20Z","updated":"2026-01-16T07:22:20.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"LLM Analysis Failed: Error: 抓取失败（已重试2次）: Waiting failed: 30000ms exceeded"}},{"id":"hf-17-1768866682141","title":"More Images, More Problems? A Controlled Analysis of VLM Failure Modes","description":"Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.","url":"https://huggingface.co/papers/2601.07812","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.141Z","metadata":{"authors":["Anurag Das","Adrian Bulat","Alberto Baldrati","Ioannis Maniadis Metaxas","Bernt Schiele","Georgios Tzimiropoulos","Brais Martinez"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2601.07812.pdf","arxivUrl":"https://arxiv.org/abs/2601.07812","arxivId":"2601.07812","coolPaperUrl":"https://papers.cool/arxiv/2601.07812","published":"2026-01-12T18:45:13Z","updated":"2026-01-12T18:45:13.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对大型视觉-语言模型（LVLMs）在多图像场景下的能力空白展开系统研究。核心问题概括为：\n\n- **多图像信息聚合失效**：现有模型难以把分散在不同图像中的视觉信息有效整合，导致计数、列举等任务性能随图像数量增加而急剧下降。\n- **多概念跟踪受限**：当需要同时关注多个物体类别时，模型表现迅速恶化，表明其并发注意力机制不足。\n- **视觉干扰鲁棒性差**：引入与查询无关的“干扰图像”后，准确率显著降低，说明模型易被无关信息误导。\n- **长序列而非多图像本身是关键瓶颈**：通过控制实验发现，性能退化主要源于视觉 token 序列变长，而非图像数量本身；模型在序列长度相当于单张或两张图像时表现最佳，暴露出“单图像行为”倾向。\n- **深层注意力机制缺陷**：层-wise 注意力分析显示，早期层尚能跨图像交互，但深层几乎只进行图像内注意力，削弱了跨图像推理能力。\n\n为解决上述问题，论文提出两条互补策略：\n\n1. **数据侧**：基于 OpenImages 构建合成多图像训练集（MIMIC 训练版），通过程序化组合单图像标注，生成带有显式跨图像监督的 198 k 样本，用于微调。\n2. **优化侧**：利用注意力掩码，在深层强制视觉 token 仅与同图像 token 交互，减少噪声并降低计算量；结合 LoRA 实现高效微调。\n\n实验表明，该方法在 MIMIC、MuirBench、Blink 等多图像基准上显著超越现有最佳结果，验证了解决方案的有效性。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三类：多图像 LVLM 架构、多图像评测基准、以及对 LVLM 内部行为的诊断分析。\n\n1\\. 多图像 / 视频 LVLM 架构\n\n- **Flamingo**（Alayrac et al., NeurIPS 2022）\n首次将冻结的视觉编码器与 LLM 通过门控交叉注意力衔接，支持交错多图像-文本输入。\n- **PaLM-E**（Driess et al., ICML 2023）\n在 PaLM 中插入视觉投影，支持多图像与机器人状态输入，用于具身推理。\n- **MiniGPT-5 / mPLUG-Owl / Video-LLaMA**（Zheng et al. 2023; Zhang et al. 2023）\n通过帧序列或图像网格扩展单图像模型，引入时序/位置嵌入。\n- **Qwen2-VL / InternVL2 / CogVLM2**（Wang et al. 2024b; Chen et al. 2024b; Hong et al. 2024）\n采用动态分辨率或像素重排，把高分辨率图像或视频帧视为长视觉 token 序列，实现“多图像即长序列”建模。\n- **Mantis / Idefics2**（Jiang et al. 2024b; Laurençon et al. 2023）\n在指令微调阶段引入交错多图像对话数据，但保持原始因果注意力不变。\n\n2\\. 多图像评测基准\n\n- **MuirBench**（Wang et al. 2024a）\n12 项任务覆盖比较、推理、 grounding，但未控制信息分布与干扰因素。\n- **Blink**（Fu et al. 2024b）\n14 项“人类易解”视觉感知任务，揭示模型在颜色、数量、空间关系上的缺陷。\n- **Visual Haystack**（Wu et al. 2025）\n长序列检索式任务，评估随着图像增多模型“找针”能力的衰减。\n- **MMIU / MIRB / MMT / NLVR2**（Meng et al. 2024; Zhao et al. 2024; Ying et al. 2024; Suhr et al. 2019）\n涵盖多图推理、多跳问答、视频-语言对齐等场景。\n以上基准均直接采用现成图像-文本对，缺乏对信息分布、干扰、概念数的细粒度控制。\n\n3\\. 模型行为诊断与改进\n\n- **Hallucination & 模态偏向**（Liu et al. 2023; Ouali et al. 2024; Deng et al. 2025）\n通过对抗性提示或偏好优化暴露视觉幻觉与文本主导偏向。\n- **注意力可视化**（Qian et al. 2024; Kaul et al. 2024）\n对单图像输入进行层-wise 探针，发现深层视觉-文本交互稀疏。\n- **长序列衰减**（Ravaut et al. 2024; Sharma et al. 2024）\n证明 LLM 存在“中间遗忘”与位置偏向，启发在视觉侧做序列压缩。\n- **Wu et al. 2025（Visual Haystack）**\n首次量化多图像序列长度与检索准确率负相关，但未剖析注意力机制或提出针对性训练策略。\n\n本文工作在上述基础上，首次将“信息如何跨图像分布”“干扰图像数量”“多概念并发跟踪”作为独立变量进行系统消融，并结合层-wise 注意力诊断提出数据-优化双路径微调，填补了多图像 LVLM 细粒度评测与针对性改进的研究空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“多图像理解失效”拆解为**数据稀缺**与**注意力机制缺陷**两大根源，并给出互补的解决方案：\n\n1\\. 数据侧：程序化合成多图像训练集\n\n- **来源**：利用 OpenImages 的 bbox+类别标注，而非仅用 MSCOCO。\n- **生成逻辑**：把单张图的标注“拼接”成 2–10 张图的序列，显式控制\n- 信息分布（instances 跨图像或集中）\n- 干扰图像数量\n- 概念数 k\n- 序列长度\n从而产出 198 k 条带有“跨图像答案”的指令样本。\n- **训练方式**：与原有 LLaVA-OV 58 万单/多图像指令数据混合，**全参数微调**投影层+LLM，冻结视觉编码器，直接灌输“跨图像计数、找共同、找异常”等监督信号。\n\n2\\. 优化侧：层-wise 注意力掩码\n\n- **观察**：第 3.2 节分析显示\n- 浅层仍有 inter-image attention\n- 深层几乎退化为 intra-image\n导致后期无法整合跨图信息。\n- **掩码策略**：\n- 仅对**视觉 token** 施加 block-diagonal mask：同一图像内的 token 可互见，跨图像不可见。\n- **文本 token** 保持全可见。\n- 仅作用于深部层（12–23 层），早期层保留跨图路径。\n- **效率实现**：采用 LoRA（r=128）训练掩码层，其余冻结；FLOPs 下降约 81%，显存与速度均受益。\n\n3\\. 联合流程\n\n1. 任选 LLaVA-OV 0.5B / 7B 做起点。\n2. **数据-centric** 分支：全参数微调 → 得到 “Ours”\n3. **优化-centric** 分支：LoRA + 掩码微调 → 得到 “Ours (Masked)”\n两条路线均显著优于基线，且可叠加（文中未显式组合，但逻辑兼容）。\n\n4\\. 效果验证\n\n- 在自建的 MIMIC 评测上，0.5B 模型平均得分从 26.4 → 49.4（Masked）。\n- 在公开 MuirBench、Blink、MMIU、MIRB、MMT、NLVR2 六个基准上，7B 掩码模型平均提升 2.9–15.4 个百分点，取得新 SOTA。\n- 计算成本：掩码版仅 19 % FLOPs 却反超全参数版，证明“**限制跨图交互 + 早期保留跨图**”既省算力又增精度。\n\n综上，论文通过**可控合成数据**提供显式跨图像监督，再用**层选择性注意力掩码**抑制深层噪声，两者互补地解决了 LVLM 无法聚合多图像信息、易受干扰、多概念跟踪弱的核心缺陷。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“诊断→归因→改进→验证”四步展开，共 4 组核心实验与 3 组扩展分析。所有实验均在自建的 MIMIC 探针基准与 6 个公开多图像基准上完成，受试模型包括 LLaVA-OV、Qwen2-VL、InternVL2 等。\n\n1 诊断实验（零样本，仅评测）\n\n| 实验目的 | 关键控制变量 | 主要结论 |\n| --- | --- | --- |\n| ① 信息分布对计数的影响 | 固定总实例数，改变“跨图像张数” | 实例一旦分散，准确率趋零 → 无法聚合 |\n| ② 视觉干扰鲁棒性 | 固定查询图像，追加 0–34 张干扰图 | 干扰越多性能越差，分散时更敏感 |\n| ③ 多概念跟踪 | 同时查询 k 个类别，k=1→5 | k≥3 时准确率骤降 → 并发注意力不足 |\n| ④ 序列长度 vs 图像数量 | ①直接加图 ②1-D 池化减 token ③像素降采样控信息 | 性能下降主因是序列长度而非“图像张数” |\n| ⑤ 注意力模式可视化 | 4 图/6 图输入，层-wise 平均 attention 热图 | 浅层跨图，深层仅图内 → 深层整合缺失 |\n\n2 改进实验（微调）\n\n| 设置 | 训练数据 | 可训参数 | 注意力机制 | 目标 |\n| --- | --- | --- | --- | --- |\n| Ours（全参数） | LLaVA-OV 580 k + MIMIC-OpenImages 198 k | 投影层+LLM 全部 | 标准因果 | 灌输跨图监督 |\n| Ours (Masked) | 同上 | 仅 LoRA 层（r=128） | 深层视觉 block-diagonal 掩码 | 抑制噪声+提速 |\n\n3 验证实验\n\n3.1 公开基准结果\n\n- **MuirBench** 12 子任务 → 0.5B 提升 6.8 pp，7B 提升 9.6 pp\n- **Blink / MMIU / MIRB / MMT / NLVR2** → 平均提升 2.9–15.4 pp，**新 SOTA**\n\n3.2 自建 MIMIC 结果\n\n| 模型 | Common | Counting | Odd-One | Listing (F1) | 平均 |\n| --- | --- | --- | --- | --- | --- |\n| LLaVA-OV-0.5B | 44.7 | 29.7 | 8.3 | 22.8 | 26.4 |\n| Ours (Masked) | 68.9 | 35.8 | 50.9 | 42.0 | 49.4 (↑23.0) |\n| LLaVA-OV-7B | 71.5 | 29.7 | 58.1 | 56.6 | 54.0 |\n| Ours (Masked) | 75.5 | 51.2 | 72.1 | 55.0 | 63.8 (↑9.8) |\n\n4 消融与扩展\n\n| 实验 | 设计 | 结果 |\n| --- | --- | --- |\n| ④-1 跨任务泛化 | 单任务训练→其余任务零样本测试 | Common↔Odd-One 互补；Listing 对全任务增益最大 |\n| ④-2 掩码层选择 | 早层(0-11) / 深 layer(12-23) / 全掩 | 仅掩深层 最佳，掩早层严重掉点 |\n| ④-3 计算效率 | 实测 FLOPs | 掩码版 0.5B 节省 81 % 计算，性能反而更高 |\n| ④-4 拼接对照 | 把多图拼成一张大图，token 数相近 | 拼接略优于多图，但仍远低于微调后模型 → 问题核心在序列建模而非“图的数量” |\n| ④-5 更大/更新模型 | LLaVA-OV-72B、Qwen2.5-7B、Qwen3-VL-8B | 随实例分散而下降的趋势依旧存在，验证问题普遍性 |\n\n5 可视化与案例\n\n- **answer-to-image 注意力图**： baseline 漏看第三图的盆栽 → 计数错误； masked 模型三图均衡关注 → 计数正确。\n- **Counting 细粒度曲线**：4 实例分在 4 图时，baseline 9 % → masked 45.8 %，直接证明跨图聚合能力被激活。\n\n以上实验从**现象诊断**到**机制改进**再到**充分对比**，闭环地验证了“数据+注意力掩码”双路径对多图像理解失效问题的有效性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**数据与任务**、**模型架构**、**训练策略**、**评测体系**四大类，均直接对应论文尚未穷尽或仅初步触及的要点。\n\n1\\. 数据与任务扩展\n\n- **跨领域程序化生成**\n- 文档页面、医学影像、遥感、工业检测等带有专业标注的数据集，可用相同“拼接-分散-加干扰”范式合成多图任务，验证方法是否跳出 COCO/OpenImages 风格域。\n- **时序/事件级多图像**\n- 将视频事件切片成 5–20 帧的“关键帧包”，引入事件顺序或因果标签，考察模型对动作演化、状态转换的跨帧聚合能力。\n- **高密度小目标**\n- 本文以 ≥5 % 面积过滤目标；可专门合成“小至 0.1 %”的计数/检测任务，探针像素级感知与跨图聚合的联合瓶颈。\n- **多模态干扰**\n- 在图像序列中混入广告文字、表情包、噪声帧，同时保持文本提问与视觉答案一致，测试视觉-语言对齐的鲁棒性。\n\n2\\. 模型架构创新\n\n- **自适应视觉 Token 压缩**\n- 根据图像内容复杂度动态减少 Token（如 Deformable Attention、Token Merge），在“长序列”与“细节保留”之间做在线权衡。\n- **Late/Early Fusion 混合**\n- 早期层做单图表示，后期层引入 Cross-Image Transformer Block，显式建模图-图关系；可叠加在现有 LVLM 之上，与掩码策略互补。\n- **记忆增强机制**\n- 引入外部记忆槽或递归单元，让模型在浏览后续图像时仍能持续访问已压缩的前图语义，缓解“中间遗忘”。\n- **图结构或超图推理**\n- 把每张图视为节点，用动态边权重编码语义相似度，执行消息传递，再与 LLM 自回归路径联合训练。\n\n3\\. 训练与优化策略\n\n- **课程 + 强化聚合**\n- 先短序列高准确率易样本，再逐渐提升图像数与干扰比例；配合 REINFORCE/LoRA 微调，直接优化“跨图聚合”这一可度量目标。\n- **多任务权衡损失**\n- 同时优化单图、多图、 stitched 图像三种输入，引入梯度掩码或损失加权，防止模型退化为“单图专家”。\n- **对抗-合作联合训练**\n- 训练一个“干扰生成器”网络，实时产生最难被关注的干扰图像，与主模型做 minimax 博弈，提升鲁棒性。\n- **Continual Multi-Image Tuning**\n- 研究在多图像任务上持续微调后，模型对单图下游任务的遗忘程度，探索遗忘抑制策略（EWC、KL-reg 等）。\n\n4\\. 评测与可解释性\n\n- **细粒度可解释基准**\n- 提供 instance-level 标注，同时评估“定位-计数-聚合”三步各自准确率，定位错误来源。\n- **人机一致性协议**\n- 引入眼动或人工注视序列，对比模型 answer-to-image 注意力图，量化“关注一致性”指标。\n- **长序列 Scaling Law**\n- 在 100–1000 张图像范围内系统测试性能衰减曲线，验证掩码或压缩策略的极限与转折点。\n- **封闭模型验证**\n- 对 GPT-4V、Gemini Ultra 等黑盒模型实施相同探针，验证论文结论是否普遍适用于异构架构。\n\n5\\. 应用与系统层面\n\n- **边缘设备多图推理**\n- 结合掩码带来的 80 %+ FLOPs 节省，研究在手机/NPU 上的量化与推理延迟，推动“多图理解”落地。\n- **交互式多图对话**\n- 允许用户在多轮对话中动态增删图像，模型需实时更新跨图记忆，考验增量聚合与遗忘机制。\n- **Auto-ML for Mask 策略**\n- 用 NAS 或强化学习搜索每层最优注意力模式（全局、块对角、稀疏窗口），而非人工指定 12–23 层掩码。\n\n通过在这些方向深入，可进一步释放多图像视觉-语言模型的上限，并建立更完善的理论-数据-评测闭环。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有大型视觉-语言模型（LVLM）在多图像场景下普遍失效，表现为跨图信息聚合困难、多概念跟踪弱、易被干扰且性能随图像数量增加而骤降。\n- **诊断**：通过新建可控基准 MIMIC 系统实验，发现核心瓶颈是“视觉 token 序列变长”而非“图像张数”本身；层-wise 注意力分析揭示深层几乎放弃跨图交互，退化为单图模型。\n- **方法**：\n1. **数据侧**——用 OpenImages 标注程序化合成 198 k 多图像训练样本，显式监督跨图推理；\n2. **优化侧**——在深层引入 block-diagonal 注意力掩码（视觉 token 仅与同图交互），配合 LoRA 微调，减少 81 % FLOPs。\n- **结果**：在 MIMIC、MuirBench、Blink 等 7 个多图像基准上，0.5 B→7 B 模型平均提升 9–23 个百分点，取得新 SOTA，同时显著降低计算成本。\n- **结论**：可控合成数据与针对性注意力约束可互补地破解 LVLM 的“单图行为”，为多图像理解提供新的数据-优化范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是More%20Images%2C%20More%20Problems%3F%20A%20Controlled%20Analysis%20of%20VLM%20Failure%20Modes，链接是https://arxiv.org/pdf/2601.07812，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.07812。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1768866682141","title":"AstroReason-Bench: Evaluating Unified Agentic Planning across Heterogeneous Space Planning Problems","description":"Recent advances in agentic Large Language Models (LLMs) have positioned them as generalist planners capable of reasoning and acting across diverse tasks. However, existing agent benchmarks largely focus on symbolic or weakly grounded environments, leaving their performance in physics-constrained real-world domains underexplored. We introduce AstroReason-Bench, a comprehensive benchmark for evaluating agentic planning in Space Planning Problems (SPP), a family of high-stakes problems with heterogeneous objectives, strict physical constraints, and long-horizon decision-making. AstroReason-Bench integrates multiple scheduling regimes, including ground station communication and agile Earth observation, and provides a unified agent-oriented interaction protocol. Evaluating on a range of state-of-the-art open- and closed-source agentic LLM systems, we find that current agents substantially underperform specialized solvers, highlighting key limitations of generalist planning under realistic constraints. AstroReason-Bench offers a challenging and diagnostic testbed for future agentic research.","url":"https://huggingface.co/papers/2601.11354","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.141Z","metadata":{"authors":["Weiyi Wang","Xinchi Chen","Jingjing Gong","Xuanjing Huang","Xipeng Qiu"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2601.11354.pdf","arxivUrl":"https://arxiv.org/abs/2601.11354","arxivId":"2601.11354","coolPaperUrl":"https://papers.cool/arxiv/2601.11354","published":"2026-01-16T15:02:41Z","updated":"2026-01-16T15:02:41.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nAstroReason-Bench 旨在填补“通用智能体规划”在**真实物理约束场景**中的评估空白。具体而言，论文聚焦以下核心问题：\n\n- 现有智能体基准大多停留在符号、文本或弱接地环境，无法检验大模型在**高代价、长周期、硬物理约束**任务中的可靠性。\n- 航天规划（SPP）具有异构目标、严格轨道-能源-姿态约束、不可逆可行性边界，是传统专用优化算法主导的领域，却缺乏**统一、可复现、面向智能体**的测试平台。\n- 因此，作者提出首个面向异构航天规划问题的统一基准 AstroReason-Bench，用于**零样本评测**各类前沿 LLM 智能体在真实轨道动力学、资源耦合与多任务调度下的规划能力，并系统揭示其与专用求解器的性能差距，为未来“通用规划智能体”研究提供诊断性参考。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两条主线，并在第 2 节系统回顾。可归纳为以下要点：\n\n1. 卫星规划与调度（Satellite Planning & Scheduling）\n\n- 深空网调度：从早期启发式修复（Johnston 2006）到 MILP（Δ-MILP, 2022）和强化学习基准 SatNet（2021）。\n- 敏捷对地观测：需考虑姿态机动动力学，主流方法包括 ALNS、PSO、重复地面轨迹设计等（Lemaître 2002；Lu 2023）。\n- 通感一体化星座：引入实时多跳路由，近年采用多智能体 RL（Lyu 2024）。\n共同痛点：各子领域算法碎片化、接口与评价指标不兼容，缺乏统一平台供“通用智能体”零样本迁移。\n2. 智能体规划与推理（Agentic Planning & Reasoning）\n\n- 符号推理基准：PlanBench、TravelPlanner 等侧重动作序列可行性，但去掉物理细节。\n- 交互式工具使用：τ-bench、WebArena、Voyager 等验证工具链与反馈循环， yet 环境动力学高度简化。\n- “代码即策略”范式：Code-as-Policies、Claude Code 等把自然语言翻译成 API 调用，为航天领域“副驾驶”提供雏形， yet 尚无工作在高保真轨道-能源-姿态约束下系统评估。\n\n综上，AstroReason-Bench 首次把“专用航天优化文献”与“通用 LLM 智能体文献”对接，构建可复现的物理级测试床，用以诊断智能体在真实约束下的规划短板。\n\nQ3: 论文如何解决这个问题？\n\n论文并未直接“解决”航天规划问题本身，而是**构建了一套诊断性基准**，把原本碎片化的航天调度任务封装成统一、可复现、面向智能体的测试环境，从而系统暴露当前 LLM 智能体在真实物理约束下的关键短板。具体做法分三步：\n\n1. 统一问题家族\n将 5 类异构航天规划（深空网排班、重访优化、区域覆盖、立体成像、时延优化）纳入同一物理引擎，强制所有任务共享轨道动力学、能源-存储耦合与姿态约束，形成“异构但同核”的评估集合。\n\n2. 零样本接口协议\n\n- 提供语义层 MCP 工具：人类可读的 JSON 状态摘要、约束违规解释，支持交互式调试。\n- 提供 Python API：弥补 LLM 算术短板，可批量计算可见性、机动时间、链路延迟等。\n两种接口均通过“场景管理器”加锁，保证状态一致性，实现真正的“agent-ready”沙箱。\n3. 大规模诊断实验\n对 6 个前沿 LLM 智能体（Claude Sonnet 4.5、Gemini 3 Flash、DeepSeek V3.2 等）进行 150 场全任务链仿真，每场景 2 小时真实时长、16 GB/8 核受限资源，与专用优化基线（MILP、SA、领域贪婪法）对比。\n结果量化呈现“通用智能体”在组合搜索、资源生命周期、空间推理上的系统性差距，并给出可复现的失败痕迹，供后续研究针对性改进。\n\n通过“统一封装 + 零样本接口 + 大规模诊断”，论文把“评估缺口”转化为可量化的“能力地图”，为下一代具备物理一致性的通用规划智能体提供基准与方向。\n\nQ4: 论文做了哪些实验？\n\n论文在第 5 章展开两轮实验：\n\n- **定量基准测试**（5.1–5.2）：把 6 个前沿 LLM 智能体与 4 类传统优化基线放在 5 项航天规划任务上同台对比；\n- **定性案例剖析**（5.3）：通过 3 个典型失败/干预场景，拆解智能体的认知短板。\n\n实验规模与设定\n\n- 150 次完整任务仿真：5 项任务 × 6 模型 × 5 随机场景；\n- 在线零样本：仅通过 MCP/Python API 访问环境，无任务微调；\n- 资源上限：单轮 2 h 墙钟、16 GB 内存、8 CPU 核；\n- 评价指标：任务专用指标（见下表）+ 统一失败痕迹日志。\n\n| 任务 | 核心指标 | 最佳传统基线 | 最佳智能体 | 关键发现 |\n| --- | --- | --- | --- | --- |\n| SatNet 深空网 | 𝑈_rms ↓ | MILP 0.30 | Gemini 3 Flash 0.53 | 智能体略优于贪婪，远逊于组合优化 |\n| Revisit 重访 | 𝑀_gap(h) ↓ | SA 13.65 h | Claude 18.83 h | 存储-下行链路生命周期管理是分水岭 |\n| Regional 区域覆盖 | 𝑀_cov ↑ | SA 3 % | Gemini 11 % | 全部方法均低分；靠“条带-轨道”对齐难 |\n| Stereo 立体成像 | 𝑀_cov ↑ | 0 % | Qwen3 Coder 18 % | 基线完全失效；智能体靠脚本找“双星对” |\n| Latency 时延优化 | 𝑀_avail ↑, 𝑀_lat ↓ | 0 % | Kat Coder 7 %, 58 ms | 仅 1 个模型悟出“多跳 ISL 链” |\n\n案例研究（5.3）\n\n1. 物理不可能性：90 颗 LEO 星连洲际单跳——全部失败，仅 Kat Coder 手动算出 ISL 中继。\n2. 探索-利用失衡：Regional 任务中 Claude 先注册条带后查轨道，覆盖率 0 %；人工提示“先分析轨道”后升至 8 %。\n3. RAG-增强规划：在 SatNet 最难过订场景注入相关论文，默认 ReAct 略读即行动致性能下降；强制“Plan Mode”深读并综合 MILP+回溯+贪婪三阶段策略，𝑈\\_rms 从 0.55 降至 0.50。\n\n结论性证据\n\n- 组合搜索型任务：专用优化仍占绝对优势；\n- 复合约束/拓扑型任务：零样本智能体可凭脚本与推理取得“>0”突破；\n- 统一基准首次把“能力地图”量化，为后续改进提供可复现的诊断数据。\n\nQ5: 有什么可以进一步探索的点？\n\n- **大模型规模与推理深度扩展**\n- 对比“Flash-class”与 Gemini-3-Pro、Claude-Opus-4.5 等更大推理模型，量化参数规模对长周期资源管理的影响。\n- 引入显式规划模块（Hierarchical ReAct、Tree-of-Thought、Self-Correction）观察能否逼近 MILP 性能。\n- **样本效率与交互预算**\n- 设定**计算匹配**原则：固定 CPU-小时或搜索节点数，公平比较智能体与专用求解器。\n- 研究少量“warm-start”演示（1–5 条专家轨迹）能否通过 in-context imitation 显著降低 revisit gap。\n- **多模态感知与空间推理**\n- 将轨道几何、地面轨迹、传感器 FOV 以图像或 3D 点云形式输入，测试 VLM 是否能缓解“条带方向错位”与“单跳中继”误区。\n- 引入可微分渲染或神经网络轨道传播，作为可学习的物理先验。\n- **层次化与分布式智能体**\n- 星座级任务天然适合“星座-卫星-载荷”三级 agent：上层负责任务分解，下层做局部调度，通过消息传递协同。\n- 探索 Multi-Agent RL + LLM 混合范式，用 RL 学底层策略，LLM 做高阶冲突协商。\n- **在线学习与持续适应**\n- 在 4-day 规划窗口内允许 agent 根据实时 TLE 更新、太阳暴或设备故障进行重规划，衡量“鲁棒-恢复”指标。\n- 引入非稳态目标（突发灾害响应），测试快速知识编辑与灾难性遗忘程度。\n- **可验证安全性与形式化约束**\n- 将能源、姿态、链路约束写成 SMT/Convex 规范，结合 LLM 的 Code-as-Policies 输出，调用形式化验证器（dReal, Z3）做动作签发，提供最坏情况违约概率上界。\n- **任务扩展与全生命周期设计**\n- 把 AstroReason-Bench 从“运营调度”扩展到“架构设计”（轨道选型、载荷配置）和“深空轨迹优化”（低推力、行星借力），形成覆盖概念设计-运营-维护的全栈基准。\n- 引入经济-碳排-风险多目标，研究 LLM 对高维帕累托前沿的近似能力。\n- **可复现性与社区驱动**\n- 开源更多 TLE 年代、星座家族与随机种子，建立持续集成 leaderboard，支持外部提交新任务或新 agent。\n- 提供轻量级容器化版本，降低 16 GB 门槛，方便社区低成本迭代。\n\n这些方向既可直接在 AstroReason-Bench 上增量实验，也可催生新一代“物理一致、安全可信”的通用航天规划智能体。\n\nQ6: 总结一下论文的主要内容\n\n**AstroReason-Bench 主要内容速览**\n\n1. 问题背景\n\n- 现有 LLM 智能体基准多停留在符号或弱接地环境，无法验证其在**高代价、长周期、硬物理约束**任务中的可靠性。\n- 航天规划（SPP）涵盖深空网排班、敏捷对地观测、通感一体星座等，需同时满足轨道-能源-姿态-链路耦合约束，是传统**专用优化算法**主导的领域，缺乏统一、可复现、面向智能体的测试平台。\n2. 解决方案\n\n- 构建首个统一基准 **AstroReason-Bench**，将 5 类异构航天调度任务封装到同一物理引擎（SGP4 轨道动力学 + 能源/存储/姿态模型）。\n- 提供**零样本接口**：语义层 MCP（JSON 状态 + 违规解释）与 Python API（批量可见性、机动计算），支持 ReAct 类智能体直接调用。\n- 设计 150 场大规模仿真（6 前沿 LLM × 5 任务 × 5 随机场景），与 MILP、模拟退火、领域贪婪等传统基线对比，量化“通用规划智能体”在真实约束下的能力缺口。\n3. 关键结果\n\n- 组合搜索型任务（SatNet、Revisit）：专用优化仍占绝对优势；智能体略优于简单贪婪，但 Urms 或平均重访间隔落后 40–70%。\n- 复合约束/拓扑型任务（Stereo、Latency）：传统基线**完全失效**（0%），智能体凭脚本与零样本推理取得 7–18% 突破，显示“>0”适应性。\n- 案例剖析揭示三大短板：\n① 空间-几何推理不足（单跳中继误区）；\n② 探索-利用失衡（未查轨道即注册条带）；\n③ 知识获取方式原始（RAG 需强制 Plan Mode 才能有效利用文献）。\n4. 贡献与意义\n\n- 首次把碎片化航天规划任务统一为**异构但同核**的智能体基准，提供标准化接口与诊断指标。\n- 实证展示当前 LLM 智能体在物理一致环境下的“能力地图”：零样本可识别复合约束，但缺乏系统搜索与资源生命周期管理。\n- 为后续研究（更大模型、层次化多智能体、在线学习、形式化安全验证等）提供可复现、社区可扩展的测试床与改进方向。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是AstroReason-Bench%3A%20Evaluating%20Unified%20Agentic%20Planning%20across%20Heterogeneous%20Space%20Planning%20Problems，链接是https://arxiv.org/pdf/2601.11354，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2601.11354。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-19-1768866682142","title":"Language of Thought Shapes Output Diversity in Large Language Models","description":"Output diversity is crucial for Large Language Models as it underpins pluralism and creativity. In this work, we reveal that controlling the language used during model thinking-the language of thought-provides a novel and structural source of output diversity. Our preliminary study shows that different thinking languages occupy distinct regions in a model's thinking space. Based on this observation, we study two repeated sampling strategies under multilingual thinking-Single-Language Sampling and Mixed-Language Sampling-and conduct diversity evaluation on outputs that are controlled to be in English, regardless of the thinking language used. Across extensive experiments, we demonstrate that switching the thinking language from English to non-English languages consistently increases output diversity, with a clear and consistent positive correlation such that languages farther from English in the thinking space yield larger gains. We further show that aggregating samples across multiple thinking languages yields additional improvements through compositional effects, and that scaling sampling with linguistic heterogeneity expands the model's diversity ceiling. Finally, we show that these findings translate into practical benefits in pluralistic alignment scenarios, leading to broader coverage of cultural knowledge and value orientations in LLM outputs. Our code is publicly available at https://github.com/iNLP-Lab/Multilingual-LoT-Diversity.","url":"https://huggingface.co/papers/2601.11227","source":"HuggingFace Papers","timestamp":"2026-01-19T23:51:22.142Z","metadata":{"authors":["Shaoyang Xu","Wenxuan Zhang"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.CL","cs.CY"],"pdfUrl":"https://arxiv.org/pdf/2601.11227.pdf","arxivUrl":"https://arxiv.org/abs/2601.11227","arxivId":"2601.11227","coolPaperUrl":"https://papers.cool/arxiv/2601.11227","published":"2026-01-16T12:14:16Z","updated":"2026-01-16T12:14:16.000Z","zh_summary":"翻译失败: Translation limit exceeded. Please try it again later.\nResponse status: 401 (Unauthorized)\nResponse body  : \"{\\\"ShowCaptcha\\\":false}\"","llm_analysis":"LLM Analysis Failed: Error: 抓取失败（已重试2次）: Navigation timeout of 10000 ms exceeded"}}]}