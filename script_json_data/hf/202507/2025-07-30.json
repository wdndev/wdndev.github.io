{"timestamp":"2025-07-30T01:47:46.059Z","totalItems":1,"items":[{"id":"hf-0-1753840037354","title":"MOVE: Motion-Guided Few-Shot Video Object Segmentation","description":"This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few shot motion understanding, establishing a solid foundation for future research in this direction.","url":"https://huggingface.co/papers/2507.22061","source":"HuggingFace Papers","timestamp":"2025-07-30T01:47:17.354Z","metadata":{"authors":["Kaining Ying","Hengrui Hu","Henghui Ding"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2507.22061.pdf","arxivUrl":"https://arxiv.org/abs/2507.22061","arxivId":"2507.22061","coolPaperUrl":"https://papers.cool/arxiv/2507.22061","published":"2025-07-29T17:59:35Z","updated":"2025-07-29T17:59:35.000Z","zh_summary":"本工作针对运动引导的少样本视频目标分割（FSVOS），旨在根据少量具有相同运动模式的标注示例对视频中的动态物体进行分割。现有的FSVOS数据集和方法通常集中于物体类别，这些静态属性忽视了视频中丰富的时间动态，因此限制了它们在需要运动理解的场景中的应用。为了填补这一空白，我们推出了MOVE，这是一个专门为运动引导的FSVOS设计的大规模数据集。在MOVE的基础上，我们对来自3个不同相关任务的6种最先进方法在2个实验设置下进行了全面评估。我们的结果揭示了当前方法在解决运动引导FSVOS时的困难，这促使我们分析相关挑战并提出基线方法——解耦运动外观网络（DMA）。实验表明，我们的方法在少样本运动理解方面表现优异，为未来在这一方向上的研究奠定了坚实基础。","llm_analysis":"**Q**: 这篇论文试图解决什么问题？\n\n**A**: 这篇论文试图解决**基于运动引导的少样本视频目标分割（Motion-guided Few-Shot Video Object Segmentation, FSVOS）**问题。传统的少样本视频目标分割方法主要关注目标的类别（即静态属性），忽略了视频中丰富的时序动态信息，例如目标的运动和时序依赖性。这限制了它们在需要理解运动的场景中的应用。为了解决这一问题，论文提出了一个新的大规模数据集**MOVE**，并设计了一个新的方法**Decoupled Motion-Appearance Network (DMA)**，用于提取解耦的运动和外观原型，从而更好地理解和匹配支持视频和查询视频之间的运动模式。\n\n**Q**: 有哪些相关研究？\n\n**A**: 论文中提到了以下几类相关研究：\n\n视频目标分割（Video Object Segmentation, VOS）\n\n*   **早期基于深度神经网络的方法**：例如OSVOS \\[2\\] 和MoNet \\[58\\]，这些方法在推理过程中微调网络参数以建模帧间相关性。\n*   **基于原型的方法**：例如OSMN \\[62\\] 和LML \\[1\\]，使用第一帧及其掩码作为提示，生成用于与后续帧进行像素级匹配的原型。\n*   **基于记忆的方法**：例如STM \\[43\\] 引入记忆模块存储历史帧信息，STCN \\[7\\] 提高了记忆使用效率，XMem \\[6\\] 和Cutie \\[8\\] 进一步改进了记忆机制。\n\n少样本视频目标分割（Few-Shot Video Object Segmentation, FSVOS）\n\n*   **早期工作**：例如DANet \\[3\\] 首次定义了FSVOS任务，并提出了采样查询代理以关注注意力。\n*   **基于原型学习的方法**：例如TTI \\[52\\] 和VIPMT \\[37\\]，专注于通过不同粒度的原型进行时间一致性建模。\n*   **基于图注意力和最优传输的方法**：例如HPAN \\[41\\] 和CoCoNet \\[40\\]，进一步改进了时间建模。\n\n运动中心任务（Motion-centric Tasks）\n\n*   **早期人类中心动作识别**：例如 \\[49, 56, 73\\]。\n*   **动作检测和时空定位**：例如 \\[50, 64, 71\\]。\n*   **语言视频模型（LVLMs）**：例如 \\[22, 38, 67\\]，关注与时间运动相关的任务。\n*   **指代视频目标分割（Referring Video Object Segmentation, RVOS）**：例如 \\[11, 18, 2"}}]}