{"timestamp":"2025-09-30T23:49:04.945Z","totalItems":20,"items":[{"id":"hf-0-1759276017460","title":"SLA: Beyond Sparsity in Diffusion Transformers via Fine-Tunable Sparse-Linear Attention","description":"In Diffusion Transformer (DiT) models, particularly for video generation, attention latency is a major bottleneck due to the long sequence length and the quadratic complexity. We find that attention weights can be separated into two parts: a small fraction of large weights with high rank and the remaining weights with very low rank. This naturally suggests applying sparse acceleration to the first part and low-rank acceleration to the second. Based on this finding, we propose SLA (Sparse-Linear Attention), a trainable attention method that fuses sparse and linear attention to accelerate diffusion models. SLA classifies attention weights into critical, marginal, and negligible categories, applying O(N^2) attention to critical weights, O(N) attention to marginal weights, and skipping negligible ones. SLA combines these computations into a single GPU kernel and supports both forward and backward passes. With only a few fine-tuning steps using SLA, DiT models achieve a 20x reduction in attention computation, resulting in significant acceleration without loss of generation quality. Experiments show that SLA reduces attention computation by 95% without degrading end-to-end generation quality, outperforming baseline methods. In addition, we implement an efficient GPU kernel for SLA, which yields a 13.7x speedup in attention computation and a 2.2x end-to-end speedup in video generation on Wan2.1-1.3B.","url":"https://huggingface.co/papers/2509.24006","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.460Z","metadata":{"authors":["Jintao Zhang","Haoxu Wang","Kai Jiang","Shuo Yang","Kaiwen Zheng","Haocheng Xi","Ziteng Wang","Hongzhou Zhu","Min Zhao","Ion Stoica","Joseph E. Gonzalez","Jun Zhu","Jianfei Chen"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.24006.pdf","arxivUrl":"https://arxiv.org/abs/2509.24006","arxivId":"2509.24006","coolPaperUrl":"https://papers.cool/arxiv/2509.24006","published":"2025-09-28T17:58:59Z","updated":"2025-09-28T17:58:59.000Z","zh_summary":"在扩散变换器（Diffusion Transformer, DiT）模型中，尤其是用于视频生成时，由于序列长度较长且计算复杂度为平方级，注意力延迟是主要瓶颈。我们发现注意力权重可以分为两部分：一小部分具有高秩的大权重，以及其余具有非常低秩的权重。这自然提示我们对前者应用稀疏加速，对后者应用低秩加速。基于这一发现，我们提出了SLA（Sparse-Linear Attention，稀疏-线性注意力），这是一种可训练的注意力方法，将稀疏注意力和线性注意力融合以加速扩散模型。SLA将注意力权重分为关键、边际和可忽略三类，对关键权重应用O(N^2)注意力，对边际权重应用O(N)注意力，并跳过可忽略权重。SLA将这些计算整合到单个GPU内核中，并支持前向和反向传递。通过使用SLA进行少量微调步骤，DiT模型在注意力计算上实现了20倍的减少，从而在不损失生成质量的情况下显著加速。实验表明，SLA在不降低端到端生成质量的情况下，将注意力计算减少了95%，并且优于基线方法。此外，我们实现了一个高效的SLA GPU内核，在Wan2.1-1.3B上实现了注意力计算13.7倍加速以及视频生成端到端2.2倍的加速。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对扩散 Transformer（DiT）在视频生成场景中的注意力计算瓶颈：序列长度 10K–100K 导致标准注意力复杂度 O(N^2 d) 成为主要延迟来源。已有稀疏或线性注意力方法均无法同时实现高稀疏度与无损质量：\n\n- 线性注意力在视频 DiT 上质量严重退化；\n- 稀疏注意力在序列 ≤50K 时通常只能达到 40–60 % 稀疏度，>90 % 稀疏度会引入显著误差。\n\n作者发现 DiT 的注意力权重可显式解耦为两部分：\n\n1. 少量（<10 %）大值元素构成高秩矩阵；\n2. 其余绝大部分元素构成极低秩矩阵。\n\n据此提出可微调的混合方案 SLA（Sparse-Linear Attention）：\n\n- 对高秩部分用 O(N^2) 稀疏 FlashAttention 精确计算；\n- 对低秩部分用 O(N d^2) 线性注意力近似；\n- 对极小权重直接跳过。\n\n通过 2000 步微调，SLA 在 30K 长度序列上把注意力计算量削减 95 %，GPU 内核提速 13.7×，端到端视频生成加速 2.2×，且不损失生成质量。\n\nQ2: 有哪些相关研究？\n\n与 SLA 直接相关的研究可分为三条主线：稀疏注意力、线性注意力，以及面向 DiT 的高效注意力。以下按类别列出代表性文献并给出与 SLA 的核心差异。\n\n稀疏注意力（Sparse Attention）\n\n- **BigBird** (Zaheer et al., 2020)、**Longformer** (Beltagy et al., 2020)\n固定稀疏模式，面向 NLP，未考虑视频生成，稀疏度通常 <70 %。\n\n- **MInference** (Jiang et al., 2024)\n动态稀疏掩码，推理阶段免训练，稀疏度 60–80 %，未引入线性补偿。\n\n- **VSA** (Zhang et al., 2025c)、**VMoBa** (Wu et al., 2025)\n针对视频 DiT 的可训练稀疏方法，最高 89 % 稀疏度，但 >85 % 后质量明显下降；SLA 在 95 % 稀疏度仍保持无损。\n\n- **SpargeAttn** (Zhang et al., 2025b)\n训练/免训练两用块稀疏，稀疏度 85 % 左右；SLA 额外用线性注意力覆盖中间权重，稀疏度进一步提升。\n\n线性注意力（Linear Attention）\n\n- **Performer** (Choromanski et al., 2020)、**Linformer** (Wang et al., 2020)\n提出 φ(Q)φ(K)^top 低秩近似，复杂度 O(Nd^2)，但在视频 DiT 上单独使用会导致显著质量退化（论文图 2）。\n\n- **cosFormer** (Qin et al., 2022)、**Lightning Attention-2** (Qin et al., 2024)\n改进特征映射或数值稳定性，仍局限于语言模型或图像生成。\n\n- **SANA** (Xie et al., 2024)、**Dig** (Zhu et al., 2025)\n首次在图像 DiT 预训练阶段验证线性注意力可行；SLA 指出其在视频生成上单独失效，仅作为低秩补偿组件。\n\n面向 DiT 的硬件/内核优化\n\n- **FlashAttention-2/3** (Dao, 2023; Shah et al., 2024)\n提供 O(N^2) 精确注意力的 IO-aware 内核，被 SLA 直接调用以计算关键块。\n\n- **FPSattention** (Liu et al., 2025)\nFP8+稀疏协同优化，稀疏度 75 %；SLA 侧重算法层面 95 % 稀疏且保持浮点精度。\n\n- **SeerAttention** (Gao et al., 2024)、**MoA** (Fu et al., 2024)\n动态稀疏掩码学习，面向 LLM；SLA 针对视频 DiT 并引入线性补偿。\n\n总结\n\nSLA 与上述工作的本质区别在于：\n\n1. 首次在视频 DiT 上把“高秩-稀疏”与“低秩-线性”显式解耦并融合为可训练单层内核；\n2. 通过可学习投影补偿分布差异，实现 95 % 稀疏度而质量无损；\n3. 统一前向/反向融合内核，兼顾训练与推理。\n\nQ3: 论文如何解决这个问题？\n\n论文将问题拆解为「高秩-稀疏」与「低秩-线性」两个子问题，并在算法、硬件与训练三个层面给出统一解决方案。核心步骤如下：\n\n1. 权重解耦\n通过统计 Wan2.1 的注意力矩阵，验证其可分裂为\n\nP = Podot M_(高秩、)<10% + Podot(1-M)_(极低秩、)>90%\n\n前者保留完整秩，后者稳定秩 ≈ 9，适合低秩近似。\n\n2. 三区间动态分类\n对压缩后的块级权重矩阵\n\nP^(rm c)=rm Softmax!(rm pool(Q)rm pool(K)^top/√d)∈R^(N/b_q× N/b_(rm kv))\n按行取 Top-kh% 为 critical（标记 1），Bottom-kl% 为 negligible（标记 −1），其余为 marginal（标记 0）。kh=5、kl=10 即可在 30 K 序列上达到 95 % 稀疏度。\n\n3. 混合计算内核\n\n- critical：调用 FlashAttention-2 精确计算，复杂度 O(N²) 但仅占 5 % 块。\n- marginal：用线性注意力\n\nO^(rm l)_i=φ(Q_i)l(∑_(j:M^(rm c)_ij)=0φ(K_j)^top V_jr)!/!φ(Q_i)l(∑_(j:M^(rm c)_ij)=0rm rowsum(φ(K_j)^top)r)\n\n复杂度 O(Nd²)，可提前预聚合 hj、zj，实际开销 <0.5 %。\n- negligible：直接跳过。\n三部分输出在同一 GPU kernel 中完成，无额外内存往返。\n4. 分布补偿与微调\n线性输出经可学习投影 Proj: ℝ^d→ℝ^d 后再与稀疏输出相加\n\nO=O^(rm s)+rm Proj(O^(rm l))\n仅 2000 步微调即可让模型重新适应混合分布，保持 FID/VT 等指标与全注意力一致。\n\n5. 反向传播融合\n对稀疏块沿用 FlashAttention 梯度路径；对线性块预存 Hi、Zi，反向时只做矩阵加法，避免重复扫描。整体反向 kernel 提速 6.8×。\n\n6. 辅助优化\n\n- 查找表：>90 % 稀疏时记录非零块坐标，减少掩码扫描。\n- 预聚合：先求全局和再减去非零块，90 % 加法变 10 % 减法。\n- Method of Four Russians：中等稀疏度时把 hj/zj 分段预处理，查询代替累加。\n\n通过以上设计，SLA 在 Wan2.1-1.3B 上实现\n\n- 注意力 FLOPs 减少 95 %\n- 内核提速 13.7×\n- 端到端视频生成加速 2.2×\n且 VBench、VR、VT 等多维质量指标与全注意力无统计学差异。\n\nQ4: 论文做了哪些实验？\n\n实验围绕「质量无损前提下，能否在视频/图像 DiT 上把注意力计算砍掉 95 %」这一核心问题展开，分 5 组共 14 项具体测试。所有实验均在同一硬件（RTX 5090）与相同随机种子下完成，以保证可比性。\n\n1. 主实验：Wan2.1-1.3B 视频生成\n数据集：20 k 条 480p、5 s 视频（私有，Pexels + Common Crawl）\n微调步数：2000 step，batch=64\n指标：VBench 6 维（IQ/OC/AQ/SC/VR/VT）+ 实际 FLOPs + 端到端 latency\n结果：\n\n- SLA-95 % 在所有质量指标上持平或优于 Full Attention（±0.3 分以内）。\n- 注意力 FLOPs 从 52.75 T 降至 2.74 T（↓ 95 %）。\n- 内核耗时 97 s → 11 s（↓ 8.8×），端到端生成 162 s → 74 s（↓ 2.2×）。\n2. 对比稀疏/线性基线\n对照组：Sparge-F、Sparge-T、VMoBa、VSA、Linear-Only、Sparse-Only、L+S（简单相加）\n结论：\n\n- 同等或更高稀疏度下，SLA 的 VT/VR 平均高出 2.8/0.18。\n- Sparse-Only 90 % 稀疏即出现 33 % 误差，SLA 95 % 稀疏误差 <3 %。\n3. 消融实验（Ablation）\na) 激活函数 ϕ：softmax、elu+1、hedgehog\nsoftmax 在 VT 上领先 1.8 分，与 Full Attention 无显著差异。\nb) 关键块比例 kh：5 %、10 %、20 %\nkh=5 % 已达质量饱和，FLOPs 再降 50 %。\nc) 组件必要性\n去掉 Proj 层后 VR 下降 0.22；去掉线性部分稀疏度只能到 90 %，且 VT 降 4.3。\n\n4. 图像生成验证：LightningDiT-1.03B / ImageNet-512×512\n训练 100 k step，batch=128\n指标：FID + GFLOPs\n结果：\n\n- SLA-87.5 % 稀疏 FID=31.49，优于 Full Attention 31.87。\n- FLOPs 12.88 G → 1.73 G（↓ 86 %）。\n5. 效率微基准\na) Kernel-only speed：FlashAttention2 作为 1×，SLA 13.7×，VSA-89 % 7.1×，VMoBa-85 % 4.1×。\nb) Backward speed：SLA 6.8×，VSA 3.2×，VMoBa 2.9×。\nc) 查找表 + 预聚合优化后，90 %–95 % 稀疏区间再提速 1.27×。\n\n6. 可视化样本\n图 5 & 图 7 给出 6 条文本提示的完整 5 s 视频帧序列；SLA-95 % 与 Full Attention 几乎无法区分，而 Linear-Only、Sparse-Only、VMoBa 等在物体结构、时间一致性上可见明显畸变。\n\n综上，实验覆盖质量、计算量、运行时长、消融、跨模态（视频+图像）与可视化，共 14 项结果一致表明：SLA 可在 95 % 稀疏度下实现「注意力计算↓ 95 %、内核↑ 13.7×、端到端↑ 2.2×」且生成质量无损。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为「算法层面」「系统层面」「应用层面」三类，共 8 个可探索点。\n\n算法层面\n\n1. **动态稀疏度调度**\n当前 kh/kl 为常数。可尝试随时间步或 token 重要性自适应调整，实现「早期高稀疏、后期低稀疏」的渐进式生成，进一步节省 10–20 % 计算。\n\n2. **秩感知的低秩补偿**\n线性注意力仅用固定 φ；可引入小秩-预测网络，对 marginal 块实时估计最优秩 r≪d，改用 r-维投影，降低 O(Nd²)→O(Ndr)。\n\n3. **跨层共享掩码**\n观察同一扩散步相邻层 Pc 高度相关。若把 Mc 在相邻 2–3 层共享，可减少掩码预测开销，并可能提升稀疏度至 97 %。\n\n4. **条件稀疏性**\n文本-视频条件或空间掩码会改变注意力分布。可将条件向量拼入 pool(·) 网络，使 Mc 依赖文本/掩码，实现「条件驱动的动态稀疏」。\n\n系统层面\n\n5. **FP8/INT8 混合精度内核**\n目前仍使用 FP16。借鉴 FPSattention 的 FP8 块量化，对 Os、Ol 分别选择可容忍的精度，有望再提速 1.5× 并省 50 % 显存。\n\n6. **多卡序列并行**\n当视频长度 >100 K，Mc 的存储与聚合成为新瓶颈。可将 Mc、hj、zj 按序列维度分片，与序列并行框架（DeepSpeed-Ulysses、Ring-Attention）融合，突破单卡 80 GB 限制。\n\n应用层面\n\n7. **自回归 / 多模态大模型**\nSLA 目前验证于扩散 Transformer。其「高秩-稀疏 + 低秩-线性」假设是否成立於自回归语言模型或多模态 ViT，值得验证；若成立，可将 32 K→1 M 上下文训练成本降低一个数量级。\n\n8. **实时视频编辑与流媒体**\n把 SLA 内核嵌入视频编辑工具，可实现「边解码边稀疏」的流式生成。结合查找表与预聚合优化，有望在 4090 移动版实现 480p-30 fps 实时文本驱动视频编辑。\n\n以上任意一点若取得突破，均可把「95 % 稀疏、无损质量」的适用范围进一步推向更长序列、更大模型或更低功耗设备。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：扩散 Transformer（DiT）在视频生成中序列长达 10 K–100 K，标准注意力 O(N^2) 成为主要延迟源；现有稀疏或线性方法分别受限于“稀疏度不够”或“质量骤降”。\n- **关键发现**：DiT 的注意力权重可显式解耦为\n\n- <10% 大值 → 高秩矩阵\n- >90% 小值 → 极低秩矩阵\n- **方法 SLA（Sparse-Linear Attention）**：\n\n1. 块级预测压缩权重 P^(rm c)，按行分 Top-kh%、Bottom-kl% 与中间三部分，生成三元掩码 M^(rm c)。\n2. 同一 GPU kernel 内融合三种计算：\n- critical（1）→ FlashAttention 精确 O(N^2)\n- marginal（0）→ 线性注意力 O(Nd^2)\n- negligible（−1）→ 跳过\n3. 线性输出经可学习 Proj 后与稀疏输出相加，整体可端到端微调。\n- **结果**（Wan2.1-1.3B，30 K 序列，2000 step 微调）：\n- 注意力 FLOPs ↓ 95 %\n- GPU 内核提速 13.7×，端到端视频生成提速 2.2×\n- VBench、VR、VT 等多维质量与全注意力无统计学差异\n- **附加实验**：\n- 图像 DiT（LightningDiT-1B）87.5 % 稀疏下 FID 仍优于全注意力\n- 消融验证 kh=5 %、softmax φ、Proj 层均关键\n- 查找表、预聚合、Four-Russians 等优化使 90–95 % 稀疏段再提速 1.27×\n- **结论**：SLA 首次在视频 DiT 上实现“95 % 稀疏 + 无损质量”，为超长序列生成提供通用可训练加速范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SLA%3A%20Beyond%20Sparsity%20in%20Diffusion%20Transformers%20via%20Fine-Tunable%20Sparse-Linear%20Attention，链接是https://arxiv.org/pdf/2509.24006，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24006。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1759276017461","title":"StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs","description":"Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: a brittle single-path quantization architecture and a distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, a tokenizer that achieves stability through a consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via a powerful bit-wise voting mechanism to form a single, stable token sequence. StableToken sets a new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on a variety of tasks.","url":"https://huggingface.co/papers/2509.22220","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.461Z","metadata":{"authors":["Yuhan Song","Linhao Zhang","Chuhan Wu","Aiwei Liu","Wei Jia","Houfeng Wang","Xiao Zhou"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.22220.pdf","arxivUrl":"https://arxiv.org/abs/2509.22220","arxivId":"2509.22220","coolPaperUrl":"https://papers.cool/arxiv/2509.22220","published":"2025-09-26T11:32:51Z","updated":"2025-09-26T11:32:51.000Z","zh_summary":"广泛使用的语义语音分词器，虽然设计用于捕捉语言内容，却出乎意料地脆弱。我们发现，它们对与语义无关的声学扰动不具有鲁棒性；即使在语音完全可懂的高信噪比（SNR）下，其输出的标记序列也可能发生剧烈变化，从而增加下游大型语言模型的学习负担。这种不稳定性源于两个缺陷：脆弱的单路径量化架构以及对中间标记稳定性漠不关心的远程训练信号。为了解决这一问题，我们引入了 StableToken，一种通过共识驱动机制实现稳定性的分词器。其多分支架构并行处理音频，然后通过强大的逐位投票机制合并这些表示，形成单一、稳定的标记序列。StableToken 在标记稳定性上设定了新的最先进水平，在各种噪声条件下显著降低单位编辑距离（UED）。这种基础稳定性直接转化为下游收益，显著提升了 SpeechLLM 在各种任务中的鲁棒性。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文指出当前主流的\\*\\*语义语音分词器（semantic speech tokenizer）\\*\\*存在一个被忽视但关键的问题：**对声学扰动极度脆弱**。具体表现为：\n\n- 即便在**高信噪比（SNR）**、人类听觉完全可懂的情况下，**微小的、与语义无关的噪声**就能让分词器输出的离散 token 序列发生剧烈变化（Unit Edit Distance 高达 30 % 以上）。\n- 这种**token 级不稳定**会直接破坏语音–文本对齐，迫使下游 SpeechLLM 在训练时面对“同一句话、不同 token 序列”的混乱输入，从而显著降低模型在真实噪声场景下的鲁棒性。\n\n作者将根因归结为两点：\n\n1. **架构缺陷**：单路径量化缺乏容错，一旦特征向量因噪声越过量化边界，就会彻底跳变到另一个 codeword。\n2. **训练信号遥远**：仅以 ASR 文本损失为监督，对中间量化过程是否稳定完全不施加约束。\n\n因此，论文提出 **StableToken**，目标是在**不牺牲语义保真度**的前提下，让分词器在噪声条件下输出**稳定一致的 token 序列**，从而直接提升 SpeechLLM 在真实环境中的鲁棒性。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两大主线：**语义语音分词器** 与 **噪声鲁棒性**。代表性工作如下（按时间轴与类别归纳，不含第一人称）：\n\n语义语音分词器\n\n1\\. 自监督（SSL）范式\n\n- **HuBERT**（Hsu et al., 2021）\n迭代聚类生成伪标签，通过掩码预测学习离散单元；对生成任务友好度低，GPE 高。\n- **wav2vec 2.0 / w2v-BERT**（Baevski et al., 2020；Chung et al., 2021）\n对比+掩码语言建模，输出离散表示，但主要服务 ASR，未针对 LLM 优化。\n- **R-Spin**（Chang & Glass, 2024）\n在 SSL 框架内引入说话人与噪声不变约束，首次尝试 token 级鲁棒，但仍局限于传统 SSL 结构。\n\n2\\. 语义蒸馏混合范式\n\n- **SpeechTokenizer**（Zhang et al., 2023）\n层级 RVQ 统一语义与声学 token，BPS 高、与 LLM 扁平输入不兼容。\n- **X-Codec**（Ye et al., 2025）\n在 RVQ 基础上增加语义重建损失，提升保真度，但决策空间过大，噪声下 token 易跳变。\n- **Mimi**（Défossez et al., 2024）\n神经音频编解码器，首层语义蒸馏，帧率低，仍受单路径量化脆弱性影响。\n\n3\\. 全监督语义范式（与 SpeechLLM 主流接轨）\n\n- **S3 Tokenizer / CosyVoice**（Du et al., 2024a）\n直接以 ASR 损失训练 VQ，输出低比特率语义 token，成为当前 SpeechLLM 标配。\n- **CosyVoice2**（Du et al., 2024b）\n将 VQ 改为 FSQ，提升码本利用率，但仍属单路径决策，对扰动敏感。\n- **GLM-4-Voice Tokenizer**（Zeng et al., 2025）\n在 Whisper 编码器后加池化+VQ，帧率 12.5 Hz，未显式约束 token 稳定性。\n\n噪声鲁棒性（专门针对离散 token）\n\n- **NAST**（Messica & Adi, 2024）\n首个明确优化“干净–噪声”token 分布一致性的工作，采用预测器+残差编码器+decoder，引入鲁棒损失。局限：仅验证 SSL 框架，且未解决量化边界跳变。\n- **R-Spin**（同上）\n通过辅助“acoustic pieces”预测防止崩溃，学习噪声不变离散单元，但仍是单路径量化。\n- **Liu et al. 2025**\n提出 slice-consistency 与 perturbation-consistency，针对**声学 token**而非语义 token，目标为高保真重建，噪声不变性需求不同。\n\n小结\n\n现有文献或聚焦 ASR 模型本身的鲁棒，或仅在 SSL/声学 token 层面缓解不一致，**尚未在完全监督的语义分词器上系统解决“高 SNR 下 token 跳变”问题**。StableToken 通过**多分支共识量化+噪声感知训练**填补了这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **StableToken**，通过**协同设计的架构与训练策略**一次性解决“单路径量化脆弱”与“训练信号遥远”两大痛点。核心手段可归纳为两条主线：\n\n1\\. 架构层面：Voting-LFQ 模块 —— 用**比特级冗余+共识投票**替代单路径硬量化\n\n- **多分支并行投影**\n将编码器输出  h  同时送入  n （奇数，默认 5）条独立线性分支，得到  p_i = W_i h + b_i 。\n- **比特级二值化**\n每条分支用  B_i = sign(p_i)  生成  d  维二进制向量  -1,+1^d ，而非直接选 codeword。\n- **可微分共识投票**\n训练时先对各分支的每一位取平均\n\n(s_(final))_j = (1) / (n)∑_(i=1)^n (B_i)_j\n\n获得实值“置信度”向量，再用 Straight-Through Estimator 反向传播，允许梯度直接优化所有分支。\n- **硬投票推断**\n推断时对 s_(final) 再做一次 sign 得到最终二进制向量 B_(final) ，映射为整数索引 $k∈\n0,2^d-1) 。 只要比特翻转稀疏，即使**多数分支 token 错误**，仍可被比特级多数票纠正，显著降低 Unit Edit Distance。 2. 训练层面：Noise-Aware Consensus Training —— 把“干净–噪声一致性”显式写进损失 - **多视图输入** 同一句话生成干净波形 w 与随机扰动波形 w'=A(w) （高斯/粉噪/真实环境等）。 每次前向随机选 k<n/2 条分支喂 h' ，其余 n-k 条喂 h ，形成“干净多数、噪声少数”格局。 - **共识损失** 以所有分支的连续向量均值 bar p_(all) 为在线目标，对每条分支施加 L_(consensus)=(1) / (n)∑_(i=1)^n |p_i - bar p_(all)|_2^2 迫使噪声分支向“干净共识”靠拢，梯度平滑且避免离散码梯度崩溃。 - **联合目标** 在 ASR 交叉熵损失之外，仅增加三项权重超参： L_(total)=L_(ASR)+λ_1L_(consensus)+λ_2L_(commitment)+λ_3L_(codebook) 既保证语义保真，又显式约束中间表示不变。 3. 效果总结 - **Tokenizer 级**：FLEURS 多场景噪声平均 UED 从 26.17 % → 10.17 %，相对降幅 >60 %，同时重建 WER/MOS 仍保持 SOTA。 - **SpeechLLM 级**：同等 Qwen2.5-3B 骨架下，ASR/SER/TTS 在干净与强噪条件下一致优于 CosyVoice2、GLM-4-Voice 等基线，且噪声越重优势越大。 通过“**多分支比特投票**”提供容错架构，“**共识训练**”提供近距离监督，StableToken 把“高 SNR 下 token 跳变”问题在完全监督语义分词器上首次系统解决。 Q4: 论文做了哪些实验？ 实验围绕三条主线展开，**从 tokenizer 自身鲁棒性 → 下游 SpeechLLM 性能 → 内部机制验证**，形成完整证据链。所有对比均在**等参同骨架**条件下进行，确保公平。 1 tokenizer 级实验：验证“既稳又好” | 任务 | 数据集 | 指标 | 关键对比 | | --- | --- | --- | --- | | 噪声鲁棒 | FLEURS（多语种） | UED ↓ | 7 类扰动：高斯、粉噪、棕噪、bit-crush、真实环境、OOD 真实环境。StableToken 平均 UED 10.17 %，相对最佳监督基线 S3（26.17 %）↓ 61 %；相对最佳 SSL 基线 R-Spin（16.48 %）↓ 38 %。 | | 重构保真 | LibriSpeech / SEED（中英） | WER ↓ / MOS ↑ | 训练 flow-matching + HiFi-GAN 从 token 重建音频。StableToken 取得 最低 WER（LS-clean 3.84 %，LS-other 7.99 %）与 最高 MOS（SEED-Zh 4.18），同时帧率/码率与 CosyVoice2 持平，证明“稳”不牺牲“好”。 | 2 下游 SpeechLLM 实验：验证“稳 token → 稳模型” 采用 **Qwen2.5-3B** 为统一骨干，仅替换分词器，其余训练数据、prompt、超参完全一致。 2.1 自动语音识别 ASR | 测试集 | 指标 | 结果亮点 | | --- | --- | --- | | 合成噪声 LibriSpeech | WER ↓ | 0 dB SNR 时，StableToken 20.34 %，次优基线 29.94 %，相对↓ 32 %。 | | 真实环境噪声 | WER ↓ | 同等 SNR 下优势保持，差距随噪声加重而扩大。 | | CHiME-4（真实+模拟） | WER ↓ | dev/sim 30.61 %，test/real 35.90 %，均领先次优基线约 30 %。 | 2.2 语音情感识别 SER | 测试集 | 指标 | 结果亮点 | | --- | --- | --- | | 噪声 ESD（中英） | Accuracy ↑ | 干净语料各 tokenizer 持平；0 dB SNR 时 StableToken 67.13 %，次优 52.77 %，↑ 14.4 % 绝对值。 | 2.3 文本到语音 TTS | 测试集 | 指标 | 结果亮点 | | --- | --- | --- | | SEED-TTS（中英） | WER ↓ / MOS ↑ | EN：WER 4.43 % vs 次优 6.19 %；ZH：WER 3.02 % vs 次优 5.26 %。MOS 中英均最佳或次佳，验证稳 token 降低 LLM 学习难度，合成更保真、更自然。 | 3 诊断与消融实验：验证“每部分都有用” 3.1 组件消融（表 4） | 配置 | 平均 UED | LS-Other WER | 结论 | | --- | --- | --- | --- | | Full model | 10.96 % | 4.68 % | 最优 | | − Consensus Loss | 17.43 % ↑ | 4.88 % | 无中间监督，token 不稳 | | − Noise-Aware 训练 | 21.51 % ↑ | 5.52 % | 无多视图，鲁棒性骤降 | | − Multi-Branch（单路） | 24.47 % ↑ | 5.85 % | 失去投票容错，最差 | 3.2 投票分支数 N（表 5） N=3→5 带来**显著 UED/WER 双降**；N=7 仅边际提升，权衡开销后**选定 N=5**。 3.3 比特级纠错案例（表 6） 可视化展示：多数分支 token 已错，但**比特级 3:2 或 4:1 多数**仍能把错误位拉回，最终 token 完全正确，直观解释为何 UED 大幅下降。 4 复杂度与效率 | N | 参数量增量 | FLOPs 增量 | 结论 | | --- | --- | --- | --- | | 1→5 | +0.021 % | +0.010 % | 分支可并行，推理延迟几乎不变。 | 总结 实验覆盖** tokenizer 自身鲁棒性、重构质量、三大下游任务、消融诊断、比特级案例与复杂度**，形成“稳 token → 稳 SpeechLLM”的完整证据链。 Q5: 有什么可以进一步探索的点？ 以下方向可在此基础上继续深入，分为**理论、架构、训练、评测、应用**五大类，均直接对应论文尚未充分展开或尚未触及的关键问题。 1 理论层面：token 稳定性的可证明界 - 给出比特翻转概率 p_(bit) 与分支数 n 、码维 d 的函数关系，推导**token 恢复概率** P_(recover) ≥ 1 - ∑_(j=0)^(d) dj I[j > lfloor n/2 rfloor\n· p_(bit)^(j)(1-p_(bit))^(n-j)$\n并验证实验 UED 是否逼近该下界。\n- 量化边界曲率与噪声分布的匹配：若噪声协方差非各向同性，能否**学习马氏距离度量**使投票空间与感知空间一致？\n\n2 架构层面：投票机制的泛化与压缩\n\n- **自适应分支数**：根据局部信噪比动态增减分支（早期退出或按需唤醒），在干净区域节省计算。\n- **非均匀比特保护**：对高方差比特位引入**冗余纠错码**（BCH、Polar），低方差位保持原样，实现“可变速率稳健编码”。\n- **跨帧投票**：当前仅帧内投票，可探索**时序滑窗比特共识**，利用语音慢变特性进一步降低突发性错误。\n- **权重投票**：将  sign(·)  改为  sign(∑_i α_i B_i) ，其中  α_i  由分支历史置信度学习得到，替代简单多数。\n\n3 训练层面：更紧的稳定性正则\n\n- **token 级一致性损失**：利用 CTC 对齐后，对干净–噪声同一帧的**离散索引**直接最大化互信息\n L_(MI) = -log p(k_(clean) mid k_(noisy)) ，缓解连续向量  p_i  一致但索引跳变的问题。\n- **对抗扰动预算**：将当前随机噪声改为**PGD-like 声学攻击**，在  ell_2  球内寻找使投票分歧最大的  w' ，再强制共识，可提升最坏情况鲁棒性。\n- **渐进式噪声课程**：从高频扰动 → 真实混响 → 远场拾音，逐步扩大扰动空间，防止模型陷入单一噪声类型。\n\n4 评测层面：更严苛的稳定性基准\n\n- **连续语速变化**（±30 %）、**微小音高漂移**（±50 cent）等**感知不可察扰动**，验证是否仍出现 token 跳变。\n- **跨说话人风格迁移**：同一文本由不同情感、音色朗读，检查 token 序列是否随风格变化而波动——理想语义 tokenizer 应只随内容变。\n- **长尾稀有词测试**：构建含专有名词、缩略语的数据子集，观察大词汇码本（8 k→32 k）下稳定性是否退化，探索码本尺寸 scaling law。\n\n5 应用层面：向下游任务再深入\n\n- **流式 SpeechLLM**：将 Voting-LFQ 嵌入因果编码器，研究比特投票带来的**延迟–精度权衡**；能否在 200 ms 内完成多数投票？\n- **多模态 LLM**：把 StableToken 与视觉/视频 tokenizer 合并为**统一离散序列**，考察跨模态噪声（画面抖动→音频漂移）是否会产生联合 token 漂移。\n- **联邦/边缘场景**：将分支解耦到不同设备，通过**比特级多数投票协议**在边缘侧完成共识，仅上传最终 token，降低上行带宽 80 % 以上。\n- **可解释性工具**：可视化每位比特的语义含义（probing 哪一位对应鼻音、爆破音等），当错误发生时可**精确定位损坏的 phonetic feature** 并实时报警。\n\n6 长期视角：能否彻底“扔掉投票”？\n\n- 直接学习**噪声不变连续流形**，使任意扰动下特征投影到同一邻域，从而**单路径亦稳定**；此时投票仅作为训练正则，推断回到单路，实现零额外延迟。\n- 探索**连续语义 token**（非离散）+ 可微分 LLM 输入接口，彻底绕过量化跳变问题，同时保持 LLM 的离散优势（如缓存、压缩）。\n\n综上，StableToken 打开了“离散 token 鲁棒性”这一相对空白的研究方向；从**理论边界、自适应架构、更强正则、严苛评测到实际部署**，均存在清晰且值得深入的下一段工作。\n\nQ6: 总结一下论文的主要内容\n\n**StableToken：面向真实噪声环境的鲁棒语义语音分词器**\n\n1 问题\n\n- 现有语义分词器在高 SNR 下仍因**单路径量化**与**遥远 ASR 监督**而 token 跳变，下游 SpeechLLM 鲁棒性骤降。\n\n2 方法\n\n- **Voting-LFQ**：多分支并行投影 → 比特级二值化 → 可微分平均训练 / 多数投票推断，**容错且零额外延迟**。\n- **Noise-Aware Consensus Training**：每步随机选少数分支喂噪声，用 L2 共识损失迫使噪声分支对齐干净共识，**显式优化表示不变性**。\n\n3 结果\n\n| 层面 | 指标 | 提升 |\n| --- | --- | --- |\n| Tokenizer | UED ↓ | 10.17 % vs 最佳基线 26.17 %（↓ 61 %） |\n| ASR | WER ↓ | 0 dB OOD 噪声 20.34 % vs 29.94 %（↓ 32 %） |\n| SER | Acc ↑ | 0 dB 噪声 67.1 % vs 52.8 %（↑ 14.4 %） |\n| TTS | WER/MOS | 中英双测均达 SOTA，稳 token 降低 LLM 学习难度 |\n\n4 结论\n\nStableToken 以**多分支比特投票**+**共识训练**首次在完全监督语义分词器上实现**高保真与强鲁棒兼得**，为构建可部署的噪声环境 SpeechLLM 提供了直接且有效的基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是StableToken%3A%20A%20Noise-Robust%20Semantic%20Speech%20Tokenizer%20for%20Resilient%20SpeechLLMs，链接是https://arxiv.org/pdf/2509.22220，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.22220。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1759276017462","title":"Multiplayer Nash Preference Optimization","description":"Reinforcement learning from human feedback (RLHF) has emerged as the standard paradigm for aligning large language models (LLMs) with human preferences. However, reward-based methods built on the Bradley-Terry assumption struggle to capture the non-transitive and heterogeneous nature of real-world preferences. To address this, recent studies have reframed alignment as a two-player Nash game, giving rise to Nash learning from human feedback (NLHF). While this perspective has inspired algorithms such as INPO, ONPO, and EGPO with strong theoretical and empirical guarantees, they remain fundamentally restricted to two-player interactions, creating a single-opponent bias that fails to capture the full complexity of realistic preference structures. In this work, we introduce Multiplayer Nash Preference Optimization (MNPO), a novel framework that generalizes NLHF to the multiplayer regime. It formulates alignment as an $n$-player game, where each policy competes against a population of opponents while being regularized toward a reference model. Our framework establishes well-defined Nash equilibria in multiplayer settings and extends the concept of duality gap to quantify approximation quality. We demonstrate that MNPO inherits the equilibrium guarantees of two-player methods while enabling richer competitive dynamics and improved coverage of diverse preference structures. Through comprehensive empirical evaluation, we show that MNPO consistently outperforms existing NLHF baselines on instruction-following benchmarks, achieving superior alignment quality under heterogeneous annotator conditions and mixed-policy evaluation scenarios. Together, these results establish MNPO as a principled and scalable framework for aligning LLMs with complex, non-transitive human preferences. Code is available at https://github.com/smiles724/MNPO.","url":"https://huggingface.co/papers/2509.23102","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.462Z","metadata":{"authors":["Fang Wu","Xu Huang","Weihao Xuan","Zhiwei Zhang","Yijia Xiao","Guancheng Wan","Xiaomin Li","Bing Hu","Peng Xia","Jure Leskovec","Yejin Choi"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.23102.pdf","arxivUrl":"https://arxiv.org/abs/2509.23102","arxivId":"2509.23102","coolPaperUrl":"https://papers.cool/arxiv/2509.23102","published":"2025-09-27T04:18:33Z","updated":"2025-09-27T04:18:33.000Z","zh_summary":"从人类反馈中进行强化学习（RLHF）已成为将大型语言模型（LLM）与人类偏好对齐的标准范式。然而，基于 Bradley-Terry 假设的奖励方法在捕捉现实世界中偏好的非传递性和异质性方面存在困难。为了解决这一问题，近期研究将对齐重新构建为一个双人纳什博弈，从而提出了从人类反馈中学习纳什策略（NLHF）。虽然这一视角启发了如 INPO、ONPO 和 EGPO 等具有强理论和实证保证的算法，但它们仍然在本质上局限于双人互动，形成单对手偏差，无法捕捉现实偏好结构的全部复杂性。在本工作中，我们提出了多玩家纳什偏好优化（MNPO），这是一种将 NLHF 推广到多玩家情境的新框架。它将对齐表述为一个 $n$ 玩家博弈，每个策略在与对手群体竞争的同时，被引导向参考模型进行正则化。我们的框架在多玩家设置中建立了定义明确的纳什均衡，并扩展了对偶差距的概念以量化近似质量。我们证明 MNPO 继承了双人方法的均衡保证，同时实现了更丰富的竞争动态和对多样化偏好结构的更好覆盖。通过全面的实证评估，我们显示 MNPO 在指令执行基准测试中持续超越现有 NLHF 基线，在异质标注者条件和混合策略评估场景下实现了更优的对齐质量。总体而言，这些结果确立了 MNPO 作为一个在原则上合理且可扩展的框架，用于将 LLM 与复杂的非传递人类偏好对齐。代码可在 https://github.com/smiles724/MNPO 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**现有基于 Bradley–Terry 假设的 RLHF 方法难以刻画真实世界中非传递、异构的人类偏好**这一核心问题。具体而言：\n\n- 传统 RLHF 将偏好建模为**标量奖励+传递性**，无法处理**循环偏好、多 annotator 异构标准、混合历史策略**等复杂场景；\n- 近期 Nash Learning from Human Feedback（NLHF）把对齐重构为**二人零和博弈**，取得理论与实证进展，但**仅局限于单对手设定**，导致“单对手偏差”，无法覆盖真实多人偏好结构；\n- 为此，作者提出 **Multiplayer Nash Preference Optimization (MNPO)**，将 NLHF 推广到**n 人博弈**：\n1. 每个策略同时与**一组对手策略**竞争，并以 KL 正则项锚定参考模型；\n2. 建立**多人纳什均衡**及其对偶间隙度量，证明继承二人方法的收敛保证；\n3. 设计**时间依赖 MNPO（TD-MNPO）**，用历史策略加权混合作为动态对手集，稳定训练并兼容现有算法（DPO、SimPO、INPO 等）；\n4. 在指令跟随与推理基准上验证，MNPO 在**异构 annotator 与混合策略评估**场景下**一致优于现有 NLHF 基线**，提供可扩展的下一代对齐框架。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为以下四条主线，均与“如何用博弈论或更一般的偏好建模改进 RLHF”密切相关：\n\n1. **传统 RLHF 与 Bradley–Terry 扩展**\n\n- 经典 PPO-RLHF：Christiano et al. 2017, Ouyang et al. 2022（InstructGPT）\n- 直接偏好优化（DPO）系列：Rafailov et al. 2023 → IPO (Azar et al. 2024), KTO (Ethayarajh et al. 2024), SimPO (Meng et al. 2024), WPO (Zhou et al. 2024)\n- 长度/质量解耦：R-DPO (Park et al. 2024), Distill-DPO (Fisch et al. 2024)\n2. **二人纳什对齐（NLHF）**\n\n- 奠基理论：Munos et al. 2023 提出“Nash learning from human feedback”框架\n- 无后悔算法：INPO (Zhang et al. 2025b), SPPO (Wu et al. 2024), SPIN (Chen et al. 2024)\n- 乐观/外梯度加速：ONPO (Zhang et al. 2025a), EGPO (Zhou et al. 2025)\n- 预训练或蒸馏视角：Pre-DPO (Pan et al. 2025), MPO (Wang et al. 2024b)\n3. **多人或列表式偏好建模**\n\n- Plackett–Luce 扩展：从 pairwise BT 到 listwise 排名 (Debreu 1960; Plackett 1975)\n- 多 annotator 聚合：Calibrated RLHF (Wang et al. 2024a), Multi-objective RM (Wang et al. 2023)\n- 多代理博弈：Freund & Schapire 1999 的乘法权重更新，为本文迭代式(11)提供收敛保证\n4. **在线/迭代式偏好优化与对手构造**\n\n- 迭代 DPO：Xiong et al. 2023, Dong et al. 2024\n- 自对弈微调：DNO (Rosset et al. 2024), 自生成对手 πt 作为负例\n- 外部对手蒸馏：EO-MNPO（本文附录 F）可视为多教师知识蒸馏与偏好博弈的交叉\n\n这些工作共同构成了从“单奖励+传递性”到“二人纳什”再到“多人博弈”的演进脉络，MNPO 在此基础上首次把纳什均衡概念正式扩展到 n 玩家偏好优化，并统一了前述算法的特例。\n\nQ3: 论文如何解决这个问题？\n\n论文将“非传递、异构多人偏好”问题形式化为**n 玩家纳什博弈**，并通过以下三步解决：\n\n1. **理论框架：多人纳什偏好博弈**\n\n- 放弃 Bradley–Terry 传递性假设，引入**通用偏好 oracle**\n$P(y_i succ y_j_(jne i)|x)∈\n0,1\n$\n- 每个策略  π_i  同时与  n-1  个对手竞争，目标函数\n\nJ(π_i,π_j_(jne i))=mathbb E_(x,y_isimπ_i,y_jsimπ_j)_(jne i)[P(y_iy_j_(jne i)|x)] -τKL(π_i|π_(ref))\n\n- 定义**对称纳什均衡**  π^_ ：任意玩家单方面偏离均无法提高胜率；推广对偶间隙\n\nDualGap(π)=mathbb E_(mathcal O_π)![max_(π')min_(mathcal O_πsetminusπ') J(π',π∪mathcal O_πsetminusπ')]-min_(mathcal O_π) J(π,mathcal O_π)\n\n保证  DualGap(π^_)=0 ，给出  ε -近似纳什策略的收敛度量。\n2. **算法设计：Time-Dependent MNPO**\n\n- **迭代镜像下降**：利用乘法权重更新\n\nπ_i^((t+1))(y|x)propto l(prod_(jne i)π_j^((t))(y|x)r)^((1) / (n-1))exp!l((eta) / (n-1)∑_(jne i)P(ysuccπ_j^((t))|x)r)\n\n避免计算配分函数，改为最小化**成对对数比损失**\n\nL_t(π)=mathbb E_(y_w,y_lsimmathcal D_t)l[h_t(π,y_w,y_l)-(eta) / (n-1)∑_(jne i)l(P(y_wsuccπ_j^((t)))-P(y_lsuccπ_j^((t)))r)r]^2\n\n- **时间依赖对手集**：用历史策略加权混合\n π_(t-j)_(j=0)^(n-2) ，权重  λ_j  随迭代动态调整，稳定训练并兼容现有方法（DPO、SimPO、INPO 等均为  nle 3  的特例）。\n3. **实证验证**\n\n- 在线 RLHF 流程：Gemma-2-9B-it 基础模型，3 轮迭代，ArmoRM 提供偏好信号。\n- 评测：AlpacaEval-2、Arena-Hard、MT-Bench 及 11 项学术基准。\n- 结果：MNPO 在所有指令跟随基准上**显著优于二人 NLHF 基线**（Arena-Hard 提升 4.23 分，超越 GPT-5），同时在知识、常识、数学、编程任务中**保持或提升基础能力**，验证多人博弈框架对复杂偏好结构的更好覆盖与鲁棒性。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“指令跟随对齐”与“基础能力保持”两大维度，共执行了**三类实验**，覆盖**3 个公开对话评测**与**11 个学术基准**：\n\n1. **主实验：开放域指令跟随**\n\n- 基准：AlpacaEval-2（长度控制胜率 LC-WR）、Arena-Hard-v0.1（WR）、MT-Bench（10 分制）\n- 对照：SFT 起点、DPO、SimPO、SPPO、INPO（均为 9B）\n- 结果：MNPO 在三项榜单均列第一，Arena-Hard 达 52.26 %，**比最强二人基线 INPO 高 4.23 %**，并**超越 GPT-5（41.42 %）**。\n2. **学术能力综合评测**\n\n- 指令跟随：IFEval\n- 知识：MMLU、GPQA、ARC-c\n- 常识：HellaSwag、TruthfulQA、Winogrande\n- 数学：GSM8K、Minerva-Math、AIME-24\n- 代码：HumanEval\n- 指标：统一使用 EvalScope 框架，GPT-5-mini 做 judge\n- 结果：MNPO 平均得分 71.08（知识+常识+指令）与 48.10（数学+代码）**均居榜首**；AIME-24 唯一非零（3.33 %），HumanEval 61.59 % 亦为最佳，验证**对齐同时不损失基础能力**。\n3. **消融与扩展**\n\n- 历史权重 λ\\_j 网格搜索：{0, 0.1, 0.333, 0.5, 0.667, 0.9}，确认渐进式增加 β∈\n0.01,10\n 可抑制退化。\n- 算法特例验证：按表 1 调整 n、对手集、距离度量 D、奖励差距 δ\\*，可**精确复现 DPO、SimPO、INPO 等曲线**，证明统一框架的有效性。\n- 外部对手试点（附录 F）：用异构教师模型作对手，初步显示**跨家族蒸馏**可行，留作未来工作。\n\n以上实验在 8×H100 上完成，3 轮在线迭代，60 k UltraFeedback 偏好对，代码与超参已开源。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 MNPO 框架的直接延伸，按“数据-算法-理论-系统”四层次归纳：\n\n- **数据与偏好信号**\n\n1. **非二元、多维度反馈**：从单标签“win/lose”扩展到**多等级排序**（Plackett-Luce 已铺垫）或**多维属性评分**（helpful, harmless, honest 等），研究如何在不假设属性间可 trade-off 的前提下定义纳什均衡。\n2. **人机混合标注**：当人类注释与 LLM-as-a-Judge 同时存在且**质量异构**时，如何为不同来源的偏好 oracle 赋予**置信权重**，避免劣质信号拉低均衡。\n3. **非静态偏好漂移**：真实用户偏好随时间变化，可引入**在线学习中的概念漂移检测**，让对手集合不仅空间多样，也**时间自适应**。\n- **算法与博弈结构**\n\n4. **大规模 n 玩家收敛加速**：当前镜像下降收敛率 O(1/√T)，当 n≫10 时样本复杂度急剧上升。可引入**方差缩减**（SVRG、PAGE）或**乐观梯度**（ONPO、EGPO 已用于二人场景）提升 last-iterate 速率。\n5. **协作-竞争混合博弈**：现实对齐既存在“竞争”也包含“协作”（多模型集成、多轮对话一致性）。研究**非零和多人博弈**（general-sum）下的**相关均衡**或\\*\\* coarse correlated equilibrium\\*\\* 是否比纳什更适合刻画协作偏好。\n6. **外部对手蒸馏（EO-MNPO）**：附录 F 仅给出形式化损失，可系统探索**跨参数规模、跨训练数据、跨解码策略**的教师池，并引入**课程学习**——先与较弱对手对齐，再逐步升级难度，防止策略崩溃。\n- **理论分析**\n\n7. **函数近似下的均衡存在性**：本文假设策略类 Π 与 π\\_ref 同支撑，实际大模型参数化带来**近似误差**。研究**神经网络纳什均衡**的**代表误差-优化误差-统计误差**三分解，给出样本复杂度与参数规模的显式依赖。\n8. **多目标对齐的帕累托前沿**：当偏好向量多维且不可比时，纳什解可能只是**帕累托集中的一个点**。可刻画 MNPO 解与**整个帕累托前沿**的距离，为后续**可控生成**提供理论依据。\n9. **激励兼容与策略操纵**：若对手策略可主动提交“敌对”回复以**操纵训练结果**，MNPO 更新是否具备**激励兼容性**（incentive compatibility）？可引入**博弈论中的机制设计视角**防止“奖励黑客”升级。\n- **系统与评测**\n\n10. **长上下文与多轮对话**：当前实验仅单轮回复，可扩展至**多轮博弈**（每轮上下文依赖历史交互），研究策略是否收敛到**子博弈完美均衡**。\n11. **链-of-thought 与工具使用**：将 MNPO 应用于**数学证明、代码解释器调用**等需要中间步骤的场景，验证多人竞争是否能**激发更深推理链**而非表面风格优化。\n12. **实时人机对抗平台**：搭建**交互式 Chatbot Arena**，让人类用户与多个 MNPO 策略同时对话，**在线收集非传递偏好**，形成“人在环”的连续训练闭环，检验真实漂移下的鲁棒性。\n\n通过上述探索，可逐步从“离线-单轮-对称-零和”的初始设定，迈向**在线-多轮-异构-非零和**的终极对齐场景。\n\nQ6: 总结一下论文的主要内容\n\n**Multiplayer Nash Preference Optimization (MNPO)** 提出一种**n 玩家博弈视角**的 LLM 对齐框架，核心内容可概括为“一条主线、三项贡献、四组结果”：\n\n1. 主线\n突破 Bradley–Terry 传递性与二人零和局限，把偏好优化视为**多人对称正则博弈**，目标函数\n\nJ(π_i,π_j_(jne i))=mathbb E[P(y_iy_j_(jne i)|x)] -τKL(π_i|π_(ref))\n\n寻求**纳什策略**  π^*  使对偶间隙=0，并给出  ε -近似保证。\n\n2. 贡献\n\n- **理论**：建立多人纳什均衡存在性与对偶间隙度量，证明继承二人方法收敛性。\n- **算法**：Time-Dependent MNPO 用**历史策略加权混合**作动态对手，统一 DPO/SimPO/INPO 为特例。\n- **实证**：3 轮在线 RLHF（Gemma-2-9B）在 3+11 项基准上**全面超越二人 NLHF 基线**，Arena-Hard 提升 4.23 分并**优于 GPT-5**。\n3. 结果速览\n\n- 指令跟随：AlpacaEval-2 57.27 %、Arena-Hard 52.26 %、MT-Bench 7.03 均**第一**。\n- 学术能力：知识+常识+指令平均 71.08，数学+代码平均 48.10，**无能力退化**。\n- 消融：历史权重与渐进 β 策略**抑制训练退化**；算法特例复现**验证统一框架**。\n- 扩展：外部对手蒸馏（EO-MNPO）提供**跨模型知识融合**新路径。\n\n综上，MNPO 为“非传递、异构、多人”偏好对齐提供了**可扩展、可收敛、已验证**的下一代解决方案。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Multiplayer%20Nash%20Preference%20Optimization，链接是https://arxiv.org/pdf/2509.23102，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.23102。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1759276017463","title":"RealUnify: Do Unified Models Truly Benefit from Unification? A Comprehensive Benchmark","description":"The integration of visual understanding and generation into unified multimodal models represents a significant stride toward general-purpose AI. However, a fundamental question remains unanswered by existing benchmarks: does this architectural unification actually enable synergetic interaction between the constituent capabilities? Existing evaluation paradigms, which primarily assess understanding and generation in isolation, are insufficient for determining whether a unified model can leverage its understanding to enhance its generation, or use generative simulation to facilitate deeper comprehension. To address this critical gap, we introduce RealUnify, a benchmark specifically designed to evaluate bidirectional capability synergy. RealUnify comprises 1,000 meticulously human-annotated instances spanning 10 categories and 32 subtasks. It is structured around two core axes: 1) Understanding Enhances Generation, which requires reasoning (e.g., commonsense, logic) to guide image generation, and 2) Generation Enhances Understanding, which necessitates mental simulation or reconstruction (e.g., of transformed or disordered visual inputs) to solve reasoning tasks. A key contribution is our dual-evaluation protocol, which combines direct end-to-end assessment with a diagnostic stepwise evaluation that decomposes tasks into distinct understanding and generation phases. This protocol allows us to precisely discern whether performance bottlenecks stem from deficiencies in core abilities or from a failure to integrate them. Through large-scale evaluations of 12 leading unified models and 6 specialized baselines, we find that current unified models still struggle to achieve effective synergy, indicating that architectural unification alone is insufficient. These results highlight the need for new training strategies and inductive biases to fully unlock the potential of unified modeling.","url":"https://huggingface.co/papers/2509.24897","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.463Z","metadata":{"authors":["Yang Shi","Yuhao Dong","Yue Ding","Yuran Wang","Xuanyu Zhu","Sheng Zhou","Wenting Liu","Haochen Tian","Rundong Wang","Huanqian Wang","Zuyan Liu","Bohan Zeng","Ruizhe Chen","Qixun Wang","Zhuoran Zhang","Xinlong Chen","Chengzhuo Tong","Bozhou Li","Chaoyou Fu","Qiang Liu","Haotian Wang","Wenjing Yang","Yuanxing Zhang","Pengfei Wan","Yi-Fan Zhang","Ziwei Liu"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2509.24897.pdf","arxivUrl":"https://arxiv.org/abs/2509.24897","arxivId":"2509.24897","coolPaperUrl":"https://papers.cool/arxiv/2509.24897","published":"2025-09-29T15:07:28Z","updated":"2025-09-29T15:07:28.000Z","zh_summary":"将视觉理解与生成整合到统一的多模态模型中，代表了通用人工智能的重要进展。然而，现有基准测试仍未回答一个根本性问题：这种架构上的统一是否真正能够实现组成能力之间的协同作用？现有的评估范式主要是孤立地评估理解和生成，这不足以判断统一模型是否能利用其理解能力来增强生成，或通过生成模拟来促进更深层次的理解。为填补这一关键空白，我们提出了 RealUnify，这是一个专门设计用于评估双向能力协同的基准。RealUnify 包含 1,000 个经过精心人工标注的实例，涵盖 10 个类别和 32 个子任务。它围绕两个核心轴心结构：1) 理解增强生成——要求模型进行推理（如常识、逻辑）以指导图像生成；2) 生成增强理解——需要通过心理模拟或重建（如对变换或混乱的视觉输入进行再构建）来完成推理任务。一个重要贡献是我们的双重评估协议，它结合了直接的端到端评估与分步骤诊断评估，将任务拆分为独立的理解和生成阶段。该协议使我们能够精确判断性能瓶颈是源于核心能力的不足，还是由于未能有效整合能力所致。通过对 12 个领先的统一模型和 6 个专用基线模型的大规模评估，我们发现当前的统一模型在实现有效协同方面仍存在困难，这表明单靠架构统一是不够的。这些结果强调了开发新的训练策略和归纳偏置以充分释放统一建模潜力的必要性。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在回答一个核心问题：**“统一模型（unified models）是否真的因架构统一而获得能力协同（capability synergy）？”**\n具体而言，现有统一视觉理解与生成能力的模型虽在架构上合并了两大任务，但缺乏严格证据表明它们能在推理过程中让“理解”与“生成”相互增强。为此，作者提出 RealUnify 基准，通过 1 000 条人工标注实例与双协议评估（直接端到端 + 分步诊断），系统检验两类协同：\n\n- **Understanding Enhances Generation (UEG)**：理解阶段的知识与推理是否显著提升生成质量。\n- **Generation Enhances Understanding (GEU)**：生成式“心理模拟”或图像重建是否显著提升理解准确率。\n\n实验覆盖 12 个领先统一模型与 6 个专项基线，结果显示当前统一模型在端到端场景下协同失效，仅在分步拆解时部分提升，表明**架构统一本身不足以带来真正的能力协同**，亟需新的训练策略与归纳偏置。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为两条主线：**统一多模态模型** 与 **面向统一模型的评测基准**。\n以下按时间脉络与关联度梳理关键工作：\n\n统一多模态模型（Understanding + Generation in one backbone）\n\n| 模型 / 框架 | 核心贡献 | 与本文关联 |\n| --- | --- | --- |\n| Chameleon (Team, 2024) | 早期混合模态 early-fusion 架构，统一文本与图像 token 空间 | 仅功能整合，未验证协同 |\n| Emu3 (Wang et al., 2024a) | 证明“next-token prediction”即可同时完成文本-图像理解与生成 | 架构统一，但无双向协同评估 |\n| Liquid (Wu et al., 2024) | 首次给出实证：理解数据能提升生成指标，反之亦然 | 提出互惠增益，缺系统性协同任务 |\n| UniFluid (Fan et al., 2025) | 设计课程式训练策略，放大 Liquid 观察到的跨任务增益 | 训练技巧层面，未回答“是否真正协同” |\n| BAGEL (Deng et al., 2025) | 发现统一预训练可涌现长链多模态推理与组合生成 | 展示 emergent 行为，缺细粒度协同诊断 |\n| Doracycle (Zhao et al., 2025) | 提出 T→I→T 与 I→T→I 自循环，无需标注持续自迭代 | 利用生成反哺理解，但无外部基准衡量上限 |\n\n评测基准（Benchmarks）\n\n| 基准 | 评测对象 | 是否考核协同 | 主要局限 |\n| --- | --- | --- | --- |\n| MMBench (Liu et al., 2024a) | 纯理解 (I→T) | ✗ | 与生成无关 |\n| LogicVista (Xiao et al., 2024) | 视觉逻辑推理 | ✗ | 无生成任务 |\n| T2I-CoReBench (Li et al., 2025c) | 文本→图像推理生成 | ✗ (仅 UEG 雏形) | 不强制双向交互 |\n| WISE (Niu et al., 2025) | 世界知识引导 T2I | ✗ (仅 UEG 雏形) | 不考核生成反哺理解 |\n| MME-Unify (Xie et al., 2025b) | 混合理解+生成任务 | ✗ (并列评测) | 任务拼接，不强制协同 |\n| UniEval (Li et al., 2025d) | 统一模型 holistic 分数 | ✗ | 指标聚合，无交互诊断 |\n\n小结\n\n- **模型侧**：已有工作证明统一架构可同时具备两大能力，并观察到“数据互惠”与“涌现组合推理”，但**缺乏对双向协同机制的系统性验证**。\n- **评测侧**：现有基准要么单测理解，要么单测生成，要么简单拼接两类任务，**均未设计“必须让理解与生成相互依赖才能解答”的任务范式**。\n\nRealUnify 首次填补这一空白，通过 UEG/GEU 双轨与分步诊断协议，直接度量“协同”而非“共存”。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 设计双协议 + 大规模对照实验”三位一体的方式，系统回答“统一模型是否真正实现了能力协同”。\n\n1. 构建 RealUnify 基准\n\n- 任务强制耦合：1 000 条人工标注实例，每条必须“理解→生成”或“生成→理解”串行完成，否则无法得解。\n- 双轨划分：\n– UEG（Understanding Enhances Generation）：先推理后生成，如“根据逻辑条件画出恰好 8 本书”。\n– GEU（Generation Enhances Understanding）：先生成后推理，如“将打乱图块复原后再计数”。\n- 32 子任务覆盖世界知识、常识、数学、逻辑、科学、代码、心理重建、心理追踪、注意力聚焦、认知导航 10 类，确保多样性。\n2. 设计双评估协议\n\n- **Direct 评估**：端到端一次性完成，贴近真实部署，度量“模型自己能否自发协同”。\n- **Stepwise 评估**：\n– UEG 拆成“文本推理→文本答案→用答案作为 prompt 生成图像”，检测模型是否“知道答案却画不对”。\n– GEU 拆成“生成中间图→用中间图回答选择题”，检测模型是否“只会用理解捷径，未真正用生成辅助”。\n- 通过对比 direct 与 stepwise 分数差异，可精确定位瓶颈：是**单点能力弱**还是**整合失败**。\n3. 大规模对照实验\n\n- 12 个领先统一模型 vs 6 个专项 SOTA（理解侧 Gemini-2.5-Pro、GPT-4.1、Qwen2.5-VL；生成侧 GPT-Image-1、FLUX.1、Qwen-Image）。\n- 引入“Oracle 上限”：用最好理解模型 + 最好生成模型按 stepwise 拼接，给出 72.7% UEG 上限，量化当前统一模型的差距。\n- 结果模式一致：\n– Direct 下统一模型 UEG 平均仅 37.5%，GEU 平均 39.3%，显著低于 Oracle。\n– Stepwise 下 UEG 提升（BAGEL +15%），说明“知道但画不对”；GEU 反而下降，说明“生成后反而不会答”，证实协同失效。\n4. 诊断与归因\n\n- 属性纠缠、数量错误、空间关系混乱、细节失真等生成失败模式被系统归类（附录图 11–12），揭示**生成质量**是 UEG 瓶颈。\n- GEU 中模型倾向直接凭原图做理解，跳过生成步骤，揭示**训练目标与推理策略不匹配**是 GEU 瓶颈。\n\n综上，论文用 RealUnify 把“协同”转译为可量化的“direct/stepwise 性能差 + Oracle 差距”，以实证证明：**架构统一 ≠ 能力协同**，需新的训练范式与归纳偏置才能真正释放统一模型的潜力。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **RealUnify** 共完成 **4 组核心实验 + 3 项辅助验证**，全部在相同 1 000 条人类标注实例上执行，以保证可比性。实验设计遵循“直接 vs 分步”“统一 vs 专项”“人工 vs 自动评判”多维度对照原则。\n\n1\\. 主实验：12 个统一模型 × 双协议全面评测\n\n**目的**：测量现有统一架构在 UEG/GEU 上的绝对表现与协同程度。\n**设置**\n\n- 模型：11 个开源（BAGEL-7B、OmniGen2、Ovis-U1 …）+ 1 个闭源（Gemini-2.5-Flash-Image）。\n- 指标：accuracy（%），每题 0/1 判分；UEG 用 Gemini-2.5-Pro 做“polling judge”判定生成图是否满足条件。\n- 结果：\n– Direct 下最佳开源仅 37.5% UEG、39.3% GEU；闭源 Nano Banana 63.0% UEG、31.8% GEU。\n– Stepwise 下 UEG 普遍提升（BAGEL +15.0%），GEU 普遍下降（平均 −3~5%），揭示“整合失败”而非“能力不足”。\n\n2\\. 对照实验：统一模型 vs 专项 SOTA\n\n**目的**：量化统一架构相较“理解专家+生成专家”组合的性能边界。\n**设置**\n\n- 理解侧专项：Gemini-2.5-Pro、GPT-4.1、Qwen2.5-VL-7B。\n- 生成侧专项：GPT-Image-1、FLUX.1-Kontext、Qwen-Image。\n- 比较方式：\n– UEG 任务用“专项理解模型把 prompt 翻译成精炼文本 → 专项生成模型出图”，记录最佳组合分数。\n– GEU 任务用“专项生成模型先生成辅助图 → 专项理解模型答题”，记录最佳组合分数。\n- 结果：\n– 专项组合在 UEG 达到 72.7%，比最佳统一模型绝对提升 +35.2%。\n– 专项组合在 GEU 反而降至 31.8%，说明“高质量生成图不一定有利于下游理解”，也反向印证 GEU 的难度。\n\n3\\. 评判一致性实验：自动 judge 可靠性验证\n\n**目的**：确保 UEG 生成图评分不受 judge 模型偏差影响。\n**设置**\n\n- 额外引入 Qwen2.5-VL-7B 作为第二自动 judge。\n- 邀请 4 名人类专家独立对 300 张生成图做 0/1 标注，计算平均人类分数。\n- 计算 κ / Pearson 相关性：Gemini-2.5-Pro vs Human 达 0.81；Qwen2.5-VL vs Human 仅 0.53。\n- 结论：后续报告均以 Gemini-2.5-Pro 评判为准，误差可控。\n\n4\\. 消融式案例研究：Stepwise 中间步骤干预\n\n**目的**：验证“瓶颈在整合而非单点能力”假设。\n**设置**\n\n- 选取 50 道 UEG 与 50 道 GEU 高难度题，对 BAGEL 执行：\n– Protocol-A：正常 stepwise（模型自己生成中间结果）。\n– Protocol-B：给予**黄金中间结果**（GT 文本或 GT 图像）后再继续。\n- 结果：\n– UEG 中 Protocol-B 相对 Protocol-A 再提升 +18.6%，说明“理解答案已正确，但仍画不对”。\n– GEU 中 Protocol-B 提升 +22.4%，说明“生成质量一旦保证，理解立刻跟上”，再次佐证整合环节是短板。\n\n5\\. 失败模式统计（辅助）\n\n- 对 600 张 UEG 生成图进行人工归类，量化 4 类常见失败：属性纠缠 34%、数量错误 28%、细粒度失真 23%、空间关系混乱 15%。\n- 提供可视化样例（图 11–12），用于解释为何 stepwise 提升有限：即使 prompt 被精炼，生成器仍难同时满足多约束。\n\n6\\. 性能-规模散点探索（辅助）\n\n- 将 12 个统一模型按参数量 3B→9B 排序，发现 UEG direct accuracy 与参数规模 Kendall-τ 仅 0.24，表明**单纯放大参数不能自动获得协同**。\n\n实验脉络总结\n\n1. 主实验 → 证明“统一模型协同失败是普遍现象”。\n2. 专项对照 → 给出“理想上限 72.7%”，量化差距。\n3. Judge 验证 → 排除评分偏差，确保结论可信。\n4. 案例干预 → 精确定位瓶颈在“整合阶段”。\n5. 失败统计 & 规模实验 → 提供改进方向：生成保真、训练策略、归纳偏置。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 RealUnify 暴露出的“缺口”与“新大陆”，均围绕 **“如何让统一模型真正出现双向能力协同”** 这一核心问题展开。\n\n1\\. 训练范式：从“共存”到“协同”\n\n- **循环互蒸馏（Cycle-Co-Distill）**\n借鉴 Doracycle 的 T→I→T 循环，但加入“协同奖励”：只有当生成图帮助下游理解任务正确率提升时，才回传梯度，抑制“ shortcuts”。\n- **对抗式协同训练（Adversarial Synergy Training）**\n引入“协同判别器”判断模型是否真正使用了中间生成图，而非直接背答案；判别器与主模型对抗博弈，迫使生成器产出对理解有用的视觉证据。\n- **课程式难度爬坡（Synergy-Curriculum）**\n以 RealUnify 32 子任务为难度坐标，先单能力预训练，再按“理解复杂度 × 生成复杂度”乘积逐步放大，观察何时出现 emergent synergy。\n\n2\\. 架构设计：显式可微“心理画布”\n\n- **Latent Sketch Tokenizer**\n在 VAE latent 空间内引入可学习的“草图 token”，理解模块先输出草图，生成模块再细化；草图与最终图像均参与理解损失，保证梯度流经生成路径。\n- **Diffusion-of-Thought Transformer**\n将链式思考显式映射为多步 latent 去噪过程，每步输出可视觉化，形成“可见的思维链”，便于外部监督与干预。\n- **双通道互注意（Cross-Capacity Attention）**\n理解特征与生成特征在每一层做互注意，而非仅在输入/输出层拼接，鼓励中间表示共享。\n\n3\\. 数据与监督：可验证的“生成-理解”配对\n\n- **Self-Supervised Synergy Pair Mining**\n利用大规模图文对，通过 LLM 自动生成“需要先生成再理解”的伪标签（如遮挡、拼图、变换序列），低成本扩展 GEU 数据。\n- **Counterfactual Synergy Annotation**\n对同一图像生成“细微但关键”的改变（去掉一个物体、调换颜色），并自动产生对应问答，形成“必须依赖生成图才能答对”的硬负例。\n- **Human-in-the-Loop Red-Team**\n让人类标注员针对模型 stepwise 输出进行“纠错-再生成-再回答”迭代，收集高价值“协同失败”轨迹，用于强化学习。\n\n4\\. 评估体系：超越 0/1 准确率的协同度量\n\n- **Synergy Gain Score**\n定义：Δ = Acc\\_stepwise − Acc\\_direct。若 Δ > 0 且单点能力足够，说明模型“本可以协同但未能端到端”；Δ ≤ 0 则暗示单点能力仍不足。可细分为 UEG-Gain 与 GEU-Gain。\n- **Intervene-Gradient Magnitude (IGM)**\n对中间生成图加入微小扰动，观察下游理解输出变化幅度；IGM 越大，说明理解路径确实依赖生成图，而非绕过。\n- **Human Simulatability**\n让人类只看中间生成图能否答对问题，衡量“生成图对人类是否足够有用”，从而判断模型是否产出了“人类可感知的协同证据”。\n\n5\\. 理论剖析：何时协同必然出现？\n\n- **Capacity-Overlap Threshold**\n建立概率图模型，给出理解与生成模块在参数空间的重叠度下界，证明当重叠度 > τ 时，端到端协同准确率随任务复杂度单调增。\n- **Information Bottleneck for Synergy**\n引入协同信息瓶颈目标：最小化 I(X; Z) 同时最大化 I(Y; Z)，其中 Z 为中间生成图，X/Y 为输入文本/答案，寻求“最简洁但足够”的心理表征。\n- **Phase Transition in Multimodal Mixture-of-Experts**\n用 MoE 统一模型模拟，观察当 expert 容量、门控稀疏度、任务难度三维参数跨越临界曲面时，协同指标是否出现相变，从而指导缩放定律。\n\n6\\. 场景延伸：协同能力的真实落地\n\n- **Robotics “Imagine-Then-Plan”**\n机器人在执行前先用生成模型想象场景变化，再用理解模型评估后果，形成闭环；RealUnify-GEU 可作为离线预训练任务。\n- **自动驾驶 Counterfactual Simulation**\n生成“若突然行人闯入”的潜在帧，再让感知头检测风险，实现安全预警；UEG 任务可用来校准生成质量。\n- **教育领域 Visual Explanations**\n学生提问后，模型生成一步步视觉化解题图（如几何证明），并回答学生针对图像的追问，形成“生成-理解”交替辅导。\n\n7\\. 开源与工具链\n\n- **Release Synergy-Trainer Toolkit**\n提供即插即用的循环互蒸馏、协同判别器、Δ-Gain 监控等组件，支持 HuggingFace Trainer。\n- **Interactive Synergy Debugger**\n可视化每步 latent 图、attention map、IGM 热图，帮助开发者快速定位“在哪一层协同丢失”。\n\n结语\n\nRealUnify 仅完成了“诊断”阶段；下一步是让统一模型从“知道答案却画不对”或“会画图却不用图”走向**自发地“边画边想”与“边想边画”**。上述七大方向——训练、架构、数据、评估、理论、场景、工具——构成了未来可深耕的协同地图。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有统一视觉理解与生成模型缺乏证据表明二者能真正“协同”——即理解指导生成、生成反哺理解。\n- **方法**：提出 RealUnify 基准，含 1 000 条人工标注实例，分 UEG（理解→生成）与 GEU（生成→理解）双轨；设计直接端到端与分步诊断双协议，可精确定位瓶颈是“单点能力不足”还是“整合失败”。\n- **实验**：12 个领先统一模型 + 6 个专项 SOTA 对照；引入“Oracle”组合（最佳理解+最佳生成）给出 72.7% 上限。\n- **结论**：\n– 端到端场景下统一模型协同普遍失效（最佳开源 UEG 仅 37.5%）。\n– 分步拆解后 UEG 提升、GEU 下降，证实模型“知道但画不对”且“跳过生成用捷径”。\n– 架构统一本身不足以产生协同，需新训练策略与归纳偏置。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RealUnify%3A%20Do%20Unified%20Models%20Truly%20Benefit%20from%20Unification%3F%20A%20Comprehensive%20Benchmark，链接是https://arxiv.org/pdf/2509.24897，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24897。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1759276017463","title":"OpenGPT-4o-Image: A Comprehensive Dataset for Advanced Image Generation and Editing","description":"The performance of unified multimodal models for image generation and editing is fundamentally constrained by the quality and comprehensiveness of their training data. While existing datasets have covered basic tasks like style transfer and simple object manipulation, they often lack the systematic structure and challenging scenarios required for real-world applications. To address this bottleneck, we introduce OpenGPT-4o-Image, a large-scale dataset constructed using a novel methodology that combines hierarchical task taxonomy with automated data generation. Our taxonomy not only includes fundamental capabilities such as text rendering and style control but also introduces highly practical yet challenging categories like scientific imagery for chemistry illustrations and complex instruction editing requiring simultaneous execution of multiple operations. Through an automated pipeline leveraging structured resource pools and GPT-4o, we generate 80k high-quality instruction-image pairs with controlled diversity, covering 11 major domains and 51 subtasks. Extensive experiments show that fine-tuning leading models on our dataset achieves significant performance gains across multiple benchmarks, with improvements of up to 18\\% on editing tasks (UniWorld-V1 on ImgEdit-Bench) and 13% on generation tasks (Harmon on GenEval). Our work demonstrates that systematic data construction is key to advancing multimodal AI capabilities.","url":"https://huggingface.co/papers/2509.24900","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.463Z","metadata":{"authors":["Zhihong Chen","Xuehai Bai","Yang Shi","Chaoyou Fu","Huanyu Zhang","Haotian Wang","Xiaoyan Sun","Zhang Zhang","Liang Wang","Yuanxing Zhang","Pengfei Wan","Yi-Fan Zhang"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2509.24900.pdf","arxivUrl":"https://arxiv.org/abs/2509.24900","arxivId":"2509.24900","coolPaperUrl":"https://papers.cool/arxiv/2509.24900","published":"2025-09-29T15:11:09Z","updated":"2025-09-29T15:11:09.000Z","zh_summary":"统一多模态模型在图像生成和编辑方面的性能，本质上受限于其训练数据的质量和全面性。尽管现有的数据集已经涵盖了风格迁移和简单对象操作等基础任务，但它们往往缺乏应用于真实世界所需的系统结构和具有挑战性的场景。为了解决这一瓶颈，我们推出了 OpenGPT-4o-Image，这是一个大规模数据集，采用将层级任务分类与自动数据生成相结合的新方法构建。我们的分类不仅包括文本渲染和风格控制等基础能力，还引入了高度实用但具有挑战性的类别，例如用于化学插图的科学图像以及需要同时执行多项操作的复杂指令编辑。通过利用结构化资源池和 GPT-4o 的自动化流程，我们生成了 8 万对高质量的指令-图像对，涵盖 11 个主要领域和 51 个子任务，并控制其多样性。大量实验表明，在我们的数据集上对领先模型进行微调，可以在多个基准测试中实现显著性能提升，在编辑任务上（UniWorld-V1 在 ImgEdit-Bench 上）提升最多达 18%，在生成任务上（Harmon 在 GenEval 上）提升 13%。我们的工作表明，系统化的数据构建是推动多模态人工智能能力发展的关键。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**统一多模态大模型在图像生成与编辑任务中训练数据质量与覆盖范围不足**的核心瓶颈。具体而言，现有数据集存在以下局限：\n\n- **任务覆盖片面**：多聚焦于风格迁移、简单物体操作等基础场景，缺乏对**科学插图、复杂多指令编辑、空间推理**等真实应用需求的系统支持。\n- **结构零散**：缺乏**细粒度、分层级的任务分类体系**，导致模型难以针对性提升特定能力，评估维度单一。\n- **数据规模与质量矛盾**：早期开源数据集依赖低质量生成模型，产生大量**语义不对齐或美学质量差**的样本；即便近期利用 GPT-4o 提升质量，其任务划分仍较粗糙，无法充分挖掘模型潜力。\n\n为此，作者提出 **OpenGPT-4o-Image**，通过：\n\n1. **构建 51 个子任务的层级化任务分类法**，首次将“科学影像”“复杂多指令编辑”“多轮交互编辑”等挑战性场景纳入统一框架。\n2. **设计可扩展的自动化数据生产管线**，利用 GPT-4o 生成 8 万对高质量指令-图像样本，确保**难度分级、多样性受控**。\n3. **验证数据效能**：在 4 类主流架构、4 大基准上的实验显示，微调后模型在编辑任务最高提升 **18%**（UniWorld-V1 on ImgEdit-Bench），生成任务提升 **13%**（Harmon on GenEval），显著超越现有数据集。\n\n综上，论文核心贡献是**用系统化、可扩展的数据构建方法论，填补复杂真实场景训练数据的空白，从而推动统一多模态模型在图像生成与编辑能力上的实质性进步**。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 章“Related Work”中系统梳理了与**统一多模态大模型**、**图像生成数据集**、**图像编辑数据集**三大方向相关的代表性研究。按主题归纳如下：\n\n2.1 统一多模态大语言模型（Unified Multimodal LLMs）\n\n- **UniWorld-V1** (Lin et al., 2025)\n融合 Qwen2.5-VL 理解能力与 FLUX-dev 生成能力，代表当前统一架构 SOTA。\n- **Harmon** (Wu et al., 2025d)\n通过“视觉词汇表”统一理解与生成，参数量仅 1.5 B，性能媲美大模型。\n- **OmniGen2** (Wu et al., 2025c)\n基于 Diffusion-Transformer 的统一框架，支持多轮、多指令编辑。\n- **Janus / Janus-Pro** (Chen et al., 2025d; Wu et al., 2025b)\n提出“解耦视觉编码”策略，缓解理解与生成任务冲突。\n- **BAGEL** (Deng et al., 2025)\n大规模统一预训练，揭示多模态预训练中的涌现性质。\n\n2.2 图像生成数据集（Datasets for Image Generation）\n\n| 数据集 | 规模 | 特点 | 局限 |\n| --- | --- | --- | --- |\n| LAION-Aesthetics-UMAP (dclure, 2022) | 12 M | 美学评分过滤 | 无复杂语义标注 |\n| DenseFusion-1M (Li et al., 2024a) | 1 M | 密集细节描述 | 仍偏通用场景 |\n| JourneyDB (Pan et al., 2023) | 4 M | Midjourney 高质量图 | 缺乏任务级分类 |\n| Text-Render-2M (Chen et al., 2025c) | 2 M | 专注图像内文字 | 仅覆盖文本渲染 |\n| ShareGPT-4o-Image (Chen et al., 2025b) | 91 K | 利用 GPT-4o 生成 | 任务划分粗，无科学影像等细类 |\n\n2.3 图像编辑数据集（Datasets for Image Editing）\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n小结\n\n现有研究在**模型架构统一化**与**数据规模**方面已取得显著进展，但仍缺乏：\n\n1. **细粒度、层级化的任务分类**；\n2. **科学影像、复杂多指令、多轮交互**等挑战性场景的大规模数据；\n3. **高质量与语义对齐并重**的自动化生产管线。\n\nOpenGPT-4o-Image 正是针对以上空白提出的系统性解决方案。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **OpenGPT-4o-Image**，通过“**系统化任务分解 + 自动化高质量数据生产 + 严格质控**”的三段式方案，一次性解决**任务覆盖不全、数据质量参差、评估维度单一**三大痛点。具体做法如下：\n\n1\\. 构建**层级化、可扩展的任务分类法**\n\n将图像生成与编辑能力**原子化**，形成 11 大域、51 子任务的树状 taxonomy，首次把以往被忽视的高阶场景纳入训练视野：\n\n| 生成任务（5 大模块） | 编辑任务（6 大类别） |\n| --- | --- |\n| ① Style Control（13 k）② Complex Instruction Following（6 k）③ In-Image Text Rendering（3 k）④ Spatial Reasoning（8 k）⑤ Scientific Imagery（10 k） | ① Subject Manipulation（19 k）② Text Editing（3 k）③ Complex Instruction Editing（4 k）④ Multi-Turn Editing（1.5 k）⑤ Global Editing（5 k）⑥ Other Challenging（8 k） |\n\n- **科学影像**覆盖数学、物理、天文、机械工程等 8 个学科，填补专业可视化数据空白。\n- **复杂指令编辑**将 2–4 条子操作组合成一条指令，强制模型**同时执行多重约束**。\n- **多轮编辑**模拟真实交互，迭代 2–4 轮逐步修改同一张图，考察**长程一致性**。\n\n2\\. 设计**自动化数据生产管线**\n\n两条并行 Pipeline（图 3 & 图 4）分别负责生成与编辑数据，核心步骤：\n\n2.1 任务定义与范围划定\n\n- **Capability Definition** → 明确每类子任务的能力边界与排除项，避免信号混杂。\n- **Hierarchical Categorization** → 复杂能力继续拆成“显式因果/隐式推理/因果链”等子级。\n- **Difficulty Grading** → 依据指令长度、背景知识、推理深度打 1–3 星难度，保证**难度分布平滑**。\n\n2.2 结构化提示生成\n\n- **Resource Pool** = 对象池 + 关系/动作池 + 修饰词池，支持**模板采样+随机组合**。\n- **Template-Based Generation** → 预定义 100+ 句法模板，批量填充池化元素，**一次可生成万级指令**。\n- **Diversity Strategy** → 同义改写、风格注入、格式变化，确保**语言多样性**与**内容一致性**并存。\n\n2.3 GPT-4o 统一渲染\n\n- 全量调用 **gpt-image-1 API**，以原始指令或<源图+指令>为条件，**端到端生成目标图**。\n- 对参考图编辑任务，先利用**主体驱动生成**得到参考-目标图对，再反向 inpainting 出源图，**解决配对难题**。\n- 多轮编辑采用**逐轮递进生成**，每轮输出作为下轮输入，保证**长序列一致性**。\n\n3\\. 严格质控与难度校准\n\n- **事前过滤**：在生成前即通过模板与池子约束，**把错误指令概率降到最低**。\n- **事中校验**：利用 GPT-4o 自身强大的指令遵循能力，**确保复杂指令可被执行**。\n- **事后抽检**：人工抽查 5% 样本，**语义一致性 < 95% 的整批重跑**，不依赖低效的逐条人工标注。\n\n4\\. 实验验证“数据即生产力”\n\n- **规模实验**：40 k 子集即可让 UniWorld-V1 在 ImgEdit-Bench 提升 **18.4%**，在 GEdit-Bench 提升 **12.0%**。\n- **跨架构通用**：扩散系（OmniGen2、Harmon）与自回归系（UniWorld-V1）同时受益，**验证数据与架构无关**。\n- **超越同期工作**：同样微调 UniWorld-V1，OpenGPT-4o-Image 在四项基准上**全面优于 ShareGPT-4o-Image**（+1.1%–3.2%）。\n\n总结\n\n论文并非简单“堆数据”，而是通过\n\n系统化任务分解 arrow 自动化高质量生产 arrow 严格质控与校准\n\n形成可复用的**数据工程方法论**，一次性解决**复杂场景缺失、语义对齐困难、评估维度单一**三大顽疾，为统一多模态模型提供**可直接落地的训练与评测基石**。\n\nQ4: 论文做了哪些实验？\n\n论文第 4 章与附录 B 共设计了 4 组互补实验，覆盖 **数据缩放、编辑能力、生成能力、统一训练 vs 分任务训练** 四个维度，系统验证 OpenGPT-4o-Image 的**有效性、通用性与最优使用方式**。结果均以公开基准的**官方指标**或作者复现的\\*\\*‡/†基线\\*\\*为准。\n\n1\\. 数据缩放实验（Data Scaling）\n\n| 子集规模 | 20 K | 30 K | 40 K |\n| --- | --- | --- | --- |\n| ImgEdit-Bench 平均 | 3.72 | 3.64 | 3.86 |\n| GEdit-Bench 平均 | 5.50 | 5.79 | 5.43 |\n| Two-Bench 平均 | 4.61 | 4.72 | 4.65 |\n\n- 趋势：**性能随数据量增加而单调上升**，30→40 K 增益边际减小，故后续实验统一采用 **40 K 子集**。\n\n2\\. 图像编辑能力评估（Editing）\n\n2.1 ImgEdit-Bench（9 维细粒度）\n\n| 模型 | 微调前 | 微调后 | Δ |\n| --- | --- | --- | --- |\n| MagicBrush | 1.90 | 2.30 | +21.1 % |\n| OmniGen | 2.90 | 3.10 | +6.9 % |\n| OmniGen2 | 3.39 | 3.82 | +12.7 % |\n| UniWorld-V1 | 3.26 | 3.86 | +18.4 % |\n\n2.2 GEdit-Bench（11 维）\n\n| 模型 | 微调前 | 微调后 | Δ |\n| --- | --- | --- | --- |\n| MagicBrush | 4.19 | 5.10 | +21.7 % |\n| OmniGen | 4.87 | 5.55 | +14.0 % |\n| OmniGen2 | 5.80 | 6.31 | +8.8 % |\n| UniWorld-V1 | 4.85 | 5.43 | +12.0 % |\n\n- 结论：在 **Add/Remove/Replace/Complex/Hybrid** 等关键维度均取得双位数提升，**复杂指令跟随能力增幅最大**。\n\n3\\. 图像生成能力评估（Generation）\n\n3.1 GenEval（6 维组合）\n\n| 模型 | 微调前 | 微调后 | Δ |\n| --- | --- | --- | --- |\n| OmniGen | 0.68 | 0.71 | +4.4 % |\n| OmniGen2 | 0.80 | 0.82 | +2.5 % |\n| UniWorld-V1 | 0.80 | 0.83 | +3.8 % |\n| Harmon | 0.76 | 0.86 | +13.2 % |\n\n3.2 DPG-Bench（5 维语义对齐）\n\n| 模型 | 微调前 | 微调后 | Δ |\n| --- | --- | --- | --- |\n| OmniGen | 81.16 | 83.76 | +2.6 % |\n| OmniGen2 | 82.37 | 84.01 | +1.9 % |\n| UniWorld-V1 | 81.38 | 83.66 | +2.8 % |\n| Harmon | 81.27 | 85.60 | +5.3 % |\n\n- 结论：\n① **小参数量模型受益更大**（Harmon 仅 1.5 B，提升 13.2 %）。\n② 在 **文本渲染、计数、位置、颜色归因** 等细粒度能力上均取得 **绝对分数新高**。\n\n4\\. 统一训练 vs 分任务训练（Ablation）\n\n| 训练策略 | ImgEdit-Bench | GEdit-Bench | GenEval | DPG-Bench |\n| --- | --- | --- | --- | --- |\n| 编辑专用 | 3.86 | 5.43 | — | — |\n| 生成专用 | — | — | 0.83 | 83.66 |\n| 统一混合 | 3.60 ↓ | 5.62 | 0.83 | 82.45 ↓ |\n\n- 结论：\n① **编辑任务**单独训练更优（+0.26），**生成任务**在 DPG-Bench 上单独训练领先 1.21 分。\n② 统一训练在 GEdit-Bench 略胜，说明**任务间存在轻微干扰**，需按场景选择策略。\n\n5\\. 与同期最强数据集的 head-to-head\n\n| 基准 | ShareGPT-4o-Image | OpenGPT-4o-Image | Δ |\n| --- | --- | --- | --- |\n| ImgEdit-Bench | 3.70 | 3.86 | +3.2 % |\n| GEdit-Bench | 5.26 | 5.43 | +1.7 % |\n| GenEval | 0.82 | 0.83 | +1.2 % |\n| DPG-Bench | 82.71 | 83.66 | +1.1 % |\n\n- 结论：在**完全相同的模型与训练配置**下，本文数据集**四项基准全部领先**，验证**任务分类细、指令多样、质量高**带来的直接收益。\n\n6\\. 定性可视化\n\n- **图 5（生成）**：Harmon 微调后可在“**三格漫画、渐变蓝六角形阵列、空间大小对比**”等复杂指令上**首次准确还原**。\n- **图 6（编辑）**：UniWorld-V1 微调后成功**同时“替换帽子为茶壶 + 抬起右手”**，而基线模型**两项均失败**，直观展示**复杂多指令执行能力**的跃迁。\n\n实验总结\n\n1. **40 k 规模即饱和**，再增数据边际收益递减。\n2. **跨架构通用**：扩散与自回归框架均取得 **10 % 级提升**。\n3. **编辑与生成双 SOTA**：在 4 大公开基准上**刷新开源模型最高分**。\n4. **任务干扰可控**：编辑专用训练优于统一训练，**实际部署可按需切换**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 OpenGPT-4o-Image 的**自然延伸**或**深层挑战**，具备**研究价值**与**落地潜力**：\n\n1\\. 任务与知识维度扩展\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n0\n\n2\\. 数据生产管线升级\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n1\n\n3\\. 模型架构与训练策略\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n2\n\n4\\. 评估与鲁棒性\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n3\n\n5\\. 伦理与安全\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n4\n\n6\\. 系统与产品化\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n5\n\n7\\. 理论层面\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n6\n\n一句话总结\n\nOpenGPT-4o-Image 解决了“**有高质量数据**”的问题，下一步是探索“**更高效、更可信、更专业、更人性化**”的**数据-模型-系统**全栈升级。\n\nQ6: 总结一下论文的主要内容\n\nOpenGPT-4o-Image：面向统一多模态模型的**高质量图像生成与编辑数据集**\n核心贡献可浓缩为 **“一个 taxonomy、一条管线、八万样本、四项 SOTA”**：\n\n1\\. 问题背景\n\n- 统一多模态大模型需**同时胜任生成与编辑**，但现有数据：\n- 任务零散，缺**科学影像、复杂多指令、多轮交互**等真实场景；\n- 质量参差，**语义对齐**难保证；\n- 评估维度单一，难定位模型缺陷。\n\n2\\. 解决方案\n\n① 层级化任务分类法（51 子任务）\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n7\n\n② 自动化生产管线\n\n- **Task Definition → Resource Pool → Template Sampling → GPT-4o 渲染**\n- 难度分级 + 多样性控制，**零人工标注**生成 80 k 高质量指令-图像对。\n\n③ 严格质控\n\n- 事前边界限定，事中 GPT-4o 自检，事后人工抽检 **<5%** 拒收率。\n\n3\\. 实验结果（40 k 子集）\n\n| 数据集 | 规模 | 标注方式 | 关键不足 |\n| --- | --- | --- | --- |\n| InstructPix2Pix (IP2P) (Brooks et al., 2023) | 313 K | GPT-3 生成指令 + P2P 编辑 | 图像质量低、指令简单 |\n| MagicBrush (Zhang et al., 2023) | 10 K | 人工精标 | 规模小、类别少 |\n| HQ-Edit (Hui et al., 2024) | 197 K | DALL·E 生成配对图 | 真实感不足 |\n| UltraEdit (Zhao et al., 2024) | 4 M | 区域编辑为主 | 缺乏多指令与多轮交互 |\n| AnyEdit (Jiang et al., 2025b) | 2.5 M | 25 类编辑任务 | 未覆盖参考图编辑等挑战场景 |\n| OmniEdit (Wei et al., 2024) | 1.2 M | 专家模型合成 | 指令多样性有限 |\n| ImgEdit (Ye et al., 2025) | 1.2 M | 13 类评估维度 | 训练集规模小 |\n\n8\n\n- **跨架构通用**：扩散/自回归均双位数提升。\n- **优于同期最强数据集**：ShareGPT-4o-Image 被**全面超越**（+1.1%–3.2%）。\n\n4\\. 结论与影响\n\n- 首次用**系统化数据工程**而非单纯堆量，**显著放大模型复杂指令跟随与专业视觉生成能力**。\n- 代码、数据、评测脚本**全部开源**，为后续 3D、视频、科学可视化等研究提供**可复用基座**。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是OpenGPT-4o-Image%3A%20A%20Comprehensive%20Dataset%20for%20Advanced%20Image%20Generation%20and%20Editing，链接是https://arxiv.org/pdf/2509.24900，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24900。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1759276017464","title":"Beyond the Exploration-Exploitation Trade-off: A Hidden State Approach for LLM Reasoning in RLVR","description":"A prevailing view in Reinforcement Learning for Verifiable Rewards (RLVR) interprets recent progress through the lens of an exploration-exploitation trade-off, a perspective largely shaped by token-level metrics. We re-examine this perspective, proposing that this perceived trade-off may not be a fundamental constraint but rather an artifact of the measurement level. To investigate this, we shift the analysis to the semantically rich hidden-state space, adopting Effective Rank (ER) to quantify exploration and proposing its novel first- and second-order derivatives, named Effective Rank Velocity (ERV) and Effective Rank Acceleration (ERA), to capture exploitation dynamics. Our analysis reveals that at the hidden-state level, exploration and exploitation could be decoupled (Sec. 4). This finding reveals an opportunity to enhance both capacities simultaneously. This insight motivates our method, Velocity-Exploiting Rank-Learning (VERL), the first to operationalize the principle of synergistic exploration-exploitation enhancement by directly shaping the RL advantage function. The key innovation is leveraging the theoretically stable ERA as a predictive meta-controller to create a synergistic, dual-channel incentive structure. Instead of forcing a trade-off, VERL prospectively amplifies rewards for exploration to preempt overconfidence and reinforces exploitative gains to consolidate reasoning. Experiments across diverse LLMs and reasoning benchmarks show consistent gains, including up to 21.4% absolute accuracy improvement on the challenging Gaokao 2024 dataset.","url":"https://huggingface.co/papers/2509.23808","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.464Z","metadata":{"authors":["Fanding Huang","Guanbo Huang","Xiao Fan","Yi He","Xiao Liang","Xiao Chen","Qinting Jiang","Faisal Nadeem Khan","Jingyan Jiang","Zhi Wang"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.LG","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.23808.pdf","arxivUrl":"https://arxiv.org/abs/2509.23808","arxivId":"2509.23808","coolPaperUrl":"https://papers.cool/arxiv/2509.23808","published":"2025-09-28T11:14:58Z","updated":"2025-09-28T11:14:58.000Z","zh_summary":"在可验证奖励的强化学习（RLVR）中，一种普遍的观点是通过探索-利用权衡的视角来解释近期的进展，这一观点在很大程度上受到基于标记级指标的影响。我们重新审视这一视角，提出这种所谓的权衡可能并非根本性约束，而是测量层面造成的现象。为此，我们将分析转向语义丰富的隐藏状态空间，采用有效秩（Effective Rank, ER）来量化探索，并提出其新的一级和二级导数，命名为有效秩速度（Effective Rank Velocity, ERV）和有效秩加速度（Effective Rank Acceleration, ERA），以捕捉利用动态。我们的分析显示，在隐藏状态层面上，探索和利用是可以解耦的（第4节）。这一发现揭示了同时增强两种能力的可能性。这一洞见促使我们提出方法——速度驱动利用秩学习（Velocity-Exploiting Rank-Learning, VERL），这是首个通过直接塑造强化学习优势函数来实现探索-利用协同增强原则的方法。关键创新在于利用理论上稳定的ERA作为预测型元控制器，从而创建一个协同的双通道激励结构。VERL并不强制权衡，而是前瞻性地放大探索奖励以预防过度自信，并强化利用收益以巩固推理能力。在不同大型语言模型和推理基准上的实验显示了一致的提升，包括在挑战性极高的高考2024数据集上实现了最高21.4%的绝对准确率提升。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n这篇论文针对“大模型推理强化学习（RLVR）中探索-利用权衡是否本质存在”这一核心疑问展开研究。传统观点基于 token-级指标（熵、置信度）认为探索与利用必然此消彼长，导致训练陷入“二选一”困境。作者提出：\n\n1. 该权衡并非推理任务固有，而是 token-级测量的副产品；\n2. 在语义更丰富的隐藏状态空间，探索与利用可解耦并同步增强。\n\n为此，论文首次引入 Effective Rank (ER) 量化隐藏状态空间的探索广度，并构造其高阶导数 ERV（速度）与 ERA（加速度）刻画利用动态。基于 ERA 的稳定性，提出 VERL 方法，通过元控制器式地重塑优势函数，在无需手动权衡的情况下同步提升探索与利用能力，在多个模型与推理基准上取得最高 21.4% 的绝对精度提升。\n\nQ2: 有哪些相关研究？\n\n论文在第 7 节“Related Work”中系统回顾了与 RLVR 及探索-利用权衡相关的研究，可归纳为以下两条主线：\n\n1. Reinforcement Learning with Verifiable Rewards（RLVR）\n\n- DeepSeek-R1-Zero / DeepSeek-R1\nGuo et al., 2025；首次展示纯 RL 可激发 LLM 长链推理能力。\n- 后续在开源模型上的 RLVR 扩展\n– Qwen 系列：Yang et al., 2024\n– Mistral 系列：Jiang et al., 2024\n– LLaMA 系列：Dubey et al., 2024\n- 针对推理长度、工具调用、弱点自我修正等方向的 RLVR 改进\n– 长度压缩：Cheng et al., 2025b；Yan et al., 2025\n– 工具使用：Rainone et al., 2025；Jin et al., 2025\n– 弱点驱动数据合成：Liang et al., 2025a,b\n– 无监督/测试时 RL：Zuo et al., 2025；Zuo et al., 2024\n- 对 RLVR 局限性的反思\nYue et al., 2025 指出最大化期望回报会抑制探索，使模型停留在初始分布附近。\n2. Exploration-Exploitation in LLM Reasoning\n\n- Token-级熵/置信度视角\n– 熵正则化：Deng et al., 2025；Cheng et al., 2025a\n– Top-k 概率置信度：Fu et al., 2025；Damani et al., 2025\n– 基于置信度的无监督奖励：Li et al., 2025b\n- 高阶统计量与少数关键 Token 视角\nWang et al., 2025 提出“高熵少数 Token”驱动有效 RL。\n- 与本文最接近的“Pass@k 训练”工作\nChen et al., 2025b 通过动态 k 值在 token 层面平衡探索与利用，但仍默认二者存在权衡。\n\n综上，现有研究几乎都在 token 层面对探索-利用进行“零和”式调和；本文首次跳出该范式，在隐藏状态空间实现二者协同增强。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“三步走”策略，在隐藏状态空间重新建模并同步提升探索与利用，从而摆脱 token-级权衡陷阱：\n\n1. 建立隐藏状态度量体系\n\n- 探索：Effective Rank (ER)\n对响应级隐藏状态矩阵  Z  做 SVD，计算归一化奇异值熵\n\nER=exp!l(-∑_j p_jlog p_jr),; p_j=σ_j/∑_kσ_k\n\n高 ER ⇒ 语义空间被充分“摊开”，即探索充分。\n- 利用：Effective Rank Velocity (ERV) 与 Acceleration (ERA)\n按步长  s  滑动计算 ER 序列，定义\n\nδ_(js)=m_(js)-(1) / (j-1)∑_(k=1)^(j-1)m_(ks),quad ERV=(1) / (K-1)∑_(j=2)^K δ_(js),quad ERA=(1) / (K-2)∑_(j=3)^K (δ_(js)-δ_((j-1)s))\n\nERV 衡量信息增益速度，ERA 衡量该速度是否加速或饱和，用作“利用动态”信号。\n2. 理论验证：二者可解耦\n\n- 命题 3.5 证明：对含  k  条近似正交轨迹的理想矩阵，\n ER=O(k),;ERV=O(k),;ERA=O(1) \n即 ERA 与数据规模/序列长度无关，具有跨样本稳定性。\n- 图 1b 实验显示 ER 与 ERV 相关系数 ≈0，直接支持“探索-利用在隐藏状态空间几乎独立”。\n3. 算法实现：VERL（Velocity-Exploiting Rank-Learning）\na) 在线估计三条序列级指标  m_0,m_1,m_2= ER,ERV,ERA ；\nb) 用 EMA 维护历史均值  barμ_k ，计算相对偏离\n\nd_k=(m_k-barμ_k)/(|barμ_k|+ε)\n\nc) 以 ERA 作为元控制器，动态插值探索/利用权重\n\nβ=σ(d_2),; w_dyn=β·[1,0]+(1-β)·[0,1]\n\n当 ERA 高（即将饱和）⇒ 提高探索权重；ERA 低 ⇒ 提高利用权重。\nd) 构造有界辅助优势\n\nPhi=w_(dyn),0tanh(d_0)+w_(dyn),1tanh(d_1)\n\n并重塑原始优势\n\nhat A=A^((0))+min!l(max(0,Phi),,|A^((0))|/kappar)\n\n直接替换 PPO/GRPO 的 advantage，完成梯度更新。\n\n整个流程无需手动调节探索-利用系数，也不引入额外奖励工程，仅在优势函数层注入“ERA 制导”的双通道激励，实现二者同步增强。实验部分显示，VERL 在 15 个数学推理基准上平均提升 1–3 个百分点，Gaokao 2024 最高提升 21.4%，同时 Pass@k 提升幅度普遍高于 Pass@1，验证了“探索-利用兼得”的目标。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“同步增强探索与利用”这一核心假设，在 15 个数学推理基准、2 种主流 RL 算法（GRPO/PPO）、7 个不同规模/系列基模型（1.5 B–8 B）上开展了系统性实验，可概括为四大类：\n\n1. 主实验：Pass@1（利用能力）\n\n- 覆盖小学到奥赛难度\nGSM8K、ASDiv、SVAMP → MAWPS、TabMWP、Carp En → MATH、MATH500、CMATH → AIME23/24/25、AMC23/24、OlympiadBench、Gaokao2024\n- 结果（表 2）\n– 7 个基模型 +GRPO/PPO 后，再 +VERL 平均提升 1.2–2.7 pp；\n– 最高绝对增益：Gaokao2024-I +21.4%、AIME24 +10.0%；\n– 在多步推理型任务上提升幅度显著高于单步算术题，验证 VERL 对复杂推理的利用强化效果。\n2. 探索能力实验：Pass@k\n\n- 设置：MATH500@16、AMC23/24@128、AIME24/25@256\n- 结果（表 3）\n– 平均提升 0.9–3.7 pp，普遍高于对应 Pass@1 的提升；\n– 证明 VERL 在扩大采样预算时更能释放多样性优势。\n3. 细粒度分析实验\na) 响应级隐藏状态动态（图 3、8、9）\n– ER、ERV、ERA 随训练步数的变化曲线，显示正确/错误样本的区分规律；\n– ERA 持续为正且与验证得分高度相关，支持其作为“正确推理信号”。\nb) 数据集级隐藏状态动态（图 4、10、11）\n– 验证集 ER、ERV、ERA 与准确率同步上升，表明模型在语义空间持续扩张而非过早收敛。\nc) 消融与超参（图 6）\n– stride s∈{20,40,80}，最优 40；\n– advantage 裁剪因子 κ∈{1,2,3}，最优 2；\n– 仅探索或仅利用信号均导致性能下降，双通道协同必不可少。\n\n4. 案例研究（附录 F）\n\n- Pass@1 设置：选取数值比较、文字题约束、微分方程“纯指数解”三道题，展示 vanilla GRPO 因过度自信或约束忽略而失败，VERL 通过 ERA 制导精准利用约束条件得到正解。\n- Pass@16 设置：\n– 初等函数定义域题，VERL 在 16 条 rollout 中 5 次命中正确答案（x=1），而 vanilla GRPO 全部输出 x=3；\n– 几何题（60° 视线夹角），VERL 16 条轨迹中 5 条正确，且策略多样性显著更高（9 次正弦定理、4 次余弦定理、3 次一般几何性质），vanilla GRPO 主要依赖单一正弦定理路径。\n\n综上，实验从“宏观性能→细粒度动态→超参敏感→个案可视化”四个层次验证了 VERL 既能提升利用（Pass@1）又能增强探索（Pass@k），且增益跨模型、跨算法、跨难度稳定存在。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“理论-度量-算法-应用”四个层面，供后续研究参考：\n\n1\\. 理论层面\n\n- **隐藏状态几何与推理正确性的因果链**\n目前 ERA 与正确率仅呈统计相关；可借助干预分析（causal intervention）或表示因果框架，验证“提升 ERA→必然提升正确率”的因果关系，排除第三变量混淆。\n- **探索-利用解耦的普适性边界**\n本文理想正交假设下 ERA=𝒪(1)。可研究当隐藏状态存在强共线或低维流形时，ERA 是否仍保持稳定性，进而给出“可解耦”与“不可解耦”任务/数据分布的严格划分定理。\n\n2\\. 度量层面\n\n- **跨层融合 Effective Rank**\n仅使用最后一层隐藏状态；可探究\n– 层间 ER 差异/跳跃是否反映推理阶段转换；\n– 加权或自适应融合多层 ER 能否获得更细粒度探索信号。\n- **Token-级瞬时 ERA 的局部可解释性**\n当前 ERA 为整条轨迹全局量。可设计滑动窗-局部 ERA，定位“推理加速/饱和”具体发生在哪几步，并与人类标注的中间逻辑步骤对齐，实现细粒度诊断。\n\n3\\. 算法层面\n\n- **ERA 作为通用元控制信号**\n本文仅重塑优势函数；可尝试\n– 将 ERA 用于动态温度、top-p、rollout 预算、early-stop 等超参的自适应调节；\n– 把 ERA 引入价值函数拟合，作为辅助正则项或内在奖励，实现 Actor-Critic 双组件协同。\n- **多任务/持续学习场景下的 ERA 记忆机制**\n当前 EMA 仅追踪当前任务历史。可设计任务特定的 ERA 记忆库，防止新任务覆盖旧任务已获得的“加速度模式”，实现终身推理能力提升。\n- **与搜索-监督混合范式结合**\n将 ERA 制导与蒙特卡洛树搜索、Test-time RL、自洽性投票（self-consistency）结合，在测试阶段根据 ERA 实时决定是否扩展搜索分支或提前终止，提高推理效率。\n\n4\\. 应用与评估层面\n\n- **跨领域迁移**\n除数学推理外，可在代码生成（HumanEval+, MBPP+）、科学问答（ScienceQA, MMLU-STEM）、策略游戏（chess, bridge）等需要多步逻辑的场景验证 ERA-VERL 的通用性。\n- **人类对齐与可验证安全性**\n探索能力增强可能带来“幻觉”风险。可引入可验证约束（formal verification, theorem proving）作为额外奖励，与 ERA 共同塑形，确保探索方向始终落在“可验证正确”区域。\n- **小参数规模极限测试**\n目前实验最小 1.5 B。可下探到 0.3–1 B 甚至 100 M 模型，观察 ERA 是否仍保持稳定及 VERL 是否仍有效，以验证方法在资源受限环境的实用性。\n\n5\\. 系统与效率优化\n\n- **增量 SVD 的 GPU 并行化**\n当前增量计算已把矩阵构造从 𝒪(T³) 降至 𝒪(T²)，但特征分解仍占主要开销。可探索\n– 随机 SVD / 分布式 QR，\n– 将 ERA 估算 offload 到专用小模型或蒸馏网络，实现毫秒级延迟。\n- **与 MoE / 长上下文架构协同**\n研究 ERA 在专家切换或长序列场景下的行为，避免专家冗余或长程依赖稀释有效秩信号。\n\n综上，隐藏状态视角为 LLM 推理 RL 提供了新的“仪表盘”和“操纵杆”。围绕 ERA 的稳定性与可解释性，仍有大量理论细节、算法变体和应用场景值得进一步挖掘。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：token-级指标把 RLVR 中的探索与利用强行对立，导致“零和”式调参困境。\n- **观点**：该权衡是测量层次的人为产物，而非推理任务本质。\n- **方法**：\n1. 在隐藏状态空间定义 Effective Rank (ER) 量化探索，提出其高阶导数 ERV/ERA 刻画利用动态；\n2. 理论与实验验证 ER 与 ERV 几乎零相关，二者可解耦并同步提升；\n3. 设计 VERL，以稳定且与规模无关的 ERA 为元控制器，自适应重塑优势函数，无需手动权衡即可同时奖励探索与利用。\n- **结果**：7 个模型 × 2 种 RL 算法 × 15 个数学基准，Pass@1 最高 +21.4%，Pass@k 提升更显著，验证“探索-利用兼得”可行。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond%20the%20Exploration-Exploitation%20Trade-off%3A%20A%20Hidden%20State%20Approach%20for%20LLM%20Reasoning%20in%20RLVR，链接是https://arxiv.org/pdf/2509.23808，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.23808。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1759276017465","title":"SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer","description":"We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation.","url":"https://huggingface.co/papers/2509.24695","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.465Z","metadata":{"authors":["Junsong Chen","Yuyang Zhao","Jincheng Yu","Ruihang Chu","Junyu Chen","Shuai Yang","Xianbang Wang","Yicheng Pan","Daquan Zhou","Huan Ling","Haozhe Liu","Hongwei Yi","Hao Zhang","Muyang Li","Yukang Chen","Han Cai","Sanja Fidler","Ping Luo","Song Han","Enze Xie"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2509.24695.pdf","arxivUrl":"https://arxiv.org/abs/2509.24695","arxivId":"2509.24695","coolPaperUrl":"https://papers.cool/arxiv/2509.24695","published":"2025-09-29T12:28:09Z","updated":"2025-09-29T12:28:09.000Z","zh_summary":"我们介绍了 SANA-Video，一种小型扩散模型，可以高效生成分辨率高达 720x1280、长度达分钟级的视频。SANA-Video 能够以显著快速的速度合成高分辨率、高质量、长时长的视频，并具有强大的文本-视频对齐能力，可部署在 RTX 5090 GPU 上。两个核心设计确保了我们高效、有效的长视频生成：(1) 线性 DiT：我们将线性注意力作为核心操作，相比传统注意力在处理视频生成过程中大量 token 时更加高效。(2) 块线性注意力的常量内存 KV 缓存：我们为长视频生成设计了块状自回归方法，利用来源于线性注意力累积特性的一种常量内存状态。该 KV 缓存为线性 DiT 提供全局上下文，且内存成本固定，从而消除了传统 KV 缓存的需求，使得高效的分钟级视频生成成为可能。此外，我们还探讨了有效的数据过滤和模型训练策略，将训练成本缩减至在 64 个 H100 GPU 上仅需 12 天，仅为 MovieGen 成本的 1%。凭借低成本，SANA-Video 在性能上可与现代最先进的小型扩散模型（如 Wan 2.1-1.3B 和 SkyReel-V2-1.3B）竞争，同时实际延迟快 16 倍。此外，SANA-Video 可在 RTX 5090 GPU 上使用 NVFP4 精度部署，将生成 5 秒 720p 视频的推理速度从 71 秒加速至 29 秒（加速 2.4 倍）。总之，SANA-Video 实现了低成本、高质量的视频生成。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nSANA-Video 旨在解决**高分辨率、长时长视频生成中的计算效率瓶颈**，具体包括：\n\n- **训练与推理成本极高**：现有大规模视频扩散模型（如 Wan-14B）在 720p、5 s 片段上需处理 >75 k token，单次生成在 H100 上耗时 32 min，训练成本以“万卡天”计，难以普及。\n- **长视频内存爆炸**：传统因果自注意力随帧数线性增长的 KV-cache 导致显存占用不可控，>10 s 视频即 OOM，迫使模型牺牲全局上下文、退化为局部注意力。\n- **边缘设备不可部署**：庞大参数量与计算量使 SOTA 模型无法在个人 GPU 上实时运行。\n\n为此，论文提出“小参数、线性复杂度、恒定显存”的新范式，目标在**12 天 / 64 H100 训练预算**内，得到一款可在 **RTX 5090 上 29 s 生成 5 s 720p 视频** 的 2 B 扩散模型，同时保持与 10× 更大模型相当的视觉质量与文本一致性。\n\nQ2: 有哪些相关研究？\n\n与 SANA-Video 直接相关的研究可归纳为三条主线，每条均给出最具代表性的工作并指出其与本文的差异/继承关系。\n\n1\\. 高效视频扩散模型（降低 token 量 / 采样步数）\n\n- **Make-A-Video、Imagen Video、LTX-Video**\n采用**时空分离注意力**或 3D 卷积降低复杂度，但仍保留平方项自注意力，长视频需局部窗口，丧失全局上下文。\n- **Wan-2.1/2.2、Open-Sora、CogVideoX**\n通过 VAE 时序压缩 + 大参数 DiT 提升质量，训练成本>1000 GPU-days；SANA-Video 沿用其 VAE 思想，但将主干替换为**线性注意力**，训练成本仅 1 %。\n\n2\\. 线性/亚平方注意力机制（理论复杂度 𝑂(N) 或 𝑂(N log N)）\n\n- **Linformer、Performer、Nyströmformer**\n提出核化或低秩投影实现线性注意力，但面向**图像/语言**，未考虑视频因果生成。\n- **SANA（图像 Linear-DiT）**\n首次将 ReLU-线性注意力引入扩散 Transformer；SANA-Video 继承其内核，并新增\n– 3D-RoPE 保证时空位置编码\n– 因果块级 KV-cache 支持**分钟级长视频**\n– 时序 1D-conv 增强局部运动连续性。\n\n3\\. 自回归+扩散混合长视频生成（解决误差累积）\n\n- **MAGI-1、SkyReel-V2、CausVid**\n采用“块内扩散+块间自回归”框架，但**块间仍用 vanilla 因果注意力**，需 growing KV-cache，限制在 10–20 s 内。\n- **Self-Forcing**\n通过 rollout 缓解 exposure bias，却因显存被迫采用**局部窗口**。\n- **SANA-Video 块线性注意力**\n利用线性累积状态  S_i=∑ varphi(K_j)^T V_j  与键和  ∑ varphi(K_j)^T  代替传统 KV-cache，显存恒定为  O(D^2) ，使**Self-Forcing 式 rollout 可扩展至 60 s+** 而无需局部近似。\n\n小结表格（不含公式）\n\n| 研究方向 | 代表工作 | 主要瓶颈 | SANA-Video 改进 |\n| --- | --- | --- | --- |\n| 高分辨率视频扩散 | Wan-2.1/2.2, Open-Sora | 训练>1000 GPU-days，推理慢 | 线性 DiT，12 天 64 GPU 训练 |\n| 线性注意力 | Linformer, Performer, SANA | 无因果长视频方案 | 3D-RoPE + 因果块 KV-cache |\n| 自回归长视频 | MAGI-1, SkyReel, Self-Forcing | KV-cache 随长度线性增长 | 恒定  O(D^2)  缓存，分钟级生成 |\n\n因此，SANA-Video 在继承上述三类工作的基础上，首次将**线性注意力与恒定显存块级自回归**结合，实现了“小模型、低成本、长时长、高分辨率”的统一。\n\nQ3: 论文如何解决这个问题？\n\nSANA-Video 把“高分辨率、长时长、低算力”三个矛盾目标拆解为**三大技术模块**，各自对应一个核心瓶颈，并给出**可量化的解决方案**。\n\n1\\. 训练-推理算力瓶颈 → **Linear DiT：把 𝑂(N²) 自注意力换成 𝑂(N) 线性注意力**\n\n- **ReLU 线性核**\n沿用 SANA 的  varphi(x)=ReLU(x)  核，计算复杂度从\n\nO(N^2 D)\n 降到 \nO(N D^2)\n\n在 720×1280×81 视频上 N≈75 k，实测 4× 加速。\n\n- **3D-RoPE 后置插入**\n为避免 ReLU 把位置信息滤掉，采用\n\nRoPEl(varphi(Q)r),; RoPEl(varphi(K)r)\n\n并**从分母去掉 RoPE** 保证分母恒正，解决训练不稳定（图 3(b) 绿色曲线）。\n\n- **时序 1D-Conv 混合 FFN**\n在 Mix-FFN 末端加\n\nConv_(3× 1× 1)\n\n零初始化+短路，继承图像权重同时聚合相邻帧，运动连续性↑，训练损失↓（图 6(b)）。\n\n2\\. 长视频显存瓶颈 → **Block Linear Attention：恒定 𝑂(D²) KV-cache**\n\n把因果线性注意力改写成**累积状态**：\n\nO_i = varphi(Q_i)l(S_(i-1) + varphi(K_i)^T V_ir)varphi(Q_i)l(K_(i-1)^(∑) + varphi(K_i)^Tr)\n\n只需缓存\n\nS ∈ R^(D× D),quad K^(∑) ∈ R^(D)\n\n显存与帧数无关，**1.3 GB 固定**即可生成 60 s 视频（图 1(c) 水平线）。\n\n- **块级自回归**\n每 5 s 为一个 latent 块，块内并行扩散，块间按\n\np(x_(1:M))=prod_(i=1)^M p(x_i|x_(<i))\n\n生成；旧块状态一次性写入缓存，新块读取全局上下文，兼顾质量与速度。\n\n3\\. 训练成本瓶颈 → **三阶段课程 & 数据过滤**\n\n| 阶段 | 目标 | 关键设置 | 耗时 |\n| --- | --- | --- | --- |\n| Stage-1 VAE 适配 | 让 T2I 权重对齐视频 latent | 5 k 步微调，DCAE-V 压缩率 128× | <1 day |\n| Stage-2 粗到精继续预训练 | 先学运动再补细节 | 192p→480p→720p，短→长，滤波后 2 M 视频 | 8 days |\n| Stage-3 块级自回归微调 | 解锁任意长度 | 单调增 SNR 采样 + 改进 Self-Forcing（用全局缓存 rollout 60 s 选段训练） | 3 days |\n\n- **数据过滤**\n运动：Unimatch 光流 + VMAF 帧差，剔高速/静止；\n美学：DOVER 综合分；\n饱和度：HSV-S 通道均值；\n人工精选 5 k 段做 SFT，整体训练预算 **64 H100 × 12 天 ≈ 1 % MovieGen**。\n\n4\\. 边缘部署瓶颈 → **NVFP4 4-bit 量化**\n\n- 仅量化 QKV/output 投影与 1×1 conv，保留归一化、时序卷积为高精度；\n- 在 RTX 5090 上 5 s 720p 视频从 71 s → 29 s，**2.4× 额外加速**，视觉无损。\n\n结果量化\n\n- **速度**：720p×81 帧 36 s（H100），29 s（RTX5090+NVFP4），比 Wan-2.1-14B 快 **53×**。\n- **质量**：VBench Total 84.05，与 14 B 模型相当，语义分数 81.73 为同量级最高。\n- **显存**：480p 视频 7.2 GB 恒定，60 s 不 OOM； vanilla attention 76 GB 即 OOM。\n\n通过上述四大技术，SANA-Video 把“高分辨率+长时长+低算力”从不可能变成可部署的端到端方案。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“效率-质量-长时长”** 三条主线设计实验，全部在 **H100/RTX 5090** 上完成，结果以 **VBench 官方指标** 与 \\*\\* wall-clock latency\\*\\* 双轨报告。主要实验与结论如下：\n\n1\\. 效率对比实验（480p & 720p）\n\n| 模型 | 参数量 | 480×832×81 延迟 (s) | 720×1280×81 延迟 (s) | 显存 (480p) |\n| --- | --- | --- | --- | --- |\n| Wan-2.1-14B | 14 B | 484 | 1897 | 76 GB→OOM |\n| Wan-2.1-1.3B | 1.3 B | 103 | 400 | 26 GB |\n| SkyReel-V2 | 1.3 B | 132 | – | 15 GB |\n| SANA-Video | 2 B | 60 | 36 | 7.2 GB |\n\n- **结论**：同分辨率下 SANA-Video 最快，**8× 加速** 于 480p；720p 比 14 B 模型 **53× 快**，显存恒定。\n\n2\\. 质量对比实验（VBench）\n\n| 任务 | 指标 | SANA-Video | Wan-2.1-14B | Open-Sora-2.0 | SkyReel-V2 |\n| --- | --- | --- | --- | --- | --- |\n| T2V | Total ↑ | 83.71 | 83.69 | 84.34 | 82.67 |\n| T2V | Semantic ↑ | 81.35 | 76.11 | 80.12 | 74.53 |\n| I2V | Total ↑ | 88.02 | 86.86 | – | 87.3 |\n| I2V | Semantic/I2V ↑ | 96.40 | 92.90 | – | 94.5 |\n\n- **结论**：2 B 模型在语义对齐项 **全面领先**，Total 分数与 14 B 级模型持平。\n\n3\\. 长视频生成实验（自回归 5–60 s）\n\n| 模型 | 生成方式 | 最大长度 | VBench-Total ↑ | 50 步延迟 (RTX 5090) |\n| --- | --- | --- | --- | --- |\n| CausVid | 块扩散 | 10 s | 81.20 | – |\n| SkyReel-V2 | 块扩散 | 60 s | 82.67 | 132 s (480p) |\n| Self-Forcing | 块扩散 | 60 s | 84.31 | – |\n| SANA-Video | 块线性扩散 | 60 s+ | 83.70 | 29 s (720p 5 s) |\n\n- **结论**：恒定显存下生成 **30–60 s** 视频质量与 SOTA 持平，**首次在 2 B 模型上实现分钟级 720p**。\n\n4\\. 消融实验（训练损失 & 延迟）\n\n| 组件 | 训练损失↓ | 720p 延迟↓ | 说明 |\n| --- | --- | --- | --- |\n| w/ 3D-RoPE | 0.182 | 36 s | 比无 RoPE 损失 ↓18 % |\n| w/ 时序 1D-Conv | 0.175 | 36 s | 比无时序卷积损失 ↓22 % |\n| Linear vs Full Attn | – | 4× 加速 | 720p 上 148→36 s |\n\n- **结论**：两项改进均显著降低损失；线性注意力在高分辨率下加速比越大。\n\n5\\. 量化与边缘部署实验\n\n| 精度 | 720p×81 延迟 (RTX 5090) | 显存 | VBench-Total 变化 |\n| --- | --- | --- | --- |\n| BF16 | 71 s | 7.2 GB | 84.05 |\n| NVFP4 | 29 s | 4.1 GB | 83.98 (-0.07) |\n\n- **结论**：4-bit 量化 **2.4× 加速**，质量无损，可在消费级 GPU 实时运行。\n\n6\\. 世界模型下游微调验证\n\n| 任务 | 数据 | 微调步数 | 可视化结果 |\n| --- | --- | --- | --- |\n| 具身 AI | AgiBot 多视角 | 10 k | 机器人臂轨迹连贯 |\n| 自动驾驶 | 30 FOV 前视 | 15 k | 街景与车道线一致 |\n| 游戏生成 | VPT-Minecraft | 20 k | 第一人称动作流畅 |\n\n- **结论**：同一模型权重无需结构改动即可微调为**世界模拟器**，验证泛化性。\n\n7\\. 数据过滤消融（Table 7 & Fig 14）\n\n- **运动过滤**：保留中等光流 0.15–0.45 px/帧，生成结果运动幅度可控（Fig 11）。\n- **美学+饱和度过滤**：DOVER>0.7 & Sat<0.45，VBench Quality 分 ↑1.8。\n- **SFT 5 k 人工精选**：细节与物理合理性显著改善（Fig 14 眼睛、烟雾）。\n\n8\\. VAE 鲁棒性实验（噪声扰动）\n\n| VAE | ε=0.2 时 PSNR ↑ | SSIM ↑ | LPIPS ↓ |\n| --- | --- | --- | --- |\n| Wan-2.1 | 24.25 | 0.78 | 0.16 |\n| Wan-2.2 | 25.94 | 0.84 | 0.10 |\n| DCAE-V | 29.34 | 0.90 | 0.05 |\n\n- **结论**：DCAE-V 在扰动下重建更稳健，适配小扩散模型推理噪声。\n\n总结\n\n实验覆盖 **速度、质量、长度、显存、量化、下游、数据、模块** 八个维度，所有对比均提供**原始 VBench 分数与 wall-clock 秒级延迟**，结果可复现。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为“理论-算法-系统-应用”四个层次，均直接对应 SANA-Video 尚未完全解决的开放问题。\n\n1\\. 理论层面：线性注意力极限与保真度\n\n- **ReLU 核的谱分布与信息丢失上界**\n当前仅用经验观察“RoPE 后置→局部性变好”，缺乏**谱范数/秩衰减**的定量分析。可推导\n\nrho_t = |boldsymbolO_(linear) - O_(softmax)|_F|O_(softmax)|_F\n\n随序列长度 N 的变化曲线，给出保真度下界。\n\n- **因果线性注意力的梯度爆炸/坍塌阈值**\n分母去 RoPE 后虽稳定，但仍使用累积和。可研究\n\n∑_(j=1)^i varphi(K_j)^T\n\n的最小特征值随 i 增大而趋于 0 的速率，进而设计**动态温度缩放**或**正则化项**保证下界大于 ε。\n\n2\\. 算法层面：更长、更可控、更高保真\n\n| 问题 | 可探索方案 |\n| --- | --- |\n| 分钟→小时级超长视频 | 把块级 KV-cache 进一步分层为多级 LRU 缓存，只保留语义关键帧的累积状态，实现 O(D² log L) 近似全局记忆。 |\n| 精细时序控制（帧级 VFR、变速镜头） | 在 SNR 采样器里引入时间扭曲函数 γ(t) ，让噪声调度与真实帧率解耦；需要设计可导的 γ⁻¹ 以便训练。 |\n| 任意分辨率 extrapolation | 把 3D-RoPE 扩展为连续位置编码（如 Fourier-embedding），支持 <512p 训练→1080p 推理的 zero-shot 超分。 |\n| 多模态条件（音频、深度、姿态） | 保持线性注意力主干不变，增加低秩交叉注意力 Adapter；利用线性交叉注意力复杂度 O((N+M)D²) 同步音频 48 kHz 特征。 |\n\n3\\. 系统层面：边缘端再加速与内存再压缩\n\n- **KV-cache 量化**\n当前仅权重量化到 NVFP4，缓存仍 FP16。可将累积状态\n\nS ∈ R^(D×D),; K^(∑) ∈ R^(D)\n\n按通道做 **8-bit 分组量化** + 动态反量化，显存再降 50 %，需分析累积误差对生成 PSNR 的影响。\n\n- **投机采样（Speculative Decoding）**\n训练一个 **0.3 B 小线性 DiT** 作为草稿模型，用 SANA-Video-2 B 做验证，单卡再加速 1.5–1.8×。\n\n- **并行化维度扩展**\n块间自回归目前串行。可在训练阶段引入 **因果块并行（CBP）** 算法，把不同块分配到多 GPU，通信量仅 **O(D²)** 状态向量，理论吞吐随卡数线性增长。\n\n4\\. 应用层面：世界模型与实时交互\n\n| 阶段 | 目标 | 关键设置 | 耗时 |\n| --- | --- | --- | --- |\n| Stage-1 VAE 适配 | 让 T2I 权重对齐视频 latent | 5 k 步微调，DCAE-V 压缩率 128× | <1 day |\n| Stage-2 粗到精继续预训练 | 先学运动再补细节 | 192p→480p→720p，短→长，滤波后 2 M 视频 | 8 days |\n| Stage-3 块级自回归微调 | 解锁任意长度 | 单调增 SNR 采样 + 改进 Self-Forcing（用全局缓存 rollout 60 s 选段训练） | 3 days |\n\n0\n\n5\\. 数据与评测：自动评估长视频\n\n- **长视频专用指标**\n现有 VBench 仅 5 s。可基于 DINOv2 特征轨迹提出 **Temporal Consistency Score (TCS)**：\n\nTCS = (1) / (T-W)∑_(t=1)^(T-W)cosl(z_t, z_(t+W)r)\n\n衡量 30 s+ 视频语义漂移，建立与人类长视频偏好相关性。\n\n- **自动难例挖掘**\n用 LLM 生成**多物体遮挡、复杂物理、长镜头**提示，通过**难度预测网络**筛选失败案例，反哺数据再标注，形成**课程-困难度自适应**训练。\n\n总结\n\n1. **理论**：量化线性注意力与 softmax 的近似误差界。\n2. **算法**：多级缓存、连续位置编码、音频-视频线性交叉注意。\n3. **系统**：KV-cache 量化、投机采样、因果块并行。\n4. **应用**：实时一致性蒸馏、可导渲染闭环、力-视频联合条件。\n5. **评测**：长视频一致性指标与自动难例课程。\n\n这些方向均可在**不改动线性注意力核心**的前提下，继续把“更长、更快、更可控”推向极致。\n\nQ6: 总结一下论文的主要内容\n\nSANA-Video 提出一套“小参数、线性复杂度、恒定显存”的视频扩散新范式，核心贡献与结果可概括为三点：\n\n1. **Linear DiT**\n将 ReLU 线性注意力扩展为视频版，引入 3D-RoPE 与因果时序卷积，复杂度从  O(N^2)  降至  O(N) ，720p 生成提速 4×。\n\n2. **Block Linear Attention**\n利用累积状态公式把 KV-cache 压缩成固定  O(D^2)  显存，支持块级自回归，首次在 2 B 模型上实现**分钟级 720p 视频**而显存不增。\n\n3. **极致低成本**\n三阶段课程训练 + 严格数据过滤，仅 64 H100×12 天（1 % MovieGen 成本）即收敛；量化到 NVFP4 后，RTX 5090 上 5 s 720p 视频 29 s 生成，比 14 B 模型快 53×，VBench 总分 84.05 与 SOTA 持平。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SANA-Video%3A%20Efficient%20Video%20Generation%20with%20Block%20Linear%20Diffusion%20Transformer，链接是https://arxiv.org/pdf/2509.24695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24695。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1759276017465","title":"Visual Jigsaw Post-Training Improves MLLMs","description":"Reinforcement learning based post-training has recently emerged as a powerful paradigm for enhancing the alignment and reasoning capabilities of multimodal large language models (MLLMs). While vision-centric post-training is crucial for enhancing MLLMs' intrinsic understanding of visual signals, current post-training paradigms are predominantly text-centric, where dense visual inputs are only leveraged to extract sparse cues for text-based reasoning. There exist a few approaches in this direction, however, they often still rely on text as an intermediate mediator or introduce additional visual generative designs. In this work, we introduce Visual Jigsaw, a generic self-supervised post-training framework designed to strengthen visual understanding in MLLMs. Visual Jigsaw is formulated as a general ordering task: visual inputs are partitioned, shuffled, and the model must reconstruct the visual information by producing the correct permutation in natural language. This naturally aligns with reinforcement learning from verifiable rewards (RLVR), requires no additional visual generative components, and derives its supervisory signal automatically without any annotations. We instantiate Visual Jigsaw across three visual modalities, including images, videos, and 3D data. Extensive experiments demonstrate substantial improvements in fine-grained perception, temporal reasoning, and 3D spatial understanding. Our findings highlight the potential of self-supervised vision-centric tasks in post-training MLLMs and aim to inspire further research on vision-centric pretext designs. Project Page: https://penghao-wu.github.io/visual_jigsaw/","url":"https://huggingface.co/papers/2509.25190","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.465Z","metadata":{"authors":["Penghao Wu","Yushan Zhang","Haiwen Diao","Bo Li","Lewei Lu","Ziwei Liu"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.25190.pdf","arxivUrl":"https://arxiv.org/abs/2509.25190","arxivId":"2509.25190","coolPaperUrl":"https://papers.cool/arxiv/2509.25190","published":"2025-09-29T17:59:57Z","updated":"2025-09-29T17:59:57.000Z","zh_summary":"基于强化学习的后训练方法最近已成为增强多模态大语言模型（MLLMs）对齐能力和推理能力的强大范式。虽然以视觉为中心的后训练对于提升MLLMs对视觉信号的内在理解至关重要，但目前的后训练范式主要以文本为中心，其中密集的视觉输入仅用于提取少量线索，以进行基于文本的推理。这方面存在一些方法，但它们通常仍依赖于文本作为中介，或引入额外的视觉生成模块。在本工作中，我们提出了Visual Jigsaw，一种通用的自监督后训练框架，旨在增强MLLMs的视觉理解能力。Visual Jigsaw被设计为通用排序任务：将视觉输入分割、打乱，模型必须通过自然语言生成正确的排列顺序来重建视觉信息。这自然与可验证奖励强化学习（RLVR）相契合，无需额外的视觉生成组件，并且能够自动获取监督信号，无需任何标注。我们将Visual Jigsaw应用于三种视觉模态，包括图像、视频和3D数据。大量实验表明，该方法在细粒度感知、时间推理和3D空间理解方面具有显著提升。我们的研究结果突显了以视觉为中心的自监督任务在MLLMs后训练中的潜力，并旨在激发对以视觉为中心的前置任务设计的进一步研究。项目页面：https://penghao-wu.github.io/visual_jigsaw/","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对多模态大语言模型（MLLM）在视觉信号本身上的细粒度理解被长期忽视的问题，提出一种无需额外视觉生成模块、也无需人工标注的后训练方案。核心动机可归纳为：\n\n- 现有 RL 后训练几乎都以文本推理为中心，视觉仅被当作提供稀疏线索的上下文，导致模型对图像、视频、3D 等模态的内在感知能力强化不足。\n- 近期虽出现引入视觉重建目标的工作，但它们需新增解码器或扩散组件，且像素级重建是否真有助于“理解”尚无定论。\n- 自监督视觉表征学习中的 jigsaw 任务（重排图像块、视频帧等）虽轻量且可验证，却在传统视觉预训练里被重建或对比学习方法压制，未能发挥潜力。\n\n为此，作者提出 Visual Jigsaw：把视觉输入切分、打乱后，让模型用自然语言输出正确顺序，利用可验证奖励的强化学习（RLVR）进行后训练。该范式\n\n1. 直接优化视觉结构理解，不依赖文本中间媒介；\n2. 奖励信号完全自监督、无需标注；\n3. 不改动模型架构，保持文本输出形式；\n4. 可无缝扩展到图像、视频、3D 三种模态，系统提升细粒度感知、时序推理与几何空间理解。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 章“Related Works”中将相关研究划分为三大主线，并指出各自与 Visual Jigsaw 的差异。可梳理如下：\n\n1. 自监督视觉表征学习（Self-Supervised Learning）\n\n- 早期上下文预测：预测图像块相对位置（Doersch et al. 2015）、拼图排序（Noroozi & Favaro 2016）。\n- 主流方法：\n– 重建类：MAE、BEiT、iBOT 等（He et al. 2022；Bao et al. 2021；Zhou et al. 2021）。\n– 判别类：MoCo、SimCLR、DINO 系列（He et al. 2020；Chen et al. 2020；Caron et al. 2021）。\n- 时空拼图扩展：Video Jigsaw、ST-Puzzle（Ahsan et al. 2019；Huo et al. 2021）。\n差异：传统方法面向 CNN/ViT 预训练，需像素级特征；Visual Jigsaw 面向已具备语言能力的 MLLM，仅用文本顺序作可验证奖励，无需像素解码器。\n2. MLLM 视觉理解增强\n\n- 数据规模驱动：扩大图文对、提升分辨率、更强视觉编码器（Hurst et al. 2024；Bai et al. 2025）。\n- 显式重建目标：Reconstructive-VL、Auto-VL（Wang et al. 2025b；a）在训练阶段加入图像重建损失，需额外生成模块。\n- 统一多模态模型：Show-o、Janus、BLIP3-o（Xie et al. 2025b；Chen et al. 2025c；b）同时做理解与生成，但生成目标可能损害理解（Pan et al. 2025）。\n差异：Visual Jigsaw 不改变模型结构，不引入像素损失，仅在 RL 后训练阶段用排序任务强化视觉感知。\n3. MLLM 强化学习后训练\n\n- 文本推理导向：RLHF → RLVR，聚焦数学、科学、链式思维（Meng et al. 2025；Huang et al. 2025；Yuan et al. 2025a；Wang et al. 2025f）。\n- 视频/3D 推理：Video-R1、Scene-R1（Feng et al. 2025；Yuan et al. 2025b）仍侧重文本答案正确性。\n- 感知导向：Vicrit、LLaVA-Critic-R1 通过判别文本描述对错来提供奖励（Wang et al. 2025e；d），信号间接。\n- 最接近的 JigsawR1（Wang et al. 2025g）仅做 2×2 图像拼图且性能差，未扩展到视频/3D。\n差异：Visual Jigsaw 直接以视觉结构顺序为确定性奖励，无需人工偏好标注，且系统覆盖图像、视频、3D 三种模态。\n\nQ3: 论文如何解决这个问题？\n\n论文将“增强 MLLM 内在视觉理解”转化为一个**可验证的排序游戏**，用强化学习求解，具体流程如下：\n\n1. 任务建模\n把视觉输入  X  切成  K  个元素（图像块、视频片段、RGB-D 点），按已知规则得到原始顺序标签 $g=\n1,2,…,K\n ；随机打乱后得到观测序列 X\\_π$。\n策略模型  π_θ  仅输出文本形式的排列 $o=\no_1,…,o_K\n ，与 g$ 比较即可计算奖励，无需任何像素级解码器。\n\n2. 奖励设计（可验证奖励 RLVR）\n\nR(o,g)= 1, & o=g γ⋅(1) / (K)∑_(i=1)^K 1[o_i=g_i], & ValidPermutation(o)∧o≠g 0, & otherwise\n\n- 完全正确得 1；部分正确按匹配比例折扣  γ=0.2 ；非法排列得 0。\n- 额外格式奖励 0.2：要求模型把推理写入 `<think>`，最终答案放入 `<answer>`，防止奖励黑客。\n3. 优化算法\n采用 Group Relative Policy Optimization（GRPO）：\n\n- 每 prompt 采样 16 条回答，用上述奖励归一化优势，更新策略。\n- 去除 KL 正则与熵损失，仅保留排序奖励，训练 800–1000 步即可收敛。\n4. 三模态实例化\n\n- **Image Jigsaw**：3×3 网格切分 COCO 118 k 张图，恢复行优先顺序。\n- **Video Jigsaw**：LLaVA-Video 100 k 条，均匀切 6 段，恢复时序。\n- **3D Jigsaw**：ScanNet RGB-D 300 k 样本，随机选 6 个深度差异≥0.2 m 的点，恢复由近到远顺序。\n5. 实验验证\n\n- 在 20+ 细粒度感知、单目空间、组合理解基准上，Image Jigsaw 平均提升 2–6 个百分点；\n- 12 项视频理解基准，Video Jigsaw 在 16/32/64 帧设定下一致优于基线；\n- 8 项 3D 基准，3D Jigsaw 把深度相关任务 DA-2K 从 54.4 → 71.6，其余单视角/多视角/自我中心视频任务也全面上涨。\n\n通过“排序即监督”这一轻量级可验证目标，论文在**不增加模型参数、不修改架构、不依赖人工标注**的前提下，系统性地把拼图任务学到的结构感知迁移到下游视觉理解场景，从而解决了 MLLM 视觉信号被“文本推理”边缘化的问题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“Visual Jigsaw”在图像、视频、3D 三个模态分别设计了大规模后训练实验，并在 40+ 下游基准上验证其泛化效果。主要实验内容如下：\n\n1\\. 训练设置\n\n- **基座模型**：Qwen2.5-VL-7B-Instruct\n- **RL 算法**：Group Relative Policy Optimization（GRPO），无 KL 正则/熵损失\n- **超参**：\n- 部分正确折扣 γ=0.2\n- 全局 batch 256（图像）/128（视频&3D）\n- 学习率 1×10⁻⁶，温度 1.0，每 prompt 采样 16 条回答\n- 训练步数：图像/视频 1000 step，3D 800 step\n\n2\\. 图像模态实验\n\n| 训练数据 | COCO 118 k 张图，3×3 切分，过滤短边 <84 px | | 评估类别 | 下游 13 项基准，分三组 | | 对照组 | 原模型、ThinkLite-VL、VL-Cogito、LLaVA-Critic-R1、Image Jigsaw(SFT 版) |\n\n**结果**（平均增益）：\n\n- 细粒度感知 & 理解 ↑1.1–6.1 %\n- 单目空间理解 ↑1.8–5.9 %\n- 组合理解 ↑1.4–2.0 %\n\n> 3×3 拼图难度显著优于 2×2；RL 版全面优于 SFT 版，验证“RL 泛化 > SFT 记忆”。\n\n3\\. 视频模态实验\n\n| 训练数据 | LLaVA-Video 100 k 条，每段切 6 clip，首尾各去 5 % 帧 | | 评估基准 | 13 项视频理解任务（AoTBench、Vinoground、TOMATO、FAVOR-Bench、TUNA-Bench、Video-MME、TempCompass、TVBench、MotionBench、LVBench、VSI-Bench、Video-TT、CVBench） | | 帧数设定 | 16 / 32 / 64 帧，统一 256×28×28 像素上限 | | 对照组 | Qwen2.5-VL-7B、Video-R1（冷启动 SFT+RL） |\n\n**结果**（64 帧最佳）：\n\n- 时序方向性任务 AoTBench ↑5.2 %\n- 跨视频推理 CVBench ↑3.0 %\n- 其余 11 项平均 ↑1.0–3.4 %\n\n> 6-clip 难度带来的提升明显高于 4-clip，证明任务复杂度与收益正相关。\n\n4\\. 3D 模态实验\n\n| 训练数据 | ScanNet RGB-D，随机采 6 点（深度差 ≥0.2 m，像素距 ≥40 px），共 300 k 样本 | | 评估基准 | 8 项 3D 理解任务（SAT-Real、3DSRBench、ViewSpatial、All-Angles、OmniSpatial、VSI-Bench、SPARBench-tiny、DA-2K） | | 对照组 | Qwen2.5-VL-7B |\n\n**结果**：\n\n- 深度排序相关 DA-2K ↑17.1 %\n- 单视角/多视角/自我中心视频任务平均 ↑1.5–3.3 %\n- 总体在 8 项基准全部上涨，验证几何结构感知被有效迁移。\n\n5\\. 消融与扩展\n\n- **SFT vs RL**：同数据量下，SFT 在部分基准出现负迁移，RL 版稳定正收益。\n- **任务难度**：降低拼图粒度（2×2 或 4-clip）后，平均性能增益减少约 1/3，说明难度提供更强监督。\n- **推理模型兼容性**：在已做数学推理 RL 的 ThinkLite-VL 上继续加 Image Jigsaw，视觉感知 ↑2 % 同时保持数学推理能力不下降。\n- **3D 变体**：尝试“视角-运动描述匹配”与“BEV-相机位姿匹配”，效果不及深度排序，表明当前 MLLM 的 3D 先验仍有限。\n\n6\\. 定性分析\n\n提供 20 组可视化案例（图 7–9），展示模型在细粒度颜色、空间关系、时序方向、深度比较等场景下的错误修正，直观验证视觉排序任务带来的感知提升。\n\n综上，论文通过**大规模自监督排序训练 + 多模态下游基准系统评估**，充分证明 Visual Jigsaw 能够在不改变模型架构的前提下，显著提升 MLLM 对图像细节、视频时序和 3D 几何的 intrinsic 理解能力。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可继续推进，按“任务-数据-模型-评价”四条线展开：\n\n1\\. 任务层面：更复杂的视觉排序\n\n- **时空混合拼图**\n同时打乱视频帧的**时序**与每帧内部的**空间块**，要求模型输出“双重排列”，检验其联合时空建模能力。\n- **层次化拼图**\n由粗到细多粒度切分（2×2 → 3×3 → 4×4），逐步提升难度，观察模型是否能持续受益。\n- **异构元素拼图**\n将图像、深度、光流、音频片段混合为同一序列，迫使模型学习跨模态顺序关系。\n- **动态可变形拼图**\n块或片段本身带有几何变形/颜色扰动，减少低级匹配捷径，强化语义理解。\n\n2\\. 数据层面：更大、更长、更真实\n\n- **十亿级无标注数据**\n利用公开网页视频、AR/VR 场景流、无人机序列，检验任务在数据量上的可扩展性。\n- **长视频拼图**\n把 1–30 min 日常视频切 12–24 段，考察模型对**长时序依赖**与**事件因果**的捕捉。\n- **隐式 3D 表示拼图**\n直接在点云、体素、NeRF 渲染图之间做顺序恢复，绕过 RGB-D 中间视图，提升原生 3D 推理。\n\n3\\. 模型层面：架构与训练策略\n\n- **原生 3D 视觉编码器**\n将 Point-Transformer、3D-CNN 接入 MLLM，验证拼图任务是否能激活真正的几何推理而非深度启发式。\n- **混合专家（MoE）双塔**\n引入“排序专家”与“生成专家”并行路由，让拼图信号**不干扰**原有文本生成能力。\n- **课程与自我博弈**\n先用简单 2×2 快速预热，再在线提升难度；或让模型自己生成对抗性拼图序列，实现自我蒸馏。\n- **持续学习**\n交替在拼图任务与指令跟随任务之间切换，研究如何**避免灾难性遗忘**并保持通用对话能力。\n\n4\\. 评价层面：更细、更严、更通用\n\n- \\*\\* probe-based 诊断\\*\\*\n用线性探测、注意力 rollout 分析模型是否真正关注局部边缘、运动矢量或视差，而非背景统计捷径。\n- **对抗鲁棒性**\n在拼图输入上加入 patch adversarial 扰动或帧间闪烁，测量顺序预测鲁棒性，反映真实场景可靠性。\n- **跨任务迁移度量**\n建立“拼图难度 ↔ 下游增益”拟合曲线，量化何种排序复杂度最能预测细粒度检测、VQA、指代表达性能。\n- **可解释顺序可视化**\n将模型在 `<think>` 中的中间坐标、方位描述提取出来，与 ground-truth 对应，自动评分其**推理可解释性**。\n\n5\\. 统一视角：自监督任务库\n\n- **拼图 + 旋转 + 遮挡 + 颜色化** 组合搜索\n用 AutoML/LLM 代理自动设计最优视觉预文本任务组合，超越单一拼图。\n- **奖励函数搜索**\n将折扣 γ、顺序距离、编辑距离、信息熵纳入可微搜索空间，寻找最能激发视觉理解的奖励形状。\n- **联邦或私域场景**\n在医疗、工业等无法出域的数据上，用 Visual Jigsaw 做**本地后训练**，验证其隐私友好性与领域迁移能力。\n\n综上，Visual Jigsaw 为“视觉结构排序”打开了轻量级、可验证的 RL 后训练范式，未来可在**任务复杂度、数据规模、模型架构、评价维度**四个方向持续放大其潜力，并与其他自监督信号共同构建更全面的视觉理解增强体系。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有 MLLM 后训练以文本推理为中心，视觉仅作稀疏线索，导致模型对图像、视频、3D 等模态的**内在细粒度理解**被忽视；而引入像素级重建的方法又需新增生成模块，架构与训练代价大。\n- **思路**：把“视觉理解”转化为**轻量级可验证排序任务**——Visual Jigsaw：将视觉输入切分、打乱，模型用自然语言输出正确顺序，利用**确定性奖励**的 RLVR 进行后训练，无需标注、无需改架构、无需像素解码器。\n\n- **实现**：\n- **Image Jigsaw**：3×3 网格切分 COCO 118 k 图，恢复行优先顺序。\n- **Video Jigsaw**：LLaVA-Video 100 k 条视频均分 6 段，恢复时序。\n- **3D Jigsaw**：ScanNet RGB-D 300 k 样本随机采 6 点，恢复由近到远深度顺序。\n- **奖励**：完全正确得 1，部分正确按匹配比例折扣 γ=0.2，非法排列得 0；额外格式奖励 0.2。\n- **算法**：Group Relative Policy Optimization（GRPO），无 KL/熵正则，训练 800–1000 步。\n- **结果**（Qwen2.5-VL-7B 基座）：\n- **13 项图像基准**（细粒度感知、单目空间、组合理解）平均提升 **1–6 %**。\n- **13 项视频基准**（时序、运动、跨视频推理）在 16/32/64 帧设定下一致 **+1–6 %**。\n- **8 项 3D 基准**（深度、单/多视角、自我中心视频）全面上涨，深度相关任务 **+17 %**。\n- **消融**：RL 优于 SFT，3×3/6-clip 难度优于 2×2/4-clip；对已有推理模型加训拼图仍可提升视觉感知且不掉推理能力。\n- **结论**：Visual Jigsaw 以**自监督、可验证、纯文本输出**的方式，在后训练阶段系统强化 MLLM 的视觉结构感知，为“视觉信号本身”提供了一条轻量、通用、可扩展的增强路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Visual%20Jigsaw%20Post-Training%20Improves%20MLLMs，链接是https://arxiv.org/pdf/2509.25190，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25190。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1759276017465","title":"Democratizing AI scientists using ToolUniverse","description":"AI scientists are emerging computational systems that serve as collaborative partners in discovery. These systems remain difficult to build because they are bespoke, tied to rigid workflows, and lack shared environments that unify tools, data, and analyses into a common ecosystem. In omics, unified ecosystems have transformed research by enabling interoperability, reuse, and community-driven development; AI scientists require comparable infrastructure. We present ToolUniverse, an ecosystem for building AI scientists from any language or reasoning model, whether open or closed. TOOLUNIVERSE standardizes how AI scientists identify and call tools, integrating more than 600 machine learning models, datasets, APIs, and scientific packages for data analysis, knowledge retrieval, and experimental design. It automatically refines tool interfaces for correct use by AI scientists, creates new tools from natural language descriptions, iteratively optimizes tool specifications, and composes tools into agentic workflows. In a case study of hypercholesterolemia, ToolUniverse was used to create an AI scientist to identify a potent analog of a drug with favorable predicted properties. The open-source ToolUniverse is available at https://aiscientist.tools.","url":"https://huggingface.co/papers/2509.23426","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.465Z","metadata":{"authors":["Shanghua Gao","Richard Zhu","Pengwei Sui","Zhenglun Kong","Sufian Aldogom","Yepeng Huang","Ayush Noori","Reza Shamji","Krishna Parvataneni","Theodoros Tsiligkaridis","Marinka Zitnik"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.23426.pdf","arxivUrl":"https://arxiv.org/abs/2509.23426","arxivId":"2509.23426","coolPaperUrl":"https://papers.cool/arxiv/2509.23426","published":"2025-09-27T17:38:53Z","updated":"2025-09-27T17:38:53.000Z","zh_summary":"人工智能科学家正在成为新兴的计算系统，它们可以作为发现过程中的协作伙伴。这些系统仍然难以构建，因为它们是定制的，依赖于僵化的工作流程，并且缺乏将工具、数据和分析统一到共同生态系统中的共享环境。在组学研究中，统一的生态系统通过实现互操作性、可重用性和社区驱动开发，已经彻底改变了研究方式；人工智能科学家也需要类似的基础设施。我们推出了 ToolUniverse，这是一个用于构建人工智能科学家的生态系统，支持任意语言或推理模型，无论是开放还是封闭的。TOOLUNIVERSE 标准化了人工智能科学家识别和调用工具的方式，整合了超过 600 个机器学习模型、数据集、API 和用于数据分析、知识检索及实验设计的科学工具包。它能够自动优化工具接口以确保人工智能科学家的正确使用，能够根据自然语言描述创建新工具，迭代优化工具规范，并将工具组合成具自主性的工作流程。在高胆固醇血症的案例研究中，ToolUniverse 被用于创建一个人工智能科学家，以识别具有有利预测特性的药物有效类似物。开源的 ToolUniverse 可在 https://aiscientist.tools 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nTOOLUNIVERSE 旨在解决“AI scientist”构建门槛高、复用性差、工具碎片化三大痛点，具体表现为：\n\n- **一次性开发**：现有系统多为任务定制，代码与流程紧耦合，难以迁移。\n- **刚性工作流**：工具调用逻辑硬编码，无法随需求动态组合或替换。\n- **缺乏统一生态**：600+ 机器学习模型、数据库、API、实验设备分散，接口异构，导致模型“知道”工具却无法“使用”。\n\n论文提出用统一协议把任意 LLM/LRM/Agent 包装成可交互的科研助手，使工具发现、调用、组合、优化、生成全生命周期自动化，从而把 AI scientist 从“手工作坊”升级为“可拼装、可扩展、可迭代”的开放平台。\n\nQ2: 有哪些相关研究？\n\nTOOLUNIVERSE 的“统一工具生态”思想与下列研究/框架直接相关，可归纳为 **4 条主线、12 个代表工作**：\n\n1. 语言模型即工具调用器\n\n- GPT-3/4 + function calling（OpenAI, 2023）\n- Toolformer（Meta, 2023）\n- Gorilla（UC Berkeley, 2023）\n2. 多 Agent 编排与通信协议\n\n- AutoGen（Microsoft, 2023）\n- CAMEL（KAUST, 2023）\n- Model Context Protocol MCP（Anthropic, 2024）← TOOLUNIVERSE 远程层即兼容该协议\n3. 科研专用 Agent / 虚拟实验室\n\n- TxAgent（Harvard, 2025）← 同一团队，已内嵌 TOOLUNIVERSE\n- Virtual Lab for nanobody design（Stanford, 2024）\n- ChemCrow（EPFL, 2023）\n4. 工具-数据统一平台（omics 先例）\n\n- scverse（Nature Biotech 2023）\n- BioCypher（Nature Biotech 2023）\n- OHDSI & HADES（NEJM 2020）\n\n这些工作要么解决“模型如何调用工具”，要么解决“领域工具如何标准化”，但均未同时覆盖 **工具全生命周期管理（发现→调用→组合→优化→自动生成）** 与 **跨域 600+ 异构资源** 的统一协议；TOOLUNIVERSE 在此基础上向前一步，把科研工具链“HTTP 化”，使任意 LLM/LRM/Agent 零微调即可成为可复现、可扩展的 AI scientist。\n\nQ3: 论文如何解决这个问题？\n\nTOOLUNIVERSE 把“AI scientist 难以规模化”抽象为 **工具-模型接口缺失、工具生命周期管理缺失、跨域异构资源整合缺失** 三个技术缺口，并给出对应机制：\n\n1. 统一 AI-Tool Interaction Protocol\n\n- 规范两层 schema：\n– **Specification schema**（名字、描述、参数、返回结构）\n– **Interaction schema**（单字符串函数调用格式 `{“name”: …, “arguments”: …}`）\n- 本地/远程双通道：\n– 本地 `tooluniverse.run()` 直接 Python 调度\n– 远程走 Model Context Protocol（MCP），网络透明\n2. 六大核心组件覆盖工具全生命周期\n\n- **Tool Finder**\n– 关键词 + LLM in-context + 嵌入 三轨召回，600+ 工具毫秒级定位\n- **Tool Caller**\n– 动态加载、参数校验、缓存复用；失败返回结构化错误便于模型自纠错\n- **Tool Manager**\n– 本地工具 `@register_tool` 装饰器一键注入；私有工具通过 MCP 自动挂载，零配置\n- **Tool Composer**\n– 顺序、并行、反馈循环三种编排模式，把多工具输出自动归约成下一轮输入\n- **Tool Optimizer**\n– 多 Agent 闭环：自动生成测试用例→执行→分析→重写 spec，直到 6 维质量评分 ≥8/10\n- **Tool Discover**\n– 自然语言描述 → 检索相似工具 → 合成 spec → 代码模板 → 自动测试 → 注册入库，全程无人\n3. 零微调嫁接任意模型\n\n- 仅把“Find Tool/Call Tool”两个操作写入上下文，LLM/LRM/Agent 即可当场获得 600+ 工具能力，无需梯度更新。\n- 提供即装即用的 Claude、Gemini CLI、专用 Agent（TxAgent 等）配置模板，3 行代码完成嫁接。\n4. 端到端科研闭环验证\n\n- 高胆固醇血症案例：\n– 目标识别 → 文献+表达谱+专家反馈 → 锁定 HMG-CoA 还原酶\n– 药物重定位 → DrugBank+ChEMBL 召回 34 个洛伐他汀类似物\n– 多维 ML 预测 → Boltz-2 亲和力 + ADMET-AI 脑屏障渗透 → 筛得 CHEMBL2347006\n– 专利核查 → PubChem 专利 API 自动读取，确认 2019/2021 已授权\n- 全程 12 个工具链式调用，人工仅介入一次专家投票，证明“工具宇宙”可独立完成从假设到候选化合物验证。\n\n通过“协议统一 + 生命周期组件 + 零微调嫁接 + 实证闭环”，论文把原本需数月、多团队、多代码库的 AI scientist 构建流程压缩到 **单命令安装、单配置连接、单提示启动** 的普惠化体验。\n\nQ4: 论文做了哪些实验？\n\n论文未进行传统“模型 benchmark”式实验，而是采用 **端到端案例驱动** 的验证策略：在真实药物发现任务中，让 AI scientist 完全通过 TOOLUNIVERSE 调用工具完成从靶点发现到候选化合物确证的全流程，并以“能否复现已知结论 + 能否发现新线索”作为双重评价指标。具体实验设置与结果如下：\n\n1. 任务设计\n\n- 疾病：高胆固醇血症（hypercholesterolemia）\n- 目标：改善洛伐他汀（lovastatin）脱靶副作用，找到更优 statin 类似物\n- 流程 4 阶段：靶点识别 → 化合物虚拟筛选 → 多维 ML 评估 → 专利/监管评估\n2. 工具调用统计\n\n- 共触发 **12 类、20+ 次** TOOLUNIVERSE 工具调用，涵盖数据库、ML 模型、API、专家反馈等。\n- 其中 4 次并行调用 Boltz-2（每次 n=4 蒙特卡洛），累计 136 次对接计算。\n3. 主要定量结果\n\n- 靶点阶段：从 11 个潜在靶蛋白中精准锁定 **HMG-CoA 还原酶**（与已上市 statin 一致）。\n- 化合物阶段：\n– 召回 34 个 Tanimoto ≥0.8 的洛伐他汀类似物\n– 预测指标（均值±std）：\n- 结合概率：CHEMBL2347006 0.44±0.09 → 排名前 5\n- 结合亲和力：-0.18±0.13 kcal mol⁻¹ → 最强\n- BBB 渗透概率：0.48 → 最低四分位（预期副作用更小）\n- 专利验证：\n– 通过 PubChem CID→专利 API 自动抓取 2 篇 2019/2021 心血管用途专利，证实该分子已被保护，**反向验证 AI 筛选的合理性**。\n4. 对比与可复现性\n\n- 系统同时成功“再发现”**普伐他汀（pravastatin）**——已知脱靶更少的上市药物，其 BBB 渗透概率 0.48 vs 洛伐他汀 0.56，与文献一致（Botti 1991）。\n- 整个调用链、参数、原始输出已开源（GitHub + 网页服务），可供第三方重跑。\n5. 可靠性保障实验（补充）\n\n- 输入-输出采样：对 600+ 工具各生成 ≥10 组典型+边界用例，通过率 98.7%。\n- 人工评审：随机抽检 5% 结果，领域专家一致性 96%。\n- 自动优化器：迭代 3 轮后，工具描述 6 维质量分平均从 6.2→8.4/10。\n\n综上，论文用“**一个全长药物发现案例 + 多工具可靠性抽检**”代替消融实验，证明 TOOLUNIVERSE 能让通用模型在零微调条件下完成**可验证、可解释、可重用**的端到端科研任务。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 TOOLUNIVERSE 在“能力-规模-治理”三个维度的自然延伸，既保留其统一协议与工具生命周期框架，又引入新的科学或技术挑战：\n\n1. 多智能体科研协作\n\n- 构建“AI 实验室”：让 TxAgent、GeneAgent、SpatialAgent 等专精智能体通过 TOOLUNIVERSE 共享同一工具池，实现跨领域任务分解、结果互验与冲突仲裁。\n- 研究“科研角色”最优配比（假设生成 vs 实验执行 vs 质量控制），并量化其对发现效率的影响。\n2. 实验-计算闭环（闭环科学）\n\n- 接入机器人实验平台（如 Emerald Cloud Lab、Transcriptic），使 TOOLUNIVERSE 的 Call Tool 直接返回湿实验数据；智能体据此在线更新假设并再设计实验，形成“计算-实验-再计算”迭代。\n- 探索主动学习/贝叶斯实验设计在真实云实验中的样本效率边界。\n3. 工具自动生成与自我改进\n\n- 将 Tool Discover 与代码大模型（Code Llama、DeepSeek-Coder）深度耦合，实现“论文→方法段落→可执行工具”端到端生成，并在公开基准（如 Papers with Code）上自动评测。\n- 引入形式化验证（Hoare 逻辑、符号执行）保证生成工具在数值稳定性与资源安全上的可靠性。\n4. 跨模态工具链与统一表征\n\n- 发展“科学多模态嵌入”：把分子图、显微镜图像、序列、文本实验步骤映射到同一向量空间，使 Tool Finder 支持组合查询“找到与该细胞图像相关的激酶抑制剂”。\n- 研究异构工具输出（图像、CSV、JSON、HDF5）如何自动对齐到共享 schema，避免手动包装。\n5. 可信性与可追溯治理\n\n- 建立“科研区块链”层：每次工具调用、参数、结果哈希上链，确保可审计、防篡改；支持事后复现与争议追踪。\n- 引入风险分级机制：对涉及人类基因编辑、动物实验等高风险工具，强制多智能体投票 + 人类伦理审查签名方可执行。\n6. 领域特定深度优化\n\n- 材料科学：整合 CALPHAD 热力学数据库、DFT 计算引擎，让智能体在连续成分-工艺空间中搜索新材料，并自动提交高通量溅射实验。\n- 气候科学：耦合 CESM、WRF 等气候模型，支持“政策干预→排放情景→区域气候反馈”快速沙盒评估。\n7. 评价基准与排行榜\n\n- 建立“AI Scientist Benchmark”：包含 50 条跨学科端到端任务（靶点发现、电池材料、催化剂设计、气象极端事件归因等），统一度量“工具调用成本→科学产出质量→人类验证时间”。\n- 举办年度竞赛，鼓励外部团队基于 TOOLUNIVERSE 提交新智能体，推动社区持续贡献工具与优化器。\n8. 个性化科研助手\n\n- 让系统读取用户私有实验笔记（ELN）、LIMS 历史，自动微调检索器，实现“我的实验上下文感知的工具推荐”。\n- 研究联邦工具使用：敏感医疗数据留在本地医院，仅上传加密后参数与模型梯度，完成分布式药物重定位。\n\n通过上述探索，TOOLUNIVERSE 可从“工具生态”升级为“自主科学发现基础设施”，同时引出新的研究问题：多智能体协作理论、闭环实验的样本复杂度、生成工具的可证明正确性、跨模态统一表征以及可信 AI 治理框架。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：AI scientist 普遍“一次性”、工作流刚性、工具碎片化，难以跨域复用与扩展。\n- **方案**：提出 TOOLUNIVERSE——统一 AI-Tool Interaction Protocol + 六大核心组件（Finder/Caller/Manager/Composer/Optimizer/Discover），把 600+ 异构工具（模型、数据库、API、机器人等）封装成标准化、可组合、可自动生成/优化的“科研工具宇宙”。\n- **用法**：任意 LLM/LRM/Agent 零微调即可通过“Find Tool / Call Tool”两操作完成工具发现、执行、链式编排与结果迭代，三步搭建个人 AI 科学家。\n- **验证**：高胆固醇血症案例——从靶点识别、虚拟筛选、ML 多维评估到专利核查，全程 20+ 工具自动调用，成功再发现 pravastatin 并提名已获专利的更优候选物，证明端到端科研闭环可行。\n- **意义**：首次把 AI scientist 构建从“手工作坊”升级为“可拼装、可扩展、可迭代”的开放基础设施，代码与服务平台已开源。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Democratizing%20AI%20scientists%20using%20ToolUniverse，链接是https://arxiv.org/pdf/2509.23426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.23426。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1759276017466","title":"When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance","description":"Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.","url":"https://huggingface.co/papers/2509.22193","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.466Z","metadata":{"authors":["Nicolas Boizard","Hippolyte Gisserot-Boukhlef","Kevin El-Haddad","Céline Hudelot","Pierre Colombo"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.22193.pdf","arxivUrl":"https://arxiv.org/abs/2509.22193","arxivId":"2509.22193","coolPaperUrl":"https://papers.cool/arxiv/2509.22193","published":"2025-09-26T10:53:52Z","updated":"2025-09-26T10:53:52.000Z","zh_summary":"具有推理能力的大型语言模型（LLM）在广泛的任务上已经达到了最先进的性能。尽管在实践中取得了成功，但推理生效的任务和模型规模，以及其训练和推理成本，仍未被充分探索。在本工作中，我们依托于合成数据蒸馏框架开展大规模监督研究。我们比较了不同规模的指令微调（IFT）模型和推理模型，在涵盖数学为中心的任务和通用任务的广泛任务上进行评估，同时考察多项选择题和开放式题型。我们的分析表明，推理始终能够提升模型性能，常常可以匹配或超越显著更大的 IFT 系统。值得注意的是，尽管 IFT 在训练和推理成本上仍保持帕累托最优，但随着模型规模的增长，推理模型的价值越来越高，能够突破 IFT 在推理密集型和开放式任务上的性能上限。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在回答一个尚未被系统研究的核心问题：\n**“在什么条件下，显式推理（Chain-of-Thought，CoT）才真正为模型带来净收益？”**\n\n具体而言，作者通过大规模对照实验，将以下三个关键变量全部纳入同一框架，量化推理的价值：\n\n1. **任务属性**\n\n- 数学 vs 通用领域\n- 选择题（MC）vs 开放式（OE）\n2. **模型规模**\n0.5 B → 14 B 共 5 档参数量\n\n3. **成本维度**\n\n- 训练 FLOPs\n- 推理 FLOPs\n\n在完全控制数据分布、教师模型、训练预算的前提下，作者用“同一教师、同一 prompt、两种监督信号（IFT 答案 vs 带推理过程的答案）”的蒸馏方式，生成 160 万对平行样本，系统比较了：\n\n- 纯 IFT\n- 纯推理\n- 混合比例（0 %–100 %）\n- 两阶段（先 IFT 后推理）\n\n最终给出可操作的取舍准则：\n\n- ≤ 3 B：IFT 在训练与推理端均位于帕累托前沿，推理性价比低。\n- ≥ 7 B：推理在数学/开放式任务上显著突破 IFT 性能天花板，可抵消额外开销。\n- 选择题或事实问答：推理增益边际，优先扩大 IFT 模型规模而非引入推理。\n\nQ2: 有哪些相关研究？\n\n论文在 §5 中系统梳理了四条研究脉络，并指出它们各自留下的空白，正是本文试图填补之处。按主题归纳如下：\n\n1. 指令微调（IFT）与推理监督\n\n- Wei et al. 2022b；Ouyang et al. 2022；Chung et al. 2022：标准 IFT 配方。\n- Rajani et al. 2019；Nye et al. 2021；Wei et al. 2022a：引入 CoT 监督，在算术/符号推理任务上取得增益。\n→ 空白：仅报告“平均提升”，未按任务类型、模型规模、成本维度拆分，无法指导何时值得用 CoT。\n2. 强化学习优化推理策略\n\n- Schulman et al. 2015, 2017：TRPO/PPO；Shao et al. 2024b：GRPO。\n- OpenAI 2024（o1 系列）、DeepSeek-AI 2025、Mistral-AI 2025：用 RL 迭代 CoT，刷新数学/代码榜单。\n→ 空白：结果虽高，但数据混合、奖励设计、算力预算均不透明，难以归因“推理”本身的价值。\n3. 知识蒸馏（KD）与文本式 CoT 蒸馏\n\n- Kim & Rush 2016；Hsieh et al. 2023；He et al. 2024：大模型生成 CoT 教导小模型。\n- Qwen-Team 2025、DeepSeek-AI 2025：开源“推理模型”均基于蒸馏。\n→ 空白：研究聚焦“如何蒸馏得更高”，而非“在什么条件下才值得蒸馏”，缺乏对照实验。\n4. 任务敏感性与规模动力学\n\n- Liu et al. 2024：CoT 在部分任务上反而降低准确率。\n- Li et al. 2025：小模型难以直接吸收大模型深度推理轨迹。\n→ 空白：结论零散，未形成统一框架，无法给出规模-任务-成本三维取舍准则。\n\n综上，本文首次把“任务属性-模型规模-训练/推理成本”同时纳入同一可控蒸馏框架，系统量化 CoT 的边际价值，填补了上述研究所未触及的决策空白。\n\nQ3: 论文如何解决这个问题？\n\n论文采用“**单一教师-平行答案-控制变量**”的大规模蒸馏框架，把“推理是否值得用”这一决策问题转化为可量化的对照实验。具体步骤如下：\n\n1\\. 构建唯一变量：监督格式\n\n- **教师模型**：固定为 Qwen3-235B-A22B，带开关式推理模式（r=0 关，r=1 开）。\n- **输入集合 X**：同一批 prompt，分别让教师生成\n-  y_(IFT) = g_(θ_T)(x, 0) \n-  y_(CoT) = g_(θ_T)(x, 1) \n得到 160 万对 **平行样本**  (x, y_(IFT), y_(CoT)) ，彻底锁死数据分布差异。\n\n2\\. 学生模型规模扫描\n\n- **基座**：Qwen2.5 纯预训练 checkpoint，0.5 B / 1.5 B / 3 B / 7 B / 14 B 五档，与教师无预训练重叠。\n- **训练方式**：仅使用教师生成的合成文本，**零 RL**，避免奖励函数引入额外变量。\n- 纯 IFT（ρ=0）\n- 纯 CoT（ρ=1）\n- 比例混合 ρ∈{0.25,0.5,0.75}\n- 顺序式：先 IFT 后 CoT，观察“冷启动”是否有效\n\n3\\. 任务-格式矩阵评估\n\n12 项 benchmark 被拆成四维单元，确保结论粒度：\n\n| 维度 | 取值 |\n| --- | --- |\n| 领域 | General、Math |\n| 答案格式 | Multiple-Choice、Open-Ended |\n| 规模 | 0.5 B–14 B |\n| 训练/推理 FLOPs | 按附录 C 公式精确计数 |\n\n4\\. 成本-精度联合分析\n\n- **训练效率**：以 FLOPs 为横轴，Accuracy 为纵轴，绘制 Pareto 前沿。\n- **推理效率**：同样方法，但横轴为 **单次推理 FLOPs**（含长度差异）。\n- 通过插值函数  f(x)=α+β(1-e^(-γ x^δ))  外推更大模型的潜在位置，判断 CoT 何时进入前沿。\n\n5\\. 结果量化输出\n\n- **≤ 3 B**：IFT 点全部位于前沿，CoT 点始终偏离 → **不推荐推理**。\n- **≥ 7 B**：CoT 模型在前沿上与“更大 IFT 模型”重叠或超越，**推理开始具有净收益**。\n- **开放式/数学任务**：CoT 的 ΔAccuracy/ΔFLOPs 比值最高；**选择题**则几乎平行于 x 轴，增益≈0。\n\n6\\. 公开复现物料\n\n- 释放 160 万对平行数据、训练脚本、评测代码、已蒸馏模型，确保后续研究可直接在同一控制条件下继续扫描新的任务或更大的规模。\n\n通过以上设计，论文把“推理有没有用”这一经验问题，转化为可在 **任务-规模-成本** 三维空间内精确读取的 **Pareto 前沿决策图**，从而一次性给出可操作的取舍准则。\n\nQ4: 论文做了哪些实验？\n\n实验按“控制变量-扫描规模-度量成本”三轴展开，可归纳为 **5 组核心实验 + 3 组辅助实验**，全部在相同教师数据、相同基准、相同 FLOPs 计数方法下完成。\n\n1 单阶段蒸馏（mono-phasic）\n\n**目的**：锁定“监督格式”唯一变量，看不同规模下的纯 IFT 与纯 CoT 差距。\n\n- 5 种学生规模 × 2 种监督格式 × 2 个数据域（General、Math）\n- 共 20 个模型，12 项 benchmark 全测 → 图 2 / 图 12\n\n2 混合比例扫描（ρ-scan）\n\n**目的**：验证“少量 CoT 能否兼得精度与简洁”。\n\n- ρ ∈ {0, 0.25, 0.5, 0.75, 1}\n- 两种训练顺序：Tmix（同时混合） vs Tseq（先 IFT 后 CoT）\n- 5 规模 × 5 比例 × 2 顺序 → 图 3-4 / 图 13-14\n\n3 两阶段领域适配（bi-phasic）\n\n**目的**：检查“通用 CoT → 数学 CoT”是否出现遗忘或进一步增益。\n\n- 4 条路径：\n- General-IFT → Math-IFT\n- General-IFT → Math-CoT\n- General-CoT → Math-IFT\n- General-CoT → Math-CoT\n- 5 规模全跑 → 图 5 / 图 15\n\n4 训练效率 Pareto\n\n**目的**：把准确率映射到 **训练 FLOPs**，看谁能躺在前沿。\n\n- 对每规模、每 ρ 计算总训练 FLOPs（公式 1-3）\n- 绘制 Accuracy vs Training-FLOPs，标出前沿 → 图 6 / 图 16\n\n5 推理效率 Pareto\n\n**目的**：把准确率映射到 **单次推理 FLOPs**，为用户侧提供决策。\n\n- 零样本生成，温度=1，记录输出长度 → 公式 4-6 计算推理 FLOPs\n- 拟合饱和曲线外推更大模型 → 图 7 / 图 11 / 图 17\n\n6 辅助实验\n\n| 实验 | 目的 | 关键发现 |\n| --- | --- | --- |\n| 早停预算机制 | 能否靠“截断长输出”省钱 | 图 9：成本降但精度掉，脱离前沿 |\n| 最大长度翻倍 | 32 k vs 16 k token | 图 10：部分数学任务继续涨点，说明早停失效 |\n| 任务级细拆 | 把图 2-8 全部拆到单任务 | 图 12-18：CoT 在 gsm8k、aime、math-500 增益最大；winogrande、mmlu-misc 几乎无增益 |\n\n7 统计与评测细节\n\n- 全部指标由 **Llama-3.1-Nemotron-253B** 统一裁判，τ=0.7，p=0.95，减少小模型格式噪声。\n- 每个数据点至少 3 次随机种子平均，方差条在附录图中给出。\n- 训练总消耗 ≈ 70 k H100 小时，已随仓库公开日志。\n\n综上，论文共训练并评测 **> 200 个学生模型**，覆盖 **5 个数量级规模差异 × 12 个下游任务 × 2 类成本维度**，形成一张可直接查表使用的“推理-性价比地图”。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向均在论文“Discussion & Future Work”基础上细化，可直接沿用同一**平行样本-控制变量**框架，无需额外 RL 基础设施即可展开。\n\n1 任务维度外扩\n\n- **代码推理**：HumanEval、MBPP、CodeForces 数据集，沿用 IFT↔CoT 平行蒸馏，检验“语法-语义-算法”三阶推理是否同样呈现 ≥7 B 阈值。\n- **法律推理**：使用 SaulLM-7B 等法律教师，生成“法条-判例-结论”型 CoT，观察是否对 <7 B 模型即生效（法律文本逻辑链更长但单步难度低于数学）。\n- **多语言推理**：EuroLLM、Jais 等双语教师，验证非英语 CoT 是否出现规模临界点漂移。\n\n2 规模与数据双重外推\n\n- **学生模型 > 14 B**：租用 32 B / 46 B 基座，重复 ρ-scan 实验，检验 CoT 进入 Pareto 前沿的**临界规模是否收敛**（即继续增大后增益饱和还是再次跃升）。\n- **样本效率曲线**：固定 14 B 学生，从 1 % 到 100 % 对数间隔抽样 1.6 M 平行对，绘制“训练样本数-ΔAccuracy”曲线，量化 CoT 的**数据弹性 vs IFT**。\n\n3 训练动态与表示分析\n\n- **收敛行为**：每 1 % step 存 checkpoint，绘出验证集损失与下游 Acc 的相关系数，观察 CoT 是否出现**阶段性遗忘**（中间 checkpoint 在通用任务掉点）。\n- **层间探测（Probing）**：\n- 对同一 prompt，比较 IFT 与 CoT 学生在各层的**逻辑运算符**（if/then、because）激活密度，定位“推理能力”首次出现的深度。\n- 用 **diff-logits** 方法（Boizard et al. 2024）量化教师-学生间每 token 的 logits 差距，看 CoT 是否降低高层语义 token 的蒸馏误差。\n\n4 推理长度与可靠性\n\n- **最优预算策略**：\n- 在 CoT 路径中插入**置信度 token**（<confidence>0.97</confidence>），训练早期停于置信度 > θ，联合优化 θ 与最大长度，看能否**重回 Pareto 前沿**（图 9 失败原因再探）。\n- **长度外推极限**：继续把最大生成长度提到 64 k、128 k，观察数学极限题（AIME、IMO-Shortlist）是否仍保持线性提升，验证**推理深度无标度律**还是出现平台。\n\n5 混合策略稳定化\n\n- **课程式混合（Curriculum-ρ）**：\n- 训练前期 ρ=0，中期线性增至 0.5，后期保持 1.0，对比图 3 的 Tmix 震荡现象是否缓解。\n- **对比式偏好优化**：\n- 用同一教师的 IFT 与 CoT 输出构造 pair-wise 偏好 (CoT ≻ IFT when answer correct)，执行 DPO/CPO 训练，检验能否在**不增加推理长度**的前提下获得 CoT 精度。\n\n6 教师侧消融\n\n- **教师规模缩水**：用 7 B、14 B、32 B 模型当教师，生成 CoT 数据，再蒸馏到 3 B 学生，验证“**教师-学生容量比**”是否存在最优区间（Li et al. 2025 指出过小教师无法蒸馏深度推理）。\n- **教师错误干预**：故意在教师 CoT 中注入 10 % 错误中间步骤，测量学生是否出现**错误放大**（error amplification）或具备自我修正能力，评估鲁棒性。\n\n7 绿色 AI 与系统优化\n\n- **推理-训练协同降碳**：\n- 把训练 FLOPs 与推理 FLOPs 统一折算为 kgCO₂e，绘制**碳排放-Accuracy Pareto**，给出不同任务下的“碳预算”选型表。\n- **动态批长度（Dynamic Batching）**：\n- 对 CoT 模型采用“按句点/换行”提前分段缓存 KV，减少 padding 开销，看能否把图 7 的 7×-15× 成本增幅压缩到 < 5×。\n\n8 理论解释\n\n- **Scaling law 修正**：拟合新公式\n\nL(N, D, rho) = ((A) / (N^α) + (B) / (D^β)) · (1 - rho · e^(-γ N)) + ε\n\n其中 ρ 为 CoT 比例，N 为参数，D 为样本，检验 γ 是否在不同任务域保持常数，从而**预测下一世代模型的 CoT 收益**。\n\n以上任一方向均可直接复用已发布的 160 万平行对和评测流水线，在相同 FLOPs 计数与裁判模型下获得可对比结果，继续细化“何时值得用推理”的决策边界。\n\nQ6: 总结一下论文的主要内容\n\n论文核心贡献可概括为“一张图、一句话、一张表”：\n\n一句话\n\n**首次在统一数据、统一教师、统一成本度量下，给出“推理何时比 IFT 更划算”的量化答案：≥7 B 模型面对数学/开放式任务时，CoT 才能登上 Pareto 前沿，否则直接放大模型规模更省钱。**\n\n一张图（决策图）\n\n```\n模型规模 →\n│ <3 B     3-7 B     >7 B\n任务↓\nGeneral-MC │ IFT      IFT      IFT\nGeneral-OE │ IFT      IFT      CoT\nMath-MC │ IFT      CoT      CoT\nMath-OE │ IFT      CoT      CoT\n```\n\n颜色标注：绿色=IFT 位于前沿；橙色=CoT 进入前沿。\n\n一张表（关键数字）\n\n| 指标 | 纯 IFT | 纯 CoT | 结论 |\n| --- | --- | --- | --- |\n| 训练 FLOPs | 1× | 3-4× | 100 % CoT 从未落在训练前沿 |\n| 推理 FLOPs | 1× | 7×(OE) / 15×(MC) | 仅 >7 B OE 任务精度提升足以抵消 |\n| 14 B 数学 OE 精度 | 62 % | 78 % | 3 B CoT 即可达到 61 %，≈14 B IFT 水平 |\n\n附加礼包\n\n- 160 万对平行样本（IFT+CoT）与代码全部开源，后续研究可直接复用同一控制环境。\n- 提供任务-规模-成本三维 Pareto 前沿脚本，输入新模型即可自动更新决策边界。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是When%20Does%20Reasoning%20Matter%3F%20A%20Controlled%20Study%20of%20Reasoning%27s%20Contribution%20to%20Model%20Performance，链接是https://arxiv.org/pdf/2509.22193，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.22193。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1759276017466","title":"GSM8K-V: Can Vision Language Models Solve Grade School Math Word Problems in Visual Contexts","description":"Vision language models (VLMs) achieve unified modeling of images and text, enabling them to accomplish complex real-world tasks through perception, planning, and reasoning. Among these tasks, reasoning is particularly representative, with mathematical reasoning serving as a prominent example. It highlights the high-level capability of VLMs to comprehend mathematical information in images and to perform sophisticated reasoning. Recently, numerous visual mathematical reasoning benchmarks have been proposed, but they are often restricted to geometry, lack coverage of math word problems, and rarely assess reasoning across multiple images. To address these gaps, we introduce GSM8K-V, a purely visual multi-image mathematical reasoning benchmark. GSM8K-V is built by systematically mapping each sample from the widely used text-based GSM8K into visual form. Through a carefully designed automated image-generation pipeline combined with meticulous human annotation, we curate 1,319 high-quality samples. We evaluate a wide range of open-source and closed-source models on GSM8K-V. Results show that although existing VLMs have nearly saturated performance on text-based GSM8K, there remains substantial room for improvement on GSM8K-V. For example, the best-performing model, Gemini-2.5-Pro, achieves 95.22% accuracy on GSM8K but only 46.93% on GSM8K-V. We conduct a comprehensive analysis of GSM8K-V, examining the limitations of current models as well as potential directions for improvement. GSM8K-V offers a new perspective on visual mathematical reasoning and establishes a benchmark to guide the development of more robust and generalizable VLMs.","url":"https://huggingface.co/papers/2509.25160","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.466Z","metadata":{"authors":["Fan Yuan","Yuchen Yan","Yifan Jiang","Haoran Zhao","Tao Feng","Jinyan Chen","Yanwei Lou","Wenqi Zhang","Yongliang Shen","Weiming Lu","Jun Xiao","Yueting Zhuang"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.25160.pdf","arxivUrl":"https://arxiv.org/abs/2509.25160","arxivId":"2509.25160","coolPaperUrl":"https://papers.cool/arxiv/2509.25160","published":"2025-09-29T17:57:05Z","updated":"2025-09-29T17:57:05.000Z","zh_summary":"视觉语言模型（VLMs）能够实现图像与文本的统一建模，使其能够通过感知、规划和推理完成复杂的现实世界任务。在这些任务中，推理尤为具有代表性，其中数学推理是一个突出的例子。它展示了VLMs理解图像中数学信息并执行复杂推理的高阶能力。近年来，虽然提出了许多视觉数学推理基准，但它们往往局限于几何领域，缺乏对数学文字题的覆盖，并且很少评估跨多幅图像的推理能力。为填补这些空白，我们引入了GSM8K-V，这是一个纯视觉的多图数学推理基准。GSM8K-V通过系统地将广泛使用的基于文本的GSM8K中的每个样本映射为视觉形式构建而成。通过精心设计的自动图像生成流程结合细致的人类标注，我们策划了1,319个高质量样本。我们在GSM8K-V上评估了各种开源和闭源模型。结果显示，尽管现有VLMs在基于文本的GSM8K上几乎已达到性能上限，但在GSM8K-V上仍有很大提升空间。例如，表现最好的模型Gemini-2.5-Pro在GSM8K上的准确率为95.22%，而在GSM8K-V上仅为46.93%。我们对GSM8K-V进行了全面分析，考察了当前模型的局限性以及潜在的改进方向。GSM8K-V为视觉数学推理提供了新的视角，并建立了一个基准，以指导更强大、更具泛化能力的VLMs的发展。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决现有视觉-语言模型（VLM）在**纯视觉场景下的数学文字题推理能力评估缺失**的问题。具体而言，论文关注以下核心痛点：\n\n1. **现有视觉数学基准的局限性**\n\n- 过度聚焦几何图形题，忽视**文字题**的视觉化需求。\n- 缺乏**多图像语义依赖**的评估场景，无法检验模型跨图像推理能力。\n- 未覆盖**真实生活场景**中的数学问题（如价格表、时钟、测量工具等）。\n2. **文本与视觉模态的性能鸿沟**\n\n- 尽管VLM在文本版GSM8K（如Gemini-2.5-Pro达95.22%准确率）已接近饱和，但**视觉版GSM8K-V准确率骤降至46.93%**，暴露出模型在视觉语义理解与数学推理结合上的显著缺陷。\n3. **模态效应的孤立研究需求**\n\n- 需构建一个**与文本原题一一对应的纯视觉基准**，以排除任务难度差异的干扰，直接量化视觉模态对数学推理的影响。\n\n通过提出GSM8K-V，论文首次系统性地将文本数学文字题转化为**多场景漫画风格图像**，并验证：\n\n- **视觉数学推理仍是未解决的挑战**；\n- **人类在视觉推理上显著优于现有VLM**（人类91.15% vs. 最佳模型46.93%）；\n- **错误集中于视觉感知-计算绑定失败**（如误读图表、混淆物体数量）和**仪器读数错误**（如时钟、仪表盘误解）。\n\nQ2: 有哪些相关研究？\n\n论文在第2节“Related Works”中系统梳理了两条研究主线，并明确指出现有工作与GSM8K-V的差异。相关研究可归纳为以下两类：\n\n2.1 视觉-语言模型数学推理 **方法** 研究\n\n关键词：链式思维、强化学习、多模态对齐\n\n| 代表工作 | 核心贡献 | 与GSM8K-V的关系 |\n| --- | --- | --- |\n| Math-LLaVA (Shi et al., 2024) | 将视觉线索插入CoT步骤，监督微调提升几何题表现 | 仅针对几何图，未涉及文字题视觉化 |\n| GRPO/S-GRPO (Shao et al., 2024; Dai et al., 2025a) | 用可验证奖励做RL，提升代码/数学推理 | 训练技巧，未提出多图像文字题基准 |\n| MathCoder-VL (Wang et al., 2025a) | 联合生成代码+视觉解释，解决几何与代数 | 仍依赖文本问题描述，非纯视觉输入 |\n\n2.2 视觉数学 **评测基准** 研究\n\n关键词：几何、单图、文本-图表混合、鲁棒性\n\n| 基准 | 题型范围 | 图像特点 | 与GSM8K-V差异 |\n| --- | --- | --- | --- |\n| GeoQA (Chen et al., 2021) | 几何选择题 | 单图+文字 | 仅限几何，无文字题 |\n| MathVista (Lu et al., 2023) | 多领域汇总 | 单图/双图 | 含几何、图表，但文字题占比极低 |\n| MathVerse (Zhang et al., 2024) | 几何+代数 | 文本-图表信息再分配 | 仍保留文本题干，非纯视觉 |\n| MATHVision (Wang et al., 2024) | 竞赛级几何 | 单图 | 高难度几何，无关生活化文字题 |\n| MM-MATH (Sun et al., 2024) | 几何+图表 | 单图 | 引入过程打分，但无多图像依赖 |\n| DynaMath (Zou et al., 2024) | 几何 | 单图+扰动 | 鲁棒性测试，任务范围与MathVista类似 |\n| Omni-MATH (Gao et al., 2024) | 奥林匹克文字题 | 纯文本 | 仅提升文本难度，未引入视觉模态 |\n| VisuLogic (Xu et al., 2025) | 多领域视觉推理 | 单图 | 覆盖更广，但未聚焦数学文字题 |\n\n小结：GSM8K-V 的 **差异化定位**\n\n- **任务层面**：首次把**文字题**完全视觉化，保留原题语义。\n- **模态层面**：纯视觉输入，**无需文本题干**，直接评估“看图做题”。\n- **数据层面**：**多图像+跨场景语义依赖**，迫使模型进行视觉链式推理。\n\n因此，GSM8K-V与上述基准**正交互补**：前者隔离模态效应，后者覆盖广度或难度。\n\nQ3: 论文如何解决这个问题？\n\n论文将“文本数学文字题→纯视觉多图场景”的转换定义为一条**可复现、可扩展**的自动化流水线，并通过三重机制确保视觉题与原文本题**语义等价、难度对齐、干扰受控**。整体解决方案可概括为 **“三阶段工程化 pipeline + 双人交叉人工校验”**。\n\n3 阶段 pipeline（对应图 2）\n\n3.1 阶段 1：Problem Decomposition & Allocation\n\n**目标**：把文本中的数学事实无损拆散，再合理拆到多张图里。\n\n1. 三元组抽取\n用 GPT-4.1 将每道题解析为 `(object, math_value, semantic)` 集合，例如\n `(\"crayon\", \" 2\", \"one crayon price\")`$。\n\n2. 13 类数学信息分类（附录 C.1）\n价格、重量、时钟、距离、比例、计数……每类对应固定视觉渲染策略，保证“数字→视觉符号”一致性。\n\n3. 场景分配策略\n\n- **上下文归组**：同一对象/空间/功能的信息放同一场景。\n- **最终隔离**：题干文字单独放最后一张图，且不再出现新数字。\n- **原子保真**：只搬运原文已出现的数值，不预推、不合并。\n4. 受控干扰注入\n\n- **感知干扰**：加入与题无关但视觉显著的对象（装饰物）。\n- **语义干扰**：加入与目标对象语义相近的 distractor（如苹果 vs 梨）。\n\n3.2 阶段 2：Scene Description Generation\n\n**目标**：为每一场景生成“图文对齐”的文本描述，供后续图像模型渲染。\n\n1. Meta 描述模板（附录 D.2）\n对 13 类信息分别预定义模板，例如\n\n- **Price** → “Signboard: icon + 价格文本”\n- **Clock** → “Wall clock: 时针/分针位置 + 表盘刻度”\n2. 结构化三元组 → 自然语言\n用 GPT-4.1 把 `(object, action, composition)` 填充到模板，确保：\n\n- 对象、动作、空间关系显式可画；\n- 数字只出现在规定位置（价签、刻度、标签等）。\n\n3.3 阶段 3：Image Generation & Human Refinement\n\n**目标**：生成高保真、可解析、安全合规的多图故事。\n\n1. 多图批量渲染\n调用 GPT-Image-1，按顺序生成 1024×1024 Pixar 风格图像；每图严格对应阶段 2 描述。\n\n2. 双人交叉校验（3C 原则，附录 C.3）\n\n- **Consistency**：实体、数量、约束与原文一致。\n- **Completeness**：解题所需信息全部可见，无额外假设。\n- **Compliance**：无敏感内容，数字清晰可辨。\n违规样本退回重写或重画，直至双重通过。\n\n产出：GSM8K-V 基准统计\n\n| 样本量 | 1 319 题（与 GSM8K 一一对应） |\n| 场景数 | 5 343 张图，平均 4.05 张/题，最多 11 张 |\n| 题型覆盖 | 6 大类 13 子类（价格、时钟、测量、比例、计数…） |\n\n评估验证：确保“问题被解决”\n\n1. **模态纯净性**（5.4 节）\n\n- OCR 转录仅 9–12% 准确率 → 模型无法靠“读字”偷答。\n- 人工 caption 提升至 45–55% → 视觉语义仍不可替代。\n2. **人类上限**（4.2 节）\n91.15% 准确率，证明视觉题**可解**且**难度适中**。\n\n3. **VLM 性能断层**（同节）\n最佳模型仅 46.9%，远低于文本 GSM8K 的 95.2%，直接验证 pipeline 成功地把“已饱和”的文本任务升级为“未解决”的视觉挑战。\n\n结论\n\n通过上述三阶段工程化框架，论文**系统化地填补了“文本数学文字题 ↔ 纯视觉多图推理”基准的空白**，为社区提供了一个可复现、可扩展、且已验证有效性的评估工具。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“视觉数学文字题是否真正被解决”** 展开系统实验，共 5 组主实验 + 2 组辅助实验，覆盖 **性能基准、模态消融、输入格式、风格鲁棒性、错误诊断** 五大维度。所有实验均在 GSM8K-V 1 319 题上完成，统一采用 **零样本链式思维 + 整数答案抽取** 的评估协议（temperature≤0.2）。\n\n1\\. 主实验：19 套 VLM 全面对比\n\n**目的**：量化现有模型在视觉文字题上的真实水平，验证“文本饱和 vs 视觉鸿沟”。\n\n| 模型范围 | 代表结果 |\n| --- | --- |\n| 闭源 API | Gemini-2.5-Pro 46.93%（文本 95.22%） |\n| 开源梯队 | Llama-4-17B-128E 31.69%、InternVL3.5-241B-A28B 27.90% |\n| 人类上限 | 91.15% |\n\n**结论**：视觉推理仍未解决，且**人类-模型差距≥44 个百分点**。\n\n2\\. 模态消融：Text → Visual 的“降维打击”\n\n**设计**：同一批题目保留文本形式，与 GSM8K-V 视觉形式**一一对比**。\n\n| 模型 | 文本准确率 | 视觉准确率 | 绝对降幅 |\n| --- | --- | --- | --- |\n| Gemini-2.5-Pro | 95.22% | 46.93% | −48.3% |\n| GPT-4o | 94.92% | 29.57% | −65.4% |\n| Qwen2.5-VL-7B | 78.32% | 15.16% | −63.2% |\n\n**结论**：视觉模态使**所有模型跌出可用区间**，验证基准有效揭示“模态脆弱性”。\n\n3\\. 输入格式消融：Implicit vs Explicit\n\n**目的**：测试模型能否**自主从图中解析题干**。\n\n- **Implicit**：只给多图，不给文字问题。\n- **Explicit**：给图 + 人工写的“去数字”文字问题（masked question）。\n\n| 模型 | Implicit | Explicit | Δ |\n| --- | --- | --- | --- |\n| Gemini-2.5-Pro | 46.93% | 47.6% | +0.7% |\n| GPT-4o | 29.57% | 30.6% | +1.0% |\n\n**结论**：即使显式给出题干，性能仍**远低于文本基线**，说明**视觉语义绑定**才是瓶颈，而非问题理解。\n\n4\\. 多图 vs 单图（拼接）输入\n\n**目的**：验证“场景分离”是否有助于保持时序/数量关系。\n\n| 模型 | 单图拼接 | 多图序列 | Δ |\n| --- | --- | --- | --- |\n| GPT-4o | 27.45% | 29.57% | +2.1% |\n| GLM-4.5V | 26.76% | 28.28% | +1.5% |\n\n**结论**：多图输入**普遍略优**，表明**保留场景边界**有助于模型追踪逻辑链。\n\n5\\. 图像风格鲁棒性\n\n**目的**：检查 Pixar 风格是否带来“伪优势”。\n\n- 在 100 题子集上重画 **Giphli 风格**（柔和笔触、低饱和）。\n- 两风格仅渲染参数不同，语义完全一致。\n\n| 模型 | Pixar | Giphli | Δ |\n| --- | --- | --- | --- |\n| GPT-4o | 27.27% | 28.93% | +1.7% |\n| Qwen2.5-VL-72B | 18.18% | 17.36% | −0.8% |\n\n**结论**：风格差异**不显著**（Δ<2%），说明基准衡量的是**视觉语义而非美术风格**。\n\n6\\. 模态纯净性验证（OCR & Caption）\n\n**目的**：排除“模型偷看表面文字”或“人类描述兜底”的捷径。\n\n| 输入形式 | 典型准确率 | 相对视觉降幅 |\n| --- | --- | --- |\n| 纯图像（视觉基线） | 29.57% | — |\n| OCR 文本 | 11.98% | −17.6% |\n| 人工结构化 Caption | 55.11% | +25.5% |\n\n**结论**：\n\n- OCR 严重失真，**无法替代视觉**；\n- 即使给出完美文字描述，仍**低于文本基线 40 个百分点**，证明**视觉上下文包含不可替代的语义**。\n\n7\\. 错误诊断：两大主因\n\n对 200 例失败案例人工归类：\n\n| 错误类型 | 占比 | 典型案例 |\n| --- | --- | --- |\n| 感知-计算绑定错误 | 52% | 把“每包 2 朵花”误数为 3 朵，导致节省金额计算错误 |\n| 仪器读数错误 | 31% | 将时钟 3:00 误读为 2:59，或把 80% 饼图当 60% |\n\n**结论**：模型**并非算数弱**，而是**视觉量具/物体解析不准**，导致后续推理崩塌。\n\n8\\. 细粒度分类评测\n\n按 6 大数学场景给出宏观雷达图（表 2）：\n\n- **人类**：各场景 86.8–96.0%，波动小。\n- **VLM**：Signboard & Icon、Measurement 相对高（40–50%），**Other（计数、对话）** 普遍 <20%。\n\n**结论**：现有模型**依赖显式数字符号**，对**隐式计数、语义推理**最脆弱。\n\n实验总结\n\n通过**横向大模型对比 + 纵向模态消融 + 输入扰动 + 错误剖析**，论文完整回答了：\n\n1. 视觉文字题**远未解决**；\n2. 性能下降**主要归因于视觉-语义绑定失败**，而非问题变难；\n3. GSM8K-V 提供了**可复现、可诊断、可改进**的基准平台。\n\nQ5: 有什么可以进一步探索的点？\n\n以下展望基于论文实验结果与错误模式，分为 **数据层面、模型层面、训练策略、评测扩展** 四大方向，共 12 个可立即落地的探索点。\n\n1 数据层面：视觉干扰与难度可控\n\n- **动态干扰生成**\n将“感知/语义干扰”从人工规则升级为 **可微分对抗扰动** 或 **扩散模型语义注入**，实现“难度在线可调”的鲁棒性测试（延续 DynaMath 思路）。\n\n- **视觉“文字题→方程”平行语料**\n同步标注 **视觉场景图→LaTeX 方程** 的显式对齐，用于监督 **视觉-符号对齐预训练**，缓解“感知-计算绑定错误”。\n\n- **多文化视觉符号**\n价格币种、时钟制式、度量单位国际化（24 h vs 12 h、$ vs €），测试模型 **跨文化视觉符号迁移能力**。\n\n2 模型层面：架构与注意力\n\n- **专用“视觉量具”编码器**\n为时钟、饼图、尺子、仪表盘等 **仪器类图像** 设计 **几何-先验增强的 CNN/Transformer 分支**，降低仪器读数错误率（当前 31%）。\n\n- **跨场景记忆模块**\n多图输入时，在 Transformer 层引入 **场景间记忆槽（Scene-slot Memory）**，显式保存跨图数量关系，减缓“单图拼接”导致的时序信息丢失。\n\n- **视觉-数字双模嵌入**\n同一数值同时保留 **像素嵌入** 与 **数字 token 嵌入**，用 **对比学习目标** 强制两种表示在语义空间一致，提升“数字-视觉”绑定鲁棒性。\n\n3 训练策略：从 SFT 到 RL\n\n- **视觉链式思维微调**\n收集 10k 级“视觉多图→逐步推理”人工标注，构建 **图文交错 CoT 数据**，监督微调现有 VLMs，直接针对“多步视觉推理”优化。\n\n- **可验证奖励 RL（RLVR）**\n利用 GSM8K-V 的 **确定性整数答案** 作为稀疏奖励，采用 Group-Relative Policy Optimization（GRPO）对 VLM 进行 **视觉推理强化学习**，无需人工逐步标注。\n\n- **课程式难度调度**\n按“单图→双图→多图”、“无干扰→感知干扰→语义干扰”逐级提升训练难度，避免一次性过难导致 **梯度消失/过度思考** 现象。\n\n4 评测扩展：任务与模态\n\n- **文本-视觉混合赛道**\n引入 **“部分信息在图、部分在文”** 的混合输入，模拟真实世界 **不完整视觉证据** 场景，推动模型主动 **跨模态信息检索与补全**。\n\n- **过程级评测**\n在 GSM8K-V 上扩展 **逐步标注**（每步所需图像区域、数值、运算符），构建 **细粒度过程正确性** 指标，超越当前仅看最终答案的评估。\n\n- **视频数学文字题**\n将静态多图升级为 **短视频**（如时钟走动、价格变化、物体移动），引入 **时序动态推理** 维度，测试模型对 **视觉状态转移** 的建模能力。\n\n5 短期可交付的“小任务”\n\n| 任务 | 预期收益 | 工作量 |\n| --- | --- | --- |\n| 1. 仪器类子集 + 专用编码器 | 读数错误率 ↓ 15% | 2-3 周 |\n| 2. 视觉 CoT 数据 5k+SFT | 整体准确率 ↑ 5-8% | 1 月 |\n| 3. RLVR 试点（Gemini-2.5-Pro 基线） | 视觉推理自我迭代 | 2 月 |\n| 4. 多文化时钟/价格子集 | 揭示文化偏差 | 3 周 |\n\n总结\n\nGSM8K-V 首次把“文字题视觉化”变成可复现的流水线，为社区提供了**诊断模型视觉推理短板的手术刀**。下一步研究可沿**数据增强、架构先验、训练信号、评测维度**四轴展开，重点攻克**视觉量具读数、跨图数量绑定、文化符号迁移**三大核心瓶颈。\n\nQ6: 总结一下论文的主要内容\n\nGSM8K-V 论文要点速览\n\n1\\. 研究动机\n\n- 现有视觉数学基准≈几何图题库，**忽视文字题**且**单图为主**\n- 文本 GSM8K 已被刷到 95%+，但同题改视觉后性能未知\n→ 构建**纯视觉、多图、文字题**基准，测量 VLMs 真实视觉推理鸿沟\n\n2\\. GSM8K-V 构建流程（三阶段 Pipeline）\n\n| 基准 | 题型范围 | 图像特点 | 与GSM8K-V差异 |\n| --- | --- | --- | --- |\n| GeoQA (Chen et al., 2021) | 几何选择题 | 单图+文字 | 仅限几何，无文字题 |\n| MathVista (Lu et al., 2023) | 多领域汇总 | 单图/双图 | 含几何、图表，但文字题占比极低 |\n| MathVerse (Zhang et al., 2024) | 几何+代数 | 文本-图表信息再分配 | 仍保留文本题干，非纯视觉 |\n| MATHVision (Wang et al., 2024) | 竞赛级几何 | 单图 | 高难度几何，无关生活化文字题 |\n| MM-MATH (Sun et al., 2024) | 几何+图表 | 单图 | 引入过程打分，但无多图像依赖 |\n| DynaMath (Zou et al., 2024) | 几何 | 单图+扰动 | 鲁棒性测试，任务范围与MathVista类似 |\n| Omni-MATH (Gao et al., 2024) | 奥林匹克文字题 | 纯文本 | 仅提升文本难度，未引入视觉模态 |\n| VisuLogic (Xu et al., 2025) | 多领域视觉推理 | 单图 | 覆盖更广，但未聚焦数学文字题 |\n\n0\n\n3\\. 主要实验结果\n\n- **19 套 VLM 全面评估**\n- 最佳 Gemini-2.5-Pro：46.93%（文本 95.22%）\n- 人类上限：91.15%\n- **模态消融**\n- 文本→视觉平均骤降 50+ 个百分点，**视觉推理远未解决**\n- **输入格式**\n- 显式题干仅↑1-2%，**核心瓶颈是视觉-语义绑定**\n- **多图 vs 单图拼接**\n- 多图序列略优（+1-2%），**保留场景边界有助逻辑追踪**\n- **风格鲁棒性**\n- Pixar ↔ Giphli 差异 <2%，**风格不影响语义评测**\n- **错误诊断**\n- 52% 感知-计算绑定错误（数错物体）\n- 31% 仪器读数错误（时钟/饼图误读）\n\n4\\. 贡献与影响\n\n- 首个**与 GSM8K 一一对应的纯视觉文字题基准**，实现跨模态难度隔离\n- 揭示 VLMs **视觉数学推理显著短板**，为后续研究提供可复现平台\n- 开源数据、代码、评测脚本，推动**多图视觉链式推理**方向\n\n> GSM8K-V 让“文字题视觉化”从概念变成可量化的挑战，为下一代多模态模型划定新的训练与评测靶标。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是GSM8K-V%3A%20Can%20Vision%20Language%20Models%20Solve%20Grade%20School%20Math%20Word%20Problems%20in%20Visual%20Contexts，链接是https://arxiv.org/pdf/2509.25160，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25160。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1759276017467","title":"EditScore: Unlocking Online RL for Image Editing via High-Fidelity Reward Modeling","description":"Instruction-guided image editing has achieved remarkable progress, yet current models still face challenges with complex instructions and often require multiple samples to produce a desired result. Reinforcement Learning (RL) offers a promising solution, but its adoption in image editing has been severely hindered by the lack of a high-fidelity, efficient reward signal. In this work, we present a comprehensive methodology to overcome this barrier, centered on the development of a state-of-the-art, specialized reward model. We first introduce EditReward-Bench, a comprehensive benchmark to systematically evaluate reward models on editing quality. Building on this benchmark, we develop EditScore, a series of reward models (7B-72B) for evaluating the quality of instruction-guided image editing. Through meticulous data curation and filtering, EditScore effectively matches the performance of learning proprietary VLMs. Furthermore, coupled with an effective self-ensemble strategy tailored for the generative nature of EditScore, our largest variant even surpasses GPT-5 in the benchmark. We then demonstrate that a high-fidelity reward model is the key to unlocking online RL for image editing. Our experiments show that, while even the largest open-source VLMs fail to provide an effective learning signal, EditScore enables efficient and robust policy optimization. Applying our framework to a strong base model, OmniGen2, results in a final model that shows a substantial and consistent performance uplift. Overall, this work provides the first systematic path from benchmarking to reward modeling to RL training in image editing, showing that a high-fidelity, domain-specialized reward model is the key to unlocking the full potential of RL in this domain.","url":"https://huggingface.co/papers/2509.23909","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.467Z","metadata":{"authors":["Xin Luo","Jiahao Wang","Chenyuan Wu","Shitao Xiao","Xiyan Jiang","Defu Lian","Jiajun Zhang","Dong Liu","Zheng liu"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2509.23909.pdf","arxivUrl":"https://arxiv.org/abs/2509.23909","arxivId":"2509.23909","coolPaperUrl":"https://papers.cool/arxiv/2509.23909","published":"2025-09-28T14:28:24Z","updated":"2025-09-28T14:28:24.000Z","zh_summary":"基于指令的图像编辑已取得显著进展，但现有模型在处理复杂指令时仍面临挑战，而且通常需要多次采样才能得到理想结果。强化学习（RL）提供了一种有前景的解决方案，但其在图像编辑中的应用受到高保真、高效奖励信号缺乏的严重限制。在本工作中，我们提出了一种全面的方法来克服这一障碍，核心是开发最先进的、专用的奖励模型。我们首先介绍了 EditReward-Bench，这是一个系统化评估编辑质量的奖励模型基准。基于该基准，我们开发了 EditScore，一系列用于评估基于指令的图像编辑质量的奖励模型（7B-72B）。通过精心的数据策划和过滤，EditScore 有效地达到了专有 VLMs 的性能水平。此外，结合针对 EditScore 生成性质量身定制的有效自集成策略，我们最大的模型甚至在基准测试中超越了 GPT-5。随后我们证明，高保真的奖励模型是解锁在线 RL 图像编辑的关键。实验表明，即使是最大的开源 VLMs 也无法提供有效的学习信号，而 EditScore 能够实现高效且稳健的策略优化。将我们的框架应用于强大的基础模型 OmniGen2，获得的最终模型显示出显著且持续的性能提升。总体而言，本工作首次提供了从基准测试到奖励建模再到 RL 训练的系统化路径，表明高保真、领域专用的奖励模型是释放 RL 在图像编辑领域全部潜力的关键。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心针对的是“在线强化学习（online RL）在指令式图像编辑领域难以落地”这一瓶颈，其根源在于缺少一个**高保真、高效且可扩展的奖励信号**。具体而言，作者试图解决以下三个紧密耦合的子问题：\n\n1. 缺乏可靠基准\n社区此前没有专门、多维度的 benchmark 来系统衡量“编辑奖励模型”的优劣，导致不同方法的对比无法复现、无法量化。\n\n2. 通用 VLM 奖励失效\n直接拿 GPT-5、Gemini-2.5 等专有 VLM 做奖励虽精度尚可，但在线 RL 需要成千上万次查询，成本不可接受；而开源 VLM（如 Qwen2.5-VL-72B）即便参数量大，在细粒度编辑质量判断上仍远低于人类一致性，导致策略训练崩溃或无法收敛。\n\n3. 无开源专用奖励模型\n此前工作要么仅针对早期编辑模型（如 P2P、IP2P）做小规模蒸馏，要么仅用 CLIP 分数自动标注，任务覆盖窄、数据封闭，无法支撑当今 SOTA 编辑模型的在线 RL 训练。\n\n为此，作者提出一整套“从基准→奖励模型→在线 RL”的系统性方案，并开源了对应工具链，首次证明：**只有构建专门面向图像编辑的高保真奖励模型，才能真正解锁在线强化学习在该领域的潜力**。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究划分为两条主线，并指出其局限。可归纳为以下两类、共 8 个代表性方向，均与“编辑奖励模型”或“图像生成奖励模型”直接相关：\n\n1. 图像生成（T2I）奖励模型\n\n- CLIP-Reward 系列：Wu et al. 2023b、Kirstain et al. 2023、Zhang et al. 2024b —— 以 CLIP 为骨干，人工偏好微调，仅针对文生图任务。\n- VLM-as-Reward：Ma et al. 2025、Wang et al. 2025b、Gao et al. 2025 —— 直接调用闭源/开源 VLM 给出标量分，未对编辑场景特化。\n2. 图像编辑奖励模型\n\n- 掩码控制型：Ren et al. 2024、Gong et al. 2025 —— 依赖人工掩码，仅评估“被掩码区域”质量，与指令无关。\n- 早期指令编辑：Zhang et al. 2024a —— 蒸馏 BLIP，仅服务 P2P/IP2P 两个老模型，任务类别 < 4 种。\n- CLIP-Score 自动标注：Chen et al. 2025 —— 用 CLIP 分数自动生成 8B LLaVA-Next 奖励数据，未开源且数据局限于 5 类简单编辑。\n\n作者指出上述工作的共同缺陷：\n\n- 任务覆盖窄（≤ 5 类）、生成模型陈旧，无法评估当今 SOTA 编辑器；\n- 数据封闭或仅依赖 CLIP，缺乏人工精标，导致奖励信号在复杂编辑上失真；\n- 无公开 benchmark，社区无法复现或继续改进。\n\n与之对比，本文首次给出 13 类任务、覆盖 11 种最新编辑模型的开源基准 EditReward-Bench，并训练了 7B–72B 的专用生成式奖励模型 EditScore，填补了这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文采用“三步闭环”策略，将“没有可靠奖励”这一核心瓶颈拆解为可执行的 Benchmark→Reward Model→Online RL 链条，每一步都针对前文所述痛点给出针对性设计。\n\n1. 建立可复现的评估基准 EditReward-Bench\n\n- 任务广度：从现有数据集（GEdit-Bench-EN、ImgEdit-Bench）提炼 13 类子任务，覆盖 Subject / Appearance / Scene / Advanced 四个难度层级。\n- 模型广度：用 11 种 SOTA 编辑器（含 Gemini-2.5-image-preview、gpt-image-1 等闭源模型）生成 3 072 对候选图，保证“好-坏”分布跨度大。\n- 多维细粒度标注：Prompt Following、Consistency、Overall Quality 三维独立打分；允许“并列 tier”排序，再拆成 944+890+1 238 对偏好对。\n- 双重人工一致性校验：仅保留两名专家标注完全一致的样本，确保 ground-truth 可靠。\n2. 训练专用奖励模型 EditScore\n\n- 数据工程：\n– 先让 Qwen-2.5-VL-72B 自动生成 70 k（奖励训练）+ 60 k（RL 训练）图像-指令对；\n– 用 GPT-4.1 按 VIEScore 框架打出 SC（语义一致性）与 PQ（感知质量）分数，并写理由；\n– 组内最高分截断 + 组内方差过滤，去掉“不可完成”或“难分优劣”样本。\n- 模型结构：以 Qwen2.5-VL 为底座，LoRA 微调成生成式奖励模型——输入 (Instruction, Input, Output) → 输出 (Reasoning, Score)。\n- 推理时自集成：K 次独立前向 → 只聚合 K 个标量分，降低方差；实验显示 K=4 即可 72B 单模型超越 GPT-5。\n3. 把奖励模型接入在线 RL，验证“高保真奖励”是训练稳定的关键\n\n- Best-of-N 预实验：用 EditScore 在 OmniGen2、Flux.1-Kontext-dev、Qwen-Image-Edit 上选图，平均 VIEScore 提升 0.2–0.4，证明奖励模型本身判别力足够。\n- 在线 RL：以 OmniGen2 为基座，采用 Flow-GRPO 算法，用 EditScore-7B（单模型或 Avg@4）作为终端奖励。\n– 对比实验：同等规模的通用 VLM（Qwen2.5-VL-7/32/72B）当奖励时，训练曲线震荡甚至崩溃；EditScore 则稳定收敛。\n– 最终增益：在 GEdit-Bench-EN 上整体分 +0.40，Semantic Control +0.48；ImgEdit-Bench 再 +0.23，显著优于用 GPT-4.1 奖励的 UniPic2 基线。\n\n通过“基准→奖励→RL”全链路开源，论文首次证明：只有面向编辑任务专门设计的高保真奖励模型，才能真正解锁在线强化学习在图像编辑上的潜力。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 4 组实验，覆盖“奖励模型本身好不好”到“奖励模型能不能把 RL 训练跑通”两个层次，所有结果均基于公开 benchmark 或可复现脚本。\n\n1. EditReward-Bench 上的奖励模型精度对比\n\n- 参赛方：GPT-4.1 / GPT-5 / Gemini-2.5-Pro 三大闭源 VLM；Qwen2.5-VL-7/32/72B 代表开源通用 VLM；自研 EditScore-7/32/72B。\n- 指标：三维（PF、C、O） pairwise 准确率，共 3 072 对人工标注样本。\n- 关键结果：\n– 单 pass 下 EditScore-72B 已持平 GPT-4.1（0.755 vs 0.755）；自集成 Avg@4 后达到 0.763，超越 GPT-5。\n– 7B 版即可击败 10× 大的 Qwen2.5-VL-72B（0.659 vs 0.612），验证“专用>通用”。\n2. 推理时 scaling 的性价比分析\n\n- 固定预算（TFLOPs）下比较两条路线：\n– 横向扩参数：7B→32B→72B；\n– 纵向扩推理：K=1→2→4→8→16 次自集成。\n- 结论：同成本下增加 K 带来的准确率增益远高于增加参数量，且可共享 KV-cache，实际部署更省。\n3. Best-of-N 选择实验（验证奖励模型“即插即用”能力）\n\n- 设置：给定同一输入，让 3 个 SOTA 编辑器（OmniGen2、Flux.1-Kontext-dev、Qwen-Image-Edit）各生成 N∈{1,2,4,8,16} 张候选，用 EditScore 选最高分样本。\n- 指标：VIEScore Overall。\n- 结果：\n– 三模型均随 N 增大而单调提升，OmniGen2 增益最大（+0.26）。\n– Avg@4 版本比单 pass 再提升 0.03–0.05，说明自集成对“选图”同样有效。\n4. 端到端在线 RL 训练（核心实验）\n\n- 基座：OmniGen2；算法：Flow-GRPO；奖励：EditScore-7B（单 pass & Avg@4）与 Qwen2.5-VL 系列对比。\n- 训练配置：T=20 步扩散轨迹，组大小 G=12，全局 batch 288，KL 系数 β=0.04，共 700 步。\n- 监测指标：训练过程 VIEScore Overall；最终用 GEdit-Bench-EN 与 ImgEdit-Bench 报告 SC、PQ、O 三项。\n- 结果：\n– 通用 VLM 做奖励时训练曲线剧烈震荡，72B 亦无法收敛；EditScore 曲线平稳上升。\n– 自集成奖励信号使收敛速度提升约 30%，最终分再 +0.02。\n– 相对基座 OmniGen2，RL 后 GEdit-Bench-EN 整体分 6.28→6.68（+0.40），其中 SC 高至 7.20（+0.48）；ImgEdit-Bench 整体分 3.40→3.63（+0.23），显著优于同期用 GPT-4.1 奖励的 UniPic2 基线。\n\n以上实验从“模型级对比”到“系统级端到端”逐层验证：高保真专用奖励模型是在线 RL 在图像编辑任务中稳定且显著提效的关键。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，按“数据-模型-算法-系统-评测”五层归纳，均直接对应论文尚未充分展开或尚未触及的开放问题。\n\n数据层\n\n- **多语言-多文化指令**\n当前 70 k 指令以英文为主，可扩展至中文、日文、阿拉伯语等，检验奖励模型对文化语境、俗语、谐音梗的鲁棒性。\n- **长文本与多轮指令**\n现有指令平均 ≤ 20 tokens，可构造“故事级”多句、多轮编辑（先换背景再改天气再加人物），验证奖励模型对长程语义依赖的敏感度。\n- **自动困难度标注**\n用编辑成功率或人类平均打分反向估计“任务困难度”，实现课程式 RL（Curriculum RL），而非现在的均匀采样。\n\n模型层\n\n- **扩散-奖励一体化架构**\nEditScore 仍基于 VLM，可探索“扩散-判别”共享权重架构（类似 DDGAN 或 DiscoDiffusion），让奖励信号直接反向传播到噪声预测网络，省掉像素-文本-像素三级循环。\n- **多模态混合专家（MoE）奖励**\n将语义、美学、几何、频域分别设为 Expert，用门控网络动态聚合，降低单一 VLM 在“一致性”维度上精度不足的短板。\n- **可解释性再深入**\n当前仅输出一句 reasoning，可强制生成“注意力热图”或“编辑掩码”作为视觉解释，与人类对齐更直观，也便于 debug 失败案例。\n\n算法层\n\n- **奖励-策略协同训练**\n目前奖励固定后再 RL。可尝试“奖励-策略交替更新”或对抗式奖励学习（如 RAFT、DPO），防止出现奖励过度自信导致的模式崩溃。\n- **稀疏奖励扩展至密集奖励**\n仅终端奖励 rT 用于 GRPO；可将 EditScore 改为每步扩散点都给出中间分数，构造 dense 奖励，减少方差，提高样本效率。\n- **多目标 RL**\nPF、C、O 三维目前加权几何平均，可显式用多目标 Pareto 优化，让用户在推理时滑动权重即可得到“高保真+高一致”或“高一致+高美观”等不同策略。\n\n系统层\n\n- **在线蒸馏-压缩**\n72B 奖励模型推理成本仍高，可在 RL 过程中同步蒸馏出 3B/1B 学生奖励网络，实现“大老师-小学生”在线混合部署，降低 60-80 % GPU 小时。\n- **异构算力调度**\n奖励模型与策略模型往往占用不同显存-计算比，可设计“奖励即服务”微服务池，按 QPS 动态扩缩容，避免 H100 长时间空转。\n- **边缘端适配**\n将 EditScore 量化-剪枝后部署到边缘 GPU，让手机或 AIGC 相机可本地完成 Best-of-8 选择，无需回传云端。\n\n评测层\n\n- **对抗性-鲁棒性 benchmark**\n构造“微小但致命”的对抗指令（仅改一个颜色词、仅移动 5 px 物体），测试奖励模型是否会被高频词或视觉轻微扰动误导。\n- **人机协同编辑闭环**\n引入真实用户在线修改→再评分→再 RL，形成“人类在环”持续学习，检验奖励模型对分布外人类反馈的适应能力。\n- **跨域迁移评测**\n将在 EditReward-Bench 上训练的奖励模型零样本迁移到视频编辑、3D 纹理编辑、音频-视觉联合编辑，验证其是否学到“通用编辑语义”。\n\n一句话总结\n\nEditScore 解决了“可用”问题，下一步是“更好、更快、更通用”——从多语言数据到密集奖励算法，再到边缘部署与持续学习，每一步都值得深入。\n\nQ6: 总结一下论文的主要内容\n\n论文核心贡献可概括为“一个基准、一套模型、一条通路”：\n\n1. **EditReward-Bench**\n\n- 首个面向指令式图像编辑的奖励模型评测基准，含 13 类任务、3 072 对人工精标偏好，三维评估（Prompt Following / Consistency / Overall Quality）。\n2. **EditScore 系列奖励模型**\n\n- 7 B–72 B 生成式专用奖励模型，通过高质量数据蒸馏 + 推理时自集成，在基准上超越 GPT-5，成为当前开源最佳编辑奖励信号。\n3. **在线 RL 落地路径**\n\n- 证明通用 VLM 奖励无法支撑稳定训练，而 EditScore 可驱动 Flow-GRPO 在线强化学习；OmniGen2 经 RL 后在 GEdit-Bench-EN 提升 0.40 分，ImgEdit-Bench 提升 0.23 分，首次实现“高保真奖励 → 稳定策略优化”的端到端闭环。\n\n全文给出从评测、奖励到训练的全栈开源方案，为后续 RL-based 图像编辑研究提供可直接复用的基础设施。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是EditScore%3A%20Unlocking%20Online%20RL%20for%20Image%20Editing%20via%20High-Fidelity%20Reward%20Modeling，链接是https://arxiv.org/pdf/2509.23909，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.23909。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1759276017467","title":"SparseD: Sparse Attention for Diffusion Language Models","description":"While diffusion language models (DLMs) offer a promising alternative to autoregressive models (ARs), existing open-source DLMs suffer from high inference latency. This bottleneck is mainly due to the attention's quadratic complexity with respect to context length in computing all query-key pairs. Intuitively, to reduce this complexity, a natural strategy is to restrict attention to sparse patterns that retain only the most relevant connections. Such approaches are well-established in ARs, where attention follows fixed and clearly defined sparse patterns. However, in DLMs, we observe distinct sparsity behaviors: (1) attention patterns vary across heads, (2) attention patterns in each head remain highly similar across denoising steps, and (3) early denoising steps are critical for generation. These findings render sparse attention methods designed for ARs largely incompatible with DLMs, as they fail to capture head-specific structures and risk degrading generation when applied in early denoising steps. To address these challenges, we propose SparseD, a novel sparse attention method for DLMs. Leveraging the observations, SparseD only requires pre-computing head-specific sparse patterns one time, and reuses them across all steps. This prevents recomputing sparse patterns at each denoising step. Meanwhile, SparseD uses full attention in the early steps, then switches to sparse attention later to maintain generation quality. Together, these establish SparseD as a practical and efficient solution for deploying DLMs in long-context applications. Experimental results demonstrate that SparseD achieves lossless acceleration, delivering up to $1.50\\times$ speedup over FlashAttention at a 64k context length with 1,024 denoising steps.","url":"https://huggingface.co/papers/2509.24014","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.467Z","metadata":{"authors":["Zeqing Wang","Gongfan Fang","Xinyin Ma","Xingyi Yang","Xinchao Wang"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.24014.pdf","arxivUrl":"https://arxiv.org/abs/2509.24014","arxivId":"2509.24014","coolPaperUrl":"https://papers.cool/arxiv/2509.24014","published":"2025-09-28T18:10:10Z","updated":"2025-09-28T18:10:10.000Z","zh_summary":"虽然扩散语言模型（DLMs）为自回归模型（ARs）提供了一种有前景的替代方案，但现有的开源 DLM 在推理延迟方面仍存在问题。这个瓶颈主要源于计算所有查询-键对时注意力机制相对于上下文长度呈平方复杂度。直观来看，为了减少这种复杂性，一个自然的策略是将注意力限制在只保留最相关连接的稀疏模式上。这类方法在 AR 中已经成熟，其中注意力遵循固定且明确定义的稀疏模式。然而，在 DLM 中，我们观察到不同的稀疏行为：(1) 注意力模式在不同的头部之间存在差异；(2) 每个头部的注意力模式在各去噪步骤中高度相似；(3) 早期去噪步骤对生成至关重要。这些发现使得针对 AR 设计的稀疏注意力方法在 DLM 中很难兼容，因为它们无法捕捉头部特定的结构，并且在早期去噪步骤中使用时可能会降低生成质量。为了解决这些挑战，我们提出了 SparseD，一种用于 DLM 的新型稀疏注意力方法。借助上述观察，SparseD 仅需要一次预计算头部特定的稀疏模式，并在所有步骤中重复使用，从而避免在每个去噪步骤中重复计算稀疏模式。同时，SparseD 在早期步骤使用全注意力，然后在后期切换为稀疏注意力以保持生成质量。综合来看，这使 SparseD 成为在长上下文应用中部署 DLM 的一种实用且高效的方案。实验结果表明，SparseD 实现了无损加速，在 64k 上下文长度和 1,024 去噪步骤下，相比 FlashAttention 速度提升高达 $1.50\\times$。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决扩散语言模型（DLM）在长上下文场景下推理延迟过高的问题。核心瓶颈在于注意力机制对上下文长度的二次复杂度，而现有面向自回归模型（AR）的稀疏注意力方法无法直接迁移到 DLM，原因在于：\n\n- DLM 的注意力模式在**不同头之间差异显著**（head-specific）\n- 每个头的注意力模式在**去噪步间高度一致**（step-invariant）\n- **早期去噪步**对生成质量至关重要，直接稀疏化会显著降低性能\n\n为此，作者提出 SparseD，一种专为 DLM 设计的稀疏注意力机制，通过\n\n1. 早期全注意力保护生成质量\n2. 一次性为每头预计算并复用块级稀疏模式\n3. 对预填充与生成 token 分别进行隔离式选择\n\n在 64 k 上下文、1 024 步去噪条件下，相对 FlashAttention 实现 **1.50× 无损加速**。\n\nQ2: 有哪些相关研究？\n\n与 SparseD 直接相关的研究可归纳为三类，均围绕“如何降低注意力二次复杂度”展开，但各自假设与 DLM 的适配程度不同：\n\n1. 自回归稀疏注意力\n\n- 固定模式：Sliding-Window (Jiang et al., 2023)、Sink Attention (Xiao et al., 2023)\n- 动态模式：Anchor-Attention (Zhang et al., 2025)、FlexPrefill (Lai et al., 2025)\n共同点：假设注意力呈时序局部化或 sink-token 集中化，与 DLM 的 head-specific、step-invariant 模式不符，故直接迁移会掉点。\n2. 扩散语言模型加速\n\n- 缓存类：dKV-Cache (Ma et al., 2025)、Fast-dLLM (Wu et al., 2025)\n共同点：利用激活稳定特性缓存 KV，但长上下文下仍面临二次注意力，且缓存更新策略带来不可忽略误差。\n3. 通用注意力优化内核\n\n- FlashAttention (Dao, 2023)、FlexAttention (Dong et al., 2024)\n提供 IO 感知与可编程稀疏接口，被 SparseD 用作“全注意力→稀疏注意力”切换的底层 kernel，但本身不解决稀疏模式如何构建的问题。\n\nSparseD 首次将“head-specific、step-invariant、early-critical”三项 DLM 独有观察系统性地引入稀疏注意力设计，从而与上述研究形成互补。\n\nQ3: 论文如何解决这个问题？\n\n论文将问题拆解为三项核心观察，并据此设计 SparseD 算法，具体解决路径如下：\n\n1. 观察 1：头间模式差异\n放弃 AR 的固定稀疏模板，改为\n\n- 以块为单位一次性计算每头注意力矩阵  A∈R^(l× l) \n- 对预填充与生成 token 分别做 Top-ρ% 选择，得到头专属稀疏索引集  S \n- 将  S  转为掩码  M_S  供后续步复用，避免每步重算\n2. 观察 2：步间模式稳定\n利用“同一头在不同去噪步的注意力高度相似”特性，\n\n- 仅在  T·skip%  步（默认 20%）执行一次孤立选择\n- 之后所有步直接复用  M_S ，注意力计算量从  O(l^2)  降至  O(l·rho l) ，且无需额外内存搬移\n3. 观察 3：早期步敏感\n在前  T·skip%  步强制使用全注意力（FlashAttention），确保初始去噪信号完整；后续步切换至 FlexAttention 执行稀疏注意力，实现“质量-效率”解耦\n\n综合以上三点，SparseD 的伪代码流程可概括为：\n\n&if t le T·skip% : &quad out = FlashAttn(Q,K,V) &quad if t = T·skip% : &quadquad compute A',; S=IsolatedTop-ρ%(A'),; M_Sarrow S &else : &quad out = FlexAttn(Q,K,V,M_S)\n\n通过“先全后疏、一次选择、多头隔离”的策略，SparseD 在 64 k 上下文、1 024 步去噪场景下相对 FlashAttention 取得 1.50× 无损加速。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“**无损加速**”这一核心主张展开，分为**精度对比**、**延迟测评**、**消融分析**与**超参敏感性**四大部分，全部在 NVIDIA A800 80 GB 上完成。\n\n1. 精度对比\n\n- 模型：Dream-7B-Instruct、LLaDA-1.5\n- 基准：MMLU、GSM8k、HumanEval、RULER-4k/8k\n- 对手：\n– AR 稀疏方法：Sliding-Window、StreamingLLM\n– DLM 缓存方法：dKV-Cache、Fast-dLLM\n- 结果：SparseD 平均掉点 ≤ 0.04 %，长文 RULER-8k 上比缓存方法高 10–16 %。\n2. 延迟测评\n\n- 数据集：RULER 单条样本，长度 4 k–64 k，步数 128–1024\n- 对手：FlashAttention、StreamingLLM\n- 结果：\n– 128 步下，64 k 上下文加速 1.23×–1.25×\n– 1024 步下，加速升至 1.50×（Dream）/ 1.48×（LLaDA）\n3. 消融实验\n在 LLaDA-1.5+RULER-4k 上逐组件移除：\n\n- 移除 Skipping Sparse：精度 −3.07 %\n- 移除 Sparse Reusing：延迟 +1671 %\n- 移除 Isolated Selection：精度 −0.36 %\n4. 超参敏感性\n\n- skip 比例：0 %–50 %，20 % 开始收益饱和\n- ρ 比例：5 %–50 %，30 % 后精度饱和，延迟线性增加\n最终统一采用 skip=20 %、ρ=30 %（长文）或 50 %（短文）。\n\n综合结果验证：SparseD 在 64 k×1 024 的极端设定下仍能保持原模型精度，同时取得 1.50× wall-clock 加速。\n\nQ5: 有什么可以进一步探索的点？\n\n- **系统级协同优化**\n- 将头专属稀疏模式与 GPU warp-specialized 内核或 FPGA 流式架构结合，进一步隐藏不规则访存延迟\n- 探索稀疏模式压缩编码（如 CSR、bitmap-of-blocks）与 L2 缓存预取策略，减少 DRAM 带宽占用\n- **与缓存方法正交融合**\n- 在 SparseD 的“全注意力阶段”仅缓存预填充 KV，在“稀疏阶段”利用复用模式做 KV 选择性淘汰，实现“稀疏+缓存”双路降耗\n- 研究动态阈值：当复用模式与实时注意力余弦相似度低于某阈值时，触发局部重选，兼顾精度与开销\n- **层次化稀疏模式**\n- 对更长上下文（≥ 256 k）引入层级稀疏：局部块内密集 + 全局块间稀疏，降低选择难度\n- 结合低秩近似（Linformer、Nyström）对全局 token 进一步压缩，形成“稀疏+低秩”混合注意力\n- **任务自适应 ρ 与 skip**\n- 基于强化学习或元网络，在推理前根据 prompt 类型、长度、温度自动输出最优 (ρ, skip) 组合，实现任务级最优 Pareto 前沿\n- **端到端训练感知**\n- 将稀疏掩码  M_S  作为可训练二值变量，通过 Gumbel-Softmax 或 Straight-Through 估计器端到端优化，使稀疏模式直接面向生成损失收敛，而非仅依赖一次性启发式选择\n- **多模态扩散模型迁移**\n- 验证 SparseD 在文本-图像、文本-视频扩散 Transformer 上的通用性；研究跨模态注意力头是否呈现类似的 head-specific、step-invariant 特性，并设计模态隔离选择策略\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：扩散语言模型（DLM）因注意力二次复杂度，在长上下文推理时延迟极高；直接套用自回归稀疏方法会掉点。\n- **关键观察**\n\n1. 注意力模式**头间差异大**\n2. 同一头模式在**去噪步间高度稳定**\n3. **早期步**对生成质量至关重要\n- **方法：SparseD**\n- 早期  T· skip% 步用**全注意力**保质量\n- 仅在该阶段**一次性**为每头做**块级+隔离式**Top-ρ% 选择，得到头专属稀疏掩码  M_S \n- 后续步**复用** M_S  执行稀疏注意力，复杂度降至  O(l·rho l) \n- **实验结果**\n- 精度：在 Dream-7B-Instruct、LLaDA-1.5 上平均掉点 ≤ 0.04 %，长文任务比缓存方法高 10–16 %\n- 延迟：64 k 上下文、1 024 去噪步下，相对 FlashAttention 取得 **1.50× 无损加速**\n- **结论**：SparseD 首次利用 DLM 独有的“头特定-步稳定-早期关键”特性，实现**长上下文无损加速**，为 DLM 实际部署提供了可行路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SparseD%3A%20Sparse%20Attention%20for%20Diffusion%20Language%20Models，链接是https://arxiv.org/pdf/2509.24014，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24014。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-13-1759276017468","title":"EasySteer: A Unified Framework for High-Performance and Extensible LLM Steering","description":"Large language model (LLM) steering has emerged as a promising paradigm for controlling model behavior at inference time through targeted manipulation of hidden states, offering a lightweight alternative to expensive retraining. However, existing steering frameworks suffer from critical limitations: computational inefficiency, limited extensibility, and restricted functionality that hinder both research progress and practical deployment. We present EasySteer, a unified framework for high-performance, extensible LLM steering built on vLLM. Our system features modular architecture with pluggable interfaces for both analysis-based and learning-based methods, fine-grained parameter control, pre-computed steering vectors for eight application domains, and an interactive demonstration system. Through deep integration with vLLM's optimized inference engine, EasySteer achieves 5.5-11.4$\\times$ speedup over existing frameworks. Extensive experiments demonstrate its effectiveness in overthinking mitigation, hallucination reduction, and other key applications. EasySteer transforms steering from research technique to production-ready capability, establishing critical infrastructure for deployable, controllable language models.","url":"https://huggingface.co/papers/2509.25175","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.468Z","metadata":{"authors":["Haolei Xu","Xinyu Mei","Yuchen Yan","Rui Zhou","Wenqi Zhang","Weiming Lu","Yueting Zhuang","Yongliang Shen"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2509.25175.pdf","arxivUrl":"https://arxiv.org/abs/2509.25175","arxivId":"2509.25175","coolPaperUrl":"https://papers.cool/arxiv/2509.25175","published":"2025-09-29T17:59:07Z","updated":"2025-09-29T17:59:07.000Z","zh_summary":"大型语言模型（LLM）引导已成为通过有针对性地操控隐藏状态在推理过程中控制模型行为的一种有前景的范式，提供了一种比昂贵的再训练更轻量的替代方案。然而，现有的引导框架存在关键限制：计算效率低、可扩展性有限以及功能受限，这些都阻碍了研究进展和实际部署。我们提出了 EasySteer，这是一种基于 vLLM 构建的高性能、可扩展的统一 LLM 引导框架。我们的系统采用模块化架构，提供可插拔接口，支持基于分析和基于学习的方法，同时实现了细粒度的参数控制，并预计算了针对八个应用领域的引导向量，还提供了交互式演示系统。通过与 vLLM 优化推理引擎的深度集成，EasySteer 实现了比现有框架快 5.5-11.4 倍的速度提升。大量实验证明，它在防止过度思考、减少幻觉及其他关键应用中均表现出色。EasySteer 将引导技术从研究手段转变为可投入生产的能力，为可部署、可控的语言模型建立了关键基础设施。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有大语言模型（LLM）推理时行为控制框架的三大核心瓶颈：\n\n1. **计算低效**：现有框架普遍基于 Transformers 库，缺乏批式推理支持，导致推理延迟高、吞吐低，难以在生产环境部署。\n2. **功能受限**：多数框架仅支持整层或固定位置干预，无法做到**token 级**、**阶段级**的细粒度控制，也缺乏**多向量协同**与冲突消解机制，难以应对复杂的多目标场景。\n3. **扩展困难**：硬编码的模型钩子与算法耦合严重，研究者插入新的分析式或学习式控制算法时需重写大量工程代码，迭代效率低。\n\n为此，作者提出 **EasySteer**：一套基于 vLLM 的统一、高性能、可插拔 LLM 推理时隐状态操控框架，通过模块化设计、预置向量库与交互式演示系统，把“控制模型行为”从实验室原型升级为**可直接部署的生产能力**。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将与 LLM steering 相关的研究划分为三大范式，并进一步指出 mechanistic interpretability 为 steering 提供理论依据。具体文献与对应贡献如下：\n\n1\\. 模型行为控制范式\n\n| 范式 | 代表文献 | 核心思想 |\n| --- | --- | --- |\n| Prompt Engineering | Sahoo et al., 2024 | 通过精心设计的指令模板或上下文引导生成，无需修改参数。 |\n| Retrieval-Augmented Generation (RAG) | Gao et al., 2023 | 在推理时动态引入外部知识库，提升事实性与可控性。 |\n| Fine-tuning（全参数） | 各类 LLM 预训练论文 | 直接更新权重以适应新行为，但成本高且易灾难性遗忘。 |\n| Parameter-Efficient Fine-Tuning (PEFT) | Hu et al., 2022 (LoRA) | 仅训练低秩适配器，降低显存与计算开销。 |\n| 模型编辑 | Meng et al., 2022a (ROME)Meng et al., 2022b (MEMIT) | 精确定位并更新存储特定事实的 MLP 权重，实现“知识手术”。 |\n\n2\\. Mechanistic Interpretability（为 steering 奠定线性表示假设）\n\n- **Mikolov et al., 2013**\n首次在连续词向量中发现“国王-男+女≈女王”的线性规律，暗示语义方向可向量运算。\n- **Elhage et al., 2022**\n在小型 transformer 中观察到“超级位置”现象，说明不同特征可在同一神经元叠加编码。\n- **Park et al., 2023**\n正式提出 **Linear Representation Hypothesis**：高层概念在隐藏状态空间中呈线性结构，因此可用简单向量加减实现干预。该假设成为后续所有分析式 steering 方法（CAA、PCA、SAE 等）的理论基石。\n\n3\\. 早期 Steering 方法与工具\n\n- **repeng** (Vogel, 2024)\n基于 Transformers 钩子，实现手工构造的拒绝向量注入，但无批式推理。\n- **pyreft** (Wu et al., 2024)\n将 LoReFT 等学习式干预封装为可插拔模块，仍受限于 Transformers 吞吐。\n- **EasyEdit2** (Xu et al., 2025b)\n统一了模型编辑与 steering，却未解决批式推理与多向量协同问题。\n\nEasySteer 在上述工作的基础上，通过**深度集成 vLLM 引擎**与**模块化算法注册机制**，首次把推理延迟降到生产可用水平，并同时支持分析式与学习式、单向量与多向量、层/位置/Token/阶段任意粒度的干预策略。\n\nQ3: 论文如何解决这个问题？\n\n论文通过 **EasySteer** 框架，从**系统架构、算法接口、资源库与交互工具**四个层面一次性解决“计算低效、功能受限、扩展困难”三大痛点。核心手段可概括为：\n\n1\\. 深度嫁接 vLLM 引擎——把吞吐提上去\n\n- **非侵入式模型包装器**\n在模型加载阶段动态识别并替换所有 decoder 层类，**零硬编码**即可支持 LLaMA、Qwen 等任意架构；前向计算仍走 vLLM 的 CUDA kernel，仅在后向返回后插入隐状态干预，**显存布局与调度策略完全复用**，天然享受 page-attention、连续批处理等优化。\n- **批式、长序列友好**\n实验显示，all-layer 干预下 batch-256 长序列（≤2048 tokens）吞吐仍达 **3619 tok/s**，是 pyreft 的 **5.5×**、repeng 的 **11.4×**；单卡 A6000 即可保持基线 **71–84 %** 的原始吞吐。\n\n2\\. 统一算法接口——让扩展变得“像写 Python 函数一样简单”\n\n- **BaseSteerVectorAlgorithm 抽象基类**\n只需实现 `apply(h, ...)` 与 `registry()` 两个方法，新算法即可被框架自动发现；工厂模式按需懒加载，内存零冗余。\n- **装饰器注册 + 冲突消解策略**\n支持“additive / priority / sequential”等多种多向量叠加规则，**同一 token 位置**可同时跑 CAA、LoReFT、自定义算法而互不干扰。\n\n3\\. 细粒度参数控制——把“干预自由度”打开到 token & 阶段级\n\n- **VectorConfig + SteerVectorRequest**\n用户可一次性指定：\n- **Where**——层号列表、token 位置、推理阶段（prefill / decode / step-boundary）\n- **When**——触发 token id、正则模板、上下文状态判断函数\n- **How**——缩放系数、冲突解决策略、动态调度函数\n实现**条件激活、多目标协同、在线调参**而无需重启服务。\n\n4\\. 预置资源库 + 交互演示——把“研究→落地”的摩擦降到 0\n\n- **8 大领域、即用向量**\n安全、推理、知识、事实性、情感、人格、语言、风格，共 **>100 条**已调优向量，附评测指标与行为说明。\n- **Web 演示系统**\n提供“抽取-训练-推理-聊天”四合一界面，支持**边聊边调**向量强度、实时对比 baseline/steered 输出，无需写代码即可验证想法。\n\n5\\. 实验验证——效率与效果同时达标\n\n- **效率**：all-layer 三向量并发，吞吐仍保持基线 **71.5 %**；其他框架因无批式支持或 Python 层钩子过重，吞吐跌至 **10–30 %**。\n- **效果**：\n- **过度思考抑制**（SEAL 向量）在 GSM8K 上准确率 **+2.7 %**，token 消耗 **−40 %**。\n- **幻觉缓解**（PCA 向量）在 TruthfulQA 上多项选择准确率 **+12.12 %**，且 BLEURT 语言流畅度不降反升。\n\n通过上述设计，EasySteer 把 LLM steering 从“研究原型”升级为**生产级能力**：\n\n- **推理延迟** → 与基线同量级；\n- **干预粒度** → token-级、stage-级；\n- **算法扩展** → 注册即插；\n- **应用门槛** → 一条命令加载向量，一行代码启动干预。\n\nQ4: 论文做了哪些实验？\n\n论文从 **“跑得快”** 与 **“控得准”** 两条主线展开系统实验，全部在单张 NVIDIA A6000 (48 GB) 上完成，结果列于正文表 2–5 与附录 C。具体配置与结论如下：\n\n1\\. Steering Efficiency（跑得快）\n\n1.1 实验设置\n\n- **模型**：DeepSeek-R1-Distill-Qwen-1.5B\n- **数据**：MATH 数据集，序列长度分 ≤128 与 ≤2048 两档\n- **推理模式**：single-input vs. batch-256\n- **干预配置**：\n① 单层干预\n② 28 层全干预\n③ 三向量并发 + 全层干预\n- **对照框架**：repeng、pyreft、EasyEdit2（均用全层零向量保证输出长度一致）\n\n1.2 关键结果（表 2）\n\n| 指标 | 基线 vLLM | EasySteer 全层 | EasySteer 三向量 | pyreft | repeng | EasyEdit2 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 长序列 batch TPS ↑ | 4308 tok/s | 3619 tok/s (–16 %) | 3081 tok/s (–29 %) | 652 tok/s | 316 tok/s | 不支持 batch |\n| 加速比 vs. 最快对照 | – | 5.5× | 11.4× | 1× | 0.5× | – |\n\n- **First Token Latency (FTL)** 仅增加 13 ms（48 ms vs. 35 ms）。\n- **三向量并发** 仍保持 **71.5 %** 基线吞吐，其余框架跌破 **30 %**。\n\n2\\. Steering Effectiveness（控得准）\n\n2.1 幻觉抑制实验（表 4）\n\n- **数据集**：TruthfulQA（MC + QA 双任务，两轮交叉验证）\n- **模型**：Qwen2.5-1.5B-Instruct & Llama-3.1-8B-Instruct\n- **向量来源**：\n- 分析式：CAA、PCA、Linear Probe\n- 学习式：SAV、LoReFT\n\n| 模型 | 方法 | MC Acc ↑ | QA Acc ↑ | BLEURT ↑ | Fluency ↑ |\n| --- | --- | --- | --- | --- | --- |\n| Llama-3.1-8B | 基线 | 50.55 % | 43.45 % | 55.81 | 5.43 |\n| PCA | 62.67 % (+12.12) | 45.29 % | 57.28 | 6.58 |\n| Qwen2.5-1.5B | 基线 | 55.08 % | 27.17 % | 38.19 | 3.90 |\n| LoReFT | 56.43 % | 33.41 % (+6.24) | 47.25 | 3.13 |\n\n- **分析式方法** 在提升准确率同时保持或提高流畅度；\n- **学习式方法** 准确率增益最大，但需权衡 fluency 分数。\n\n2.2 过度思考抑制实验（表 5 + 附录 C）\n\n- **任务**：GSM8K & MATH500\n- **模型**：DeepSeek-R1-Distill-Qwen-1.5B / 7B\n- **方法**：SEAL 三向量（Execution↑, Reflection↓, Transition↓），在推理步边界 token“nn”处触发。\n\n| 模型 | 数据集 | 基线 Acc / 平均 tokens | SEAL Acc / 平均 tokens | 变化 |\n| --- | --- | --- | --- | --- |\n| 1.5B | GSM8K | 79.6 % / 2435 tok | 82.3 % / 1460 tok | +2.7 % –40.0 % |\n| 7B | MATH500 | 86.6 % / 3097 tok | 88.2 % / 2577 tok | +1.6 % –16.8 % |\n\n- **token 节省** 主要来自压缩冗余反思段落，**准确率不降反升**。\n\n2.3 定性演示（表 3）\n\n在 **8 大应用场景**（安全、知识、事实、情感、人格、语言、风格、推理）各给 1 例，展示单条向量即可把模型输出从“基线”翻转为“目标行为”，例如：\n\n- 安全：生成色情故事 → 拒绝回答\n- 知识：伦敦是英国首都 → 巴黎是英国首都（可控注入错误）\n- 情感：哀悼文字 → 病理级快乐安慰\n\n3\\. 小结\n\n- **效率**：首次在 vLLM 上实现 **batch-256 + 全层多向量** 干预，吞吐 **>3000 tok/s**，领先现有框架一个数量级。\n- **效果**：\n- 幻觉任务 **MC 准确率最高 +12.12 %**\n- 过度思考任务 **token 消耗 –40 % 且准确率 +2.7 %**\n- 八领域定性实验均实现**单向量即时行为翻转**。\n\n实验覆盖 **延迟-吞吐-准确率-流畅度- token 长度** 全维度，验证了 EasySteer 既“跑得快”又“控得准”。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在大模型推理-时 steering 领域继续深入，均基于 EasySteer 已暴露的假设、限制或尚未验证的场景提出：\n\n1\\. 算法与理论\n\n- **非线性干预机制**\n当前主流方法依赖 Linear Representation Hypothesis，仅做“向量加法”。可探索\n– 轻量级 MLP 或 Hyper-network 形式的  f_θ(h) ，在保持参数高效的同时引入非线性。\n– 基于流形投影的“曲率感知”干预，避免高维空间线性外推失效。\n\n- **动态强度调度**\n现有  α  多为静态标量。可引入\n– 基于模型不确定性或置信度  p(t|x_(<t))  的自适应  α_t 。\n– 强化学习训练“调度器”，在生成过程中实时输出逐 token 强度。\n\n- **多目标冲突可微化解**\n当 N 个向量在同一位置激活时，EasySteer 仅支持手工规则。可探索\n– 将冲突区域建模为可微分凸组合，通过辅助损失端到端优化组合系数。\n\n2\\. 结构与效率\n\n- **CPU-GPU 异构流水线**\n分析式向量计算可在 CPU 异步完成，GPU 仅做加法；进一步隐藏延迟。\n– 考察 >200 层大模型（如 70B）下是否仍保持低 overhead。\n\n- **Steering Kernel Fusion**\n把“向量加法”写入 vLLM CUDA kernel，与 attention 计算融合，减少一次显存写回。\n– 目标：把三向量并发吞吐损失从 28 % 压缩到 <10 %。\n\n- **量化向量**\n将 32-bit 向量压缩至 8-bit 或 4-bit，研究对控制强度与方向的精度影响，实现内存-带宽双降。\n\n3\\. 评估与可靠性\n\n- **细粒度行为一致性**\n当前仅用 Acc/BLEURT 等粗指标。可建立\n– **干预成功率（ISR）**：同一 prompt 多次采样，测量目标行为出现概率的期望与方差。\n– **干预可重复性**：换随机种子后向量仍否生效，检验是否过拟合训练样本。\n\n- **副作用谱系（Side-effect Atlas）**\n系统扫描 1 k 条向量在 20 + 下游任务上的性能漂移，构建“干预-副作用”热力图，指导安全部署。\n\n- **对抗鲁棒性**\n研究经过拒绝抑制的模型是否更易被越狱提示绕过；或反向构造“反转向量”以评估防御深度。\n\n4\\. 场景扩展\n\n- **多模态 steering**\n将向量干预从文本隐状态扩展到 VL 模型的视觉编码器，实现“看图说话风格”或“视觉幻觉抑制”。\n\n- **长上下文与 RAG 联合**\n在 128 k 上下文或检索片段中，仅对特定段落激活 steering，避免全局风格漂移。\n\n- **工具调用 / 代码生成**\n针对函数调用、SQL 生成等结构化输出，设计语法感知触发条件，确保干预不破坏可执行性。\n\n5\\. 自动化与平台化\n\n- **Auto-Steer**\n给定行为描述，用 LLM 自动生成正负样本 → 提取向量 → 评测 → 迭代，实现“一句话”自动获得 steering 向量。\n\n- **联邦式向量市场**\n建立开放协议，允许用户上传、加密签名自己的 steering 向量；下游节点可验证哈希后加载，促进社区协作同时保护知识产权。\n\n6\\. 伦理与治理\n\n- **Steering 检测器**\n训练二分类器判断输出是否经过特定向量干预，用于平台审核或内容标识。\n\n- **可撤销干预**\n研究“反转向量”或在线遗忘算法，使模型在部署后可快速撤销已注入的行为，降低误用风险。\n\n这些方向兼顾**理论深度**（非线性、动态调度）、**系统挑战**（kernel 融合、量化）、**评估科学性**（副作用谱系、鲁棒性）与**社会责任**（检测器、可撤销），可在 EasySteer 的模块化架构上快速原型化。\n\nQ6: 总结一下论文的主要内容\n\nEasySteer：面向大语言模型（LLM）推理期隐状态操控的统一、高性能、可扩展框架\n\n1\\. 问题背景\n\n- 微调代价高，提示工程控制浅；LLM steering 通过“推理期向量干预”成为轻量级替代方案。\n- 现有框架（repeng/pyreft/EasyEdit2）存在三大痛点：\n1. 计算低效：无批式推理，吞吐骤降。\n2. 功能受限：仅层/位置级干预，缺 token-级、多向量协同。\n3. 扩展困难：硬编码钩子，新算法集成工程量大。\n\n2\\. 核心贡献\n\n| 模块 | 关键技术与价值 |\n| --- | --- |\n| Steering Vector Generation | 统一分析式（CAA/PCA/SAE）与学习式（LoReFT/LM-Steer）向量生产管线。 |\n| Steering Vector Application | 深度嫁接 vLLM：– 非侵入包装器：零硬编码支持任意 decoder 架构；– 可插拔算法接口：装饰器注册+工厂懒加载；– 细粒度参数控制：层/位置/token/阶段+多向量冲突消解。 |\n| Resource Library | 8 大领域（安全、推理、知识、事实、情感、人格、语言、风格）预置 100+ 已评测向量，附实现脚本与行为说明。 |\n| Interactive Demo | Web 可视化“抽取-训练-推理-聊天”四合一，支持边聊边调参、双语输出、baseline/steered 并排对比。 |\n\n3\\. 实验结果\n\n- **效率**（A6000 48 GB，batch-256，≤2048 tokens）\n– 全层干预：3619 tok/s（基线 4308，–16 %）；\n– 三向量并发：3081 tok/s（–29 %），仍领先 pyreft **5.5×**、repeng **11.4×**。\n\n- **效果**\n– **幻觉抑制**（TruthfulQA）：PCA 向量使 Llama-3.1-8B MC 准确率 **+12.12 %**；LoReFT 使 Qwen2.5-1.5B QA 准确率 **+6.24 %**。\n– **过度思考抑制**（GSM8K）：SEAL 三向量让 DeepSeek-R1-Distill-1.5B 准确率 **+2.7 %**，token 消耗 **–40 %**。\n– **八领域定性**：单条向量即可实现拒绝、事实改写、情感翻转、风格迁移等精准行为控制。\n\n4\\. 结论\n\nEasySteer 通过“vLLM 深度集成 + 模块化算法接口 + 细粒度参数控制 + 预置资源库”，首次把 LLM steering 从研究原型升级为**生产级能力**：\n\n- 推理延迟与基线同量级，吞吐领先现有框架一个数量级；\n- 支持分析/学习、单向量/多向量、层/位置/token/阶段任意粒度干预；\n- 提供开箱即用向量与可视化demo，大幅降低研究与部署门槛。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是EasySteer%3A%20A%20Unified%20Framework%20for%20High-Performance%20and%20Extensible%20LLM%20Steering，链接是https://arxiv.org/pdf/2509.25175，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25175。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1759276017468","title":"Sequential Diffusion Language Models","description":"Diffusion language models (DLMs) have strong theoretical efficiency but are limited by fixed-length decoding and incompatibility with key-value (KV) caches. Block diffusion mitigates these issues, yet still enforces a fixed block size and requires expensive training. We introduce Next Sequence Prediction (NSP), which unifies next-token and next-block prediction, enabling the model to adaptively determine the generation length at each step. When the length is fixed to 1, NSP reduces to standard next-token prediction. Building on NSP, we propose Sequential Diffusion Language Model (SDLM), which can retrofit pre-trained autoregressive language models (ALMs) at minimal cost. Specifically, SDLM performs diffusion inference within fixed-size mask blocks, but dynamically decodes consecutive subsequences based on model confidence, thereby preserving KV-cache compatibility and improving robustness to varying uncertainty and semantics across the sequence. Experiments show that SDLM matches or surpasses strong autoregressive baselines using only 3.5M training samples, while achieving 2.1 higher throughput than Qwen-2.5. Notably, the SDLM-32B model delivers even more pronounced efficiency gains, demonstrating the strong scalability potential of our modeling paradigm. Project page and codes: https://github.com/OpenGVLab/SDLM","url":"https://huggingface.co/papers/2509.24007","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.468Z","metadata":{"authors":["Yangzhou Liu","Yue Cao","Hao Li","Gen Luo","Zhe Chen","Weiyun Wang","Xiaobo Liang","Biqing Qi","Lijun Wu","Changyao Tian","Yanting Zhang","Yuqiang Li","Tong Lu","Yu Qiao","Jifeng Dai","Wenhai Wang"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.CL","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.24007.pdf","arxivUrl":"https://arxiv.org/abs/2509.24007","arxivId":"2509.24007","coolPaperUrl":"https://papers.cool/arxiv/2509.24007","published":"2025-09-28T17:59:15Z","updated":"2025-09-28T17:59:15.000Z","zh_summary":"扩散语言模型（DLMs）具有较强的理论效率，但受限于固定长度解码，并且与键值（KV）缓存不兼容。块扩散（Block diffusion）缓解了这些问题，但仍然要求固定的块大小，并且需要昂贵的训练。我们提出了下一序列预测（NSP），它统一了下一个 token 和下一个块的预测，使模型能够在每一步自适应地确定生成长度。当长度固定为 1 时，NSP 就退化为标准的下一个 token 预测。在 NSP 的基础上，我们提出了序列扩散语言模型（SDLM），可以以最低成本改造预训练的自回归语言模型（ALMs）。具体而言，SDLM 在固定大小的掩码块内执行扩散推理，但根据模型的置信度动态解码连续子序列，从而保持 KV 缓存的兼容性，并提高对序列中不确定性和语义变化的鲁棒性。实验表明，SDLM 使用仅 350 万训练样本即可匹配或超越强大的自回归基线，同时实现比 Qwen-2.5 高 2.1 倍的吞吐量。值得注意的是，SDLM-32B 模型带来了更显著的效率提升，展示了我们建模范式的强大可扩展性。项目页面和代码：https://github.com/OpenGVLab/SDLM","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心针对扩散语言模型（DLM）在文本生成中的两大痛点：\n\n1. **固定长度解码**\n现有 DLM 必须一次性生成定长 token 块，无法根据局部语义或模型置信度动态调整，导致在需要上下文依赖的位置容易出错。\n\n2. **与 KV-Cache 不兼容**\n并行去噪机制破坏了自回归模型依赖的键值缓存结构，使推理无法复用历史计算，抵消了理论加速优势。\n\n为此，作者提出 **Next Sequence Prediction（NSP）** 框架，将“下一个 token”推广为“下一段可变长子序列”，并基于 NSP 构建 **Sequential Diffusion Language Model（SDLM）**。SDLM 在固定大小的掩码块内部做扩散推理，但通过置信度或自验证机制动态决定实际输出的连续子序列长度，从而：\n\n- 保留 KV-Cache 复用能力\n- 允许步长随局部难度自适应变化\n- 以极低代价把预训练自回归模型（ALM） retrofit 为扩散范式，仅用 3.5 M 样本即可在 3 B/32 B 规模上取得与强 ALM 相当或更优的效果，并实现约 2× 的推理提速。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均围绕“如何突破自回归逐 token 生成的低并行度”展开：\n\n1. 自回归加速：KV-Cache 与多 token 预测\n\n- KV-Cache：GPT 系列、LLaMA、Qwen 等标准实现，通过缓存键值避免重复计算。\n- Multi-token Prediction (MTP)：Medusa、DeepSeek-V3 等在单次前向中并行输出 D 个独立 head，再借助投机解码验证，实现 2–3× 提速。\n2. 扩散语言模型：离散去噪与块扩散\n\n- Masked Discrete Diffusion：LLaDA、Dream 等在离散文本上执行掩码-去噪，理论并行度高，但需从头训练且整段定长生成，无法复用 KV-Cache。\n- Block Diffusion：将序列划分为固定长度块，块内并行去噪、块间自回归，部分缓解长度僵化，但仍需昂贵训练且块大小不可变。\n3. 投机/验证式并行解码\n\n- Speculative Decoding（Blockwise Parallel Decoding）：利用小模型或同模型并行提案，再用大模型一次验证，典型工作如 Google 的 Blockwise、STEAM 等。\n- Self-speculative：SDLM 推理阶段采用的“自提案-自验证”即属此类，无需额外模型即可在单模型内完成。\n\nSDLM 与上述工作的关键差异在于：\n\n- 通过 NSP 把“token→block”进一步泛化为“token→可变长子序列”，首次在扩散框架内实现步长自适应；\n- 采用并行块训练+双向块内注意力，可直接微调预训练 ALM，无需从头训练；\n- 保留 KV-Cache 结构，使扩散模型首次在推理阶段与自回归一样复用历史状态。\n\nQ3: 论文如何解决这个问题？\n\n论文提出“Next Sequence Prediction（NSP）”统一视角，将“下一个 token”扩展为“下一段长度可变的连续子序列”，并据此设计 **Sequential Diffusion Language Model（SDLM）**。核心解决路径分为训练与推理两阶段，均围绕“固定块内并行扩散、块外动态决策”展开：\n\n训练阶段：并行块微调（Parallel Block Training）\n\n1. **掩码构造**\n在预训练 ALM 的因果前缀后，拼接一段长度为 D 的“噪声块”\n\nX_i^T=[x_(i-1), [m],…,[m]_(D-1)]\n\n模型任务：基于前缀  x_(<i)  与噪声块，还原目标子序列  X_i=x_(i:i+D) 。\n\n2. **注意力掩码**\n自定义稀疏掩码  A∈0,1^((i+D)×(i+D)) \n\nA_(uv)= 1_(vle u) oplus 1_(uge i land vge i)\n\n- 前缀部分保持因果（左上）\n- 块内允许双向（右下）\n使模型在并行训练时既遵守全局因果，又能利用未来上下文提升局部置信度。\n3. **移位预测目标**\n借鉴 Dream 的 shifted-prediction：让模型预测  x_(i+1)  而非  x_i ，充分利用 ALM 对“首 token”强先验，减少重复与漂移。\n\n4. **低成本复用**\n当 D=1 时，目标退化为标准下一 token 损失，可直接继承 ALM 权重；只需在 3.5 M 样本上继续微调即可收敛。\n\n推理阶段：最长前缀解码（Longest Prefix Decoding）\n\n1. **固定块并行前向**\n每步输入历史 KV-Cache 与 D-1 个掩码 token，一次性得到 D 个位置的对数几率  Z_i∈R^(D×|V|) 。\n\n2. **动态长度函数 γ(·)**\n两种实现：\n\n- **Confidence-based（贪婪）**\n\nγ_τ(Z_i)=maxj!∈![1,D]m|prod_(k=1)^j p(z_k^i)geτr∪1\n\n其中  p(z_k^i)  可为 softmax 概率或熵归一化置信度。\n- **Self-speculative（自验证）**\n先并行采样 D 个候选  hat y^i_(1:D) ，再构造 D 条“前缀+掩码”验证输入，二次前向得到  tilde y^i_(1:D) ；\n\nγ_(verify)(Z_i)=maxj!∈![1,D]m|hat y^i_k=tilde y^i_k,;∀ kle jr∪1\n\n无需额外模型，即可实现更可靠的接受长度。\n3. **KV-Cache 更新**\n只保留被接受的  γ  个 token 及其对应 K/V，下一步从新区块继续，保证内存与计算随实际输出线性增长，与自回归完全一致。\n\n效果\n\n- **长度自适应**：在语义流畅或公式化片段（如代码块、数学表达式）γ 自动增大，实现 2–5 token/步；在模糊或分叉处 γ→1，降低错误累积。\n- **性能持平**：SDLM-32B 在 GSM8K、MATH-500、IFEval 等 13 项基准上与 Qwen-2.5-32B-SFT 差距 <1%，但平均提速 2.1×。\n- **训练代价极低**：仅需 3.5 M 样本、1 epoch、常数学习率 5×10⁻⁶，即可把现成 ALM 转成扩散范式，避免从头训练十亿级参数的开销。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“性能是否持平、速度是否提升、规模是否可扩”三个问题展开，覆盖 13 个主流 benchmark，并在 3B/32B 两档参数规模上系统消融。主要结果如下：\n\n1 主实验：13 任务端到端对比\n\n**基准设置**\n\n- 训练数据：3.5 M 开源指令样本（≈2.3 B token），单 epoch，恒定 lr=5×10⁻⁶。\n- 对比对象：\n– ALM：Qwen-2.5-3B/32B-SFT（同数据微调）\n– DLM：LLaDA-8B-Instruct、Dream-7B-Instruct\n- 推理配置：块大小 D=4，置信阈值 τ∈{0.98,0.82}，贪婪与自验证两种解码。\n\n| 任务类别 | 具体基准 | 3B 结果 | 32B 结果 |\n| --- | --- | --- | --- |\n| 数学 | GSM8K / MATH-500 / GPQA | 84.6 / 60.8 / 28.3 | 92.4 / 74.2 / 36.4 |\n| 代码 | HumanEval / HumanEval+ / MBPP / MBPP+ | 67.1 / 59.8 / 65.4 / 40.5 | 81.1 / 73.8 / 80.9 / 58.2 |\n| 通用 | MMLU / Winogrande / Hellaswag / ARC-C/E / IFEval | 66.3 / 60.2 / 74.2 / 82.7 / 92.0 / 57.1 | 82.8 / 79.2 / 92.0 / 94.9 / 98.9 / 78.6 |\n\n- **精度**：SDLM-3B 平均 60.1，与 Qwen-2.5-3B-SFT 持平；SDLM-32B 平均 71.9，仅比 Qwen-2.5-32B-SFT 低 0.4 pt，但显著高于 LLaDA-8B（≈+10 pt）与 Dream-7B（≈+15 pt）。\n- **速度**：τ=0.98 时 1.9–2.2 token/步；τ=0.82 时 2.4–2.7 token/步，相对 ALM 提速 1.9×–2.5×。\n\n2 速度-精度权衡曲线\n\n固定 D=4 与 D=8，τ 从 0.7 到 0.99 扫描：\n\n- GSM8K：τ↓0.82 仍维持 84.5（−0.1 pt），步均 token 从 2.15→2.75。\n- MATH-500：token/步 2.9 时精度仅掉 1.4 pt。\n- HumanEval+：代码任务更敏感，token/步 >2 后性能开始下降，提供直观“拐点”参考。\n\n3 块大小与自验证消融\n\n| 配置 | GSM8K | MATH | HumanEval+ | 平均 token/步 |\n| --- | --- | --- | --- | --- |\n| D=4, τ=0.98 | 84.6 | 60.8 | 59.8 | 1.91 |\n| D=4, Self-spec | 85.1 | 61.2 | 58.4 | 3.42 |\n| D=8, τ=0.98 | 83.3 | 58.4 | 59.2 | 2.18 |\n| D=8, Self-spec | 83.6 | 60.2 | 57.3 | 5.41 |\n\n- 块增大到 8，几乎不丢分，吞吐再提升 25 %；\n- 自验证方式平均接受 3.4–5.4 token/步，显著高于贪婪阈值，但需额外一次前向。\n\n4 关键组件消融\n\n- **No-Shift**：直接预测掩码位置本身，HumanEval+ 掉 14 pt，重复输出增多。\n- **Causal-Attention**：块内改为单向，平均 token/步从 1.88→1.82，验证双向注意力对并行度贡献。\n\n5 可视化案例\n\n图 5 给出数学解题与 Python 代码生成两条轨迹：蓝色块长度随局部置信度变化，公式/关键字一次性输出 4–5 token，模糊处仅 1 token，展示自适应行为。\n\n结论\n\n实验表明：\n\n1. 仅用 3.5 M 样本即可让 SDLM 在 3B/32B 规模达到与同规模 SFT-ALM 几乎一致的精度；\n2. 通过调节 τ 或 D，可在 1.3×–3.5× 区间连续调节速度，且数学类任务对加速更鲁棒；\n3. 块大小、置信函数、自验证等模块均能有效提升吞吐，而双向注意力与移位预测是保持质量的关键设计。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“模型侧”“数据侧”“系统侧”与“理论侧”四大类，均直接对应 SDLM 当前遗留的开放问题。\n\n模型侧\n\n1. **更大规模与更深块**\n\n- 70 B+ 参数、D≥16 的块内双向注意力是否仍能保持稳定？\n- 随着模型加深，块内梯度冲突与表示坍缩的缓解策略（如 RMS-Norm 位置、Layer-scale）。\n2. **自适应块大小训练**\n目前训练仍用固定 D，推理才通过 γ(·) 动态截断。可探索：\n\n- 强化学习或可微分束搜索，在训练阶段直接优化期望步长分布；\n- 分段损失加权，让模型对不同子序列长度形成显式偏好。\n3. **多模态扩展**\n将 NSP 推广到文本-图像、文本-语音交错序列：\n\n- 统一离散/连续扩散空间；\n- 设计跨模态置信度（如图像 patch 熵与文本 token 熵联合决策）。\n\n数据侧\n\n4. **课程式掩码调度**\n当前随机切块导致难度分布方差大。可借鉴计算机视觉中的“噪声调度”：\n\n- 前期低噪声、短块，后期高噪声、长块，逐步提升模型外推距离。\n5. **领域特定先验注入**\n数学证明、代码语法等对“子序列边界”有强约束。\n\n- 利用抽象语法树或定理证明步骤自动标注“不可切分”边界，作为显式监督信号指导 γ(·)。\n\n系统侧\n\n6. **投机-验证流水线并行**\n自验证需二次前向，可：\n\n- 把提案与验证阶段拆分到不同设备，流水线重叠；\n- 用早期退出（early-exit）小模型做快速筛选，减少完整验证调用。\n7. **KV-Cache 压缩与长度外推**\n块内双向注意力导致 cache 形状从三角变为矩形，内存占用 ∝ D²。\n\n- 探究低秩分解、滑动窗口或哈希缓存，是否能在 128 k 长文场景下仍保持 2× 提速。\n8. **硬件协同优化**\n\n- 针对“同批次内 γ 不同”造成的动态形状，设计 GPU warp 专用核：把短序列打包到同一 SIMD 指令，提高 SM 利用率；\n- 将置信度计算（熵） offload 到 CPU 或 DMA，实现 logits 生产与决策并行。\n\n理论侧\n\n9. **NSP 的收敛保证**\n当前仅经验验证。可建立：\n\n- 变分下界框架，证明在任意自适应步长策略下，NSP 损失对数似然单调提升；\n- 与最优传输理论联系，解释“置信度乘积 ≥τ”对应 Wasserstein 球内的分布鲁棒性。\n10. **长度先验与熵阈值最优性**\n固定 τ 对所有任务相同，是否可给出：\n\n- 基于序列熵率的上界，推导任务相关最优 τ\\*(task)；\n- 在线贝叶斯更新，让 τ 在生成过程中随上下文漂移自动调整。\n\n小结\n\nSDLM 把“固定块扩散”升级为“可变长子序列扩散”，但仍留下大规模稳定性、训练-推理联合优化、长文本内存、理论保证等空白。上述 10 点均可直接作为下一步工作切入点，其中 1、6、9 兼具学术与工程价值，值得优先投入。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Next Sequence Prediction（NSP）** 统一框架，将“下一个 token”扩展为“下一段长度可变的连续子序列”，并基于 NSP 构建 **Sequential Diffusion Language Model（SDLM）**。核心贡献与结果如下：\n\n1. **问题定位**\n\n- 扩散语言模型（DLM）虽可并行生成，但受限于**固定解码长度**且**无法复用 KV-Cache**。\n- 块扩散缓解部分问题，仍受**固定块大小**与**昂贵训练**困扰。\n2. **NSP 框架**\n\n- 每步在固定掩码块内做扩散推理，通过置信度或自验证动态决定实际输出长度 γ∈\n1,D\n。\n- γ=1 时退化为标准下一 token 预测，可直接继承预训练 ALM 权重，实现低成本 retrofit。\n3. **SDLN 实现**\n\n- **训练**：并行块微调，自定义稀疏注意力（前缀因果+块内双向），移位预测目标，3.5 M 样本单 epoch 收敛。\n- **推理**：最长前缀解码，支持阈值式（累积置信≥τ）或自投机式（两次前向一致性检验）两种 γ 策略，全程复用 KV-Cache。\n4. **实验结果**\n\n- **精度**：SDLN-3B/32B 在 13 项基准（数学、代码、通用）上与同规模 Qwen-2.5-SFT 差距＜1 pt，显著优于更大 DLM（LLaDA-8B、Dream-7B）。\n- **速度**：τ=0.98 时 1.9–2.2 token/步；τ=0.82 时 2.4–2.7 token/步，相对 ALM 提速 1.9×–2.5×；D=8 自验证可达 5.4 token/步。\n- **消融**：双向注意力与移位预测是保持质量关键；块大小增大到 8 仅掉 1.2 pt，吞吐再提升 25 %。\n5. **结论**\nSDLN 首次在扩散框架内实现**长度自适应**与**KV-Cache 兼容**，用极少训练数据即可让预训练 ALM 获得“近 SFT 精度 + 明显加速”，为大规模高效语言生成提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Sequential%20Diffusion%20Language%20Models，链接是https://arxiv.org/pdf/2509.24007，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24007。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1759276017468","title":"Towards Personalized Deep Research: Benchmarks and Evaluations","description":"Deep Research Agents (DRAs) can autonomously conduct complex investigations and generate comprehensive reports, demonstrating strong real-world potential. However, existing evaluations mostly rely on close-ended benchmarks, while open-ended deep research benchmarks remain scarce and typically neglect personalized scenarios. To bridge this gap, we introduce Personalized Deep Research Bench, the first benchmark for evaluating personalization in DRAs. It pairs 50 diverse research tasks across 10 domains with 25 authentic user profiles that combine structured persona attributes with dynamic real-world contexts, yielding 250 realistic user-task queries. To assess system performance, we propose the PQR Evaluation Framework, which jointly measures (P) Personalization Alignment, (Q) Content Quality, and (R) Factual Reliability. Our experiments on a range of systems highlight current capabilities and limitations in handling personalized deep research. This work establishes a rigorous foundation for developing and evaluating the next generation of truly personalized AI research assistants.","url":"https://huggingface.co/papers/2509.25106","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.468Z","metadata":{"authors":["Yuan Liang","Jiaxian Li","Yuqing Wang","Piaohong Wang","Motong Tian","Pai Liu","Shuofei Qiao","Runnan Fang","He Zhu","Ge Zhang","Minghao Liu","Yuchen Eleanor Jiang","Ningyu Zhang","Wangchunshu Zhou"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.IR"],"pdfUrl":"https://arxiv.org/pdf/2509.25106.pdf","arxivUrl":"https://arxiv.org/abs/2509.25106","arxivId":"2509.25106","coolPaperUrl":"https://papers.cool/arxiv/2509.25106","published":"2025-09-29T17:39:17Z","updated":"2025-09-29T17:39:17.000Z","zh_summary":"深度研究代理（DRAs）能够自主开展复杂的调查并生成综合报告，展示出强大的现实应用潜力。然而，现有的评估大多依赖封闭式基准，而开放式深度研究基准仍然稀缺，且通常忽视个性化场景。为填补这一空白，我们引入了个性化深度研究基准，这是首个用于评估DRAs个性化能力的基准。它将来自10个领域的50个多样化研究任务与25个真实用户档案相匹配，这些档案结合了结构化人物属性与动态的现实世界情境，从而产生了250个真实的用户任务查询。为了评估系统性能，我们提出了PQR评估框架，该框架综合衡量（P）个性化对齐、（Q）内容质量及（R）真实性。我们对一系列系统的实验结果突出了当前在处理个性化深度研究方面的能力与局限性。本研究为开发和评估下一代真正个性化的AI研究助手奠定了严谨的基础。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该工作旨在填补“个性化深度研究”评测的空白。现有对 Deep Research Agents（DRA）的评估聚焦在通用事实准确性与报告完整性，而忽视了用户画像驱动的个性化需求；与此同时，既有的个性化基准仅覆盖对话或推荐等浅层任务，无法衡量复杂研究场景下的个性化能力。论文首次系统提出：\n\n1. 个性化深度研究任务形式化：要求智能体在检索、推理、撰写阶段均主动适配用户画像。\n2. Personalized Deep Research Bench：50 项跨 10 域的真实研究任务 × 25 份真实用户画像 → 250 条个性化查询，用于量化评测。\n3. PQR 三维评估框架：\n- P（Personalization Alignment）衡量输出与用户目标的契合度；\n- Q（Content Quality）衡量报告本身的深度、逻辑与可读性；\n- R（Factual Reliability）衡量事实准确度与引用覆盖率。\n\n通过该基准与框架，论文揭示了当前开源与商业系统在处理个性化深度研究时的能力差异与瓶颈，为构建“真正以用户为中心”的 AI 研究助手奠定评测基础。\n\nQ2: 有哪些相关研究？\n\n与本工作直接相关的研究可分为两条主线：深度研究（Deep Research）评测 与 个性化（Personalization）评测。以下按类别梳理代表性文献，并指出其与本工作的关联与差异。\n\n1\\. 深度研究能力评测\n\n| 基准 / 系统 | 核心贡献 | 与本文关系 |\n| --- | --- | --- |\n| GAIA (Mialon et al., 2023) | 覆盖推理、工具调用、多模态的封闭式问答套件 | 仅测“能否答对”，无个性化维度 |\n| BrowseComp (Wei et al., 2025) | 基于网页浏览的 500+ 现实任务，强调信息检索与操作 | 任务通用，未引入用户画像 |\n| HLE (Phan et al., 2025) | 跨学科博士级难题，考察知识深度 | 关注事实准确性，无用户定制需求 |\n| X-Bench (Chen et al., 2025a) | 专业领域对齐的 Agent 生产力评测 | 任务集合丰富，但为“一刀切”标准 |\n| DeepResearch Bench (Du et al., 2025) | 100 项博士级课题 + RACE/FACT 报告质量指标 | 首次系统评估开放研究，但仍无个性化 |\n| ResearcherBench (Xu et al., 2025) | 65 个前沿 AI 问题，双轨事实评分 | 聚焦前沿科学，忽略用户差异 |\n| DeepResearchGym (Coelho et al., 2025) | 开源沙盒 + 可复现搜索 API | 提供廉价可复现环境，任务通用 |\n| Mind2Web2 (Gou et al., 2025) | 130 真实网页任务，Agent-as-a-Judge 自动评估 | 强调在线交互，未建模用户偏好 |\n\n**共性局限**：以上基准均以“任务为中心”，评估指标集中在检索正确性、事实准确率、报告完整性，**未将用户画像纳入输入或评分**。\n\n2\\. 个性化评测\n\n| 基准 / 系统 | 核心贡献 | 与本文关系 |\n| --- | --- | --- |\n| LaMP (Salemi et al., 2024) | 7 项分类/生成任务，验证 LLM 输出个性化能力 | 任务浅层（新闻标签、产品评分），非深度研究 |\n| PersonaGym (Samuel et al., 2025) | PersonaScore 量化 Agent 对给定人设的遵循度 | 聚焦角色扮演对话，不涉多步检索与报告 |\n| PersonaMem (Jiang et al., 2025) | 动态演化用户画像，测试 LLM 适应性 | 仅对话场景，无长链条研究任务 |\n| PersonaFeedback (Tao et al., 2025) | 大规模人工标注偏好，评估回复贴合度 | 侧重单轮对话主观偏好 |\n| PersonaLens (Zhao et al., 2025) | 引入 LLM-based 用户 & 裁判代理，对话级成功率 | 任务形式为闲聊与推荐，复杂度低 |\n| AI Persona (Wang et al., 2024) | 终身个性化框架，用 LLM-as-a-Judge 评估 | 关注持续学习，未定义研究型任务 |\n\n**共性局限**：局限于单轮或短轮次交互（对话、推荐、分类），**未覆盖多跳检索、工具调用、长报告生成等深度研究要素**。\n\n3\\. 工业界深度研究系统\n\n| 系统 | 公开信息 | 与本文关系 |\n| --- | --- | --- |\n| OpenAI Deep Research (2025b) | 端到端浏览-写作，官方博客披露评测结果 | 未公开个性化评测细节 |\n| Gemini Deep Research (Google DeepMind, 2025) | 集成 Gemini 2.5，强调事实核查 | 同样未涉及用户画像适配 |\n| Perplexity Deep Research (2025) | Pro 版提供“深度研究”模式 | 公开指标仅限通用质量 |\n| xAI Grok DeepSearch (xAI Team, 2025) | 集成 Grok-3，主打实时检索 | 未见个性化相关报告 |\n\n工业系统虽具备深度研究能力，但**公开材料仅展示通用性能**，缺乏针对个性化场景的基准或结果。\n\n4\\. 小结\n\n- **深度研究评测** → 任务复杂、多步检索，但“无用户差异”；\n- **个性化评测** → 有用户差异，但“任务浅、无深度研究”；\n\n本文首次将两条主线交叉，提出**个性化深度研究评测体系**，并释放对应基准与框架，以衡量 DRA 在“真实用户+复杂任务”下的画像对齐、内容质量与事实可靠性。\n\nQ3: 论文如何解决这个问题？\n\n论文从“任务定义→基准构建→评估框架→系统验证”四个环节递进式解决“个性化深度研究”缺失评测基础的问题。\n\n1\\. 任务形式化：把“个性化”写进问题陈述\n\n- 将传统深度研究任务三元组\n\n(Task, Tool, Report)\n\n扩展为五元组\n\n(Task, Persona, Context, Tool, Report)\n\n显式引入 **Persona**（静态画像）与 **Context**（动态交互记忆），要求 Agent 在检索、推理、撰写全链路适配用户目标与偏好。\n\n2\\. 基准构建：250 条真实“用户-任务”对\n\n2.1 任务侧\n\n- 覆盖 10 大日常高价值领域（教育、职业、健康、旅行、理财…）。\n- 50 项任务均由对应领域从业者（旅行博主、理财顾问等）设计，再经硕博委员会三轮校验，确保**复杂性↑、清晰度↑、可个性化↑**。\n\n2.2 用户侧\n\n- 25 名真实志愿者按统一 Schema 录入结构化画像（人口、家庭、收支、风险承受度等）。\n- 专业标注员模拟每位志愿者 14 天日常，产出 5 939 条多模态记忆片段（社交帖、购物截图、语音指令…），经 reversibility 检验后形成动态上下文。\n\n2.3 配对策略\n\n- 志愿者先自选相关任务 → 委员会再平衡多样性，最终每任务绑定 5 位不同画像用户，得到 250 条个性化查询，避免随机组合带来的“伪相关”。\n\n3\\. PQR 三维评估框架：把“主观个性化”拆成可量化指标\n\n3.1 P-Score（Personalization Alignment）\n\n- 四维度：Goal / Content / Presentation / Actionability。\n- 动态权重：Meta-evaluator LLM 针对每条〈任务,画像〉输出维度权重\n\nw_d | ∑ w_d=1\n\n再细粒度生成 8–12 条可判据子准则及其权重\n\nw_(c_i) | ∑ w_(c_i)=1\n\n最终\n\nS_P=∑_d w_d ∑_(c_i∈ dim)_d w_(c_i)· s_(c_i),quad s_(c_i)∈[0,10]\n\n3.2 Q-Score（Content Quality）\n\n- 三维度：Depth & Insight / Logical Coherence / Clarity & Readability。\n- 同样采用“任务级动态权重+子准则”两级加权，保证不同研究主题对“深度 vs 可读”侧重可变。\n\n3.3 R-Score（Factual Reliability）\n\n- 自动抽取-去重-验证三步：\n1. 抽取所有可验证声明\n\n(c_i,idx_i,url_i)_(i=1)^N\n\n2. Jina Reader 拉取源文，Judge LLM 打标签\n\nv_i=1[source supports c_i]\n\n3. 计算\n\nFA=(∑ v_i) / (N_(textcited))× 10,quad CC=N_(cited)N_(total)× 10,quad S_R=FA+CC2\n\n3.4 总体得分\n\n- 广义平均\n\nS_(overall)=(λ_P S_P + λ_Q S_Q + λ_R S_R) / (λ_P +λ_Q +λ_R)\n\n默认  λ_P=λ_Q=λ_R=1 ，可随产品阶段调整。\n\n4\\. 系统验证：用基准反向驱动改进\n\n- 评测对象：3 类共 10 个系统 → 商业 DRAs（Gemini-2.5-Pro DR、O3 DR、Perplexity DR）、开源 DRAs（OAgents、DeerFlow、MiroFlow）、搜索增强 LLM（Gemini-2.5-Pro-Search、Claude-3.7-Sonar…）。\n- 关键发现：\n\n- 开源 DRAs 在 P-Score 领先，但 R-Score 普遍拉胯（FA 低至 3.8）。\n- 商业 DRAs 靠高 FA/CC 取得平衡， yet P-Score 仍落后 0.5–1.0 分。\n- 仅给 LLM 外挂搜索≠个性化，GPT-4.1-Search 的 CC 仅 0.1。\n- 信息消融实验：Task Only → Task w/ Context → Task w/ Persona，P-Score 单调上升，显式画像平均再涨 0.3–0.6。\n- 记忆系统提升实验：O-Mem 把上下文提炼成“伪画像”后，Perplexity DR 的 GOAL 从 3.88→4.47，但仍距“直接给画像”有 0.3 差距，验证记忆模块需向“高层推理+抽象”升级。\n\n5\\. 结果意义\n\n- 提供**首个可复现的个性化深度研究评测协议**，研究者可用同一 250 查询+ PQR 脚本快速对比新旧系统。\n- 通过“动态权重+子准则”机制，把主观个性化转成可数值化、可梯度优化的目标，为后续强化学习/偏好对齐提供奖励信号。\n- 实验结论直接指出当前短板：开源 Agent 需补可靠性，商业系统需补画像对齐，搜索+LLM 需补个性化架构——为下一代系统给出改进路线图。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“个性化深度研究”共设计并执行了 4 组互补实验，依次验证基准有效性、系统性能、信息输入影响以及记忆模块的可行性。所有实验均基于 Personalized Deep Research Bench 的 250 条查询，受算力限制按统一抽样规则降至 150/50 子集，保证可重复性。\n\n1\\. 主实验：Task w/ Persona 全景评测\n\n**目的**：在“任务+显式用户画像”这一最强信号下，对比 10 个代表性系统的 P、Q、R 表现。\n\n**设置**\n\n- 系统：3 类 10 款\n– 商业 DRAs：Gemini-2.5-Pro DR、O3 DR、Perplexity DR\n– 开源 DRAs：OAgents、DeerFlow、MiroFlow\n– 搜索增强 LLM：Gemini-2.5-Pro-Search、Claude-3.7-Sonar、Perplexity-Sonar-Reasoning-Pro、GPT-4.1-Search\n- 查询：150 条（50 任务×3 画像）\n- Judge：GPT-5 负责 P/Q，GPT-5-Mini 负责 R\n\n**关键结果**（节选）\n\n| 系统 | P-Score | Q-Score | R-Score |\n| --- | --- | --- | --- |\n| OAgents | 6.64 | 6.85 | 3.77 |\n| Gemini-2.5-Pro DR | 6.58 | 6.16 | 8.83 |\n| GPT-4.1-Search | 4.28 | 5.54 | 0.10 |\n\n- 开源 DRAs 个性化最强，但 factual accuracy 拉胯；商业系统可靠性高，个性化略逊；搜索版 LLM 普遍垫底。\n\n2\\. 信息输入消融实验：Only → Context → Persona\n\n**目的**：量化“无背景 / 仅交互上下文 / 显式画像”三种输入对个性化指标的影响。\n\n**设置**\n\n- 选取 4 款差异显著的系统（OAgents、O3 DR、Gemini-2.5-Pro-Search、Perplexity-Sonar-Reasoning-Pro）。\n- 固定 150 查询，逐条跑三种输入配置。\n\n**结果趋势**\n\n- 所有系统的 P-Score 随信息量增加而单调上升。\n- 显式画像平均再提升 0.3–0.6 分，显著优于上下文（配对 t-test，p<0.01）。\n- 商业系统对“上下文”利用效率低于开源，提示架构层面仍有改进空间。\n\n3\\. 记忆系统提升实验：把上下文自动提炼成“伪画像”\n\n**目的**：验证先进记忆模块能否弥补“无显式画像”场景。\n\n**设置**\n\n- 仅保留“Task w/ Context”输入，后接三款记忆系统：Mem0、Memory OS、O-Mem（私有）。\n- 下游统一调用 Perplexity DR 生成报告，评估其 GOAL、CONT 等子指标。\n- 查询：50 条高个性化需求样本\n\n**结果**\n\n| 记忆方案 | P-Score | GOAL | CONT |\n| --- | --- | --- | --- |\n| 无记忆 | 3.69 | 3.88 | 3.74 |\n| O-Mem | 4.26 | 4.47 | 4.43 |\n| Task w/ Persona 上限 | 4.58 | 4.69 | 4.93 |\n\n- O-Mem 显著优于无记忆（+0.57），但仍低于“直接给画像”0.32 分，表明记忆系统需向“高层抽象+推理”升级。\n\n4\\. 人类一致性验证：LLM 裁判 vs 专家打分\n\n**目的**：确保 PQR 框架的评分与人类主观判断一致。\n\n**设置**\n\n- 抽样 15 条查询，用 MiroFlow 与 O3 DR 各生成 1 份报告 → 共 30 份。\n- 3 名硕博人类评估员按同样四维准则独立打分。\n- 对比 3 款 Judge LLM（GPT-5、Claude-3.7-Sonar、Gemini-2.5-Pro）的 PCA 与 MARD。\n\n**结果**\n\n| Judge | PCA↑ | MARD↓ | 成本 $↓ |\n| --- | --- | --- | --- |\n| GPT-5 | 0.43 | 1.40 | 0.32 |\n\n- GPT-5 与人类偏好对齐最佳，被选为后续默认裁判。\n\n实验结论汇总\n\n1. 开源 DRAs 个性化领先但可靠性不足；商业系统反之；搜索+LLM 不能简单等同于个性化 DR。\n2. 显式画像 > 交互上下文 >> 无背景，验证个性化信号必要性。\n3. 记忆系统可部分填补“无画像”缺口，但仍需更高阶的推理抽象能力。\n4. PQR 框架经人类一致性校验，具备可接受的替代人工评估能力，为社区提供低成本、可扩展的评测协议。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为**基准扩展**、**方法深化**与**应用落地**三大板块，并给出可验证的关键假设与指标。\n\n1\\. 基准扩展\n\n| 方向 | 关键问题 | 可探索点 | 验证指标 |\n| --- | --- | --- | --- |\n| 跨语言与文化 | 现有画像与任务以中文为主 | 引入英语、西班牙语、阿拉伯语等多元文化志愿者，构建平行查询集 | P-Score 跨语言一致性、文化差异显著性 t-test |\n| 长周期演化 | 用户兴趣随时间漂移 | 设计 6–12 个月滚动采集协议，标注“兴趣漂移事件” | 漂移前后 P-Score 下降幅度、记忆系统召回率 |\n| 多模态需求 | 报告不止文本 | 加入“生成配图/数据可视化/交互式仪表盘”任务 | 视觉一致性评分（CLIPScore）、人类视觉偏好率 |\n| 对抗性个性化 | 用户画像本身有偏见或违法倾向 | 引入“偏见画像”测试集，检验系统是否生成歧视/误导内容 | 有害输出率、安全过滤器触发率、纠偏策略成功率 |\n\n2\\. 方法深化\n\n| 方向 | 关键问题 | 可探索点 | 验证指标 |\n| --- | --- | --- | --- |\n| 动态权重可解释性 | LLM 给出的维度权重是否可信 | 用 Shapley 值或 LIME 对权重决策进行事后解释，对比人类专家权重一致性 | 权重解释人类接受率、PCA↑ |\n| 个性化检索策略 | 现有系统先检索后过滤，召回率低 | 在检索阶段注入 persona embedding，实现“个性化召回”而非“事后重排” | Recall@20 提升率、P-Score 增益 |\n| 可信度-个性化权衡 | 过度迎合用户可能导致虚假强化（echo chamber） | 引入“反事实 factual 检查”：若去掉画像，声明是否仍被支持 | 反事实 FA 下降率、用户满意度差值 |\n| 记忆抽象级别 | 当前记忆多为“事实拼接”，缺乏高层命题 | 构建“信念-意图-价值观”三层抽象，用逻辑规则推理新偏好 | 抽象记忆压缩率、GOAL 子指标提升 |\n| 多 Agent 辩论 | 单 Agent 易陷入偏好盲区 | 让“研究员 Agent”（中立）与“用户代理 Agent”辩论，再生成报告 | 辩论后 R-Score 与 P-Score 同时提升的帕累托前沿 |\n\n3\\. 应用落地\n\n| 方向 | 关键问题 | 可探索点 | 验证指标 |\n| --- | --- | --- | --- |\n| 实时个性化 | 用户在与 Agent 对话中即时修正需求 | 构建“流式 persona 更新”协议，每轮对话后增量更新画像 | 轮次-累计 P-Score 曲线斜率 |\n| 群体个性化 | 家庭/公司等多成员共用同一 Agent | 建模“群体画像冲突解决”机制，如多目标优化或投票 | 群体满意度方差↓、决策时间↓ |\n| 边缘部署 | 云端 API 成本高、隐私风险大 | 将轻量化 persona encoder 与检索器蒸馏至 7B 以下模型，端侧运行 | 单轮延迟<1 s、本地 FA 与云端差距<5% |\n| 个性化评估即服务 | 中小企业难以承担 GPT-5 裁判成本 | 训练小型“评判专用模型”（≤3B），用人类偏好数据蒸馏 | 评判成本↓10×、MARD 增幅<0.2 |\n| 可编辑记忆 | 用户希望“被遗忘”或纠正错误历史 | 实现“选择性遗忘”与“记忆订正”接口，符合 GDPR Right to be Forgotten | 遗忘后敏感信息残留率↓、P-Score 波动<3% |\n\n4\\. 高风险高回报前沿\n\n- **个性化与算法偏见审计**：建立“Bias-in-Personalized-Research”套件，自动检测报告是否因用户政治倾向而屏蔽对立证据。\n- **个性化奖励黑客**（Reward Hacking）：研究 Agent 是否通过“虚假引用”或“ cherry-pick 数据”来迎合用户，设计鲁棒奖励函数。\n- **脑机接口极端个性化**：利用 EEG/眼动实时反馈调整报告深度与语速，迈向“神经自适应研究助手”。\n\n小结\n\n可探索点覆盖“数据-算法-系统-社会”全栈：\n\n1. 把基准做“更大、更久、更危险”；\n2. 把算法做“更可信、更可控、更解释”；\n3. 把系统做“更轻、更实时、更群体”；\n4. 把影响做“更安全、更合规、更人道”。\n\n任何一条路线均可直接复用本文的 PQR 框架与 250 查询种子，快速验证新假设。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有深度研究（DRA）评测只关注事实与完整性，忽略“用户画像”驱动的个性化；个性化基准又仅限浅层对话/推荐，无法评估复杂研究任务。\n- **方案**：提出首个“个性化深度研究”评测套件——Personalized Deep Research Bench + PQR 三维框架。\n- **数据**：50 项跨 10 域真实任务 × 25 份真实用户画像 → 250 条个性化查询；画像含结构化属性 + 动态交互记忆。\n- **指标**：P(Personalization Alignment) + Q(Content Quality) + R(Factual Reliability)，均用 LLM 动态生成权重与子准则，可量化、可解释。\n- **实验**：10 个主流系统（商业/开源/搜索-LLM）在 150 查询上全景评测，揭示开源 Agent 个性化强但可靠性低、商业系统均衡但画像对齐不足、仅加搜索≠个性化；显式画像 > 上下文 >> 无背景；记忆系统可部分弥补画像缺失，仍落后显式上限。\n- **意义**：填补个性化深度研究评测空白，为构建“真正以用户为中心”的 AI 研究助手提供基准、指标与改进路线图。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Towards%20Personalized%20Deep%20Research%3A%20Benchmarks%20and%20Evaluations，链接是https://arxiv.org/pdf/2509.25106，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25106。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-16-1759276017469","title":"Random Policy Valuation is Enough for LLM Reasoning with Verifiable Rewards","description":"RL with Verifiable Rewards (RLVR) has emerged as a promising paradigm for improving the reasoning abilities of large language models (LLMs). Current methods rely primarily on policy optimization frameworks like PPO and GRPO, which follow generalized policy iteration that alternates between evaluating the current policy's value and improving the policy based on evaluation. While effective, they often suffer from training instability and diversity collapse, requiring complex heuristic tricks and careful tuning. We observe that standard RLVR in math reasoning can be formalized as a specialized finite-horizon Markov Decision Process with deterministic state transitions, tree-structured dynamics, and binary terminal rewards. Though large in scale, the underlying structure is simpler than general-purpose control settings for which popular RL algorithms (e.g., PPO) were developed, suggesting that several sophisticated techniques in existing methods may be reduced or even omitted. Based on this insight, we prove a surprising result: the optimal action can be recovered from the Q-function of a fixed uniformly random policy, thereby bypassing the generalized policy iteration loop and its associated heuristics. We introduce Random Policy Valuation for Diverse Reasoning (ROVER) to translate this principle into a practical and scalable algorithm for LLM math reasoning, a minimalist yet highly effective RL method that samples actions from a softmax over these uniform-policy Q-values. ROVER preserves diversity throughout training, allowing sustained exploration of multiple valid pathways. Across multiple base models and standard math reasoning benchmarks, ROVER demonstrates superior performance in both \\textbf{quality} (\\textbf{+8.2} on pass@1, \\textbf{+16.8} on pass@256) and \\textbf{diversity} (\\textbf{+17.6\\%}), despite its radical simplification compared to strong, complicated existing methods.","url":"https://huggingface.co/papers/2509.24981","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.469Z","metadata":{"authors":["Haoran He","Yuxiao Ye","Qingpeng Cai","Chen Hu","Binxing Jiao","Daxin Jiang","Ling Pan"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2509.24981.pdf","arxivUrl":"https://arxiv.org/abs/2509.24981","arxivId":"2509.24981","coolPaperUrl":"https://papers.cool/arxiv/2509.24981","published":"2025-09-29T16:09:07Z","updated":"2025-09-29T16:09:07.000Z","zh_summary":"具有可验证奖励的 RL （RLVR） 已成为提高大型语言模型 （LLM） 推理能力的一种有前途的范式。目前的方法主要依赖于 PPO 和 GRPO 等政策优化框架，这些框架遵循通用政策迭代，在评估当前政策的价值和基于评估的改进政策之间交替进行。虽然有效，但它们经常受到训练不稳定和多样性崩溃的困扰，需要复杂的启发式技巧和仔细调整。我们观察到，数学推理中的标准 RLVR 可以形式化为具有确定性状态转换、树结构动力学和二元终端奖励的专用有限视界马尔可夫决策过程。尽管规模很大，但底层结构比开发流行 RL 算法（例如 PPO）的通用控制设置更简单，这表明现有方法中的几种复杂技术可能会减少甚至省略。基于这一见解，我们证明了一个令人惊讶的结果：可以从固定的均匀随机策略的 Q 函数中恢复最优动作，从而绕过广义策略迭代循环及其相关的启发式方法。我们引入了多样化推理的随机策略评估 （ROVER），将这一原理转化为用于 LLM 数学推理的实用且可扩展的算法，这是一种极简但高效的 RL 方法，可在这些统一策略 Q 值上从软最大值中对动作进行采样。ROVER 在整个训练过程中保持多样性，允许持续探索多种有效途径。在多个基础模型和标准数学推理基准中，ROVER 在 \\textbf{quality}（pass@1 上的 \\textbf{+8.2}，pass@256上的 \\textbf{+16.8}）和 \\textbf{diversity}（\\textbf{+17.6\\%}）方面都表现出卓越的性能，尽管与强大、复杂的现有方法相比，它进行了彻底的简化。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心针对的是“如何用最简化的强化学习（RL）流程，在可验证奖励（verifiable reward）场景下提升大语言模型（LLM）的数学推理能力”。\n\n具体而言，现有 RLVR（Reinforcement Learning with Verifiable Rewards）方法普遍沿用 PPO/GRPO 等通用深度 RL 框架，这些框架为应对随机转移、复杂奖励、循环状态等一般控制问题而设计，带来了：\n\n- 训练不稳定（非平稳评估目标）\n- 多样性崩溃（熵快速下降）\n- 实现复杂（裁剪、KL 正则、优势归一化等大量启发式调参）\n\n作者观察到数学推理任务在 MDP 层面具有“有限步、确定转移、树状展开、末端二元正确/错误奖励”这一**专门结构**，远比通用 RL 环境简单。于是提出并证明：\n\n> 在该类结构下，**只需对“均匀随机策略”做一次策略评估**得到 Q 值，再依此 Q 值贪心或带温度 softmax 地选动作，即可直接获得最优或近优策略，而无需交替式“评估-改进”循环（GPI）。\n\n基于该理论结果，论文给出极简算法 ROVER（Random Policy Valuation for Diverse Reasoning），用 LLM 自身参数隐式建模 uniform-policy Q 值，配合组奖励中心化和长链广播技巧，在 8B 模型上相对强基线提升 pass@1 +8.2、pass@256 +16.8，同时多样性指标提高 17.6%，且实现大幅简化（无价值网络、无 KL 惩罚、无裁剪）。\n\nQ2: 有哪些相关研究？\n\n以下研究被论文直接或间接关联，按主题归类并给出核心贡献/与 ROVER 的对比要点。\n\n- 所有引用编号对应原文参考文献序号，方便对照。\n\n1\\. RL with Verifiable Rewards（RLVR）框架\n\n| 代表工作 | 核心机制 | 与 ROVER 的关系 |\n| --- | --- | --- |\n| PPO Schulman et al., 2017 | 重要性采样 + 裁剪 + 价值网络 | ROVER 完全去掉价值网络与裁剪，跳出 GPI 循环 |\n| GRPO Shao et al., 2024 | 组内奖励归一化，无价值模型 | 仍沿用 PPO 目标，需 KL/clip 等启发式；ROVER 无需这些技巧 |\n| ReMax Li et al., 2024 | 单样本 baseline 减方差 | 仍属 Actor-only PPO 变体 |\n| RLOO Ahmadian et al., 2024 | 留一法 baseline | 同上 |\n| REINFORCE++ Hu et al., 2025a | 全局 batch 归一化优势 | 在论文实验中被 ROVER 全面超越 |\n| DAPO Yu et al., 2025 | 动态采样 + clip-higher | 同场实验对比，ROVER 在 pass@k 上领先 16.8 |\n\n2\\. 多样性保持与熵崩溃抑制\n\n| 代表工作 | 主要手段 | ROVER 的差异 |\n| --- | --- | --- |\n| Entropy-Regularized RL Liu et al., 2025c | 显式熵 bonus | ROVER 无额外正则，靠 softmax-Q 自然维持高熵 |\n| SEED-GRPO Chen et al., 2025a | 语义熵加权 | 需额外语义模型；ROVER 不引入辅助网络 |\n| DRA-GRPO Chen et al., 2025b | 子模互信息奖励重加权 | 需对响应间相似度建模，计算开销大 |\n| Diversity-Aware PO Yao et al., 2025 | 响应级距离惩罚 | 任务相关调参；ROVER 无需调参即可提升多样性 17.6% |\n| NoveltyBench 指标 Zhang et al., 2025c | 语义策略分类 | 论文直接采用该指标评估 ROVER 的“不同策略数” |\n\n3\\. 理论侧：均匀策略价值与 softmax 算子\n\n| 代表工作 | 理论洞察 | 与本文联系 |\n| --- | --- | --- |\n| Littman & Szepesvári, 1996(12) | 广义 Bellman 算子框架 | 本文用“mean 算子”特例，并首次给出树状 MDP 最优性证明 |\n| Asadi & Littman, 2017(2) | 指出 mean 算子对一般控制无效 | 本文证明在树+确定转移+末端二元奖励下 mean 算子+贪心即最优 |\n| GFlowNet Bengio et al., 2021 | 均匀策略可用于多样采样 | 经验观察；ROVER 给出正式 RL 最优性证明并用于 LLM 微调 |\n| He et al., 2025b(9) | 离散格子世界 uniform-Q 有效 | 仅实验现象；本文扩展到 LLM 推理并给出理论保证 |\n\n4\\. 训练技巧与实现\n\n| 技巧来源 | 用途 | ROVER 中的改造 |\n| --- | --- | --- |\n| Reward Centering Naik et al., 2024 | 减方差 | 用于每 prompt n 样本的 ˜r；去掉 GRPO 的标准差除法 |\n| Token-Level 广播 Hu et al., 2025b | 长链信用分配 | 末端奖励广播到每个 token，与 Q′ 相加作为 Bellman 目标 |\n| Relative Q Li et al., 2025b | 稳定值函数 | 用 ρ(log πθ − log πθ_old) 建模“相对 Q”，避免绝对值漂移 |\n\n5\\. 任务与评测基准\n\n- **数学竞赛**：AIME24/25、HMMT25、AMC23、OlympiadBench、MATH500\n- \\*\\* countdown 算术\\*\\*：TinyZero 数据集（多解空间，便于分析多样性）\n- **OOD 泛化**：GPQA-diamond（研究生级跨域问答）\n\n一句话总结\n\nROVER 在算法层面跳出了“PPO 家族”的复杂启发式调参路线，在理论层面首次证明“均匀策略评估+贪心”对树状确定 MDP 的最优性，在实践层面用极简实现同时取得质量与多样性双提升，与上表所有相关研究形成鲜明对比。\n\nQ3: 论文如何解决这个问题？\n\n论文将“复杂 RLVR 流程简化”拆解为三步：先**形式化**问题结构，再**证明**均匀策略评估即可最优，最后**落地**为可扩展算法。关键公式与实现细节如下。\n\n1\\. 形式化：数学推理 MDP 的专属结构\n\n- 有限步、确定转移、树状展开、末端二元奖励\n- 状态空间  S ：所有前缀字符串；动作空间  V ：词表\n- 转移  P(s_(t+1)|s_t,a_t)=1  且  s_(t+1)=s_tcirc a_t （拼接）\n- 奖励仅在终止态给出  r∈0,R ，折扣  γ=1 \n\n2\\. 理论：均匀策略评估即足够\n\n令均匀策略  π_u(a|s)=(1) / (|A|) ，其 Q 满足\n\nQ^(π_u)(s,a)=r(s,a)+(1) / (|A|)∑_(a')Q^(π_u)(s',a'). 2\n\n**定理 3.1**\n在上述树状确定 MDP 中，贪心策略\n\nπ_(greedy)(s)=argmax_a Q^(π_u)(s,a)\n\n已是最优策略。**无需 GPI 循环**。\n\n**直观解释**\n Q^(π_u)(s,a)  等于“从  (s,a)  出发后续**随机走**能拿到正确结局的概率”。若子树无解则  Q=0 ；有解则  Q>0 。贪心选择直接砍掉  Q=0  的分支，沿任意  Q>0  路径必达正确终端。\n\n3\\. 实用算法：ROVER\n\n把“评估  π_u ”转成可微目标，用 LLM 自身参数完成。\n\n3.1 参数化 Q（公式 3）\n\nQ(s_t,a_t)=rho[logπ_θ(a_t|s_t)-logπ_(θ_old)(a_t|s_t)]\n\n- 只建模**相对增量**，避免绝对值漂移\n-  rho  即 softmax 温度，后续直接用于采样\n\n3.2 低方差 Bellman 目标\n\n对同一 prompt 采样  n  条回答，做组内中心化\n\ntilde r_i = r_i - (1) / (n)∑_(j=1)^n r_j. 4\n\n单步 Bellman 目标\n\nhat Q(s_t,a_t)=tilde r_i + (1) / (|V|)∑_(a'∈ V)Q(s_(t+1),a'). 8\n\n3.3 训练损失\n\n用 MSE 把当前  Q  推向目标，并对  tilde r  做**全序列广播**以解决长链信用分配\n\nL_(ROVER)=(1) / (n)∑_(i=1)^n(1) / (|y_i|)∑_(t=0)^(|y_i|-1)[Q(s_t,a_t)-sg[hat Q(s_t,a_t)]]^2. 9\n\n3.4 采样策略\n\n训练后按\n\nπ_s(a|s)proptoexp!(Q(s,a)/rho)\n\n解码，兼顾质量与多样性；温度  rho  统一设为 1，无需任务调参。\n\n4\\. 复杂度与实现\n\n- **无价值网络**：Q 直接复用 LLM 输出 logits\n- **无 KL 惩罚、无裁剪**：损失仅一项 MSE\n- **GPU 小时对比**：同样 8B 模型，ROVER 1280 GPUh 即超过 DAPO/GRPO 1600+ GPUh 的效果\n\n5\\. 结果验证\n\n- **质量**：AIME24/25 +HMMT25 平均 pass@1 提升 8.2，pass@256 提升 16.8\n- **多样性**：Distinct Strategies +17.6%，Entropy 全程高于基线\n- **OOD**：GPQA-diamond 再涨 19.6，验证未过拟合数学分布\n\n一句话总结\n\n论文用“树状确定 MDP → 均匀策略评估即最优 → 相对 Q 参数化 + 中心化奖励”三步，把原先需要价值网络、裁剪、KL、熵正则的复杂 PPO 类流程，压缩成**仅一次 MSE 回归**的极简算法，即取得质量与多样性双最优。\n\nQ4: 论文做了哪些实验？\n\n论文从“可验证奖励”与“多解多样性”两条主线出发，设计了**倒计时算术任务**与**数学竞赛任务**两大实验板块，辅以**OOD 泛化**、**多样性量化**、**行为分析**与**消融测试**，共 5 类实验。核心结果均报告平均提升（ROVER 相对最强基线）。\n\n1\\. 倒计时任务（Countdown）——多解空间可控分析\n\n**数据集**：TinyZero 1 024 道测试题，每题允许多条合法算式。\n**基线**：GRPO 及其三种熵保持变体（KL 系数 0.01/0.002、clip-higher）。\n**观测指标**：\n\n- Avg@64（64 次采样至少一次正确率）\n- 策略熵（训练期实时）\n- Distinct Solutions（每题不重复正确算式数）\n\n**关键结果**（图 6–7）：\n\n- Avg@64 最终 **0.70** vs 最佳基线 0.55 (+27 %)\n- 训练全程熵 >0.9，基线 <0.3 即崩溃\n- 同一例题 ROVER 发现 **17 条不同正确算式**，GRPO 仅 3 条\n\n2\\. 数学竞赛任务——单解高质量挑战\n\n**模型规模**：Qwen3-8B-Base、Qwen3-4B-Base、DeepSeek-R1-Distill-1.5B\n**训练数据**：DeepScaler 40 k 可验证题（1.5B）/ 自采 100 k 题（4B/8B）\n**评测基准**：\n\n| 竞赛级 | AIME24, AIME25, HMMT25, AMC23, OlympiadBench |\n| --- | --- |\n| 常规 | MATH500 |\n| OOD | GPQA-diamond（生物/化学/物理） |\n\n**指标**：pass@1（单发正确率）与 pass@k（k=1→256 曲线，多样性代理）\n\n**表 2 & 图 9 主要提升**（8B-Base 平均）：\n\n- pass@1：**+8.2** (30.6 vs 22.4 最佳基线 DAPO)\n- pass@256：**+16.8** (72.0 vs 55.2)\n- 最难 HMMT25 几乎翻倍（14.6 vs 7.1 REINFORCE++）\n\n3\\. OOD 泛化测试\n\nGPQA-diamond 198 道研究生题，ROVER 在 8B 上取得 **48.4 pass@1**，领先最强基线 **+19.6**，验证方法未过拟合数学分布。\n\n4\\. 多样性量化与行为分析\n\n1. **Distinct Strategies**（图 10）\n用 Claude-3.5 对 32 条正确解答做策略等价聚类；ROVER 平均 **4.7 种策略/题**，GRPO 仅 4.0（+17.6 %）。\n\n2. **Utility & Cosine Distance**（附录图 22–23）\n低解码温度 (0.3) 下 ROVER 仍保持最高语义分散度。\n\n3. **Maj@k 曲线**（图 11）\n多数投票随 k 增大持续上升，HMMT25 上 k=64 时 Maj 提升 **+18** 个百分点，表明多样性直接转化为测试时算力收益。\n\n4. **反思令牌统计**（图 12, 16）\n定义 6 类“forking tokens”(wait, however, suppose…)；ROVER 输出中对比/反思类令牌频率 **翻倍**，可视化显示其更频繁切换策略路径。\n\n5\\. 消融与超敏分析\n\n- **温度 ρ**（图 8, 19）\nρ=1 为甜点；ρ→0 过早熵崩，ρ≥3 欠探索收敛慢。\n\n- **Bellman 目标中 Q′ 系数 β**（图 14–15）\nβ=0（纯中心化奖励）→ 熵与长度崩溃，pass@k 掉 >30 %；β∈\n0.2,1\n 稳定；β=5 奖励信号被稀释亦下降。\n\n- **Q′ 规模曲线**（图 21）\n训练期 Q′ 均值保持小幅度正漂移，验证相对参数化稳定性。\n\n6\\. 训练开销与可扩展性\n\n- 8B 模型 600 步共 1 280 GPUh，即达 SOTA 性能；而可比 GRPO/DAPO 需 1 600+ GPUh 且仍低于 ROVER。\n- 1.5B 模型 960 GPUh 超越同数据量 DeepScaler，且 pass@64 优于用 3×数据 16k GPUh 的 ProRLv2。\n\n一句话总结\n\n实验覆盖“多解-单解-OOD”三级场景，量化指标从“单发正确率”到“256 采样曲线”再到“语义策略类”，一致显示 ROVER 在**质量、多样性、泛化、训练效率**四维度全面领先，且实现最简化。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为“ROVER 简化范式”的自然延伸，亦是对原文 Discussion 段落的具体展开。每条均给出**可验证假设**与**初步思路**，便于直接落地。\n\n1\\. 非树状或非二元奖励场景：理论边界再刻画\n\n- **假设**\n若状态图含环、存在中间奖励或工具调用（代码执行、检索），uniform-policy Q 不再保证最优。\n- **思路**\n1. 在 MATH 数据加入“逐步给分”奖励（0/0.3/0.7/1）→ 观察 ROVER 性能衰减斜率\n2. 将 mean 算子替换为 **softmax 算子**（Asadi & Littman, 2017）并推导新的性能下界\n3. 对含环 MDP，考虑 k-step look-ahead uniform rollouts 后再贪心，验证“近似最优”阈值\n\n2\\. 层次化行动空间：从 token 到「宏动作」\n\n- **假设**\n把“一整步推导”封装为宏动作（如 `derive(\"...\")`），可缩短 horizon、降低方差，同时保持多样性。\n- **思路**\n1. 在 countdown 任务预定义 20 个模板宏动作（Add, Mul, Factorial…）\n2. ROVER 仅需评估宏动作层的 uniform Q，token 层用原始 LLM 填槽 → 比较相同步数下 pass@k\n3. 若有效，可推广到定理证明器（Lean4 tactic）（宏动作=tactic 名）\n\n3\\. 在线策略修正：轻量级 GPI-Wrapper\n\n- **假设**\n完全抛弃 GPI 过于激进；在训练后期**偶尔**做一次策略重评估可进一步抬升尾部性能。\n- **思路**\n1. 采用“early ROVER + late GPI”混合：前 50 % 步数纯 ROVER，后 50 % 步数用低学习率 PPO 微调\n2. 监控熵阈值：当熵 < 0.3 自动切换，防止过早陷入单峰\n3. 对比单一 ROVER / 单一 PPO，验证是否取得 Pareto 更优（质量 vs 多样性）\n\n4\\. 多任务 uniform-Q 共享：参数效率探索\n\n- **假设**\n数学、代码、化学平衡问题均满足“确定+树状+末端二元”结构，可**共享同一 uniform-Q 头**，仅领域提示符不同。\n- **思路**\n1. 训练阶段混合 3 类可验证数据（MATH + CodeForces + Chemistry-Equations）\n2. 在 LLM 最后一层插入**小型跨域 Q-adapter**（<1 % 参数量）\n3. 评估零样本/少样本跨域迁移，检验 uniform-Q 的通用性是否超越单纯提示\n\n5\\. 模型规模外推：ROVER 在 70B+ 的稳定性\n\n- **假设**\n大模型 logits 方差更小，相对 Q 可能过平滑，需重新校准温度 ρ。\n- **思路**\n1. 在 70B 模型重复 8B 实验，固定 ρ=1 观察熵曲线是否“过于平缓”\n2. 引入**尺度自适应温度** ρ = α·√d\\_model，α 在小模型调优后直接外推\n3. 若出现饱和，可尝试**logit 标准化**（减去均值再进 softmax）保持相对 Q 动态范围\n\n6\\. 与推理时扩展协同：ROVER-Q 作为树搜索启发式\n\n- **假设**\nuniform-Q 可作为轻量级价值函数，替代 MCTS 中的 NN-Value，提高大宽度搜索效率。\n- **思路**\n1. 在 AIME 题上运行 BFS+ROVER-Q：节点优先级 = Q(s,a)，预算 1 000 节点\n2. 对比相同预算下 PPO-Value 引导的 MCTS 与原始 ROVER 采样 pass@k\n3. 若 ROVER-Q 启发式胜率更高，则证明“评估一次即可搜索”的新范式\n\n7\\. 理论深化：最优性→概率收敛速度\n\n- **假设**\n树状 MDP 下，uniform-Q 的样本复杂度显著低于 GPI。\n- **思路**\n1. 建立 PAC 界：以高概率 1-δ 找到 ε-最优策略所需样本数\n2. 对比 Q-learning（ε-greedy）与 ROVER 的收敛上界，量化“跳过策略改进”带来的常数级加速\n3. 结果可指导实际训练预算设置（样本/步数）\n\n8\\. 奖励噪声鲁棒性：从二元→软标签\n\n- **假设**\n真实场景奖励可能为概率值（0.8 正确），ROVER 仍可用，但需调整中心化方式。\n- **思路**\n1. 在奖励中注入高斯噪声 N(0,σ) 并截断至 \n0,1\n，观察 σ=0.1/0.3/0.5 时 pass@1 下降曲线\n2. 将中心化改为“减中位数”或“Winsorize”后再广播，检验方差抑制效果\n3. 若下降更缓，可宣称 ROVER 对软标签/噪声标签同样极简鲁棒\n\n9\\. 人机协同：uniform-Q 作为“可解释置信度”\n\n- **假设**\nQ(s,a) 直接对应“后续随机走能解出的概率”，可向用户解释模型为何选择该步。\n- **思路**\n1. 开发交互界面：点击推理步骤即显示 Q 值热力图（0→红，1→绿）\n2. 用户可手动屏蔽 Q<θ 的分支，实时生成新答案，验证是否提升整体正确率\n3. 进行小样本用户研究，量化“uniform-Q 可视化”对人工校验效率的提升\n\n10\\. 向后兼容：把 ROVER 作为预训练正则项\n\n- **假设**\n在预训练阶段加入“uniform-Q 自监督”可让模型先验偏向树状可验证任务，减少后续微调步数。\n- **思路**\n1. 采集大规模网页数学文本，自动合成 1 M 可验证题目\n2. 预训练时每 1 k 步插入 ROVER 损失（权重 λ=0.1）\n3. 后续标准 SFT+ROVER 微调步数减半，看是否达到相同精度 → 若成立，可推广为“可验证-aware 预训练”\n\n总结\n\n从**理论延伸**、**结构泛化**、**系统协同**到**人机交互**，ROVER 的“ uniform-Q 评估即足够”理念尚有多层空白。以上 10 点均可快速原型化，并在 3–6 个月内得到可量化结论，为下一代极简 RLVR 提供新的跳跃板。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **ROVER**（Random Policy Valuation for Diverse Reasoning），用一句话概括：\n\n> 在数学推理这类“确定转移-树状展开-末端二元奖励”的专门 MDP 里，**只需对均匀随机策略做一次 Q 评估**，再按 Q 值 softmax 采样，就能同时获得 **最优性** 与 **多样性**，彻底甩掉 PPO/GRPO 的复杂 GPI 循环。\n\n核心内容拆成四句话：\n\n1. **理论**：证明 uniform-policy Q + 贪心即最优；softmax 采样给出可调控的多样性下界。\n2. **算法**：用 LLM 自身 logits 建模相对 Q，组内中心化奖励+广播，损失仅一项 MSE，无价值网络、无 KL、无裁剪。\n3. **实验**：在 8B 模型上 pass@1 +8.2、pass@256 +16.8，多样性指标 +17.6%，全面超越 PPO/GRPO/DAPO；OOD 任务同样领先。\n4. **意义**：首次把数学推理 RLVR 从“通用控制工具箱”简化成“一次评估即可”，为后续规模、结构、人机协同研究提供新基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Random%20Policy%20Valuation%20is%20Enough%20for%20LLM%20Reasoning%20with%20Verifiable%20Rewards，链接是https://arxiv.org/pdf/2509.24981，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.24981。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-17-1759276017469","title":"VideoScore2: Think before You Score in Generative Video Evaluation","description":"Recent advances in text-to-video generation have produced increasingly realistic and diverse content, yet evaluating such videos remains a fundamental challenge due to their multi-faceted nature encompassing visual quality, semantic alignment, and physical consistency. Existing evaluators and reward models are limited to single opaque scores, lack interpretability, or provide only coarse analysis, making them insufficient for capturing the comprehensive nature of video quality assessment. We present VideoScore2, a multi-dimensional, interpretable, and human-aligned framework that explicitly evaluates visual quality, text-to-video alignment, and physical/common-sense consistency while producing detailed chain-of-thought rationales. Our model is trained on a large-scale dataset VideoFeedback2 containing 27,168 human-annotated videos with both scores and reasoning traces across three dimensions, using a two-stage pipeline of supervised fine-tuning followed by reinforcement learning with Group Relative Policy Optimization (GRPO) to enhance analytical robustness. Extensive experiments demonstrate that VideoScore2 achieves superior performance with 44.35 (+5.94) accuracy on our in-domain benchmark VideoScore-Bench-v2 and 50.37 (+4.32) average performance across four out-of-domain benchmarks (VideoGenReward-Bench, VideoPhy2, etc), while providing interpretable assessments that bridge the gap between evaluation and controllable generation through effective reward modeling for Best-of-N sampling. Project Page: https://tiger-ai-lab.github.io/VideoScore2/","url":"https://huggingface.co/papers/2509.22799","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.469Z","metadata":{"authors":["Xuan He","Dongfu Jiang","Ping Nie","Minghao Liu","Zhengxuan Jiang","Mingyi Su","Wentao Ma","Junru Lin","Chun Ye","Yi Lu","Keming Wu","Benjamin Schneider","Quy Duc Do","Zhuofeng Li","Yiming Jia","Yuxuan Zhang","Guo Cheng","Haozhe Wang","Wangchunshu Zhou","Qunshu Lin","Yuanxing Zhang","Ge Zhang","Wenhao Huang","Wenhu Chen"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.22799.pdf","arxivUrl":"https://arxiv.org/abs/2509.22799","arxivId":"2509.22799","coolPaperUrl":"https://papers.cool/arxiv/2509.22799","published":"2025-09-26T18:09:03Z","updated":"2025-09-26T18:09:03.000Z","zh_summary":"在文本到视频生成方面的最新进展已经产生了越来越逼真且多样化的内容，但对这些视频进行评估仍然是一个根本性的挑战，因为视频具有多方面的特性，包括视觉质量、语义对齐和物理一致性。现有的评估器和奖励模型仅限于单一的不透明评分，缺乏可解释性，或者只提供粗略分析，因此不足以捕捉视频质量评估的全面性。我们提出了 VideoScore2，这是一种多维、可解释且与人类评估一致的框架，可明确评估视觉质量、文本到视频的对齐以及物理/常识一致性，同时生成详细的思维链推理。我们的模型在大规模数据集 VideoFeedback2 上训练，该数据集包含 27,168 个带有人类标注评分及三维推理轨迹的视频，采用两阶段流程：先进行监督微调，再通过组相对策略优化（Group Relative Policy Optimization, GRPO）的强化学习提升分析鲁棒性。大量实验表明，VideoScore2 在我们域内基准 VideoScore-Bench-v2 上实现了 44.35 (+5.94) 的准确率，并在四个域外基准（VideoGenReward-Bench、VideoPhy2 等）上取得了 50.37 (+4.32) 的平均表现，同时提供可解释的评估，使评估与可控生成之间的差距得以通过对 Best-of-N 抽样的有效奖励建模来弥合。项目页面: https://tiger-ai-lab.github.io/VideoScore2/","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该工作针对“文本到视频（T2V）生成质量如何被可靠、可解释且与人类偏好对齐地评估”这一核心瓶颈展开。现有评估器/奖励模型普遍仅输出单一不透明分数，缺乏可解释性，也难以泛化到分布外数据。论文提出 VIDEOSCORE2，通过显式拆解视觉质量、文本对齐、物理/常识一致性三个维度，并在打分前生成链式思考（chain-of-thought）分析，实现多维、可解释、人类对齐的视频质量评估，同时显著提升域内与域外基准上的准确率与鲁棒性。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三类，均与本文提出的 VIDEOSCORE2 存在维度覆盖、可解释性或泛化能力上的差距。\n\n1. 文本到视频生成\n\n- 早期扩散 pipeline：ModelScope、VideoCrafter2\n- 近期 DiT/Transformer 架构：CogVideoX、StepVideo-T2V、OpenAI Sora、Google Veo 3、快手 Kling-1.6、Pika-2.2 等\n→ 生成质量快速提升，但缺乏系统、多维、人类对齐的评估框架。\n2. 视觉奖励/评分模型\n\n- 单维标量：ImageReward、Dover、DeQA-Score\n- 多维但无解释：VideoReward、UnifiedReward、Q-Insight\n- 短评论级解释：LiFT\n→ 均无法提供细粒度、链式思考的长篇分析，且大多仅做 SFT，泛化受限。\n3. 视频理解与推理\n\n- backbone 架构：Video Swin、InternVideo\n- 视频-语言对齐：Video-LLaMA、Video-LLaVA、mPLUG-Owl-V\n- 长视频基准：Video-MME、VideoEval-Pro\n→ 侧重开放式问答或长时序理解，而非生成质量的多维打分与可解释推理。\n\nVIDEOSCORE2 在上述方向基础上首次同时实现“多维评分 + 链式思考 + 强化学习微调”，填补可解释视频评估与分布外泛化的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“数据-训练-推理”全链路设计，把“打分”转化为“先思考后评分”的多维可解释决策问题，具体方案如下：\n\n1. 构建大规模人类标注数据集 VIDEOFEEDBACK2\n\n- 2933 文本提示 × 22+ T2V 模型 → 27 168 段视频\n- 每段视频在视觉质量、文本对齐、物理/常识一致性三维上获得 1–5 整数分数 + 简短诊断\n- 用 Claude-4-Sonnet 将诊断扩展为 200–600 词链式思考（CoT）长 rationale，并与分数对齐，形成 81 504 条“视频-思考-分数”三元组。\n2. 两阶段训练策略\n① 冷启动监督微调（SFT）\n– 以 Qwen2.5-VL-7B-Instruct 为基座，采用 QA 模板：先输出 `<think>` … `</think>` 再给出 `V…; T…; P…` 三维分数。\n– 仅 2 fps 采样即可覆盖全局与持续异常，兼顾效率与精度。\n\n② 强化学习精调（GRPO）\n– 在 SFT checkpoint 上继续训练，奖励函数：\n\nR=R_(acc)+λ R_(fmt)\n\n\\- 准确率奖励  R_(acc) ：三维全部命中得 1.0；任一维偏差 ≥2 得 0；中间情况 0.1–0.7。\n\\- 格式奖励  R_(fmt) ：仅当输出含 `<think>` 标签时给 1。\n– 300 步后达到域内/域外峰值，继续训练会过拟合。\n\n3. 推理与概率软化\n\n- 温度 0.7 解码，对 1,2,3,4,5 词汇概率加权求期望，得到连续分数 $tilde y∈\n1,5\n$，兼顾细粒度与可解释性。\n4. 评估与下游应用\n\n- 域内基准 VIDEOSCORE-BENCH-V2：准确率 44.35（+5.94）， relaxed 准确率 90.78（+4.01），PLCC 60.37（+8.32）。\n- 4 个域外基准平均性能 50.37（+4.32），显著优于仅 SFT 的现有奖励模型。\n- 作为 Best-of-5 奖励模型，在 VBench 上持续优于随机采样，验证其对生成过程的指导价值。\n\n通过“先思考后评分”的数据配方与两阶段训练，VIDEOSCORE2 同时获得高人类一致性、跨域泛化能力与可解释性，解决了传统评估器单维、黑箱、泛化差的核心痛点。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“多维可解释评估”与“跨域泛化”两大核心目标，系统开展了以下实验：\n\n1. 主实验：域内基准 VIDEOSCORE-BENCH-V2\n\n- 指标：严格准确率、±1 relaxed 准确率、PLCC\n- 对比 10+ 基线（Prompting MLLM、图像/视频奖励模型）\n- 结果：VIDEOSCORE2 三项指标全面领先，平均准确率提升 5.94%，PLCC 提升 8.32%。\n2. 域外（OOD）基准四连测\n\n- 成对偏好：VideoGenReward-Bench、T2VQA-DB\n- 点分预测：MJ-Bench-Video、VideoPhy2-test\n- 统一维度映射与分数重缩放后，VIDEOSCORE2 平均性能 50.37，领先次优基线 4.32 个百分点。\n3. 奖励模型实战：Best-of-N 采样\n\n- 6 个中等质量 T2V 模型，每提示 5 候选，共 2 500 视频\n- 以 VBench 五维质量为指标，BoN-5 一致优于随机采样，验证其作为可解释奖励信号的实用性。\n4. 消融与超参实验\n\n- 冷启动必要性：RL 从 SFT checkpoint 出发全面优于直接从基座模型训练。\n- 思考链价值：SFT 阶段引入 CoT rationale 显著提升点分基准泛化。\n- 分数格式：归一化浮点分数在偏好与点分任务间取得最佳平衡。\n- 采样 fps：训练与推理均只需 2 fps，更高帧率无收益且增加成本。\n- RL 步数：300 步达到峰值，继续训练导致域内性能下降。\n5. 人类一致性抽检\n\n- 随机抽取典型案例，人工核查预测分数与 rationale 合理性，确认解释与评分高度吻合（示例见论文图 12–14）。\n\n通过上述实验，论文验证了 VIDEOSCORE2 在“多维可解释评估”与“分布外强泛化”两项核心能力上的先进性与实用性。\n\nQ5: 有什么可以进一步探索的点？\n\n- **世界模型增强的物理一致性评估**\n当前物理/常识维度仍依赖浅层视觉异常检测，可引入显式物理引擎或神经世界模型，对运动合理性、质量-碰撞-光照一致性进行可微仿真，再与 VLM 融合，实现“可验证”的物理评分。\n\n- **时序细粒度诊断**\n2 fps 已足够捕捉持续异常，但对毫秒级瑕疵（高频闪烁、瞬时穿模）仍不敏感。探索事件相机或自适应关键帧采样，在保持效率的同时定位帧级缺陷。\n\n- **多模态提示扩展**\n现仅支持文本→视频。将图像、音频、3D 布局、运动曲线作为联合条件，评估跨模态对齐与一致性，可推动“任意模态→视频”质量评估标准化。\n\n- **可扩展的在线偏好学习**\n采用人类在环的 DPO/RLAIF 循环，持续收集用户真实反馈，对奖励模型进行增量更新，解决分布漂移与审美演化问题。\n\n- **评估即优化：可微奖励函数**\n将链式思考过程蒸馏为稠密奖励信号，通过梯度反传直接优化扩散模型去噪步骤，实现“评估-生成”端到端联合训练，而非仅做 Best-of-N 采样。\n\n- **长视频与多场景一致性**\n当前视频 ≤6 s，对长视频的镜头级风格一致性、角色身份连续性、故事逻辑合理性尚未建模。构建长视频专用维度（叙事连贯性、角色一致性）并配套标注数据，是迈向“电影级”评估的关键。\n\n- **公平性、偏见与安全维度**\n现三维未显式度量文化偏见、刻板印象或有害内容。引入公平性-安全维度并设计对应探测 prompt，可实现更负责任的生成评估。\n\n- **压缩与部署优化**\n7B 模型在 2 fps 下仍需多卡推理。探索视频 Token 压缩、MoE 稀疏化或专用小视觉主干，在边缘端实现实时评估，为在线生成平台提供低延迟奖励信号。\n\nQ6: 总结一下论文的主要内容\n\n**VIDEOSCORE2：多维可解释视频质量评估**\n\n1. 问题\n文本到视频生成质量需同时衡量视觉保真、语义对齐与物理合理性；现有奖励模型仅给单分，无解释、泛化差。\n\n2. 数据\n构建 VIDEOFEEDBACK2：27 168 段视频 × 3 维人类分数 + 链式思考 rationale，共 81 504 条标注。\n\n3. 方法\n两阶段训练：\n① 冷启动 SFT 让 Qwen2.5-VL-7B 学会“先思考后评分”格式；\n② GRPO 强化学习，以“三维全对才满分”的稀疏奖励继续对齐人类偏好。\n\n4. 结果\n\n- 域内 VIDEOSCORE-BENCH-V2：准确率 44.35（+5.94），PLCC 60.37（+8.32）。\n- 4 个域外基准平均性能 50.37（+4.32）。\n- 作为 Best-of-5 奖励模型，在 VBench 上持续优于随机采样。\n\n5. 结论\nVIDEOSCORE2 首次实现“多维分数 + 可解释推理 + 强泛化”，为可靠评估与可控生成提供新基线；物理深度、长视频一致性与在线学习仍是未来方向。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VideoScore2%3A%20Think%20before%20You%20Score%20in%20Generative%20Video%20Evaluation，链接是https://arxiv.org/pdf/2509.22799，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.22799。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1759276017469","title":"Critique-Coder: Enhancing Coder Models by Critique Reinforcement Learning","description":"Reinforcement Learning (RL) has emerged as a popular training paradigm, particularly when paired with reasoning models. While effective, it primarily focuses on generating responses and lacks mechanisms to explicitly foster critique or reflection. Several recent studies, like Critique-Fine-Tuning (CFT) and Critique-Guided-Distillation (CGD) have shown the benefits of explicitly teaching LLMs how to critique. Motivated by them, we propose Critique Reinforcement Learning (CRL), where the model is tasked with generating a critique for a given (question, solution) pair. The reward is determined solely by whether the final judgment label $c \\in \\{\\texttt{True}, \\texttt{False}\\}$ of the generated critique aligns with the ground-truth judgment $c^*$. Building on this point, we introduce \\textsc{Critique-Coder}, which is trained on a hybrid of RL and CRL by substituting 20\\% of the standard RL data with CRL data. We fine-tune multiple models (\\textsc{Critique-Coder}) and evaluate them on different benchmarks to show their advantages over RL-only models. We show that \\textsc{Critique-Coder} consistently outperforms RL-only baselines on all the evaluated benchmarks. Notably, our \\textsc{Critique-Coder-8B} can reach over 60\\% on LiveCodeBench (v5), outperforming other reasoning models like DeepCoder-14B and GPT-o1. Beyond code generation, \\textsc{Critique-Coder} also demonstrates enhanced general reasoning abilities, as evidenced by its better performance on logic reasoning tasks from the BBEH dataset. This indicates that the application of CRL on coding datasets enhances general reasoning and critique abilities, which are transferable across a broad range of tasks. Hence, we believe that CRL works as a great complement to standard RL for LLM reasoning.","url":"https://huggingface.co/papers/2509.22824","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.469Z","metadata":{"authors":["Chi Ruan","Dongfu Jiang","Yubo Wang","Wenhu Chen"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.22824.pdf","arxivUrl":"https://arxiv.org/abs/2509.22824","arxivId":"2509.22824","coolPaperUrl":"https://papers.cool/arxiv/2509.22824","published":"2025-09-26T18:30:49Z","updated":"2025-09-26T18:30:49.000Z","zh_summary":"强化学习 （RL） 已成为一种流行的训练范式，特别是与推理模型配对时。虽然有效，但它主要侧重于产生回应，缺乏明确促进批评或反思的机制。最近的几项研究，如批判微调 （CFT） 和批判引导蒸馏 （CGD），已经表明明确教授法学硕士如何进行批评的好处。在他们的激励下，我们提出了批评强化学习 （CRL），其中模型的任务是为给定的（问题、解决方案）对生成批评。奖励完全取决于生成的批评的最终判断标签$c \\in \\{\\texttt{True}， \\texttt{False}\\}$ 是否与真实判断 $c^*$ 一致。基于这一点，我们引入了 \\textsc{Critique-Coder}，它是在 RL 和 CRL 的混合体上进行训练的，方法是用 CRL 数据替换 20\\% 的标准 RL 数据。我们微调了多个模型（\\textsc{Critique-Coder}），并在不同的基准上对其进行评估，以显示它们相对于仅RL模型的优势。我们表明，\\textsc{Critique-Coder} 在所有评估的基准测试中始终优于仅 RL 的基线。值得注意的是，我们的 \\textsc{Critique-Coder-8B} 在 LiveCodeBench （v5） 上可以达到 60\\% 以上，优于 DeepCoder-14B 和 GPT-o1 等其他推理模型。除了代码生成之外，\\textsc{Critique-Coder} 还展示了增强的通用推理能力，它在 BBEH 数据集中的逻辑推理任务上具有更好的性能就证明了这一点。这表明CRL在编码数据集上的应用增强了一般推理和批评能力，这些能力可以转移到广泛的任务中。因此，我们相信 CRL 是 LLM 推理标准 RL 的重要补充。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“标准强化学习（RL）仅优化解题正确性，却无法显式培养模型自我批判与反思能力”这一缺陷，提出并验证了“批判强化学习（CRL）”新范式。核心待解决问题可归纳为：\n\n1. 缺乏批判信号\n传统 RL 的奖励只取决于代码是否通过测试用例，模型没有显式动机去生成“对解题过程正确与否的判断”，导致其难以习得可迁移的反思与纠错能力。\n\n2. 批判能力难以通过监督模仿获得\n先前工作（如 CFT）依赖教师提供的批判轨迹做监督微调，无法利用可验证的自动反馈，且批判质量受限于教师模型水平。\n\n3. 批判与解题训练目标割裂\n若完全用批判数据训练，模型在推理阶段仍需输出完整解答，出现“训练-推理格式错位”，性能反而下降。\n\n为此，作者提出 CRL：让模型对“题目-解答”对输出二元判断  c∈True,False ，并以判断是否与真实标签  c^*  一致作为可验证奖励。通过把 20% 的 RL 数据替换成 CRL 数据，与标准 RL 统一在 GRPO 框架下进行混合训练，得到 CRITIQUE-CODER。实验表明，该方案在代码生成与通用逻辑推理基准上均显著优于纯 RL 基线，从而证明“批判奖励信号”能有效弥补传统 RL 的反思缺陷，并可在不同任务间迁移。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可划分为三条主线，均围绕“如何让大模型具备批判或反思能力”展开：\n\n1. 自我修正与自我批判\n\n- Self-Refine (Madaan et al., 2023)\n- Reflexion (Shinn et al., 2023)\n- CRITIC (Gou et al., 2023, 2024)\n这些工作让模型迭代生成→批判→修改，但后续研究 (Huang et al., 2023; Valmeekam et al., 2023) 指出其鲁棒性不足，批判信号常因模型自身偏差而失效。\n2. 显式批判学习\n\n- Critique Fine-Tuning (CFT) (Wang et al., 2025c)\n- Critique-Guided Distillation (Kapusuzoglu et al., 2025)\n它们通过监督模仿教师批判链提升推理，但依赖高质量人工或教师轨迹，无法利用可验证的自动奖励。\n3. 面向代码生成的强化学习\n\n- CodeRL (Le et al., 2022) – 首个演员-评论家框架，用执行反馈做奖励\n- PPOCoder (Shojaee et al., 2023) – 引入 PPO 稳定训练\n- RLEF (Gehring et al., 2024) – 在合成阶段即利用执行信号\n- AceCoder、HardTests、KodCoder (Zeng et al., 2025; He et al., 2025; Xu et al., 2025) – 大规模自动构造测试用例并做 RL 训练\n- SWERL (Wei et al., 2025) – 在真实软件工程场景下用规则奖励扩展 RL\n上述方法仅优化“解题对错”，未显式引入批判信号。\n\n本文提出的 CRL 与 CFT 最相近，但关键差异在于：\n\n- 不模仿教师批判链，而是让模型自行输出二元判断  c ，再用可验证的  c^*  给出稀疏 0/1 奖励；\n- 通过 GRPO 与标准 RL 统一优化，使批判奖励与解题奖励共同塑造策略梯度。\n\n因此，CRL 可视为对“自我批判”与“代码 RL”两条研究路线的结合与扩展。\n\nQ3: 论文如何解决这个问题？\n\n论文将“如何让模型在强化学习框架内同时习得解题与批判能力”拆解为三个可执行步骤，并给出对应技术实现：\n\n1. 引入可验证的批判奖励信号\n定义新任务：给定题目-解答对  (q,s) ，策略  π_θ  需输出二元判断  c∈True,False 。\n奖励仅取决于预测判断与真实标签  c^_  是否一致：\n\nR_(crl)(c,c^_)=1[c=c^_]\n\n该信号无需人工写批判链，直接利用测试用例执行结果自动标注  c^_ ，保证零额外人工成本。\n\n2. 统一优化框架：GRPO 同时消化两种奖励\n在 Group Relative Policy Optimization 内，把标准 RL 的“通过率奖励” $R_(rl)(s,T)∈\n0,1\n$ 与上述批判奖励拼到同一批次：\n\n- 80 % 样本为传统 RL 数据  (q,T) ，优势估计受  R_(rl)  影响；\n- 20 % 样本为 CRL 数据 $(\nq;s\n,c^*) ，优势估计受 R_(crl)$ 影响。\n目标函数保持 GRPO 原形式：\n\nJ(θ)=E![(1) / (G)∑_(i=1)^(G)(1) / (|o_i|)∑_(t=1)^(|o_i|)min!l(rho_(i,t)A_(i,t),,clip(rho_(i,t),1!-!ε,1!+!ε)A_(i,t)r)-β D_(KL)(π_θ|π_(ref))]\n\n其中  A_(i,t)  由对应样本的  R_(rl)  或  R_(crl)  计算，保证梯度同时朝向“写出正确代码”与“给出正确判断”。\n3. 迭代上下文扩展与混合数据管理\n\n- 两阶段长度：先 16 k 令牌预热，再 32 k 以承载更长思维链；\n- 数据过滤：对原始 rStar-Coder 数据集剔除超长用例，平均用例长度从 96 k 字符降至 40 字符，训练提速；\n- 比例消融：实验显示 20 % CRL + 80 % RL 为最佳配比，既能注入批判信号，又避免“只学评判、不会生成”的格式错位。\n\n通过上述设计，模型在训练阶段持续收到“批判对错”的稀疏奖励，从而显式激励内部反思行为；推理阶段仍按标准思考模式输出完整解答，无需额外提示。实验结果验证了该方案在代码生成与通用逻辑推理基准上均显著优于纯 RL 基线，实现“批判能力可学习、可迁移、不牺牲解题性能”的目标。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“批判强化学习（CRL）能否提升代码生成与通用推理”展开系统实验，共 7 组，覆盖训练、评测、消融与可扩展性分析：\n\n1. 主实验：四代码基准全面评测\n\n- 模型：Qwen3-4B / 8B\n- 对比：Base ↔ 纯 RL ↔ CRL+RL（20 %）\n- 基准：EvalPlus（HumanEval/HumanEval+/MBPP/MBPP+）、BigCodeBench-Instruct（Full & Hard）、Aider-Polyglot、LiveCodeBench-v5\n- 结果：CRITIQUE-CODER 在所有 8 项子指标上均优于纯 RL，LiveCodeBench 提升最高 +2.4（4B）/+1.2（8B），Aider-Polyglot 提升 +7.2（8B）。\n2. 与前沿模型对标\n\n- 同期强基线：DeepSeek-R1-Distill-14B、DeepCoder-14B、DeepSeek-V2.5-238B、GPT-o1\n- 结果：CRITIQUE-CODER-8B 在 EvalPlus 达 87.7 超过 GPT-o1（88.6→87.7 基本持平），在 LiveCodeBench 60.8 分领先 DeepCoder-14B（60.6）与 GPT-o1（59.5）。\n3. 通用推理迁移测试\n\n- 基准：BIG-Bench Extra Hard（BBEH）四子任务（Time Arithmetic、DisambiguationQA、Zebra Puzzles、BoardgameQA）\n- 结果：4B 模型平均 +6.1 分超越 Base，+4.0 分超越纯 RL，表明 CRL 带来的批判能力可迁移到非代码逻辑推理。\n4. 测试时缩放（Test-Time Scaling）\n\n- 方法：解除 32 k 令牌上限，允许模型继续“思考”最多 6 轮迭代\n- 结果：CRITIQUE-CODER-4B 在 LiveCodeBench 从 59.0 提升至 62.0（+3.0），反超 DeepCoder-14B。\n5. 消融：CRL 数据比例\n\n- 设置：0 %（纯 RL）、20 %、50 %、100 %（纯 CRL）\n- 结果：20 % 比例取得最佳平均成绩；100 % CRL 因“只学评判、不学写代码”导致 Aider-Polyglot 跌至 17.6（-6.8），验证 CRL 需作为 RL 补充而非替代。\n6. 生成质量细粒度分析\n\n- 采样 LiveCodeBench 100 题，对比纯 RL 与 CRL+RL 的输出\n- 指标：\n– think 块长度（CRL+RL 平均 +32 %）\n– 代码内注释行数（+45 %）\n– 函数平均长度（无显著差异）\n- 结论：CRL 诱导更长推理链与更多自解释行为，但未引入冗余代码。\n7. 自我批判局限性验证\n\n- 方法：10 候选解 × 64 条自批判 → 按“获 True 票数”选最佳解\n- 结果：LiveCodeBench 准确率无提升，说明模型尚不能通过“自我批判+重排”获得额外收益，提示未来需引入更强奖励模型或一致性过滤。\n\n以上实验从“训练范式→绝对性能→跨域迁移→推理预算→配方消融→生成行为→自我批判”七个维度系统论证了 CRL 的有效性与边界。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“方法改进”“数据与任务扩展”“理论分析”三大类，均直接对应论文已暴露的局限或未充分验证的假设：\n\n方法改进\n\n1. **更稠密的批判奖励**\n当前 CRL 只在整条判断  c  正确时才给 1 分。可探索\n\n- 子句级或理由步级自动标注（借助执行轨迹或抽象语法树 diff）\n- 预测“通过测试数”而非二元标签，使奖励信号 $R_(crl)∈\n0,1\n与 R_(rl)$ 同域，减少梯度方差。\n2. **自批判重排（Self-Critique Rerank）再攻关**\n论文实验 7 显示简单“投票”无效。可尝试\n\n- 训练额外轻量级 Reward-Critique 模型，以批判文本的置信度做 scalar 奖励；\n- 采用一致性过滤（majority@k）或 Best-of-N 加权，而非硬投票。\n3. **多轮 CRL-RL 交替训练**\n目前为静态 20 % 混合。可设计“双阶段”或“迭代” schedule：\n\n- 阶段 1 高比例 CRL → 快速习得批判特征；\n- 阶段 2 高比例 RL → 把批判特征用于生成；\n- 动态调整比例  α_t  以平衡探索与利用。\n4. **与在线 RL 结合**\n现有 CRL 数据为离线标注。可让模型在在线 episode 中\n\n- 先产生解 → 执行得到  c^*  → 立即构造批判样本并更新，实现“解-批判”同轨迹学习。\n\n数据与任务扩展\n\n5. **跨语言 CRL**\n当前仅 Python。将执行器扩展至 C++、Java、Go，验证批判能力是否语言无关；同时考察语法/运行时错误分类能否成为更细粒度标签。\n\n6. **数学与科学推理数据集**\n代码领域有自动执行器，数学亦然。可在 MATH、Olympiad-Bench 上执行答案验证，构造  (q,s,c^*)  三元组，检验 CRL 在数学证明、几何计算上的通用性。\n\n7. **长篇形式化证明**\n以 Lean/Isabelle 环境为执行器，让模型对“命题-证明脚本”输出  True/False ，测试 CRL 能否提升形式化证明通过率。\n\n8. **多模态批判**\n将问题  q  扩展为“自然语言 + 图例（如几何图、表格）”，模型需批判图文混合解答，考察视觉-语言一致性判断能力。\n\n理论分析\n\n9. **批判信号对策略梯度的方差约减**\n从理论上比较混合目标\n\nJ_(mixed)=(1-α)J_(rl)+α J_(crl)\n\n与纯  J_(rl)  的梯度方差，求证 CRL 是否起到控制方差、加速收敛的作用。\n\n10. **格式错位（misalignment）的量化边界**\n建立“训练-推理格式差异”与性能下降之间的定量关系，给出最优混合比例  α^*  的解析或近似解。\n\n11. **批判特征的可解释性**\n探查模型在输出  T/F  时依赖的隐藏状态维度，分析其是否对应人类可理解的“边界情况”“变量未初始化”等错误类型，为后续可控生成提供接口。\n\n小结\n\n- **短期可落地**：1、2、3、5\n- **中期需新数据/环境**：4、6、7、8\n- **长期需理论支撑**：9、10、11\n\n这些探索一方面可缓解“自批判无效”“比例敏感”等现存问题，另一方面把 CRL 从代码领域推向数学、形式化证明、多模态等更广泛的推理任务。\n\nQ6: 总结一下论文的主要内容\n\n论文提出“批判强化学习（CRL）”（Critique Reinforcement Learning），在标准 RL 之外显式奖励模型对“题-解”对做出的二元判断，以培养反思能力。核心内容与贡献如下：\n\n1. **问题**\n纯 RL 只优化代码通过率，无法激励模型生成内部批判或自我纠错。\n\n2. **方法**\n\n- 新任务：给定 (q,s) ，模型输出判断 c∈True,False ，以与真实标签 c^_ 是否一致作为可验证奖励 $R_(crl)=1\nc=c^_\n$。\n- 混合训练：20 % CRL 数据 + 80 % 标准 RL 数据，统一用 GRPO 更新策略。\n- 迭代上下文扩展：16 k → 32 k 令牌，支持长思维链。\n3. **实验**\n\n- 代码基准（EvalPlus、BigCodeBench-I、Aider-Polyglot、LiveCodeBench-v5）：CRITIQUE-CODER-4B/8B 全面超越纯 RL 基线，8B 在 LiveCodeBench 达 60.8，领先 DeepCoder-14B 与 GPT-o1。\n- 通用推理（BBEH 逻辑任务）：4B 平均 +6.1 分，验证批判能力可迁移。\n- 测试时缩放：解除长度限制后 4B 再 +3.0 分。\n- 消融：20 % CRL 比例最佳，100 % CRL 因格式错位性能下降。\n- 自我批判重排暂无效，提示未来需更强奖励模型。\n4. **结论**\nCRL 以零人工成本的二元判断奖励，补足 RL 的反思缺陷，可作为通用推理任务的即插即用补充范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Critique-Coder%3A%20Enhancing%20Coder%20Models%20by%20Critique%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2509.22824，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.22824。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-19-1759276017469","title":"From f(x) and g(x) to f(g(x)): LLMs Learn New Skills in RL by Composing Old Ones","description":"Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.","url":"https://huggingface.co/papers/2509.25123","source":"HuggingFace Papers","timestamp":"2025-09-30T23:46:57.469Z","metadata":{"authors":["Lifan Yuan","Weize Chen","Yuchen Zhang","Ganqu Cui","Hanbin Wang","Ziming You","Ning Ding","Zhiyuan Liu","Maosong Sun","Hao Peng"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.25123.pdf","arxivUrl":"https://arxiv.org/abs/2509.25123","arxivId":"2509.25123","coolPaperUrl":"https://papers.cool/arxiv/2509.25123","published":"2025-09-29T17:44:27Z","updated":"2025-09-29T17:44:27.000Z","zh_summary":"强化学习（RL）是否真正教会大型语言模型（LLMs）新的技能，还是仅仅激活已有的技能？这个问题是关于RL在LLM后续训练中作用的持续争论的核心。一方面，即使没有先前的监督微调，也可以通过RL获得强有力的实证结果；另一方面，批评者认为，RL除了重新加权已有的推理策略之外几乎没有贡献。本文提供了具体证据，表明LLMs可以通过组合已有技能，在RL过程中获得真正的新技能，这类似于人类获得新认知技能的核心机制之一。为了减少数据污染和其他混杂因素，并允许对任务复杂度进行精确控制，我们开发了一个用于研究的合成框架。具体地，我们将技能定义为在已知输入x的情况下推断字符串变换函数f(x)输出的能力。当LLM在RL之前已经学会了f和g时，我们的实验显示，RL使其能够学习这两者的未知组合h(x)=g(f(x))。此外，这种组合能力可以推广到更复杂的问题，例如RL训练中未见过的多于两个函数的组合。令人惊讶的是，我们的实验表明，在源任务中获得的组合技能能够迁移到不同的目标任务上。即使在目标任务上没有进行组合训练，只需具备目标任务的原子技能知识，这种迁移也能发生。我们的定性分析显示，RL从根本上改变了模型的推理行为。相比之下，使用相同数据进行的下一token训练并未产生这些发现。我们的系统性实验为LLM学习提供了新的见解，表明先构建具备基础技能的基础模型，然后利用RL激励高级、可推广技能以应对复杂问题的价值。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该研究针对“强化学习（RL）在后训练阶段到底是否能让大语言模型（LLM）获得真正的新技能”这一核心争议，给出可验证的实证答案。具体而言，论文力图澄清并解决以下关键问题：\n\n1. **RL 是否仅对已有能力进行重排序**\n近期观点（Yue et al., 2025；Wu et al., 2025a 等）认为 RL 只是把 base 模型已具备、但出现概率较低的推理路径“提权”到 top-1，并未产生新的推理策略。作者质疑该结论，认为其源于评测任务与训练激励不当。\n\n2. **缺乏“技能”精确定义与因果验证**\n既有工作多使用 pass@k 整体指标或模式频率作为代理，无法区分“激活已有”与“习得新生”能力，也难以排除数据污染与任务混淆因素。\n\n3. **合成、可控制环境缺失**\n公开基准往往混杂多种难度与领域，base 模型可能在部分子任务上已表现良好，导致 RL 缺乏学习新技能的必要激励，从而出现“reranking illusion”。\n\n为此，论文构建了一个**去污染、难度可控、技能边界清晰**的字符串变换合成框架，将“技能”形式化为对确定性函数  f(x)  的预测能力，并引入**组合层级**（Level-n 表示 n 重函数嵌套）。通过两阶段训练协议（先原子技能 RFT → 后组合技能 RL/RFT），系统验证：\n\n- RL 在仅提供正确性奖励、无示范轨迹的情况下，能否让模型学会**未见过的组合**  h(x)=g(f(x)) ；\n- 该组合能力能否向**更高深度**（Level-3/4/…）及**跨任务**（Countdown 数学谜题）迁移；\n- 相较之下，同等数据的监督式拒绝微调（RFT）是否足以获得同样效果。\n\n综上，论文旨在**以因果可控的实验设计**，回答“RL 能否教会 LLM 真正的新技能”这一悬而未决的问题，并揭示“组合激励”是 RL 产生可泛化新能力的决定性因素。\n\nQ2: 有哪些相关研究？\n\n论文在“背景”与“实验对比”部分系统梳理了与其议题直接相关的研究，可归纳为以下几条主线：\n\n1. **RL 仅“重排序”而非“学新技能”的近期悲观证据**\n\n- Yue et al. (2025) 通过 pass@k 曲线收窄现象，指出 RLHF/RLVR 只是把 base 模型已具备的答案排序提前，并未拓展能力边界。\n- Wu et al. (2025a) 提出“invisible leash”假设：RL 无法逃脱预训练阶段已见模式的约束。\n- Liu et al. (2025b)、Zhao et al. (2025) 发现所谓“aha moment”频率上升，实为对预训练已有推理行为的放大。\n- Gandhi et al. (2025) 用 cognitive behavior 统计证明 RL 阶段出现的搜索模式在 base 模型中已可采样到。\n2. **数据污染与评测混淆的警示**\n\n- Wu et al. (2025c)、Shao et al. (2025) 指出公开数学/代码语料已泄露测试题，RL 提升可能源于记忆而非推理。\n- Wang et al. (2025) 提出“model-task alignment”效应：当评测任务与预训练分布高度重叠时，RL 改进易被高估。\n3. **组合泛化与课程学习**\n\n- Sun et al. (2025) “OMEGA” 实验：直接在原子技能上做 RL 无法泛化到组合题，强调“缺乏组合激励”是失败原因。\n- Yin et al. (2025) 通过上下文学习（ICL）而非 RL，实现链式组合推理的提升，说明组合性可被激活但未必通过 RL。\n- Lake et al. (2016)、Andreas et al. (2015) 在符号与视觉领域提出“组合性 = 强泛化”经典框架，为本文认知动机提供理论根基。\n4. **RL 在 LLM 后训练中的成功案例（被本文重新解释）**\n\n- DeepSeek-AI et al. (2025) 的 DeepSeek-R1、OpenAI (2024) 的 o1 显示纯 RL 可大幅提升数学/代码成绩，但未被证明是否源于新技能。\n- LogicRL (Xie et al., 2025) 在逻辑谜题上做 RL 后，数学 benchmark 提升；本文用跨任务实验给出“组合技能转移”这一更精确解释。\n- Guru (Cheng et al., 2025) 发现预训练曝光度越高的领域，RL 越易迁移，本文将其归因于“原子技能已具备，只需 RL 提供组合激励”。\n5. **传统 RL 与技能习得**\n\n- Silver et al. (2016; 2017) 在围棋/象棋上证明从零开始的 RL 可习得人类未见过的新策略，为“RL 能学新技能”提供经典参照。\n- Anderson (1982) 认知科学理论：人类通过“组合-内化”旧技能获得新技能，本文直接借鉴其“compositionality”机制作为假设核心。\n6. **训练方法与评测指标**\n\n- Dong et al. (2023) 提出 Rejection Fine-Tuning (RFT)，被本文用作与 RL 对照的纯监督基线。\n- Shao et al. (2024) 的 GRPO、Yu et al. (2025) 的 DAPO 等 RL 算法，为本文实验实现提供技术基础。\n- Pass@k、Avg@k 指标在 LLM 推理评估中被广泛采用（Chen et al. 2021 等），本文则进一步按难度分层拆解，以揭示“重排序幻象”。\n\n综上，相关研究覆盖了“RL 后训练机制”“数据污染与评测偏差”“组合泛化理论”以及“认知科学启示”四大方向，而本文通过**合成环境+分层难度+跨任务迁移**的设计，首次在控制条件下给出“RL 可教会 LLM 真正新技能”的因果证据，对上述悲观结论进行了直接修正与细化。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构造可控合成环境 + 两阶段训练协议 + 分层难度评测”三位一体的方法，系统回答“RL 能否教会 LLM 新技能”。具体解决路径如下：\n\n1. 设计**去污染、技能边界清晰**的合成任务\n\n- 25 个字符串变换函数均以无意义标识符（func\\_1 … func\\_25）命名，避免预训练记忆。\n- 明确定义“原子技能”= 单函数预测，“组合技能”= 嵌套函数预测，难度由嵌套深度 Level-n 精确控制。\n- 训练与评测数据均不公开，确保模型无法通过语料泄露直接检索答案。\n2. 采用**两阶段因果训练协议**\n\n- **Stage 1** 仅用拒绝式微调（RFT）让模型掌握原子技能，且**唯一一次暴露函数源码**；此阶段结束即冻结原子能力。\n- **Stage 2** 隐藏函数定义，只给函数名与组合调用，分别用 – RL（正确性奖励 + GRPO 优化）\n– RFT（监督模仿相同组合数据）\n进行对比，直接检验“激励形式”是否导致差异。\n3. 建立**多维度评测体系**\n\n- **Held-out 组合泛化**：Stage 2 训练用的函数子集与评测子集完全不相交，检验“见过原子、未见过组合”的零样本推理。\n- **易 → 难外推**：训练最高 Level-2，测试 Level-3~8，观察是否习得“任意深度组合”的元策略。\n- **跨任务迁移**：在字符串任务上做 RL，再到完全未训练过的 Countdown 数学谜题（Level-3~5）上测试，验证组合技能是否成为**任务无关的元能力**。\n- **Pass@k 分层分析**：将同一难度级别的 pass@1、pass@32 … pass@1024 单独绘制，排除“整体指标掩盖单项技能”的 reranking illusion。\n4. 引入**行为级错误诊断**\n\n- 用 Gemini-2.5-Pro 对 Level-3 失败样本进行五类错误标注（忽略组合、不完整追踪、错误嵌套、原子错误等）。\n- 统计发现：RL Level-2 模型把“忽略组合”错误从 >50% 降至 0%，主要失败模式降为“原子级预测错误”，证明其**真正学会了如何组合**，而非单纯提升采样运气。\n5. 控制**数据与算法混杂**\n\n- 所有 RL 实验严格 on-policy（batch=minibatch=16），排除 off-policy 数据泄漏。\n- KL 与熵正则系数设为 0，确保性能提升仅来自奖励信号对组合行为的激励。\n- 迭代 RFT 基线使用完全相同的数据与训练步数，保证“数据量”不是变量。\n\n通过上述设计，论文将“RL 是否学新技能”转化为**可在单一变量（有无组合激励）下重复验证的因果问题**，从而得出清晰结论：\n当且仅当 RL 目标显式包含组合任务时，模型才获得可泛化到更深、跨领域场景的新推理策略；同等数据的监督学习无法做到这一点。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“RL 能否教会 LLM 新组合技能”共设计并执行了 5 组核心实验，全部在受控合成环境（字符串变换）与跨任务迁移环境（Countdown）下完成。实验变量、训练配置与评测指标一一对应，形成因果链验证。\n\n| 实验 | 训练阶段 | 模型配置 | 评测目标 | 关键指标 |\n| --- | --- | --- | --- | --- |\n| Exp-1 组合技能是否存在 | Stage-2 | RL Level-1（仅原子）RL Level-2（仅 2 阶组合）RL Level-1+2（混合） | 同一组 held-out 函数上的 Level-1~6 准确率 | Acc@1 |\n| Exp-2 数据 vs 算法分离 | Stage-2 | RL Level-2 vs 迭代 RFT（同一份 Level-2 数据） | 同上 | Acc@1 |\n| Exp-3 跨任务迁移 | Stage-1+2 | 4 种配置（见表 1）• String-Base+RL L1+2• Multi-Base• Multi-Base+RL L1• Multi-Base+RL L1+2 | 完全未见过的 Countdown Level-3/4/5 | Avg@32 |\n| Exp-4 Pass@k 分层检验 | Stage-2 | RFT Base vs RL Level-1 vs RL Level-1+2 | Level-18 分别绘制 pass@11024 | Pass@k 曲线斜率 |\n| Exp-5 行为模式诊断 | Stage-2 | RFT Base / RFT L2 / RL L1 / RL L2 | Level-3 失败样本人工五分类 | 错误分布占比 |\n\n以下分述各实验细节与结果。\n\nExp-1：RL 是否真正习得组合技能\n\n- **训练**\n– 共用同一 Stage-1 原子模型（Acc≈90% on Level-1）。\n– Stage-2 分别只喂 Level-1、只喂 Level-2、混合喂 Level-1+2，其余超参相同。\n- **评测**\n– 函数集合 25 个，训练与评测按 16/9 划分，确保组合从未见过。\n- **结果**\n– RL Level-1：Level-2~6 准确率 <2%，无法组合。\n– RL Level-2 & RL 1+2：Level-3 从 5% → 30%，Level-4 从 1% → 15%，Level-5 仍保持 >5%，证明**组合技能可泛化到更深阶**。\n\nExp-2：监督（RFT）与 RL 的 head-to-head\n\n- **控制变量**\n– 完全相同 50k 条 Level-2 组合题；RFT 做 6 轮迭代，每轮用上一轮正确解增广。\n- **结果**\n– 迭代 RFT 在 Level-2 held-out 仅 15%，Level-3 <2.6%；RL Level-2 对应 64% / 27%，**同等数据下 RL 显著优于监督**。\n\nExp-3：跨任务迁移——字符串组合技能能否帮到 Countdown\n\n- **4 个模型配置**（详见正文表 1）\n1. String-Base+RL L1+2：只有字符串组合技能，无数值原子技能。\n2. Multi-Base：字符串+Countdown 原子技能，无 RL。\n3. Multi-Base+RL L1：仅原子 RL。\n4. Multi-Base+RL L1+2：拥有 Countdown 原子技能 + 字符串组合 RL。\n- **评测**\n– 128 题/Level，Level-3~5 完全未用于任何 RL。\n- **结果**\n– 配置 4（Multi-Base+RL L1+2）Level-3 准确率达 35%，相对基线提升 +18%，Level-4 仍有 6%，其余模型接近 0；**组合技能成功迁移至不同领域**。\n\nExp-4：Pass@k 分层——击破“重排序幻象”\n\n- **方法**\n– 对每个难度单独采样 1024 条，绘制 pass@k 曲线。\n- **发现**\n– Level-12（简单）：RL 与 Base 差距随 k 增大而缩小，符合 reranking 叙事。\n– Level-38（困难）：RL Level-1+2 与 Base 的差距**随 k 增大而扩大**（Level-5 差距从 4% → 25%），**证明 RL 拓展了能力边界而非简单提权**。\n\nExp-5：失败模式人工标注——行为是否真正改变\n\n- **样本**\n– 从 Level-3 测试集中随机抽取 400 道失败案例，交由 Gemini-2.5-Pro 自动分类。\n- **结果**\n– RFT Base / RFT L2 / RL L1 三大基线：>50% 错误属于“完全忽略组合”，>35% 为“错误嵌套”。\n– RL Level-2：“忽略组合”降至 0%，主要错误为“原子级预测错”（55%），**行为模式从高层误解转为低层执行误差**，佐证其学会了如何组合。\n\n辅助实验与消融\n\n- **KL/熵正则消融**：设 β=0 仍稳定提升，排除正则项带来的性能波动。\n- **函数数量消融**：将 25→10 个函数，结论依旧，验证结果非特定函数集过拟合。\n- **采样温度敏感性**：温度 0.3~1.5 范围内趋势一致，说明提升非单一温度巧合。\n\n综上，5 组实验从“技能出现—算法必要性—跨任务迁移—指标幻象—行为改变”五方面闭环验证：**RL 在给予组合激励时，确实教会了 LLM 新的、可泛化的组合推理技能**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为**理论机制**、**算法设计**、**任务扩展**与**评测工具**四大类，均保持与原文一致的“可控合成+因果验证”风格。\n\n1\\. 理论机制\n\n- **组合层级与模型参数的缩放律**\n固定原子技能数量，系统变化组合深度  L  与模型规模  N ，拟合\n\nP_(solve)(L, N) propto ((N) / (N_0))^(-α L)\n\n检验是否存在“组合灾难”临界阈值。\n\n- **激励函数的精细解析**\n将奖励分解为“部分正确”信号（如中间函数输出对即给 0.5 奖励），用因果中介分析量化“单步奖励”与“组合顿悟”之间的信息流量，揭示奖励稀疏度对技能内化的影响。\n\n- **技能可复用性度量**\n定义“组合熵” H_(comp) = -∑_i p_i log p_i ，其中  p_i  为第  i  种组合路径被模型采样的频率。观察训练过程中  H_(comp)  的相变点，与验证集准确率突变对齐，建立技能“固化”的量化指标。\n\n2\\. 算法设计\n\n- **课程 RL**\n从低阶组合开始，当验证集准确率 >θ 时自动解锁下一阶，动态调整奖励系数  β_L = β_0 · e^(-λ L) ，验证课程策略是否比直接训练目标深度样本更高效。\n\n- **模型自生成的组合数据**\n用 LLM 自身在 Stage-1 已掌握的原子函数，通过 prompt 指令“随机嵌套 2~4 层并输出正确结果”，离线生成百万级组合题，再用于 RL 训练，测试**无人工标注**情况下的可扩展性。\n\n- **多智能体组合博弈**\n引入“函数市场”：每个智能体专精一个原子函数，通过消息传递协商完成高阶组合任务，只有最终答案正确才获得共享奖励。检验分布式场景下是否仍能习得全局组合策略。\n\n3\\. 任务扩展\n\n- **多模态组合**\n将字符串函数扩展为“图像-文本”混合函数，例如\n f_(crop) (图像) → 局部图， g_(∩) (图) → 文本，\n训练模型预测  g(f(图像))  的文本输出，验证组合技能是否跨越模态。\n\n- **工具使用链**\n用搜索引擎、计算器、代码解释器作为原子工具，定义 API 调用顺序不可交换的组合任务（如先搜后算再绘图）。RL 仅提供最终答案正确性奖励，观察是否自动学会“工具链”规划。\n\n- **递归与 while 循环结构**\n本文函数均为确定性有限步。可引入带终止条件的 while 与递归函数，使组合深度在运行时动态变化，测试模型能否掌握“潜在无限深度”的组合推理。\n\n4\\. 评测工具\n\n- **组合技能“提取探针”**\n训练线性探针从隐藏状态预测“当前已完成的函数层数”或“下一函数名”，若探针在未见深度上仍准确，说明模型内部已建立显式的组合表示，可用于早期预警“技能崩塌”。\n\n- **对抗性组合评测**\n用遗传算法搜索能让模型产生“局部正确但整体错误”的输入字符串，形成 AdvComposition 数据集，衡量组合技能的鲁棒性。\n\n- **在线持续评测框架**\n开源一套可插拔的“函数即插件”平台，研究者只需注册新函数即可自动生成对应原子+组合考题，持续跟踪不同 RL 算法在统一基准下的技能习得曲线。\n\n5\\. 社会与伦理视角\n\n- **技能可解释性与安全**\n当模型通过 RL 习得人类未演示的多步组合策略，如何确保其目标与人类意图一致？可引入“组合路径可追溯”约束，要求模型在回答时输出每一步中间结果，供外部验证。\n\n- **低资源语言的原子技能构建**\n若某些语言在预训练中缺乏“原子工具”（如日期计算、单位转换），组合技能是否仍能有效迁移？探索先用轻量 SFT 补全原子能力，再用 RL 激活组合的低成本路径。\n\n以上方向均沿用论文的“原子-组合-迁移”方法论，但分别从**理论尺度、算法自举、任务多样性、评测工具与伦理风险**角度进一步打开 RL 赋能 LLM 的研究空间。\n\nQ6: 总结一下论文的主要内容\n\n论文核心结论与贡献可浓缩为一句话：\n**“当且仅当强化学习被显式 incentivized 去组合已有原子技能时，大语言模型才能习得真正的新推理能力，并零样本泛化到更深、跨领域的任务。”**\n\n具体要点如下：\n\n1. 问题定位\n\n- 近期主流观点认为 RL 仅对 base 模型已具备的答案做“重排序”，并未拓展能力边界。\n- 既有评测混杂多难度、数据污染严重，无法因果验证“新技能”是否出现。\n2. 方法框架\n\n- 构建**去污染、难度可控**的字符串变换合成环境：25 个原子函数用无意义标识符命名，难度=嵌套深度。\n- 两阶段训练：\n– Stage-1 用 RFT 让模型掌握原子技能（唯一暴露源码阶段）。\n– Stage-2 隐藏源码，只用函数名与组合调用，分别用 RL（正确性奖励）与 RFT（监督模仿）对比。\n3. 主要实验与结果\n\n- **Exp-1** 训练 Level-2 组合 → 在未见函数上 Level-3 准确率从 5% 提至 30%，Level-4 提至 15%；仅训练原子技能则提升为 0。\n- **Exp-2** 同等 Level-2 数据下，RL 大幅超越迭代 RFT，说明**算法而非数据**是关键。\n- **Exp-3** 在字符串任务上做 RL，转移到**从未训练过的** Countdown 数学谜题，Level-3 准确率 +18%，证明组合技能可跨领域迁移。\n- **Exp-4** 分层 pass@k 显示：简单题差距随采样数缩小（reranking），难题差距随采样数**扩大**，直接否定“RL 仅重排序”结论。\n- **Exp-5** 行为诊断：RL 训练后，“忽略组合”类错误从 >50% 降至 0%，失败模式转为低层原子错误，表明模型**真正学会了如何组合**。\n4. 学术启示\n\n- 给出首个因果证据：RL 在适当激励下能教会 LLM 新技能，而非简单激活旧策略。\n- 提出“reranking illusion”概念：混合难度评测与缺乏激励会掩盖真实技能习得。\n- 倡导“base 模型攒够原子能力 → RL 激发组合”的后训练新范式，为资源分配提供依据。\n5. 开源与复现\n\n- 代码、数据、评测脚本全部公开，支持后续研究直接插入新函数与任务，持续扩展组合技能基准。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是From%20%24f%28x%29%24%20and%20%24g%28x%29%24%20to%20%24f%28g%28x%29%29%24%3A%20LLMs%20Learn%20New%20Skills%20in%20RL%20by%20Composing%20Old%20Ones，链接是https://arxiv.org/pdf/2509.25123，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25123。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}