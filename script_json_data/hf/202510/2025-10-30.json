{"timestamp":"2025-10-29T23:49:30.274Z","totalItems":20,"items":[{"id":"hf-0-1761781638489","title":"InteractComp: Evaluating Search Agents With Ambiguous Queries","description":"Language agents have demonstrated remarkable potential in web search and information retrieval. However, these search agents assume user queries are complete and unambiguous, an assumption that diverges from reality where users begin with incomplete queries requiring clarification through interaction. Yet most agents lack interactive mechanisms during the search process, and existing benchmarks cannot assess this capability. To address this gap, we introduce InteractComp, a benchmark designed to evaluate whether search agents can recognize query ambiguity and actively interact to resolve it during search. Following the principle of easy to verify, interact to disambiguate, we construct 210 expert-curated questions across 9 domains through a target-distractor methodology that creates genuine ambiguity resolvable only through interaction. Evaluation of 17 models reveals striking failure: the best model achieves only 13.73% accuracy despite 71.50% with complete context, exposing systematic overconfidence rather than reasoning deficits. Forced interaction produces dramatic gains, demonstrating latent capability current strategies fail to engage. Longitudinal analysis shows interaction capabilities stagnated over 15 months while search performance improved seven-fold, revealing a critical blind spot. This stagnation, coupled with the immediate feedback inherent to search tasks, makes InteractComp a valuable resource for both evaluating and training interaction capabilities in search agents. The code is available at https://github.com/FoundationAgents/InteractComp.","url":"https://huggingface.co/papers/2510.24668","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.489Z","metadata":{"authors":["Mingyi Deng","Lijun Huang","Yani Fan","Jiayi Zhang","Fashen Ren","Jinyi Bai","Fuzhen Yang","Dayi Miao","Zhaoyang Yu","Yifan Wu","Yanfei Zhang","Fengwei Teng","Yingjia Wan","Song Hu","Yude Li","Xin Jin","Conghao Hu","Haoyu Li","Qirui Fu","Tai Zhong","Xinyu Wang","Xiangru Tang","Nan Tang","Chenglin Wu","Yuyu Luo"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.24668.pdf","arxivUrl":"https://arxiv.org/abs/2510.24668","arxivId":"2510.24668","coolPaperUrl":"https://papers.cool/arxiv/2510.24668","published":"2025-10-28T17:35:54Z","updated":"2025-10-28T17:35:54.000Z","zh_summary":"语言代理在网页搜索和信息检索方面展现出了显著潜力。然而，这些搜索代理假设用户查询是完整且明确的，而这一假设与现实情况有所偏离——用户通常以不完整的查询开始，需要通过交互来澄清。然而，大多数代理在搜索过程中缺乏交互机制，现有的基准也无法评估这种能力。为了解决这一问题，我们引入了 InteractComp，一个旨在评估搜索代理是否能够识别查询中的歧义并在搜索过程中主动交互以解决问题的基准。遵循“易于验证，通过交互消除歧义”的原则，我们通过目标-干扰方法在9个领域构建了210个专家策划的问题，这种方法产生的真实歧义只能通过交互解决。对17个模型的评估显示出惊人的失败：尽管在完整上下文下的准确率为71.50%，但最佳模型的准确率仅为13.73%，暴露了系统性的过度自信，而非推理能力缺陷。强制交互产生了显著提升，展示了当前策略未能激发的潜在能力。纵向分析显示，交互能力在15个月内停滞不前，而搜索性能却提高了七倍，揭示了一个关键盲点。这种停滞，加上搜索任务固有的即时反馈，使 InteractComp 成为评估和训练搜索代理交互能力的宝贵资源。代码可在 https://github.com/FoundationAgents/InteractComp 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有搜索型语言智能体在**面对模糊查询时缺乏主动交互澄清能力**的问题。核心论点如下：\n\n- **现实用户查询往往不完整且存在歧义**，而当前搜索智能体普遍假设用户一次性给出完整、无歧义的查询，导致在真实场景中直接给出错误答案或浪费算力。\n- **现有基准无法评估交互澄清能力**：搜索基准（如 GAIA、BrowseComp）提前提供全部必要信息；交互基准（如 IN3、Tau-Bench）聚焦通用对话，缺乏可验证的搜索任务与客观对错信号。\n- **亟需新基准衡量“识别歧义→主动提问→再搜索”的闭环能力**，并揭示模型是“不会问”还是“不想问”。\n\n为此，作者提出 INTERACTCOMP 基准，通过“易验证、需交互才能消歧”的 210 条专家构造题，系统评估智能体能否在搜索过程中主动识别并解决查询歧义。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两条主线，并指出它们与 INTERACTCOMP 的差异。\n\n1. 搜索基准与智能体\n\n- 网页级搜索：BrowseComp（Wei et al., 2025）及其中文、多模态、增强变体\n- 工具增强：GAIA（Mialon et al., 2023）、WebWatcher（Geng et al., 2025）\n- 强化学习训练：R1-Searcher、Search-R1\n- 数据合成与工作流：WebSailor、WebExplorer、自设计/手工设计智能体\n**共同点**：假设查询已完整，无需澄清即可检索答案。\n2. 交互基准与智能体\n\n- 多轮协作推理：SWEET-RL/ColBench\n- 用户偏好挖掘：UserBench、UserRL\n- 隐式意图理解：IN3\n- 工具-用户协同：Tau-Bench\n**共同点**：聚焦通用对话或工具使用，缺乏可验证的搜索任务与客观对错信号。\n\n差异总结\n\n- INTERACTCOMP 首次把“搜索”与“交互澄清”耦合：任务答案客观可验证，歧义必须通过提问解决，且搜索成功/失败提供干净奖励信号，可直接用于强化学习训练。\n\nQ3: 论文如何解决这个问题？\n\n论文并未直接“解决”模型在模糊查询下不会主动提问的问题，而是**构建了一个可度量、可训练、可诊断的基准环境**，为后续研究与算法改进提供基础。具体做法分为三步：\n\n1. 构造诊断基准 INTERACTCOMP\n\n- 设计原则：“易验证、需交互才能消歧”\n- 目标-干扰物方法：选取一个冷门实体 A 与一个热门实体 B，仅用二者共享属性生成模糊问句，把 A 独有的区分属性隐藏为“用户私有上下文”。\n- 双重验证：\n– 完整性验证：问句+上下文必须唯一指向 A\n– 交互必要性验证：前五页 Google 结果无法直接给出答案，且 3 个强模型在 5 轮纯搜索下均答错\n- 产出 210 道英汉双语题，覆盖 9 个领域，答案 1–2 词即可验证。\n2. 建立可扩展的实验协议\n\n- 统一采用 ReAct 框架，支持三种配置：\n– Answer-only（纯回忆）\n– Answer+Search（可检索）\n– Answer+Search+Interact（可提问）\n- 模拟用户 responder：仅用隐藏上下文对智能体的 yes/no 问题返回 “yes / no / I don’t know”，保证反馈一致且可复现。\n- 提供干净奖励信号：答案与标准完全匹配即 1，否则 0，适合后续 RLVR 训练。\n3. 通过系统实验定位瓶颈\n\n- 17 个模型在“完整上下文”下可达 71.5% 准确率，但在允许交互的真实设置下最高仅 13.7%，证明**能力存在却因过度自信未被调用**。\n- 强制交互实验：把“至少问 8 次”作为硬性前提，GPT-5 准确率从 20% → 40%，表明**问题不是不会问，而是不愿问**。\n- 15 个月纵向对比：BrowseComp 性能提升 7 倍，INTERACTCOMP 几乎停滞，揭示**交互能力是业界盲点**。\n\n综上，论文通过“构建基准 + 设计可复现环境 + 诊断实验”三板斧，把“模糊查询下的主动澄清”转化为可量化、可训练的任务，为后续用强化学习或不确定性建模方法真正“解决”该问题提供了数据、协议与经验基线。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **“模型在模糊查询下能否主动交互澄清”** 这一核心问题，设计并执行了 **5 组系统化实验**，覆盖行为观测、能力消融、规模缩放、强制干预与纵向趋势。所有实验均在 **INTERACTCOMP 210 题** 上进行，统一采用 ReAct 框架，保证可比性。\n\n1\\. 主实验：17 模型端到端行为画像\n\n**目的**：量化当前 SOTA 在“可交互”设置下的真实表现与行为模式。\n**设置**：Answer + Search + Interact，最多 10 轮。\n**观测指标**：\n\n- 平均轮次（Round）\n- 提问利用率 IR（% 轮次使用 ask）\n- 准确率 Acc.\n- 校准误差 C.E.\n- 美元成本\n\n**关键结论**（Table 2）：\n\n- 最高 Acc 仅 **13.73 %**（GPT-5），多数模型 < 10 %。\n- 行为两极化：GPT-4o-mini 提问率 74 % 但 Acc 7.1 %；GLM-4.5 提问率 0.25 % 同样 7.1 %。\n- 高提问率模型校准误差显著更低（37 vs 84），说明**多问至少让模型更自知**。\n\n2\\. 三模式消融：定位瓶颈到底是“不会推理”还是“不会提问”\n\n**目的**：验证交互是否是唯一短板。\n**设置**（Table 3）：\n\n- Answer-only：纯内部知识\n- Search-only：可检索但不可提问\n- With-context：直接把隐藏上下文喂给模型（性能上限）\n\n**结果**：\n\n| 模式 | 最佳 Acc | 相对提升 |\n| --- | --- | --- |\n| Answer-only | 7.6 % | — |\n| Search-only | 9.5 % | +25 % |\n| With-context | 71.5 % | +13.8× |\n\n**结论**：\n\n- 推理与检索能力已足够，**缺的是“识别歧义→主动提问”环节**。\n- 交互缺口是主瓶颈，而非知识或搜索缺陷。\n\n3\\. 缩放实验：多给轮次能否自发多问？\n\n**目的**：检验模型是否因“轮次不够”而沉默。\n**设置**：在 50 题子集上把上限从 5 → 10 → 20 轮，其余自由。\n\n**结果**（Table 4 & Figure 3a）：\n\n- GPT-5 平均提问数仅从 1.14 → 1.90，Acc 14 % → 20 %。\n- Claude-Sonnet-4 提问数 0.16 → 0.78，Acc 几乎不变。\n\n**结论**：\n\n- **轮次不是瓶颈，过度自信才是**；模型即使有余量也选择提前作答。\n\n4\\. 强制交互实验：把“不愿问”改成“必须问”\n\n**目的**：区分“能力缺失”与“策略缺失”。\n**设置**：要求模型在作答前至少提问 {2,4,6,8,10} 次，否则回合强制结束。\n\n**结果**（Figure 3b）：\n\n- GPT-5 在“必须问 8 次”条件下 Acc 从 20 % → **40 %**（翻倍）。\n- GPT-4o-mini 问得太多反而 Acc 下降，说明**提问质量差异显著**。\n\n**结论**：\n\n- 一旦被迫收集信息，部分模型可立即受益，\\*\\* latent capability 存在\\*\\*。\n- 有效提问是独立技能，不同架构差距巨大。\n\n5\\. 15 个月纵向追踪：交互能力是否随时间改善？\n\n**数据**：2024-05 至 2025-09 期间 10 个模型版本在 BrowseComp 与 INTERACTCOMP 上的公开或自测分数。\n\n**结果**（Figure 1）：\n\n- BrowseComp 准确率从 10 % → 70 %（**7×**）。\n- INTERACTCOMP 始终徘徊在 **6 %–14 %**，无显著上升趋势。\n\n**结论**：\n\n- 行业全力优化检索与推理，**交互澄清维度被系统性忽视**，形成“盲点”。\n\n附：可靠性验证\n\n- **Responder 稳定性**：同一上下文-问题对重复 3 次，> 97 % 回答一致。\n- **Grader 一致性**：GPT-4o(t=0) 自动判分与人工抽检一致性 96 %。\n\n综上，实验从“行为观测 → 能力消融 → 规模探测 → 强制干预 → 历史趋势”五个层面，**既定位了瓶颈（过度自信而非能力缺失），也证明了该瓶颈可被强制解除**，为后续 RLVR 或不确定性导向训练奠定数据与协议基础。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为 **数据与任务扩展、模型算法、训练框架、评价指标、理论与应用** 五大类。\n\n1\\. 数据与任务扩展\n\n- **多轮开放问答**：现有 responder 仅允许 yes/no，可升级为“简短事实回复”或“支持多跳追问”，考察模型提出开放式问题的能力。\n- **多模态歧义**：引入图像、视频、图表作为隐藏上下文，测试视觉-语言联合澄清能力。\n- **动态干扰物**：在线插入新热门实体，使歧义随时间漂移，考察模型持续更新与自适应提问。\n- **用户模型个性化**：同一查询对不同用户有不同真实意图，引入用户画像与偏好，研究个性化澄清策略。\n- **跨语言歧义**：利用中英双语之外的小语种，验证低资源场景下的交互可迁移性。\n\n2\\. 模型算法\n\n- **不确定性驱动的提问策略**\n– 采用 **熵、互信息、BALD** 等指标作为提问启发，替代人工规则。\n– 结合 **贝叶斯深度网络** 或 **MC Dropout**，实时估计答案分布，决定何时停止提问。\n- **分层规划机制**\n– 上层 Planner 生成“信息缺口列表”，下层 Searcher & Asker 并行填补，减少重复提问。\n- **反问一致性检查**\n– 对同一事实从不同角度提问，利用 **逻辑一致性损失** 检测 self-contradiction，降低错误确认。\n- **检索-提问联合优化**\n– 将 **ask 动作与 search 查询** 共同建模为统一策略，避免二者竞争预算。\n\n3\\. 训练框架\n\n- **RLVR（可验证奖励 RL）**\n– 利用 INTERACTCOMP 的 0/1 答案奖励，训练专用策略模型；探索 **R1-Searcher** 式的搜索-提问共训。\n- **课程强化学习**\n– 从“已给部分上下文”逐渐到“零上下文”，让模型先学会简单澄清，再过渡到完全自主提问。\n- **自监督数据飞轮**\n– 用强制交互产生的高分轨迹做自动标注，迭代蒸馏到更小模型，实现 **低成本自我改进**。\n- **人类偏好对齐**\n– 收集“提问是否简洁、礼貌、高效”的人类偏好对，采用 **DPO/KTO** 对齐，减少冗余或冒犯式提问。\n\n4\\. 评价指标\n\n- **提问效率**\n– 引入 **EMR（Expected Marginal Reward）** 衡量每多一次提问带来的期望准确率提升，惩罚无效提问。\n- **用户负担**\n– 统计 **用户阅读字符数、平均等待轮次**，建立 **User Effort Score**，在准确率与体验之间做帕累托前沿。\n- **澄清覆盖率**\n– 自动抽取“关键属性”并计算 **F1-coverage**，衡量提问是否系统覆盖所有必要信息维度。\n- **鲁棒性测试**\n– 对上下文做 **对抗扰动**（同义词替换、数字微改），观察模型是否仍能识别歧义并正确追问。\n\n5\\. 理论与应用\n\n- **最小可辨识信息理论**\n– 形式化定义 **Minimum Clarification Information (MCI)**，量化解决特定歧义所需的最少比特数，为提问策略提供理论下界。\n- **人机协同界面**\n– 将提问策略与 **实时搜索界面** 结合，开发“可解释澄清条”，让用户可视化模型为何需要该信息。\n- **垂直领域落地**\n– 医疗诊断、法律条文、技术客服等高风险场景，部署交互式搜索助手，用 **仿真患者/客户** 评估安全性与合规性。\n- **多智能体辩论**\n– 引入 **Proposer** 与 **Verifier** 两个角色，一方提出候选答案，另一方负责提问质疑，通过辩论减少单一模型的过度自信。\n\n快速可行动作\n\n1. 在 INTERACTCOMP 上实现 **Uncertainty-Ask** 基线，与现有 ReAct 对比。\n2. 开源 **RL 训练 pipeline**，复现 BrowseComp→INTERACTCOMP 的奖励信号迁移。\n3. 举办 **Clarification Hackathon**，鼓励社区提交高效提问策略，推动指标与数据共同演进。\n\n通过以上探索，可逐步把“主动澄清”从当前的盲点转变为下一代搜索智能体的核心能力。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 INTERACTCOMP 基准，系统评估搜索智能体在**模糊查询下主动交互澄清**的能力，揭示并量化当前模型的系统性过度自信盲点。\n\n1. 问题定义\n现实用户查询常含歧义，需通过提问才能确定意图；现有搜索智能体假设查询完整，缺交互机制，且主流基准无法衡量“识别歧义→主动提问”能力。\n\n2. INTERACTCOMP 构建\n\n- 原则：易验证、需交互才能消歧\n- 方法：目标-干扰物设计，仅用二者共享属性生成模糊问句，把冷门实体独有属性隐藏为“用户上下文”\n- 规模：210 道英汉双语题，9 领域，答案 1–2 词可验证；经双重验证确保搜索无法直接解决\n3. 实验与发现\n\n- 17 个 SOTA 模型在“可交互”设置下最高仅 13.7 %；给出完整上下文后跃升至 71.5 %，证明**能力存在但未被调用**\n- 强制模型至少问 8 次，准确率翻倍至 40 %，确认**过度自信是主瓶颈**\n- 15 个月纵向对比：BrowseComp 性能提升 7 倍，INTERACTCOMP 停滞于 6–14 %，揭示交互能力成业界**盲点**\n4. 贡献\n① 首个面向搜索场景的交互澄清基准，带干净 0/1 奖励信号，适合 RLVR 训练\n② 大规模诊断证据：模型失败源于**策略不愿问**而非推理或检索缺陷\n③ 提供数据、协议与基线，推动未来研究把“主动提问”纳入智能体核心能力\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是InteractComp%3A%20Evaluating%20Search%20Agents%20With%20Ambiguous%20Queries，链接是https://arxiv.org/pdf/2510.24668，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24668。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1761781638490","title":"Tongyi DeepResearch Technical Report","description":"We present Tongyi DeepResearch, an agentic large language model, which is specifically designed for long-horizon, deep information-seeking research tasks. To incentivize autonomous deep research agency, Tongyi DeepResearch is developed through an end-to-end training framework that combines agentic mid-training and agentic post-training, enabling scalable reasoning and information seeking across complex tasks. We design a highly scalable data synthesis pipeline that is fully automatic, without relying on costly human annotation, and empowers all training stages. By constructing customized environments for each stage, our system enables stable and consistent interactions throughout. Tongyi DeepResearch, featuring 30.5 billion total parameters, with only 3.3 billion activated per token, achieves state-of-the-art performance across a range of agentic deep research benchmarks, including Humanity's Last Exam, BrowseComp, BrowseComp-ZH, WebWalkerQA, xbench-DeepSearch, FRAMES and xbench-DeepSearch-2510. We open-source the model, framework, and complete solutions to empower the community.","url":"https://huggingface.co/papers/2510.24701","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.490Z","metadata":{"authors":["Tongyi DeepResearch Team","Baixuan Li","Bo Zhang","Dingchu Zhang","Fei Huang","Guangyu Li","Guoxin Chen","Huifeng Yin","Jialong Wu","Jingren Zhou","Kuan Li","Liangcai Su","Litu Ou","Liwen Zhang","Pengjun Xie","Rui Ye","Wenbiao Yin","Xinmiao Yu","Xinyu Wang","Xixi Wu","Xuanzhong Chen","Yida Zhao","Zhen Zhang","Zhengwei Tao","Zhongwang Zhang","Zile Qiao","Chenxi Wang","Donglei Yu","Gang Fu","Haiyang Shen","Jiayin Yang","Jun Lin","Junkai Zhang","Kui Zeng","Li Yang","Hailong Yin","Maojia Song","Ming Yan","Peng Xia","Qian Xiao","Rui Min","Ruixue Ding","Runnan Fang","Shaowei Chen","Shen Huang","Shihang Wang","Shihao Cai","Weizhou Shen","Xiaobin Wang","Xin Guan","Xinyu Geng","Yingcheng Shi","Yuning Wu","Zhuo Chen","Zijian Li","Yong Jiang"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.IR","cs.LG","cs.MA"],"pdfUrl":"https://arxiv.org/pdf/2510.24701.pdf","arxivUrl":"https://arxiv.org/abs/2510.24701","arxivId":"2510.24701","coolPaperUrl":"https://papers.cool/arxiv/2510.24701","published":"2025-10-28T17:53:02Z","updated":"2025-10-28T17:53:02.000Z","zh_summary":"我们介绍 Tongyi DeepResearch，这是一种具代理能力的大型语言模型，专为长周期、深度信息搜索的研究任务而设计。为了激励自主深度研究行为，Tongyi DeepResearch 通过端到端训练框架开发，结合了代理中期训练和代理后期训练，实现了在复杂任务中可扩展的推理和信息搜索。我们设计了一个高度可扩展的数据合成流程，完全自动化，无需昂贵的人类标注，并支持所有训练阶段。通过为每个阶段构建定制环境，我们的系统能够在整个过程中实现稳定且一致的交互。Tongyi DeepResearch 拥有 305 亿个总参数，每个 token 仅激活 33 亿个参数，在多项具有代理能力的深度研究基准测试中实现了最先进的性能，包括 Humanity's Last Exam、BrowseComp、BrowseComp-ZH、WebWalkerQA、xbench-DeepSearch、FRAMES 和 xbench-DeepSearch-2510。我们开源了模型、框架及完整解决方案，以赋能社区。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何以完全开源、可复现的方式，训练出具备**长周期、深度信息搜寻与研究能力**的自主智能体（Deep Research Agent）”这一核心问题。具体而言，其关注以下痛点：\n\n1. 现有深度研究系统多为闭源，中间过程不可见，社区难以复现或改进。\n2. 传统大模型仅依赖预训练+指令微调，缺乏**面向研究任务的 agentic 先验**，导致在复杂多步推理与工具调用场景下表现次优。\n3. 高质量研究级数据稀缺，人工标注成本极高，难以支撑规模化训练。\n4. 真实环境交互昂贵、非平稳，直接用于全阶段训练会带来不稳定与不可控成本。\n\n为此，论文提出 Tongyi DeepResearch，通过“端到端 agentic 训练框架”统一**中训练（mid-training）与后训练（post-training）**，并配套**全自动化、可扩展的合成数据管线**与**分阶段定制环境**，首次在 30B/3.3B 激活参数规模下实现与顶级闭源系统比肩甚至超越的深度研究性能，且全部开源。\n\nQ2: 有哪些相关研究？\n\n与 Tongyi DeepResearch 直接相关的研究可归纳为以下四条主线，每条均给出代表性文献并指出其与本工作的关联与差异。\n\n1\\. 深度研究 / 自主浏览智能体\n\n- **OpenAI DeepResearch** (2025a)\n闭源标杆，首次展示多步搜索、浏览、合成报告能力；本工作对标其性能并全部开源。\n\n- **Gemini DeepResearch** (Gemini Team, 2025)\n谷歌闭源方案，强调多模态与长上下文；Tongyi 在纯文本基准上已超越。\n\n- **Kimi Researcher** (Kimi, 2025)\n国内闭源端到端 RL 训练尝试；Tongyi 提供可复现框架并引入 mid-training。\n\n- **WebThinker** (Li et al., 2025d)\n提出“推理+浏览”双循环，但仅做 prompting 级集成；Tongyi 将其能力内化为模型参数。\n\n2\\. Agent 训练策略与 RL\n\n- **GRPO** (Shao et al., 2024)\n群体相对策略优化，Tongyi 的 RL 目标函数在其基础上引入 clip-higher、leave-one-out 稳定技巧。\n\n- **DAPO** (Yu et al., 2025)\n大规模 LLM RL 系统，强调 token-level 策略梯度；Tongyi 借鉴其 token 级损失以提升样本效率。\n\n- **rLLM** (Tan et al., 2025)\n异步 rollout 框架；Tongyi 基于其思想实现 step-level 异步采样，解决长轨迹阻塞问题。\n\n- **Chen et al. (2025)**\n长周期交互式 RL，提出环境非平稳性对策；Tongyi 通过“模拟→现实”两阶段环境缓解同一问题。\n\n3\\. 合成数据与可验证奖励\n\n- **WebSailor / WebDancer** (Li et al., 2025b; Wu et al., 2025a)\n利用随机游走+知识图合成高难度 QA；Tongyi 将其扩展为“不确定性注入+集合论形式化”两步，提升难度可控性与可验证性。\n\n- **WebWeaver** (Li et al., 2025e)\n动态大纲生成长证据链；Tongyi 在报告生成阶段采用类似压缩策略，但嵌入上下文管理范式。\n\n- **Tao et al. (2025)**\n提出信息搜寻问题的集合论形式化；Tongyi 直接采用该形式化实现“原子操作”级难度递增与自动验证。\n\n4\\. 上下文管理与长程推理\n\n- **ReSum** (Wu et al., 2025c)\n基于 Markov 状态压缩的长程搜索；Tongyi 将其思想固化为上下文管理 rollout，并用于 RL 训练。\n\n- **WebResearcher** (Qiao et al., 2025)\n提出无界推理代理；Tongyi 通过 128K 上下文+动态摘要实现相似目标，但参数更少且开源。\n\n- **Qwen3-30B-A3B-Base** (Yang et al., 2025)\n基座模型，提供 30B 总参 / 3.3B 激活的 MoE 结构；Tongyi 在其上首次验证 agentic mid-training 有效性。\n\n小结\n\nTongyi DeepResearch 在以上四条主线上均做出**开源、可复现**的推进：\n\n1. 对标并超越闭源深度研究系统；\n2. 将 agent RL 稳定训练范式公开；\n3. 提供全自动、可验证的合成数据管线；\n4. 把长程上下文管理内化为模型能力而非外部提示。\n\nQ3: 论文如何解决这个问题？\n\n论文将“如何低成本、可复现地训练出顶尖水平的深度研究智能体”拆解为三大子问题，并对应给出系统级解法。整体思路可概括为：\n\n> **用合成数据代替人工标注，用分层环境降低交互成本，用端到端 agentic 训练把“研究能力”直接内化到模型参数。**\n\n1\\. 缺乏研究先验 → **Agentic Mid-training**\n\n| 关键障碍 | 解法 | 技术要点 |\n| --- | --- | --- |\n| 通用基座模型无“工具调用/多步规划”偏置 | 在预训练与后训练之间插入两段式 Agentic CPT | 1. 32 K → 128 K 渐进上下文2. 引入 64 K-128 K 长轨迹合成数据3. 保留 10 % 通用语料防止灾难性遗忘 |\n| 无高质量长轨迹 | 全自动合成管线 | 1. 实体锚定的开放世界记忆 → 多样问题2. 拒绝采样+双阶段推理链过滤 → 高质量规划/推理/决策动作3. 环境规模化 → 10 K+ 函数调用场景 |\n\n**结果**：得到具备强 agentic 偏置的“中训基座”，为后续 RL 提供稳定起点。\n\n2\\. 人工标注昂贵 → **Synthetic Data Centric Scaling**\n\n| 阶段 | 数据需求 | 合成策略 |\n| --- | --- | --- |\n| 中训 | 大规模、多样、长轨迹 | 先验世界+模拟环境离线生成，零 API 成本 |\n| 后训-SFT | 高置信演示 | 1. 随机游走知识图 → 子图/子表 QA2. 可控“原子操作”提升难度 → 超人类水平3. 集合论形式化自动验证答案一致性 |\n| 后训-RL | 可验证奖励+持续挑战 | 动态数据策展：实时剔除过易/过难题，补充模型“刚好学不会”的新题，形成数据飞轮 |\n\n**结果**：全程零人工标注，数据分布随模型能力同步进化，避免“天花板”或“梯度消失”。\n\n3\\. 真实环境昂贵且非平稳 → **分层环境 + 异步 RL**\n\n| 环境类型 | 职责 | 关键技术 |\n| --- | --- | --- |\n| Prior World | 中训数据自举 | 用基座知识“想象”工具调用，零成本、无限规模 |\n| Simulated | 算法验证+快速迭代 | 2024 Wiki 离线库+本地 RAG，QPS 无限，用于调参/消融 |\n| Real World | 最终能力评估 | 统一沙箱：限流、缓存、重试、降级、 failover，保证训练轨迹不被 API 波动污染 |\n\n**训练流程**\n\n1. 中训：Prior + Simulated → 低成本生成百万级轨迹\n2. 后训-SFT：Simulated 精选演示 → 冷启动\n3. 后训-RL：Simulated 调超参 → Real World 上线\n4. 异步 rollout：推理与工具调用双服务器，千条轨迹并行，单卡利用率 > 90 %\n\n**算法级稳定技巧**\n\n- 严格 on-policy GRPO，零重要性权重\n- clip-higher 鼓励探索，leave-one-out 减方差\n- 负样本过滤：无答案/超长轨迹直接丢弃，防止策略塌陷\n\n4\\. 上下文爆炸 → **Markovian 上下文管理**\n\n- 每步只保留：任务问题 + 压缩报告  S_(t-1)  + 上一步动作与观察  (a_(t-1), o_(t-1)) \n- 报告  S_t  由模型自己生成，长度可控，128 K 窗口下可跑 > 100 轮工具调用\n- 天然抑制“摘要-膨胀”循环，兼顾长程一致性与计算效率\n\n5\\. 测试时扩展 → **Heavy Mode**\n\n- n 个并行 agent 各自用上下文管理产出压缩报告  S^u_T \n- 合成模型一次性读取 n 份报告，生成最终答案\n- 实现“交互维度”而非“输出 token 维度”的 test-time scaling，在 Humanity’s Last Exam 上提升 5.4 pp\n\n总结\n\n通过“中训注入先验 → 合成数据飞轮 → 分层环境降本 → 上下文管理防溢 → 并行合成扩测”，论文首次在 30B/3.3B 激活参数规模下，把闭源系统才能做到的深度研究能力**完全开源复现**，并在 8 项公开基准上取得 SOTA 或次 SOTA 成绩。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“深度研究能力”与“通用能力”两条主线共展开 4 组实验，覆盖 11 个公开基准、3 种上下文长度、2 种推理模式，并给出训练动态与消融分析。所有实验均在固定超参（temperature=0.85, top-p=0.95, max-tool-call=128, context=128 K）下重复 3 次，以 Avg@3 为主指标，同时报告 Pass@1 与 Pass@3。\n\n1\\. 主实验：7 大深度研究基准\n\n| Benchmark | 指标 | 对比系统 | 关键结果 |\n| --- | --- | --- | --- |\n| Humanity’s Last Exam (2 154 题) | Acc@3 | OpenAI-o3、DeepSeek-V3.1、Gemini-DR 等 | 32.9 ↑ SOTA（↑+6.0 pp vs 次优） |\n| BrowseComp (900 题) | Acc@3 | 同上 + GLM-4.5、Claude-4-Sonnet | 43.4 ↑ SOTA |\n| BrowseComp-ZH (900 题) | Acc@3 | 同上 | 46.7 ↑ SOTA |\n| GAIA (Level-1-3) | Acc@3 | 同上 | 70.9 ↑ SOTA |\n| WebWalkerQA | Acc@3 | 同上 | 72.2 ↑ SOTA |\n| xbench-DeepSearch | Acc@3 | 同上 | 75.0 ↑ SOTA |\n| FRAMES (100 长文档) | Acc@3 | ChatGPT-5-Pro、SuperGrok 等 | 90.6 ↑ SOTA |\n\n> 结论：30B/3.3B 激活参数即实现全面领先，验证“中训+后训”范式效率。\n\n2\\. Heavy Mode：测试时交互维度扩展\n\n- 并行 16 个 agent + 1 个合成模型，单卡 128 K 上下文内完成\n- Humanity’s Last Exam：38.3 %（+5.4 pp）\n- BrowseComp：58.3 %（+14.9 pp）\n- BrowseComp-ZH：58.1 %（+11.4 pp）\n\n> 结论：首次展示“交互轮次”而非“输出 token”维度的 test-time scaling 有效性。\n\n3\\. 细粒度分析实验\n\n3.1 训练动态\n\n- **Reward 曲线**：500 步内从 0.45 → 0.65 单调上升，EMA 平滑后无平台。\n- **熵曲线**：初始小幅上升→快速收敛至 0.35，无塌陷/爆炸，证明算法稳定性。\n\n3.2 上下文长度消融\n\n| 上下文上限 | 最终 Reward | 平均响应长度 | 现象 |\n| --- | --- | --- | --- |\n| 64 K | 0.64 | 30 K | 充分利用长度，性能最高 |\n| 48 K | 0.58 | 24 K | 平稳收敛 |\n| 32 K | 0.52 | 17 K ↓ | 被迫学会更紧凑策略，长度反降 |\n\n> 结论：动态数据策展机制下，小模型自动进化出“短而精”行为，验证课程难度自适应。\n\n3.3 交互轮次缩放\n\n- 在 BrowseComp 上固定 32 K-128 K 上下文，逐步放宽工具调用上限\n- 8 → 128 轮：准确率线性提升 12.5 % → 50 %\n- 证明性能瓶颈主要在于“信息获取广度”而非“单步推理深度”\n\n3.4 模拟→现实一致性\n\n- 在 2024-Wiki 离线环境重复 RL 训练，奖励曲线与真实环境皮尔逊 r=0.97\n- 模拟环境单卡日迭代 12 次→真实环境仅 2 次，开发效率提升 6×\n\n4\\. 通用基准验证\n\n| Benchmark | Tongyi DR | Qwen3-30B-A3B-Thinking | Qwen3-235B-A22B-Thinking |\n| --- | --- | --- | --- |\n| AIME25 | 85.0 | 71.4 | 92.3 |\n| HMMT25 | 100 | 83.9 | 100 |\n| SimpleQA | 98.6 | 47.1 | 100 |\n\n> 结论：引入工具后，小参数 agent 在知识密集型任务上逼近甚至等于大参数纯推理模型，说明“工具+推理”协同优于单纯放大参数。\n\n5\\. 可复现性材料\n\n- 代码与工具实现：GitHub 完全开源，含替代 API 方案\n- 评测脚本：官方指标脚本一键复跑\n- 样例轨迹：放出 3 条 64 K-token 完整 ReAct 轨迹供社区比对\n\n总结\n\n实验矩阵覆盖“英/中、长/短、开/闭、单/多 agent、模拟/真实”六大维度，既验证 SOTA 性能，也揭示 scaling 规律与训练稳定性，为后续开源社区提供可复现的端到端基线。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向按“可落地紧迫性”→“长期挑战性”递进，均直接对应论文第 5 节 Limitations 与 Discussion 中留出的开放口，可供社区与工业界接力探索。\n\n1\\. 上下文长度与记忆机制\n\n- **>128 K 稀疏注意力**：在 256 K–1 M token 上验证“模拟环境课程→真实环境微调”是否仍保持线性提升\n- **层次化记忆仓库**：将压缩报告 St 存为可检索向量，突破单窗口上限，实现“数日级”连续调研\n- **记忆写回策略**：探索“写回先验世界”形成自我演化知识库，缓解真实网页时效漂移\n\n2\\. 模型规模与效率\n\n- **MoE 稀疏路由可视化**：解释 3.3 B 激活参数为何足以击败 100 B 级稠密模型，指导更小边缘端部署\n- **混合精度 + 投机推理**：利用 1.8 B/0.5 B “小助手”模型并行生成工具调用草稿，主模型仅做验证，实现 2–3× 加速\n- **部分 Rollout 与 Off-policy RL**：只回传高不确定性片段，解决长轨迹 GPU 内存随步数线性增长问题\n\n3\\. 数据与奖励\n\n- **可验证奖励外延**：引入“信息溯源精度”“引用覆盖率”等细粒度奖励，缓解 0/1 稀疏信号导致的梯度方差\n- **自批评数据飞轮**：用更大模型当“裁判”，对当前 agent 生成的报告打细分分数，自动生成新训练对\n- **多语言合成**：将中文合成管线扩展到日、韩、德、西等语种，验证跨文化研究任务是否出现语言特有策略\n\n4\\. 工具与动作空间\n\n- **可插拔工具商城**：从 5 工具→ 50 + 工具（数据库、GIS、MATLAB、实验仪器 API），研究工具冲突检测与自动选择\n- **多模态工具**：引入截图+OCR、图像编辑、3-D 可视化，评测 agent 在“图表复现”任务上的可靠性\n- **工具创建**：让 agent 根据需求自动生成 Python 包或 REST API 并注册到沙箱，实现“自给自足”的扩展闭环\n\n5\\. 环境层级与 Sim-to-Real\n\n- **可微分网页仿真**：用可微分渲染把 Wiki 离线库升级为“可编辑”环境，支持梯度直接回传至策略网络，实现超高效规划\n- **对抗式环境漂移**：主动注入 DNS 污染、404、付费墙等真实故障，训练策略对非平稳的鲁棒性\n- **联邦式真实环境**：多家机构共享各自内部搜索 API 作为“联邦环境”，在隐私不泄露前提下扩大真实分布覆盖\n\n6\\. 安全与对齐\n\n- **研究幻觉检测**：构建“事实性错误”自动标注器，对比多源答案一致性，形成幻觉率指标并加入奖励\n- **偏好对齐 RLHF**：收集科研人员对“报告深度/简洁/格式”的多维打分，用 DPO/KTO 对齐不同领域偏好\n- **恶意信息防护**：针对合成数据环节引入“红队”模型，自动过滤可能生成生物安全、隐私泄露等危险内容的问题\n\n7\\. 领域与任务泛化\n\n- **科学发现代理**：把实验仪器 API、实验数据仓库接入动作空间，目标让 agent 完成“提出假设→设计实验→分析结果”全链条\n- **法律/政策深度调研**：引入法规数据库、判例系统，评测 agent 在跨法域、跨年代条文冲突中的引用准确率\n- **金融投资研究**：接入财报 API、宏观经济数据库，考察 agent 在“多指标归因→未来情景推演”中的鲁棒性与合规性\n\n8\\. 宏观趋势\n\n- **从 Deep Research → General Agent**：逐步取消“研究类 prompt”先验，仅给任务描述，检验模型是否能自动决定何时进入研究模式\n- **自我改进极限**：让 agent 负责自己训练数据的生成、过滤、难度升级，观察几代后性能是否出现可证明的收敛或奇点\n- **开放代理网络**：多代理节点通过区块链共享最新轨迹与奖励，形成去中心化、可审计的“集体智能”持续学习生态\n\n小结\n\n论文已把“开源 + 合成数据 + 分层环境”的范式跑通，留下的是**长度、规模、工具、安全、领域、自我改进**六大维度的空白。任何一点突破都可直接复用其代码与数据飞轮，快速验证并回馈社区。\n\nQ6: 总结一下论文的主要内容\n\nTongyi DeepResearch 技术报告核心内容速览\n\n1\\. 研究目标\n\n以**完全开源、可复现**的方式，训练出具备**长周期、多步推理与信息搜寻**能力的深度研究智能体，缩小乃至超越闭源系统（OpenAI DeepResearch、Gemini DeepResearch 等）的性能差距。\n\n2\\. 关键挑战与对应解法\n\n| 挑战 | 本文解法 | 一句话总结 |\n| --- | --- | --- |\n| 基座模型缺乏 agent 先验 | Agentic Mid-training | 在预训练-后训练之间插入两段式“持续预训练”，用百万级合成轨迹注入工具调用与规划偏置 |\n| 人工标注昂贵且稀缺 | 全自动合成数据管线 | 随机游走知识图→子图 QA→不确定性注入→集合论形式化验证，零人工生成超人类难度数据 |\n| 真实环境昂贵、非平稳 | 分层环境策略 | Prior World（零成本）→Simulated（快速迭代）→Real World（最终验证），逐层降低交互成本 |\n| 长轨迹上下文爆炸 | Markovian 上下文管理 | 每步仅保留“任务+压缩报告+上一步交互”，128 K 窗口可支撑 >100 轮工具调用 |\n| 训练不稳定 | On-policy 异步 RL + 动态数据策展 | 严格 on-policy GRPO，实时替换过易/过难题，奖励与熵曲线双稳定 |\n\n3\\. 训练流程（端到端三阶段）\n\n```\nQwen3-30B-A3B-Base\n├─ Agentic CPT-1 (32 K) ──► Agentic CPT-2 (128 K) … 中训\n└─ Agentic SFT (40 K→128 K) ──► Agentic RL (Real/Sim) … 后训\n```\n\n4\\. 模型规格\n\n- **总参数量**：30.5 B\n- **每 token 激活**：3.3 B（MoE-A3B 结构）\n- **最大上下文**：128 K\n- **工具集**：Search、Visit、Python、Google Scholar、File Parser（统一沙箱限流+缓存）\n\n5\\. 主要结果（Avg@3）\n\n| Benchmark | 分数 | 相对提升 |\n| --- | --- | --- |\n| Humanity’s Last Exam | 32.9 | +6.0 pp vs 次优 |\n| BrowseComp | 43.4 | SOTA |\n| BrowseComp-ZH | 46.7 | SOTA |\n| GAIA | 70.9 | SOTA |\n| WebWalkerQA | 72.2 | SOTA |\n| xbench-DeepSearch | 75.0 | SOTA |\n| FRAMES | 90.6 | SOTA |\n\n**Heavy Mode**（16 并行 agent + 合成）：\nHumanity’s Last Exam 38.3 %（+5.4 pp），BrowseComp 58.3 %（+14.9 pp）\n\n6\\. 通用基准\n\n| AIME25 | HMMT25 | SimpleQA |\n| --- | --- | --- |\n| 85.0 | 100 | 98.6 |\n\n工具增强的小模型在数学与知识问答上逼近 235 B 纯推理模型。\n\n7\\. 贡献总结\n\n- **范式**：首次开源“中训+后训”统一框架，零人工合成数据即可训练顶尖研究智能体\n- **性能**：30 B 级模型在 8 项深度研究基准全面领先，验证参数效率\n- **复现**：代码、模型、工具、评测脚本完全公开，社区可直接使用与改进\n\n8\\. 未来方向\n\n1. >128 K 上下文或层次化记忆\n2. 部分 rollout、off-policy RL 提速\n3. 多模态、可编辑模拟环境\n4. 自批评数据飞轮与幻觉检测\n5. 从 Deep Research → 通用自主代理\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Tongyi%20DeepResearch%20Technical%20Report，链接是https://arxiv.org/pdf/2510.24701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24701。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1761781638490","title":"AgentFold: Long-Horizon Web Agents with Proactive Context Management","description":"LLM-based web agents show immense promise for information seeking, yet their effectiveness on long-horizon tasks is hindered by a fundamental trade-off in context management. Prevailing ReAct-based agents suffer from context saturation as they accumulate noisy, raw histories, while methods that fixedly summarize the full history at each step risk the irreversible loss of critical details. Addressing these, we introduce AgentFold, a novel agent paradigm centered on proactive context management, inspired by the human cognitive process of retrospective consolidation. AgentFold treats its context as a dynamic cognitive workspace to be actively sculpted, rather than a passive log to be filled. At each step, it learns to execute a `folding' operation, which manages its historical trajectory at multiple scales: it can perform granular condensations to preserve vital, fine-grained details, or deep consolidations to abstract away entire multi-step sub-tasks. The results on prominent benchmarks are striking: with simple supervised fine-tuning (without continual pre-training or RL), our AgentFold-30B-A3B agent achieves 36.2% on BrowseComp and 47.3% on BrowseComp-ZH. Notably, this performance not only surpasses or matches open-source models of a dramatically larger scale, such as the DeepSeek-V3.1-671B-A37B, but also surpasses leading proprietary agents like OpenAI's o4-mini.","url":"https://huggingface.co/papers/2510.24699","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.490Z","metadata":{"authors":["Rui Ye","Zhongwang Zhang","Kuan Li","Huifeng Yin","Zhengwei Tao","Yida Zhao","Liangcai Su","Liwen Zhang","Zile Qiao","Xinyu Wang","Pengjun Xie","Fei Huang","Siheng Chen","Jingren Zhou","Yong Jiang"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.24699.pdf","arxivUrl":"https://arxiv.org/abs/2510.24699","arxivId":"2510.24699","coolPaperUrl":"https://papers.cool/arxiv/2510.24699","published":"2025-10-28T17:51:50Z","updated":"2025-10-28T17:51:50.000Z","zh_summary":"基于大型语言模型（LLM）的网页代理在信息搜索方面展现出巨大的潜力，但它们在长周期任务上的有效性受到上下文管理中根本性权衡的制约。现有基于ReAct的代理在累积嘈杂、原始历史信息时会遭遇上下文饱和，而那些在每一步固定总结完整历史的方法，则存在关键细节不可逆丢失的风险。为解决这些问题，我们提出了AgentFold，一种以主动上下文管理为核心的新型代理范式，其灵感来源于人类认知中回顾性整合的过程。AgentFold将上下文视为一个动态的认知工作空间，需主动雕琢，而非被动填充的日志。在每一步操作中，它学习执行“折叠”操作，以多层次管理历史轨迹：可以进行细粒度的压缩以保留关键细节，也可以进行深度整合以抽象化整个多步骤子任务。在主要基准测试中，结果非常显著：通过简单的监督微调（无需持续预训练或强化学习），我们的AgentFold-30B-A3B代理在BrowseComp上取得36.2%的成绩，在BrowseComp-ZH上取得47.3%。值得注意的是，这一表现不仅超越或匹配了规模远大于自身的开源模型，如DeepSeek-V3.1-671B-A37B，同时也超过了领先的专有代理，如OpenAI的o4-mini。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**长程（long-horizon）网络智能体在上下文管理上的根本权衡困境**：\n\n- **ReAct 类智能体**采用“只追加”历史，保留全部原始轨迹，导致上下文随时间线性膨胀，噪声淹没关键信号，推理性能下降。\n- **每步全局摘要**方法虽保持上下文简洁，却在每一次压缩中都可能**不可逆地丢失关键细节**；步数越多，信息存活率指数下降（100 步后仅剩 ≈ 36.6 %，500 步后 ≈ 0.66 %）。\n\nAgentFold 提出**主动式、多尺度“折叠”机制**，把上下文视为可动态塑形的认知工作区，在每一步自主决定：\n\n1. **细粒度凝缩**（Granular Condensation）——仅把最新一步压缩成独立摘要，保留高分辨率细节；\n2. **深度整合**（Deep Consolidation）——将最新一步与一段历史一起抽象为一条粗粒度摘要，剔除已完成子任务的中间噪声。\n\n通过可学习的折叠策略，AgentFold 在 100 步交互后上下文仅约 7 k tokens，可扩展至 500 步而不溢出，显著提升了长程信息搜寻任务的准确率与效率。\n\nQ2: 有哪些相关研究？\n\n论文在 §2 中将与自身相关的研究划分为两条主线，并逐条对比其差异。以下按原文脉络归纳：\n\n1\\. Web Agents\n\n- **ReAct 范式**\n- 代表：WebThinker、WebDancer、WebSailor、WebSailor-V2、WebShaper、WebExplorer、X-Master、BrowseMaster 等\n- 共同问题：采用“append-only”上下文，随交互步数线性增长，长程任务出现 context saturation，关键信号被噪声淹没。\n- AgentFold 差异：引入可学习的折叠操作，主动塑形上下文，让步数-上下文长度变为亚线性甚至常数级。\n\n2\\. Context Management\n\n2.1 External Context Augmentation（外部知识注入）\n\n- 代表：MEM0、Memory3、A-Mem、Memos 等\n- 特点：把**用户画像、历史对话、外部知识库**等额外信息注入 prompt，实现个性化或长期记忆。\n- AgentFold 差异：聚焦**任务内部轨迹的自管理**（Intra-Trajectory Curation），而非引入外部知识。\n\n2.2 Intra-Task Context Curation（任务内上下文精炼）\n\n- 代表：\n- MEM1 —— 每步对**完整历史**做一次硬性摘要；\n- MemAgent —— 同样采用**步级刚性压缩**，主要验证于 HotpotQA 等检索型短任务。\n- 共同问题：\n- 摘要策略**固定、单尺度**；\n- 每步重写历史，**关键细节被不可逆地丢弃**，且随步数指数衰减。\n- AgentFold 差异：\n- 提出**多尺度、回顾式折叠**（Granular vs. Deep），可延迟整合直至子任务完成；\n- 通过监督学习把折叠策略**内化**为模型参数，而非手工规则；\n- 面向**长程、多跳、开放域网页任务**进行系统评估。\n\n3\\. 训练数据与范式\n\n- 现有工作多依赖**提示工程 + 人工规则**生成轨迹。\n- AgentFold 提出 Fold-Generator，采用**拒绝采样 + 格式过滤**自动构造高质量“折叠-动作”成对数据，再通过**纯监督微调**即可让 30B 模型掌握复杂上下文管理，无需额外 RL 或继续预训练。\n\nQ3: 论文如何解决这个问题？\n\n论文把“上下文管理”从被动记录改写成**主动、可学习的推理动作**，通过三项核心设计解决长程权衡困境：\n\n1\\. 认知工作区：把上下文拆成「双池」\n\n- **Multi-Scale State Summaries**\n有序、可变尺度的历史块序列\n S_t = (s_(x_1,y_1), …, s_(x_m,y_m)) \n每个  s_(x,y)  既可以是单步细粒度摘要（ x=y ），也可以是多步深度整合（ x<y ）。\n\n- **Latest Interaction**\n完整保留最近一步的「思考-动作-观测」三元组，充当高保真工作记忆。\n\n上下文输入统一写成\n C_t = (Q, T, S_(t-2), I_(t-1)) \n既保证长期概览，又保证当前细节无损。\n\n2\\. 折叠指令：让「摘要」成为模型输出的显式结构\n\n每步 AgentFold 并行生成四元组\n R_t = (th_t_(思考), f_t_(折叠指令), e_t_(解释), a_t_(动作)) \n\n其中折叠指令为 JSON\n$f\\_t = {\"range\":\nk,t-1\n, \"summary\":\"σ_t\"}$\n\n- **Granular Condensation** ——  k=t-1 ：仅把最新一步压成一条细粒度块，追加到  S 。\n- **Deep Consolidation** ——  k<t-1 ：把 $\nk,t-1\n$ 内所有块**整体替换**为一条粗粒度结论，直接剪掉中间噪声。\n\n模型通过自回归一次生成该指令，随后**立即执行**更新  S_(t-2)to S_(t-1) ，再进入下一步。折叠与动作规划在同一前向传播内完成，形成「感知→推理→折叠→行动」闭环。\n\n3\\. 数据与训练：把「折叠策略」蒸馏进参数\n\n- 构建 Fold-Generator：用强 LLM + 拒绝采样批量产出符合格式的「折叠-动作」轨迹；\n- 仅做**监督微调**（SFT）即可让 30B 模型内生化折叠技能，无需 RL 或继续预训练；\n- 推理阶段一次前向同时输出「该怎么折」与「下一步做什么」，折叠开销为零。\n\n效果\n\n- 上下文长度亚线性增长：100 步平均 7 k tokens，500 步仍 <20 k；\n- 信息存活率不再指数衰减，关键细节可被**独立块永久豁免**重复压缩；\n- BrowseComp 等长程基准上，30B 模型击败 671B 开源与 o4-mini 等商用智能体，验证「主动折叠」即可弥合规模差距。\n\nQ4: 论文做了哪些实验？\n\n论文在 §4 中系统评估了 AgentFold-30B-A3B 的**任务性能、上下文效率与可扩展性**，共包含 4 组实验：\n\n1\\. 主任务基准（4 个公开数据集）\n\n| 数据集 | 任务特点 | 指标 |\n| --- | --- | --- |\n| BrowseComp / BrowseComp-ZH | 长程、跨站、难定位信息 | 官方 exact-match 准确率 |\n| WideSearch-en | 广域、多源、集合式检索 | Item-F1 |\n| GAIA（text-only 子集） | 通用助理能力 | 官方 exact-match |\n\n**结果**\n\n- AgentFold-30B-A3B 在 4 个基准全部**刷新开源 SOTA**；\n- 在 BrowseComp 上 36.2 %，**超越 20× 参数量的 DeepSeek-V3.1-671B-A37B（30.0 %）**；\n- 与领先**商用智能体**打平或更优：BrowseComp 高于 o4-mini（28.3 %），WideSearch 62.1 % 超过 o3（60.0 %）。\n\n2\\. 上下文效率实测（BrowseComp 200 条轨迹）\n\n- **Token 增长曲线**\n- 100 轮内平均长度从 ≈3.5 k → ≈7 k，**亚线性**；\n- 终止时仅占用 128 k 上限的 5 %，剩余 120 k+ 可用。\n- **块数增长曲线**\n- ReAct 线性爆炸（1 block/步）；\n- AgentFold 因 Deep Consolidation 块数**亚线性**，100 轮后结构复杂度降低 5×+。\n- **同轨迹对比**\n- 100 轮时 AgentFold 比 ReAct **节省 84 k tokens（≈92 %）**，对应单次推理内存下降约 7 GB。\n\n3\\. 交互步数可扩展性（Scaling Turns）\n\n- 在 BrowseComp 上把**最大调用次数**从 32 逐步放宽到 256、500。\n- **GLM-4.5-355B-A32B**（ReAct 式）64 轮后性能饱和并掉分；\n- **AgentFold-30B-A3B** 准确率随步数**单调上升至 256 轮**，500 轮上下文仍 <20 k tokens，首次展示“**几百步级**”稳定求解能力。\n\n4\\. 案例可视化（Case Study）\n\n- 给出两条真实轨迹的**每轮上下文快照**（Turn-by-Turn 块列表）。\n- 展示 AgentFold 如何在**连续失败子序列**后执行 Deep Consolidation，把 11 步无效搜索压成一句结论，随后**切换策略**并继续探索，验证折叠机制的可解释性与错误恢复能力。\n\n结论摘要\n\n实验从“**任务指标-资源消耗-步数上限-行为可解释**”四维度证明：\n仅靠监督微调学到的**主动多尺度折叠**，即可让 30B 模型在长程信息搜寻场景**击败 671B 开源模型与商用 o4-mini**，同时保持上下文紧凑、可扩展至数百步。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 AgentFold 范式的直接延伸，按“算法-数据-系统-评测”四层归纳，供后续研究参考：\n\n1\\. 算法层：折叠策略的自主演进\n\n- **强化学习折叠**\n当前仅用 SFT 模仿高质量折叠，下一步可让策略在环境中**直接优化任务奖励**，自动发现非显而易见的多尺度压缩方案（例如对“尚未失败但信息增益递减”的子任务提前整合）。\n\n- **层次化或递归折叠**\n引入**多级摘要树**：深层整合后的块仍可再次被上层机制合并，实现“摘要的摘要”，支持**千步以上**的超长轨迹。\n\n- **不确定性感知折叠**\n在折叠指令中显式输出**信息置信度**或**遗忘风险估计**，对高方差关键证据延迟整合，降低不可逆丢失概率。\n\n2\\. 数据层：更丰富、更专用的折叠语料\n\n- **跨语言/跨文化折叠**\n现有数据以英文、中文为主；扩展至多语种网页，可研究**文化语境差异**对“哪些信息值得保留”的影响。\n\n- **多模态折叠**\n当观测包含**图像、PDF、音频**时，需决定“是否保留原始文件 / 仅保留 OCR 文本 / 仅保留视觉描述”。构建多模态折叠轨迹可推动**统一 Agent**。\n\n- **失败轨迹重用**\n收集“**折叠后仍失败**”的长轨迹，用反向增强或负样本学习，使模型具备“**自我回滚**”能力——在发现摘要误导时主动展开旧块重新检视。\n\n3\\. 系统层：高效推理与记忆卸载\n\n- **折叠-解折叠内存层级**\n把深度整合后的块存到**外部向量库**，需要时再“解折叠”召回，实现**无限上下文**而 GPU 显存仅保持当前工作集。\n\n- **投机折叠（Speculative Folding）**\n并行生成多条候选折叠指令，用**轻量级价值模型**提前评估哪条摘要最有利于后续搜索，减少主模型反复自回归开销。\n\n- **端-云协同**\n端侧小模型负责**高频细粒度折叠**，云端大模型定期执行**深度整合与全局复盘**，兼顾延迟与质量。\n\n4\\. 评测层：更严苛的长程场景\n\n- **千步级基准**\n现有 BrowseComp 平均 <100 步。构建**BrowseComp-1k** 或**MultiSession-Research** 任务，要求跨天、跨会话持续搜集，验证折叠机制在**人类级研究周期**下的鲁棒性。\n\n- **可验证遗忘率**\n设计带**黄金探针**的数据集：在轨迹前端植入关键证据，后端提问；量化不同折叠策略的**信息存活率**并与理论指数衰减曲线对比。\n\n- **对抗性折叠测试**\n引入“**摘要-敏感**”问题：恶意网页在第六步给出唯一正确答案，随后被后续噪声淹没；测试 AgentFold 是否**选择性保留**该高价值片段而不被后期整合误删。\n\n5\\. 理论与认知建模\n\n- **折叠与 human memory consolidation 的对齐度**\n用认知心理学实验对比人类被试在同类多步信息搜寻中的“笔记行为”与 AgentFold 的块级摘要，验证**机器折叠是否复现人类遗忘-保留模式**。\n\n- **信息论视角的最优折叠率**\n建立**率-失真**风格的目标函数，在“上下文长度（码率）”与“任务答案失真”之间寻找理论最优折叠策略，指导实际模型设计。\n\n综上，AgentFold 把“上下文管理”从静态启发式变为**可学习的动作空间**，为长程智能体研究打开了新的算法-系统-评测维度；上述任何一点的深入都可能带来下一代“**可工作数百至数千步**”的通用信息搜寻代理。\n\nQ6: 总结一下论文的主要内容\n\n**AgentFold: Long-Horizon Web Agents with Proactive Context Management**\n提出一种**主动上下文折叠**的新范式，解决长程网络智能体在“保留细节”与“控制长度”之间的根本权衡，核心贡献如下：\n\n1\\. 问题\n\n- **ReAct 类智能体**：只追加历史 → 上下文线性爆炸，噪声淹没关键信号。\n- **每步全局摘要**：反复重写历史 → 重要细节不可逆丢失，存活率指数下降（100 步后 ≈ 36 %）。\n\n2\\. 方法\n\n- **认知工作区**\n上下文 = {不变用户问题, 工具列表, **Multi-Scale State Summaries**, **Latest Interaction**}\n\n- Summaries：可变尺度块序列，可单步亦可多步整合。\n- Interaction：完整最近一步，保证即时细节无损。\n- **折叠指令**（模型每步并行输出）\n- **Granular Condensation**——仅压最新一步，追加细粒度摘要。\n- **Deep Consolidation**——合并一段历史，用一条粗粒度结论替换，剪除噪声。\n指令立即执行，更新工作区，形成“感知→推理→折叠→行动”闭环。\n- **训练**\n构建 Fold-Generator（拒绝采样+格式过滤）自动产出高质量“折叠-动作”轨迹，仅用**监督微调**即可让 30B 模型内生化折叠策略。\n\n3\\. 实验\n\n- **4 大基准**（BrowseComp / BrowseComp-ZH / WideSearch / GAIA）\nAgentFold-30B-A3B **刷新开源 SOTA**，击败 20× 参数量的 DeepSeek-V3.1-671B，与 o4-mini 等商用智能体打平或更优。\n\n- **上下文效率**\n100 步平均 7 k tokens（亚线性），比 ReAct 节省 92 % 长度，对应内存降 7 GB。\n\n- **可扩展性**\n放宽步数至 500 轮，准确率仍上升，上下文 <20 k tokens，首次展示“数百步级”稳定求解能力。\n\n4\\. 结论\n\nAgentFold 把上下文管理从被动日志变为**可学习的多尺度折叠动作**，在紧凑预算内保留关键细节，实现**长程、高效、可扩展**的网页信息搜寻，为下一代深度研究代理提供了新基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentFold%3A%20Long-Horizon%20Web%20Agents%20with%20Proactive%20Context%20Management，链接是https://arxiv.org/pdf/2510.24699，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24699。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1761781638491","title":"RoboOmni: Proactive Robot Manipulation in Omni-modal Context","description":"Recent advances in Multimodal Large Language Models (MLLMs) have driven rapid progress in Vision-Language-Action (VLA) models for robotic manipulation. Although effective in many scenarios, current approaches largely rely on explicit instructions, whereas in real-world interactions, humans rarely issue instructions directly. Effective collaboration requires robots to infer user intentions proactively. In this work, we introduce cross-modal contextual instructions, a new setting where intent is derived from spoken dialogue, environmental sounds, and visual cues rather than explicit commands. To address this new setting, we present RoboOmni, a Perceiver-Thinker-Talker-Executor framework based on end-to-end omni-modal LLMs that unifies intention recognition, interaction confirmation, and action execution. RoboOmni fuses auditory and visual signals spatiotemporally for robust intention recognition, while supporting direct speech interaction. To address the absence of training data for proactive intention recognition in robotic manipulation, we build OmniAction, comprising 140k episodes, 5k+ speakers, 2.4k event sounds, 640 backgrounds, and six contextual instruction types. Experiments in simulation and real-world settings show that RoboOmni surpasses text- and ASR-based baselines in success rate, inference speed, intention recognition, and proactive assistance.","url":"https://huggingface.co/papers/2510.23763","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.491Z","metadata":{"authors":["Siyin Wang","Jinlan Fu","Feihong Liu","Xinzhe He","Huangxuan Wu","Junhao Shi","Kexin Huang","Zhaoye Fei","Jingjing Gong","Zuxuan Wu","Yugang Jiang","See-Kiong Ng","Tat-Seng Chua","Xipeng Qiu"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.RO","cs.CL","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.23763.pdf","arxivUrl":"https://arxiv.org/abs/2510.23763","arxivId":"2510.23763","coolPaperUrl":"https://papers.cool/arxiv/2510.23763","published":"2025-10-27T18:49:03Z","updated":"2025-10-27T18:49:03.000Z","zh_summary":"近年来，多模态大语言模型（MLLMs）的进展推动了机器人操作中视觉-语言-动作（VLA）模型的快速发展。尽管在许多场景中效果显著，但当前方法在很大程度上依赖于明确的指令，而在现实世界的交互中，人类很少直接发出指令。有效的协作要求机器人能够主动推测用户意图。在本研究中，我们提出了跨模态上下文指令，这是一种新设定，其中意图来自口语对话、环境声音和视觉线索，而非明确的命令。为应对这一新设定，我们提出了RoboOmni，一个基于端到端全模态大语言模型的Perceiver-Thinker-Talker-Executor框架，实现了意图识别、互动确认和动作执行的统一。RoboOmni在空间和时间上融合听觉和视觉信号，实现稳健的意图识别，同时支持直接语音交互。针对机器人操作中主动意图识别缺乏训练数据的问题，我们构建了OmniAction，包括14万条实验数据、5000余名讲者、2400种事件声音、640种背景和六类上下文指令类型。仿真和真实场景实验表明，RoboOmni在成功率、推理速度、意图识别和主动辅助方面均优于基于文本和自动语音识别（ASR）的基线方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“机器人如何在没有显式指令的情况下，主动从多模态上下文推断用户意图并完成操作”这一核心问题。具体而言，现有视觉-语言-动作（VLA）方法普遍依赖**显式文本或语音命令**，而真实人机交互中人类很少直接发出“把可乐放到桌上”这类明确指令；更多时候，意图隐藏在**对话、语调、环境音、视觉场景**的交叉线索里。为此，作者提出：\n\n1. \\*\\*交叉模态上下文指令（cross-modal contextual instructions）\\*\\*新设定：机器人必须融合语音、环境声、视觉信息，**主动推断并确认**用户潜藏意图，而非被动等待明确命令。\n2. **RoboOmni**框架：端到端统一语音-视觉-语言-动作空间，实现“感知-推理-对话-执行”闭环。\n3. **OmniAction**数据集：填补“带音频的隐式意图”训练数据空白，含140k 条多说话人、多背景、多事件音频片段与六类上下文指令。\n\n总结：论文首次系统研究**机器人基于语音+环境音+视觉的主动意图推断与确认**，突破传统VLA 对显式指令的依赖。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两大主线，并指出其局限：\n\n1. **Omni-Modal LLMs**\n\n- 早期模块化方案：语音、视觉、文本分别编码后融合（Wu et al. 2023、Zhan et al. 2024），时序对齐困难，难以捕捉情境语义。\n- 近期端到端 omni-modal 模型（Hurst et al. 2024、Xu et al. 2025b、Xie & Wu 2024）把语音-视觉-文本统一建模，但**仅输出文本或音频**，不生成机器人动作，未进入具身领域。\n2. **Vision-Language-Action (VLA) 模型**\n\n- 端到端 VLA（Brohan et al. 2023、Kim et al. 2024、Black et al. 2024）直接把“图像+文本”映射到动作，但假设**短而明确的文本指令**，对隐含或多步意图表现差。\n- 级联/分层 VLA（Huang et al. 2023、Shi et al. 2025）用大模型做高层规划，再调用低层控制器，存在**接口碎片化、误差累积**问题。\n- 语音接口扩展：多数工作先用 ASR 转文本再输入 VLA（Shi et al. 2025、Khan et al. 2025），丢弃副语言线索；Zhao et al. 2025 直接以语音命令驱动 VLA，但**仅支持原子指令**，且无法语音回应，也未利用环境声。\n\n综上，现有研究要么缺动作输出，要么依赖显式文本/ASR，尚无工作**端到端地联合建模语音、环境声、视觉与动作**，并支持**主动意图推断与语音交互闭环**，这正是 RoboOmni 试图填补的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出“**RoboOmni**”框架，从**问题设定、模型架构、数据构造、训练范式**四个层面系统解决“机器人如何主动推断并执行跨模态上下文指令”：\n\n1. **新设定：交叉模态上下文指令**\n\n- 不再假设人类给出显式文本/语音命令，而是要求机器人**实时融合对话音频、环境声、视觉观测**，主动推断潜藏意图，并通过**语音交互确认**后再执行。\n2. **端到端统一架构：Perceiver–Thinker–Talker–Executor**\n\n- **Perceiver**：将视觉  v_t 、音频  s_t 、文本  c_t  编码为统一令牌序列 $X\\_t=\nv_t;s_t;c_t\n$。\n- **Thinker**：基于 omni-modal LLM 在**共享词汇空间**  V∪ A  上自回归生成文本、语音离散表示或动作令牌。\n- **Talker**：将文本/语义表示转换为**自然语音波形**，实现低延迟语音回应。\n- **Executor**：利用 FAST+ 离散动作编码，把动作令牌序列  r_(t:t+N)  解码为 7-DoF 连续控制向量  a_(t:t+N) ，实现**端到端动作输出**。\n3. **大规模训练数据：OmniAction**\n\n- 140k 条多模态 episode，含 5k+ 说话人、2.4k 事件声、640 背景声，覆盖六类上下文指令（情感、重叠、非语言、身份、二人/三人对话）。\n- 每条样本以三元组形式给出：多轮对话音频+文本、视觉帧序列、专家动作轨迹，可直接用于**联合对话与动作监督**。\n4. **统一训练目标**\n\n- 对话与动作共享**同一个自回归最大似然损失**\n\nL(θ)=-E∑_(k=1)^K log p_θ(z_k|X_t,z_(<k)),quad z_k∈ V∪ A\n\n- 通过**批量交错**同时优化对话令牌与动作令牌，无需多阶段流水线，避免传统 ASR→文本→动作的误差累积与延迟。\n\n综上，RoboOmni 用**端到端 omni-modal LLM**一次性完成“听-看-想-说-做”闭环，从而在新设定下实现**高成功率、低延迟、主动意图识别与自然语音交互**。\n\nQ4: 论文做了哪些实验？\n\n论文从**仿真基准**、**真人语音**、**真实机器人**、**认知能力**、**效率与消融**五个维度展开系统实验，验证 RoboOmni 在“交叉模态上下文指令”下的优势。\n\n1. **仿真基准：OmniAction-LIBERO-TTS**\n\n- 240 个任务 × 6 类上下文指令（情感、非语言、身份、重叠、二人、三人）\n- 与 4 个强基线（OpenVLA、OpenVLA-OFT、π0、NORA）在“真值文本”与“ASR→文本”两种输入条件下对比\n- 结果：RoboOmni 平均成功率 **85.6%**，最强基线仅 **25.9%**；在最难的 Goal/Object 套件上仍保持 **>79%**，证明端到端音频融合对副语言线索至关重要。\n2. **真人语音：OmniAction-LIBERO-Real**\n\n- 10 位志愿者现场录音，含口音、背景噪声、协同发音\n- RoboOmni 直接以原始音频输入，平均成功率 **76.6%**，显著高于最佳 ASR 级联方案 π0（73.8%）并远超 OpenVLA（40.1%），验证其对真实声学变化的鲁棒性。\n3. **真实机器人：WidowX 250S 部署**\n\n- 用 OmniAction 预训练模型 + 10 人现场语音微调\n- 现场案例（见图 5、12、13）显示：\n– **意图识别**：仅凭“门铃响”+ 对话上下文即推断应下鱼丸；\n– **主动确认**：遇到“蛋饺”负面语调，先询问“要不要换牛肉丸？”；\n– **可靠执行**：多干扰物场景下抓取成功率 >90%。\n4. **认知能力评估**\n\n- **意图识别准确率**：在 54 段人工标注上下文对话上，RoboOmni 达 **88.9%**，显著高于 Qwen2.5-Omni-3B（27.8%）、7B（50.0%）及 ASR+GPT-4o（55.6%）。\n- **交互质量**：定性实验（图 6、14）表明 RoboOmni 能\n– 主动澄清歧义（“要不要我放蛋饺？”）\n– 跨模态整合（结合门铃声与对话）\n– 保持自然协作式语言（“Would you like me to…?”）。\n5. **效率与消融**\n\n- **训练效率**：OmniAction 预训练 + 轻量 SFT 在 2k 步即达 ≈90% 成功率，从零开始 SFT 30k 步仅 30% 且后期崩溃（图 7）。\n- **推理延迟**：单 RTX 4090 上，RoboOmni 单次推理 **0.49×** 于 ASR+OpenVLA 流水线（图 9），省去 ASR 瓶颈。\n- **级联对比**：用 Qwen2.5-Omni 作高层规划器 + 文本 VLA 作控制器，平均成功率再降 20-40%，暴露语义漂移与副语言信息丢失问题（图 8）。\n\n综合实验表明，RoboOmni 在**成功率、鲁棒性、主动性、推理速度**上均显著优于现有文本或 ASR 级联方案，首次验证了端到端 omni-modal 框架在“无显式指令”场景下的可行性与优势。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步挖掘，分主题列出：\n\n- **模态与场景扩展**\n- 引入触觉、本体感觉、嗅觉等额外模态，验证框架在“全感知”场景下的可扩展性。\n- 将环境声从离散事件扩展为**连续声场**（如厨房煎炸强度、水流变化），考察细粒度声学语义对意图推断的帮助。\n- 走出桌面/厨房，测试**室外、工厂、零售**等复杂声学环境，评估混响、多径、非稳态噪声对端到端音频编码的鲁棒性。\n- **意图推理深度**\n- 当前指令仍多为单步操作，可构建**长程隐含任务**（如“准备下午茶”包含烧水、找茶叶、洗杯等），研究模型对**子目标分解与记忆管理**的能力。\n- 引入**多意图并存**场景：同一对话中多人提出冲突需求，机器人需做**多目标权衡与协商**，探索价值对齐与伦理约束机制。\n- 结合**用户情感与疲劳状态**（语调 + 视觉表情），实现**个性化时机选择**——何时主动询问、何时保持静默。\n- **数据与评价**\n- 建立**人类真实意图标注**而非仅“能否复现专家轨迹”——通过事后访谈或心理量表获得**隐式意图真值**，减少标注噪声。\n- 引入**对抗性音频测试**（轻微重音变化、同音异义、环境声掩蔽）系统评估模型对**声学分布偏移**的敏感度。\n- 设计**可解释性基准**：要求模型在确认前提供**可视化+语音理由**（“我听到门铃，结合您之前提到妈妈爱鱼丸，因此…”)，量化人机互信。\n- **模型效率与架构**\n- 研究**流式推理**下的延迟-性能权衡：在机器人必须“边听边想”时，如何以**少步前瞻**保持高成功率。\n- 探索**动作与语言解耦**方案：冻结 omni-modal 主干，仅训练轻量**动作低秩适配器**，实现多机器人 embodiment 的快速迁移。\n- 引入**事件驱动音频编码**（仅对关键声学事件计算高分辨率特征），降低 7×24 值守场景下的能耗。\n- **安全与伦理**\n- 建立**误推断代价评估**：在真实厨房中错误倒清洁剂而非饮料的安全风险巨大，需设计**不确定性阈值**与**紧急制动语音确认**机制。\n- 研究**说话人身份隐私**：儿童或老人声音易被模仿，如何结合**声纹+视觉身份一致性检测**防止恶意指令注入。\n- 探索**文化差异**对隐含意图的影响：同一语调在东亚 vs 欧美可能表达相反情绪，构建**跨文化多语种版本**并验证泛化。\n- **交互范式创新**\n- 让机器人具备**主动提醒**能力：根据环境声与视觉预测用户可能遗忘的动作（“炉具已空烧 5 分钟，是否需要关火？”），实现**双向主动性**。\n- 引入**持续对话记忆**：跨餐次、跨天记住家庭成员偏好，形成**长期个性化模型**，但需配套**数据遗忘与可撤销机制**以满足隐私法规。\n\n通过上述探索，可推动 RoboOmni 从“实验室多模态演示”走向**可部署、可信、可持续进化**的主动服务机器人系统。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有人机交互假设用户给出显式文本/语音命令，真实场景却多用**对话语调、环境声、视觉上下文**隐含意图；VLA 模型缺乏音频模态与主动推断能力。\n- **设定**：提出**交叉模态上下文指令**——机器人须实时融合**语音+环境声+视觉**，主动推理并语音确认后再执行。\n\n- **数据**：构造 140k 条的 **OmniAction** 数据集，含 5k+ 说话人、2.4k 事件声、640 背景，覆盖六类隐含指令（情感、重叠、非语言、身份、二人/三人对话）。\n- **模型**：**RoboOmni** 端到端 omni-modal LLM，采用 Perceiver–Thinker–Talker–Executor 架构，在**统一令牌空间**自回归生成文本、语音或 7-DoF 动作，无需 ASR。\n\n- **实验**：\n– 仿真基准成功率 **85.6%**，超最强文本基线 **25.9%**；\n– 真人语音鲁棒性 **76.6%**，优于 ASR 流水线；\n– 真实机器人部署，实现**主动询问-确认-执行**闭环，意图识别准确率 **88.9%**，推理延迟降至 0.49×。\n\n- **结论**：首次验证端到端 omni-modal 框架可在**无显式指令**场景下完成高成功率、低延迟、主动语音交互的机器人操作。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RoboOmni%3A%20Proactive%20Robot%20Manipulation%20in%20Omni-modal%20Context，链接是https://arxiv.org/pdf/2510.23763，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23763。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1761781638492","title":"Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents","description":"We present Game-TARS, a generalist game agent trained with a unified, scalable action space anchored to human-aligned native keyboard-mouse inputs. Unlike API- or GUI-based approaches, this paradigm enables large-scale continual pre-training across heterogeneous domains, including OS, web, and simulation games. Game-TARS is pre-trained on over 500B tokens with diverse trajectories and multimodal data. Key techniques include a decaying continual loss to reduce causal confusion and an efficient Sparse-Thinking strategy that balances reasoning depth and inference cost. Experiments show that Game-TARS achieves about 2 times the success rate over the previous sota model on open-world Minecraft tasks, is close to the generality of fresh humans in unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet in FPS benchmarks. Scaling results on training-time and test-time confirm that the unified action space sustains improvements when scaled to cross-game and multimodal data. Our results demonstrate that simple, scalable action representations combined with large-scale pre-training provide a promising path toward generalist agents with broad computer-use abilities.","url":"https://huggingface.co/papers/2510.23691","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.492Z","metadata":{"authors":["Zihao Wang","Xujing Li","Yining Ye","Junjie Fang","Haoming Wang","Longxiang Liu","Shihao Liang","Junting Lu","Zhiyong Wu","Jiazhan Feng","Wanjun Zhong","Zili Li","Yu Wang","Yu Miao","Bo Zhou","Yuanfan Li","Hao Wang","Zhongkai Zhao","Faming Wu","Zhengxuan Jiang","Weihao Tan","Heyuan Yao","Shi Yan","Xiangyang Li","Yitao Liang","Yujia Qin","Guang Shi"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.23691.pdf","arxivUrl":"https://arxiv.org/abs/2510.23691","arxivId":"2510.23691","coolPaperUrl":"https://papers.cool/arxiv/2510.23691","published":"2025-10-27T17:43:51Z","updated":"2025-10-27T17:43:51.000Z","zh_summary":"我们提出了 Game-TARS，一种通用游戏代理，它通过统一且可扩展的动作空间进行训练，该动作空间以符合人类习惯的原生键盘-鼠标输入为基础。与基于 API 或 GUI 的方法不同，这一范式能够在包括操作系统、网页和模拟游戏在内的异构领域中进行大规模持续预训练。Game-TARS 在超过 5000 亿个 token 上进行预训练，涵盖多样化的轨迹和多模态数据。关键技术包括用于降低因果混淆的递减持续损失，以及一种高效的稀疏思维策略，平衡推理深度与推理成本。实验表明，Game-TARS 在开放世界 Minecraft 任务中的成功率约为先前最先进模型的两倍，在未见过的网页 3D 游戏中接近新人类的通用性，并且在 FPS 基准测试中优于 GPT-5、Gemini-2.5-Pro 和 Claude-4-Sonnet。训练时间和测试时间的扩展结果确认，统一动作空间在扩展到跨游戏和多模态数据时仍能持续提升性能。我们的结果表明，简单且可扩展的动作表示结合大规模预训练，为实现具备广泛计算机使用能力的通用代理提供了有前景的路径。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心要解决的是“如何构建一个可扩展、跨域通用、且无需为每个新环境重新设计动作空间的游戏智能体”。具体而言，它针对以下瓶颈：\n\n- **传统游戏智能体动作空间与任务/环境强耦合**\n以往方法要么用游戏内专用 API（如 Minecraft 协议），要么依赖 GUI 元素解析，导致每换一款游戏就要重新定义动作集、标注数据，无法大规模持续预训练。\n\n- **高阶语义动作难以跨域迁移**\n基于“Craft(item)”之类高层语义的接口在视觉-语言模型（VLM）中表现良好，但缺乏像素级通用性，难以泛化到未见过的 3D 或 Web 游戏。\n\n- **模仿学习中因果混淆与长序列冗余**\n长轨迹中 90 % 以上动作为重复或空操作，标准交叉熵损失被低熵样本主导，模型易退化为“逆动力学”式抄作业，而非真正基于当前帧决策。\n\n- **推理与行动耦合度低、推理成本高**\n强行每步都输出 Chain-of-Thought 会拖慢实时性，且离线合成的推理文本常与真实行为脱节。\n\n为此，论文提出 Game-TARS 框架，其关键思路可概括为：\n\n1. **统一动作空间**\n把动作原子化到键盘-鼠标级别：\n\nA = keyPress(k), mouseMove(dx,dy), mouseClick(b), no-op\n\n该空间与任何 GUI/游戏解耦，使跨域数据可用同一格式大规模聚合。\n\n2. **持续预训练 + 衰减损失**\n对 500 B token 的多模态轨迹采用带指数衰减的权重：\n\nω_t = γ^(k_t-1),quad γ=1/2\n\n抑制连续重复动作的梯度，迫使模型关注真正需要改变决策的高熵边界。\n\n3. **稀疏思考（Sparse Thinking）**\n在线“think-aloud”采集原生推理，仅保留关键决策点的  (o_t,r_t,a_t) ，其余步骤仅  (o_t,a_t) ；推理密度通过拒绝采样进一步压缩，兼顾性能与推理成本。\n\n4. **后训练增强**\n\n- 自动动作空间增广：随机重映射按键，逼模型读 System Prompt 而非背动作先验。\n- 逆动力学辅助任务：给定  o_t,o_(t+1)  预测  a_t ，强化因果理解。\n- 长程记忆压缩：双 tier 记忆，用稀疏 thought 文本替代视觉 token，实现 2000+ 步历史压缩到 32 k 上下文内。\n5. **跨域数据混合**\n将代码生成、GUI 自动化、研究代理等非游戏轨迹纳入后训练，使模型从“游戏专家”升级为“通用计算机用户”。\n\n综上，论文旨在验证：**“简单、统一、可扩展的底层动作表示 + 大规模持续预训练”** 是通往通用数字代理的可行路径，而无需为每个新任务手工设计高层 API 或 GUI 解析器。\n\nQ2: 有哪些相关研究？\n\n以下研究按“动作空间层级”与“通用性程度”两条主线梳理，与 Game-TARS 形成对比或递进关系。所有提及公式均以 latex 行内格式给出。\n\n1\\. 环境专用/高层 API 型游戏智能体\n\n| 代表工作 | 动作空间 | 关键局限 |\n| --- | --- | --- |\n| AlphaGo/AlphaStar | 游戏规则定义的原子行动  a∈A_(game)  | 只能玩单一游戏，零跨域能力 |\n| Voyager | 调用 JavaScript API，如 bot.dig(x,y,z) | 动作语义与 Minecraft 深度绑定，无法迁移到 FPS/Web |\n| DreamerV3 | 离散/连续动作向量，由环境 API 提供 | 需为每款游戏重新设计奖励与观测包装器 |\n\n2\\. 像素→底层键鼠的原子动作（与 Game-TARS 同层级）\n\n| 代表工作 | 动作空间 | 与 Game-TARS 的差异 |\n| --- | --- | --- |\n| VPT | 同  keyPress,mouseMove,mouseClick  | 仅 Minecraft 数据 + 逆动力学预训练，未引入跨域持续预训练与稀疏思考 |\n| SIMA | 像素→键盘/鼠标，自然语言指令 | 训练任务短、游戏少，未验证 500 B token 级大规模持续预训练收益 |\n| OpenHA | Minecraft 专用，分层策略 | 动作空间仍含高层语义子策略，未做到完全统一原子动作 |\n\n3\\. GUI/元素级动作空间（介于 API 与键鼠之间）\n\n| 代表工作 | 动作空间 | 与 Game-TARS 的对比 |\n| --- | --- | --- |\n| UI-TARS | click(x,y), scroll(dir) 等 GUI 原子 | 依赖 OS/GUI 元素检测，跨平台差异大；Game-TARS 直接绕过 UI 解析 |\n| Mind2Web | HTML 元素路径 click(#submit) | 仅限 Web，无法处理 3D 游戏 |\n| OS-Atlas | 屏幕坐标级动作，但需外部 grounding 模型 | 动作空间仍受屏幕分辨率与 UI 框架约束 |\n\n4\\. 通用多任务/多模态智能体（非游戏为主）\n\n| 代表工作 | 动作空间 | 可借鉴点 |\n| --- | --- | --- |\n| Gato | 所有模态序列化为 token，动作离散化 | 动作 token 仍按领域手工分桶，未对齐人类原生输入 |\n| π0/pi0.5 | 机器人连续关节位置 | 共享“大规模预训练→下游微调”范式，但动作空间为扭矩/位姿，与键鼠异构 |\n| Claude-4 CUA | 屏幕坐标 + 键盘热键 | 闭源，未验证跨游戏泛化；Game-TARS 证明同等能力可用开源数据复现 |\n\n5\\. 因果混淆与模仿学习权重修正\n\n| 代表工作 | 关键思想 | 与 Game-TARS 衰减损失的关系 |\n| --- | --- | --- |\n| VPT（no-op 过滤） | 直接丢弃  a_t=no-op  样本 | 破坏动作分布完整性；Game-TARS 改用  ω_t=γ^(k_t-1)  软降权 |\n| Causal Confusion 文献 | 逆动力学模型误用状态差分预测动作 | Game-TARS 用视觉锚点（光标）重对齐时间戳，主动消除因果错位 |\n\n6\\. 稀疏思考与推理-行动耦合\n\n| 代表工作 | 稀疏策略 | 与 Game-TARS 的差异 |\n| --- | --- | --- |\n| ReAct | 每步强制输出 Thought | Game-TARS 仅于关键决策点触发  r_t ，减少 68→37 token/步 |\n| R1-Searcher | 用拒绝采样保留高奖励推理 | Game-TARS 将其用于“动作一致性”过滤：$I(π(a_t |\n\n7\\. 长程记忆与上下文压缩\n\n| 代表工作 | 记忆机制 | 与 Game-TARS 对比 |\n| --- | --- | --- |\n| Optimus-1 | 外部向量数据库检索 | Game-TARS 仅用文本化稀疏 thought 做滑动窗口压缩，无需额外检索器 |\n| GR-3 | 视觉 token 量化 | Game-TARS 直接丢弃过期视觉帧，保留 thought 文本，实现 2000+ 步→32 k token 内压缩 |\n\n小结\n\nGame-TARS 在动作空间层级上最接近 VPT 与 SIMA，但通过“持续预训练 + 跨域数据混合 + 稀疏思考 + 衰减损失”四重设计，首次验证了：\n\n- 统一键鼠原子动作可支撑 500 B token 级跨域预训练；\n- 在 Minecraft、FPS、Web 3D 等未见环境中零样本超越 GPT-5、Gemini-2.5-Pro、Claude-4-Sonnet 等闭源模型。\n\n因此，上述相关研究分别从不同维度（动作定义、数据规模、推理效率、记忆机制）构成 Game-TARS 的对比基线与理论基石。\n\nQ3: 论文如何解决这个问题？\n\n论文把“如何做出真正可扩展的通用游戏智能体”拆解成三个环环相扣的子问题，并给出对应的技术组件；各组件在训练-推理全链路中按顺序生效，最终形成 Game-TARS 系统。以下按“问题→解法→关键公式/算法”对应给出，避免第一人称。\n\n1\\. 动作空间与数据可扩展性\n\n**问题**\n\n- 传统 API/GUI 动作与具体环境强耦合，换游戏就要重写接口、重新标注，无法把海量异构轨迹合并成同一格式做大规模预训练。\n\n**解法**\n\n- **Human-Native Interaction 范式**：把任何操作退化到键盘-鼠标原子事件，定义与屏幕分辨率、UI 框架、游戏引擎无关的统一动作空间\n\nA= ,keyPress(k),; mouseMove(dx,dy),; mouseClick(b),; no-op,r\n\n- **跨域数据混合**：将 500+ 游戏、20 k 小时、GUI 自动化、代码生成、研究代理等轨迹全部转成上述格式，得到 526 B token 级持续预训练语料。\n\n2\\. 因果一致性与稀疏思考\n\n**问题**\n\n- 长轨迹 90 % 为重复动作，标准交叉熵损失被低熵样本淹没，模型退化成“抄上一帧”的逆动力学模型；\n- 离线给现成轨迹补思考难以对齐真实决策点，导致推理-行动因果链断裂。\n\n**解法**\n\n- **衰减损失**\n对连续相同动作指数降权：\n\nω_t = γ^(k_t-1),; γ=tfrac12,quad L= -∑_(t=1)^T ω_t logπ_θ(a_t|H_t)\n\n强制网络关注真正需要改变策略的高熵边界。\n\n- **在线 Think-Aloud + 视觉锚点对齐**\n\n1. 录制屏幕、键鼠、音频三流，用 ASR→LLM 提纯思考文本；\n2. 以鼠标光标为视觉锚点，用 grounding 模型把  (dx,dy)  信号重新对齐到真正发生的那帧，消除毫秒级延迟造成的因果错位；\n3. 只保留“关键决策点”处的  (o_t,r_t,a_t) ，其余步骤仅保留  (o_t,a_t) ，形成稀疏 ReAct 轨迹：\n\nτ=l[(r_0,a_0,o_0),,(a_1,o_1),,...,,(r_m,a_m,o_m),,...,,(r_T,a_T,o_T)r]\n\n- **拒绝采样精炼**\n先用纯行动模型  π_(action)  跑离线轨迹，把预测错误的时间步集合\n\nS_r=tmid Il[π_(action)(a_t|o_t,h_t)≠ a_t^(gt)r]=1r\n\n视为“必须加推理”的关键点；随后仅对这些步用 LLM 生成候选思考  r_t^(cand) ，只有当下述条件满足才保留：\n\nIl[π_θ(a_t^(gt)|o_t,r_t^(cand))=1r]\n\n从而保证每条思考都能因果地导向正确动作。\n\n3\\. 后训练强化与零样本泛化\n\n**问题**\n\n- 持续预训练后模型带有“动作先验”，会在新游戏里输出旧游戏的按键；\n- 连续动作（鼠标灵敏度）与离散动作（技能键语义）随环境变化，需要在线快速校准；\n- 长时任务需跨越 2000+ 步，而上下文只能装下 32 k token。\n\n**解法**\n\n- **自动动作空间增广**\n训练时随机把语义-按键映射打乱（例如把“前进”从 W 换成 X），并实时在 System Prompt 里给出当前有效映射，逼模型“看提示而不是背按键”。\n\n- **多模态 In-Context 校准**\n对每款新游戏提供 5–10 个“动作-效果”视频片段作为 prompt，模型通过对比历史帧自动推断  (dx,dy)  的实际灵敏度或技能键的真实功能，无需梯度更新。\n\n- **双 Tier 记忆压缩**\n- Tier-1（工作记忆）：保留最近  M  步完整  (o,r,a) ，占用 32 k token；\n- Tier-2（长程记忆）：超出  M  步后只保留稀疏 thought 文本，视觉帧丢弃，实现 2000+ 步历史压缩到几千 token。\n- **跨域代理数据联合微调**\n将代码 agent、GUI agent、MCP research agent 的 20 B token 轨迹与游戏数据混合后训练，进一步提升规划、反思、工具调用等高层能力，使模型从“游戏专家”泛化到“通用计算机用户”。\n\n4\\. 效果验证\n\n- **Minecraft MCU 基准**（800+ 未见任务）\nGame-TARS-MoE-mini 取得 72 % 成功率，比此前最佳专用模型提升 2×，平均步数减少 30 %。\n\n- **零样本 Web 3D / FPS / 导航 simulator**\n在 Poki 网页游戏、VizDoom、MiniWorld 等全新环境，Game-TARS-mini 平均得分超过 GPT-5、Gemini-2.5-Pro、Claude-4-Sonnet，且接近无经验人类水平。\n\n- **Scaling 实验**\n统一动作空间初期数据效率低于 GUI 方法，但当 token 从 10 B 增加到 500 B 时再提升 30 %，证实“简单表示 + 大数据”可持续受益，符合 bitter lesson 预言。\n\n总结\n\n论文通过“统一原子动作空间→因果对齐的稀疏思考→衰减损失抗重复→后训练增广与跨域混合”这一完整 pipeline，一次性解决了动作可扩展性、因果混淆、推理-行动耦合、长时记忆四大瓶颈，最终用单组权重在多个未见游戏环境里取得 SOTA 或近人类表现，验证了“底层统一动作 + 大规模持续预训练”是通往通用数字代理的有效路径。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“统一原子动作空间 + 大规模持续预训练”这一核心假设，从**域内鲁棒性**、**零样本泛化**、**推理效率**、**训练/测试时缩放规律**、**消融验证**五个维度设计实验。所有结果均基于**训练集未出现**的任务或完整未见环境。以下按实验目的→测试平台→指标→关键结论分层给出。\n\n1\\. 域内复杂任务：Minecraft MCU 基准\n\n**目的**\n验证在“熟悉但任务未见过”的开放世界里，通用模型能否超越专为 Minecraft 手工设计的强基线。\n\n**设置**\n\n- 手动构造 800+ 互不重复的起始地图，单局 ≤600 步；\n- 任务分三大类：Embodied（3D 探索+交互）、GUI（2D 合成界面）、Combat（生存战斗）；\n- 每任务 ≥3 次随机种子，报告 Average Success Rate (ASR) 与 Average Completion Steps。\n\n**主要对手**\nPolicy 类：VPT、STEVE-1、DreamerV3\nVLM 类：JARVIS-VLA、OmniJARVIS、OpenHA、UI-TARS-1.5\n\n**结果（表 3）**\n\n| 模型 | Embodied ASR | GUI ASR | Combat ASR |\n| --- | --- | --- | --- |\n| 之前最佳 UI-TARS-1.5 | 60% | 50% | 60% |\n| Game-TARS-Dense* | 85% | 55% | 70% |\n| Game-TARS-MoE-mini | 72% | 55% | 66% |\n\n- 在样本效率上，MoE-mini 平均步数比 UI-TARS 少 20–30 %；\n- 首次证明“非 Minecraft 专用”的统一动作模型可获得 2× SOTA 提升。\n\n2\\. 零样本泛化：全新视觉-交互范式\n\n**目的**\n测试模型对“训练阶段完全未接触”的网页 3D、FPS、极简 3D 导航环境的即时适应能力。\n\n2.1 Web 3D 游戏（Poki 平台）\n\n**环境**\nRace（赛车）、Jump-Only（平台跳跃）、Temple Run（无尽跑酷）、Airplane Flying（3D 避障）\n\n**指标**\nRace：到达首个检查点时间↓；Jump-Only：20 s 内通过关卡数↑；Temple Run：20 s 得分↑；Airplane：存活总得分↑\n\n**对照**\nGPT-5、无经验人类（4 人最佳成绩）\n\n**结果（图 7）**\n\n- Game-TARS-mini 在 Jump-Only 与 Airplane 两项**超越人类最佳**；\n- 四项全部优于 GPT-5，平均相对提升 25–40 %。\n\n2.2 FPS 环境 VizDoom\n\n**地图**\nBattle-1、Battle-2、Defend-the-Line、Defend-the-Center（训练集未出现）\n\n**指标**\n每局累积奖励↑\n\n**对照**\nGPT-5、Gemini-2.5-Pro、Claude-4-Sonnet\n\n**结果（图 8）**\n\n- Game-TARS-mini 在四张地图全部**显著领先**，平均奖励比第二名高 35 %；\n- 行为审计出现“拐角窥视、蛇形走位、预判射击”等高阶 FPS 技巧。\n\n2.3 极简 3D：MiniWorld Simulator\n\n**任务**\n寻物、避障、颜色-开关逻辑等共 10 张未见地图\n\n**指标**\n任务成功率↑\n\n**结果（图 9）**\n\n- 平均成功率 82 %，比专用 RL 基线（DreamerV3 移植）高 20 % 以上；\n- 表明统一动作空间可将“游戏中学到的导航与物理常识”零样本迁移到极简几何世界。\n\n3\\. 推理效率与思考模式消融\n\n**目的**\n验证“稀疏思考”是否在性能-推理成本间取得最优权衡。\n\n**对比模式**\n\n- No-thinking：直接输出动作；\n- Greedy Thinking：每步强制输出思考；\n- Efficient Thinking（论文稀疏策略）：仅关键帧思考。\n\n**测试床**\nMinecraft 2048 任务（需长程规划）、VizDoom Battle-1（高实时）\n\n**结果（表 4）**\n\n| 模式 | Minecraft ASR | 2048 Score | Doom Reward | Avg Tokens/步 |\n| --- | --- | --- | --- | --- |\n| No-thinking | 55 % | 445 | 13.2 | 22 |\n| Greedy | 45 % | 924 | 7.2 | 68 |\n| Sparse | 63 % | 924 | 11.5 | 37 |\n\n- 稀疏模式在“需推理”场景保持 Greedy 级高分，在“需反应”场景避免 latency，token 消耗减半。\n\n4\\. 训练时缩放 & 跨域数据贡献\n\n**目的**\n回答“统一动作空间是否真的随数据规模持续增益”。\n\n**协议**\n固定模型大小，逐步增加 token 类型：\n\n1. 10 B GUI 动作轨迹 → 2. 100 B 游戏原子动作 → 3. +200 B 异构游戏 → 4. +200 B 多模态&跨域代理数据\n\n**观测指标**\nMinecraft MCU 总体 ASR\n\n**结果（图 10 右）**\n\n- 阶段 1→2：ASR 从 28 % → 42 %，证实统一空间初期需要更多数据；\n- 阶段 2→3：+200 B 游戏数据 → 65 %，获得 23 % 绝对提升；\n- 阶段 3→4：再+跨域 200 B → 72 %，显示非游戏数据也能继续增强游戏表现，符合 Bitter Lesson。\n\n5\\. 测试时（推理步）缩放\n\n**目的**\n验证能否像 LLM 一样通过“多步自滚动+投票”进一步提升在线表现。\n\n**协议**\n对同一初始状态，模型进行  N=1,3,5,10  条并行 rollout，取投票或最佳轨迹；记录任务成功率。\n\n**结果（图 10 左）**\n\n- 在 Minecraft 与 2048 两类任务，ASR 随  N  单调上升；\n-  N=5  时平均提升 7 %， N=10  时提升 9.5 %，表明确实存在“推理时扩展”效应，但边际递减。\n\n6\\. 消融：衰减损失必要性\n\n**设置**\nw/ 与 w/o 衰减损失  (γ=frac12)  的两组模型，保持数据、超参相同。\n\n**评估**\n\n- 在线：Embodied、Combat、GUI 三类任务 ASR + 轨迹多样性；\n- 离线：100 K 步动作预测准确率 vs. 非重复准确率（N-Acc）。\n\n**结果（表 5）**\n\n- 在线 Combat ASR 从 14.5 % → 22.2 %；\n- 非重复准确率 12 % → 39 %，而全局准确率仅降 12 %；\n- 轨迹多样性接近翻倍，验证衰减损失有效抑制“复制上一帧”捷径。\n\n7\\. 长程记忆与上下文窗口压力测试\n\n**协议**\n在自定义“多跳寻物”地图中，目标物品与钥匙分别随机放置在相隔 1200–1800 步的两间密室；模型必须记住首次拾取颜色→返回中间门→再前往第二密室。\n\n**指标**\n长程一致性成功率（800 轮平均）\n\n**结果**\n\n- 双 Tier 记忆版本成功率 68 %；\n- 移除 Tier-2（仅 32 k 滑动窗口）成功率跌至 24 %；\n- 证明稀疏 thought 压缩是维持超长依赖的关键。\n\n结论汇总\n\n1. 统一原子动作空间在 500 B token 规模下获得**持续**增益，未出现平台；\n2. 零样本泛化覆盖网页 3D、FPS、极简 3D 导航，**平均超越 GPT-5 等闭源模型**；\n3. 稀疏思考策略在**实时性要求高的场景**比每步思考平均提速 1.8×，而长程推理任务保持同等精度；\n4. 衰减损失与双 Tier 记忆分别解决**因果混淆**与**长上下文瓶颈**，在后训练阶段仍可带来 10–20 % 绝对提升。\n\n以上实验共同支撑论文核心观点：**简单、统一、可扩展的动作表示 + 大规模持续预训练 → 通用数字代理的可行路径。**\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向按“短期可验证”→“中长期挑战”递进，均围绕 Game-TARS 留下的开放问题展开；每条给出**核心难点**与**可尝试技术路线**，供后续研究直接落地或长期探索。\n\n1\\. 动作-side 的细粒度与硬件泛化\n\n| 核心难点 | 当前键鼠原子动作仍属“离散-低精度”表示，无法表达 |\n| --- | --- |\n| - 亚像素级拖拽（Photoshop 钢笔路径） |\n| - 压感/倾斜/触控板多指手势 |\n| - 手柄摇杆连续信号、VR 6-DoF pose |\n\n| 代表工作 | 动作空间 | 与 Game-TARS 的差异 |\n| --- | --- | --- |\n| VPT | 同  keyPress,mouseMove,mouseClick  | 仅 Minecraft 数据 + 逆动力学预训练，未引入跨域持续预训练与稀疏思考 |\n| SIMA | 像素→键盘/鼠标，自然语言指令 | 训练任务短、游戏少，未验证 500 B token 级大规模持续预训练收益 |\n| OpenHA | Minecraft 专用，分层策略 | 动作空间仍含高层语义子策略，未做到完全统一原子动作 |\n\n0\n\n2\\. 实时性极限与“思考-行动”动态调度\n\n| 核心难点 | 即使稀疏思考，关键帧仍受 LLM 解码延迟 50–100 ms，对 FPS/格斗游戏仍显滞后；\n人类可在 10 ms 级完成视觉-脊髓反射。 | | --- | | 可尝试路线 | | a. **元认知门控**小模型：用 10 M-100 M 参数的轻量网络实时判断“是否调用大模型推理”，形成级联：\n\nπ_fast(a_t|o_t,h_t):confidence<τ⇒call π_large(r_t,a_t|o_t,h_t)\n\n训练方式：以 Game-TARS 为教师，对  π_fast  做蒸馏 + 强化学习，奖励 = 任务成功 − 调用延迟。 | | b. **事件相机 + 低延迟视觉编码器**，把输入延迟从 33 ms (30 FPS) 降到 1 ms 级，验证在 VizDoom 高速竞技场能否进一步提升胜率。 |\n\n3\\. 跨模态动作一致性：视觉-语言-音频-触觉\n\n| 核心难点 | 当前轨迹仅屏幕+键鼠+语音思考；真实人类还利用**游戏音效、手柄震动、触觉反馈**做决策。 | | --- | | 可尝试路线 | | a. 构建**Audio-Game-TARS**：同步录制立体声音频与震动信号，把音频波形转成 mel-spectrogram 作为额外模态输入；\n动作空间增加  haptic(f,a)  以预测震动强度，形成视听触联合策略。 | | b. 设计“**只听不看**”与“**只看只听**”两种消融，验证模型能否像人类一样通过脚步声判断敌人位置，评估模态缺失鲁棒性。 |\n\n4\\. 可验证安全与形式化约束\n\n| 核心难点 | Game-TARS 目前以最大化任务成功为唯一目标，可能在 Web 环境产生**误点击敏感按钮**（删除账户、购买确认）的风险。 | | --- | | 可尝试路线 | | a. 引入**形式化安全壳**：将“不允许动作”写成 LTL/CTL 公式，如\n\nG:negl(mouseClick(`ConfirmPurchase')r)\n\n使用 SMT-based mask 在解码阶段实时过滤非法动作，实现零样本安全约束。 | | b. **红队-对抗评估**：让另一智能体主动布置陷阱界面（钓鱼提示、虚假按钮），量化 Game-TARS 的误点击率，并迭代强化安全奖励。 |\n\n5\\. 终身学习与灾难性遗忘\n\n| 核心难点 | 持续预训练 500 B token 后，模型已“饱和”旧游戏；当新游戏数据流入，易覆盖早期知识。 | | --- | | 可尝试路线 | | a. **参数高效扩展**：冻结主干，为新游戏添加 1 %-5 % 参数的 MoE expert 或 LoRA 矩阵，使用  ell_2 -正则保持旧任务输出不变：\n\nmin_(Deltaθ):L_new+λ|Deltaθ|^2+μ·KL(π_(θ_0)|π_(θ_0+Deltaθ))\n | | b. **生成式回放**：用扩散模型把旧游戏视觉帧压缩成 latent，周期性重放，验证是否维持旧游戏 ASR ≥ 95 % 的同时吸收新游戏。 |\n\n6\\. 多智能体社会协作与 emergent 语言\n\n| 核心难点 | 当前 Game-TARS 仅单 agent；开放世界如 Minecraft 大型多人服务器需要**分工、谈判、 emergent 协议**。 | | --- | | 可尝试路线 | | a. **多体 Game-TARS**：每个实例共享统一动作空间，但接收不同视角视频流；\n在“30 分钟共建城堡”任务中，只允许多模态信道（屏幕像素+键盘公共频道）通信，观察是否自发产生**建筑区块编码**或**角色分工**（采集/搭建/防守）。 | | b. **语言-动作共生演化**：把公共键盘频道消息也视为动作的一部分，用信息论指标  I(msg;sub-task)  量化 emergent 语言复杂度，对比人类玩家聊天记录。 |\n\n7\\. 世界模型与反事实规划\n\n| 核心难点 | Game-TARS 目前为纯策略  π(a|o,h) ，缺乏可显式滚动的世界模型，难以进行“如果我现在转身，敌人将在 2 秒后出现在何处的**反事实模拟**”。 | | --- | | 可尝试路线 | | a. **视频-音频-动作联合世界模型**：输入  (o_t,a_t) ，预测  (o_(t+1),r_(t+1),audio_(t+1)) ；\n采用 Diffusion-Transformer，在 latent 空间做 16 步前瞻，再用模型预测控制 (MPC) 重选动作。 | | b. **反事实安全评估**：在 Web 购物场景，要求模型回答“如果我误点‘确认购买’，后续界面序列是什么？”\n用世界模型生成 100 条反事实轨迹，估计损失期望，实现**离线安全评估**而无需真机点按。 |\n\n8\\. 个人化与隐私保护联邦微调\n\n| 核心难点 | 真实用户键鼠轨迹含敏感信息（密码、聊天记录），无法直接上传中心服务器。 | | --- | | 可尝试路线 | | a. **联邦 LoRA**：各客户端在本地 1-2 小时个人数据上训练低秩矩阵，上传梯度前用 DP-SGD 加噪；\n服务器聚合后下发更新，验证个性化成功率（如个人快捷键适配）的同时保证  varepsilon -差分隐私。 | | b. **动作空间脱敏**：把真实密码输入一律映射到虚拟键 `KEY_PLACEHOLDER`，模型输出后再由本地驱动替换回真实键值，实现“模型不知键值”的端到端隐私。 |\n\n9\\. 科学发现：数据-规则未知环境\n\n| 核心难点 | 当前实验环境规则已知（Minecraft 合成表、Doom 奖励函数）；**若规则未知且无法咨询 Wiki**，智能体能否像科学家一样发现机制？ | | --- | | 可尝试路线 | | a. **开放实验室沙盒**：基于真实物理引擎构建“异世界”，内含隐藏公式（如“银+酸→神秘气体”）；\n只给像素与音频反馈，无文本说明。 | | b. **可复现性协议**：要求 Game-TARS 输出**自然语言报告** + **可重现实验步骤**，用外部符号验证器检查其陈述是否与物理引擎一致，量化“科学发现准确率”。 |\n\n10\\. 低资源与绿色 AI\n\n| 核心难点 | 500 B token 预训练对应上万 GPU-day，碳排高；如何在**数据-计算-性能**三面做帕累托优化？ | | --- | | 可尝试路线 | | a. **课程式小样本预训练**：先用 1 % 高价值人类演示（如世界 Top 10 速通录像）做质量筛选，再逐步加入低价值大众数据，验证能否用 50 B token 达到原版 90 % 性能。 | | b. **动态上下文剪枝**：对视觉 token 做重要性评分，实时丢弃 30 %-50 % 低分值 patch，仅对保留部分做自注意力，降低 FLOPs；在 VizDoom 测试是否维持原精度。 |\n\n以上方向既可单独成文，也可组合成“多模态-多智能体-终身-安全”统一框架，为下一代通用数字代理提供持续研究路线图。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 Game-TARS，一个“像素→键盘-鼠标”统一动作空间的通用游戏代理基础模型，核心主张是：**把动作退化到最底层、最通用的人类输入设备，从而把海量异构游戏、GUI、代码、研究轨迹合并成同一格式，进行大规模持续预训练，再经稀疏思考与后训练精炼，即可在无需重写 API 的前提下零样本泛化到任意数字环境。**\n\n主要内容可归纳为 5 句话：\n\n1. **动作空间统一**\n将操作原子化为  A=keyPress(k),,mouseMove(dx,dy),,mouseClick(b) ，与屏幕分辨率、UI 框架、游戏引擎解耦，实现跨域数据同一 token 序列。\n\n2. **500 B token 持续预训练**\n混合 500+ 游戏、20 k 小时轨迹、GUI/代码/研究代理与通用多模态数据，用**衰减损失**  ω_t=γ^(k_t-1)  抑制 90 % 重复动作带来的因果混淆，迫使模型关注高熵决策边界。\n\n3. **稀疏思考（Sparse Thinking）**\n在线“think-aloud”采集原生推理，经视觉锚点对齐后只保留关键决策点的  (o,r,a) ，其余仅  (o,a) ；后训练再用拒绝采样精炼，确保每条思考都能因果地导向正确动作，兼顾性能与推理成本。\n\n4. **后训练增强**\n自动动作空间增广、逆动力学辅助任务、多模态 In-Context 校准、双 Tier 长程记忆压缩、跨域代理数据混合，一举解决动作先验、连续动作校准、长时依赖与通用计算机使用能力。\n\n5. **实验验证**\n\n- Minecraft MCU 800+ 未见任务：72 % 成功率，比此前最佳提升 2×\n- 零样本 Web 3D、VizDoom、MiniWorld：平均超越 GPT-5、Gemini-2.5-Pro、Claude-4-Sonnet，达无经验人类水平\n- 训练/测试时缩放：数据增至 500 B token 仍持续增益；推理时多 roll-out 可再提 7–10 %\n- 消融：稀疏思考比每步思考节省 50 % token 且性能更高；衰减损失使非重复动作准确率从 12 % → 39 %\n\n综上，论文用“简单统一动作 + 大数据 + 稀疏思考”首次证明：无需为每款游戏定制 API，也能训练出通用、安全、可扩展的数字代理基础模型。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Game-TARS%3A%20Pretrained%20Foundation%20Models%20for%20Scalable%20Generalist%20Multimodal%20Game%20Agents，链接是https://arxiv.org/pdf/2510.23691，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23691。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1761781638492","title":"Uniform Discrete Diffusion with Metric Path for Video Generation","description":"Continuous-space video generation has advanced rapidly, while discrete approaches lag behind due to error accumulation and long-context inconsistency. In this work, we revisit discrete generative modeling and present Uniform discRete diffuSion with metric pAth (URSA), a simple yet powerful framework that bridges the gap with continuous approaches for the scalable video generation. At its core, URSA formulates the video generation task as an iterative global refinement of discrete spatiotemporal tokens. It integrates two key designs: a Linearized Metric Path and a Resolution-dependent Timestep Shifting mechanism. These designs enable URSA to scale efficiently to high-resolution image synthesis and long-duration video generation, while requiring significantly fewer inference steps. Additionally, we introduce an asynchronous temporal fine-tuning strategy that unifies versatile tasks within a single model, including interpolation and image-to-video generation. Extensive experiments on challenging video and image generation benchmarks demonstrate that URSA consistently outperforms existing discrete methods and achieves performance comparable to state-of-the-art continuous diffusion methods. Code and models are available at https://github.com/baaivision/URSA","url":"https://huggingface.co/papers/2510.24717","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.492Z","metadata":{"authors":["Haoge Deng","Ting Pan","Fan Zhang","Yang Liu","Zhuoyan Luo","Yufeng Cui","Wenxuan Wang","Chunhua Shen","Shiguang Shan","Zhaoxiang Zhang","Xinlong Wang"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.24717.pdf","arxivUrl":"https://arxiv.org/abs/2510.24717","arxivId":"2510.24717","coolPaperUrl":"https://papers.cool/arxiv/2510.24717","published":"2025-10-28T17:59:57Z","updated":"2025-10-28T17:59:57.000Z","zh_summary":"连续空间视频生成发展迅速，而离散方法由于误差累积和长时上下文不一致而滞后。在这项工作中，我们重新审视了离散生成建模，并提出了带度量路径的统一离散扩散（Uniform discRete diffuSion with metric pAth, URSA），这是一个简单而强大的框架，在可扩展视频生成方面弥合了与连续方法的差距。在核心上，URSA将视频生成任务表述为离散时空标记的迭代全局优化。它整合了两个关键设计：线性化度量路径（Linearized Metric Path）和分辨率依赖时间步移位机制（Resolution-dependent Timestep Shifting Mechanism）。这些设计使URSA能够高效扩展到高分辨率图像合成和长时视频生成，同时显著减少推理步骤。此外，我们引入了一种异步时间精调策略，在单一模型中统一了多种任务，包括插帧和图像到视频生成。在具有挑战性的视频和图像生成基准上进行的大量实验表明，URSA在性能上持续优于现有的离散方法，并实现了与最先进连续扩散方法相当的性能。代码和模型可在https://github.com/baaivision/URSA获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**离散空间视频生成**中存在的两大核心问题：\n\n1. **误差累积与长上下文不一致性**\n现有离散方法（如自回归 AR、掩码扩散 MDM）采用“一次生成、不可回退”的局部生成范式，导致误差随序列长度指数级累积，难以保持长视频在语义、运动、外观上的全局一致性。\n\n2. **与连续扩散模型的性能差距**\n连续扩散模型通过“全局迭代精修”在图像/视频生成上已建立显著优势，而离散方法因缺乏可回退的精修机制，在视觉保真度、动态合理性、文本对齐等方面明显落后。\n\n为此，作者提出 **URSA（Uniform discRete diffuSion with metric pAth）**，将视频生成重新定义为**离散时空 token 上的全局迭代精修任务**，用三个关键设计把离散范式推向与连续范式相当的水平：\n\n- **Linearized Metric Path**：在 token 嵌入空间构造线性可逆的概率路径，实现精细的扰动控制。\n- **Resolution-dependent Timestep Shifting**：按分辨率动态调整时间步，保证高分辨率长序列训练稳定。\n- **Asynchronous Timestep Scheduling**：每帧独立采样噪声水平，统一支持文本生成视频、图生视频、插值、外推等多任务。\n\n实验表明，URSA 在 VBench、DPG-Bench、GenEval 等基准上**全面超越现有离散方法**，并与当前最优的连续扩散模型**性能持平或更好**，首次证明离散框架可在高分辨率、长时长、多任务视频生成场景中与连续框架竞争。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为 **连续空间视觉生成** 与 **离散空间视觉生成** 两大脉络，URSA 在二者之间建立桥梁。按类别梳理如下：\n\n连续空间视觉生成\n\n- **扩散模型**\n- DDPM / Score-SDE： p_t(x)  通过高斯噪声逐步扰动，学习反向去噪。\n- 代表性工作：Ho et al. 2020；Song et al. 2021；Kong et al. 2024；HunyuanVideo；Wan2.1；Step-Video。\n- **Flow / 一致性模型**\n- 直接回归速度场  u_t(x) ，减少采样步数。\n- 代表：FLUX、SD3、Pyramid Flow、Lumina-Video。\n- **Autoregressive + 连续头**\n- MAR、NOVA：用 LLM 自回归预测连续 token，再经扩散头精修。\n\n离散空间视觉生成\n\n1) 自回归（AR）\n\n- 逐 token 顺序生成，上下文固定后不可更改。\n- 图像：VQ-GAN、DALL-E、CogView、Emu3、Lumos-1。\n- 视频：VideoPoet、VideoGPT、TATS、Loong。\n\n2) 掩码扩散 / 掩码生成\n\n- 并行预测被掩码的离散 token，可双向上下文。\n- 代表：MaskGIT、Muse、MagViT、FUDOKI、Meissonic、Bai et al. 2025。\n\n3) 离散 Flow Matching\n\n- 在离散状态空间构造概率路径  p_t(x) ，用 CTMC 速度场  u_t  迭代精炼。\n- 代表：Discrete Flow Matching (Gat et al. 2024)、Kinetic-Optimal Path (Shaul et al. 2025)。\n- URSA 直接继承并扩展该路线，首次将其应用于**长序列、高分辨率、多任务视频生成**。\n\n多任务 / 长视频训练技巧\n\n- **Diffusion Forcing**（Chen et al. 2024a）：帧级独立噪声调度，支持任意长度自回归外推。\n- **3D-RoPE / M-RoPE**（Wang et al. 2024a；Liao et al. 2025）：为视频时空序列设计的位置编码。\n- **Resolution-dependent SNR**（Esser et al. 2024）：高分辨率需要更大扰动，URSA 引入 timestep-shift 实现离散版。\n\n总结\n\nURSA 的**直接理论基石**是 Discrete Flow Matching；**方法论灵感**来自连续扩散的全局迭代精修与 diffusion forcing 的帧级异步调度；**实验对比**覆盖了当前主流的连续与离散视频/图像生成模型，形成一条从“离散 token 全局精炼”到“长视频多任务统一”的完整研究脉络。\n\nQ3: 论文如何解决这个问题？\n\n论文把“离散空间视频生成落后”的根本原因归结为**缺乏可回退的全局精修机制**，进而提出 **URSA** 框架，用三条技术路线一次性解决误差累积、长上下文不一致、与连续扩散性能差距三大难题：\n\n1\\. 把“生成”重新定义为**离散 token 上的全局迭代精修**\n\n- 不再沿用 AR“逐 token 顺序产生、一旦生成就固定”或 MDM“一次并行补全”范式。\n- 从均匀类别噪声 $x_0sim Unif(\nK\n^D) 出发，沿**可逆概率路径** p\\_t(x|x\\_1) 逐步精炼到真实数据 x\\_1$。\n- 每一步所有 token 同时更新，可随时纠正早期错误，概念上与连续扩散的“全局去噪”对齐。\n\n2\\. 构造**线性化度量概率路径**——让扰动“可预测、可控制”\n\n- 在 token 嵌入空间定义度量  d(x,x_1) ，令\n\np_t(x|x_1)=softmaxl(-β_t d(x,x_1)r), quad β_t=cl(t(t) / (1-t)r)^α\n\n通过  (c,α)  网格搜索强制  d(x_t,x_1)  与  t  呈**线性关系**，实现：\n- 相同 timestep 下扰动强度恒定，训练更稳；\n- 高分辨率/长序列也能用更少步数收敛（25–50 步即可）。\n\n3\\. 分辨率自适应——**Resolution-dependent Timestep Shifting**\n\n- 高分辨率像素更多，需要更大扰动。引入 shift 参数  λ ：\n\ntilde t=(t) / (t+λ(1-t))\n\n-  λ>1  对应“凸”映射，提前加大噪声；\n-  λ<1  对应“凹”映射，低分辨率保持温和扰动。\n- 与线性化路径正交，**按图像/视频尺寸动态校准 SNR**，长视频训练不再发散。\n\n4\\. 多任务统一——**Asynchronous Timestep Scheduling**\n\n- 每帧独立采样  t_isim U(0,1) ，而非整段视频共享同一噪声水平。\n- 结果：\n- 天然支持**图生视频**（首帧  t=1 ，其余  t<1 ）；\n- 支持**插值/外推**（已知帧  t=1 ，中间帧  t∈(0,1) ）；\n- 支持**起始-终止帧控制**（首尾  t=1 ，中间  t∈(0,1) ）。\n- 一个模型权重完成所有任务，无需额外微调。\n\n5\\. 训练 & 采样流程\n\n- **训练**：LLM 骨架接收“文本 token + 噪声视觉 token”拼接序列，最小化交叉熵\n\nmathcal L=mathbb E_(t,x_1,x_t)l[-log p_θ(x_1|x_t,text)r]\n\n- **采样**：用 Euler 求解器沿速度场  u_t  迭代更新\n\nx_(t+h)=x_t+h· u_t(x_t|hat x_1)\n\n25–50 步即可得到高质量离散序列，再经 tokenizer 解码为图像/视频。\n\n6\\. 实验验证\n\n- **VBench**：URSA 1.7B 取得 82.4 分，**超越所有离散模型**，与 13B-30B 连续模型打平。\n- **DPG-Bench**：1024×1024 图像 86.0 分，**领先此前离散方法 4+ 分**。\n- **消融实验**证实：\n- 无迭代精炼 → 步数减少即崩溃；\n- 无非线性路径 → 距离-time 相关性下降，性能下滑；\n- 无时移 → 高分辨率视频质量骤降；\n- 异步调度 → 多任务零样本通用。\n\n总结\n\nURSA 用“**离散 token 全局迭代精修**”取代“一次生成”，再用“**线性度量路径 + 分辨率时移 + 异步帧级调度**”解决长序列、高分辨率、多任务难题，**首次让离散视频生成在公开基准上与连续扩散模型不分伯仲**。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **文本-图像**、**文本-视频**、**图像-视频** 三大生成任务，以及 **长视频外推**、**起止帧控制** 两个零样本扩展场景，系统评估了 URSA 的性能与消融效果。具体实验如下：\n\n1 主实验：与 SOTA 对比\n\n| 任务 | 基准 | 对比对象 | 关键指标 |\n| --- | --- | --- | --- |\n| 文本→视频 | VBench (Huang et al. 2024a) | 10+ 连续模型（HunyuanVideo、Wan2.1、Step-Video…）+ 离散模型（Emu3、Lumos-1） | Total / Quality / Semantic / Dynamic / Imaging |\n| 图像→视频 | VBench++ (Huang et al. 2024b) | 8 款连续专用 I2V 模型（DynamiCrafter、SEINE、MAGI-1…） | I2V-Subject / I2V-Background / Camera Motion |\n| 文本→图像 | DPG-Bench (Hu et al. 2024) + GenEval (Ghosh et al. 2024) | SDXL、SD3、FLUX、NOVA、SANA、Mogao、Show-o2… | Overall / Entity / Attribute / Relation / Colors / Position |\n\n**结果一览**\n\n- VBench：URSA 1.7B → 82.4 分，**离散第一**，与 13B-30B 连续模型并列头部。\n- VBench++：86.2 分，**超越所有离散方法**，与 14B-30B 连续模型持平。\n- DPG-Bench：1024² 图像 86.0 分，**领先此前离散最佳 4.4 分**；GenEval 改写提示下 0.80 Overall，逼近 Show-o2。\n\n2 零样本长视频能力\n\n| 场景 | 设置 | 生成长度 | 可视化 |\n| --- | --- | --- | --- |\n| 视频外推 | 初始 49 帧 → 滑动窗口 13 帧自回归 | 481 帧（≈40 s） | Figure 9 |\n| 起止帧控制 | 首帧 + 末帧固定，中间 45 帧补全 | 49 帧 | Figure 10 |\n\n**结论**：异步帧级调度让同一套权重无需微调即可完成分钟级外推与精确首尾控制，**离散模型首次实现该能力**。\n\n3 消融实验（Controlled Ablations）\n\n| 变量 | 设置 | 观测指标 | 结论 |\n| --- | --- | --- | --- |\n| 迭代精炼 | 掩码扩散 vs 均匀混合路径 vs 度量路径 | GenEval / VBench 随步数变化曲线 | 度量路径 25 步≈掩码 250 步；无精炼步数减半即崩溃 |\n| 路径线性 | 固定 α∈{0.5,1.0}, c∈{1,5}；Pearson 相关 | d(xt,x1)–t 线性系数 / 训练 60 k 迭代 GenEval | α=1,c=5 相关系数 −0.997，GenEval 最高 |\n| 模型规模 | 0.6 B → 1.7 B → 4 B | DPG-Bench / GenEval / VBench | 参数量↑ 语义指标显著↑，质量指标饱和，** tokenizer 成新瓶颈** |\n| timestep 条件 | 嵌入层 / 文本提示 / 无条件 | GenEval 得分 & 训练损失方差 | 30 k 迭代后条件化无收益；嵌入层方差↑ 反而降性能 |\n| resolution shift | λ∈{1,2,3,4} | VBench Imaging & Semantic | λ=3 最优，再增大无收益；离散扩散同样需分辨率相关 SNR |\n\n4 训练效率与可扩展性\n\n- **数据**：16 M 图文对 + 12 M 视频对，共 28 M 样本。\n- **算力**：128×A100 40 GB，≈相同 epoch 下 0.6 B/1.7 B/4 B 模型训练时间线性增长。\n- **推理**：\n- 图像 1024² 25 步 ≤ 1 s（A100）；\n- 视频 49×512×320 50 步 ≈ 8 s，**步数仅为连续扩散 1/4–1/2**。\n\n5 可视化补充\n\n- **图 4**：不同路径/步数在 GenEval、VBench 上的**得分-步数曲线**。\n- **图 5**：路径线性度与收敛速度**定量相关**。\n- **图 6**：模型规模**语义收益显著，质量饱和**。\n- **图 7**：timestep 条件**早期有用，后期可完全移除**。\n- **图 8**：λ 值与 VBench 各子项**单峰关系**，验证分辨率时移必要性。\n\n一句话总结\n\nURSA 通过**主实验对标 SOTA**、**零样本长视频验证通用性**、**系统消融定位关键设计**，首次证明**离散全局迭代精炼**可在全维度与连续扩散模型竞争。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为**理论、模型、数据、系统、应用**五大类，均直接对应 URSA 尚未完全解决或尚未触及的开放问题：\n\n1 理论层面\n\n- **离散收敛速率上界**\n目前仅通过实验观察到 25–50 步即饱和，但缺乏 **Wasserstein/KL 意义下的步数-误差紧上界**，可借鉴连续扩散的 _Denoising Diffusion Theory_ 建立离散版本。\n\n- **度量路径的最优性**\n线性化路径只是嵌入空间的一种局部欧氏度量，若改用 **学习性度量**（如 GNN 或对比式度量）能否进一步缩短路径长度、降低步数？\n\n- **异步采样的数学表征**\n帧级独立  t_isim U(0,1)  目前为启发式，可探索 **最优调度分布**  p(t_i_(i=1)^n)  使似然或 Fisher 信息最大化，赋予理论最优性。\n\n2 模型与算法\n\n- **Tokenizer 瓶颈**\n实验已揭示性能随模型规模增加而**语义饱和**，主因是 8×8×4 压缩率 & 64 K 码本容量不足。可探索：\n\n- **矢量量化改进**：FSQ→LFQ、RQ、SQ；或 **连续-离散混合 tokenizer**（VAE+VQ 联合训练）。\n- **原生高分辨率 tokenizer**：16×16×1 压缩 256×256 帧仅 256 token，实现“ token 数-分辨率”线性扩展。\n- **统一 3D 连续-离散框架**\n尝试把 **连续流匹配** 与 **离散流匹配** 放在同一速度场里：空间低频用连续向量，高频细节用离散 token，实现**粒度自适应**的混合生成。\n\n- **自监督预训练**\n目前 URSA 完全依赖文本-视觉对。可引入 **无文本掩码预训练**（类似 BERT 或 MAE），再文本微调，有望降低标注需求 10× 以上。\n\n- **一步/两步蒸馏**\n离散路径已具备速度场，可应用 **一致性蒸馏** 或 **最优传输映射** 把 25→1 步，实现**实时视频生成**。\n\n3 长序列与内存\n\n- **线性注意力 / 局部-全局混合**\n全注意力在 10 K token 级别仍可行，但**分钟级 100 K+ token** 显存爆炸。可引入 **Long-short HiP attention**、**Ring-Attention** 或 **Mamba-style 线性序列混合**，把上下文长度推向 10⁶ 量级。\n\n- **时间-空间因子化路径**\n当前路径对所有 token 一视同仁，可设计 **t-space 与 t-time 分离** 的因子化速度场，降低每步 FLOPs 并提升长时一致性。\n\n4 数据与评测\n\n- **高密度视频-文本对**\n现有 24 M 视频对仍低于图文规模 1 个量级。构建 **100 M 级、平均 15 s、密集字幕** 的数据集，可验证离散模型是否继续随数据线性提升。\n\n- **细粒度可控评测**\nVBench 仅覆盖常见语义。可引入 **物理一致性基准**（碰撞、重力、流体）、**相机标定基准**（已知内参/轨迹，测几何误差），验证离散模型在**物理世界仿真**上的极限。\n\n5 系统与应用\n\n- **边缘端部署**\n探索 **INT4/INT8 量化 + 一步蒸馏** 在移动端 SoC 上跑 512×512 图像或 256×256 短视频，实现**本地文本生成视频**。\n\n- **交互式编辑**\n利用异步帧调度天然支持 **任意帧条件** 的特性，开发**实时视频编辑界面**（用户点击任意帧改内容，模型即时重生成局部片段）。\n\n- **多模态统一**\n将音频离散 token 并入同一序列，实现 **文本+音频驱动视频** 或 **视频到音频** 的端到端统一模型，迈向**全模态世界模型**。\n\n6 总结性一句话\n\nURSA 打开了“离散全局迭代精炼”的大门，**理论最优路径、原生高分辨率 tokenizer、线性长上下文、混合连续-离散、一步蒸馏与物理级评测** 将是下一步最值得深耕的五大赛道。\n\nQ6: 总结一下论文的主要内容\n\n**Uniform Discrete Diffusion with Metric Path (URSA)** 重新审视离散视觉生成，提出“全局迭代精修”范式，在图像与视频任务上首次达到连续扩散同等水平。核心内容可概括为四点：\n\n1. 问题定位\n离散方法因“一次生成不可回退”导致误差累积、长上下文不一致，性能远落后于连续扩散。\n\n2. 技术方案\n\n- **线性化度量概率路径**：在 token 嵌入空间构造  p_t(x|x_1)∝exp(-β_t d(x,x_1)) ，强制  d(x_t,x_1)∝t ，实现精细扰动控制。\n- **分辨率相关时移**：$tilde t=t/\nt+λ(1-t)\n$，高分辨率自动加大噪声，保证长序列训练稳定。\n- **异步帧级调度**：每帧独立采样  t_i∼U(0,1) ，统一支持文本-视频、图-视频、插值、外推等多任务，无需微调。\n3. 实验结果\n\n- VBench 文本→视频 82.4 分，**离散第一**，与 13 B–30 B 连续模型并列头部。\n- VBench++ 图像→视频 86.2 分，**超越所有离散方法**，与最优连续模型持平。\n- DPG-Bench 文本→图像 86.0 分，**领先此前离散最佳 4+ 分**。\n- 零样本生成 481 帧（≈40 s）外推与起止帧控制，**离散模型首次实现分钟级视频**。\n4. 结论与意义\nURSA 用“离散 token 全局迭代精炼”对齐连续扩散，**25–50 步即达高质量**，证明离散范式可扩展、可通用，为统一连续-离散生成提供新基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Uniform%20Discrete%20Diffusion%20with%20Metric%20Path%20for%20Video%20Generation，链接是https://arxiv.org/pdf/2510.24717，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24717。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1761781638493","title":"Repurposing Synthetic Data for Fine-grained Search Agent Supervision","description":"LLM-based search agents are increasingly trained on entity-centric synthetic data to solve complex, knowledge-intensive tasks. However, prevailing training methods like Group Relative Policy Optimization (GRPO) discard this rich entity information, relying instead on sparse, outcome-based rewards. This critical limitation renders them unable to distinguish informative \"near-miss\" samples-those with substantially correct reasoning but a flawed final answer-from complete failures, thus discarding valuable learning signals. We address this by leveraging the very entities discarded during training. Our empirical analysis reveals a strong positive correlation between the number of ground-truth entities identified during an agent's reasoning process and final answer accuracy. Building on this insight, we introduce Entity-aware Group Relative Policy Optimization (E-GRPO), a novel framework that formulates a dense entity-aware reward function. E-GRPO assigns partial rewards to incorrect samples proportional to their entity match rate, enabling the model to effectively learn from these \"near-misses\". Experiments on diverse question-answering (QA) and deep research benchmarks show that E-GRPO consistently and significantly outperforms the GRPO baseline. Furthermore, our analysis reveals that E-GRPO not only achieves superior accuracy but also induces more efficient reasoning policies that require fewer tool calls, demonstrating a more effective and sample-efficient approach to aligning search agents.","url":"https://huggingface.co/papers/2510.24694","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.493Z","metadata":{"authors":["Yida Zhao","Kuan Li","Xixi Wu","Liwen Zhang","Dingchu Zhang","Baixuan Li","Maojia Song","Zhuo Chen","Chenxi Wang","Xinyu Wang","Kewei Tu","Pengjun Xie","Jingren Zhou","Yong Jiang"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.24694.pdf","arxivUrl":"https://arxiv.org/abs/2510.24694","arxivId":"2510.24694","coolPaperUrl":"https://papers.cool/arxiv/2510.24694","published":"2025-10-28T17:50:40Z","updated":"2025-10-28T17:50:40.000Z","zh_summary":"基于大语言模型（LLM）的搜索代理越来越多地在以实体为中心的合成数据上进行训练，以解决复杂的、知识密集型的任务。然而，现行的训练方法如群体相对策略优化（GRPO）会丢弃这些丰富的实体信息，而是依赖稀疏的、基于结果的奖励。这一关键限制使它们无法区分信息量丰富的“近似错误”样本——即推理过程基本正确但最终答案有误的样本——与完全失败的样本，从而丢失了宝贵的学习信号。我们通过利用训练过程中被丢弃的实体来解决这一问题。实证分析显示，在代理的推理过程中识别出的真实实体数量与最终答案的准确性之间存在显著的正相关关系。基于这一洞察，我们提出了实体感知群体相对策略优化（E-GRPO），这是一种新颖的框架，它制定了密集的、实体感知的奖励函数。E-GRPO会根据实体匹配率为错误样本分配部分奖励，使模型能够有效地从这些“近似错误”中学习。在多样化问答（QA）和深度研究基准测试中的实验表明，E-GRPO在性能上持续且显著地超越了GRPO基线。此外，分析结果显示，E-GRPO不仅实现了更高的准确性，还促使生成更高效的推理策略，所需工具调用更少，从而展示了一种更有效且样本效率更高的搜索代理对齐方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对现有基于 GRPO（Group Relative Policy Optimization）的搜索智能体训练范式存在的“稀疏奖励”与“近失样本”问题，提出一种细粒度、实体感知的强化学习框架 E-GRPO。核心待解决问题可归纳为：\n\n- **奖励稀疏性**：GRPO 仅依赖最终答案正确性给出 0/1 奖励，无法区分\n\n1. 推理过程已捕获大部分关键实体、仅最后一步出错的“近失”样本；\n2. 全程推理错误的完全失败样本。\n二者被同等惩罚，导致大量有用学习信号被丢弃。\n- **过程监督难以落地**：在开放、动态、冗长的网页搜索场景下，引入 PRM 或树搜索等细粒度监督方法面临标注成本高昂、轨迹过长、计算不可行等障碍。\n- **实体信息浪费**：主流实体中心合成数据在生成阶段保留了大量支撑答案的“黄金实体”，却在训练阶段被直接丢弃，未被用作中间过程的质量信号。\n\n因此，论文旨在**不增加额外标注或模型前提下**，将合成数据中原生却未被利用的实体信息转化为**密集、可计算、可解释**的奖励信号，使策略优化能够识别并充分利用“近失”样本，从而提升搜索智能体的样本效率与最终准确率。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可划分为三条主线，均围绕“搜索智能体”“合成数据”与“强化学习奖励设计”展开：\n\n1. 搜索智能体与 ReAct 范式\n\n- ReAct (Yao et al., 2023) 提出“思考-行动”交替框架，成为后续搜索智能体的通用交互范式。\n- R1-Searcher、WebSailor、WebDancer、DeepResearcher 等 (Song et al., 2025; Li et al., 2025b; Wu et al., 2025a; Zheng et al., 2025) 沿此范式，在 QA 与深度研究任务上扩展工具集与推理长度。\n2. 实体中心合成数据生成\n\n- ASearcher (Gao et al., 2025) 通过“实体注入-模糊化”迭代提升问题复杂度。\n- SailorFog-QA (Li et al., 2025b) 基于知识图谱随机游走采样实体子图再生成问题。\n- 这些方法共同特点是：在合成阶段显式构造并保留一组“黄金实体”，但后续训练仅使用最终 QA 对，实体信息被丢弃。\n3. 强化学习与稀疏奖励缓解\n\n- GRPO 家族 (Shao et al., 2024; Yu et al., 2025; Dong et al., 2025) 采用组内相对优势，仅依赖 0/1 结果奖励，导致稀疏信号。\n- PRM/过程奖励模型 (Fan et al., 2025; Anonymous, 2025) 在数学、代码领域逐步给分，但需要昂贵的人工标注或模型训练。\n- 树搜索/在线策略采样 (Yang et al., 2025; Hou et al., 2025) 通过蒙特卡洛或 MCTS 估计中间价值，计算开销大，难以直接用于几十步的网页搜索轨迹。\n\n本文首次指出：上述实体中心合成数据已天然携带细粒度过程信号，无需额外标注即可转化为密集奖励，从而填补了“合成数据生成”与“RL 奖励设计”之间的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 Entity-aware Group Relative Policy Optimization（E-GRPO），在零额外标注、零额外模型的前提下，把被丢弃的“黄金实体”转化为密集奖励，三步解决稀疏奖励与近失样本问题：\n\n1. 实体匹配率量化\n对每条 rollout 的 `<think>` 段落做**精确字符串匹配**，统计命中黄金实体集合\n E_q  的比例，得到原始匹配率\n\nγ_i = |E_(matched)^((i))||E_q|\n\n再按组内最大值归一化，得到与问题难度无关的\n\nhatγ_i = γ_i / max_j γ_j ∈ [0,1]\n\n2. 实体感知奖励函数\n在 GRPO 的 0/1 结果奖励基础上，为**错误样本**追加与  hatγ_i  成比例的 partial credit：\n\nR_i = 1 & if correct[4pt] α · hatγ_i & if wrong[4pt] 0 & format/长度错误\n\n超参  α=0.3  平衡“答对”与“找到实体”两项信号。近失样本因  hatγ_i  高而获得更大优势，避免与完全失败样本同等惩罚。\n\n3. 组相对优势更新\n用新奖励  R_i  重新计算组内均值与标准差，得到更细粒度的优势\n\nhat A_(i,j)= (R_i - μ_R) / (σ_R)\n\n再代入标准 GRPO 的 clipped importance sampling 目标进行策略梯度更新。\n此外，移除 KL 正则、提高 clip 上限以鼓励探索；格式/过长轨迹 reward 置 0 但不参与 loss，稳定训练。\n\n通过“合成数据自带实体→零成本密集奖励→区分近失与完全失败”，E-GRPO 在不增加任何标注或辅助模型的情况下，显著提升了搜索智能体的样本效率、最终准确率与工具调用效率。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“算法有效性”与“场景鲁棒性”两条主线展开，覆盖 11 个公开基准、两种环境、两类模型规模，共 4 组对比设置：\n\n1. 基准与环境\n\n- QA 任务\n– 单跳：Natural Questions、TriviaQA、PopQA\n– 多跳：2WikiMultiHopQA、HotpotQA、Bamboogle、MuSiQue\n- 深度研究任务：GAIA、BrowseComp、BrowseComp-ZH、xbench-DeepSearch\n- 训练/评测环境\n– Local：基于 2024-Wikipedia 的封闭检索库\n– Web：实时 Google Search + Jina 页面抓取\n2. 模型与训练配置\n\n- 基座：Qwen2.5-7B-Instruct、Qwen3-30B-A3B-Instruct（MoE）\n- 阶段：\n– 冷启动 SFT：11 k SailorFog-QA 样本\n– RL：各 1 k 自建实体保留数据集，组大小 G=8，α=0.3，训练 5 epoch\n3. 主实验结果\n\n- Local-7B 在 7 项 QA 平均得分\n– SFT 60.2 → GRPO 61.4 → **E-GRPO 64.2**（+2.8）\n- 同一模型 Web 环境零样本迁移\n– **E-GRPO 67.8**，仍高于 GRPO 66.2 及其他 ≤14 B 开源对手\n- Web 环境深度研究 Pass@1\n– 7B：GRPO 6.3 → **E-GRPO 9.3**（BrowseComp）\n– 30B：GRPO 12.3 → **E-GRPO 12.9**（BrowseComp），**26.4**（BrowseComp-ZH），均居 ≤32 B 模型第一\n4. 分析实验\n\n- 训练曲线：E-GRPO 收敛更快，平均工具调用次数降低 ~10 %\n- 消融 α：α=0.3 时四项基准平均 Pass@1 最高，α=0.5 反而下降\n- 实体匹配-准确率相关性：训练过程中两者皮尔逊 r>0.85，验证实体信号有效性\n- 案例对比：同问题下 E-GRPO 轨迹命中全部 3 个黄金实体并答对；GRPO 轨迹漏掉关键实体导致错误答案\n\n综上，论文在“封闭/开放”“小/大模型”“QA/深度研究”多维度均验证了 E-GRPO 相对 GRPO 基线的一致命名提升，同时带来更高样本效率与更少工具调用。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“信号扩展”“策略优化”“场景迁移”与“理论分析”四类，供后续研究参考：\n\n- **信号扩展**\n- 超越字符串精确匹配：引入可验证的“事实三元组”或“实体链接 ID”，降低同义词或别名漏匹配带来的噪声。\n- 多粒度混合信号：将实体匹配与答案片段 F1、引用准确率、时间一致性等结合，构建多维稠密奖励向量。\n- 动态实体权重：对支撑不同推理跳的核心实体赋予更高奖励权重，弱化冗余背景实体的影响。\n- **策略优化**\n- 自适应 α：随训练进程或组内统计量自动调节实体奖励占比，避免后期过度关注中间信号而忽视最终答案。\n- 分层优势估计：对思考步与行动步分别计算优势，实现真正的“步级”信用分配，而非整条轨迹共享同一优势值。\n- 离线强化学习：利用大规模实体标注的离线轨迹，结合保守 Q 学习或对比学习，进一步降低在线交互成本。\n- **场景迁移**\n- 多模态搜索：将实体概念扩展到图像、表格、PDF 片段，验证 E-GRPO 在图文混合检索中的通用性。\n- 长周期科研助手：把“实体”泛化为“实验指标”“论文引用”等科研实体，测试在实验设计、文献调研等更长周期任务上的收益。\n- 工具扩展：在代码解释器、数据库 SQL、API 调用等异构工具环境中，定义对应的“关键实体”并重新校准奖励。\n- **理论分析**\n- 奖励 hacking 边界：量化实体匹配与真实推理质量的最小可区分度，给出 α 的理论上下界。\n- 样本复杂度：证明在实体匹配信号满足 γ-准确性条件下，E-GRPO 相比稀疏奖励降低多少采样复杂度。\n- 与潜在变量 PRM 的关系：把实体匹配视为对隐状态正确性的带噪观测，建立与潜在过程奖励模型的变分下界联系。\n\n探索以上方向可进一步释放“合成数据富信号”与“开放域强策略”之间的协同潜力。\n\nQ6: 总结一下论文的主要内容\n\n**Entity-aware Group Relative Policy Optimization (E-GRPO)** 提出了一种**零额外标注、零额外模型**的密集奖励强化学习框架，用于提升基于大模型的搜索智能体在复杂知识密集型任务上的样本效率与最终性能。核心内容可概括为四点：\n\n1. **问题洞察**\n现有 GRPO 仅使用 0/1 结果奖励，无法区分“近失”与完全失败，丢弃合成数据中天然存在的黄金实体信号，导致稀疏奖励与学习低效。\n\n2. **关键发现**\n在 11 个 QA 与深度研究基准上的实证分析表明：\n\n- 正确轨迹的实体匹配率显著高于错误轨迹（4:1 比例）\n- 匹配率与最终准确率呈强正相关（r>0.85）\n因此，实体匹配率可作为**零成本、细粒度**的过程质量代理。\n3. **方法框架**\n引入归一化实体匹配率  hatγ_i ，将错误样本奖励从统一 0 改为\n\nR_i=α·hatγ_i,quad α=0.3\n\n再按组相对优势更新策略，无需改变 GRPO 的采样与优化流程，计算开销可忽略。\n\n4. **实验效果**\n\n- 7B/30B 模型在 11 项基准上**一致超越 GRPO 基线**\n- Local 环境平均 +2.8，Web 环境零样本迁移 +1.6，深度研究 Pass@1 最高 +3.0\n- 训练收敛更快，工具调用次数减少约 10%，验证**更高样本效率与推理效率**\n\n综上，E-GRPO 通过“**把合成数据丢弃的实体转化为密集奖励**”，在开放域搜索场景中首次实现了**低成本、细粒度、可解释**的过程监督，为后续智能体对齐提供了新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Repurposing%20Synthetic%20Data%20for%20Fine-grained%20Search%20Agent%20Supervision，链接是https://arxiv.org/pdf/2510.24694，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24694。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1761781638493","title":"OSWorld-MCP: Benchmarking MCP Tool Invocation In Computer-Use Agents","description":"With advances in decision-making and reasoning capabilities, multimodal agents show strong potential in computer application scenarios. Past evaluations have mainly assessed GUI interaction skills, while tool invocation abilities, such as those enabled by the Model Context Protocol (MCP), have been largely overlooked. Comparing agents with integrated tool invocation to those evaluated only on GUI interaction is inherently unfair. We present OSWorld-MCP, the first comprehensive and fair benchmark for assessing computer-use agents' tool invocation, GUI operation, and decision-making abilities in a real-world environment. We design a novel automated code-generation pipeline to create tools and combine them with a curated selection from existing tools. Rigorous manual validation yields 158 high-quality tools (covering 7 common applications), each verified for correct functionality, practical applicability, and versatility. Extensive evaluations of state-of-the-art multimodal agents on OSWorld-MCP show that MCP tools generally improve task success rates (e.g., from 8.3% to 20.4% for OpenAI o3 at 15 steps, from 40.1% to 43.3% for Claude 4 Sonnet at 50 steps), underscoring the importance of assessing tool invocation capabilities. However, even the strongest models have relatively low tool invocation rates, Only 36.3%, indicating room for improvement and highlighting the benchmark's challenge. By explicitly measuring MCP tool usage skills, OSWorld-MCP deepens understanding of multimodal agents and sets a new standard for evaluating performance in complex, tool-assisted environments. Our code, environment, and data are publicly available at https://osworld-mcp.github.io.","url":"https://huggingface.co/papers/2510.24563","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.493Z","metadata":{"authors":["Hongrui Jia","Jitong Liao","Xi Zhang","Haiyang Xu","Tianbao Xie","Chaoya Jiang","Ming Yan","Si Liu","Wei Ye","Fei Huang"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.24563.pdf","arxivUrl":"https://arxiv.org/abs/2510.24563","arxivId":"2510.24563","coolPaperUrl":"https://papers.cool/arxiv/2510.24563","published":"2025-10-28T15:56:36Z","updated":"2025-10-28T15:56:36.000Z","zh_summary":"随着决策和推理能力的进步，多模态智能体在计算机应用场景中展现出强大的潜力。过去的评估主要集中在GUI交互技能上，而工具调用能力，例如通过模型上下文协议（MCP）实现的能力，则在很大程度上被忽视了。将具有工具调用功能的智能体与仅在GUI交互上评估的智能体进行比较，本质上是不公平的。我们提出了OSWorld-MCP，这是第一个在真实环境中全面且公平地评估计算机使用智能体的工具调用、GUI操作和决策能力的基准。我们设计了一种新颖的自动化代码生成流程来创建工具，并将其与现有工具的精选组合在一起。经过严格的人工验证，生成了158个高质量工具（涵盖7种常用应用程序），每个工具都经过正确功能、实际可用性和多功能性的验证。在OSWorld-MCP上对最先进的多模态智能体进行的广泛评估显示，MCP工具通常能够提高任务成功率（例如，OpenAI o3在15步时从8.3%提升到20.4%，Claude 4 Sonnet在50步时从40.1%提升到43.3%），凸显了评估工具调用能力的重要性。然而，即使是最强大的模型，其工具调用率仍相对较低，仅为36.3%，表明仍有提升空间，并突显了该基准的挑战性。通过明确测量MCP工具使用技能，OSWorld-MCP深化了对多模态智能体的理解，并为在复杂的工具辅助环境中评估性能设定了新的标准。我们的代码、环境和数据可在 https://osworld-mcp.github.io 上公开获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何公平、全面地评估具备工具调用能力的多模态计算机使用智能体”这一核心问题。具体而言，现有基准存在以下关键缺陷：\n\n- 只测 GUI 操作，忽视外部工具（如 Model Context Protocol, MCP）调用；\n- 将“会调用工具的模型”与“只会 GUI 的模型”直接对比，造成评估不公；\n- 缺乏统一、高质量且贴近真实场景的工具集与度量标准。\n\n为此，作者提出 OSWorld-MCP：首个同时考察 GUI 操作、MCP 工具调用与混合决策的公平基准，通过 158 个经人工校验的高质量 MCP 工具与 369 项真实任务，量化智能体在“何时调用工具、调用哪个工具、如何与 GUI 动作协同”上的能力差距，从而推动计算机使用智能体在工具增强环境下的系统评估与迭代。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 章“Related Work”中将与自身最密切的研究划分为两条主线，并指出它们各自的不足，从而凸显 OSWorld-MCP 的差异化价值。可归纳为以下两类：\n\n1. 多模态 GUI 智能体基准\n\n- 静态数据集：Mind2Web、WebLinx、OmniAct\n- 动态可交互环境：WebArena、VisualWebArena、WorkArena、MMInA、WindowsAgentArena、OSWorld\n共性局限：仅预定义“点、拖、输入”等 GUI 原子动作，无法评估外部工具调用，也不能公平比较“会调工具”与“只会 GUI”的智能体。\n2. 文本侧 MCP / 工具使用基准\n\n- MCPEval、MCP-Radar：工具数量少（数十量级），任务多样性受限\n- LiveMCPBench：用 LLM 打分，难以处理需实时知识的真实任务\n- MCP-Bench：先定工具再倒推任务，任务与真实场景脱节\n共性局限：纯文本交互，不涉及图形界面，也无法考察“GUI 与工具混合决策”。\n\n因此，现有研究要么只测 GUI，要么只测文本工具，缺乏“GUI+工具”统一环境。OSWorld-MCP 首次把 158 个真实可用的 MCP 工具嵌入到 OSWorld 的图形操作系统环境中，并设计新的度量（TIR、ACS），填补了上述两类基准之间的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 设计新工具 + 提出新指标”的三位一体方案，系统性地解决了“公平评估 GUI+工具混合能力”这一难题。具体步骤如下：\n\n1. 构建新基准 OSWorld-MCP\n\n- 以 OSWorld 真实操作系统环境为底座，覆盖 Ubuntu / Windows / macOS 三大平台。\n- 将 369 项真实计算机任务重新标注，区分“工具有益”与“工具无益”子集，确保对比公平。\n- 在每一步允许智能体**自主二选一**：执行 GUI 原子动作，或调用任一 MCP 工具，实现 GUI 与工具的无缝混合交互。\n2. 设计高质量 MCP 工具集\n\n- 自动化生成：用 OpenAI-o3 针对 OSWorld 任务写代码 → 过滤可运行代码 → 自动封装为 72 个 MCP 工具。\n- 人工精修：从现有开源 MCP Server 再筛选 192 个工具，去重、去任务特异、双人独立评审，最终保留 158 个跨 7 类常用软件（VS Code、LibreOffice、Chrome、VLC、OS、filesystem、git）的高通用工具。\n- 引入 25 个“干扰工具”，防止模型简单记忆“任务-工具”映射，强化决策难度。\n3. 提出新评估指标\n\n- Task Accuracy：沿用 OSWorld 的成败信号，衡量整体能力。\n- Tool Invocation Rate (TIR)：\n\nTIR= (n_t + n_g) / (N_t + N_g)\n\n反映“该用工具时能否调出工具，不该用时能否抑制调用”的决策精准度。\n- Average Completion Steps (ACS)：\n\nACS= (1) / (N)∑_(i=1)^N S_i\n\n衡量效率；步骤越少说明工具用得既快又准。\n4. 实验验证与消融\n\n- 在 15 步与 50 步两种预算下，对 6 个 SOTA 大多模态模型 + 1 个多智能体框架进行系统测评。\n- 引入 RAG 工具召回、工具顺序打乱等消融，验证“工具过多→上下文过长→调用率下降”等关键假设。\n- 结果量化：Claude 4 Sonnet 借助 MCP 把成功率从 40.1% 提到 43.3%，但最高 TIR 仅 36.3%，表明工具利用率仍有巨大提升空间；多工具组合任务甚至出现“四连调”零成功的极端案例，进一步揭示挑战。\n\n通过上述闭环设计，论文首次实现了“GUI 操作-工具调用-决策策略”在同一真实环境下的公平量化，为后续计算机使用智能体的工具增强研究提供了可复现、可扩展的基准平台。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“GUI 操作 vs. GUI+MCP 工具”两条主线，在统一的真实操作系统环境中开展了多组实验，系统验证 MCP 工具对多模态智能体性能、效率与决策的影响。主要实验内容如下：\n\n1. 主实验：端到端模型与多智能体框架对比\n\n- 被测对象\n– 6 个 SOTA 大视觉-语言模型：Qwen2.5-VL-72B、Qwen3-VL-Plus、Seed1.5-VL、Claude 4 Sonnet、OpenAI o3、Gemini-2.5-Pro\n– 1 个多智能体框架：Agent-S2.5（o3 做高层规划 + UI-TARS-1.5-72B 做 grounding）\n- 实验条件\n– 最大步数：15 步与 50 步两档\n– 每种条件 3 次随机运行取平均\n– 统一使用 GUI-Owl 代理配置，保证模型间可比\n- 对比设置\n– GUI-only：仅 11 种原生 GUI 原子动作\n– GUI+MCP：同一动作空间再叠加 158 个经 RAG 召回的 MCP 工具\n- 观测指标\n– Task Accuracy、TIR、ACS，并细分“Tool-Beneficial Tasks / Non-Tool-Beneficial Tasks”子集\n2. 相关性与难度分析\n\n- TIR-Accuracy-ACS 相关性：汇总所有模型在所有条件下的三元组，验证“TIR 与 Acc 正相关，ACS 与 TIR 非线性”假设。\n- 工具数量梯度实验：用 Claude 4 Sonnet 在人工标注的“可用工具数 0-1-2-≥4”任务子集上跑对比，量化“工具越多→任务越难→Acc↓TIR↓ACS↑”现象。\n3. 消融实验（以 Gemini-2.5-Pro 为受试）\n\n- w/o Tools RAG：去掉 RAG，每次向模型暴露全部 158 个工具描述，观察长上下文对调用率与精度的影响。\n- Tools Shuffle：保持 RAG，但将工具描述随机打乱，检验提示顺序对决策的敏感性。\n4. 案例研究\n\n- 选取 VS Code 设置隐藏 **pycache**、图片转 PDF、幻灯片交替复制、Calc 跨表复制“Revenue”列等 4 个代表性任务，逐步回放 GUI+MCP 轨迹，可视化展示“工具一步完成 vs. GUI 多步失败”的对比，直观验证工具带来的效率与鲁棒增益。\n\n通过上述实验，论文得出三条核心发现：\n\n1. MCP 工具显著提升多数模型的成功率并缩短步数；\n2. 工具调用率 TIR 与 Acc 稳定正相关，但当前最佳模型 TIR 仅 36.3%，仍有巨大提升空间；\n3. 当任务需要多轮、多工具组合时，性能急剧下降，揭示“工具链”决策仍是重大挑战。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 OSWorld-MCP 开启的“工具-增强 GUI 智能体”研究版图中的空白或薄弱地带，值得进一步探索：\n\n1. 工具链规划与纠错\n\n- 多工具依赖图自动构建：让模型先输出“工具调用 DAG”再执行，降低顺序错误。\n- 失败回滚与补偿：调用失败后自动触发回退或替代工具，而非直接终止。\n2. 工具空间动态扩展\n\n- 在线工具合成：面对未见任务，模型即时生成并注册新 MCP 工具（Code-as-Action → Tool-as-a-Service）。\n- 工具版本演化：模拟真实软件升级，考察智能体对 API 签名变更的鲁棒性。\n3. 长上下文与检索策略\n\n- 层次化工具记忆：把 158→1000+ 工具按领域做向量-符号混合索引，支持“粗召回-精排-验证”三阶段。\n- 工具使用频率/时效感知的 Prompt 压缩：动态丢弃低先验概率工具，减少上下文长度。\n4. 人机混合评价\n\n- 引入“人类偏好”维度：对同一任务分别用 GUI 与工具完成，收集人类对速度、可解释性、安全性的打分，建立 Alignment 指标。\n- 可纠错性：允许人类在运行中给予自然语言修正，测量智能体继续执行的成功率。\n5. 安全与伦理基准\n\n- 工具滥用检测：构造恶意任务（如批量删除系统文件、隐私外传），评估模型对危险调用的拒绝率。\n- 最小权限沙箱：为每个 MCP Server 配置细粒度 capability list，考察智能体在受限环境下的策略迁移。\n6. 跨平台与跨设备迁移\n\n- 同一任务在 Ubuntu ↔ Windows ↔ macOS 三端工具语义差异下的迁移性能。\n- 从桌面端 OSWorld-MCP 到移动端 AndroidWorld，验证工具抽象层是否足够通用。\n7. 强化学习与自我改进\n\n- 以 TIR/ACS 为奖励，用 RL/self-evolution 微调模型策略，观察能否自动发现更短或更少调用的工具路径。\n- 工具价值函数估计：对每个工具学习 Q-value，实现“工具级” credit assignment。\n8. 多智能体协作\n\n- 角色分工：Planner 负责选工具，Grounding 模型负责 GUI 补全，两者通过共享工具上下文协同。\n- 工具市场：允许多个智能体发布/订阅彼此注册的 MCP 工具，考察协议级互操作性。\n9. 统一理论框架\n\n- 将 GUI 动作、MCP 工具、CLI 命令统一抽象为“混合动作空间”，建立通用的策略梯度或搜索算法下界分析。\n- 复杂度分类：依据工具数、GUI 步数、观测噪声定义任务难度等级，形成类似“P/NP”的工具使用复杂度理论。\n10. 真实生产环境验证\n\n- 把 OSWorld-MCP 评估协议迁移到企业 RPA、软件测试、运维脚本等场景，记录模型在真实 API 速率限制、网络延迟、权限审批下的表现，验证基准结论的外推性。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有 GUI 智能体基准忽视外部工具调用，导致“会调工具”与“只会 GUI”的模型无法公平比较，缺乏统一评估框架。\n- **方案**：提出 OSWorld-MCP 基准\n– 在 OSWorld 真实操作系统环境（Ubuntu/Windows/macOS）中植入 158 个人工校验的高质量 MCP 工具，覆盖 7 类常用软件。\n– 每步允许智能体**自主二选一**：GUI 原子动作 或 MCP 工具调用，实现混合决策。\n– 引入新指标：Tool Invocation Rate (TIR) 衡量“该用即用”的决策精准度，Average Completion Steps (ACS) 衡量效率。\n\n- **实验**\n– 6 个 SOTA 大视觉-语言模型 + 1 个多智能体框架，在 15/50 步预算下对比“纯 GUI”与“GUI+MCP”。\n– 结果：MCP 工具将最佳模型准确率从 40.1% → 43.3%，但最高 TIR 仅 36.3%；多工具组合任务仍极具挑战。\n– 消融显示：去掉 RAG 召回、工具描述顺序打乱均显著影响调用率与成功率。\n\n- **结论**：OSWorld-MCP 首次公平量化 GUI+工具混合能力，验证工具调用可显著提升效率，也揭示当前模型在工具链决策、长上下文利用上仍有巨大提升空间。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是OSWorld-MCP%3A%20Benchmarking%20MCP%20Tool%20Invocation%20In%20Computer-Use%20Agents，链接是https://arxiv.org/pdf/2510.24563，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24563。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1761781638493","title":"ParallelMuse: Agentic Parallel Thinking for Deep Information Seeking","description":"Parallel thinking expands exploration breadth, complementing the deep exploration of information-seeking (IS) agents to further enhance problem-solving capability. However, conventional parallel thinking faces two key challenges in this setting: inefficiency from repeatedly rolling out from scratch, and difficulty in integrating long-horizon reasoning trajectories during answer generation, as limited context capacity prevents full consideration of the reasoning process. To address these issues, we propose ParallelMuse, a two-stage paradigm designed for deep IS agents. The first stage, Functionality-Specified Partial Rollout, partitions generated sequences into functional regions and performs uncertainty-guided path reuse and branching to enhance exploration efficiency. The second stage, Compressed Reasoning Aggregation, exploits reasoning redundancy to losslessly compress information relevant to answer derivation and synthesize a coherent final answer. Experiments across multiple open-source agents and benchmarks demonstrate up to 62% performance improvement with a 10--30% reduction in exploratory token consumption.","url":"https://huggingface.co/papers/2510.24698","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.493Z","metadata":{"authors":["Baixuan Li","Dingchu Zhang","Jialong Wu","Wenbiao Yin","Zhengwei Tao","Yida Zhao","Liwen Zhang","Haiyang Shen","Runnan Fang","Pengjun Xie","Jingren Zhou","Yong Jiang"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.24698.pdf","arxivUrl":"https://arxiv.org/abs/2510.24698","arxivId":"2510.24698","coolPaperUrl":"https://papers.cool/arxiv/2510.24698","published":"2025-10-28T17:51:50Z","updated":"2025-10-28T17:51:50.000Z","zh_summary":"平行思维扩展了探索的广度，补充了信息搜索（IS）智能体的深度探索，从而进一步提升问题解决能力。然而，在这种环境下，传统的平行思维面临两个主要挑战：一是重复从头展开导致的低效率，二是在生成答案时难以整合长远推理轨迹，因为有限的上下文容量无法完全考虑推理过程。为了解决这些问题，我们提出了ParallelMuse，一种针对深度IS智能体的两阶段范式。第一阶段，功能指定的部分展开，将生成的序列划分为功能区域，并执行基于不确定性的路径重用与分支，以提高探索效率。第二阶段，压缩推理聚合，利用推理冗余无损压缩与答案推导相关的信息，并综合生成一致的最终答案。在多个开源智能体和基准测试中的实验表明，ParallelMuse在提高性能方面可达62%，同时探索令牌消耗减少10%至30%.","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“深度信息搜寻（deep IS）智能体”在并行思考（parallel thinking）场景下的两大核心痛点提出系统性解决方案：\n\n1. 探索效率低\n传统做法每轮都“从零开始”重新 rollout，导致在探索多样性天然不足的阶段重复消耗大量 token 与算力。\n\n2. 长程推理难聚合\n采样后需把多条长轨迹整合成最终答案，但上下文容量有限，无法一次性输入全部推理过程；仅看最终答案或只取末尾几步又会丢失早期规划与问题分解信息，造成聚合偏差或信息缺失。\n\n为此，作者提出两阶段框架 PARALLELMUSE：\n\n- 阶段一：Functionality-Specified Partial Rollout——按“功能区域（reasoning vs. exploration）”划分轨迹，在各区域内独立估计不确定性，仅对高不确定度步骤做分支复用，避免无效重采样。\n- 阶段二：Compressed Reasoning Aggregation——利用深度 IS 轨迹高度可无损压缩的观察，把每条候选轨迹提炼成结构化报告，再一次性聚合全部报告生成答案，既缓解上下文溢出，又降低多数投票偏差。\n\n实验显示，该范式在多项基准上最高带来 62% 性能提升，同时节省 10–30% 的探索 token。\n\nQ2: 有哪些相关研究？\n\n论文在 §5 中系统梳理了相关研究，可归纳为两条主线：\n\n- **深度信息搜寻（deep IS）智能体**\n- 闭源标杆：OpenAI DeepResearch、Claude-4-Sonnet、Kimi Researcher、Perplexity Deep Research、Gemini 2.5 等。\n- 开源进展：WebSailor、WebDancer、WebWalker、WebThinker、SimpleDeepSearcher、WebExplorer、EvolveSearch 等。\n这些工作聚焦于“如何设计能主动调用搜索/浏览工具、在长周期交互中持续推理”的智能体，但未专门解决并行思考下的效率与聚合难题。\n- **并行思考 / 测试时扩展（test-time scaling）**\n- 早期思路：Self-Consistency（多数投票）、Weighted Vote（置信度加权）、Max-#Tool-Call 等简单答案选择策略。\n- 近期改进：\n– 部分复用 rollout：TreeRL、TreePO、Agentic RPO、Dong et al. 2025、Hou et al. 2025。\n– 答案聚合而非选择：LLM-Blender、MAD、Qiao et al. 2025 的“末尾几步聚合”。\n上述方法默认“token 同质”或仅关注纯推理任务，未考虑 agentic 场景下“推理-工具”异质功能区域的不确定度差异，也未解决长轨迹冗余导致的上下文溢出。\n\nPARALLELMUSE 在上述两条主线交叉点上提出“功能区分式部分 rollout + 无损压缩聚合”，填补了深度 IS 智能体在高效并行思考方向的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“深度信息搜寻智能体 + 并行思考”这一组合拆解为两个连续痛点，并分别给出针对性算法，最终形成两阶段框架 **PARALLELMUSE**。核心思路是“**只在高潜力区域花钱，再把长篇大论压成简报**”。\n\n1\\. 阶段一：Functionality-Specified Partial Rollout\n\n**目标**：用更少的 token 做更广、更有效的探索。\n\n1.1 功能区域划分\n\n- 利用特殊 token（`<think>`、`<tool_call>` 等）把每条轨迹切成\n- **reasoning 区**（内部思考）\n- **exploration 区**（工具调用及返回）\n- 两区的 token 对“下一步不确定性”贡献不同，不能一视同仁。\n\n1.2 离线不确定度估计\n\n- 对初始  M  条完整轨迹，每区分别计算 step-level perplexity\n\nPPL(f,t)=exp!l(-(1) / (|T_t^f|)∑_(x_t,i)∈ T_t^flog p(x_(t,i)|x_(<t,i))r),quad f∈r,e\n\n- 按  PPL  高低排序，选出 top- k  高不确定度步骤作为“分支点”。\n\n1.3 异步分支复用\n\n- 从分支点直接**复用 KV-cache** 继续解码，无需重跑前缀。\n- 异步调度  P  条分支并行展开，整体加速近似\n\nSpeedup_(total)≈ l(1+(∑_j p_j) / (∑_j s_j)r)· P\n\n其中  p_j  为复用长度， s_j  为新生长度。\n\n**结果**：在 8 条采样预算下，token 消耗降低 10–28%，同时探索质量优于“从零开始”rollout。\n\n2\\. 阶段二：Compressed Reasoning Aggregation\n\n**目标**：在有限上下文内把  N  条长轨迹“几乎无损”地拼成一份最终答案。\n\n2.1 结构化压缩\n\n对每条轨迹抽取三要素，生成一份**压缩报告**（≈ 原长度 1%）：\n\n- **Solution Planning**：子问题分解及依赖顺序\n- **Solution Methods**：用到的工具、参数、子答案\n- **Final Reasoning**：子答案如何合成最终结论\n\n该报告等价于重构了信息状态图  G_(final)⊃eq I_(answer) ，冗余实体被剔除，保留推导所必需的全部信息。\n\n2.2 推理式聚合\n\n- 把  N  份压缩报告一次性喂给模型，要求**基于推理一致性**而非“答案多数”做判断。\n- 显式禁止：\n– 仅按答案出现频率选择\n– 简单拼接不同答案\n- 不再调用额外工具，仅用报告内已存的 provenance 进行纯推理。\n\n**结果**：在同等 8 条采样下，相对 Majority Vote 再提 3–10 分；上下文占用从数万 token 降至数百 token。\n\n3\\. 整体流程（图示摘要）\n\n```markdown\n1. 初始 M 条完整 rollout\n→ 按功能区算 PPL → 选高不确定步骤\n2. 异步分支复用 (N−M 条)\n↓\n3. 得到 N 条轨迹\n→ 压缩成 N 份结构化报告\n4. 一次性输入聚合模型\n→ 输出最终答案\n```\n\n通过“**不确定度制导的分支复用**”与“**冗余剔除+结构化聚合**”双管齐下，PARALLELMUSE 在多项基准上取得最高 62% 的性能增益，同时节省 10–30% 探索 token，解决了深度 IS 智能体在并行思考场景下的效率与聚合难题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“深度信息搜寻（deep IS）智能体 + 并行思考”场景，在 4 个公开基准、4 款开源模型上展开系统实验，覆盖**性能、效率、消融、可扩展性**四个维度。主要实验一览如下（均按官方 LLM-as-a-Judge 协议评分）：\n\n1 总体对比实验（§4.2）\n\n- **基准**\n- BrowseComp / BrowseComp-zh（重深度搜索）\n- GAIA（搜索+推理均衡）\n- Humanity’s Last Exam（重推理）\n- **模型**\nGPT-OSS-20B、GPT-OSS-120B、DeepSeek-V3.1-T（671B）、Tongyi-DR-30B-A3B\n- **对手**\n无并行(No Scaling)、Majority Vote、Max-#Tool-Call、Weighted Vote（DeepConf）\n- **结果**\nPARALLELMUSE 在所有 16 组“模型×基准”上取得最高分数，最高相对提升 62%；Tongyi-DR-30B-A3B + PARALLELMUSE 达到闭源一线水平。\n\n2 消融实验\n\n2.1 阶段一消融：功能区分支 vs 全量重跑（§4.3）\n\n- 固定 8 rollout，比较三种分支策略\n- 仅 reasoning 区高 PPL 分支\n- 仅 exploration 区高 PPL 分支\n- Mixed（两区各取 top-1 分支点）\n- **结论**\n所有部分 rollout 均优于“from scratch”；不同模型偏好不同功能区，验证“功能异质”假设。\n\n2.2 阶段二消融：仅换聚合方式（§4.4）\n\n- 固定 8 条“from scratch”轨迹，只替换最终答案生成策略\n- Majority Vote / Weighted Vote / Compressed Reasoning Aggregation\n- **结论**\n压缩聚合单点即可带来 2–10 分提升，证明增益主要来自**信息整合质量**而非单纯采样量。\n\n3 效率评测（§4.5）\n\n- **token 节省**\n在 8–32 条采样预算下，Partial Rollout 相对全量重跑平均节省 10–28% 探索 token，且随预算增大而放大。\n- **上下文压缩**\n轨迹→结构化报告后，单条上下文长度缩小 **99%**（从数万 token → 数百 token），使 32 条报告一次性输入仍远低于模型上限。\n\n4 强弱模型组合实验（§4.6）\n\n- rollout 阶段用弱模型（GPT-OSS-20B），聚合阶段分别换用\n- 同尺寸模型\n- 更强模型（GPT-OSS-120B）\n- 闭源 GPT-5\n- **结果**\n聚合模型越强，最终分数持续提升（+1.5 → +6.5 分），验证“压缩报告可迁移”且高质量重构能进一步放大性能。\n\n5 与闭源标杆对比（同表 2）\n\n将最优开源配置（Tongyi-DR-30B-A3B + PARALLELMUSE）与 Claude-4-Sonnet、OpenAI-o3、Kimi Researcher、DeepResearch、ChatGPT Agent 的官方无并行成绩并列：\n在 BrowseComp-zh、GAIA、HLE 三项上**达到或超过多数闭源系统**，验证方法在实际部署场景的可行性。\n\n综上，实验从“能不能赢、赢在哪、省多少、是否通用”四个角度系统验证了 PARALLELMUSE 的有效性、高效性与可扩展性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“任务扩展”“方法深化”“理论分析”三大类，供后续研究参考：\n\n一、任务与场景扩展\n\n1. **多工具 / 多模态环境**\n目前仅 Search+Visit；若引入 CodeInterpreter、FileSystem、Vision 等工具，功能区域划分与不确定度建模需重新设计。\n\n2. **长周期动态环境**\n网页内容会随时间变化，需考虑“非平稳奖励”与“信息过期”，可结合 continual RL 或环境版本化管理。\n\n3. **开放域生成式任务**\n将框架从“问答”拓展到“生成报告、撰写论文”等输出长度不受限的场景，需定义新的压缩指标与聚合策略。\n\n4. **多智能体协作搜寻**\n让不同 agent 负责不同子领域，再用 PARALLELMUSE 聚合异构轨迹，可研究跨 agent 的冗余去除与冲突消解。\n\n二、方法深化\n\n1. **在线不确定度估计**\n当前用离线 PPL 选分支；能否在 rollout 过程中用实时熵或梯度敏感度量动态决定“何时分支”，进一步节省预算。\n\n2. **学习型分支策略**\n将“选哪步、选几支”建模为序列决策问题，用强化学习（MCTS、Policy Gradient）直接优化最终答案 reward，替代人工 top-k 规则。\n\n3. **可学习的压缩器**\n压缩报告现由强模型一次性生成，可训练专用“轨迹→摘要”小模型，兼顾压缩率、保真度与推理速度。\n\n4. **分层聚合**\n先在同质子任务级别做局部聚合，再在全局做二次聚合，减少单轮上下文长度；可结合子任务检测或实体链接自动分块。\n\n5. **置信度校准回流**\n尽管框架避免直接用置信度选答案，但校准后的不确定度仍可反哺分支策略，形成“校准-探索-聚合”闭环。\n\n三、理论与评估\n\n1. **无损压缩界限**\n基于信息状态图 G\\_final，推导“保答案最小子轨迹”的信息论下界，量化压缩报告与理论最优的差距。\n\n2. **探索-利用权衡度量**\n定义适用于 agentic IS 的“探索增益”指标，与 MCTS 的 UCB 类比，证明 Partial Rollout 的遗憾界或样本复杂度。\n\n3. **功能区域不确定度的可迁移性**\n研究“在模型 A 上测得的 PPL 峰值位置”对模型 B 是否仍然有效，从而支持跨模型零样本分支选择。\n\n4. **鲁棒性与 adversarial 评估**\n在工具返回噪声、网页恶意误导、API 超时等场景下测试框架鲁棒性，并引入 adversarial Judge 检验聚合策略是否被“多数错误”误导。\n\n5. **人类一致性细粒度评估**\n除整体分数外，记录“子计划正确性”“工具调用必要性”“最终推导链完备性”等多维指标，更精准地定位改进空间。\n\n四、系统与工程\n\n1. **KV-Cache 异构调度**\n结合分页缓存、offload 到 CPU/NVMe，实现千级分支的超大并发，支撑线上实时服务。\n\n2. **轨迹压缩缓存库**\n建立可检索的“已压缩报告库”，遇到相似提问时直接复用或做语义融合，进一步降低边际成本。\n\n3. **端到端训练框架**\n将两阶段封装成可微管道，用强化学习或可微记忆机制端到端训练，从而同时优化分支策略、压缩模板与聚合规则。\n\n综上，从“工具多样性、在线决策、学习式压缩、理论保证到系统实现”均存在广阔探索空间，既可推动深度 IS 智能体的并行思考走向通用化，也能为更广泛的 agentic test-time scaling 提供方法论参考。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **PARALLELMUSE**，一套面向“深度信息搜寻（deep IS）智能体”的两阶段并行思考范式，解决传统并行思考在 agentic 场景下的**重复 rollout 费 token** 与**长轨迹难聚合**两大痛点。\n\n1. 阶段一：Functionality-Specified Partial Rollout\n\n- 按“推理 / 探索”功能区域分割轨迹\n- 离线计算各区 step-level perplexity，选高不确定度步骤做分支\n- 复用 KV-cache 异步展开，节省 10–28% token，实现近线性加速\n2. 阶段二：Compressed Reasoning Aggregation\n\n- 将每条长轨迹压缩成“计划-方法-结论”三要素报告，上下文缩小 99%\n- 一次性输入全部报告，基于推理一致性而非多数投票生成最终答案，避免置信度漂移\n\n实验在 4 基准 × 4 开源模型上展开，相较最强基线最高提升 62%，token 消耗降 10–30%，效率与精度双收益；聚合阶段换用更强模型可进一步持续提升，验证压缩报告可迁移。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ParallelMuse%3A%20Agentic%20Parallel%20Thinking%20for%20Deep%20Information%20Seeking，链接是https://arxiv.org/pdf/2510.24698，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24698。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1761781638494","title":"WebLeaper: Empowering Efficiency and Efficacy in WebAgent via Enabling Info-Rich Seeking","description":"Large Language Model (LLM)-based agents have emerged as a transformative approach for open-ended problem solving, with information seeking (IS) being a core capability that enables autonomous reasoning and decision-making. While prior research has largely focused on improving retrieval depth, we observe that current IS agents often suffer from low search efficiency, which in turn constrains overall performance. A key factor underlying this inefficiency is the sparsity of target entities in training tasks, which limits opportunities for agents to learn and generalize efficient search behaviors. To address these challenges, we propose WebLeaper, a framework for constructing high-coverage IS tasks and generating efficient solution trajectories. We formulate IS as a tree-structured reasoning problem, enabling a substantially larger set of target entities to be embedded within a constrained context. Leveraging curated Wikipedia tables, we propose three variants for synthesizing IS tasks, Basic, Union, and Reverse-Union, to systematically increase both IS efficiency and efficacy. Finally, we curate training trajectories by retaining only those that are simultaneously accurate and efficient, ensuring that the model is optimized for both correctness and search performance. Extensive experiments on both basic and comprehensive settings, conducted on five IS benchmarks, BrowserComp, GAIA, xbench-DeepSearch, WideSearch, and Seal-0, demonstrate that our method consistently achieves improvements in both effectiveness and efficiency over strong baselines.","url":"https://huggingface.co/papers/2510.24697","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.494Z","metadata":{"authors":["Zhengwei Tao","Haiyang Shen","Baixuan Li","Wenbiao Yin","Jialong Wu","Kuan Li","Zhongwang Zhang","Huifeng Yin","Rui Ye","Liwen Zhang","Xinyu Wang","Pengjun Xie","Jingren Zhou","Yong Jiang"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.24697.pdf","arxivUrl":"https://arxiv.org/abs/2510.24697","arxivId":"2510.24697","coolPaperUrl":"https://papers.cool/arxiv/2510.24697","published":"2025-10-28T17:51:42Z","updated":"2025-10-28T17:51:42.000Z","zh_summary":"基于大语言模型（LLM）的智能体已经成为一种在开放性问题解决中具有变革性的方案，其中信息检索（IS）是其核心能力，使自主推理和决策成为可能。虽然以往研究大多集中于提高检索深度，但我们发现当前的IS智能体往往存在搜索效率低的问题，这反过来限制了整体性能。导致这种低效率的一个关键因素是训练任务中目标实体的稀疏性，这限制了智能体学习和泛化高效搜索行为的机会。为了解决这些挑战，我们提出了WebLeaper，一个用于构建高覆盖率IS任务并生成高效解决轨迹的框架。我们将IS问题建模为树状结构的推理问题，从而能够在有限的上下文中嵌入更多的目标实体。基于精心策划的维基百科表格，我们提出了三种用于合成IS任务的变体——Basic（基础）、Union（联合）和Reverse-Union（反向联合），系统性地提高IS的效率和效果。最后，我们通过仅保留同时准确且高效的训练轨迹来精炼训练数据，确保模型在正确性和搜索性能上都得到优化。在五个IS基准测试——BrowserComp、GAIA、xbench-DeepSearch、WideSearch和Seal-0——的基础和综合场景中进行的大量实验表明，我们的方法在有效性和效率上均稳定地优于强基线。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**基于大模型的信息检索（IS）智能体搜索效率低下**这一核心问题。现有方法主要关注“检索深度”，却忽视了一个关键现象：\n\n- 训练任务中目标实体过于稀疏，导致智能体难以在有限上下文中学会高效定位信息；\n- 稀疏目标使效率指标（ISE）方差大，训练信号失真，进一步阻碍高效搜索策略的学习。\n\n为此，作者提出 WebLeaper 框架，通过以下手段系统性提升**效率与效果**：\n\n1. 树状推理任务建模：在紧凑结构内嵌入**高密度目标实体**，缓解稀疏性；\n2. 三阶数据合成：Basic→Union→Reverse-Union，逐级增加跨源整合与逆向推理难度，防止关键词捷径；\n3. 信息导向轨迹筛选：用 ISR（覆盖率）与 ISE（单位步长收益）双重阈值，仅保留**既准确又高效**的解轨迹；\n4. 混合奖励强化学习：对实体密集型任务采用**软 F 分数**，对旧数据保留原奖励，实现精细、可扩展的策略优化。\n\n实验在五个基准（BrowseComp、GAIA、xbench-DeepSearch、WideSearch、Seal-0）上表明，WebLeaper 在**同等或更少步数**下显著超越现有开源与部分闭源智能体，验证了“**以效率驱动效果**”的假设。\n\nQ2: 有哪些相关研究？\n\n论文在第 5 节“Related Work”中系统梳理了与信息检索（IS）智能体相关的三大研究脉络，并指出自身定位与差异。可归纳为以下两类：\n\n1. 信息检索智能体（Information-Seeking Agent）\n\n- 监督微调路线：通过合成 QA 数据对基座模型进行 SFT，以提升深度搜索或推理能力。\n- 代表工作：WebSailor、WebDancer、WebShaper、ASearcher、DeepDive、MiroThinker、Kimi-K2 等。\n- 共同局限：任务答案通常只含**少量实体**，训练信号稀疏，难以学习高效搜索策略。\n- 架构与规划路线：改进 ReAct、Tree-of-Thought、多智能体协作等机制，增强规划与鲁棒性。\n- 代表工作：MetaGPT、AutoGen、WebResearcher。\n- 多智能体协作：让多个智能体分工完成检索、验证、整合。\n- 代表工作：AutoGen、MetaGPT。\n\nWebLeaper 与第一类同属于“微调基座模型”路线，但首次把**实体密度**与**效率指标**作为训练核心，弥补“只追求正确性而忽视搜索代价”的空白。\n\n2. 智能体数据合成（Agent Data Synthesis）\n\n- 工具使用合成：生成 API、GUI、代码等交互轨迹，解决数据稀缺。\n- 代表工作：RAG-Synth、ShortcutsBench、Case2Code、OS-Genesis。\n- 信息检索领域合成：通过多跳、长周期规划提升任务难度。\n- 代表工作：WebWalker、WebDancer、WebShaper。\n- 共同局限：侧重**推理深度**或**步数长度**，未显式增加答案实体规模，也未对“效率”进行可量化优化。\n\nWebLeaper 的差异化在于：\n\n- 以**高覆盖率实体集**为训练目标，显式提升答案的“实体丰富度”；\n- 引入 ISR/ISE 双指标，对轨迹进行**效率-效果联合过滤**；\n- 设计 Basic → Union → Reverse-Union 三阶复杂度，迫使模型学习**跨源整合与逆向推理**，而非依赖关键词捷径。\n\nQ3: 论文如何解决这个问题？\n\n论文通过 **WebLeaper** 框架，从**任务构造**与**训练信号**两条主线协同解决“搜索效率低”的问题，具体手段如下：\n\n1. 实体密集型任务合成（Entity-Intensive Task Synthesis）\n1.1 树状建模\n\n- 将 IS 过程抽象为**三阶推理树**：根节点（问题实体）→ 二层节点（关键实体）→ 三层节点（属性实体）。\n- 一棵紧凑的树即可容纳 **O(100)** 个目标实体，天然缓解稀疏性。\n\n1.2 三阶复杂度递进\n\n- **Basic**：单维基表直接生成高密度子树，保证“量”的基数。\n- **Union**：在二分图上枚举**最大公共双团**，把跨表、跨领域的子树做**语义可解释并集**，迫使模型整合多源信息。\n- **Reverse-Union**：先给**模糊线索**（三层属性），让模型逆向推断二层锚点，再以锚点属性为**枢纽**展开新搜索，阻断直接关键词命中。\n2. 信息导向轨迹筛选（Information-Guided Trajectory Curation）\n\n- **覆盖率过滤**：仅保留 ISR = |R∩O|/|R| > α 的轨迹，确保**答案完整**。\n- **效率过滤**：仅保留 ISE = |R|/T > β 的轨迹，确保**单位步长收益高**；其中 T 只计 Visit 动作，剔除 Search 噪声。\n- 结果：训练集全部是高“实体召回”且低“动作冗余”的示范序列，直接示范“如何少步数多实体”。\n3. 混合奖励强化学习（Hybrid-Reward RL）\n\n- **实体密集型任务**：采用**软 F 分数**\nR_(WebLeaper)=((1+ω^2)· P · R_c) / (ω^2 P + R_c)\n其中软召回 R\\_c 与软精度 P 均基于**语义相似度** s(e\\_o,e\\_r) 计算，可容错“USA ↔ United States”类变异。\n- **旧有公开数据**：保留原二元或 F1 奖励 R_(legacy)，保证与基准评测协议兼容。\n- 用 **GRPO** 优化：在每组 k 条轨迹内做**相对优势**标准化，直接最大化 R_(hybrid)，使策略持续偏向**高 ISR 且高 ISE** 的动作。\n4. 端到端效果\n\n- 训练数据平均含 **≥100 实体/样本**，ISE 方差按 O(1/n) 显著降低，训练信号稳定。\n- 在五项基准上，同等或**更少平均步数**即可取得**SOTA 或媲美闭源模型**的精度，实现“**效率与效果双升**”。\n\nQ4: 论文做了哪些实验？\n\n论文在 **5 个公开信息检索基准** 上进行了系统实验，覆盖 **基础训练设置（Base）** 与 **大规模综合训练设置（Comprehensive）**，并配套多项消融与曲线分析。具体实验如下：\n\n1. 主实验：端到端性能对比\n基准\n\n- BrowseComp、GAIA、xbench-DeepSearch、Seal-0（指标：Pass@1）\n- WideSearch（指标：Success Rate、Row-F1、Item-F1）\n\n对比对象\n\n- 闭源：Claude-4-Sonnet、OpenAI-o3、OpenAI DeepResearch、GLM-4.5、Kimi-K2\n- 开源：ASearcher、DeepDive、MiroThinker、WebExplorer、WebDancer、WebSailor、WebShaper 等\n\n结果（表 1 & 图 1）\n\n- **Base 设置**：WebLeaper-Reverse-Union 在 5 项基准全部取得**开源第一**；在 GAIA、xbench-DS 上**超越闭源 Claude-4-Sonnet 与 OpenAI-o3**。\n- **Comprehensive 设置**：30B 模型混合 WebLeaper 数据后，GAIA 73.2、BrowseComp 38.8、xbench-DS 72.0，**全面领先**同期最强开源与部分闭源模型。\n2. 消融实验：数据来源贡献\n设置\n\n- 仅 WebSailor-V2-5k / 10k\n- WebSailor-V2-5k + 各 5k 的 Basic、Union、Reverse-Union\n\n结果（表 2）\n\n- Basic 平均下降 7.64 pts，验证“**实体过简→捷径过拟合**”。\n- Union +3.26 pts，Reverse-Union +4.34 pts，呈**阶梯式增益**，说明复杂度递增有效。\n3. 轨迹过滤策略消融\n设置\n\n- ISR-Only、ISE-Only、ISR+ISE 双阈值\n\n结果（图 4）\n\n- GAIA / BrowseComp：**双阈值 > 单阈值**，表明“**精度+效率**”联合信号最优。\n- WideSearch：三者持平，说明**广域搜索**本身对过滤策略不敏感，实体密度已提供足够信号。\n4. 效率-效果联合散点\n方法\n\n- 同基准同模型对比 WebLeaper 与 WebSailor-V2 的**平均步数 vs 性能分数**\n\n结果（图 5）\n\n- WebLeaper 在 4 个基准上**同时实现更高分数与更少步数**，验证“**少步多实体**”目标达成。\n5. 强化学习曲线\n设置\n\n- Comprehensive 设置下，SFT → SFT+RL（GRPO，混合奖励）\n\n结果（表 3 & 图 6）\n\n- RL 后平均提升 +3.0 pts，WideSearch SR 从 1.5→4.0，Item-F1 从 45.4→48.5。\n- 奖励曲线**单调上升**至 135 步未出现平台，表明 WebLeaper 数据可提供**持续且稳定**的稠密奖励信号。\n6. 工具调用分布分析\n方法\n\n- 统计 500 条测试轨迹的 Search、Visit、总工具调用次数\n\n结果（图 9）\n\n- 单条任务常需 50+ 动作，验证合成任务**交互深度**足够；高密度实体并未降低探索强度，反而在更少步数内完成更高召回。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深化或拓展，分为**数据、模型、评测、理论**四类，均直接对应 WebLeaper 尚未充分展开之处：\n\n1. 数据与任务层面\n\n- **跨语言实体密度**：当前仅英文维基，可引入多语言维基表格，考察“实体密度”对低资源语言是否同样有效。\n- **动态时序任务**：现有表格为静态快照，可引入**事件流**（新闻、推特、比赛直播），构造“随时间膨胀”的实体树，测试智能体**在线增量更新**能力。\n- **多模态实体**：将信息框中的**图片、info-card、地图坐标**作为第三层节点，验证密度增益是否超越纯文本。\n- **对抗性稀疏攻击**：人为把目标实体比例从 100% 逐步降至 5%，绘制“密度-性能”曲线，观察**临界稀疏点**是否出现突变，从而验证原论文假设的稳健性。\n2. 模型与训练层面\n\n- **效率-效果帕累托前沿**：固定推理预算（如 20 步），用**约束 RL**（CMDP、Lagrangian）直接优化帕累托最优策略，而非手工 α/β 阈值。\n- **步级稠密奖励**：当前 ISR/ISE 只在回合末计算，可引入**步级信息增益**\nr_t = |O_t setminus O_(t-1) ∩ R||R| - λ\n实现**每一步**都有密度信号，加速收敛。\n- **层次化策略**：将“树层”显式编码为策略输入，用**分层强化学习**（Option-Critic、HiPPO）让高层决定“下一层跳转到哪棵子树”，低层执行具体检索动作，进一步压缩步数。\n- **检索与记忆协同**：把**可写外部记忆**（A-Mem、MemGPT）接入，允许智能体把已访问实体写入记忆，下次直接读取而无需重新搜索，考察 ISE 能否突破“理论下限” 1.0。\n3. 评测与协议层面\n\n- **效率-效果双指标排行榜**：现有基准只报告 Pass@1 或 F1，建议官方新增\\*\\*“步数加权分”\\*\\*\nLeaderScore = Acc × (1 + γ N_(ref) - N_(agent)N_(ref))\n鼓励参赛者在**相同精度下用更少步数**完成，推动社区关注效率。\n- **开放域实体规模可验证基准**：当前 WideSearch 仅 1 k 问题，可发布**百万级实体标注**的开放域集合，采用**可验证哈希**（如 SHA-256 of canonical ID）防止人工标注泄露，支持**大规模实体密度**研究。\n- **在线成本-收益评测**：在真实搜索引擎计费 API 下运行，记录**美元成本**与**答案召回**，绘制“成本-收益”曲线，直接反映商业落地价值。\n4. 理论与分析层面\n\n- **ISE 的有限样本误差界**：除方差 O(1/n) 外，可进一步给出**置信区间**与**最小样本量** n^\\*，指导未来任务设计时“至少放多少实体”才能让训练信号可信。\n- **密度-泛化误差关系**：借用 Rademacher 复杂度，证明“实体密度 ≥ κ” 的任务集对假设空间的**泛化误差上界**更紧，从而提供**密度正则化**的理论依据。\n- **搜索复杂度下界**：将 IS 过程形式化为**随机树探索问题**，给出**期望步数下界**\nE\nT\n≥ (H(R)) / (log (textbranching))\n其中 H(R) 为实体集合的熵，指导算法设计是否接近理论最优。\n\n通过上述探索，可继续推进“**高密度实体驱动的高效信息检索**”这一主线，在数据规模、训练机制、评测协议与理论保障四个维度形成闭环。\n\nQ6: 总结一下论文的主要内容\n\nWebLeaper：通过“高密度实体+效率信号”双轮驱动，系统性提升大模型信息检索智能体的**搜索效率**与**最终效果**。\n\n核心问题\n\n- 现有 IS 训练任务目标实体稀疏 → 有效动作率仅 ~4 %，效率低、指标方差大、训练信号弱。\n\n解决思路\n\n1. 任务层面：把 IS 建模为**三阶推理树**，在有限上下文内塞进 **O(100) 实体**。\n\n- Basic：单维基表直接生成高密度子树。\n- Union：最大公共双团合并多表，增加跨源整合。\n- Reverse-Union：先给模糊线索逆向锚定，再展开搜索，阻断关键词捷径。\n2. 数据筛选：用 **ISR（覆盖率）+ ISE（单位步长收益）** 双阈值过滤轨迹，只保留“**答案完整且步数少**”的示范序列。\n\n3. 强化学习：设计**混合奖励**\n\n- 实体密集型任务 → 软 F 分数，容忍“USA/United States”类变异；\n- 旧数据 → 保留原奖励；\n- 采用 GRPO 优化，持续偏向高 ISR 且高 ISE 动作。\n\n实验结果\n\n- 五项基准（BrowseComp、GAIA、xbench-DS、Seal-0、WideSearch）**全面领先开源**，部分超越 Claude-4-Sonnet、OpenAI-o3；**同等/更少步数**获得更高精度，实现效率-效果双升。\n- 消融验证：Reverse-Union > Union > Basic，双阈值过滤 > 单阈值，RL 再涨 +3 pts 且奖励曲线持续上升。\n\n贡献\n\n1. 首次提出“**实体密度驱动**”的 IS 任务构造与度量框架。\n2. 给出可扩展的**三阶复杂度合成流水线**与**效率-效果联合筛选机制**。\n3. 在五大公开基准上确立新 SOTA，验证“**以效率提升效果**”的可行性。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是WebLeaper%3A%20Empowering%20Efficiency%20and%20Efficacy%20in%20WebAgent%20via%20Enabling%20Info-Rich%20Seeking，链接是https://arxiv.org/pdf/2510.24697，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24697。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1761781638494","title":"AgentFrontier: Expanding the Capability Frontier of LLM Agents with ZPD-Guided Data Synthesis","description":"Training large language model agents on tasks at the frontier of their capabilities is key to unlocking advanced reasoning. We introduce a data synthesis approach inspired by the educational theory of the Zone of Proximal Development (ZPD), which defines this frontier as tasks an LLM cannot solve alone but can master with guidance. To operationalize this, we present the AgentFrontier Engine, an automated pipeline that synthesizes high-quality, multidisciplinary data situated precisely within the LLM's ZPD. This engine supports both continued pre-training with knowledge-intensive data and targeted post-training on complex reasoning tasks. From the same framework, we derive the ZPD Exam, a dynamic and automated benchmark designed to evaluate agent capabilities on these frontier tasks. We train AgentFrontier-30B-A3B model on our synthesized data, which achieves state-of-the-art results on demanding benchmarks like Humanity's Last Exam, even surpassing some leading proprietary agents. Our work demonstrates that a ZPD-guided approach to data synthesis offers a scalable and effective path toward building more capable LLM agents.","url":"https://huggingface.co/papers/2510.24695","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.494Z","metadata":{"authors":["Xuanzhong Chen","Zile Qiao","Guoxin Chen","Liangcai Su","Zhen Zhang","Xinyu Wang","Pengjun Xie","Fei Huang","Jingren Zhou","Yong Jiang"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.24695.pdf","arxivUrl":"https://arxiv.org/abs/2510.24695","arxivId":"2510.24695","coolPaperUrl":"https://papers.cool/arxiv/2510.24695","published":"2025-10-28T17:50:47Z","updated":"2025-10-28T17:50:47.000Z","zh_summary":"在大语言模型（LLM）能力前沿的任务上训练代理，是解锁高级推理能力的关键。我们引入了一种受“最近发展区”（Zone of Proximal Development, ZPD）教育理论启发的数据合成方法，该理论将能力前沿定义为模型无法独立解决但在指导下能够掌握的任务。为了实现这一目标，我们提出了 AgentFrontier 引擎，这是一个自动化流水线，可精确生成位于 LLM ZPD 内的高质量、多学科数据。该引擎支持利用知识密集型数据进行持续预训练，也可针对复杂推理任务进行定向后训练。在同一框架下，我们还提出了 ZPD 考试，这是一个动态、自动化的基准测试，用于评估代理在这些前沿任务上的能力。我们使用合成数据训练了 AgentFrontier-30B-A3B 模型，其在《人类最后的考试》等高难度基准测试中取得了最先进的结果，甚至超过了一些领先的专有代理。我们的研究表明，以 ZPD 为指导的数据合成方法，为构建更强大的 LLM 代理提供了一条可扩展且有效的路径。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何为大型语言模型（LLM）智能体持续生成恰好位于其能力边界、且能最大化学习收益的困难训练数据”这一核心问题。具体而言，现有方法在以下三方面存在明显短板：\n\n1. 数据难度失配\n静态或启发式构造的数据往往过易（模型已掌握）或过难（任何支持也无法解决），难以精准落在模型的“最近发展区”（Zone of Proximal Development, ZPD）——即“独立不可解、但在工具与推理支持下可学会”的任务区间。\n\n2. 知识融合场景匮乏\n主流合成范式侧重单段文本的局部检索式问答，缺乏迫使模型跨文档、跨领域进行信息整合与深度推理的任务，难以培养“研究级”智能体所需的复杂认知技能。\n\n3. 评估基准饱和\n高难度人工评测（如 Humanity’s Last Exam）成本极高且静态，无法随模型能力提升而自适应更新，导致评测信号快速失效。\n\n为此，作者提出 AgentFrontier 框架，通过“教育学 ZPD 理论”指导数据与评测的协同演化，实现以下目标：\n\n- 自动合成**恰好位于模型 ZPD** 的多学科问答轨迹，用于持续预训练（知识密集型）和后训练（推理密集型）。\n- 同步生成**动态自适应基准 ZPD Exam**，保证评测始终瞄准模型能力前沿。\n- 训练得到的 30B 模型在 HLE 等前沿测试上取得 SOTA，验证 ZPD 导向的数据合成对提升智能体深度研究与工具使用能力的有效性与可扩展性。\n\nQ2: 有哪些相关研究？\n\n论文在第 6 节“Related Work”中系统梳理了三条研究脉络，并指出各自与本文工作的区别。可归纳为以下 9 个代表性方向（按出现顺序整理，不含原引用编号）：\n\n1. 面向智能体的**数据合成**\n\n- TaskCraft、MiroVerse、MegaScience 等通过可执行轨迹或教科书 QA 生成大规模任务，但侧重“可验证”而非“难度校准”，任务多可被单步检索或局部推理解决，缺乏跨源知识融合需求。\n2. 难度可控的数据生成\n\n- 近期研究尝试用约束叠加、迭代改写等方式提升难度，但仍依赖启发式规则，无法保证问题恰好落在模型“独立不可解 / 有支持可解”的 ZPD 区间，导致信号噪声大、扩展性差。\n3. 多学科评测基准\n\n- MMLU、MMLU-Pro、GPQA、SuperGPQA、R-Bench、xBench-ScienceQA 等提供本科或研究生级知识问答，但题库静态，随模型能力提升迅速饱和，难以持续区分前沿模型。\n4. 专家级静态评测\n\n- Humanity’s Last Exam（HLE）通过领域专家人工编纂极高难度题目，被视为“天花板”评测，然而成本高昂、更新缓慢，无法形成“随模型进化而进化”的飞轮。\n5. 检索增强生成（RAG）与工具使用\n\n- 传统 RAG 假设答案可在单文档中直接定位；当任务需要跨文档综合、因果推理或数值验证时性能骤降。AgentFrontier 强调“工具-推理协同”而非“先检索后回答”。\n6. 深层研究（Deep-Research）智能体\n\n- 闭源方案：OpenAI DeepResearch、Gemini DeepResearch、Kimi-Researcher 等已展示多步搜索与报告生成能力，但无公开训练数据与可复现方法。\n- 开源方案：WebDancer、WebSailor、WebShaper、WebThinker 等侧重轨迹公开，却未解决“任务难度与模型能力边界精准对齐”的问题。\n7. 自演化或动态评测\n\n- 目前缺少“随模型能力提升自动再生题目”的基准。ZPD Exam 通过“基线模型三试不可解 + 工具三试可解”的双条件过滤，实现题库与能力前沿同步移动，填补该空白。\n8. 强化学习与探索\n\n- 本文 BoN 实验显示 pass@1→pass@8 有 19 分提升，暗示模型策略分布包含多条成功轨迹，为后续 RL 探索提供先验；而现有模仿学习工作（包括本文 RFT 阶段）尚未利用该潜力。\n9. 工具创造与元工具\n\n- 当前智能体仅调用固定 API，未能根据新问题动态生成或组合工具。作者将“工具创造”列为未来工作，与本文“工具使用者”定位形成延续。\n\n综上，AgentFrontier 与既有研究的根本区别在于：\n\n- 以教育学 ZPD 理论为锚点，**双向约束**（LKP 不可解 & MKO 可解）实现难度自动校准；\n- 同步产出**训练数据**与**动态评测**，形成“数据-能力”共同演化的闭环；\n- 强调**跨文档知识融合**与**工具链协同推理**，而非单步检索或局部可验证任务。\n\nQ3: 论文如何解决这个问题？\n\n论文将“持续生成恰好落在模型能力边界（ZPD）的多学科、知识融合型训练数据”这一难题拆解为**三大技术模块**，并以**双阶段训练+动态评测**的闭环形式给出完整解决方案。核心流程见图 3 与 Algorithm 1，可概括为以下 5 步：\n\n1\\. 形式化 ZPD：把“难度刚好”变成可计算约束\n\n- 定义 **LKP**（Less Knowledgeable Peer）= 无工具基础模型\n- 定义 **MKO**（More Knowledgeable Other）= 同尺寸模型+工具+高级推理 prompt\n- 对于任意问答对  (q,a) ，仅当\n\nIsSolvableBy(LKP,q,a)=0 quad land quad IsSolvableBy(MKO,q,a)=1\n\n才被视为落在 ZPD，可进入后训练集  D_(ZPD) ；否则按难度分流到持续预训练集  D_(pretrain)  或人工复核集  D_(human) 。\n\n2\\. AgentFrontier Engine：三阶段数据合成管线\n\n| 阶段 | 关键算子 | 目标 | 技术要点 |\n| --- | --- | --- | --- |\n| I 种子生成 |  Phi_(chunk), M_(gen)  | 产生必须跨源融合才能回答的初版 QA | 1. 百万级开放文档→语义块；2. 检索+主题一致性阈值  τ_(theme)  组建三元组；3. 用 235B 模型生成种子 QA |\n| II 复杂度 escalation |  Psi_(escalate), A_(refine)  | 把种子 QA 迭代升级为高阶推理+工具验证形态 | 每轮沿 4 维提升：知识扩张、概念抽象、事实多源校验、数值/代码验证；停止条件：LKP 首次失败或达到  K_(max)=30  |\n| III ZPD 校准 |  BoN, IsCorrect  | 过滤出“LKP 失败且 MKO 至少 1 次成功”的样本 | 用 GPT-4o 作为自动裁判；语义去重（reranker 相似度  <0.7 ） |\n\n3\\. 双通道训练：让“知识”与“推理”各得其所\n\n- **持续预训练（CPT）**\n50 B tokens 来自  D_(pretrain) （知识密集型但 LKP 可解），增强模型参数记忆与语言建模能力。\n目标函数：\nL_(CPT)(θ)=-∑_(t)log p_θ(x_t|x_(<t))\n\n- **拒绝式微调（RFT）**\n12 k 条  D_(ZPD)  轨迹（LKP 不可解+MKO 可解），仅保留最终答案完全正确的整条 ReAct 轨迹。\n目标函数：\nL_(RFT)(θ)=-∑_(i=1)^(K)∑_(j=1)^(L_i)log p_θ(r_j^((i))|q^((i)),r_(j-1)^((i)),o_(j-1)^((i)))\n\n只回传 reasoning report 令牌，工具观测不参与梯度更新。\n\n4\\. ZPD Exam：与训练同框架产出的“活基准”\n\n- 题库来源：2023–2025 最新 3 万篇跨学科论文，确保参数记忆不可直接回答。\n- 准入条件：基线模型**无工具三试全错**且**有工具三试至少一次全对**——即严格落在其 ZPD。\n- 效果：自动随基线模型能力刷新，避免饱和；提供三级诊断（Intrinsic/ZPD/Mastery）。\n\n5\\. 实验验证：难度校准与性能增益\n\n- **BoN 分析**（图 7）\npass@1→pass@8 提升 19 分，证明数据集非“全有或全无”，而是富含**可探索的次优轨迹**，为后续 RL 提供信号。\n\n- **工具效能**（表 3）\n相同调用量下，AgentFrontier 模型在 Search/Scholar/Browser/Code 上的**条件准确率**均显著高于基线数据集，验证其“高功效编排”而非“高频率调用”。\n\n- **主结果**（表 4）\n30B 模型经 CPT+RFT 后，HLE text-only 得分 28.6%，**超越所有开源对手**并持平或超过多款千亿级闭源研究智能体，证实 ZPD 导向的数据合成路径可在**参数规模更小**的情况下逼近甚至突破当前能力前沿。\n\n综上，论文通过“教育学 ZPD 理论 → 可计算过滤 → 三阶段数据引擎 → 双通道训练 → 自适应评测”的完整闭环，首次把“难度刚好”的合成问题转化为可扩展的自动化流程，从而系统性地提升了 LLM 智能体在知识融合、工具编排与深度推理上的综合表现。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「ZPD 导向的数据合成是否真能提升模型在知识融合与工具推理上的极限能力」这一核心问题，设计了**多维度、可消融、可诊断**的实验矩阵。主要结果汇总于图 6、表 2–4 与图 7–8，可归纳为 5 组实验：\n\n1\\. 主实验：跨模型 backbone 的基准横向对比\n\n**目的**：验证 AgentFrontier 数据集相对于现有公开数据集的**普遍性与一致性增益**。\n**设置**：\n\n- 固定训练预算（12 k 条轨迹、3 epoch、25 600 rounds、≤40 960 tokens/round）。\n- 对比数据集：TaskCraft、MegaScience、MiroVerse。\n- 评测基准：HLE（text-only）、ZPD Exam-v1、R-Bench-T、xBench-ScienceQA。\n- 模型：Qwen3-8B / 32B / 30B-A3B（MoE）。\n\n**结果**（图 6）：\n\n- 三条 backbone 上，AgentFrontier **全线第一**，平均领先次优数据集 3.8–6.0 分。\n- 30B-A3B 在 HLE 从 9.2→25.7（+178%），带工具基线仅 10.2，**相对提升 152%**。\n\n2\\. 学科级细粒度消融（表 2）\n\n**目的**：排除“分数提升仅来自个别学科”的可能性。\n**做法**：将 HLE 按 8 大学科拆分，报告每科准确率。\n**结论**：\n\n- 8B/32B 上 AgentFrontier 在 6–7 个学科夺魁；30B-A3B **8 科全部第一**。\n- 证明数据合成策略对**数学、人文、工程、生物等异质领域均有效**。\n\n3\\. Best-of-N 难度诊断（图 7）\n\n**目的**：检验数据集是否真正落在“可学习”难度区，而非过难或模板记忆。\n**做法**：在 300 题验证集上采样 N=1…8 条独立轨迹，观察 pass@N。\n**关键结果**：\n\n- pass@1 = 21.7%，pass@8 = 40.7%，**总增益 19.0 分**。\n- 说明模型策略分布内含多条可成功路径，为后续 RL 探索提供**充足信号**。\n\n4\\. 工具使用效能剖析（表 3 + 图 8）\n\n**目的**：区分“调用更频繁”还是“调用更精准”。\n**指标**：\n\n- 条件准确率 = 用到该工具且最终答对的任务占比。\n- 平均轮数 & 累积分布（图 8）。\n**结果**：\n- AgentFrontier 模型在 Search/Scholar/Browser/Code 四项工具的条件准确率**全面领先**（宏观均值 26.3% vs 21%）。\n- 95% 问题在 ≤15 轮内解决，**同样轮数下准确率持续高于基线**，证实其**高功效编排**优势。\n\n5\\. 整体训练管线消融：CPT vs RFT（表 4 末行）\n\n**目的**：量化「持续预训练」带来的额外增益。\n**设置**：30B-A3B 分别只做 RFT、以及 CPT(50 B tokens)+RFT。\n**结果**：\n\n- HLE +2.9，ZPD Exam +2.0，R-Bench +2.7，xBench-ScienceQA +7.0。\n- 说明**基础知识强化 → 智能体任务表现**存在直接正迁移，ZPD 数据同时受益于参数记忆与推理策略。\n\n附加实验与诊断\n\n- **相似度过滤 ablation**：若去掉 ϵ=0.7 去重，HLE 分数下降 1.8 分，验证多样性机制有效。\n- **人类评审案例对比**（附录 C）：在 HLE 临床题上，AgentFrontier 正确识别“慢性骨髓炎”而 OpenAI DeepResearch 陷入 Charcot 误区，展示**异常发现→假设→文献验证**的完整研究链路。\n\n综上，实验从「宏观基准→学科细粒度→难度分布→工具效能→训练阶段」五个层次交叉验证，证明 ZPD 导向的数据合成不仅能**普遍提升模型在多学科前沿评测上的绝对表现**，而且生成的任务确实位于“可学习”难度区间，为后续强化学习、工具创造等方向奠定了可扩展的基础。\n\nQ5: 有什么可以进一步探索的点？\n\n作者在第 7 节“Limitations and Future Work”中已给出三条明确路线图，结合实验结果与框架特性，可进一步提炼出 6 个值得深入的研究切入点：\n\n1\\. 分级脚手架（Graduated Scaffolding）\n\n- **现状**：MKO 采用“一次给全解”的二元模式，缺乏人类教学中“提示→子目标→示范”的渐进支援。\n- **探索方向**：\n– 设计分层支援策略库（high-level hint、partial plan、executable sketch），用 curriculum-RL 自动学出最优支援顺序与时机。\n– 引入元认知损失，让智能体学会“何时、如何”主动索取提示，降低样本复杂度。\n\n2\\. 从模仿到探索（Imitation → RL）\n\n- **现状**：RFT 仅覆盖单条成功轨迹，BoN 实验显示 pass@1→pass@8 仍有 19 分潜力未被挖掘。\n- **探索方向**：\n– 以 RFT 模型为 warm-start，采用 REINVENT/GRPO 等 on-policy RL，把 ZPD 过滤信号直接当奖励，自动发现超越示范的新路径。\n– 结合过程奖励模型（PRM），对中间工具调用与推理步骤给予细粒度奖励，缓解稀疏奖励问题。\n\n3\\. 动态工具创造（Tool Creation）\n\n- **现状**：工具集 T 固定，智能体仅为“调用者”。\n- **探索方向**：\n1. **Hierarchical Meta-Tools**：学习把常用子序列（search→filter→compute）封装成可复用函数，通过代码生成+签名验证自动加入 T。\n2. **Program Synthesis**：面对全新问题，直接生成一次性 Python 类或 DSL 函数，实现“工具即程序”的即时扩展。\n3. **工具可微优化**：将工具参数或搜索关键词视为可微变量，利用梯度或强化学习进行端到端优化。\n\n4\\. 多智能体对抗校准（Multi-Agent ZPD）\n\n- **现状**：LKP/MKO 是同尺寸模型的两种“人格”，存在能力上限共线问题。\n- **探索方向**：\n– 引入更强“教师模型群”作为 MKO，与“学生模型”进行迭代博弈：教师持续生成把学生逼到 ZPD 边缘的任务，学生通过 RL/self-play 不断外推边界，形成“能力–数据”飞轮。\n– 用 population-based training 维护多代学生池，保证任务难度始终领先当前最强学生一小步。\n\n5\\. 跨模态 ZPD（Beyond Text）\n\n- **现状**：AgentFrontier 仅使用文本网页与代码解释器。\n- **探索方向**：\n– 将论文框架扩展到**图表+文献+实验数据**的多模态场景：用视觉模型把图表转为可执行代码或结构化数据，再进入 escalation 循环。\n– 设计“可执行文档”概念，把论文中的公式、表格、伪代码自动转换为可验证的 Python 单元测试，提升事实一致性。\n\n6\\. 持续学习与灾难性遗忘平衡\n\n- **现状**：CPT→RFT 两阶段训练，未讨论顺序接入新学科时的遗忘问题。\n- **探索方向**：\n– 在 CPT 阶段引入**弹性权重巩固（EWC）或回放机制**，确保新增 ZPD 数据不冲刷早期知识。\n– 构建“ZPD 缓冲池”，按学科–难度二维索引，动态抽取旧任务与新任务混合训练，实现**能力边界持续外推**而不过时。\n\n小结\n\nAgentFrontier 首次把教育学概念转化为可扩展的自动数据引擎，但仍在**支援粒度、探索策略、工具生态、评测维度**上留有巨大空间。上述六点任何一条的突破，都可能把当前“高质量模仿智能体”推向真正的**自驱研究智能体**。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **AgentFrontier**——一个受教育学“最近发展区”（ZPD）理论驱动的**数据合成-训练-评测**一体化框架，用于持续扩展大型语言模型智能体的能力前沿。核心内容与贡献可概括为以下四点：\n\n1\\. 问题定义\n\n- 现有训练数据难以**精准落在模型能力边界**：要么太易（已掌握），要么太难（无法学会）。\n- 静态基准迅速饱和，缺乏**可扩展、自适应**的高难度评测。\n- 真实研究任务要求**跨文档知识融合**与**多工具协同推理**，当前数据集支持不足。\n\n2\\. 方法论\n\n**A. ZPD 形式化**\n定义二元角色：\n\n- LKP（无工具基础模型）→ 独立不可解\n- MKO（工具增强同模型）→ 有支持可解\n仅保留“LKP 失败且 MKO 成功”的 QA 对，确保数据**恰好位于能力边界**。\n\n**B. AgentFrontier Engine（三阶段）**\n\n1. 种子生成：百万级开放文档 → 语义块 → 主题一致性三元组 → 235B 模型生成初版 QA。\n2. 复杂度 escalation：迭代调用搜索/学术/浏览器/代码工具，沿知识扩张、概念抽象、事实校验、数值计算四维升级。\n3. ZPD 过滤：LKP/MKO 对抗校准 + Best-of-N 验证 + 语义去重，产出后训练集  D_(ZPD)  与预训练集  D_(pretrain) 。\n\n**C. 双通道训练**\n\n- CPT：50 B tokens 知识密集型数据 → 增强参数记忆。\n- RFT：12 k 条 ZPD 轨迹 → 拒绝采样微调，仅学习成功推理路径。\n\n**D. ZPD Exam（自适应基准）**\n\n- 2023–25 最新论文为语料，自动再生题目；准入条件“基线无工具三试全错 + 有工具三试至少一次全对”，保证始终瞄准新前沿。\n\n3\\. 实验结果\n\n- **跨 backbone 全线 SOTA**：30B-A3B 在 HLE text-only 达 28.6%，**超越所有开源模型**并持平/超过多款千亿级闭源研究智能体。\n- **学科级 dominance**：HLE 八大学科全部第一，平均领先次优数据集 3.8–6.0 分。\n- **难度诊断**：BoN 实验 pass@1→pass@8 提升 19 分，验证数据集富含可探索路径。\n- **工具效能**：同等调用量下条件准确率宏观均值 26.3% vs 21%，实现**高功效编排**。\n\n4\\. 未来方向\n\n1. 分级脚手架：从“全或无”支援转向**渐进提示+元认知学习**。\n2. 探索式 RL：利用 BoN 潜力，从模仿走向**自主策略发现**。\n3. 动态工具创造：支持**程序合成**与**元工具组合**，突破静态 API 限制。\n4. 多智能体对抗校准、跨模态 ZPD、持续学习遗忘平衡等扩展。\n\n一句话总结\n\nAgentFrontier 用“教育学 ZPD 理论”把**数据难度校准、多学科知识融合、工具协同推理**转化为可自动运行的闭环系统，在更小参数规模下实现当前最强开源研究智能体，并提供随能力增长的**自进化评测**，为迈向更自主的 AGI 智能体奠定数据-训练-评测一体化新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是AgentFrontier%3A%20Expanding%20the%20Capability%20Frontier%20of%20LLM%20Agents%20with%20ZPD-Guided%20Data%20Synthesis，链接是https://arxiv.org/pdf/2510.24695，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24695。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1761781638495","title":"Group Relative Attention Guidance for Image Editing","description":"Recently, image editing based on Diffusion-in-Transformer models has undergone rapid development. However, existing editing methods often lack effective control over the degree of editing, limiting their ability to achieve more customized results. To address this limitation, we investigate the MM-Attention mechanism within the DiT model and observe that the Query and Key tokens share a bias vector that is only layer-dependent. We interpret this bias as representing the model's inherent editing behavior, while the delta between each token and its corresponding bias encodes the content-specific editing signals. Based on this insight, we propose Group Relative Attention Guidance, a simple yet effective method that reweights the delta values of different tokens to modulate the focus of the model on the input image relative to the editing instruction, enabling continuous and fine-grained control over editing intensity without any tuning. Extensive experiments conducted on existing image editing frameworks demonstrate that GRAG can be integrated with as few as four lines of code, consistently enhancing editing quality. Moreover, compared to the commonly used Classifier-Free Guidance, GRAG achieves smoother and more precise control over the degree of editing. Our code will be released at https://github.com/little-misfit/GRAG-Image-Editing.","url":"https://huggingface.co/papers/2510.24657","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.495Z","metadata":{"authors":["Xuanpu Zhang","Xuesong Niu","Ruidong Chen","Dan Song","Jianhao Zeng","Penghui Du","Haoxiang Cao","Kai Wu","An-an Liu"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.24657.pdf","arxivUrl":"https://arxiv.org/abs/2510.24657","arxivId":"2510.24657","coolPaperUrl":"https://papers.cool/arxiv/2510.24657","published":"2025-10-28T17:22:44Z","updated":"2025-10-28T17:22:44.000Z","zh_summary":"最近，基于 Diffusion-in-Transformer (DiT) 模型的图像编辑发展迅速。然而，现有的编辑方法通常缺乏对编辑程度的有效控制，这限制了其实现更定制化结果的能力。为了解决这一局限性，我们研究了 DiT 模型中的 MM-Attention 机制，并发现 Query 和 Key token 共享一个仅依赖于层的偏置向量。我们将此偏置解释为模型固有的编辑行为，而每个 token 与其对应偏置之间的差值则编码了与内容相关的编辑信号。基于这一见解，我们提出了组相对注意力引导（Group Relative Attention Guidance, GRAG），这是一种简单而有效的方法，通过重新加权不同 token 的差值来调节模型在输入图像与编辑指令之间的关注，从而实现对编辑强度的连续且细粒度的控制，无需任何参数调优。在现有图像编辑框架上进行的大量实验表明，GRAG 可以仅通过四行代码进行集成，并始终提高编辑质量。此外，与常用的无分类器引导（Classifier-Free Guidance）相比，GRAG 在控制编辑程度上实现了更平滑、更精确的效果。我们的代码将发布在 https://github.com/little-misfit/GRAG-Image-Editing.","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对基于 Diffusion-in-Transformer（DiT）的图像编辑方法在“编辑强度”控制上的不足，提出一种无需训练、可连续微调编辑力度的轻量级方案。核心待解决问题可归纳为：\n\n- **编辑-保真权衡难调**：现有 DiT 编辑器要么过度响应文本指令而破坏原图结构，要么过于保守导致编辑失败；用户只能反复调 prompt 或多次推理，无法细粒度地控制“改多少”。\n- **缺乏统一、平滑的强度调节接口**：Classifier-Free Guidance（CFG）等现有策略作用于去噪方向，粒度粗、变化不连续，难以实现“像素级”渐进式编辑。\n\n为此，作者通过分析 MM-Attention 的内部特征分布，发现 Query/Key 令牌在每一层都围绕一个**共享偏置向量**聚集，该偏置可视为“模型默认编辑行为”，而各令牌相对偏置的偏离 Δ 编码了“内容特异性编辑信号”。基于此观察，提出 **Group Relative Attention Guidance（GRAG）**，在注意力层内对 Δ 进行加权重分配，从而即时、连续地调节模型对“指令”与“原图”的相对关注度，实现**单系数平滑控制编辑强度**的目标。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究划分为两大主线，并给出对应代表工作。可归纳如下：\n\n1. Diffusion Transformers（DiT / MM-DiT）\n\n- 早期 DiT：Peebles & Xie, _Scalable diffusion models with transformers_, ICCV 2023.\n- 多模态统一注意力：Esser et al., _Scaling rectified flow transformers…_, ICML 2024（FLUX / MM-DiT）。\n- 中文/多语改进：Chen et al., _PixArt-α_；Bai et al., _Qwen2.5-VL_。\n2. Text-Driven Image Editing\n\n- 训练式方法\n- InstructPix2Pix（Brooks et al., CVPR 2023）——首个大规模指令-编辑对微调。\n- Emu Edit（Sheynin et al., CVPR 2024）、HiDream-E1（Cai et al., arXiv 2025）、Step1X-Edit（Liu et al., arXiv 2025）、Qwen-Edit（Wu et al., arXiv 2025）。\n- 免训练方法\n- Textual Inversion / DreamBooth（Gal et al., 2022；Ruiz et al., CVPR 2023）。\n- 基于 SDXL/FLUX 的流形编辑：FlowEdit（Kulikov et al., 2024）、StableFlow（Avrahami et al., 2024）。\n- 参数高效微调\n- LoRA（Hu et al., ICLR 2022）及其在 DiT 上的任务特定适配。\n3. 引导机制与可控生成\n\n- Classifier-Free Guidance（Ho & Salimans, 2022）——当前最广泛使用的条件强度调节。\n- 其他注意力/特征级干预：PnP Inversion（Ju et al., ICLR 2024）、Massive-Value 分析（Jin et al., 2025）。\n\n上述工作为本文提出的 Group Relative Attention Guidance（GRAG）提供了方法对比与实验基线。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“**在注意力层内对令牌相对偏置进行重加权**”实现连续、细粒度的编辑强度控制，具体步骤如下：\n\n1. 发现关键结构\n对 MM-DiT 的 Query/Key 嵌入做统计，发现同一层所有令牌围绕一个**共享偏置**  b  聚集，且该偏置仅与层权重有关，与输入内容、去噪步数无关。\n将任意令牌向量分解为\n\nk_i = b + Delta k_i\n\n其中  Delta k_i  编码“内容特异性”信号。\n\n2. 提出 Group Relative Attention Guidance（GRAG）\n对选定的令牌组（如源图 tokens）先计算组内平均偏置\n\nK_(bias) = (1) / (N)∑_j k_j\n\n再求各令牌偏离\n\nDelta k_i = k_i − K_(bias)\n\n用两个正实数系数  (λ,δ)  重组合：\n\nk_i = λ· K_(bias) + δ·Delta k_i\n\n-  λ  控制“组整体影响力”大小\n-  δ  控制“组内差异”被放大还是压缩，从而实现**连续、平滑**的编辑力度调节。\n3. 即插即用实现\n仅需 4 行代码：\n\n```\nKbias = K[group].mean(dim=1)\nK[group] = λ*Kbias + δ*(K[group] − Kbias)\n```\n\n无需重新训练，也不改变原去噪流程。\n\n4. 实验验证\n\n- 在 Kontext、Step1X-Edit、Qwen-Edit 等**训练式**编辑器上，GRAG 用固定  (λ,δ)  即可同时提升 **LPIPS/SSIM/EditScore**。\n- 与 Classifier-Free Guidance 对比，GRAG 的  δ  单变量调节曲线**单调且平滑**，而 CFG 改变强度时指标震荡明显。\n- 在 FlowEdit、StableFlow 等**免训练**方法上，GRAG 仍能提供可感知的保真-编辑权衡，只是稳定性略低于训练式模型。\n\n通过上述“**偏置-相对量分解 + 组内重加权**”策略，论文把“编辑强度”转化为可连续滑动的  δ  旋钮，解决了 DiT 编辑器缺乏精细、一致控制接口的问题。\n\nQ4: 论文做了哪些实验？\n\n论文在 PIE 基准上系统比较了 GRAG 与现有方法，并补充消融与可视化，共 4 组实验：\n\n1. **主实验：训练式编辑器**\n基线：Kontext-Dev、Step1X-Edit、Qwen-Edit\n指标：LPIPS↓、SSIM↑、Cons↑、PF↑、EditScore↑\n结果：GRAG 在 3 个模型上均提升 EditScore，LPIPS/SSIM 改善显著，证明**保真-编辑权衡**被优化。\n\n2. **主实验：免训练编辑器**\n基线：FlowEdit、StableFlow、StableFlow+（二阶反演）\n指标同上。\n结果：GRAG 仍能降低 LPIPS、提升 Cons，但增益幅度小于训练式模型，验证**通用适配性**。\n\n3. **消融与对比：GRAG vs. CFG**\n\n- 固定样本连续调节\n- CFG 尺度 1.0→5.0：指标曲线震荡，视觉变化不单调。\n- GRAG 仅调 δ 0.95→1.15：LPIPS、SSIM、EditScore 单调平滑，图 9 显示颜色/纹理渐变自然。\n- 双因子消融\n仅调 λ：指标几乎不变；\n仅调 δ：曲线最平滑；\n同时调 λ+δ：出现伪影，确认**δ 是主要控制旋钮**。\n4. **特征可视化与稳定性分析**\n\n- 图 4–6、附录图 S1–S6：展示 Q/K 嵌入的层间偏置分布、RoPE 频率关联、头间均值方差，支撑“**共享偏置存在且稳定**”假设。\n- 不同随机种子、不同去噪步重复实验，标准差小于 1%，验证**GRAG 对采样变化鲁棒**。\n\n综上，实验覆盖训练/免训练两大场景、定量指标与主观视觉、单因子与双因子消融，并与主流 CFG 进行对照，全面验证 GRAG 的**有效性、通用性、平滑控制能力**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入：\n\n- **偏置的语义解释与可视化**\n进一步分解  K_(bias)  在各层、各头的频域/语义分量，验证其是否对应“全局构图”“局部纹理”等抽象因子，并构建可解释面板供用户按需操控。\n\n- **动态  (λ,δ)  调度**\n当前使用固定值或人工网格搜索；可借鉴扩散采样中的“噪声调度”思想，设计步数或注意力得分感知的**渐进式**  (λ_t,δ_t) ，实现“先粗后细”的编辑策略。\n\n- **跨层联合优化**\n仅对单一层组施加 GRAG；可探索层间耦合损失，使不同层的  Delta k  重加权协同更新，避免局部过度编辑导致的伪影。\n\n- **扩展到视频 / 3D 生成**\n将“组偏置”概念迁移到时空注意力或 cross-frame attention，研究  δ  对时序一致性与运动保真的影响，实现**连续帧编辑强度一致**。\n\n- **与 CFG 互补融合**\n建立“双通道”引导：CFG 控制条件-无条件方向，GRAG 控制内部内容-风格权衡，学习二者**联合最优系数**以进一步提升 EditScore。\n\n- **自动化强度估计**\n训练轻量级回归器，以文本指令复杂度、原图内容特征为输入，直接预测最佳  δ ，实现**一键式**免调参编辑。\n\n- **训练阶段内嵌 GRAG**\n当前为推理插件；可在继续预训练/微调时把  (λ,δ)  作为可学习标量纳入损失，使模型**原生支持**连续强度控制，降低对外部系数的依赖。\n\n- **掩码驱动的局部 GRAG**\n引入用户涂鸦或分割掩码，仅对掩码内令牌应用  δ>1 ，外部  δ<1 ，实现**空间局部化**精细编辑，避免全局风格漂移。\n\n- **理论收敛性与稳定性分析**\n从随机过程角度研究重加权后的注意力矩阵谱范数变化，给出  δ  的**理论上界**，防止过度放大  Delta k  导致迭代发散。\n\n- **多语言-多文化评估**\n当前实验以英文提示为主；可构建多语言编辑基准，验证偏置分布在不同语言 tokenizer 下是否保持一致，确保 GRAG 的**跨语言通用性**。\n\nQ6: 总结一下论文的主要内容\n\n**Group Relative Attention Guidance for Image Editing** 提出一种**无需训练、四行代码即可插入**的轻量化机制，解决 DiT 图像编辑器“编辑强度”难以连续、精细控制的痛点。\n\n1. 关键发现\nMM-DiT 的 Query/Key 嵌入在每层都围绕**共享偏置**  b  聚集，且  b  仅由层权重决定；令牌相对偏置的偏离  Delta k_i  承载内容特异性信号。\n\n2. 方法：GRAG\n对选定令牌组计算平均偏置  K_(bias) ，再用两个正系数  (λ,δ)  重组合：\n\nk_i = λ K_(bias) + δ(k_i - K_(bias))\n\n-  λ  调节组整体影响力\n-  δ  实现**平滑、连续**的编辑力度旋钮\n3. 实验结果\n\n- 在 Kontext、Step1X-Edit、Qwen-Edit 等**训练式**框架上，固定  (λ,δ)  即可同时提升 LPIPS、SSIM、EditScore。\n- 与 Classifier-Free Guidance 相比，GRAG 的  δ  单变量调节曲线**单调且视觉过渡自然**。\n- 在 FlowEdit、StableFlow 等**免训练**方法上仍有效，验证通用适配。\n4. 贡献\n① 揭示 DiT 注意力层存在可分解的共享偏置；\n② 提出 GRAG，实现即插即用的细粒度强度控制；\n③ 在多种编辑器、多项指标上取得一致增益，为后续 DiT 可控编辑提供新接口。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Group%20Relative%20Attention%20Guidance%20for%20Image%20Editing，链接是https://arxiv.org/pdf/2510.24657，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24657。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1761781638511","title":"VisCoder2: Building Multi-Language Visualization Coding Agents","description":"Large language models (LLMs) have recently enabled coding agents capable of generating, executing, and revising visualization code. However, existing models often fail in practical workflows due to limited language coverage, unreliable execution, and lack of iterative correction mechanisms. Progress has been constrained by narrow datasets and benchmarks that emphasize single-round generation and single-language tasks. To address these challenges, we introduce three complementary resources for advancing visualization coding agents. VisCode-Multi-679K is a large-scale, supervised dataset containing 679K validated and executable visualization samples with multi-turn correction dialogues across 12 programming languages. VisPlotBench is a benchmark for systematic evaluation, featuring executable tasks, rendered outputs, and protocols for both initial generation and multi-round self-debug. Finally, we present VisCoder2, a family of multi-language visualization models trained on VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms strong open-source baselines and approaches the performance of proprietary models like GPT-4.1, with further gains from iterative self-debug, reaching 82.4% overall execution pass rate at the 32B scale, particularly in symbolic or compiler-dependent languages.","url":"https://huggingface.co/papers/2510.23642","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.511Z","metadata":{"authors":["Yuansheng Ni","Songcheng Cai","Xiangchao Chen","Jiarong Liang","Zhiheng Lyu","Jiaqi Deng","Kai Zou","Ping Nie","Fei Yuan","Xiang Yue","Wenhu Chen"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.SE","cs.AI","cs.CL","cs.PL"],"pdfUrl":"https://arxiv.org/pdf/2510.23642.pdf","arxivUrl":"https://arxiv.org/abs/2510.23642","arxivId":"2510.23642","coolPaperUrl":"https://papers.cool/arxiv/2510.23642","published":"2025-10-24T18:03:57Z","updated":"2025-10-24T18:03:57.000Z","zh_summary":"大型语言模型（LLMs）最近使得能够生成、执行和修改可视化代码的编程代理成为可能。然而，由于语言覆盖有限、执行不可靠以及缺乏迭代修正机制，现有模型在实际工作流程中往往表现不佳。进展受到狭窄数据集和强调单轮生成及单语言任务的基准测试的限制。为了解决这些挑战，我们引入了三个互补的资源，以推动可视化编程代理的发展。VisCode-Multi-679K 是一个大规模的监督数据集，包含 679K 个经过验证且可执行的可视化样本，并涵盖 12 种编程语言的多轮修正对话。VisPlotBench 是一个系统化评估基准，提供可执行任务、渲染输出以及初始生成和多轮自我调试的评估协议。最后，我们提出了 VisCoder2，这是一系列在 VisCode-Multi-679K 上训练的多语言可视化模型。实验表明，VisCoder2 显著优于强大的开源基线模型，并接近 GPT-4.1 等专有模型的性能，通过迭代自我调试还可进一步提升，在 32B 规模下整体执行通过率达到 82.4%，尤其在符号或依赖编译器的语言中表现突出。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有大语言模型（LLM）在可视化代码生成任务中面临的三大核心痛点：\n\n1. 语言覆盖不足\n主流方法仅支持 Python（matplotlib/plotly）或 Vega-Lite 等单一语言，无法满足科研、出版、工程等领域对 LaTeX（TikZ/PGFPlots）、LilyPond（乐谱）、Asymptote（矢量 3D）等符号型或编译型语言的多样化需求。\n\n2. 缺乏运行时验证与迭代修正\n现有数据集多为单轮、不可执行片段，模型无法根据“执行-渲染-报错”反馈进行多轮自我调试，导致生成的代码在实际环境中频繁崩溃或输出与意图不符。\n\n3. 评测体系缺失\n已有基准仅覆盖单语言、单轮生成，缺乏跨语言、可执行、带渲染结果的多轮调试评测协议，难以系统衡量模型在真实迭代工作流中的可靠性。\n\n为此，作者提出三大互补资源，构建“可执行、跨语言、可自修复”的可视化代码智能体框架：\n\n- VisCode-Multi-679K：首个 67.9 万规模、十二语言、全部经过“执行+渲染”校验的监督式指令微调数据集，并引入 6.6 万轮多回合纠错对话，用于训练模型根据运行日志修正代码。\n- VisPlotBench：覆盖 8 种语言、13 大可视化类别、888 项可执行任务的标准化基准，提供统一“执行-渲染-评分”协议，支持单轮生成与多轮自我调试两种评测模式。\n- VisCoder2：基于 Qwen2.5-Coder 在 VisCode-Multi-679K 上训练的多语言可视化代码模型族，3B–32B 参数规模均显著超越同规模开源基线，32B 在自调试模式下总体执行通过率提升至 82.4%，与 GPT-4.1 持平，并在符号/编译型语言上实现大幅领先。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 章“Related Work”中系统梳理了三条研究脉络，并指出它们与本文任务的差距。以下按领域归纳，并补充后续文献编号以便对照原文。\n\n1\\. 面向可视化的大语言模型代码生成\n\n| 代表工作 | 覆盖语言 | 是否可执行验证 | 多轮调试 | 主要局限 |\n| --- | --- | --- | --- | --- |\n| LIDA (Dibia, 2023) | Python | × | 单轮 | 仅 Python，无运行时校验 |\n| VisEval (Chen et al., 2024) | Python | × | × | 4 类图表，2 524 条静态评测 |\n| MatPlotBench (Yang et al., 2024c) | Python | × | × | 11 类图表，100 条任务 |\n| nvBench / nvBench 2.0 (Luo et al., 2021; 2025) | Vega-Lite | × | × | 声明式语法，无执行反馈 |\n| Text2Vis (Rahman et al., 2025) | Python | × | × | 10 类图表，1 985 条任务 |\n| PandasPlotBench-Enhanced (Ni et al., 2025) | Python | ✓ | ✓ | 首次引入自调试，但仅限 Python |\n\n**结论**：现有研究集中在 Python 或 Vega-Lite，缺乏跨语言、可执行、可迭代修正的统一框架。\n\n2\\. 通用代码生成与自调试代理\n\n| 代表工作 | 语言广度 | 是否可视化专用 | 多轮调试 | 与可视化差距 |\n| --- | --- | --- | --- | --- |\n| StarCoder-2 / the-stack-v2 (Lozhkov et al., 2024) | 600+ 语言 | × | × | 无可视化语法知识，渲染失败率高 |\n| OctoPack (Muennighoff et al., 2023) | 多语言 | × | ✓ | 未针对绘图库、无渲染验证 |\n| Self-Refine (Madaan et al., 2023) | Python 为主 | × | ✓ | 缺乏图表语义与视觉输出反馈 |\n| SWE-Agent (Yang et al., 2024b) | Python | × | ✓ | 面向 GitHub Issue，非可视化任务 |\n\n**结论**：通用代码代理虽支持多轮修正，但缺少可视化领域特有的“渲染结果→视觉相似度”反馈链路，难以保证图形正确性。\n\n3\\. 可视化评测基准\n\n| 基准 | 语言 | 可执行 | 多轮调试 | 类别数 | 规模 |\n| --- | --- | --- | --- | --- | --- |\n| VisEval | Python | × | × | 4 | 2 524 |\n| MatPlotBench | Python | × | × | 11 | 100 |\n| nvBench 2.0 | Vega-Lite | × | × | 5 | 7 878 |\n| PandasPlotBench-Enhanced | Python | ✓ | ✓ | 10 | 175 |\n| VisPlotBench（本文） | 8 语言 | ✓ | ✓ | 13 | 888 |\n\n**结论**：VisPlotBench 是第一个同时满足“多语言、可执行、多轮自调试、覆盖符号型语法”的评测体系，填补了该方向空白。\n\n4\\. 符号型与编译型可视化语言的相关研究\n\n- **LaTeX/TikZ**：学术出版广泛使用，但现有 LLM 支持度低，错误多为编译失败（UndefinedError、PackageError）。\n- **LilyPond**：音乐排版领域专用，语法严格，此前无大规模可执行数据集。\n- **Asymptote**：3D 矢量图形语言，依赖编译器，函数签名错误（FunctionSignatureError）频发。\n\n本文首次将这些符号型语言纳入统一的可执行数据与评测框架，并通过多轮自调试显著降低编译与运行时错误率。\n\nQ3: 论文如何解决这个问题？\n\n论文从“数据–评测–模型”三个维度协同发力，构建了一条“可执行、跨语言、可自修复”的完整技术路线，具体方案如下：\n\n1\\. 数据层：VisCode-Multi-679K\n\n**目标**：一次性解决“语言覆盖不足”与“缺乏可执行监督”两大痛点。\n\n| 关键设计 | 技术实现 | 解决什么问题 |\n| --- | --- | --- |\n| 12 语言全覆盖 | 从 the-stack-v2、CoSyn-400K、svg-diagrams 三大开源语料中，用库关键词 + GPT-4.1-mini 提取独立可视化代码块，覆盖 Python/LaTeX/LilyPond/Asymptote/Vega-Lite/SVG/HTML/Mermaid/JS/TS/C++/R | 以往数据集仅 Python/Vega-Lite，无法满足多语言需求 |\n| 运行时可执行 | 在隔离 Jupyter/内核环境严格校验：nbconvert 执行 + 渲染图像 >10 KB + 非单色过滤，失败即丢弃 | 以往 60%+ 片段无法运行，模型学到“幻觉”语法 |\n| 多轮纠错对话 | 引入 Code-Feedback 66 K 轮真实报错–修正对话，与可视化样本混合训练 | 让模型学会“根据报错信息改代码” |\n| 统一指令模板 | GPT-4.1 自动生成五段式自然语言描述（Setup + 数据/视觉描述 + 数据块 + 输出描述 + 风格描述），跨语言一致 | 消除不同来源提示风格差异，提升指令跟随一致性 |\n\n最终获得 679 K 条“指令–可执行代码–渲染图”三元组，是迄今规模最大、语言最多、全部可运行的可视化指令微调数据集。\n\n2\\. 评测层：VisPlotBench\n\n**目标**：填补“跨语言、可执行、多轮调试”评测空白，建立公平对比基准。\n\n| 关键设计 | 技术实现 | 解决什么问题 |\n| --- | --- | --- |\n| 8 语言 888 任务 | 手工筛选+执行验证，覆盖 13 大类别、116 子类型（含音乐、电路、3D、桑基图等冷门任务） | 以往基准仅 Python 或 Vega-Lite，无法衡量多语言能力 |\n| execute-render-score 协议 | 统一容器化运行环境，输出三件套：执行日志/渲染图/元数据，超时即判失败 | 保证结果可复现、可自动化 |\n| 多轮 self-debug 协议 | 首轮失败则把“指令+旧代码+截取报错”再次喂给模型，最多 3 轮，取最佳成绩 | 首次把“迭代修复”纳入正式指标，贴近真实工作流 |\n| 三维评估指标 | Execution Pass Rate（能否跑通）+ Task Score（语义对齐）+ Visual Score（视觉相似度） | 单看“跑通”不够，还需图形正确、美观 |\n\n3\\. 模型层：VisCoder2 训练策略\n\n**目标**：让模型既会“一次写对”，也会“报错后改对”。\n\n| 训练阶段 | 数据配比 | 关键技巧 | 效果 |\n| --- | --- | --- | --- |\n| 基础微调 | 100 % VisCode-Multi-679K，3 epoch，lr 5e-6，cosine，bf16 全参数 | 无 | 3B–32B 全系列同条件训练，可横向对比规模效应 |\n| 多轮对话增强 | 将 Code-Feedback 66 K 轮对话与可视化样本混合，统一模板化为多轮格式 | 采用“指令→代码→执行结果→修正代码”四元组，训练时随机截断历史，提升鲁棒性 | 模型学会见报错即定位、补全、删改，而非重新生成 |\n| 推理阶段自调试 | 温度 0.3，beam=1，失败即把报错信息截断 512 token 追加到上下文，再次生成 | 不更新权重，仅利用上下文学习 | 32B 模型在 3 轮内把总体执行率从 73.1 % 提到 82.4 %，LaTeX 提升 19 点，LilyPond 提升 13 点 |\n\n4\\. 系统级协同效果\n\n1. 数据 + 模型：VisCoder2-32B 在 12 语言平均执行率首次超越同规模通用代码模型 15 点，与 GPT-4.1 持平。\n2. 评测 + 模型：VisPlotBench 暴露的“符号语法脆弱性”被自调试机制大幅缓解——Asymptote 错误率下降 40 %，LaTeX 编译失败减少 28 %。\n3. 数据 + 评测：消融实验表明，仅使用 CoSyn 合成数据即可让 LilyPond 执行率从 5.5 % 提到 69 %；再加入 Code-Feedback 多轮对话，整体自调试收益额外 +6.2 %，证明“可执行监督”与“纠错对话”缺一不可。\n\n总结\n\n论文通过“可执行大数据 → 统一多轮评测 → 针对性训练与自调试”三位一体的闭环，首次让开源模型在 12 种可视化语言上同时实现：\n\n- 单轮生成高成功率（73 %@32B）\n- 多轮自修复接近 proprietary 水平（82 %）\n- 符号/编译型语言大幅领先 GPT-4.1-mini\n\n从而真正迈出“跨语言、可信赖、可迭代”的可视化代码智能体实用化一步。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“多语言可执行可视化代码生成与自调试”这一核心目标，设计了**四类互补实验**，覆盖**模型性能对比、语言粒度剖析、错误诊断、数据消融**四个维度，共计**30 余张定量表格与 24 组案例图**。以下按实验类别归纳：\n\n1\\. 主实验：VisPlotBench 全语言全模型对比\n\n**目的**：回答“VisCoder2 在单轮生成与多轮自调试两种模式下，是否显著超越同规模开源模型，并与 GPT-4.1 持平？”\n\n- **参评模型**\n– proprietary：GPT-4.1、GPT-4.1-mini\n– 开源基线：DeepSeek-Coder、DeepSeek-Coder-V2、Qwen2.5-Coder、VisCoder\n– 本文：VisCoder2-3/7/14/32B（共 4 个尺度）\n\n- **指标**\n– Execution Pass Rate（可执行率）\n– Task Score（0–100，LLM-judge 语义对齐）\n– Visual Score（0–100，LLM-judge 视觉相似度）\n– Good 比例（≥75 分样本占比）\n\n- **结果快照**（表 3 汇总）\n– 32B 档：VisCoder2 默认 73.1 % → 自调试 82.4 %，**首次追平 GPT-4.1（82.4 %）**，把同规模 Qwen2.5-Coder 拉开 **15.3 %** 差距。\n– 符号语言增益最大：LilyPond 从 5.5 %→69.1 %（+63.6 %），Asymptote 从 17 %→71 %（+54 %）。\n\n2\\. 细粒度剖析实验\n\n**目的**：揭示“不同语言/子类别的瓶颈到底在哪”。\n\n- **语言级拆解**\n– Python、Vega-Lite 已接近饱和（>90 %），继续放大模型主要提升在**符号/编译型**语言。\n– LaTeX：执行–语义错位显著——GPT-4.1 执行率 31 % 时 Task Score 仍达 50，说明“图画对了但编译不过”；自调试后执行率提至 66 %，Task Score 同步升至 56。\n– SVG：执行率普遍 >90 %，但 Visual Score 仅 40–50，暴露**渲染库差异**带来的“像素级”失配。\n\n- **子类别热力图**（附录 B 图 4）\n116 子类型中，**networks & flows、music、3D surface、radial polar** 四类样本最少，错误最集中，为未来数据扩充指明方向。\n\n3\\. 错误诊断与自调试轨迹实验\n\n**目的**：量化“自调试究竟修掉了哪些错误、哪些错误依旧顽固”。\n\n- **错误四分类统计**（表 5 + 附录 E）\n– Structural（语法/拼写）\n– Type & Interface（函数签名/参数）\n– Semantic / Data（变量未定义、数据形状）\n– Runtime / Environment（包缺失、渲染器崩溃）\n\n- **关键发现**\n– 结构类与接口类错误**降幅最大**：Python 接口错误 13→3，LilyPond 结构错误 14→10。\n– 语义/环境类错误**几乎不减**：LaTeX UndefinedError 28→23，Asymptote VariableError 15→11，说明需要**语法感知预训练**或**外部编译器插件**才能进一步解决。\n\n- **三轮修正曲线**（附录 D 表 11–14）\n给出每轮累计执行率，**首轮修复贡献 60–70 %，次轮 20 %，三轮边际收益 <5 %**，为实际部署提供“停轮”依据。\n\n4\\. 数据消融实验\n\n**目的**：量化“679 K 数据里，天然代码、合成代码、领域 SVG、多轮对话各自贡献多少”。\n\n- **设置**\n以 Qwen2.5-Coder-7B 为固定骨架，分别仅用 1) the-stack-v2 246 K、2) CoSyn 323 K、3) StarVector 44 K、4) Code-Feedback 66 K、5) 全量 679 K 进行同超参微调。\n\n- **结果**（表 6）\n– 单用 the-stack：整体 49 %，**LaTeX 跌至 0.9 %**——天然代码稀疏，可视化信号弱。\n– 单用 CoSyn：LilyPond 65 %、Asymptote 57 %，验证**合成数据对符号语法结构覆盖价值最大**。\n– 单用 Code-Feedback：默认 55 % 虽不突出，但**自调试后 +8 %**，证明“多轮对话”主要提升**修复能力**而非一次生成。\n– 全量融合：**70.9 % → 76.4 %**，**任意子集无法同时兼顾“一次能对”与“错了能改”**，证实多源数据**协同必要性**。\n\n5\\. 案例可视化（附录 F）\n\n**24 组成功/失败/自修复对比图**，覆盖 8 种语言，直观展示：\n\n- **成功**：代码一次跑通，视觉与 Ground Truth 一致。\n- **自修复**：首轮报错（语法/参数/网络 404），二或三轮后输出正确图。\n- **失败三轮仍错**：多为“库缺失”“语法结构深层缺陷”，需外部工具链或更强语法感知。\n\n实验结论一览\n\n1. VisCoder2 在所有参数规模上**稳定超越同规模开源基线**，32B 与 GPT-4.1 打平。\n2. 自调试带来**一致且显著的提升**，符号语言受益最大，**提升幅度与语言编译复杂度正相关**。\n3. 数据消融证实：**合成结构 + 天然用法 + 多轮纠错** 三者缺一不可，**共同构成可靠多语言可视化代码智能体的数据底座**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下展望按“数据–模型–评测–系统”四个层次归纳，均直接对应论文已暴露的瓶颈或空白，可作为后续工作切入点。\n\n1\\. 数据层：低资源符号语言的持续扩充\n\n- **极端低资源语言**：Asymptote、LilyPond 在 679 K 中仅占 22 K/12 K，样本浓度不足导致长尾子类型（如 Asymptote 3D 曲面、LilyPond 多声部合唱）仍频繁失败。\n→ 探索 **语法引导的合成数据生成**：用形式文法/编译器前端生成千万级“语法树→代码→渲染图”三元组，再经执行过滤，低成本扩充低资源语言。\n- **跨语言一致语义对齐**：同一图表（如箱线图）在 Python、LaTeX、Asymptote 中的变量命名、坐标系约定差异大，模型难以共享知识。\n→ 构建 **“多语言同图”平行语料**：自动把 Python 可视化代码转写成等效 LaTeX/Asymptote，再人工校验，形成 10–20 K 高质量平行对，用于对比学习或约束解码。\n\n2\\. 模型层：语法感知与工具增强\n\n- **符号语法注入预训练**：\n– 在 tokenizer 层为 LaTeX、LilyPond、Asymptote 引入 **语法感知子词切分**（如把 `[...]` 作为单一 token），减少结构错误。\n– 继续预训练阶段加入 **编译器返回的抽象语法树（AST）或字节码** 作为辅助任务，让模型直接优化“可编译性”目标。\n- **外部编译器即服务**：\n– 将 pdflatex、lilypond、asy 封装成 **沙盒化 REST API**，推理时模型可调用 **语法检查、错误定位、符号补全** 三种工具，实现 **工具增强可视化代码生成**（类似 Copilot+Interpreter 模式）。\n- **多模态视觉反馈循环**：\n– 当前自调试仅利用 **文本报错**，后续可把 **渲染图差异**（像素级 diff 或 CLIP 视觉特征）作为下一轮条件，实现 **像素级自我修正**。\n\n3\\. 评测层：扩展语言、场景与交互维度\n\n- **语言扩展**：\n– 新增 R ggplot2、Matplotlib（C++）、Plotly.js（React）、D3.js、Graphviz DOT、PSTricks、Metapost 等工业界常用框架，构建 **15–20 语言超集基准**。\n- **任务场景升级**：\n– **多图组合报告**：一次指令生成含 4–6 子图的完整数据分析报告，考察跨图语义一致性、编号引用、子图对齐等复杂约束。\n– **交互与动画**：Vega-Lite、D3、Plotly 支持的滑块、下拉菜单、过渡动画目前为零覆盖，需构建 **交互式可视化代码生成基准**。\n- **人机协同修正评测**：\n– 引入 **“人类轻量反馈”** 模拟：仅指出“图例颜色错”或“轴标签缺失”一句话，模型需在 1 轮内定位并修复，评估 **指令最小化场景下的修复效率**。\n\n4\\. 系统层：高效推理与可信部署\n\n- **增量编译与热更新**：\n– 对 LaTeX/Asymptote 实现 **增量编译缓存**，仅重编译改动片段，把 3 轮自调试总耗时从 30 s 降至 5 s，满足实时协作需求。\n- **可验证安全性**：\n– 可视化代码常涉及 `os.system` 或 `shell-escape` 编译开关，需引入 **静态污点分析** 与 **沙盒渲染** 相结合，确保生成代码无网络访问、文件系统越权等风险。\n- **边缘端轻量化**：\n– 探索 **8B–1B 小型模型 + 符号语法蒸馏** 方案，在笔记本/平板本地完成 80 % 常见 Python/VEGA 图表生成，降低云端依赖。\n\n5\\. 前沿交叉方向\n\n- **自然语言→可视化→反推数据**：\n给定最终图像，让模型反推 **CSV 或 DataFrame** 内容，实现 **“图→数”逆向工程**，用于数据重建、图表去伪真。\n- **可视化代码的可解释性**：\n生成同时输出 **“自然语言解释 + 代码”**，说明每一步绘图决策（为何用箱线而非直方、为何取对数坐标），提升科研报告可审计性。\n- **多语言一致性验证**：\n利用 **形式化方法**（如 Coq 证明 TikZ 与 Python 箱线图统计量一致）构建 **“跨语言等价性”测试用例库**，推动可视化代码生成走向**可证明正确**阶段。\n\n总结\n\n论文已建立“多语言可执行可视化代码”的基线，但**低资源符号语法、像素级自修复、交互式图表、人类轻量反馈、安全与效率**五大前沿仍待突破。后续研究可沿“语法感知预训练 + 工具增强推理 + 多模态视觉反馈 + 可验证安全”四条主线展开，逐步把可视化代码智能体推向**高可靠、多平台、实时协作**的工业级应用。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有 LLM 可视化代码生成受限于单语言、不可执行、无自修复，难以满足多语言真实工作流。\n- **方案**：\n– **VisCode-Multi-679K**：67.9 万条 12 语言可执行代码-图-指令三元组 + 6.6 万轮纠错对话。\n– **VisPlotBench**：8 语言 888 任务可执行基准，支持单轮与三轮自调试评测。\n– **VisCoder2**：基于 Qwen2.5-Coder 3/7/14/32B 微调，首次让开源模型在 32B 规模达到 82.4 % 整体执行率，与 GPT-4.1 持平，并在 LaTeX、LilyPond、Asymptote 等符号语言大幅领先。\n- **实验**：跨模型/语言/尺度对比、错误分类、三轮修复曲线、数据消融、24 组案例验证。\n- **结论**：提出“可执行大数据 + 多轮评测 + 自调试训练”闭环，奠定多语言可视化代码智能体的可靠基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VisCoder2%3A%20Building%20Multi-Language%20Visualization%20Coding%20Agents，链接是https://arxiv.org/pdf/2510.23642，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.23642。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-13-1761781638511","title":"Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance","description":"Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model capacity while preserving computational efficiency. Despite its notable success in large language models (LLMs), existing attempts to apply MoE to Diffusion Transformers (DiTs) have yielded limited gains. We attribute this gap to fundamental differences between language and visual tokens. Language tokens are semantically dense with pronounced inter-token variation, while visual tokens exhibit spatial redundancy and functional heterogeneity, hindering expert specialization in vision MoE. To this end, we present ProMoE, an MoE framework featuring a two-step router with explicit routing guidance that promotes expert specialization. Specifically, this guidance encourages the router to partition image tokens into conditional and unconditional sets via conditional routing according to their functional roles, and refine the assignments of conditional image tokens through prototypical routing with learnable prototypes based on semantic content. Moreover, the similarity-based expert allocation in latent space enabled by prototypical routing offers a natural mechanism for incorporating explicit semantic guidance, and we validate that such guidance is crucial for vision MoE. Building on this, we propose a routing contrastive loss that explicitly enhances the prototypical routing process, promoting intra-expert coherence and inter-expert diversity. Extensive experiments on ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods under both Rectified Flow and DDPM training objectives. Code and models will be made publicly available.","url":"https://huggingface.co/papers/2510.24711","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.511Z","metadata":{"authors":["Yujie Wei","Shiwei Zhang","Hangjie Yuan","Yujin Han","Zhekai Chen","Jiayu Wang","Difan Zou","Xihui Liu","Yingya Zhang","Yu Liu","Hongming Shan"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.24711.pdf","arxivUrl":"https://arxiv.org/abs/2510.24711","arxivId":"2510.24711","coolPaperUrl":"https://papers.cool/arxiv/2510.24711","published":"2025-10-28T17:59:02Z","updated":"2025-10-28T17:59:02.000Z","zh_summary":"专家混合（Mixture-of-Experts, MoE）已成为在保持计算效率的同时扩展模型容量的一种强大范式。尽管在大型语言模型（LLMs）中取得了显著成功，但目前将 MoE 应用于扩散变换器（Diffusion Transformers, DiTs）的尝试效果有限。我们将这一差距归因于语言与视觉令牌之间的根本差异。语言令牌在语义上密集且令牌间差异明显，而视觉令牌则表现出空间冗余和功能异质性，这阻碍了视觉 MoE 中专家的专业化。为此，我们提出了 ProMoE，一种具有两步路由器和显式路由指导的 MoE 框架，以促进专家专化。具体而言，该指导促使路由器根据功能角色通过条件路由将图像令牌划分为条件集合和无条件集合，并通过基于语义内容的可学习原型进行原型路由，优化条件图像令牌的分配。此外，原型路由在潜在空间中基于相似性的专家分配提供了一种自然机制来引入显式语义指导，我们验证了这种指导对视觉 MoE 至关重要。在此基础上，我们提出了一种路由对比损失，显式增强原型路由过程，促进专家内部一致性和专家间多样性。在 ImageNet 基准上进行的大量实验证明，ProMoE 在整流流（Rectified Flow）和 DDPM 训练目标下均超越了最先进的方法。代码和模型将公开提供。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在弥合 Mixture-of-Experts（MoE）在大语言模型（LLM）与扩散 Transformer（DiT）之间的性能差距。尽管 MoE 在 LLM 中已证明可显著扩大模型容量并保持计算效率，但直接将其迁移到 DiT 仅带来微弱提升。作者指出，核心障碍在于**视觉 token 与语言 token 的本质差异**：\n\n- **语言 token** 语义密度高、类间差异大，天然利于专家特化；\n- **视觉 token** 存在**高空间冗余**与**功能异质性**（conditional vs. unconditional），导致专家难以分化、出现同质化。\n\n为此，论文提出 **ProMoE**，通过**显式路由引导**强制实现专家特化：\n\n1. **条件路由**先按功能角色将 token 划分为 unconditional 与 conditional 集合；\n2. **原型路由**再利用可学习原型按语义内容精细分配 conditional token；\n3. 引入**路由对比损失**进一步增大类间差异、压缩类内距离，提升 intra-expert coherence 与 inter-expert diversity。\n\n实验表明，ProMoE 在 ImageNet 256×256 上显著优于同等激活参数量的 Dense-DiT 与现有 MoE 方案，验证了显式语义引导对视觉 MoE 的必要性与有效性。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三条主线：扩散模型、MoE 在视觉生成中的早期尝试，以及近期面向 DiT 的 MoE 改进。按时间顺序与关联度列举如下（避免使用“首先”“其次”）：\n\n- **扩散模型基础**\n- DDPM / DDIM：Ho et al. 2020, Song et al. 2020\n- Latent Diffusion (Stable Diffusion)：Rombach et al. 2022\n- DiT 架构：Peebles & Xie 2023\n- Rectified Flow / Flow-Matching：Lipman et al. 2022, Liu et al. 2022, Esser et al. 2024\n- **U-Net 阶段 MoE 尝试**\n- eDiff-I：Balaji et al. 2022，按去噪区间分专家\n- Raphael：Xue et al. 2023，多路径混合去噪\n- Multi-Architecture Multi-Expert：Lee et al. 2024，网络结构级专家\n- **面向 DiT 的 MoE 近期工作**\n- DiT-MoE：Fei et al. 2024，经典 Token-Choice 路由\n- EC-DiT：Sun et al. 2024，Expert-Choice 固定配额\n- DiffMoE：Shi et al. 2025，全局 token 分布再分配\n- Expert Race：Yuan et al. 2025，双向竞争路由\n- Diff-MoE：Cheng et al. 2025，引入时间-空间自适应专家\n\n这些研究共同特点是**仅依赖隐式学习或启发式规则**，未针对视觉 token 的冗余与功能异质性引入显式语义引导；ProMoE 通过条件+原型两步路由与对比损失，首次在该场景下系统性地解决了专家同质化问题。\n\nQ3: 论文如何解决这个问题？\n\n论文将“视觉 MoE 难以特化”拆解为**空间冗余**与**功能异质**两大障碍，对应提出三项互为补充的技术构件，形成 ProMoE 框架：\n\n1. **条件路由（Conditional Routing）**\n利用扩散模型中 classifier-free guidance 产生的“条件 / 无条件”二元信号，**硬划分** token 集合：\n\n- 无条件 token → 专属 unconditional expert\n- 条件 token → 进入第二步精细分配\n通过功能角色隔离，先阻断两类任务对同一专家参数的干扰。\n2. **原型路由（Prototypical Routing）**\n为每个标准专家维护可学习原型向量  p_j∈R^D ；对条件 token  x_i  计算归一化余弦相似度\n\nZ_(i,j)=α⋅x_i^⊤p_j/(∥x_i∥∥p_j∥)\n\n直接以 Identity 激活得到路由分数，Top-K 选择专家。相似度机制把“语义相近”显式映射到“同一专家”，天然注入内容引导。\n\n3. **路由对比损失（Routing Contrastive Loss, RCL）**\n在线计算各专家分配 token 的质心  m_i ，把原型  p_i  当 query，拉近其对应质心（正样本）、推远其余质心（负样本）：\n\nL_(RCL)=−1/N_a ∑_(i=1)^(N_a) log(exp(sim(p_i,m_i)/τ)) / (∑_(j=1)^(N_a)exp(sim(p_i,m_j)/τ))\n\n该损失**不依赖人工标签**，随训练动态调整原型位置，进一步压缩类内方差、扩大类间距离，实现“专家专用领域”自动成型。\n\n通过“功能隔离→语义聚类→对比精炼”三步，ProMoE 显式地同时优化 intra-expert coherence 与 inter-expert diversity，从而把语言 MoE 的成功要素迁移到视觉生成任务。\n\nQ4: 论文做了哪些实验？\n\n论文在 ImageNet-1K 256×256 上系统验证了 ProMoE 的有效性、扩展性与消融敏感性，实验覆盖训练目标、模型规模、专家数量、组件贡献及超参数。主要结果如下：\n\n- **主实验**\n- 训练目标：Rectified-Flow 与 DDPM 双协议，500K steps\n- 对比基线：Dense-DiT、DiT-MoE、EC-DiT、DiffMoE\n- 指标：FID↓、IS↑，采样 50K 张图，CFG=1.0/1.5\n- 结论：同等激活参数量下，ProMoE-S/B/L/XL 全线优于 Dense 与现有 MoE；ProMoE-L-Flow 仅用 458M 激活、1.06B 总量，FID 相对 DiffMoE-L-Flow 再降 20.5%，IS 提升 14.8%。\n- **扩展性实验**\n\n1. 模型规模：固定 14 专家，自 130M 激活扩至 675M，ProMoE 相对 Dense 的 FID/IS 优势单调放大。\n2. 专家数量：固定 ProMoE-B 骨架，专家数 4→16，性能随专家增加稳步提升，无饱和迹象。\n- **消融实验**\n- 组件：依次加入原型路由、RCL、条件路由，FID 逐步下降 8.7%→10.6%→6.0%。\n- 激活函数：Identity 优于 Sigmoid 优于 Softmax。\n- 条件路由通用性：在 K-Means 路由中移除条件划分，FID 劣化 3.5 点，验证其通用增益。\n- 负载均衡：额外加传统 balancing loss 反而略降性能，说明 RCL 已隐含语义级平衡。\n- RCL 权重 λ∈\n1,10\n：性能对权重不敏感，默认取 1。\n- **训练曲线与更长训练**\n- 相同算力预算下，ProMoE 收敛更快，损失更低；500K 的 ProMoE-L 已全面优于 1M 的 Dense-DiT-L。\n- **可视化**\n- t-SNE 显示 ProMoE 的 token-专家分配形成紧凑、分离的语义簇，而 DiT-MoE 呈散乱分布。\n- 生成样本在复杂类别仍保持高保真与语义一致性。\n\n以上实验共同表明：显式路由引导在视觉 MoE 中不可或缺，且 ProMoE 具备良好的参数效率与规模可扩展性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可延续 ProMoE 的显式路由思路，进一步挖掘潜力或拓展边界：\n\n- **跨模态扩展**\n将条件/无条件划分与原型路由迁移到文本-图像、文本-视频生成，探究“文本 token 语义密度高、视觉 token 冗余大”这一差异下的统一路由策略。\n\n- **层级-时间联合路由**\n扩散过程不同 timestep 的噪声水平对应不同频率信息，可引入“时间原型”与“层深原型”联合决策，实现时空双重特化。\n\n- **动态专家结构**\n当前专家为同构 FFN；可让路由分数同时控制专家宽度、深度或注意力头数，实现结构级稀疏，进一步节省激活参数。\n\n- **无监督语义发现**\n原型向量目前随机初始化，可预训练自监督视觉编码器提供初始聚类中心，或采用在线 EM 更新，减少冷启动阶段的错误分配。\n\n- **Top-K>1 的负载与通信优化**\n实验仅 Top-1 激活；当 K 增大时，All-to-All 通信与显存占用急剧上升，需结合 GPU 拓扑感知调度或专家缓存机制，验证大规模 K 的实际收益。\n\n- **与并行训练框架结合**\n将条件/无条件专家分片到不同设备，配合 ZeRO-3 或 Expert Parallelism，测试十亿级总量、亿级激活下的扩展曲线。\n\n- **更细粒度的功能异构**\n除 CFG 外，扩散模型还有指导尺度 w、采样步长、多尺度特征层等超参，可把这些信号编码为附加路由条件，实现“超参感知”专家。\n\n- **生成质量以外的指标**\n目前仅用 FID/IS；可补充 CLIP-Score、人类偏好、语义一致性、对抗鲁棒性等，检验专家特化是否同时改善感知保真与概念忠实。\n\n- **理论分析**\n从信息论角度量化“冗余度-路由误差-专家多样性”三角关系，给出最优专家数或原型维度的解析边界，指导超参选择。\n\nQ6: 总结一下论文的主要内容\n\n论文核心内容可概括为“一个痛点、一套框架、三项技术、四类实验”：\n\n- **痛点**\nMoE 在 LLM 中效果显著，但在 DiT 上提升有限；根源是视觉 token 空间冗余高、功能异质强，导致专家同质化。\n\n- **框架**\n提出 ProMoE：显式路由引导的稀疏专家扩散 Transformer，保持激活参数量不变即可扩大总容量。\n\n- **三项技术**\n\n1. 条件路由——按 CFG 信号硬划分无条件/条件 token，先隔离功能差异。\n2. 原型路由——用可学习原型向量与 token 余弦相似度计算路由分数，实现语义级分配。\n3. 路由对比损失——在线拉近原型与同簇质心、推离异簇质心，强化类内一致与类间分离。\n- **四类实验**\n- 主对比：ProMoE-S/B/L/XL 在 ImageNet-256 上全面超越 Dense-DiT 与现有 MoE，FID 最多降低 29.4%。\n- 扩展性：随模型规模或专家数量增加，性能单调提升。\n- 消融：逐步添加各组件，验证每一步的独立贡献；Identity 激活、条件路由均关键。\n- 可视化：t-SNE 显示 token 按语义聚类且专家边界清晰，生成样本保真度高。\n\n综上，ProMoE 通过“功能隔离+语义聚类+对比精炼”显式地实现专家特化，为视觉生成场景下的 MoE 扩展提供了可行且高效的解决方案。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Routing%20Matters%20in%20MoE%3A%20Scaling%20Diffusion%20Transformers%20with%20Explicit%20Routing%20Guidance，链接是https://arxiv.org/pdf/2510.24711，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24711。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1761781638512","title":"STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence","description":"Despite rapid progress in Multi-modal Large Language Models and Large Audio-Language Models, existing audio benchmarks largely test semantics that can be recovered from text captions, masking deficits in fine-grained perceptual reasoning. We formalize audio 4D intelligence that is defined as reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six attributes under absolute and relative regimes) with a Holistic Spatio-Temporal Reasoning setting that includes segment reordering for continuous and discrete processes and spatial tasks spanning static localization, multi-source relations, and dynamic trajectories. Our data curation pipeline uses two methods to ensure high-quality samples. For foundational tasks, we use procedurally synthesized and physics-simulated audio. For holistic data, we follow a four-stage process that includes human annotation and final selection based on human performance. Unlike prior benchmarks where caption-only answering reduces accuracy slightly, STAR-Bench induces far larger drops (-31.5\\% temporal, -35.2\\% spatial), evidencing its focus on linguistically hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared with humans and a capability hierarchy: closed-source models are bottlenecked by fine-grained perception, while open-source models lag across perception, knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear path forward for developing future models with a more robust understanding of the physical world.","url":"https://huggingface.co/papers/2510.24693","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.512Z","metadata":{"authors":["Zihan Liu","Zhikang Niu","Qiuyang Xiao","Zhisheng Zheng","Ruoqi Yuan","Yuhang Zang","Yuhang Cao","Xiaoyi Dong","Jianze Liang","Xie Chen","Leilei Sun","Dahua Lin","Jiaqi Wang"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.SD","cs.CL","eess.AS"],"pdfUrl":"https://arxiv.org/pdf/2510.24693.pdf","arxivUrl":"https://arxiv.org/abs/2510.24693","arxivId":"2510.24693","coolPaperUrl":"https://papers.cool/arxiv/2510.24693","published":"2025-10-28T17:50:34Z","updated":"2025-10-28T17:50:34.000Z","zh_summary":"尽管多模态大语言模型和大型音频-语言模型取得了快速进展，但现有的音频基准测试主要考察可以从文本描述中恢复的语义，掩盖了在细粒度感知推理方面的不足。我们形式化了音频四维智能（audio 4D intelligence）的概念，定义为对时间和三维空间中声音动态的推理，并提出了 STAR-Bench 来衡量这种智能。STAR-Bench 结合了基础声学感知设置（绝对和相对情境下的六个属性）与整体时空推理设置，后者包括连续和离散过程的片段重排序任务，以及涵盖静态定位、多源关系和动态轨迹的空间任务。我们的数据策划流程采用两种方法以确保样本高质量。在基础任务中，我们使用程序合成和物理模拟生成的音频。对于整体数据，我们遵循四阶段流程，包括人工标注和基于人类表现的最终筛选。与先前依赖仅文字描述回答略微降低准确率的基准不同，STAR-Bench 带来了更大的性能下降（-31.5% 时间维度，-35.2% 空间维度），证明其关注的是语言难以描述的线索。对 19 个模型的评估显示，与人类相比存在显著差距，并且体现了能力层次：闭源模型在细粒度感知方面受限，而开源模型在感知、知识和推理方面均落后。我们的 STAR-Bench 提供了关键见解，并为开发未来对物理世界有更稳健理解的模型指明了清晰路径。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有音频基准测试对“可文本化语义”过度依赖、无法衡量模型在**细粒度、难以用语言描述的听觉线索**上的推理能力这一核心缺陷。具体而言，它聚焦以下问题：\n\n1. 现有音频 benchmark 主要评估的是**能被文本 caption 几乎无损还原的粗粒度语义**，导致模型在仅凭 caption 答题时性能下降很小（仅 5.9%–9.0%），掩盖了其在真实听觉智能上的不足。\n2. 人类听觉系统具备**音频 4D 智能**——在三维空间+时间维度上对声源动态进行深度推理的能力（如凭倒水声判断水位、凭引擎声判断车辆轨迹）。该能力对具身智能至关重要，却缺乏系统评测工具。\n3. 因此，作者提出**STAR-Bench**基准，通过\n- **基础声学感知任务**（定量评测六维属性：音高、响度、时长、方位角、仰角、距离）\n- **整体时空推理任务**（连续/离散过程片段重排序、静态定位、多声源关系、动态轨迹跟踪）\n来探测模型是否具备**细粒度感知、物理世界知识、多步推理**三大核心能力。实验显示，现有模型在 STAR-Bench 上性能骤降（−31.5% 时间、−35.2% 空间），揭示其瓶颈，从而为未来模型提供明确改进方向。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两条主线：\n\n1. **Large Audio-Language Models (LALMs) & Omni-Language Models (OLMs)**\n2. **音频评测基准**。以下按这两条主线梳理，并补充与时空推理相关的视觉/多模态研究，方便快速定位。\n\n1\\. LALMs & OLMs 代表性工作\n\n| 类别 | 模型 | 关键特点 |\n| --- | --- | --- |\n| LALMs | LTU-AS (Gong et al., 2023) | 最早将音频编码器与 LLM 对齐，支持 ASR、AAC 等任务。 |\n| SALMONN (Tang et al., 2024) | 通用“听觉”LLM，双编码器结构，支持语音+非语音。 |\n| Qwen-Audio/-Instruct (Chu et al., 2023; 2024) | 统一大规模音频-文本预训练，支持 30+ 任务。 |\n| Audio Flamingo 2/3 (Ghosh et al., 2025; Goel et al., 2025) | 引入少样本与长音频推理，开源“think”版强化链式推理。 |\n| Step-Audio 2 (Wu et al., 2025) | 支持对话、歌唱、音效生成的一体化音频 LLM。 |\n| MiMo-Audio (Xiaomi, 2025) | 强调 few-shot 音频理解，开源“think”模式。 |\n| BAT (Zheng et al., 2024) | 唯一专门面向空间音频的 LALM，利用 HRTF 进行方位推理。 |\n| OLMs | GPT-4o (Achiam et al., 2023) | 原生多模态，支持音频输入/输出，但细节未公开。 |\n| Gemini 2.5 Pro/Flash (Comanici et al., 2025) | 强推理+多模态，官方音频 API。 |\n| Qwen-2.5-Omni (Xu et al., 2025) | 端到端音频-视觉-语言三模态，开源。 |\n| MiniCPM-O v2.6 (Yao et al., 2024) | 手机端可跑的轻量级 OLM。 |\n| Phi-4-MM (Abouelenin et al., 2025) | MoLoRA 结构，紧凑多模态。 |\n\n2\\. 音频评测基准对比\n\n| 基准 | 任务侧重 | 时空深度 | 多音频 | 备注 |\n| --- | --- | --- | --- | --- |\n| AudioBench (Wang et al., 2024) | ASR、AAC、SpokenQA | ✗ | ✗ | 纯语义级。 |\n| AIR-Bench (Yang et al., 2024) | 生成式问答 | ✗ | ✗ | 仅单音频 caption 推理。 |\n| MMAU (Sakshi et al., 2025) | 30+ 任务大集合 | ✗ | ✗ | caption-only 掉点 <9%，暴露可文本化偏差。 |\n| MMAR (Ma et al., 2025) | 音乐+音效+语音混合推理 | 浅层时序 | ✗ | 仍可用 caption 近似。 |\n| MMAU-Pro (Kumar et al., 2025) | 单音频时序+静态方位 | 部分 | ✗ | 未覆盖多源动态轨迹。 |\n| Dynamic-SUPERB Phase-2 (Huang et al., 2025) | 180 口语任务 | 浅层 | ✗ | 聚焦口语，非环境音。 |\n| STAR-Bench (本文) | 4D 时空推理+六维属性 | ✓ | ✓ | 首个强制多音频、细粒度、物理 grounding 的基准。 |\n\n3\\. 视觉/多模态时空推理（可借鉴思路）\n\n- **V-STAR** (Cheng et al., 2025)\n视频 LLM 时空推理 benchmark，提出 segment reordering 与轨迹跟踪任务，启发了 STAR-Bench 的音频片段重排序设计。\n\n- **EmbodiedBench** (Yang et al., 2025b)\n多模态具身智能基准，强调跨模态时空理解，但音频模态仅作辅助。\n\n- **MMSI-Bench** (Yang et al., 2025c)\n多图像空间智能评测，提出 relation/trajectory 类任务，与 STAR-Bench 的“multi-source spatial relation & dynamic trajectory”对应。\n\n4\\. 数据集与工具链\n\n| 名称 | 用途 | 链接/引用 |\n| --- | --- | --- |\n| Clotho (Drossos et al., 2019) | 音频 caption 语料 | 用于 STAR-Bench 时序任务真实音频来源。 |\n| FSD50K (Fonseca et al., 2022) | 音效标签数据集 | 同上。 |\n| STARSS23 (Shimada et al., 2023) | 空间标注真实录音 | 用于空间任务数据筛选。 |\n| Pyroomacoustics (Scheibler et al., 2018) | 物理声学仿真 | STAR-Bench 基础感知任务合成引擎。 |\n\n一句话总结\n\n现有 LALM/OLM 与 benchmark 主要停留在“能转录或 caption”层面，而 STAR-Bench 首次把评测焦点拉到**多音频、细粒度、物理时空推理**的 4D 听觉智能，填补了标准缺失的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“定义新范式 + 构建新基准 + 设计新协议”三步，系统地把评估焦点从“可文本化语义”推向“4D 听觉智能”。具体做法如下：\n\n1\\. 定义新范式：Audio 4D Intelligence\n\n将人类式听觉能力形式化为在**三维空间 + 时间维度**上对声源动态进行**细粒度感知与物理推理**的统一框架。\n\n- 任何样本必须同时考察三大支柱：\n1. 细粒度感知（Fine-grained Perception）\n2. 物理世界知识（Physics & Common-sense Knowledge）\n3. 多步推理（Multi-step Reasoning）\n- 缺失任一能力即导致答案错误，从而**强制模型依赖难以用语言描述的原始声学线索**，而非仅靠 caption。\n\n2\\. 构建分层基准：STAR-Bench\n\n采用“基础感知 → 整体推理”两级结构，共 2 353 道多选题，全部经**四阶段人工+专家验证**确保可解性与无歧义。\n\n2.1 Foundational Acoustic Perception（951 题）\n\n- **六维属性**：Pitch / Loudness / Duration / Azimuth / Elevation / Distance\n- **双重评估**：\n- Absolute Perception Range：建立模型“听力图”——感知极限与阈值。\n- Relative Discrimination Sensitivity：6 级难度 (∆↑)，量化 JND（Just Noticeable Difference）。\n- **合成方式**：纯音参数化生成 + Pyroomacoustics 物理仿真，保证**厘米/度/毫秒级可控**。\n\n2.2 Holistic Spatio-Temporal Reasoning（1 402 题）\n\n- **Temporal Reasoning（900 题）**\n- 连续过程：Object Spatial Motion（多普勒+反平方律）、In-situ State Evolution（流体、热力学、能量衰减、生物节律）。\n- 离散事件：Tool & Appliance Operation、Daily Scene Scripts、Event-triggered Consequences。\n- 任务形式：Audio Segment Reordering——三片段乱序，模型需凭声学细节恢复唯一时序。\n- **Spatial Reasoning（502 题）**\n- Single-source Static Localization：四象限方位、三档仰角、三档距离。\n- Multi-source Spatial Relation：同时发声，判断“谁在更右/更高/更远”。\n- Dynamic Trajectory Tracking：运动声源左右通道 ITD/ILD 变化，判断“从左到右 or 反之”。\n- 输入策略：\n– Native：直接喂立体声，考察模型能否利用隐式空间线索。\n– Channel-wise：左右通道分开展示并文字标注，降低预处理信息损失。\n\n3\\. 设计新协议：数据治理与鲁棒评测\n\n3.1 四阶段数据管道\n\n1. Taxonomy Construction：专家+Gemini 2.5 Pro 共建层次任务体系。\n2. AI-Assisted Filtering：DeepSeek-V3 → Gemini 2.5 Pro 三级漏斗，去噪并预标注。\n3. Human Annotation：10 名本科生交叉标注 + 3 名专家抽检，共识率不达标即丢弃。\n4. Human Performance Validation：至少 2/3 专家独立答对才能保留，确保“人类可解”。\n\n3.2 鲁棒评估指标\n\n- CircularEval / 多序扰动：每题多次运行，选项顺序或片段顺序随机轮换。\n- 双指标：\n- AA（Average Accuracy）：均值，反映整体水平。\n- ACR（All-Correct Rate）：全对比例，衡量稳定性。\n- 人类基线：随机抽 10% 样本由非标注大学生测试，建立 75.6%（感知）/ 88.0%（时序）/ 73.7%（空间）参考上限。\n\n4\\. 大规模诊断实验：暴露瓶颈\n\n- 19 个模型（16 开源 + 3 闭源）结果显示：\n- 闭源龙头 Gemini 2.5 Pro 仅 49.59% AA，较人类低 30+ pp；开源普遍接近随机。\n- Caption-only 实验：STAR-Bench 使 Gemini 掉点 −31.5%（时序）/ −35.2%（空间），远超 MMAU/MMAR 的 <9%，**直接验证基准真正考察“ linguistically hard-to-describe cues ”**。\n- 错误剖析：开源模型 54–84% 属于感知错误 + 知识缺口；闭源模型主要瓶颈转为“细粒度感知”。\n- 消融研究：给完整音频或全局 caption，闭源模型可升至 99%，开源几乎无提升，揭示其**无法跨片段比较与 grounding** 的结构性缺陷。\n\n5\\. 给出改进路径\n\n1. 增强密集音频 caption 预训练，突破粗粒度事件分类。\n2. 引入多音频对比学习目标，提升跨片段对齐与整合能力。\n3. 抛弃“多通道平均→单声道”惯例，研发**原生立体声编码器**，真正利用 ITD/ILD 进行空间推理。\n\n总结\n\n论文并未提出新的模型，而是通过**范式定义 + 基准构建 + 协议标准化**，把“4D 听觉智能”变成可量化、可诊断、可迭代的研究赛道，迫使未来模型必须同时攻克**细粒度感知、物理知识、多步推理**三大难关，从而推动下一代真正能“听世界”的音频大模型发展。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“STAR-Bench 能否真正暴露模型缺陷”与“缺陷具体落在哪”两大问题，设计了**四类实验**，覆盖 19 个模型、2 353 道题目、超 5 万次独立推理调用。结果均以 AA（Average Accuracy）与 ACR（All-Correct Rate）双指标呈现，并辅以显著性检验与人工错误标注。\n\n1\\. 主实验：19 模型全基准扫描\n\n**目的**：量化当前开源/闭源模型在 4D 听觉智能上的天花板与差距。\n**设置**：\n\n- 任务维度：3 大任务（感知 / 时序 / 空间）× 10 子任务\n- 输入格式：\n– 感知任务：单音频\n– 时序任务：3 片段乱序（多音频）\n– 空间任务：Native stereo vs. Channel-wise ablation\n- 评价：每题 3–8 次扰动运行，取 AA 与 ACR\n\n**核心结果**（Table 2 主表）：\n\n| 模型 | 平均 AA | 相对人类 ↓ |\n| --- | --- | --- |\n| Human | 79.11 % | — |\n| Gemini 2.5 Pro | 49.59 % | −29.5 pp |\n| GPT-4o Audio | 30.97 % | −48.1 pp |\n| 最佳开源 Qwen-2.5-Omni | 28.37 % | −50.7 pp |\n| 随机 baseline | 24.32 % | −54.8 pp |\n\n**结论**：STAR-Bench 成功拉开梯度——闭源模型仍远不及人类，开源群体接近随机。\n\n2\\. Caption-Only 消融：验证“ linguistically hard-to-describe ”假设\n\n**目的**：证明 STAR-Bench 考察的是文本难以表达的细粒度线索，而非传统 benchmark 的“caption 可近似”现象。\n**设置**：\n\n- 用 Gemini 2.5 Pro 为 MMAU、MMAR、STAR-Bench 分别生成详细 caption。\n- 仅将 caption 喂给同一模型答题，记录性能下降幅度 ∆。\n\n**结果**（Figure 1）：\n\n| Benchmark | 音频答题 | caption 答题 | ∆ |\n| --- | --- | --- | --- |\n| MMAU | 88.0 % | 82.1 % | −5.9 % |\n| MMAR | 80.7 % | 71.7 % | −9.0 % |\n| STAR-Bench Temporal | 58.5 % | 27.0 % | −31.5 % |\n| STAR-Bench Spatial | 43.6 % | 8.4 % | −35.2 % |\n\n**结论**：STAR-Bench 使 caption 失效，真正逼迫模型依赖原始声学线索。\n\n3\\. 细粒度感知极限：Audiogram 与 JND 曲线\n\n**目的**：给出模型“听力图”，定位感知瓶颈。\n**设置**：\n\n- 感知任务 6 属性 × 6 难度级，共 36 条阶梯。\n- 同批次人类受试者 10 人作为 baseline。\n- 绘制“难度-准确率”曲线，估算 75 % 阈值作为 JND。\n\n**结果**（Figure 8）：\n\n- Gemini 2.5 Pro 在 **响度差异 4 dB** 处即跌下 75 %，人类可维持到 1 dB。\n- 开源模型普遍 **≥12 dB** 即失控。\n- 音高与时长曲线呈现相同趋势，证实**细粒度感知是闭源模型的首要瓶颈**。\n\n4\\. 时序推理消融：任务简化阶梯\n\n**目的**：判断模型失败到底是因为“听不懂”还是“不会比”。\n**设置**：\n\n- 基线：片段重排序（已报告）。\n- +Global Caption：额外给出一句场景描述。\n- +Uncut Audio：提供完整长音频，只需把 3 片段对照定位即可。\n\n**结果**（Figure 9）：\n\n| 条件 | Gemini 2.5 Pro | Qwen-2.5-Omni | Xiaomi-MiMo |\n| --- | --- | --- | --- |\n| 基线 | 58.5 % | 17.0 % | 18.6 % |\n| +Caption | 76.3 % | 16.4 % | 18.9 % |\n| +Uncut | 99.0 % | 25.3 % | 24.0 % |\n\n**结论**：\n\n- 闭源模型一旦降低“跨片段对齐”难度即可逼近完美，说明**知识+推理能力已具备，缺的是细粒度感知与对齐**。\n- 开源模型几乎不随简化提升，暴露其**无法有效比较、 grounding 多音频**的结构性缺陷。\n\n5\\. 空间推理消融：Native vs. Channel-wise\n\n**目的**：量化“多通道平均→单声道”造成的信息损失。\n**设置**：\n\n- 同一套 502 道空间题，分别用两种输入格式评测。\n- 记录 AA 提升幅度 ∆。\n\n**结果**（Table 6 节选）：\n\n| 模型 | Native | Channel-wise | ∆ |\n| --- | --- | --- | --- |\n| Gemini 2.5 Pro | 43.6 % | 40.8 % | −2.8 pp（已较好） |\n| Qwen-2.5-Omni | 37.3 % | 36.1 % | −1.2 pp |\n| Audio Flamingo 3 | 38.9 % | 44.4 % | +5.5 pp |\n\n**结论**：\n\n- 绝大多数模型 Native 输入即**物理立体声信息被预处理破坏**，Channel-wise 可部分挽回。\n- 即使最优闭源模型也远低于人类 73.7 %，说明**空间音频需原生多通道编码器**。\n\n6\\. 人工错误剖析：200 例失败案例编码\n\n**目的**：给出可行动的改进方向。\n**方法**：\n\n- 均匀采样 Gemini 2.5 Pro、GPT-4o Audio、Qwen-2.5-Omni 各 60–70 例错误。\n- 三位专家独立打标签：Perception / Knowledge / Reasoning / Hallucination / Misalignment …\n\n**结果**（Figure 6）：\n\n- 时序任务：Qwen 54 % Knowledge Gap vs. Gemini 84 % Perception Error——**开源缺知识，闭源缺感知**。\n- 空间任务：所有模型 57 % 以上仍为 Perception Error，且出现 13 % 视觉幻觉（“根据视频中汽车轨迹…”）。\n\n7\\. 可靠性测试：AA vs. ACR 差距\n\n**目的**：衡量模型是否靠“猜主流选项”刷分。\n**结果**（Sec. E.1）：\n\n- Gemini 2.5 Pro 平均 AA-ACR 差距 25.0 pp；开源群体 ACR≈0，**输出极不稳定**。\n- 说明 STAR-Bench 能有效戳破“虚假高分”，逼迫模型获得**一致且可解释**的听觉推理能力。\n\n实验一览表\n\n| 实验 | 关键变量 | 主要结论 |\n| --- | --- | --- |\n| 主评测 | 19 模型 × 3 任务 | 人类 79 % → 最佳模型 50 %，梯度清晰 |\n| Caption-only | 输入模态（音频 vs. 文本） | STAR-Bench 掉点 −31 %/−35 %，远超旧基准 |\n| Audiogram | 6 属性 × 6 难度 | 模型 JND 远高于人类，感知瓶颈实锤 |\n| 时序消融 | 片段→+Caption→+Uncut | 闭源可近 100 %，开源仍 ≤25 %，缺跨段对齐 |\n| 空间消融 | Native vs. Channel-wise | 多通道平均造成信息损失，需原生立体声编码 |\n| 错误剖析 | 200 例人工编码 | 开源缺知识，闭源缺感知；空间任务幻觉频发 |\n| 可靠性 | AA vs. ACR | 开源 ACR≈0，暴露答案高度不稳定 |\n\n综上，论文通过**多维度、多扰动、多消融**的实验矩阵，不仅坐实了“现有模型在 4D 听觉智能上远未达标”的论断，更精确指出**细粒度感知与多音频对齐**是下一代模型必须攻克的两大核心技术。\n\nQ5: 有什么可以进一步探索的点？\n\n以下展望按“数据-任务-模型-评测”四条线归纳，均为 STAR-Bench 直接暴露出的空白，可立即开展后续研究。\n\n1\\. 数据与信号层面\n\n| 方向 | 关键问题 | 可探索方案 |\n| --- | --- | --- |\n| 1.1 隐式环境物理标注 | 真实录音缺乏精确 3D 坐标、材料反射系数 → 限制复杂场景推理 | 结合神经声场 (Neural Acoustic Field) 与视觉 SfM，自动反演房间几何、声源轨迹，构建“百万级真实 4D 标注”数据集。 |\n| 1.2 多模态 4D 对齐 | 仅有音频难以验证事件因果，需视觉/IMU 交叉验证 | 同步采集 360° 视频+双耳音频+IMU，构建 Audio-Visual 4D 因果对，研究跨模态时序对齐与互补推理。 |\n| 1.3 动态 HRTF 个性化 | 现有空间音频仿真用固定 HRTF，忽略人头自运动与个体差异 | 引入可学习 HRTF 插值网络，支持在线个性化；同时生成“头部旋转-声源移动”联合仿真，扩充动态轨迹数据。 |\n\n2\\. 任务与范式层面\n\n| 方向 | 关键问题 | 可探索方案 |\n| --- | --- | --- |\n| 2.1 因果推理显式化 | STAR-Bench 仅要求排序，未强制模型给出“为什么” | 设计 Audio Chain-of-Thought 数据集，要求模型输出声学证据 → 物理定律 → 结论的三段式解释，可监督微调或 RLHF。 |\n| 2.2 反事实空间问答 | 当前任务均为“发生了什么”，缺乏“如果…会怎样” | 构建 Counterfactual Spatial QA：“若声源速度×2，到达时间差多少？”需模型内部建立物理模拟器或神经微分方程。 |\n| 2.3 多智能体听觉博弈 | 单听者设定限制更复杂的社交/竞争场景 | 引入“听众-说话者-干扰者”三方博弈：听众需根据移动声源与遮挡物推断谁在说、说了什么，考验动态选择注意力与语音分离。 |\n\n3\\. 模型与架构层面\n\n| 基准 | 任务侧重 | 时空深度 | 多音频 | 备注 |\n| --- | --- | --- | --- | --- |\n| AudioBench (Wang et al., 2024) | ASR、AAC、SpokenQA | ✗ | ✗ | 纯语义级。 |\n| AIR-Bench (Yang et al., 2024) | 生成式问答 | ✗ | ✗ | 仅单音频 caption 推理。 |\n| MMAU (Sakshi et al., 2025) | 30+ 任务大集合 | ✗ | ✗ | caption-only 掉点 <9%，暴露可文本化偏差。 |\n| MMAR (Ma et al., 2025) | 音乐+音效+语音混合推理 | 浅层时序 | ✗ | 仍可用 caption 近似。 |\n| MMAU-Pro (Kumar et al., 2025) | 单音频时序+静态方位 | 部分 | ✗ | 未覆盖多源动态轨迹。 |\n| Dynamic-SUPERB Phase-2 (Huang et al., 2025) | 180 口语任务 | 浅层 | ✗ | 聚焦口语，非环境音。 |\n| STAR-Bench (本文) | 4D 时空推理+六维属性 | ✓ | ✓ | 首个强制多音频、细粒度、物理 grounding 的基准。 |\n\n0\n\n4\\. 评测与鲁棒性层面\n\n| 基准 | 任务侧重 | 时空深度 | 多音频 | 备注 |\n| --- | --- | --- | --- | --- |\n| AudioBench (Wang et al., 2024) | ASR、AAC、SpokenQA | ✗ | ✗ | 纯语义级。 |\n| AIR-Bench (Yang et al., 2024) | 生成式问答 | ✗ | ✗ | 仅单音频 caption 推理。 |\n| MMAU (Sakshi et al., 2025) | 30+ 任务大集合 | ✗ | ✗ | caption-only 掉点 <9%，暴露可文本化偏差。 |\n| MMAR (Ma et al., 2025) | 音乐+音效+语音混合推理 | 浅层时序 | ✗ | 仍可用 caption 近似。 |\n| MMAU-Pro (Kumar et al., 2025) | 单音频时序+静态方位 | 部分 | ✗ | 未覆盖多源动态轨迹。 |\n| Dynamic-SUPERB Phase-2 (Huang et al., 2025) | 180 口语任务 | 浅层 | ✗ | 聚焦口语，非环境音。 |\n| STAR-Bench (本文) | 4D 时空推理+六维属性 | ✓ | ✓ | 首个强制多音频、细粒度、物理 grounding 的基准。 |\n\n1\n\n- 前端探针：检测第 k 层是否仍保留 ITD 信息（用线性探针预测方位）。\n- 后端探针：检测 LLM 隐态是否编码物理量（速度、距离）。\n结合探针准确率与下游任务掉点，可精确定位瓶颈层。 |\n\n5\\. 应用与系统层面\n\n| 基准 | 任务侧重 | 时空深度 | 多音频 | 备注 |\n| --- | --- | --- | --- | --- |\n| AudioBench (Wang et al., 2024) | ASR、AAC、SpokenQA | ✗ | ✗ | 纯语义级。 |\n| AIR-Bench (Yang et al., 2024) | 生成式问答 | ✗ | ✗ | 仅单音频 caption 推理。 |\n| MMAU (Sakshi et al., 2025) | 30+ 任务大集合 | ✗ | ✗ | caption-only 掉点 <9%，暴露可文本化偏差。 |\n| MMAR (Ma et al., 2025) | 音乐+音效+语音混合推理 | 浅层时序 | ✗ | 仍可用 caption 近似。 |\n| MMAU-Pro (Kumar et al., 2025) | 单音频时序+静态方位 | 部分 | ✗ | 未覆盖多源动态轨迹。 |\n| Dynamic-SUPERB Phase-2 (Huang et al., 2025) | 180 口语任务 | 浅层 | ✗ | 聚焦口语，非环境音。 |\n| STAR-Bench (本文) | 4D 时空推理+六维属性 | ✓ | ✓ | 首个强制多音频、细粒度、物理 grounding 的基准。 |\n\n2\n\n快速起步建议（3 个月可落地）\n\n1. 基于 Pyroomacoustics 生成 100k 带物理参数的双耳数据，训练 **Disentangled Binaural Encoder**，在 STAR-Bench 空间任务验证原生立体声是否优于单声道。\n2. 选取 STAR-Bench 时序数据，人工标注 Chain-of-Thought 解释 5k 条，微调 Qwen-2.5-Omni，对比“解释 vs. 不解释”在 AA/ACR 上的增益，初步验证显式物理推理的有效性。\n3. 对 Gemini-2.5 Pro 实施白盒探针：在前端每层后接线性方位分类器，绘制“层-方位准确率”曲线，定位 ITD 信息最早丢失点，为后续架构改进提供证据。\n\n综上，STAR-Bench 不仅是一个评测终点，更是**多音频 4D 听觉智能研究的起跑线**；从数据标注、物理建模、架构设计到交互评测，每一步都留有大量可立即深挖的开放问题。\n\nQ6: 总结一下论文的主要内容\n\n论文核心贡献可概括为“一个范式、一个基准、三大发现、四条路径”。\n\n1\\. 提出新范式：Audio 4D Intelligence\n\n- 定义：在三维空间 + 时间维度上对声源动态进行**细粒度感知与物理推理**的听觉智能。\n- 要求：每任务必须同时考察\n① 细粒度感知\n② 物理世界知识\n③ 多步推理\n缺失任一即答错，迫使模型依赖**难以文本化的声学线索**。\n\n2\\. 构建新基准：STAR-Bench\n\n- 规模：2 353 道多选题，全部经**四阶段人工+专家验证**可解。\n- 两级任务\n– **Foundational Acoustic Perception**（951 题）\n‑ 六维属性：Pitch / Loudness / Duration / Azimuth / Elevation / Distance\n‑ 双重评估：Absolute 感知极限 + Relative JND 阶梯\n– **Holistic Spatio-Temporal Reasoning**（1 402 题）\n‑ 时序：片段重排序，覆盖连续过程与离散事件脚本\n‑ 空间：单源定位 → 多源关系 → 动态轨迹跟踪\n- 输入策略：Native 立体声 vs. Channel-wise 双通道分离，量化预处理信息损失。\n\n3\\. 三大发现\n\n| 基准 | 任务侧重 | 时空深度 | 多音频 | 备注 |\n| --- | --- | --- | --- | --- |\n| AudioBench (Wang et al., 2024) | ASR、AAC、SpokenQA | ✗ | ✗ | 纯语义级。 |\n| AIR-Bench (Yang et al., 2024) | 生成式问答 | ✗ | ✗ | 仅单音频 caption 推理。 |\n| MMAU (Sakshi et al., 2025) | 30+ 任务大集合 | ✗ | ✗ | caption-only 掉点 <9%，暴露可文本化偏差。 |\n| MMAR (Ma et al., 2025) | 音乐+音效+语音混合推理 | 浅层时序 | ✗ | 仍可用 caption 近似。 |\n| MMAU-Pro (Kumar et al., 2025) | 单音频时序+静态方位 | 部分 | ✗ | 未覆盖多源动态轨迹。 |\n| Dynamic-SUPERB Phase-2 (Huang et al., 2025) | 180 口语任务 | 浅层 | ✗ | 聚焦口语，非环境音。 |\n| STAR-Bench (本文) | 4D 时空推理+六维属性 | ✓ | ✓ | 首个强制多音频、细粒度、物理 grounding 的基准。 |\n\n3\n\n4\\. 四条改进路径\n\n1. 增强密集音频 caption 预训练，突破粗粒度事件分类。\n2. 引入多音频对比学习，解决跨片段对齐与整合难题。\n3. 抛弃“立体声→单声道”平均，研发**原生多通道编码器**以利用 ITD/ILD。\n4. 结合物理世界模型或神经声场，赋予模型显式波动定律与空间模拟能力。\n\n一句话总结：STAR-Bench 首次把音频评测从“能转录”推向“能听世界”，用 4D 时空推理任务系统暴露模型在**细粒度感知、物理知识、多步推理**上的巨大缺口，为下一代音频大模型指明攻坚路线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是STAR-Bench%3A%20Probing%20Deep%20Spatio-Temporal%20Reasoning%20as%20Audio%204D%20Intelligence，链接是https://arxiv.org/pdf/2510.24693，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24693。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1761781638512","title":"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs","description":"While Multimodal Large Language Models (MLLMs) excel at visual understanding, they often struggle in complex scenarios that require visual planning and imagination. Inspired by how humans use sketching as a form of visual thinking to develop and communicate ideas, we introduce Latent Sketchpad, a framework that equips MLLMs with an internal visual scratchpad. The internal visual representations of MLLMs have traditionally been confined to perceptual understanding. We repurpose them to support generative visual thought without compromising reasoning ability. Building on frontier MLLMs, our approach integrates visual generation directly into their native autoregressive reasoning process. It allows the model to interleave textual reasoning with the generation of visual latents. These latents guide the internal thought process and can be translated into sketch images for interpretability. To realize this, we introduce two components: a Context-Aware Vision Head autoregressively produces visual representations, and a pretrained Sketch Decoder renders these into human-interpretable images. We evaluate the framework on our new dataset MazePlanning. Experiments across various MLLMs show that Latent Sketchpad delivers comparable or even superior reasoning performance to their backbone. It further generalizes across distinct frontier MLLMs, including Gemma3 and Qwen2.5-VL. By extending model's textual reasoning to visual thinking, our framework opens new opportunities for richer human-computer interaction and broader applications. More details and resources are available on our project page: https://latent-sketchpad.github.io/.","url":"https://huggingface.co/papers/2510.24514","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.512Z","metadata":{"authors":["Huanyu Zhang","Wenshan Wu","Chengzu Li","Ning Shang","Yan Xia","Yangyu Huang","Yifan Zhang","Li Dong","Zhang Zhang","Liang Wang","Tieniu Tan","Furu Wei"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.CV","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.24514.pdf","arxivUrl":"https://arxiv.org/abs/2510.24514","arxivId":"2510.24514","coolPaperUrl":"https://papers.cool/arxiv/2510.24514","published":"2025-10-28T15:26:20Z","updated":"2025-10-28T15:26:20.000Z","zh_summary":"虽然多模态大型语言模型（MLLMs）在视觉理解方面表现出色，但在需要视觉规划和想象的复杂场景中，它们通常会遇到困难。受到人类使用速写作为视觉思维工具来发展和传达想法的启发，我们引入了 Latent Sketchpad，这一框架为 MLLMs 提供了一个内部视觉草稿板。MLLMs 的内部视觉表示传统上仅限于感知理解。我们重新利用这些表示来支持生成式视觉思维，同时不损害推理能力。在前沿 MLLMs 的基础上，我们的方法将视觉生成直接整合到其本地自回归推理过程中，使模型能够在文本推理与视觉潜表示生成之间交替进行。这些潜表示引导内部思维过程，并可以转换为速写图像以提高可解释性。为了实现这一目标，我们引入了两个组件：一个上下文感知视觉头（Context-Aware Vision Head）自回归地生成视觉表示，以及一个预训练的速写解码器（Sketch Decoder），将其渲染为人类可理解的图像。我们在新的数据集 MazePlanning 上评估了该框架。在对各种 MLLMs 的实验中，Latent Sketchpad 展示出与其骨干模型相当甚至更优的推理性能。它还能够在不同的前沿 MLLMs（包括 Gemma3 和 Qwen2.5-VL）间进行泛化。通过将模型的文本推理扩展到视觉思维，我们的框架为更丰富的人机交互和更广泛的应用开辟了新机会。更多详情和资源请访问我们的项目主页：https://latent-sketchpad.github.io/。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决多模态大语言模型（MLLM）在**复杂空间推理与动态视觉定位**任务中表现不足的问题。具体而言：\n\n- **核心痛点**：现有 MLLM 虽在视觉理解上表现优异，但在需要“视觉想象”与“空间规划”的场景（如迷宫导航）中，仅依赖文本链式思维（CoT）难以精确跟踪和更新空间状态。\n- **关键观察**：人类面对类似任务时会借助“心理草图”辅助思考，而 MLLM 缺乏内部视觉草稿本，无法边推理边生成可视化中间态。\n- **研究目标**：在不破坏原模型感知与推理能力的前提下，**将预训练视觉特征重新用作生成式“隐草图”**，使 MLLM 能够在自回归推理过程中**交替输出文本与视觉隐变量**，从而把“视觉思维”无缝融入推理链，并通过可解释草图供人检视。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大主线，每类均与 Latent Sketchpad 存在显式对比或继承关系：\n\n1. 多模态推理增强\n\n- 工具辅助：Deepeyes、Pixel Reasoner、Refocus 等借助外部检测/编辑工具在像素空间完成中间视觉操作，依赖预定义 API，扩展性受限。\n- 原生视觉思维：MVoT、Visual Planning 提出“visual chain-of-thought”，但采用统一离散 token 自回归模型，目标偏向“像素级逼真”而非“推理友好”的抽象草图，且需从头训练。\n2. 隐空间推理（Latent Reasoning）\n\n- 文本侧：Universal Transformer、Recurrent Depth、Continuous Concepts 等把中间推理隐于连续隐状态，避免显式 token。\n- 多模态侧：Machine Mental Imagery 引入一次性 latent visual token 作为答案图像，未实现“推理过程中持续生成多步视觉隐变量”。\n3. 统一多模态生成\n\n- Chameleon、Emu3、Janus-Pro、Anole 等支持图文交错输出，但模型需大规模从头预训练，且图像分支以逼真生成为主，不强调“轻量级插件式”视觉草图。\n- MetaMorph 提出 VPiT，让冻结 LLM 输出连续视觉 token，然而聚焦最终图像生成，未探索“逐步视觉思维”对复杂推理的增益。\n\nLatent Sketchpad 与上述工作的区别：\n\n- 无需外部工具或整体重训，仅通过**插件式 Vision Head** 把已有 MLLM 的预训练视觉特征转化为可自回归生成的隐草图。\n- 引入**独立 Sketch Decoder**，将隐变量映射为可解释草图，兼顾“推理友好抽象”与“人类可读”。\n- 在迷宫规划任务上首次验证：冻结骨干、仅训视觉头即可让 Gemma3、Qwen2.5-VL、GPT-4o 等不同架构一致获得空间推理增益。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 Latent Sketchpad 框架，通过两项关键设计把“视觉想象”无缝注入 pretrained MLLM 的自回归推理循环，具体实现如下：\n\n1. Context-Aware Vision Head（可插拔视觉头）\n\n- 冻结原 MLLM 全部参数，仅新增 2 层交叉注意力 + 8 层自注意力的小型模块。\n- 在每一步生成时，以当前隐藏状态为 Query，**全局历史视觉隐变量**（global context）与**同图已生成局部隐变量**（local context）为 Key/Value，执行因果交叉注意力，再经自注意力保持内部一致性。\n- 输出维度与 vision encoder latent 对齐，用 L1 回归损失监督：\n\nL_(reg)=|l^*_(X_k)-l_(X_k)|_1\n\n其中  l_(X_k)  为预训练视觉编码器对目标草图提取的 ground-truth latent。\n- 训练仅需 MAZEPLANNING 47.8 k 样本，5 epoch 内收敛，骨干能力零损伤。\n2. Pretrained Sketch Decoder（独立解码器）\n\n- 与 MLLM 完全解耦，仅用于可视化。\n- AlignerNet（12 层 encoder + 12 层 decoder）将视觉隐变量投影至 SDXL-VAE latent 空间，再用冻结 VAE-Decoder 生成草图。\n- 损失三合一：\n- 像素级 Focal 重建（强调前景）\n- 隐空间 NLL 对齐\n- 块嵌入 MSE\n公式：\n\nL=L_(rec)+L_(latent)+L_(emb)\n\n- 预训练数据用 Quick, Draw! 5 千万草图，零样本即可适配 OpenCLIP、SigLIP、Qwen2.5-VL 等不同 ViT 编码器。\n3. 推理流程\n文本 token ↔ 特殊 token `<start_of_image>` ↔ Vision Head 自回归生成  n_v=256  个视觉隐变量 ↔ `<end_of_image>` ↔ 继续文本生成……\n生成的隐变量可实时送入 Sketch Decoder 得到人类可读草图，用于调试或人机交互。\n\n4. 训练与推断策略\n\n- 统一微调：同一份交错图文轨迹，训练时以 0.5 概率随机 mask 后续图像，使同一 checkpoint 既能纯文本 CoT 也能图文 CoT。\n- 数据增强：对输入草图进行 k 次“编码-解码-再编码”循环，引入外观扰动但保留结构，提升视觉稳健性。\n- 零改动接入 GPT-4o：仅训 Vision Head，推理时把 `<actions>` 后内容截断，用 Sketch Decoder 生成新状态图再喂回 GPT-4o，实现迭代视觉反馈。\n\n通过上述设计，Latent Sketchpad 在不触碰原模型权重的前提下，赋予 MLLM“边想边画”的能力，显著提升了复杂空间推理任务的成功率与可解释性。\n\nQ4: 论文做了哪些实验？\n\n论文围绕新提出的 MAZEPLANNING 基准与 Latent Sketchpad 框架，共执行了 6 组实验，覆盖定量指标、可视化质量、分布外泛化、消融分析、不同规模迷宫灵敏度，以及跨模型兼容性测试。\n\n1. 主实验：MAZEPLANNING 任务性能\n\n- 对比对象：GPT-4o、o1、o4-mini、o3-pro（含工具）、Gemma3-12B、Qwen2.5-VL-7B，以及各自 +Latent Sketchpad（+LS）版本。\n- 指标：Success Rate（SR）与 Progress Rate（PR）。\n- 结果：\n- 专有模型平均 SR ≤ 20%，PR ≤ 50%；+LS 的 GPT-4o 绝对提升 SR +3.8%、PR +9.1%。\n- Gemma3+LS 在 5×5 困难子集上 SR 从 46.5%→48.0%，整体 70%→72.2%；Qwen2.5-VL+LS 亦有 +0.4% SR 与 +0.39% PR 的稳步增益，验证插件不损伤原能力。\n2. 可视化质量评估\n\n- 引入 Layout Consistency Rate（LCR）与 Visual Success Rate（VSR）。\n- Gemma3+LS 的 LCR 达 99.3%，VSR 75.6%，高于其文本 SR（70%），说明生成的草图本身即可成功导航，直接体现视觉思维对推理的增益。\n3. 分布外（OOD）泛化\n\n- 在 200 个 6×6 更大迷宫测试。\n- Gemma3 基线 SR 8.0%→+LS 10.0%，PR 38.8%→39.4%；Qwen2.5-VL 因视觉 token 维度高、训练数据少，提升不显著，揭示数据与架构对泛化的影响。\n4. 消融实验（Gemma3 控制变量）\n\n- 冻结 connector：SR 跌至 9.4%，方向感混乱。\n- 去掉数据增强：SR 54.2%→68.2%，VSR 同步下降。\n- 改用 cosine 回归损失：SR 71.4%，低于 L1 的 72.2%，验证 L1 更能保持空间 fidelity。\n5. 迷宫尺寸灵敏度\n\n- 3×4 到 5×5 逐级下降：Gemma3+LS 始终保持最高 SR，但在 5×5 从 85% 降至 35%，表明空间复杂度仍是瓶颈。\n6. Sketch Decoder 通用性验证\n\n- 零样本重建 MAZEPLANNING 测试集，OpenCLIP、Qwen2.5-VL、Gemma3 编码器平均 SSIM 均 ≥ 0.92，定性线条结构高保真，证明解码器与各类 ViT 兼容。\n\n综上，实验系统论证了 Latent Sketchpad 的“即插即用”安全性、跨模型通用性、可视化可解释性，以及在复杂空间推理任务上的实质增益。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“任务拓展”“技术深化”“评测与理论”三大类，均直接对应论文已暴露的局限或未触及的空白。\n\n1 任务拓展\n\n- **三维空间与视觉导航**\n将 2D 迷宫升维至 3D 场景（室内、无人机城市场景），引入高度、遮挡、动态障碍物，验证隐草图对深度估计与垂直避障的辅助作用。\n\n- **时序动态与多智能体**\n引入移动目标或其他智能体，要求模型在每一步更新“多实体位置张量”，考察隐草图对动态状态跟踪与博弈策略的增益。\n\n- **跨模态逆向设计**\n从目标图像反向推理“如何组装/调色/布局”——例如 LEGO 说明书生成、GUI 自动化脚本，测试隐草图在“视觉→动作”逆向规划中的可迁移性。\n\n2 技术深化\n\n- **视觉隐变量稀疏化与压缩**\n当前用 256 个连续 token 表示一帧；可探索 VQ-VAE 或稀疏 Transformer，将隐变量压缩至 <32 个离散码本，降低自回归长度，提高大图像/长视频的推理效率。\n\n- **递归式视觉记忆机制**\n仅保留最近 k 帧隐变量作为全局上下文，引入可学习的“视觉记忆槽”或压缩-回放策略，缓解长序列视觉上下文线性增长的内存开销。\n\n- **多尺度草图金字塔**\n先生成低分辨率草图确认全局路径，再逐级上采样细化局部细节，实现“先规划后聚焦”的 coarse-to-fine 视觉思维，与人类素描层次更一致。\n\n- **可微分渲染反传梯度至 LLM**\n将 Sketch Decoder 换成可微分矢量渲染器（SVG、线条参数化），使像素级损失可直接回传至 Vision Head，实现端到端微调而无需冻结骨干。\n\n- **强化学习奖励塑形**\n以“草图是否连通起点-终点”作为稠密奖励，配合课程学习，让模型在失败轨迹中自我修正草图，缓解 OOD 下布局漂移问题。\n\n3 评测与理论\n\n- **可解释性认知实验**\n让人类受试者仅看生成的中间草图进行迷宫复现或路径预测，量化“草图→人类推理”准确率，反向验证隐变量是否捕获了与人兼容的空间符号。\n\n- **隐空间线性探测**\n在 l\\* 上训练轻量级分类器，预测方向、坐标、墙体位置，检验视觉隐变量是否线性可解码，从而论证其语义完备性。\n\n- **复杂度下界分析**\n从信息论角度推导“完成 N×N 迷宫最少需要多少 bit 的视觉隐变量”，对比当前 256×d 维表示，评估是否存在过度参数化或压缩潜力。\n\n- **跨文化草图风格泛化**\n收集非拉丁文化的手绘迷宫（含不同箭头、拐角符号），验证 Sketch Decoder 对未见符号的零样本重建能力，减少文化先偏。\n\n- **与大脑视觉工作记忆对比**\n采用 fMRI 记录人类在迷宫任务中的视空草稿脑区激活，与模型每步隐变量相似度做相关性分析，探索“隐草图”是否与人脑表征对齐。\n\n4 数据与系统\n\n- **自监督无限生成**\n用程序无限采样迷宫+自动标注最优路径，构建持续学习管线，观察当训练步数→∞ 时视觉隐变量是否出现组合泛化或灾难遗忘。\n\n- **边缘端侧部署**\n将 Vision Head 与 Sketch Decoder 量化至 4-bit，在移动端 SoC 上运行，测量推理延迟与功耗，验证“即插即用”在真实人机交互场景中的实时性。\n\n这些探索点既可直接继承 Latent Sketchpad 的“冻结骨干+插件视觉头”范式，也可反向推动其向更高效、更可解释、更贴近人类认知的方向演进。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Latent Sketchpad**，一个即插即用框架，让现成多模态大语言模型在**不改动预训练权重**的前提下，获得“边推理边生成视觉草图”的能力，从而提升复杂空间推理任务的表现与可解释性。核心内容可概括为四点：\n\n1. 问题与动机\n\n- 现有 MLLM 仅靠文本 CoT 难以完成需要动态空间定位与视觉想象的复杂任务。\n- 人类借助“心理草图”辅助思考，而 MLLM 缺乏内部可视化草稿本。\n2. 方法框架\n\n- **Context-Aware Vision Head**：冻结骨干，新增轻量模块，自回归地生成视觉隐变量；利用全局+局部视觉上下文保持连贯。\n- **Pretrained Sketch Decoder**：独立对齐网络，将隐变量实时渲染为人类可读草图，兼容多种 ViT 编码器。\n- 统一微调策略：同一份交错图文数据，随机 mask 图像，使同一 checkpoint 支持纯文本或图文混合推理。\n3. 实验验证\n\n- 构建 **MAZEPLANNING** 数据集（47.8 k 训练 / 500 ID + 200 OOD 测试），以迷宫导航为场景。\n- 在 Gemma3、Qwen2.5-VL、GPT-4o 上“零损伤”插件化实验：\n– GPT-4o+LS 绝对提升 SR 3.8%、PR 9.1%；\n– Gemma3+LS 整体 SR 70%→72.2%，且生成草图自身 VSR 达 75.6%，直接可完成导航。\n- 消融、OOD、跨尺度、可视化质量、解码器通用性等全面分析，验证模块必要性与泛化能力。\n4. 结论与意义\n\n- 首次证明**预训练视觉特征可被重新用作生成式“隐草图”**，无需外部工具或整体重训即可增强复杂多模态推理。\n- 框架对不同 MLLM 骨干即插即用，生成可解释视觉轨迹，为更丰富的人机交互与视觉思维研究开辟新路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Latent%20Sketchpad%3A%20Sketching%20Visual%20Thoughts%20to%20Elicit%20Multimodal%20Reasoning%20in%20MLLMs，链接是https://arxiv.org/pdf/2510.24514，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24514。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-16-1761781638512","title":"Critique-RL: Training Language Models for Critiquing through Two-Stage Reinforcement Learning","description":"Training critiquing language models to assess and provide feedback on model outputs is a promising way to improve LLMs for complex reasoning tasks. However, existing approaches typically rely on stronger supervisors for annotating critique data. To address this, we propose Critique-RL, an online RL approach for developing critiquing language models without stronger supervision. Our approach operates on a two-player paradigm: the actor generates a response, the critic provides feedback, and the actor refines the response accordingly. We first reveal that relying solely on indirect reward signals from the actor's outputs for RL optimization often leads to unsatisfactory critics: while their helpfulness (i.e., providing constructive feedback) improves, the discriminability (i.e., determining whether a response is high-quality or not) remains poor, resulting in marginal performance gains. To overcome this, Critique-RL adopts a two-stage optimization strategy. In stage I, it reinforces the discriminability of the critic with direct rule-based reward signals; in stage II, it introduces indirect rewards based on actor refinement to improve the critic's helpfulness, while maintaining its discriminability via appropriate regularization. Extensive experiments across various tasks and models show that Critique-RL delivers substantial performance improvements. For example, it achieves a 9.02% gain on in-domain tasks and a 5.70% gain on out-of-domain tasks for Qwen2.5-7B, highlighting its potential.","url":"https://huggingface.co/papers/2510.24320","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.512Z","metadata":{"authors":["Zhiheng Xi","Jixuan Huang","Xin Guo","Boyang Hong","Dingwen Yang","Xiaoran Fan","Shuo Li","Zehui Chen","Junjie Ye","Siyu Yuan","Zhengyin Du","Xuesong Yao","Yufei Xu","Jiecao Chen","Rui Zheng","Tao Gui","Qi Zhang","Xuanjing Huang"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.24320.pdf","arxivUrl":"https://arxiv.org/abs/2510.24320","arxivId":"2510.24320","coolPaperUrl":"https://papers.cool/arxiv/2510.24320","published":"2025-10-28T11:37:01Z","updated":"2025-10-28T11:37:01.000Z","zh_summary":"训练批评语言模型以评估模型输出并提供反馈是改进法学硕士以执行复杂推理任务的一种有前途的方法。然而，现有方法通常依赖于更强大的监督者来注释批评数据。为了解决这个问题，我们提出了 Critique-RL，这是一种在线 RL 方法，用于在没有更强监督的情况下开发批评语言模型。我们的方法采用双人模式：演员生成回应，批评者提供反馈，演员相应地完善回应。我们首先发现，仅依靠参与者输出的间接奖励信号进行 RL 优化通常会导致不令人满意的批评：虽然他们的帮助性（即提供建设性反馈）有所提高，但可辨别性（即确定响应是否高质量）仍然很差，导致边际性能提升。为了克服这个问题，Critique-RL 采用了两阶段优化策略。在第一阶段，它通过直接的基于规则的奖励信号来加强批评者的可辨别性;在第二阶段，它引入了基于行为者细化的间接奖励，以提高批评者的帮助性，同时通过适当的正则化来保持其可辨别性。跨各种任务和模型的广泛实验表明，Critique-RL 提供了显着的性能改进。例如，它在 Qwen9.02-5.70B 的域内任务上实现了 2.5% 的收益，在域外任务上实现了 7% 的收益，凸显了其潜力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“无需更强监督即可训练出高质量语言模型批判器（critique model）”这一核心难题，具体可拆解为以下三点：\n\n1. 摆脱对更强标注者的依赖\n既有方法需用 GPT-4 等更强模型为每条 actor 输出撰写批判数据，成本高且难以扩展。论文提出完全在线的强化学习流程，仅用可自动计算的规则奖励，无需任何外部强监督。\n\n2. 同时优化“判别力”与“有用性”\n仅用 actor 最终答案对错作为间接奖励信号（如  r_(refine) 、 r_(Delta) 、 r_(correction) ）会导致批判器陷入“保守”或“激进”极端：\n\n- 保守：不敢指出错误， Delta_(ito c)  低；\n- 激进：乱改正确答案， Delta_(cto i)  高。\n根本原因是判别力（Acc@Dis）未被显式优化。论文通过两阶段 RL 将二者解耦，先显式提升判别力，再在保持判别力的前提下提升有用性。\n3. 实现可扩展的 scalable oversight\n训练出的批判器可直接部署于测试阶段，无需 oracle verifier，对分布外任务仍保持稳健增益（Qwen2.5-7B 在 SVAMP 与 TheoremQA 上分别提升 9.02 % 与 5.70 %），从而为大模型复杂推理场景提供可扩展的自动监督信号。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为三大脉络，并指出各自与 Critique-RL 的核心差异：\n\n1. 提示工程激发批判能力（Prompt-based Elicitation）\n\n- 代表工作：Self-Refine、CRITIC、Chain-of-Verification、SelfEE 等。\n- 共同点：依赖手工提示词让同一模型“自我批判”，测试阶段需外部 oracle（答案匹配或工具）判断对错，模型只需生成反馈，无需自己判别。\n- 关键缺陷：无 oracle 时判别性能骤降，无法形成可扩展的自动监督。\n2. 监督微调训练批判器（Supervised Fine-tuning）\n\n- 代表工作：Constitutional AI、Saunders et al. 的“self-critiquing models”、Retroformer、CTRL。\n- 共同点：需更强模型（如 GPT-4）预先标注大量<问题, 回答, 批判>三元组，再用 SFT 或离线 RL 训练。\n- 关键缺陷：标注成本高；强模型分布与目标 actor 分布不一致，导致判别准确率（Acc@Dis）低；无法在线迭代。\n3. 纯强化学习优化批判器（Online RL for Critics）\n\n- 代表工作：Retroformer（PPO）、CTRL（GRPO）。\n- 共同点：用 actor 两次尝试的“间接结果”作为奖励，即  r_(refine) 、 r_(Delta)  或  r_(correction) ，在线更新批判器。\n- 关键缺陷：奖励信号仅反映“有用性”，不直接监督“判别力”，训练过程出现判别崩塌——保守或激进，最终 Acc@Refine 提升有限。\n- Critique-RL 的改进：首次指出上述缺陷，提出两阶段 RL——Stage-I 用规则奖励显式优化判别力，Stage-II 在保持判别力正则下再优化有用性，从而同时提升 Acc@Dis 与 Acc@Refine，无需任何更强标注者。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Critique-RL**：一种**两阶段在线强化学习框架**，在**无需更强监督**的前提下，**同时优化批判器的判别力（discriminability）与有用性（helpfulness）**。核心流程如下：\n\n阶段一：显式优化判别力（Discriminability）\n\n- **奖励信号直接来自规则**\n用可自动计算的 **oracle 正确性** 作为金标准，定义判别奖励：\n\nr_(dis)(x, y, c) = 1![f(x, y, c) = r_(oracle)(x, y)]\n\n其中  f(·)  为批判器对原答案  y  的正确性判断。\n\n- **训练目标**\n最大化：\n\nE_(csim π_φ)![r_(dis) - β,KL(π^(SFT)_φ parallel π_φ)]\n\n仅更新批判器，**actor 固定**。此阶段迅速把 Acc@Dis 从 66.5 提升到 78.7+，解决“保守/激进”根源。\n\n阶段二：优化有用性同时保持判别力（Helpfulness + Regularization）\n\n- **引入间接奖励**\n利用 actor 根据批判  c  生成的精炼答案  y'  是否正确，定义：\n\nr_(refine) = r_(oracle)(x, y')\n\n- **联合目标函数**\n最大化：\n\nE![r_(refine) + β_1 r_(dis) - β_2,KL(π^(Stage-I)_φ parallel π_φ)]\n\n-  r_(refine)  提升反馈质量；\n-  r_(dis)  防止判别力遗忘；\n- KL 正则把策略锚定在阶段一，避免崩溃。\n\n在线迭代与通用性\n\n- **交替执行** 阶段一与阶段二可循环多次，持续提升 Acc 与 Acc@Dis。\n- **零额外标注**：全程仅依赖可自动计算的规则奖励。\n- **跨模型/任务迁移**：同一批判器可即插即用到更大或不同系列 actor，实现 weak-to-strong 监督。\n\n效果总结\n\n| 模型 | 任务 | Acc↑ | Acc@Dis↑ |\n| --- | --- | --- | --- |\n| Qwen2.5-7B | MATH | +12.66 | +85.2 |\n| Qwen2.5-7B | GSM8K | +12.05 | +90.4 |\n| Qwen2.5-7B | SVAMP(OOD) | +9.4 | — |\n\n由此，Critique-RL 在**无更强监督**条件下，**稳定地同时提升判别与反馈质量**，为 scalable oversight 提供了可扩展的在线训练范式。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“判别力+有用性”双目标，从**主实验→消融→迭代→跨模型/跨任务→计算效率→定性分析**六个层面展开系统验证，核心实验一览如下：\n\n1\\. 主实验：三数据集、双规模、五对比方法\n\n| 模型 | 数据集 | 指标 | 相对最佳基线提升 |\n| --- | --- | --- | --- |\n| Qwen2.5-3B | MATH / GSM8K / AQuA | Acc@Refine / Acc@Dis | +4.46 / +5.31 / +3.15 pp |\n| Qwen2.5-7B | 同上 | 同上 | +5.54 / +6.36 / +6.43 pp |\n| 平均 +9.02 %（in-domain）+5.70 %（OOD） |\n\n**Baseline**：No-Critic、SFT、STaR、Retroformer(PPO)、CTRL(GRPO)。\n**结论**：Critique-RL 在所有组合上**同时取得最高 Acc@Refine 与最高 Acc@Dis**，显著优于仅优化有用性的 RL 基线。\n\n2\\. 消融实验（Ablation）\n\n| 版本 | MATH Acc@Refine | Acc@Dis | 说明 |\n| --- | --- | --- | --- |\n| 完整两阶段 | 48.6 | 82.8 | — |\n| 去掉 Stage-I | 47.6 (-1.0) | 79.7 (-3.1) | 判别力下降，连带有用性受损 |\n| 去掉 Stage-II | 45.9 (-2.7) | 78.7 (-4.1) | 仅判别力，无 refinement 增益 |\n| Stage-II 去  r_(dis) +KL | 47.3 (-1.3) | 77.7 (-5.1) | 出现“激进”反噬， Delta_(cto i)  升高 |\n| 替换  r_(refine)to r_(Delta)  | 48.2 (-0.4) | 82.6 (-0.2) | 可训练但收敛略慢 |\n\n**结论**：两阶段缺一不可；在有用性优化阶段**必须保留判别奖励与 KL 正则**，否则判别崩塌。\n\n3\\. 迭代训练 & 迭代推理\n\n| 迭代轮次 | MATH Acc | Acc@Dis | 说明 |\n| --- | --- | --- | --- |\n| 1 轮 Stage-I | 45.9 | 78.7 | 判别力先升 |\n| 1 轮 Stage-II | 48.6 | 82.8 | 有用性再升 |\n| 2 轮 Stage-I | 49.5 | 85.0 | 继续提升 |\n| 2 轮 Stage-II | 51.0 | 86.5 | 累计 +5.86 pp |\n\n**迭代推理**：对同一问题连续做 4 轮 critique-refinement，Acc 从 48.6→54.3，**每轮仍正增长**，未见饱和。\n\n4\\. 跨模型/跨任务/跨规模\n\n| 设置 | 数据集 | Acc 增益 | Acc@Dis 增益 | 说明 |\n| --- | --- | --- | --- | --- |\n| DeepSeek-R1-Distill-7B 做 actor | TheoremQA | +0.75 | +29.88 | 强推理模型亦受益 |\n| Qwen2.5-72B-Instruct 做 actor | MATH-500 | +1.20 | +8.80 | weak-to-strong 监督可行 |\n| Llama3.2-3B 做批判器 | GSM8K | +3.72 | +9.20 | 不同架构同样有效 |\n\n**OOD 任务**（SVAMP、TheoremQA）\nCritique-RL 相对最佳基线再提升 **2.3 pp / 1.6 pp**，验证**分布外泛化能力**。\n\n5\\. 推理阶段计算效率\n\n- **Majority Vote@K**\n在 MATH/GSM8K 上，Critique-RL 的 **MV@1 ≈ 基线 MV@12**，同等性能下节省 **12× 生成预算**。\n- **Refine-compute 缩放**\n固定预算下，Critique-RL 的 Pass@K 曲线**始终高于 SFT 批判器 2× 采样量**的曲线，体现**计算高效**。\n\n6\\. 开放任务 & 定性分析\n\n- **CNN/DailyMail 摘要**（无规则奖励）\n用 Skywork-Reward-V2 做 oracle，Critique-RL 在 Score↑1.12 的同时，MSE@Dis 降 7.87 点，**首次证明方法适用于开放式任务**。\n- **人工质量抽查**\n随机 600 条“成功纠错”的批判，GPT-4o 评估显示：\n- **96.2 %** 判别正确；\n- **93.3 %** 反馈被评为 high-quality。\n直观示例对比（图 8/9）进一步展示 Stage-I 能检错，Stage-II 会给出**可执行修改步骤**，验证两阶段设计机理。\n\n实验覆盖度小结\n\n| 维度 | 已验证内容 |\n| --- | --- |\n| 任务类型 | 数学推理 + 摘要（开放） |\n| 模型规模 | 3B→72B，含强推理模型 |\n| 模型系列 | Qwen2.5、Llama3.2、DeepSeek-R1 |\n| 训练方式 | 在线两阶段 RL、迭代训练 |\n| 推理方式 | 单次 / 迭代 / Majority Vote / 多 refinement |\n| 评价指标 | Acc、Δ、Δ_{c→i}、Δ_{i→c}、Acc@Dis、Pass@K、MSE@Dis |\n\n整套实验**闭环验证了 Critique-RL 在训练稳定性、判别力、有用性、计算效率与跨任务泛化上的全面优势**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**理论-算法-系统-应用**四个层面，供后续研究参考：\n\n1\\. 理论层面\n\n- **判别力-有用性权衡的形式化刻画**\n目前仅用线性加权 + KL 正则做折中，可进一步建立**多目标 Pareto 前沿**的定量边界，回答“最优权衡是否存在”“不同任务前沿形状如何”等问题。\n\n- **奖励信号稀疏性下收敛保证**\n数学推理任务中  r_(text{oracle)  为 0/1，奖励稀疏且延迟。可借鉴**count-based 探索**或**好奇心奖励**，在 Stage-I 加速对判别空间的覆盖，给出样本复杂度上界。\n\n- **策略梯度方差下界分析**\n两阶段使用不同奖励函数，可对比**单阶段混合奖励**的方差-偏差权衡，理论上解释为何两阶段训练更稳定。\n\n2\\. 算法层面\n\n- **自适应  β_1,β_2  调度**\n当前  β_1,β_2  为手动网格搜索。可引入**动态 Lagrange 乘子**或**约束强化学习**（CPO、IPO），在训练过程中根据 Acc@Dis 实时调整正则强度，避免人工调参。\n\n- **多智能体协同更新**\n现有 actor 固定，仅训批判器。可尝试**双向梯度更新**（actor-critic 共训），并加入**Stackelberg 博弈目标**，让 actor 生成更具挑战性的错误，从而提升批判器的样本效率。\n\n- **层次化判别空间**\n将  r_(text{dis)  从“整体正确”细化为**逐步判别**（per-step verifier）或**子目标判别**，构建细粒度奖励，进一步提高可解释性与收敛速度。\n\n3\\. 系统与规模层面\n\n- **端到端可验证管道**\n结合**形式化验证工具**（Lean、Coq），把 Lean 证明成功/失败作为  r_(text{oracle) ，训练出可生成**机器可检证**批判的模型，实现“训练-验证”闭环。\n\n- **多模态扩展**\n将框架迁移至**图像-文本混合推理**（几何题、图表问答），用多模态 oracle（执行结果或视觉模型）提供奖励，验证判别力-有用性权衡是否依然成立。\n\n- **千亿级模型上的稳定性**\n当前最大 72 B，尚未测试 100 B+ 场景。需研究**分布式 RL 工程优化**（梯度压缩、off-policy 缓冲池）以及**大模型收敛行为**是否出现奖励黑客或模式崩塌。\n\n4\\. 应用与评测层面\n\n- **开放域生成任务**\n摘要实验仅单数据集。可扩展至**创意写作、代码生成、科研问答**等无法拿到确定性 oracle 的场景，结合**AI 反馈奖励模型**（如 GPT-4o 打分）构建“RLAIF-Critique”流程，并建立**面向批判器的元评测指标**（一致性、可解释性、人类偏好）。\n\n- **在线教育系统**\n把批判器作为**自动阅卷助手**，实时指出学生答题步骤错误并给出修改建议。需研究**个性化难度控制**（根据学生水平动态调节 actor 错误类型）以及**隐私合规**（联邦学习+批判器蒸馏）。\n\n- **安全与对齐**\n利用两阶段 RL 训练\\*\\*“红队批判器”**，专门检测大模型输出中的**有害、误导、偏见**内容，对比现有 Constitutional AI 方法，看能否在**无人工红队数据\\*\\*情况下提升召回率。\n\n- **弱-强监督可扩展性极限**\n继续放大 actor-批判器规模差距（如 1 B 批判器 → 540 B actor），验证**判别力迁移上限**；同时研究**批判器集成**（ensemble / MoE）能否进一步提升弱-强监督效果。\n\n5\\. 数据与评测基准\n\n- **构建 CritiqueBench**\n现有基准只给最终答案对错，缺少**逐步判别标签**。可发布带**步级正误、错误类别、人类可读修正**的三元组数据集，配套判别力-有用性双指标，方便社区公平对比。\n\n- **对抗性错误类型覆盖**\n当前错误主要来自采样漂移。可引入**对抗触发提示**或**迷惑性上下文**，系统评估批判器对**分布外错误模式**的鲁棒性。\n\n简言之，**理论-算法-系统-应用** 仍有广阔空间：从“为什么两阶段更有效”到“如何自动调参”，从“Lean 可验证”到“多模态-开放域”，再到“千亿规模+联邦部署”，均可作为 Critique-RL 的后续探索路线。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Critique-RL**：首个**无需更强监督**即可训练高质量语言模型批判器的**两阶段在线强化学习框架**。核心内容可概括为以下四点：\n\n1. 问题与动机\n\n- 现有批判器训练要么依赖 GPT-4 等强模型标注，成本高且难扩展；\n- 仅用 actor 最终对错作为间接奖励，导致“判别力”与“有用性”冲突，出现保守或激进崩塌。\n2. 两阶段 RL 解法\n\n- **Stage-I**：用可自动计算的规则奖励  r_(dis)=1(f(x,y,c)=r_(oracle))  显式优化判别力，先让批判器“能判断对错”。\n- **Stage-II**：引入 actor 精炼结果奖励  r_(refine)  提升有用性，同时保留  r_(dis)  与 KL 正则，防止判别力遗忘，实现“既能判断又能指导”。\n3. 实验结果\n\n- **主实验**：Qwen2.5-7B 在 MATH、GSM8K、AQuA 上分别提升 12.66 / 12.05 / 2.36 pp，判别力 Acc@Dis 平均提升 6+ pp；OOD 任务 SVAMP、TheoremQA 再提升 5.70 %。\n- **消融**：去掉任一阶段或正则项，性能显著下降，验证两阶段缺一不可。\n- **迭代/缩放**：两轮训练累计 +5.86 pp；MV@1 达到基线 MV@12 性能，计算效率提升 12×。\n- **跨模型/任务**：在 Llama3.2、DeepSeek-R1-Distill、72 B 模型及 CNN/DailyMail 摘要上均一致有效。\n4. 贡献与意义\n\n- 首次揭示“仅间接奖励训练批判器”必然判别崩塌；\n- 提出两阶段 RL 范式，无强监督即可同时优化判别力与有用性；\n- 大量实验验证稳定性、泛化性与计算效率，为 scalable oversight 提供可扩展的在线监督信号。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Critique-RL%3A%20Training%20Language%20Models%20for%20Critiquing%20through%20Two-Stage%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2510.24320，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24320。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-17-1761781638513","title":"Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents","description":"Public research results on large-scale supervised finetuning of AI agents remain relatively rare, since the collection of agent training data presents unique challenges. In this work, we argue that the bottleneck is not a lack of underlying data sources, but that a large variety of data is fragmented across heterogeneous formats, tools, and interfaces. To this end, we introduce the agent data protocol (ADP), a light-weight representation language that serves as an \"interlingua\" between agent datasets in diverse formats and unified agent training pipelines downstream. The design of ADP is expressive enough to capture a large variety of tasks, including API/tool use, browsing, coding, software engineering, and general agentic workflows, while remaining simple to parse and train on without engineering at a per-dataset level. In experiments, we unified a broad collection of 13 existing agent training datasets into ADP format, and converted the standardized ADP data into training-ready formats for multiple agent frameworks. We performed SFT on these data, and demonstrated an average performance gain of ~20% over corresponding base models, and delivers state-of-the-art or near-SOTA performance on standard coding, browsing, tool use, and research benchmarks, without domain-specific tuning. All code and data are released publicly, in the hope that ADP could help lower the barrier to standardized, scalable, and reproducible agent training.","url":"https://huggingface.co/papers/2510.24702","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.513Z","metadata":{"authors":["Yueqi Song","Ketan Ramaneti","Zaid Sheikh","Ziru Chen","Boyu Gou","Tianbao Xie","Yiheng Xu","Danyang Zhang","Apurva Gandhi","Fan Yang","Joseph Liu","Tianyue Ou","Zhihao Yuan","Frank Xu","Shuyan Zhou","Xingyao Wang","Xiang Yue","Tao Yu","Huan Sun","Yu Su","Graham Neubig"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.24702.pdf","arxivUrl":"https://arxiv.org/abs/2510.24702","arxivId":"2510.24702","coolPaperUrl":"https://papers.cool/arxiv/2510.24702","published":"2025-10-28T17:53:13Z","updated":"2025-10-28T17:53:13.000Z","zh_summary":"关于大规模监督微调 AI 代理的公开研究结果仍然相对稀少，因为收集代理训练数据存在独特的挑战。在本工作中，我们认为瓶颈并不是缺乏基础数据源，而是大量多样化的数据分散在异构的格式、工具和接口中。为此，我们引入了代理数据协议（ADP），这是一种轻量级的表示语言，用作不同格式代理数据集与统一代理训练管道之间的“中间语言”。ADP 的设计足够表达各种任务，包括 API/工具使用、浏览、编程、软件工程和通用代理工作流程，同时保持简单，可解析且无需对每个数据集进行单独工程即可训练。在实验中，我们将 13 个现有的代理训练数据集统一为 ADP 格式，并将标准化的 ADP 数据转换为适用于多个代理框架的训练所需格式。我们对这些数据进行了监督微调（SFT），并展示了比相应基础模型平均约 20% 的性能提升，同时在标准编程、浏览、工具使用和研究基准上实现或接近最先进（SOTA）的性能，无需领域特定的调优。所有代码和数据均已公开发布，希望 ADP 能帮助降低标准化、可扩展且可复现的代理训练的门槛。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**大规模监督微调（SFT）AI Agent 的数据标准化瓶颈**。尽管已有大量异构的 Agent 训练数据集，但由于格式、动作空间、观测结构各不相同，导致：\n\n- 数据难以整合与复用；\n- 每新增一个数据集或 Agent 框架都需重复编写转换代码，工程成本呈二次增长；\n- 社区难以开展规模化、可复现的 Agent SFT 研究。\n\n为此，作者提出 **Agent Data Protocol（ADP）**，一种轻量级“中间语”模式，将碎片化数据统一成可即插即用的标准化轨迹，从而把\\*\\*“二次集成代价”降为线性\\*\\*，显著降低 Agent 训练门槛并提升跨任务迁移效果。\n\nQ2: 有哪些相关研究？\n\n与 Agent Data Protocol（ADP）直接相关的研究可归纳为三条主线：\n\n1. 异构 Agent 训练数据集的收集与发布\n2. 数据格式/动作空间统一的部分尝试\n3. 大规模监督微调（SFT）Agent 的早期探索\n\n以下按时间顺序列出代表性工作，并指出其与 ADP 的关联。\n\n1\\. 异构 Agent 数据集的收集与发布\n\n| 数据集 / 项目 | 核心贡献 | 与 ADP 的关系 |\n| --- | --- | --- |\n| WebGPT (Nakano et al., 2021) | 首批“浏览-回答”人工标注轨迹 | 被 ADP 归类为 Web Browsing 源，需统一成 APIAction+WebObservation |\n| Mind2Web (Deng et al., 2023) | 真实网站人工演示 + DOM 快照 | ADP 将其 HTML/axtree 字段标准化为 WebObservation |\n| AgentInstruct (Zeng et al., 2023) | 多领域合成轨迹（OS、DB、Web 等） | 首批被 ADP 转换的 13 个数据集之一 |\n| SWE-Gym (Pan et al., 2025) | GitHub 真实 issue 的 Agent rollout | ADP 将其 bash/file 动作映射为 APIAction |\n| Orca AgentInstruct (Mitra et al., 2024) | 百万级合成工具调用指令 | 因规模过大，ADP 采用 wd=0.001 下采样 |\n| Go-Browse (Gandhi & Neubig, 2025) | 结构化探索式网页轨迹 | ADP 引入的“函数思维”覆盖率 100% 案例 |\n| Synatra (Ou et al., 2024) | 教程网页合成轨迹 | ADP 发现其平均轮次仅 1.0，最短之一 |\n\n2\\. 数据格式或动作空间统一的部分尝试\n\n| 工作 | 核心思想 | 与 ADP 的区别 |\n| --- | --- | --- |\n| Agent-FLAN (Chen et al., ACL 2024) | 为 LLM Agent 设计“扁平化”指令模板 | 仅聚焦单轮指令-回答，未定义多轮轨迹 schema |\n| AgentOhana (Zhang et al., 2024) | 将不同轨迹转成统一“对话+工具”JSON | 仍绑定特定 Agent 脚手架，未提供跨框架双向转换 |\n| xLAM (Zhang et al., NAACL 2025) | 提出“动作 x 观测”统一 JSON 格式 | 仅覆盖 API/代码动作，缺少 WebObservation 等细粒度字段 |\n| AgentGym (Xi et al., ACL 2025) | 统一环境接口，但数据侧仍保持原格式 | 重点在评估环境标准化，而非训练数据标准化 |\n| BrowserGym (de Chezelles et al., 2025) | 统一网页观测（HTML + axtree） | ADP 直接复用其 axtree 定义，并扩展出 API/Code/Message 动作 |\n\n3\\. 大规模 SFT Agent 的早期探索\n\n| 工作 | 规模 | 结论/局限 | ADP 的改进 |\n| --- | --- | --- | --- |\n| AgentTuning (Zeng et al., 2023) | 1.9K 轨迹 | 首次证明 SFT 可提升通用 Agent 能力 | 数据量小、领域有限；ADP 将其纳入并放大到 1.3M |\n| AgentBank (Song et al., EMNLP 2024) | 50K 轨迹 | 规模提升，但格式各异，未公开统一转换脚本 | ADP 提供开源 Pydantic schema 与双向转换器 |\n| SWE-smith (Yang et al., 2025) | 5K SWE 轨迹 | 仅在软件工程领域 SOTA | ADP 将其与浏览、工具数据混合，验证跨任务迁移 |\n\n小结\n\n- **数据侧**：ADP 首次把 13 个主流数据集的“原始格式”全部标准化为同一 Pydantic schema，覆盖 API、代码、消息、文本、网页五类原子元素。\n- **协议侧**：与 AgentOhana/xLAM 等“单向模板”不同，ADP 提供 **Raw→ADP→SFT** 的双向管线，使社区新增数据集或 Agent 框架时只需线性成本。\n- **训练侧**：之前最大公开 Agent SFT 数据为 ~100K 级别；ADP 发布 1.3M 轨迹，并验证在 7B→32B 参数规模上平均提升 ~20%，达到或超过 Claude-3.5-Sonnet 等闭源模型水平。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Agent Data Protocol（ADP）** 作为“轻量级中间语”，将原本碎片化的异构 Agent 训练数据统一成可即插即用的标准化轨迹，从而把“每新增一个数据集或 Agent 框架就要重写全套转换代码”的 **二次代价** 降为 **线性代价**。核心解决路径分为三步：\n\n1\\. 设计统一 schema——把任意轨迹拆成“动作+观测”原子\n\n- **Pydantic 实现**：`Trajectory = id + content[] + details{}`\n- `content[]` 是 **Action ↔ Observation** 的严格交替序列\n- **三大 Action 原子**\n- `APIAction`：函数名 + kwargs + 可选思维\n- `CodeAction`：语言 + 代码段 + 可选思维\n- `MessageAction`：自然语言字符串\n- **两大 Observation 原子**\n- `TextObservation`：来源(user/environment) + 文本\n- `WebObservation`：html + axtree + url + viewport + 可选截图\n\n> 该 schema 已覆盖代码、软件工程、API/工具、网页浏览等 13 个公开数据集的全部语义，且可验证（自动类型检查 + 自定义规则）。\n\n2\\. 双向转换管线——“Raw→ADP→SFT” Hub-and-Spoke\n\n```\nRaw 数据集 ──once──► ADP 标准化 ──once──► 任意 Agent SFT 格式\n↑ ↑\n│ │\n└────────── 线性 O(D+A) 成本 ───────────┘\n```\n\n- **Raw→ADP**：每数据集只需写 **一次** 转换脚本（平均 ~380 行）\n- **ADP→SFT**：每 Agent 框架只需写 **一次** 反向模板（平均 ~77 行）\n- **质量闸门**：自动化验证工具调用格式、思维覆盖率、会话结束符等，保证下游训练稳定。\n\n3\\. 大规模实证——1.3 M 轨迹、3 套 Agent、4 项 Benchmark\n\n- **数据混合**：按 wd 系数对大数据集下采样（Orca 0.001×）、小数据集上采样（SWE-Gym 3×），并做**域内过滤**（OpenHands/SWE-Agent 仅用代码+工具部分；AgentLab 仅用网页部分）。\n- **训练设置**：统一用 LLaMA-Factory 对 Qwen-2.5/-3 进行 3-epoch 纯 SFT，无任务特定调参。\n- **结果摘要**（相对 base 平均提升 ≈ 20%）：\n- **SWE-Bench Verified**：7B 从 0.4%→20.2%；14B 从 2.0%→34.4%；32B 达到 40.3%，**超过 Claude-3.5-Sonnet 33.6%**。\n- **WebArena**：7/14/32B 分别提升 16.5/16.7/12.0 个百分点。\n- **AgentBench OS**：7B 提升 23.6 个百分点；32B 提升 6.9 个百分点。\n- **跨任务迁移**：在同一评估环境内，**混合 ADP 数据** 一致优于 **单领域数据**，避免负迁移。\n\n4\\. 社区成本量化——从 O(D×A) 到 O(D+A)\n\n- 无 ADP：100 个框架 × 13 个数据集 ≈ 48 万行代码\n- 有 ADP：13 个数据集 + 100 个框架 ≈ 1.3 万行代码\n\n> 节省 97% 工程量；新数据集或新框架可**即时接入**已有生态。\n\n结论\n\nADP 通过“标准化原子语义 + 双向转换管线 + 自动化验证”，把原本碎片化、重复造轮子的 Agent 数据整合问题转化为**一次转换、处处可用**的线性工程，从而首次在公开社区实现了 **百万轨迹级、跨领域、可复现** 的 Agent 监督微调。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **Agent Data Protocol（ADP）** 的“标准化能力”与“训练收益”展开系统实验，共 4 组 18 张结果表/图，覆盖 3 个参数规模、3 套 Agent 框架、4 大公开基准。实验设计遵循 **“同模型、同框架、同 benchmark”** 原则，确保提升可归因于 ADP 数据本身，而非工程调参。核心实验如下：\n\n1\\. 主实验：ADP 统一数据 vs 基线模型\n\n**目的**：验证“用 ADP 标准化后的混合轨迹做纯 SFT”能否在多项任务上同时涨点。\n\n| 变量控制 | 详情 |\n| --- | --- |\n| 基线 | 原始指令模型（Qwen-2.5-7/14/32B-Coder-Instruct，Llama-3.1-8B 等） |\n| 训练数据 | 1.3 M ADP 轨迹（13 数据集按 §5.1 采样权重混合） |\n| 训练流程 | LLaMA-Factory，3 epoch，lr=5e-5，cosine，无任务特定 trick |\n| 评测基准 | SWE-Bench Verified、WebArena、AgentBench-OS、GAIA |\n\n**关键结果**（△ 为绝对提升）\n\n- **SWE-Bench Verified**\n- 7B：0.4 % → 20.2 %（△+19.8）\n- 14B：2.0 % → 34.4 %（△+32.4，> Claude-3.5-Sonnet 33.6 %）\n- 32B：2.2 % → 40.3 %（△+38.1）\n- **WebArena**\n- 7/14/32B 平均 +15.1 %，且随模型规模单调上升\n- **AgentBench-OS**\n- 7B：3.5 % → 27.1 %（△+23.6）\n- 32B：27.8 % → 34.7 %（△+6.9，已接近上限）\n- **GAIA**\n- 7B：7.3 % → 9.1 %（△+1.8，任务本身极难，提升仍显著）\n\n> 结论：ADP 数据在 **代码、网页、操作系统、通用推理** 四大域同时带来两位数提升，且增益随模型规模扩大而保持，**首次在 7–32 B 级别实现“无领域调参”即 SOTA 或近 SOTA**。\n\n2\\. 跨任务迁移实验：混合数据 vs 单领域数据\n\n**目的**：检验“把多域数据一次性混合”是否比“只在目标域训练”更好，并观察是否出现负迁移。\n\n| 设置 | 训练数据 | SWE-Bench | WebArena | AgentBench-OS | GAIA |\n| --- | --- | --- | --- | --- | --- |\n| 单域 | SWE-smith Only | 1.0 % | – | – | – |\n| 单域 | Go-Browse Only | – | 16.0 % | – | – |\n| 单域 | AgentInstruct Only | – | – | 21.5 % | 0.6 % |\n| 混合 | ADP Data | 10.4 % | 20.1 % | 25.7 % | 9.1 % |\n\n> 结论：混合 ADP 数据 **全面打败** 单域数据，且在 SWE-Bench 上提升 10×，**未观察到负迁移**；说明 ADP 标准化保留了各域有效信号，同时利用跨域正则化提升泛化。\n\n3\\. 消融：不同采样权重对性能的影响（附录 B）\n\n- 对 Orca AgentInstruct（1 M+ 轨迹）设置 wd=0.001，防止工具调用样本淹没其他域；\n- 对 SWE-Gym 设置 wd=3，弥补原始数据仅 0.5 k 的不足；\n- 经网格扫描，最终混合比例在 SWE-Bench 上带来 2.3 % 绝对增益，验证 **均衡采样策略有效**。\n\n4\\. 工程代价评估：代码行数（LOC）统计\n\n| 阶段 | 平均 LOC | 总工作量（13 数据集 × 100 框架） |\n| --- | --- | --- |\n| Raw→ADP | ～380 / 数据集 | 13 × 380 ≈ 4.9 k |\n| ADP→SFT | ～77 / 框架 | 100 × 77 ≈ 7.7 k |\n| 合计 | O(D+A) ≈ 12.6 k | 无 ADP 需 O(D×A) ≈ 489 k |\n\n> 结论：ADP 把社区集成成本压缩 **97%**，新数据集或新框架**仅需一次 77 行脚本即可接入全量数据**。\n\n5\\. 可复现性验证\n\n- 公开全部 Pydantic schema、转换脚本、训练超参与随机种子；\n- 提供 **一键重跑脚本** 可从原始 13 个数据集再生 ADP-V1 训练语料；\n- 在 OpenReview 与 GitHub 同步发布模型权重与评测日志，确保数字可复现。\n\n实验总结\n\n1. **收益性**：7–32 B 模型在 4 大基准平均 **+20 %**，首次用公开数据匹配 Claude-3.5-Sonnet。\n2. **泛化性**：混合多域数据 > 单域数据，无负迁移。\n3. **经济性**：线性 O(D+A) 替代二次 O(D×A)，社区工程成本降低 97%。\n4. **可复现性**：完整开源数据、代码、模型与评测协议。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 ADP 的“直接延伸”或“深层扩展”，均围绕 **协议本身、数据生态、训练策略、评估体系** 四个维度展开，供后续研究快速落地。\n\n1\\. 协议层面：原子动作/观测的语义升级\n\n- **多模态原子**\n- 将 `WebObservation` 扩展为 `ScreenObservation`，引入 **屏幕截图/UI 树/屏幕录制** 三通道，支持桌面端 Agent。\n- 新增 `ImageObservation`、`AudioObservation` 原子，打通 **GUI 自动化+语音交互** 任务。\n- **连续控制原子**\n- 引入 `MouseAction(dx, dy, button)`、`KeyboardAction(key_seq)`，让 ADP 从“离散 API”走向“连续像素级操作”，适配 **VLA（Vision-Language-Action）模型**。\n- **思维链标准化**\n- 在现有 `description` 字段外，定义 `ThoughtAction(content, type=plan/revise/reflect)`，支持 **显式思维链蒸馏** 与 **隐式推理数据增强**。\n\n2\\. 数据生态：自动化、合成、持续迭代\n\n- **Auto-Converter**\n- 基于 LLM 的 **“self-transpiler”**：输入数据集原始 JSON 示例，自动生成 Raw→ADP 转换脚本，实现 **零人工写码** 接入新数据源。\n- **Self-Improvement Loop**\n- 用已训 ADP-Agent 在 **未标注环境** 滚动，产生新轨迹→ADP 标准化→质量过滤器→加入下一轮训练，构建 **“数据-模型”双螺旋增长**。\n- **困难样本定向合成**\n- 针对 SWE-Bench 剩余 60 % 未解 issue，使用 **故障定位+补丁模板+变异测试** 合成 **高难度轨迹**，填补尾部分布。\n\n3\\. 训练策略：混合目标、增量、参数高效\n\n- **多粒度目标函数**\n- 在标准 LM 损失外，加入 **动作类型分类损失** 与 **工具参数回归损失**，显式优化 **动作结构正确性**。\n- **课程式微调**\n- 按轨迹长度/难度（通过率）分层采样，先短后长、先易后难，缓解 **“长程信用分配”灾难**。\n- **参数高效扩展**\n- 仅对 Action/Observation Token 施加 **LoRA+AdaLoRA** 增量矩阵，减少 50 % 可训练参数量，保持 ADP 跨域迁移能力。\n\n4\\. 评估体系：标准化环境+协议级指标\n\n- **ADP-Eval Suite**\n- 把 SWE-Bench、WebArena、AgentBench、GAIA 的 **环境接口** 统一封装成 **Docker-Compose 模板**，实现 **“一键拉起”相同 eval 环境**，降低评估漂移。\n- **协议级指标**\n- 定义 **Action Accuracy**（原子动作格式是否合法）、**Thought Coverage**（≥80 % 动作带思维）、**Observation Fidelity**（网页字段完整性）等 **数据质量指标**，用来自动衡量新数据集接入后的“标准化程度”。\n- **跨域鲁棒性基准**\n- 构建 **CrossTask-Robustness Bench**：每个任务刻意混入 **其他域的干扰子任务**（如在 SWE 任务里插入“查文档网页”步骤），测量 Agent **抗干扰与任务切换** 能力。\n\n5\\. 社区与工具链\n\n- **在线数据仓库**\n- 建立 **“ADP Hub”**——类似 Hugging Face Datasets，支持 **拖拽上传原始轨迹→自动转换→质量看板→公开 DOI 引用**，激励数据贡献者。\n- **VSCode 插件**\n- 开发 **ADP-Inspector**：实时可视化轨迹、高亮缺失字段、一键补全 schema，降低新手使用门槛。\n- **法律与伦理过滤器**\n- 在转换管线中集成 **PII 脱敏+许可证扫描** 模块，确保新数据集 **合规发布**，避免后续开源风险。\n\n6\\. 长期愿景：从“数据协议”到“Agent 协议”\n\n- **环境-数据-评估三位一体**\n- 将 ADP 思想延伸到 **环境接口**（ADP-Env）与 **评估协议**（ADP-Eval），形成 **“任何环境、任何数据、任何 Agent”** 均可插拔的标准化栈，推动 **Agent 研究可复现性** 进入 “ImageNet 时代”。\n\nQ6: 总结一下论文的主要内容\n\n论文核心速览\n\n1\\. 痛点\n\n- 现有 Agent 训练数据集格式、动作空间、观测结构各异 → 整合难、复用难、工程代价 O(D×A)\n- 导致大规模监督微调（SFT）Agent 在公开社区罕见，进展受限\n\n2\\. 解决方案——Agent Data Protocol（ADP）\n\n- **轻量级“中间语”**：统一把任意轨迹表示成 **Action ↔ Observation** 交替序列\n- Action = APIAction | CodeAction | MessageAction\n- Observation = TextObservation | WebObservation\n- **Pydantic 实现** + 自动验证，保证数据质量\n- **双向转换管线**：\n- Raw→ADP（一次写入，永久通用）\n- ADP→SFT（一次模板，即插即用）\n- 复杂度从 **O(D×A) 降为 O(D+A)**\n\n3\\. 数据规模\n\n- 已转换 **13 个主流数据集** → 1.3 M 轨迹（公开最大 Agent SFT 语料）\n- 按域均衡采样，避免大语料淹没小语料\n\n4\\. 实验结果\n\n| 基准 | 7B 提升 | 14B 提升 | 32B 提升 | 备注 |\n| --- | --- | --- | --- | --- |\n| SWE-Bench Verified | +19.8% | +32.4% | +38.1% | 32B 超 Claude-3.5-Sonnet |\n| WebArena | +16.5% | +16.7% | +12.0% | 单调随规模增长 |\n| AgentBench-OS | +23.6% | +18.0% | +6.9% | 7B 涨点最猛 |\n| GAIA | +1.8% | — | — | 极难任务仍显著 |\n\n- **跨任务迁移**：混合 ADP 数据 **全面优于** 单领域微调，无负迁移\n- **工程代价**：社区集成代码行数压缩 **97%**\n\n5\\. 开源与复现\n\n- 全部 schema、转换脚本、训练权重、评测日志已开源\n- 提供一键重跑脚本，可完整复现 ADP-V1 语料与实验结果\n\n6\\. 结论\n\nADP 用标准化“动作+观测”原子，把碎片化 Agent 数据变成可即插即用的公共资源，首次实现 **百万轨迹级、跨领域、无调参** 的 Agent 监督微调，平均提升约 **20%** 并达到或超越同规模 SOTA，显著降低社区进入门槛。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Agent%20Data%20Protocol%3A%20Unifying%20Datasets%20for%20Diverse%2C%20Effective%20Fine-tuning%20of%20LLM%20Agents，链接是https://arxiv.org/pdf/2510.24702，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.24702。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1761781638513","title":"Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models","description":"Reinforcement learning with verifiable rewards (RLVR) has delivered impressive gains in mathematical and multimodal reasoning and has become a standard post-training paradigm for contemporary language and vision-language models. However, the RLVR recipe introduces a significant risk of capability regression, where models forget foundational skills after prolonged training without employing regularization strategies. We empirically confirm this concern, observing that open-source reasoning models suffer performance degradation on core capabilities such as perception and faithfulness. While imposing regularization terms like KL divergence can help prevent deviation from the base model, these terms are calculated on the current task, thus they do not guarantee broader knowledge. Meanwhile, commonly used experience replay across heterogeneous domains makes it nontrivial to decide how much training focus each objective should receive. To address this, we propose RECAP-a replay strategy with dynamic objective reweighting for general knowledge preservation. Our reweighting mechanism adapts in an online manner using short-horizon signals of convergence and instability, shifting the post-training focus away from saturated objectives and toward underperforming or volatile ones. Our method is end-to-end and readily applicable to existing RLVR pipelines without training additional models or heavy tuning. Extensive experiments on benchmarks based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our method, which not only preserves general capabilities but also improves reasoning by enabling more flexible trade-offs among in-task rewards.","url":"https://huggingface.co/papers/2510.21978","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.513Z","metadata":{"authors":["Hoang Phan","Xianjun Yang","Kevin Yao","Jingyu Zhang","Shengjie Bi","Xiaocheng Tang","Madian Khabsa","Lijuan Liu","Deren Lei"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.21978.pdf","arxivUrl":"https://arxiv.org/abs/2510.21978","arxivId":"2510.21978","coolPaperUrl":"https://papers.cool/arxiv/2510.21978","published":"2025-10-24T19:08:48Z","updated":"2025-10-24T19:08:48.000Z","zh_summary":"具有可验证奖励的强化学习（RLVR）在数学和多模态推理方面取得了显著进展，并已成为当代语言模型和视觉-语言模型的标准后训练范式。然而，RLVR方法引入了能力倒退的显著风险，即在长时间训练后，如果不采用正则化策略，模型可能会遗忘基础技能。我们通过实验证实了这一担忧，观察到开源推理模型在核心能力如感知和可信度方面出现了性能下降。尽管施加诸如KL散度的正则化项可以帮助防止模型偏离基础模型，但这些项是针对当前任务计算的，因此无法保证更广泛的知识。同时，在异构领域中常用的经验回放使得决定每个目标应获得多少训练关注变得不容易。为了解决这一问题，我们提出了RECAP——一种具有动态目标重权的回放策略，用于通用知识的保持。我们的重权机制以在线方式适应，利用收敛和不稳定性的短期信号，将后训练的关注从饱和目标转向表现不佳或波动的目标。我们的方法是端到端的，并且可以直接应用于现有的RLVR流程，无需训练额外模型或进行大量调参。基于Qwen2.5-VL-3B和Qwen2.5-VL-7B的基准测试的广泛实验表明，我们的方法不仅能保持通用能力，还能通过在任务内奖励之间实现更灵活的权衡来提升推理能力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决\\*\\*强化学习可验证奖励（RLVR）在提升大模型推理能力时引发的“通用能力灾难性遗忘”\\*\\*问题。具体而言：\n\n- **现象**：RLVR 仅针对数学、图表等可验证推理任务进行优化，导致模型在预训练阶段获得的感知、OCR、鲁棒性、事实性等通用能力显著退化（图 1、图 2）。\n- **核心难点**：\n1. 传统 KL 正则只在当前任务分布上约束，无法保证保留任意非目标技能；\n2. 经验回放需同时处理异构领域（感知、OCR、推理等），手动设定各目标权重既昂贵又次优；\n3. 不同奖励信号收敛速度差异大（格式奖励 50 步即饱和，推理奖励长期高方差，图 4），静态或人工调权容易过优化简单目标、忽视困难目标。\n- **目标**：在**不引入额外模型、不增加大量计算**的前提下，提出一种**即插即用的在线目标重加权回放策略**，使 RLVR 在持续提升推理成绩的同时，**保持甚至增强原始通用能力**。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统回顾了相关研究，可归纳为四大脉络（按时间递进，不含第一人称）：\n\n1\\. 基础模型与后训练范式\n\n- **预训练骨干**：GPT 系列、LLaMA、Qwen 等通过大规模自监督获得通用能力。\n- **监督微调（SFT）**：ULMFiT、FLAN、Flan-T5 将通用 LM 转化为指令跟随模型。\n- **偏好对齐**：InstructGPT、Anthropic 的 RLHF 框架引入 Bradley-Terry 模型 + PPO，兼顾有用性与安全性。\n- **可验证奖励 RL（RLVR）**：DeepSeek-R1、Math-Shepherd、ProR 等用规则或执行结果直接给奖励，取代人工偏好，提升数学/代码/图表推理。\n\n2\\. 灾难性遗忘与持续学习\n\n- **正则化方法**：\n- Elastic Weight Consolidation (EWC, Kirkpatrick et al. 2017)\n- Synaptic Intelligence (SI, Zenke et al. 2017)\n- Memory-Aware Synapses (MAS, Aljundi et al. 2018)\n- **功能约束/蒸馏**：Learning without Forgetting (LwF, Li & Hoiem 2016) 用旧模型输出蒸馏新模型。\n- **经验回放**：\n- 小容量 episodic memory（iCaRL、GEM、A-GEM）\n- Dark Experience Replay (DER, Buzzega et al. 2020)\n- 大规模 replay（Rolnick et al. 2019）在 NLP 持续指令调优中有效缓解领域遗忘。\n\n3\\. 大模型持续后训练中的遗忘证据\n\n- **指令调优序列化**：Luo et al. 2023、He et al. 2023 发现领域知识、阅读理解性能持续下降。\n- **机制研究**：Kotha et al. 2024 揭示微调改变隐式任务推断，使模型更少调用预训练技能。\n- **RLHF 中的 KL 惩罚**：Ouyang et al. 2022、Zheng et al. 2023 表明 KL 可减缓知识漂移，但部分 RLVR 工作为提升探索性而降低或去除 KL（Hu et al. 2025a; Hao et al. 2025），加剧遗忘。\n\n4\\. 后训练实践中的缓解策略\n\n- **混合目标**：InstructGPT 预训练梯度与 RLHF 交替；Zhang et al. 2025、Fu et al. 2025 引入验证 rollout 或增广数据惩罚。\n- **多奖励加权**：MiMo-VL-7B 技术报告披露在 RL 阶段同时优化数学、格式、感知等多目标，但具体采样/加权机制未公开。\n- **思考/反思机制**：Chu et al. 2025 的 BRPO 在 RL 目标中引入视觉再注意，减少幻觉。\n\n与本文区别\n\n- 上述工作要么仅正则化当前任务分布（KL、LwF），要么依赖人工或离线搜索的固定混合比例（MiMo、MoDoMoDo）。\n- RECAP 首次在 **RLVR 阶段** 引入**在线、无辅助模型、基于短窗收敛率与不稳定性的动态重加权**，兼顾推理增益与通用能力保持。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Replay-Enhanced CApability Preservation（RECAP）**，一种**即插即用的在线目标重加权回放机制**，在 RLVR 训练过程中同时维持通用能力。核心思路与步骤如下（不含第一人称）：\n\n1\\. 问题建模：多目标异构损失\n\n- 训练数据来自 N 个领域，共 K≥N 个目标（格式、准确率、IoU、下一 token 预测等）。\n- 每步 mini-batch 得到各目标损失  L_k^((t)) ，传统方法对所有  L_k^((t))  等权或手工调权，易导致简单目标过优化、困难目标被忽视。\n\n2\\. 在线监测：短窗收敛率与不稳定度\n\n对每一目标 k 维护长度 2W 的滑动窗，计算：\n\n- **收敛率**\n\nc_k^((t))=μ_k^((t))tildeμ_k^((t))= (frac1) / (W)∑_(s=t-W+1)^(t)L_k^((s))(1) / (W)∑_(s=t-2W+1)^(t-W)L_k^((s))\n\n c_k^((t))!>!1  表示近期仍在下降； c_k^((t))!≈!1  表示饱和。\n\n- **不稳定度（逆信噪比）**\n\ni_k^((t))=σ_k^((t))μ_k^((t))+tildeμ_k^((t)), quad σ_k^((t))=√(1) / (W-1)∑_(s=t-W+1)^(t)l(L_k^((s))-μ_k^((t))r)^2\n\n i_k^((t))  大说明损失高方差、尚未稳定。\n\n3\\. 动态重加权：温度控制 softmax\n\n将上述信号合并为优先级得分并归一化：\n\ns_k^((t))=c_k^((t))+i_k^((t)), quad λ_k^((t))=K·exp(s_k^((t))/T)∑_(i=1)^(K)exp(s_i^((t))/T)\n\n温度  T=5 （默认），系数和保持  (1) / (K)∑_kλ_k^((t))=1 ，无需额外超参搜索。\n\n4\\. 总体训练目标\n\n每步最小化加权损失：\n\nL^((t))(θ)=(1) / (K)∑_(k=1)^(K)λ_k^((t)),L_k^((t))\n\n- 饱和且稳定的目标  λ_k^((t))↓ \n- 难收敛或高方差目标  λ_k^((t))↑ \n\n5\\. 数据回放策略\n\n- **RLVR-Only 设置**：在纯 RL 阶段同时采样几何、代数、图表（推理）与检测、分割、OCR（通用）数据。\n- **Hybrid 设置**：RL 奖励与 SFT 风格 next-token 损失共存，统一按上述  λ_k^((t))  加权，无需额外模型或梯度截断。\n\n6\\. 实现特点\n\n- **端到端**：不训练辅助网络，不增加推理开销。\n- **幅度无关**：对不同量级损失自动适应，无需手工归一化。\n- **即插即用**：任何现有 RLVR 管道只需把静态  λ_k=1/K  换成动态  λ_k^((t))  即可。\n\n效果摘要\n\n- 在 Qwen2.5-VL-3B/7B 上，RECAP 相对“仅推理”基线：\n- 通用感知任务（LISA、OCRBench 等）**绝对提升 2–10 分**；\n- 推理任务（MathVista、MMMU 等）**持平或更好**；\n- 生成思维链长度缩短 **60 %**，推理准确率不变，显著降低推理延迟。\n\nQ4: 论文做了哪些实验？\n\n论文在两种互补设置下系统验证了 RECAP 的有效性，共涉及 **2 个基座模型**（Qwen2.5-VL-3B / 7B）、**15 个评测基准**、**6 种强基线** 与 **2 组消融实验**。主要实验一览如下（无第一人称，按设置分块）：\n\n1 RLVR-Only 设置（小规模纯 RL）\n\n| 配置 | 细节 |\n| --- | --- |\n| 基座 | Qwen2.5-VL-3B |\n| 训练数据 | 几何、代数、图表、ScienceQA、SAT 等可验证推理集 |\n| 对比方法 | - 仅推理（no replay）- Uniform 采样（等权，无重加权）- MoDoMoDo（静态最优混合，需额外代理模型） |\n| 评测基准 | SAT、ScienceQA、MathVista-mini、ChartQA、InfoVQA、MMMU |\n| 关键结果 | - 相对 base 模型平均↑8.2 pp- 相对 MoDoMoDo 平均↑2.9 pp，无需反复搜索混合比例 |\n\n2 Hybrid 设置（大规模 RL+SFT 混合）\n\n| 配置 | 细节 |\n| --- | --- |\n| 基座 | Qwen2.5-VL-7B |\n| 训练数据 | ThinkLite-VL-70k（推理）+ RefCOCO + LLaVA-OneVision-OCR（通用） |\n| 对比方法 | - 仅推理- PropMix（按来源大小采样）- Uniform- Coreset（半量回放）- LwF（KL=0.01） |\n| 评测基准 | 通用能力：LISA、MMMU-Pro、AI2D、MMBench、VizWiz、OCRBench-v2推理能力：MathVista、MathVision、MathVerse |\n| 关键结果 | - 通用任务平均↑2.1 pp，LISA 分割 IoU 从 57.6→67.2- 推理任务平均↑1.8 pp，全部 9 项均优于或持平最强基线 |\n\n3 消融与行为分析\n\n| 实验 | 目的 | 主要发现 |\n| --- | --- | --- |\n| 3.1 重加权消融 | 固定其他条件，仅关闭动态 λ | 格式奖励早期领先，后期被 RECAP 反超；准确率全程领先，验证重加权有效性（图 5） |\n| 3.2 思考长度演化 | 对比“仅推理”与 RECAP 的生成长度 | RECAP 把平均思考长度从 67.3 词压至 27.3 词，缩短 60 % 且准确率不降（图 7） |\n| 3.3 统一思考奖励副作用 | 在感知任务上也加“格式/思考”奖励 | 模型迅速学会输出极短思考甚至直接给框，证明需按任务类型区分奖励（图 6） |\n\n4 训练过程可视化\n\n- **收敛行为**：格式奖励 50 步即饱和（c≈1, i≈0），IoU 与准确率长期高方差，**支撑 c+i 加权策略**（图 4）。\n- **奖励动态**：单步奖励标准差峰值达 0.9，**说明必须用滑动窗而非瞬时量**（图 11）。\n- \\*\\* rollout 演变\\*\\*：训练 0→5 000 样本，模型从完整思考→仅输出坐标→把坐标直接放 `<think>`，**直观展示思考冗余**（附录 A.4）。\n\n5 结果一览表（节选）\n\n| 模型 | LISA IoU | MMMU-Pro | MathVista | OCRBench-v2 |\n| --- | --- | --- | --- | --- |\n| Base | 65.13 | 25.55 | 61.70 | 39.49 |\n| 仅推理 | 57.58 ↓7.6 | 33.87 | 65.50 | 38.55 |\n| Uniform | 65.18 | 31.91 | 65.60 | 38.06 |\n| RECAP | 67.24 ↑2.1 | 34.15 | 66.70 | 39.72 |\n\n6 结论\n\n- **15 项基准全部提升或持平**，首次在 RLVR 阶段实现“推理↑ 通用能力↑”双增益。\n- **零额外参数、零辅助模型、即插即用**，可直接嵌入现有 RLVR 管道。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，按“理论-数据-方法-评测-系统”五层归纳：\n\n1 理论层面\n\n- **收敛-方差权衡的数学刻画**\n当前  s_k=c_k+i_k  为启发式，可建立多目标随机优化的动态遗憾界，证明优先级  λ_k^((t))  在何种条件下保证帕累托最优或最小化最大遗忘。\n\n- **与持续学习理论的连接**\n将 RECAP 视为在线任务分配策略，与 Gradient Episodic Memory (GEM) 的约束形式对比，探讨其是否隐含满足“无遗忘”充分条件。\n\n2 数据与分布\n\n- **通用能力数据选择策略**\n仅使用 RefCOCO+OCR 子集，可引入“数据价值估计器”在线决定哪些通用样本最值得回放，减少 30–50 % 回放量而保持效果。\n\n- **非 RL 目标的扩展**\n目前聚焦可验证奖励。可将摘要、翻译、对话等不可验证任务转为 DPO/KTO 损失，验证动态重加权是否仍优于静态混合。\n\n- **长序列与多轮交互遗忘**\n本文任务以单轮 VQA 为主。可研究多轮对话中的“指令遵循遗忘”或“安全对齐遗忘”，并引入轮级收敛信号。\n\n3 方法改进\n\n- **温度 T 与窗长 W 的自适应**\n现用固定  T=5, W=10 。可让  T^((t))propto entropy(λ^((t)))  或  W^((t))propto 1/grad-var ，实现“先探索后精调”的自动退火。\n\n- **梯度量级归一化**\n不同目标梯度范数差异大时，仅重加权损失可能不足。可结合 GradNorm/PCGrad 做二级校正，保证参数更新方向不冲突。\n\n- **层级或模块级重加权**\n当前对所有参数共享同一  λ_k^((t)) 。可监测各层梯度一致性，对“通用能力相关层”施加更强正则，对“推理顶层”保持高塑性。\n\n4 评测与可解释性\n\n- **细粒度能力雷达图**\n除总体 Acc 外，可拆解为 object grounding、color recognition、numeracy、robustness to typo 等 20+ 子项，观察 RECAP 是否对某些子项仍失效。\n\n- **机制解释**\n用探测任务 (probing) 检查中间激活，验证回放数据是否真正“巩固”了早期层视觉特征，而非仅输出层格式记忆。\n\n- **遗忘曲线量化**\n借鉴心理学“遗忘曲线”，每 50 步存一次 checkpoint，绘制通用任务性能随推理训练步数的变化  P(t)=Ae^(-bt)+C ，对比 RECAP 与基线的衰减系数  b 。\n\n5 系统与部署\n\n- **推理成本-性能前沿**\n继续压缩思维链长度：结合 speculative decoding 或早停阈值，当  <think>  内熵值低于某门限即截断输出，实现“毫秒级”加速。\n\n- **在线服务中的持续学习**\n在真实对话日志流上部署 RECAP，解决“用户偏好漂移”与“新知识注入”双重挑战，同时防止旧技能（OCR、安全回复）被遗忘。\n\n- **多模态大模型横向扩展**\n验证方法是否适用于音频-文本、视频-文本等不同模态组合，或跨模态检索、grounded captioning 等任务。\n\n6 总结\n\nRECAP 通过“短窗收敛信号+动态优先级”首次在 RLVR 场景同时提升推理与通用能力，但仍留下理论保证、数据选择、自适应策略、细粒度评测、真实部署等开放问题。上述任一方向深入，均可推动大模型后训练从“单一指标最优”走向“多能力可持续增长”。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：RLVR 仅优化可验证推理奖励，导致大模型遗忘预训练获得的感知、OCR、鲁棒性等通用能力。\n- **观察**：不同目标收敛速度差异大，格式奖励 50 步即饱和，推理奖励长期高方差。\n- **方法**：提出 RECAP——在 RLVR 阶段回放通用数据，并用短窗收敛率 c 与不稳定度 i 在线动态重加权各目标损失，无需辅助模型。\n- **结果**：Qwen2.5-VL-3B/7B 上 15 项基准全部提升或持平，通用能力↑2–10 pp，推理能力↑1–3 pp，思维链长度压缩 60 %，零额外参数即插即用。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Beyond%20Reasoning%20Gains%3A%20Mitigating%20General%20Capabilities%20Forgetting%20in%20Large%20Reasoning%20Models，链接是https://arxiv.org/pdf/2510.21978，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.21978。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-19-1761781638513","title":"ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality","description":"Scaling laws research has focused overwhelmingly on English -- yet the most prominent AI models explicitly serve billions of international users. In this work, we undertake the largest multilingual scaling laws study to date, totaling 774 multilingual training experiments, spanning 10M-8B model parameters, 400+ training languages and 48 evaluation languages. We introduce the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual pretraining, which outperforms existing scaling laws' out-of-sample generalization often by more than 0.3 R^2. Our analyses of the experiments shed light on multilingual learning dynamics, transfer properties between languages, and the curse of multilinguality. First, we derive a cross-lingual transfer matrix, empirically measuring mutual benefit scores between 38 x 38=1444 language pairs. Second, we derive a language-agnostic scaling law that reveals how to optimally scale model size and data when adding languages without sacrificing performance. Third, we identify the computational crossover points for when to pretrain from scratch versus finetune from multilingual checkpoints. We hope these findings provide the scientific foundation for democratizing scaling laws across languages, and enable practitioners to efficiently scale models -- beyond English-first AI.","url":"https://huggingface.co/papers/2510.22037","source":"HuggingFace Papers","timestamp":"2025-10-29T23:47:18.513Z","metadata":{"authors":["Shayne Longpre","Sneha Kudugunta","Niklas Muennighoff","I-Hung Hsu","Isaac Caswell","Alex Pentland","Sercan Arik","Chen-Yu Lee","Sayna Ebrahimi"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.CL","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.22037.pdf","arxivUrl":"https://arxiv.org/abs/2510.22037","arxivId":"2510.22037","coolPaperUrl":"https://papers.cool/arxiv/2510.22037","published":"2025-10-24T21:45:22Z","updated":"2025-10-24T21:45:22.000Z","zh_summary":"关于规模定律的研究几乎完全集中在英语上——然而，最突出的人工智能模型明确面向数十亿国际用户。在本研究中，我们开展了迄今为止最大规模的多语言规模定律研究，总计774个多语言训练实验，涵盖从1000万到80亿的模型参数，训练语言超过400种，评估语言达48种。我们提出了用于单语和多语预训练的自适应迁移规模定律（ATLAS），其在样本外泛化能力上通常优于现有规模定律超过0.3的R²。我们对实验结果的分析揭示了多语言学习动态、语言间迁移特性以及多语性的诅咒。首先，我们推导了跨语言迁移矩阵，实证测量了38 x 38=1444个语言对之间的互惠得分。其次，我们推导出一种语言无关的规模定律，揭示在添加语言时如何在不牺牲性能的前提下，优化模型规模和数据量。第三，我们识别了从零预训练与从多语检查点微调的计算临界点。我们希望这些发现为跨语言民主化规模定律提供科学基础，并使从业者能够高效地扩展模型——超越以英语为主的人工智能。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该研究针对“现有缩放定律（scaling laws）研究几乎完全以英语为中心，而主流大模型却面向全球多语用户”这一矛盾，系统性地填补了多语预训练、微调与解码场景下的缩放规律空白。核心待解决问题可归纳为：\n\n- **多语缩放规律缺失**：既无面向任意语言组合的容量-数据联合缩放公式，也无量化的跨语迁移度量，导致从业者无法预判增加语种、扩大模型或增多数据时的性能变化。\n- **“多语诅咒”不可控**：随训练语种数  K  增加，各语损失普遍上升，但缺乏可操作的  N 、 D  补偿规则。\n- **训练策略选择缺乏依据**：对于目标语言，何时应从头预训练、何时应基于多语检查点微调，没有计算-最优决策边界。\n\n论文通过 774 组实验（10 M–8 B 参数、400+ 训练语、48 评估语）建立 **ATLAS**（Adaptive Transfer Scaling Law），一次性解决上述三项痛点，并提供\n\n1. 38×38 经验迁移矩阵，\n2. 扩容  K→rK  时的 iso-loss 封闭解，\n3. 预训练 vs 微调的计算交越公式，\n从而把“英语优先”的缩放定律扩展为真正的“多语通用”缩放定律。\n\nQ2: 有哪些相关研究？\n\n以下研究被论文直接或间接关联，按主题归类并给出核心贡献：\n\n- **通用语言模型缩放定律**\n- Kaplan et al. (2020) 提出  L(N,D)  幂律雏形。\n- Hoffmann et al. (2022) 给出“Chinchilla”形式  E+A/N^α+B/D^β ，成为后续基线。\n- Muennighoff et al. (2024) 引入数据重复场景下的 Data-Constrained Scaling Law（DCSL）。\n- **多语/跨语迁移与诅咒**\n- Conneau et al. (2019) 首次提出“curse of multilinguality”概念。\n- Chang et al. (2024) 在 250 种语言上验证容量瓶颈，但模型小于 45 M。\n- He et al. (2024) 给出 Multilingual Scaling Law（MSL），仅按语系加权，无单语-迁移分离。\n- Protasov et al. (2024) 用 1.2 B 模型测量高-低资源语言迁移，覆盖语对有限。\n- **数据配比与混合缩放**\n- Hashimoto (2021)、Ye et al. (2024)、Xie et al. (2024) 研究多源数据混合的缩放行为，但聚焦英语或单模态。\n- Shukor et al. (2025) 提出英文领域混合的回归型缩放律。\n- **机器翻译缩放**\n- Gordon et al. (2021)、Fernandes et al. (2023) 限定在 2–3 个语对的 NMT 场景，给出翻译专用缩放指数。\n- **微调与继续预训练**\n- Que et al. (2024) 提出 D-CPT Law，研究领域继续预训练，但未涉及多语迁移。\n- **词汇与表示共享**\n- Tao et al. (2024) 提出 vocabulary-insensitive loss，用于公平跨语比较，被本文采用为评估指标。\n\n这些工作共同构成背景，但均未同时解决：\n\n1. 任意语种组合的迁移量化；\n2. 扩容语种时的容量-数据联合补偿；\n3. 预训练 vs 微调的策略选择。\n\nATLAS 在以上三点实现首次统一建模。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“实验-建模-验证”三阶段路线，把多语预训练、微调与解码的缩放问题转化为可预测、可优化的封闭形式，具体步骤如下：\n\n1. 大规模系统实验\n\n- 774 组独立训练：10 M–8 B 参数、单语/双语/多语混合、20–50 种评估语言，覆盖 400+ 训练语。\n- 统一 64 k SentencePiece 词表，采用 vocabulary-insensitive loss 消除词表偏差。\n- 固定超参（WSD 调度、AdamW、2e-4 峰值学习率等），确保对比可比性。\n2. 提出 ATLAS 统一缩放律\n核心公式\n\nL(N,D_(eff))=E+(A) / (N^α)+(B) / (D_(texteff))^β\n\n其中有效数据\n\nD_(eff)=S_λ(D_t;U_t)+∑_(i∈ K_t)τ_i S_λ(D_i;U_i)+τ_(other)S_λ(D_(other);U_(other))\n\n-  S_λ(·;·)  为重复感知饱和函数，自动衰减多 epoch 收益。\n- 分离“目标语数据 + 迁移语数据 + 剩余数据”，用可学习  τ  系数捕捉跨语迁移。\n- 单阶段端到端拟合，无需像 DCSL 分两段。\n3. 量化跨语迁移\n\n- 定义 Bilingual Transfer Score（BTS）：双语 50/50 训练与单语训练达到同等损失所需 token 相对差值。\n- 实测 38×38 语言对，构建迄今最大经验迁移矩阵；用随机森林补全未观测语对（R²=0.85）。\n- 发现“同文同族”迁移对称且正向，异文异族常呈非对称干扰，为后续  τ_i  初始化提供先验。\n4. 建模“多语诅咒”\n扩展 ATLAS 显式引入语种数  K ：\n\nL(K,N,D_t)=L_∞+(A K^φ) / (N^α)+(B K^psi) / (D_t^β)\n\n-  φ>0  量化容量惩罚， psi<0  量化正向迁移带来的数据节省。\n- 拟合得  φ=0.11,psi=-0.04 ，证实诅咒存在但可被迁移部分抵消。\n- 导出 iso-loss 封闭解：当语言覆盖  Kto rK  时，\n\n(N') / (N)=r^(φ/α),quad D'_(tot)D_(tot)=r^(1+psi/β),quad (C') / (C)=r^(1+φ/α+psi/β)\n\n给出计算-最优扩容路径（图 5）。\n5. 预训练 vs 微调决策公式\n\n- 对 48 种语言分别测量“从头训练”与“Unimax 检查点→单语继续预训练”的损失-计算曲线。\n- 记录交越点：当计算预算  Cge 6ND  超过  1.1×10^7· N^(1.65)  FLOPs 时，从头训练开始更优（图 7）。\n- 提供即插即用的预算阈值，无需重复实验即可决策。\n6. 严格泛化验证\n按  R^2_((N)),R^2_((D)),R^2_((C)),R^2_((M))  四维留一验证，ATLAS 在 monolingual 场景  R^2_((N))=0.88 ，多语场景  R^2_((M))=0.82 ，均显著优于 CSL、DCSL 与 MSL（表 1）。\n\n通过上述步骤，论文把原本依赖经验或直觉的多语模型设计问题，转化为可直接外推的量化公式，实现“增加语种-不损性能-最小计算”的闭环优化。\n\nQ4: 论文做了哪些实验？\n\n论文共执行 **774 组独立训练实验**，按研究问题拆分为 5 大实验板块，覆盖 10 M–8 B 参数、20–50 种语言、单语/双语/多语混合、从头预训练与微调全场景。具体配置如下（均使用 64 k SentencePiece 词表、WSD 调度、AdamW 优化器，统一超参保证可比性）：\n\n1. Monolingual Scaling（140 组）\n\n- 语言：en, fr, ru, zh, hi, sw, yo\n- 变量：20 档模型规模（10 M–8 B）\n- 对比：单语词表 vs 多语词表 × 单语数据\n- 目的：拟合各语言专属 α,β 并验证 ATLAS 重复感知项。\n2. Multilingual Vocabulary + Monolingual Data（70 组）\n\n- 同上 7 语言，改用多语词表，仍只喂单语数据\n- 目标：量化“词表多语化”本身带来的 compute-tax。\n3. Unimax Massively Multilingual（20 组）\n\n- 固定 420 语言 Unimax 配比，1 T tokens\n- 规模：20 档参数\n- 用途：提供后续微调与迁移测量的统一检查点。\n4. Language-Pair Transfer（240 组）\n\n- 双语 50/50 混合：10 选 2 → 45 对，每对 2 B 模型，d=42 B tokens\n- 扩展：再测 15 组重点语对 × 11 档规模 → 165 组\n- 额外：48 语言各测 1 次微调适配实验 → 48 组\n- 产出：实测 90 语对 BTS，余下 1 354 语对用随机森林外推，得到 38×38 迁移矩阵。\n5. Curse-of-Multilinguality（120 组）\n\n- 固定 25 B tokens，语种数 K=4,6,8,12,16,24,32,50 八档\n- 每 K 档测 11 档模型规模 → 88 组\n- 额外：固定 2 B 参数，总 tokens 25 B–125 B 五档 → 30 组\n- 目标：拟合 ϕ,ψ 指数并验证 iso-loss 公式。\n6. Finetune-vs-Scratch（130 组）\n\n- 以 Unimax 1 T 检查点为起点，对 48 种语言分别继续单语预训练（30 k+ steps）\n- 同步记录 48 语验证损失，与“从头训练”曲线对比\n- 规模：7 档参数 × 12 条语言（7+10 重叠） → 84 组；余下 48 语各 1 组 → 130 组\n- 产出：图 6 交越点与图 7 幂律 log C=1.1×10⁷·N¹·⁶⁵。\n7. 评估与验证\n\n- 48 语言各留 20 M token 词汇不敏感测试集，用于全部损失计算。\n- 四维留一 R²：最大模型、最大数据、最大计算、未见语言混合，共 2 400+ 外推点。\n\n总计 140+70+20+240+120+130 = **774 组独立训练**，构成迄今最大规模的多语缩放实验矩阵，支撑 ATLAS 定律、迁移矩阵、诅咒建模与策略选择四项核心贡献。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“理论-模型”“数据-训练”“评测-应用”三大主题，并给出可验证假设与潜在实验设计：\n\n理论-模型\n\n1. **非均匀采样下的 ATLAS 扩展**\n\n- 假设：当语言采样权重 ∝ 资源量^γ 时，γ 存在与模型规模相关的最优轨迹。\n- 实验：固定 K=50，在 100 M→2 B 区间扫描 γ∈\n0,1\n，观察 ψ 是否由负转正，并拟合 γ\\*(N)。\n2. **多语与多模态联合缩放**\n\n- 假设：视觉/音频编码器引入后，ϕ 会额外依赖模态数 M，即 ϕ=ϕ₀+ϕ₁·M。\n- 实验：在 1 B 图文平行数据上，保持文本 K=20，逐步增加 1–3 个模态，检验 Δϕ。\n3. **“迁移系数可压缩性”**\n\n- 假设：τ\\_i 可进一步分解为“脚本距离 + 语族距离 + 词汇重叠”三因子低秩近似。\n- 实验：用 38×38 矩阵做矩阵分解，若秩 ≤4 仍能保持 R²≥0.9，则可用 3×38 参数替代 38×38 表。\n\n数据-训练\n\n4. **课程学习与动态 τ\\_i**\n\n- 假设：训练初期高迁移语占比高，后期逼近目标语，可让 τ\\_i 随步数 t 衰减，减少最终损失。\n- 实验：设计 τ\\_i(t)=τ\\_i·exp(−t/T₀)，与固定 τ\\_i 对比，测量同计算预算下的 ΔL。\n5. **数据重复 λ 的语言依赖性**\n\n- 假设：低资源语对重复更敏感，λ 应与 Unique-token 比率 U/D 成反比。\n- 实验：在 sw、hi、en 上分别扫描 λ∈\n0.01,0.5\n，看最优 λ 是否与 U/D 线性相关。\n6. **参数高效微调 (LoRA/AdaLoRA) 的缩放**\n\n- 假设：当可训练参数量 < 1 % N 时，诅咒指数 ϕ 不再适用，而出现新的瓶颈指数 ϕ\\_peft。\n- 实验：对 2 B 模型采用秩 r∈\n1,128\n 的 LoRA，测量 K=10→40 时的损失增长斜率，拟合 ϕ\\_peft(r)。\n\n评测-应用\n\n7. **下游任务迁移矩阵**\n\n- 假设：BTS 高的语对，在 QA/NER 任务上也呈正迁移，但幅度受任务类型调节。\n- 实验：在 10×10 语对 + 3 类任务（抽取式 QA、NER、情感分类）上建立“任务级迁移矩阵”，看与 BTS 的秩相关系数。\n8. **人类可读稀疏解释**\n\n- 假设：用 LIME 或注意力 rollout 可在子词层面解释 τ\\_i 的正负号。\n- 实验：对 es→pt（正）与 en→zh（负）采样 1 k 句，可视化共享子词是否对应高注意力贡献。\n9. **推理阶段多语解码策略**\n\n- 假设：当生成语言 ≠ 训练高权重语时，动态缩放 logits 可补偿诅咒。\n- 实验：在 K=50 的 2 B 模型上，为低资源语引入温度缩放因子 ΔT=α·(1−w\\_lang)，扫描 α 看 BLEU 变化。\n10. **“小模型大覆盖”极限实验**\n\n- 假设：若 ψ 足够负，存在 K\\_max(N) 使继续增加语言不再提升总损失。\n- 实验：在 100 M 模型上持续增语言至 K=200，观察损失是否收敛，拟合 K\\_max∝N^ξ。\n\n可立即验证的“小步快跑”\n\n- 用已发布 38×38 矩阵 + 随机森林代码，替换特征为“字节级编辑距离 + 语族 one-hot”，看是否保持 ρ≥0.85。\n- 在 2 B 模型上仅调整 λ∈{0.05,0.1,0.2}，重跑 sw、vi 单语曲线，验证 ATLAS 重复项对 R² 的贡献是否 > 0.03。\n\n以上任何一条均可直接利用论文开源的 774 组日志与检查点作为基线，无需从头训练，即可在 1–2 周内得到初步结论。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有缩放定律几乎只研究英语，而主流大模型需服务全球多语用户；缺乏跨语迁移度量、多语诅咒量化及“预训练 vs 微调”决策公式。\n- **方法**：在 10 M–8 B 参数、400+ 训练语、48 评估语上完成 774 组独立实验，统一词表与超参，提出 **ATLAS**（Adaptive Transfer Scaling Law）。\n\n- **模型**：\n- 核心式  L(N,D_(eff))=E+(A) / (N^α)+(B) / (D_(texteff))^β \n- 有效数据  D_(eff)  分解“目标语 + 迁移语 + 剩余语”并引入重复感知饱和函数，可学习迁移系数  τ_i 。\n- **发现**：\n\n1. 38×38 经验迁移矩阵：同文同族语对高度对称且正向，异文异族常呈非对称干扰。\n2. 多语诅咒可建模为  L(K,N,D)=L_∞+(AK^φ) / (N^α)+(BK^psi) / (D^β) ，拟合得  φ=0.11,psi=-0.04 ；给出 iso-loss 封闭解：扩容  Kto rK  需  N'/N=r^(φ/α),D'_(tot)/D_(tot)=r^(1+psi/β) 。\n3. 预训练 vs 微调交越：当计算预算  Cge 1.1×10^7· N^(1.65)  时，从头训练更优。\n- **效果**：ATLAS 在 monolingual 与 multilingual 场景四维留一 R² 分别达 0.88 与 0.82，显著优于 Chinchilla、DCSL、MSL 等基线。\n- **贡献**：首次把“英语优先”缩放定律扩展为可预测任意语种组合、可量化诅咒、可决策训练策略的多语通用缩放框架。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ATLAS%3A%20Adaptive%20Transfer%20Scaling%20Laws%20for%20Multilingual%20Pretraining%2C%20Finetuning%2C%20and%20Decoding%20the%20Curse%20of%20Multilinguality，链接是https://arxiv.org/pdf/2510.22037，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.22037。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}