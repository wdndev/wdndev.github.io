{"timestamp":"2025-10-13T23:51:12.888Z","totalItems":20,"items":[{"id":"hf-0-1760399244927","title":"D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI","description":"Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/","url":"https://huggingface.co/papers/2510.05684","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.927Z","metadata":{"authors":["Suwhan Choi","Jaeyoon Jung","Haebin Seong","Minchan Kim","Minyeong Kim","Yongjun Cho","Yoonshik Kim","Yubeen Park","Youngjae Yu","Yunsung Lee"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.AI","cs.CV","cs.RO"],"pdfUrl":"https://arxiv.org/pdf/2510.05684.pdf","arxivUrl":"https://arxiv.org/abs/2510.05684","arxivId":"2510.05684","coolPaperUrl":"https://papers.cool/arxiv/2510.05684","published":"2025-10-07T08:40:33Z","updated":"2025-10-07T08:40:33.000Z","zh_summary":"大型语言模型利用互联网级别的文本数据，而具身人工智能仍受到物理轨迹采集高昂成本的限制。桌面环境——尤其是游戏——提供了一个引人注目的替代方案：它们在大规模下提供丰富的感知运动交互，同时保持对具身学习至关重要的结构化观测-动作耦合。我们提出了 D2E（Desktop to Embodied AI，桌面到具身人工智能）框架，展示了桌面交互可以作为机器人具身 AI 任务的有效预训练基础。与之前工作仍局限于特定领域（如面向《Minecraft》的 VPT）或保持数据私有（如 SIMA）不同，D2E 建立了从可扩展的桌面数据收集到验证性具身领域迁移的完整流水线。我们的框架包括三个组成部分：（1）OWA 工具包，将多样的桌面交互统一为标准化格式，实现 152 倍压缩；（2）Generalist-IDM，通过基于时间戳的事件预测，在未见过的游戏中实现强大的零样本泛化能力，从而实现互联网级别的伪标签生成；（3）VAPT，将桌面预训练的表示迁移到物理操作和导航中。使用 1300+ 小时的数据（259 小时人工演示，1K+ 小时伪标签游戏游玩），我们在 LIBERO 操作基准上实现了总成功率 96.6%，在 CANVAS 导航基准上实现 83.3%。这验证了数字交互中的感知运动原语具有足够的不变性，可以有意义地迁移到物理具身任务，确立了桌面预训练作为机器人实际可行的范式。我们将公开所有工作，包括 OWA 工具包、人类收集和伪标注的数据集，以及 VAPT 训练模型，可在 https://worv-ai.github.io/d2e/ 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在破解“具身智能难以像大语言模型那样利用互联网级数据”的核心瓶颈。\n具体而言，它聚焦以下问题：\n\n- **物理轨迹获取成本极高**：真实机器人数据需专用硬件、人力操作与安全环境，规模受限，无法形成“数据飞轮”。\n- **桌面交互蕴含丰富传感器-运动耦合**：游戏等数字环境提供标准化、大规模、人类中心的观测-动作对，但此前缺乏统一采集格式与跨域验证。\n- **互联网视频动作标签缺失**：YouTube 等虽有海量游戏录像，却因无动作标注而难以直接用于预训练。\n\n为此，作者提出 **D2E 框架**，验证“桌面数字交互可作为具身智能预训练的廉价且可扩展基底”，并通过以下手段解决上述问题：\n\n1. **统一采集与压缩**：OWA Toolkit 将 31 款游戏 335 小时人类演示压缩 152×，解决存储与格式碎片化。\n2. **通用伪标签**：Generalist-IDM 以时间戳事件预测实现跨游戏零样本泛化，自动为 1 000+ 小时 YouTube 视频生成动作标签，突破人工标注瓶颈。\n3. **桌面到物理迁移**：VAPT 将桌面预训练表示迁移至机器人操纵与导航，在 LIBERO 与 CANVAS 基准分别取得 96.6 % 与 83.3 % 成功率，首次实证数字传感器-运动原语对物理任务具有足够不变性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将与自身最密切的文献归为两条主线，并指出其差异。以下按这两条主线梳理相关研究，并补充 D2E 的定位。\n\n1. 大规模 Vision-Action（或 VLA）预训练的数据来源\n\n- 真实机器人数据\n- RT-1 / RT-2（Brohan et al., 2022；Zitkovich et al., 2023）\n- Open X-Embodiment & RT-X（O’Neill et al., 2024）\n- LeRobot（Cadene et al., 2024）\n- Octo（Octo Model Team et al., 2024）\n- π0（Black et al., 2024）\n共同点：强调机器人本体采集，规模受硬件与安全限制。\n- 桌面/游戏数据\n- VPT（Baker et al., 2022）——仅 Minecraft，人工+伪标签，未跨域。\n- SIMA（Raad et al., 2024）——跨游戏但数据与接口闭源。\n- PLAICraft（He et al., 2025）——Minecraft 多模态日志，环境单一。\n- GUI-world（Chen et al., 2025）——GUI 视频基准，未研究机器人迁移。\nD2E 差异：开源、跨 31 款游戏、统一 OWAMcap 格式，并验证桌面→物理迁移。\n2. 逆动力学模型（IDM）用于互联网规模伪标签\n\n- 机器人视频\n- UniPi（Du et al., 2023）——文本引导视频生成+策略学习。\n- LAPA（Ye et al., 2024）——潜在动作预训练。\n- 桌面/游戏视频\n- VPT 的 Specialist-IDM——固定 50 ms 窗口，仅 Minecraft。\n- IDM-K（Tot et al., 2025）——利用未来帧，但编码完整轨迹，计算量大。\nD2E 提出 Generalist-IDM：\n- 时间戳事件驱动，避免“no-op”冗余；\n- NEP-τ 目标仅前移观测窗口 τ 步，兼顾未来信息且轻量；\n- 零样本泛化到新游戏，实现 1000+ 小时 YouTube 伪标签。\n\n简言之，D2E 在“数据来源”上把桌面交互正式确立为机器人预训练的替代 substrate，并在“伪标签工具”上首次给出开源、跨游戏、可扩展的 Generalist-IDM，从而将两条先前独立的研究路线合并为一条完整 pipeline。\n\nQ3: 论文如何解决这个问题？\n\n论文把“桌面交互→具身智能”这一设想拆成三段可执行的技术链路，每段对应一个模块化组件，依次解决**采集、标注、迁移**三大痛点。\n\n1\\. OWA Toolkit：把碎片化的桌面轨迹变成“机器人级”统一数据\n\n| 关键障碍 | 论文对策 | 量化收益 |\n| --- | --- | --- |\n| 无跨应用统一格式，存储浪费 | 提出 OWAMcap 双层格式：• MCAP 容器存事件元数据（纳秒同步、崩溃安全）• MediaRef 外链 H.265 视频，217× 压缩 | 335 h 数据从 1.06 TiB → 7.12 GiB（152×） |\n| 录制工具只重画质、不重同步 | ocap 录制器用 Windows DXGI/WASAPI 硬编，60 Hz 同步屏幕+键鼠+音频 | CPU 占用↓6×，单卡即可 FHD 实时采集 |\n\n2\\. Generalist-IDM：用“时间戳事件预测”把 YouTube 变成免费动作标签机\n\n| 关键障碍 | 论文对策 | 量化收益 |\n| --- | --- | --- |\n| 逐 tick 预测浪费上下文，且只能单游戏 | 提出 NEP-τ 目标： L_(NEP-τ) = -E∑_(t=1)^T log P_θ!l(a_tmid o_(1:min(t+τ,T)),a_(1:t-1)r) 事件驱动，跳过无动作帧，零样本跨域 | 31 款训练游戏内平均键盘准确率 +28.8%；2 款未见游戏键盘准确率×2.3 |\n| 人工标注成为规模天花板 | 用上述模型直接给 1 055 h YouTube gameplay 伪标签，无需人工 | 数据总量扩至 1.3k 小时，成本 ≈ 0 |\n\n3\\. VAPT：把桌面预训练权重“热启动”到机器人下游\n\n| 关键障碍 | 论文对策 | 量化收益 |\n| --- | --- | --- |\n| 数字→物理域差异大，迁移无保证 | 保持 InternVL3-1B 结构，仅替换 tokenizer 为事件 token；先在 1.3k h 桌面数据做 NEP-τ 预训练，再少量机器人数据微调 | LIBERO 操纵 96.6 %（+11.8 vs 基线）CANVAS 导航 83.3 %（+8.0 vs 基线） |\n\n总结\n\n通过“**统一采集→通用伪标签→下游微调**”三段式 pipeline，论文把桌面游戏这一廉价、互联网级资源正式转化为可替代真实机器人大数据的预训练基底，首次在标准具身基准上验证了数字传感器-运动原语对物理任务的可迁移性。\n\nQ4: 论文做了哪些实验？\n\n论文围绕三条技术链路分别设计实验，既验证“方法本身有效”，也验证“对机器人下游任务有用”。所有实验代码、数据、脚本已随仓库开源（Reproducibility Statement）。\n\n1\\. OWA Toolkit 实验\n\n**目的**：证明新格式/录制器在压缩率、I/O、训练吞吐上全面优于现有方案。\n\n- **压缩对比**\n- VPT 原数据集 1.06 TiB → OWAMcap 7.12 GiB （152×）\n- CS:GO 原数据集 689 GiB → OWAMcap 20 GiB （34×）\n- **解码吞吐 & 磁盘读**（表 1）\n640×360@20Hz Minecraft 64 段视频，统一序列长度 4096 token：\n\n- 基线单帧解码：11.68 img/s，63.46 KB/img\n- 优化 x264 + 自适应批解码：**119.16 img/s，18.73 KB/img** （10.2× 吞吐，3.4× 省 IO）\n- **训练吞吐**（表 2）\n单 H100 上 InternVL3-1B 微调：\n\n- 基线需 16 worker 才到 4.55 it/s\n- OWA 管道 **1 worker 4.77 it/s**（16× 人力省）\n\n2\\. Generalist-IDM 实验\n\n**目的**：证明“同一套权重”跨游戏、跨域、跨鼠标灵敏度仍能准确反推动作。\n\n2.1 域内评估（31 款训练游戏内抽 6 款）\n\n指标：Pearson 相关、Scale-Ratio、键盘/鼠标准确率\n\n- 2D 游戏（Stardew Valley 等）键盘准确率 **+28.8%** 平均\n- 3D FPS（Apex、GTA、Minecraft）鼠标 Pearson **+19.4** 平均\n\n2.2 域外评估（训练时完全未见）\n\n| 游戏 | Specialist-IDM | Generalist-IDM 零样本 | +少量前缀(FS) |\n| --- | --- | --- | --- |\n| Battlefield 6 (3D FPS) | 62.4 % 键盘 | 47.8 % → 52.6 % (FS) | 鼠标尺度误差从 3.56× 降到 1.05× |\n| Ogu Forest (2D 冒险) | 11.7 % 键盘 | 27.8 % 零样本 |\n\n2.3 伪标签规模\n\n- 用上述零样本模型跑 1 055 h YouTube 视频 → 生成 20 款游戏伪标签（表 12）。\n- 无人工过滤 HUD/菜单，仍被模型自动消化。\n\n3\\. VAPT 下游迁移实验\n\n**目的**：桌面预训练是否能“热启动”真实机器人基准，且伪标签在不同任务中的价值差异。\n\n3.1 机器人操纵 — LIBERO 基准\n\n500 episode/ 套件，共 4 套件 + 长时程 LIBERO-10\n\n| 方法 | 参数量 | Spatial | Object | Goal | Long-10 | Total |\n| --- | --- | --- | --- | --- | --- | --- |\n| 基线 InternVL3-1B | 1 B | 94.4 | 97.0 | 93.6 | 54.2 | 84.8 |\n| VAPT 仅人类 259 h | 1 B | 95.8 | 98.4 | 98.6 | 93.6 | 96.6 |\n| VAPT + 伪标签 1 kh | 1 B | 89.6 | 98.2 | 93.8 | 87.2 | 92.2 |\n| 对比：OpenVLA-7B | 7 B | 84.7 | 88.4 | 79.2 | 53.7 | 76.5 |\n\n→ **1 B 桌面预训练模型击败 7 B 机器人专用模型**，且伪标签对**精细操纵无益**（文中给出假设：操纵更依赖高精度人工动作）。\n\n3.2 机器人导航 — CANVAS 基准\n\n150 episode，5 模拟环境 × 两种指令（精确/误导）\n\n| 设置 | 总体成功率 | 误导场景提升 |\n| --- | --- | --- |\n| 基线 | 75.3 % | – |\n| VAPT 仅人类 | 75.3 % | – |\n| VAPT + 伪标签 | 83.3 % | +20 %（sim_orchard, sim_street_sidewalk） |\n\n→ 伪标签在**高层规划型任务**（导航）上价值显著，与操纵任务形成对比。\n\n4\\. 消融与效率\n\n- **τ 选择**：τ = 3 在验证集 NEP 损失最低，后续实验固定。\n- **训练成本**：Generalist-IDM 8×H100 24 h ≈ 800 USD；VAPT 单节点 8×A100 数小时完成。\n\n实验结论\n\n1. OWA 格式/管道把“存储-读取-训练”瓶颈降到可消费级。\n2. Generalist-IDM 首次实现“同一权重”跨游戏零样本反推动作，并支撑 1 kh 级伪标签。\n3. 桌面预训练可**直接**提升物理机器人标准基准，且数据类型（人工/伪标签）对不同下游任务呈现**差异化价值**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下问题在论文“Limitations”与实验结果中已露端倪，值得后续深入：\n\n1\\. 真实机器人验证\n\n- 目前只在 **LIBERO/CANVAS 仿真** 完成迁移；需验证桌面预训练对**真实硬件**的样本效率、鲁棒性与安全影响。\n- 可探索\\*\\* sim-to-real 鸿沟\\*\\*是否与“桌面-to-sim”误差累积，需研究联合域随机化或自适应微调策略。\n\n2\\. 任务相关的伪标签价值机制\n\n- 实验显示伪标签**提升导航却降低操纵**；需量化分析任务对“动作精度 vs 场景多样性”的敏感度，建立**选择准则**决定何时引入伪标签。\n- 可引入**不确定性加权**或**质量筛选器**（基于 IDM 置信度、动作一致性、视觉-语言对齐度）动态过滤伪标签。\n\n3\\. 事件表示与动作空间泛化\n\n- 当前仅离散键盘+鼠标；可扩展至**触控、手柄、VR 6-DoF** 等多模态输入，验证事件 token 方案是否仍足够紧凑。\n- 探索**连续动作回归头**与离散 token 的混合架构，兼顾高精度低延迟控制。\n\n4\\. 时间偏移 τ 与因果性\n\n- NEP-τ 利用未来 τ 步观测；在**实时机器人闭环**中未来帧不可见，需研究**τ=0 在线版本**或**预测式未来编码器**以维持性能。\n- 可引入**因果约束掩码**逐步降低 τ，实现“离线预训练 → 在线部署”平滑过渡。\n\n5\\. 跨平台大规模采集\n\n- 当前 OWA 基于 Windows API；可移植到**X11/Wayland、Android、WebGL**，构建真正“全平台”桌面-移动端数据集，检验通用性。\n- 探索**联邦采集**模式：志愿者仅上传 OWAMcap 元数据，视频留在本地，兼顾隐私与规模。\n\n6\\. 与 VLM/VLA 基础模型的深度融合\n\n- 目前仅替换 InternVL3 的 tokenizer；可继续**保留 VLM 的图文对齐权重**，在桌面数据上做**continual multimodal pretraining**，观察是否能同时提升语言-视觉-动作三端。\n- 探索**多任务联合目标**：NEP-τ + 字幕生成 + 下一帧预测，看是否出现**涌现的跨模态推理**能力。\n\n7\\. 数据配比与Scaling Law\n\n- 固定 259 h 人工 + 1 kh 伪标签；需系统实验**人工/伪标签比例、游戏类型分布、视频长度**对下游成功率的影响，拟合“桌面数据规模-机器人性能”曲线。\n- 检验是否存在**临界数据量**，超过后边际收益骤降，指导低成本采集预算。\n\n8\\. 安全与伦理双重用途\n\n- 桌面级键盘鼠标预测具备**潜在恶意控制**风险；需开发**对抗性过滤**与**使用审计**机制，并发布**负责任使用协议**。\n- 研究**差分隐私事件编码**，在不影响迁移性能前提下隐藏用户击键/鼠标生物特征。\n\n9\\. 自动课程与任务生成\n\n- 利用**大模型生成游戏关卡/任务脚本**，实现**自动课程**；检验桌面智能体能否通过课程零样本迁移到**更长周期、多阶段机器人任务**。\n- 探索**可逆环境生成**（ProcGen/Minecraft 脚本）产生无限场景，测试 IDM 与策略的**持续学习**能力。\n\n10\\. 低资源 democratization\n\n- 验证**1×RTX 4090 或 MacBook M 系列**是否能复现 Generalist-IDM 训练（当前 8×H100）；通过**梯度检查点、LoRA、QLoRA、8-bit 优化器**降低门槛。\n- 发布**小型基准子集**（如 50 h 覆盖 5 游戏）供学术实验室快速验证假设，形成社区迭代闭环。\n\n这些方向若取得突破，可进一步巩固“桌面即互联网级具身预训练”范式，并推动通用机器人基础模型的平民化。\n\nQ6: 总结一下论文的主要内容\n\n论文核心速览\n\n**题目**：D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI\n**目标**：用廉价、互联网规模的桌面交互替代昂贵的真实机器人数据，实现具身智能预训练“飞轮”。\n\n1\\. 要解决的关键障碍\n\n- 真实轨迹采集贵（硬件、人力、安全）→ 数据规模受限\n- 桌面/游戏数据丰富但格式碎片化、无统一动作标签 → 难以直接预训练\n- 域差异大 → 数字传感器-运动原语能否迁移到物理机器人未知\n\n2\\. 三段式技术方案\n\n| 模块 | 关键创新 | 量化收益 |\n| --- | --- | --- |\n| OWA Toolkit | • ocap：Win API 60 Hz 同步录屏+键鼠+音频• OWAMcap：MCAP+H.265，152×压缩 | 335 h 数据从 1.06 TiB → 7.12 GiB；解码吞吐↑10×，磁盘读↓41× |\n| Generalist-IDM | • 事件级 token + 时间戳 NEP-τ 目标• 零样本跨游戏反推动作 | 31 款训练游戏键盘准确率+28.8%；未见游戏×2.3；自动伪标签 1 055 h YouTube |\n| VAPT | 桌面 1.3k h 预训练 → 机器人微调 | LIBERO 操纵 96.6 %（+11.8），CANVAS 导航 83.3 %（+8.0）；1 B 参数击败 7 B 专用模型 |\n\n3\\. 实验验证\n\n- **OWA**：压缩、随机访问、训练吞吐全面优于 TorchCodec/JSONL/HDF5\n- **IDM**：域内/域外游戏均显著超越单游戏 Specialist-IDM；伪标签规模成本≈0\n- **机器人**：仿真基准上首次证实“桌面预训练→物理任务”正向迁移，且伪标签对导航增益高、对精细操纵无益\n\n4\\. 结论与影响\n\n- 桌面交互可作为具身智能的**互联网级预训练基底**\n- 开源全链路工具+数据+模型，训练成本＜800 USD，存储省两个数量级\n- 为社区提供“免硬件”范式，推动通用机器人基础模型平民化\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是D2E%3A%20Scaling%20Vision-Action%20Pretraining%20on%20Desktop%20Data%20for%20Transfer%20to%20Embodied%20AI，链接是https://arxiv.org/pdf/2510.05684，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.05684。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1760399244930","title":"Thinking with Camera: A Unified Multimodal Model for Camera-Centric Understanding and Generation","description":"Camera-centric understanding and generation are two cornerstones of spatial intelligence, yet they are typically studied in isolation. We present Puffin, a unified camera-centric multimodal model that extends spatial awareness along the camera dimension. Puffin integrates language regression and diffusion-based generation to interpret and create scenes from arbitrary viewpoints. To bridge the modality gap between cameras and vision-language, we introduce a novel paradigm that treats camera as language, enabling thinking with camera. This guides the model to align spatially grounded visual cues with photographic terminology while reasoning across geometric context. Puffin is trained on Puffin-4M, a large-scale dataset of 4 million vision-language-camera triplets. We incorporate both global camera parameters and pixel-wise camera maps, yielding flexible and reliable spatial generation. Experiments demonstrate Puffin superior performance over specialized models for camera-centric generation and understanding. With instruction tuning, Puffin generalizes to diverse cross-view tasks such as spatial imagination, world exploration, and photography guidance. We will release the code, models, dataset pipeline, and benchmark to advance multimodal spatial intelligence research.","url":"https://huggingface.co/papers/2510.08673","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.930Z","metadata":{"authors":["Kang Liao","Size Wu","Zhonghua Wu","Linyi Jin","Chao Wang","Yikai Wang","Fei Wang","Wei Li","Chen Change Loy"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.08673.pdf","arxivUrl":"https://arxiv.org/abs/2510.08673","arxivId":"2510.08673","coolPaperUrl":"https://papers.cool/arxiv/2510.08673","published":"2025-10-09T17:59:29Z","updated":"2025-10-09T17:59:29.000Z","zh_summary":"以相机为中心的理解和生成是空间智能的两大支柱，但它们通常是分开研究的。我们提出了 Puffin，这是一个统一的以相机为中心的多模态模型，可以沿相机维度扩展空间感知。Puffin 集成了语言回归和基于扩散的生成方法，以从任意视角解读和创建场景。为了弥合相机与视觉语言之间的模态差距，我们提出了一种将相机视为语言的全新范式，从而实现以相机进行思考。这引导模型将空间基础的视觉线索与摄影术语对齐，同时在几何上下文中进行推理。Puffin 在 Puffin-4M 上进行训练，这是一个包含 400 万视觉-语言-相机三元组的大规模数据集。我们同时整合了全局相机参数和逐像素相机映射，从而实现灵活且可靠的空间生成。实验表明，Puffin 在以相机为中心的生成与理解任务中表现优于专用模型。通过指令微调，Puffin 能泛化到多样的跨视角任务，如空间想象、世界探索和摄影指导。我们将发布代码、模型、数据集流程和基准，以推动多模态空间智能研究的发展。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在**统一相机中心理解与生成**这两个长期被割裂的任务，提出一个多模态框架 Puffin，使机器能够：\n\n- **从任意单张图像推断相机参数**（理解端）\n- **根据文本描述与相机参数生成对应视角的图像**（生成端）\n\n核心挑战在于**相机参数与视觉-语言模态之间的巨大语义鸿沟**：\n\n- 相机参数（如 roll/pitch/FoV）是低维数值，缺乏直观语义；\n- 现有 LMM 要么忽略这些参数，要么将其粗暴地当作额外标签，导致空间不一致。\n\n为此，作者提出“**把相机当作语言**”的新范式，通过\n\n1. **thinking with camera** 机制，将数值参数映射为摄影术语（如 Dutch angle、tilt-up），再与视觉线索对齐，实现可解释的空间推理；\n2. **离散相机 token + 连续相机 latent（像素级相机图）** 的双路表征，兼顾全局控制与局部几何细节；\n3. **统一自回归+扩散架构**，让理解与生成共享同一套相机-语言表示，实现双向增强。\n\n最终，Puffin 在单目相机标定、可控图像生成以及跨视角空间想象、世界探索、摄影指导等下游任务上，均超越专用模型或强基线，验证了“相机即语言”这一统一视角的有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第2节“Related Work”中系统梳理了三条主线，并指出它们与本文任务的差距：\n\n1. 大视觉-语言模型（Large Multimodal Models, LMMs）\n\n- 代表工作：BLIP-2、LLaVA、Qwen-VL、Cambrian-1、Emu3、Janus、Show-o 等\n- 共性：视觉编码器+大语言模型，擅长高层语义问答、检测、OCR\n- 缺口：对深度、几何、相机内参/外参几乎无先验，难以完成空间精确定位或相机参数回归\n2. 统一多模态理解与生成（Unified Multimodal Models）\n\n- 自回归路线：Chameleon、Emu3、VILA-U、MetaMorph 等把图像离散成 token，与文本同序列自回归\n- 扩散路线：UniDiff、Janus、U-ViT、Show-o2、Transfer-MQ 等用 LLM 隐状态条件扩散模型\n- 缺口：一律默认“正面、固定 FoV”的简化相机假设，无法按任意 roll/pitch/FoV 进行可控生成，也不能反向推断相机\n3. 单目相机几何估计（Camera Geometry from Vision）\n\n- 传统方法：Pollefeys 自标定、Hartley-Zisserman 多视几何、Lochman 最小求解器、Pautrat 的 UVP（消失点+重力先验）\n- 深度学习方法：\n– 直接回归参数：DeepCalib、Perceptual、CTRL-C、ParamNet、GeoCalib\n– 中间几何表征：Perspective Field、Distortion Distribution Map、Ray-based 方法、Incidence Field 等\n- 缺口：只能“理解”不能“生成”，且表征多为低/中层局部特征，缺乏与高层语义的协同推理\n\n本文首次把上述三条线纳入同一框架，通过“相机即语言”的 thinking 机制填补相机参数与视觉-语言模态之间的语义鸿沟，实现相机-centric 的统一理解与生成。\n\nQ3: 论文如何解决这个问题？\n\n论文将“相机中心理解”与“相机可控生成”统一为同一多模态任务，通过三项关键设计解决“相机参数-视觉-语言”模态鸿沟：\n\n1. 把相机写成自然语言——“thinking with camera”\n\n- 参数-术语映射：将数值 roll/pitch/FoV 量化成专业摄影词汇（Dutch angle、tilt-up、wide-angle 等）。\n- 空间推理链：强制模型在 `<think>` 段落里先“说人话”——用视觉线索（地平线、消失线、天空占比等）对齐摄影术语，再在 `<answer>` 输出精确参数。\n- 统一提示模板：理解与生成共用同一套“术语-线索”语言，降低跨任务迁移代价。\n2. 双路相机表征——离散 token + 连续 latent\n\n- 离散相机 token：把 (roll, pitch, yaw, FoV) 经 tokenizer 变成与文本同序列的 ID，供 LLM 直接自回归。\n- 连续相机 latent：将像素级相机图（Perspective Field，每像素含 up-vector 与 latitude）经 VAE 编码成 512×512×3 的 latent，作为扩散模型的条件图。\n- 连接器：64 个可学习 query 把 LLM 隐状态压缩成 6 层 Transformer 的条件向量，同步注入扩散 U-Net，实现“语义+几何”双通路控制。\n3. 四阶段统一训练策略\n\n- Stage I 对齐：只训 MLP 投影器与 query-connector，冻结其他权重，让视觉-文本-相机 token 对齐。\n- Stage II 联合微调：解冻 LLM 与扩散模型，同时优化理解（交叉熵）与生成（扩散）损失，建立共享空间表征。\n- Stage III 引入 thinking：在数据集中加入“空间推理”标注，强制模型输出 `<think>` 段落，进一步缩小语义鸿沟。\n- Stage IV 指令微调：加入跨视角数据（spatial imagination / world exploration / photographic guidance），让单一模型即可处理单视角标定、任意视角生成、摄影建议等多任务。\n\n通过“语言化相机+双路表征+统一训练”，Puffin 在 4M 规模数据集上实现：\n\n- 理解端：roll/pitch/FoV 误差低于所有专用标定网络；\n- 生成端：相机图误差比最强基线 PreciseCam 降低 46%，且风格多样性更高；\n- 下游：零样本完成 3D 物体插入、环绕式世界探索、摄影指导等任务。\n\nQ4: 论文做了哪些实验？\n\n论文在 §5 实验部分围绕三条主线展开，共构建/使用了 4 个评测集，给出定量指标、定性可视化与消融分析，并展示下游应用。\n\n1. 相机理解（Camera Understanding）\n评测指标\n\n- 误差：roll/pitch/FoV 的中位角度误差（°）\n- 召回：AUC@1°/5°/10°\n\n评测数据集\n\n- 公开基准：MegaDepth、TartanAir、LaMAR\n- 自建挑战集：Puffin-Und（1 000 张，大姿态/弱纹理）\n\n对比方法\n\n- 纯视觉深度法：DeepCalib、Perceptual、CTRL-C、MSCC、ParamNet、GeoCalib（SOTA）\n- 传统几何法：SVA、UVP\n\n主要结果（表 3）\n\n- 在 MegaDepth 与 Puffin-Und 上三项误差全面最低；LaMAR 上 roll 误差 0.38° 与 GeoCalib 相当，pitch/FoV 更优。\n- AUC 曲线显著高于所有基线，验证“thinking”对弱纹理、大倾斜场景的鲁棒性。\n\n补充实验\n\n- Stanford2D3D 额外对比（表 A2）：Puffin roll 误差 0.26°，AUC@1° 达 96.6%。\n- 真实照片 & AIGC 图像定性结果（图 A4、A3）：地平线估计与相机图可视化一致。\n2. 相机可控生成（Camera-Controllable Generation）\n评测指标\n\n- 像素级相机图误差：up-vector / latitude / gravity 的 mean & median 角度误差（°）\n\n评测数据集\n\n- 自建 Puffin-Gen：650 组〈caption, 相机参数〉，覆盖室内、户外、建筑、自然多场景。\n\n对比方法\n\n- 文本到图像模型：GPT-4o、Qwen-Image、Nano-Banana（Gemini-2.5-Flash-Image）\n- 专用相机控制方法：PreciseCam（CVPR 2025）\n\n主要结果（表 4 & 图 6）\n\n- 三项误差均最低：up-vector median 10.12°（PreciseCam 17.47°；GPT-4o 22.86°）。\n- 生成图像与 GT 相机参数散点图（图 7）显示 Puffin 分布最接近 y=x 直线，roll 控制尤其明显优于基线。\n- 视觉对比：误差图颜色更暗，说明空间布局与 GT 相机图高度一致（图 6、A5）。\n3. 消融与互增益分析（§5.4）\n\n- 架构消融（表 5）\n– 直接微调现有 VLM（InternVL3、Qwen2.5-VL）→ 误差大幅上升；\n– 换用几何对齐视觉编码器（C-RADIOv3-H）后性能提升；\n– 再加“thinking”机制，pitch 误差从 0.91°→0.74°，FoV 误差从 1.48°→1.21°。\n\n- 生成端消融（图 8）\n– 无 thinking：生成天花板缺失、透视错位；\n– 无相机图：出现几何倒置、边缘扭曲；\n– 两者皆用，误差降至 1.8°。\n\n- 统一 vs 单任务（图 9）\n– 仅训练理解任务 → 表现下降；联合优化理解+生成，理解端 AUC@10° 提升 5 点以上，生成端在极端 FoV 场景误差降低 20%。\n\n4. 下游应用演示（§5.5 & 图 10 / A7–A9）\n\n- 3D 物体插入：先预测野生图像相机参数，再按同一姿态渲染虚拟对象并合成。\n- 空间想象：给定初始视图+目标相机，输出一句自然语言描述目标侧景象。\n- 世界探索：连续变化 yaw，生成环绕视角，并用 VGGT 做稀疏点云重建，验证跨视角一致性。\n- 摄影指导：输入初始照片，模型给出“yaw +3.43° / pitch +9.74°”建议，使美学评分从 6→8（10 分制）。\n\n综上，实验覆盖\n\n- 3 项公开理解基准 + 2 项自建基准\n- 1 项自建生成基准\n- 多维度消融与互增益分析\n- 4 类真实下游任务\n\n结果均表明：Puffin 在相机标定精度、生成空间保真度及跨任务通用性上显著优于现有专用或通用模型。\n\nQ5: 有什么可以进一步探索的点？\n\n- 多尺度训练与任意分辨率推理\n当前固定 512×512 输入导致大宽高比图像被迫中心裁剪，显著削弱理解性能。构建金字塔或分块相机图，并在扩散模型中引入分辨率-条件归一化，可实现任意分辨率端到端推理。\n\n- 径向畸变与通用相机模型\n数据集仅考虑针孔模型，未建模  k_1,k_2  等径向畸变参数。将畸变图与 Perspective Field 级联，扩展 tokenizer 通道，即可支持鱼眼、广角镜头生成与校正。\n\n- 时序扩展——相机中心视频理解与生成\n把 yaw/pitch/roll/FoV 扩展为帧间 6-DoF 轨迹，引入时序一致性损失与跨帧相机图，可统一实现「视频相机标定 + 相机轨迹可控的视频生成」，服务 AR/VR、机器人导航。\n\n- 可微相机层与自监督优化\n在扩散模型后接入可微渲染层（如 NeRF、Gaussian Splatting），以生成图像与真实图像的光度误差为信号，反传微调相机参数或生成网络，实现“自监督精修”。\n\n- 更强几何评估器\n目前生成评测依赖离线 GeoCalib，存在累积误差。训练一个「相机理解专用判别器」或采用立体匹配-光束法平差（SfM）作为评估器，可提供更严格、可微的空间一致性指标。\n\n- 跨模态相机控制\n除文本外，引入草图、深度、语音描述等多模态条件，研究不同模态对相机参数敏感度差异，可拓展到“语音调视角”“草图定构图”等交互场景。\n\n- 物理合规性与安全性\n大角度 roll 或极端 pitch 可能生成违反重力/建筑规范的图像。引入物理先验或对抗式安全过滤器，确保生成内容符合人类空间常识，避免“倒置城市”等幻觉。\n\n- 轻量化与端侧部署\n当前四阶段训练依赖 64×A100。采用 LoRA/DoRA 稀疏微调、8-bit 量化扩散采样、以及 NNAPI/CoreML 级相机图解码器，可将 Puffin 压缩至移动端实时运行。\n\n- 开放世界 continual learning\n构建在线反馈环：用户上传照片→模型估计相机→用户修正→回流入增量训练，实现“越拍越准”的终身空间智能。\n\nQ6: 总结一下论文的主要内容\n\n**论文核心贡献**\n提出 **Puffin**——首个将“相机中心理解”与“相机可控生成”统一于同一多模态框架的模型，实现任意视角下的空间推理与图像合成。\n\n1\\. 待解决问题\n\n- 相机参数（roll/pitch/FoV）与视觉-语言模态存在语义鸿沟\n- 现有 LMM 只能语义对齐，无法精确推断或控制相机几何\n- 理解与生成任务长期被当作独立问题研究\n\n2\\. 关键思路：把相机当作语言\n\n| 模块 | 做法 | 效果 |\n| --- | --- | --- |\n| thinking with camera | 数值参数↔摄影术语↔视觉线索，强制模型输出 <think> 推理段 | 桥接低维几何与高层语义，提升可解释性与精度 |\n| 双路相机表征 | 离散 token（全局）+ 连续相机图 latent（像素级） | 同时控制整体姿态与局部透视细节 |\n| 统一架构 | 几何对齐视觉编码器 → LLM → 扩散模型，共享参数 | 理解-生成双向增益，一套权重完成多任务 |\n\n3\\. 数据与训练\n\n- **Puffin-4M**：400 万〈图像，文本，相机〉三元组，含跨视角与美学标注\n- **四阶段训练**：对齐→联合微调→引入 thinking→指令微调（跨视角/摄影指导）\n\n4\\. 实验结果\n\n| 任务 | 数据集 | 主要指标 | 结果 |\n| --- | --- | --- | --- |\n| 相机理解 | MegaDepth/TartanAir/LaMAR/Puffin-Und | 中位误差 / AUC | 全面优于 GeoCalib 等专用标定网络 |\n| 可控生成 | 自建 Puffin-Gen | 相机图误差 | 比 GPT-4o、PreciseCam 降低 46% |\n| 下游应用 | 野外图像 | 3D 插入、环绕探索、摄影建议 | 零样本可用，重建几何一致 |\n\n5\\. 进一步方向\n\n多尺度训练、径向畸变、视频级相机轨迹、可微渲染自监督、端侧轻量化、物理合规安全性等。\n\n**一句话总结**\nPuffin 用“相机即语言”统一了“看图猜相机”与“按相机生图”，让机器首次在单一模型内完成精确空间推理与视角可控生成，为机器人、AR/VR、摄影辅助等应用提供了通用的空间智能基座。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Thinking%20with%20Camera%3A%20A%20Unified%20Multimodal%20Model%20for%20Camera-Centric%20Understanding%20and%20Generation，链接是https://arxiv.org/pdf/2510.08673，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08673。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1760399244932","title":"AutoPR: Let's Automate Your Academic Promotion!","description":"As the volume of peer-reviewed research surges, scholars increasingly rely on social platforms for discovery, while authors invest considerable effort in promoting their work to ensure visibility and citations. To streamline this process and reduce the reliance on human effort, we introduce Automatic Promotion (AutoPR), a novel task that transforms research papers into accurate, engaging, and timely public content. To enable rigorous evaluation, we release PRBench, a multimodal benchmark that links 512 peer-reviewed articles to high-quality promotional posts, assessing systems along three axes: Fidelity (accuracy and tone), Engagement (audience targeting and appeal), and Alignment (timing and channel optimization). We also introduce PRAgent, a multi-agent framework that automates AutoPR in three stages: content extraction with multimodal preparation, collaborative synthesis for polished outputs, and platform-specific adaptation to optimize norms, tone, and tagging for maximum reach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates substantial improvements, including a 604% increase in total watch time, a 438% rise in likes, and at least a 2.9x boost in overall engagement. Ablation studies show that platform modeling and targeted promotion contribute the most to these gains. Our results position AutoPR as a tractable, measurable research problem and provide a roadmap for scalable, impactful automated scholarly communication.","url":"https://huggingface.co/papers/2510.09558","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.932Z","metadata":{"authors":["Qiguang Chen","Zheng Yan","Mingda Yang","Libo Qin","Yixin Yuan","Hanjing Li","Jinhao Liu","Yiyan Ji","Dengyun Peng","Jiannan Guan","Mengkang Hu","Yantao Du","Wanxiang Che"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.09558.pdf","arxivUrl":"https://arxiv.org/abs/2510.09558","arxivId":"2510.09558","coolPaperUrl":"https://papers.cool/arxiv/2510.09558","published":"2025-10-10T17:08:36Z","updated":"2025-10-10T17:08:36.000Z","zh_summary":"随着同行评审研究的数量激增，学者们越来越依赖社交平台进行学术发现，而作者也投入大量精力推广自己的研究，以确保其可见性和引用率。为了简化这一流程并减少对人工的依赖，我们提出了自动推广（AutoPR），这是一项创新任务，旨在将研究论文转化为准确、引人入胜且及时的公众内容。为了实现严格评估，我们发布了PRBench，一种多模态基准，将512篇同行评审文章与高质量的推广帖链接起来，从三方面评估系统：忠实度（准确性和语气）、参与度（受众针对性和吸引力）以及对齐度（发布时间和渠道优化）。我们还提出了PRAgent，一种多智能体框架，自动执行AutoPR的三个阶段：多模态内容提取、协作合成以生成精炼输出，以及针对特定平台的适配，以优化规范、语气和标签，最大化覆盖率。与直接使用大语言模型（LLM）流程在PRBench上的表现相比，PRAgent显示出显著提升，包括总观看时间增加604%、点赞数增加438%，以及整体参与度至少提高2.9倍。消融研究表明，平台建模和针对性推广是这些提升的关键因素。我们的结果表明，AutoPR是一个可行、可衡量的研究问题，并为可扩展、高影响力的自动化学术传播提供了路线图。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文提出并解决的核心问题是：\n\n> **如何自动化地将学术论文转化为高质量、平台适配、受众导向的社交媒体推广内容，以提升论文的可见度与影响力，同时显著降低人工撰写推广材料所需的时间与成本。**\n\n具体而言，论文识别了以下关键痛点：\n\n1. **学术出版量激增**导致研究者难以手动跟踪所有相关文献，社交媒体成为发现新研究的主要渠道。\n2. **作者若不做主动推广**，其论文的引用与影响力会显著下降，但人工撰写推广内容耗时费力（>7 000 秒/篇）。\n3. **现有大模型直接生成推广帖**存在三大瓶颈：\n- 保真度低：遗漏或误述核心贡献、术语与数值；\n- 参与度弱：缺乏引人入胜的叙事钩子与情感共鸣；\n- 平台对齐差：使用通用标签、忽视平台文化与受众偏好。\n\n为此，论文首次形式化**自动学术推广（AutoPR）任务**，并给出可量化目标函数：\n\nP = argmax_P ; F(P) = argmax_P ; α_1 S_(Fidelity)(P|D) + α_2 S_(Align)(P|T_P) + α_3 S_(Engage)(P|T_A)r\n\n其中  S_(Fidelity)  衡量与原文的事实一致性， S_(Align)  衡量与目标平台规范的对齐度， S_(Engage)  衡量对目标受众的吸引力。\n\n为支撑该任务的可衡量研究，论文进一步发布**PRBench**——一个包含 512 篇同行评审论文及其人工撰写多模态推广帖的基准，覆盖保真度、参与度、对齐度三大维度共 11 项细粒度指标。\n\n最后，论文提出**PRAgent**——三阶段多智能体框架：\n\n1. **内容抽取**：层次化摘要 + 图文配对；\n2. **多智能体协同合成**：逻辑草稿、视觉分析、文本润色、图文交织；\n3. **平台特定适配**：自动调整语调、格式、标签与发布时间。\n\n实验表明，PRAgent 在 PRBench 上将最强基线（GPT-5 直接提示）的平均分提升 20%+，真实社交平台 10 天 A/B 测试显示总观看时长提升 604%，点赞提升 438%，综合互动提升 ≥2.9×。\n\nQ2: 有哪些相关研究？\n\n论文将自身置于“AI for Research（AI4Research）”与“学术传播自动化”两大交叉领域，相关研究可归纳为以下六条主线：\n\n1. 学术内容生成与摘要\n\n- 科学写作助手：SciGen、WriteBench 等探索 LLM 生成论文段落或评审意见。\n- 平语言摘要：Plain-language summarization 研究（如 Guo et al. 2025）指出 LLM 摘要虽流畅，却可能降低非专家理解度，凸显“保真-可读”张力。\n- 相关工作生成：Li & Ouyang 2024 综述显示，自动生成引用文本仍面临事实一致性挑战。\n2. 多模态学术海报 / 幻灯片自动生成\n\n- P2P（Sun et al. 2025）首次提出“paper-to-poster”任务，建立细粒度海报基准，但仅聚焦静态会议海报，未涉及社交媒体动态传播。\n- PosterGen（Zhang et al. 2025）引入审美感知多智能体，优化视觉布局，同样未解决平台适配与受众定位问题。\n3. 科学新闻与故事化传播\n\n- JRE-L（Jiang et al. 2025）用记者-读者-编辑三智能体循环，将论文改写成大众新闻，然而输出为长文报道，缺乏平台原生短帖与标签策略。\n- 早期 Science-Journalism NLG 工作多基于模板，未利用现代 LLM 的多模态与互动能力。\n4. 社交媒体学术扩散与 Altmetrics\n\n- Betz et al. 2021/2023 的 #TweetTheJournal 系列研究量化 Twitter 推广对引用提升的因果效应，为“自动推广可带来真实学术影响”提供实证支撑。\n- Bornmann 2014、Ouchi et al. 2019 探讨 altmetrics 与引用的相关性，指出社交媒体可见度能补充传统引用指标，但需高质量内容才能转化。\n5. AI4Research 多智能体系统\n\n- The AI Scientist（Lu et al. 2024）与 Tree-Planner（Hu et al. 2024）展示多智能体可完成选题、实验、写作闭环，然而“推广”环节仅被提及，未实现端到端自动化。\n- 近期“Agentic Science”综述（Wei et al. 2025; Gridach et al. 2025）将研究流程划分为假设生成、实验、评审、传播四阶段，但传播阶段仍停留在愿景层面。\n6. 平台风格与受众建模\n\n- 平台可供性理论（Marabelli et al. 2018）指出不同社交媒体对语调、视觉、标签有独特规范；AutoPR 首次将其量化为  S_(Align)  指标并引入优化目标。\n- 传播学中的“叙事参与”研究（Montes et al. 2025）强调钩子、故事弧与 CTA 对科学视频的重要性，PRAgent 将其转化为可计算的 Engagement 子评分。\n\n综上，现有工作要么聚焦单一输出格式（海报、新闻、平语言摘要），要么仅做人工推广效果分析，尚无系统框架将“论文→多模态、多平台、受众导向的短帖”自动化，也未建立可量化基准。AutoPR、PRBench 与 PRAgent 共同填补了这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“自动化学术推广”拆解为可执行的三段式流水线，并配套构建评估体系，形成“任务定义—基准评测—系统实现”闭环。具体解决方案如下：\n\n1\\. 形式化任务与多目标优化\n\n把输入视为三元组\n\n- 研究文档  D=(D_T,D_V,D_S) ：正文、图表、补充材料\n- 传播平台  T_P ：Twitter / RedNote 等\n- 目标受众  T_A ：同行专家、政策制定者、大众等\n\n输出为一条多模态帖子  P 。\n核心目标写成可计算的多目标函数：\n\nP= argmax_P α_1 underbraceS_(Fidelity)(P|D)_(保真) +α_2 S_(Align)(P|T_P)_(平台对齐) +α_3 S_(Engage)(P|T_A)_(受众吸引) r\n\n2\\. 构建 PRBench：可量化评测基准\n\n- 512 篇 arXiv 论文 + 人工撰写的 Twitter/RedNote 推广帖（图文并存）\n- 11 项细粒度指标，分三大轴：\n1. **Fidelity**：作者/标题准确性、加权事实清单得分\n2. **Engagement**：钩子强度、逻辑吸引力、视觉吸引力、CTA\n3. **Alignment**：语境相关性、图文整合度、标签/提及策略\n- 人工三重标注 + LLM-as-Judge（Qwen-2.5-VL-72B）校准，确保可扩展评估\n\n3\\. PRAgent：三阶段多智能体框架\n\nStage 1 内容抽取与结构化\n\n- **Textual Agent**：PyMuPDF→HTML→层次化摘要，长文递归分段合并\n- **Visual Agent**：DocLayout-YOLO 检测图表→最近邻匹配标题→生成 `(图, caption)` 对\n\nStage 2 多智能体协同合成\n\n四类专业智能体并行工作：\n\n1. **Logical Draft Agent**：按“研究问题→贡献→方法→结果”模式输出高密 Markdown 草稿\n2. **Visual Analysis Agent**：多模态 LLM 逐图生成“内容- takeaway- 论证作用”说明\n3. **Textual Enrich Agent**：依平台提示词将草稿改写成带钩子、emoji、CTA 的纯文本帖\n4. **Visual-Text-Interleave Agent**：决定最佳插图位置，生成图文交织故事稿\n\nStage 3 平台特定适配与发布\n\n- **Orchestration Agent**：\n- 依据平台语调、格式、标签策略重写终稿\n- 自动替换 Markdown 占位符为图片链接\n- 打包成可直接发布的 `.md` + 图像资源\n\n4\\. 训练-推理策略与优化\n\n- 无需额外训练，全流水线基于冻结 LLM + 提示工程\n- 关键提示（附录 D）强制“专家视角、去口语化、平台原生”\n- 推理阶段支持文本-only 或图文-rich 两种输出模式\n\n5\\. 实验验证\n\n- **PRBench-Core**（128 篇子集）上，PRAgent 相对直接提示平均提升 **≥ 7.15%**；在 GPT-5-mini 上达 **+20%**\n- **真实平台 A/B 测试**（RedNote，10 天 10 篇）：\n- 总观看时长 **+604%**\n- 点赞 **+438%**\n- 综合互动 **≥2.9×**\n- **消融实验**：移除任一阶段均显著拉低对齐度与保真度，验证三段式必要性\n\n综上，论文通过“任务形式化→基准建立→多智能体流水线”三位一体方案，首次实现从原始 PDF 到平台原生推广帖的端到端自动化，并在保真、吸引、对齐三维度同步取得可量化提升。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **AutoPR 任务** 与 **PRAgent 框架** 开展了系统实验，覆盖 **基准评测、真实场景 A/B 测试、消融分析、策略对比** 四大类，具体如下：\n\n1\\. PRBench 基准评测实验\n\n**目的**：量化现有 LLM 在自动学术推广上的天花板与瓶颈。\n\n| 设置 | 内容 |\n| --- | --- |\n| 数据集 | PRBench-Core（128 对论文-帖子） |\n| 对照 | 直接提示（Direct Prompt）——截取前 80 k 字符 + “请生成推广帖” |\n| 受试模型 | 6 大系列 20 余个模型，参数 7 B–235 B，含 GPT-4o/GPT-5/Gemini-2.5 等 |\n| 评估方式 | LLM-as-Judge（Qwen-2.5-VL-72B）+ 人类三重标注校准 |\n| 观测指标 | 11 项细指标 → 三大轴平均得分 |\n\n**主要结果**\n\n- 最强基线 GPT-5 直接提示仅 63.97 分；PRAgent 普遍提升 **7.15 %–20 %**。\n- fidelity 瓶颈最突出：SOTA 模型事实得分仍 **< 60 %**（遗漏核心术语/数值）。\n- 对齐度薄弱：生成标签与人类标签 Jaccard 相似度 **0.03**，表明缺乏平台深度建模。\n\n2\\. 真实社交媒体 A/B 测试\n\n**目的**：验证 PRAgent 在真实平台流量下的增益。\n\n| 设置 | 内容 |\n| --- | --- |\n| 平台 | RedNote（小红书国际版） |\n| 周期 | 10 天 |\n| 账号 | 2 个新注册匿名账号，资料、头像、简介保持一致 |\n| 样本 | 10 篇 2025-08 最新 NLP/CV arXiv 论文（未被大 V 推广过） |\n| 发布协议 | 每天 12:00（北京时间）同时发同一篇论文的两种帖子；零互动、零关注 |\n| 变量控制 | 基线统一用“论文首页截图”做配图；PRAgent 自动选图 |\n\n**核心指标提升**\n\n- 总观看时长 **+604 %**\n- 点赞 **+438 %**\n- 收藏 **+294 %**\n- 分享 **+387 %**\n- 主页访客 **+575 %**\n\n3\\. 消融实验（Ablation）\n\n**目的**：验证三阶段设计是否缺一不可。\n\n| 消融版本 | 说明 | 对齐得分降幅 | fidelity 降幅 |\n| --- | --- | --- | --- |\n| w/o Stage 1 | 去掉层次摘要，直接全文截断 | –2.38 | –4.38 |\n| w/o Stage 2 | 去掉多智能体协同，单模型一次生成 | –3.09 | –2.01 |\n| w/o Stage 3 | 去掉平台适配，仅输出通用草稿 | –8.02 | –7.82 |\n\n结论：平台特定适配环节对最终质量影响最大；三段式流水线均显著贡献。\n\n4\\. 策略对比实验\n\n**目的**：检验通用改进策略在 AutoPR 上的有效性。\n\n4.1 长链式思维（Long CoT）\n\n- 在 Qwen3 系列开启/关闭“thinking”模式\n- 结果：**无一致提升**，235 B 模型甚至下降 → 说明过度推理易引入“规格漂移”\n\n4.2 参数规模缩放\n\n- 7 B → 235 B 四系列横向对比\n- 结果：整体得分随参数增加而提升，符合缩放律；但同规模不同系列差异明显，提示架构与训练数据同样关键。\n\n4.3 推理时缩放（think token 数量）\n\n- 固定模型，逐步增加推理 token 预算\n- 结果：**轻微负相关**（r = −0.16）→ 更多推理步并未带来更高推广质量\n\n4.4 In-Context Learning（1-shot vs 0-shot）\n\n- 给出一个高质量人类帖子作为示例\n- 结果：提升 **不稳健**，部分模型 fidelity 升而 engagement 降 → ICL 对 AutoPR 并非普适良药\n\n5\\. 人类偏好实验\n\n**目的**：直接比较 PRAgent 生成内容与人类撰写帖子。\n\n- 采样 PRBench-Core 128 对，招募 3 位领域研究生盲评\n- 选择偏好：PRAgent **64.8 %** | 打平 **23.4 %** | 人类 **11.7 %**\n- 显著胜出（p < 0.01），验证自动推广已具备“人级”甚至“超人”可读性与吸引力。\n\n6\\. 视觉策略对比\n\n**Naive Visual Baseline**：每帖固定用“论文首页截图”\n**PRAgent 智能选图**：自动抽取 3–4 张关键图表并配说明\n\n- 视觉吸引力得分平均提升 **+12.4**\n- 图文整合度提升 **+15.7**\n表明主动视觉分析与选择显著优于静态截图。\n\n综上，论文通过 **基准量化 → 真实流量 → 组件消融 → 策略扫描 → 人类盲评 → 视觉消融** 六大实验，系统验证了 PRAgent 在保真、吸引、对齐三维度同时超越强 LLM 基线，并在真实社交平台获得 **>6× 观看时长、>4× 点赞** 的显著提升。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 AutoPR 与 PRAgent 的“下一步高价值问题”，按研究成熟度由近及远、由易到难排序：\n\n1\\. 平台与受众侧：从单平台到跨平台-多受众协同\n\n- **跨平台联合优化**\n当前 PRAgent 为给定  T_P  单独生成帖子；可扩展为一次输入、同时输出 Twitter-Thread + RedNote 长帖 + LinkedIn 文章，并显式建模平台间流量互引（如 Twitter 预告→RedNote 深度）。\n- **受众颗粒度细化**\n将  T_A  从“专家/大众”二分类细化为多维度画像（领域子方向、职业角色、知识水平、语言文化）。引入“受众模拟器”——用 LLM 扮演不同角色，对候选帖子进行滚动反馈，实现**强化学习式受众对齐**。\n\n2\\. 时间轴与生命周期建模\n\n- **最佳发布时机预测**\n引入“学术日历”特征（会议 deadline、奖项公布、热点新闻、节假日）与历史受众在线曲线，用轻量级时序模型预测未来 7 天各时段的期望 engagement，实现**时间槽 RL 调度**。\n- **多阶段持续推广**\n目前一次生成即发布；可探索“1 篇论文 → N 条时间分散微故事”策略：预印本放出时、正式录用时、代码开源时、数据集更新时分别推送不同角度内容，最大化**长尾引用**。\n\n3\\. 多语言与地域化\n\n- **非英语学术圈渗透**\n将 PRAgent 扩展为“中英双语同步生成 + 文化语境本地化”(e.g., 日语强调敬语、德语偏好长句)。可构建平行语料 PRBench-XL，评测**跨语言 fidelity**（术语一致性）与**地域文化 alignment**。\n- **低资源语言零样本推广**\n利用英语内容做 pivot，通过多语 LLM 零样本生成 Swahili / Hindi 等推广帖，评估其对当地研究可见度的真实提升（altmetrics 地理细分数据）。\n\n4\\. 视觉模态升级\n\n- **自动生成“短视频摘要”**\n将关键图表 → 15 秒动态幻灯（含配音+字幕），适配 TikTok、B 站、YouTube Shorts；引入扩散模型生成**概念可视化**（把抽象模型框图变成 3D 动画）。\n- **交互式可视化卡片**\n输出单文件 HTML/SVG，嵌入可滑动对比、hover 解释，适合 Mastodon、Bluesky 的“长文+互动”生态；同时记录用户交互行为（点击、停留）反哺 engagement 模型。\n\n5\\. 反馈闭环与自迭代\n\n- **在线强化学习**\n真实发布后抓取点赞、转发、评论、点击-through 率，用**多目标 Bandit/RL** 微调提示词权重  α_i  或智能体顺序，实现“越推越优”。\n- **人类-在环主动学习**\n低置信帖子（entropy 高）触发“作者审核小程序”，一键接受/拒绝/编辑；编辑 diff 作为高质量人类偏好数据，持续蒸馏到更小模型，降低推理成本。\n\n6\\. 可信度、伦理与对抗风险\n\n- **事实安全过滤器**\n建立“学术 hallucination 黑名单”：数值±5 % 误差、方法名称拼写错误、引用格式失效等自动检测；引入**对抗样本测试**（故意输入带错误 PDF）评估系统鲁棒性。\n- **垃圾推广与“学术标题党”治理**\n研究过度夸张 hook 对学术生态的负面效应（引用泡沫、标题党化），在目标函数里增加**诚信正则项**（夸张度惩罚、与论文摘要的语义一致性奖励）。\n\n7\\. 垂直领域适配\n\n- **临床与生命科学**\n医学论文常含患者隐私、复杂伦理声明；需额外加入**HIPAA/伦理审查语句检测**与**患者可阅读版本**（<6 年级阅读水平）并行生成。\n- **数学与理论计算机科学**\n证明思路可视化极度困难；探索用**Lean4 形式化摘要** + 交互式证明树动画，辅助 Twitter 线程逐步展开定理直觉。\n\n8\\. 经济学与因果推断\n\n- **推广投入-引用产出因果估计**\n联合 ORCID、CrossRef、Twitter API，构建作者-论文-推广事件面板数据，用**双重差分+工具变量**估计“自动推广”相对于无推广的**边际引用增益**，给出不同学科、不同职业阶段作者的“最优推广强度”曲线。\n- **成本-效益优化**\n把 API 调用费用、人类校对时间折算成美元，建立\\*\\*$/引用**指标，探索在预算约束下的**Pareto 最优推广策略\\*\\*（小模型+轻量视觉 vs. 大模型+重视觉）。\n\n9\\. 模型压缩与边缘部署\n\n- **10 B 以下小模型专用蒸馏**\n利用 PRAgent 产生的 50 万条“论文-帖子”平行数据，蒸馏出 3 B 参数“AutoPR-Small”，在手机端离线运行，服务网络不畅地区的学者。\n- **量化-推理协同**\n结合论文自身量化研究成果，对 PRAgent 流水线做**W8A8 量化**，验证是否出现“自己推广自己”的循环增益或性能损失。\n\n10\\. 开放科学与社区共建\n\n- **众包扩展 PRBench**\n建立“PRBench-Contrib”平台，允许作者上传自己的论文+推广帖+ engagement 数据，GitHub 式 PR 评审，持续扩大基准规模与多样性。\n- **在线挑战赛**\n仿照 Kaggle，举办“AutoPR Challenge”，设立“最佳钩子”、“最佳视觉故事”、“最佳低资源语言”等分项，推动社区提出更创新的推广策略。\n\n综上，从**平台受众细化、时间生命周期、多语言视觉、反馈强化、伦理治理、垂直领域、经济学因果、模型压缩到社区共建**，AutoPR 仍有一片广阔的“学术传播自动化”研究版图待挖掘。\n\nQ6: 总结一下论文的主要内容\n\n**AutoPR 论文核心内容速览**\n\n1. **任务提出**\n首次形式化“自动化学术推广（AutoPR）”任务：给定论文 PDF + 目标平台 + 受众，自动生成高保真、高参与、平台原生的多模态社交帖子，并给出可量化三目标函数\n\nmax_P α_1 S_(fidelity)+α_2 S_(align)+α_3 S_(engage)r\n\n2. **基准 PRBench**\n\n- 512 篇 arXiv 论文与人工 Twitter/RedNote 推广帖配对\n- 11 项细指标 → 保真 / 参与 / 对齐三大轴\n- 人工三重标注 + LLM-as-Judge 校准，供社区端到端评测\n3. **方法 PRAgent**\n三阶段多智能体流水线：\n① 内容抽取（层次摘要 + 图文配对）\n② 多智能体协同（逻辑草稿、视觉分析、文本润色、图文交织）\n③ 平台特定适配（语调、格式、标签、emoji、发布时间）\n\n4. **实验结果**\n\n- PRBench 上较最佳直接提示基线平均提升 **7–20 %**；事实准确率绝对提升 **>12 %**\n- 真实 RedNote 10 天 A/B 测试：总观看时长 **+604 %**、点赞 **+438 %**、综合互动 **≥2.9×**\n- 消融：去掉任一阶段显著拉低对齐与保真；平台适配环节贡献最大\n- 人类盲评：PRAgent 帖子偏好率 **64.8 %** 胜人类 **11.7 %**\n5. **结论与意义**\nAutoPR 被确立为可衡量、可扩展的新研究方向；PRAgent 证明“内容抽取-协同合成-平台适配”流水线能一次性解决事实、吸引力与平台规范冲突，为自动化学术传播提供即插即用工具与公开基准。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是AutoPR%3A%20Let%27s%20Automate%20Your%20Academic%20Promotion%21，链接是https://arxiv.org/pdf/2510.09558，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09558。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1760399244933","title":"TAG:Tangential Amplifying Guidance for Hallucination-Resistant Diffusion Sampling","description":"Recent diffusion models achieve the state-of-the-art performance in image generation, but often suffer from semantic inconsistencies or hallucinations. While various inference-time guidance methods can enhance generation, they often operate indirectly by relying on external signals or architectural modifications, which introduces additional computational overhead. In this paper, we propose Tangential Amplifying Guidance (TAG), a more efficient and direct guidance method that operates solely on trajectory signals without modifying the underlying diffusion model. TAG leverages an intermediate sample as a projection basis and amplifies the tangential components of the estimated scores with respect to this basis to correct the sampling trajectory. We formalize this guidance process by leveraging a first-order Taylor expansion, which demonstrates that amplifying the tangential component steers the state toward higher-probability regions, thereby reducing inconsistencies and enhancing sample quality. TAG is a plug-and-play, architecture-agnostic module that improves diffusion sampling fidelity with minimal computational addition, offering a new perspective on diffusion guidance.","url":"https://huggingface.co/papers/2510.04533","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.933Z","metadata":{"authors":["Hyunmin Cho","Donghoon Ahn","Susung Hong","Jee Eun Kim","Seungryong Kim","Kyong Hwan Jin"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.04533.pdf","arxivUrl":"https://arxiv.org/abs/2510.04533","arxivId":"2510.04533","coolPaperUrl":"https://papers.cool/arxiv/2510.04533","published":"2025-10-06T06:53:29Z","updated":"2025-10-06T06:53:29.000Z","zh_summary":"最近的扩散模型在图像生成方面达到了最先进的性能，但经常会出现语义不一致或幻觉问题。虽然各种推理时的引导方法可以增强生成效果，但它们通常通过依赖外部信号或架构修改间接操作，这会带来额外的计算开销。在本文中，我们提出了切向放大引导（TAG），这是一种更高效且直接的引导方法，仅依靠轨迹信号运行而无需修改底层扩散模型。TAG 利用中间样本作为投影基，并放大与该基相关的估计分数的切向分量，以校正采样轨迹。我们通过利用一阶泰勒展开形式化了这一引导过程，证明放大切向分量可以将状态引导向更高概率区域，从而减少不一致性并提高样本质量。TAG 是一个即插即用、与架构无关的模块，可在最小计算增加的前提下提高扩散采样的保真度，为扩散引导提供了新的视角。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对扩散模型在采样阶段出现的“幻觉”现象——即生成样本偏离真实数据分布、出现语义不一致或解剖结构错误（如多余手指）——提出一种无需重训练、不增加额外去噪网络调用的轻量级推理时校正方法。核心观察是：采样轨迹在低密度“谷地”插值不同模式时易滑出数据流形，导致离分布样本。为此，作者提出 Tangential Amplifying Guidance（TAG），通过将单步更新矢量分解为径向（平行于当前状态）与切向（正交于当前状态）分量，仅放大切向分量，把轨迹“拉回”高概率区域，从而抑制幻觉并提升保真度。\n\nQ2: 有哪些相关研究？\n\n- **Classifier-free guidance (CFG)**\nHo & Salimans, 2021 —— 用条件/无条件分支残差缩放实现推理引导，是后续所有“残差缩放”类方法的基线。\n\n- **Self-attention guidance (SAG)**\nHong et al., ICCV 2023 —— 利用模型自身注意力图生成内部引导信号，无需额外分类器。\n\n- **Perturbed-attention guidance (PAG)**\nAhn et al., ECCV 2024 —— 通过对注意力层施加轻微扰动产生引导方向，强化结构一致性。\n\n- **Smoothed energy guidance (SEG)**\nHong, NeurIPS 2024 —— 在注意力能量曲率较小的方向施加引导，减缓过度饱和与伪影。\n\n- **Representative/Token perturbation guidance**\nDinh et al., ICLR 2025；Rajabi et al., arXiv 2025 —— 通过扰动代表性特征或 token 嵌入产生引导信号。\n\n- **TCFG (Tangential damping CFG)**\nKwon et al., CVPR 2025 —— 发现条件-无条件分支的不一致主要存在于切向分量，提出阻尼策略。\n\n- **Mode interpolation analysis**\nAithal et al., NeurIPS 2024 —— 指出幻觉源于轨迹在模式间低密度区域插值，为 TAG 的几何解释提供理论支撑。\n\n- **Geometry-aware/limit-interval guidance**\nSadat et al., arXiv 2024；Kynkäänniemi et al., NeurIPS 2024 —— 强调在特定区间或特定方向施加引导，避免全局残差缩放带来的副作用。\n\n- **DDIM / DPM-Solver / EDM 预处理**\nSong et al., 2020a；Lu et al., 2022；Karras et al., 2022 —— 提供确定性 ODE 求解器与噪声调度，为 TAG 的“单步增量分解”奠定实现基础。\n\nQ3: 论文如何解决这个问题？\n\n论文将“抑制幻觉”转化为**在推理阶段修正采样轨迹**的问题，提出**Tangential Amplifying Guidance（TAG）**。核心思路与实施步骤如下：\n\n1. 几何分解\n在每一步 ODE/DDIM 更新后，将基础增量\n\nDelta_(k+1)= tilde x_k - x_(k+1)\n\n沿当前状态方向  x_(k+1)  做正交分解：\n\n- 径向（法向）分量： P_(k+1)Delta_(k+1) ，负责维持噪声调度规定的半径/信噪比；\n- 切向（正交）分量： P^perp_(k+1)Delta_(k+1) ，携带数据流形的局部结构信息。\n2. 仅放大切向\n保持径向分量不变，对切向分量乘以放大系数  etage 1 ：\n\nx_k = x_(k+1) + P_(k+1)Delta_(k+1) + eta P^perp_(k+1)Delta_(k+1)\n\n该操作不引入额外去噪网络调用，也不修改预训练权重。\n\n3. 理论保证\n利用 score 与对数似然的局部线性关系，证明放大切向会**单调增加一阶泰勒增益**\n\nG(eta)= langleDelta^(TAG)_(k+1),∇_xlog p(x|t_(k+1))rangle\n\n从而把采样点推向高概率区域，抑制离分布漂移（Theorem 4.1）。\n\n4. 条件生成扩展（C-TAG）\n对 Classifier-Free Guidance 的“条件-无条件”残差  g_k  同样做切向提取  g^perp_k=P^perp(x_k)g_k ，再将其投影回条件 score 并加权，消除两分支在切向的不一致，进一步减少语义错误。\n\n5. 即插即用\nTAG 作为后处理步骤插入任何现有采样器（DDIM、DPM-Solver、Flow-Matching 等），无需重训练或额外模型，仅增加可忽略的投影计算，即可在 25–50 NFE 内达到甚至超过原 100 NFE 的 FID/IS/CLIPScore。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“无条件生成、条件生成、不同主干网络、不同采样器、不同放大系数”五个维度展开，主要结果如下：\n\n| 维度 | 数据集 / backbone | 指标 | 关键结论 |\n| --- | --- | --- | --- |\n| 无条件生成 | ImageNet-val 30K，SD v1.5 | FID↓ IS↑ | DDIM 50 NFE 基线 FID 76.9→TAG 67.8；IS 14.8→16.5。 |\n| 与现有 guidance 组合 | 同上 | 同上 | SAG+TAG FID 再降 6.6；PAG+TAG、SEG+TAG 均刷新原最佳。 |\n| 少 NFE 挑战 | 同上 | 同上 | DDIM 25 NFE+TAG 已优于原 50 NFE；DPM++ 10 NFE+TAG 优于原 10 NFE 基线 11.7 FID。 |\n| 跨主干 | ImageNet 10K，SD v2.1 / SDXL | FID↓ IS↑ | SD v2.1 100.9→88.8；SDXL 124.4→113.8，均显著改进。 |\n| Flow-Matching | ImageNet 30K，SD3 | FID↓ IS↑ | SD3 基线 96.38→TAG 91.71，证明方法对 rectified-flow 同样有效。 |\n| 条件生成 | MS-COCO 2014 10K prompt | FID↓ CLIPScore↑ | CFG 100 NFE FID 26.27→C-TAG 30 NFE 23.41；CLIPScore 从 22.60→22.82。PAG/SEG 加 C-TAG 后 FID 再降 2~6 点。 |\n| 放大系数 η 消融 | SD v1.5 & SD3 | FID / IS 曲线 | η≈1.15 最佳；η>1.3 开始退化，与理论二阶扰动项预测一致。 |\n| 视觉对比 | 图 3、6、7、9 | 人工打分 | 在 50 NFE 下，TAG 细节清晰度优于 250 NFE 基线；条件任务中文字忠实度与伪影显著减少。 |\n| 运行时间 | RTX 4090 100 轮平均 | 秒 | DDIM 50 NFE 1.97 s→TAG 1.97 s（<1% 增量）；25 NFE 1.02 s 即可超越原 50 NFE 质量。 |\n\n综上，实验覆盖**模型架构无关性、采样器无关性、条件/无条件双场景、计算预算缩减**四方面，均验证 TAG 在**零重训练、零额外网络调用**前提下，可一致降低 FID、提升 IS/CLIPScore，并显著压缩推理步数。\n\nQ5: 有什么可以进一步探索的点？\n\n- **自适应放大系数**\n当前 η 全局固定；可基于局部曲率、梯度范数或信噪比设计步级自适应 η\\_k，实现免调参 TAG。\n\n- **高阶修正与守恒约束**\n论文仅保证一阶径向项不变；可引入二阶李导数或能量守恒项，抑制 η 过大时的半径漂移与过度平滑。\n\n- **与流匹配/扩散-流混合框架的深度集成**\n探索 TAG 在 RF、Consistency-Model 或 Poisson-Flow 中的增量分解形式，验证是否普遍适用于“非-score”ODE。\n\n- **跨模态扩展**\n将切向分解推广到文本、音频、视频或 3D 点云扩散，检验“语义切向”是否同样携带结构信息并减少幻觉。\n\n- **与低秩/稀疏投影结合**\n用随机 SVD 或稀疏掩码近似 P^⊥，进一步降低投影复杂度，实现移动端实时采样。\n\n- **理论深化**\n建立 η 与 Wasserstein 误差、流形距离之间的显式 bound；研究连续时间极限下 TAG 对应的修正 SDE/ODE。\n\n- **条件-无条件差异的在线校正**\n仅对残差 g\\_k 的切向部分进行动态阻尼或增益，可缓解 CFG 过引导导致的饱和与多样性损失。\n\n- **多分辨率或层级采样**\n在潜空间金字塔的不同层级施加差异化 η，以兼顾全局结构与细节纹理。\n\n- **与扩散编辑/逆问题联合**\n将 TAG 引入 SDEdit、DPS、Plug-and-Play 等逆求解器，考察能否减少重建伪影并提高数据一致性。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**\n扩散模型在采样阶段常因“模式插值”滑出数据流形，产生语义不一致或解剖错误（幻觉）。\n\n- **核心观察**\n单步更新向量可分解为：\n\n- 径向分量——维持噪声调度半径；\n- 切向分量——携带数据流形局部结构。\n放大切向可把轨迹推向高概率区域，抑制离分布漂移。\n- **方法**\n提出 **Tangential Amplifying Guidance（TAG）**：\n\n1. 用正交投影将 DDIM/ODE 增量拆为  PDelta  与  P^perpDelta ；\n2. 仅对  P^perpDelta  乘以系数  etage 1  后重组；\n3. 无需重训练、零额外网络调用，即插即用。\n条件生成再扩展 **C-TAG**，对 CFG 残差做同样切向放大，缓解条件- unconditional 不一致。\n- **理论**\n基于 score 与对数似然的一阶泰勒增益，证明放大切向单调增加  G(eta) ，从而保证样本向高密度区域移动。\n\n- **实验**\n- 无条件 ImageNet：SD 系列 / SDXL / SD3 在 25–50 NFE 下 FID 降 6–12，IS 升 1–2。\n- 条件 MS-COCO：30 NFE C-TAG 优于 100 NFE CFG；与 SAG、PAG、SEG 组合再降 FID 2–6 点。\n- Flow-matching、DPM++、少步采样一致提升；运行时间增量 <1%。\n- **结论**\nTAG 以可忽略成本显著减少幻觉、提升保真，适用于任意扩散/流匹配 backbone 与采样器，为推理时几何校正提供新视角。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是TAG%3ATangential%20Amplifying%20Guidance%20for%20Hallucination-Resistant%20Diffusion%20Sampling，链接是https://arxiv.org/pdf/2510.04533，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.04533。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1760399244933","title":"Multimodal Prompt Optimization: Why Not Leverage Multiple Modalities for MLLMs","description":"Large Language Models (LLMs) have shown remarkable success, and their multimodal expansions (MLLMs) further unlock capabilities spanning images, videos, and other modalities beyond text. However, despite this shift, prompt optimization approaches, designed to reduce the burden of manual prompt crafting while maximizing performance, remain confined to text, ultimately limiting the full potential of MLLMs. Motivated by this gap, we introduce the new problem of multimodal prompt optimization, which expands the prior definition of prompt optimization to the multimodal space defined by the pairs of textual and non-textual prompts. To tackle this problem, we then propose the Multimodal Prompt Optimizer (MPO), a unified framework that not only performs the joint optimization of multimodal prompts through alignment-preserving updates but also guides the selection process of candidate prompts by leveraging earlier evaluations as priors in a Bayesian-based selection strategy. Through extensive experiments across diverse modalities that go beyond text, such as images, videos, and even molecules, we demonstrate that MPO outperforms leading text-only optimization methods, establishing multimodal prompt optimization as a crucial step to realizing the potential of MLLMs.","url":"https://huggingface.co/papers/2510.09201","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.933Z","metadata":{"authors":["Yumin Choi","Dongki Kim","Jinheon Baek","Sung Ju Hwang"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.09201.pdf","arxivUrl":"https://arxiv.org/abs/2510.09201","arxivId":"2510.09201","coolPaperUrl":"https://papers.cool/arxiv/2510.09201","published":"2025-10-10T09:41:25Z","updated":"2025-10-10T09:41:25.000Z","zh_summary":"大型语言模型（LLMs）已经显示出显著的成功，其多模态扩展（MLLMs）进一步释放了跨越图像、视频以及其他文本以外模态的能力。然而，尽管出现了这种转变，旨在减少手动提示设计负担并最大化性能的提示优化方法仍局限于文本，这最终限制了MLLMs的全部潜力。基于这一差距，我们提出了多模态提示优化的新问题，将先前的提示优化定义扩展到由文本提示与非文本提示组成的多模态空间。为了解决这一问题，我们提出了多模态提示优化器（MPO），这是一个统一框架，不仅通过保持对齐的更新执行多模态提示的联合优化，还利用早期评估作为先验，通过基于贝叶斯的选择策略指导候选提示的选择过程。通过在超越文本的多种模态（如图像、视频甚至分子）上的大量实验，我们证明了MPO优于现有的文本优化方法，确立了多模态提示优化作为实现MLLMs潜力的关键步骤。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“多模态大语言模型（MLLM）的提示优化仍被限制在纯文本空间”这一核心瓶颈。具体而言：\n\n- **问题背景**：尽管 MLLM 已具备同时处理文本、图像、视频、分子结构等多模态输入的能力，现有自动提示优化（APO）方法仅对文本提示进行迭代，完全忽视非文本模态的提示潜力，导致模型表达空间被人为压缩，性能次优。\n- **形式化定义**：作者首次提出“多模态提示优化”新任务，将提示从纯文本  t  扩展为文本-非文本对  (t,m) ，目标是在组合空间  T×M  内找到最优多模态提示\n\n(t^_,m^_)=argmax_((t,m)∈ T×M)E_((q,a)sim D),f!l(smallrm MLLM(t,m,q),ar).\n\n- **关键挑战**\n1. 组合空间庞大，需保证文本与非文本部分语义一致，避免跨模态冲突；\n2. 高质量提示稀疏，传统无先验的候选评估策略在冷启动阶段浪费大量预算。\n\n为此，作者提出统一框架 MPO，通过“对齐保持的探索”与“先验继承的贝叶斯-UCB 选择”两大组件，联合优化文本与非文本提示，在图像、视频、分子三大模态、10 个数据集上显著超越现有文本-only 方法，验证“必须利用多模态提示才能充分释放 MLLM 潜力”的核心假设。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中系统梳理了三条与多模态提示优化密切相关的研究脉络，并指出它们与本文任务的差异。可归纳为以下三类：\n\n1. 多模态大语言模型（MLLMs）\n\n- 典型工作：Flamingo、GPT-4V、Gemini-2.5、Qwen2.5-VL、InternVL-3.5 等。\n- 核心贡献：通过大规模预训练将视觉/音频/分子等编码器与 LLM 对齐，支持图文交错输入，已在分类、字幕、医学影像问答、药物性质预测等任务上验证有效性。\n- 与本文关系：MPO 把这些模型当作黑盒优化目标，首次针对它们的“多模态输入空间”进行提示优化，而非仅利用其固定能力。\n2. 自动提示优化（APO）——**仅文本模态**\n\n- 梯度式方法：Khattak et al. MAPLE、Zeng et al. ModalPrompt、Wang et al. M2PT 等，学习连续软提示，需访问模型参数且解释性差。\n- 无梯度/LLM 驱动方法：\n– APE（Zhou et al. 2023）——反向生成指令+复述；\n– OPRO（Yang et al. 2024）——用 LLM 做历史分数驱动的优化器；\n– EvoPrompt（Guo et al. 2024）——进化算法中的交叉/变异算子由 LLM 实现；\n– ProTeGi（Pryzant et al. 2023）——“文本梯度”失败分析+束搜索；\n– SEE（Cui et al. 2025）——四阶段交替探索-利用，联合优化指令与上下文示例。\n- 与本文差异：以上方法全部限定在文本空间  p=t ，未触及图像、视频、分子等非文本提示维度；MPO 将优化空间扩展到  T×M  并解决跨模态对齐与选择效率问题。\n3. 实例级（query-specific）多模态提示——**单次推理增强**\n\n- 代表工作：MM-CoT、Visual Prompting（边界框/点）、文本到图像/视频生成中的动态提示（Manas et al. 2024、Mo et al. 2024、Gao et al. 2025）等。\n- 特点：针对单个查询即时生成或调整提示，不追求跨样本复用。\n- 与本文区别：MPO 属于任务级提示优化，目标是找到一个固定多模态提示，在整体验证集上持续提升性能，而非逐例调整。\n\n综上，现有研究或聚焦 MLLM 架构训练，或仅在文本空间做提示优化，或仅做实例级增强。本文首次把“提示优化”正式扩展到多模态联合空间，并针对由此带来的组合爆炸与评估稀疏问题提出系统解决方案，填补了该交叉领域的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将“多模态提示优化”形式化为在组合空间  T× M  中寻找最优  (t^_,m^_)  的搜索问题，并针对两大核心挑战——**跨模态一致性**与**候选选择效率**——提出统一框架 **MPO（Multimodal Prompt Optimizer）**。整体流程可拆解为两大模块、五类关键技术：\n\n1\\. 对齐保持的探索（Alignment-Preserving Exploration）\n\n目标：在庞大组合空间中同步更新文本  t  与非文本  m ，避免二者语义冲突，同时充分覆盖候选区域。\n\n1.1 统一反馈生成（Cohesive Backpropagation）\n\n- 对当前多模态提示  p=(t,m) ，先在训练集上执行一次前向推断，收集失败样本集合\n\nF=(q,a,y)mid y≠ a.\n\n- 将  F  作为“错误上下文”输入 MLLM，让模型用自然语言输出**跨模态弱点摘要**\n\n∇_p=(∇_t,∇_m)=smallrm MLLM(t,m;F).\n\n该文本形式的单一梯度信号同时指导文本与非文本的后续修正，避免独立更新导致的语义漂移。\n\n1.2 联合多模态更新（Joint Multimodal Update）\n\n- 利用同一  ∇_p ，让 LLM 生成\n- 改进后的文本提示  t' ；\n- 模态相关的**条件文本** c （描述非文本应如何改变）。\n-  c  被送入**模态专用生成器** g(·) （如 GPT-Image、Wan2.1、SMILES 工具）产生新的非文本提示  m'=g(c,·) ，确保  m'  与  t'  在同一语义方向演化。\n\n1.3 三种互补探索算子\n\n为兼顾“全局开荒-局部精修-中间重组”，MPO 循环调用以下算子生成多样化子提示：\n\n| 算子 | 条件输入 | 生成方式 | 作用 |\n| --- | --- | --- | --- |\n| Generation |  c_(rm gen)  | 从零生成  m'=g(c_(rm gen),varnothing)  | 跳出局部极值，早期快速覆盖 |\n| Edit |  c_(rm edit)  | 在上一版  m  上局部编辑  m'=g(c_(rm edit),m)  | 保留有效结构，微调细节 |\n| Mix |  c_(rm mix)  | 融合多个父提示  m_i   m'=g(c_(rm mix),m_i)  | 组合互补优势，探索中间解 |\n\n每次迭代随机选用一种算子，保证探索轨迹既连贯又多样。\n\n2\\. 先验继承的贝叶斯-UCB 选择（Prior-Inherited Bayesian UCB）\n\n目标：在候选数量爆炸、高质量提示稀疏的情况下，用最少评估预算锁定高潜力提示。\n\n2.1 父子性能关联实证\n\n论文在优化轨迹上统计发现：**父提示得分与其子提示平均得分呈强正相关**（Pearson  r=0.88 ）。因此，父代后验可充当子代先验，显著缓解冷启动浪费。\n\n2.2 先验初始化\n\n对每个新生子提示  p_i ，用其父代后验均值  hatμ_(rm par)(i)  构造 Beta 先验\n\nα_i=hatμ_(rm par)(i)· S+1,quad β_i=(1-hatμ_(rm par)(i))· S+1,quad S>0.\n\n相当于一次性赋予  S  笔“伪观测”，实现 warm-start。\n\n2.3 迭代选择流程\n\n1. 维护每条候选提示的 Beta 后验  rm Beta(α,β) ；\n2. 每轮按 UCB 指数\n\nq_t=rm BetaQuantile!l(1-t(1) / (t)(log N)^c;α,βr)\n\n选取上置信界最高者，用小批量数据评估并更新后验；\n3. 预算耗尽后，返回期望得分最高的  b  条提示进入下一轮探索。\n\n理论保证（Proposition 3.1）：若父先验比均匀先验更接近真实性能分布，则期望识别最优臂的采样次数**不增**，即节省预算。\n\n3\\. 整体算法流程（Algorithm 1 & 2）\n\n1. 初始阶段仅用 Generation 算子产生  b^2  个多模态提示；\n2. 用 Prior-Inherited Bayesian-UCB 选 top- b  作为父代；\n3. 对于  T  轮迭代\n- 对每个父提示随机应用 Generation/Edit/Mix 生成  b  个子提示；\n- 同样用 Bayesian-UCB 选 top- b  进入下一轮；\n4. 最终输出后验均值最高的  (t^_,m^_) 。\n\n4\\. 效果验证\n\n- **10 个跨模态数据集**（图像/视频/分子）平均提升 **+5.1~+11.9 pp**，全部显著优于最佳文本-only 方法；\n- **消融实验**表明：\n- 三种探索算子组合 > 任一单独算子；\n- 先验继承策略在同等性能下节省 **42%~70% 评估预算**；\n- 跨模态对齐得分与最终性能高度相关（ r=0.78 ），验证“对齐保持”必要性。\n\n通过“联合更新+多样算子+父子先验”三位一体，MPO 在理论上保证采样效率，在实践上系统性地释放了 MLLM 的多模态提示潜力。\n\nQ4: 论文做了哪些实验？\n\n论文在 4 个维度、共 10 个跨模态数据集上进行了系统实验，覆盖**图像、视频、分子**三大模态，并辅以消融、可视化和计算成本分析。具体实验一览如下（按实验目的分类）：\n\n1\\. 主实验：跨模态全面评测\n\n**目的**：验证 MPO 相对现有文本-only 优化方法的绝对提升。\n\n| 模态 | 数据集（子任务数） | 任务类型 | 指标 |\n| --- | --- | --- | --- |\n| 图像 | PlantVillage（4 类叶片病）CUB-200-2011（12 细粒度鸟类）SLAKE（3 医学影像 VQA）DrivingVQA（真实驾驶场景 VQA）RSVQA（遥感 VQA） | 分类 / VQA | Accuracy |\n| 视频 | Drive&Act（驾驶员行为）VANE-Bench（异常事件检测 VQA） | 分类 / VQA | Accuracy |\n| 分子 | Absorption（4 ADME 子任务）BBBP（血脑屏障穿透）CYP Inhibition（5 种酶抑制） | 二分类 | Accuracy / F1 |\n\n**对照组**：\n\n- 人工提示：Human、CoT、1/3/5-shot\n- 文本-only APO：APE、OPRO、EvoPrompt、PE2、ProTeGi、SEE\n\n**结果**（表 1）：MPO 在所有 13 列任务上均取得最高平均分 **65.1**，比最佳文本-only 方法 SEE（59.1）提升 **+6.0 pp**；在最具挑战的分子 BBBP 任务上 F1 提升 **+5.5 pp**。\n\n2\\. 通用性实验：更换 backbone 仍有效\n\n**目的**：验证 MPO 对“基础模型+优化器+模态生成器”的鲁棒性。\n\n| 组件 | 可变 backbone 举例 | 结论 |\n| --- | --- | --- |\n| 基础 MLLM | Qwen2.5-VL-72B、Gemma3-12B、InternVL-3.5-14B、GPT-4.1-nano | MPO 均领先同等规模文本-only 方法，且随模型变大增益放大（表 2 Top） |\n| 优化器 LLM | Qwen2.5-VL-7B、Gemini-2.5-Flash、GPT-4o-mini、GPT-4o | 换用更强优化器后，MPO 提升 +8.8 pp，始终优于 SEE（表 2 Bottom Left） |\n| 模态生成器 | SANA1.5-1.6B、NanoBanana、GPT-Image-Low/Medium | 即使用 1.6B 轻量级生成器，MPO 仍比文本-only 最佳方法高 +7.4 pp（表 2 Bottom Right） |\n\n3\\. 消融实验：验证各组件必要性\n\n| 消融对象 | 设置 | 关键结果 |\n| --- | --- | --- |\n| 跨模态对齐 | ① 顺序优化文本→图像② 随机图像提示③ 同分布图像查询④ 跨分布图像查询 | 仅 MPO 全联合更新获得最高对齐得分 DSG 与最大性能增益（图 4） |\n| 模态贡献 | 仅保留 MPO-text 或 MPO-image | 单模态已超 Human 基线，但双模态组合再提升 +16~+20 pp（表 3） |\n| 探索算子 | 单独使用 Generation、Edit、Mix | 三者互补，全组合在 PlantVillage 4 作物平均 Acc 76.4 优于最佳单算子 74.8（表 4） |\n| 选择策略 | Uniform / UCB / w/o Prior / MPO | MPO 用 30% 预算即达 Uniform 最佳性能，节省 70% 计算（图 5） |\n\n4\\. 训练动态与表示分析\n\n- **学习曲线**（图 7）：在 CUB 上，MPO 在第 3 轮后仍持续上升 **+6.4 pp**，而 ProTeGi 仅 **+1.1 pp** 即饱和，显示多模态搜索可逃离文本局部极值。\n- **隐藏状态可视化**（图 8）：对 MLLM 中间层嵌入做 PCA，文本-only 方法聚于同一区域；MPO 完整多模态提示将表示推向新区域，解释性能提升来源。\n- **先验强度 S 敏感度**（图 9）：S 过小失去 warm-start，过大导致过信父代；中间值 S=10（10% 预算）最佳。\n\n5\\. 计算成本对比\n\n以“模型请求次数”为代理，MPO 比文本-only 方法仅增加 **~117 次模态生成器调用**（表 5），在轻量级生成器下可忽略，却带来 **+6.0 pp** 绝对提升。\n\n6\\. 定性案例\n\n- **图像**：图 6 展示 CUB 鸟类提示进化——Generation 引入新姿态，Edit 微调喙部纹理，Mix 融合背景与标注，最终参考图被模型明确调用。\n- **分子**：图 14 显示 BBBP 任务中，Edit 保留父分子芳香环，Mix 重组两父分子优势子结构，生成的新分子更符合 BBB 穿透规则。\n- **视频/分子文本提示**：表 9-12 给出优化后的多模态指令，均显式要求模型“利用参考图像/分子”进行决策，充分发挥 MLLM 跨模态推理能力。\n\n综上，实验从“性能-通用性-消融-效率-可视化-定性”六个层面系统论证：**只有把提示优化扩展到多模态空间，并辅以对齐保持与先验继承策略，才能充分释放 MLLM 的跨模态潜力。**\n\nQ5: 有什么可以进一步探索的点？\n\n以下展望按“理论-方法-系统-应用”四个层面整理，均为 MPO 开启但尚未深入的方向，可供后续工作继续挖掘。\n\n1\\. 理论层面\n\n- **多模态搜索空间复杂度刻画**\n当前仅通过实验验证组合空间更大、稀疏性更高，缺乏对  |T||M|  联合空间的样本复杂度或 regret 下界分析。可引入多臂老虎机中的“模态相关臂”或“层次臂”模型，给出先验继承策略的极小最优（minimax）界。\n\n- **跨模态对齐的度量与可验证性**\n论文使用经验 DSG 分数衡量对齐，未来可研究“对齐”与任务误差之间的因果链，建立可验证的充分/必要条件，甚至引入逻辑或契约（contract）机制，在生成阶段即保证文本条件  arrow  非文本输出的一致性。\n\n- **多目标优化视角**\n现实场景往往同时要求准确率、鲁棒性、可解释性、推理成本。可将提示优化视为多目标 Pareto 前沿搜索，引入超体积（hyper-volume）或约束贝叶斯优化，实现“性能-成本”自动权衡。\n\n2\\. 方法层面\n\n- **端到端可微提示**\nMPO 采用黑盒 LLM+外部生成器，梯度信息仅用于文本反馈。若将视觉/分子生成器替换为可微扩散模型或 SMILES-VAE，可设计“真正”的跨模态梯度反向传播，实现连续-离散混合优化。\n\n- **分层或级联提示**\n当前  (t,m)  为单层提示。可扩展为“系统提示+任务提示+实例提示”三级，每级均含多模态组件，形成层次贝叶斯先验，进一步压缩搜索空间。\n\n- **动态模态选择**\n并非所有任务都需要图像/视频/分子同时出现。可学习一个“模态策略网络” π(a|q) ，在推理时自动决定启用哪些模态，降低计算与标注成本。\n\n- **鲁棒性与对抗攻防**\n多模态提示引入新的攻击面：对抗噪声图像、误导性分子子结构等。可研究针对  (t,m)  的对抗训练或 certified robustness，保证提示在分布漂移或恶意输入下仍稳定。\n\n3\\. 系统与工程\n\n- **在线/流式优化**\n现实数据常动态到达。需把 MPO 的批处理 Bayesian-UCB 改为在线 bandit 或强化学习（ continual RL ），支持实时更新提示，避免全量重训练。\n\n- **异构评估预算分配**\n不同模态生成成本差异巨大（文本 << 图像 << 视频）。可引入“成本感知的贝叶斯优化”(cost-aware BO)，把 GPU 时间或美元花费直接写进采集函数，实现经济最优。\n\n- **分布式异步搜索**\n大规模实验显示，MPO 的模态生成器与评估器可完全解耦。未来可部署异步并行架构（类似 Hyperband/Vizier），在云端实现千卡级并行提示进化，缩短优化周期。\n\n4\\. 应用与生态\n\n- **更多模态与跨域迁移**\n除图像、视频、分子外，可扩展至音频、3D 点云、时序传感器、网页截图等；并研究“提示迁移”——在医学影像优化的  (t,m)  能否通过轻量微调直接用于遥感或工业检测。\n\n- **领域专家在环（human-in-the-loop）**\n药物研发中化学家的一次反馈成本远高于 GPU。可结合主动学习与偏好学习（RLHF），让专家仅对最有信息量的  (t,m)  给予评分，进一步降低标注负担。\n\n- **法律与伦理审查**\n多模态提示可能生成误导性医学影像、虚假分子结构。需建立“提示审计”协议：记录每次  (t,m)  的生成来源、性能与潜在风险，满足 FDA、EMA 等对 AI 可溯源性的监管要求。\n\n- **多语言-多文化适配**\n当前实验以英文为主。不同语言对同一图像的语义侧重点可能不同，可研究跨语言的多模态提示联合优化，实现“同一图像+不同语言提示”在本地化场景下的性能一致。\n\n5\\. 短期可落地的“小步快跑”\n\n1. 将 MPO 封装成 HuggingFace Gradio 插件，支持上传自定义数据集，一键运行多模态提示优化；\n2. 对 Stable Diffusion 3 或 Sora 级视频生成器做“提示+初始帧”联合搜索，验证 MPO 在创意生成场景的易用性；\n3. 在 MIT 许可下开源优化轨迹与中间提示库，建立社区共享的“多模态提示 zoo”，促进复用与对比。\n\n综上，MPO 首次把提示优化从纯文本推向多模态空间，但“组合搜索”“跨模态对齐”“成本-性能权衡”等方向仍留有大量理论与应用空白，值得长期深耕。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有自动提示优化仅局限纯文本，未能挖掘多模态大语言模型（MLLM）对图像、视频、分子等非文本提示的潜力，导致性能次优。\n- **目标**：首次提出“多模态提示优化”——在联合空间  (t,m)  中寻找最优提示对，使任务性能最大化。\n- **挑战**：①组合空间大，需保持跨模态语义一致；②高质量提示稀疏，冷启动评估代价高。\n- **方法**：Multimodal Prompt Optimizer（MPO）\n1. **对齐保持探索**——用统一文本反馈  ∇_p  同步更新文本  t  与非文本  m ，并通过 Generation/Edit/Mix 三种互补算子广泛探索。\n2. **先验继承 Bayesian-UCB**——利用父子性能强相关（ r=0.88 ）以父代后验 warm-start 子代，加速候选筛选，理论证明可降低采样成本。\n- **实验**：在图像、视频、分子共 10 数据集上，MPO 平均提升 **+6.0 pp**，超越最佳文本-only 方法；消融显示三算子互补、先验策略节省 70% 评估预算，且对多种 backbone/生成器均稳健。\n- **结论**：将提示优化扩展至多模态是释放 MLLM 能力的关键步骤，MPO 为这一新方向提供了可行且高效的通用框架。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Multimodal%20Prompt%20Optimization%3A%20Why%20Not%20Leverage%20Multiple%20Modalities%20for%20MLLMs，链接是https://arxiv.org/pdf/2510.09201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09201。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1760399244936","title":"Webscale-RL: Automated Data Pipeline for Scaling RL Data to Pretraining Levels","description":"Large Language Models (LLMs) have achieved remarkable success through imitation learning on vast text corpora, but this paradigm creates a training-generation gap and limits robust reasoning. Reinforcement learning (RL) offers a more data-efficient solution capable of bridging this gap, yet its application has been constrained by a critical data bottleneck: existing RL datasets are orders of magnitude smaller and less diverse than web-scale pre-training corpora. To address this, we introduce the Webscale-RL pipeline, a scalable data engine that systematically converts large-scale pre-training documents into millions of diverse, verifiable question-answer pairs for RL. Using this pipeline, we construct the Webscale-RL dataset, containing 1.2 million examples across more than 9 domains. Our experiments show that the model trained on this dataset significantly outperforms continual pretraining and strong data refinement baselines across a suite of benchmarks. Notably, RL training with our dataset proves substantially more efficient, achieving the performance of continual pre-training with up to 100$\\times$ fewer tokens. Our work presents a viable path toward scaling RL to pre-training levels, enabling more capable and efficient language models.","url":"https://huggingface.co/papers/2510.06499","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.936Z","metadata":{"authors":["Zhepeng Cen","Haolin Chen","Shiyu Wang","Zuxin Liu","Zhiwei Liu","Ding Zhao","Silvio Savarese","Caiming Xiong","Huan Wang","Weiran Yao"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.06499.pdf","arxivUrl":"https://arxiv.org/abs/2510.06499","arxivId":"2510.06499","coolPaperUrl":"https://papers.cool/arxiv/2510.06499","published":"2025-10-07T22:30:59Z","updated":"2025-10-07T22:30:59.000Z","zh_summary":"大型语言模型（LLMs）通过在海量文本语料上进行模仿学习取得了显著成功，但这种范式会产生训练与生成之间的差距，并限制了稳健推理能力。强化学习（RL）提供了一种更高效的数据解决方案，有能力弥合这一差距，但其应用受制于一个关键的数据瓶颈：现有的RL数据集在规模和多样性上远远不及网络规模的预训练语料。为解决这一问题，我们提出了Webscale-RL流水线，这是一种可扩展的数据引擎，能够系统地将大规模预训练文档转换为数百万个多样化且可验证的问答对，用于强化学习。利用这一流水线，我们构建了Webscale-RL数据集，包含来自9个以上领域的120万个示例。我们的实验证明，在该数据集上训练的模型在一系列基准测试中显著优于持续预训练和强数据精炼基线。值得注意的是，使用我们的数据集进行的RL训练效率大幅提升，可以用多达100倍更少的训练token达到持续预训练的性能。我们的工作为将强化学习扩展到预训练水平提供了可行路径，从而能够打造更强大、更高效的语言模型。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在弥合预训练（pre-training）与强化学习（reinforcement learning, RL）之间的“数据鸿沟”。\n现有大模型预训练依赖万亿级 token 的网页文本，而 RL 所需的可验证问答对规模通常不足百亿 token，且领域狭窄（多为数学、代码）。这种数量级差异导致 RL 难以在通用知识上发挥样本效率优势，被限制在少量后训练任务中。\n\n为此，作者提出 Webscale-RL 自动数据管线，将大规模预训练语料系统性地转换为**百万级、多领域、可验证的问答对**，使 RL 训练首次在数据规模与多样性上可比肩预训练，从而用**少 1–2 个数量级的 token** 获得优于持续预训练的性能，实现“把 RL 放大到预训练级别”的目标。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为两条主线：\n\n1. 为 LLM 提供大规模、可验证训练数据的合成 pipeline；\n2. 在 LLM 训练周期中引入强化学习的算法与数据实践。\n\n主要文献按主题分列如下：\n\n- **大规模预训练/后训练数据合成**\n- RedPajama、FineWeb-Edu、DCLM、Stack-v2 等超 1 T token 网页与代码语料，奠定“web-scale”基础。\n- Cosmopedia、SmolLM3 用 LLM 把网页改写成教科书或指令样本，用于继续预训练或 SFT。\n- NaturalReasoning、Nemotron、OpenThoughts、OpenR1-Math 通过蒸馏教师模型生成 0.2 M–4 M 问答对，覆盖数学、代码、科学，但查询来源有限，难以再放大。\n- **面向推理的 RL 数据构造**\n- DeepScaler 从竞赛题收集 40 K 数学样本，依赖人工标注，扩展性低。\n- OpenMathReasoning、OpenCodeReasoning 用规则或奖励模型筛选网络题解，规模仍在 100 K 级别。\n- **LLM 强化学习算法**\n- PPO、GRPO 等策略优化算法被用于数学、代码、工具调用等任务，代表工作包括 DeepSeek-Math、CodeRL、ToolRL。\n- o1/o3、DeepSeek-R1、Grok-4 等将 RL 扩展到预训练早期或持续 RL，证明长时间 RL 可提升推理，但受限于数据规模。\n- **数据质量提升与课程学习**\n- QuRating、ProX、GDR 用 LLM 打分、程序清洗或再生成来提升预训练语料质量，属于“更好模仿”而非“在线探索”。\n\nWebscale-RL 与上述工作的区别：\n\n- 不依赖教师模型求解，而是**从现成预训练文档直接提取可验证答案**，回避蒸馏天花板；\n- 通过 persona 与领域库控制生成，**把网页多样性原样迁移到 RL 数据**，实现“预训练级”规模与广度；\n- 在 1 M→1 B token 区间与持续预训练进行头对头比较，首次量化 RL 的**百级样本效率优势**。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Webscale-RL 端到端数据管线**，把“万亿级预训练语料”自动转换成“百万级可验证问答对”，使 RL 训练在规模与多样性上首次对齐预训练。核心步骤与机制如下：\n\n1. 四阶段管线\n\n1. 数据过滤：启发式+LLM 并行剔除低信息、非自洽片段。\n2. 领域分类与 Persona 分配：LLM 先打 9+ 领域标签，再为每篇文档分配 1–3 个角色（患者、记者、专家等），强制生成视角多样化。\n3. 可验证 QA 生成：\n- 从领域示范库中少样本提示；\n- 只抽取**短答案**（数字、日期、专有名词），降低生成难度；\n- 问题自带上下文，保证 RL 阶段无需回看原文。\n4. 质量与泄露检查：独立 LLM 验证“答案是否源文档可证”+“问题是否泄露答案”，不通过即丢弃。\n2. 规模与效率设计\n\n- 全链路基于公开 LLM（GPT-4.1/mini），成本远低于人工标注或教师模型蒸馏。\n- 源文档→QA 对为**一对多映射**（1 篇最多 3 persona），天然支持并行放大；只要预训练语料增加，QA 对线性扩展，**理论上可直达“预训练级”数十亿对**。\n3. 训练策略\n\n- 先用 10 k 高质量 SFT 样本做指令对齐，避免 RL 因指令遵循不足而评估失真。\n- 采用 GRPO 算法，二元奖励：最终答案与 ground-truth 完全一致得 1，否则 0；奖励稀疏但可验证，保证信号无偏。\n4. 数据效率验证\n\n- 以“原始预训练语料 token 数”为统一计量单位，RL 仅用 ~10 M token 即可在 MMLU-pro 上媲美 1 B token 的持续预训练，实现 **100× 样本效率**。\n- 在 7 项基准平均得分上，3 B 模型+Webscale-RL 超越同等规模持续预训练 3.4 分，与 7 B 模型差距从 10.6 分缩小到 6.1 分。\n\n通过“把网页文本直接变成可验证 RL 燃料”，论文突破了 RL 数据规模瓶颈，让强化学习在大模型时代真正具备与预训练同量级的扩展路径。\n\nQ4: 论文做了哪些实验？\n\n实验围绕两条主线展开：\n\n1. **Webscale-RL 数据是否带来一致的性能提升**；\n2. **RL 相比持续预训练在“相同原始语料 token 预算”下的缩放效率**。\n\n实验设置与结果要点如下（均基于 Qwen2.5-3B，除特别说明外全部经过 10 k SFT 对齐以消除指令遵循偏差）：\n\n| 实验类别 | 关键条件 | 观测指标 | 主要结论 |\n| --- | --- | --- | --- |\n| 主基准对比 | 7 项综合 benchmark（MMLU-pro、Big-Bench、GPQA-diamond、MATH500、GSM8K、MBPP、EvalPlus） | 绝对得分、与 7 B 差距 | Webscale-RL 平均 52.1 分，超最强数据精炼基线 GDR 3.4 分；与 7 B 差距从 10.6 分缩小到 6.1 分。 |\n| 领域细分 | 通用知识/推理 vs 数学 vs 代码 | 单项得分 | 通用任务提升最大（MMLU-pro +3.8，Big-Bench +2.3），MATH500 从 47.6→58.0，代码任务微增，与语料占比一致。 |\n| 缩放效率 | 固定“原始预训练语料 token”为计量单位，RL 取 1 M、10 M、100 M token 对应 QA 对；持续预训练用同等原文 token | MMLU-pro、Big-Bench、平均 7 项 | 10 M token 的 RL 已打平 1 B token 的持续预训练，效率提升 ≈100×；随数据量增加，RL 曲线斜率显著更陡。 |\n| 数据量消融 | 在 150 k QA 对内部做 25 %、50 %、100 % 采样 | 平均 7 项得分 | 性能随 QA 对数单调上升，未出现平台，验证可继续放大。 |\n| 奖励稀疏性检验 | 二元 vs 5 档细粒度奖励 | MATH500、MMLU-pro | 二元奖励已足以收敛，且推理成本更低，未显著低于多档奖励。 |\n| 泄露与质量审计 | 随机 5 k 样本人工+LLM 二次审查 | 答案可验证率、泄露率 | 可验证率 98.7 %，泄露率 0.9 %，满足 RL 训练要求。 |\n\n综上，实验不仅展示了 Webscale-RL 在多领域的**绝对增益**，也量化了其相对持续预训练的**百级样本效率优势**，为“把 RL 放大到预训练级别”提供了实证支撑。\n\nQ5: 有什么可以进一步探索的点？\n\n- **域再平衡**\n当前代码类 QA 占比低，导致 MBPP/EvalPlus 增益有限。可定向引入 GitHub 星级仓库、Commit diff 与文档，按函数级切片生成 QA，把代码比例提至 30 % 以上，观察是否出现“代码推理跃迁”。\n\n- **连续动作与过程奖励**\n现用二元结果奖励稀疏且延迟。可结合执行反馈（unit-test 通过率、定理证明器状态）设计稠密过程奖励  r_t ，或采用 advantage 分解  A_t = ∑_(k≥ t)γ^(k-t)r_k - V_φ(s_t) ，缩短信用分配路径，加速收敛。\n\n- **可验证性自动扩展**\n除“短答案匹配”外，探索自动构造形式化规范（SMT、Lean 命题）把开放域陈述转化为 machine-checkable claim，使文学、历史等“非精确”领域也能获得 0/1 可验证信号。\n\n- **奖励模型效率**\n每 rollout 都调用一次 LLM judge，GPU 时间占训练总成本 40 %。可训练小型蒸馏奖励模型  r_psi （≤3 B），或采用异步 off-policy 缓存与 Monte-Carlo roll-out 复用，降低 5–10× 推理开销。\n\n- **Scaling Law 细化**\n固定模型参数量（3 B），仅改变 RL 数据量  N_(QA)  与原始语料 token 预算  T ，拟合\n\nL(N_(QA), T) = a N_(QA)^(-α) + b T^(-β) + L_∞\n\n量化  α/β  比值，回答“继续加数据 vs 加 token 哪个更划算”。\n\n- **与继续预训练的正交组合**\n交替阶段：先 RL 100 M token → 继续预训练 1 B token → 再 RL；监测是否出现“能力遗忘—重获”循环，寻找最优混合周期。\n\n- **多模态可验证对**\n把图表、地理栅格、化学结构图与文本同时喂入多模态 LLM，生成“图→短答案”或“文本→图像定位”QA，检验视觉推理是否同样享受样本效率红利。\n\n- **Persona 的因果效应**\n固定同一文档，消融 persona 数量  k=0,1,3,5 ，测量下游任务性能方差，用因果推断 $E\nY|do(k)\n$ 量化“视角多样性”究竟带来多少边际增益。\n\n- **RL 策略初始化策略**\n对比“SFT→RL”与“继续预训练→RL”两条初始化路径，检验继续预训练是否因分布偏移更小反而使 RL 收敛更快，从而修正当前 pipeline 的 warmup 策略。\n\n- **开源与再生产**\n发布管线代码、奖励模型检查点与 10 B 级 QA 对，允许社区复现并提交新领域“可验证转换”插件，形成持续增长的开放 RL 数据枢纽。\n\nQ6: 总结一下论文的主要内容\n\n- **背景**：大模型主流范式是模仿海量文本，存在训练-推理分布偏移，样本效率低；RL 可在线纠错，却受限于 <10 B token 的小规模、窄领域可验证数据，无法与 >1 T token 的预训练语料同日而语。\n- **方法**：提出 Webscale-RL 四阶段自动管线\n\n1. 过滤低质文档\n2. 分类领域并分配多 persona\n3. 少样本生成短答案、自包含 QA\n4. LLM 验证答案正确性与泄露\n把公开预训练语料直接转换成 1.2 M 多领域可验证问答对，规模可随原始 corpora 线性扩展。\n- **实验**：基于 Qwen2.5-3B + GRPO\n- 7 项基准平均提升 3.4 分，与 7 B 模型差距缩小 4.5 分\n- 通用知识/数学增益最大，代码次之\n- 以“原始语料 token”计，RL 用 10 M token 即可媲美 1 B token 持续预训练，实现约 100× 样本效率\n- **结论**：首次将 RL 数据放大到“预训练级别”，验证“在线探索 + 可验证奖励”在通用语言模型上的可扩展性与高样本效率，为后续继续放大 RL 提供可行路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Webscale-RL%3A%20Automated%20Data%20Pipeline%20for%20Scaling%20RL%20Data%20to%20Pretraining%20Levels，链接是https://arxiv.org/pdf/2510.06499，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.06499。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1760399244936","title":"StreamingVLM: Real-Time Understanding for Infinite Video Streams","description":"Vision-language models (VLMs) could power real-time assistants and autonomous agents, but they face a critical challenge: understanding near-infinite video streams without escalating latency and memory usage. Processing entire videos with full attention leads to quadratic computational costs and poor performance on long videos. Meanwhile, simple sliding window methods are also flawed, as they either break coherence or suffer from high latency due to redundant recomputation. In this paper, we introduce StreamingVLM, a model designed for real-time, stable understanding of infinite visual input. Our approach is a unified framework that aligns training with streaming inference. During inference, we maintain a compact KV cache by reusing states of attention sinks, a short window of recent vision tokens, and a long window of recent text tokens. This streaming ability is instilled via a simple supervised fine-tuning (SFT) strategy that applies full attention on short, overlapped video chunks, which effectively mimics the inference-time attention pattern without training on prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a new benchmark with videos averaging over two hours that requires dense, per-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM achieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time performance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy also enhances general VQA abilities without any VQA-specific fine-tuning, improving performance on LongVideoBench by +4.30 and OVOBench Realtime by +5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.","url":"https://huggingface.co/papers/2510.09608","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.936Z","metadata":{"authors":["Ruyi Xu","Guangxuan Xiao","Yukang Chen","Liuning He","Kelly Peng","Yao Lu","Song Han"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.09608.pdf","arxivUrl":"https://arxiv.org/abs/2510.09608","arxivId":"2510.09608","coolPaperUrl":"https://papers.cool/arxiv/2510.09608","published":"2025-10-10T17:59:58Z","updated":"2025-10-10T17:59:58.000Z","zh_summary":"视觉-语言模型（VLMs）可以为实时助手和自主智能体提供动力，但它们面临一个关键挑战：在不增加延迟和内存使用的情况下理解近乎无限的视频流。对整个视频进行全注意力处理会导致平方级的计算成本，并且在长视频上表现不佳。与此同时，简单的滑动窗口方法也存在缺陷，因为它们要么破坏连贯性，要么由于冗余的重新计算而造成高延迟。在本文中，我们提出了StreamingVLM，这是一种旨在实现无限视觉输入的实时、稳定理解的模型。我们的方法是一个将训练与流式推理对齐的统一框架。在推理过程中，我们通过重用注意力汇的状态、最近视觉令牌的短窗口以及最近文本令牌的长窗口来维持一个紧凑的KV缓存。这种流式能力通过一种简单的监督微调（SFT）策略得以实现，该策略在短的重叠视频片段上应用全注意力，有效地模拟了推理时的注意力模式，而无需在过长的上下文上进行训练。为了评估，我们构建了Inf-Streams-Eval，这是一个新基准，视频平均时长超过两小时，需要逐秒地对帧与文本进行密集对齐。在Inf-Streams-Eval上，StreamingVLM在与GPT-4O mini的对比中取得66.18%的胜率，并在单个NVIDIA H100上实现了高达8 FPS的稳定实时性能。值得注意的是，我们的SFT策略还增强了通用视觉问答（VQA）能力，无需任何VQA特定的微调，在LongVideoBench上的性能提升了+4.30，在OVOBench Realtime上的性能提升了+5.96。代码可在 https://github.com/mit-han-lab/streaming-vlm 获得。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**视觉-语言模型（VLM）在无限长视频流实时理解场景下的三大核心矛盾**：\n\n1. **计算复杂度与内存随视频长度二次增长**\n传统 Full Attention 的复杂度为  O(T^2) ，当  Tto∞  时显存与延迟迅速爆炸，无法实时。\n\n2. **滑动窗口策略的连贯性与效率不可兼得**\n\n- 无重叠窗口：每轮重置上下文，破坏长程语义连贯。\n- 有重叠窗口：需对重叠部分重复计算注意力，导致延迟  O(TW^2) ，丧失实时性。\n3. **训练-推理长度不对齐**\n训练阶段无法使用无限长视频，而推理阶段却要求模型在任意长视频上稳定运行；现有 KV-cache 压缩方法未针对跨模态流式场景做训练-推理一致性设计。\n\n为此，作者提出 **StreamingVLM**，通过“训练短片段-推理无限流”的统一框架，在单卡 H100 上实现 **8 FPS、平均 2 小时以上视频的稳定实时解说**，并在新基准 Inf-Streams-Eval 上相对 GPT-4o mini 取得 66.18 % 的胜率。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均与“长上下文、流式推理、视频-语言建模”交叉：\n\n1. **长上下文与流式文本 LLM**\n\n- Attention Sink + 滑动窗口：StreamingLLM（Xiao et al., 2024）用早期“沉锚” token 稳定无限生成。\n- 位置外推：YaRN、LongRoPE、LongLoRA（Peng et al., 2023；Ding et al., 2024；Chen et al., 2024b）通过 RoPE 缩放或微调扩展上下文窗口。\n- KV-cache 压缩：H2O、SnapKV、ReKV（Zhang et al., 2023；Li et al., 2024c；Di et al., 2025）按“重要性”驱逐键值对以降低内存。\n上述方法聚焦纯文本，未解决跨模态流式场景的训练-推理对齐问题。\n2. **视频-语言模型（离线/有限长度）**\n\n- 统一图像-视频架构：LLaVA-OneVision（Li et al., 2024a）、Video-LLaMA 2（Cheng et al., 2024）在短片段上表现良好，但一次性输入全部帧，显存随长度二次增长。\n- 长视频编码器：InternVideo2/2.5（Wang et al., 2024；2025b）、LongVILA（Chen et al., 2025b）通过稀疏采样或序列并行处理数小时视频，然而仍属“离线批处理”，不保证实时低延迟。\n- 实时解说数据集：LiveCC（Chen et al., 2025a）提供 526 k 对齐的〈帧，解说〉对，但未设计流式推理机制，超过 5 min 后性能骤降。\n3. **流式/在线视频理解（同期工作）**\n\n- VideoLLM-online/LIVE（Chen et al., 2024a）把离线数据转成流式对话，但上下文长度固定，未解决无限输入。\n- VideoStreaming（Qian et al., 2024）用固定视觉 token 预算处理长视频，未在训练阶段对齐驱逐策略，长时连贯性不足。\n- ReKV（Di et al., 2025）提出“免训练” KV-cache 检索，然而与任务特定微调后的模型存在格式冲突，常出现无输出。\n\nStreamingVLM 与以上工作的核心区别：\n\n- 首次将“attention sink + 滑动窗口 + 连续 RoPE”范式扩展到**跨模态流式场景**；\n- 提出**重叠短片段全注意力训练**，在仅 24 s 视频片段上模拟推理时的驱逐模式，实现训练-推理严格对齐；\n- 发布 Inf-Streams-Eval 基准，要求**逐秒级帧-文本对齐**，更贴近自动驾驶、机器人等实时应用需求。\n\nQ3: 论文如何解决这个问题？\n\n论文将“无限视频流实时理解”拆解为**训练-推理一体化**的三段式方案，核心思路是：**用短片段训练出长流能力，用固定预算缓存实现无限输入，用连续位置编码保证分布内推理**。具体手段如下：\n\n1\\. 推理端：Streaming-aware KV Cache + Contiguous RoPE\n\n**目标**：在恒定内存与延迟下，保持长时连贯性与实时性。\n\n| 组件 | 设计要点 | 公式/参数 |\n| --- | --- | --- |\n| Attention Sink | 保留最早 512 个文本 token（系统提示+历史解说），稳定注意力分布。 |  T_(sink)=512  |\n| Text Window | 保留最近 512 个文本 token，维护长期语义记忆。 |  T_(window)=512  |\n| Vision Window | 仅保留最近 16 秒视觉 token（≈128 帧），覆盖当前动作。 |  V_(window)=16,s  |\n| Contiguous RoPE | 每驱逐一次，即把后续 token 的 3D-RoPE 索引左移，使有效位置始终落在训练区间  [0, L_(train)]  内，避免外推漂移。 | 索引映射： p' = p - Delta_(evict)  |\n\n**复杂度**：缓存大小恒定 → 每 token 延迟  O(1) ，显存  O(W) ，与视频长度无关。\n\n2\\. 训练端：Overlapped-Chunk Full-Attention SFT\n\n**目标**：只在**短片段**上做全注意力监督，却让模型学会推理时的“sink+窗口”模式。\n\n1. 数据切片\n\n- 每段长  W=24,s ，相邻段重叠  O=12,s ，保证跨段语义连贯。\n- 每秒交错 1 帧+1 句解说（无解说则用占位符“...”），模拟流式输入顺序。\n2. 注意力掩码\n在 24 s 片段内做**全注意力**；但 Loss 只计算**与当前秒对齐的文本位置**，迫使模型：\n\n- 学会利用“早段 sink + 最近文本 + 最近视觉”组合；\n- 学会“何时沉默、何时解说”，实现帧级同步。\n3. 两阶段微调\n\n- 阶段 1：525 k 重叠片段 → 掌握无限流范式。\n- 阶段 2：14 k 高质量“实时动作”片段 → 抑制幻觉，提升人类体验。\n\n3\\. 数据端：Inf-Streams-Train & Inf-Streams-Eval\n\n- **训练集**：4 449 场完整赛事，>4 000 小时，经 GPT-5 清洗+对齐，保证视觉-解说强相关。\n- **评测集**：20 场平均 2.12 小时赛事，每秒人工标注，用 GPT-5 做“裁判”比较模型输出与参考解说，严格考核**长时记忆+实时对齐**。\n\n效果总结\n\n- **延迟**：单卡 H100 上 8 FPS，每 token 延迟 ≤50 ms，稳定 3 小时不漂移。\n- **精度**：在 Inf-Streams-Eval 无限模式下，对 GPT-4o mini 胜率 66.18 %；对 LiveCC-7B 胜率 99.12 %。\n- **零样本 VQA 提升**：未做任何 VQA 专门训练，LongVideoBench +4.30，OVOBench +5.96，证明流式 SFT 亦增强通用视觉能力。\n\nQ4: 论文做了哪些实验？\n\n论文从**精度、效率、稳定性、消融**四个维度展开系统实验，全部基于自建的超长视频解说基准 Inf-Streams-Eval（平均 2.12 小时）及公开 VQA/字幕套件。核心结果如下表所示，正文共 6 组主实验 + 4 组消融。\n\n1\\. 主实验\n\n| 实验目的 | 数据集 | 对比对象 | 关键指标 | 主要结果 |\n| --- | --- | --- | --- | --- |\n| A. 无限解说精度 | Inf-Streams-Eval ∞/† | GPT-4o-mini、LiveCC-7B、ReKV、Qwen2.5-VL-7B | GPT-5 评判 pairwise win rate | StreamingVLM ∞ 胜率达 66.18 %，较第二名的 LiveCC-7B†（15.73 %）提升 50+ pp |\n| B. 短片段解说泛化 | LiveCC-Sports-3K CC (49 类运动，≥10 s) | 同上 | win rate vs 人工参考 | 胜率 56.19 %，全面高于 LiveCC/Gemini/GPT-4o |\n| C. 零样本 VQA 提升 | LongVideoBench / OVOBench / MVBench / VideoMME | 基座 Qwen2.5-VL-7B-Instruct | 准确率 | 无 VQA 微调情况下，LongVideoBench +4.30，OVOBench +5.96，其余持平或略升 |\n| D. 训练-推理一致性 | Inf-Streams-Eval | ReKV（训练无关流式方法） | 能否正常输出 + win rate | ReKV 在 StreamingVLM 上输出为空率 >90 %，win rate=0；原生策略 66.18 % |\n| E. 延迟-长度曲线 | 2 h 足球直播 | Full Attention / Sliding w/o overlap / Sliding w/ overlap | 每 token 延迟 | Full → OOM；w/o overlap 周期性飙升至 180 ms；w/ overlap 保持 120 ms；StreamingVLM 恒 ≤50 ms |\n| F. 长时稳定性 | 五等分 2 h 视频 | 同上 | 分段 win rate | StreamingVLM 在 0–20 %、…、80–100 % 段胜率 66.0–68.5 %，无下降趋势 |\n\n2\\. 消融实验\n\n| 消融维度 | 变量 | 结果摘要 |\n| --- | --- | --- |\n| G. Contiguous RoPE | Native vs Contiguous | Native ∞ 胜率降至 25 %；100 s 分块可回弹到 63 %，但牺牲长程记忆；Contiguous 维持 66 % 且无限流畅 |\n| H. Sink & Text Window 大小 | Tsink/Twindow ∈ {0, 256, 512, 1024} | 512/512 最佳；Tsink=0 掉 5+ pp；证明早期文本沉锚必要 |\n| I. Vision Window 长度 | Vwindow ∈ {0,1,4,8,16,32} s | 16 s 最佳；0 s 掉 13 pp，验证“短时视觉上下文”对动作连贯关键 |\n| J. 训练策略与数据 | ① 基座 ② +Live-WhisperX ③ +Inf-Streams-Train ④ +High-Quality Annealing | 逐步叠加后，Inf-Streams-Eval 胜率从 0.01 → 32.17 → 63.46 → 66.18 %；VQA 同步提升 |\n\n3\\. 额外分析\n\n- **FPS-显存实测**：在 H100 上 8 FPS 时峰值显存 42 GB，与窗口大小成正比，与视频长度无关。\n- **幻觉统计**：随机截取 100 min 连续解说，人工统计事实性错误率 3.8 %，低于 LiveCC 的 9.2 %。\n- **跨运动泛化**：篮球、足球、冰球、棒球、橄榄球五类单独胜率 64–69 %，无显著差异，表明策略不依赖特定运动先验。\n\n综上，实验覆盖了**长时精度、实时延迟、内存占用、训练-推理对齐、模块必要性、数据贡献**等全链路验证，充分说明 StreamingVLM 在“无限视频流实时理解”任务上达到当前最佳水平。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**模型能力、系统效率、数据与评测、场景落地**四大类，均直接对应 StreamingVLM 尚未充分验证或尚未触及的开放问题。\n\n1\\. 模型能力\n\n- **多模态流式协同外推**\n当前 Contiguous RoPE 仅保证视觉-文本位置不越界，但未理论分析 3D-RoPE 在时空维度上的外推极限；可探索 **时空分离的混合位置编码**（时间用 Contiguous，空间用插值），进一步压缩视觉窗口而不掉点。\n\n- **音频-视觉-文本三模态流**\n原文仅利用 ASR 文本，未在模型端联合处理原始音频波形。可引入 **音频 token 流**，与视觉 1 s 对齐，实现“解说+环境声+口型”联合推理，验证是否降低幻觉。\n\n- **事件级记忆而不仅是 token 级记忆**\n目前 KV 缓存为“token 窗口”，当赛事出现超长暂停（>5 min）后恢复，模型仍可能遗忘比分。可探索 **事件摘要 token**（learnable memory token）定期写入 sink，显式维护比赛状态变量。\n\n2\\. 系统效率\n\n- **动态视觉帧率/分辨率调度**\n固定 24 FPS + 360-720P 在静态镜头仍冗余。可引入 **感知哈希差异检测**，仅在画面突变时提升帧率或分辨率，理论上可把视觉 token 量再降 30–50 %，突破 10 FPS 实时上限。\n\n- **KV-cache 量化与异构存储**\n现用 FP16 保存 512+512 文本 KV，占显存大头。可尝试 **INT4/INT8 逐头量化** + CPU 内存换页，把 sink 部分 offload 到主存，需要时通过 PCIe 异步拉回，实现“单卡 24 h+”不间断直播。\n\n- **端侧流式推理**\n将视觉编码器拆成 **MobileVLM** 并在 Orin/NPU 上运行，仅把文本解码留在 GPU；探索 **视觉 token 提前早停**（early-exit）策略，验证在边缘设备上 3-5 W 功耗下能否维持 4 FPS。\n\n3\\. 数据与评测\n\n- **更细粒度时间对齐**\nInf-Streams-Eval 以 1 s 为最小单元，但体育解说常有 0.3 s 延迟要求。可构建 **Frame-wise 250 ms 对齐数据集**，引入专业解说员重录，评测模型在亚秒级延迟下的 BLEU-1 与事实准确率。\n\n- **非体育域无限流基准**\n当前数据与评测集中在体育。可拓展至 **自动驾驶舱外视频、第一视角机器人、监控安防**，各建 100+ 小时流式 benchmark，验证策略跨域鲁棒性。\n\n- **流式多选题评测**\n现有 VQA 为离线问答。可设计 **“正在发生的是 A/B/C？”** 的流式多选，每 5 s 弹出一次，考察模型 **实时因果推理** 而非事后总结。\n\n4\\. 场景落地\n\n- **互动式解说**\n允许观众在直播中随时提问（“刚才越位了吗？”），模型需在 1 s 内检索视觉记忆并回答。可扩展 StreamingVLM 为 **双向流**：输入流（视频）（问题）并行，输出流（解说）（回答）交错。\n\n- **多机位切换**\n实际转播有 8-16 路机位。可研究 **机位 ID 作为额外模态**，在视觉 token 前加 camera embedding，让模型自动决定“用哪一路画面”生成解说，实现 **导演级语义导播**。\n\n- **持续自我监督微调**\n部署后利用观众实时弹幕反馈（“解说错了！”）作为弱监督信号，在线 **强化学习微调** sink 与窗口策略，使模型在赛季中越用越准，而不需重新训练。\n\n5\\. 理论分析\n\n- **窗口大小-记忆容量-幻觉率 三角关系**\n建立概率模型，量化“给定窗口长度 W 与 sink 长度 S，模型在 T→∞ 后的事实漂移率”，为自动调节窗口提供理论依据。\n\n- **跨模态注意力沉锚的梯度分析**\n通过可视化各层注意力，验证“文本 sink 是否真正吸收早期高方差信号”，并探究 **不同模态 sink 的最优比例**（文本 vs 视觉）。\n\n综上，StreamingVLM 已验证“短训长推”可行性，但**多模态外推、事件级记忆、端侧部署、跨域评测、在线学习**等方向仍属空白，值得后续深入。\n\nQ6: 总结一下论文的主要内容\n\n**StreamingVLM：用短片段训练实现无限视频流实时理解**\n\n1\\. 问题\n\n视觉-语言模型要走向自动驾驶、机器人等实时应用，必须**边看边解说**，但面临三重矛盾：\n\n- 全注意力： O(T^2)  显存与延迟随长度爆炸\n- 滑动窗口：无重叠则断片，有重叠则重复计算，均无法实时\n- 训练-推理长度错位：训练无法用无限长视频，推理却要稳定跑无限流\n\n2\\. 方法（统一训练-推理框架）\n\n| 阶段 | 关键机制 | 配置 |\n| --- | --- | --- |\n| 推理 | 固定预算 KV 缓存：attention sink 512 + 近期文本 512 + 近期视觉 16 s | 显存常数，延迟 ≤50 ms |\n| Contiguous 3D-RoPE：索引左移，位置永不离散 | 支持任意长视频 |\n| 训练 | 重叠短片段全注意力：24 s 片段、12 s 重叠，每秒帧-文交错 | 模拟推理时的 sink+窗口模式 |\n| 数据 | 4 000+ 小时体育解说 → 525 k 片段 + 14 k 高质量动作片段 | 两阶段微调，零 VQA 额外数据 |\n\n3\\. 结果\n\n- **精度**：自建 2 h 平均时长基准 Inf-Streams-Eval，对 GPT-4o mini 胜率 66.18 %；LongVideoBench +4.30，OVOBench +5.96\n- **效率**：单卡 H100 稳定 8 FPS，2 h 直播延迟无增长\n- **稳定性**：五段式评测胜率 66–68 %，无尾部衰减\n\n4\\. 贡献一句话\n\nStreamingVLM 用“短训长推”首次让 7 B 模型在单卡上**实时、无限、连贯**地理解视频流，为 VLM 走向真实直播、自动驾驶和机器人提供可行路径。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是StreamingVLM%3A%20Real-Time%20Understanding%20for%20Infinite%20Video%20Streams，链接是https://arxiv.org/pdf/2510.09608，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09608。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-7-1760399244937","title":"R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?","description":"Recent trends in test-time scaling for reasoning models (e.g., OpenAI o1, DeepSeek-R1) have led to remarkable improvements through long Chain-of-Thought (CoT). However, existing benchmarks mainly focus on immediate, single-horizon tasks, failing to adequately evaluate models' ability to understand and respond to complex, long-horizon scenarios. To address this incomplete evaluation of Large Reasoning Models (LRMs), we propose R-HORIZON, a method designed to stimulate long-horizon reasoning behaviors in LRMs through query composition. Based on R-HORIZON, we construct a long-horizon reasoning benchmark, comprising complex multi-step reasoning tasks with interdependent problems that span long reasoning horizons. Through comprehensive evaluation of LRMs using the R-HORIZON benchmark, we find that even the most advanced LRMs suffer significant performance degradation. Our analysis reveals that LRMs exhibit limited effective reasoning length and struggle to allocate thinking budget across multiple problems appropriately. Recognizing these limitations, we use R-HORIZON to construct long-horizon reasoning data for reinforcement learning with verified rewards (RLVR). Compared to training with single-horizon data, RLVR with R-HORIZON not only substantially improves performance on the multi-horizon reasoning tasks, but also promotes accuracy on standard reasoning tasks, with an increase of 7.5 on AIME2024. These results position R-HORIZON as a scalable, controllable, and low-cost paradigm for enhancing and evaluating the long-horizon reasoning capabilities of LRMs.","url":"https://huggingface.co/papers/2510.08189","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.937Z","metadata":{"authors":["Yi Lu","Jianing Wang","Linsen Guo","Wei He","Hongyin Tang","Tao Gui","Xuanjing Huang","Xuezhi Cao","Wei Wang","Xunliang Cai"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.08189.pdf","arxivUrl":"https://arxiv.org/abs/2510.08189","arxivId":"2510.08189","coolPaperUrl":"https://papers.cool/arxiv/2510.08189","published":"2025-10-09T13:16:22Z","updated":"2025-10-09T13:16:22.000Z","zh_summary":"在推理模型的测试时扩展（例如 OpenAI o1、DeepSeek-R1）中的最新趋势，通过长链式思维（Chain-of-Thought, CoT）实现了显著的性能提升。然而，现有的基准测试主要关注即时的、单阶段任务，未能充分评估模型理解和应对复杂、长周期场景的能力。为了解决大推理模型（Large Reasoning Models, LRMs）评估不完整的问题，我们提出了 R-HORIZON，一种通过查询组合激发 LRMs 长周期推理行为的方法。基于 R-HORIZON，我们构建了一个长周期推理基准，包含跨越长推理周期的复杂多步骤、相互依赖的问题。通过使用 R-HORIZON 基准对 LRMs 进行全面评估，我们发现即使是最先进的 LRMs 也存在显著的性能下降。我们的分析表明，LRMs 的有效推理长度有限，并且难以合理分配多个问题的思维资源。认识到这些局限性后，我们使用 R-HORIZON 构建了用于经过验证奖励的强化学习（Reinforcement Learning with Verified Rewards, RLVR）的长周期推理数据。与使用单周期数据训练相比，基于 R-HORIZON 的 RLVR 不仅在多周期推理任务上显著提升了性能，还促进了标准推理任务的准确性，在 AIME2024 上提高了 7.5 分。这些结果使 R-HORIZON 成为一种可扩展、可控且低成本的范式，用于增强和评估 LRMs 的长周期推理能力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对现有大型推理模型（LRM）在\\*\\*长程推理（long-horizon reasoning）\\*\\*场景下的评估与训练缺口，提出并解决以下核心问题：\n\n1. **评估缺口**\n主流基准仅测试孤立、单步（single-horizon）任务，无法衡量模型在**多步且相互依赖**的长序列推理中的表现。\n\n2. **训练缺口**\n强化学习（RL）训练数据同样以单题为主，导致模型缺乏**跨越多步推理**的能力，表现为“有效推理长度”受限、思考预算分配失衡以及过度思考（overthinking）。\n\n3. **方法提出**\n引入 **R-HORIZON** 范式：\n\n- 通过**查询组合（query composition）**将单题串联成具有**显式依赖链**的多步任务。\n- 构建**长程推理评测基准**（覆盖数学、代码、Agent 三类任务），揭示即使最强模型（如 DeepSeek-R1）随推理步数增加也出现**显著性能衰减**。\n- 利用组合数据开展**可验证奖励的强化学习（RLVR）**，在提升多步任务准确率的同时，**单步任务也获得额外增益**（AIME2024 +7.5）。\n\n简言之，论文首次系统性地**量化并缓解了 LRM 在长程推理广度与深度上的不足**，提供可扩展、低成本且可控的评测与训练新范式。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将与自身密切相关的研究划分为两条主线，并指出其局限，进而凸显 R-HORIZON 的差异化价值。以下按 markdown 列表归纳：\n\n- **Test-Time Scaling in LRMs**\n- OpenAI o1、DeepSeek-R1 等通过延长推理链（long CoT）实现测试时计算扩展。\n- 近期工作发现“overthinking”现象：模型在简单题上生成过多 token，边际准确率提升极小。\n- 长度控制策略（L1、Think-less、Adaptive Length Reward）试图抑制冗余推理，但仅针对**单题**场景，无法解决**多步依赖**下的预算分配失效问题。\n- R-HORIZON 首次把“计算预算浪费”问题放到**长序列、多步依赖**条件下系统研究。\n- **Effective Reasoning Length of LRMs**\n- GSM8k、MATH500、AIME 等单题基准被用来探索最优 CoT 长度，发现超过某一阈值后准确率反而下降。\n- NEST 通过**简单拼接**多题暴露模型在多上下文压力下的崩溃，但拼接无逻辑依赖，不能反映真实长程推理。\n- R-HORIZON 进一步**引入显式依赖函数**  f_i(a_i)=m_(i+1) ，构造必须**顺序求解**的复合问题，从而放大并精确诊断模型在长程推理中的失效模式（早期停止、依赖计算错误、反思局部化等）。\n\n综上，现有研究聚焦**单题推理长度**或**无依赖多题拼接**，而 R-HORIZON 首次将“顺序依赖+长序列”作为核心变量，填补了评测与训练范式在长程推理场景下的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过提出 **R-HORIZON** 框架，从“评测–训练–分析”三个环节系统解决长程推理难题，核心手段是\\*\\*查询组合（query composition）**与**可验证奖励强化学习（RLVR）\\*\\*的结合。具体步骤如下：\n\n1. 构造显式依赖的多步任务\n对任意种子题  (q_i, a_i) ，提取关键数值变量  m_i∈ K(q_i) ，定义依赖函数\n\nf_i(x)=x+(m_(i+1)-a_i)\n\n将下一题的对应变量替换为占位符  v_(i+1) ，并追加约束  v_(i+1)=f_i(a_i) 。由此得到必须**顺序求解**的题链  Q=(q_1,q'_2,dots,q'_n) ，模型若中途出错，后续题即不可解。\n\n2. 建立长程评测基准\n\n- 覆盖数学（MATH500/AIME/AMC）、代码（LiveCodeBench）、Agent（WebShaper）共 6 个数据集，题链长度 $n∈\n2,16\n$。\n- 采用**全对或全错**指标\n\nAcc(Q)=hat a_i=a_i,;∀ i\n\n并给出理论期望\n\nAcc_(expected)(Q)=prod_(i=1)^n p_i\n\n用于量化模型随  n  增大的性能衰减。\n3. 生成长程训练数据并做 RLVR\n\n- 用同样组合流程把 Skywork-OR1 的单题训练集改造成 18 k 条多步题链，保证  Acc_(expected)>0.25 。\n- 采用 GRPO 算法，设计两种奖励：\n-  R_(last) ：仅最终答案正确即得 1；\n-  R_(all) ：要求**所有中间答案**全对才得 1。\n- 实验显示，用  n=2  组合数据训练 R1-Qwen-7B，相比单题训练：\n- 多步任务 AIME24 (n=2) 准确率 +17.4%；\n- 单步任务 AIME24 (n=1) 准确率 +7.5%；\n- 响应长度下降，思考预算分配更均衡，过度思考现象缓解。\n4. 系统诊断与消融\n\n- **有效推理长度**：7 B 模型在 4–6 k token、32 B 模型在 8–10 k token 后错误率陡升。\n- **反思范围**：>50 % 的题链无跨题长程反思；组合训练后长程反思率显著提升。\n- **预算分配**：基线模型 60 % token 消耗在前 20 % 题目；组合训练后趋于均匀。\n- **消融**：去掉依赖（独立拼接）后准确率反而更高，证明**依赖链**才是挑战核心；同时发现模型存在“**前题错误却后题正确**”的异常，推测与数据污染有关。\n\n通过以上设计，R-HORIZON 同时实现：\n\n- 可扩展、低成本的长程推理能力**评测**；\n- 直接利用现有单题数据合成**训练**数据，无需人工标注；\n- 显著**提升**模型在多步与单步任务上的准确率，并抑制过度思考。\n\n由此填补了 LRM 在长程推理广度与深度上的评测与训练空白。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“评测—训练—分析”三条主线，共开展 4 组核心实验与多项消融验证，全部基于自建的 R-HORIZON 数据集完成。结果均以“随推理步数 n 增大”的纵向对比为统一变量，确保一致性与可重复性。\n\n1. 长程推理评测实验（25 模型 × 6 任务）\n\n- 数学：MATH500(n=1,2,4,8,16)、AIME24/25、AMC23(n=1–5)\n- 代码：LiveCodeBench(n=1–5)\n- Agent：WebShaper(n=1–5)\n指标：全对准确率 Acc(Q) 与理论期望 Acc\\_expected；最大生成长度 64 k token。\n结论：所有 SOTA 模型（DeepSeek-R1、o4-mini、Qwen3-235B-Thinking 等）均呈**显著衰减**；7 B 模型在 n=16 时准确率跌至 0 %，32 B 模型同设定仍降 34 %。\n2. RLVR 训练对比实验（R1-Qwen-7B）\n\n- 训练数据：原单题数据 vs R-HORIZON 组合数据(n=2)\n- 算法：GRPO，奖励 R\\_last，最大长度 40 k token\n结果：\n- 组合数据在 AIME24(n=2) 上 +17.4 %，在单题 AIME24(n=1) 上 +7.5 %。\n- 训练曲线显示组合数据 reward 更快上升，响应长度最终降低约 15 %。\n3. 组合长度与奖励函数消融（同模型同训练预算）\n\n- 训练数据：n=1,2,4 及 1–4 混合，共 4 组\n- 奖励：R\\_last vs R\\_all\n结果：n=2 综合增益最大；R\\_all 在多题场景下平均再提升 2–3 %。\n4. 错误模式与能力边界诊断\n\n- 错误分类：Problem Reasoning、Dependency Reasoning、Early Stop、Truncation\n- 有效推理长度：记录首次出错位置，7 B 模型集中在 4–6 k token，32 B 模型 8–10 k token\n- 反思统计：长程反思率随 n 增加而上升，但基线仍 <50 %；组合训练后提升 20 % 以上\n- 思考预算：基线模型 60 % token 消耗于第 1 题；组合训练后各题消耗趋于均衡\n5. 消融与鲁棒性验证\n\n- 依赖消融：去掉变量依赖→准确率显著高于顺序依赖版本，验证“依赖链”是挑战核心\n- 指标消融：Acc\\_all vs Acc\\_last 出现显著差异，揭示模型“前题错后题对”的异常现象，提示数据污染\n- 难度排序：easy→hard vs hard→easy，更大模型（32 B、DeepSeek-R1）在 hard→easy 条件下性能提升，而小模型几乎无收益\n\n以上实验共同证明：R-HORIZON 既能**精准暴露**当前 LRM 在长程推理中的瓶颈，也能通过组合数据+RLVR 实现**可扩展、低成本且显著有效**的能力提升。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接延续 R-HORIZON 的框架与代码基线，无需额外人工标注即可展开，具备可验证性与落地价值：\n\n1. 依赖函数空间扩展\n当前仅用线性映射  f_i(x)=x+(m_(i+1)-a_i) 。可尝试：\n\n- 非线性或分段函数，考察模型对**复合代数变换**的鲁棒性；\n- 多变量依赖（即  v_(i+1)=f_i(a_i, a_(i-1)) ），引入**远期依赖**，进一步拉长有效推理长度。\n2. 动态推理预算分配策略\n基线模型在题链前端过度消耗 token。可探索：\n\n- 在 RL 奖励中加入**长度惩罚项**  -λ|o_i| ，或引入**思考-停止**特殊 token，让模型自主决定何时结束当前题；\n- 采用分层策略：先快速生成草稿答案，再按不确定性加权分配二次反思长度。\n3. 多步反思机制显式化\n实验显示 >50 % 题链无跨题反思。可：\n\n- 设计**全局反思槽**（global reflection slot），强制模型在每 k 题后插入一段“跨题一致性检查”；\n- 用 R\\_all 奖励与对比学习联合训练，鼓励模型显式指出前题潜在错误并修正。\n4. 组合复杂度自动课程\n目前手动设置 n=1,2,4…。可：\n\n- 以 Acc\\_expected 为信号，**自动扩增**题链长度与依赖深度，实现课程式 RL；\n- 引入 adversarial composer，对抗性地生成使模型**错误定位最早**的依赖链，用于最难负例挖掘。\n5. 跨模态长程依赖\n现有数学→数学、代码→代码。可构建：\n\n- 数学→代码：先解方程，再用所得参数生成满足约束的程序；\n- Agent→数学：网页检索得到数值，再代入后续数学推导。\n该混合链可检验模型在**异构工具-推理**场景下的连贯性。\n6. 异常“后题对”现象深挖\n实验发现模型前题错却后题对，疑似数据污染。可：\n\n- 用成员推理（membership inference）量化训练集泄漏比例；\n- 在 RL 阶段加入**反污染正则**，对高似然记忆片段施加负奖励，提升组合泛化。\n7. 理论性能边界估计\n当前 Acc\\_expected 仅乘单题通过率。可：\n\n- 引入**误差传播模型**  P_(fail)(n)=1-(1-ε)^n+α n^2 ，估计因依赖链带来的二次误差；\n- 用信息论指标（互信息、熵增）量化题链长度与模型置信度退化速度，指导最优停止策略。\n8. 在线/交互式长程推理\n将 R-HORIZON 从离线一次性生成改为**多轮交互**：\n\n- 每解一题后，环境返回真值并决定是否追加新题；\n- 模型需动态更新内部记忆，支持**终身持续推理**场景，如对话式科研助手。\n9. 小模型专用压缩方案\n7 B 模型在 n≥8 时准确率归零。可：\n\n- 用知识蒸馏让大模型生成**最短充分推理链**作为监督，训练小模型“跳步”能力；\n- 结合 LoRA/ MoE 仅对“依赖计算”子网络增容，实现参数高效的长程推理。\n10. 可验证奖励的自动发现\n目前奖励需人工设计 R\\_last/R\\_all。可：\n\n- 采用形式化验证器（Lean4、Isabelle）自动生成**证明检查信号**，把“证明步合法”作为稠密奖励；\n- 探索自动化定理证明（ATP）反馈，扩展到非数值依赖链，实现更通用的长程推理训练。\n\n上述方向均可在 R-HORIZON 已开源的 pipeline 上直接接入，无需额外人工标注，具备快速验证与持续迭代的优势。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：现有大推理模型（LRM）的评测与训练均聚焦孤立单步任务，无法衡量或培养“多步且相互依赖”的长程推理能力。\n- **方法**：提出 R-HORIZON——通过**查询组合**把单题串联成显式依赖链，构建覆盖数学、代码、Agent 的**长程评测基准**，并生成多步训练数据用于**可验证奖励强化学习（RLVR）**。\n- **实验**：25 个主流 LRM 在基准上随推理步数 n 增大均显著衰减；用组合数据训练 7B 模型，多步任务 AIME24 准确率 +17.4%，单步任务亦 +7.5%，响应长度与过度思考同步下降。\n- **结论**：R-HORIZON 以可扩展、低成本方式同时**暴露**并**提升** LRM 的长程推理广度与深度，为后续复杂推理数据合成与模型训练提供新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是R-Horizon%3A%20How%20Far%20Can%20Your%20Large%20Reasoning%20Model%20Really%20Go%20in%20Breadth%20and%20Depth%3F，链接是https://arxiv.org/pdf/2510.08189，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08189。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1760399244937","title":"BigCodeArena: Unveiling More Reliable Human Preferences in Code Generation via Execution","description":"Crowdsourced model evaluation platforms, such as Chatbot Arena, enable real-time evaluation from human perspectives to assess the quality of model responses. In the coding domain, manually examining the quality of LLM-generated content is extremely challenging, as it requires understanding long chunks of raw code and deliberately simulating code execution. To this end, we introduce BigCodeArena, an open human evaluation platform for code generation backed by a comprehensive and on-the-fly execution environment. Built on top of Chatbot Arena, BigCodeArena enables the execution of LLM-generated code and allows humans to interact with the execution process and outcomes. We collected over 14,000 raw code-centric conversation sessions across 10 widely used LLMs, spanning 10 languages and 8 types of execution environments. Among these conversations, we identified more than 4,700 multi-turn samples with pairwise human preferences. Further analysis uncovers underexplored preferences of LLMs in fine-grained domains characterized by tasks, languages, and frameworks. To systematically examine code understanding and generation capabilities of frontier LLMs, we curated two benchmarks based on the collected data, namely BigCodeReward and AutoCodeArena. For BigCodeReward, we post-processed the 4,700 conversations and evaluated the consistency between reward models and human preferences. The evaluation shows that most LLMs have superior performance in judging coding preferences when the execution results are available. Inspired by these findings, we propose AutoCodeArena, an automatic Elo rating benchmark designed to assess the coding quality of LLMs without human involvement. We find that proprietary LLMs like GPT-5, Claude-Sonnet-4, and Claude-Opus-4 still lead in code generation performance among recent emerging models.","url":"https://huggingface.co/papers/2510.08697","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.937Z","metadata":{"authors":["Terry Yue Zhuo","Xiaolong Jin","Hange Liu","Juyong Jiang","Tianyang Liu","Chen Gong","Bhupesh Bishnoi","Vaisakhi Mishra","Marek Suppa","Noah Ziems","Saiteja Utpala","Ming Xu","Guangyu Song","Kaixin Li","Yuhan Cao","Bo Liu","Zheng Liu","Sabina Abdurakhmanova","Wenhao Yu","Mengzhao Jia","Jihan Yao","Kenneth Hamilton","Kumar Shridhar","Minh Chien Vu","Dingmin Wang","Jiawei Liu","Zijian Wang","Qian Liu","Binyuan Hui","Meg Risdal","Ahsen Khaliq","Atin Sood","Zhenchang Xing","Wasi Uddin Ahmad","John Grundy","David Lo","Banghua Zhu","Xiaoning Du","Torsten Scholak","Leandro von Werra"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.SE","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.08697.pdf","arxivUrl":"https://arxiv.org/abs/2510.08697","arxivId":"2510.08697","coolPaperUrl":"https://papers.cool/arxiv/2510.08697","published":"2025-10-09T18:01:47Z","updated":"2025-10-09T18:01:47.000Z","zh_summary":"众包模型评估平台，如 Chatbot Arena，可以从人类视角实现实时评估，以评估模型响应的质量。在编码领域，手动检查 LLM 生成内容的质量极具挑战性，因为这需要理解大量原始代码片段，并有意模拟代码执行。为此，我们推出了 BigCodeArena，这是一个开放的人类评估代码生成的平台，支持全面、即时的执行环境。BigCodeArena 构建在 Chatbot Arena 基础上，实现了对 LLM 生成代码的执行，并允许人类与执行过程及结果进行交互。我们收集了来自 10 个广泛使用的 LLM、涉及 10 种语言和 8 种执行环境的 14,000 多个以代码为中心的对话会话。在这些对话中，我们识别出超过 4,700 个具有成对人类偏好的多轮样本。进一步分析揭示了 LLM 在任务、语言和框架等细粒度领域中尚未充分探索的偏好。为了系统地检验前沿 LLM 的代码理解和生成能力，我们基于收集的数据策划了两个基准，分别为 BigCodeReward 和 AutoCodeArena。在 BigCodeReward 中，我们对 4,700 个对话进行了后处理，并评估了奖励模型与人类偏好之间的一致性。评估显示，当执行结果可用时，大多数 LLM 在判断编码偏好方面表现优异。受这些发现启发，我们提出了 AutoCodeArena，这是一个自动 Elo 评级基准，旨在在不涉及人类的情况下评估 LLM 的编码质量。我们发现，像 GPT-5、Claude-Sonnet-4 和 Claude-Opus-4 这样的专有 LLM 在代码生成性能上仍然在近期新兴模型中领先。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**大语言模型（LLM）生成代码的可靠人工评估难题**。传统基于静态源码阅读的人工评测方式在代码场景下极易失效，原因包括：\n\n1. 代码片段往往较长，仅凭肉眼检查难以判断其正确性、鲁棒性或是否真正满足需求。\n2. 人类在没有执行反馈时容易误判代码质量，研究表明即使专家也可能高估表面“合理”的错误程序。\n3. 现有众包评测平台（如 Chatbot Arena）仅呈现文本，无法让评估者直观体验程序运行时的行为、UI 效果或可视化结果，导致偏好信号噪声大、可重复性差。\n\n为此，作者提出并落地 **BIGCODEARENA**：一个完全开源、带实时沙箱执行与交互式调试功能的代码生成众包评测平台。通过“边生成-边执行-边体验”的方式，让评估者基于**可观察的运行时行为**而非仅静态源码做出偏好判断，从而收集更可靠、更贴近真实使用场景的人类偏好数据，并进一步构建面向奖励模型与自动评测的两个基准（BIGCODEREWARD、AUTOCODEARENA），推动代码 LLM 的透明、可执行、可重复的社区评估生态。\n\nQ2: 有哪些相关研究？\n\n论文在第 6 节“Related Work”中系统梳理了三条研究脉络，并指出 BIGCODEARENA 与它们的差异。以下按原文逻辑归纳，并补充对应参考文献：\n\n- **Training LLMs on Code**\n- 早期代码专用模型：CodeGen、StarCoder、InCoder 等在大规模源码语料上训练，擅长补全与合成，但自然语言指令跟随能力弱。\n- 后期统一预训练：GPT-3.5、Claude、Gemini、Qwen、DeepSeek 等将代码与文本混合训练，缩小了通用模型与代码专用模型的差距，为“指令驱动的代码生成”奠定基础。\n- **Benchmarking Code Generation Quality**\n- 文本-代码匹配基准：HumanEval、MBPP 通过 docstring→代码的单元测试判定正确性。\n- 实用场景扩展：DS-1000、APIBench、BigCodeBench 强调库调用与复杂指令；Web2Code、Design2Code、Plot2Code 等考察多模态输出（网页、图表、SVG）。\n- 共同局限：静态测试或人工看代码，**缺乏运行时反馈**，难以评估 UI/UX、可视化、交互正确性。BIGCODEARENA 首次把“执行结果可交互”纳入众包评测流程。\n- **Judging LLMs via Human Preference**\n- 对话/视觉 arena：Chatbot Arena、Vision Arena、Search Arena 等用 pairwise 投票计算 Elo，但仅呈现文本或图片。\n- 代码 arena：Copilot Arena 聚焦 IDE 补全；WebDev Arena 仅支持 Next.js 前端，且均未开源数据或执行环境。\n- BIGCODEARENA 差异：\n1. 全栈开源（代码、数据、沙箱）；\n2. 10 语言×8 执行环境，覆盖 Web、游戏、数据科学、图表等场景；\n3. 强制“先执行后投票”，偏好信号直接关联运行时行为。\n\n综上，现有研究要么停留在静态代码测试，要么仅支持有限场景且封闭；BIGCODEARENA 首次把**可执行、可交互、可复现**的众包评估引入代码生成领域。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“平台-数据-基准”三位一体的路线，把“人类难以仅凭源码判断代码好坏”的核心痛点转化为**可执行、可交互、可复现**的评测流程。具体手段如下：\n\n1. 构建 BIGCODEARENA 众包平台\n\n- 双盲+同步执行：两模型返回的代码在沙箱内**同时完成编译与运行**后才一次性展示，消除延迟偏差。\n- 10 语言×8 运行时：覆盖 Python/JS/C++/Java/Go/Rust 与 React/Vue/PyGame/Streamlit/Mermaid 等，真实复现 Web、游戏、数据可视化、算法演示等场景。\n- 交互式调试：用户可实时编辑代码、重新运行、点选 UI，验证功能与鲁棒性后再投票。\n2. 收集高质量人类偏好数据\n\n- 5 个月线上运营，14K 对话、4.7K 带明确偏好的 multi-turn 样本；每模型≥700 票。\n- 加权采样策略：新模型临时升权，保证各模型曝光均衡；Elo 计算采用 100 次 bootstrap 估计置信区间。\n3. 提炼两大基准\n\n- BIGCODEREWARD：用 4.7K 偏好对评测奖励模型，对比“仅看代码”vs“代码+执行结果”两种输入，量化执行反馈对对齐的提升。\n- AUTOCODEARENA：600 代表性提示+本地 Docker 沙箱，用强 LLM（Claude-3.7-Sonnet）做裁判，自动输出 Elo 排名，实现无需人工的“实时 Arena”。\n4. 系统级设计保障可靠性与公平\n\n- 沙箱隔离：E2B 云端/本地 Docker 双模式，一次性容器、非 root 用户、能力降权、资源上限，防止恶意代码与侧信道。\n- 视觉证据优先：裁判 prompt 强制先比较截图/可视化输出，再读源码，降低环境错误导致的误判。\n- 完全开源：平台代码、采样权重、执行镜像、原始对话与偏好数据全部公开，社区可复现、可扩展、可审计。\n\n通过“**先执行后评价**”的机制转换，论文把人类评估从“读代码”升级为“用程序”，显著提升了偏好信号的可靠性与细粒度，为后续奖励模型训练和自动评测提供了可落地的数据与基准。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“执行驱动的代码评测”共开展三类实验，覆盖**平台运营→人类偏好→自动评测**全链路，核心结果如下：\n\n1\\. 平台运营实验（BIGCODEARENA 线上收集）\n\n| 实验目的 | 设置 | 关键结果 |\n| --- | --- | --- |\n| 验证“执行反馈能否让人类更可靠地判断代码优劣” | 5 个月公开运营，10 个前沿模型（o3-mini、Claude-3.5-Sonnet、GPT-4o 等）两两 PK，同步执行后投票 | 14K 对话→4.7K 有效偏好对；o3-mini 与 o1-mini 稳居 Elo 前二，闭源模型整体领先，但开源 Qwen2.5-VL-72B 在部分场景（Diagram、Game）可媲美闭源 |\n| 控制语言/环境混淆因子 | 三种 Elo 计算：All Data / Environment-Matched / Language-Matched | 排名趋势一致，证明执行反馈带来的偏好信号稳定，不受语种或框架差异干扰 |\n| 细粒度胜率热图 | 按编程语言与执行环境拆分 pairwise 胜率 | 同一模型在不同语言/框架下差异显著：Gemini-2.0-Pro 在 Rust 胜率最高，但在 Go 垫底；Qwen 系列在 PyGame、Vue 等交互环境明显下滑 |\n\n2\\. 奖励模型对齐实验（BIGCODEREWARD）\n\n| 实验目的 | 设置 | 关键结果 |\n| --- | --- | --- |\n| 量化“执行输出”对奖励模型准确率的提升 | 16 个开源/闭源 RM 在 4.7K 偏好对上做三分类投票（A/B/Tie），对比“仅看代码”vs“代码+执行结果（截图/日志）” | 平均准确率绝对提升 +6.3%；最大提升出现在 Diagram Creation（+15.4%）与 Game Development（+11.7%）。闭源系列（Claude-Sonnet-4、GPT-4o）领先，开源 Qwen2.5-VL-72B 可达 66.2%，超越部分闭源模型 |\n| 稳定性测试 | Macro-F1、按主题细分 | InternVL3 系列在引入执行结果后反而下降 8-10%，揭示多模态信号对弱 RM 可能存在噪声 |\n\n3\\. 全自动排行榜实验（AUTOCODEARENA）\n\n| 实验目的 | 设置 | 关键结果 |\n| --- | --- | --- |\n| 无需人类，快速更新模型排名 | 600 代表性提示（分布与 4.7K 人工数据一致）→本地 Docker 沙箱并行执行→Claude-3.7-Sonnet 做裁判→Bradley-Terry 计算相对 GPT-4.1 的胜率 | GPT-5 以 79.4% 胜率刷新 SOTA；Claude-Opus-4 与 Claude-Sonnet-4 并列第二（≈72%）。开源模型中 Kimi-K2、GLM-4.5、Qwen3-Coder 率先突破 60%，显著缩小与闭源差距 |\n| 主题级自动评测 | 同样流程在 6 大主题分别计算胜率 | GPT-5 在 Web、Game、Problem Solving 均 >75%；开源模型在 Creative Coding 表现最接近闭源 |\n\n4\\. 相关性 & 可重复性验证\n\n- Spearman 秩相关：BIGCODEARENA vs Chatbot Arena-Coding **ρ=0.68**，vs BigCodeBench **ρ=0.43**，证明“执行+交互”信号与传统静态基准存在显著差异，更接近真实用户偏好。\n- 开源可复现：平台、沙箱镜像、600 提示、裁判 prompt 与原始 14K 对话全部公开，社区可直接重跑实验。\n\n综上，论文通过**大规模在线运营→奖励模型对齐→全自动排行榜**三级实验，系统验证了“执行反馈显著提升代码评测可靠性与效率”的核心假设，并给出可复制的数据与基准。\n\nQ5: 有什么可以进一步探索的点？\n\n论文第 7 节“Future Work”已给出五条明确方向，结合最新结果可进一步细化出以下可探索点：\n\n1. 执行环境与语言版图扩张\n\n- 新增系统级场景：移动（Swift/Kotlin）、嵌入式（Arduino）、异构加速（CUDA、OpenCL）、智能合约（Solidity）。\n- 引入多文件/多服务编排：评测模型对微服务、Serverless、数据库迁移脚本等“非单文件”任务的正确性与部署体验。\n- 支持异步长时任务：允许后台持续运行（>5 min），考察模型对日志、监控、异常重试的生成能力。\n2. 动态与交互式评测深化\n\n- LLM-as-Agent 主动探查：让裁判模型像人类一样点击、输入、缩放、拖拽，生成“交互轨迹”后再打分，而非仅看初始截图。\n- 用户行为克隆：利用 BIGCODEARENA 记录的 5.5K 条真实鼠标-键盘轨迹训练策略模型，实现“模拟人类习惯”的自动测试智能体。\n- 对抗性扰动测试：在沙箱内注入网络延迟、依赖降级、屏幕尺寸变化等扰动，评估模型代码的鲁棒性与自适应提示能力。\n3. 偏好信号细粒度与可解释性\n\n- 多维度拆解：将原本单票“偏好”拆分为 correctness、efficiency、maintainability、accessibility、security 等子项，构建细粒度奖励模型。\n- 可解释投票：要求人类标注者用自然语言给出“缺陷清单”，训练“点评生成”奖励模型，实现“指出具体错误行”的反馈。\n- 因果分析：利用 DoWhy、LiNGAM 等工具估计“执行反馈”对投票结果的因果效应，量化不同视觉元素（布局、配色、动画）对偏好的边际贡献。\n4. 可持续与绿色评估\n\n- 碳排放-性能 Pareto 前沿：记录每次沙箱执行能耗与 GPU/CPU 时间，绘制“胜率 vs CO₂”曲线，鼓励低碳高分模型。\n- 增量评测：对仅权重差分 Δ 的小版本模型，采用“热启动”沙箱缓存依赖，减少 60-80% 重复能耗。\n- 公共碳排仪表盘：实时公开各模型在 AutoCODEARENA 上的累计能耗，推动社区关注评估阶段的可持续性。\n5. 安全、隐私与伦理\n\n- 恶意代码生成探测：在提示中植入“写入敏感文件、反向 shell”等隐蔽需求，评估模型拒绝能力，建立红队基准。\n- 差分隐私数据发布：对 14K 对话进行 DP-SGD 脱敏，公开发布“ε-差分隐私”版本，允许外部在无 PII 风险下做二次分析。\n- 公平性审计：检查不同自然语言 prompt（非英语）或非主流框架（如 Rust、Mermaid）是否被系统性低估，纠正采样权重。\n6. 持续刷新与社区共治\n\n- 活基准（Living Benchmark）：每月用 GPT-4.1 生成 100 条新提示，经社区投票后纳入 AUTOCODEARENA，防止静态题库过拟合。\n- 去中心化沙箱节点：鼓励高校/企业部署符合开放 API 的本地执行节点，形成“联邦评测”网格，降低单云依赖。\n- 激励兼容：对贡献高质量提示、漏洞修复、新语言模板的社区开发者，按“代码影响力-偏好增量”给予算力积分或奖金，形成正循环。\n\n通过上述方向，可把 BIGCODEARENA 从“10 语言×8 环境”的初始形态，扩展为覆盖全栈、全场景、可持续、可解释、安全可信的**代码大模型终身评估基础设施**。\n\nQ6: 总结一下论文的主要内容\n\nBIGCODEARENA 工作可概括为 **“一个平台、两套基准、三大发现”**：\n\n① 一个平台：BIGCODEARENA\n\n- **首个完全开源**的代码生成众包评测平台，支持 10 语言 × 8 执行环境（React、PyGame、Streamlit 等）。\n- **核心机制**：两模型代码**同步编译运行**后一次性展示，用户可交互、编辑、再运行，再投票，确保偏好基于**可观察的运行时行为**而非静态源码。\n- **数据规模**：5 个月线上运营，收集 14K 对话 → 4.7K 高质量 pairwise 偏好对。\n\n② 两套基准\n\n| 基准 | 数据 | 任务 | 结论 |\n| --- | --- | --- | --- |\n| BIGCODEREWARD | 4.7K 人工偏好对 | 评测 16 个奖励模型“是否与人类一致” | 引入执行结果（截图/日志）后，整体准确率 +6.3%；最大提升在 Diagram(+15.4%)、Game(+11.7%)。 |\n| AUTOCODEARENA | 600 代表性提示 | 本地 Docker 沙箱 + LLM-as-Judge 自动排名 | GPT-5 胜率 79.4% 刷新 SOTA；开源 Kimi-K2/GLM-4.5/Qwen3-Coder 突破 60%，显著缩小与闭源差距。 |\n\n③ 三大发现\n\n1. **执行反馈不可或缺**：人类或模型在“仅看代码”时易误判，提供运行截图/交互结果后偏好信号一致性大幅提升。\n2. **闭源仍领先，但开源逼近**：o3-mini、Claude-Sonnet-4、GPT-5 位列前三；开源 Qwen2.5-VL-72B 在部分场景媲美闭源。\n3. **语言/环境差异显著**：同一模型在不同语言或框架下胜率波动大，说明**静态基准难以反映真实部署表现**，需执行驱动评测。\n\n一句话总结\n\nBIGCODEARENA 通过“**先执行后投票**”的开源众包平台，收集并验证了执行反馈对代码评测可靠性的关键作用，发布两大基准，揭示闭源模型领先但开源快速追赶的现状，为社区提供了可复现、可扩展、执行驱动的代码 LLM 评估新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是BigCodeArena%3A%20Unveiling%20More%20Reliable%20Human%20Preferences%20in%20Code%20Generation%20via%20Execution，链接是https://arxiv.org/pdf/2510.08697，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08697。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1760399244937","title":"SpaceVista: All-Scale Visual Spatial Reasoning from mm to km","description":"With the current surge in spatial reasoning explorations, researchers have made significant progress in understanding indoor scenes, but still struggle with diverse applications such as robotics and autonomous driving. This paper aims to advance all-scale spatial reasoning across diverse scenarios by tackling two key challenges: 1) the heavy reliance on indoor 3D scans and labor-intensive manual annotations for dataset curation; 2) the absence of effective all-scale scene modeling, which often leads to overfitting to individual scenes. In this paper, we introduce a holistic solution that integrates a structured spatial reasoning knowledge system, scale-aware modeling, and a progressive training paradigm, as the first attempt to broaden the all-scale spatial intelligence of MLLMs to the best of our knowledge. Using a task-specific, specialist-driven automated pipeline, we curate over 38K video scenes across 5 spatial scales to create SpaceVista-1M, a dataset comprising approximately 1M spatial QA pairs spanning 19 diverse task types. While specialist models can inject useful domain knowledge, they are not reliable for evaluation. We then build an all-scale benchmark with precise annotations by manually recording, retrieving, and assembling video-based data. However, naive training with SpaceVista-1M often yields suboptimal results due to the potential knowledge conflict. Accordingly, we introduce SpaceVista-7B, a spatial reasoning model that accepts dense inputs beyond semantics and uses scale as an anchor for scale-aware experts and progressive rewards. Finally, extensive evaluations across 5 benchmarks, including our SpaceVista-Bench, demonstrate competitive performance, showcasing strong generalization across all scales and scenarios. Our dataset, model, and benchmark will be released on https://peiwensun2000.github.io/mm2km .","url":"https://huggingface.co/papers/2510.09606","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.937Z","metadata":{"authors":["Peiwen Sun","Shiqiang Lang","Dongming Wu","Yi Ding","Kaituo Feng","Huadai Liu","Zhen Ye","Rui Liu","Yun-Hui Liu","Jianan Wang","Xiangyu Yue"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.09606.pdf","arxivUrl":"https://arxiv.org/abs/2510.09606","arxivId":"2510.09606","coolPaperUrl":"https://papers.cool/arxiv/2510.09606","published":"2025-10-10T17:59:46Z","updated":"2025-10-10T17:59:46.000Z","zh_summary":"随着空间推理探索的最新热潮，研究人员在理解室内场景方面取得了显著进展，但在机器人和自动驾驶等多样化应用中仍面临挑战。本文旨在通过解决两个关键问题，推进各种场景下的全尺度空间推理：1）数据集构建对室内3D扫描和劳动密集型手工标注的高度依赖；2）缺乏有效的全尺度场景建模，往往导致对单个场景的过拟合。在本文中，我们提出了一个整体解决方案，集成了结构化空间推理知识系统、尺度感知建模和渐进式训练范式，这是我们知之为止首次尝试将多模态大语言模型（MLLMs）的全尺度空间智能扩展。通过任务特定的、由专家驱动的自动化流程，我们整理了覆盖5种空间尺度的3.8万多个视频场景，创建了SpaceVista-1M数据集，包含约100万条跨19种多样任务类型的空间问答对。尽管专家模型可以注入有用的领域知识，但在评估中并不可靠。随后，我们通过手动记录、检索和组装视频数据构建了具有精确标注的全尺度基准。然而，由于潜在的知识冲突，使用SpaceVista-1M进行简单训练往往会产生次优结果。因此，我们引入了SpaceVista-7B，一种接受超越语义的密集输入并以尺度作为尺度感知专家和渐进式奖励锚点的空间推理模型。最后，在包括我们自己的SpaceVista-Bench在内的5个基准上的广泛评估显示其表现具有竞争力，在所有尺度和场景中展现了强大的泛化能力。我们的数据集、模型和基准将发布在 https://peiwensun2000.github.io/mm2km。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“全尺度视觉空间推理”这一尚未被系统研究的问题，核心挑战体现在两方面：\n\n1. 数据维度\n现有空间推理数据集几乎集中在室内场景（1–30 m），依赖昂贵的 3D 扫描与人工标注，难以覆盖毫米（ 10^(-3)  m）到千米（ 10^(3)  m）的六阶尺度跨度。\n\n2. 模型维度\n主流 MLLM 在跨尺度混合训练时存在“知识冲突”——相同视觉模式在不同尺度下需被赋予不同物理含义，导致过拟合单一尺度、泛化失效。\n\n为此，作者提出一套覆盖数据-模型-评测的完整方案：\n\n- SpaceVista-1M：基于专家模型自动构建，含 38 K 真实视频、约 1 M QA，横跨 5 大尺度与 19 类任务。\n- SpaceVista-7B：在 Qwen2.5-VL-7B 基础上引入\n– 超越语义的自监督稠密特征（DINOv3）增强几何感知；\n– LoRA-like 尺度专家 + 尺度路由器，实现参数高效的尺度解耦；\n– 以尺度为锚点的渐进奖励函数，强制语义→尺度→答案的推理路径。\n- SpaceVista-Bench：手工精确标注的 3 K QA 视频基准，对齐物理世界测量，用于可靠评测。\n\n实验表明，该方案在 5 大公开基准及自建全尺度基准上均取得 SOTA 或可比性能，首次验证了单一模型在 mm–km 范围内的一致空间推理能力。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 章“Related Works”中将相关研究划分为三条主线，并指出其局限，从而凸显 SpaceVista 的差异化价值。以下按主题归纳，并给出关键文献出处（仅列代表，原文引用号保留）：\n\n1. 通用视觉推理 / MLLM\n\n- 通用多模态大模型：LLaVA、Qwen2.5-VL、InternVL3.5 等 \nBai et al. 2025, Wang et al. 2025c\n\n- 视频推理增强：Video-R1、Video-ChatR1、LongViLA 等 \nFeng et al. 2025, Li et al. 2025d, Chen et al. 2024c\n\n→ 局限：空间能力仅为副产品，缺乏显式几何与尺度建模。\n\n2. 空间推理专用数据集与模型\n\n- 室内扫描类：ScanNet、ScanNet++、ARKitScenes、SPAR-7M、SpaceR、InternSpatial \nDai et al. 2017, Yeshwanth et al. 2023, Ouyang et al. 2025, Deng et al. 2025b\n\n- 视频基准：VSI-Bench、STI-Bench、MMSI-Bench、SPAR-Bench \nYang et al. 2025a, Li et al. 2025e, Yang et al. 2025b, Zhang et al. 2025e\n\n- 几何增强模型：Spatial-MLLM、VG-LLM、VILASR \nWu et al. 2025a, Zheng et al. 2025, Wu et al. 2025b\n\n→ 局限：场景单一（室内为主）、尺度狭窄（≈1–30 m）、依赖人工 3D 标注，无法覆盖 mm–km。\n\n3. 多尺度/跨尺度视觉理解\n\n- 深度估计：Metric3D-v2、UniDepthV2、Video-Depth-Anything \nHu et al. 2024, Piccinelli et al. 2025, Chen et al. 2025\n\n- 大场景重建：DL3DV-10K、VGGSfM、VGGT \nLing et al. 2024, Wang et al. 2024a, Wang et al. 2025a\n\n- 极端尺度对象：Uncommon Objects in 3D \nLiu et al. 2025a\n\n→ 局限：侧重重建或深度，未系统研究“推理”任务；数据未公开或缺乏 QA 标注。\n\n综上，现有工作要么聚焦室内小尺度，要么仅提供几何先验而无跨尺度推理框架。SpaceVista 首次将“全尺度”作为独立问题，提出自动化数据生产、尺度解耦模型与物理对齐评测，填补了该空白。\n\nQ3: 论文如何解决这个问题？\n\n论文从 **数据、模型、训练策略、评测** 四个维度协同发力，形成一套可复现、可扩展的全尺度空间推理解决方案。核心思路是：**用“尺度”作为统一锚点**，把六阶量级的物理世界拉平到同一个学习框架里。具体做法如下：\n\n1\\. 数据层：SpaceVista-1M\n\n**目标**：低成本获得 mm→km 的真实场景、带物理真值的 QA 对。\n**关键设计**：\n\n| 模块 | 做法 | 解决痛点 |\n| --- | --- | --- |\n| 来源混合 | 38 K 视频，横跨 5 大尺度（Tiny / Tabletop / Indoor / Outdoor / Drone） | 避免室内偏差 |\n| 自动标注管线 | ① 已知相机内外参 → ② Metric3Dv2+UniDepthV2 深度 → ③ DINO-X/SAM2 实例跟踪 → ④ 19 类任务工作流自动生成 QA | 摆脱人工 3D 标注 |\n| 真值对齐 | 自采 500 段视频，用卷尺/激光测距仪现场测量；户外场景检索维基百科/官方图纸 | 保证评测物理一致性 |\n| 格式统一 | 同一问题同时给出「自然语言+回归值+多选题」三种答案，方便 SFT 与 RL 两用 | 一鱼三吃 |\n\n2\\. 模型层：SpaceVista-7B\n\n**目标**：让 7 B 模型在任意尺度都能“先判尺度、再算几何、最后给答案”。\n**核心模块**：\n\n| 模块 | 公式/机制 | 功能 |\n| --- | --- | --- |\n| 超越语义编码器 |  F'_V = CA(F_V, F'_D, F'_D) + F_V  其中  F'_D  为 DINOv3 稠密特征 | 显式注入深度/法向/模式线索 |\n| 尺度专家路由 |  h = W_0x + ∑_(i=1)^(M)(α_i·λ_i)_(router 输出)B_iA_ix  每层 4 个 LoRA-like 专家，仅占 0.5 % 参数 | 把“室内”“桌面”“户外”“无人机”知识隔离，避免冲突 |\n| 渐进奖励函数 |  bar R_(answer)=R_(semantic)+R_(scale)+R_(answer)  顺序必须按语义→尺度→答案，缺失项自动退化为单奖励 | 强制模型先找参照物、再估尺度、最后出数值，对齐人类思考链 |\n\n3\\. 训练策略：两阶段\n\n| 阶段 | 数据 | 目标 |\n| --- | --- | --- |\n| Cold-start SFT | SpaceVista-1M 含 CoT 的 1 M QA | 让模型学会“先写推理、再给答案” |\n| RL 精调 | 同一批数据转多选/回归格式，用 GRPO 群体相对策略优化 | 在 2.5 k 步内把尺度误差再降 1-2 % |\n\n4\\. 评测层：SpaceVista-Bench\n\n- 3 K 手工 QA，99 % 物理真值，覆盖 5 尺度\n- 公开 Leaderboard，支持后续社区持续提交\n\n5\\. 结果验证\n\n| 基准 | 提升（开源 7 B 档） | 备注 |\n| --- | --- | --- |\n| VSI-Bench | 48.6 % → 新 SOTA | 视频室内 |\n| SpaceVista-Bench | 36.7 %，领先次优 6 % 以上 | 全尺度平均 |\n| 跨尺度泛化 | 在未见过的“无人机+微小物体”混合场景仍优于 GPT-4o | 验证知识解耦有效 |\n\n一句话总结\n\n论文用“**尺度作为第一性原理**”——先自动造出对齐物理世界的多尺度数据，再用“**尺度专家+渐进奖励**”把冲突隔离、把推理路径锁死，最终让 7 B 模型在 mm→km 六阶空间连续体上一次性取得稳健的空间推理能力。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「全尺度空间推理」共设计了 5 组实验，覆盖 **基准评测、消融分析、鲁棒性验证、Scaling-up 探索** 与 **错误诊断**，形成完整证据链。所有实验均在 5 个公开基准 + 自建 SpaceVista-Bench 上进行，结果以表格/雷达图/可视化形式呈现，避免使用冲突符号。\n\n1\\. 主评测：5 大基准横向对比\n\n| 基准 | 类型 | 场景 | 指标 | 最佳开源 7 B 结果 |\n| --- | --- | --- | --- | --- |\n| VSI-Bench | 视频 | 室内 | 平均准确率 | 48.6 % ↑ 6.0 |\n| STI-Bench | 视频 | 混合 | 平均准确率 | 38.2 % ↑ 3.2 |\n| SPAR-Bench | 多图 | 室内 | 平均准确率 | 41.6 % ↑ 4.9 |\n| MMSI-Bench | 多图 | 室内 | 平均准确率 | 32.3 % ↑ 1.6 |\n| SpaceVista-Bench | 视频 | 全尺度 | 平均准确率 | 36.7 % ↑ 6.2 |\n\n> 表格仅列关键数字，↑ 表示相比次优开源模型的绝对提升。\n\n2\\. 消融实验：模块贡献度\n\n在 **Qwen2.5-VL-3B** 上逐模块移除，控制数据与训练步数完全一致：\n\n| 配置 | VSI-Bench | SpaceVista-Bench |\n| --- | --- | --- |\n| 基线（无额外模块） | 44.4 | 31.0 |\n| + 尺度专家路由 | 45.7 (+1.3) | 32.9 (+1.9) |\n| + 渐进奖励 | 46.8 (+2.4) | 35.4 (+4.4) |\n| + DINOv3 稠密特征 | 46.4 (+2.0) | 32.1 (+1.1) |\n\n3\\. 鲁棒性验证：输入质量降级\n\n固定模型权重，仅改变输入分辨率/深度质量：\n\n| 输入降级 | VSI-Bench | SpaceVista-Bench |\n| --- | --- | --- |\n| 原始 | 45.6 | 33.0 |\n| 低分辨率深度 | 43.9 (-4 %) | 32.3 (-2 %) |\n| 完全去掉深度 | 40.7 (-10 %) | 29.1 (-12 %) |\n\n> 2.5D 渲染特征比纯 3D 几何特征下降更小，验证其鲁棒性。\n\n4\\. Scaling-up 分析\n\n在 **同一份 SpaceVista-1M** 上训练不同规模基座：\n\n| 基座 | 参数量 | VSI-Bench | SpaceVista-Bench |\n| --- | --- | --- | --- |\n| Qwen2.5-VL-3B | 3 B | 43.5 | 29.5 |\n| Qwen2.5-VL-7B | 7 B | 46.3 | 34.5 |\n| Qwen2.5-VL-32B | 32 B | 49.0 | 36.3 |\n\n> 性能随参数平稳提升，未出现饱和，说明数据集仍有上行空间。\n\n5\\. 错误诊断与可视化\n\n- **专家激活可视化**（Fig. 5）\n对同层 router 输出 softmax(λ\\_i) 进行阈值化，显示“无人机”场景激活 Outdoor 专家权重最高，Tiny 场景激活 Tiny 专家，验证解耦有效。\n\n- **推理-记忆分离实验**（Tab. D15）\n构造 Seen / Seen-various-scales / Unseen 三个子集，SpaceVista-7B 在「同物不同尺」与「未见物体」上分别提升 2.1 % 与 9.7 %，说明模型真正学到尺度推理而非记忆平均尺寸。\n\n- **奖励曲线**（Fig. D23）\nGRPO 训练 1 k 步后奖励收敛，无 reward hacking 现象。\n\n6\\. 附加实验（附录）\n\n- **2.5D vs 3D 表征**：渲染 12 视角的 2.5D 图比直接喂 VGGT 几何 token 在降级输入下高 8 %。\n- **专家数量**：从 1→4→8 专家，性能增益边际递减，4 专家为效率-精度 sweet spot。\n- **人类一致性**：随机抽 1 K QA 做 MTurk 复标，人类一致率 97.2 %，模型一致率 36.7 %，验证 benchmark 难度合理。\n\n一句话总结\n\n实验从「横向基准-纵向消融-输入扰动-模型扩容-人类对齐」五维度证明：**SpaceVista 的数据+专家+奖励三件套，是 7 B 模型实现 mm→km 稳健空间推理的最小充分条件**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下展望按“数据–模型–任务–评测–应用”五维度列出，均为论文尚未深入或尚未覆盖的方向，可直接作为后续工作切入点。\n\n1\\. 数据层面\n\n- **极端尺度补全**\n– 微米-纳米级：半导体晶圆、MEMS 器件显微视频；需与电子显微镜/AFM 厂商合作采集。\n– 十公里级：卫星遥感视频（Planet SkySat、Sentinel-2 时序）；需解决超大基线、弱纹理、无相机内参问题。\n\n- **动态非刚性场景**\n现有视频以静态为主，可引入流体、形变、可变形物体（布料、柔性 PCB）以考察尺度推理在“形状随时间变化”条件下的鲁棒性。\n\n- **多模态真值**\n同步记录激光跟踪仪、全站仪、毫米波雷达点云，建立“像素-真值”一一映射，用于更严格的误差分析。\n\n2\\. 模型层面\n\n- **尺度连续化**\n当前专家为离散 4 类。可尝试\n– 用混合专家（MoE）把尺度作为连续变量  s∈ R^+  输入 router，实现“无限尺度”平滑切换；\n– 或以神经辐射场（NeRF）/ 3D-GS 为 tokenizer，直接在 3D 空间做度量推理。\n\n- **跨模态尺度对齐**\n同一物体在 RGB、深度、热红外、声学、毫米波下的“尺度感知”不一致。可研究多模态统一度量空间，解决“模态间尺度漂移”。\n\n- **轻量化部署**\n尺度专家目前仍存 7 B 主骨干。可探索\n– 专家蒸馏到 1 B 甚至 200 M，用于无人机/AR 眼镜；\n– 动态专家卸载：只在需要时加载对应 LoRA，减少端侧显存。\n\n3\\. 任务层面\n\n- **4D 尺度推理**\n在“视频+时间”上预测尺度变化，例如：\n– 土方工程 7 天内体积变化；\n– 植物萌芽 72 h 毫米级生长追踪。\n\n- **尺度-因果推理**\n引入干预式问题：“若把 A 物体换成 2× 大小，相机需后移多少才能保持原视角？” 需要反事实模拟能力。\n\n- **语言-尺度双模生成**\n给定文本描述（“直径 3 mm 的螺丝”）→ 模型生成对应尺度视频；或反向生成带度量描述的自然语言报告。\n\n4\\. 评测层面\n\n- **不确定性量化**\n当前仅输出点估计。可增加：\n– 预测分布  p(d)  而非单值  d ，用 NLL 或 CRPS 评分；\n– 校准曲线，检验置信区间是否覆盖真实物理值。\n\n- **对抗尺度攻击**\n设计“尺度对抗补丁”——在图像中植入微小但深度错误的纹理，观察模型是否被误导，评估鲁棒下限。\n\n- **跨文化尺度偏差**\n收集不同国家/地区的“日常参照物”（日本榻榻米、印度 Handa 砖），检验模型是否对特定文化尺寸先验过拟合。\n\n5\\. 应用层面\n\n- **闭环机器人**\n将 SpaceVista-7B 作为尺度感知模块，与机械臂闭环：实时输出“抓取点+移动向量（cm）”，在真机上验证抓取成功率。\n\n- **AR 度量工具**\n替代传统 ARKit/ARCore 平面检测，直接通过单目视频给出毫米级家具尺寸，用于家装/电商场景。\n\n- **自动驾驶长尾**\n– 识别高速路上“罕见超大货物”（风力叶片 60 m）的准确长度，用于路径规划；\n– 地下停车场 3 cm 高减速带检测，避免底盘刮擦。\n\n- **医学显微导航**\n在内窥镜视频中实时推理器械尖端到组织的毫米级距离，辅助医生避免穿孔。\n\n6\\. 理论层面\n\n- **尺度可解释性**\n可视化 router 的尺度决策边界，验证模型是否学到“以物体高度/相机高度比”作为尺度锚点，或仅利用焦距/分辨率隐式线索。\n\n- **最小可辨识尺度**\n系统探究“像素-物理尺寸”极限：给定图像分辨率  r  与物体像素大小  p ，模型在  p/r < ε  时误差突增，可推导  ε  的理论下限。\n\n- **尺度-语义纠缠度量化**\n定义纠缠系数  E = I(scale;semantic) / H(scale) ，衡量不同层专家是否真正解耦；若  E→0  则解耦成功。\n\n总结\n\nSpaceVista 首次把“尺度”显式纳入视觉推理体系，但离“全尺度通用智能”仍有差距。下一步可沿“更极端数据-更连续模型-更因果任务-更严格评测-更闭环应用”五线推进，最终目标是让 AI 像人类一样，对从纳米到行星级的任何物体“一眼即知尺寸”。\n\nQ6: 总结一下论文的主要内容\n\n**SpaceVista 论文核心内容一览**\n\n1. 问题\n现有视觉-语言大模型只能做“室内米级”空间推理，无法覆盖毫米到千米六阶真实尺度，且依赖昂贵人工 3D 标注。\n\n2. 方案总览\n提出**数据-模型-评测**全栈解决方案，首次实现**单模型全尺度空间推理**。\n\n3. 数据：SpaceVista-1M\n\n- 38 K 真实视频，mm→km 五档尺度\n- 自动管线：已知相机参数→度量深度→实例跟踪→19 类任务自动生成 1 M QA\n- 物理对齐：自采实测+权威资料，保证真值\n4. 模型：SpaceVista-7B\n\n- 超越语义：DINOv3 稠密特征与视频 token 交叉融合\n- 尺度专家：LoRA-like 离散专家+路由器，仅 0.5 % 参数解决跨尺度知识冲突\n- 渐进奖励：强制“语义→尺度→答案”链式推理，可解释且易 RL\n5. 评测：SpaceVista-Bench\n3 K 手工精标 QA，99 % 物理真值，覆盖五尺度\n\n6. 结果\n\n- 5 大公开基准全部 SOTA 或可比\n- 全尺度平均领先次优开源模型 6 % 以上\n- 消融证明：专家+奖励+稠密特征各贡献 1-4 % 绝对提升\n7. 意义\n首次让 7 B 模型在**毫米到千米**连续空间实现稳健度量推理，为机器人、AR、自动驾驶、遥感等提供统一空间感知基座。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SpaceVista%3A%20All-Scale%20Visual%20Spatial%20Reasoning%20from%20mm%20to%20km，链接是https://arxiv.org/pdf/2510.09606，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09606。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1760399244938","title":"BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities","description":"Embodied capabilities refer to a suite of fundamental abilities for an agent to perceive, comprehend, and interact with the physical world. While multimodal large language models (MLLMs) show promise as embodied agents, a thorough and systematic evaluation of their embodied capabilities remains underexplored, as existing benchmarks primarily focus on specific domains such as planning or spatial understanding. To bridge this gap, we introduce BEAR, a comprehensive and fine-grained benchmark that evaluates MLLMs on atomic embodied capabilities. BEAR comprises 4,469 interleaved image-video-text entries across 14 domains in 6 categories, including tasks from low-level pointing, trajectory understanding, spatial reasoning, to high-level planning. Extensive evaluation results of 20 representative MLLMs reveal their persistent limitations across all domains of embodied capabilities. To tackle the shortfall, we propose BEAR-Agent, a multimodal conversable agent that integrates pretrained vision models to strengthen MLLM perception, 3D understanding, and planning capabilities. It substantially enhances MLLM performance across diverse embodied capabilities on BEAR, yielding a 9.12% absolute gain and a relative improvement of 17.5% on GPT-5. Furthermore, our experiments indicate that improving MLLM embodied capabilities can benefit embodied tasks in simulated environments. Project website: https://bear-official66.github.io/","url":"https://huggingface.co/papers/2510.08759","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.938Z","metadata":{"authors":["Yu Qi","Haibo Zhao","Ziyu Guo","Siyuan Ma","Ziyan Chen","Yaokun Han","Renrui Zhang","Zitiantao Lin","Shiji Xin","Yijian Huang","Kai Cheng","Peiheng Wang","Jiazheng Liu","Jiayi Zhang","Yizhe Zhu","Wenqing Wang","Yiran Qin","Xupeng Zhu","Haojie Huang","Lawson L. S. Wong"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.CV","cs.RO"],"pdfUrl":"https://arxiv.org/pdf/2510.08759.pdf","arxivUrl":"https://arxiv.org/abs/2510.08759","arxivId":"2510.08759","coolPaperUrl":"https://papers.cool/arxiv/2510.08759","published":"2025-10-09T19:18:36Z","updated":"2025-10-09T19:18:36.000Z","zh_summary":"具身能力指的是代理体感知、理解并与物理世界互动的一系列基本能力。虽然多模态大语言模型（MLLMs）作为具身代理显示出潜力，但其具身能力的全面系统评估尚未充分进行，因为现有基准测试主要集中于特定领域，如规划或空间理解。为弥补这一空白，我们引入了 BEAR，这是一个全面且细粒度的基准，用于评估 MLLMs 的基本具身能力。BEAR 包含 4,469 个交错的图像-视频-文本条目，涵盖 6 类 14 个领域任务，包括从低级的指向、轨迹理解、空间推理到高级规划的任务。对 20 个代表性 MLLMs 的广泛评估结果显示，它们在所有具身能力领域仍存在持续的局限性。为解决这一不足，我们提出了 BEAR-Agent，这是一种多模态可对话代理，整合了预训练视觉模型以增强 MLLM 的感知、三维理解和规划能力。在 BEAR 基准上，它显著提升了 MLLM 在多样具身能力上的表现，在 GPT-5 上实现了 9.12% 的绝对提升和 17.5% 的相对改进。此外，我们的实验表明，提升 MLLM 的具身能力能够促进模拟环境中具身任务的性能。项目网站: https://bear-official66.github.io/","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在系统性地回答三个核心问题：\n\n1. 当前多模态大语言模型（MLLMs）在原子级具身能力（atomic embodied capabilities）上究竟表现如何？\n2. 制约其表现的关键因素是什么？\n3. 如何有针对性地提升这些能力，从而构建更可靠的具身智能体？\n\n为回答上述问题，作者提出 BEAR 基准，首次将“具身能力”拆解为 6 大类 14 项原子技能，并在 4 469 条交错图文视频数据上进行细粒度评测；继而设计 BEAR-Agent，通过引入预训练视觉模型与 3D 理解工具，在 GPT-5 上实现 9.12% 的绝对提升（相对 17.5%），验证增强原子能力可直接转化为仿真环境中超过 20% 的任务成功率增益。\n\nQ2: 有哪些相关研究？\n\n与 BEAR 直接相关的研究可归纳为三类，每类均给出代表性文献及其与本文的差异：\n\n- **多模态大模型（MLLMs）**\n- Flamingo、GPT-4V、InstructBLIP、PaLM-E、Gemini-Robotics 等将视觉编码器与 LLM 衔接，实现零样本视觉问答与指令跟随。\n- 差异：它们聚焦通用 VQA 或高层决策，未系统评估“原子级”具身技能（如指向、轨迹、3D 空间推理）。\n- **具身能力评测基准**\n- 单技能类：Where2Place、Point-Bench、LocateBench、VSI-Bench、EgoPlan 系列仅覆盖指向/空间/规划等单域。\n- 整体任务类：EmbodiedBench、EmbodiedAgentInterface 直接报告任务成功率，未把每一步映射到可解释的原子技能。\n- 差异：BEAR 首次把 14 项原子技能统一在相同 VQA 格式下，并引入“长时程”分解验证技能可组合性。\n- **MLLM 作为具身智能体**\n- SayCan、Voyager、MM-React、MOKA 等用 LLM 做高层规划或关键点生成，但依赖外部控制器或仿真 API。\n- 差异：BEAR-Agent 不修改 MLLM 权重，仅通过对话调用视觉/3D 工具，即可在离线评测与在线操控两端同步提升性能。\n\nQ3: 论文如何解决这个问题？\n\n论文采用“先诊断、后干预”的两段式方案，具体步骤如下：\n\n1. 构建细粒度评测体系\n\n- 提出 BEAR 基准，将“具身能力”形式化为 6 大类 14 项原子技能（指向、包围盒、轨迹推理、空间推理、任务规划、长时程组合）。\n- 从 13 个公开数据集中筛选并人工校验 4 469 条图文/视频交错样本，统一为 VQA 格式；每个样本均含难度控制、干扰项平衡与伦理过滤。\n- 设计“长时程”子集，把 35 个仿真任务拆成可对应原子技能的问答链，验证技能可组合性。\n2. 大规模诊断实验\n\n- 在 20 个代表性 MLLM（含 GPT-5、Gemini-2.5-Pro、InternVL3 等）上执行直接提示评测，发现：\n– 整体准确率仅 20–52%，远低于人类 89%；\n– 闭源模型平均领先开源 13.4 分；\n– CoT 与测试时缩放仅带来 ≤6% 的边际增益；\n– 主要瓶颈为“全视觉能力”（细粒度定位、箭头方向识别）与“3D 空间能力”（egocentric 方向、跨帧对齐）。\n3. 设计可对话增强框架\n\n- 提出 BEAR-Agent：一个基于 AutoGen 的多轮对话代理，内部集成：\n– 视觉工具：GroundingDINO、DepthAnything、Set-of-Mark，用于检测、分割、深度估计；\n– 轨迹工具：箭头延长与颜色判别函数，辅助判断运动方向；\n– 知识库：显式存储“右手法则”“抽屉侧视轨迹应平行于前沿”等物理/操作常识；\n– 语义场景图 & 历史笔记本：跨帧对齐对象、记录子目标，支撑长时程规划。\n- 交互流程：模型在每轮生成 Python 代码调用工具→代理返回可视化结果→模型更新推理，直至输出答案并发送终止信号。\n4. 实验验证\n\n- 在 BEAR 上，BEAR-Agent 将 GPT-5 整体分从 52.17% 提至 61.29%（+9.12 绝对，17.5% 相对），InternVL3-14B 从 33.93% 提至 36.24%；提升覆盖全部 6 大类。\n- 在 ManiSkill 仿真抓取任务中，把 BEAR-Agent 接入 MOKA 关键点评选流程，平均成功率再提升 20.17%，证明原子能力增强可向下游控制任务迁移。\n\n通过“基准→诊断→工具增强→闭环验证”的完整链路，论文不仅定位了 MLLM 在具身场景的关键短板，也给出了可复现、可扩展的改进路径。\n\nQ4: 论文做了哪些实验？\n\n论文共执行三类实验，覆盖“离线评测—在线操控—消融验证”完整链路，具体设置与结果如下：\n\n1 离线能力评测（BEAR 基准）\n\n| 实验目的 | 验证 20 个代表性 MLLM 在 14 项原子技能上的分布表现 |\n| --- | --- |\n| 数据规模 | 4 469 题（Pointing 960、BBox 946、Trajectory 909、Spatial 814、Planning 600、Long-horizon 240） |\n| 评测指标 | Pointing/Spatial/Planning/Long-horizon：准确率 BBox：平均 IoU×100 |\n| 模型列表 | 开源 12 款（InternVL2/3、Qwen2.5-VL、DeepSeek-VL…）闭源 8 款（GPT-4o、GPT-5、Gemini-2.5 系列、Claude-3.7/4 等） |\n| 主要结果 | - 最强模型 GPT-5 仅 52.2%，距人类 89.4% 差距巨大- 闭源平均 39.2%，领先开源 25.8% 达 13.4 分- 长时程类别最难（平均 25%），Pointing 与 BBox 误差主要来自细粒度定位失败 |\n\n2 增强策略对比实验\n\n| 实验目的 | 检验 Chain-of-Thought、Test-time Scaling、BEAR-Agent 各能带来多少增益 |\n| --- | --- |\n| 子实验 A | CoT 提示 vs 直接提示（13 模型） |\n| 结果 | - 整体提升 ≤5%，且对 Spatial Reasoning 普遍负向- CoT 只在 Trajectory/Planning 对闭源模型小幅 (+2~10%) |\n| 子实验 B | Test-time Scaling（BEAR-mini 40 题×6 类） |\n| 方法 | Majority Voting / Best-of-N / Tournament，N=4,8,16，Gemini-2.0-Flash 自评 |\n| 结果 | Best-of-N 最高 +6%，但继续增大 N 反而下降 |\n| 子实验 C | BEAR-Agent（GPT-5 & InternVL3-14B） |\n| 结果 | - GPT-5 绝对 +9.12%，相对 +17.5%；InternVL3-14B +2.3%- 六大类全部正向，Pointing(+11.5%)、BBox(+11.6%)、Trajectory(+14.7%) 增益最大 |\n\n3 仿真操控实验（ManiSkill）\n\n| 实验目的 | 验证“原子能力增强”能否迁移到真实操控任务 |\n| --- | --- |\n| 环境 | ManiSkill2 桌面场景，Franka Panda 臂 |\n| 任务组 | General / Spatial / Part 共 3 组，每组 4 条语言指令（表 9） |\n| baseline | MOKA（GPT-4V + 关键点评选） |\n| 干预 | 把 BEAR-Agent 嵌入 MOKA 的关键点选择阶段，其余管线不变 |\n| 指标 | 20 次 rollout 平均任务成功率 |\n| 结果 | BEAR-Agent 将 MOKA 成功率从 46.3% 提至 66.5%，平均相对提升 20.17%，三组任务一致受益 |\n\n4 模型规模与帧数消融\n\n| 实验目的 | 观察“变大模型”或“加多帧”是否即可自然获得更好具身能力 |\n| --- | --- |\n| 设置 | InternVL2 系列 4B→8B→26B→40B；Qwen2.5-VL 7B→32B；帧数 16→32 |\n| 结果 | 参数继续增大性能呈倒 U 型或饱和；帧数翻倍仅 +1~2%，说明瓶颈在“视觉-空间推理质量”而非数据量 |\n\n通过上述实验，论文既量化了当前 MLLM 的具身短板，也证明“工具-对话式增强”比单纯放大模型或采样次数更能有效提升表现，且改进可直接转化为机器人操控收益。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 BEAR 开启的“可继续挖掘清单”，按“数据-模型-评测-系统”四条线展开：\n\n1 数据与知识\n\n- **真实机器人轨迹注入**\n目前轨迹类题目仍以仿真或 YouTube 级视频为主，若能引入大规模真实机器人 TCP 轨迹（如 BridgeData v3、RH20T），可缓解“知识库手工写规则”的扩展瓶颈。\n- **可微分渲染+反向标注**\n利用可微分仿真（NVIDIA Isaac、Mujoco-X）自动生成带物理正确的箭头、深度、遮挡轨迹，再反向标注到图像，降低人工校验 3D 方向成本。\n- **具身常识图谱自动扩充**\n将 BEAR-Agent 的“右手法则”类条目扩展为持续学习系统：从操作手册、WikiHow、YouTube 字幕自动抽取“对象-动作-轨迹”三元组，持续注入知识库。\n\n2 模型架构\n\n- **原生 3D 视觉 tokenizer**\n当前仍用 2D 检测+深度后处理，可探索：\n- 点云-文本对齐的 3D LLM（类似 Point-LLM、SAM-3D）；\n- 将 Egocentric RT 矩阵作为“位置编码”直接输入 Transformer，让模型自发现 egocentric↔allocentric 映射。\n- **视觉-动作联合预训练**\n借鉴 PaLM-E、RoboFlamingo，但把 BEAR 的 14 类原子技能做成“技能语言建模”目标：给定图文，自回归预测下一条箭头坐标或 3D 关键点，再逆优化视觉骨干。\n- **多模态思维链的自监督**\n利用 BEAR 已标注的“正确箭头”做弱监督信号，训练模型自动生成“中间视觉草稿”（类似 Visual Sketchpad），而非靠人工写 Python 函数。\n\n3 评测与理论\n\n- **在线闭环评测协议**\n目前 BEAR 长时程是“离线问答式”成功率为 40%，下一步把问答链接入真实或仿真闭环：每答对一步才执行动作，观测下一帧，再答下一题，可衡量“错误累积”与“自我纠错”能力。\n- **技能可组合性度量**\n定义“组合难度系数”= 完成整条任务链所需原子技能数 × 各技能模型得分倒数之和，用于预测长时程成功率上限，验证 BEAR 技能划分是否完备。\n- **人类-模型协作评测**\n引入“人在环”干预：允许模型在不确定时向人类发 1 条澄清问题（如“我该向左还是向右？”），评测“最小人类干预率”作为实用落地指标。\n\n4 系统与落地\n\n- **边缘端实时化**\nBEAR-Agent 目前调用 GroundingDINO+DepthAnything 往返云端，延迟 2~3 s。可：\n- 把检测-深度-箭头延长做成 ONNX-TensorRT 级联，运行在 Jetson Orin；\n- 用 8-bit 量化+LoRA 合并，把视觉工具与 7B 级 MLLM 共部署，目标 200 ms 以内。\n- **跨 embodiment 迁移**\n验证“同一套原子技能”是否可零样本迁移到四足导航、无人机抓取、可穿戴臂等不同 embodiment：只需替换知识库中“gripper→paw/rotor”等关键词，观察性能下降曲线。\n- **持续自我改进**\n构建“数据飞轮”：部署 10 台桌面机器人，每天把失败案例（IoU<0.2 或轨迹错）自动回传云端，夜间用拒绝采样+RLHF 微调，次日推送新权重，测量“越用越聪明”的斜率。\n\n5 风险与伦理\n\n- **空间推理偏见**\n发现部分模型在“左-右”指令上呈现系统性颠倒，需研究是否源于训练语料以右利手视角为主，探索数据重平衡或对抗纠错机制。\n- **工具滥用风险**\n当 BEAR-Agent 被允许调用“extend\\_arrow”任意修改图像时，需设计视觉水印+校验码，防止模型生成误导性视觉证据链。\n\n简言之，BEAR 目前完成了“原子技能可评测、可干预”的从 0 到 1；下一步重点是“让模型自己生成工具、自己采集数据、自己在真实世界闭环”，从而走向持续进化的通用具身智能体。\n\nQ6: 总结一下论文的主要内容\n\n**BEAR: Benchmarking and Enhancing Multimodal Language Models for Atomic Embodied Capabilities**\n一句话总结：本文首次系统拆解“具身智能”为 14 项原子技能，发布 4 469 题图文-视频交错基准 BEAR，诊断 20 个 MLLM 的短板，并提出工具增强对话代理 BEAR-Agent，使 GPT-5 绝对提升 9.12%，仿真任务成功率再涨 20%。\n\n1 背景与动机\n\n- 具身智能体需层次化技能：指哪→轨迹→空间→规划。\n- 现有基准只测单域或整任务成功率，无法细粒度诊断。\n- 急需“原子能力”统一标尺与针对性增强方案。\n\n2 BEAR 基准\n\n| 规模 | 4 469 题，13 数据源，64% 单图 / 22% 单视频 / 13% 交错 | | 技能 | 6 大类 14 原子技能：Pointing(3)｜BBox(3)｜Trajectory(3)｜Spatial(3)｜Planning(2)｜Long-horizon | | 特色 | 每题人工≥3 轮校验；难度、干扰、伦理控制；长时程 35 任务可拆成问答链 |\n\n3 大规模评测结果\n\n- 20 模型平均 20–52%，人类 89%；闭源领先开源 13.4 分。\n- CoT 与 Test-time Scaling 仅+6% 边际增益。\n- 主要瓶颈：细粒度定位、箭头方向、egocentric 左右混淆、跨帧对齐。\n\n4 BEAR-Agent 增强框架\n\n- 对话式代理，AutoGen 异步多轮；模型可调用\n– 视觉工具：GroundingDINO、DepthAnything、Set-of-Mark\n– 轨迹工具：箭头延长、颜色判别\n– 知识库：右手法则、抽屉侧视轨迹等 30+ 条操作常识\n– 语义场景图 & 历史笔记本：跨帧对齐、长程记忆\n- 零梯度增强：不改模型权重，仅工具+提示。\n\n5 实验验证\n\n| 离线 | GPT-5 52.17 → 61.29 (+9.12 绝对，+17.5% 相对)，14 项技能全面涨 | | 在线 | ManiSkill 桌面操控：接入 MOKA 关键点评选，成功率再 +20.17% | | 消融 | 模型规模↑/帧数↑无持续增益，验证瓶颈在“视觉-空间推理质量”。\n\n6 贡献与影响\n\n1. 首个原子级具身能力评测体系与公开数据。\n2. 揭示 MLLM 普遍且细粒度的失败模式。\n3. 提供即插即用增强框架，离线&仿真双验证有效。\n\n→ 为构建“能看、会想、善动”的通用具身智能体奠定可评测、可迭代的基础。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是BEAR%3A%20Benchmarking%20and%20Enhancing%20Multimodal%20Language%20Models%20for%20Atomic%20Embodied%20Capabilities，链接是https://arxiv.org/pdf/2510.08759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08759。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1760399244938","title":"KORMo: Korean Open Reasoning Model for Everyone","description":"This work presents the first large-scale investigation into constructing a fully open bilingual large language model (LLM) for a non-English language, specifically Korean, trained predominantly on synthetic data. We introduce KORMo-10B, a 10.8B-parameter model trained from scratch on a Korean-English corpus in which 68.74% of the Korean portion is synthetic. Through systematic experimentation, we demonstrate that synthetic data, when carefully curated with balanced linguistic coverage and diverse instruction styles, does not cause instability or degradation during large-scale pretraining. Furthermore, the model achieves performance comparable to that of contemporary open-weight multilingual baselines across a wide range of reasoning, knowledge, and instruction-following benchmarks. Our experiments reveal two key findings: (1) synthetic data can reliably sustain long-horizon pretraining without model collapse, and (2) bilingual instruction tuning enables near-native reasoning and discourse coherence in Korean. By fully releasing all components including data, code, training recipes, and logs, this work establishes a transparent framework for developing synthetic data-driven fully open models (FOMs) in low-resource settings and sets a reproducible precedent for future multilingual LLM research.","url":"https://huggingface.co/papers/2510.09426","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.938Z","metadata":{"authors":["Minjun Kim","Hyeonseok Lim","Hangyeol Yoo","Inho Won","Seungwoo Song","Minkyung Cho","Junhun Yuk","Changsu Choi","Dongjae Shin","Huige Lee","Hoyun Song","Alice Oh","Kyungtae Lim"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.09426.pdf","arxivUrl":"https://arxiv.org/abs/2510.09426","arxivId":"2510.09426","coolPaperUrl":"https://papers.cool/arxiv/2510.09426","published":"2025-10-10T14:31:25Z","updated":"2025-10-10T14:31:25.000Z","zh_summary":"这项工作首次大规模研究了为主要在合成数据上训练的非英语语言（特别是韩语）构建完全开放的双语大型语言模型 （LLM）。我们介绍了 KORMo-10B，这是一个在韩英语料库上从头开始训练的 10.8B 参数模型，其中 68.74% 的韩语部分是合成的。通过系统实验，我们证明，合成数据在经过精心策划时，具有平衡的语言覆盖范围和多样化的教学风格，在大规模预训练期间不会导致不稳定或退化。此外，该模型在广泛的推理、知识和指令遵循基准中实现了与当代开放权重多语言基线相当的性能。我们的实验揭示了两个关键发现：（1）合成数据可以可靠地维持长期预训练而不会崩溃，（2）双语指令调整使韩语的近乎母语的推理和话语连贯性成为可能。通过全面发布包括数据、代码、训练配方和日志在内的所有组件，这项工作为在低资源环境中开发合成数据驱动的完全开放模型（FOM）建立了一个透明的框架，并为未来的多语言LLM研究树立了可重复的先例。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文《KORMo: Korean Open Reasoning Model for Everyone》旨在解决以下核心问题：\n\n- **非英语语言缺乏完全开放的大型语言模型（FOM）**：当前主流的开源大模型多为英语主导，其他语言（尤其是韩语）面临数据稀缺、版权限制、 tokenizer 设计困难等挑战，导致难以构建可复现、可审计的完全开放模型。\n- **合成数据能否支撑大规模预训练而不引发模型崩溃**：针对低资源语言，论文系统验证以合成数据为主（韩语部分 68.74% 为合成）进行长时间、大尺度预训练的可行性与稳定性，回应“自吞噬”导致模型崩溃的疑虑。\n\n- **双语 tokenizer 与语言混合比例的优化**：在合成数据占比较高的情况下，探索 tokenizer（BPE/Uni-gram/byte-level）、词汇表规模、语言混合比例对压缩效率与下游泛化的影响，给出可复用的设计指南。\n- **合成数据是否会引入源模型的语言文化偏差**：通过定量实验检验合成过程是否会将源模型的偏见迁移至目标语言，损害韩语的语言细节与文化特色。\n\n综上，论文首次系统论证了“以合成数据为主、完全开放”的韩语–英语双语 10B 级大模型训练 pipeline，并公开全部数据、代码、超参与日志，为非英语 FOM 提供可复现范式。\n\nQ2: 有哪些相关研究？\n\n以下研究被论文直接或间接引用，可视为 KORMo 工作的相关背景或对比基线。按主题归类，并给出关键贡献或关联点。\n\n1\\. 完全开放模型（FOM）与可复现性\n\n- **OLMo** (OLMo et al., 2025)\n首次完整公开 7B/13B 英语模型全部训练配方（数据、代码、日志、超参），奠定 FOM 标准范式。\n- **BLOOM** (Workshop et al., 2023)\n176B 多语开放模型，提供多语 tokenizer 与数据治理经验，但韩语占比极低（<0.1%）。\n- **SmolLM3** (Bakouch et al., 2025a)\n近期 1B–3B 级完全开放多语模型，强调推理与长上下文，用作英文基线对比。\n\n2\\. 合成数据与模型崩溃\n\n- **Textbooks are all you need** (Gunasekar et al., 2023; Li et al., 2023)\n证明“教科书风格”合成数据可让小模型在数学/代码任务上媲美大模型。\n- **Nemotron-CC** (Su et al., 2025)\n工业级万亿 token 合成管道，展示大规模网络语料→高质量合成数据→持续预训练的可行性。\n- **Self-consuming loops** (Shumailov et al., 2023a;b; Alemohammad et al., 2023)\n提出“模型崩溃”理论，警告递归生成数据会导致分布漂移与性能下降，KORMo 用 1B proxy 实验直接回应此风险。\n\n3\\. Tokenizer 与双语压缩\n\n- **SentencePiece & BPE** (Kudo & Richardson, 2018; Radford et al., 2019)\n字节级 BPE 消除 OOV，成为多语 tokenizer 默认方案。\n- **Tokenizer 选择对 LLM 影响** (Ali et al., 2024; Chai et al., 2024)\n系统比较词汇表规模、语言比例与下游性能的非单调关系，为 KORMo 的 125k vs 196k 实验提供理论依据。\n\n4\\. 注意力掩码与长上下文\n\n- **Longformer** (Beltagy et al., 2020)\n滑动窗口+局部+全局掩码，启发 KORMo 采用 intra-document causal masking 以抑制跨文档噪声。\n- **ProLong** (Gao et al., 2025b)\n提供 64k 英文长文档语料，被 KORMo 直接复用并截断至 32k 进行 mid-training。\n\n5\\. 多阶段课程预训练\n\n- **Compute-optimal scaling** (Kaplan et al., 2020; Hoffmann et al., 2022)\n提出参数-数据权衡与阶段性课程，KORMo 的两阶段（低质 web→高质+合成+推理）据此设计。\n- **DeepSeek-V3** (DeepSeek-AI et al., 2025b)\n采用 MTP（multi-token prediction）与高质量推理语料，KORMo 在 1B proxy 上对比 NTP vs MTP，发现小模型下 NTP 更稳定。\n\n6\\. 偏好学习与后训练\n\n- **DPO** (Rafailov et al., 2023)、**ORPO** (Hong et al., 2024)\n无需奖励模型的偏好优化方法，KORMo 计划用于后续 RL 阶段。\n- **LIMA** (Zhou et al., 2023)\n证明“少而精”SFT 即可对齐，KORMo 的 Instruction-Following SFT 采用类似的高质量筛选策略。\n\n7\\. 韩语 LLM 与评测\n\n- **KANANA-1.5** (Han et al., 2025)\n目前唯一公开 base 版本的韩语密集模型，用作韩语基线对比。\n- **KMMLU / KoBEST / Haerae** (Son et al., 2025a; Jang et al., 2022a)\n近期发布的韩语知识+推理 benchmark，构成 KORMo 韩语评估套件的核心。\n\n以上研究共同构成了 KORMo 的学术与工程背景，涵盖“开放-合成-双语-稳定”四大关键词。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“先验证、再放大、全公开”的三段式路线，系统性地解决了“非英语、合成数据为主、完全开放”的三大难题。具体手段与对应问题如下：\n\n1\\. 稳定性（RQ1：合成数据会不会让模型崩溃？）\n\n- **1B-proxy 对照实验**\n- 60 B token 规模，100 % 合成 vs 100 % 真实网页语料\n- 监控 loss 尖刺频率、下游 9 项基准\n- 结论：Pre-LN + intra-doc masking 下，合成数据无额外不稳定，平均性能反而↑2.3 pp\n- **放大到 10B 仍保持相同配置**\n- Pre-LN、RMSNorm、SwiGLU、RoPE、GQA 全部沿用，验证“小→大”可迁移\n\n2\\. Tokenizer & 语言混合（RQ2：如何兼顾压缩与泛化？）\n\n- **双语 BPE 压缩实验**\n- 变量：合成/爬取比例、词汇量 125 k vs 196 k、代码域增广\n- 指标：Bytes-Per-Token + 下游 17 项英韩任务\n- 发现：\n– 英语 60 % 合成、韩语 20 % 合成时压缩最佳\n– 196 k 仅↑2–7 % BPT，但参数量↑，下游略降 → 选 125 k\n– 加入代码域可补韩语代码短板 → 最终采用 EPK-125k\n- **语言比例两阶段课程**\n- Stage-1：1 T token，韩语 5.6 %（低质 web 为主）\n- Stage-2：1.8 T token，韩语维持 5.6 %，但引入 100 B 合成+推理，保证高质量信号\n\n3\\. 偏见与质量（RQ3：合成是否会迁移源模型偏见？）\n\n- **三级过滤管道**\n1. 启发式规则（长度、符号、重复等）\n2. Bloom-Filter 跨语料去重（Old-both 模式，韩语去重 70 %，性能↑+2.6 pp）\n3. fastText 质量分类器（自训韩语，Version-2 随机负例最优）\n- **偏见审计**\n- 与 4 个韩语商用 tokenizer 对比，EK/EPK 候选的敏感词数量最低；含爬取数据会↑敏感词，因此终版 tokenizer 仅用合成+开源高质语料\n\n4\\. 数据生产流水线（解决“韩语缺数据”）\n\n- **五路合成策略**\n- Synth-FineWeb2、Synth-UltraFineWeb、Synth-Nemo-HQ、Kosmopedia、Ko-Reasoning\n- 种子、prompt、模型三者均不重复，防止单一模型偏差\n- 总量 100 B token，覆盖百科、QA、数学、代码、对话，经难度筛选后 10 B 进入 Stage-2\n\n5\\. 训练阶段设计（解决“双语能力不均衡”）\n\n- **两阶段预训练 → 两阶段 mid-training → 两阶段 SFT**\n- Pretrain：先粗后精，快速获得语言结构\n- Midtrain：\n– Long-context（32 k）双语 needle-in-haystack 数据 → 英文 99 %/韩文 69 % 召回\n– Reasoning：150 B 英 + 10 B 韩带 `<think>` 轨迹，难度双模型共识过滤，MMLU-Pro ↑8.9 pp\n- SFT：\n– Base-SFT：推理版 vs 混合版（可开关推理）\n– IF-SFT：多轮对话+格式遵循，韩文指令数据量>任何可比开源模型\n\n6\\. 全公开（解决“可复现”）\n\n- **发布内容**\n- 模型：0.4 B→1 B→10.8 B 全 checkpoint\n- 数据：合成语料、去重脚本、质量/偏见过滤代码、tokenizer 训练脚本\n- 训练：超参、日志、loss 曲线、GPU 拓扑、FSDP 配置\n- 评估：26 项英韩 benchmark 脚本与 GPT-4o 评测 prompt\n- 许可证：Apache-2.0 & CC-BY-4.0，允许商用与再分发\n\n结果验证\n\n- **10.8 B 参数，2.9 T token 训练**\n- 英文平均 64.2（与 OLMo2-13B 65.3 相当）\n- 韩文平均 58.2（低于 36 T token 的 Qwen3-8B 63.4，但 token 效率更高）\n- MT-Bench 8.61，KO-MT-Bench 8.75，LogicKor 8.96，均进入 8.5+ 第一梯队\n\n通过“小模型验证风险→放大到 10B→全程开源”的闭环，论文首次证明：\n\n> 在精心控制多样性、质量、语言比例的前提下，**合成数据可成为非英语 FOM 的主粮**，而不会引发模型崩溃或文化漂移。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“合成数据驱动的韩语-英语完全开放模型”共设计 6 组实验，覆盖 tokenizer、架构、预训练、mid-training、SFT 与最终评测。所有实验均先以 1 B/60 B token 的 proxy 模型做“小成本验证”，再直接应用于 10.8 B 目标模型。以下按主题列出关键实验与结论。\n\n1\\. Tokenizer 压缩与下游性能（RQ2）\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 合成比例 | 0 %–100 % 梯度混合（英/韩分别 20 GB） | 英语 60 %、韩语 20 % 合成时 BPT 最高 |\n| 词汇量 | 125 k vs 196 k | 196 k 仅↑2–7 % BPT，下游略降 → 选 125 k |\n| 领域增广 | +代码语料（EPK） | 韩语代码任务↑，整体韩语 BPT↑0.3 |\n| 下游验证 | 9 B 韩+51 B 英继续训练，17 项 benchmark | EPK-125k 韩语平均 39.4，优于 LLaMA tok 36.9 |\n\n2\\. 架构与训练稳定性（RQ1）\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 归一化 | Pre-LN vs MixLN | Pre-LN 平均 43.38 > 41.28，且合成数据无额外 loss spike |\n| 注意力掩码 | Causal / Sliding / Intra-doc / Sliding-Intra | Intra-doc 最优 44.48；合成数据下仍稳定 |\n| 训练目标 | NTP vs MTP（4-token） | 1 B 规模 NTP 43.38 > MTP 41.35；MTP 在 QA 任务局部↑ |\n\n3\\. 预训练超参与语言比例\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 学习率扫描 | {1e-4,…,3e-3}×2000 step | 7e-4 收敛最快且稳定，被选为终版 |\n| 韩语比例 | 1.5 %–10 % 小范围扫描 | ≥5 % 才能稳定高于随机基线；终版 5.6 % |\n| 阶段划分 | 1 T 低质 web → 1.8 T 高质+合成 | Stage-2 英语+4.45 pp，韩语+6.88 pp |\n\n4\\. 合成数据多样性消融（RQ1 深化）\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 单一生成器 | 仅用 Synth-Nemo-HQ（Qwen3-30B） | 图 8 出现不可逆崩溃，韩语平均跌回 Stage-1 水平 |\n| 多生成器 | 5 路种子×多 prompt×多模型 | 稳定提升，证明“多样性”是合成数据生命线 |\n\n5\\. Mid-training 两阶段\n\n5.1 长上下文\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 上下文长度 | 32 k 混合（ProLong+韩文书+needle） | 英文 NIAH 99 %，韩语 69 %；>13 k 韩语开始下降，需后续优化 |\n\n5.2 推理增强\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 难度过滤 | 双模型共识（Qwen-30B & 4B） | 英文 MMLU-Pro ↑8.9 pp，韩语 Clinical-QA ↑4.4 pp |\n| 语言平衡 | 150 B 英 + 10 B 韩 <think> 轨迹 | 韩推理数据虽少，但对高难度任务增益显著 |\n\n6\\. SFT 与偏好学习\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| Base-SFT | 推理版（全轨迹算 loss） vs 混合版（1:1 空轨迹） | 混合版在 MT-Bench 得 8.61，用户可开关推理 |\n| IF-SFT | 多轮+格式+韩文指令（Magpie 风格） | KO-MT-Bench 8.75 > 英文 8.32，证明韩语指令数据优势 |\n| 偏好数据 | APO vs ORPO（100 k×5 模型尺度） | 仅构建数据集，训练留作未来工作 |\n\n7\\. 最终对比评测\n\n| 实验变量 | 设置 | 结论 |\n| --- | --- | --- |\n| 26 项英韩 benchmark | 5-shot 准确率 vs 开源基线 | 英文 64.2（≈OLMo2-13B），韩语 58.2（token 效率最优） |\n| LLM-as-Judge | GPT-4o 统一打分 | 平均 8.61，与 Gemma3-12B 8.56 持平，无 RLHF 即达商业级 |\n\n所有实验均公开脚本与日志，确保可复现。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接延续 KORMo 的开放基础设施，进一步拓展“合成数据 + 非英语 FOM”的边界。按短期（≤6 个月）、中期（6–18 个月）、长期（>18 个月）三档给出可执行课题。\n\n短期（快速验证）\n\n1. **韩语长上下文退化根因**\n\n- 假设：tokenizer 字节对齐不稳定 + 合成长文档主题偏新闻/百科 → 分布外推理\n- 实验：\n– 对比字节级 vs 字符级 BPE 在 32–128 k 范围的 NIAH 曲线\n– 引入韩文小说、剧本、论文等真实长文档 5 B token，观察 69 % → ? % 提升空间\n2. **推理模式 RL 放大**\n\n- 已发布 APO/ORPO 偏好数据，可直接跑 GRPO/DPO\n- 指标：MMLU-Pro、GPQA-main、KMMLU-Pro 绝对分 + 推理长度压缩率\n- 目标：无人工标注前提下，推理版 KORMo-10B 突破 45 %（MMLU-Pro）\n3. **词汇量-参数权衡细粒度扫描**\n\n- 固定 10 B 参数，扫描词汇量 {96 k, 125 k, 196 k, 256 k}\n- 观察压缩率、embedding 参数量、下游任务三者的帕累托前沿，为社区提供选型表\n\n中期（半年到一年半）\n\n4. **多语扩展：日语、越南语低资源三角**\n\n- 利用相同合成管道，保持韩语 5 %、新增日语 5 %、越南语 3 %，英语 87 %\n- 研究问题：三语共享 tokenizer 时，形态差异（黏着 vs 音节 vs 字母）对压缩与语法的耦合影响\n- 输出：首个 10 B 级完全开放东南亚三语模型 + 评测套件 SEA-MMLU\n5. **领域专用 FOM：医疗与法律**\n\n- 韩语临床-QA 已显优势（77 %），继续放大：\n– 合成韩文医学教科书 20 B token（基于 USMLE-KR 题库种子）\n– 法律方向用韩国判例公开数据库（≈1 M 判决文书）做 seed，生成 10 B 合成判例问答\n- 最终发布 KORMo-Med-10B、KORMo-Law-10B 两个领域分支，沿用相同训练脚本\n6. **持续预训练（Continual Pre-training）vs 灾难性遗忘**\n\n- 在 KORMo-10B 上继续训练 1 T 新合成数据，冻结 50 % 最底层 vs 全参更新对比\n- 指标：新域增益 vs 旧域遗忘斜率，给出非英语模型持续学习的最优策略\n7. **文化对齐评测与修正**\n\n- 构建 Korean-Culture-Probe（传统礼仪、历史事件、流行文化共 3 k 题）\n- 若合成数据出现“美式回答”漂移，用对比学习把韩文维基与百科真实片段作为正例，合成片段作为负例，微调 1 epoch，观察文化准确率回升幅度\n\n长期（>18 个月）\n\n8. **自改进循环（Self-Improvement Loop）上限**\n\n- 三代自训练：KORMo-10B → 生成 100 B 新数据 → 训练 KORMo-10B-v2 → 循环第三次\n- 监控每一代在 RealToxicityPrompts、HateXplain 的毒性增长率与任务性能，量化“非英语模型自噬阈值”\n9. **MoE + 合成数据：把韩语做 Expert**\n\n- 8×1.3 B MoE，韩语专属 2 expert，其余共享\n- 研究：固定 10 B 总参，MoE 是否比 Dense 在相同韩语数据量下获得更高 KMMLU？同时保持英文不掉点\n10. **端到端数据治理标准**\n\n- 基于 KORMo 开放日志，提出“非英语 FOM 数据治理 Checklist”（去重强度、质量阈值、合成多样性、文化审核、许可证合规）\n- 目标：被 LF AI & Data 或 ISO 采纳为低资源语言大模型训练参考规范\n\n附加：工具与基准缺口\n\n- **韩语推理链自动评估器**\n目前依赖 GPT-4o 打分，可训练韩语-specific reward model（KR-Reward-1B）降低成本\n- **超长上下文韩语文档集合**\n公开可用的>64 k 韩文小说、论文不足 1 G，急需构建并开源 “KoLong-100M” 语料\n\n以上任何一项均可直接复用 KORMo 已发布的代码、checkpoint 与数据管道，形成“可验证、可合并、可发表”的后续工作。\n\nQ6: 总结一下论文的主要内容\n\n论文《KORMo: Korean Open Reasoning Model for Everyone》首次系统验证了“以合成数据为主、完全开放”的非英语大模型训练范式，核心内容可概括为以下四点：\n\n1\\. 问题与目标\n\n- **非英语缺乏完全开放模型（FOM）**：数据稀缺、版权受限、tokenizer 设计复杂。\n- **合成数据能否支撑大规模预训练而不崩溃**尚缺量化证据。\n- **目标**：构建并公开一个 10 B 级韩语-英语双语模型，68 % 韩语数据为合成，验证稳定性、压缩效率与文化保真度，提供可复现 pipeline。\n\n2\\. 方法论\n\n① 小模型先行验证（1 B/60 B token）\n\n- **稳定性**：Pre-LN + intra-doc 掩码下，100 % 合成 vs 100 % 真实网页无 loss spike，性能↑2.3 pp。\n- **Tokenizer**：英 60 %、韩 20 % 合成时压缩最佳；125 k 词汇量兼顾效率与下游表现。\n- **训练目标**：NTP 在 1 B 规模优于 MTP。\n\n② 数据工程\n\n- **韩语合成五路并进**：FineWeb2、UltraFineWeb、Nemotron-HQ、Cosmopedia、Reasoning，共 100 B token，难度+去重+质量三阶过滤。\n- **语言比例**：固定韩语 5.6 %，两阶段课程（1 T 低质 web → 1.8 T 高质+合成+推理）。\n\n③ 模型架构\n\n- 10.8 B 参数，Pre-LN、RMSNorm、SwiGLU、RoPE、GQA、intra-doc 掩码、flash-attention-3。\n- 自定义 EPK-125k tokenizer，韩语压缩率优于 GPT-4/Llama。\n\n④ 训练后阶段\n\n- **Mid-training**：\n– 长上下文 32 k → 英文 NIAH 99 %，韩语 69 %。\n– 推理增强 150 B 英+10 B 韩带 `<think>` 轨迹，MMLU-Pro ↑8.9 pp。\n- **SFT**：\n– Base-SFT 提供“可开关推理”混合版；Instruction-SFT 强调多轮与格式，韩语指令数据量业界最大。\n- **偏好学习**：构建 100 k 英韩 APO/ORPO 数据集，留待后续 RL。\n\n3\\. 实验结果\n\n- **英文基准**：平均 64.2（与 OLMo2-13B 65.3 相当），推理与常识任务强，知识密集型略弱。\n- **韩语基准**：平均 58.2，Clinical-QA 77.3 领先，KMMLU 略低于 36 T token 的 Qwen3-8B，但 token 效率最高。\n- **指令跟随**：MT-Bench 8.61，KO-MT-Bench 8.75，LogicKor 8.96，无 RLHF 即达商业级水平。\n- **消融**：单一生成器导致 Stage-2 崩溃，多样性是合成数据生命线。\n\n4\\. 开放与影响\n\n- **全量发布**：模型 0.4 B→1 B→10.8 B 所有 checkpoint、3.5 T 原始语料、过滤/合成/训练脚本、超参与日志，Apache-2.0 可商用。\n- **结论**：合成数据在精心控制多样性、质量、语言比例的前提下，可成为非英语 FOM 的主粮，不必然引发模型崩溃或文化漂移。\n- **未来**：提供可复现框架，推动低资源语言透明、可审计、可持续的大模型生态。\n\n一句话总结：\nKORMo 用 2.9 T token、68 % 合成韩语数据训练出 10 B 双语模型，性能媲美同级开源模型，并首次完整公开全部配方，证明“合成+开放”在非英语场景可行。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是KORMo%3A%20Korean%20Open%20Reasoning%20Model%20for%20Everyone，链接是https://arxiv.org/pdf/2510.09426，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09426。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1760399244940","title":"Don't Waste Mistakes: Leveraging Negative RL-Groups via Confidence Reweighting","description":"Reinforcement learning with verifiable rewards (RLVR) has become a standard recipe for improving large language models (LLMs) on reasoning tasks, with Group Relative Policy Optimization (GRPO) widely used in practice. Yet GRPO wastes substantial compute on negative groups: groups in which no sampled response is correct yield zero advantage and thus no gradient. We ask whether negative groups can be leveraged without extra supervision. Starting from a maximum-likelihood (MLE) objective in reward modeling, we show that the MLE gradient is equivalent to a policy gradient for a modified value function. This value function adds a confidence-weighted penalty on incorrect responses, imposing larger penalties on more confident mistakes. We refer to this as \\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples (\\textbf{LENS}). LENS modifies GRPO to assign non-zero, confidence-dependent rewards to incorrect generations, making negative groups informative and converting previously wasted samples into useful gradient updates. On the MATH benchmark with Llama-3.1-8B and Qwen-2.5-3B, the proposed variant consistently outperforms GRPO baseline, with significant gains on harder items. These results demonstrate a principled and practical way to \"rescue\" negative groups, improving efficiency and performance in RLVR.","url":"https://huggingface.co/papers/2510.08696","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.940Z","metadata":{"authors":["Yunzhen Feng","Parag Jain","Anthony Hartshorn","Yaqi Duan","Julia Kempe"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.08696.pdf","arxivUrl":"https://arxiv.org/abs/2510.08696","arxivId":"2510.08696","coolPaperUrl":"https://papers.cool/arxiv/2510.08696","published":"2025-10-09T18:01:44Z","updated":"2025-10-09T18:01:44.000Z","zh_summary":"具有可验证奖励的强化学习（RLVR）已成为改进推理任务大型语言模型（LLM）的标准方法，群体相对策略优化（GRPO）在实践中得到了广泛应用。然而，GRPO 在负组上浪费了大量计算：没有正确的采样响应的组产生零优势，因此没有梯度。我们询问是否可以在没有额外监督的情况下利用负面群体。从奖励建模中的最大似然 （MLE） 目标开始，我们表明 MLE 梯度等同于修改后的价值函数的策略梯度。该价值函数对错误回答增加了置信度加权惩罚，对更自信的错误施加更大的惩罚。我们将其称为 \\textbf{L}ikelihood \\textbf{E}stimation with \\textbf{N}egative \\textbf{S}amples （\\textbf{LENS}）。LENS 修改 GRPO，为不正确的生成分配非零的、依赖于置信度的奖励，使负组提供信息，并将以前浪费的样本转换为有用的梯度更新。在 Llama-3.1-8B 和 Qwen-2.5-3B 的 MATH 基准测试中，所提出的变体始终优于 GRPO 基线，在较难的项目上具有显着收益。这些结果展示了一种“拯救”负群体的原则性和实用方法，提高了 RLVR 的效率和性能。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心关注的问题是：在基于可验证奖励的强化学习（RLVR）框架下，Group Relative Policy Optimization（GRPO）算法对“负样本组”（negative groups）——即一组采样答案全部错误的情形——完全丢弃，导致大量已生成的轨迹被浪费、无法提供任何梯度信号。作者提出并回答：能否在不引入额外监督的前提下，以**有理论依据的方式**利用这些负样本组，使其产生有效的策略梯度？\n\n具体而言，论文试图解决以下子问题：\n\n1. 负样本组在GRPO中贡献为零，造成计算资源浪费，尤其在训练初期和难题上比例高达35%–45%。\n2. 现有方法要么直接丢弃负样本，要么仅对混合组（含正解）进行优化，缺乏对负样本内部差异的利用。\n3. 希望仅依赖**结果可验证的0/1奖励**（无需人工标注或过程奖励）就能让负样本产生非零、且能指导探索的奖励信号。\n\n为此，作者提出“Likelihood Estimation with Negative Samples”（LENS）框架：\n\n- 从最大似然（MLE）角度重新形式化奖励建模，推导出一种**置信度加权**的修正价值函数，对错误答案施加与模型置信度成正比的惩罚。\n- 将修正后的奖励直接嵌入GRPO的优势估计，使负样本组内部因置信度差异而产生非零优势，从而把原本被浪费的样本转化为有用的梯度更新。\n\n综上，论文旨在\\*\\*“救活”负样本组\\*\\*，提升RLVR在数学推理等任务中的样本效率与最终性能。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节与附录 A 中系统梳理了相关研究，可归纳为以下四条主线（按出现频次与关联度排序，不引用第一人称）：\n\n1\\. RLVR 与 GRPO 系列\n\n- **DeepSeekMath**（Shao et al., 2024）首次提出 Group Relative Policy Optimization，用组内均值-方差估计优势，去掉价值网络，成为后续 RLVR 事实基准。\n- **DeepSeek-R1**（Guo et al., 2025）、**Kimi k1.5**（Team et al., 2025）等将 GRPO 扩展到更大规模数学与代码数据，验证了 RLVR 的“涌现”推理行为。\n- 以上工作均把“全负组”优势置零，直接触发本文动机。\n\n2\\. 负样本利用与不对称 REINFORCE\n\n- **Tapered Off-Policy REINFORCE**（Roux et al., 2025）通过重要性采样不对称地削弱负样本信号，缓解 off-policy 不稳定性。\n- **Asymmetric REINFORCE**（Arnal et al., 2025）在 off-policy 场景下直接降低负样本权重，提升收敛速度。\n- **Negative-only Training**（Zhu et al., 2025）将正样本奖励设 0、负样本设 −1，意外提升大 k 的 Pass@k，表明负样本本身携带可学习信号。\n- **Spectral-RPO**（Chen et al., 2025a）、**Gem-from-Stones**（Yang et al., 2025）在全负组内部按子步骤或嵌入相似度分配分数，但需人工规则或过程奖励。\n- 与上述方法不同，LENS 仅依赖**结果可验证的 0/1 奖励**，通过置信度加权把负样本转化为梯度，无需额外标注或子步骤解析。\n\n3\\. 探索（Exploration）与置信度奖励\n\n- **RND-style 探索**（Burda et al., 2018；Gao et al., 2025）用随机网络蒸馏鼓励新颖轨迹，但未在负样本组内做差异化奖励。\n- **熵/熵正则化方法**（Cheng et al., 2025；Zheng et al., 2025）通过最大化策略熵保持多样性，同样不区分负样本内部置信度。\n- **置信度替代奖励**（Prabhudesai et al., 2025；Zhou et al., 2025；Li et al., 2025）用 KL 距或生成概率取代规则奖励，通常只训练百步以内，存在奖励黑客风险。\n- LENS 并不替换规则奖励，而是**在原有 0/1 奖励基础上加一项置信惩罚**，与上述探索目标正交且可叠加。\n\n4\\. 并发解决“负组零梯度”的工作\n\n- **NG-RPO**（Nan et al., 2025）实时调整 baseline，使负组优势非零。\n- **SSPO**（Xu & Ding, 2025）用动态 baseline 达到同样目的。\n- **Entropy-Guided Advantage Shaping**（Le et al., 2025）在负组内部按熵值排序产生优势差异。\n- **Reinforce-ADA**（Xiong et al., 2025）自适应增加难题采样数，减少负组出现概率。\n- 这些研究多为**启发式修正**，LENS 则从 MLE 与策略梯度等价性出发，给出**统计上无偏且一致**的修正目标，被视为更具理论根基的解决方案。\n\n小结时间线（按首次 arXiv 版本）\n\n| 方向 | 代表工作 | 与 LENS 主要区别 |\n| --- | --- | --- |\n| 2024-02 | DeepSeekMath (GRPO) | 负组优势 = 0，触发本文问题 |\n| 2024-05 | Tapered REINFORCE | 削弱负信号，未利用置信度 |\n| 2024-08 | Negative-only Training | 仅用 −1/0 奖励，无置信加权 |\n| 2024-10 | Spectral-RPO / Gem-from-Stones | 需子步骤或嵌入匹配 |\n| 2025-01 | DeepSeek-R1, Kimi k1.5 | 沿用 GRPO，负组仍丢弃 |\n| 2025-03 | NG-RPO, SSPO, Entropy-Shaping | 启发式 baseline 修正 |\n| 2025-06 | LENS（本文） | MLE 推导的置信惩罚，理论一致，plug-in 替换 GRPO 奖励 |\n\n以上研究共同构成了“如何不浪费负样本”这一问题的演进脉络，LENS 通过**奖励建模-策略优化等价性**提供了新的理论支点。\n\nQ3: 论文如何解决这个问题？\n\n论文将“负样本组零梯度”问题转化为**最大似然估计（MLE）与策略梯度等价性**的推导问题，通过三步把负样本“救活”并嵌入 GRPO，实现即插即用。核心流程如下：\n\n1\\. 重新形式化：策略学习 ⇄ 奖励建模\n\n- 假设存在未知“正确概率”  p^_(q,o)∈0,1 ，最优策略应满足\n\np^_(q,o)=(1) / (D(q))π^_(o|q),quad D(q)=l(∑_(o)p^_(q,o)r)^(-1)∈(0,1].\n\n于是学习最优策略  π^_  等价于用 MLE 估计  p^_ 。\n\n- 给定数据集  mathcal D=(q_i,o_i,r_i) ，把策略  π_θ  重参数化为\n\np_θ(q,o)=(1) / (D(q))π_θ(o|q)\n\n并最大化对数似然\n\nmathcal L(θ)=frac1n∑_(i=1)^nl[r_ilogπ_θ(o_i|q_i)+(1-r_i)logl(1-(π_θ(o_i|q_i)) / (D(q_i))r)r].\n\n2\\. 推导“置信加权”策略梯度\n\n对  mathcal L(θ)  求梯度得到\n\n∇_θmathcal L(θ)=frac1n∑_(i=1)^nl[r_i-(1-r_i)(π_θ(o_i|q_i)) / (D(q_i)-π_θ(o_i|q_i))r]_(修正奖励 ) tilde r_i∇_θlogπ_θ(o_i|q_i).\n\n- 当  r_i=1  时  tilde r_i=1 ，与标准策略梯度一致。\n- 当  r_i=0  时  tilde r_i=-(π_θ(o_i|q_i)) / (D(q_i)-π_θ(o_i|q_i))<0 ，**负样本获得非零信号**，且绝对值随置信度  π_θ  增大而增大 → 惩罚“高置信错误”。\n\n3\\. 嵌入 GRPO：即插即用的新奖励\n\n保持 GRPO 的组内均值-方差归一化框架不变，仅把原始奖励  r_i  替换为\n\ntilde r_i=r_i-(1-r_i)barπ_θ^(old)(o_i|q_i)D(q)-barπ_θ^(old)(o_i|q_i),\n\n其中\n\n-  barπ_θ^(old)(o|q)=π_θ^(old)(o|q)^(1/|o|)  用几何平均消除长度偏差；\n-  D(q)  用同组样本做重要性采样估计，并保守取\n\nD(q)=maxl(frac1G∑_(i=1)^G(r_i) / (barπ_θ^(textold))(o_i|q)r)^(-1),;2·max_jbarπ_θ^(old)(o_j|q)r\n\n保证分母为正且数值稳定。\n\n对于**纯负组**，额外引入单超参数  α=0.25  对其梯度做降权，兼顾训练稳定性。\n\n4\\. 效果：负样本组 → 有效梯度\n\n- 负组内部因  barπ_θ^(old)  不同而产生差异，经组标准化后得到**非零优势**， Previously wasted samples now contribute useful gradient updates.\n- 在 MATH 基准上，Llama-3.1-8B 与 Qwen-2.5-3B 的 Pass@1–Pass@16 全部提升，难题子集（Levels 4–5）增益更大，验证“高置信惩罚”驱动对困难问题的探索。\n\n一句话总结\n\n论文**把 MLE 的负样本校正项直接搬到 GRPO 奖励里**，让“全错组”按模型置信度产生差异化优势，无需额外标注即可把原先零梯度的样本转化为有效更新，实现理论一致、实现简洁、性能提升。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“数学推理”场景展开，旨在验证 LENS 能否稳定提升 GRPO 并有效利用负样本组。具体设置与结果如下：\n\n1\\. 训练数据与模型\n\n- **训练集**\n- MATH 训练 split（7 500 题） + DAPO 随机子集（17 500 题）\n- 为验证鲁棒性，另做一组 MATH + Numina-1.5（Li et al., 2024a）实验，结果见附录。\n- **评测集**\n- MATH test（500 题，官方 split）\n- 额外报告 Levels 4–5（高难度）子集结果。\n- **基座模型**\n- Llama-3.1-8B-Instruct\n- Qwen-2.5-3B-Base\n两者均用 temperature=1.0、top-p=1.0 采样，每组问题生成 G=16 条解答，全局 batch=512（32 问题 ×16 样本）。\n\n2\\. 训练协议\n\n- 学习率：Llama 3e-7，Qwen 1e-6；clip ratio=0.2；无 KL 正则。\n- 最大 1 600 步（Llama）/1 400 步（Qwen），全程稳定无崩溃。\n- 负组权重超参 α=0.25，仅对纯负组梯度降权；其余与 GRPO 完全一致，实现“即插即用”。\n\n3\\. 主实验结果\n\n3.1 训练曲线（图 5）\n\n- 全测试集准确率：LENS 在两模型上均持续高于 GRPO，且后期仍在上升。\n- Levels 4–5 子集：差距更大，说明负样本信号主要帮助难题探索。\n\n3.2 Pass@k 对比（表 1，DAPO 训练）\n\n| 模型 | 方法 | Pass@1 | Pass@2 | Pass@4 | Pass@8 | Pass@16 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Llama-3.1-8B | GRPO | 54.09 | 61.09 | 67.05 | 69.29 | 72.70 |\n| LENS | 56.63 | 63.05 | 68.36 | 72.18 | 75.34 |\n| Qwen-2.5-3B | GRPO | 67.06 | 72.86 | 77.12 | 80.83 | 82.67 |\n| LENS | 68.59 | 74.79 | 79.33 | 82.62 | 84.44 |\n\n→ 所有 k 均显著提升，Pass@16 增益 ≥2.6 pp，表明多样性同步增加。\n\n3.3 换数据验证（附录 E，Numina 训练）\n\n曲线趋势与主实验一致，Pass@k 提升幅度相近，验证方法不依赖特定数据集。\n\n4\\. 消融实验（表 2，Llama + Numina）\n\n| 设置 | Pass@1 | Pass@2 | Pass@4 | Pass@8 | Pass@16 |\n| --- | --- | --- | --- | --- | --- |\n| GRPO baseline | 56.88 | 65.42 | 72.08 | 78.34 | 82.80 |\n| 仅改混合组 | 57.42 | 65.82 | 73.08 | 78.80 | 83.20 |\n| 仅改负组 | 58.14 | 66.48 | 73.46 | 79.79 | 83.40 |\n| 完整 LENS | 58.64 | 66.08 | 73.98 | 79.46 | 83.40 |\n\n- 单独启用负组即可取得近完整效果，证实“负样本组零梯度”是主要瓶颈。\n- 混合组微调对 Pass@1 更友好，两者结合获得最佳平衡。\n\n5\\. 额外观测\n\n- **训练稳定性**：连续 1 500+ 步无 reward hacking 或崩溃，验证置信惩罚不会导致过度探索。\n- **负组比例**：LENS 训练后期负组比例与 GRPO 相近，但其内部差异被充分利用，而非丢弃。\n- **超参敏感性**：α∈\n0.1,0.5\n 均能带来增益，0.25 被设为默认。\n\n一句话总结\n\n论文在两大模型、两套训练集、超千步训练上系统对比，结果显示 LENS 稳定超越 GRPO，且主要增益来自“负样本组”被重新激活，与理论预期完全一致。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为对 LENS 框架的“直接延伸”或“底层追问”，均未被本文系统实验，具备理论与应用双重价值：\n\n1\\. 理论侧：负样本权重与超参自由化\n\n- **零超参目标**：当前仅用单参数 α 调和负组/混合组梯度，能否从渐近方差-偏差权衡出发，推导出**自适应权重** α(q) 甚至完全取消 α？\n- **非二元奖励**：若 p\\*(q,o)∈\n0,1\n（部分正确、软标签），定理 1 仍然成立，但需重新估计 D(q) 与置信惩罚强度；可系统实验软标签场景（如人类打分、模型自洽性）。\n- **收敛速度与集中不等式**：LENS 的修正奖励引入额外方差，能否给出有限样本意义下的梯度方差上界，并据此设计“方差缩减”版本？\n\n2\\. 算法侧：与价值函数、优势估计的深度耦合\n\n- **引入价值网络**：GRPO 无 critic，若允许轻量级价值网络，可把 LENS 的置信惩罚视为“固有奖励”，学习 V(q) 以进一步降低方差。\n- **多步 Bellman 扩展**：将置信惩罚写成即时奖励 r\\_t，构建多步回报，考察在长链推理（≥8k tokens）上的稳定性。\n- **与熵/信息增益联合**：同时优化策略熵与置信惩罚，探索“高熵-低置信”区域，防止过早收敛到局部模式。\n\n3\\. 场景侧：走出数学推理\n\n- **代码生成**：单元测试可给出 0/1 奖励，负样本同样海量；验证 LENS 是否提升 Pass@k 与代码修复能力。\n- **工具调用 / 多轮决策**：最终成功标志可验证，但中间步骤全错；LENS 的置信惩罚或可缓解“工具死锁”现象。\n- **对话-安全性**：在安全过滤任务中，负样本（有害回复）内部也有“高置信违规”与“边缘违规”之别，可用 LENS 降低高危违规概率。\n\n4\\. 工程侧：难度函数 D(q) 的在线估计\n\n- **增量更新**：当前每步重算 D(q) 需保留全组样本，能否用 streaming 估计或指数移动平均，降低内存？\n- **问题-难度模型**：用少量特征（题干长度、知识点 ID、历史准确率）直接回归 D(q)，实现“零样本”难度预测，减少每题 16 次采样的开销。\n- **动态组大小**：依据 D(q) 自适应调整 G：难题多采样，简单题少采样，整体计算预算不变但梯度质量提升。\n\n5\\. 偏好与公平侧：置信惩罚是否会放大偏差？\n\n- **答案长度偏好**：已给出长度加权公式，但未实验；需验证短答案是否被过度强化，导致模型“偷懒”。\n- **语言/文化偏差**：高置信错误若与训练语料分布相关，惩罚项可能系统压制某些语言风格或非主流解法，需审计公平性指标。\n- **可解释性**：提供 per-sample 的置信惩罚可视化，帮助开发者识别“过度自信”知识点，指导数据再平衡。\n\n6\\. 组合拓展\n\n- **与拒绝采样（Rejection Sampling）联用**：先用 LENS 微调，再对高置信错误区域做拒绝采样，迭代式提升数据质量。\n- **与蒙特卡洛树搜索（MCTS）结合**：在叶节点展开阶段，用 LENS 的修正奖励作为叶价值，引导搜索往“低置信但潜在正确”路径探索。\n- **多任务 RLVR**：共享置信惩罚项，考察是否能提升跨任务迁移与通用推理能力。\n\n一句话展望\n\nLENS 把“负样本置信度”首次系统地纳入策略梯度理论，未来可在**算法深度**（无超参、方差缩减）、**场景宽度**（代码、工具、对话）、**工程效率**（在线 D(q)、动态组大小）三个维度继续挖掘，为“不浪费任何一次错误”提供持续动力。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：GRPO 对“全负样本组”优势恒为零，大量已生成轨迹被浪费。\n- **思路**：从最大似然估计（MLE）出发，把奖励建模梯度等价成策略梯度，发现负样本应携带**置信加权惩罚**——模型越自信地犯错，惩罚越大。\n- **方法**：提出 LENS，将 GRPO 的 0/1 奖励替换为\n\ntilde r = r - (1-r)(π_θ) / (D(q)-π_θ)\n\n并做长度归一化与数值保护；负组由此产生非零优势，实现即插即用。\n- **实验**：在 MATH 基准上训练 Llama-3.1-8B 与 Qwen-2.5-3B，LENS 在所有 Pass@k 上稳定超越 GRPO，难题子集增益更大，且训练 1 500+ 步无崩溃。\n- **结论**：负样本组可被**无额外监督、理论一致地“救活”**，提升样本效率与最终性能。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t%20Waste%20Mistakes%3A%20Leveraging%20Negative%20RL-Groups%20via%20Confidence%20Reweighting，链接是https://arxiv.org/pdf/2510.08696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08696。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-13-1760399244941","title":"ARES: Multimodal Adaptive Reasoning via Difficulty-Aware Token-Level Entropy Shaping","description":"Recent advances in multimodal large reasoning models (MLRMs) have substantially improved their ability to solve complex textual and visual tasks. However, these models tend to overthink on simple problems, producing unnecessarily lengthy reasoning traces, while under-exploring on challenging ones, leading to missed solutions. To address this imbalance, we propose ARES, a unified open-source framework for adaptive reasoning that dynamically allocates exploration effort based on task difficulty. Our approach is motivated by two key empirical findings: (i) while single-token entropy is noisy, high window-entropy (HWE) tokens (token-level entropies averaged under a sliding window) can reliably capture reasoning-critical moments; and (ii) reducing HWE usage benefits easy problems, while increasing it is essential for solving hard ones. Building on these insights, ARES introduces a two-stage training pipeline. In the Adaptive Cold-Start stage, we curate multimodal and textual data paired with reasoning traces of length proportional to problem difficulty, equipping the model with initial difficulty awareness. In the second stage, we develop Adaptive Entropy Policy Optimization (AEPO), which uses HWE tokens as exploration triggers to decide when to explore, and a hierarchical entropy reward with dynamic KL control to decide how much to explore. Extensive experiments demonstrate that ARES achieves superior performance and reasoning efficiency across diverse mathematical, logical, and multimodal benchmarks, while closing the gap to leading commercial systems under significantly lower inference costs.","url":"https://huggingface.co/papers/2510.08457","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.941Z","metadata":{"authors":["Shuang Chen","Yue Guo","Yimeng Ye","Shijue Huang","Wenbo Hu","Haoxi Li","Manyuan Zhang","Jiayu Chen","Song Guo","Nanyun Peng"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.08457.pdf","arxivUrl":"https://arxiv.org/abs/2510.08457","arxivId":"2510.08457","coolPaperUrl":"https://papers.cool/arxiv/2510.08457","published":"2025-10-09T17:03:28Z","updated":"2025-10-09T17:03:28.000Z","zh_summary":"多模态大推理模型（MLRM）的最新进展极大地提高了它们解决复杂文本和视觉任务的能力。然而，这些模型往往会过度思考简单的问题，产生不必要的冗长推理痕迹，而对具有挑战性的问题的探索不足，导致错过解决方案。为了解决这种不平衡问题，我们提出了 ARES，这是一个用于自适应推理的统一开源框架，它根据任务难度动态分配探索工作。我们的方法受到两个关键实证发现的推动：（i）虽然单标记熵是噪声的，但高窗口熵（HWE）标记（在滑动窗口下平均的标记级熵）可以可靠地捕捉推理关键时刻;（ii） 减少 HWE 的使用有利于简单的问题，而增加它对于解决棘手的问题至关重要。基于这些见解，ARES 引入了两阶段培训管道。在自适应冷启动阶段，我们整理多模态和文本数据，并配对与问题难度成正比的长度推理轨迹，为模型提供初始难度感知。在第二阶段，我们开发了自适应熵策略优化（AEPO），它使用HWE代币作为探索触发器来决定何时进行探索，并使用动态KL控制来决定探索多少的分层熵奖励。大量实验表明，ARES 在各种数学、逻辑和多模态基准测试中实现了卓越的性能和推理效率，同时在显着降低的推理成本下缩小了与领先商业系统的差距。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决多模态大推理模型（MLRMs）在推理过程中“过度思考简单问题、复杂问题探索不足”的失衡现象。具体而言：\n\n- **过度思考**：对简单问题生成冗长、不必要的推理链，增加推理成本与延迟。\n- **探索不足**：对复杂问题未能充分展开多路径探索，导致正确率下降。\n\n为此，作者提出统一开源框架 **ARES**，通过“难度感知的 token-级熵塑形”实现自适应推理，使模型能根据问题难度动态分配推理深度：简单问题给出简洁回答，复杂问题则进行充分探索，从而在性能与效率之间取得平衡。\n\nQ2: 有哪些相关研究？\n\n论文在“6 RELATED WORK”部分系统梳理了相关研究，可归纳为三大主线：\n\n1. 多模态大推理模型（MLRMs）\n\n- 链式思维微调：LLaVA-CoT、LlamaV-o1 等将 CoT 引入多模态场景。\n- 强化学习推进：MM-EUREKA、R1-V、LMM-R1、VLM-R1、Visual-RFT、Seg-Zero 等利用可验证奖励或规则型奖励，在数学、几何、视觉定位等任务上延长推理轨迹。\n- 共同痛点：推理长度固定或盲目增长，缺少“按难度调节”机制。\n2. 自适应推理（Adaptive Reasoning）\n2.1 免训练方法\n\n- 显式预算：硬截断、token 预算提示（Token-Budget-Aware）、wait-token 强制（s1k）。\n- 动态早停：置信度触发提前退出（FlashThink、Dynamic Early Exit）。\n\n2.2 训练型方法\n\n- 变长数据 SFT：长-短 CoT 混合监督（Long-Short Mixture、OpenThoughts）。\n- 长度惩罚 RL：在奖励中直接减去长度（Arora & Zanette 2025、Ling et al. 2025）。\n- 难度感知 RL：AdaCtrl、DAST、Just-Enough-Thinking 等按难度桶施加不同长度惩罚，但仍可能鼓励“难题过度探索”。\n- 空白：现有工作聚焦纯文本，多模态场景的自适应推理尚缺系统研究。\n3. 熵在强化学习中的应用\n\n- 传统深度 RL：最大熵正则、SAC、PG-Entropy 等用全局熵奖励平衡探索-利用。\n- LLM-RL 的新发现：全局熵奖励收益有限，甚至导致崩溃；转而采用熵裁剪或惩罚（Zhang et al. 2024a、Cui et al. 2025）。\n- 本文差异：不施加全局熵奖励，而是将“高窗熵 token”作为局部探索信号，并结合难度进行“何时探索、探索多少”的精细塑形。\n\n综上，ARES 在现有 MLRM 与自适应推理研究基础上，首次把“局部窗熵触发 + 难度感知分层奖励”引入多模态强化学习，兼顾性能与推理效率。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **ARES**（Adaptive Reasoning via difficulty-aware token-level Entropy reward Shaping）框架，通过“两段式”训练流程与“熵-难度”耦合机制，系统性解决“简单题过思考、复杂题欠探索”的问题。核心思路可概括为：\n\n1\\. 发现关键经验规律\n\n- **窗熵（HWE）比单 token 熵更可靠**：\n滑动窗口内平均熵  bar H_(t:w)=frac 1w∑_(τ=t)^(t+w-1)H_τ  能稳定标记“推理分叉点”，而单点熵常被标点、公式等噪声主导。\n- **熵-难度交互**：\n- 简单题：减少 HWE 数量 → 答案更短且更准确。\n- 复杂题：增加 HWE 数量 → 答案更长且更准确。\n\n2\\. 两段式训练\n\nStage 1：Adaptive Cold-Start（AdaCS）\n\n- **目标**：让模型先天地“知道”难度与长度应匹配。\n- **做法**：\n- 收集 22 万条文本+多模态 STEM 题，每条用强教师模型（DeepSeek-R1 / Seed-1.5-VL）生成 8 条回答。\n- 按 pass-rate  p  划分难度，用线性插值确定目标长度\n\nL_(target)(p)=(1-p)· L^((0))+p· L^((1))\n\n只采样长度接近该目标的回答，保证“越简单越短、越困难越长”。\n\nStage 2：Adaptive-Entropy Policy Optimization（AEPO）\n\n- **目标**：在 RL 阶段精细控制“何时探索、探索多少”。\n- **两大机制**：\n\n| 模块 | 触发信号 | 控制量 | 效果 |\n| --- | --- | --- | --- |\n| When to Explore | 窗熵超过动态阈值  τ_(high) （批次 95 分位） | 在该 token 位置并行展开多条 rollout | 只在“持续高不确定”区域分叉，节省算力 |\n| How Much to Explore | 难度在线分桶（easy/medium/hard） | 分层熵奖励 + 动态 KL 预算 | 简单题惩罚多余 HWE，复杂题奖励额外 HWE，同时用 KL 控制器防止漂移 |\n\n- **奖励函数**（统一无超参）：\n\nR(x,y;d)=R_(acc)(x,y)-mathbb 1_(acc)=0· λ_d· g_d(Delta)\n\n-  Delta=N_(HE)-N_(HE)^(target)(d) ：实际 HWE 与难度目标的偏差。\n-  g_d(·)  按难度定义不同方向：easy 惩罚  Delta>0 ，hard 奖励  Delta>0 。\n- **动态 KL 预算**（token-级）：\n\nβ_(i,t)=β_d·rho_t,quad rho_t=rho(<1)& t∈HWE window1&otherwise\n\n在推理关键窗口内“放松”KL 约束，其余位置严格限制，实现“ token-级思考预算”。\n\n3\\. 推理时行为\n\n- 简单题：窗熵 rarely 超过阈值 → 线性生成，答案短而准。\n- 复杂题：窗熵频繁触发分叉 → 模型持续探索，输出长链直至收敛。\n\n4\\. 结果验证\n\n- 3B/7B 模型在 9 个多模态+文本 benchmark 上平均提升 **+8.4/+9.7 pts**，且响应长度随难度自适应变化（见图 6、8）。\n- 消融实验表明：仅熵塑形或仅动态 KL 均能带来增益，二者联合效果最佳，证明“何时探索”与“探索多少”缺一不可。\n\n综上，ARES 通过“窗熵触发 + 难度感知分层奖励 + 动态 KL 预算”三元组，首次在多模态场景下实现了“难度-自适应”推理，兼顾准确率与推理效率。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“多模态+文本推理准确率”与“推理长度效率”两大维度，设计了**系统实验矩阵**，可概括为以下 5 组：\n\n1\\. 主实验：9 基准全面评测\n\n| 领域 | benchmark（选取子集） | 指标 | 对比对象 |\n| --- | --- | --- | --- |\n| 多模态数学 | MathVerse-V, MathVision, DynaMath-W, WeMath, MathVista, LogicVista, MMMU, MMMU-Pro, CharXIV, MMStar | pass@1 | 3B/7B 开源 SoTA + 4 个商用模型 |\n| 文本数学 | AIME24, AIME25, MATH-500 | avg@16 / pass@1 | 同上 |\n| 通用推理 | MMLU-Pro, GPQA, BBEH | pass@1 | 同上 |\n\n**结果**（表 2、3）\n\n- ARES-3B 平均 **+8.4 pts** 超越开源最佳；ARES-7B 平均 **+9.7 pts**，在 MathVision 上领先 **+19.0 pts**，在 AIME25 上领先 **+58.4 pts**。\n- 与商用系统差距显著缩小，推理成本远低于 Gemini-2.5-Pro-Thinking。\n\n2\\. 自适应行为验证\n\n- **长度-难度一致性**（图 6、8 & 表 4）\n- 冷启动后模型已具备“难题长答、易题短答”倾向；AEPO 进一步强化：\n- 易题（GSM8K、MathVista）长度↓ 20-30 %\n- 难题（AIME25、OlympiadBench）长度↑ 30-50 %\n- 线性相关检验：高熵 token 数 vs 响应长度 Pearson r>0.92，验证熵作为“推理努力”代理。\n\n3\\. 消融实验（表 5）\n\n| 设置 | ΔAvg（相对于冷启动+GRPO） | 说明 |\n| --- | --- | --- |\n| 仅动态 KL | +1.3 pts | 缩短路径，提升效率 |\n| 仅熵塑形 | +1.8 pts | 抑制过思考，鼓励必要探索 |\n| 完整 ARES | +5.0 pts | 二者互补，准确率与长度双优 |\n\n4\\. 训练动态分析（图 6、9）\n\n- 训练 250 步内：\n- 准确率曲线：ARES > 熵消融 > KL 消融 > GRPO/DAPO\n- 长度曲线：ARES 最终最短且方差最小，表明“更快收敛到高效策略”。\n\n5\\. 案例可视化（附录 M & 图 7、8）\n\n- **易题**（三角形求 cos）：ARES 用 20 个高熵 token 即给出 1 行答案； vanilla 模型生成 51 个高熵 token 并反复“wait”。\n- **难题**（三圆公切）：ARES 产生 4 173 token，多次自我质疑、验证三角形不等式，最终正确；长度与熵值同步增长，体现“深度探索”。\n\n附加分析\n\n- **窗大小敏感性**（图 2a）：w=4-8 时 F1 最佳，单 token 噪声大，长窗稀释信号。\n- **在线难度桶稳定性**（附录 G）：pass@8 桶每迭代重算，κ\\_d 控制器 50 步内收敛到目标 KL，训练无震荡。\n\n综上，实验从**宏观性能**→**长度自适应**→**组件贡献**→**训练行为**→**个案可解释性**形成完整证据链，证明 ARES 在“准确率-效率”双指标上均达到 SOTA 且具备可解释的自适应推理行为。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“方法-视角”“数据-场景”“理论-分析”三条线，供后续研究参考：\n\n方法-视角\n\n1. **连续难度建模**\n当前仅用三桶离散难度；可改用 pass-rate 回归或认知诊断模型，实现“细粒度、样本级”难度信号，避免边界硬截断带来的梯度突变。\n\n2. **多粒度窗熵融合**\n并行使用字符级、子词级、句子级多窗口，再学一个注意力权重，让模型自行选择“在哪一层、用多大窗口”触发探索。\n\n3. **逆向 KL / 双向 KL 预算**\n目前仅限制 πθ→πref；可引入 DKL(πref∥πθ) 防止模式坍塌，或采用 J-S 散度实现“对称预算”，在探索-保守间更平滑过渡。\n\n4. **测试时自适应（Test-time Adaptation）**\n将 AEPO 的“窗熵触发分叉”机制从训练阶段搬到推理阶段：遇到高窗熵即时 fork-若干续写，再投票或自洽性筛选，实现“无训练、纯推理”的自适应。\n\n数据-场景\n\n5. **多轮对话与工具调用**\n把 ARES 拓展到多轮视觉-语言交互、函数调用或代码生成场景：难度信号由“是否调用工具”“调用次数”定义，熵触发决定“是否继续思考 or 立即执行”。\n\n6. **视频与长文档推理**\n将窗熵概念从“一维 token 序列”扩展到“二维时空块”，对长视频或数百页文档进行“关键帧/关键页”定位，再动态分配长链推理。\n\n7. **低资源语言与跨语言迁移**\n检验窗熵-难度机制是否在非英语、非拉丁语料依然成立；若否，可用 meta-RL 让难度桶与熵阈值在目标语言上自动重学。\n\n理论-分析\n\n8. **熵-难度因果性验证**\n采用因果中介分析或 do-intervention，区分“窗熵高 → 性能提升”是因果还是相关；若证实因果，可为后续课程学习提供理论依据。\n\n9. **KL 预算与长度上界紧密度**\n当前用 Donsker–Varadhan 给出期望长度上界；可进一步推导高概率尾界（concentration bound），指导用户“在 95 % 置信下最多用多少 token”。\n\n10. **探索-利用的相变点**\n建立“难度-最优探索长度”的相变模型：当问题复杂度超过临界值 c\\* 时，增加 HWE token 的边际收益由负转正，可用统计物理方法估计 c\\*。\n\n11. **奖励 hacking 与熵奖励鲁棒性**\n监测模型是否通过“故意写模糊词”人为抬高窗熵以骗取奖励；设计对抗检验+正则项，确保熵信号与真实推理质量一致。\n\n工程与系统\n\n12. **异构算力调度**\n结合窗熵触发，实现“弹性推理”：高熵段分配 GPU/TPU，低熵段回落 CPU，整体节省 30-50 % 算力，适合边缘部署。\n\n13. **与投机解码（Speculative Decoding）协同**\n用小型草稿模型生成低熵段，大模型仅在高熵窗口介入，进一步降低延迟。\n\n小结\n\nARES 首次把“局部窗熵 + 难度感知”耦合进多模态 RL，但仍在离散难度、单窗口、单向 KL 等层面留有简化。后续可从“连续化、多粒度、因果理论、新场景”四翼展开，推动自适应推理从“能工作”走向“可解释、可扩展、可部署”。\n\nQ6: 总结一下论文的主要内容\n\nARES 论文核心内容速览\n\n1 问题\n\n- 多模态大推理模型（MLRM）**简单题过度思考、复杂题探索不足** → 答案冗长且错率高，推理成本高。\n\n2 关键发现\n\n- **滑动窗熵**（window-entropy）比单 token 熵更能稳定标记\"推理分叉点\"。\n- **熵-难度交互**：简单题减少高窗熵（HWE）token 既短又准；复杂题增加 HWE token 才能提升准确率。\n\n3 方法：ARES 两阶段\n\n| 阶段 | 目标 | 关键机制 |\n| --- | --- | --- |\n| AdaCS（冷启动） | 建立\"难度⇄长度\"先验 | 按 pass-rate 插值目标长度采样数据，微调模型 |\n| AEPO（RL） | 精细控制\"何时探索、探索多少\" | ① HWE 触发分叉；② 难度桶在线分档；③ 分层熵奖励+动态 KL 预算 |\n\n4 实验\n\n- **9 基准**（多模态数学+文本数学+通用推理）\nARES-3B/7B 平均领先开源 SoTA **+8.4/+9.7 pts**；AIME25 提升 **+58.4 pts**。\n- **自适应行为**：易题长度↓20-30%，难题长度↑30-50%，准确率同步提升。\n- **消融**：熵塑形与动态 KL 互补，联合带来 **+5.0 pts** 平均增益。\n\n5 结论\n\nARES 用\"窗熵信号+难度感知奖励\"实现推理深度自适应，在性能与推理效率间取得新平衡，为多模态自适应推理提供了可复现的开源基线。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ARES%3A%20Multimodal%20Adaptive%20Reasoning%20via%20Difficulty-Aware%20Token-Level%20Entropy%20Shaping，链接是https://arxiv.org/pdf/2510.08457，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08457。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1760399244941","title":"DISCO: Diversifying Sample Condensation for Efficient Model Evaluation","description":"Evaluating modern machine learning models has become prohibitively expensive. Benchmarks such as LMMs-Eval and HELM demand thousands of GPU hours per model. Costly evaluation reduces inclusivity, slows the cycle of innovation, and worsens environmental impact. The typical approach follows two steps. First, select an anchor subset of data. Second, train a mapping from the accuracy on this subset to the final test result. The drawback is that anchor selection depends on clustering, which can be complex and sensitive to design choices. We argue that promoting diversity among samples is not essential; what matters is to select samples that $\\textit{maximise diversity in model responses}$. Our method, $\\textbf{Diversifying Sample Condensation (DISCO)}$, selects the top-k samples with the greatest model disagreements. This uses greedy, sample-wise statistics rather than global clustering. The approach is conceptually simpler. From a theoretical view, inter-model disagreement provides an information-theoretically optimal rule for such greedy selection. $\\textbf{DISCO}$ shows empirical gains over prior methods, achieving state-of-the-art results in performance prediction across MMLU, Hellaswag, Winogrande, and ARC. Code is available here: https://github.com/arubique/disco-public.","url":"https://huggingface.co/papers/2510.07959","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.941Z","metadata":{"authors":["Alexander Rubinstein","Benjamin Raible","Martin Gubri","Seong Joon Oh"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.07959.pdf","arxivUrl":"https://arxiv.org/abs/2510.07959","arxivId":"2510.07959","coolPaperUrl":"https://papers.cool/arxiv/2510.07959","published":"2025-10-09T08:53:59Z","updated":"2025-10-09T08:53:59.000Z","zh_summary":"评估现代机器学习模型的成本已经高得令人望而却步。像 LMMs-Eval 和 HELM 这样的基准测试每个模型都需要数千 GPU 小时。昂贵的评估降低了参与的普及性，延缓了创新周期，并加剧了环境影响。典型的方法包括两个步骤。首先，选择一个锚点数据子集。其次，训练一个从该子集上的准确率映射到最终测试结果的模型。其缺点是锚点选择依赖于聚类，而聚类可能复杂且对设计选择敏感。我们认为，促进样本多样性并非必需；重要的是选择那部分能$\textit{最大化模型响应多样性}$的样本。我们的方法，$\textbf{样本多样化浓缩（DISCO）}$，选择模型分歧最大的 top-k 样本。这种方法使用贪心的、基于样本的统计，而不是全局聚类。该方法在概念上更为简单。从理论角度来看，模型间分歧为这种贪心选择提供了信息论上最优的规则。$\textbf{DISCO}$ 在经验上优于以往方法，在 MMLU、Hellaswag、Winogrande 和 ARC 的性能预测中实现了最先进的结果。代码可在此获取：https://github.com/arubique/disco-public。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现代机器学习模型评估成本过高的问题。随着模型规模、测试时计算量以及评估维度（内容+风格）的激增，完整跑一次基准测试（如 LMMs-Eval、HELM）往往需要数百到数千 GPU 小时，严重拖慢研发迭代并加剧环境负担。为此，作者提出“高效评估”框架：\n\n1. 从原始测试集中选出一个极小子集（anchor set）；\n2. 仅用该子集上的模型输出即可准确预测模型在全集上的性能。\n\n现有方法普遍依赖“样本间多样性”或聚类来选子集，再借助 IRT 等复杂心理测量模型进行性能外推。论文指出这两条路线均过度复杂，且并非最优。核心观察是：**真正重要的是“模型响应的多样性”而非“样本本身的多样性”**。因此，作者提出 DISCO（Diversifying Sample Condensation），用**模型间分歧度**（inter-model disagreement）作为信息量的唯一准则，贪心选取最能引发模型不一致的 top-k 样本，再以极简的“模型签名”回归直接预测全集精度。\n\n一句话总结：\n\n> 在保持预测误差仅约 1 个百分点的前提下，DISCO 将评估开销压缩 99% 以上，并在语言、视觉两大领域一致超越现有聚类+IRT 类方法。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归为三条主线，并指出 DISCO 与它们的本质区别。\n\n1. 评估成本激增\n\n- 大规模基准：GLUE、HELM、BIG-bench、LMMs-Eval、Prometheus、GAIA 等\n- 单模型评估需 30–4000+ GPU 小时（Liang et al. 2022; Zhang et al. 2024）\n2. 标签高效评估（Label-efficient evaluation）\n\n- 主动测试 / 主动采样：仅标注信息量大的样本即可估计精度（Majumdar & Niksic 2017; Ji et al. 2021; Deng & Zheng 2021; Kossen et al. 2021, 2022; Hu et al. 2023; Huang et al. 2024）\n- 局限：需先对全集做推理才能挑样本，不适用于“推理昂贵”场景\n3. 测试集压缩 / Anchor-point 方法（LLM 时代主流）\n\n- 聚类+加权精度：Anchor-conf / Anchor-corr（Vivek et al. 2023）\n- 聚类+IRT 参数估计：Anchor-IRT、tinyBenchmarks（Polo et al. 2024）\n- 迭代搜索+IRT 能力得分：Metabench / ability-IRT（Kipnis et al. 2024）\n- 动态锚点：Fluid-Bench（Hofmann et al. 2025）"}},{"id":"hf-15-1760399244941","title":"Bridging Reasoning to Learning: Unmasking Illusions using Complexity Out of Distribution Generalization","description":"Recent progress has pushed AI frontiers from pattern recognition tasks toward problems that require step by step, System2 style reasoning, especially with large language models. Yet, unlike learning, where generalization and out of distribution (OoD) evaluation concepts are well formalized, there is no clear, consistent definition or metric for reasoning ability. We propose Complexity Out of Distribution (Complexity OoD) generalization as a framework and problem setting to define and measure reasoning. A model exhibits Complexity OoD generalization when it maintains performance on test instances whose minimal required solution complexity, either representational (richer solution structure) or computational (more reasoning steps/program length), exceeds that of all training examples. We formalize complexity via solution description Kolmogorov complexity and operational proxies (e.g., object/relation counts; reasoning step counts), clarifying how Complexity OoD differs from length and compositional OoD. This lens unifies learning and reasoning: many cases solvable with System1 like processing at low complexity become System2 like under complexity pressure, while System2 can be viewed as generalization over solution structures. We translate this perspective into practice with recommendations for operationalizing Complexity OoD across the stack: incorporating complexity into benchmark and evaluation metric design, rethinking supervision to target solution traces, seeking and designing inductive biases for Complexity OoD generalization, addressing learning to reason spillovers such as spurious shortcuts, semantic robustness, catastrophic forgetting, and step wise calibration. Because Complexity OoD cannot be solved by scaling data alone, progress toward robust reasoning will require architectures and training regimes that explicitly model and allocate computation with respect to complexity.","url":"https://huggingface.co/papers/2510.06274","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.941Z","metadata":{"authors":["Mohammad Mahdi Samiei Paqaleh","Arash Marioriyad","Arman Tahmasebi-Zadeh","Mohamadreza Fereydooni","Mahdi Ghaznavai","Mahdieh Soleymani Baghshah"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.06274.pdf","arxivUrl":"https://arxiv.org/abs/2510.06274","arxivId":"2510.06274","coolPaperUrl":"https://papers.cool/arxiv/2510.06274","published":"2025-10-06T13:08:31Z","updated":"2025-10-06T13:08:31.000Z","zh_summary":"翻译失败: Unexpected end of JSON input","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在为“推理能力”提供一个可形式化、可度量的定义，并据此建立一套不依赖数据规模、能抵御数据污染的评价框架。核心问题可归纳为：\n\n- 现有 System-2（逐步、多步、逻辑）任务缺乏像 System-1 分类任务那样清晰的“分布外”概念，导致无法判断模型究竟是真正在推理，还是仅仅在更复杂的训练分布里做记忆式匹配。\n- 因此，需要一种**专门刻画“解法复杂度”的分布外设定**，即 Complexity Out-of-Distribution（Complexity OoD），用来统一“学习”与“推理”的评估视角，并指导模型架构、训练范式与基准测试的重新设计。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归入四大线索，并强调它们实质上都从不同侧面“触碰”了 Complexity OoD 挑战，但缺乏统一视角。按原文结构梳理如下：\n\n1. 可变长度表示（Variable-length Representation）\n\n- 对象-中心表征学习\n– Slot Attention（Locatello et al., 2020）\n– 后续因果/组合式扩展（Didolkar et al., 2024; Kapl et al., 2025）\n- 自发语言（Emergent Language）\n– 多智能体参考博弈下离散的、长度可变的通信协议（Havrylov & Titov, 2017; Lazaridou et al., 2018, 2022; Peters et al., 2025）\n2. 可变深度计算（Variable-length Computation）\n\n- 自适应计算时间 ACT / Universal Transformer（Graves, 2016; Dehghani et al., 2019a,b）\n- 符号回归与程序合成\n– Neural Symbolic Regression（Biggio et al., 2021）\n– DreamCoder 库学习与子程序复用（Ellis et al., 2021）\n– ARC 2024 优胜方案普遍采用程序生成（Chollet et al., 2024; Li et al., 2024b; Ouellette, 2024）\n3. 大模型内部“System-1/2 嫁接”尝试\n\n- 链式思维 CoT（Wei et al., 2022c）\n- 测试时搜索\n– Tree/Graph of Thoughts（Yao et al., 2024; Besta et al., 2023; Bi et al., 2024）\n- 重复采样 + 自一致性（Wang et al., 2022; Brown et al., 2024）\n- 结果/过程奖励模型 ORM/PRM（Cobbe et al., 2021; Lightman et al., 2023; Wang et al., 2023b）\n- 强化"}},{"id":"hf-16-1760399244942","title":"Which Heads Matter for Reasoning? RL-Guided KV Cache Compression","description":"Reasoning large language models exhibit complex reasoning behaviors through the extended chain-of-thought generation, creating unprecedented Key-Value (KV) cache overhead during the decoding phase. Existing KV cache compression methods underperform on reasoning models: token-dropping methods break reasoning integrity by discarding critical information, while head-reallocating methods mistakenly compress reasoning-critical heads since they are designed for retrieval tasks, resulting in significant performance degradation as compression rates increase. We hypothesize that KV heads exhibit functional heterogeneity in reasoning models-some heads are critical for chain-of-thought consistency while others are compressible. To validate and exploit this insight, we propose RLKV, a novel reasoning-critical head identification framework, which uses reinforcement learning to directly optimize the relationship between each head's cache usage and reasoning quality. As RLKV produces rewards from actual generated samples during training, it naturally identifies heads relevant to reasoning behaviors. We then allocate full KV cache to these heads while applying compressed constant KV cache to others for efficient inference. Our experiments reveal that only a small fraction of attention heads is essential for reasoning, enabling our KV compression approach to outperform baseline methods while achieving 20-50% cache reduction with near lossless performance compared to uncompressed results.","url":"https://huggingface.co/papers/2510.08525","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.942Z","metadata":{"authors":["Wenjie Du","Li Jiang","Keda Tao","Xue Liu","Huan Wang"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.08525.pdf","arxivUrl":"https://arxiv.org/abs/2510.08525","arxivId":"2510.08525","coolPaperUrl":"https://papers.cool/arxiv/2510.08525","published":"2025-10-09T17:50:00Z","updated":"2025-10-09T17:50:00.000Z","zh_summary":"推理大型语言模型通过扩展的思维链生成表现出复杂的推理行为，在解码阶段产生了前所未有的键值（KV）缓存开销。现有的 KV 缓存压缩方法在推理模型上表现不佳：标记删除方法通过丢弃关键信息来破坏推理完整性，而头重新分配方法错误地压缩了推理关键头，因为它们是为检索任务而设计的，导致随着压缩率的增加而显着降低性能。我们假设 KV 头在推理模型中表现出功能异质性——一些头对于思维链的一致性至关重要，而另一些则是可压缩的。为了验证和利用这一见解，我们提出了 RLKV，这是一种新型的推理关键头识别框架，它使用强化学习直接优化每个头的缓存使用与推理质量之间的关系。由于 RLKV 在训练过程中从实际生成的样本中产生奖励，因此它自然会识别与推理行为相关的头部。然后，我们将完整的 KV 缓存分配给这些头，同时将压缩的常量 KV 缓存应用于其他头部以实现高效推理。我们的实验表明，只有一小部分注意力头对于推理至关重要，这使得我们的 KV 压缩方法能够优于基线方法，同时实现 20-50% 的缓存减少，与未压缩的结果相比，性能近乎无损。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“推理型大语言模型在解码阶段因超长思维链（CoT）带来前所未有的 KV 缓存开销”这一瓶颈，提出系统性的 KV 缓存压缩方案。核心待解决问题可归纳为：\n\n- **推理模型与压缩方法不兼容**\n现有两类压缩策略——token-dropping 与 head-reallocation——在推理模型上性能骤降：\n\n- token-dropping 会丢弃维持思维链一致性的关键 token，导致循环重复或逻辑断裂；\n- head-reallocation 原本面向“检索头”设计，误判并压缩了对推理至关重要的注意力头，造成推理过程发散、生成无效步骤。\n- **KV 缓存角色转变带来的新约束**\n在推理模型中，KV 缓存不仅是计算优化手段，更是承载“推理状态”的载体，需持续保存自反、回溯等复杂行为所需信息。传统压缩方法未意识到这种“功能性异质”，将全体头或 token 等同对待，从而破坏推理行为。\n\n- **缺乏“哪些头对推理真正关键”的判别机制**\n先前工作无针对推理行为的头级重要性度量，无法区分“推理头”与可压缩头，导致高压缩率下性能悬崖式下跌。\n\n为此，论文提出 RLKV 框架，通过强化学习直接优化“每个头的 KV 缓存使用量”与“推理质量”之间的关系，自动识别少量真正的推理头并给予完整缓存，其余头共享压缩缓存，实现 20–50% 显存降低的同时保持近乎无损的推理精度。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为三大主线，均围绕“大模型推理效率”与“KV 缓存压缩”展开，并指出它们与推理模型的适配缺陷。\n\n1. 高效 LLM 推断（Efficient LLM Inference）\n\n- 结构级：\n– GQA、MLA 通过减少 KV 头数量降低内存，但需从头预训练。\n– Linear Attention、Mamba 等线性/状态空间模型把复杂度降到 O(n)，却牺牲部分建模容量。\n- 系统级：\n– KV 量化（KIVI、SKVQ、WKVQuant、RotateKV 等）降低位宽。\n– PagedAttention、KV 复用、稀疏加载等调度优化。\n共同点：把 KV 缓存视为“透明数据”，未挖掘头/Token 的稀疏语义。\n2. KV 缓存压缩（KV Cache Compression）\n\n- Token-dropping：H2O、SnapKV、R-KV、PyramidInfer、CAKE 等按注意力得分淘汰 Token。\n问题：高压缩率下丢弃 CoT 关键 Token → 重复、逻辑断裂。\n- Head-reallocation：Ada-KV、RazorAttention、DuoAttention、PruLong 等仅给“检索头”完整缓存。\n问题：静态指标或单前向训练，无法捕捉动态推理行为，误压缩推理关键头。\n3. 强化学习用于效率（RL for Efficiency）\n\n- NAS 与剪枝：早期用 RL 搜索结构或层-wise 剪枝比例，空间大、成本高。\n- 推理长度控制：ThinkPrune、Adaptive-Length RL 通过奖励塑形缩短 CoT，间接减少缓存。\n本文差异：首次用轻量 RL 直接学习“头级门控”α，优化空间仅 L×H 维，兼顾稀疏正则与可验证奖励，实现推理行为保持下的内存压缩。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 RLKV 框架，把“哪些头对推理不可或缺”建模为一个稀疏门控优化问题，并用强化学习直接求解。整体流程分三步：\n\n1. 混合注意力 + 可学习门控\n对每层每个头引入标量门控 αᵢⱼ ∈ \n0,1\n，把该头输出写成\n\nout_(mix)^(i,j) = α_(i,j)·out_(full) + (1-α_(i,j))·out_(streaming)\n\n- full 分支访问完整 KV-cache\n- streaming 分支仅保留固定 sink+局部 token（常数缓存）\n所有 LLM 权重冻结，仅优化 L×H 个 α 参数，空间极小。\n2. 强化学习训练（GRPO）\n\n- 奖励信号：在可验证数学/代码题上采样 4 条解答，用最终答案正确性给出稀疏 0/1 奖励。\n- 目标函数：\n\nmax_(boldsymbol α) E[GRPO 代理目标] - (β) / (L·H)|boldsymbol α|_1\n\n去掉 KL 惩罚以放大奖励区分度，L1 正则持续压迫 α 稀疏。\n- 训练技巧：\n– 自蒸馏采样：只取模型原本能做对的 3 k 题，按输出长度课程排序，保证奖励稳定。\n– 自适应惩罚权重：当平均奖励 ¯r 下降时按 β′(¯r,τ)=I(¯r>τ)·β·(e^{¯r}-1) 自动削弱 L1，防止“稀疏奖励↔稠密惩罚”恶性循环导致崩溃。\n3. 推理阶段压缩部署\n训练结束后按 α 值排序，取前 k 个头作为“推理头”给予完整 KV-cache；其余头恒用 sink+local 常数缓存。由此在 20–50% 显存缩减下维持几乎无损的推理准确率，并在部分高难度基准（AIME24）上因抑制非推理头噪声而略优于全缓存基线。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“压缩率-准确率-内存”三条主线展开，系统验证 RLKV 在推理模型上的有效性与优越性。\n\n1. 主实验：四基准、两模型、四压缩率\n模型：Llama-3.1-8B-R1、Qwen-2.5-7B-R1\n任务：\n\n- GSM8K（小学数学）\n- MATH500（中学竞赛）\n- AIME24（高难竞赛）\n- MBPP（Python 代码生成）\n压缩率：0.2、0.4、0.6、0.8\n对比：Full KV、H2O、R-KV、DuoAttention\n结果：\n- RLKV 在所有 32 组“模型×任务×压缩率”中均取得最高或并列最高准确率；0.4–0.6 高压缩区优势最大。\n- 在 AIME24 上，Llama-0.4 与 Qwen-0.2 两组甚至反超 Full KV（↑3.3 pp、↑6.7 pp），验证“去噪”效应。\n2. 无损压缩阈值测试\n定义“无损”为性能下降 ≤1 pp，统计各方法可承受的最大稀疏度：\n\n- Llama：RLKV 在 GSM8K/Math500/AIME24/MBPP 分别达 0.4/0.5/0.4/0.4，显著高于基线（最佳基线仅 0.2–0.3）。\n- Qwen：RLKV 在 GSM8K 可达 0.4，而基线普遍 ≤0.2。\n3. 头重要性对照实验\n方法：按 α 值、DuoAttention 检索头分数、随机顺序，分别将 Top-10%→Top-40% 的头替换成压缩缓存，观察准确率跌落斜率。\n结果：\n\n- 推理头曲线最陡峭；Top-20% 被替换即导致 20–40 pp 下跌。\n- 检索头与随机头曲线平缓，证实“推理头”功能独特且集中。\n4. 错误模式剖析\n在 Math500 上把错误分为重复、答案错误、超长三类：\n\n- 压缩推理头 → 重复率显著增高（CoT 逻辑断裂）。\n- 压缩检索头 → 错误模式分散，超长比例升高，但重复率较低。\n说明两类头承担不同语义角色。\n5. 内存实测\n以 32 k 长度、BF16 估算：\n\n- Full KV 需 16 GB；RLKV 在 0.4 稀疏下仅 8.4 GB（–47%），在 0.6 稀疏下 6.4 GB（–60%），且准确率仍保持或优于全缓存。\n6. 消融实验（Qwen-2.5-7B-R1 / Math500）\n\n- 去掉自适应惩罚权重：训练崩溃，准确率跌至 <20%。\n- 去掉自蒸馏采样：奖励信号剧烈抖动，最终准确率 –8 pp。\n- 调节 β：0.001 取得最佳稀疏-性能平衡；过大(0.005) 过度压缩，过小(0.0002) 无法稀疏。\n7. 与 instruct 模型对比\n同压缩方法在 Llama-3.1-8B-Inst 与 Qwen-2.5-7B-Inst 上仅轻微下降，而在对应 R1 推理模型上暴跌，验证“推理长度放大压缩危害”的动机假设。\n\nQ5: 有什么可以进一步探索的点？\n\n论文在第 6 节已给出三点未来方向，结合实验结果与当前局限，可进一步细化为以下可落地探索：\n\n1. 推理头的“功能细粒度”解析\n\n- 跨层联动：统计不同压缩率下被保留头的层-头分布，观察是否形成固定“推理子图”，并用因果干预（ablation/intervention）验证其是否负责自反、回溯、假设验证等子行为。\n- 任务特异性：将 RLKV 分别只在代码、几何、逻辑谜题上训练，比较所得推理头集合的交集/差异，构建“任务-头”映射表，为模型编辑或专家混合路由提供依据。\n- 可解释性工具：把高 α 头的注意力热图与 CoT 步骤对齐，检验是否持续关注关键中间变量（如方程中的“x”、代码中的循环变量），从而把“推理头”转译为“符号跟踪头”。\n2. 极端压缩（≥80%）下的性能悬崖再突破\n\n- 动态头预算：当前按全局 top-k 选取，可改为“层内自适应”——每层根据当前步骤复杂度实时决定保留头数，用轻量 RNN 控制器预测下一步需激活的推理头集合。\n- 递归压缩：对非推理头再引入二级压缩（量化+低秩投影），形成“完整/局部/量化”三档缓存，实现 80–90% 显存削减的同时把性能下降控制在 3 pp 以内。\n- 投机推理：用小型“草稿模型”生成候选 Token，仅对验证失败的位置回退并临时展开完整 KV，进一步摊销长序列开销。\n3. 与长度控制正交联合\n\n- 双目标 RL：在 GRPO 奖励中同时引入“正确性 + 长度惩罚”，让模型自主权衡“缩短 CoT”与“保留更多推理头”，寻找给定显存约束下的帕累托前沿。\n- 阶段式训练：先以 ThinkPrune 类方法把平均输出长度减半，再运行 RLKV 进行头级压缩，验证两种正交策略能否叠加获得乘法级内存收益。\n4. 跨模型与跨模态迁移\n\n- 蒸馏推理头：把大模型（70B）上发现的推理头索引蒸馏给小模型（7B），检验小模型仅通过复制头掩码是否即可获得一致提升，降低重复训练成本。\n- 多模态推理：将 RLKV 扩展到视觉-语言推理模型（如 Gemini-1.5 / GPT-4o），观察视觉编码器与文本解码器中是否出现“跨模态推理头”，并验证对其压缩是否同样造成逻辑断裂。\n5. 系统级协同优化\n\n- 与分页/量化联合：在已支持 paged attention 的推理框架里集成 RLKV 掩码，实现“页级+头级”两级淘汰；再引入 2-bit KV 量化，测试三管齐下能否在 32 k 上下文场景把单卡吞吐提升 5× 以上。\n- 自适应缓存调度：根据实时批次长度分布动态调整 sink+local 窗口大小，使 RLKV 在不同负载下始终处于最优显存-延迟平衡点。\n6. 理论侧：稀疏门控的收敛保证\n\n- 分析带 L1 正则的 GRPO 收敛条件，给出奖励稀疏度与惩罚系数 β 的临界关系，为后续调参提供理论 upper/lower bound，避免再次“训练崩溃”。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：推理大模型因超长思维链导致 KV 缓存爆炸，现有 token-dropping 与 head-reallocation 方法在高压缩率下性能骤降，无法保持推理一致性。\n- **核心观察**：KV 头在推理模型中呈“功能异质”——仅少数头对链式推理至关重要，其余可压缩。\n\n- **方法（RLKV）**：\n\n1. 混合注意力 + 可学习门控 α∈\n0,1\n^{L×H}，量化每头对“完整缓存”的依赖。\n2. 用可验证奖励的强化学习（GRPO）优化 α，辅以 L1 稀疏正则与自适应惩罚权重，自动识别“推理头”。\n3. 推理时只对高 α 头保留完整 KV，其余用常数 sink+local 缓存，实现 20–50% 显存削减。\n- **实验**：在 Llama-3.1-8B-R1 与 Qwen-2.5-7B-R1 的 GSM8K、MATH500、AIME24、MBPP 上，RLKV 在各压缩率均优于 H2O、R-KV、DuoAttention；0.4–0.6 稀疏下仍近乎无损，部分高难任务甚至反超全缓存基线。\n- **结论**：首次系统验证并利用了“推理头”概念，为推理模型的高效部署提供新的稀疏压缩范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Which%20Heads%20Matter%20for%20Reasoning%3F%20RL-Guided%20KV%20Cache%20Compression，链接是https://arxiv.org/pdf/2510.08525，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08525。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-17-1760399244942","title":"Progressive Gaussian Transformer with Anisotropy-aware Sampling for Open Vocabulary Occupancy Prediction","description":"The 3D occupancy prediction task has witnessed remarkable progress in recent years, playing a crucial role in vision-based autonomous driving systems. While traditional methods are limited to fixed semantic categories, recent approaches have moved towards predicting text-aligned features to enable open-vocabulary text queries in real-world scenes. However, there exists a trade-off in text-aligned scene modeling: sparse Gaussian representation struggles to capture small objects in the scene, while dense representation incurs significant computational overhead. To address these limitations, we present PG-Occ, an innovative Progressive Gaussian Transformer Framework that enables open-vocabulary 3D occupancy prediction. Our framework employs progressive online densification, a feed-forward strategy that gradually enhances the 3D Gaussian representation to capture fine-grained scene details. By iteratively enhancing the representation, the framework achieves increasingly precise and detailed scene understanding. Another key contribution is the introduction of an anisotropy-aware sampling strategy with spatio-temporal fusion, which adaptively assigns receptive fields to Gaussians at different scales and stages, enabling more effective feature aggregation and richer scene information capture. Through extensive evaluations, we demonstrate that PG-Occ achieves state-of-the-art performance with a relative 14.3% mIoU improvement over the previous best performing method. Code and pretrained models will be released upon publication on our project page: https://yanchi-3dv.github.io/PG-Occ","url":"https://huggingface.co/papers/2510.04759","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.942Z","metadata":{"authors":["Chi Yan","Dan Xu"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.CV","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.04759.pdf","arxivUrl":"https://arxiv.org/abs/2510.04759","arxivId":"2510.04759","coolPaperUrl":"https://papers.cool/arxiv/2510.04759","published":"2025-10-06T12:36:07Z","updated":"2025-10-06T12:36:07.000Z","zh_summary":"近年来，3D 占据预测任务取得了显著进展，在基于视觉的自动驾驶系统中起着至关重要的作用。传统方法通常仅限于固定的语义类别，而近期的方法则倾向于预测文本对齐特征，以在真实世界场景中实现开放词汇的文本查询。然而，在文本对齐的场景建模中存在权衡：稀疏高斯表示难以捕捉场景中的小物体，而稠密表示会带来显著的计算开销。为了解决这些限制，我们提出了 PG-Occ，一种创新的渐进式高斯变换器框架，实现了开放词汇的 3D 占据预测。我们的框架采用渐进式在线致密化策略，这是一种前馈机制，能够逐步增强 3D 高斯表示，从而捕捉细粒度的场景细节。通过迭代增强表示，框架实现了日益精确和详细的场景理解。另一个关键贡献是引入了具有时空融合的各向异性采样策略，该策略能够自适应地为不同尺度和阶段的高斯分配感受野，从而实现更有效的特征聚合并捕捉丰富的场景信息。通过大量评估，我们展示了 PG-Occ 在性能上达到了最先进水平，相比之前性能最佳的方法，mIoU 提升了 14.3%。代码和预训练模型将在论文发布后通过我们的项目页面提供：https://yanchi-3dv.github.io/PG-Occ","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决\\*\\*开放词汇三维占用预测（open-vocabulary 3D occupancy prediction）\\*\\*中的两个核心矛盾：\n\n1. **稀疏高斯表征难以捕捉细小物体**\n现有方法（如 GaussTR）利用稀疏的 3D 高斯分布降低计算量，却因表征过于稀疏而丢失精细结构，导致小目标漏检或边界模糊。\n\n2. **密集表征带来不可接受的计算开销**\n若直接采用密集 voxel 或高维文本-对齐特征场，内存与延迟随空间分辨率立方增长，难以满足车载实时需求。\n\n为此，作者提出 **PG-Occ**，一种**渐进高斯 Transformer 框架**，在保持稀疏表征高效性的同时，通过\\*\\*在线前馈加密（progressive online densification）**逐步补全细节，并引入**各向异性感知采样（anisotropy-aware sampling）\\*\\*自适应调整感受野，实现：\n\n- 任意文本提示下的开放词汇语义占用查询\n- 仅依赖 2D 图像与文本嵌入即可训练，无需 3D 占用真值\n- 在 Occ3D-nuScenes 上取得 **14.3% 相对 mIoU 提升**且推理速度提高 41%\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节系统回顾了相关研究，可归纳为三大脉络：\n\n- **闭集 3D 占用感知**\n依赖固定类别标签，代表工作：\n\n- SurroundOcc（Wei et al. 2023）——BEV 加高维交叉注意力\n- TPVFormer（Huang et al. 2023）——三视角视图分解降参\n- FastOcc（Hou et al. 2024）——2D BEV 卷积替代 3D 卷积\n- GaussianFormer（Huang et al. 2024b）——稀疏高斯减少空域计算\n- **开放词汇 3D 占用感知**\n通过文本-视觉特征对齐实现任意词汇查询，代表工作：\n\n- OVO（Tan et al. 2023）——蒸馏 2D 开放词汇分割器 + CLIP 文本编码器\n- POP-3D（Vobecky et al. 2024）——半监督三模态融合\n- VEON（Zheng et al. 2024a）——组装两种 2D 基础模型\n- LangOcc（Boeder et al. 2024）——NeRF 实现纯视觉开放词汇\n- GaussTR（Jiang et al. 2024）——稀疏高斯 blob 前馈推理，固定查询数\n- **前馈式 3D 高斯溅射**\n解决原始 3D-GS 需逐场景离线优化问题，代表工作：\n\n- Splatter Image（Szymanowicz et al. 2024）——单视图直接预测高斯参数\n- pixelSplat（Charatan et al. 2024）——双视图前馈重建\n- GPS-Gaussian（Zheng et al. 2024b）——人体前馈重建\n- DrivingForward（Tian et al. 2025）——稀疏视角驾驶场景前馈重建\n- GaussianFlowOcc（Boeder et al. 2025）——高斯流估计处理动态场景\n\nPG-Occ 在上述基础上首次提出**渐进加密**与**各向异性感知采样**，兼顾效率与精细度，实现开放词汇 3D 占用预测的新 SOTA。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **PG-Occ** 框架，通过三项核心设计系统性地解决“稀疏-精细”矛盾与“开放词汇”需求：\n\n1. **渐进高斯建模（Progressive 3D Gaussian Modeling）**\n\n- **Base Layer**：利用伪深度快速初始化稀疏高斯，覆盖场景粗结构。\n- **B 个 Progressive Layer**：每层执行 **Progressive Online Densification (POD)**——以前层渲染误差为线索，前馈式补充高斯，仅对欠重建区域加密，避免全局密集化带来的计算爆炸。\n结果：高斯数量随推理动态增长，细节逐步补全，而计算预算集中在“需要的地方”。\n2. **各向异性感知采样（Anisotropy-aware Feature Sampling, AFS）**\n对任一高斯，按其 **尺度 s + 旋转四元数 r** 生成 16 个椭圆体采样偏移，投影到多视角、多时间戳的 2D 特征图，再聚合得到更具代表性的嵌入。\n好处：\n\n- 采样点真实落在高斯有效支撑域内，提升几何-语义一致性；\n- 自适应感受野，小高斯采样范围小，大高斯覆盖广，减少冗余计算。\n3. **非对称自注意力（Asymmetric Self-Attention, ASA）**\n在渐进加密过程中，新插入的高斯尚未充分优化。ASA 通过掩码限制其只能“向旧高斯学习”，而旧高斯不受新高斯干扰，保证训练稳定、信息单向传播。\n\n4. **纯 2D 监督训练**\n无需 3D 占用真值，仅利用：\n\n- **深度渲染损失**  L_(depth) （SILog + L1 + 时序光度一致性）\n- **特征渲染损失**  L_(feat) （Cosine + MSE）\n总损失：\n\nL_(total) = λ_(depth)L_(depth) + λ_(feat)L_(feat)\n\n训练阶段高斯→2D 渲染，推理阶段高斯→体素化，实现任意分辨率开放词汇查询。\n\n通过“**粗到细前馈加密 + 各向异性采样 + 非对称注意力 + 2D 渲染监督**”，PG-Occ 在 Occ3D-nuScenes 上取得 **15.15 mIoU**，比此前最佳方法提升 **14.3%**，同时帧率提升 **41%**，训练时间缩短 **25%**。\n\nQ4: 论文做了哪些实验？\n\n论文在 Occ3D-nuScenes 与 nuScenes-retrieval 两大公开基准上进行了系统实验，覆盖**开放词汇占用预测**、**深度估计**、**检索任务**、**消融分析**与**效率对比**五大方面：\n\n1. **开放词汇 3D 占用预测（Occ3D-nuScenes）**\n\n- 指标：IoU / mIoU / Ray-IoU\n- 对比方法：SelfOcc、OccNeRF、GaussianOcc、VEON、DistillNeRF、LangOcc、GaussTR 等 8 篇最新工作\n- 结果：PG-Occ 仅用车载相机+文本监督，取得 **15.15 mIoU**，相对此前最佳 **+14.3%**；在 car、truck、bus、barrier 等中等尺寸类别上优势显著。\n2. **开放词汇检索（nuScenes-retrieval）**\n\n- 任务：给定文本提示，对 105 段场景的 LiDAR 点进行语义检索\n- 指标：mAP / mAP(v)（可见点）\n- 结果：PG-Occ 取得 **21.2 mAP(v)**，优于纯视觉方法 LangOcc（18.2）与 POP-3D（17.5）。\n3. **深度估计（nuScenes 验证集）**\n\n- 指标：Abs Rel / Sq Rel / RMSE / RMSE-log\n- 结果：PG-Occ **Abs Rel = 0.139**，比监督标签源 Metric3D V2（0.170）提升 **18.2%**，验证渐进高斯对几何一致性的增益。\n4. **消融实验**\n\n- **POD**（渐进加密）：移除后 mIoU ↓ 0.31，Ray-IoU ↓ 1.34\n- **AFS**（各向异性采样）：移除后 mIoU ↓ 0.12\n- **ASA**（非对称注意力）：移除后 mIoU ↓ 0.30\n- 扩展高斯数量：1000 个增量最佳，2000 个因 voxel 分辨率饱和反而略降\n- 基层高斯数：4000 个最佳，8000 个注意力过载导致下降\n- 损失函数：五者联合最优；缺少任一均下降 0.2–0.5 点\n- 光度监督：加入颜色损失反而使 mIoU ↓ 0.19，说明室外场景颜色-语义耦合弱\n- 位姿噪声：0.01–0.5 std 范围内 mIoU 波动 < 0.04，验证鲁棒性\n5. **效率对比**\n\n- 训练时间：PG-Occ 9 h，GaussTR 12 h，LangOcc >48 h\n- 推理速度：PG-Occ 2.40 FPS，较 GaussTR（1.04 FPS）提升 **+41%**\n- 层内耗时：Base 27.4 ms → Progressive-1 58.3 ms → Progressive-2 60.6 ms，与加密后高斯数量线性相关\n6. **可视化与视频演示**\n\n- 提供 BEV、 ego-centric、第三视角的零样本语义占用对比，显示小目标（行人、自行车、交通锥）与薄结构（护栏、杆）重建更完整\n- 项目网页附视频，展示“Locate the garbage bin”等文本查询在 3D 占用网格中的精确定位过程\n\n综合以上实验，论文从**精度、检索能力、几何准确性、模块有效性、计算效率**等多维度验证了 PG-Occ 的先进性与实用性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 PG-Occ 框架的自然延伸，亦对应论文第 5 节指出的局限与未来工作：\n\n- **4D 高斯扩展**\n当前高斯仅编码单时间步状态。将高斯显式扩展为时序轨道（4D Gaussian with temporal basis）可直接建模动态场景，缓解遮挡与运动模糊，同时保持前馈推理优势。\n\n- **深度方向多视角约束**\n驾驶场景相机基线窄，深度估计在 Z 轴尺度易漂移。引入跨帧光流一致性、IMU 预积分或弱 LiDAR 稀疏信号作为正则，可进一步抑制“popping”伪影。\n\n- **自适应内存管理**\n渐进加密导致高斯数量随场景复杂度增长，峰值 GPU 占用不可控。研究基于信息增益或重要性采样的在线剪枝/合并策略，实现“加密-剪枝”闭环，可在保证精度的同时维持 O(N) 内存预算。\n\n- **层级蒸馏与早期退出**\n对加密层进行层级蒸馏，让浅层网络即可对简单区域给出足够置信预测；引入早期退出机制，可在车载低算力平台根据不确定性动态终止后续加密，提升平均延迟。\n\n- **文本-几何联合嵌入**\n目前文本特征仅在高斯级别做点积匹配。探索将 CLIP 视觉-语言先验蒸馏为统一的 3D 语义-几何空间，使“语言描述”直接参与高斯参数回归，有望提升稀有类别与复合描述（如“停在公交站前的红色巴士”）的召回率。\n\n- **多任务统一框架**\n将 3D 检测、运动预测、语义补全与高斯表征共享同一套加密引擎，利用任务间一致性损失（如检测框-高斯重叠损失）作为加密信号，实现“一次加密、多任务共享”的高效感知栈。\n\n- **极端天气与夜间鲁棒性**\n当前监督依赖 Metric3D V2 与 MaskCLIP，二者在雨雪、夜间性能下降。引入事件相机、毫米波雷达或红外模态，并通过多模态对齐损失指导高斯加密，可提升恶劣条件下的几何-语义一致性。\n\n- **真实时车载部署**\n加密操作基于 CUDA 栅格化实现，仍受限于高斯-栅格化核函数内存带宽。探索 TensorRT/INT8 量化、高斯参数结构化稀疏（如通道级分组）与 GPU-DLA 异构调度，可把帧率推至 >10 FPS，满足真实时要求。\n\n- **自监督文本提示挖掘**\n目前文本提示需人工给定。结合视觉基础模型（GLIP、SAM）在 2D 检测结果上自动生成候选词，再通过 3D-2D 一致性过滤，可实现“场景-自适应”提示库，进一步扩展开放词汇上限。\n\n- **闭环仿真-到-真实**\n利用高斯可微渲染特性，在仿真引擎（NVIDIA Omniverse）生成无限文本-场景对，进行域随机化与对抗训练，缩小仿真到真实的差距，降低实车数据采集成本。\n\n这些方向既涵盖表征升级（4D、多模态）、计算优化（内存、早期退出），也涉及应用落地（车载部署、闭环仿真），为后续研究提供了清晰的技术路线。\n\nQ6: 总结一下论文的主要内容\n\n论文核心速览\n\n**题目**：PG-Occ：面向开放词汇三维占用预测的渐进高斯 Transformer（含各向异性感知采样）\n\n1 待解决问题\n\n- 稀疏高斯表征：细节缺失，小物体难捕获\n- 密集高维特征场：内存/计算爆炸，车载难实时\n- 传统占用基准：语义类别固定，无法响应任意文本查询\n\n2 核心思路\n\n“**由粗到细、边推理边加密**”——仅用稀疏高斯初始预算，通过**前馈式在线加密**逐步补全场景，同时用**各向异性采样**提升多视角-时序特征聚合质量，实现高效且精细的开放词汇三维占用预测。\n\n3 方法框架\n\n| 模块 | 功能 |\n| --- | --- |\n| Base Layer | 伪深度→初始稀疏高斯，覆盖粗几何 |\n| Progressive Layer ×B | 每层三步：① POD：渲染误差驱动→新增高斯② ASA：新高斯仅受旧高斯影响，训练稳定③ AFS：按 s,r 生成椭圆采样点→投影多视角→聚合特征 |\n| Voxelization | 推理时将最终高斯通过 exp(-0.5(x-μ)ᵀΣ⁻¹(x-μ)) 累加为任意分辨率占用/语义场 |\n| 训练监督 | 纯2D：深度渲染损失 + 文本对齐特征渲染损失，无需3D真值 |\n\n4 主要结果\n\n- **Occ3D-nuScenes**\n- mIoU **15.15**（+14.3% 相对提升），仅相机+文本，优于此前最佳 VEON\n- **nuScenes-retrieval**\n- mAP(v) **21.2**，超越 LangOcc 18.2\n- **深度估计**\n- Abs Rel **0.139**，比监督源 Metric3D V2 提升 18.2%\n- **效率**\n- 训练 9 h（−25%）| 推理 2.40 FPS（+41%）\n\n5 贡献提炼\n\n1. 渐进高斯 Transformer：首次将“前馈-在线加密”引入三维占用，兼顾效率与细节\n2. 各向异性感知采样：利用高斯椭圆几何自适应决定感受野，提升多视角-时序特征融合\n3. 开放词汇 SOTA：在两大基准同时刷新精度与速度，且无需3D标注即可训练\n\n6 未来展望\n\n4D高斯时序建模、深度方向多视角约束、自适应内存剪枝、多任务统一框架、极端天气多模态融合、车载实时量化部署等方向可进一步探索。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Progressive%20Gaussian%20Transformer%20with%20Anisotropy-aware%20Sampling%20for%20Open%20Vocabulary%20Occupancy%20Prediction，链接是https://arxiv.org/pdf/2510.04759，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.04759。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1760399244943","title":"StatEval: A Comprehensive Benchmark for Large Language Models in Statistics","description":"Large language models (LLMs) have demonstrated remarkable advances in mathematical and logical reasoning, yet statistics, as a distinct and integrative discipline, remains underexplored in benchmarking efforts. To address this gap, we introduce \\textbf{StatEval}, the first comprehensive benchmark dedicated to statistics, spanning both breadth and depth across difficulty levels. StatEval consists of 13,817 foundational problems covering undergraduate and graduate curricula, together with 2374 research-level proof tasks extracted from leading journals. To construct the benchmark, we design a scalable multi-agent pipeline with human-in-the-loop validation that automates large-scale problem extraction, rewriting, and quality control, while ensuring academic rigor. We further propose a robust evaluation framework tailored to both computational and proof-based tasks, enabling fine-grained assessment of reasoning ability. Experimental results reveal that while closed-source models such as GPT5-mini achieve below 57\\% on research-level problems, with open-source models performing significantly lower. These findings highlight the unique challenges of statistical reasoning and the limitations of current LLMs. We expect StatEval to serve as a rigorous benchmark for advancing statistical intelligence in large language models. All data and code are available on our web platform: https://stateval.github.io/.","url":"https://huggingface.co/papers/2510.09517","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.943Z","metadata":{"authors":["Yuchen Lu","Run Yang","Yichen Zhang","Shuguang Yu","Runpeng Dai","Ziwei Wang","Jiayi Xiang","Wenxin E","Siran Gao","Xinyao Ruan","Yirui Huang","Chenjing Xi","Haibo Hu","Yueming Fu","Qinglan Yu","Xiaobing Wei","Jiani Gu","Rui Sun","Jiaxuan Jia","Fan Zhou"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.09517.pdf","arxivUrl":"https://arxiv.org/abs/2510.09517","arxivId":"2510.09517","coolPaperUrl":"https://papers.cool/arxiv/2510.09517","published":"2025-10-10T16:28:43Z","updated":"2025-10-10T16:28:43.000Z","zh_summary":"大规模语言模型（LLMs）在数学和逻辑推理方面展现出了显著进步，但统计学作为一门独特而综合的学科，在基准测试中仍然未得到充分探索。为弥补这一空白，我们引入了\\textbf{StatEval}，这是首个专注于统计学的综合性基准测试，涵盖各难度层次的广度和深度。StatEval 包含13,817个基础问题，覆盖本科和研究生课程，以及从权威期刊中提取的2,374个研究级证明任务。为了构建这一基准测试，我们设计了一个可扩展的多智能体流程，并结合人工验证，实现了大规模问题提取、改写和质量控制的自动化，同时确保学术严谨性。我们还提出了一个针对计算任务和证明任务的稳健评估框架，使推理能力能够得到细粒度评估。实验结果显示，封闭源模型如 GPT5-mini 在研究级问题上的表现不到57%，而开源模型的表现显著更低。这些发现凸显了统计推理的独特挑战以及当前大规模语言模型的局限性。我们期望 StatEval 能作为推动大规模语言模型统计智能发展的严格基准。所有数据和代码均可在我们的网站平台获取：https://stateval.github.io/。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nStatEval 旨在填补现有大模型评测体系在“统计推理”维度上的空白。\n具体而言，论文聚焦以下核心问题：\n\n- 统计学科的独特性：与纯数学或通用数值推理不同，统计学强调在不确定性下进行推断，需要同时掌握概率论、假设检验、回归分析、贝叶斯方法、渐近理论等多元知识，并要求具备严谨的证明与推导能力。\n- 评测覆盖不足：现有主流推理基准（MMLU、MATH、GSM-8K 等）中，统计类题目占比不足 3%，且多为孤立的概率趣题，缺乏对推断链条、证明过程及研究级难度的系统考察。\n- 训练数据偏差：大模型语料对机器学习、深度学习等“热门”主题过采样，导致基础概率、线性模型等核心方向暴露不足，模型能力分布失衡。\n\n为此，作者构建并开源了 StatEval——首个面向统计领域的大规模评测基准，通过以下手段直接回应上述问题：\n\n1. 双层难度轴\n\n- 基础数据集：13 817 道本硕阶段习题，覆盖 30+ 子领域，题型包括选择题与开放问答。\n- 研究数据集：2 374 道可验证的证明题，直接抽取自 18 本顶级期刊 2020–2025 年的定理与引理，要求给出封闭形式、收敛速率或常数明确的界。\n2. 细粒度学科轴\n按“概率-统计-机器学习”三大域继续细分高维建模、因果推断、实验设计、贝叶斯非参、强化学习等方向，并引入“理论性质”二级标签（渐近性、可识别性、最优性、检验有效性等），实现跨难度、跨主题的双重切片分析。\n\n3. 可扩展的多智能体构建管线\n文件转换→上下文分割→问题生成→质量控制的四段式流程，辅以人工复核与小样本反馈迭代，保证题目自洽、无答案泄露、可量化验证，同时支持大规模自动化更新。\n\n4. 面向推导与证明的评分框架\n对开放题采用“步骤提取→结果提取→LLM 评判→三次保守取最小”的四步流程，分别度量推理准确性、步骤完整性与最终答案正确性，解决统计证明多路径、多表达带来的评判不稳定问题。\n\n5. 实证揭示的能力缺口\n在 StatEval-mini 上的实验显示，即使最强的闭源模型（GPT-5-mini）在研究级题目上的准确率也仅 57.6%，开源模型最高仅 51.1%，且在机器学习理论、泛化误差界等类别上普遍落后 10–20 个百分点，证明统计推理对当前大模型仍具显著挑战性。\n\n综上，StatEval 通过“大规模、可验证、双轴分层”的基准设计，首次系统量化了大模型在统计学科的基础知识掌握与前沿理论推导能力，为后续提升统计智能提供了明确的评测坐标与改进方向。\n\nQ2: 有哪些相关研究？\n\n与 StatEval 相关的研究可归纳为三条主线：\n\n1. 数学与通用推理评测\n2. 科研级定理证明评测\n3. 统计应用与代码评测\n\n以下按类别列出代表性工作，并说明与 StatEval 的关联与差异。\n\n1\\. 数学与通用推理评测\n\n| 基准 | 核心特点 | 与 StatEval 的关系 |\n| --- | --- | --- |\n| MMLU (Hendrycks et al., 2020) | 57 学科多选，含高中统计概念题 | 统计题占比 <1%，仅概念回忆，无推导 |\n| MATH (Hendrycks et al., 2021) | 12.5k 竞赛数学题，含少量概率计数 | 概率题孤立，无统计推断或证明 |\n| GSM-8K (Cobbe et al., 2021) | 小学–初中文字应用题 | 完全不含统计或不确定性推理 |\n| MathBench (Liu et al., 2024) | 分层数学题库，含本科统计章节 | 统计部分仅为基础计算，无研究级任务 |\n| Omni-MATH (Gao et al., 2024) | 奥赛难度，概率题为主 | 聚焦解题技巧，不考察统计理论或证明 |\n| UG-MathBench (Xu et al., 2025) | 大学数学期末题 | 统计题量少，无期刊定理证明 |\n\n**结论**：上述基准要么统计含量极低，要么停留在计算/竞赛层面，无法评估“不确定性下的系统推断”与“定理证明”能力，StatEval 直接补全这一空白。\n\n2\\. 科研级定理证明评测\n\n| 基准 | 核心特点 | 与 StatEval 的关系 |\n| --- | --- | --- |\n| TheoremQA (Chen et al., 2023) | 大学定理应用题，跨数学/物理/CS | 仅 5% 题目涉及统计，且为“套用定理”而非证明 |\n| RealMath (Zhang et al., 2025) | 从 arXiv 提取证明题 | 数学分析为主，统计论文占比 <3%，无随机过程或渐近理论 |\n| PaperBench (Starace et al., 2025) | 人工复现 ICML 论文实验 | 侧重实验复现，不含理论推导或误差界证明 |\n| PutnamBench (Tsoukalas et al., 2024) | 形式化 Putnam 竞赛题 | 纯数学，无随机变量或统计推断 |\n| FormalMath (Yu et al., 2025) | Lean4 形式化证明 | 当前无法处理随机变量、渐近符号与测度论统计证明 |\n\n**结论**：科研级基准要么排除统计，要么因形式化限制无法表达统计证明的核心要素（随机变量、渐近阶、误差界）。StatEval 首次把“期刊原文定理→可验证 QA”流程自动化，并提供非形式化但可量化判对的评分标准。\n\n3\\. 统计应用与代码评测\n\n| 基准 | 核心特点 | 与 StatEval 的关系 |\n| --- | --- | --- |\n| StatQA (Zhu et al., 2024) | 表格数据上的方法选择、假设检验 | 仅应用层面，无理论推导或证明 |\n| DSBench / DSCodeBench (Jing et al., 2024; Ouyang et al., 2025) | 数据科学工作流与代码生成 | 关注代码正确性，不考察统计理论 |\n| StatLLM (Song et al., 2025) | R/Python 统计脚本生成 | 评估脚本能否跑出结果，无误差界或渐近性质 |\n| MLAgentBench (Huang et al., 2023) | 端到端机器学习实验 | 侧重实验可复现性，不含理论保证 |\n\n**结论**：这类基准把统计当作“数据分析任务”，目标是跑通代码或得到 P 值，而非推导估计量的渐近分布或证明误差界。StatEval 与之互补，专注“理论正确性”与“推导严谨性”。\n\n4\\. 评测方法论相关\n\n| 工作 | 核心贡献 | 与 StatEval 的关联 |\n| --- | --- | --- |\n| LLM-as-a-Judge (Ashktorab et al., 2025) | 用 LLM 做二元对错评判 | 直接用于 StatEval 的多轮步骤评分，但本文加入“三次保守取最小”降低位置偏差 |\n| PoE 比较框架 (Liusie et al., 2024) | 减少 pairwise 比较次数 | StatEval 采用单次 LLM 步骤评分+保守最小化，兼顾成本与稳定性 |\n| Lean4 形式化 (Moura & Ullrich, 2021) | 提供完全严谨证明检查 | 统计证明涉及随机变量、渐近阶，目前难以形式化，StatEval 采用“非形式化但可量化”折中 |\n\n总结\n\n现有研究要么停留在基础计算/应用层面，要么因领域限制（纯数学、代码生成）无法覆盖统计学科的核心——**不确定性推理与定理证明**。StatEval 首次把“本科–研究生–期刊前沿”全链路题目统一到一个可扩展、可验证、可细粒度评分的基准中，为评估并提升大模型的统计智能提供了新的实验平台。\n\nQ3: 论文如何解决这个问题？\n\nStatEval 并非在“算法”层面提出新的训练方法，而是从**基准构建与评测范式**入手，系统性地解决“统计推理缺乏高质量、可扩展、可验证评测”这一根本痛点。具体策略可拆解为四条技术路径：\n\n1\\. 双层难度 + 双轴学科 的题库设计\n\n- **难度轴**\n- 基础层：13 817 题，覆盖本硕 50+ 教材与真题，保证课程完整性。\n- 研究层：2 374 题，全部取自 2020–2025 年顶刊（AoS、JASA、Biometrika 等），确保前沿性。\n- **学科轴**\n- 一级：Probability / Statistics / Machine Learning。\n- 二级：30+ 子域（高维建模、因果推断、贝叶斯非参、强化学习等）。\n- 研究层再引入“理论性质”二级标签（渐近性、可识别性、最优性、检验有效性等），实现同一题目“领域+性质”双重切片。\n\n**效果**：任何模型在 StatEval 上的得分可立即定位到“本科-研究生-研究”哪一阶段、哪一子领域、哪一类理论性质存在短板，为后续微调或数据增强提供精确坐标。\n\n2\\. 多智能体 + 人在回路 的自动化构建管线\n\npipeline 四段式：\n\n| 阶段 | 智能体 | 关键技术 | 输出质量保障 |\n| --- | --- | --- | --- |\n| 文件转换 | MinerU 多模态模型 | OCR + LaTeX 重建 | 保留公式、表格、交叉引用 |\n| 上下文分割 | Gemini-Flash-Lite | 动态正则 + 1 M token 长窗口 | 定理、引理、假设一次性完整抽取 |\n| 问题生成 | GPT-5 reasoning 版 | 基于五元 rubric 的约束解码 | 难度适中、无答案泄露、单答案、可量化验证 |\n| 质量复核 | GPT-5 独立裁判 | 一致性+完整性检查 | 自动筛除 30% 不合格题 |\n\n- **人在回路**：每周抽样 5% 由统计 PhD 标注正确性与难度，错误样例以 few-shot 形式回注到分割与生成 agent，实现“不停车”迭代。\n\n**效果**：把过去需要全职数学家数月才能完成的“定理→QA”手工转化，压缩到 24 h 内自动处理上千篇 PDF，同时保持学术严谨度。\n\n3\\. 面向推导与证明的细粒度评分框架\n\n针对不同题型设计“过程-结果”双轨指标：\n\n- **选择题**：0/1 精确匹配，标准考场规则。\n- **开放题（含证明）**：\n1. 步骤提取：用 LLM 把模型长答案拆成“假设→推导→中间结果→结论”链。\n2. 结果提取：每步骤输出是否给出可验证的符号表达式、数值或分布。\n3. LLM 裁判：以参考解答为 gold，逐 step 打三维 0/1 分\n- 推理准确性（思路对即可）\n- 步骤完整性（计算结果也对）\n- 最终答案正确性\n4. 保守聚合：\n\nS_(final)=min_(i=1,2,3)l(α S_r^((i))+β S_s^((i))+(1-α-β)S_a^((i))r),quad α=0.4,;β=0.3\n\n三次随机种子取最小，降低 LLM 评判方差。\n\n**效果**：同一份模型输出可得到“最终分 + 过程分”两张成绩单，既能反映“会不会算”，也能揭示“哪里推错”，为后续诊断提供可解释信号。\n\n4\\. 大规模实证诊断与能力缺口曝光\n\n- **评测规模**：StatEval-mini 3 300 题（1 300 基础 + 2 000 研究），覆盖全学科与性质标签。\n- **模型谱系**：闭源（GPT-5/ Gemini-2.5）（开源（LLaMA-3.1、Qwen、DeepSeek 等）共 11 个。\n- **关键发现**：\n1. 研究级任务平均准确率 <57%，远低于模型在 AIME 等数学竞赛的表现，说明“统计证明”具有独特难度。\n2. 基础层：概率论、线性模型得分低于机器学习类，暴露训练语料主题偏差。\n3. 研究层：GPT-5 在“可识别性与一致性”“检验有效性”两项可达 70%+，但在“泛化误差界”“最优性结果”上骤降 15–20%，指向高阶理论推导的系统性不足。\n\n**效果**：用数据把“统计推理短板”具体化，为社区提供明确的 benchmark leaderboard，后续研究可直接以 StatEval 为靶标进行继续预训练、指令微调或 RLVR。\n\n总结\n\n论文通过“双层题库 → 自动构建管线 → 细粒度评分 → 大规模诊断”四位一体方案，首次把统计学科从“零散概率题”提升到“覆盖本硕博+期刊前沿、可量化验证、可定位短板”的系统级评测高度，从而解决了“大模型统计推理无法被严格评估与改进”的核心问题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 StatEval-mini 进行了两轮系统实验，分别对应**基础数据集**与**研究数据集**，旨在量化现有大模型在“本科–研究生–期刊前沿”三级统计推理上的真实表现。实验设计、观测指标与关键结论如下。\n\n1\\. 实验设置\n\n| 要素 | 配置 |\n| --- | --- |\n| 评测集 | StatEval-mini（从完整 16 k 题中分层抽样）• 基础：1 300 题（本科 600 + 研究生 700）• 研究：2 000 题（期刊定理证明） |\n| 评分协议 | 第 4 章框架• 选择题为 0/1• 开放题三次保守最小，权重 α=0.4, β=0.3 |\n| 评判模型 | GPT-5（temperature=0，统一裁判，减少位置偏差） |\n| 被测模型 | 闭源 4 款：GPT-5、GPT-5-mini、Gemini-2.5-Pro、Gemini-2.5-Flash开源 7 款：LLaMA-3.1-8B、GPT-OSS-20B/120B、DeepSeek-V3.1、Qwen2.5-72B、Qwen3-30B、Qwen3-235B |\n| 随机种子 | 3 次独立采样与打分，报告 min 分（保守结果） |\n| 主要指标 | 1. 总体准确率 = 总得分 / 总题数2. 分层准确率：本科/研究生/研究级；概率/统计/机器学习；各理论性质标签 |\n\n2\\. 基础数据集实验（1 300 题）\n\n2.1 本科 vs 研究生\n\n| 模型 | 本科平均 | 研究生平均 | 总体 |\n| --- | --- | --- | --- |\n| GPT-5 | 87.24 | 78.72 | 82.85 |\n| GPT-5-mini | 84.69 | 76.50 | 80.37 |\n| Gemini-2.5-Pro | 86.90 | 75.43 | 80.88 |\n| Qwen3-235B | 80.57 | 73.13 | 76.96（开源最佳） |\n| DeepSeek-V3.1 | 62.88 | 45.13 | 53.98 |\n| LLaMA-3.1-8B | 45.30 | 39.98 | 42.79 |\n\n**观察**\n\n- 闭源平均领先开源 ≈10–15 pp；研究生段差距更大。\n- 同一模型研究生段普遍下降 6–12 pp，说明高阶概念与推导难度明显提升。\n\n2.2 子域细目（本科段）\n\n| 领域 | GPT-5 | Qwen3-235B | LLaMA-3.1-8B |\n| --- | --- | --- | --- |\n| 概率 | 88.2 | 80.1 | 46.3 |\n| 统计 | 87.5 | 77.6 | 44.2 |\n| 机器学习 | 85.9 | 78.4 | 43.1 |\n\n**观察**\n\n- 概率 > 统计 ≈ ML，符合课程复杂度排序。\n- 开源模型在概率段下降最陡，暴露语料偏向应用 ML、缺少传统概率训练。\n\n3\\. 研究数据集实验（2 000 定理证明题）\n\n3.1 总体 leaderboard\n\n| 模型 | 总体准确率 | 最佳性质类别 |\n| --- | --- | --- |\n| GPT-5-mini | 57.62 | Identifiability & Consistency (77%) |\n| GPT-5-nano | 47.05 | Testing Validity (64%) |\n| Gemini-2.5-Flash | 51.14 | Structural Guarantees (60%) |\n| Qwen3-235B | 51.10（开源最佳） | Probability (62%) |\n| GPT-OSS-120B | 49.49 | Testing Validity (66%) |\n| DeepSeek-V3.1 | 42.21 | — |\n\n**观察**\n\n- 闭源最高 57.6%，开源最高 51.1%，远低于同一模型在 AIME25 的 70–80% 级别，说明“统计证明”具有额外难度。\n- 所有模型在“Generalization & Error Bounds”与“Optimality Results”两项普遍下降 15–20 pp，揭示高阶理论推导仍是集体短板。\n\n3.2 子域对比（GPT-5-mini 为例）\n\n| 子域 | 准确率 |\n| --- | --- |\n| Probability | 66.5 |\n| Statistics | 59.5 |\n| Machine Learning | 48.6 |\n\n**观察**\n\n- ML 类定理（深度网络泛化、RL 误差界）得分最低，与基础数据集趋势一致，表明语料热点≠理论难度。\n\n3.3 理论性质细目（平均值）\n\n| 基准 | 核心特点 | 与 StatEval 的关系 |\n| --- | --- | --- |\n| TheoremQA (Chen et al., 2023) | 大学定理应用题，跨数学/物理/CS | 仅 5% 题目涉及统计，且为“套用定理”而非证明 |\n| RealMath (Zhang et al., 2025) | 从 arXiv 提取证明题 | 数学分析为主，统计论文占比 <3%，无随机过程或渐近理论 |\n| PaperBench (Starace et al., 2025) | 人工复现 ICML 论文实验 | 侧重实验复现，不含理论推导或误差界证明 |\n| PutnamBench (Tsoukalas et al., 2024) | 形式化 Putnam 竞赛题 | 纯数学，无随机变量或统计推断 |\n| FormalMath (Yu et al., 2025) | Lean4 形式化证明 | 当前无法处理随机变量、渐近符号与测度论统计证明 |\n\n0\n\n**观察**\n\n- “误差界”类别断崖式落后，再次印证当前 LLM 对非渐近、高阶常数型界推导的系统性不足。\n\n4\\. 交叉趋势分析\n\n- **模型规模效应**：在开源族内，20B→120B→235B 参数提升带来约 +8–10 pp 绝对增益，但仍未跨越闭源门槛。\n- **effort-tuned 效应**：GPT-5-mini 相对 GPT-5-nano +10.6 pp，证明推理预算增加对统计证明显著有效。\n- **评判稳定性**：三次保守最小分相比单次打分平均降低 2.3 pp，方差缩减 40%，说明该策略有效抑制 LLM-as-a-judge 的位置偏差与随机波动。\n\n5\\. 结论性数字（一句话 summary）\n\n- 最强闭源模型在**本科基础**达 87%，**研究生基础**降至 79%，**期刊证明**再降至 58%；\n- 最强开源模型对应数字为 81%、73%、51%，**三级阶梯式下降**成为统计推理的显著特征；\n- 所有模型在“泛化误差界”类别**集体低于 30%**，是当前统计智能的公开短板。\n\nQ5: 有什么可以进一步探索的点？\n\nStatEval 的实验结果与 pipeline 设计暴露出多条值得深挖的研究路径，可归纳为**数据、算法、评测、应用**四个维度：\n\n1\\. 数据与知识增强\n\n- **定理级继续预训练**\n将 StatEval 研究层 2 374 道期刊证明 + 对应原始论文上下文作为 Corpus，构建「统计继续预训练」任务（Stat-CT），验证是否能在不损失通用能力的前提下提升研究级准确率。\n\n- **反向数据合成**\n利用 LLM 把已验证的正确证明反向改写成“同义不同形”的新定理（paraphrase + 结构扰动），自动生成百万级统计证明语料，缓解期刊样本稀缺问题。\n\n- **多语言与跨文化教材**\n当前题库以英文教材/期刊为主，可引入中文、德文、法文经典教材（如《概率论教程》、Вентцель 教材）考察语言分布对统计概念理解的影响，构建 StatEval-i18n。\n\n2\\. 算法与推理机制\n\n- **Lean4 / Isabelle 形式化统计库**\n针对“误差界”类证明集体失灵现象，尝试把常见的 Hoeffding、Bernstein、Talagrand 不等式以及 U-statistic 渐近正态性等模块形式化，建立 StatLean 库，让模型在交互式证明环境中逐步学会高阶常数推导。\n\n- **工具增强统计推理**\n给模型外挂符号计算（SymPy、Mathematica）与数值验证（R/Stan），形成「理论推导–符号求导–数值双检」闭环，检验工具使用能否提升 Optimality & Error Bounds 两类题目。\n\n- **过程监督 RLVR**\n以 StatEval 的步骤级 0/1 标签为可验证奖励，用 RLVR 微调模型，奖励不再只依赖最终答案，而是「每步推理正确性」，观察能否在 1–2B 小模型上复现 GPT-5-mini 级性能。\n\n3\\. 评测框架扩展\n\n- **置信区间与假设检验型答案**\n当前答案要求单值/单式，可扩展至「置信区间覆盖」「p 值范围」或「拒绝域表达式」，引入「统计意义下正确」的容差评分，进一步贴近真实统计咨询场景。\n\n- **多模态统计图证明**\n将含有图形（QQ-plot、ROC、残差图）的论文定理纳入，构建 StatEval-V，考察模型能否读懂图形信息并给出正确推导，补齐纯文本之外的推理通道。\n\n- **人机协作评分（Crowd-Stat）**\n对 5% 高争议证明题引入众包+专家双重标注，建立「人类一致率 vs LLM-as-a-judge」映射曲线，量化裁判模型在不同难度区间的可靠性，为后续自动评分提供置信权重。\n\n4\\. 应用与风险评估\n\n- **统计教育个性化**\n利用细粒度子域得分，为学生生成「能力雷达图」并推荐对应教材章节与练习题，实现 LLM 辅助的个性化统计教学系统。\n\n- **科研助理自动化**\n在 Overleaf 或 arXiv 插件中嵌入 StatEval-Research API，实时检查作者证明草稿：给出错误定位、缺失引理或更紧的误差界建议，降低理论文章的人工审校成本。\n\n- **模型校准与不确定性量化**\n用 StatEval 的置信区间题检验 LLM 自身不确定性表达是否校准（预测区间是否真正覆盖 1−α），揭示大模型在「教别人统计」时是否也「低估自身误差」。\n\n5\\. 交叉学科延伸\n\n- **因果推断基准子集**\n将研究层 481 道因果题单独抽取，结合 DoWhy、CausalML 代码实现，构建 CausalEval，考察模型能否同时给出识别假设、估计量与代码实现三重正确性。\n\n- **生物统计与临床试验**\n与医药企业合作，把Ⅲ期试验方案与 FDA 反馈转化为可验证 QA，测试 LLM 能否正确推导样本量、把握度与期中分析边界，评估其在监管级统计文档中的可用性。\n\n- **联邦学习与隐私误差界**\n引入差分隐私、联邦学习场景下的收敛界与隐私损失 δ 表达式，构建 Privacy-Stat 子集，检验模型能否同时处理统计精度和隐私预算双重约束。\n\n一句话总结\n\nStatEval 已经把「统计推理评测」从 0 到 1 搭建完成，下一步值得沿着**更大规模数据、更严谨形式化、更细粒度评测、更垂直场景落地**四个方向继续探索，把大模型真正推向“可靠统计学家”级别。\n\nQ6: 总结一下论文的主要内容\n\nStatEval：首个面向统计学科、贯穿“本科教材—研究生课程—顶刊定理”全链路的大规模推理基准。\n\n1\\. 背景与动机\n\n- 大模型数学推理评测日趋成熟，但统计专项几乎空白：现有 benchmark 统计题占比 <3%，且多为孤立概率趣题，缺乏对“不确定性下推断+严格证明”的系统考察。\n- 统计学科独特：融合概率、推断、回归、贝叶斯、渐近理论，强调形式化证明与误差界，训练语料却偏向热门 ML 主题，导致模型基础概率与理论推导集体短板。\n\n2\\. 数据集构建\n\n| 基准 | 核心特点 | 与 StatEval 的关系 |\n| --- | --- | --- |\n| TheoremQA (Chen et al., 2023) | 大学定理应用题，跨数学/物理/CS | 仅 5% 题目涉及统计，且为“套用定理”而非证明 |\n| RealMath (Zhang et al., 2025) | 从 arXiv 提取证明题 | 数学分析为主，统计论文占比 <3%，无随机过程或渐近理论 |\n| PaperBench (Starace et al., 2025) | 人工复现 ICML 论文实验 | 侧重实验复现，不含理论推导或误差界证明 |\n| PutnamBench (Tsoukalas et al., 2024) | 形式化 Putnam 竞赛题 | 纯数学，无随机变量或统计推断 |\n| FormalMath (Yu et al., 2025) | Lean4 形式化证明 | 当前无法处理随机变量、渐近符号与测度论统计证明 |\n\n1\n\n- 双轴结构：难度轴（本科/研究生/研究）+ 学科轴（Probability | Statistics | Machine Learning × 高维/因果/贝叶斯等）。\n- 多智能体 pipeline：PDF→LaTeX→上下文分割→问题生成→质量复核+人工反馈，实现大规模自动抽取与学术级严谨并存。\n\n3\\. 评测框架\n\n- 选择题 0/1 匹配。\n- 开放题四步评分：步骤提取→结果提取→LLM 裁判→三次保守最小，输出“过程分+最终分”，可定位推导错误位置。\n- 用 GPT-5 作统一裁判，降低人工开销与位置偏差。\n\n4\\. 实验结果\n\n| 基准 | 核心特点 | 与 StatEval 的关系 |\n| --- | --- | --- |\n| TheoremQA (Chen et al., 2023) | 大学定理应用题，跨数学/物理/CS | 仅 5% 题目涉及统计，且为“套用定理”而非证明 |\n| RealMath (Zhang et al., 2025) | 从 arXiv 提取证明题 | 数学分析为主，统计论文占比 <3%，无随机过程或渐近理论 |\n| PaperBench (Starace et al., 2025) | 人工复现 ICML 论文实验 | 侧重实验复现，不含理论推导或误差界证明 |\n| PutnamBench (Tsoukalas et al., 2024) | 形式化 Putnam 竞赛题 | 纯数学，无随机变量或统计推断 |\n| FormalMath (Yu et al., 2025) | Lean4 形式化证明 | 当前无法处理随机变量、渐近符号与测度论统计证明 |\n\n2\n\n- 三级阶梯：本科 ≈80% → 研究生 ≈73% → 期刊证明 <58%，暴露统计证明独特难度。\n- 集体短板：泛化误差界、最优性结果两类平均 <30%，为后续算法与数据增强提供明确靶标。\n\n5\\. 贡献与影响\n\n1. 提出 StatEval——首个覆盖统计全学科、可验证、可细粒度诊断的大规模推理基准。\n2. 设计可扩展的多智能体+人在回路构建管线，平衡规模与学术严谨。\n3. 建立面向推导与证明的步骤级评分框架，解决统计多路径证明评判不稳定问题。\n4. 实证揭示闭源与开源模型在统计推理上的显著差距与理论推导短板，为继续预训练、RLVR 与形式化证明研究提供基准坐标。\n\n一句话总结\n\nStatEval 把“统计推理”从边缘概率题升级为贯穿教材到顶刊定理、可自动构建与细粒度评测的系统级基准，首次量化并公开了大模型成为“可靠统计学家”还缺多远。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是StatEval%3A%20A%20Comprehensive%20Benchmark%20for%20Large%20Language%20Models%20in%20Statistics，链接是https://arxiv.org/pdf/2510.09517，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09517。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-19-1760399244943","title":"MRMR: A Realistic and Expert-Level Multidisciplinary Benchmark for Reasoning-Intensive Multimodal Retrieval","description":"We introduce MRMR, the first expert-level multidisciplinary multimodal retrieval benchmark requiring intensive reasoning. MRMR contains 1,502 queries spanning 23 domains, with positive documents carefully verified by human experts. Compared to prior benchmarks, MRMR introduces three key advancements. First, it challenges retrieval systems across diverse areas of expertise, enabling fine-grained model comparison across domains. Second, queries are reasoning-intensive, with images requiring deeper interpretation such as diagnosing microscopic slides. We further introduce Contradiction Retrieval, a novel task requiring models to identify conflicting concepts. Finally, queries and documents are constructed as image-text interleaved sequences. Unlike earlier benchmarks restricted to single images or unimodal documents, MRMR offers a realistic setting with multi-image queries and mixed-modality corpus documents. We conduct an extensive evaluation of 4 categories of multimodal retrieval systems and 14 frontier models on MRMR. The text embedding model Qwen3-Embedding with LLM-generated image captions achieves the highest performance, highlighting substantial room for improving multimodal retrieval models. Although latest multimodal models such as Ops-MM-Embedding perform competitively on expert-domain queries, they fall short on reasoning-intensive tasks. We believe that MRMR paves the way for advancing multimodal retrieval in more realistic and challenging scenarios.","url":"https://huggingface.co/papers/2510.09510","source":"HuggingFace Papers","timestamp":"2025-10-13T23:47:24.943Z","metadata":{"authors":["Siyue Zhang","Yuan Gao","Xiao Zhou","Yilun Zhao","Tingyu Song","Arman Cohan","Anh Tuan Luu","Chen Zhao"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.IR"],"pdfUrl":"https://arxiv.org/pdf/2510.09510.pdf","arxivUrl":"https://arxiv.org/abs/2510.09510","arxivId":"2510.09510","coolPaperUrl":"https://papers.cool/arxiv/2510.09510","published":"2025-10-10T16:14:56Z","updated":"2025-10-10T16:14:56.000Z","zh_summary":"我们介绍了 MRMR，这是首个需要深入推理的专家级多学科多模态检索基准。MRMR 包含 1,502 个查询，涵盖 23 个领域，且正样本文档均经过人工专家的严格验证。与以往的基准相比，MRMR 引入了三个关键进展。首先，它在不同专业领域对检索系统提出挑战，使得跨领域的模型对比更加细致。其次，查询具有高度推理性，其中的图像需要更深入的解读，例如对显微镜切片进行诊断。我们还引入了“矛盾检索”任务，这是一项要求模型识别概念冲突的新任务。最后，查询和文档被构建为图文交错序列。不同于早期仅限于单张图像或单模态文档的基准，MRMR 提供了一个具有多图像查询和混合模态语料文档的真实场景。我们对 MRMR 上的 4 类多模态检索系统和 14 个前沿模型进行了广泛评估。使用 LLM 生成图像标题的文本嵌入模型 Qwen3-Embedding 取得了最高性能，凸显了多模态检索模型仍有大量提升空间。虽然最新的多模态模型如 Ops-MM-Embedding 在专家领域查询上表现具有竞争力，但在高推理任务上仍显不足。我们相信，MRMR 为在更真实且具有挑战性的场景中推进多模态检索铺平了道路。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有**多模态检索基准**在**专家级、推理密集型场景**下的不足，具体聚焦于以下三个核心缺陷：\n\n1. **学科覆盖不足**\n现有基准多基于维基百科等通用领域，缺乏对医学、工程、科学等高风险专家领域的深度覆盖，难以评估模型在真实专家知识场景下的表现。\n\n2. **推理深度缺失**\n传统任务侧重语义匹配或信息查找，而专家级查询（如病理切片诊断、工程图纸合规性检查）需**跨模态逻辑推理**（如定理应用、矛盾检测），现有基准未系统量化此类能力。\n\n3. **数据格式简化**\n主流基准仅支持**单图查询**或**纯文本文档**，而实际检索场景中的查询与文档多为**图文交错序列**（如多图医学报告、含图表的定理证明），现有数据格式无法模拟真实输入复杂度。\n\n为填补上述空白，论文提出**MRMR**（Multidisciplinary Reasoning-intensive Multimodal Retrieval）基准，通过以下创新实现突破：\n\n- **23个专家领域**的1,502组查询-文档对，均经人类专家标注正例，覆盖医学诊断、定理证明、设计规则冲突检测等推理密集型任务；\n- **矛盾检索**新任务，要求模型识别图文信息中的逻辑冲突（如车辆设计违反安全规范）；\n- **图文交错序列**作为统一输入格式，查询与文档均支持多图-多文本混合，逼近真实检索环境。\n\n实验表明，当前多模态检索模型在MRMR上显著落后于**文本检索器+图像字幕**的组合（最佳多模态模型Ops-MM-Embedding落后6.7 nDCG@10），尤其在定理与矛盾任务上性能骤降30分以上，验证了基准对推动专家级多模态推理检索研究的必要性。\n\nQ2: 有哪些相关研究？\n\n论文在第2节“Related Work”中系统梳理了与MRMR密切相关的两条研究脉络，并辅以表1给出14份代表性基准的横向对比。可归纳为以下两类：\n\n1. 多模态检索基准\n\n- 语义匹配/图像相似度：NIGHTS、FashionIQ、CIRR、CIRCO\n- 图文互检（Caption Retrieval）：SciMMIR、EDIS、Wiki-SS\n- 视觉问答证据检索：WebQA、ViDoRe、MMDocIR、InfoSeek、OVEN\n- 图文交错文档检索：wikiHow-TIIR\n共同局限：仅单图查询、通用领域、无推理要求，如InfoSeek 1.35M问答对但均为维基开放域。\n2. 多模态检索模型与MM-RAG\n\n- 双编码器范式：CLIP、BLIP、SigLIP、EV"}}]}