{"timestamp":"2025-10-02T23:50:33.947Z","totalItems":20,"items":[{"id":"hf-0-1759448804157","title":"DeepSearch: Overcome the Bottleneck of Reinforcement Learning with Verifiable Rewards via Monte Carlo Tree Search","description":"Although RLVR has become an essential component for developing advanced reasoning skills in LLMs, contemporary studies have documented training plateaus that emerge following thousands of optimization steps, demonstrating notable decreases in performance gains despite increased computational investment. This limitation stems from the sparse exploration patterns inherent in current RLVR practices, where models rely on limited rollouts that often miss critical reasoning paths and fail to provide systematic coverage of the solution space. We present DeepSearch, a framework that integrates Monte Carlo Tree Search directly into RLVR training. In contrast to existing methods that rely on tree search only at inference, DeepSearch embeds structured search into the training loop, enabling systematic exploration and fine-grained credit assignment across reasoning steps. Through training-time exploration, DeepSearch addresses the fundamental bottleneck of insufficient exploration, which leads to diminishing performance improvements over prolonged training steps. Our contributions include: (1) a global frontier selection strategy that prioritizes promising nodes across the search tree, (2) selection with entropy-based guidance that identifies confident paths for supervision, and (3) adaptive replay buffer training with solution caching for efficiency. Experiments on mathematical reasoning benchmarks show that DeepSearch achieves 62.95% average accuracy and establishes a new state-of-the-art for 1.5B reasoning models - using 5.7x fewer GPU hours than extended training approaches. These results highlight the importance of strategic exploration over brute-force scaling and demonstrate the promise of algorithmic innovation for advancing RLVR methodologies. DeepSearch establishes a new direction for scaling reasoning capabilities through systematic search rather than prolonged computation.","url":"https://huggingface.co/papers/2509.25454","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.157Z","metadata":{"authors":["Fang Wu","Weihao Xuan","Heli Qi","Ximing Lu","Aaron Tu","Li Erran Li","Yejin Choi"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.25454.pdf","arxivUrl":"https://arxiv.org/abs/2509.25454","arxivId":"2509.25454","coolPaperUrl":"https://papers.cool/arxiv/2509.25454","published":"2025-09-29T20:00:29Z","updated":"2025-09-29T20:00:29.000Z","zh_summary":"尽管RLVR已成为在大型语言模型（LLM）中发展高级推理能力的重要组成部分，但现代研究已经记录了在数千次优化步骤后出现的训练平台期，尽管增加了计算投入，性能提升却显著下降。这一限制源于当前RLVR实践中固有的稀疏探索模式，模型依赖有限的展开，这些展开常常错过关键的推理路径，并且未能系统性地覆盖解空间。我们提出了DeepSearch，这是一个将蒙特卡洛树搜索直接集成到RLVR训练中的框架。与现有方法仅在推理阶段使用树搜索不同，DeepSearch将结构化搜索嵌入训练循环，使得在推理步骤中能够进行系统探索和精细的信任分配。通过训练时探索，DeepSearch解决了探索不足这一根本瓶颈，从而避免在长时间训练步骤中性能提升的递减趋势。我们的贡献包括：(1) 一种全球前沿选择策略，在搜索树中优先考虑有潜力的节点；(2) 基于熵的选择引导，用于识别可用于监督的可靠路径；(3) 自适应重放缓冲训练与解缓存以提高效率。在数学推理基准上的实验表明，DeepSearch实现了62.95%的平均准确率，并为1.5B推理模型建立了新的最先进成绩，同时使用的GPU时间比扩展训练方法少5.7倍。结果突显了策略性探索优于简单扩展计算的重要性，并展示了算法创新在推进RLVR方法中的潜力。DeepSearch为通过系统搜索而非长时间计算来扩展推理能力开辟了新的方向。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“强化学习+可验证奖励（RLVR）在训练后期出现性能平台”这一核心瓶颈展开研究。已有工作表明，即使继续增加训练步数，模型在数学推理等任务上的增益迅速递减，其根本原因是训练阶段探索不足：策略只能接触到少量采样轨迹，大量关键推理路径从未被探索，导致监督信号稀疏且重复。\n\n为此，作者提出将蒙特卡洛树搜索（MCTS）直接嵌入训练循环，构建 **DeepSearch** 框架，用“训练时系统化搜索”替代“推理时搜索+训练时简单 rollout”的传统范式，从而在相同甚至更少算力下持续扩大解空间覆盖度，实现更细粒度的信用分配，最终突破 RLVR 的平台期。\n\nQ2: 有哪些相关研究？\n\n论文在附录 A 中系统回顾了相关研究，可归纳为三大脉络：\n\n1. **基于搜索的推理（Search-based Reasoning）**\n\n- 代表工作：Tree-of-Thoughts (Yao et al., 2023)、REST-MCTS\\* (Zhang et al., 2024a)、Forest-of-Thought (Bi et al., 2024) 等。\n- 共同点：在**推理阶段**用 MCTS、beam search 或自一致性采样扩大测试时计算，但**训练阶段仍依赖简单 rollout**，未将探索信号反馈给策略更新。\n2. **可验证奖励强化学习（RLVR）**\n\n- 代表工作：DAPO (Yu et al., 2025)、DeepScaleR (Luo et al., 2025b)、ProRL (Liu et al., 2025a) 等。\n- 共同点：用可自动验证的奖励（数学答案正确性）做 PPO/GRPO，但**仅做直接采样**，缺乏系统探索，导致后期训练 plateau（Wu et al., 2025）。\n3. **蒙特卡洛树搜索与深度学习的结合**\n\n- 代表工作：AlphaGo (Silver et al., 2016)、Hypertree (Lample et al., 2022)、MCTS-LLM 在路径规划或定理证明中的应用 (Kemmerling et al., 2023)。\n- 共同点：在**游戏、机器人、定理证明**等领域将 MCTS 与神经网络策略结合，但**极少用于语言模型 RLVR 训练阶段**。\n\nDeepSearch 首次把“训练时 MCTS + 可验证奖励”引入 1.5 B 规模语言模型，填补了上述三条脉络的交叉空白：\n\n- 既不同于纯推理期搜索，也不同于纯 RLVR 的简单 rollout；\n- 借鉴了 AlphaGo 式“搜索即训练信号”思想，但针对 LLM 的离散 token 生成、稀疏奖励、高计算成本做了三项关键改进（全局前沿选择、熵引导负例、缓存式回放），从而突破平台期。\n\nQ3: 论文如何解决这个问题？\n\n论文把“训练阶段探索不足”这一瓶颈重新表述为\\*\\*「如何在策略更新前系统性地生成高质量、多样化且信用分配精细的推理轨迹」\\*\\*。为此，作者提出 **DeepSearch** 框架，将蒙特卡洛树搜索（MCTS）直接嵌入 RLVR 训练循环，用三处核心设计取代传统“采样-验证-更新”流水线：\n\n1. **全局前沿选择 + 熵引导扩展**\n\n- 放弃传统 root-to-leaf UCT 遍历，维护整棵搜索树的所有叶节点（frontier），用可微的优先级函数\n\nF(s)=λ_1tanh(Q_(parent))+λ_2 Hl(π_θ(s|o)r)+λ_3√d(s)/d_T\n\n一次性选出最有潜力的节点展开。\n- 若某轮未找到正确答案，则选“平均轨迹熵最低”的**自信负例**作为监督信号，迫使模型修正过度确信的错误路径。\n2. **细粒度 Q-值备份与 Tree-GRPO 目标**\n\n- 对选中的正确/自信负例轨迹，按衰减函数  γ(i,l)=max(i/l,γ_(min))  把终端奖励  q(s_(end))∈+1,-1  逐层回传，得到节点级 Q 值。\n- 用  tanh  软裁剪将中间节点 Q 值压入 $\n-1,1\n后，直接作为优势 hat A_(j,k)=q(s_j)-μ_t$ 进行 **Tree-GRPO** 更新，实现“每一步 token”都能感知自己在整条推理路径中的贡献，而不再只依赖终端稀疏奖励。\n3. **自适应训练 + 缓存回放**\n\n- 每轮先用当前策略做一次 **Pass@K** 筛查，仅对成功率低于阈值  δ^((i))  的“硬例”构建搜索树，避免在已充分掌握的问题上浪费算力。\n- 曾经搜到正确解但模型仍常做错的题目，把其正确轨迹  (x,t_(cached))  存入回放缓冲区；后续迭代遇到该题时，直接拼接缓存轨迹与少量新采样，用于训练，既防止灾难遗忘又省去重复 MCTS 开销。\n\n通过“**训练即搜索**”这一范式转换，DeepSearch 在同等 GPU 时长下探索的轨迹多样性提升一个量级，使 1.5 B 模型在 6 个数学基准上平均准确率从 61.70% → 62.95%，且仅用 330 GPU 小时（≈ 5.7× 少于纯堆步数的强基线），验证了**系统性探索优于暴力加深训练**的核心论点。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「数学推理能力」与「训练效率」两条主线展开实验，全部在 128×H100 96G 集群完成，共 4 组评测：\n\n1. 主基准对比（Table 1）\n数据集：AIME24/25、AMC23、MATH500、Minerva、Olympiad\n对比对象：12 个 1.5B 级别模型（含 DeepSeek-R1-Distill、DeepScaleR、Nemotron-v2 等）\n指标：Pass@1（n=32）\n结果：DeepSearch-1.5B 平均 62.95%，较前 SOTA（Nemotron-v2）绝对提升 1.25pp，六 dataset 全部领先。\n\n2. 训练效率对比（Table 2 & Figure 2）\n控制变量：均以 Nemotron-v2 为初始化，仅改变后续优化方式。\n\n- 延长训练：+325/+785/+1875 步，GPU 小时 326→1883，分数 61.78→62.02 即 plateau。\n- DeepSearch：仅追加 50 步 Tree-GRPO + MCTS，330 GPU 小时获得 62.95%，效率提升 5.7×，曲线斜率显著更陡。\n3. 搜索策略消融（Table 3）\n在 DeepMath-103K hardest 1.2k 子集上固定预算，考察：\n\n- 全局 frontier vs. 传统 UCT：迭代次数 −10.4%，轨迹奖励 −0.82→−0.65。\n- 深度奖励函数：√(d/d\\_T) 在 189 次迭代内取得最佳质量-效率平衡；线性深度虽最快（480 s/tree）但奖励下降。\n- 熵 bonus：λ2=0.4 时探索熵 1.23→1.31，验证不确定性引导的有效性。\n4. 算法演化消融（Table 4）\n逐步叠加 DeepSearch 各组件，观察 AIME/AMC/MATH 等平均分数：\nNemotron-v2 61.70\n→ 简单 MCTS 60.27\n→ 约束 Q-backup 61.38\n→ 节点级优势 61.85\n→ 均值归一化 62.32\n→ 全局 frontier 62.95\n显示每一模块均贡献正向增益，其中「全局 frontier 选择」带来最大单点提升（+0.63pp）。\n\n综上，实验既验证了 **SOTA 精度**，也量化了 **5.7× 训练效率优势**，并通过多维度消融确认了三项核心设计的必要性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为「算法层面」「验证层面」「系统层面」「应用层面」四类：\n\n算法层面\n\n1. **非数学领域的近似验证器**\n\n- 为代码生成、物理推理、生物序列设计等构造「可验证但非绝对」奖励，研究 MCTS 如何与不确定奖励共存。\n- 探索「置信度加权奖励」或「人类-in-the-loop 验证」下的节点 Q 值更新理论。\n2. **更复杂的树内策略学习**\n\n- 当前 πθ 仅负责展开，下一步可让策略在树内即进行「微调-on-the-fly」：用实时 Q 值做梯度步，实现深度强化学习与搜索的完全耦合。\n- 引入「价值函数网络」与策略网络共同备份，减少终端奖励稀疏性。\n3. **多目标/多模态 frontier 评分**\n\n- 除准确率外，同时优化推理长度、计算预算、可读性等多目标，采用 Pareto frontier 选择节点。\n- 在多模态任务（图文几何题、函数图像推理）中，把视觉置信度纳入 F(s) 评分。\n\n验证层面\n\n4. **可验证奖励的自动合成**\n\n- 研究 LLM 自动生成 unit-test、assertion、形式化规约（Hoare 逻辑、Lean 定理）作为奖励函数，减少人工标注成本。\n- 构建「验证器质量」与「策略性能」之间的 scaling law，量化验证器错误率对收敛速度的影响。\n5. **节点级过程奖励的鲁棒性**\n\n- 当前仅终端可验证，中间 Q 值由备份公式推导。可引入轻量级过程判别器（如结果倒推验证、子目标检查点）与备份公式融合，降低错误传播。\n\n系统层面\n\n6. **异构算力调度与分布式 MCTS**\n\n- 将 CPU rollout workers + GPU policy 推理分离，实现「树搜索-神经网络」异构流水线，进一步压缩 GPU 小时。\n- 探索分布式叶节点并行：全局 frontier 选择拆分为多机异步更新，提高大模型大预算下的扩展性。\n7. **与长上下文/稀疏注意力协同**\n\n- 推理链长度 >16k 时，注意力呈二次开销。可结合「滑动窗口-树缓存」或「令牌驱逐策略」，使 MCTS 在深树条件下仍保持线性内存增长。\n\n应用层面\n\n8. **跨领域迁移：数学→科学问答→代码竞赛**\n\n- 检验 DeepSearch 在 APPS、Codeforces、PhysicsQA 等需要「多步推导+可验证答案」的任务上是否同样打破 plateau。\n- 研究「数学预训练 + 目标领域小规模搜索」的迁移曲线，验证是否无需从头训练。\n9. **小模型/端侧推理的极限压榨**\n\n- 0.5 B 甚至 100 M 量级模型在端侧部署时，用 DeepSearch 训练阶段「一次性烧算力」换取推理阶段「零额外成本」，测试「训练-推理算力置换比」的极限。\n10. **教育与个性化辅导**\n\n- 利用树搜索记录的学生常见错误路径，生成个性化纠错提示；把「自信负例」理念转化为「高置信误区诊断」，用于自适应教学系统。\n\n以上方向既有助于把 DeepSearch 从数学专用框架推向通用推理引擎，也能在理论、系统与产品三个维度持续挖掘「训练期系统化探索」的潜力。\n\nQ6: 总结一下论文的主要内容\n\n**DeepSearch：把蒙特卡洛树搜索嵌入 RLVR 训练，以“训练期系统化探索”替代“推理期搜索+训练期简单 rollout”，突破数学推理模型在数千步后出现的性能平台。**\n\n1. 问题\n纯堆训练步数的 RLVR 在 1.5 B 模型上迅速 plateau，根本原因是训练阶段探索稀疏，大量关键推理路径从未被采样。\n\n2. 方法\n\n- **全局 frontier 选择**：一次性比较整棵树所有叶节点，用\n F(s)=λ_1tanh(Q_(parent))+λ_2 H(π_θ)+λ_3√d(s)/d_T \n选出最有潜力节点展开，省去传统 UCT 冗余根到叶遍历。\n- **熵引导负例监督**：若某轮无正确答案，选“平均轨迹熵最低”的自信错误路径进行惩罚，迫使模型修正过度确信区域。\n- **细粒度 Q 备份**：终端奖励 ±1 按衰减函数  γ(i,l)=max(i/l,γ_(min))  回传至每一步，经  tanh  软裁剪后作为优势  hat A_(j,k)=q(s_j)-μ_t  进行 Tree-GRPO 更新，实现节点级信用分配。\n- **自适应训练 + 缓存回放**：每轮仅对 Pass@K < δ 的“硬例”构建搜索树；已找到正确解但仍常出错的题目把轨迹缓存，后续迭代直接复用，避免灾难遗忘与重复算力。\n3. 实验\n\n- 六大数据集平均准确率 62.95%，较此前 1.5 B SOTA（Nemotron-v2）提升 1.25 pp，全部榜单第一。\n- 仅用 330 GPU 小时（+50 步）超越延长训练 1 875 步的 1 883 GPU 小时基线，效率提升 5.7×。\n- 消融显示全局 frontier 选择降低 10.4% 迭代次数并显著提高轨迹奖励；√(d/d\\_T) 深度奖励在效率与质量间取得最佳平衡；各组件逐步叠加累计带来 1.25 pp 最终增益。\n4. 结论\n**系统化探索 > 暴力加深训练**。DeepSearch 通过把 MCTS 写进训练循环，以算法创新实现精度与算力的双优，为 RLVR 提供“扩广度而非扩深度”的新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DeepSearch%3A%20Overcome%20the%20Bottleneck%20of%20Reinforcement%20Learning%20with%20Verifiable%20Rewards%20via%20Monte%20Carlo%20Tree%20Search，链接是https://arxiv.org/pdf/2509.25454，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25454。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1759448804158","title":"GEM: A Gym for Agentic LLMs","description":"The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.","url":"https://huggingface.co/papers/2510.01051","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.158Z","metadata":{"authors":["Zichen Liu","Anya Sims","Keyu Duan","Changyu Chen","Simon Yu","Xiangxin Zhou","Haotian Xu","Shaopan Xiong","Bo Liu","Chenmien Tan","Chuen Yang Beh","Weixun Wang","Hao Zhu","Weiyan Shi","Diyi Yang","Michael Shieh","Yee Whye Teh","Wee Sun Lee","Min Lin"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.01051.pdf","arxivUrl":"https://arxiv.org/abs/2510.01051","arxivId":"2510.01051","coolPaperUrl":"https://papers.cool/arxiv/2510.01051","published":"2025-10-01T15:55:57Z","updated":"2025-10-01T15:55:57.000Z","zh_summary":"大语言模型（LLM）的训练范式正在从静态数据集转向基于经验的学习，即代理通过与复杂环境交互来获取技能。为了促进这一转变，我们推出了 GEM（General Experience Maker，通用经验生成器），一个为 LLM 时代设计的开源环境模拟器。类似于传统强化学习（RL）的 OpenAI-Gym，GEM 提供了标准化的环境-代理接口框架，包括用于高吞吐量的异步向量化执行，以及便于扩展的灵活包装器。GEM 还提供了多样化的环境套件、稳健的集成工具，以及展示如何在五种流行 RL 训练框架中使用 GEM 的单文件示例脚本。此外，我们还提供了一套基于 24 个环境的基准测试，使用带有回报批量归一化（ReBN）的 REINFORCE 方法，该方法与 GRPO 不同，它兼容每回合密集奖励的完整 RL 设置，并提供更好的信用分配。我们进一步使用 GEM 在单回合和多回合设置中对 PPO、GRPO 和 REINFORCE 进行了公平对比基准测试，以揭示算法设计的差异。最后，GEM 除了作为训练环境之外，还可作为便捷的评估工具。我们希望该框架能够加速未来具代理能力的 LLM 研究。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nGEM 旨在解决“如何让大语言模型（LLM）在**多轮、长周期、工具增强**的复杂环境中通过强化学习（RL）获得经验并提升能力”这一核心问题。具体而言，论文针对以下痛点：\n\n1. 现有 RL-for-LLM 研究过度简化：\n绝大多数工作把任务做成**单轮**（如一次问答），导致算法（例如 GRPO）只能处理“上下文老虎机”形式，无法应对真正需要多轮规划、试错、工具调用的场景。\n\n2. 缺乏统一的多轮基准与接口：\n各工作自建环境、奖励函数不一致，难以公平比较算法；社区缺少像 OpenAI-Gym 那样的标准化“试验场”。\n\n3. 算法兼容性不足：\n流行的 group-relative 方法（GRPO 系列）为了利用样本内优势估计，只能使用**整条轨迹的稀疏奖励**且**折扣因子被迫为 1**，丧失了对“尽快完成任务”的激励，也无法做细粒度的每轮奖励塑形。\n\n4. 训练-环境耦合严重：\n现有框架往往把环境逻辑硬编码在训练代码里，新任务需要重写整套数据流，重复造轮子。\n\n因此，GEM 给出一条系统级解决方案：\n\n- 提供**标准化、可扩展、异步并行**的多轮环境接口（≈ OpenAI-Gym for LLM agents）。\n- 内置 7 大类、数十种任务（数学、代码、游戏、QA、终端、MCP 工具等），并支持即插即用的 Python/Search/MCP 工具。\n- 提出并验证**兼容密集奖励与任意折扣因子**的基线算法：REINFORCE + Return Batch Normalization（ReBN），可直接利用每轮奖励进行信用分配，而无需像 GRPO 那样做指数级分支采样。\n- 通过统一实验协议，对 PPO、GRPO、REINFORCE、ReBN 等进行**苹果对苹果**对比，量化不同算法在多轮与单轮场景下的优劣。\n- 额外提供评估模式，方便在相同协议下测试 GPT-5、Gemini-2.5-Pro、Claude-Sonnet-4 等强模型在数据库操作与终端任务上的表现。\n\n综上，GEM 的目标是把 LLM-agent 的 RL 研究从“单轮+稀疏奖励”的小圈子推向“多轮+工具+密集奖励”的通用体验学习时代，并降低后续研究与开发的门槛。\n\nQ2: 有哪些相关研究？\n\n与 GEM 直接相关的研究可归纳为 4 条主线：\n\n1. 单轮/稀疏奖励 RL-for-LLM\n2. 多轮工具增强 RL 框架\n3. 多智能体与自我对弈\n4. 统一环境接口与基准\n\n以下按时间序列出代表性工作，并指出其与 GEM 的关联与差异。\n\n1\\. 单轮/稀疏奖励 RL-for-LLM\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Ziegler et al. 2019 | 首次用 PPO 对 GPT 进行人类偏好微调，token-as-action，稀疏结局奖励 | 仅单轮对话，无工具；GEM 转向 per-turn 奖励 |\n| Ouyang et al. 2022 (InstructGPT) | 经典 RLHF 流程，单轮偏好建模 | 同上单轮设定，GEM 提供多轮扩展 |\n| Shao et al. 2024 (DeepSeekMath/GRPO) | 提出 Group Relative Policy Optimization，单轮数学问答 SOTA | GRPO 被 GEM 作为对照算法，指出其无法直接用于多轮密集奖励 |\n| Guo et al. 2025 (DeepSeek-R1) | 大规模数学推理 RL，仍用 γ=1 稀疏奖励 | GEM 实验表明 γ<1 对“最快策略”至关重要 |\n| Xiong et al. 2025 (Minimalist Rejection→REINFORCE) | 0/1 奖励 + 拒绝采样→REINFORCE，单轮 | GEM 沿用 REINFORCE 但引入 ReBN 与多轮折扣 |\n\n2\\. 多轮工具增强 RL 框架\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Singh et al. 2023 (Beyond Human Data) | 自训练+工具解数学题，单轨迹奖励 | GEM 采用相同 0/1 奖励但拆成 per-turn 并支持 γ<1 |\n| Yao et al. 2024 (τ-bench) | 定义带工具的多轮对话基准，固定状态=完整历史 | GEM 支持可定制状态包装器，避免上下文无限增长 |\n| Cao et al. 2025 (SkyRL-v0) | 真实世界长周期任务，轨迹级奖励 | 同 GEM 目标但耦合在训练框架；GEM 解耦环境 |\n| Jiang et al. 2025 (VerlTool) | 在 Verl 中硬编码工具调用，轨迹级 GRPO | GEM 提供即插即用 MCP，无需改训练框架 |\n| Jin et al. 2025 (Search-R1) | 用搜索工具做多轮推理，仍用 GRPO/γ=1 | GEM 实验显示 γ<1 才能学到最优搜索策略 |\n| Feng et al. 2025a (ReTool) | 工具使用 RL，单轨迹奖励 | GEM 支持 per-turn 奖励与任意折扣 |\n| Xue et al. 2025 (SimpleTIR) | 多轮工具推理，强制最大轮数代替折扣 | GEM 通过 γ<1 自然产生“尽快完成”激励，无需硬截断 |\n| Chen et al. 2025b (Verlog) | 把环境逻辑写进 Verl，轨迹级奖励 | GEM 把环境剥离成独立库，可跨训练框架复用 |\n\n3\\. 多智能体与自我对弈\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Liu et al. 2025a (SPIRAL) | 多智能体零和语言游戏，自我对弈提升推理 | GEM 提供 MultiAgentEnv 基类，可直接复现此类设定 |\n\n4\\. 统一环境接口与基准\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Brockman et al. 2016 (OpenAI Gym) | 传统 RL 统一接口 | GEM 明确称其为“LLM 时代的 Gym” |\n| Stojanovski et al. 2025 (ReasoningGym) | 100+ 单轮可验证推理任务 | GEM 直接集成其全部任务并扩展为多轮 |\n| Guertler et al. 2025 (TextArena) | 多轮语言博弈基准 | GEM 把其中游戏移植为可 RL 训练的环境 |\n| Team 2025a (MCPMark) / 2025b (Terminal-Bench) | 工具使用评估集 | GEM 原生支持 MCP 与 Docker 终端，可直接运行上述基准 |\n\n小结\n\n- 早期 RL-for-LLM 聚焦**单轮+稀疏奖励**，算法与评价环境均简化。\n- 近期出现**多轮+工具**研究，但普遍采用**整条轨迹奖励+γ=1**，并直接把环境逻辑嵌入训练框架，可复现性与扩展性差。\n- GEM 首次提供**解耦式、可扩展、支持 per-turn 密集奖励与任意折扣**的统一环境库，并验证 REINFORCE+ReBN 在该设定下的有效性，从而把上述多条研究路线纳入同一基准，实现公平对比与快速迭代。\n\nQ3: 论文如何解决这个问题？\n\n论文从“环境-接口-算法-评估”四个层面系统性地解决“LLM 如何在多轮、工具增强、长周期任务中通过强化学习获得经验”这一核心问题。具体手段如下：\n\n1\\. 环境层：构建标准化、可扩展的多轮试验场\n\n- **统一接口**\n完全对齐 OpenAI Gym 语义：`reset()` → `step(action)` → 返回 `(obs, reward, terminated, truncated, info)`。\n新增 `MultiAgentEnv` 基类，支持多智能体字典式交互。\n\n- **即插即用工具**\n将 Python 解释器、搜索引擎、任意 MCP Server 封装成 **Wrapper**；同一任务可零代码切换“无工具/单工具/多工具”模式，把原本单轮任务（如数学、ReasoningGym）自动变成多轮工具调用任务。\n\n- **异步向量化 & 自动重置**\n内置 `AsyncVectorEnv`，环境在后台通过异步 IO 批量跑 episode；`step()` 返回 `terminated` 后自动重置，无需手写调度逻辑，实现高吞吐体验采集。\n\n- **快速注册新任务**\n仅需继承 `gem.core.Env` 并实现 `reset/step`，一行 `register()` 即可把自定义任务接入整个生态；数学/代码/QA 类任务只需提供数据集路径即可自动生成环境。\n\n2\\. 接口层：灵活的状态-动作包装器\n\n- **Observation Wrapper**\n支持“仅最新输出”“历史拼接”“摘要历史”等多种状态表示，避免上下文无限增长；可像乐高一样叠加。\n\n- **Action Wrapper**\n自动解析 `<tool_call> … </tool_call>` 标签并路由到对应工具，再把工具返回包进下一观测，无需改动模型输出格式。\n\n3\\. 算法层：提出兼容多轮密集奖励的基线\n\n- **问题诊断**\nGRPO 等“组内相对优势”方法要求**同一状态可多次采样完整轨迹**，在多轮场景下导致指数爆炸；且强制整条轨迹奖励相同、折扣因子≈1，丧失“尽快完成”激励。\n\n- **解决方案——REINFORCE + Return Batch Normalization (ReBN)**\n- 采用“动作=一次回复”视角，天然支持 per-turn 密集奖励与任意 γ≤1。\n- 不学习 critic，仅对**批量内所有 transition 的折扣回报**做标准化：\n\nA^(ReBN)_(t) = (G_t – μ_G)/σ_G\n\n既抑制梯度方差，又保留细粒度信用分配。\n- 理论兼容 proximal update（可接 PPO 裁剪），实现单文件即可训练。\n- **系统级验证**\n在 8 个代表性环境（单轮/多轮、工具/无工具）上与 PPO、GRPO、vanilla REINFORCE 进行**苹果对苹果**对比：\n\n- GRPO 在单轮任务表现好，到多轮因常数优势估计而崩溃；\n- PPO 需学 critic，环境复杂时价值函数误差大；\n- ReBN 在所有环境上**无需 critic、无额外超参**，稳定地达到或超越 SOTA。\n\n4\\. 评估层：一键切换“训练模式⇄评测模式”\n\n- **跨框架即插即用**\n提供与 Oat、Verl、OpenRLHF、ROLL、RL2 等 5 个主流训练框架的**单文件示例脚本**；同一环境代码不改即可在不同框架下跑出一致曲线，验证 GEM 对训练侧无侵入。\n\n- **强模型基准**\n用同一套环境接口评估 GPT-5、Gemini-2.5-Pro、Claude-Sonnet-4：\n\n- 数据库操作（MCPMark-PostgreSQL）\n- 终端任务（Terminal-Bench Docker 容器）\n结果可直接对比成功率与平均交互轮数，展示 GEM 作为**统一评测工具包**的价值。\n\n5\\. 关键实验验证设计选择\n\n- **折扣因子 γ 的作用**\n在 GuessTheNumber（1-50 猜数字）中，γ=0.9 的 agent 自动学到二分搜索策略，平均 5.6 轮收敛；γ=0.999 则因缺乏“速度惩罚”耗尽 10 轮仍无法稳定获胜——**首次在 LLM 多轮 RL 中定量展示 γ<1 对最优策略的必要性**。\n\n- **工具增强效果**\n同一基础模型在数学/QA 任务上，**“+工具 +RL”** 相比纯基线平均提升 10-30 个百分点，且工具使用频率随 ReBN 训练自然升高，无需额外约束。\n\n- **跨任务泛化**\n仅在 Sudoku-easy 上训练，ReasoningGym 的 needle\\_haystack、circuit\\_logic 等未见任务准确率同步上升，说明多轮环境学到的策略具备一定迁移性。\n\n总结\n\nGEM 通过“标准化环境接口 + 异步并行数据收集 + 灵活包装器 + 兼容密集奖励的 ReBN 基线 + 统一评测协议”，把原先分散、耦合、只能跑单轮/稀疏奖励的 LLM-RL 研究，升级为**多轮、工具增强、长周期、可公平对比**的新范式，从而系统性地解决了“如何让 LLM 在复杂交互中通过经验学习”这一难题。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“多轮、工具增强、长周期”这一核心场景，共设计了 **5 组实验 18 幅学习曲线 + 3 张评测表**，覆盖算法、折扣因子、工具、泛化、框架与强模型评估六个维度。所有实验均基于 GEM 的统一环境接口与相同超参协议，确保苹果对苹果对比。\n\n1\\. 算法基准测试（8 环境）\n\n**目的**：验证 ReBN 是否能在单轮 & 多轮、工具 & 无工具场景下稳定优于 PPO、GRPO、vanilla REINFORCE。\n**环境**\n\n- 单轮可验证：ReasoningGym 的 `arc_1d / acre / mini_sudoku`\n- 多轮语言游戏：`GuessTheNumber / Sudoku / Minesweeper`\n- 多轮工具任务：`qa:MixedQA+Tool / math:DeepScaleR40K+Tool`\n\n**观测指标**\n\n- 平均 episode return（主图 4）\n- 样本效率（相同步数下的 asymptotic 性能）\n- 训练稳定性（3 组随机种子误差阴影）\n\n**结论**\n\n- GRPO 在单轮尚可，多轮因常数优势估计崩溃。\n- PPO 需学 critic，复杂环境价值误差大（Minesweeper 掉分）。\n- ReBN 无需额外网络，在所有 8 个环境取得最佳或并列最佳，验证“简单即通用”。\n\n2\\. 折扣因子 γ 消融（GuessTheNumber）\n\n**目的**：首次在 LLM 多轮 RL 中定量展示 γ<1 对“最快策略”的必要性。\n**协议**\n固定模型 Qwen3-1.7B、算法 ReBN，仅改 γ∈{0.9, 0.99, 0.999}。\n\n**观测**\n\n- 平均 episode return（图 5a 上）\n- 平均 episode length（图 5a 下）\n\n**结论**\nγ=0.9 时 agent 自动学到二分搜索，平均 5.6 轮猜中 1-50 的数；γ=0.999 几乎无速度惩罚，10 轮仍无法收敛。——说明 GRPO（强制 γ=1）无法获得最优效率策略。\n\n3\\. 工具增强效果评估\n\n3.1 数学工具（Python 解释器）\n\n**基准** AIME24 / AMC / MATH500 / MinervaMath / OlympiadBench\n**模型** Qwen3-4B-Base → 无工具 / 有工具 / 无工具+RL / 有工具+RL\n\n**结果表 1**\nBase 平均 35.3 → +工具 36.2 → +RL 41.4 → +工具+RL **49.8**\n**结论** 工具与 RL 的增益正交且可叠加，Python 解释器带来 8-15 分绝对提升。\n\n3.2 QA 工具（搜索引擎）\n\n**基准** 单跳 NQ/TriviaQA/PopQA + 多跳 HotpotQA/2Wiki/Musique/Bamboogle\n**协议** 同上一致，仅把工具换成 Search。\n\n**结果表 2**\nBase 平均 10.2 → 无工具+RL 23.9 → 有工具+RL **45.5**\n**结论** 搜索工具在多跳数据集上增益更大（↑30-40 分），再次验证工具+RL 的互补性。\n\n4\\. 跨任务泛化\n\n**协议** 只在 `game:sudoku-v0-easy` 训练，每隔一定步数在 3 个未见 ReasoningGym 任务（circuit\\_logic、needle\\_haystack、mini\\_sudoku）上测 zero-shot 准确率。\n\n**结果图 6**\n训练 400 步后，未见任务准确率分别从 0.25→0.55、0.15→0.35、0.85→0.95，表明多轮策略学习具备可迁移性。\n\n5\\. 训练框架兼容性\n\n**目的** 证明 GEM 对训练侧零侵入。\n**协议** 同一环境 `GuessTheNumber / rg:LetterCounting`、同一超参，分别用 Oat、Verl、OpenRLHF、ROLL、RL2 的五份**单文件脚本**训练。\n\n**结果图 7a**\n5 条学习曲线趋势一致，微小差异仅来自底层生成引擎（vLLM vs HF）。\n\n**额外效率测试**\n开启 RL2 的异步 rollout 后，wall-clock 时间减半（图 7b），展示“环境-训练解耦”带来的工程红利。\n\n6\\. 强模型统一评测（GEM 作为评测工具包）\n\n6.1 数据库操作（MCPMark-PostgreSQL 子集 20 任务）\n\n**模型** GPT-5 / Gemini-2.5-Pro / Claude-Sonnet-4\n**指标** 成功率 & 平均交互轮数\n\n**结果图 8 左**\nGPT-5 成功率 0.65，轮数 10.0；Gemini 0.35/14.3；Claude 0.30/20.1 —— 展示 GEM 可快速复现强模型工具使用排行榜。\n\n6.2 终端操作（Terminal-Bench 子集 57 任务）\n\n**环境** Docker 容器内 unix shell\n**结果图 8 右**\nGPT-5 成功率 0.49，轮数 17.4；Claude 0.33/42.8；Gemini 0.30/89.5 —— 再次验证 GEM 作为“统一评测沙盒”的通用性。\n\n附加实验（附录）\n\n- **ReBN 消融**：0/1 奖励无法激发工具，±1 固定负奖励易过使用；ReBN 用 0/1 即可稳定调用工具（图 9）。\n- **视觉-语言 RL**：在 Geometry3K 上 RL 微调 Qwen2.5-VL-3B/7B，验证 GEM 支持图像输入（图 10-11）。\n- **多智能体**：TAU-bench 零售场景，两 agent（用户+助手）共 9 种模型组合，发现“强用户”对成功率提升高于“强助手”（图 12）。\n\n总结\n\n实验矩阵从“算法-理论”到“工程-评测”全链条覆盖，核心结论：**GEM 提供的多轮、密集奖励、任意折扣接口不仅可行，而且让简单 REINFORCE+ReBN 就能在多样任务上达到或超越现有 PPO/GRPO 最佳结果，同时兼容五大训练框架与强模型评估。**\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接基于 GEM 的接口与数据格式继续深入，无需重新造轮子；每条都给出**可验证的实验指标**与**预期创新点**，方便后续工作快速落地。\n\n1\\. 算法层：更精细的信用分配\n\n| 探索点 | 实验指标 | 预期创新 |\n| --- | --- | --- |\n| 1.1 多步 TD 优势：将 ReBN 与 GAE 混合，按 turn 做 λ-回报 | 同环境样本效率 ↑，variance ↓ | 在 LLM 场景给出 λ 最优区间 |\n| 1.2 模型基线 critic：用小型 LM（1B）给大 LM（7B）估计 V(s) | PPO 价值误差 ↓，训练步数 ↓ | 验证“小老师”能否缓解价值过度估计 |\n| 1.3 轨迹子片段奖励重加权：对失败轨迹自动识别关键 turn 并放大负信号 | 相同样本下成功率 ↑，episode 长度 ↓ | 无人工奖励塑形即可加速收敛 |\n\n2\\. 折扣与奖励塑形\n\n| 探索点 | 实验指标 | 预期创新 |\n| --- | --- | --- |\n| 2.1 动态 γ 调度：训练初期 γ≈1 探索，后期 γ→0 提速 | 平均轮数 ↓，asymptotic 成功率持平 | 首次在 LLM-RL 给出 γ 退火曲线 |\n| 2.2 基于 token 成本的奖励：把真实 API 计费/延迟折算为 rt | 训练成本 ↓，胜率-成本 Pareto 前沿 | 提供“经济最优”策略基准 |\n| 2.3 稀疏→密集自动转换：用 LLM 自评 0/1/2 分，再与 ground-truth 对齐 | 人工一致性 κ↑，RL 收敛步数 ↓ | 降低人工标注 90% 工作量 |\n\n3\\. 工具使用策略\n\n| 探索点 | 实验指标 | 预期创新 |\n| --- | --- | --- |\n| 3.1 工具链搜索：允许一次调用返回多个候选，agent 做最佳组合 | 工具调用次数 ↑，最终成功率 ↑ | 超越“单工具单结果”假设 |\n| 3.2 工具拒绝学习：给模型“放弃”选项，奖励 −0.1 避免过度调用 | 平均调用 ↓，F1-like 工具效率 ↑ | 解决工具过度使用问题 |\n| 3.3 新工具零样本接入：用 MCP 描述文件自动生成 prompt，不重新训练 | 新工具成功率 vs 微调差距 ↓ | 验证描述文件足够让模型泛化 |\n\n4\\. 多智能体 & 自我对弈\n\n| 探索点 | 实验指标 | 预期创新 |\n| --- | --- | --- |\n| 4.1 用户-助手共训练：交替优化双方策略，观测是否出现“对抗性任务” | 助手成功率 vs 用户难度同步 ↑ | 揭示能力-挑战性平衡曲线 |\n| 4.2 多语言博弈：在 TextArena 狼人杀/谈判游戏训练联盟 & 背叛策略 | 胜率 ↑，对话策略可视化 | 首次展示 LLM 在纯语言博弈中的联盟演化 |\n| 4.3 裁判模型：引入第三方 LLM 做实时规则判定，替代手工奖励 | 裁判-人工一致率 ↑，训练稳定 | 把“规则奖励”自动化，减少环境定制成本 |\n\n5\\. 状态表示与上下文管理\n\n| 探索点 | 实验指标 | 预期创新 |\n| --- | --- | --- |\n| 5.1 思考压缩：用小型摘要模型把历史思考 token 压缩 50% | 相同上下文长度下成功率 ↓<3% | 给出压缩比-性能折中曲线 |\n| 5.2 分层观测：环境返回结构化 JSON（可用字段/工具列表） | 解析失败率 ↓，首轮工具调用准确率 ↑ | 验证结构化输入优于纯文本 |\n| 5.3 视觉-语言工具：把屏幕截图或 PDF 作为观测，用 VLM 做工具调用 | 视觉任务成功率 ↑ | 扩展 GEM 到 GUI 自动化/文档问答 |\n\n6\\. 跨任务与持续学习\n\n| 探索点 | 实验指标 | 预期创新 |\n| --- | --- | --- |\n| 6.1 任务课程：按难度递增顺序自动调度 GEM 环境 | 总学习时间 ↓，最终平均准确率 ↑ | 给出 LLM-RL 的课程学习基线 |\n| 6.2 灾难性遗忘回放：训练新任务时回放旧环境 10% 样本 | 旧任务准确率保持 ↑，遗忘 ↓ | 首次在 LLM-RL 量化遗忘率 |\n| 6.3 参数高效微调：LoRA/AdaLoRA 对比全量微调在 24 环境平均性能 | 可训练参数量 ↓10×，性能差距 <2% | 为大规模多任务训练提供工程指南 |\n\n7\\. 安全与可解释\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Singh et al. 2023 (Beyond Human Data) | 自训练+工具解数学题，单轨迹奖励 | GEM 采用相同 0/1 奖励但拆成 per-turn 并支持 γ<1 |\n| Yao et al. 2024 (τ-bench) | 定义带工具的多轮对话基准，固定状态=完整历史 | GEM 支持可定制状态包装器，避免上下文无限增长 |\n| Cao et al. 2025 (SkyRL-v0) | 真实世界长周期任务，轨迹级奖励 | 同 GEM 目标但耦合在训练框架；GEM 解耦环境 |\n| Jiang et al. 2025 (VerlTool) | 在 Verl 中硬编码工具调用，轨迹级 GRPO | GEM 提供即插即用 MCP，无需改训练框架 |\n| Jin et al. 2025 (Search-R1) | 用搜索工具做多轮推理，仍用 GRPO/γ=1 | GEM 实验显示 γ<1 才能学到最优搜索策略 |\n| Feng et al. 2025a (ReTool) | 工具使用 RL，单轨迹奖励 | GEM 支持 per-turn 奖励与任意折扣 |\n| Xue et al. 2025 (SimpleTIR) | 多轮工具推理，强制最大轮数代替折扣 | GEM 通过 γ<1 自然产生“尽快完成”激励，无需硬截断 |\n| Chen et al. 2025b (Verlog) | 把环境逻辑写进 Verl，轨迹级奖励 | GEM 把环境剥离成独立库，可跨训练框架复用 |\n\n0\n\n8\\. 系统与规模\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Singh et al. 2023 (Beyond Human Data) | 自训练+工具解数学题，单轨迹奖励 | GEM 采用相同 0/1 奖励但拆成 per-turn 并支持 γ<1 |\n| Yao et al. 2024 (τ-bench) | 定义带工具的多轮对话基准，固定状态=完整历史 | GEM 支持可定制状态包装器，避免上下文无限增长 |\n| Cao et al. 2025 (SkyRL-v0) | 真实世界长周期任务，轨迹级奖励 | 同 GEM 目标但耦合在训练框架；GEM 解耦环境 |\n| Jiang et al. 2025 (VerlTool) | 在 Verl 中硬编码工具调用，轨迹级 GRPO | GEM 提供即插即用 MCP，无需改训练框架 |\n| Jin et al. 2025 (Search-R1) | 用搜索工具做多轮推理，仍用 GRPO/γ=1 | GEM 实验显示 γ<1 才能学到最优搜索策略 |\n| Feng et al. 2025a (ReTool) | 工具使用 RL，单轨迹奖励 | GEM 支持 per-turn 奖励与任意折扣 |\n| Xue et al. 2025 (SimpleTIR) | 多轮工具推理，强制最大轮数代替折扣 | GEM 通过 γ<1 自然产生“尽快完成”激励，无需硬截断 |\n| Chen et al. 2025b (Verlog) | 把环境逻辑写进 Verl，轨迹级奖励 | GEM 把环境剥离成独立库，可跨训练框架复用 |\n\n1\n\n快速起步建议\n\n1. 换算法：直接继承 `gem.wrappers.RewardWrapper` 实现 2.1/2.2 的奖励塑形，跑 `math:Orz57K` 看收敛曲线。\n2. 加工具：按 MCP 协议写一个新 Server（如 Slack、Jira），注册 `mcp:YourTool-v0`，即可在现有训练脚本里零代码调用。\n3. 做多智能体：继承 `gem.envs.multiagent.MultiAgentEnv`，把 TAU-bench 换成其他领域（医疗、售后），复用 4.1 的共训练脚本。\n\nGEM 的模块化设计让上述任何方向都可在**单文件改动**内完成原型验证，大幅降低后续研究门槛。\n\nQ6: 总结一下论文的主要内容\n\nGEM: A Gym for Agentic LLMs\n一句话总结：面向多轮、工具增强、长周期任务的开源“LLM 版 OpenAI Gym”，提供统一环境接口、异步并行数据收集、兼容任意折扣与 per-turn 奖励的 ReBN 基线，以及跨 5 大训练框架与强模型的即插即用实验-评测一体化平台。\n\n1\\. 背景痛点\n\n- 现有 RL-for-LLM 聚焦**单轮**数学问答，算法（GRPO 等）只能处理稀疏轨迹奖励且强制 γ=1，无法激励“尽快完成”或做细粒度信用分配。\n- 各工作自建环境、奖励不一致，缺乏像 OpenAI Gym 那样的标准化试验场，难以公平比较。\n- 环境代码与训练框架深度耦合，新任务需重写数据流，重复造轮子。\n\n2\\. GEM 框架总览\n\n| 工作 | 核心贡献 | 与 GEM 的关系 |\n| --- | --- | --- |\n| Singh et al. 2023 (Beyond Human Data) | 自训练+工具解数学题，单轨迹奖励 | GEM 采用相同 0/1 奖励但拆成 per-turn 并支持 γ<1 |\n| Yao et al. 2024 (τ-bench) | 定义带工具的多轮对话基准，固定状态=完整历史 | GEM 支持可定制状态包装器，避免上下文无限增长 |\n| Cao et al. 2025 (SkyRL-v0) | 真实世界长周期任务，轨迹级奖励 | 同 GEM 目标但耦合在训练框架；GEM 解耦环境 |\n| Jiang et al. 2025 (VerlTool) | 在 Verl 中硬编码工具调用，轨迹级 GRPO | GEM 提供即插即用 MCP，无需改训练框架 |\n| Jin et al. 2025 (Search-R1) | 用搜索工具做多轮推理，仍用 GRPO/γ=1 | GEM 实验显示 γ<1 才能学到最优搜索策略 |\n| Feng et al. 2025a (ReTool) | 工具使用 RL，单轨迹奖励 | GEM 支持 per-turn 奖励与任意折扣 |\n| Xue et al. 2025 (SimpleTIR) | 多轮工具推理，强制最大轮数代替折扣 | GEM 通过 γ<1 自然产生“尽快完成”激励，无需硬截断 |\n| Chen et al. 2025b (Verlog) | 把环境逻辑写进 Verl，轨迹级奖励 | GEM 把环境剥离成独立库，可跨训练框架复用 |\n\n2\n\n3\\. 算法贡献：REINFORCE + ReBN\n\n- 问题：GRPO 在多轮需指数级采样，且只能整条轨迹奖励、γ=1。\n- 方案：回到“动作=一次回复”视角，用**批量归一化折扣回报**当优势：\n\nA^(ReBN)_t = (G_t – μ_G)/σ_G\n\n无需 critic，兼容 per-turn 密集奖励与任意 γ≤1。\n- 结果：在 8 个环境对比 PPO、GRPO、vanilla REINFORCE，ReBN **无额外网络即达或超越 SOTA**；γ=0.9 时 GuessTheNumber 自动学到二分搜索（5.6 轮最优）。\n\n4\\. 实验亮点\n\n1. 算法基准：首次苹果对苹果展示 GRPO 在多轮崩溃，PPO 价值误差大，ReBN 稳定领先。\n2. 工具增强：Python 解释器使数学平均 +14 分；搜索引擎使 QA 平均 +35 分；增益与 RL 正交。\n3. 泛化：仅在 Sudoku-easy 训练，未见 ReasoningGym 任务准确率同步提升。\n4. 框架兼容：单文件脚本无缝跑通 Oat/Verl/OpenRLHF/ROLL/RL2；开启 RL2 异步 rollout 壁钟提速 2×。\n5. 强模型评测：统一协议下 GPT-5 在 MCPMark-PostgreSQL 成功率 65%，Terminal-Bench 49%，均优于 Gemini-2.5-Pro 与 Claude-Sonnet-4。\n\n5\\. 结论与影响\n\nGEM 把 LLM-agent RL 从“单轮+稀疏奖励”小圈子推向“多轮+工具+密集奖励”通用体验学习时代，提供：\n\n- 即装即用的标准化环境库\n- 简单 yet SOTA 的 ReBN 基线\n- 训练-评估-对比一体化平台\n\n后续研究可零成本探索新算法、新工具、多智能体、课程持续学习等方向，无需再重复造环境。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是GEM%3A%20A%20Gym%20for%20Agentic%20LLMs，链接是https://arxiv.org/pdf/2510.01051，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.01051。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1759448804162","title":"VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators","description":"Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.","url":"https://huggingface.co/papers/2510.00406","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.162Z","metadata":{"authors":["Hengtao Li","Pengxiang Ding","Runze Suo","Yihao Wang","Zirui Ge","Dongyuan Zang","Kexian Yu","Mingyang Sun","Hongyin Zhang","Donglin Wang","Weihua Su"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.RO","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.00406.pdf","arxivUrl":"https://arxiv.org/abs/2510.00406","arxivId":"2510.00406","coolPaperUrl":"https://papers.cool/arxiv/2510.00406","published":"2025-10-01T01:33:10Z","updated":"2025-10-01T01:33:10.000Z","zh_summary":"视觉-语言-动作（VLA）模型能够实现具身决策，但高度依赖模仿学习，容易导致错误累积，并在分布变化下表现出较差的鲁棒性。强化学习（RL）可以缓解这些问题，但通常需要昂贵的真实环境交互或受到仿真到现实差异的影响。我们提出了 VLA-RFT，一种强化微调框架，它利用数据驱动的世界模型作为可控模拟器。通过真实交互数据训练，该模拟器能在动作条件下预测未来的视觉观测，从而允许策略滚动并基于达成目标的参考生成密集的轨迹级奖励。这一设计提供了高效且与动作对齐的学习信号，大大降低了样本需求。在小于 400 步微调后，VLA-RFT 超过了强大的监督学习基线，并比基于模拟器的 RL 更高效。此外，它在扰动条件下表现出较强的鲁棒性，能够维持稳定的任务执行。我们的结果表明，基于世界模型的 RFT 是一种实用的后训练范式，可增强 VLA 模型的泛化能力和鲁棒性。更多详情请参考 https://vla-rft.github.io/。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对 Vision-Language-Action（VLA）模型在模仿学习范式下暴露出的三大核心缺陷——误差累积、分布外脆弱性、以及强化学习（RL）落地的高成本——提出了一种“世界模型驱动的强化微调”框架 VLA-RFT。具体而言，论文试图解决以下关键问题：\n\n1. 模仿学习带来的复合误差\n纯行为克隆在分布偏移时会因一步偏差导致后续状态越偏越远，最终任务失败。\n\n2. 真实环境 RL 的样本与安全隐患\n在线 RL 需要百万级实机交互，既昂贵又可能损坏硬件；离线 RL 无法探索，分布偏移依旧严重。\n\n3. 仿真-到-真实（sim-to-real）鸿沟\n传统物理仿真器手工建模，视觉与动力学与真实场景差异大，迁移困难。\n\n4. 数据效率与训练速度\n现有基于仿真的 RL 方法需 10K–40K 迭代才能提升几个百分点，难以满足快速迭代需求。\n\n为此，VLA-RFT 通过“数据驱动的世界模型”充当高保真、可交互的虚拟环境，在 400 次微调迭代内即可将基线成功率从 86.6 % 提升至 91.1 %，并在扰动场景下保持更高鲁棒性，从而用极低样本成本桥接模仿学习与强化学习之间的鸿沟。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为四条主线，每条均与 VLA-RFT 的动机或技术组件直接对应：\n\n1. Vision-Language-Action（VLA）模型\n\n- RT-1、RT-2、π₀、OpenVLA、Octo、CoT-VLA、SpatialVLA、WorldVLA 等利用大规模模仿学习将预训练 VLM 对齐到机器人动作空间。\n- VLA-Adapter、Long-VLA、GR00T N1 通过轻量化 Adapter 或扩散头提升数据效率，但仍受限于行为克隆的分布偏移问题。\n2. VLA + 强化学习\n\n- 在线 RL：VLA-RL、RLDG、Improving VLA with Online RL 直接在真实机器人上采样，需万级交互且存在安全成本。\n- 离线 RL：ARFM、RWR、ReinboT、TGRPO、ConRFT 仅利用静态数据集，无法自主探索，分布外性能衰减明显。\n- 仿真 RL：RLRC、VLA-RL-Sim 依赖手工物理引擎，面临 sim-to-real 视觉与动力学差异。\n3. 世界模型（World Models）\n\n- 潜在动力学：DreamerV3、DINO-WM、RoboDreamer 在隐空间预测未来，用于策略规划，但奖励稀疏且难以提供像素级验证信号。\n- 视频生成式：iVideoGPT、Stable Video Diffusion、V-JEPA、DayDreamer 直接输出像素帧，具备高维感知一致性，为 VLA-RFT 的“可验证奖励”奠定基础。\n- 指令条件生成：WorldVLA、FlowVLA、Video Prediction Policy 将语言或目标融入未来帧预测，与 VLA 动作输出对齐。\n4. 强化微调 / 奖励设计\n\n- RLHF：InstructGPT、RLHF-VLM 将人类偏好转化为奖励，用于语言或多模态大模型。\n- 轨迹级奖励：G-RLHF、GRPO、ReinFlow 引入组相对优势估计，缓解高方差，VLA-RFT 在此基础上扩展了“世界模型 rollout + 像素-感知混合奖励”。\n\n综上，VLA-RFT 首次把“数据驱动的视频世界模型”作为可交互仿真器嵌入 VLA 的强化微调流程，兼收并蓄了 VLA 大模型、世界模型视频生成、组相对策略优化三方面的最新进展，形成低样本、高保真、可验证的 RL 微调范式。\n\nQ3: 论文如何解决这个问题？\n\n论文将问题拆解为“训练信号缺失”与“交互成本高昂”两大瓶颈，提出两阶段 pipeline：先用离线数据习得一个**高保真世界模型（WM）**充当可交互仿真器，再在该仿真器内执行**可验证奖励的强化微调（RFT）**。核心步骤如下：\n\n1. 两阶段训练范式\n\n- **Stage I：预训练**\n– 世界模型：以自回归 Transformer 学习视频-动作联合分布\n p_φ(o_(t+1)|o_t,a_t,dots,o_1,a_1) \n通过最大似然估计最小化帧级预测误差，获得像素级动力学。\n– VLA 策略：冻结 VLM 编码器，仅训练流匹配（Flow-Matching）动作头，使初始策略输出稳定、连续的动作块。\n\n- **Stage II：强化微调**\n策略在 WM 内闭环 rollout，利用“合成轨迹 vs 专家轨迹”的像素-感知差异构造**稠密、可验证奖励**，并通过 GRPO 进行端到端更新；WM 始终冻结，保证无真实交互成本。\n\n2. 可验证奖励设计\n对同一初始帧，WM 分别展开：\n\n- 策略动作序列  hatτ  生成的帧序列  hat o_(t+1),dots,hat o_(t+T) \n- 专家动作序列  τ^_  生成的帧序列  o_(t+1)^_,dots,o_(t+T)^_ \n奖励函数\n\nR = -∑_(k=1)^(T) [λ_1|hat o_(t+k)-o_(t+k)^_|_1 + λ_(lp)LPIPS(hat o_(t+k),o_(t+k)^*)]\n\n完全在生成空间内比较，避免真实图像与生成图像的域差，降低方差。\n3. 随机微分策略参数化\n将确定性流匹配扩展为 SDE：\n\na_(kδ)simmathcal N!(μ_k,Sigma_k=(σ_k^psi)^2)\n\n并在 K 步去噪路径上平均对数似然，得到可微分的策略概率  barell_(θ,psi) ，为 GRPO 提供稳定的重要性采样比。\n\n4. 组相对优势估计\n同一起始状态下采样 N 条轨迹，用组内平均奖励作 baseline：\n\nAdv_n = R_n - bar R_(group)\n\n结合裁剪目标与熵正则，总体损失\n\nmathcal L_(GRPO) = -mathbb E[clip(r,1!-!ε,1!+!ε)Adv] + λ_(mse)mathcal L_(MSE) - α H(π)\n\n兼顾策略提升与训练稳定。\n\n5. 极端数据效率\n仅 400 次迭代、每次 16 条 WM 轨迹，即可在 LIBERO 上将基线成功率从 86.6 % 提升到 91.1 %，并显著超越需 10 k–40 k 步的在线/离线 RL 方法；同时在物体位置、目标位置、机器人初始状态及组合扰动下保持 +3 %–+7 % 的绝对增益，实现“低成本交互-强泛化-高鲁棒”三重目标。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“世界模型能否充当可靠仿真器”“RFT 是否真能提高 VLA 性能”“哪些设计带来增益”三个研究问题，共设计 4 组实验、12 张表格/图示，全部在 LIBERO 基准完成。核心实验一览如下（按出现顺序归纳）：\n\n1. 世界模型作为仿真器的保真度验证\n\n- 设置：随机抽取 4 个 suite（Spatial/Object/Goal/Long）共 2 560 段真实轨迹，用 WM 逐帧 rollout，与真值对比。\n- 指标：MSE↓、PSNR↑、SSIM↑、LPIPS↓\n- 结果：平均 MSE 0.0039，PSNR 25.23 dB，SSIM 0.906，LPIPS 0.059；生成帧与真值在物体位移、遮挡、光影变化上高度一致，说明 WM 已捕获像素级动力学。\n2. 标准 LIBERO 套件性能对比\n\n- 对比对象：Base-SFT（3 w 步）、Base-SFT（15 w 步）、VLA-RFT（400 步）\n- 指标：成功率 SR（%）\n- 结果：\n– Base 15 w 平均 86.6 % → VLA-RFT 91.1 %（+4.5 pp），四 suite 全部提升，最长 Long suite 从 77.2 % → 80.2 %。\n– 训练步数减少 97 %，验证“样本效率”优势。\n3. 分布外鲁棒性（Perturbation Suites）\n\n- 扰动类型：\n– Object Position：物体初始 (x,y) 偏移 ±2.5 cm / ±5 cm\n– Goal Position：目标物体位移 ±2.5 cm / ±5 cm\n– Robot State：夹爪高度 & 水平偏移 ±20 mm / ±50 mm\n– Combination：三种扰动同时施加\n- 指标：SR(%) 与 ΔSR\n- 结果：\n– 小扰动平均提升 +2.5–+4.2 pp；大扰动提升 +3.0–+6.7 pp；组合扰动下 Base 34 % → VLA-RFT 37 %，展现失效恢复与重试能力。\n– 动作分布可视化显示 RFT 策略在 X/Z 维度覆盖更宽，解释鲁棒来源。\n4. 关键因子消融：奖励设计 & 世界模型作用\n\n- 奖励类型：\n– R1：仅动作 L1 距离（无 WM）\n– R2：WM 生成单帧 vs 真值图像（MAE+LPIPS）\n– R3：WM 生成完整轨迹 vs 专家轨迹（MAE+LPIPS），即 VLA-RFT 采用方案\n- 结果：\n– R1 +1.1 pp，R2 +0.5 pp，R3 +4.5 pp，证实“同一生成空间轨迹对比”至关重要。\n- WM 规模消融：保持 138 M 参数不变，仅改变训练步数（0.5 k→1.5 k→3 k），发现 1.5 k 步后增益饱和，验证当前规模已足够。\n5. 与主流 VLA 方法对比\n\n- 列表：Diffusion Policy、Octo、MDT、OpenVLA、SpatialVLA、WorldVLA、CoT-VLA、TraceVLA、π₀、VLA-Adapter\n- 结果：VLA-RFT 平均 SR 91.1 % 排名第一，在 Spatial/Object/Goal 三 suite 均刷新 SOTA，Long suite 仅次于 π₀（80.2 % vs 74.2 %）。\n6. 与 VLA+RL 方法对比\n\n- 在线 RL：VLA-RL（10 k 实机交互）\n- 离线 RL：ARFM、RWR、ReinboT（40 k 梯度步）\n- 结果：\n– VLA-RFT 用 400 步将基线 86.6 % → 91.1 %（+4.5 pp），增益与在线 VLA-RL 持平，但交互成本降低 25×，训练步数降低 100×。\n– 离线方法最高 +4.0 pp，仍需 40 k 步，凸显世界模型驱动的数据效率优势。\n7. 可视化与失败案例\n\n- 提供原始 vs 扰动场景对比图、Base vs RFT 策略执行序列、WM rollout 与真值帧的像素级对齐视频，定性展示恢复能力与生成质量。\n\n综上，实验从“仿真保真→标准性能→鲁棒性→消融因子→SOTA 对比”五维度系统验证：世界模型可在像素级可靠替代真实环境，VLA-RFT 以 400 步级微调实现性能与鲁棒性双提升，且增益主要来自“轨迹级可验证奖励”而非简单动作监督。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为**世界模型**、**奖励与策略**、**系统级扩展**三大层面，供后续研究参考：\n\n1\\. 世界模型层面\n\n| 方向 | 关键问题 | 可探索方法 |\n| --- | --- | --- |\n| 规模与数据多样性 | 138 M 模型已出现性能饱和，更大参数/更丰富数据能否提升 OOD 泛化？ | 继续放大至 1 B+，引入跨 embodiment、跨任务、跨环境的视频-动作对；研究 scaling law。 |\n| 长程一致性 | 当前 rollout 8–16 帧后细节漂移，导致长任务奖励失真。 | 引入时序一致性损失、扩散式视频生成、或分层 WM（高层语义、低层像素）。 |\n| 物理一致性 | 像素级损失无法保证物体质量、碰撞、遮挡合理性。 | 在 WM 中嵌入可微物理引擎或神经物理网络，显式建模接触、动力学。 |\n| 可干预/可控生成 | 现有 WM 仅条件于动作，无法接收“中间子目标”或“约束”。 | 研究 goal-conditional 或 constraint-conditional WM，支持规划器在线重设目标。 |\n\n2\\. 奖励与策略层面\n\n| 方向 | 关键问题 | 可探索方法 |\n| --- | --- | --- |\n| 超越专家轨迹 | 验证奖励仍以“像专家”为准，难以发现更优策略。 | 引入学习奖励模型（VLAC、Preference Model），用人类视频或语言反馈提供任务相关奖励。 |\n| 稀疏任务奖励 | 当前密集形状奖励对“是否达成”不敏感，导致末端精度不足。 | 结合语义分割/目标检测器输出 success classifier，提供稀疏但准确的终端奖励。 |\n| 多任务/持续学习 | 每任务需独立采样专家轨迹作参考，成本高。 | 研究 task-agnostic 奖励，即无需参考轨迹，仅用 WM 预测“达成状态”与初始状态差异。 |\n| 策略架构泛化 | 目前仅验证 Flow-Matching + SDE 头。 | 将框架迁移至 Diffusion Policy、Transformer-Policy、连续/离散混合动作空间，验证通用性。 |\n\n3\\. 系统级与落地扩展\n\n| 方向 | 关键问题 | 可探索方法 |\n| --- | --- | --- |\n| WM 与规划闭环 | 当前 WM 仅作前向仿真，未反向优化动作序列。 | 把 WM 作为可微动力学，引入 MBRL 规划器（MPC、DQN、AlphaZero）做模型预测控制。 |\n| sim-to-real 再迁移 | WM 毕竟学自真实数据，仍可能存在系统偏差。 | 采用领域随机化+在线适配（DAP、DRIM）微调 WM 或策略，量化剩余 gap。 |\n| 安全与不确定性估计 | WM 预测失误会导致策略更新方向错误。 | 让 WM 输出 epistemic uncertainty（ ensembles 或 NNs with variance head），在高不确定度区域拒绝 rollout。 |\n| 跨 embodiment 迁移 | 当前动作空间固定为 7-DoF 夹爪，无法直接迁移到双指、五指或移动底盘。 | 研究 action-space alignment 或 embodiment-agnostic tokenization，使同一 WM 服务多种机器人。 |\n| 实时推理加速 | 自回归 WM 每帧需串行 12 层 Transformer，推理延迟高。 | 采用蒸馏至并行扩散模型、VQ-VAE 低维潜空间 rollout、或 GPU/TensorRT 级优化，实现 ≥10 Hz 闭环。 |\n\n4\\. 理论分析\n\n- **样本复杂度边界**：在 POMDP 框架下给出 WM + GRPO 的收敛率与样本复杂度，明确“世界模型误差 → 价值估计误差 → 策略性能”传递系数。\n- **奖励偏差与最优性差距**：量化“验证奖励”与“真实任务奖励”之间的偏差，对最终策略次优性给出上界。\n\n综上，VLA-RFT 虽已验证“低成本仿真器 + 可验证奖励”的有效性，但在**模型规模、物理一致性、奖励超越专家、规划闭环、跨 embodiment 迁移**等方向仍留有巨大探索空间。后续工作可沿上述路线逐一突破，推动世界模型成为 VLA 后训练的通用基础设施。\n\nQ6: 总结一下论文的主要内容\n\n**VLA-RFT: Vision-Language-Action Reinforcement Fine-Tuning with Verified Rewards in World Simulators**\n一句话总结：用**数据驱动的视频世界模型**当仿真器，仅 400 步强化微调就把 VLA 的成功率从 86.6 % 提到 91.1 %，同时大幅提升分布外鲁棒性，**零实机交互、零手工建模**。\n\n1\\. 要解决的问题\n\n- 纯模仿学习 → 一步偏差滚雪球，分布外失效\n- 在线 RL → 百万级实机交互，昂贵且危险\n- 传统仿真 → sim-to-real 视觉/动力学鸿沟\n- 离线 RL → 无法探索，分布偏移依旧\n\n2\\. 核心思路\n\n**两阶段 pipeline**\n\n| Stage | 目标 | 做法 |\n| --- | --- | --- |\n| I 预训练 | 得到可靠初始化 | ① 自回归 Transformer 在离线视频-动作对上训练世界模型 $p_φ(o_(t+1) |\n| II 强化微调 | 低成本提升性能与鲁棒性 | ① VLA 生成  N  组动作块 → WM 闭环 rollout 得合成视频轨迹；② 与专家轨迹在同一生成空间比较，计算像素(L1)+感知(LPIPS) 奖励；③ 用组相对策略优化(GRPO) 更新策略，辅以熵正则与流匹配 MSE，全程零实机交互。 |\n\n3\\. 关键公式\n\n- 世界模型自回归生成\n\nhat o_(i+t+1)=g_φ(o_(i:i+t),a_(i:i+t))\n\n- 可验证轨迹奖励\n\nR=-∑_(t=0)^(T-1)[λ_1|hat o_(i+t+1)-o_(i+t+1)^_|_1+λ_(lp)LPIPS(hat o_(i+t+1),o_(i+t+1)^_)]\n\n- SDE 策略参数化\n\na_(kδ)simmathcal N!(μ_k,(σ_k^psi)^2),quad r=exp(barell_(θ,psi)-barell_(old))\n\n- GRPO 目标\n\nmathcal L_(GRPO)=-mathbb E[clip(r,1!-!ε,1!+!ε)Adv]+λ_(mse)mathcal L_(MSE)-α H(π)\n\n4\\. 实验结果（LIBERO）\n\n| 设置 | Base (15k 步) | VLA-RFT (400 步) | 提升 |\n| --- | --- | --- | --- |\n| 标准四套件平均 | 86.6 % | 91.1 % | +4.5 pp |\n| 大扰动（±5 cm） | 34.0 %–48.0 % | 37.0 %–52.5 % | +3–+7 pp |\n| 训练耗时 | 15k 步 → 400 步（97 % 削减） |\n\n- 世界模型保真：MSE 0.0039，PSNR 25.2 dB，SSIM 0.906，LPIPS 0.059\n- 消融：无 WM 仅 +1.1 pp；同空间轨迹奖励 +4.5 pp，验证**WM 是增益核心**\n- 对比 SOTA：超越 π₀、OpenVLA、Diffusion Policy 等，**400 步达到新 SOTA**\n\n5\\. 贡献与意义\n\n1. 首次把**数据驱动视频世界模型**当可交互仿真器，实现**零实机成本**的 VLA 强化微调。\n2. 提出**同空间轨迹级验证奖励**，解决 sim-to-real 视觉差异与奖励稀疏难题。\n3. 用\\*\\*<400 步\\*\\*将基线提升 4.5 pp，扰动场景再涨 3–7 pp，**样本效率比现有 RL 高两个数量级**。\n4. 开源框架兼容任意 VLA 骨干，为“后训练”阶段提供**通用、高效、安全**的新范式。\n\n6\\. 局限与未来\n\n- 奖励仍依赖专家轨迹，难以发现**超越人类**策略\n- WM 规模与物理一致性待放大、显式物理注入\n- 可引入**学习奖励模型**、**不确定性估计**、**MPC 规划闭环**、**跨 embodiment 迁移**等方向继续突破。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是VLA-RFT%3A%20Vision-Language-Action%20Reinforcement%20Fine-tuning%20with%20Verified%20Rewards%20in%20World%20Simulators，链接是https://arxiv.org/pdf/2510.00406，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.00406。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1759448804162","title":"Knapsack RL: Unlocking Exploration of LLMs via Optimizing Budget Allocation","description":"Large Language Models (LLMs) can self-improve through reinforcement learning, where they generate trajectories to explore and discover better solutions. However, this exploration process is computationally expensive, often forcing current methods to assign limited exploration budgets to each task. This uniform allocation creates problematic edge cases: easy tasks consistently succeed while difficult tasks consistently fail, both producing zero gradients during training updates for the widely used Group Relative Policy Optimization (GRPO). We address this problem from the lens of exploration budget allocation. Viewing each task's exploration as an \"item\" with a distinct \"value\" and \"cost\", we establish a connection to the classical knapsack problem. This formulation allows us to derive an optimal assignment rule that adaptively distributes resources based on the model's current learning status. When applied to GRPO, our method increases the effective ratio of non-zero policy gradients by 20-40% during training. Acting as a computational \"free lunch\", our approach could reallocate exploration budgets from tasks where learning is saturated to those where it is most impactful. This enables significantly larger budgets (e.g., 93 rollouts) for especially challenging problems, which would be computationally prohibitive under a uniform allocation. These improvements translate to meaningful gains on mathematical reasoning benchmarks, with average improvements of 2-4 points and peak gains of 9 points on specific tasks. Notably, achieving comparable performance with traditional homogeneous allocation would require about 2x the computational resources.","url":"https://huggingface.co/papers/2509.25849","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.162Z","metadata":{"authors":["Ziniu Li","Congliang Chen","Tianyun Yang","Tian Ding","Ruoyu Sun","Ge Zhang","Wenhao Huang","Zhi-Quan Luo"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.25849.pdf","arxivUrl":"https://arxiv.org/abs/2509.25849","arxivId":"2509.25849","coolPaperUrl":"https://papers.cool/arxiv/2509.25849","published":"2025-09-30T06:41:57Z","updated":"2025-09-30T06:41:57.000Z","zh_summary":"大型语言模型（LLMs）可以通过强化学习自我提升，在此过程中它们生成轨迹以探索并发现更优的解决方案。然而，这一探索过程计算成本高昂，通常迫使现有方法为每个任务分配有限的探索预算。这种统一分配会产生问题边缘情况：简单任务持续成功，而困难任务持续失败，在广泛使用的组相对策略优化（Group Relative Policy Optimization, GRPO）训练更新过程中，两者都产生零梯度。我们从探索预算分配的角度解决这一问题。将每个任务的探索视为具有不同“价值”和“成本”的“物品”，我们建立了与经典背包问题的联系。这一公式化方法使我们能够推导出一种最优分配规则，根据模型当前的学习状态自适应地分配资源。应用于GRPO时，我们的方法在训练期间将非零策略梯度的有效比例提高了20-40%。作为一种计算上的“免费午餐”，我们的方法可以将探索预算从已学习饱和的任务重新分配到最能产生影响的任务。这使得对特别具有挑战性的问题分配显著更大的预算（例如，93次rollout）成为可能，而在统一分配下这是计算上不可承受的。这些改进在数学推理基准上带来了实质性的收益，平均提升2-4分，特定任务上峰值提升9分。值得注意的是，要使用传统的同质分配实现相当的性能，约需2倍的计算资源。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对大语言模型（LLM）在强化学习（RL）后训练阶段普遍存在的“探索预算分配低效”问题，提出以经典背包问题（Knapsack）为框架的解决方案。核心痛点与贡献可归纳如下：\n\n1. 问题诊断\n\n- 在基于可验证奖励的 RL（如 GRPO）中，只有当同一 prompt 的采样结果同时包含成功与失败样本时，才会产生非零策略梯度。\n- 现有做法对所有 prompt 采用**均一采样预算**（如每题 8 条 rollout），导致：\n– 简单题几乎全对，产生“全正”零梯度；\n– 难题几乎全错，产生“全负”零梯度。\n- 结果：超过 40% 的采样被浪费，有效梯度比例随训练阶段下降，学习停滞。\n2. 理论刻画\n\n- 将 prompt 的成功概率  p_i  建模为伯努利变量，推导出产生非零梯度所需的最小采样量：\n- 期望意义上：$E\nN_(first)\n= (1) / (p_i) + (1) / (1-p_i) - 1$；\n- 高概率意义上： N gtrsim (ln(1-α)) / (lnmaxp_i,1-p_i) 。\n- 证明均一预算 8 次采样仅能覆盖 $p_i ∈\n0.1,0.9\n$ 的任务，对极难或极易题严重不足。\n3. 方法：Knapsack-RL\n\n- 把“为每个 prompt 分配多少条 rollout”建模为 0-1 背包问题：\n- 物品：prompt–预算二元组  (i, N_i) ；\n- 重量： N_i （GPU 算力消耗）；\n- 价值：$Value(N_i,p_i)=l\n1-p_i^(N_i)-(1-p_i)^(N_i)r\n· p_i(1-p_i)^2$；\n- 容量：总预算  N_(total)  固定。\n- 每轮动态求解背包，获得异构预算分配：难题可获 90+ 条 rollout，易题仅 2–4 条，无需额外算力。\n4. 实验效果\n\n- 在 Qwen 1B–7B 系列、DAPO-Math-17K 数据集、1000 次迭代训练下：\n- 有效梯度比例提升 20–40%；\n- 数学推理基准（AIME、AMC、MATH 等）平均提升 2–4 分，单任务最高 +9 分；\n- 与均一预算相比，达到同等性能约节省 50% GPU 小时，实现“免费午餐”式增益。\n\n简言之，论文首次从“跨任务探索预算最优分配”视角，将 RL 训练中的采样效率问题形式化为背包优化，显著提升了 LLM 在数学推理等可验证奖励场景下的后训练效果。\n\nQ2: 有哪些相关研究？\n\n论文在第 6 节“Related Work”中系统梳理了四条紧密相关的研究脉络，并明确区分了自身与它们的本质差异。以下按主题归纳，并补充若干代表性文献（括号内给出原文或经典出处，便于快速定位）。\n\n1\\. 数据异质性与梯度稀疏\n\n- **核心矛盾**：多任务 prompt 难度差异大 → 奖励分布非平稳 → 梯度估计方差高。\n- **已有解法**：\n- 改进 baseline 或优势估计以降低方差，如 Remax（Li et al. 2024）、RLOO（Ahmadian et al. 2024）。\n- 光谱策略优化（Spectral-PO, Chen et al. 2025）对 GRPO 的“全正/全负”零梯度现象做后验校正。\n- **区别**：上述工作聚焦**利用阶段**（exploitation）的梯度方差，而 Knapsack-RL 直接重塑**探索阶段**（exploration）的采样预算，从源头减少零梯度产生。\n\n2\\. Prompt 选择与课程学习\n\n- **典型方法**：\n- Self-Evolving Curriculum（Chen et al. 2025）用模型自身优势估计动态排序难度。\n- Prepo!（Sun et al. 2025）以困惑度为难度指标，先做简单题再做难题。\n- DAPO 的 Dynamic Sampling（Yu et al. 2025）在每轮只保留“正负样本共存”的 prompt，过滤极端题。\n- **区别**：这类方法决定“**哪些题值得练**”，仍保持每题**同质预算**；Knapsack-RL 决定“**每题练多少次**”，对**全部 prompt**做异构预算分配，可与它们正交叠加。\n\n3\\. 资源分配与在线背包 RL\n\n- **传统 OR/系统方向**：\n- 作业调度、GPU 集群任务放置（Hussain et al. 2013；Katoh et al. 2013）。\n- **在线背包约束 RL**：\n- Brantley et al. 2020 研究单任务 episodic RL 在总成本上限下的 regret 界。\n- **区别**：这些工作面向**单任务**或**系统层调度**，Knapsack-RL 首次把**跨多 prompt 的探索采样**显式建模为**中央化背包优化**，直接服务 LLM 后训练。\n\n4\\. 推理阶段计算缩放（Test-time Scaling）\n\n- **相关研究**：\n- “Large Language Monkeys”（Brown et al. 2024）通过 best-of-N 或 majority voting 在推理时投更多样本换准确率。\n- “Every Rollout Counts”（Wang et al. 2025）在 rejection-sampling 阶段优化样本计算量。\n- **区别**：它们把额外计算花在**推理/拒采样**；Knapsack-RL 把计算花在**训练期探索**，以产生更高质量的学习信号。\n\n5\\. 同期或后续高度相关但未在原文列出的研究\n\n- **Tree-RL / TreePO**（Hou et al. 2025；Li et al. 2025）\n用蒙特卡洛树搜索做状态回滚，提升探索深度；与 Knapsack-RL 的“广度预算”互补，二者结合被作者列为未来工作。\n- **ProRL**（Liu et al. 2025）\n提出“延长训练时间 + 大回放池”来扩大 RL 规模，同样关注训练期计算投入，但未显式优化单轮采样分配。\n\n一句话总结\n\nKnapsack-RL 与现有文献的最大分野在于：\n\n> **不挑 prompt，而挑“每题采多少样本”**——用背包优化把固定算力从“饱和题”挪到“高价值题”，在零额外成本下显著增加有效梯度，从而提升 LLM 后训练效率。\n\nQ3: 论文如何解决这个问题？\n\n论文将“如何为不同难度的 prompt 分配探索 rollout”转化为一个**带预算约束的组合优化问题**，并给出**可即插即用**的完整 pipeline。核心步骤与关键技术如下（按执行顺序拆解）：\n\n1\\. 诊断：零梯度根源于“均匀预算”\n\n- 对 GRPO 建立 Bernoulli 模型，推导\n\nP(g_i ≠ 0)=1-p_i^N-(1-p_i)^N\n\n证明当  p_ito 0  或  p_ito 1  时，**固定小 N** 必使概率趋于 0 → 浪费算力。\n\n2\\. 建模：把“采样分配”变成 0-1 背包\n\n**变量**\n\n- $N_i∈\nN_(low),N_(up)\n ：分配给 prompt i$ 的 rollout 数（整数）\n\n**目标**\n\n- 最大化总学习价值\n\nmax_(N_i)∑_(i=1)^M [1-p_i^(N_i)-(1-p_i)^(N_i)]_(ProbNonZeroGradient)· p_i(1-p_i)^2_(InfoGain)\n\n**约束**\n\n- 总预算固定： ∑_i N_i = N_(total) （与均匀策略持平，不额外花钱）\n- 上下界： 2le N_ile 128 （防退化、防爆炸）\n\n**等价性**\n\n- 每个 prompt–预算二元组  (i,N_i)  视为一件“物品”，重量= N_i ，价值=上式；背包容量= N_(total) 。\n- 物品规模  M×(N_(up)-N_(low)) ，可用**动态规划**在秒级求解（Numba 加速）。\n\n3\\. 在线估计：用上一轮经验近似  p_i \n\n- 首轮均匀采 8 样 → 记录每题 success 频率  hat p_i 。\n- 后续每轮用  hat p_i  代入价值函数，重新解背包； hat p_i  随训练滚动更新，延迟 1-epoch，噪声可容忍。\n\n4\\. 极端值处理与负载均衡\n\n- ** hat p_i=1 **：已稳解题，仍强制给  N_(low)=2  样例，防止评估偏差。\n- ** hat p_i=0 **：从未成功题，先按背包分给“可解题”后，**剩余预算全部 fallback 给这些难题**，避免被饿死。\n- **GPU 负载不均**：把高预算 prompt 的 rollout 拆成**单条作业**，随机分发给 vLLM worker；若 prompt 很长，再用 Karmarkar–Karp 做二次装箱，保证显存/时延平衡。\n\n5\\. 算法流程（伪代码级）\n\n```python\n每轮迭代\nprev_success = get_success_counts(batch) # 上一轮统计\n\np_hat = prev_success / prev_rollouts\n\nbudget = solve_knapsack(p_hat, N_total, N_low=2, N_up=128)\njobs = []\nfor idx, b in enumerate(budget):\njobs.extend([idx]*b) # 一条 rollout 一个作业\n\njobs = random_shuffle(jobs) # 负载均衡\n\nresponses = vllm_generate(jobs) # 批量推理\n\nbatch = compute_reward_advantage(responses)\nactor.update(batch) # 标准 GRPO 梯度步\n\n```\n\n6\\. 效果：零额外成本换 20–40% 有效梯度\n\n- 在 Qwen2.5-Math-7B 1 000 迭代实验里，背包策略把**单题预算动态拉到 93 条 rollout**，而总预算仍 =2048。\n- 有效梯度比例从 ~55% 提到 ~75%，下游平均成绩 +2–4 分，最高 +9 分；**用均匀策略想达到同性能需 2×GPU**。\n\n一句话总结\n\n论文**不改模型、不增算力**，仅靠“每轮解一次背包”重新排布采样配额，就把即将被浪费的 rollout 从易题挪到难题，**直接增加非零梯度事件**，从而免费吃到 LLM 后训练性能提升。\n\nQ4: 论文做了哪些实验？\n\n论文围绕「Knapsack-RL 是否能在真实大模型、真实数学数据集上带来免费且显著的性能提升」这一核心问题，设计了**多模型、多预算、多维度**的实验矩阵。主要实验与结果可归纳为以下 5 类：\n\n1\\. 主实验：4 类模型 × 6 个基准 → 一致提升\n\n| 模型 | 参数量 | 训练数据 | 评估基准（avg@16） |\n| --- | --- | --- | --- |\n| DPSK-R1-Distill-1.5B | 1.5 B | DAPO-Math-17K | AIME24+25、AMC、MATH、MINERVA、OLYMP、GPQA |\n| Qwen3-4B-Base | 4 B | 同上 | 同上 |\n| Qwen3-4B-Instruct | 4 B | 同上 | 同上 |\n| Qwen2.5-Math-7B | 7 B | 同上 | 同上 |\n\n- 每模型对比 **Vanilla GRPO** vs **Knapsack-GRPO**（仅替换预算分配，其余超参完全一致）。\n- 训练 1 000 iteration，单卡 A100 下 7 B 模型约 1 400 GPU h。\n- **结果**：平均成绩提升 **2–4 分**，单任务最高 **+9.1**（Qwen3-4B-Base 在 AMC）；全部 24 组「模型-基准」对中 **22 组取得正向增益**，仅 2 组微降 ≤0.5 分。\n\n2\\. 梯度有效性分析：20–40% 有效梯度增量\n\n- 定义指标 **effective-gradient-ratio**（公式 4）。\n- 在 3 个模型（1.5 B/4 B/7 B）训练全程记录：\n- Vanilla GRPO 曲线从 55% 降至 20%（后期易题全对、难题全错）。\n- Knapsack-GRPO 始终维持 **70–80%**；相对提升 **+20% abs**（约 **+40% rel**）。\n- 可视化预算分布：单轮最多给**同一难题分配 93 条 rollout**，而均匀策略固定 8 条。\n\n3\\. 任务难度迁移热力图：更多「极难题」被攻克\n\n- 按 success-rate 把 17 K prompt 动态分为 5 档（extremely-hard → extremely-easy）。\n- 统计训练前后状态转移矩阵：\n- **extremely-hard 自我吸收率** 47.1%→43.4%（降低 3.7 pt）。\n- **extremely-easy 最终占比** 8 676→9 274（+598 题）。\n- 证明背包策略确实把算力从已饱和题**挤出**并**注入**未解题，提升数据效率。\n\n4\\. 总预算消融：低资源场景优势更大\n\n- 固定模型（Qwen2.5-Math-7B），仅改变**全局预算** N\\_total：\n- 1024（≈ N=4）\n- 2048（≈ N=8，主实验）\n- 4096（≈ N=16）\n- 结果（平均分数）：\n\n| 预算 | GRPO | Knapsack-GRPO | Δ |\n| --- | --- | --- | --- |\n| 1024 | 39.8 | 45.5 | +5.7 |\n| 2048 | 45.2 | 47.5 | +2.3 |\n| 4096 | 47.8 | 49.5 | +1.7 |\n\n- **结论**：资源越紧张，背包优化越能「拆东墙补西墙」，低预算场景提升 **>5 分**；高预算时仍有 +1–2 分稳定收益。\n\n5\\. 与 DAPO「动态采样」正交实验\n\n- **动态采样**（DAPO, Yu et al.）= 只保留「正负样本共存」的 prompt 进入训练批次（同质预算）。\n- 设计 4 组对比：\n\n1. GRPO\n2. GRPO + 动态采样\n3. Knapsack-GRPO\n4. Knapsack-GRPO + 动态采样\n- **按「探索迭代」计**（总生成次数相同）：\n- 2 比 1 平均 +1.0 分；4 比 2 再 +0.3 分 → 二者**可叠加**，但主要增益来自背包。\n- **按「梯度更新」计**（同等更新步数）：\n- 3、4 的样本效率显著高于 1、2，验证「有效梯度越多，每步提升越大」。\n\n附加消融与鲁棒性\n\n- **fallback 策略**：若去掉「零成功题强制再分配」机制，AMC 成绩掉 3.2 分，训练曲线震荡。\n- **N\\_low / N\\_up** 边界：N\\_low=0 时极端题被饿死；N\\_up=32 仍优于均匀基线，说明上限 128 仅为加速 DP 而设。\n- **长 prompt 场景**：用 Karmarkar–Karp 二次装箱，GPU 空闲率从 18% 降至 4%，吞吐几乎无损。\n\n一句话总结\n\n实验从「最终精度-梯度效率-资源弹性-组件消融」四轴共同验证：**在零额外算力前提下，Knapsack-RL 能稳定、显著且可叠加地提升 LLM 数学推理后训练效果**。\n\nQ5: 有什么可以进一步探索的点？\n\n论文在第 7 节已列出三条未来方向，结合实验结果与近期社区进展，可进一步拓展出以下**七条高价值探索路径**（按“立即可做”→“中长期”排序）：\n\n1\\. rollout 数 → 更细粒度“计算成本”\n\n- 现状仅把“采样条数”当重量，实际 GPU 时间还受**输出长度、decoding 步数、batch 显存碎片**影响。\n- 可做：\n– 把重量改为**预期总 token 数**或**wall-clock 秒**，用轻量级预测器在线估计。\n– 背包目标改为“**每美元梯度期望**”，实现真·成本最优。\n\n2\\. 价值函数再建模：二阶或模型化\n\n- 现用一阶 Taylor  p_i(1-p_i)^2  近似信息增益，**高估高方差、低估低方差**区域。\n- 可做：\n– 用**验证集一步外推**（lookahead）直接测量  Delta p_i ，拟合神经网络价值模型，在线蒸馏。\n– 引入**不确定性量化**（EIG、BALD）把“可学习性”与“可纠正性”显式拆开。\n\n3\\. 与 Tree-RL / MCTS 探索复合\n\n- 背包只解决“**广度预算**”，Tree-RL 解决“**深度回溯**”。\n- 可做：\n– 节点展开次数受背包重量约束，**树节点级预算分配**再套一层轻量级背包。\n– 在“极难题”上自动切换 rollout→tree，实现**广-深自适应混合探索**。\n\n4\\. 多轮交互/代理任务扩展\n\n- 数学任务一次生成即得奖励；**代理场景**（coding、embodied）需多轮交互，预算=**轮数×每轮 rollout**。\n- 可做：\n– 把“轮数”与“每轮条数”联合决策，变成**二维背包**或**分层 MDP**。\n– 用**options framework**把“继续交互”vs“停止收集”建模为额外动作，学习何时提前终止。\n\n5\\. 在线参数化预算：与 Meta-RL 接轨\n\n- 当前每轮重解背包仍属“**启发式调度**”，可让模型**自己学会预算策略**。\n- 可做：\n– 把  (p_i, history)  作为状态，**输出连续预算**或**停止信号**，用 meta-gradient 或 RL² 训练。\n– 奖励=下一迭代验证集提升，实现“**学会探索**”而非“**用固定公式探索**”。\n\n6\\. 跨样本&跨任务重用：经验回放 + 终身背包\n\n- 实验发现 20% 极难题**在 1 000 迭代内仍未解决**，但已有**部分成功轨迹**。\n- 可做：\n– 建立**优先回放池**，价值函数除  p_i  外再加“**上次成功距今步数**”，防止遗忘。\n– 多 epoch 训练时把**总预算视为终身资源**，引入**滑动窗口+折旧因子**，实现**终身背包调度**。\n\n7\\. 系统-算法协同：knapsack-in-the-loop 调度器\n\n- 大集群训练常同时跑多组实验（sweep），全局 GPU 池是**动态共享**。\n- 可做：\n– 把集群级**剩余 GPU-hour** 实时注入背包容量  N_(total) ，实现**实验间弹性预算**。\n– 与 vLLM 的**paged-attention**联动，把**显存碎片率**作为额外约束，**算法-系统联合最优**。\n\n小结（一句话 takeaway）\n\n> **“把重量做得更准、把价值估得更真、把广度与深度联合、让模型自己学会预算、再把集群剩余资源拉进来在线调节”**——这五步可望把 Knapsack-RL 从“免费午餐”升级为“永久自助餐厅”。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：LLM 强化学习后训练中，均匀 rollout 预算导致易题全对、难题全错，GRPO 出现大量零梯度，计算浪费。\n- **视角**：将「为每个 prompt 采样多少条轨迹」视为背包问题——物品=任务-预算组合，重量=轨迹数，价值=期望非零梯度×信息增益，容量=固定总预算。\n- **方法**：在线估计 success rate → 每轮解一次 0-1 背包 → 异构分配 rollout；极难题自动获得 90+ 样本，易题仅 2–4 条，无需额外算力。\n- **结果**：Qwen 1B–7B 在 DAPO-Math-17K 上训练 1 000 迭代，有效梯度比例提升 20–40%，数学基准平均 +2–4 分、最高 +9 分；同等性能下节省约 50% GPU 时间。\n- **意义**：首次把「跨任务探索预算最优分配」形式化为背包优化，为 LLM-RL 提供“免费午餐”式计算重分配框架，可与树探索、课程学习等正交叠加。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Knapsack%20RL%3A%20Unlocking%20Exploration%20of%20LLMs%20via%20Optimizing%20Budget%20Allocation，链接是https://arxiv.org/pdf/2509.25849，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25849。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-4-1759448804163","title":"PIPer: On-Device Environment Setup via Online Reinforcement Learning","description":"Environment setup-the process of configuring the system to work with a specific software project-represents a persistent challenge in Software Engineering (SE). Automated environment setup methods could assist developers by providing fully configured environments for arbitrary repositories without manual effort. This also helps SE researchers to scale execution-based benchmarks. However, recent studies reveal that even state-of-the-art Large Language Models (LLMs) achieve limited success in automating this task. To address this limitation, we tune a specialized model for environment setup. We combine supervised fine-tuning for generating correct Bash scripts and Reinforcement Learning with Verifiable Rewards (RLVR) to adapt it to the task of environment setup. On EnvBench-Python, our method enables Qwen3-8B (a model runnable on consumer hardware) to perform on par with larger models-Qwen3-32B and GPT-4o. The training code and model checkpoints are available online: https://github.com/JetBrains-Research/PIPer.","url":"https://huggingface.co/papers/2509.25455","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.163Z","metadata":{"authors":["Alexander Kovrigin","Aleksandra Eliseeva","Konstantin Grotov","Egor Bogomolov","Yaroslav Zharov"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.SE","cs.AI","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.25455.pdf","arxivUrl":"https://arxiv.org/abs/2509.25455","arxivId":"2509.25455","coolPaperUrl":"https://papers.cool/arxiv/2509.25455","published":"2025-09-29T20:03:05Z","updated":"2025-09-29T20:03:05.000Z","zh_summary":"环境设置——将系统配置为可与特定软件项目配合使用的过程——在软件工程（SE）中一直是一个持续的挑战。自动化环境设置方法可以通过为任意代码库提供完全配置好的环境来帮助开发者，而无需手动操作。这也有助于软件工程研究人员扩展基于执行的基准测试。然而，最近的研究表明，即使是最先进的大型语言模型（LLMs）在自动化完成此任务方面也仅取得有限的成功。为了解决这一限制，我们对环境设置进行了专门的模型调优。我们结合了生成正确 Bash 脚本的监督微调和可验证奖励（RLVR）的强化学习，以使其适应环境设置任务。在 EnvBench-Python 上，我们的方法使 Qwen3-8B（一款可在消费者硬件上运行的模型）表现可与更大模型 Qwen3-32B 和 GPT-4o 相媲美。训练代码和模型检查点可在以下网址获取：https://github.com/JetBrains-Research/PIPer。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“环境配置（environment setup）”这一软件工程中的顽固难题：\n如何在不依赖人工的前提下，为任意 GitHub 仓库自动生成可一次性运行成功的 Bash 配置脚本，使得代码库中的依赖能够正确安装、导入无报错，从而\n\n1. 降低开发者手动配置负担；\n2. 让基于执行的 SE 基准（如 SWE-bench）能够低成本、可扩展地构建与复现。\n\n现有最强 LLM 在该任务上成功率极低（EnvBench 上 GPT-4o 仅 6.69%），因此作者提出用\\*\\*在线强化学习+可验证奖励（RLVR）\\*\\*对小型开源模型 Qwen3-8B 进行专门训练，最终在消费级硬件上达到与 4× 规模大模型及闭源模型相当的性能。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三条主线，均围绕“让 LLM 自动完成环境配置”展开：\n\n1. 环境配置基准与数据\n\n- EnvBench（Eliseeva et al., 2025）——329 个高难度 Python 仓库，用 Pyright 静态检查判定成功。\n- Repo2Run（Hu et al., 2025b）——420 个仓库，用 pytest 采集测试是否通过判定成功。\n- SetupBench、Terminal-Bench 等扩展了多语言、多轮终端交互场景。\n2. 自动化配置方法\n\n- 零样本/单轮脚本生成（Badertdinov et al., 2025；Eliseeva et al., 2025）——直接让 LLM 一次性输出 bash 脚本。\n- 多轮 Agent 工作流（Milliken et al., 2025；Bouzenia & Pradel, 2025；Zhang et al., 2025）——允许模型在容器里迭代执行命令、观察反馈。\n- 以上工作均以通用 LLM 为骨干，未做任务特化训练，成功率有限（SWE-Rebench 仅 31%，EnvBench 最佳 6.69%）。\n3. 训练策略与验证信号\n\n- 强化学习+可验证奖励（RLVR）在数学、代码生成中的成功：Reinforce++、GRPO、VAPO 等算法（Luo et al., 2025；Golubev et al., 2025）。\n- 无人工标签、仅用规则或 LLM-as-a-Judge 提供奖励的信号源（Zhao et al., 2025；Zhou et al., 2025）。\n- 蒸馏+RL 两阶段训练在代码任务上的协同效应（Liu et al., 2025b；Yoshihara et al., 2025）。\n\n本文首次将“在线 RLVR + 轻量级可验证奖励”引入环境配置任务，并在消费级 8B 模型上实现与 32B/闭源模型相当的性能。\n\nQ3: 论文如何解决这个问题？\n\n论文采用“两阶段、无人工标签”的 pipeline，把环境配置问题转化为**可验证奖励的在线强化学习任务**，核心步骤如下：\n\n1. 数据获取：蒸馏而非人工标注\n\n- 用更大的 Qwen3-32B 在 EnvBench 228 个训练仓库上零样本 rollout，收集“脚本能跑通（exit 0）且 Pyright 无 import 错误”的 2500 条成功轨迹。\n- 以此作为“伪真值”，对小模型 Qwen3-8B 做监督微调（SFT），得到会写“可执行脚本”的初始化策略。\n2. 奖励设计：轻量级、可验证、无需容器\n\n- 用 GPT-4.1 充当“LLM-as-a-Judge”，输入脚本+仓库上下文，预测\n– 脚本退出码  c ∈ 0,1 \n– Pyright 剩余 import 错误数  n \n- 奖励函数\n\nR_(LLM)(s)= -1.0, & if s 为空 0.0, & if c≠ 0[2pt] max!l(1.0-hatn100,,0r), & otherwise\n\n值域 \n-1,1\n，完全可验证、无需真实执行，训练时可 4×H200 GPU 并行 64 条轨迹。\n3. 在线强化学习：REINFORCE++ 微调\n\n- 在 228 个训练仓库上持续采样→得奖励→更新策略，共 45 步。\n- 训练集与 96 仓库验证集奖励同步上升，无显著过拟合。\n4. 推理阶段：零样本、单轮、本地可跑\n\n- 沿用与训练一致的 prompt，模型一次输出 bash 脚本，Docker 内直接执行即可。\n- 8B 体量可在消费级 GPU 甚至 CPU 运行，成本仅为 GPT-4o 的 1/14。\n\n通过“SFT 学会写脚本 → RLVR 学会写‘能过检查’的脚本”，EnvBench 成功率从 2.6% 提升到 27%，与 GPT-4o/Qwen3-32B 持平；在 Repo2Run 上进一步以 103/420 超越所有基线。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“能否低成本、可复现地训练出擅长环境配置的 8B 小模型”展开，分四类共 12 组结果：\n\n1. 训练过程诊断\n\n- 曲线：RLVR 在 base 与 SFT 起点上的平均奖励随步数变化（Figure 2）。\n- 结论：SFT 先让奖励从≈0 提升到 0.7，RL 再将其推至 0.85；验证集无过拟合。\n2. 主基准 EnvBench-Python（329 仓库）\n指标：pass@5、#Success、#Failed、avgFixRate\n\n- 全量测试（Table 2）：PIPER 27 仓库通过，与 GPT-4o（29）、Qwen3-32B（29）持平，8× 于 base。\n- 消融（Table 2 下半）：仅 SFT 25→13.0，仅 RL 25→11.8，两阶段组合最优。\n- 成本-性能（Figure 3b）：同样 avg@1 性能下，PIPER 推理成本 ≤1/14 GPT-4o。\n- 多采样 scaling（Figure 3a）：pass@3 已超 GPT-4o pass@2，pass@5 超 GPT-5 pass@1。\n3. 跨域泛化\n\n- Repo2Run（420 仓库，pytest 采集判定）\n– pass@5：PIPER 103 > Qwen3-32B 71 > GPT-4o-mini 84（Table 1）。\n- Terminal-Bench（80 任务，多轮 agentic）\n– pass@10：PIPER 4，RL-only 9，SFT-only 2，base 8；证实 SFT 对多轮场景有负迁移，RL 更鲁棒。\n4. 记忆化检验\n\n- 训练/验证拆分：96 仓库完全不出现在训练轨迹中。\n- 验证集结果（Table 6）：PIPER pass@5 达 6，与 Qwen3-32B 持平，显著优于 base，排除简单记忆。\n\n所有实验均用同一开源评估框架，Docker、Pyright、pytest 判定脚本成败；代码、模型权重与原始日志已公开。\n\nQ5: 有什么可以进一步探索的点？\n\n可进一步探索的方向按“数据-算法-系统-评测”四维度归纳如下：\n\n- **数据与任务外延**\n- 多语言：将 RLVR 框架迁移至 Java/Node/C++ 等生态，验证奖励设计是否依旧“轻量且可验证”。\n- 私有仓库 & 企业内网：研究如何在不泄露源码的前提下，用脱敏上下文或向量记忆完成配置。\n- 增量/演进式配置：仓库依赖随时间变化，引入“增量 diff 奖励”建模升级路径，而非一次性脚本。\n- **算法与模型**\n- 多轮交互：把现有多轮 agent 工作流（Terminal-Bench 类）直接作为环境，用 RL 训练“观察-执行”策略，而非单轮脚本。\n- 算法对比：用 GRPO、VAPO、DAPO 等最新 RL 算法替换 REINFORCE++，观察样本效率与峰值性能。\n- 模型规模：在 3B/1B 甚至 0.5B 上重复蒸馏+RLVR，绘制“参数-性能-成本”帕累托前沿，实现真正的边缘端部署。\n- 思考模式：对比 Qwen3“thinking vs non-thinking”模式，量化长思维链对环境配置是否有帮助。\n- **奖励与反馈**\n- 混合奖励：LLM-as-a-Judge 与真实容器执行按置信度动态加权，逐步从“无执行”过渡到“高保真执行”。\n- 细粒度奖励：把奖励拆成“语法正确→依赖解析成功→ import 无错→测试通过”多级信号，用课程式 RL 逐步优化。\n- 安全奖励：引入“不可变容器 diff”作为额外正则项，防止模型生成破坏宿主机或泄露凭证的脚本。\n- **系统与工程**\n- 并行仿真：用轻量 QEMU/Firecracker 微虚拟机替代 Docker，降低 512 并发训练时的隔离开销，实现秒级启动。\n- 缓存复用：对相同基础镜像层、相同依赖包做 nix-style 哈希复用，减少重复下载与安装时间，提升 RL 采样吞吐。\n- 在线蒸馏：RL 训练同时把最新策略的“高奖励脚本”实时回收到 SFT 数据池，形成“自增强”循环，减少对外部大模型依赖。\n- **评测与可解释性**\n- 对抗性仓库：构造含恶意 setup.py、循环依赖、硬件限制等极端案例，测试模型鲁棒性与安全底线。\n- 可解释生成：要求模型在脚本中加入逐行注释与回滚命令，便于开发者审计；用 RL 奖励同时度量“可读性”指标。\n- 人机协同：开放“人机混合”赛道——人类给出高层指示（如“用 poetry 且支持 GPU”），模型生成初版脚本后再由人编辑，量化编辑成本与成功率，评估真实落地价值。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：环境配置（为任意 GitHub Python 仓库生成一次性可运行脚本）仍依赖人工，SOTA LLM 在 EnvBench 成功率仅 6.69%。\n- **方法**：两阶段无标签训练\n1. 蒸馏 SFT：用 Qwen3-32B 成功轨迹蒸馏出 Qwen3-8B 的“可执行脚本”能力。\n2. 在线 RLVR：以 LLM-as-a-Judge 预测 exit code 与 Pyright 错误数，给出可验证奖励，用 REINFORCE++ 继续优化。\n- **结果**：8B 模型（PIPER）在 EnvBench 达 27% pass@5，与 GPT-4o/Qwen3-32B 持平，成本仅 1/14；在 Repo2Run 以 103/420 超越所有基线；Terminal-Bench 多轮任务上 RL 版本仍保持泛化。\n- **贡献**：首次将在线 RLVR 与轻量可验证奖励用于环境配置；消费级硬件可跑的 8B 模型实现“性能-成本”最优；代码、权重、日志全部公开。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是PIPer%3A%20On-Device%20Environment%20Setup%20via%20Online%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2509.25455，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.25455。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1759448804163","title":"SINQ: Sinkhorn-Normalized Quantization for Calibration-Free Low-Precision LLM Weights","description":"Post-training quantization has emerged as the most widely used strategy for deploying large language models at low precision. Still, current methods show perplexity degradation at bit-widths less than or equal to 4, partly because representing outliers causes precision issues in parameters that share the same scales as these outliers. This problem is especially pronounced for calibration-free, uniform quantization methods. We introduce SINQ to augment existing post-training quantizers with an additional second-axis scale factor and a fast Sinkhorn-Knopp-style algorithm that finds scales to normalize per-row and per-column variances, thereby minimizing a novel per-matrix proxy target for quantization: the matrix imbalance. Our method has no interactions between layers and can be trivially applied to new architectures to quantize any linear layers. We evaluate our method on the Qwen3 model family and DeepSeek-V2.5. SINQ improves WikiText2 and C4 perplexity significantly against uncalibrated uniform quantization baselines and can be further enhanced by combining it with calibration and non-uniform quantization levels. Code to reproduce the results of this work and to easily quantize models using SINQ is available at https://github.com/huawei-csl/SINQ.","url":"https://huggingface.co/papers/2509.22944","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.163Z","metadata":{"authors":["Lorenz K. Müller","Philippe Bich","Jiawei Zhuang","Ahmet Çelik","Luca Benfenati","Lukas Cavigelli"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2509.22944.pdf","arxivUrl":"https://arxiv.org/abs/2509.22944","arxivId":"2509.22944","coolPaperUrl":"https://papers.cool/arxiv/2509.22944","published":"2025-09-26T21:22:54Z","updated":"2025-09-26T21:22:54.000Z","zh_summary":"训练后量化已成为以低精度部署大语言模型的最广泛使用策略。然而，现有方法在比特宽度小于或等于4时表现出困惑度下降，部分原因是表示异常值会导致与这些异常值共享相同量化尺度的参数出现精度问题。对于无需校准的均匀量化方法，这一问题尤为突出。我们提出了SINQ，通过增加一个额外的第二轴尺度因子并使用快速的Sinkhorn-Knopp风格算法，来增强现有的训练后量化器，该算法能够找到尺度以规范每行和每列的方差，从而最小化一个新的矩阵级量化代理目标：矩阵不平衡性。我们的方法不涉及层间交互，可以轻松应用于新的架构中，对任何线性层进行量化。我们在Qwen3模型系列和DeepSeek-V2.5上评估了该方法。SINQ在WikiText2和C4的数据上相比未校准的均匀量化基线显著改善了困惑度，并且结合校准和非均匀量化水平能进一步提升效果。用于重现本工作的代码以及使用SINQ轻松量化模型的工具可在 https://github.com/huawei-csl/SINQ 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决**大语言模型（LLM）在极低比特（≤4 bit）后训练量化（PTQ）时因权重异常值（outliers）导致的精度退化**问题。具体而言：\n\n- **异常值与共享尺度的冲突**：传统单尺度量化方法中，异常值迫使整个行或列使用较大量化尺度，导致同尺度下的其他参数量化误差显著增加。\n- **无校准均匀量化的劣势**：无校准、均匀整数量化虽部署简单，但在极低比特下因上述冲突，困惑度（perplexity）显著劣于需校准或非均匀方法。\n\n为此，作者提出 **SINQ（Sinkhorn-Normalized Quantization）**，通过引入**双轴尺度因子**与**快速 Sinkhorn–Knopp 型算法**，在矩阵行、列两个维度上联合归一化方差，最小化新提出的“矩阵不平衡度”代理指标，从而在不依赖校准数据、保持均匀量化级别的前提下，显著降低量化误差，使无校准均匀量化逼近甚至超越校准/非均匀方法的精度。\n\nQ2: 有哪些相关研究？\n\n与 SINQ 相关的研究可归纳为 4 条主线，均聚焦于**如何在不重新训练的前提下，把 LLM 权重压到 ≤4 bit**：\n\n1. 无校准、均匀整数量化\n\n- **RTN**（round-to-nearest + 单尺度/零点）\n- **HQQ**（Badri & Shaji 2023）：在 RTN 基础上进一步优化 shift 向量，使  L_(0.7)  范数重建误差最小。\n- **Hadamard 变换**（Tseng et al. 2024, QuaRot, HIGGS）：对权重矩阵做 Walsh-Hadamard 旋转，让分布更接近高斯，降低异常值幅度。\n2. 非均匀量化\n\n- **NF4**（Dettmers et al. 2023）：用正态分布的分位数量化级别，4 bit 表示权重。\n- **HIGGS**（Malinovskii et al. 2025）：Hadamard 旋转 + 逐层最优非均匀级别。\nSINQ 可直接把 NF4 作为内层量化器，因此与这条线正交。\n3. 校准型量化\n\n- **GPTQ**（Frantar et al. 2022）：利用 Hessian 信息逐层补偿量化误差。\n- **AWQ**（Lin et al. 2024b）：基于激活幅值学习逐通道尺度，最小化输出  L_2  误差。\nSINQ 可与 AWQ 顺序叠加（A-SINQ），说明其与校准方法兼容。\n4. 权重空间变换\n\n- **SmoothQuant**、**SpinQuant**、**FlatQuant**、**DuQuant** 等：通过旋转、置换、仿射变换把异常值“抹平”，再量化。\n这些变换通常需要校准数据或引入架构依赖；SINQ 仅用双轴尺度 + Sinkhorn 迭代，层间无依赖，因而更轻量且架构无关。\n\n综上，SINQ 的核心差异在于：**首次把“行列双尺度”与“矩阵不平衡度”作为优化目标**，用 Sinkhorn 型迭代快速求解，在**无校准、均匀量化**场景下取得 SOTA，同时可无缝嵌入现有校准或非均匀框架。\n\nQ3: 论文如何解决这个问题？\n\n论文通过以下三步把“异常值-共享尺度冲突”转化为可优化的数值问题，并在**无校准、均匀量化**前提下显著降低 ≤4 bit 的误差：\n\n1. 引入**双轴参数化**\n对任意权重子矩阵  W∈R^(m× n)  采用\n\nW_(≈)=sodot(Q+z)odott,quad s∈R^(m),;t∈R^(n)\n\n其中  Q  为均匀整数量化矩阵。异常值  W_(ij)  可通过“放大  s_i 、缩小  t_j ”把误差**在行-列间灵活转移**，而传统单尺度只能固定把误差压到整行或整列。\n\n2. 提出**矩阵不平衡度**代理指标\n定义\n\nI(W)=maxlmax_iσ_(row)_i,max_jσ_(col)_jrminmin_iσ_(row)_i,min_jσ_(col)_jr\n\n该标量越大，说明行/列标准差差异越大，量化时必然出现“大尺度-大误差”现象。最小化  I  即可让各行、各列动态均衡，降低异常值影响。\n\n3. 设计**Sinkhorn-Knopp 型迭代**快速求解\n算法交替执行\n\n- 列归一： Warrow W/σ_(col) \n- 行归一： Warrow W/σ_(row) \n通常 5–10 次迭代即可把  I  压到 ≈1，同时累积  s,t  到对数域保证数值稳定。整个过程**层间无依赖**，量化耗时仅比 RTN 多 10%。\n\n完成上述归一化后，用任意现有均匀量化器（RTN、INT4、NF4 等）对  W  做低比特离散化，最终返回\n\nQ,;s_(final)=sodotσ_(row),;t_(final)=σ_(col)\n\n即可在推理时按双轴尺度重建。实验表明，该流程在 3–4 bit 上**持续优于 HQQ、Hadamard+RTN、GPTQ/AWQ 等强基线**，且可与 AWQ、NF4 进一步叠加获得额外增益。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“**无校准 ≤4 bit 权重量化**”与“**能否与现有校准/非均匀方法叠加**”两条主线，共设计 6 组实验，覆盖 0.6 B–685 B 规模、多种架构与评测协议。主要结果如下（均取公开代码与相同硬件复现）：\n\n1. **无校准均匀量化主实验**\n模型：Qwen3-0.6 B/1.7 B/4 B/8 B/14 B/32 B\n协议：WikiText2 & C4 困惑度↓，HellaSwag/PIQA/MMLU flip-rate↓\n位宽：3 bit、4 bit（group-size=64）\n结果：SINQ 在所有 24 项指标中**全部优于 RTN/HQQ/Hadamard+RTN**；3 bit 下最高把与 bf16 的差距缩小 55 %。\n\n2. **Pareto 前端（质量-内存权衡）**\n把上述模型与 Llama-2-7 B、Llama-3-8 B/70 B 的结果汇总为“内存占用-WikiText2 ppl”散点。\n结论：SINQ 在 1.5 GB–65 GB 整个内存区间**几乎独占 Pareto 前沿**；与 bf16 相比，4 bit 版平均只损失 0.05–0.3 ppl。\n\n3. **超大 MoE/MLA 模型验证**\n模型：DeepSeek-V2.5-236 B、Qwen3-235 B-A22 B、DeepSeek-V3-685 B\n位宽：3 bit、4 bit\n结果：SINQ 继续保持最低困惑度；在 685 B 模型上 4 bit WikiText2 ppl 从 5.38→5.31，**首次在无校准均匀方法中跌破 bf16+0.1 ppl 区间**。\n\n4. **与非均匀量化叠加**\n把 Alg.1 中的量化器换成 NF4，得到 SINQ-NF4。\n对比：BnB-NF4、HIGGS（非均匀+Hadamard）\n结果：SINQ-NF4 在 Qwen3-32 B 上 C4 ppl 11.21→10.97，**优于专用非均匀方案**；且 SINQ-INT4 甚至略优于 SINQ-NF4，说明“双轴归一化”本身已足够抑制异常值。\n\n5. **与校准方法叠加（A-SINQ）**\n先执行 Sinkhorn 归一化，再用 AWQ 的激活感知尺度微调，最后量化。\n对比：GPTQ、AWQ、Hadamard+GPTQ\n结果：3 bit 下 A-SINQ 在 Qwen3-14 B 上平均 flip-rate 7.56 %，**低于 GPTQ 的 8.06 %**；4 bit 下与 AWQ 打平或略优，但量化耗时仅 AWQ 的 1/4。\n\n6. **消融与效率**\n\n- 1D/2D tiling、有无 shift、辅助变量 INT8/FP16：1D+shift 略占 Pareto 前端，但差距均 < 0.1 ppl。\n- 量化耗时（单 GPU）：SINQ 平均 1.09×RTN，8.54×RTN 仍远快于 GPTQ（62×）与 AWQ（34×）。\n- 推理吞吐：双轴尺度可合并到输出激活缩放，无额外 Kernel 级开销。\n\n综上，实验从 0.6 B 到 685 B、从 dense 到 MoE、从纯 uniform 到 NF4/AWQ 组合，**一致验证 SINQ 在精度-速度-通用性三方面的优势**。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续推进，分为“理论-算法”、“系统-硬件”与“应用-扩展”三大板块：\n\n理论-算法层面\n\n1. **代理指标再思考**\n\n- imbalance 只考虑二阶矩；可探索同时最小化高阶矩（kurtosis）或信息熵的复合目标，看能否在 ≤3 bit 再降 0.1-0.2 ppl。\n- 把 imbalance 推广到块级（block-wise）或通道子空间，适配卷积/混合专家网络。\n2. **收敛与最优性**\n\n- Sinkhorn 迭代目前用经验早停；可给出固定步数收敛到 ε-不平衡的理论上界，或改用 Nesterov 加速、Anderson 加速减少迭代数。\n- 研究行列尺度是否等价于某类矩阵平衡问题，从而借用最优传输的熵正则解法进一步提速。\n3. **联合激活量化**\n\n- 现论文仅权重；可把 imbalance 目标扩展为“输入-权重耦合方差”，探索能否在 W4A4 或 W4A8 场景保持零校准。\n4. **学习-量化协同**\n\n- 允许尺度向量 s,t 在预训练或继续预训练阶段以量化感知方式更新，观察是否比纯后训练模式再提升。\n\n系统-硬件层面\n\n5. **Kernel 级融合**\n\n- 双轴尺度可拆成 `GEMM + 向量乘`；可写 CUDA/Metal 核心把 `s·(Q+z)·t` 与 GEMM 输出缩放合并成一次 kernel launch，实测端到端加速。\n- 研究 INT4 双轴权重打包格式，使内存布局对齐 128 bit，减少 shared-memory bank conflict。\n6. **辅助参数量化策略**\n\n- 目前实验仅把 s,z 压到 INT8；可尝试 4-6 bit 对数量化或矢量量化，进一步削 5-10 % 模型尺寸，观察是否仍保持 Pareto 优势。\n- 探索层间共享尺度码本（类似 VQ-VAE），把 s,t 压成索引，减少 DRAM 传输。\n7. **分布式/边缘推理**\n\n- 在 128 KB L2 cache 的 MCU 上验证 SINQ-INT3，对比 Hadamard 方法（需额外 1-2 KB 旋转矩阵）是否更省 SRAM。\n- 结合 MoE 的 expert-offloading 场景，看双轴尺度能否减少 PCIe 传输带宽。\n\n应用-扩展层面\n\n8. **其他模态与架构**\n\n- 视觉 Transformer、多模态 LLM（CLIP、ViT-Llama）：异常值分布与 LLM 不同，验证 imbalance 指标是否依然有效。\n- 扩散模型 UNet、Stable Diffusion 权重：4 bit 权重生成 512×512 图像，看 FID/CLIP-score 能否保持。\n9. **与压缩方法正交组合**\n\n- 剪枝 + SINQ：先 magnitude 剪枝 20 % 权重，再跑 Sinkhorn 归一化，看是否“稀疏+低比特”双收益。\n- 低秩分解 + SINQ：把权重拆成半秩矩阵对，每对用双轴尺度，探索能否在 2-3 bit 仍收敛。\n10. **自动比特分配**\n\n- 以层-wise imbalance 下降量为信号，给“难量化”层自动分配 4 bit，其余 2-3 bit，形成混合精度策略，全局显存预算固定时再降 0.2-0.3 ppl。\n11. **可解释与可视化**\n\n- 可视化 Sinkhorn 前后异常值热力图，量化误差分布从“长尾”到“近似高斯”的演变，帮助理解为何困惑度下降。\n- 分析哪些层/注意力头对 imbalance 最敏感，为后续结构改进提供线索。\n\n一句话总结\n\nSINQ 把“行列方差平衡”引入后训练量化，仍有大量空白：从**更紧的理论界**到**芯片级 Kernel**，再到**跨模态/跨压缩技术**的叠加，都值得继续深挖。\n\nQ6: 总结一下论文的主要内容\n\n**SINQ: Sinkhorn-Normalized Quantization for LLMs** 提出一种**无校准、均匀整数量化**方案，在 ≤4 bit 场景下显著降低大语言模型权重误差，核心内容与贡献如下：\n\n1\\. 问题背景\n\n- 后训练量化（PTQ）是部署 LLM 的主流手段，但 ≤4 bit 时异常值迫使整行/列共享大尺度，导致同尺度参数误差爆炸。\n- 现有解法依赖校准数据或非均匀级别，带来过拟合、查表开销或架构耦合；**无校准均匀量化**精度落后。\n\n2\\. 核心方法\n\n**双轴尺度参数化**\n\nW_(≈) = s odot (Q+z) odot t, quad s∈R^(m),;t∈R^(n)\n\n异常值误差可在行-列间灵活转移，传统单尺度无法做到。\n\n**矩阵不平衡度代理指标**\n\nI(W)= maxmax_iσ_(row)_i,max_jσ_(col)_jmin_iσ_(row)_i,min_jσ_(col)_j\n\n I  越大量化越困难；最小化  I  即可均衡行列方差。\n\n**Sinkhorn-Knopp 型迭代**\n交替把行、列除以各自标准差，5–10 次迭代即收敛；层间无依赖，耗时仅 RTN 的 1.1×。\n\n3\\. 实验结果\n\n- **Qwen3 0.6 B–32 B**：3–4 bit WikiText2/C4 困惑度**全部刷新无校准均匀量化 SOTA**；3 bit 最高缩小与 bf16 差距 55 %。\n- **Llama-2/3、DeepSeek-V2.5-236 B、Qwen3-235 B-A22 B、DeepSeek-V3-685 B**：一致优于 HQQ、Hadamard+RTN、BnB-NF4。\n- **与 AWQ 叠加（A-SINQ）**：3 bit 平均 flip-rate 再降 0.5–0.8 %，量化耗时仅为 AWQ 的 1/4。\n- **与非均匀 NF4 叠加**：SINQ-NF4 在 32 B 模型 C4 上 11.21→10.97，**优于专用非均匀方案**。\n\n4\\. 结论\n\nSINQ 用“行列双尺度 + 不平衡度最小化”在**无校准、均匀级别**前提下逼近甚至超越校准/非均匀方法，且**架构无关、量化速度极快**，可即插即用于任何线性层。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是SINQ%3A%20Sinkhorn-Normalized%20Quantization%20for%20Calibration-Free%20Low-Precision%20LLM%20Weights，链接是https://arxiv.org/pdf/2509.22944，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.22944。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1759448804164","title":"Code2Video: A Code-centric Paradigm for Educational Video Generation","description":"While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.","url":"https://huggingface.co/papers/2510.01174","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.164Z","metadata":{"authors":["Yanzhe Chen","Kevin Qinghong Lin","Mike Zheng Shou"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.CL","cs.HC","cs.MM"],"pdfUrl":"https://arxiv.org/pdf/2510.01174.pdf","arxivUrl":"https://arxiv.org/abs/2510.01174","arxivId":"2510.01174","coolPaperUrl":"https://papers.cool/arxiv/2510.01174","published":"2025-10-01T17:56:48Z","updated":"2025-10-01T17:56:48.000Z","zh_summary":"尽管近期的生成模型在像素空间的视频合成方面取得了进展，但它们在制作专业教育视频方面仍然有限，而教育视频需要学科知识、精确的视觉结构和连贯的过渡，这限制了它们在教育场景中的应用。直观上，这类需求更适合通过可渲染环境的操作来满足，该环境可以通过逻辑指令（如代码）进行显式控制。在这项工作中，我们提出了Code2Video，一种以代码为中心的代理框架，通过可执行的Python代码生成教育视频。该框架包含三个协作代理：(i) Planner，负责将教学内容结构化为时间上连贯的流程，并准备相应的视觉素材；(ii) Coder，将结构化指令转换为可执行的Python代码，同时引入作用域引导的自动修复以提升效率；(iii) Critic，利用视觉-语言模型（VLM）结合视觉锚点提示来优化空间布局并确保清晰度。为了支持系统性评估，我们构建了MMMC，这是一个包含专业制作、学科特定教育视频的基准数据集。我们在多个维度上评估MMMC，包括基于VLM的审美评分、代码效率，特别是TeachQuiz，一种全新的端到端指标，用于量化VLM在“去学习”后通过观看生成视频恢复知识的能力。我们的结果展示了Code2Video作为一种可扩展、可解释且可控的方法的潜力，其在直接代码生成上实现了40%的提升，并生成了可与人工制作教程相媲美的视频。代码和数据集可从https://github.com/showlab/Code2Video获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**现有生成式模型难以产出高质量、学科专业、时空连贯的教育视频**这一核心问题。具体而言：\n\n- 像素级视频生成（Text2Video）虽在短片段逼真度上取得进展，但在**长程逻辑、符号精确对齐、逐步推理**等教育场景必需的能力上表现薄弱。\n- 教育视频要求**学科深度知识、清晰空间布局、连贯时间展开**，而黑箱像素模型缺乏可解释、可控制、可扩展的机制来满足这些需求。\n\n为此，作者提出**以可执行代码（Manim）为统一媒介**的“代码中心”新范式，将视频生成转化为**可编程渲染任务**，通过三智能体协作（Planner–Coder–Critic）实现：\n\n1. 可解释：每一步布局、动画、转场均由代码显式定义，可审计、可复现。\n2. 可控制：通过代码精确指定时空结构，支持符号级对齐与逐步推理。\n3. 可扩展：模块化集成新可视化与外部资产，无需重新训练模型。\n\n同时构建面向教育目标的**MMMC 基准与 TeachQuiz 评价协议**，量化视频带来的**知识迁移增益**，从而系统验证代码中心范式在教育场景下的有效性。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Work”中将相关研究归为两条主线，并指出各自与本文任务的差距。可梳理如下：\n\n2.1 视频生成（Video Generation）\n\n| 子方向 | 代表工作 | 主要局限（对教育视频） |\n| --- | --- | --- |\n| 扩散式 Text-to-Video | Imagen Video, OpenSora-v2, Wan2.2-T2V-A14B, Veo3 | 像素空间合成→难以实现符号级精确定位、板书记号、跨帧一致；缺乏可解释性。 |\n| 自回归/长视频 | Loong, Lumos-1, ArLon, Freelong | 仍面向娱乐/短片段，对“板书式”分步讲解、长程逻辑结构支持不足。 |\n| 多智能体视频 pipeline | DreamFactory, GenMAC, StoryAgent | 首次把“任务分解-协作-迭代”引入视频生成，但未涉及代码级渲染，也未面向教育评估。 |\n\n→ 本文首次将**多智能体协作**与**可执行代码渲染**结合，填补教育场景对“符号精确+逻辑连贯+可解释”需求。\n\n2.2 编程智能体（Coding Agents）\n\n| 子方向 | 代表工作 | 与本文关系 |\n| --- | --- | --- |\n| 工具调用 & 项目级代码生成 | Gorilla, CodeNav, ProjectEval | 证明 LLM 可自主调用 API、维护跨文件上下文 |"}},{"id":"hf-7-1759448804167","title":"ACON: Optimizing Context Compression for Long-horizon LLM Agents","description":"Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.","url":"https://huggingface.co/papers/2510.00615","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.167Z","metadata":{"authors":["Minki Kang","Wei-Ning Chen","Dongge Han","Huseyin A. Inan","Lukas Wutschitz","Yanzhi Chen","Robert Sim","Saravan Rajmohan"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.00615.pdf","arxivUrl":"https://arxiv.org/abs/2510.00615","arxivId":"2510.00615","coolPaperUrl":"https://papers.cool/arxiv/2510.00615","published":"2025-10-01T07:43:49Z","updated":"2025-10-01T07:43:49.000Z","zh_summary":"大型语言模型（LLM）正越来越多地作为智能体部署在动态的现实环境中，在这些环境中，成功依赖于推理能力和有效的工具使用。对于智能体任务的一个核心挑战是不断增长的上下文长度，因为智能体必须累积长时间的操作和观察历史。这种扩展增加了成本并降低了长周期任务的效率，而以往关于上下文压缩的研究大多集中在单步任务或狭窄的应用场景中。我们提出了智能体上下文优化（ACON），这是一个统一的框架，可以将环境观察和交互历史最佳压缩为简洁且信息丰富的内容。ACON 利用自然语言空间中的压缩指南优化：对于成对的轨迹，当完整上下文成功但压缩上下文失败时，能力强的 LLM 会分析失败原因，并相应更新压缩指南。此外，我们提出将优化后的 LLM 压缩器蒸馏到更小的模型中，以减少额外模块的开销。在 AppWorld、OfficeBench 和多目标问答任务上的实验表明，ACON 可将内存使用量减少 26-54%（峰值 tokens），同时在很大程度上保持任务性能，将其蒸馏到更小的压缩器中可保留超过 95% 的准确率，并使较小的语言模型作为长周期智能体的性能提升最多可达 46%.","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**长时程（long-horizon）大语言模型智能体**在执行多步交互任务时面临的\\*\\*上下文膨胀（context explosion）\\*\\*问题。具体而言：\n\n1. **问题背景**\n\n- LLM 智能体在动态环境中需持续维护**交互历史与环境观测**，导致上下文长度随时间**无界增长**。\n- 长上下文带来双重代价：\n- **推理成本**随 token 数线性上升；\n- **信息稀释**，无关或过时内容干扰决策，降低准确率。\n2. **现有方法的局限**\n\n- 对话系统常用的**会话级摘要**或**分层记忆**仅关注连贯性，难以保留多步任务所需的结构化信号（如 API 参数、状态变量、因果依赖）。\n- 面向单步 QA 或文档检索的压缩方法假设“回答即结束”，无法应对**状态持续演化**的智能体场景。\n- 近期面向智能体的压缩工作要么局限于特定域（如网页可访问性树），要么采用**手工启发式规则**，泛化性与最优性不足。\n3. **核心挑战**\n\n- 无监督信号：压缩目标没有“金标准”，只有**稀疏的终端奖励**。\n- 离散优化：token 数离散，无法直接梯度回传。\n- 环境昂贵：每轮评估需完整 rollout，计算成本极高。\n4. **论文目标**\n提出**Agent Context Optimization (ACON)**，一种**统一、可蒸馏、任务自适应**的上下文压缩框架，在**不降任务成功率**的前提下，将峰值 token 消耗降低 **26–54%**，并使小模型在长时程任务上获得 **20–46%** 的性能提升。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 章“Related Works”中将相关研究划分为三大主线，并指出它们对长时程智能体场景的不足。以下按主题归纳：\n\n1\\. 长时程 LLM 智能体\n\n- **代表工作**\n- ReAct (Yao et al., 2023)\n- WebArena (Zhou et al., 2024) / AppWorld (Trivedi et al., 2024)\n- OfficeBench (Wang et al., 2024b)\n- SWE-Agent (Yang et al., 2024b) / OSWorld (Xie et al., 2024)\n- **核心特征**\n- 将 LLM 从“单次回答”扩展到**多步观察-行动-推理**循环，需长期保持任务状态。\n- **与 ACON 的关系**\n- 上述工作均**未提供系统性的上下文压缩机制**，仅靠截断或手工规则，为 ACON 提供了实验场景与基线。\n\n2\\. 面向 LLM 的上下文压缩\n\n2.1 文档/检索式压缩\n\n- **方法**：抽取式摘要、信息抽取、检索增强压缩\n- **代表**\n- LongLLMLingua (Jiang et al., 2024)\n- RECOMP (Xu et al., 2024)\n- COMPACT (Yoon et al., 2024)\n- **局限**：假设“读完即答”，**一次性压缩**后即丢弃上下文，不维护跨步状态。\n\n2.2 对话记忆压缩\n\n- **方法**：递归摘要、分层记忆、会话级蒸馏\n- **代表**\n- MemGPT (Packer et al., 2023)\n- A-MEM (Xu et al., 2025)\n- 递归摘要 (Wang et al., 2025a)\n- **局限**：聚焦**对话连贯性**，不保留 API 参数、变量绑定等结构化信息，导致多步任务失败。\n\n2.3 KV-Cache 级压缩\n\n- **方法**：注意力下沉、令牌驱逐、稀疏缓存\n- **代表**\n- StreamingLLM (Xiao et al., 2024)\n- LightThinker (Zhang et al., 2025)\n- EpiCache (Kim et al., 2025)\n- **局限**：仅解决**推理显存**，不改变输入 token 数；且多为单轮场景，未考虑智能体状态一致性。\n\n3\\. 面向智能体的专用压缩\n\n- **代表**\n- Mind2Web (Deng et al., 2023) – 仅针对网页可访问性树\n- Lee et al. (2025) – 手工提示压缩网页观察\n- OpenHands (Smith, 2025) – 简单 FIFO 截断\n- **共性问题**\n- **领域狭窄**或**启发式规则**，缺乏任务自适应优化，跨环境泛化能力弱。\n\n4\\. 与 ACON 最邻近的两类研究\n\n| 类别 | 与 ACON 的差异 |\n| --- | --- |\n| Prompt Optimization（DSPy, Textual Gradient Descent） | 优化任务提示，而非压缩提示；且多面向单次推理。 |\n| 知识蒸馏（Kim & Rush, 2016） | ACON 首次将“压缩器”作为独立模块进行蒸馏，实现小模型替代大模型压缩器，降低部署成本。 |\n\n总结\n\n现有研究要么**场景单一**（单步 QA、对话），要么**压缩策略静态**（FIFO、手工规则）。ACON 首次把“**长时程智能体的历史与观测压缩**”视为**可学习、可蒸馏、可任务自适应**的优化问题，填补了该交叉领域的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Agent Context Optimization (ACON)**，通过“**自然语言空间内的压缩指引优化 + 小型模型蒸馏**”两阶段框架，系统性地解决长时程 LLM 智能体的上下文膨胀问题。核心思路是：**不更新模型参数，仅优化压缩提示（guideline）**，使大模型自己学会“哪些信息必须保留、哪些可以丢弃”，再把这一能力蒸馏到小模型以降低推理开销。技术路线可概括为“**对比失败找信号 → 文本梯度改提示 → 交替优化求最短 → 蒸馏小模降成本**”。\n\n1\\. 问题建模：把压缩当成带约束的优化\n\n- **环境**：部分可观察 MDP ⟨S,A,O,T,R⟩\n- **目标**：最大化任务成功率，同时最小化上下文代价\n\nmax_(psi=(φ,P)) El[Rl(s_T(psi)r)r] - λ,El[Cl(H'(psi)r)r]\n\n-  H'(psi) ：压缩后的历史+观测序列\n-  C(·) ：token 数（或峰值）\n- **挑战**：无监督、稀疏奖励、离散不可导 → 传统 RL 训大模型代价极高。\n\n2\\. 解法总览：指引优化（Guideline Optimization）\n\n| 步骤 | 输入 | 输出 | 关键机制 |\n| --- | --- | --- | --- |\n| UT（Utility） | 成功全上下文 vs 失败压缩上下文 | 自然语言“失败归因” | 对比式文本梯度 |\n| CO（Compression） | 仅成功且压缩仍成功的轨迹 | 冗余片段反馈 | 交替式长度最小化 |\n| 蒸馏 | 教师大模型 + 优化后指引 | 学生小模型 | 序列级知识蒸馏 |\n\n整个流程**零梯度更新大模型参数**，完全在**提示空间**完成，适用于黑盒 API。\n\n3\\. 算法细节\n\n3.1 对比反馈（Contrastive Feedback）\n\n1. 在训练集  D_(train)  运行\n- 无压缩 → 得成功轨迹集合  D^+ \n- 用当前指引  P^((r))  压缩 → 得压缩轨迹\n2. 收集“**成功 vs 失败**”成对样本  D_(cont)=(H,H') \n3. 用**分析 LLM** 生成自然语言反馈（缺失变量、错误摘要、冗余循环等）\n\nFeedback=LLM(FeedbackInstr, H, H')\n\n3.2 文本梯度更新（Textual Gradient Descent）\n\n- 把多条反馈拼接后，让**更新 LLM** 重写指引\n\nP^((r+1))=LLM(UpdateInstr, P^((r)), |_i Feedback_i)\n\n- 一次生成  K  个候选，用**小规模验证集**选最佳 → 最大化成功率（UT 步）。\n\n3.3 交替压缩最大化（CO 步）\n\n- 仅对“**压缩后仍成功**”的轨迹，让 LLM 指出“**哪些字段/句子实际未被后续用到**”。\n- 再次更新指引，鼓励**更短但充分**的摘要，显式降低 token 数。\n- 目标改为\n\nmax SuccessRate - λ · NormCost\n\n形成 UT ↔ CO 交替优化，直到收敛或预算耗尽。\n\n4\\. 蒸馏：把优化后的压缩器变小\n\n- **数据**：教师模型在  D^+  上生成的 (原始上下文, 压缩摘要) 对\n- **训练**：LoRA 微调小模型（Qwen3-14B/8B、Phi-4）最小化交叉熵\n\nmin_(φ_S) E_((x,y)sim D^+) [ -∑_(t=1)^(|y|) log f(y_tmid x,y_(<t); φ_S, P^*) ]\n\n- **推理**：学生模型完全替代教师执行压缩，**大模型仅用于决策**，实现“**大模型能力，小模型成本**”。\n\n5\\. 系统级流程（推理阶段）\n\n1. 每步接收  (h_(t-1), o_t) \n2. 若  |h_(t-1)|>T_(hist)  → 用**蒸馏后的小模型**生成  h'_t \n3. 若  |o_t|>T_(obs)  → 生成  o'_t \n4. 智能体基于  (h'_t, o'_t)  决策，**压缩开销 < 原推理 5%**\n\n6\\. 结果验证\n\n- **峰值 token 降低 26–54%**，任务成功率**持平或提升**（AppWorld +4.5%，8-objective QA +7.4% F1）。\n- 蒸馏后\\*\\*<14B 模型保留 >95% 教师准确率\\*\\*，API 成本下降 60%+。\n- 小 agents（Qwen3-14B）借助 ACON 在 hard 任务上**绝对提升 18–46%**，首次逼近大模型水平。\n\n一句话总结\n\nACON 把“**如何压缩**”转化为“**如何写压缩指令**”，通过**失败对比→文本梯度→交替求精→蒸馏小模**四连击，在**不碰模型权重**的前提下，让长时程智能体**显著减支不降智**。\n\nQ4: 论文做了哪些实验？\n\n论文在 **3 个长时程智能体基准**上开展系统实验，覆盖 **生产力办公、多应用协同、深度研究问答** 等多模态场景，共涉及 **4 组核心问题、12 种方法对照、3 类模型尺寸、2 种压缩粒度**，并辅以 **消融、阈值、成本、案例** 等分析。主要实验一览如下（均公开可复现，Azure OpenAI 固定快照 + 开源代码）。\n\n1\\. 实验设计总览\n\n| 维度 | 内容 |\n| --- | --- |\n| Benchmarks | AppWorld (168 测试任务, 9 应用, 42.5 步/任务) OfficeBench (95 任务, 6 办公应用, 1-3 应用协同) 8-objective QA (100 任务, 8 独立问题/任务, 15+ 搜索步) |\n| Agent 模型 | gpt-4.1 / gpt-4.1-mini / gpt-5-chat / Qwen3-14B(蒸馏) |\n| Compressor 模型 | gpt-4.1（教师）→ 蒸馏至 Qwen3-14B/8B、Phi-4、gpt-4.1-mini |\n| 压缩类型 | ① 历史压缩 ② 观测压缩 ③ 二者联合 |\n| 评估指标 | 任务成功率 (↑)、平均步数 (↓)、峰值 token (↓)、依赖面积 (↓)、API 成本 ($) |\n\n2\\. 四组核心实验\n\n2.1 主实验：ACON 能否“降 token 不降成绩”？\n\n- **对照**\nNo Compression | FIFO | Retrieval | LLMLingua | Naive Prompting | ACON-UT | ACON-UTCO\n- **结果（表 1–2，图 1）**\n- **gpt-4.1 历史压缩**：AppWorld 峰值 token ↓ 26%，准确率 56.0 → 56.5（**不降反升**）；OfficeBench ↓ 30% token，准确率 76.8 → 72.6（-4.2pp，可接受）。\n- **观测压缩**：8-objective QA 峰值 token ↓ 54.5%，EM/F1 **超无压缩基线**（0.366→0.373/0.494）。\n- **联合压缩**：token ↓ 40%+，但性能下降明显，论文建议**单独使用历史或观测**。\n\n2.2 蒸馏实验：小模型能继承压缩能力吗？\n\n- **设置**：用 gpt-4.1 教师生成 90 训练任务压缩语料，LoRA 微调学生。\n- **结果（图 4、8，表 10）**\n- Qwen3-14B 学生**保留 >95% 教师准确率**，峰值 token 与教师几乎重合。\n- 推理耗时 ↓ 60%，API 成本 ↓ 70%（图 7）。\n\n2.3 小 Agent 增益：ACON 能否让“小模型变大”？\n\n- **场景**：Qwen3-14B 原生 agent 在长步骤 hard 任务上因上下文干扰严重失败。\n- **结果（图 5，表 6–8）**\n- AppWorld hard 任务：26.8 → 33.9%（**+26% 相对提升**）；8-objective QA EM：0.158 → 0.197（**+25%**）。\n- 峰值 token 同步 ↓ 30%+，实现“**更轻但更强**”。\n\n2.4 阈值与消融：多少 token 才“值得压”？\n\n- **历史阈值**：{2k, 4k, 8k}；观测阈值：{512, 1k, 2k}\n- **结果（图 6）**\n- 4k（历史）+ 1k（观测）为**帕累托最优点**：压缩频率适中，准确率与无压缩持平，token ↓ 30%。\n- 阈值过小 → 压缩过频，丢失关键状态；过大 → 节省有限。\n\n3\\. 附加分析\n\n| 分析项 | 结论 |\n| --- | --- |\n| Prompt 优化器选择（表 3） | o3 + 对比反馈 > gpt-4.1/gpt-5，绝对提升 3.6pp |\n| 交替轮数（表 11） | UT→CO 一轮即饱和，再追加 UT 反而过拟合 |\n| 成本核算（图 7） | 观测压缩显著省钱；历史压缩因 KV-cache 重算可能略增总成本，蒸馏后抵消 |\n| 案例研究（例 E.2–E.3） | gpt-4.1-mini 无压缩连续 401 失败→压缩后保留 token 用法，一步成功 |\n\n4\\. 可复现性\n\n- **数据与代码**：将发布于 GitHub，含 Azure OpenAI 快照号、prompt 模板、LoRA 权重。\n- **随机性控制**：temperature=0.0，seed=42，API 快照 gpt-4.1-2025-04-14；多次运行标准差 <1.2pp。\n\n一句话总结\n\n实验从“**大模型自身→蒸馏小模型→小模型 agent**”三层面系统验证：ACON 在 **26–54% 峰值 token 节省** 的同时，**保持甚至提升任务成功率**，并首次让 **14B 小模型逼近 4 倍大模型** 的长时程表现。\n\nQ5: 有什么可以进一步探索的点？\n\n以下展望基于论文第 5 章“Limitations & Future Work”与实验结果提炼，分为**技术深化**、**场景拓展**、**系统优化**与**理论分析**四个维度，共 12 个可立即着手或长期探索的方向。\n\n1\\. 技术深化\n\n| 方向 | 关键问题 | 可能路径 |\n| --- | --- | --- |\n| 1.1 KV-Cache 级压缩 | 历史压缩打断 KV-cache，导致重算 | 将 ACON 的“文本摘要”信号转化为令牌级 eviction/sink策略，实现无损缓存复用 |\n| 1.2 端到端强化学习 | 当前仅优化提示，未触碰模型参数 | 用离线 RL（如 Decision Transformer）或奖励加权 SFT，把压缩器与策略网络联合训练 |\n| 1.3 多模态压缩 | 观测含图像/音频/文档时如何统一压缩 | 引入跨模态对齐评分，对视觉 token 与文本 token 一起做重要性采样 |\n| 1.4 在线自适应 | 训练后指引固定，遇新环境需重训 | ① 元学习初始化提示；② 运行时少量步骤用失败反馈继续文本梯度更新 |\n\n2\\. 场景拓展\n\n| 方向 | 关键问题 | 可能路径 |\n| --- | --- | --- |\n| 2.1 多智能体协作 | 群聊/分布式 agent 的共享上下文池膨胀 | 将 ACON 扩展为去中心化压缩协议：每个 agent 本地摘要，全局仅同步共识状态变量 |\n| 2.2 工具链动态扩展 | 新 API 不断加入，压缩指引过时 | 构建工具语义嵌入索引，实时检索“相关 API 子集”并增量更新保留字段 |\n| 2.3 真实生产环境 | 仿真 benchmark 与真实用户差距 | 在Microsoft 365 Copilot 日志（脱敏）上做离线回放，评估 ACON 对真实任务完成时长的影响 |\n\n3\\. 系统优化\n\n| 方向 | 关键问题 | 可能路径 |\n| --- | --- | --- |\n| 3.1 压缩-推理协同调度 | 压缩本身引入延迟，抵消 token 节省 | ① 预测压缩收益模型：仅当“预期节省 token > 阈值”才触发压缩；② 异步流水线：后台线程预压下轮上下文 |\n| 3.2 端侧部署 | 14B 蒸馏仍超出手机显存 | ① 分级蒸馏：4B→1B→100M 量化；② 投机压缩：小模型生成草稿，大模型一次验证 |\n| 3.3 能量-准确率联合优化 | 仅优化 token 数，未直接度量焦耳 | 在目标函数显式加入能耗模型（J/token），做帕累托前沿搜索 |\n\n4\\. 理论分析\n\n| 方向 | 关键问题 | 可能路径 |\n| --- | --- | --- |\n| 4.1 信息保留上界 | 如何量化“最小充分统计量” | 引入部分可观察信息论（POMDP 充分性维度），证明压缩后上下文与原始上下文的互信息下界 |\n| 4.2 错误传播界限 | 单步摘要错误随步数放大 | 用误差传播递推给出累积偏差上界，指导最大压缩比与步数的权衡公式 |\n| 4.3 提示空间收敛性 | 文本梯度下降是否收敛 | 将提示视为字符串流形，定义离散梯度与邻域光滑度，证明 UT-CO 交替优化的不动点存在性 |\n\n即刻可做的短期课题\n\n1. **KV-cache 感知的令牌级 ACON**：把摘要信号映射到 token importance score，与 StreamingLLM 结合，一周内可见压缩-缓存双重收益。\n2. **在线小样本自适应**：用最新 10 条失败轨迹，每夜自动更新一次压缩指引，持续监控成功率漂移。\n3. **端侧 1B 模型压缩器**：以 Phi-3-mini 为教师，蒸馏 350M 模型，在手机上跑通 20 步 AppWorld 任务，验证峰值内存 <2 GB。\n\n总结\n\nACON 打开了“**语言空间优化 + 蒸馏部署**”的新范式，未来可在**更深（理论保证）、更广（多模态/多 agent）、更实（在线/端侧）**三条线持续迭代，最终让长时程 LLM 智能体**既轻量又可信地落地生产环境**。\n\nQ6: 总结一下论文的主要内容\n\n**ACON: Optimizing Context Compression for Long-Horizon LLM Agents**\n一句话总结：\n**用自然语言提示优化+蒸馏，把长时程智能体的上下文压缩 26–54%，不降准确率，还让 14B 小模型任务表现提升 46%。**\n\n1\\. 痛点\n\n- 多步 agent 的交互历史与观测**无限增长** → 推理贵、信息稀释、小模型直接“失忆”。\n- 现有压缩只针对**单轮 QA 或对话摘要**，无法保留 API 参数、状态变量、因果依赖等**结构化决策信号**。\n\n2\\. 解法（ACON）\n\n两阶段框架：\n**① 指引优化（UT-CO）**\n\n- 失败对比 → 自然语言“文本梯度” → 迭代改写压缩提示，**零梯度、黑盒可用**。\n- UT 步：最大化任务成功率；CO 步：在成功基础上再剪长度，**交替求最短充分摘要**。\n\n**② 蒸馏部署**\n\n- 用大模型+优化提示生成“压缩语料”，LoRA 微调 14B/8B/Phi-4 等小模型，**API 成本 ↓70%**，性能保持 >95%。\n\n3\\. 实验结果\n\n| 场景 | 峰值 token↓ | 准确率变化 | 小模型增益 |\n| --- | --- | --- | --- |\n| AppWorld | 26% | 56.0→56.5% ↑ | 26.8→33.9% |\n| OfficeBench | 30% | 持平 | – |\n| 8-obj QA | 54% | EM/F1 ↑ | 15.8→19.7 EM |\n\n4\\. 贡献\n\n- **首个**面向通用长时程 agent 的**统一上下文压缩框架**（历史+观测）。\n- **梯度无关**的提示优化 pipeline，闭源 API 直接可用。\n- **压缩器可蒸馏**，实现“大模型能力，小模型成本”。\n- **三基准**验证：token 显著↓，任务成功率持平或↑，小模型逼近大模型水平。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ACON%3A%20Optimizing%20Context%20Compression%20for%20Long-horizon%20LLM%20Agents，链接是https://arxiv.org/pdf/2510.00615，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.00615。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1759448804168","title":"It Takes Two: Your GRPO Is Secretly DPO","description":"Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%.","url":"https://huggingface.co/papers/2510.00977","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.168Z","metadata":{"authors":["Yihong Wu","Liheng Ma","Lei Ding","Muzhi Li","Xinyu Wang","Kejia Chen","Zhan Su","Zhanguang Zhang","Chenyang Huang","Yingxue Zhang","Mark Coates","Jian-Yun Nie"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.LG","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.00977.pdf","arxivUrl":"https://arxiv.org/abs/2510.00977","arxivId":"2510.00977","coolPaperUrl":"https://papers.cool/arxiv/2510.00977","published":"2025-10-01T14:52:11Z","updated":"2025-10-01T14:52:11.000Z","zh_summary":"群体相对策略优化（GRPO）是一种用于大规模语言模型（LLMs）后训练的著名强化学习算法。人们普遍认为，为了通过精确的统计估计保证训练的稳定性，GRPO 需要较大的群体规模，这会带来显著的计算开销。在本研究中，我们通过将 GRPO 重构为一种对比学习形式来挑战这一假设，这揭示了其与直接偏好优化（DPO）之间的基本联系。受 DPO 实证成功的启发，我们研究了最小的两次回滚情况（2-GRPO），这是此前被认为不可行的配置。我们提供了严格的理论分析来验证 2-GRPO，并通过实验证明，尽管仅使用 1/8 的回滚且训练时间减少超过 70%，其性能仍可与 16-GRPO 相媲美。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n这篇论文旨在解决**Group Relative Policy Optimization (GRPO)** 在大规模语言模型（LLM）后训练阶段中**计算开销过大**的问题。\n\n具体而言，论文挑战了“GRPO 必须依赖大组规模（如每组 16 条 rollout）才能稳定训练”的传统假设，提出并理论验证了**仅使用两条 rollout 的极简变体——2-GRPO** 的可行性。通过将 GRPO 重新阐释为一种**对比学习**形式，论文揭示了其与 Direct Preference Optimization (DPO) 的本质联系，进而证明：\n\n- 2-GRPO 在**梯度无偏性**、**优势估计**与**探索能力**上均不逊色于标准 GRPO；\n- 在同等性能下，2-GRPO 可减少 **70% 以上**的训练时间，仅消耗 **12.5%** 的 rollout 数量。\n\n因此，论文的核心贡献是**打破“大组规模必要”的迷思**，为资源受限场景下的高效 RL 后训练提供了一条新路径。\n\nQ2: 有哪些相关研究？\n\n以下研究与本工作直接相关，按主题分组并给出关键贡献或关联点。\n\n- **GRPO 与 RLVR**\n- Shao et al., 2024 —— 提出 GRPO，用组内奖励归一化取代价值网络，成为 DeepSeek-R1 的核心算法。\n- Guo et al., 2025 —— 在 DeepSeek-R1 中大规模验证 GRPO 对数学推理的提升效果。\n- **PPO 及其变体**\n- Schulman et al., 2017 —— 原始 PPO，引入 clipped importance sampling 与价值基线。\n- Schulman et al., 2015 —— GAE，系统分析优势估计与方差权衡，为 GRPO 的组内归一化提供对比基准。\n- **DPO 与偏好优化**\n- Rafailov et al., 2023 —— 将 RLHF 简化为单阶段对比损失，无需奖励模型。本文证明 GRPO 与 DPO 同属对比学习目标，从而启发 2-GRPO 的极简 pairwise 设计。\n- **对比学习理论框架**\n- Wang & Isola, 2020；Chen et al., 2020；He et al., 2020 —— 自监督对比学习的对齐-均匀性分析。\n- Tao et al., 2022 —— 统一梯度形式，本文借其 Definition 3.1 证明 GRPO/DPO 均为对比损失。\n- **组规模与方差控制**\n- Liu et al., 2025 ——"}},{"id":"hf-9-1759448804168","title":"BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses","description":"Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.","url":"https://huggingface.co/papers/2510.00232","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.168Z","metadata":{"authors":["Xin Xu","Xunzhi He","Churan Zhi","Ruizhe Chen","Julian McAuley","Zexue He"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.CL","cs.AI","cs.CY","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.00232.pdf","arxivUrl":"https://arxiv.org/abs/2510.00232","arxivId":"2510.00232","coolPaperUrl":"https://papers.cool/arxiv/2510.00232","published":"2025-09-30T19:56:54Z","updated":"2025-09-30T19:56:54.000Z","zh_summary":"针对大规模语言模型（LLMs）的偏见缓解方法的现有研究，使用了多样的基线和评价指标来评估去偏性能，导致不同方法之间的比较不一致。此外，这些评估大多基于比较 LLM 对有偏与无偏上下文的概率，而忽略了这种评估与用户通过阅读模型响应、期望获得公平且安全输出的实际使用场景之间的差距。为了实现跨去偏方法的一致评估并弥合这一差距，我们引入了 BiasFreeBench，这是一个实证基准，通过将现有数据集重新组织为统一的查询-响应形式，全面比较八种主流偏见缓解技术（涵盖四种基于提示和四种基于训练的方法）在两种测试场景（多选问答和开放式多轮问答）中的表现。我们进一步引入了基于响应的指标 Bias-Free Score，用于衡量 LLM 响应的公平性、安全性及反刻板性。在关键维度上系统比较和分析去偏性能：提示法与训练法范式、模型规模以及不同训练策略对未见偏见类型的泛化能力。我们将公开发布该基准，旨在建立统一的偏见缓解研究测试平台。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该研究针对现有大语言模型（LLM）去偏方法评估碎片化、指标不统一、且过度依赖内部概率而忽视真实响应内容的问题，提出并构建了一个统一评测基准 BIASFREEBENCH，以系统比较主流提示式与训练式去偏技术在真实 query-response 场景下的效果，并引入面向响应级别的 Bias-Free Score 指标，从而弥合学术评测与用户实际体验之间的差距。\n\nQ2: 有哪些相关研究？\n\n与 BIASFREEBENCH 直接相关的研究可分为三类：\n\n1. 面向 BERT/GPT-2 等小模型的早期去偏\n\n- 反事实数据微调：Zmigrod et al. 2019、Lu et al. 2020、Xu et al. 2022\n- 表示投影消偏：Liang et al. 2020、Ravfogel et al. 2020、Shi et al. 2024\n- 高效微调策略：Gira et al. 2022\n2. 面向现代 LLM 的提示级消偏（inference-time）\n\n- 显式提示干预：Gehman et al. 2020、Sheng et al. 2020、Guo et al. 2022\n- 零样本自消偏：Self-Debiasing（Gallegos et al. 2025）、FairSteer（Li et al. 2025）\n- 认知偏差提示：Echterhoff et al. 2024、Furniturewala et al. 2024、Kamruzzaman & Kim 2024\n3. 面向现代 LLM 的训练/编辑级消偏（training-time）\n\n- 对齐微调：DPO（Allam 2024; Zhang et al. 2025）、SFT（Raza et al. 2024）、Safe RLHF（Dai et al. 2024）\n- 模型编辑/卸载：Task Vector（Ilharco et al. 2023）、DAMA（Limisiewicz et al. 2024）、机器卸载（Dige et al. 2024）\n- 参数高效适配：Kumar et al. 2023、Yu et al. 2023\n4. 响应级偏见评测与数据集\n\n- StereoSet、CrowS-Pairs（Nadeem et al. 2021; Nangia et al. 2020）——概率式评测\n- BBQ（Parrish et al. 2022）——多选问答带偏见标注\n- FairMT-Bench（Fan et al. 2025a）、CEB（Wang et al. 2025）、BiasAlert/BiasGuard（Fan et al. 2024; 2025b）——对话响应评测\n\nBIASFREEBENCH 首次将上述提示与训练两类方法统一置于相同的 query-response 评测协议下，并用同一指标 Bias-Free Score 进行横向比较。\n\nQ3: 论文如何解决这个问题？\n\n论文通过以下三步解决“评估碎片化、指标不统一、脱离真实响应”的核心问题：\n\n1. 统一评测协议\n\n- 将现有偏见数据集（BBQ、FairMT-Bench）全部重构成“用户查询–模型回复”的单/多轮对话格式，使所有方法在相同输入下生成自然语言响应，而非仅比较内部概率。\n- 固定系统提示、采样参数与输出长度，确保不同去偏技术可比。\n2. 提出响应级指标 Bias-Free Score\n\n- 对 BBQ：\n\nBFS_(BBQ) = N_(anti) + N_(UNK)N_(biased) + N_(anti) + N_(UNK)\n\n其中 anti 表示反刻板答案，UNK 表示“无法确定/安全拒答”。\n- 对 FairMT-Bench：\n\nBFS_(FairMT) = N_(safe/fair/UNK)N_(biased) + N_(safe/fair/UNK)\n\n- 用 GPT-4o-mini、Llama-Guard、Moderation API 三重裁判 + 人工校验，实现自动、可复现的响应标签。\n3. 系统对照实验\n\n- 同时实现 4 种提示式（Self-Awareness、Self-Reflection、Self-Help、CoT）与 4 种训练式（SFT、DPO、Task Vector、Safe RLHF）方法，覆盖 7 个不同规模/类型的 LLM。\n- 从“提示 vs. 训练”“模型规模缩放”“单偏见类型泛化”三个维度报告 BFS，量化各方法在真实响应上的去偏效果与通用能力损失。\n\n通过以上设计，论文首次给出可直接比较、贴近用户场景的去偏结果，并开源代码与数据，形成统一测试床 BIASFREEBENCH。\n\nQ4: 论文做了哪些实验？\n\n为系统评估去偏效果，论文共设计并执行了 4 组实验，全部以提出的 **Bias-Free Score（BFS）** 为统一指标，覆盖 **单轮选择题** 与 **多轮开放对话** 两种真实使用场景。\n\n1\\. 主实验：8 种去偏技术 × 7 个模型\n\n- **数据集**\n- BBQ（仅保留含歧义上下文样本，15 678 题）\n- FairMT-Bench（10 195 段 5 轮对话，测最后一轮）\n- **对比方法**\n- Prompting 类（4）：Self-Awareness、Self-Reflection、Self-Help、CoT\n- Training 类（4）：SFT、DPO、Task Vector、Safe RLHF\n- **基座模型（7）**\nLlama-3.1-8B-Instruct、Mistral-7B-Instruct-v0.3、Qwen2.5-7B-Instruct、deepseek-llm-7b-chat、DeepSeek-R1-Distill-Llama-8B、Qwen3-8B、gpt-4o-mini\n\n- **结果**\nprompting 平均 BFS 显著高于 training；CoT 与 Self-Awareness 分列两项数据集榜首；Task Vector 去偏虽高但通用能力暴跌（见后续实验）。\n\n2\\. 通用能力保持实验\n\n- **基准**\nBoolQ（理解）、COPA（推理）、TruthfulQA（真实性）\n\n- **观测指标**\n相对于 vanilla 模型的准确率/BLEU 差值 Δ\n\n- **关键发现**\n- SFT、DPO、Safe RLHF 的 Δ 绝对值 ≤ 1%\n- Task Vector 在 BoolQ 上最高掉 22.57%，COPA 掉 34%，显著牺牲通用能力\n\n3\\. 模型规模缩放实验\n\n- **对象**\nQwen2.5 系列 0.5B→1.5B→3B→7B→14B 共 5 个尺寸\n\n- **变量控制**\n固定训练数据与超参，仅变动模型大小\n\n- **结论**\n- Prompting 方法 BFS 随参数增大单调提升，方差亦增大\n- Training 方法 BFS 几乎与规模无关，曲线平坦\n\n4\\. 偏见类型泛化实验\n\n- **训练设置**\n仅用 StereoSet 中单类偏见数据（Gender、Race、Religion、Profession）分别训练 SFT/DPO，再在 BBQ 全部 9 类偏见上测试\n\n- **指标**\nΔBFS = 单类训练后 BFS − 全类训练后 BFS\n\n- **结果**\n- DPO 曲线更外凸，Gender-only DPO 即可逼近全类 DPO，跨类泛化最强\n- SFT 仅在全类数据下才获得正向泛化，单类训练常导致负迁移\n\n辅助分析\n\n- **数据均衡消融**：证明加权采样后的均衡训练集在 62.5% 情况下优于原始长尾分布\n- **token 开销**：Self-Help 因需二次改写，输入 token 达 6.9 M，为单轮方法的 5.7×\n- **人工校验**：GPT-4o-mini 裁判与人工一致性 κ=1.0（BBQ）/0.7（FairMT），验证自动评估可靠\n\n以上实验共同构成 BIASFREEBENCH 的实证基础，完整展现了不同去偏技术在真实响应层面的优劣与权衡。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“评测扩展”“方法改进”“理论剖析”三大类，供后续研究参考。\n\n1\\. 评测扩展\n\n- **多语言与文化迁移**\n- 将 BIASFREEBENCH 迁移至中文、阿拉伯语等低资源语言，观察去偏方法在文化语境差异下的鲁棒性。\n- **多模态偏见**\n- 引入图文/视频对话数据（如 MM-StereoSet），检验视觉-语言联合偏见是否削弱现有文本去偏效果。\n- **长程记忆与个性化**\n- 在 100+ 轮长期对话或用户画像持续更新的场景下，测量偏见是否随上下文累积而“回弹”。\n- **对抗性评测**\n- 构造隐含偏见模板（dog-whistle prompts），测试模型在表面无害但实质歧视的输入下是否仍能保持低偏见输出。\n\n2\\. 方法改进\n\n- **Prompting-Training 协同**\n- 设计“即插即用”的强化提示，将 DPO 学到的偏好信号实时注入解码过程，实现训练-提示混合去偏。\n- **参数高效策略**\n- 用 LoRA/AdaLoRA 仅更新 0.1% 参数，对比 Task Vector 的全量权重差分，探索能否在保持通用能力的同时达到同等去偏增益。\n- **动态激活干预**\n- 借鉴 FairSteer 的“推理时激活偏移”，结合 BIASFREEBENCH 的 BFS 作为在线奖励，实现实时梯度-free 优化。\n- **可解释性驱动编辑**\n- 利用稀疏自编码器定位“偏见神经元”，再对对应 FFN 权重进行掩码或符号翻转，减少编辑副作用。\n\n3\\. 理论剖析\n\n- **偏见-知识纠缠度量**\n- 定义参数空间中“偏见方向”与“事实方向”的余弦重叠度，量化为何 Task Vector 会损害通用能力。\n- **规模律与偏见**\n- 在 1B–100B 区间系统采样，拟合 BFS ∝ α log(N) + β，检验去偏收益是否随规模饱和。\n- **数据质量 vs. 数据覆盖**\n- 控制偏见类型、样本难度、标注一致性三变量，用因果图方法分离“高质量单类”与“广覆盖多类”对 DPO/SFT 的独立贡献。\n- **用户感知对齐**\n- 引入真人 pairwise 偏好标注，将 BFS 与人类“公平-有用”权衡曲线对齐，验证自动指标是否过度奖励保守拒答。\n\n4\\. 伦理与社会影响\n\n- **恶意滥用检测**\n- 监控去偏模型是否可被轻易“重注入”偏见（如逆向 Task Vector），建立篡改风险评估协议。\n- **交叉公平性**\n- 同时考虑种族+性别+SES 等交叉群体，检验现有方法是否无意中加剧少数-少数群体的边缘化。\n\n探索上述方向可进一步缩小“实验室公平”与“真实世界公平”之间的差距，并推动去偏研究从单一指标优化走向可信、可控、可解释的系统工程。\n\nQ6: 总结一下论文的主要内容\n\n**BIASFREEBENCH：统一评测大模型去偏响应的实证基准**\n\n1. 问题\n\n- 现有去偏研究各自为政：基线不同、指标各异，且普遍用“内部概率差”代替真实响应内容，导致实验结果无法横向比较，也与用户实际体验脱节。\n2. 解决方案\n\n- 构建统一评测基准 **BIASFREEBENCH**，把 BBQ、FairMT-Bench 等数据集全部改写成 query-response 格式，引入响应级指标 **Bias-Free Score**（BFS），用 GPT-4o-mini 等裁判直接给模型回复打标签，实现“同一 prompt、同一指标、同一裁判”的公平比较。\n- 系统实现 4 种提示式（Self-Awareness、Self-Reflection、Self-Help、CoT）与 4 种训练式（SFT、DPO、Task Vector、Safe RLHF）方法，覆盖 7 个不同规模/类型 LLM，形成可复现的开源测试床。\n3. 主要实验与发现\n\n- **去偏效果**：提示法平均 BFS 显著高于训练法；CoT 与 Self-Awareness 分列两数据集第一。\n- **能力保持**：Task Vector 在 BoolQ/COPA 上最高掉 34% 准确率，其余训练法基本无损。\n- **规模缩放**：提示法 BFS 随模型增大稳步提升，训练法与规模无关。\n- **类型泛化**：DPO 仅用性别数据即可泛化到 9 类偏见，SFT 需全类数据才能持平。\n4. 贡献\n\n- 首次把主流去偏技术置于统一响应级评测框架，提供可复现代码与数据，填补“概率评测”与“真实交互”之间的空白，为后续研究建立可靠基准。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是BiasFreeBench%3A%20a%20Benchmark%20for%20Mitigating%20Bias%20in%20Large%20Language%20Model%20Responses，链接是https://arxiv.org/pdf/2510.00232，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.00232。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1759448804169","title":"Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls","description":"Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.","url":"https://huggingface.co/papers/2510.00184","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.169Z","metadata":{"authors":["Xiaoyan Bai","Itamar Pres","Yuntian Deng","Chenhao Tan","Stuart Shieber","Fernanda Viégas","Martin Wattenberg","Andrew Lee"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.00184.pdf","arxivUrl":"https://arxiv.org/abs/2510.00184","arxivId":"2510.00184","coolPaperUrl":"https://papers.cool/arxiv/2510.00184","published":"2025-09-30T19:03:26Z","updated":"2025-09-30T19:03:26.000Z","zh_summary":"语言模型的能力日益增强，但在看似简单的多位数乘法任务上仍然会失败。在本研究中，我们通过逆向工程一个能够通过\\emph{隐式链式思维}成功学习乘法的模型来研究原因，并报告三个发现：(1) 长程结构的证据：Logit归因和线性探针表明，该模型能够编码多位数乘法所需的长程依赖关系。(2) 机制：该模型通过注意力机制编码长程依赖关系，以构建有向无环图（DAG）来“缓存”和“检索”成对的部分乘积。(3) 几何结构：模型在注意力头中实现部分乘积，通过形成数字对之间的闵可夫斯基和，并且数字使用傅里叶基表示，这些都是直观且高效的表示，而标准微调模型缺乏这些表示。基于这些见解，我们重新审视了标准微调的学习动态，发现模型会收敛到缺乏必要长程依赖关系的局部最优解。我们进一步通过引入一个辅助损失来验证这一理解，该损失通过线性回归探针预测“运行和”，提供了一种归纳偏置，使模型能够成功学习多位数乘法。总之，通过对隐式链式思维模型的机制进行逆向工程，我们揭示了Transformer学习长程依赖关系的一个陷阱，并提供了一个关于如何通过正确的归纳偏置解决该问题的示例。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文试图回答的核心问题是：\n\n**为什么 Transformer 在标准微调（standard fine-tuning）下无法学会多位数乘法，而在隐式思维链（implicit chain-of-thought, ICoT）训练下却可以？**\n\n具体而言，作者通过逆向工程（reverse-engineering）一个成功学会 4×4 位数乘法的 ICoT 模型，揭示出标准微调失败的根本原因在于：\n\n- **未能习得长程依赖（long-range dependencies）**：乘法需要把“所有满足 i+j≤k 的局部积 a\\_i b\\_j”汇总到中间量 ˆc\\_k，而标准微调只凭自回归损失无法让模型自动建立这种跨越多个 token 的依赖。\n- **陷入局部最优**：梯度下降先快速拟合最低位 c\\_0、c\\_1 和最高位 c\\_7，中间位 c\\_3–c\\_6 的损失长期停滞，模型始终学不到正确的累加与进位机制。\n- **缺乏恰当归纳偏置（inductive bias）**：ICoT 通过逐步内化中间步骤，相当于在潜空间提供“过程监督”，从而诱导出类似二叉树的注意力缓存-检索机制；标准微调缺少这种偏置，无法自发形成同样结构。\n\n在找出症结后，作者进一步验证：**只需额外加一个轻量级线性探针损失，强制模型在每个输出位预测“运行部分和”ˆc\\_k，即可在无需 ICoT 的情况下让 2 层 Transformer 达到 99% 准确率**。这表明问题不在模型容量，而在训练目标与优化动态。\n\n综上，论文把“Transformer 学不会乘法”这一经验现象转化为对**长程依赖学习机制**的深入剖析，并给出任务特定的归纳偏置示例，提示未来可设计更通用的改进方向。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为以下四条主线，并给出代表性文献：\n\n1. 算术/算法任务上的 Transformer 机制解析\n\n- Nanda et al. (2023a) 用机制可解释性研究“grokking”现象，揭示 Transformer 如何完成模加。\n- Lee et al. (2023) 系统探讨如何“教”小 Transformer 做多位数加法、乘法，发现标准训练难以泛化到更长输入。\n- Kantamneni & Tegmark (2025) 证明大模型用三角函数（Fourier 基底）执行加法，本文乘法场景下的“五棱柱”几何结构与之呼应。\n- Ye et al. (2024) 剖析小学数学题的隐藏推理过程，强调中间步骤表示的重要性。\n2. 过程监督 / 中间步骤显式训练\n\n- Uesato et al. (2022) 在 GSM8K 上比较“过程奖励”与“结果奖励”，显示过程监督显著减少推理错误。\n- Lightman et al. (2023) 通过人工逐步反馈训练数学任务奖励模型，提升 MATH 数据集表现。\n- Zhong et al. (2023) 的 Math-Shepherd 用自动续写验证生成逐步奖励，继续提高 GSM8K+MATH 成绩。\n- Deng et al. (2024) 提出的 Implicit Chain-of-Thought（ICoT）把上述思想搬到潜空间：先给显式中间 token，再逐步裁剪，迫使模型内化推理链。本文正是以 ICoT 为对照，揭示其成功关键在于诱导长程依赖。\n3. 长程依赖与 Transformer 局限性\n\n- Tay et al. (2020) 的 Long Range Arena 基准系统评估各类高效 Transformer 在长程任务上的差距，指出标准自注意力仍难捕获稀疏、远距离交互。\n- Allen-Zhu & Li (2023a,b) 的“Physics of Language Models”系列从理论角度分析 Transformer 如何在不同算法任务中存储与提取知识，强调结构偏置的必要性。\n4. 表示几何与可解释性\n\n- Nanda et al. (2023b) 提出线性探针与 attribution 方法，用于验证模型是否编码了特定算法变量；本文沿用其探针思路检测 ˆc\\_k。\n- Park et al. (2024a,b) 研究竞争动态如何塑造上下文学习表示，同样借助注意力可视化与几何分析，与本文“注意力树”“Minkowski 和”观察方法一致。\n\n这些工作共同构成了理解“Transformer 在算法任务上何时成功、为何失败”的理论与实证背景，本文在此基础上首次针对“多位数乘法”失败案例给出完整的机制级诊断与修正方案。\n\nQ3: 论文如何解决这个问题？\n\n论文并未提出“通用万能解”，而是沿着“诊断 → 验证 → 打补丁”三步走，给出一条针对**多位数乘法**且**无需显式思维链 token** 的解决路径。核心思路是：**用辅助损失强行注入“长程依赖”这一归纳偏置**，从而把模型从局部最优里拉出来。\n\n1\\. 诊断：标准微调为何失败\n\n- 自回归损失 + 梯度下降只能迅速拟合“梯度信号强”的位：\n– 最低位 c₀、c₁ 依赖少，最快收敛；\n– 最高位 c₇ 因数据分布简单也被快速学会；\n– 中间位 c₃–c₆ 需要累加所有 i+j≤k 的 a\\_i b\\_j，梯度信号弱，损失长期 plateau。\n- 线性探针显示：中间隐藏状态几乎编码不了“运行部分和” ˆc\\_k，说明模型**从未建立长程依赖**。\n\n2\\. 验证：ICoT 成功的关键机制\n\n- **注意力树缓存-读取**：\n– 第 1 层在各 timestep 把 pairwise product a\\_i b\\_j “写”进早期 token 的隐藏状态；\n– 第 2 层在输出 c\\_k 时只稀疏 attend 到这些“缓存位”，形成一棵跨时间步的二叉树。\n- **几何结构**：\n– 数字嵌入落在**五棱柱**（Fourier 基底）；\n– 注意力头输出呈**Minkowski 和** αA⊕(1−α)B，天然适合计算乘积。\n标准微调既无树状注意力，也无五棱柱几何，因而无法复现上述机制。\n\n3\\. 打补丁：给标准训练加一个“运行部分和”探针损失\n\n**做法**\n\n- 在第二层 H=2 个 attention head 输出上挂**线性回归头** w\\_h ∈ ℝ^d：\n\nz_h = w_h^⊤ ATT_h^((2)), quad L_(aux) = (1) / (H) ∑_(h∈H) (1) / (8) ∑_(k=0)^7 (z_h − c_k)^2\n\n- 总损失：\n\nL = L_(LM) + λ L_(aux), quad λ=1 即可\n\n**效果**\n\n- 2 层 4 头模型在**无需任何中间 CoT token** 的情况下，4×4 乘法准确率从 <1% 提升到 **99%**。\n- 梯度与损失曲线显示：中间位 c₃–c₆ 的 loss 不再 plateau，而是按 c₂→c₃→c₄→c₆→c₅ 顺序被逐步学会。\n- 注意力可视化表明：模型**自发复现了类似 ICoT 的稀疏二叉树结构**，并在某一头中额外出现“同时 attend 全部所需数位”的平行四边形模式，进一步验证长程依赖已被内化。\n\n4\\. 局限与启示\n\n- 该辅助损失是**任务专用**的（需要事先知道 ˆc\\_k 监督）。\n- 但实验明确展示：**一旦提供正确归纳偏置，Transformer 完全有能力用纯自回归方式学会乘法**。\n- 未来工作可探索更通用的长程偏置，例如\n– 可学习的连续缓存机制；\n– 基于子梯度或记忆增强的通用过程监督；\n– 结构化的注意力稀疏模式正则化。\n\n一句话总结\n\n论文通过“诊断局部最优 → 复现 ICoT 机制 → 引入 ˆc\\_k 探针损失”三步，**首次在标准 Transformer 上实现无需显式思维链的多位数乘法完美学习**，证明失败根源是训练目标缺乏长程依赖归纳偏置，而非模型容量不足。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“为什么标准微调学不会 4×4 乘法”这一核心问题，设计了三组互相对照、层层递进的实验。所有实验均在**同一任务、同一规模**（2 层 4 头 Transformer，仅 4×4 乘法）下完成，以保证变量单一、结论可溯。具体实验如下：\n\n1\\. 基线对比：ICoT vs 标准微调（SFT）\n\n**目的**\n确认“ICoT 能而 SFT 不能”这一现象在最小有效架构上成立，并量化差距。\n\n**设置**\n\n- 训练数据：80 k 样本，输入格式“a₀a₁a₂a₃\\*b₀b₁b₂b₃%%####c₀…c₇”（数字按最低位优先）。\n- ICoT：13 个 epoch 内每 epoch 去掉 8 个中间 CoT token，最终只剩操作数与答案。\n- SFT：完全无中间 token，相同超参（lr=5e-5，epoch=13）。\n\n**观测指标**\n\n- 整体准确率、逐位准确率。\n- 梯度范数与逐位损失曲线（图 7a）。\n\n**结果**\n\n- ICoT：100 % 整体准确率。\n- SFT：<1 % 整体准确率；中间位 c₃–c₆ 损失 plateau，梯度几乎归零。\n\n2\\. 机制诊断实验\n\n2.1 Logit Attribution\n\n**目的**\n验证模型是否利用“远距离”操作数位预测当前输出位。\n\n**方法**\n\n- 对 1 k 样本，随机替换单个操作数位，观察输出 logit 变化 Δt,k。\n- 绘制热图（图 2）：行=被扰动位，列=输出位。\n\n**结果**\n\n- ICoT：低 t 位也能显著影响高 k 位，符合长程依赖。\n- SFT：仅相邻位有微弱影响，缺失长程。\n\n2.2 线性探针 ˆc\\_k\n\n**目的**\n量化隐藏状态是否编码“运行部分和” ˆc\\_k。\n\n**方法**\n\n- 在第二层 attention 后与 MLP 前插入线性回归探针 w\\_k：\n\nw_k h_(t_c_k)^(2.mid) arrow c_k\n\n- 报告 MAE（图 3）。\n\n**结果**\n\n- ICoT：MAE ≈ 1–2，表明 ˆc\\_k 可被准确解码。\n- SFT：MAE 高达 70–110，几乎未编码。\n\n2.3 注意力可视化与“注意力树”重建\n\n**目的**\n揭示 ICoT 如何“缓存-读取”局部积。\n\n**方法**\n\n- 对 1 k 测试样本平均注意力权重，手动追踪每层每头在预测 c\\_k 时 attend 的位置。\n- 以 c₂ 为例（图 4）：列出所需局部积 a₂b₀, a₁b₁, a₀b₂，反向验证这些对在哪些 timestep 被第一层缓存、第二层读取。\n\n**结果**\n\n- 成功画出跨时间步的二叉依赖树；对所有 c₀–c₇ 均可复现（图 10）。\n\n3\\. 几何分析实验\n\n3.1 Minkowski 和检验\n\n**目的**\n验证 attention 头输出是否为两数字嵌入的凸组合/Minkowski 和。\n\n**方法**\n\n- 对第一层仅 attend 两个数字位 (a\\_i, b\\_j) 的头，抽样不同 α，观察输出分布。\n- 3D PCA（图 5）显示“全局-局部”自相似嵌套结构，与理论 covariance 公式一致。\n\n3.2 数字嵌入 Fourier 拟合\n\n**目的**\n检查模型是否用 Fourier 基底表示数字。\n\n**方法**\n\n- 用 6 维实 Fourier 基（k=0,1,2,5）拟合：\n– 嵌入矩阵 E\n– 最终 MLP 输出权重 W\\_out\n– 最后隐藏状态 h\\_L\n- 报告中位数 R²（表 1）。\n\n**结果**\n\n- ICoT：R² 0.84–0.99；扩展到 8 维即可完美拟合。\n- SFT：无显著 Fourier 结构（R² 低且几何无规律，图 6）。\n\n4\\. 消融与修正实验\n\n4.1 放大模型容量\n\n**目的**\n排除“只是模型太小”这一假设。\n\n**方法**\n\n- 把 SFT 放大到 12 层 8 头，总参数量≈10×，训练 60 epoch。\n\n**结果**\n\n- 整体准确率仍 <1 %，中间位损失依旧 plateau（图 9），证实是训练目标问题而非容量。\n\n4.2 辅助损失（Auxiliary Loss）注入\n\n**目的**\n验证“只要强加长程偏置即可学会”这一核心论断。\n\n**方法**\n\n- 在 SFT 基础上加线性探针损失\n\nL_(aux) = (1) / (H)∑_(h∈H)(1) / (8)∑_(k=0)^7 (w_h^⊤ ATT_h^((2)) − c_k)^2\n\n总损失 \nL = L_(LM) + L_(aux)\n，λ=1，训练 13 epoch。\n\n**观测指标**\n\n- 整体与逐位准确率、梯度/损失曲线（图 7b）、注意力模式（图 8）。\n\n**结果**\n\n- 准确率从 <1 % → 99 %；中间位 loss 依次下降，学习顺序与 ICoT 类似。\n- 注意力出现与 ICoT 相似的稀疏二叉树，并在某一头同时 attend 全部所需位，形成平行四边形模式，表明模型自发复现了关键机制。\n\n5\\. 可重复性补充\n\n- 代码与数据已开源（github.com/ajyl/icot）。\n- 附录 A 给出超参、数据格式、训练曲线；附录 B 给出 Fourier 拟合细节；附录 C、D 提供 12 层模型梯度曲线与全部注意力热图，供复现与进一步分析。\n\n实验全景一览（无表格版）\n\n```text\n基线对比 → 量化 ICoT 成功、SFT 失败\n↓\n机制诊断 → 长程依赖缺失的实证\n↓\n几何分析 → 五棱柱+Fourier 基底与 Minkowski 和\n↓\n容量消融 → 12 层仍失败，排除“模型小”\n↓\n辅助损失修正 → 无 CoT token 也能 99 %，验证归纳偏置即可\n```\n\n整套实验由“现象 → 归因 → 几何 → 消融 → 修正”闭环构成，共同支撑论文结论：**Transformer 学不会乘法并非容量不足，而是标准训练目标缺乏对长程依赖的归纳偏置。**\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为“把本文结论再往前推一步”的开放问题，既包含理论深挖，也包含工程落地与通用化探索。为方便后续研究，按“问题—可行思路—预期收获”三段式列出。\n\n1\\. 通用长程依赖归纳偏置\n\n**问题**\n辅助损失依赖人工给出 ˆc\\_k，能否**自动发现**任意算法任务所需的“中间压缩量”？\n\n**思路**\n\n- 将探针损失升级为**可学习压缩器**：\n– 训练一个轻量 HyperNetwork，凭当前任务前缀自动生成探针目标；\n– 或采用变分信息瓶颈，令模型自己学习最紧凑的 sufficient statistic z\\_t。\n- 在更宽算法任务（除法、开方、排序、图算法）上元学习这类压缩器，检验迁移性。\n\n**预期收获**\n得到“任务无关、自动适配”的长程监督信号，摆脱对人工中间量的依赖。\n\n2\\. 注意力稀疏模式先验\n\n**问题**\n“二叉缓存树”是乘法的专用结构，能否**一次性给 Transformer 注入多种稀疏掩码**，让网络按需选择？\n\n**思路**\n\n- 将注意力改为**结构化稀疏掩码库**（local + dilated + global + tree），用 L\\_0 正则或 Gumbel-Softmax 做掩码选择；\n- 训练时加**稀疏性损失**，鼓励模型为不同算法任务自动激活不同掩码；\n- 与专家混合（MoE）结合，把“稀疏模式”作为专家路由依据。\n\n**预期收获**\n把“手工可视化才能发现的树结构”变成**可训练、可解释、可复用**的归纳偏置模块。\n\n3\\. Fourier-几何表示的理论化\n\n**问题**\n为何五棱柱+Fourier 基底对数字运算如此高效？能否**证明其表达力与样本复杂度优势**？\n\n**思路**\n\n- 在环面 Z/10Z 上建立线性表示论，证明 Fourier 基是**最小维数的同态嵌入**；\n- 用群论视角把“加法进位”看作群扩张序列，分析 Transformer 何时能学习该序列；\n- 对比标准 one-hot、binary、BCD 编码，给出 Fourier 基在长度泛化上的 PAC 下界。\n\n**预期收获**\n为“用什么基底表示数字”提供**理论最优解**，并推广到其他循环群结构（模运算、置换群）。\n\n4\\. 长度泛化与持续学习\n\n**问题**\nICoT 与辅助损失模型在 4×4 上完美，但**直接推 5×5 或更长**会怎样？\n\n**思路**\n\n- 采用课程学习：从 2×2 → 3×3 → … → 8×8，监控何时崩溃；\n- 引入**位置-内容解耦**的相对位置编码，减少长度漂移；\n- 用神经微分方程或液态网络，让“累加器状态”随长度连续化，而非离散 token。\n\n**预期收获**\n找出“长度阈值”与表示漂移的定量关系，给出**长度泛化的临界条件**。\n\n5\\. 多层次过程监督的自动化\n\n**问题**\n人工写 CoT 标注代价高，能否**用程序生成+ verifier** 自动提供过程监督？\n\n**思路**\n\n- 对任意算法，先写**确定性 verifier**（如乘法可用小学竖式算法验证）；\n- 用 LLM 自生成多条 CoT 候选，verifier 只保留正确路径，再蒸馏到 Transformer；\n- 结合本文辅助损失思想，把 verifier 的中间变量作为可微探针目标，实现**无人工标注的 ICoT**。\n\n**预期收获**\n把“乘法故事”升级为**任意算法都能自动内化推理链**的通用 pipeline。\n\n6\\. 硬件-算法协同优化\n\n**问题**\n注意力树与 Fourier 基底能否**映射到定制芯片**，实现超低功耗算术推理？\n\n**思路**\n\n- 将五棱柱嵌入用**相位编码 ROM** 存储，乘法直接查表+Minkowski 和；\n- 稀疏注意力树用**片上 SRAM 缓存**局部积，第二级网络-on-chip 读取；\n- 与 RISC-V 扩展指令集结合，把“语言模型做乘法”做成**专用指令**。\n\n**预期收获**\n让“大模型算 4×4 乘法”从软件 trick 变成**可部署在边缘设备上的硬化算子**。\n\n7\\. 反向应用：用乘法机制做可解释水印\n\n**问题**\n能否利用“模型必须计算 ˆc\\_k”这一刚性机制，植入**可验证水印**？\n\n**思路**\n\n- 在训练数据里隐含特定数字-进位模式（如固定位出现质数乘积）；\n- 事后用探针检测 ˆc\\_k 分布，若出现该模式则判定为“水印模型”；\n- 对抗微调或剪枝时，观察水印消失是否同步带来准确率下降，验证机制鲁棒性。\n\n**预期收获**\n把“机制可解释”转化为**模型版权保护与溯源**的工具。\n\n8\\. 跨模态长程依赖\n\n**问题**\n文本-数字任务需要长程依赖，**图像-文本混合的算术**（如手写竖式拍照问答）是否同样适用？\n\n**思路**\n\n- 用 ViT 编码图像竖式，Transformer 解码答案，同步加入 ˆc\\_k 探针损失；\n- 研究视觉 token 与数字 token 是否也能形成“跨模态注意力树”；\n- 若失败，用**共享 Fourier 嵌入空间**把数字图像与符号 ID 映射到同一五棱柱。\n\n**预期收获**\n验证“Fourier-树”机制是否**模态无关**，为多模态推理提供可解释支架。\n\n小结\n\n| 方向 | 关键词 | 可能产出 |\n| --- | --- | --- |\n| 通用长程偏置 | 自动压缩器、信息瓶颈 | 任务无关辅助损失 |\n| 稀疏注意力先验 | 结构化掩码库、L_0 正则 | 可训练稀疏树 |\n| Fourier 理论化 | 群表示、同态嵌入 | 最优数字编码定理 |\n| 长度泛化 | 课程学习、连续累加器 | 长度阈值定律 |\n| 自动过程监督 | Verifier+蒸馏 | 无人工 CoT pipeline |\n| 硬件协同 | 片上 ROM+SRAM | 边缘低功耗算术芯 |\n| 可解释水印 | 刚性机制植入 | 模型溯源工具 |\n| 跨模态扩展 | 共享 Fourier 空间 | 多模态推理可解释性 |\n\n这些探索点既可直接继承本文的“注意力树 + Fourier 几何”发现，也能把“乘法特例”推向“算法通用”“硬件落地”“理论最优”等多维边界。\n\nQ6: 总结一下论文的主要内容\n\n论文主旨：\n**用逆向工程解释“Transformer 在标准微调下学不会 4×4 乘法”的根本原因，并证明只需给训练目标注入“长程依赖”归纳偏置即可让最小模型达到 99 % 准确率。**\n\n1\\. 现象与对照\n\n- 相同 2 层 4 头 Transformer、相同数据\n– 隐式思维链（ICoT）：100 % 整体正确\n– 标准微调（SFT）：<1 % 整体正确，中间位永远 plateau\n\n2\\. 逆向工程发现（ICoT 为何成功）\n\n| 模块 | 关键机制 |\n| --- | --- |\n| 长程依赖 | 用线性探针与 logit attribution 证实隐藏状态完整编码“运行部分和” ˆc_k |\n| 注意力树 | 第 1 层在各 timestep 稀疏 attend 两数字，缓存局部积 a_i b_j；第 2 层在输出 c_k 时只读取对应缓存，形成跨时间二叉树 |\n| 几何结构 | 数字嵌入落在 Fourier 基底的五棱柱；attention 头输出呈 Minkowski 和 αA⊕(1−α)B，天然适合乘积计算 |\n\n3\\. 失败诊断（SFT 为何失败）\n\n- 自回归损失先让梯度信号强的 c₀、c₁、c₇ 迅速收敛，中间位 c₃–c₆ 梯度趋零，模型陷入**缺乏长程依赖的局部最优**\n- 放大到 12 层 8 头仍 <1 %，排除容量问题\n\n4\\. 简单修正（无需 CoT token）\n\n- 在第二层加**线性探针辅助损失**，强制预测 ˆc\\_k：\n\nL_(aux)=(1) / (H)∑_(h)(1) / (8)∑_(k=0)^(7)(w_h^⊤ATT_h^((2))−c_k)^2\n\n- 总损失 \nL=L_(LM)+λL_(aux)\n，λ=1\n- 结果：2 层模型准确率 **<1 % → 99 %**，注意力自发复现类似 ICoT 的稀疏树结构\n\n5\\. 结论\n\n- Transformer 学不会乘法**不是容量不足**，而是标准训练目标**不鼓励建立长程依赖**\n- 提供正确归纳偏置（哪怕是任务专用的辅助损失）即可让模型在纯自回归设定下完美掌握多位数乘法\n- 为未来设计**通用、自动、硬件友好**的长程依赖机制提供了实证入口与理论线索\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Why%20Can%27t%20Transformers%20Learn%20Multiplication%3F%20Reverse-Engineering%20Reveals%20Long-Range%20Dependency%20Pitfalls，链接是https://arxiv.org/pdf/2510.00184，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.00184。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-11-1759448804169","title":"EditReward: A Human-Aligned Reward Model for Instruction-Guided Image Editing","description":"Recently, we have witnessed great progress in image editing with natural language instructions. Several closed-source models like GPT-Image-1, Seedream, and Google-Nano-Banana have shown highly promising progress. However, the open-source models are still lagging. The main bottleneck is the lack of a reliable reward model to scale up high-quality synthetic training data. To address this critical bottleneck, we built \\mname, trained with our new large-scale human preference dataset, meticulously annotated by trained experts following a rigorous protocol containing over 200K preference pairs. \\mname demonstrates superior alignment with human preferences in instruction-guided image editing tasks. Experiments show that \\mname achieves state-of-the-art human correlation on established benchmarks such as GenAI-Bench, AURORA-Bench, ImagenHub, and our new \\benchname, outperforming a wide range of VLM-as-judge models. Furthermore, we use \\mname to select a high-quality subset from the existing noisy ShareGPT-4o-Image dataset. We train Step1X-Edit on the selected subset, which shows significant improvement over training on the full set. This demonstrates \\mname's ability to serve as a reward model to scale up high-quality training data for image editing. Furthermore, its strong alignment suggests potential for advanced applications like reinforcement learning-based post-training and test-time scaling of image editing models. \\mname with its training dataset will be released to help the community build more high-quality image editing training datasets.","url":"https://huggingface.co/papers/2509.26346","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.169Z","metadata":{"authors":["Keming Wu","Sicong Jiang","Max Ku","Ping Nie","Minghao Liu","Wenhu Chen"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.26346.pdf","arxivUrl":"https://arxiv.org/abs/2509.26346","arxivId":"2509.26346","coolPaperUrl":"https://papers.cool/arxiv/2509.26346","published":"2025-09-30T14:51:04Z","updated":"2025-09-30T14:51:04.000Z","zh_summary":"最近，我们在自然语言指令驱动的图像编辑方面取得了显著进展。一些闭源模型，如 GPT-Image-1、Seedream 和 Google-Nano-Banana，已经展现出非常有前景的进展。然而，开源模型仍然落后。主要瓶颈在于缺少可靠的奖励模型来扩大高质量合成训练数据。为了解决这一关键瓶颈，我们构建了 \\mname，并用我们新的大规模人类偏好数据集对其进行了训练。该数据集由经过训练的专家按照严格的协议精心标注，包含超过 20 万对偏好对。\\mname 在指令引导的图像编辑任务中表现出与人类偏好的高度一致性。实验表明，\\mname 在 GenAI-Bench、AURORA-Bench、ImagenHub 以及我们新的 \\benchname 等已建立的基准上，实现了最先进的人类相关性，超过了广泛使用的 VLM 作为评判模型。此外，我们还使用 \\mname 从现有噪声数据集 ShareGPT-4o-Image 中筛选出高质量子集。在选定的子集上训练 Step1X-Edit，相较于在完整数据集上训练显示出显著提升。这证明了 \\mname 作为奖励模型在扩大高质量图像编辑训练数据方面的能力。此外，其高度一致性也显示了在强化学习后训练和图像编辑模型测试时扩展等高级应用中的潜力。\\mname 及其训练数据集将向社区发布，以帮助构建更多高质量的图像编辑训练数据集。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“开源指令引导图像编辑模型性能落后”这一核心瓶颈，提出瓶颈根源在于**缺乏一个与人类偏好高度对齐、可用来筛选高质量训练数据的奖励模型**。为此，工作从数据、模型、评测三个层面系统发力，旨在**为社区提供一种可扩展、可复现的手段，以大规模合成并筛选高质量图像编辑训练数据**，从而缩小开源与闭源模型之间的差距。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三大脉络，均围绕“指令引导图像编辑”与“奖励/评测模型”展开：\n\n1. 指令引导图像编辑方法\n\n- 轨迹式编辑：InstructPix2Pix、MagicBrush、Emu Edit、AnyEdit 等基于扩散模型或流匹配（Flux-Kontext）实现单句指令编辑。\n- 自回归/序列式：OmniGen2、BAGEL 等将大视觉-语言模型与生成式解码器结合，支持多轮对话式编辑。\n2. 通用视觉生成奖励与评测\n\n- 感知度量：LPIPS、FID、KID 等无需参考即可评估逼真度，但无法判断语义对齐。\n- 图文对齐：CLIP-Score、CLIP-R-Precision 等衡量文本-图像一致性，对“编辑语义”不敏感。\n- VLM-as-Judge：VIEScore、GPT-4o/5、Gemini-2.x 等利用闭源或开源多模态大模型直接打分，零样本使用，未针对编辑任务专门优化。\n3. 面向人类偏好的奖励模型\n\n- 文本到图像领域：ImageReward、HPS v3、VisionPrefer 等通过人工或模型伪标签训练奖励模型，但数据噪声大或存在模型偏差。\n- 图像编辑领域：ADIEE、HIVE、GenAI-Bench、AURORA-Bench、ImagenHub 等提供小规模人工标注或自动合成标签，缺乏大规模、细粒度、专家级偏好数据，导致奖励信号不足以支撑高质量数据筛选。\n\nQ3: 论文如何解决这个问题？\n\n论文提出“三位一体”方案，闭环解决“开源缺乏高质奖励信号”这一痛点：\n\n1. 数据层：构建 **EDITREWARD-DATA**\n\n- 规模：20 万对人工标注偏好，来源覆盖 6 大公开编辑基准 + 300 条内部难例。\n- 质量：训练后专家按双维度（Instruction Following & Visual Quality）4 分制细粒度打分，交叉校验保证一致性。\n- 多样性：对每条指令用 6 个 SOTA 模型各生成 12 张候选，随机抽 7 张进行标注，降低模型偏差。\n2. 模型层：训练 **EDITREWARD**\n\n- 架构：以 Qwen2.5-VL-7B / MiMo-VL-7B 为骨干，全参数微调；MLP 头输出双高斯分布，分别建模两个维度的均值与不确定性。\n- 损失：提出 **Multi-Dimensional Uncertainty-Aware Ranking Loss**\n– 先对两维度均值做聚合（min/avg/sum），再按 HPS v3 方式计算概率偏好，负对数似然优化。\n– 同时支持回归版本：直接拟合“两维度分数之和”的 MSE。\n- 数据增强：对“平局”样本做 **Dimensional Tie-Disentanglement**——按维度优劣拆成两条相反偏好，迫使模型学习细粒度权衡，提升 2+ 个百分点。\n3. 评测层：推出 **EDITREWARD-BENCH**\n\n- 500 组高难度多选样本，含三元/四元排序，要求一次性预测所有内部序对，评估一致性。\n- 人工三重标注 + 多数表决，保证 ground-truth 可靠。\n4. 应用闭环：奖励驱动的数据筛选\n\n- 用 EDITREWARD 对 46 K 噪声数据（ShareGPT-4o-Image）打分，取前 20 K 高质量子集微调 Step1X-Edit。\n- 结果：GEdit-Bench 整体分从 6.7→7.1，超越全量训练，验证“高质信号 > 大数据”原则，可直接用于 RLHF 或 test-time scaling。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 4 组实验，覆盖“模型对齐能力→基准横向对比→数据筛选实战→内部消融”完整证据链：\n\n1. 人类对齐主实验\n\n- 基准：GenAI-Bench、AURORA-Bench、ImagenHub、自提 EDITREWARD-BENCH\n- 指标：pair-wise 准确率 / Spearman 相关系数 / 多选一致率\n- 结果：EDITREWARD(MiMo-VL-7B) 在 GenAI-Bench 达 65.72%，显著超越 GPT-5(59.61%) 与最强开源 ADIEE(59.96%)；在 AURORA-Bench 领先 GPT-4o 12+ 分；在 EDITREWARD-BENCH 四元组任务领先 Gemini-2.5-Flash 0.4 个百分点，验证细粒度排序优势。\n2. 奖励模型即“数据监管器”实验\n\n- 设置：用 EDITREWARD 对 ShareGPT-4o-Image 4.6 万样本打分，取 Top-20 K 子集微调 Step1X-Edit\n- 评测：GEdit-Bench（EN/CN 全量 & 交集）\n- 指标：Semantic Consistency / Perceptual Quality / Overall（0-10）\n- 结果：Overall 从全量训练的 6.78 提升到 7.09，反超 Doubao(6.98)，证明“高质量信号 > 大数据量”。\n3. 消融实验\n\n- 损失函数：pair-wise 不确定排序 vs. 点回归 → 23 点 GenAI-Bench 提升\n- 聚合策略：mean > sum > min；多独立头 > 共享头\n- 数据规模：130 K → 200 K 并启用 Tie-Disentanglement，ImagenHub 提升 3.7 点\n- 骨干缩放：Qwen2.5-VL-3B → 7B → MiMo-VL-7B，性能单调上升，验证框架通用性。\n4. 偏置与稳定性分析\n\n- 位置偏置：在 GenAI-Bench 发现 Gemini-2.0-Flash 左/右偏好差距 5.11%，强调随机顺序必要性\n- 训练曲线：Tie-Disentanglement 使验证集准确率曲线更平滑，收敛更快。\n\nQ5: 有什么可以进一步探索的点？\n\n- **多轮对话式编辑奖励**\n当前仅针对单条指令的一次性编辑；将 EDITREWARD 扩展为支持多轮对话历史（I₀,P₁,I₁,P₂,I₂,…）的序列奖励，可用于强化对话编辑模型的策略优化。\n\n- **细粒度掩码奖励**\n仅输出全局分数无法定位错误区域。可在 VLM 后接入像素级 MLP 头，预测每 token 的不确定性热图，实现“哪里没编好”的可解释反馈。\n\n- **动态聚合策略学习**\n均值/最小/求和均为手工规则。可引入轻量超网络，根据指令类别或图像内容自适应地生成聚合权重，进一步提升对齐率。\n\n- **跨域与风格迁移鲁棒性**\n现有数据以真实照片为主。可引入卡通、医学、卫星等跨域编辑偏好，检验并提升奖励模型在分布外风格上的泛化能力。\n\n- **在线 RL 微调编辑模型**\n目前仅做离线数据筛选。可将 EDITREWARD 作为即时奖励接入 PPO/DPPO，对编辑扩散模型进行在线策略微调，验证其作为强化学习信号源的稳定性与样本效率。\n\n- **Test-time Scaling**\n利用奖励模型在推理阶段对多候选重新排序或指导迭代修正（类似 Best-of-N + 反思），探索“推理时算力换质量”的新范式。\n\n- **伦理与对抗样本研究**\n系统评估奖励模型对恶意指令（deepfake、隐式广告）是否给出高分，构建对抗测试集并加入安全正则，防止奖励被滥用。\n\nQ6: 总结一下论文的主要内容\n\n**EDITREWARD 核心内容速览**\n\n1. 问题\n开源指令式图像编辑模型落后，根源是**缺乏高质量、人类对齐的奖励信号**来筛选训练数据。\n\n2. 方案\n\n- **数据**：发布 20 万对专家标注偏好集 **EDITREWARD-DATA**，双维度 4 分制，覆盖 6 大基准、7 类编辑。\n- **模型**：提出 **EDITREWARD**——在 Qwen2.5-VL / MiMo-VL 上微调，输出双高斯分布刻画“指令遵循”与“视觉质量”的不确定性；用多维不确定排序损失 + Tie-Disentanglement 训练。\n- **评测**：新建多选排序基准 **EDITREWARD-BENCH**（500 组三元/四元），更严格检验一致性。\n- **应用**：用奖励对 46 K 噪声数据集打分，取 Top-20 K 微调 Step1X-Edit，GEdit-Bench 整体分从 6.7→7.1，反超 Doubao。\n\n3. 结果\n\n- 公开基准全面 SOTA：GenAI-Bench 65.72%，AURORA-Bench 63.62%，显著领先 GPT-5、GPT-4o 等。\n- 23+ 点性能提升证明**奖励框架本身**是核心贡献，非单纯大 backbone。\n- 数据、模型、 benchmark 全部开源，推动社区以“高质奖励”实现数据规模化与 RLHF。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是EditReward%3A%20A%20Human-Aligned%20Reward%20Model%20for%20Instruction-Guided%20Image%20Editing，链接是https://arxiv.org/pdf/2509.26346，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2509.26346。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1759448804171","title":"QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL","description":"Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.","url":"https://huggingface.co/papers/2510.00967","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.171Z","metadata":{"authors":["Cong Yu","Valter Uotila","Shilong Deng","Qingyuan Wu","Tuo Shi","Songlin Jiang","Lei You","Bo Zhao"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.AI","quant-ph"],"pdfUrl":"https://arxiv.org/pdf/2510.00967.pdf","arxivUrl":"https://arxiv.org/abs/2510.00967","arxivId":"2510.00967","coolPaperUrl":"https://papers.cool/arxiv/2510.00967","published":"2025-10-01T14:40:04Z","updated":"2025-10-01T14:40:04.000Z","zh_summary":"设计和优化针对特定任务的量子电路对于充分利用量子计算的优势至关重要。近年来，基于大型语言模型（LLM）的量子电路生成已经成为一种有前景的自动化解决方案。然而，根本性挑战仍未得到解决：(i) 参数化量子门需要精确的数值以实现最佳性能，这还取决于多个方面，包括量子门的数量、参数以及电路的布局/深度。(ii) 由于缺乏量子领域的专业知识，LLM往往生成低质量或不正确的量子电路。我们提出了QUASAR，这是一个基于工具增强型LLM的量子电路生成和优化的自主强化学习(RL)框架。为了使LLM与量子专用知识对齐并改进生成的量子电路，QUASAR设计了：(i) 利用外部量子模拟器的量子电路验证方法，以及(ii) RL训练中的复杂分层奖励机制。广泛的评估显示，生成的量子电路在语法和语义性能方面均有所提升。在增强4B LLM时，QUASAR在Pass@1中实现了99.31%的有效性，在Pass@10中实现了100%，优于工业级LLM如GPT-4o、GPT-5和DeepSeek-V3，以及多种仅监督微调(SFT)或仅RL的基线方法。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**大语言模型（LLM）自动生成量子汇编代码（OpenQASM 3.0）时面临的两大核心难题**：\n\n1. **参数化量子门的数值精度问题**\n量子优化算法（如 QAOA、VQE）依赖大量连续参数，LLM 难以一次性输出使电路性能最优的精确数值；参数质量直接影响门数、线路深度与最终能量期望。\n\n2. **量子语义正确性验证困难**\nOpenQASM 的合法性不仅要求语法可编译，还需满足量子语义：\n\n- 概率性测量结果与目标分布对齐\n- 在给定哈密顿量  H  下的能量期望  langlepsi|H|psirangle  接近最优\n缺乏量子领域知识的 LLM 易生成“语法对但语义错”的电路（图 1 的 a–c 情形）。\n\n为此，作者提出 **QUASAR**：一种**工具增强的 Agentic RL 后训练框架**，通过外部量子模拟器实时验证电路，并以四级分层奖励机制同时优化语法、分布对齐、能量期望与后续优化步数，使 4B 参数的 LLM 在 Pass@1 达到 99.31 % 语法正确率与 22.41 % 语义成功率，超越 GPT-4o、GPT-5 等工业级模型。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related Works”中系统梳理了与“LLM 生成量子线路”相关的三条研究脉络，并指出它们与 QUASAR 的差异。按研究主题归纳如下：\n\n- **LLM-for-Qiskit / 高阶 SDK 代码生成**\n- IBM Quantum (2025) 首次微调 LLM 得到 Qiskit Code Assistant，仅支持 Python 层 DSL，后由 Dupuis et al. (2025a) 加入量子可验证奖励做 RL 后训练，但仍绑定 Qiskit。\n- Campbell et al. (2025) 在多智能体场景下用 CoT+RAG 合成 Qiskit 线路，未涉及低层 OpenQASM。\n- **LLM-for-ansatz 设计**\n- Liang et al. (2023)、Ueda & Matsuo (2025) 用 LLM 为 VQE/QGAN 生成可变结构 ansatz，但只输出拓扑草图，不生成可执行参数化代码。\n- Gujju et al. (2025) 将 LLM 用于金融领域 QCBM 的 ansatz 指导，同样停留在 Python DSL。\n- **纯 Transformer/GPT 线路生成**\n- Apak et al. (2024) 的 KetGPT 在 OpenQASM 2.0 语料上训练，仅支持无参门，无法处理 QAOA/VQE 的连续参数。\n- Tyagin et al. (2025) 的 QAO"}},{"id":"hf-13-1759448804172","title":"Flash-Searcher: Fast and Effective Web Agents via DAG-Based Parallel Execution","description":"Large language models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks when equipped with external tools. However, current frameworks predominantly rely on sequential processing, leading to inefficient execution particularly for tasks requiring extensive tool interaction. This paper introduces Flash-Searcher, a novel parallel agent reasoning framework that fundamentally reimagines the execution paradigm from sequential chains to directed acyclic graphs (DAGs). Flash-Searcher decomposes complex tasks into subtasks with explicit dependencies, enabling concurrent execution of independent reasoning paths while maintaining logical constraints. Through dynamic workflow optimization, our framework continuously refines the execution graph based on intermediate results, effectively integrating summary module. Comprehensive evaluations across multiple benchmarks demonstrate that Flash-Searcher consistently outperforms existing approaches. Specifically, it achieves 67.7% accuracy on BrowseComp and 83% on xbench-DeepSearch, while reducing agent execution steps by up to 35% compared to current frameworks. Furthermore, when distilling this parallel reasoning pipeline into single models, we observe substantial performance gains across diverse backbone architectures, underscoring the generalizability of our methodology. Our work thus represents a significant advance in agent architecture design, offering a more scalable and efficient paradigm for complex reasoning tasks.","url":"https://huggingface.co/papers/2509.25301","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.172Z","metadata":{"authors":["Tianrui Qin","Qianben Chen","Sinuo Wang","He Xing","King Zhu","He Zhu","Dingfeng Shi","Xinxin Liu","Ge Zhang","Jiaheng Liu","Yuchen Eleanor Jiang","Xitong Gao","Wangchunshu Zhou"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.25301.pdf","arxivUrl":"https://arxiv.org/abs/2509.25301","arxivId":"2509.25301","coolPaperUrl":"https://papers.cool/arxiv/2509.25301","published":"2025-09-29T17:39:30Z","updated":"2025-09-29T17:39:30.000Z","zh_summary":"大型语言模型（LLMs）在配备外部工具时，在复杂推理任务中表现出显著能力。然而，当前的框架主要依赖顺序处理，这导致在需要大量工具交互的任务中执行效率低下。本文提出了Flash-Searcher，一种新型的并行智能体推理框架，从根本上将执行范式从顺序链条重新构想为有向无环图（DAG）。Flash-Searcher将复杂任务分解为具有明确依赖关系的子任务，从而在保持逻辑约束的同时，实现独立推理路径的并行执行。通过动态工作流优化，我们的框架能够根据中间结果持续优化执行图，并有效整合摘要模块。在多个基准测试中的全面评估表明，Flash-Searcher始终优于现有方法。具体而言，它在BrowseComp上达到67.7%的准确率，在xbench-DeepSearch上达到83%，同时将智能体执行步骤相比现有框架减少了多达35%。此外，当将这一并行推理流程蒸馏到单一模型中时，我们观察到在不同骨干架构下性能获得显著提升，彰显了我们方法的通用性。因此，我们的工作代表了智能体架构设计的一项重要进展，为复杂推理任务提供了更具可扩展性和高效性的范式。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心针对的是“复杂任务中工具调用型智能体（tool-augmented agents）普遍采用**顺序执行**导致的效率瓶颈”。具体而言，现有方法在以下两方面存在尖锐矛盾：\n\n1. **多智能体系统（MAS）**\n多角色协作带来冗余通信、链式验证与反射，交互步数动辄 20–40 步，执行时间以小时计。\n\n2. **工具集成推理（TIR）**\n把多步工具链蒸馏进单模型，仍受限于上下文窗口，长链推理效率低，难以扩展。\n\n因此，论文试图**在保持甚至提升任务精度的同时，显著压缩执行步数与端到端延迟**，使深度研究类任务具备**用户可接受的响应速度**。为此，作者提出 FLASH-SEARCHER，其关键思路是：\n\n- 把任务分解为带显式依赖的子任务，构建**动态有向无环图（DAG）** 而非线性链；\n- 通过**依赖感知的并行调度**让多条推理路径并发推进，并实时交叉验证；\n- 基于中间结果**持续优化 DAG**，提前剪枝冗余工具调用，实现“边执行边规划”。\n\n实验表明，该框架在 BrowseComp、xbench-DeepSearch、GAIA、HLE 等基准上取得 SOTA 或接近 SOTA 的准确率，同时将平均步数降低约 35%，从而**打破“精度–效率”零和困境**。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究划分为两条主线，并在第 2 章给出系统回顾。以下按“多智能体系统”与“高效框架”两大方向归纳，同时补充与 DAG/图结构推理的对比讨论（附录 B）。\n\n多智能体系统（MAS）\n\n| 代表工作 | 核心机制 | 与 FLASH-SEARCHER 的差异 |\n| --- | --- | --- |\n| CAMEL (Li et al., 2023) | 角色扮演对话，逐步推理 | 纯顺序对话，无并行调度 |\n| MetaGPT (Hong et al., 2024) | 结构化 SOP：经理→设计→编码 | 静态流水线，阶段间严格串行 |\n| ChatDev (Qian et al., 2023) | 软件开发生命周期多角色 | 同样顺序阶段，无动态 DAG |\n| MagneticOne (Fourney et al., 2024) | 中央调度器动态委派子任务 | 子任务仍顺序执行，未显式并行 |\n| AgentVerse (Chen et al., 2023) | 招募–决策–执行–评估循环 | 循环内步骤串行，全局同步点阻塞 |\n| Workforce / OWL (Hu et al., 2025) | 规划-协调-执行模块化 | 模块间顺序握手，无并发工具调用 |\n| Alita (Qiu et |"}},{"id":"hf-14-1759448804172","title":"BroRL: Scaling Reinforcement Learning via Broadened Exploration","description":"Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.","url":"https://huggingface.co/papers/2510.01180","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.172Z","metadata":{"authors":["Jian Hu","Mingjie Liu","Ximing Lu","Fang Wu","Zaid Harchaoui","Shizhe Diao","Yejin Choi","Pavlo Molchanov","Jun Yang","Jan Kautz","Yi Dong"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.LG","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.01180.pdf","arxivUrl":"https://arxiv.org/abs/2510.01180","arxivId":"2510.01180","coolPaperUrl":"https://papers.cool/arxiv/2510.01180","published":"2025-10-01T17:59:02Z","updated":"2025-10-01T17:59:02.000Z","zh_summary":"可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards，RLVR）已成为释放大语言模型复杂推理能力的关键因素。近期的工作ProRL显示，通过增加训练步骤数扩展强化学习具有潜力。然而，在数千步之后，性能趋于平稳，为额外训练分配更多计算资源带来的收益明显递减。在本工作中，我们研究了一种补充的强化学习扩展范式——BroRL，通过将每个示例的rollout次数增加到数百次，以全面拓宽探索，从而在ProRL在扩展训练步骤数时观察到的饱和点之后获得持续的性能提升。我们的方法受质量守恒方程分析的启发，使我们能够表征在强化学习过程中正确和错误token概率质量的变化率。我们证明，在一步强化学习假设下，采样的rollout token总是有助于正确概率质量的扩展，而rollout外未采样的token可能根据其分布和净奖励平衡导致收益或损失。重要的是，随着每个示例的rollout次数N增加，未采样项的影响会减小，从而确保整体正确概率质量的扩展。为了验证我们的理论分析，我们在更宽松的条件下进行了模拟，发现足够大的rollout规模N——对应足够的探索——能够保证所有正确token的概率质量增加。在经验上，BroRL使在3K ProRL训练步骤后达到饱和的模型复苏，并展示出稳健的、持续的改进，在1.5B模型的各类基准测试中达到了最先进的结果。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心关注的问题是：\n在“可验证奖励的强化学习”（RLVR）范式下，当继续沿“训练步数”维度扩展已出现性能饱和后，如何再获得持续且稳定的提升。具体而言：\n\n- ProRL 等工作表明，单纯增加训练步数在数千步后收益骤减，甚至反向下跌。\n- 作者提出并论证\\*\\*“每 prompt 的 rollout 数量 N”**是一条被忽视的互补扩展轴，通过**大幅拓宽探索（BroRL）\\*\\*来克服步数扩展带来的瓶颈。\n- 为此给出质量守恒视角的理论分析，证明增大 N 可抑制未采样 token 引入的负耦合项，保证正确 token 概率质量单调上升，从而避免知识收缩并实现稳定改进。\n\n简言之，论文试图回答：\n\n> 当 RLVR 训练因步数扩展而陷入平台期时，如何通过\\*\\*系统性增大每步探索宽度（rollout 数 N）\\*\\*来重启并持续放大模型性能，同时兼顾数据效率与硬件利用率。\n\nQ2: 有哪些相关研究？\n\n论文第 5 节“Related Work”将相关研究归为两条主线，并指出自身定位。可提炼为：\n\n1. **面向推理的 RLVR 算法**\n\n- RLHF 框架：Christiano et al. 2017；Ouyang et al. 2022\n- 针对推理任务的可验证奖励变体：\n– GRPO（Shao et al. 2024）\n– RLOO（Ahmadian et al. 2024）\n– REINFORCE++（Hu et al. 2025a）\n– DAPO（Yu et al. 2025）\n- 长思维链推理模型：DeepSeek-R1（Guo et al. 2025）、OpenAI-o3（Jaech et al. 2024）\n这些工作主要聚焦算法设计或测试时扩展，未系统研究“训练-时长”或“rollout 宽度”扩展律。\n2. **RL 训练时的扩展律研究**\n\n- ProRL（Liu et al. 2025a；Hu et al. 2025b）首次沿“训练步数”轴扩展，发现数千步后收益递减。\n- 其他开源尝试多停留在狭窄领域或仅增模型/数据规模，对“每 prompt 采样数 N”作为独立扩展轴无系统理论与实验。\n\nBroRL 在这两条线之间补缺：\n\n- 理论侧，首次将 rollout 大小 N 形式化为关键扩展维度，给出质量守恒分析；\n- 实验侧，在 ProRL 已饱和的 3K-step 检查点上继续训练，验证“拓宽探索”可稳定提升性能并提高硬件吞吐。\n\nQ3: 论文如何解决这个问题？\n\n论文从“理论-算法-系统”三条线协同解决“步数扩展饱和后如何继续提升”的问题：\n\n1. 理论：质量守恒视角的 rollout 扩展律\n\n- 建立单步 RLVR 更新对“正确 token 概率质量” Q_(pos)  的分解式\n\nDelta Q_(pos) = eta,frac{(R_c-S_R)Q_(neg)A_2+(S_R-R_w)Q_(pos)B_2N}_(采样项)ge 0 + eta,frac{S_R(Q_(pos)U_(neg),2-Q_(neg)U_(pos),2)N}_(未采样耦合项)\n\n- 证明：\n– 前两项恒非负，始终推动正确质量上升；\n– 第三项可正可负，但其系数 U_(pos),2,U_(neg),2 随 rollout 数 N 指数衰减 $mathbb E\nU_2(p)\n=p^2(1-p)^N$；\n– 因而只要  N  足够大， Delta Q_(pos)ge 0  高概率成立，彻底消除“知识收缩”。\n2. 算法：BroRL 训练配方\n\n- 在饱和的 ProRL 检查点上继续训练，但把每 prompt 采样数从  N=16  直接拉到  N=512 （乃至 51200 的模拟）。\n- 保持总步数少量增加，仅 107–191 步即可；同时按  √B_(new)/B_0  缩放学习率，维持 PPO 稳定。\n- 沿用 ProRLv2 的 clipped-PPO、REINFORCE++ 优势归一化、动态采样等模块，但用大  N  带来的“探索宽度”取代“步数深度”。\n3. 系统：GPU 级效率优化\n\n- 算法层：大  N  提高动态采样通过率（41 % → 62 %），减少被过滤的废样本。\n- 硬件层：一次生成 512 条长 CoT 样本可把 GPU 从 memory-bound 转为 compute-bound，算术强度翻倍，生成吞吐 36.5 → 72.4 samples/s。\n- 结果：同等 wall-clock 时间内，BroRL 用更少 GPU 小时却完成更多“有效样本”，实现性能与效率双赢。\n\n综合效果：\n\n- 数学、代码、推理 gym 三大基准上，ProRL 继续跑 134 h 出现停滞或下滑，而 BroRL 仅用 98 h 就把 1.5 B 模型推到新 SOTA（Math 63.03，Code 54.20，Reasoning Gym 63.09）。\n- token 级模拟亦验证：当  N  足够大时，所有正确 token 的概率单调上升，知识收缩现象消失，与理论预测完全吻合。\n\nQ4: 论文做了哪些实验？\n\n论文共设计 **token 级仿真** 与 **大模型继续训练** 两大实验板块，用以验证“增大 rollout 数 N 可抑制负耦合、重启提升”的核心假设。\n\n1. Token 级仿真（§4.1）\n目的：在极简可控环境下检验定理 1 的预测。\n\n- 设置：词汇量 128 k，其中 10 k 为“正确”token；logits 初始可带先验；奖励 ±1；用 TRPO 风格线性 surrogate 与 AdamW 更新 1000 步。\n- 变量：rollout 数 N∈{4,8,16,512,51200}，其余超参固定。\n- 观测指标：\n– 正确概率总质量 Qpos\n– 相对初始步“概率增加”的正确 token 占比\n– 所有正确 token 中的最大概率跌幅（知识收缩）\n- 结果（图 3）：\n– N 越大，Qpos 上升越快且方差越小；\n– 小 N 出现概率回撤（知识收缩），N≥512 时完全消失；\n– 与理论“未采样二阶矩随 N 指数衰减”定量吻合。\n2. LLM 继续训练实验（§4.2）\n目的：验证在真实长思维链场景下，BroRL 能否打破 ProRL 的“步数饱和”。\n2.1 实验设置\n\n- 基线：公开 ProRLv2-1.5B 模型（已训 3000 RL 步，ctx 8 k）。\n- 任务族：math（AIME/AMC/MATH/OlympiadBench）、code（APPS/CodeContests/TACO）、science、IFEval、Reasoning Gym 共 5 类。\n- 训练框架：veRL，64×H100，ctx 扩到 16 k。\n- 对照：\n– ProRL 继续：N=16，再跑 +225 / +535 步（约 56 h / 134 h）。\n– BroRL：N=512，prompt 数减至 128，再跑 +107 / +134 / +191 步（约 98 h / 123 h / 174 h）。\n- 学习率按 √(B\\_new/B\\_0) 缩放，保持 PPO mini-batch 数不变。\n\n2.2 主要结果\n\n- 轨迹对比（图 1 & 图 4）：\n– 多数任务 ProRL 后期停滞或下滑，BroRL 持续上升；\n– 配对 t 检验（>10 k 题）∆=0.33 %，p=6.5×10⁻⁷，显著优于 ProRL。\n- 最终得分（表 1,3-5）：\n– Math：62.02 → 63.03\n– Code：52.74 → 54.20\n– Reasoning Gym：61.45 → 63.09\n均在更少 GPU 小时达成。\n\n2.3 效率剖析（表 2）\n\n- 算法层：动态采样通过率 41 % → 62 %，废样减少。\n- 硬件层：生成吞吐 36.5 → 72.4 samples/s，内存瓶颈转为计算瓶颈，算术强度翻倍。\n\n综上，实验从微观（token 概率）到宏观（1.5 B 模型多基准）双重验证：\n**足够大的 rollout 数 N 既能保证理论上的正确质量单调扩张，也能在真实训练中获得更高性能、更好泛化与更高硬件利用率。**\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为对 BroRL 的直接延伸或深层扩展，均围绕“rollout 数 N 的扩展律”尚未回答的问题展开：\n\n1. **精细 scaling 曲线**\n\n- 在 16–1024 之间按 2× 梯度系统扫描 N，绘制“性能-计算”帕累托前沿，确定边际收益≈0 的临界 N\\*。\n- 引入“每美元性能”指标，结合 GPU 功耗与显存成本，给出经济最优 N。\n2. **任务-难度相关的 N 调度**\n\n- 监测在线优势方差或 SR 值，动态升降 N：难题自动放大探索，简单题回退小 N 节省算力。\n- 与课程学习结合，训练初期用大海捞针（大 N），后期逐步减小 N 做微调。\n3. **与测试时扩展协同**\n\n- 研究“训练时大 N + 测试时 k-采样”的联合 scaling law：是否存在互补或饱和边界。\n- 比较 BroRL 训得的策略在 pass@k 曲线上的斜率，相对 ProRL 是否更陡峭（即同样推理预算下更高成功率）。\n4. **理论深化**\n\n- 把单步质量守恒推广到多步 MDP，给出累积错误质量的上界与 N 的函数关系，解释何时“遗忘”仍会发生。\n- 引入正则项（KL 上限、熵奖励）后，重新推导 ∆Q\\_pos 表达式，看正则是否等效于“虚拟增大 N”。\n5. **奖励噪声与稀疏奖励场景**\n\n- 在奖励仅 0/1 且稀疏的代码生成任务上，验证大 N 是否仍能抑制方差；若失败，可结合 Rollout-to-λ、TRPO 的置信域进一步约束。\n6. **更大模型与多模态**\n\n- 将 BroRL 直接搬到 7 B/30 B 乃至 MoE 模型，观察 N 的临界值是否随参数规模线性/次线性增长。\n- 扩展到数学图形、几何题等多模态推理，验证大 N 对图像-文本交错空间探索是否同样有效。\n7. **系统级优化**\n\n- 用 vLLM、SGLang 等高性能推理框架替换当前 rollout 引擎，测试当 N≈2048 时能否再获线性加速。\n- 探索 prefix cache 与 paged attention 的极限：当 N 极大时，cache 命中率反而下降，需要新的调度策略。\n8. **负面效应与安全性**\n\n- 监控大 N 探索是否更易生成有害 CoT 或欺骗性答案；建立“安全通过率”指标，与安全强化学习联合训练。\n- 研究大 N 是否加剧奖励黑客（reward hacking）——更宽的搜索空间可能发现更多漏洞奖励。\n9. **与离线 RL 或合成数据结合**\n\n- 用离线 RL（如 DPO、XPO）把 BroRL 产生的大量 rollout 直接当作静态数据集，比较“在线大 N”与“离线大数据”两种范式效率。\n- 利用大 N rollout 过滤出高质量中间步骤，反向生成更难的“元问题”做课程数据增强。\n10. **自动超参搜索**\n\n- 以 N、学习率、clip 范围、温度为一组联合超参，用贝叶斯优化或强化学习控制器自动搜索每个任务的最优配置，减少人工调参成本。\n\n这些方向既涵盖理论深化（多步、正则、奖励噪声），也覆盖系统与经济效益（动态调度、硬件协同、安全监控），可为后续研究提供持续探索路径。\n\nQ6: 总结一下论文的主要内容\n\n论文核心主张：\n**“rollout 数 N”是一条与“训练步数”互补且更高效的 RLVR 扩展轴**——只要每 prompt 采样足够多，就能在理论层面保证正确 token 概率质量单调扩张，在实践层面打破步数饱和、提升性能与硬件吞吐。\n\n主要内容可概括为四句话：\n\n1. **问题**\nProRL 通过“加长训练步数”提升推理模型，但 3 K 步后性能停滞甚至下滑；需要新扩展维度。\n\n2. **理论**\n建立单步质量守恒公式\n\nDelta Q_(pos)=(eta) / (N)l[(R_c-S_R)Q_(neg)A_2+(S_R-R_w)Q_(pos)B_2+S_R(Q_(pos)U_(neg),2-Q_(neg)U_(pos),2)r]\n\n证明前两项恒 ≥0，而第三项（未采样耦合）随 N 指数衰减 → 大 N 高概率保证  Delta Q_(pos)ge 0 ，消除知识收缩。\n\n3. **算法与系统**\nBroRL 把每 prompt 采样从 16 提到 512，继续训练已饱和的 1.5 B 模型；配合动态采样、clip-PPO、√B 学习率缩放，使 GPU 从 memory-bound 转为 compute-bound，生成吞吐翻倍。\n\n4. **实验结果**\n\n- Token 级仿真：N≥512 时所有正确 token 概率只升不降，与理论一致。\n- 真实训练：同等 wall-clock 下，Math+1.34 pt、Code+2.20 pt、Reasoning Gym+1.80 pt，新 SOTA；ProRL 同期停滞或退步。\n\n结论：\n**性能平台并非 RLVR 的本质极限，而是探索不足导致的算法假象；大幅拓宽 rollout 是更数据-计算高效的新 scaling 法则。**\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是BroRL%3A%20Scaling%20Reinforcement%20Learning%20via%20Broadened%20Exploration，链接是https://arxiv.org/pdf/2510.01180，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.01180。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1759448804172","title":"Making, not Taking, the Best of N","description":"Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.","url":"https://huggingface.co/papers/2510.00931","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.172Z","metadata":{"authors":["Ammar Khairi","Daniel D'souza","Marzieh Fadaee","Julia Kreutzer"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.00931.pdf","arxivUrl":"https://arxiv.org/abs/2510.00931","arxivId":"2510.00931","coolPaperUrl":"https://papers.cool/arxiv/2510.00931","published":"2025-10-01T14:14:31Z","updated":"2025-10-01T14:14:31.000Z","zh_summary":"在现代大语言模型（LLM）中，获得高质量生成内容在很大程度上被视为一个选择问题：从多样的 N 个样本池中识别出单个最佳生成，即 Best-of-N（BoN）。然而，这种方法本质上是零和的，会丢弃池中多样且潜在有用的信息。相反，我们探索了一种协作设置，其中所有候选生成都有可能对最终获胜生成做出贡献。为此，我们提出了 Fusion-of-N（FusioN）：一种使用通用 LLM 评审机制，将每个样本中最有信息量的元素综合成单一最终答案的方法。我们将 FusioN 与 BoN 在两种场景下进行比较：(i) 测试时扩展，即在测试时从单个模型中采样并汇总；(ii) 合成数据生成，即融合来自多样教师模型池的样本以提升学生模型。我们在 11 种语言、3 类不同任务以及不同模型规模上对这两种设置进行了广泛基准测试。在所有基准中，FusioN 始终优于 BoN，显示出在测试时扩展和合成数据生成带来的下游收益方面的多功能性和稳健性。我们还对 FusioN 进行了深入分析，结果显示其在各种挑战环境下表现出惊人的强度和稳健性。这些结果表明，我们应当改变对 LLM 生成内容的评估和利用方式，从单一的质量衡量转向拥抱其多元特性。这一转变使我们能够整合多样优势，释放潜在潜力，并实现通过单纯选择无法获得的提升。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现代大语言模型（LLM）在“聚合多候选输出”环节普遍采用的 Best-of-N（BoN）策略所固有的零和、浪费与质量天花板问题。BoN 只选一条“最高分”回答，其余全部丢弃，导致：\n\n1. 计算浪费：生成的大量样本仅用于打分，信息未被充分利用。\n2. 多样性丢失：不同样本中互补的推理路径、事实或表达被舍弃。\n3. 奖励黑客风险：打分模型偏好的未必是真正最准确或最有用的答案。\n4. 质量上限：最终输出受限于候选池的“最好”个体，无法超越。\n\n为此，作者提出将“选”改为“融”——Fusion-of-N（FusioN）：用一个通用 LLM 作为“融合器”（fusor），把 N 条候选中的优质片段综合成一条新回答。该方法在测试时扩展（test-time scaling）与合成数据生成（synthetic data generation）两大场景下，均显著优于 BoN，且样本效率更高、对弱教师池更鲁棒，从而重新定义了如何评估与利用 LLM 的生成结果：从单块质量观转向多块协作观。\n\nQ2: 有哪些相关研究？\n\n- **Learning from Ensembles**\n- Huang et al. (2024) 通过输出概率融合多模型\n- Lee et al. (2023) 在 Self-Instruct 中利用多教师共识\n- Wan et al. (2024) 提出持续预训练目标以蒸馏多教师知识\n- **Synthesis-based Ensembling / Mixture-of-Agents (MoA)**\n- Wang et al. (2024) 提出多层代理迭代增强框架 MoA\n- Jiang et al. (2023) LLM-Blender：先 pairwise 排名再生成式融合\n- Qi et al. (2025b); Zhao et al. (2025); Li et al. (2025b) 训练专用聚合模块，聚焦数学/代码可验证任务\n- Jayalath et al. (2025) 发现单教师 rollout 融合后仍可为 RL 提供有效监督\n- **Test-time Scaling**\n- Snell et al. (2025); Brown et al. (2024) 重复采样+RM 选择\n- Yao et al. (2023) Tree-of-Thoughts；Welleck et al. (2024) 并行+串行搜索"}},{"id":"hf-16-1759448804173","title":"Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum","description":"Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.","url":"https://huggingface.co/papers/2510.00526","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.173Z","metadata":{"authors":["Gaotang Li","Ruizhong Qiu","Xiusi Chen","Heng Ji","Hanghang Tong"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.CL","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.00526.pdf","arxivUrl":"https://arxiv.org/abs/2510.00526","arxivId":"2510.00526","coolPaperUrl":"https://papers.cool/arxiv/2510.00526","published":"2025-10-01T05:17:47Z","updated":"2025-10-01T05:17:47.000Z","zh_summary":"监督微调（SFT）是大型语言模型（LLM）后训练的标准方法，但它通常表现出有限的泛化能力。我们将这一限制归因于其默认的训练目标：负对数似然（NLL）。虽然在从零开始训练时，NLL 是经典上的最优选择，但后训练则处于不同的范式，并可能违反其最优性假设，因为模型已经编码了与任务相关的先验，而监督信息可能既长又噪杂。为此，我们研究了一类通用的基于概率的目标，并在不同条件下评估它们的有效性。通过在 7 个模型骨干、14 个基准测试和 3 个领域进行全面实验和广泛消融研究，我们发现了支配目标行为的关键维度：模型能力连续体。在模型强的一端，倾向先验的目标（对低概率 token 降权的目标，例如 $-p$, $-p^{10}$, 阈值变体）始终优于 NLL；在模型弱的一端，NLL 占主导地位；在中间部分，没有单一目标胜出。我们的理论分析进一步阐明了目标如何在整个连续体中互换位置，为根据模型能力调整目标提供了原则性基础。我们的代码可在 https://github.com/GaotangLi/Beyond-Log-Likelihood 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在回答一个核心问题：\n在大型语言模型（LLM）的后训练阶段，**默认使用负对数似然（NLL）作为监督微调（SFT）目标是否仍然最优**？\n\n传统观点认为 NLL 是“万能”目标，但作者发现：\n\n1. **后训练 ≠ 从零训练**\n预训练模型已编码大量任务相关先验，且监督序列长、噪声大，NLL 的经典最优性假设被削弱。\n\n2. **模型能力存在连续谱**\n不同任务/域与预训练语料的契合度差异巨大，形成“模型–能力连续体”（model-capability continuum）：\n\n- **模型强端（MS）**：先验可靠，NLL 过度强调低概率 token 反而拖后腿。\n- **模型弱端（MW）**：先验缺失，必须靠 NLL 强制学习所有 token。\n- **中间段（MI）**：无单一目标始终占优。\n3. **需要概率型目标的“工具箱”**\n提出一族可微、单调降的概率目标  f_α(p)= (1-p^α) / (α) ，通过调节  α  连续切换“先验偏好”与“先验规避”行为，实证显示在 7 个骨干、14 个基准、3 个域上，**按模型能力选用目标可带来最高 16% 的绝对提升**。\n\n综上，论文首次系统论证了 **SFT 目标必须与模型先验强度匹配**，并给出理论-实证一体的“选目标地图”，突破了一直以来“NLL 唯一”的范式。\n\nQ2: 有哪些相关研究？\n\n论文在附录 A 与第 1–2 页对相关研究做了系统梳理，可归纳为三大主线（均给出出处，便于追踪）：\n\n1. **LLM 后训练范式与 SFT 局限**\n\n- SFT 作为轻量级对齐手段已被广泛采用（Zhang et al. 2023; Chung et al. 2024）。\n- 大量工作指出纯 SFT 易过拟合、泛化差（Howard & Ruder 2018; Dodge et al. 2020; Ouyang et al. 2022）。\n- 主流补救方案是“SFT+RLHF”两阶段配方（Bai et al. 2022; Achiam et al. 2023; Kirk et al. 2024; Chu et al. 2025），但**均未质疑 NLL 本身是否最优**。\n2. **从 RL 视角改进 SFT 目标**\n\n- 将 SFT 视为隐式奖励学习：Wang et al. 2025 用 KL-散度或更小学习率提升 DPO；Qin & Springenberg 2025 引入重要性采样；Zhu et al. 2025 用 PPO-style 裁剪目标限制策略漂移。\n- Wu et al. 2025 提出“"}},{"id":"hf-17-1759448804174","title":"On Predictability of Reinforcement Learning Dynamics for Large Language Models","description":"Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.","url":"https://huggingface.co/papers/2510.00553","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.174Z","metadata":{"authors":["Yuchen Cai","Ding Cao","Xin Xu","Zijun Yao","Yuqing Huang","Zhenyu Tan","Benyi Zhang","Guiquan Liu","Junfeng Fang"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.LG","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.00553.pdf","arxivUrl":"https://arxiv.org/abs/2510.00553","arxivId":"2510.00553","coolPaperUrl":"https://papers.cool/arxiv/2510.00553","published":"2025-10-01T06:13:50Z","updated":"2025-10-01T06:13:50.000Z","zh_summary":"大型语言模型（LLM）推理能力的最新进展在很大程度上由强化学习（RL）驱动，但RL训练过程中的参数动态仍不甚清楚。本研究识别了LLM中RL引发的参数更新的两个基本特性：（1）Rank-1主导性，即参数更新矩阵的顶级奇异子空间几乎完全决定了推理性能的提升，可恢复超过99%的性能增益；（2）Rank-1线性动态，即该主导子空间在整个训练过程中线性演化，使得可以从早期检查点进行准确预测。跨8个LLM和7种算法的广泛实验验证了这些特性的普适性。更重要的是，基于这些发现，我们提出了AlphaRL，这是一种插件式加速框架，通过利用早期训练的短窗口外推最终参数更新，实现了最高2.5倍加速，同时保持超过96%的推理性能，无需额外模块或超参数调优。这将我们的发现定位为大型RL中多功能且实用的工具，为LLM训练提供了一个有原则、可解释且高效的路径。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该工作聚焦于“大语言模型（LLM）在强化学习（RL）训练过程中的参数动力学可解释性与效率”这一核心问题。具体而言，论文试图回答以下两个紧密关联的子问题：\n\n1. **RL 引导的参数更新是否存在一致、可复现的结构性规律？**\n现有研究多聚焦于 RL 训练后的模型行为解释（post-hoc interpretability），而对训练过程中参数如何变化、变化是否遵循某种低维规律缺乏系统刻画。\n\n2. **若存在规律，能否利用其直接预测并加速 RL 训练，同时保持推理能力？**\n传统 RL 训练需完整执行长周期采样-优化流程，计算开销巨大；若能提前预测最终有效参数更新，则可显著压缩训练时间。\n\n为此，论文首次揭示了两个普适现象：\n\n- **Rank-1 Dominance**：RL 产生的参数增量矩阵 Δ𝑾 的 top-1 奇异子空间几乎完全决定推理性能提升（平均恢复 ≥99% 增益）。\n- **Rank-1 Linear Dynamics**：该 top-1 子空间在整个训练过程中呈近似线性演化，可用早期短窗口准确外推后期状态（平均 𝑅²>0.91，预测误差 <5%）。\n\n基于上述发现，作者提出 **AlphaRL**——无需额外模块或超参调优的“即插即用”加速框架，仅利用 10–40% 训练步即可外推最终 Rank-1 更新，实现 **≤2.5× 训练加速** 且 **保留 ≥96% 推理性能**。综上，论文将 RL 训练从“黑箱多步优化”转化为“可预测的低维线性轨迹”，为大规模 LLM 强化学习提供了可解释、高效的新范式。\n\nQ2: 有哪些相关研究？\n\n论文在第 5 节“Related Work”中系统梳理了与本研究直接相关的两条主线，并指出其与本工作的差异。以下按主题归纳，并补充若干代表性文献（括号内给出原文引用编号或公开信息）：\n\n1. **面向 LLM 的强化学习算法演进**\n\n- RLHF 阶段：以 PPO（Schulman et al., 2017）为代表，聚焦指令遵循与人类偏好对齐（Ouyang et al., 2022）。\n- 可验证奖励 RL（RLVR）：OpenAI o1（OpenAI, 2025）首次展示大规模数学推理能力，后续 DeepSeek-R1（DeepSeek-AI et al., 2025）提出 GRPO 与“Zero RL”范式，Kimi-K2（Team et al., 2025）、Qwen3（Yang et al., 2025）跟进。\n- 最新算法改进：DAPO（Yu et al., 2025）、VAPO（Yue et al., 2025b）、GSPO（Zheng et al., 2025）等针对采样效率、策略约束、课程奖励做进一步优化。\n**差异**：上述工作聚焦算法设计，本工作首次揭示其**共性低维动力学规律**，与具体算法正交。\n2. **RL 训练行为的可解释性与涌现现象**\n\n- 采样行为差异：Yue et al. (2025a) 发现 RL 提升 pass@1 主因是“采样效率”而非新增推理模式。\n- 熵塌缩与探索：Cui et al. (2025) 观察到早期熵快速下降导致探索能力过早丧失。\n- 高熵 token 作用：Wang et al. (2025a) 指出高熵少数 token 是推理路径分支点。\n- 遗忘与泛化：Shenfeld et al. (2025)、Feng et al. (2025) 表明 RL 相比 SFT 更少灾难性遗忘，泛化更强。\n**差异**：这些研究侧重**行为层或输出层**解释，本工作首次**直接刻画参数空间**的 Rank-1 低秩结构与线性演化，提供可预测性。\n3. **参数更新低秩性及其应用（补充背景）**\n\n- 低秩自适应：LoRA（Hu et al., 2021）**预定义**低秩分解，而本工作**事后发现**全参数 RL 更新天然近似秩 1。\n- 增量奇异值分析：Zhang (2015) 给出通用矩阵低秩近似理论，本工作将其用于解释 RL 更新。\n- 正交子空间编辑：O-edit（Cai & Cao, 2024）利用正交投影分离知识方向，本工作借用其工具验证 Rank-1 主导性。\n4. **训练轨迹预测与加速（补充背景）**\n\n- 学习率外推与元学习：前人用曲线拟合或超网络预测损失-步数曲线，但**未触及参数方向**。\n- 早期停步与热启动：通常依赖验证集损失，本工作直接**预测最终参数更新方向**，无需验证集。\n\n综上，本研究首次把“RL 训练动力学”与“低秩参数更新可预测性”两大视角结合，为现有算法提供了统一的解释框架与免费的加速插件。\n\nQ3: 论文如何解决这个问题？\n\n论文采用“现象发现 → 机理验证 → 应用落地”的三段式路线，将“RL 训练黑箱”转化为“可预测的低维线性轨迹”。具体步骤如下：\n\n1\\. 现象发现：两步拆解参数更新矩阵 Δ𝑾\n\n| 步骤 | 目的 | 关键技术 | 输出 |\n| --- | --- | --- | --- |\n| 1.1 奇异值分解 | 定位“有效方向” | 对 Δ𝑾 做 SVD： Δ𝑾=∑_(i=1)^r σ_i 𝒖_i𝒗_i^⊤  | 得到 Rank-1 近似 Δ𝑾^((1))=σ_1 𝒖_1𝒗_1^⊤  |\n| 1.2 能量归一化 | 排除幅度差异 | 令 Δ̂𝑾^((1))=α Δ𝑾^((1)),quad α=(∥Δ𝑾∥_2) / (∥Δ𝑾^((1))∥_2)  | 与完整更新同范数的 Rank-1 更新 |\n\n**结论（Property 1）**：仅将 \nΔ̂𝑾^((1))\n 加回基座模型即可恢复 ≥99% 推理性能，且该现象在 8 个模型、7 种 RL 算法、6 个推理基准上普适成立。\n\n2\\. 机理验证：Rank-1 子空间线性演化\n\n| 步骤 | 目的 | 关键技术 | 输出 |\n| --- | --- | --- | --- |\n| 2.1 轨迹提取 | 获得方向序列 | 每 checkpoint 保存 𝒖₁^{(t)}，组成模块级轨迹 U_1=𝒖1^((t))t=1^T  | 高维向量时间序列 |\n| 2.2 降维与可视化 | 检验几何形状 | PCA→50 维，再 t-SNE→2 维 | 轨迹呈“光滑直线” |\n| 2.3 线性拟合 | 量化可预测性 | 以 𝒖₁^{(t)} 为自变量、对应准确率 y^{(t)} 为因变量，做单分量 PLS 回归 | 平均 𝑅²=0.914，最高≈1 |\n\n**结论（Property 2）**：Rank-1 方向随训练步近似线性变化，可用早期窗口外推后期状态，预测误差 <5%。\n\n3\\. 应用落地：AlphaRL 插件式加速\n\n| 步骤 | 目的 | 关键技术 | 输出 |\n| --- | --- | --- | --- |\n| 3.1 幅度-方向联合预测 | 同时估计“方向+大小” | 用缩放后向量 𝒖̂^((t))=α^((t))σ_1^((t))𝒖_1^((t)) 与 y^{(t)} 做 PLS，建立 y≈β₀+β₁ 𝒖̂  | 线性模型 |\n| 3.2 目标反解 | 直接算出最终更新 | 给定目标准确率 y*，反解 𝒖̂^=(y^−β₀)/β₁ 再组合 𝒗₁ 得模块级 Rank-1 更新 | 预测的最终参数增量 |\n| 3.3 即插即用 | 零额外超参 | 不修改原 RL 算法，仅在前 10–40% 步收集数据，完成上述回归后立即停止训练 | 平均 2.5× 加速，>96% 性能保留 |\n\n4\\. 对照实验：排除非 RL 因素\n\n- **SFT / 蒸馏**：同样流程测试，发现\n– 更新范数大 1–2 个数量级；\n– Rank-1 恢复率 <70%；\n– 轨迹 𝑅² 仅 0.3–0.5。\n说明“Rank-1 主导+线性”是 RL 独有特征。\n\n5\\. 开源与复现\n\n代码、checkpoint、运行脚本已匿名开源（ [https://anonymous.4open.science/r/AlphaRL](https://anonymous.4open.science/r/AlphaRL) ），支持复现所有现象与加速效果。\n\n综上，论文通过**低秩分解**→**线性外推**→**目标反解**的简洁流水线，把原本需要完整采样-优化循环的 RL 训练压缩为“早期观测+一次回归”，在**不引入额外模块、无需调参**的前提下实现显著加速，同时保持推理能力。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“Rank-1 主导性”与“Rank-1 线性动力学”两条主线，共设计 5 组互相关联的实验，覆盖 8 个模型、7 种训练范式、6 个推理基准，并辅以可视化与消融。具体实验一览如下（均按原文章节顺序归纳）：\n\n1\\. Rank-1 主导性验证（Section 2）\n\n| 实验 | 目的 | 设置 | 关键结果 |\n| --- | --- | --- | --- |\n| 1.1 全量模型对比 | 验证“Top-1 子空间≈完整更新” | 8 模型 × 7 算法 × 6 基准（MATH-500 为主图） | Rank-1 恢复率均值 99.17%；图 3a 显示 RLOO/GRPO/DAPO 甚至 >100% |\n| 1.2 训练过程跟踪 | 主导性是否贯穿全程 | 每 5% step 保存 checkpoint，重复 1.1 流程 | 图 3b：早期略低，后期与全量曲线完全重合 |\n| 1.3 单个子空间消融 | 排除“其他奇异方向也重要” | 仅保留第 i 大奇异值子空间，i=1…10 | 图 4a：Rank-1 显著高于其余；高阶方向集体贡献仍远低于 Rank-1 |\n| 1.4 强度缩放实验 | 验证“足够即饱和” | 对 Δ̂𝑾^(1) 乘 λ∈[0.1,1] 再评估 | 图 4b：λ≈0.7 达峰值，之后边际收益趋零 |\n| 1.5 跨数据集稳健性 | 检验是否只在 MATH 有效 | 同 1.1 设置，换 AIME24/25、MINERVA、GPQA、GSM8K | 图 7：Rank-1 在 6 数据集均保持 ≥97% 相对精度 |\n\n2\\. 低秩成因分析（Section 2.3）\n\n| 实验 | 目的 | 设置 | 关键结果 |\n| --- | --- | --- | --- |\n| 2.1 更新范数与集中度 | 解释为何 RL 可用低秩 | 统计 ∥Δ𝑾∥₂ 并计算 Rank-1/1% 占总能量的比例 | 图 5a：RL 范数比 SFT/DIST 小 10–100×，但 Rank-1 能量占比 30–50% |\n| 2.2 嵌入空间漂移 | 排除“全局表示被重写” | 对同一批 token 用 PCA+t-SNE 可视化 base vs 各训练后 embedding | 图 5b & 图 8：SFT/DIST 出现整体偏移；RL 几乎无漂移 → 仅微调高层通路 |\n| 2.3 近似秩检验 | 量化 Δ𝑾 的谱衰减 | 画奇异值归一化累积曲线 | 附录图 9：RL 曲线在前 1% 奇异值即达 90%+ 能量；SFT/DIST 需 20%+ |\n\n3\\. Rank-1 线性动力学（Section 3）\n\n| 实验 | 目的 | 设置 | 关键结果 |\n| --- | --- | --- | --- |\n| 3.1 轨迹可视化 | 直观查看“是否成直线” | 每模块收集 𝒖₁^(t)，PCA→50D→t-SNE→2D | 图 6a & 图 15-28：颜色随步数渐变，轨迹光滑且呈直线 |\n| 3.2 线性拟合度 | 量化“可预测性” | 用 PLS 将 𝒖₁^(t) 对 accuracy y^(t) 回归，报告 𝑅² | 图 6b & 图 14：MLP 中高层 𝑅²≈0.96–0.99；自注意力模块普遍 0.7–0.8 |\n| 3.3 模块重要性排序 | 验证“高 𝑅²⇒高贡献” | 按 𝑅² 降序取滑动窗口，仅注入对应 Rank-1 更新 | 图 6c：窗口最小 𝑅² 越高，最终精度越高，单调关系明显 |\n\n4\\. AlphaRL 加速评估（Section 4）\n\n| 实验 | 目的 | 设置 | 关键结果 |\n| --- | --- | --- | --- |\n| 4.1 主表对比 | 看“早期+AlphaRL”能否≈“全量” | Qwen3-8B × RLOO/GRPO/DAPO，分别跑 10%、40%、100% step，再跑 10%+AlphaRL、40%+AlphaRL | 表 1：40%+AlphaRL 平均精度达全量 96–102%；10%+AlphaRL≈无加速 40% 水平 |\n| 4.2 几何题案例 | 检验生成质量是否一致 | 抽取同一几何题对比 base/全量 RL/Rank-1-AlphaRL 三段输出 | 表 2：AlphaRL 答案、数值、推理格式与全量 RL 完全一致 |\n| 4.3 加速倍数统计 | 量化 wall-clock 收益 | 记录达到全量 95% 性能所需 step 比例 | 正文：平均 2.5× 加速，最高 3.1×（DAPO on Qwen3-8B） |\n\n5\\. 行为层验证（Appendix C）\n\n| 实验 | 目的 | 设置 | 关键结果 |\n| --- | --- | --- | --- |\n| 5.1 贪婪-非贪婪 token 分析 | 看 RL 如何改变解码偏好 | 用 base 模型对 RL-CoT 逐 token 贪婪续写，标记不一致位置 | 图 11a：RL 非贪婪比例 <15%，SFT/DIST 达 40–60% |\n| 5.2 困惑度测量 | 验证分布偏移幅度 | base 模型对 RL-CoT vs 自生成 CoT 计算 PPL | 图 11b：RL-CoT PPL 几乎不变，SFT/DIST 提升 >50% |\n| 5.3 前缀引导实验 | 定位“关键 token”位置 | 用 Rank-1 模型前 n 个 token 作为前缀喂给 base，继续生成并测精度 | 图 13：仅前 20 token（≈1 句）即可让 base 精度从 45%→73%，接近完整 Rank-1 的 76% |\n\n6\\. 可重复性保障（Appendix A & Ethics）\n\n- 公开 8 模型、7 算法、6 数据集的详细超参、模板、收敛条件。\n- 所有指标均基于公开 benchmark，代码与 checkpoint 已匿名发布。\n- 统计显著性：每个基准 32 样本×温度 0.6，标准误差 <0.5%。\n\n综上，论文通过\\*\\*“低秩恢复→线性外推→加速落地→行为对照”\\*\\*四层实验，系统证明了 Rank-1 主导与线性动力学在 LLM-RL 训练中的普适性，并展示 AlphaRL 的实际加速效果。\n\nQ5: 有什么可以进一步探索的点？\n\n以下可探索方向按“理论深化 → 方法扩展 → 场景迁移 → 监控与调控”四级递进，均直接承接论文结论，可作为后续工作清单：\n\n1\\. 理论深化：从经验规律到可证明命题\n\n- **低秩源起**\n- 建立“KL-正则化 → 梯度协方差谱衰减”显式界，证明在 KL 球约束下 Hessian 近似低秩 ⇒ Δ𝑾 近似低秩。\n- **线性演化**\n- 将 Rank-1 方向视为动力系统慢流形（slow manifold），用奇异摄动理论证明其线性吸引子性质；给出步长→斜率 𝑅² 下界。\n- **通用性边界**\n- 探讨“任务可验证性 + 奖励稀疏性”是否为 Rank-1 主导的必要条件；若奖励密集或连续，是否转向高秩？\n\n2\\. 方法扩展：突破线性 + 单峰预测\n\n- **非线性外推**\n- 用核回归或神经 ODE 对 𝒖₁^(t) 轨迹建模，处理可能的“饱和-拐点”现象，提高 5× 以上加速比。\n- **多奇异值协同**\n- 以 Rank-k 子空间为状态变量，建立向量自回归（VAR）模型，兼顾高阶奇异值在困难任务（定理证明、几何题）上的贡献。\n- **元学习初始化**\n- 在多款基座模型上预训练“Rank-1 斜率预测器”，新模型直接读取初始 5% 轨迹即可输出终止点，实现“零-shot 加速”。\n\n3\\. 场景迁移：走出数学推理\n\n- **代码生成 & 工具调用**\n- 验证 Rank-1 主导是否成立；若代码任务需多跳抽象，是否出现 Rank-2-3 等“逻辑块”结构。\n- **多模态 RLVR**\n- 图文交错推理（MathVista、MMMU）下，视觉编码器与 LLM 的 Δ𝑾 是否共享同一 Rank-1 方向，或呈“块对角低秩”。\n- **大尺度模型**\n- 在 100B+ 参数模型上检验低秩假设；若批量增大导致噪声梯度升高，探讨微-batch 与梯度累积对谱衰减的影响。\n- **在线/对抗环境**\n- 将 AlphaRL 嵌入多智能体博弈或网页浏览任务，观察非稳态奖励是否破坏线性动力学。\n\n4\\. 监控与调控：把 Rank-1 变成“旋钮”\n\n- **早期预警**\n- 实时监测 𝒖₁^(t) 斜率突变或 𝑅² 骤降，作为奖励黑客或过度拟合的信号，自动触发 KL 正则系数调整。\n- **混合高阶修正**\n- 设计“Rank-1 主导 + 高阶残差”双通道更新：用 AlphaRL 快速逼近 95% 性能，再以小学习率补全剩余子空间，兼顾效率与细微改进。\n- **逆向优化**\n- 把目标 Rank-1 向量设为可优化变量，通过可微 SVD 或隐式微分反向推导“最优奖励函数”，实现奖励设计自动化。\n- **遗忘-保留控制**\n- 在持续学习场景，用 Rank-1 子空间与原始子空间角度作为遗忘度量，动态加入正交约束，实现任务增量学习而不覆盖旧能力。\n\n5\\. 系统级优化\n\n- **GPU 通信降维**\n- 只广播 Rank-1 向量（float16 下 ≈ 2× 模型维度字节），完成全局平均后再本地展开，减少数据并行 10× 通信量。\n- **量化与稀疏化**\n- 将 Rank-1 外积进一步量化为 4-bit 或引入稀疏掩码，验证是否保持 95% 性能，实现“极限压缩”增量更新。\n\n6\\. 交叉领域启示\n\n- **扩散模型微调**\n- 探讨扩散式文本-到-图像模型在 RL 微调（如人类美学奖励）是否同样出现卷积核 Δ𝑾 的 Rank-1 主导。\n- **强化学习控制**\n- 在连续控制任务（MuJoCo、Dexterous Manipulation）观察策略网络更新矩阵，验证低秩-线性现象是否普遍存在于高维策略空间。\n\n综上，从“证明为何低秩”到“把 Rank-1 当旋钮”，再到“走出 NLP、走向多模态与系统级”，均可在本文框架上直接延伸，形成一条“理论–方法–系统–应用”完整研究路线图。\n\nQ6: 总结一下论文的主要内容\n\n**论文主旨**\n揭示大语言模型（LLM）在强化学习（RL）训练中的参数更新规律，并据此提出“即插即用”加速框架 AlphaRL，实现**可解释、可预测、高效率**的 RL 训练。\n\n1\\. 核心发现（两条“定律”）\n\n| 定律 | 内容 | 数据支撑 |\n| --- | --- | --- |\n| Rank-1 Dominance | 参数增量矩阵 Δ𝑾 的 top-1 奇异子空间几乎完全决定推理性能提升 | 8 模型 × 7 算法 × 6 基准：平均恢复 99.17% 精度 |\n| Rank-1 Linear Dynamics | 该子空间方向随训练步近似线性演化，可早期外推 | 平均 𝑅² = 0.914；预测误差 < 5% |\n\n2\\. 方法框架（AlphaRL）\n\n1. **早期观测**：仅用 10–40% 训练步收集 (𝒖₁, σ₁, α) 与对应精度 y。\n2. **线性拟合**：建立 𝒖̂ = ασ₁𝒖₁ 与 y 的单分量 PLS 模型。\n3. **目标反解**：给定目标准确率 y\\*，直接算出所需 𝒖̂\\*，组合 𝒗₁ 得到最终 Rank-1 更新。\n4. **停止训练**：一次性注入预测更新，无需后续采样-优化循环。\n\n**效果**：平均 **2.5× 加速**，保留 **>96%** 全量性能；零额外模块、零超参调优。\n\n3\\. 实验规模\n\n- **模型**：7B–32B，涵盖 Qwen2.5、Qwen3 等 8 款。\n- **算法**：PPO、RLOO、GRPO、Dr.GRPO、DAPO、SFT、蒸馏。\n- **基准**：AIME24/25、MATH-500、MINERVA、GPQA、GSM8K。\n- **消融**：逐子空间、逐训练步、逐模块、逐强度 λ 系统验证。\n\n4\\. 关键结论\n\n- RL 更新天然**低秩且线性**，与 SFT/蒸馏的“高秩-非线性”形成鲜明对比。\n- 复杂多步优化可被**单秩方向+线性外推**近似，挑战“黑箱”认知。\n- AlphaRL 与现有算法正交，可**叠加**其他加速手段，为大规模 RL 提供“免费午餐”。\n\n5\\. 一句话总结\n\n本文首次证明“LLM 的 RL 训练 ≈ 一条可预测的 Rank-1 直线”，并基于该定律实现**即插即用、2.5× 加速、性能无损**的 AlphaRL 框架，为大规模强化学习带来可解释且高效的新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是On%20Predictability%20of%20Reinforcement%20Learning%20Dynamics%20for%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2510.00553，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.00553。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-18-1759448804174","title":"Infusing Theory of Mind into Socially Intelligent LLM Agents","description":"Theory of Mind (ToM)-an understanding of the mental states of others-is a key aspect of human social intelligence, yet, chatbots and LLM-based social agents do not typically integrate it. In this work, we demonstrate that LLMs that explicitly use ToM get better at dialogue, achieving goals more effectively. After showing that simply prompting models to generate mental states between dialogue turns already provides significant benefit, we further introduce ToMAgent (ToMA), a ToM-focused dialogue agent. ToMA is trained by pairing ToM with dialogue lookahead to produce mental states that are maximally useful for achieving dialogue goals. Experiments on the Sotopia interactive social evaluation benchmark demonstrate the effectiveness of our method over a range of baselines. Comprehensive analysis shows that ToMA exhibits more strategic, goal-oriented reasoning behaviors, which enable long-horizon adaptation, while maintaining better relationships with their partners. Our results suggest a step forward in integrating ToM for building socially intelligent LLM agents.","url":"https://huggingface.co/papers/2509.22887","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.174Z","metadata":{"authors":["EunJeong Hwang","Yuwei Yin","Giuseppe Carenini","Peter West","Vered Shwartz"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2509.22887.pdf","arxivUrl":"https://arxiv.org/abs/2509.22887","arxivId":"2509.22887","coolPaperUrl":"https://papers.cool/arxiv/2509.22887","published":"2025-09-26T20:07:34Z","updated":"2025-09-26T20:07:34.000Z","zh_summary":"心智理论（Theory of Mind, ToM）——对他人心理状态的理解——是人类社会智能的一个关键方面，然而，聊天机器人和基于大型语言模型（LLM）的社交代理通常并不整合这一能力。在本研究中，我们展示了明确使用心智理论的LLM在对话中表现得更好，更有效地实现目标。在展示了仅仅通过提示模型在对话轮次之间生成心理状态就能带来显著益处之后，我们进一步引入了ToMAgent（ToMA），一个以心智理论为核心的对话代理。ToMA通过将心智理论与对话前瞻结合训练，以产生对实现对话目标最有用的心理状态。在Sotopia互动社交评估基准上的实验表明，我们的方法在多种基线条件下都有效。综合分析显示，ToMA表现出更具策略性、目标导向的推理行为，从而实现长远适应，同时与其对话伙伴保持更好的关系。我们的研究结果表明，在构建具有社会智能的LLM代理方面，整合心智理论是向前迈出的一步。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在回答“如何为大型语言模型（LLM）注入可真正提升其社交推理能力的 Theory of Mind（ToM）”这一核心问题。具体而言，作者观察到：\n\n- 尽管 ToM 是人类社交智能的关键，现有基于 LLM 的聊天机器人或社交代理通常缺乏显式的他人心理状态建模；\n- 仅通过通用推理基准或端到端对话优化，难以让模型在真实社交互动中兼顾目标达成、关系维系与规范遵守；\n- 过去对 LLM 的 ToM 研究多停留在静态问答评测，未验证其在动态、多轮、目标导向对话中的实际效用。\n\n为此，论文提出 ToMAgent（TOMA）框架，通过“先显式生成自我与他人的信念、意图、情绪等心理状态，再以对话前瞻模拟筛选高价值状态–话语对”进行监督微调，使模型在互动中具备战略性与长程适应性。实验在开放式社交评测环境 Sotopia 上表明，TOMA 相对强基线平均提升 6.6–18.9%，验证了显式 ToM 建模对构建“社交智能 LLM 代理”的必要性与有效性。\n\nQ2: 有哪些相关研究？\n\n相关研究可分为三大类，均围绕“大模型是否具备、如何获得、怎样利用 Theory of Mind”展开：\n\n1. LLM 的 ToM 能力评估与增强\n\n- 评估：Kosinski (2023, 2024)、Strachan et al. (2024) 在经典错误信念任务上报告 LLM 表现接近人类；Ullman (2023)、Shapira et al. (2024)、Nickel et al. (2024) 则发现模型对微小扰动极其脆弱，结论为“表面化”。\n- 增强：Wilf et al. (2024)、Shinoda et al. (2025) 用“换位思考”链式提示提升问答准确率；Sclar et al. (2023) 把 LLM 与符号信念追踪器耦合；Ying et al. (2023) 采用贝叶斯逆向规划；Kim et al. (2025) 在推理时生成多假设并投票。这些工作均聚焦静态问答，而非交互式社交对话。\n2. 社交对话环境与代理训练\nSotopia (Zhou et al. 2024) 提供开放式场景与多维度评价，后续工作如 Sotopia-π (Wang et al. 2024)、SDPO (Kong et al. 2025)、Sotopia-RL (Yu et al. 2025) 用强化学习或直接偏好优化让代理最大化目标得分，但均未显式建模对话双方的心理状态，"}},{"id":"hf-19-1759448804174","title":"GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness","description":"Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.","url":"https://huggingface.co/papers/2510.00536","source":"HuggingFace Papers","timestamp":"2025-10-02T23:46:44.174Z","metadata":{"authors":["Kung-Hsiang Huang","Haoyi Qiu","Yutong Dai","Caiming Xiong","Chien-Sheng Wu"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.00536.pdf","arxivUrl":"https://arxiv.org/abs/2510.00536","arxivId":"2510.00536","coolPaperUrl":"https://papers.cool/arxiv/2510.00536","published":"2025-10-01T05:37:54Z","updated":"2025-10-01T05:37:54.000Z","zh_summary":"基于视觉-语言模型构建的图形用户界面（GUI）代理已经成为自动化人机工作流程的一种有前景的方法。然而，它们也面临效率挑战，因为需要处理高分辨率截图的长序列并完成长时间跨度的任务，使得推理过程缓慢、成本高且受内存限制。虽然键值（KV）缓存可以缓解这一问题，但在图像密集型场景下存储完整缓存是不现实的。现有的缓存压缩方法并不理想，因为它们未考虑GUI的空间和时间冗余。在本工作中，我们首先分析了GUI代理工作负载中的注意力模式，发现与自然图像不同，在所有Transformer层中注意力稀疏性均保持较高。这一发现启发了我们采用简单的均匀预算分配策略，并通过实验证明其优于更复杂的层间变化方案。在此基础上，我们提出了GUI-KV，一种适用于GUI代理的即插即用KV缓存压缩方法，无需重新训练。GUI-KV结合了两项创新技术：（i）空间显著性引导，通过将隐藏状态的L2范数与注意力得分结合，更好地保留语义重要的视觉符号；（ii）时间冗余评分，将前一帧的键投影到当前帧的键子空间，以优先剪枝冗余历史。在标准GUI代理基准测试和模型上，GUI-KV优于竞争性的KV压缩基线，在适度的预算下准确率接近全缓存性能。值得注意的是，在AgentNetBench基准的5张截图设置中，GUI-KV在比全缓存基线增加4.1%步骤准确率的同时，将解码FLOPs减少了38.9%。这些结果表明，利用GUI特定的冗余信息可以实现高效且可靠的代理性能。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对基于视觉-语言模型（VLM）的图形用户界面（GUI）智能体在推理阶段面临的**高计算与内存开销**问题，提出了一种无需重训、即插即用的 KV 缓存压缩方法 GUI-KV。核心待解决问题可归纳为：\n\n1. **长序列高分辨率截图导致 KV 缓存膨胀**\n多步交互需要保留历史屏幕与动作信息，缓存大小随截图数量线性增长，易触发显存溢出（如 UI-TARS-1.5-7B 在 5 张截图、50 步场景下峰值占用 >80 GB）。\n\n2. **现有 KV 压缩策略在 GUI 场景失效**\n\n- 自然图像的“层间稀疏度差异大”假设在 GUI 屏幕不成立：注意力稀疏度在所有 Transformer 层均 >0.99，呈极端平坦分布。\n- 基于层间稀疏度差异动态分配缓存预算的方法（PyramidKV、VL-Cache）会过度放大微小差异，造成预算错配。\n3. **GUI 屏幕存在显著的时空冗余却未被利用**\n\n- 空间：大面积纯色背景、重复 UI 元素。\n- 时间：相邻截图视觉重叠高，仅局部区域变化。\n\n为此，论文首先通过注意力稀疏度分析得出“**均匀预算分配优于层变预算**”的结论，进而提出 GUI-KV，利用两项机制针对性剔除冗余 token：\n\n- **空间显著性引导**：以视觉 token 隐藏状态 L2 范数量化“信息载荷”，补充注意力分数，保留语义显著区域。\n- **时间冗余评分**：对当前帧 key 做 QR 分解得到子空间，将历史帧 key 投影其上，残差越小越冗余，优先淘汰。\n\n实验表明，GUI-KV 在 6 个 GUI 基准、2 个 VLM 上，仅用 10–20% 缓存即可逼近全缓存精度，解码 FLOPs 降低 38.9%，同时 step 准确率提升 4.1%，实现**显存-精度双优化**。\n\nQ2: 有哪些相关研究？\n\n与 GUI-KV 直接相关的研究可划分为两条主线：\n\n1. **视觉 Token 压缩**（降低输入侧计算量）\n2. **KV 缓存压缩**（降低推理侧内存与计算量）\n\n以下按类别列举代表性工作，并指出其与 GUI 场景的适配差异。\n\n1\\. 视觉 Token 压缩（Vision Token Compression）\n\n| 方法 | 核心思想 | 与 GUI 场景的差异 |\n| --- | --- | --- |\n| LLaVA-PruMerge (Shang et al., 2024) | 基于注意力重要性动态合并视觉 token | 未考虑 GUI 屏幕的空间同质性，合并策略对大面积纯色区域不敏感 |\n| MADTP (Cao et al., 2024) | 多模态对齐 |"}}]}