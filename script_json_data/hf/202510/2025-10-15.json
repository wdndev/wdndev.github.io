{"timestamp":"2025-10-14T23:51:35.666Z","totalItems":20,"items":[{"id":"hf-0-1760485638276","title":"QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs","description":"We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.","url":"https://huggingface.co/papers/2510.11696","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.276Z","metadata":{"authors":["Wei Huang","Yi Ge","Shuai Yang","Yicheng Xiao","Huizi Mao","Yujun Lin","Hanrong Ye","Sifei Liu","Ka Chun Cheung","Hongxu Yin","Yao Lu","Xiaojuan Qi","Song Han","Yukang Chen"],"date":"","rank":1,"llmAnalysis":"","categories":["cs.LG","cs.CL","cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.11696.pdf","arxivUrl":"https://arxiv.org/abs/2510.11696","arxivId":"2510.11696","coolPaperUrl":"https://papers.cool/arxiv/2510.11696","published":"2025-10-13T17:55:09Z","updated":"2025-10-13T17:55:09.000Z","zh_summary":"我们提出了 QeRL，一种针对大语言模型（LLMs）的量化增强强化学习框架。虽然强化学习对于 LLMs 的推理能力至关重要，但其资源消耗巨大，需要大量 GPU 内存和长时间的 rollout。QeRL 通过将 NVFP4 量化与低秩适配（LoRA）相结合，解决了这些问题，在降低内存开销的同时加速了 RL 的 rollout 阶段。除了提高效率之外，我们的研究表明，量化噪声会增加策略熵，从而增强探索能力，并在强化学习过程中发现更优策略。为了进一步优化探索，QeRL 引入了自适应量化噪声（AQN）机制，可在训练过程中动态调整噪声。实验表明，QeRL 在 rollout 阶段可实现超过 1.5 倍的加速。此外，这是首个能够在单个 H100 80GB GPU 上进行 32B LLM 强化学习训练的框架，同时在强化学习训练中实现总体加速。它在奖励增长速度和最终准确率上也超过了 16-bit LoRA 和 QLoRA，并且在数学基准测试（如 7B 模型的 GSM8K 90.8% 和 MATH 500 77.4%）上与全参数微调性能相当。这些结果确立了 QeRL 作为 LLMs 强化学习训练的高效且有效的框架。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决大语言模型（LLM）在强化学习（RL）训练阶段面临的**高资源消耗与低 rollout 效率**这一核心矛盾，并进一步挖掘**量化噪声在 RL 中的正向价值**。具体可归纳为以下三点：\n\n1. **资源瓶颈**\n\n- RL 需同时加载策略、参考等多个 16-bit 模型副本，显存占用极高；\n- rollout 阶段反复采样长序列，计算量大，导致训练周期长。\n2. \\*\\* rollout 速度瓶颈\\*\\*\n\n- 现有 QLoRA 采用 NF4 量化，反量化查表操作使矩阵乘法效率低， rollout 反而比 16-bit LoRA 慢 1.5–2×；\n- 8-bit 量化方案（FlashRL）需同时维护 8-bit 与 16-bit 两份权重，显存翻倍。\n3. **探索不足**\n\n- 传统观点认为量化噪声有害，但论文发现**静态量化误差可等效为参数噪声**，提升策略熵，增强探索；\n- 然而静态噪声无法随训练阶段动态调整，难以兼顾“早期多探索、后期多利用”。\n\n为此，作者提出 **QeRL** 框架：\n\n- 用 **NVFP4** 量化权重 + **LoRA** 微调，在单卡 80 GB H100 上完成 32B 模型的 RL 训练；\n- 设计 **Adaptive Quantization Noise (AQN)**，通过指数衰减调度动态注入通道级高斯噪声，将静态量化误差转化为可控探索信号；\n- 将噪声合并至 RMSNorm 缩放参数，实现**零额外显存开销**。\n\n实验结果显示，QeRL 在 GSM8K、MATH500 等数学推理基准上**达到全参数微调精度**，同时 rollout 阶段提速 **1.5× 以上**，显存降至 25 %–30 %，首次在单卡上完成 32B 模型的 RL 训练。\n\nQ2: 有哪些相关研究？\n\n相关研究可划分为四大类，均围绕“LLM 高效 RL”“量化”“参数高效微调”与“噪声探索”展开。以下按类别列出代表性工作，并指出与 QeRL 的关联与差异。\n\n1\\. 面向 LLM 的强化学习框架\n\n| 工作 | 核心贡献 | 与 QeRL 的关系 |\n| --- | --- | --- |\n| DeepSeek-R1 (DeepSeek-AI, 2025) | 纯 RL 激发推理能力，提出 GRPO 算法 | QeRL 直接以 GRPO/DAPO 为优化算法，但解决其 rollout 显存与速度瓶颈 |\n| DAPO (Yu et al., 2025) | 去 KL 惩罚、token 级损失，缓解熵塌 | QeRL 训练 7–32 B 模型时采用 DAPO，证明量化噪声可替代部分熵正则化 |\n| FlashRL (Liu et al., 2025a) | 8-bit 权重 rollout + 16-bit logits，用重要性采样校正 | 需同时存 8/16-bit 两份权重，显存不降反增；QeRL 用单份 NVFP4 权重，无需校正 |\n| Tina (Wang et al., 2025) | 小模型 LoRA-RL 也能推理 | 仅验证 LoRA 参数高效，未解决 rollout 慢与显存占用问题 |\n\n2\\. 超低比特量化与高效推理\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n3\\. 参数高效微调（PEFT）\n\n| 工作 | 方案 | 与 QeRL 的结合点 |\n| --- | --- | --- |\n| LoRA (Hu et al., 2022) | 低秩增量 ∆W=BA | QeRL 冻结 4-bit 主权重，仅训练 LoRA 增量，显存降至 25 % |\n| DoRA (Liu et al., 2024) | 方向-幅度分解 | 可与 NVFP4 正交，未来可替换 LoRA 部分 |\n| LongLoRA (Chen et al., 2024b) | 稀疏局部注意力+LoRA | 聚焦长文本 SFT；QeRL 证明量化+LoRA 同样适用于长 rollout RL |\n\n4\\. 噪声驱动的 RL 探索\n\n| 工作 | 噪声类型 | 与 AQN 的对比 |\n| --- | --- | --- |\n| Parameter Noise (Plappert et al., 2017) | 高斯噪声注入参数 | 需额外缓存噪声张量；AQN 把噪声合并到 RMSNorm，零开销 |\n| Pink Noise (Eberhard et al., 2023) | 有色噪声提升探索 | 需在线采样；AQN 直接复用量化误差+可调高斯，调度更简单 |\n| NoisyNet (Fortunato et al., 2018) | 线性层变分噪声 | 训练时增加参数量；AQN 不改变参数量，仅改 RMSNorm 缩放 |\n\n小结\n\nQeRL 在算法层继承 GRPO/DAPO，在系统层借助 NVFP4 硬件指令与 Marlin 内核，在探索层首次**将“量化误差”显式转化为“可控噪声”**，并与 LoRA 参数高效微调无缝集成，从而同时解决**显存、速度、探索**三大痛点，与上述研究形成互补。\n\nQ3: 论文如何解决这个问题？\n\n论文通过 **QeRL** 框架把“量化-效率”与“噪声-探索”两条看似独立的路径合并为同一机制，一次性解决 RL 训练 LLM 时的**显存爆炸、rollout 慢、探索不足**三大痛点。具体实现分为三步：\n\n1\\. 用 NVFP4 量化权重 → 显存↓、rollout 速度↑\n\n- **格式**：权重存成 4-bit NVFP4（E4M3 缩放因子，16 元素一块），激活/梯度仍保持 16-bit。\n- **内核**：基于 Marlin 的 `NVFP4×BF16` 矩阵乘，Hopper/Blackwell GPU 上原生支持，无查表反量化开销。\n- **效果**：\n- 7 B 模型权重从 15.2 GB 压缩到 5.9 GB（≈ 39 %）；\n- rollout 阶段 token 吞吐提升 **1.3×–2.0×**，batch 越大优势越明显；\n- 首次在单卡 80 GB H100 上完成 **32 B 模型 GRPO 训练**（表 8）。\n\n2\\. 把“量化误差”变成“免费探索信号” → 奖励曲线更快\n\n- **观察**：量化引入的静态误差 ∆ε = ˆW − W 相当于参数空间噪声，使策略熵\n\nH(π)=−∑_v π_v log π_v\n\n平均提高 10 %–20 %（图 5）。\n- **验证**：在 GSM8K/BigMath 上，仅使用静态 NVFP4 噪声的 LoRA 已比 16-bit LoRA 收敛快 **200 vs 500 steps**（图 7）。\n\n3\\. 引入 Adaptive Quantization Noise (AQN) → 噪声可调度、零参数开销\n\n- **动态调度**：\n每轮重新采样通道级高斯噪声\n\nZ_(noisy) sim mathcal N(0,σ^2 I),\n\n并按指数衰减\n\nσ(k)=σ_(start)!(σ_(end)σ_(start))^((k-1) / (K-1))\n\n实现“早期大噪声、后期小噪声”自动过渡。\n- **零开销实现**：\n利用\n\nRMSNorm_(noise)(x)=(w+Z_(noise))odot x{√frac 1 N ∑ x_i^2+δ}\n\n把加性噪声等价转为**乘性权重噪声**，无需额外缓存，也不破坏 NVFP4 内核（公式 10–12）。\n- **效果**：\n- 在 3 B/7 B 模型上，+AQN 再提升 GSM8K **+0.4–2.3 个百分点**，最终 7 B 达到 **90.8 %**，**超越 16-bit LoRA** 并**追平全参数微调**（表 1）。\n\n端到端收益总结\n\n| 维度 | 16-bit LoRA | QLoRA (NF4) | QeRL (NVFP4+AQN) |\n| --- | --- | --- | --- |\n| 显存 | 100 % | ≈ 35 % | ≈ 30 % |\n| rollout 速度 | 1.0× | 0.7–0.8× | 1.5–2.0× |\n| 收敛步数 | 500+ | 500+ | ≈ 200 |\n| 7B GSM8K | 88.1 % | 85.0 % | 90.8 % |\n| 32B 单卡训练 | ❌ OOM | ❌ OOM | ✅ 首成 |\n\n通过“**4-bit 量化解决资源瓶颈**”与“**可控噪声解决探索瓶颈**”的协同设计，QeRL 把原本被视为副产物的量化误差转化为**免费、可调、零参数的探索信号**，在效率与效果上同时超越 16-bit LoRA 与 QLoRA。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **效率** 与 **效果** 两条主线，在 **3 B → 32 B** 四个规模、**GSM8K / BigMath** 两大数学数据集、**GRPO / DAPO** 两种 RL 算法上共完成 **7 组核心实验 + 4 组消融实验 + 3 组效率基准测试**，覆盖准确率、收敛速度、显存占用、吞吐率、单卡可行性等多维度指标。\n\n1\\. 主实验：数学推理准确率对比\n\n| 模型 | 数据 | 算法 | 对比基准 | 主要结果 |\n| --- | --- | --- | --- | --- |\n| Qwen2.5-3B | GSM8K | GRPO | BF16 LoRA / Full / QLoRA | NVFP4+AQN 83.7 %，+7.6 优于 QLoRA，-0.7 追平 Full |\n| Qwen2.5-7B | GSM8K | GRPO | 同上 | 90.8 %，+2.7 优于 BF16 LoRA，+5.8 优于 QLoRA |\n| Qwen2.5-7B | BigMath | DAPO | 同上 | 平均得分 36.4，+0.7 优于 BF16 LoRA |\n| Qwen2.5-14B | BigMath | DAPO | 同上 | 平均 42.0，+1.8 优于 LoRA；AMC23 57.5 反超 Full 55.0 |\n| Qwen2.5-32B | BigMath | DAPO | 同上 | 平均 45.6，+3.3 优于 LoRA；单卡 80 GB 训练首成 |\n\n2\\. 收敛速度实验\n\n- **图 7 & 图 12–13**：相同超参下，QeRL 在 200 steps 内奖励升至 0.6，**16-bit LoRA 需 500+ steps**；32 B 模型差距缩小，但仍持续领先。\n\n3\\. 策略熵与探索分析\n\n- **图 5 & 图 14**：量化模型 rollout 熵全程高于 16-bit，**AQN 保持熵值不降**，验证“噪声→探索”假设。\n\n4\\. 消融实验\n\n| 消融变量 | 设置 | 结论 |\n| --- | --- | --- |\n| AQN 开关 | w/ vs w/o 动态噪声 | 图 8：+AQN 在收敛平台期仍能再涨 3–5 个百分点 |\n| 噪声调度器 | linear / cosine / log / exponential | 图 9：exponential decay 后期更稳定，最终选用 |\n| LoRA rank | 16, 32, 64, 128 | 图 10：rank=16 已饱和，更高 rank 无显著收益 |\n| 学习率 | 3e-5 vs 5e-6 | 图 16–17：16-bit 模型在 3e-5 下崩溃，QeRL 仍稳定且提速 2× |\n\n5\\. 效率基准\n\n| 指标 | 测试条件 | 结果 |\n| --- | --- | --- |\n| 显存占用 | 7 B / 14 B / 32 B | 权重压缩至 25 %–30 %；32 B 从 62.3 GB → 20.7 GB |\n| rollout 吞吐 | vLLM batch=1, 256→2048 | 14 B 2.1×；32 B 2.2×（图 11） |\n| 端到端步速 | GRPO 全链路计时 | 7 B 1.5×；14 B 1.4×；32 B 2.0×（表 3, 5–8） |\n| 单卡可行性 | 32 B 模型 80 GB H100 | QLoRA & BF16 OOM；QeRL 成功训练（表 8） |\n\n6\\. 跨精度格式对比\n\n- **图 4 & 表 1–2**：同 rank=32，**NVFP4 > MXFP4 > NF4 > BF16 LoRA**，验证 NVFP4 格式本身在 RL 场景下的优势。\n\n7\\. 其他鲁棒性测试\n\n- **长序列生成**：max 8192 tokens，QeRL 加速比随长度增大而扩大（附录 J）。\n- **不同 GPU 利用率**：0.2–0.45 区间加速比稳定，排除内存带宽瓶颈假象。\n\n实验覆盖度小结\n\n| 维度 | 覆盖情况 |\n| --- | --- |\n| 模型规模 | 3 B / 7 B / 14 B / 32 B |\n| 数据集 | GSM8K (7.5 k) / BigMath (122 k) |\n| 算法 | GRPO + DAPO |\n| 评价指标 | Pass@1 准确率、奖励曲线、熵值、吞吐、显存、步速 |\n| 消融变量 | AQN、调度器、rank、学习率 |\n| 硬件场景 | 单卡 80 GB H100 → 32 B 首成；8 卡用于大 batch 生产训练 |\n\n以上实验共同证明：**QeRL 在 4-bit 显存水平下，同时实现 >1.5× 训练提速、更快收敛、持平或超越 16-bit LoRA 的最终精度**，并首次给出 32 B 模型单卡 RL 训练的可行方案。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 QeRL 的“直接外延”或“深层机理”问题，既包含工程放大，也包含理论深挖，均具备可验证的实验入口。\n\n1\\. 规模外推与系统极限\n\n| 课题 | 关键问题 | 可验证指标 |\n| --- | --- | --- |\n| 70 B+ 模型 | 量化噪声是否仍保持探索优势？ | 在 70 B / 110 B 上复现单卡/双卡 RL 训练可行性；记录 reward 曲线、显存墙位置。 |\n| 更长上下文 | 8 k→32 k 序列下，NVFP4 累积误差是否导致熵衰减？ | 训练步长 vs 熵值、下游长程推理基准（例如 L-Eval）。 |\n| 多模态 RL | 视觉-语言混合模型中，量化噪声对图像 Token 与文本 Token 的熵增是否一致？ | VL 任务（MathVista）RL 微调后的准确率、图像部分熵值变化。 |\n\n2\\. 噪声机理与理论分析\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n0\n\n3\\. 自适应量化格式\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n1\n\n4\\. 与其他 PEFT 方法正交组合\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n2\n\n5\\. 任务与奖励函数扩展\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n3\n\n6\\. 系统级优化\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n4\n\n7\\. 自动化调参与鲁棒性\n\n- **超参灵敏度**：对 σ\\_start、σ\\_end、K、rank 做 Sobol/Morris 全局灵敏度分析，给出“即插即用”默认值。\n- **失效模式**：当 σ\\_start 过大或调度过慢时，训练是否出现 NaN、模式崩溃？建立早期预警指标（如梯度 L2 / 熵值二阶矩）。\n\n8\\. 理论延伸：量化-探索正则化\n\n- 将量化噪声视为 **免费数据增强**，建立与 **SGLD、Dropout** 的统一正则化框架；\n- 推导量化 RL 的 **泛化界**，量化误差项如何进入 PAC-Bayesian 边界，解释“低比特→更好泛化”现象。\n\n小结\n\n> 从“规模-理论-系统-任务”四个象限出发，可形成**短-中-长期**研究路线图：\n>\n> - **短期**（1–3 个月）：70 B 单卡实验、代码任务迁移、混合精度位宽调度。\n> - **中期**（3–6 个月）：噪声功率谱分析、元学习调度器、DoRA 组合。\n> - **长期**（6–12 个月）：分布式量化 RL 系统、PAC-Bayesian 理论、多模态长上下文统一框架。\n\n这些方向既可直接嵌入 QeRL 开源代码基线，也能反向推动超低比特量化与 RL 理论的交叉发展。\n\nQ6: 总结一下论文的主要内容\n\nQeRL：把 4-bit 量化噪声变成“免费探索信号”的大模型强化学习框架\n——一句话总结：用 NVFP4 权重压缩解决显存与 rollout 速度瓶颈，再用自适应量化噪声（AQN）把静态误差转化为可控熵增，实现“低比特、高速度、更强探索”的三赢。\n\n1\\. 背景痛点\n\n- RL 训练 LLM 需同时维护策略/参考模型，显存爆炸，rollout 反复采样长序列，耗时占 70 % 以上。\n- QLoRA（NF4）反量化查表慢，rollout 反而 0.7×；8-bit 方案需存双精度模型，显存不降。\n- 传统观点认为量化噪声有害，RL 探索需求与 SFT 模仿目标不同，噪声能否“变废为宝”？\n\n2\\. 核心观察\n\n- 4-bit 权重引入的 ∆ε = ˆW − W 等效于参数空间高斯噪声，策略熵平均提升 10 %–20 %。\n- 在 GSM8K/BigMath 上，仅静态 NVFP4 已让 7 B 模型 reward 收敛步数从 500 降到 200。\n\n3\\. QeRL 框架\n\n1. **NVFP4 + LoRA**\n\\-权重 4-bit NVFP4（E4M3 缩放），激活/梯度 16-bit；Marlin 内核实现原生 `NVFP4×BF16` 矩阵乘，无查表。\n\n- 显存降至 25 %–30 %，rollout 吞吐提升 1.3×–2.0×；首次在单卡 80 GB H100 完成 32 B 模型 GRPO 训练。\n\n2. **Adaptive Quantization Noise (AQN)**\n\n- 每轮采样通道级高斯噪声 Z\\_noisy∼N(0,σ^2 I)，按指数调度 σ(k) 从 1e-2 衰减到 5e-4。\n- 把加性噪声合并到 RMSNorm 缩放因子，零参数开销，等效乘性权重噪声，增强探索且后期稳定。\n\n4\\. 实验结果\n\n| 工作 | 技术要点 | 与 QeRL 的差异 |\n| --- | --- | --- |\n| QLoRA (Dettmers et al., 2023a) | NF4 + LoRA，首次 4-bit 微调 | NF4 查表反量化慢， rollout 阶段 0.7–0.8×；QeRL 用 NVFP4 硬件乘加， rollout 1.5× |\n| NVFP4 / MXFP4 (NVIDIA, 2024; OCP, 2023) | 原生 FP4 矩阵乘指令 | QeRL 首次将其引入 RL 训练，并揭示其噪声可提升探索 |\n| QuIP# (Tseng et al., 2024) | 2-bit 量化+Hadamard 变换 | 聚焦 PTQ 压缩率，未涉及 RL 训练阶段 |\n\n| GPTQ (Frantar et al., 2022) | 逐层二阶误差最小化 PTQ | 静态量化，噪声不可控；QeRL 通过 AQN 动态调制噪声 |\n\n5\n\n- 收敛步数：200 vs 500+\n- 端到端速度：1.5×–2.0×\n- 显存：30 %\n\n5\\. 贡献清单\n\n1. 首次揭示**量化噪声在 RL 中的正向作用**，颠覆“低比特=性能降”传统认知。\n2. 提出 **AQN 零开销动态噪声机制**，把静态误差转化为可调探索信号。\n3. 实现 **NVFP4+LoRA 一体化内核**，rollout 阶段 1.5×–2.0× 提速，显存压缩 70 %。\n4. **7 B–32 B 全尺度验证**：在数学推理基准上**持平全参数微调**，**超越 16-bit LoRA 与 QLoRA**。\n\n6\\. 可用口号\n\n“4-bit 权重跑 RL，噪声不是敌人，而是免费探索器。”\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是QeRL%3A%20Beyond%20Efficiency%20--%20Quantization-enhanced%20Reinforcement%20Learning%20for%20LLMs，链接是https://arxiv.org/pdf/2510.11696，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11696。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-1-1760485638277","title":"Diffusion Transformers with Representation Autoencoders","description":"Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.","url":"https://huggingface.co/papers/2510.11690","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.277Z","metadata":{"authors":["Boyang Zheng","Nanye Ma","Shengbang Tong","Saining Xie"],"date":"","rank":2,"llmAnalysis":"","categories":["cs.CV","cs.LG"],"pdfUrl":"https://arxiv.org/pdf/2510.11690.pdf","arxivUrl":"https://arxiv.org/abs/2510.11690","arxivId":"2510.11690","coolPaperUrl":"https://papers.cool/arxiv/2510.11690","published":"2025-10-13T17:51:39Z","updated":"2025-10-13T17:51:39.000Z","zh_summary":"潜在生成建模中，预训练自编码器将像素映射到扩散过程的潜在空间，已成为扩散变换器（Diffusion Transformers, DiT）的标准策略；然而，自编码器组件几乎没有发展。大多数DiT仍然依赖原始的VAE编码器，这带来了若干限制：陈旧的骨干网络影响架构简洁性、低维潜在空间限制信息容量，以及纯粹基于重建训练导致的弱表示能力，最终限制了生成质量。在本工作中，我们探索用预训练表示编码器（如DINO、SigLIP、MAE）替换VAE，并配合训练好的解码器，形成我们称之为表示自编码器（Representation Autoencoders, RAE）的模型。这些模型不仅提供高质量的重建，还能提供语义丰富的潜在空间，同时允许构建可扩展的基于变换器的架构。由于这些潜在空间通常是高维的，一个关键挑战是使扩散变换器能够在其中有效运作。我们分析了造成这一困难的原因，提出理论支持的解决方案，并通过实验进行验证。我们的方法在不使用辅助表示对齐损失的情况下实现了更快的收敛。使用配备轻量宽DDT头的DiT变体，我们在ImageNet上实现了强大的图像生成效果：256x256时FID为1.51（无指导），256x256和512x512时均为1.13（有指导）。RAE具有明显优势，并应成为扩散变换器训练的新默认方案。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该工作针对当前 Diffusion Transformer（DiT）在 latent 空间生成时对传统 VAE 的严重依赖，提出并系统回答了三个关键问题：\n\n1. **“语义编码器只能做语义，不能做重建”是否成立？**\n通过 Representation Autoencoder（RAE）框架，将冻结的 DINOv2/SigLIP/MAE 等预训练编码器与轻量级 ViT 解码器配对，在 256×256 ImageNet 上获得 0.16–0.49 的 rFID，显著优于 SD-VAE（0.62），证明冻结语义编码器同样可以实现高保真重建。\n\n2. **“高维 latent 会让扩散模型训练崩溃”是否必然？**\n理论分析与过拟合实验表明，当 DiT 隐层宽度 d 小于 token 维度 n 时，损失存在下界\n\nLge (n-d) / (n)\n\n导致无法收敛；一旦满足 d≥n 并配合维度相关的噪声调度与噪声增强解码器，高维 RAE latent 反而带来 47× 收敛加速与更低的最终 FID。\n\n3. **“高维 latent 必然带来高计算开销”是否不可避免？**\n提出 DiTDH 架构：在固定 token 数（256）的前提下，仅用一个 2 层、2048 维的浅层宽头即可满足宽度需求，避免把整个 DiT backbone 拓宽，实现 FLOP 仅增加 1% 而 FID 从 4.28 降至 2.16。\n\n最终，RAE-DiTDH 在 ImageNet 256×256/512×512 上分别取得 1.51/1.13 FID（无/有引导），刷新同类模型最佳结果，同时训练成本降低一个数量级，从而确立了“用预训练表征编码器替代 VAE”作为扩散模型 latent 空间的新默认范式。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可归纳为三条主线，每条均列出最具代表性的工作并指出其与 RAE-DiTDH 的异同。\n\n1\\. 改进 VAE latent 以提升生成质量\n\n| 方法 | 核心思想 | 与 RAE 的关键区别 |\n| --- | --- | --- |\n| REPA (Yu et al., 2025) | 在 DiT 中间层加 MSE 对齐 DINOv2 特征，加速收敛 | 仍需 VAE 压缩；对齐是辅助损失，RAE 直接在高维表征空间扩散 |\n| DDT (Wang et al., 2025c) | 把 DiT 拆成编码-解码，用 REPA 损失约束编码输出 | 仍基于 VAE；DiTDH 的宽头灵感来源于 DDT，但用于高维 RAE |\n| VA-VAE (Yao et al., 2025) | 训练 VAE 时额外对齐 DINO 特征 | 低维 latent 不变，仅改善 VAE；RAE 完全抛弃 VAE |\n| MAETok / l-DEtok (Chen et al., 2025a; Yang et al., 2025) | 用 MAE 或去噪目标训练 VAE 编解码器 | 仍保持 32×4×4 压缩，RAE 直接采用 256×768 高维 token |\n\n2\\. 直接采用表征编码器做重建或生成\n\n| 方法 | 核心思想 | 与 RAE 的关键区别 |\n| --- | --- | --- |\n| UniLIP (Tang et al., 2025) | 在 InternViT 特征上加 1 步卷积解码器，rFID 优于 SD-VAE | 需对 ViT 做大规模微调；RAE 冻结原始权重，训练成本更低 |\n| VFMTok / DiGIT (Zheng et al., 2025; Zhu et al., 2024) | 把 DINO/SigLIP 特征向量量化后供 AR 模型使用 | 引入量化瓶颈；RAE 保持连续高维，无信息容量损失 |\n| ReDi (Kouzelis et al., 2025b) | 联合扩散 VAE latent 与 DINO PCA 分量 | 仍依赖 VAE；RAE 完全以表征编码器为 latent 空间 |\n\n3\\. 高维/像素空间扩散模型\n\n| 方法 | 核心思想 | 与 RAE 的关键区别 |\n| --- | --- | --- |\n| SiD² / PixNerd (Hoogeboom et al., 2025; Wang et al., 2025b) | 端到端像素扩散，FID≈1.5 | 768 维原始像素无结构，RAE 提供语义结构，训练效率更高 |\n| RIN / PixelFlow (Jabri et al., 2023; Chen et al., 2025e) | 自适应计算或流匹配在像素空间 | 同样面临高维无结构问题；RAE 通过预训练编码器引入语义先验 |\n| ADM / EDM2 (Dhariwal & Nichol, 2021; Karras et al., 2024) | 经典像素扩散基线 | 512×512 FID 1.25；RAE-DiTDH 在相同分辨率下 FID 1.13，训练步数减少 3× |\n\n4\\. 训练技巧与理论分析\n\n- **维度相关噪声调度** (Esser et al., 2024; Chen, 2023)\n原用于高分辨率像素或 VAE，本文首次将其推广到“token 维度×通道维度”定义的有效数据维度，并给出理论验证。\n\n- **噪声增强解码器** (Dinh et al., 2017; Ho et al., 2019)\n在归一化流中用于平滑离散支持；本文首次系统研究其对“冻结编码器+可训解码器”在扩散采样时的鲁棒性提升。\n\n- **宽度-维度匹配定理** (本文定理 1)\n给出当 DiT 宽度 d < token 维度 n 时损失下界为  ∑_(i=d+1)^n λ_i ，为高维 latent 扩散模型设计提供理论依据。\n\n综上，RAE-DiTDH 首次将“冻结预训练表征编码器 + 轻量可训解码器”确立为 VAE 的替代范式，并通过宽度匹配、维度感知调度与噪声增强解码三大技术，使高维语义 latent 在扩散模型中兼具训练稳定性、计算高效性与生成质量。\n\nQ3: 论文如何解决这个问题？\n\n论文将“用预训练表征编码器替代 VAE”拆解为三大技术障碍，并逐一给出**理论分析 + 架构改进 + 训练策略**的完整解决方案。核心路线图如下：\n\n1\\. 障碍：高维 token 使 DiT 无法收敛\n\n**理论诊断**\n\n- 建立最小宽度定理（Theorem 1）：当 DiT 隐层宽度 d < token 维度 n 时，flow-matching 损失存在严格正下界\n\nLge(1) / (n)∑_(i=d+1)^(n)λ_i\n\n导致模型连单张图像都无法过拟合。\n\n**架构对策**\n\n- **宽度匹配**：固定 d ≥ n（DINOv2-B 768 维 ⇒ DiT-XL 1152 维），而非盲目加深。\n- **轻量宽头 DiTDH**：把“整网络拓宽”改为“仅加 2 层 2048 维浅层 Transformer 头”，FLOPs 增加 <1%，即可满足 d≥n 且避免全局二次增长。\n\n2\\. 障碍：高维 latent 噪声强度不足 → 信息破坏过少\n\n**理论诊断**\n\n- 高维球面体积集中效应：通道数 C 越大，同 σ 下信噪比越高，扩散过程“太简单”而泛化差。\n\n**训练对策**\n\n- **维度相关调度**：将 Esser 等人的“分辨率偏移”推广为\n\nα=√m/n,quad t'=(α t) / (1+(α-1)t)\n\n其中 m = token 数 × 通道数（RAE 256×768=196 608），n₀=4096 为像素基准。偏移后 gFID 从 23.08 降至 4.81。\n\n3\\. 障碍：RAE 解码器只在“干净” latent 上训练，推理时面对扩散噪声 OOD\n\n**训练对策**\n\n- **噪声增强解码**：训练阶段给编码器输出加随机噪声\n\nzsim p(z),;nsimmathcal N(0,σ^2 I),;σsim|mathcal N(0,τ^2)|\n\n使解码器学会对邻域分布鲁棒；τ=0.8 时 gFID 再降至 4.28，rFID 仅轻微上升（0.49→0.57）。\n\n4\\. 系统级整合\n\n将上述三点组合成完整流程：\n\n1. 冻结 DINOv2-B → 256×768 高维 token\n2. DiTDH-XL：backbone 1152 维 + 2048 维 2-layer 宽头\n3. 维度相关噪声调度 + 噪声增强解码\n4. 标准 flow-matching 训练 80 epoch\n\n**结果**\n\n- ImageNet 256×256：无引导 FID 1.51，有引导 1.13\n- 512×512：同一模型 FID 1.13，训练计算仅 4×10¹¹ FLOPs，比同期最佳 EDM2 少 40×。\n\n5\\. 附加效率技巧\n\n- **解码器上采样**：保持扩散仍用 256 token，仅把解码器 patch 尺寸减半，即可输出 512 图像，gFID 仅 0.48↑，训练成本再降 4×。\n- **AutoGuidance**：用早期 14-epoch DiTDH-S checkpoint 当引导模型，无需额外 CFG 超参搜索，最终 1.13 FID。\n\n综上，论文通过“**理论下限 → 宽度匹配 → 维度感知调度 → 噪声鲁棒解码 → 轻量宽头**”的闭环设计，首次让冻结预训练表征编码器在扩散生成中同时实现**高保真、高语义、低训练成本**，彻底摆脱了对传统 VAE 的依赖。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“RAE 能否替代 VAE”与“DiTDH 如何在高维 latent 上高效扩散”两条主线，共设计 6 组实验，覆盖重建、单图过拟合、生成质量、缩放行为、消融分析与可视化。所有定量指标均在 ImageNet-1K 256×256 标准协议下完成（512×512 另有说明），使用 50 K 样本 FID（gFID）或重建 FID（rFID）。\n\n1\\. 重建能力对比（§3）\n\n| 实验目的 | 验证冻结表征编码器 + 轻量 ViT 解码器能否超越 SD-VAE | | 变量 | 编码器类型 / 解码器规模 / 编码器规模 | | 主要结果（表 1） |\n\n- rFID：MAE-B 0.16，DINOv2-B 0.49，SigLIP2-B 0.53，均 < SD-VAE 0.62\n- 线性 probing Top-1：DINOv2-B 84.5 %，SD-VAE 仅 8.0 %\n- 计算量：ViT-XL 解码器 106 GFLOPs ≈ 1/3 SD-VAE 310 GFLOPs\n\n2\\. 单图过拟合诊断（§4.1 + 图 3）\n\n| 实验目的 | 找出 DiT 在高维 latent 上“连一张图都学不了”的根因 | | 变量 | 模型宽度 d ∈ {384,512,640,768,896}；深度 l ∈ {4,12,16,24}；token 维 n=768 | | 主要结果 |\n\n- d < n 时损失收敛至理论下界  (n-d)/n ，图像无法重建；d ≥ n 瞬间过拟合，验证定理 1。\n- 深度增至 24 层仍无法弥补宽度不足，确认“宽度瓶颈”而非“容量不足”。\n\n3\\. 生成基准与收敛速度（§4 + 图 4）\n\n| 实验目的 | 在完整 ImageNet 上验证 RAE-DiT 是否比 VAE 基线收敛更快、最终更好 | | 设置 | 80 epoch，相同算力（≈1×10¹¹ FLOPs） | | 主要结果 |\n\n- gFID：RAE-DiT-XL 4.28 vs SiT-XL(VAE) 8.61 → 47× 训练步数节省\n- 720 epoch 继续训练，RAE-DiT 降至 2.39，优于 REPA-XL 16×。\n\n4\\. DiTDH 缩放与规模律（§5 + 图 6）\n\n| 实验目的 | 验证宽头架构是否比单纯加深加宽 DiT 更 FLOP-高效 | | 变量 | 模型族 {S,B,L,XL}，固定 2 层 2048 维 DDT 头；对比相同 FLOPs 的 vanilla DiT | | 主要结果 |\n\n- DiTDH-B 仅用 40 % FLOPs 即超越 DiT-XL；DiTDH-XL 800 epoch gFID 2.16，再降 47 %。\n- 缩放曲线（图 6c）显示：参数从 0.3 B→0.8 B→1.5 B，gFID 单调下降 6.07→3.38→2.16，无饱和迹象。\n\n5\\. 消融实验（§4.2-4.3 + §G）\n\n| 模块 | 变量 | 主要结果 |\n| --- | --- | --- |\n| 噪声调度 | w/ vs w/o 维度偏移 | gFID 23.08 → 4.81 |\n| 解码鲁棒 | τ ∈ {0,0.5,0.8,1.0} | τ=0.8 时 gFID 4.28 最佳，rFID 仅 +0.08 |\n| DDT 头设计 | 深度 2/4/6 + 宽度 1152/2048 | 2 层 2048 维最优；4 层同宽度 FLOPs×2 无收益 |\n| 编码器规模 | DINOv2-S/B/L | 越大 encoder 需越宽头：S/B 2048 维饱和，L 仍需 2688 维 |\n\n6\\. 高分辨率与引导实验（§5.1 + §I）\n\n| 实验目的 | 验证 RAE 是否无需重训即可上采样，以及 AutoGuidance 相对 CFG 的优势 | | 设置 | 256 扩散模型直接接 512 上采样解码器；对比 AutoGuidance vs CFG-interval | | 主要结果 |\n\n- 512×512：直接训练 gFID 1.13，上采样解码器 1.61，计算节省 4×。\n- AutoGuidance 1.42 尺度下 256×256 gFID 1.13，优于最佳 CFG-interval 1.15，且调参成本极低。\n\n7\\. 无条件生成与可视化（§L-M）\n\n- 无条件 ImageNet 256×256：DiTDH-XL + AG 取得 gFID 4.96，优于 DiT-XL+VAE 30.68，与专用方法 RCG 4.89 持平。\n- 提供 512×512 未筛选样本 250 张（图 10-15），展示细粒度纹理与多样性。\n\n综上，论文通过**重建-诊断-生成-缩放-消融-上采样-可视化**的完整实验矩阵，系统验证了 RAE 替代 VAE 的可行性与 DiTDH 的扩展优势，并给出每一设计选择的定量依据。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“理论-算法-系统-应用”四个层面，均直接源于 RAE-DiTDH 尚未解决或仅初步触及的问题。\n\n1\\. 理论层面\n\n- **宽度-维度定律的紧界与指数收敛**\n当前定理 1 给出下界  (n-d)/n  仅对线性投影+白化噪声成立。若考虑\n\n- 非线性激活的谱衰减\n- 数据协方差  λ_i  的幂律分布\n可推导更紧的样本复杂度与迭代复杂度，并预测“临界宽度”随噪声调度变化的缩放指数。\n- **高维 latent 的 intrinsic dimension 与扩散步数关系**\n用局部 PCA 或 MLE 估计 RAE latent 的 ID，验证是否满足\n\nIDll n,quad 但quad 所需扩散步数propto n/ID\n\n从而解释为何 RAE 只需 50 步即可媲美像素扩散 1000 步。\n\n2\\. 算法层面\n\n- **自适应宽度 / 动态头**\n让 DDT 头宽度随时间步  t  或信噪比 SNR( t ) 动态调整：早期高噪声用宽头，后期低噪声缩回窄头，减少 30–40 % 总 FLOPs。\n\n- **多尺度 RAE**\nencoder 仍用单尺度 patch，但 decoder 引入级联或 NUWA-style 金字塔，实现 1 K→2 K 超分时扩散 token 数不变，验证 rFID 与 gFID 的权衡曲线。\n\n- **量化-连续混合 latent**\n在高维连续 token 上轻量插入 FSQ 或 LFQ 量化（如 8 群 8 级），保持语义连续的同时把有效维度降至 1/8，探索是否可进一步加速收敛或降低采样步数。\n\n- **自监督解码器预训练**\n当前解码器完全随机初始化。可先用 MAE 或扩散前置任务预训练解码器，再接入冻结 encoder，观察能否在 <10 epoch 内达到相同 rFID。\n\n3\\. 系统与优化\n\n- **MMoE / 专家混合头**\n把 2048 维宽头拆成 8×256 维专家，按 latent 的 K-means 簇 ID 路由，实现推理时仅激活 1/8 参数，维持宽头性能但计算量接近窄头。\n\n- **FlashAttention-3 与序列并行**\nRAE token 维 768–1024，Head dim 大，利于 FlashAttention 的 split-K 调度。结合序列并行可把 512×512 训练从 8×A100 降至 4×A100。\n\n- **AutoGuidance 的理论最优尺度**\n建立“引导强度-噪声预测误差”闭环模型，在线估计最优引导尺度，避免网格搜索，实现零成本自适应引导。\n\n4\\. 应用与跨模态\n\n- **视频 RAE**\n冻结 DINOv2-Video 或 VideoMAE encoder，训练 3D ViT 解码器，保持 16×16×8 空间-时间 patch；考察扩散模型是否同样享受“高维语义 latent”带来的帧一致性与细节提升。\n\n- **文本到图像 RAE**\n用 SigLIP-2B 文本塔对齐图像塔，构建跨模态 RAE，扩散模型直接对联合 latent 去噪，验证是否可取消交叉注意力层，仍保持精细文本控制。\n\n- **多任务编码器共享**\n同一冻结 encoder 同时服务生成、检测、分割任务，验证“生成-理解共享高维 latent”是否带来相互增益（类似 Uni-Perceiver 但无压缩）。\n\n5\\. 评价与基准\n\n- **Beyond-FID 指标**\n高维语义 latent 可能过度优化 Inception 特征。引入 DINOv2、CLIP、SwAV 多特征空间的 FID-ensemble，以及人类偏好对齐度量（MUSIQ、LIPIE），观察 RAE 是否仍领先。\n\n- **压缩率-生成质量帕累托前沿**\n系统扫描 token 数（64–1024）与通道维（384–1536）组合，绘制“比特/像素-gFID”曲线，找出 RAE 相对 VAE 的绝对优势区间与失效点。\n\n综上，RAE-DiTDH 打开了“高维语义 latent 扩散”这一新赛道，从理论紧界、自适应架构、跨模态扩展到全新评价维度，均有大量空白值得后续工作深挖。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **Representation Autoencoder (RAE)**，用**冻结的预训练表征编码器（DINOv2/SigLIP/MAE）+ 轻量 ViT 解码器**取代传统 VAE，作为 Diffusion Transformer 的 latent 空间，并设计 **DiTDH** 架构解决高维 token 带来的训练难题，实现**更快收敛、更低 FID、更少计算**。\n\n核心贡献与结果概括为：\n\n1. **高保真重建**\n256×256 ImageNet rFID：MAE-B 0.16 / DINOv2-B 0.49，均优于 SD-VAE 0.62，且解码器计算仅 1/3。\n\n2. **理论下限与宽度匹配**\n证明当 DiT 隐层宽度 d < token 维 n 时，flow-matching 损失下界为  (n-d)/n ；实践中 d≥n 即可稳定训练。\n\n3. **维度感知噪声调度 + 噪声增强解码**\n\n- 按“token 数×通道数”偏移调度，gFID 23→4.8。\n- 解码器训练时加噪，增强对扩散输出 OOD 鲁棒，gFID 再降至 4.3。\n4. **轻量宽头 DiTDH**\n在固定 token 数前提下，仅加 2 层 2048 维浅层头，满足 d≥n 且避免全局二次计算；相比 vanilla DiT-XL，训练 FLOPs 节省 60 %，最终 gFID 2.16。\n\n5. **SOTA 结果**\nImageNet 256×256：无引导 FID 1.51，有引导 1.13；512×512：1.13，均刷新同类模型最佳，且训练时间缩短一个数量级。\n\n6. **高效上采样与无条件生成**\n解码器 patch 减半即可 4× 上采样，无需重训扩散模型；无条件生成 gFID 4.96，媲美专用方法 RCG。\n\nRAE 将自动编码从“压缩”转变为“表征”，确立“冻结语义编码器 + 可训解码器”作为扩散模型 latent 空间的新默认范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Diffusion%20Transformers%20with%20Representation%20Autoencoders，链接是https://arxiv.org/pdf/2510.11690，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11690。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-2-1760485638279","title":"OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs","description":"Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.","url":"https://huggingface.co/papers/2510.10689","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.279Z","metadata":{"authors":["Caorui Li","Yu Chen","Yiyan Ji","Jin Xu","Zhenyu Cui","Shihao Li","Yuanxing Zhang","Jiafu Tang","Zhenghao Song","Dingling Zhang","Ying He","Haoxiang Liu","Yuxuan Wang","Qiufeng Wang","Zhenhe Wu","Jiehui Luo","Zhiyu Pan","Weihao Xie","Chenchen Zhang","Zhaohui Wang","Jiayi Tian","Yanghai Wang","Zhe Cao","Minxin Dai","Ke Wang","Runzhe Wen","Yinghao Ma","Yaning Pan","Sungkyun Chang","Termeh Taheri","Haiwen Xia","Christos Plachouras","Emmanouil Benetos","Yizhi Li","Ge Zhang","Jian Yang","Tianhao Peng","Zili Wang","Minghao Liu","Junran Peng","Zhaoxiang Zhang","Jiaheng Liu"],"date":"","rank":3,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.10689.pdf","arxivUrl":"https://arxiv.org/abs/2510.10689","arxivId":"2510.10689","coolPaperUrl":"https://papers.cool/arxiv/2510.10689","published":"2025-10-12T16:34:00Z","updated":"2025-10-12T16:34:00.000Z","zh_summary":"近年来，多模态大语言模型（MLLMs）在视频理解方面展现出了巨大的潜力。然而，现有的基准测试未能全面评估跨音频和视觉模态的协同推理能力，通常会忽略其中一种模态，或者以逻辑上不一致的方式将它们整合。为弥补这一空白，我们推出了 OmniVideoBench，这是一个大规模且经过严格设计的基准，用于评估协同音视频理解能力，特别强调模态互补性和逻辑一致性。具体而言，OmniVideoBench 包含 1000 个高质量的问答（QA）对，每个问答对都带有逐步推理轨迹注释，来源于 628 个不同的视频，长度从几秒到 30 分钟不等，并经过人工验证以保证完全正确且唯一。此外，OmniVideoBench 涵盖了 13 种精心设计的问题类型，包括时间推理、空间定位、计数、因果推理、总结等，从而捕捉视频理解的核心挑战。在 OmniVideoBench 上对多种 MLLM 进行评估显示，模型表现与人类推理之间存在明显差距，开源模型明显落后于闭源模型，这进一步凸显了真实音视频推理的内在难度。我们将发布 OmniVideoBench，以促进具备更强、更具普适性推理能力的 MLLM 发展。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nOmniVideoBench 旨在解决现有评测无法**系统、严谨地衡量多模态大模型（MLLM）在音-视协同推理能力**上的缺陷。具体而言，论文指出当前基准存在三大核心问题：\n\n1. **忽视音频-视觉互补性**\n已有视频评测往往只把音频当作辅助信息，或干脆只测视觉，导致模型无需真正“听”就能答对，无法检验跨模态整合能力。\n\n2. **逻辑一致性缺失**\n音-视信息必须被同时解析并在时间线上对齐，才能支撑因果、计数、空间关系等复杂推理。现有数据缺少对“逻辑链”是否自洽的验证，使得模型可以靠单模态捷径或语言先验蒙对答案。\n\n3. **长时序与细粒度推理缺位**\n短视频片段难以覆盖长程事件依赖；同时缺乏对“步骤级”推理轨迹的标注，研究者无法诊断模型在哪一步、哪一模态上出错。\n\n为此，OmniVideoBench 构建了一个**1000 对高质量选择题**的评测集，配套**逐条原子化推理链**（每一步标注模态、证据、推断），强制要求模型必须同时利用音频与视觉信息，且保证答案唯一、无歧义。通过 13 类任务（时序、因果、计数、音乐理解等）与 628 条真实长视频（数秒至 30 分钟），该基准首次系统量化了 MLLM 在**音-视协同、长时序一致性与细粒度推理**上的差距，揭示开源模型几乎随机，闭源模型亦未达及格线，从而推动社区向真正的“全模态”推理能力迈进。\n\nQ2: 有哪些相关研究？\n\n与 OmniVideoBench 直接相关的研究可划分为三类：\n\n1. 面向音-视协同理解的评测基准\n2. 面向视频-语言或多模态推理的通用评测\n3. 近期“全模态”大模型及其评测协议\n\n以下按类别列出代表性文献（按时间排序，括号内给出与本文对比的关键差异）。\n\n1\\. 音-视协同评测基准\n\n| 基准 | 模态 | 数据形式 | 时长 | 核心差异 |\n| --- | --- | --- | --- | --- |\n| Music-AVQA (Li et al., CVPR 2022) | V+A | 短视频 | ≈ 60 s | 仅问答音乐相关事件，无长程推理链 |\n| AV-Odyssey (Gong et al., arXiv 2024) | I+A | 单帧+音频 | — | 图像而非视频，缺少时序维度 |\n| Daily-Omni (Zhou et al., arXiv 2025) | V+A | 日常短视频 | 30/60 s | 无逐步推理标注，任务类型少 |\n| WorldSense (Hong et al., arXiv 2025) | V+A | 真实长视频 | 15–656 s | 提供 MCQ，但未强制音-视互补，无原子推理链 |\n\n2\\. 视频-语言 / 长视频推理评测\n\n| 基准 | 模态 | 数据形式 | 时长 | 核心差异 |\n| --- | --- | --- | --- | --- |\n| VaTeX (Wang et al., ICCV 2019) | V→T | 双语字幕 | 10 s | 侧重字幕生成，无音频 |\n| Value (Li et al., ACL 2021) | V→T | 多任务 | 10–30 s | 无音频，任务以描述/检索为主 |\n| MVBench (Li et al., CVPR 2024) | V→T | 多任务 MCQ | 10–60 s | 无音频，强调时序感知 |\n| MMBench-Video (Fang et al., arXiv 2024) | V→T | 长镜头多片段 | 数分钟 | 无音频，任务以整体理解为主 |\n| LongVideoBench (Wu et al., arXiv 2024) | V+T | 交错视频-文本 | 最多 30 min | 无音频，聚焦长上下文对齐 |\n\n3\\. “全模态”大模型与评测协议\n\n| 模型/协议 | 输入模态 | 输出模态 | 与本文关系 |\n| --- | --- | --- | --- |\n| GPT-4o (Hurst et al., 2024) | 任意→任意 | 文本/语音/图像 | 闭源标杆，OmniVideoBench 对其评测 |\n| Gemini-1.5/2.0/2.5 (Team 2024; Comanici et al., 2025) | V+A+T | 文本 | 闭源最强基线，用于对比 |\n| Qwen2.5-Omni / Qwen3-Omni (Xu et al., 2025b,c) | V+A+T | 文本/语音 | 开源代表，OmniVideoBench 显示其音-视协同差距 |\n| Baichuan-Omni-1.5 (Li et al., 2025) | V+A+T | 文本 | 开源模型，被纳入本文实验 |\n| MiniCPM-o / HumanOmni (Yao et al., 2024; Zhao et al., 2025) | V+A+T | 文本 | 开源小参数量模型，用于对比 |\n\n4\\. 方法论相关（推理链与质量过滤）\n\n- **MME-CoT** (Jiang et al., 2025)\n提出多模态思维链质量评估框架，OmniVideoBench 借鉴其“原子步骤”思想，但额外约束每一步必须显式标注模态与证据。\n\n- **MLLM-Bench** (Ge et al., 2025)\n建立逐样本细粒度评测协议，OmniVideoBench 在此基础上引入“音-视互补”过滤与语义距离一致性校验。\n\n总结\n\n现有研究或缺少音频，或仅覆盖短视频/单帧，或未对推理链进行细粒度标注。OmniVideoBench 首次将**长视频、音-视互补、逐步原子推理链**三者同时纳入统一基准，填补了“全模态”评测的空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建-过滤-精修”三段式 pipeline，把“如何迫使模型必须同时、一致地利用音频与视觉进行长时序、多步骤推理”这一核心问题拆解为四个可执行子任务，并给出对应技术方案。\n\n1\\. 数据构建：强制音-视互补\n\n- **视频采集策略**\n– 时长 4 s–30 min，覆盖 8 大场景 68 子类，确保长程依赖与领域多样性。\n– 显式剔除“音频冗余”或“视觉冗余”片段：\n\n- 纯背景音乐不与画面事件对应 → 丢弃\n- 视觉静态或字幕覆盖 → 丢弃\n– 仅保留 2024-06 之后发布视频，降低训练集泄露。\n- **问题设计协议**\n13 类任务模板（时序、因果、计数、音乐理解等）均满足\n Answer = f(V_(evidence), A_(evidence)) \n即单模态无法推出唯一答案。\n额外约束：\n\n- 选项语义距离一致化： d(o_i,o_j)=|S_i△S_j|  恒定，防止模型利用长度/风格捷径。\n- 答案≤5 词，减少语言先验泄露。\n\n2\\. 自动过滤：剔除“单模态可解”与“文本可解”\n\n- **V+A→MLLM 过滤**\n用 Gemini-2.0-Flash 仅输入视觉帧（静音）作答，若仍能给出合理推理链→标记为“视觉可解”并剔除；同理对音频轨道做“只听”测试。\n约 40 % 问题被筛掉，保证剩余样本必须音-视协同。\n\n- **文本泄露过滤**\n用 DeepSeek-V3.1 仅阅读题干+选项，若凭常识或措辞线索即可答对→视为“文本可解”：\n\n- 常识类→直接丢弃\n- 措辞类→人工重写题干/选项，直至模型无法仅通过文本命中答案\n再筛 30 %，最终保留 1 103 题。\n\n3\\. 人工精修：构建“原子推理链”\n\n- 三重校验\n① 答案正确且唯一\n② 无外部知识依赖\n③ 音-视证据均能在时间轴上定位\n\n- 原子链标注格式\n每一步三元组：\n (Modality_(∈V,A), Evidence_(时间戳+内容), Inference_(中间结论)) \n平均 5.68 步，46 % 步骤依赖音频，54 % 依赖视觉，确保跨模态交替推理。\n\n4\\. 评测协议：暴露模型缺陷\n\n- **多选→开放端双重指标**\n先报 MCQ 准确率，再隐藏选项做 open-ended QA，防止随机猜测与选项线索夸大性能。\n\n- **细粒度诊断**\n- 按音频类型（Speech/Sound/Music）与时长（0–1/1–5/5–10/10–30 min）切片，定位音乐理解与长视频衰退。\n- 按 13 类任务切片，定位因果、计数、空间等薄弱技能。\n- 帧数消融：32→256 帧，量化时序采样密度对音-视对齐的影响。\n\n结果验证问题已解决\n\n- 开源模型在 OmniVideoBench 上仅≈ 30 %（随机 25 %），说明过滤有效——无法靠单模态或语言先验“蒙”对。\n- Gemini-2.5-Pro 音乐类仅 38.5 %，长视频 10–30 min 段下降 3 个百分点，证明基准成功暴露“音乐-情感”与“长时序一致性”两大盲区。\n- 开放端评测相对 MCQ 绝对下降 10–15 %，进一步确认模型真实协同推理能力远低于传统多选分数。\n\n通过上述四步，论文把“音-视协同、逻辑一致、长时序、可诊断”四个需求同时落地，首次给出了可量化、可复现、可定位缺陷的严谨评测方案。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“音-视协同推理到底难在哪”这一主线，设计了**4 组共 9 项实验**，从模型层、输入层、任务层、题型层四个维度系统挖掘瓶颈。所有实验均在 OmniVideoBench 1 000 对 QA 上完成，统一采用**零样本提示 + 贪婪解码**，以保证可复现性。\n\n1\\. 模型层：全栈基线横扫\n\n| 实验 | 目的 | 关键结果 |\n| --- | --- | --- |\n| E1. 开源 vs. 闭源 Omni-MLLM | 量化生态差距 | Gemini-2.5-Pro 58.9 %，最佳开源 Qwen3-Omni-30B-A3B 仅 38.4 %，差距 > 20 点 |\n| E2. 视觉-only 消融 | 验证“音频不可或缺” | 同一模型去掉音频后平均再掉 6–9 点，音乐类掉 10–15 点 |\n| E3. 纯文本 LLM 上界 | 排除语言先验 | DeepSeek-V3.1 纯文本 27.6 %，接近随机，说明文本泄露已被过滤 |\n\n2\\. 输入层：音频类型与时长消融\n\n| 实验 | 子实验 | 结果摘要 |\n| --- | --- | --- |\n| E4. 音频类型切片 | Speech / Sound / Music | Gemini-2.5-Pro 61.7 → 57.7 → 38.5 %；开源模型音乐类普遍 < 30 % |\n| E5. 时长切片 | 0–1 / 1–5 / 5–10 / 10–30 min | 10–30 min 段平均下降 3–8 点；Qwen3-Omni 在长视频段跌至 35 % |\n| E6. 帧数密度 | 32 / 64 / 128 / 256 帧 | 256 帧比 32 帧平均提升 6–10 点；长视频受益更大（≥ 8 点） |\n\n3\\. 任务层：13 类细粒度雷达\n\n| 实验 | 指标 | 发现 |\n| --- | --- | --- |\n| E7. 任务级 accuracy | 13 任务雷达图 | 关系推理、摘要有 > 80 %；背景&音乐理解 < 50 %；开源模型在“因果”“时序”上普遍 < 35 % |\n\n4\\. 题型层：MCQ 是否虚高？\n\n| 实验 | 设置 | 结果 |\n| --- | --- | --- |\n| E8. MCQ → Open-ended | 同一模型隐藏选项 | Gemini-2.0-Flash 41.5 → 27.1 %（−14.4 %）；Qwen2.5-Omni 29.3 → 17.3 %（−12.0 %） |\n| E9. ASR 替代真实音频 | 视觉+ASR vs 视觉+真实波形 | 音乐/环境声场景 ASR 几乎无效；Speech 场景 ASR 可恢复 70 % 性能，但仍低于真实音频 8–10 点 |\n\n实验结论一览\n\n1. 闭源模型领先，但**音乐情感与长时序**仍是普遍短板。\n2. 开源模型**跨模态融合模块**明显欠拟合，帧数增加或加入 ASR 均无法弥补音频语义缺失。\n3. MCQ 形式**显著高估**真实推理能力，开放端下降 ≥ 10 点。\n4. 帧数密度与音频类型呈**正交增益**：想涨分，既要“看得细”，也要“听得懂”。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 OmniVideoBench 暴露出的“盲区”与“空白”，均具备可验证、可度量、可复现的研究价值。\n\n1\\. 音乐-情感-事件对齐\n\n- **问题**\n音乐类准确率普遍 <40%，模型无法将低语义声学特征（调式、节拍、情感强度）映射到高层事件因果。\n- **可探索**\n- 构建大规模“情感-事件”伪标签，采用对比式音-视预训练，目标函数：\n\nL = -log(exp(s(v_i,a_j)/τ)) / (∑_(k)exp(s(v_i,a_k)/τ))\n\n其中负样本跨视频采样，强制模型区分“同情感不同事件”。\n- 引入音乐理论先验（chroma、beat-sync 表示）作为辅助分支，验证先验对下游 QA 的迁移增益。\n\n2\\. 长视频记忆与遗忘机制\n\n- **问题**\n10–30 min 段平均掉分 3–8 点，开源模型尤甚；尚不清楚是“视觉遗忘”还是“音频漂移”。\n- **可探索**\n- 在相同视频上逐段滑动窗口，绘制“段级准确率 vs 距关键事件时间间隔”曲线，量化遗忘斜率。\n- 对比不同记忆策略：\n– 压缩记忆（StreamingLLM）\n– 检索增强（RAG-Video）\n– 时间戳感知的 KV 隔离\n- 提出新指标 **FORGET-AV@k**：关键事件出现在视频前 k% 时的 QA 准确率，与人类遗忘曲线对比。\n\n3\\. 开放端生成评测\n\n- **问题**\nMCQ 高估 ≥10 点，但开放端尚无自动、细粒度评估器。\n- **可探索**\n- 构建“音-视链式证据标注”版本：每条答案句子关联时间戳与模态标签，训练一个可检查“证据完备性”的判别器。\n- 设计 **AV-F1** 指标：\n\nAV-F1 = 2·R_(evidence)· P_(fact)R_(evidence)+P_(fact)\n\n其中  R_(evidence)  衡量召回必要音-视证据， P_(fact)  衡量事实正确性。\n- 验证该指标与人类评分的 Kendall-τ 相关性，替代 BLEU/ROUGE。\n\n4\\. 模态缺失与鲁棒性\n\n- **问题**\n真实场景常出现“音频噪声”或“画面遮挡”，模型是否仍能做互补推理？\n- **可探索**\n- 构建扰动套件：\n– 音频：加噪、混响、带宽裁剪\n– 视觉：帧丢失、模糊、黑屏片段\n- 报告 **mACC-robust** = 平均扰动后准确率，观察何时出现“模态崩溃”（性能跌破视觉-或音频--only 上界）。\n- 训练“模态 dropout”微调策略，验证鲁棒性是否可习得。\n\n5\\. 事件级时间定位\n\n- **问题**\n当前 QA 只关心“答案”，未检验模型是否能**指出证据发生在哪一秒**。\n- **可探索**\n- 将 1 000 条 QA 升级为“答案 + 起止时间戳”双标签，形成 OmniVideoBench-Temporal。\n- 评测指标采用 **Temporal-IoU**：预测区间与真值 IoU ≥ 0.5 视为定位正确。\n- 对比端到端回归 vs 先检索后问答两段式框架，验证定位误差对问答准确率的影响。\n\n6\\. 低资源语言与方言音频\n\n- **问题**\n当前音频以英语/中文普通话为主，方言或低资源语言下 ASR 失效，模型是否仍能从语音韵律推理？\n- **可探索**\n- 引入粤语、闽南语、西班牙语视频子集，人工标注 QA，观察\\*\\*ASR 错误率↑时 QA 准确率↓\\*\\*的敏感曲线。\n- 训练“无文本语音编码器”：采用离散自监督单元 (HuBERT) 替代 ASR，验证是否保留推理能力。\n\n7\\. 模型编辑与可解释性\n\n- **问题**\n当模型在音乐任务失败时，我们能否“局部编辑”而不影响其他任务？\n- **可探索**\n- 采用 Rank-One Model Editing (ROME) 定位音乐-情感前馈层，插入新键值对。\n- 评测编辑后音乐类准确率提升幅度 vs 其他任务遗忘率，衡量编辑局部性。\n- 结合推理链可视化（梯度热图 + 音-视注意力 rollout），解释编辑前后模型关注区域的变化。\n\n8\\. 实时流式评测\n\n- **问题**\n现有评测为离线整段输入，无法衡量直播、AR 等流式场景。\n- **可探索**\n- 将 OmniVideoBench 转码为 1 fps 实时流，引入“延迟-准确率”帕累托曲线：\n– x 轴：允许最大延迟（0.5 s / 1 s / 2 s）\n– y 轴：QA 准确率\n- 对比不同流式架构：\n– 窗口滑动 + KV 复用\n– 事件触发缓存（silent 跳过）\n- 提出新指标 **Streaming-AV@latency**，为实时多模态模型提供基准。\n\n9\\. 多模态链式自我训练\n\n- **问题**\n人工标注推理链成本高昂，能否用模型自生成链并迭代提升？\n- **可探索**\n- 采用 **Reject Sampling Fine-tuning**：\n1. 用现有最强模型生成链式推理；\n2. 只保留答案正确且音-视证据通过 VLM+ASR 校验的样本；\n3. 微调小模型并重复迭代。\n- 监控“自训练塌陷”现象：链长变短、模态引用率失衡 → 引入多样性正则。\n\n10\\. 统一音-视-文本 tokenizer\n\n- **问题**\n当前模型普遍“视觉 ViT + 音频 CNN + 文本 LLM”三段式，存在异构表示鸿沟。\n- **可探索**\n- 训练 **Omni-Tokenizer**：将 25 fps 视频与 16 kHz 音频同时离散化为统一词表，目标函数为\n\nL = L_(VL) + L_(AL) + L_(VA-match)\n\n- 在 OmniVideoBench 上对比原生端到端 vs 拼接式架构，验证“同一词表”是否能提升音乐与长视频任务。\n\n以上 10 条方向均直接对应 OmniVideoBench 暴露的短板，且每条都可给出量化指标与可复现实验，适合后续研究深入。\n\nQ6: 总结一下论文的主要内容\n\nOmniVideoBench 是一项针对“音-视协同推理”缺失系统评测而提出的大规模基准，核心贡献与发现可概括为以下四点。\n\n1\\. 基准构建\n\n- **628 条真实长视频**（4 s–30 min），覆盖 8 大场景 68 子类，确保长程依赖与领域多样性。\n- **1 000 对人工 QA** 配备 5.68 步原子推理链，每一步标注模态（V/A）、证据（时间戳+内容）、推断，强制音-视互补且答案唯一。\n- **13 任务类型**（因果、计数、音乐理解、时序、空间等）+ 四档时长 + 三档音频类型（Speech/Sound/Music），形成细粒度评测空间。\n\n2\\. 质量保障\n\n- **双阶段过滤**\n– 单模态可解 → 自动剔除\n– 文本可解（常识/措辞泄露）→ 人工重写或丢弃\n- **语义距离一致性**\n选项间对称差集大小  d(o_i,o_j)=|S_i△S_j|  保持恒定，防止模型利用长度/风格捷径。\n\n3\\. 实验结果\n\n| 维度 | 主要发现 |\n| --- | --- |\n| 模型差距 | 闭源 Gemini-2.5-Pro 58.9 %，最佳开源 Qwen3-Omni-30B-A3B 仅 38.4 %；其余开源接近随机。 |\n| 音频类型 | 音乐类平均 <40 %，显著低于语音（61.7 %）与环境声（57.7 %）。 |\n| 视频时长 | 10–30 min 长视频段平均掉分 3–8 点，揭示长时序记忆瓶颈。 |\n| 题型偏差 | 开放端 QA 相比 MCQ 再掉 10–15 点，说明多选显著高估真实推理能力。 |\n| 帧数密度 | 256 帧 vs 32 帧提升 6–10 点，长视频受益更大，验证密集采样对跨模态对齐的重要性。 |\n\n4\\. 结论与影响\n\n- 首次量化证明：**即使最强模型也未达到及格线（60 %）**，音-视协同、音乐情感、长时序一致性仍是 MLLM 的普遍盲区。\n- 提供带原子推理链的高质量数据与诊断工具，推动未来研究向“真正的全模态、可解释、长上下文推理”迈进。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是OmniVideoBench%3A%20Towards%20Audio-Visual%20Understanding%20Evaluation%20for%20Omni%20MLLMs，链接是https://arxiv.org/pdf/2510.10689，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.10689。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-3-1760485638279","title":"Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States","description":"Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.","url":"https://huggingface.co/papers/2510.11052","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.279Z","metadata":{"authors":["Qinglin Zhu","Yizhen Yao","Runcong Zhao","Yanzheng Xiang","Amrutha Saseendran","Chen Jin","Philip Alexander Teare","Bin Liang","Yulan He","Lin Gui"],"date":"","rank":4,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.11052.pdf","arxivUrl":"https://arxiv.org/abs/2510.11052","arxivId":"2510.11052","coolPaperUrl":"https://papers.cool/arxiv/2510.11052","published":"2025-10-13T06:38:13Z","updated":"2025-10-13T06:38:13.000Z","zh_summary":"自回归（AR）模型仍然是自然语言生成的标准，但由于严格的顺序解码，仍存在高延迟问题。最近受扩散方法启发的方案，如 LlaDA 和 Dream，通过并行生成来缓解这一问题，但它们存在两个核心局限性：信息丢失，因为非最终化令牌的预测分布在每一步都会被丢弃；以及过早承诺，即在缺乏足够全局协调的情况下做出局部决策。我们提出了潜变量精炼解码（Latent Refinement Decoding, LRD），这是一个包含潜变量精炼和预测反馈环的两阶段框架。第一阶段将被掩盖的位置保持为预测令牌和掩码嵌入的分布混合，从而让模型建立更全局一致的信念。第二阶段逐步确定有信心的令牌，同时保留不确定的令牌以进行迭代反馈。KL 散度动态提供了收敛和提前停止的原则性和可靠标准。在编码（HumanEval +6.3, MBPP +2.6）和推理（GSM8K +2.9, MATH500 +3.8）等实验中显示，LRD 不仅提高了准确率，还实现了最高 10.6 倍的加速，使其成为并行序列生成的强大且多功能的替代方案。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决扩散式语言模型（diffusion-based language models, dLLMs）在并行解码过程中出现的两大核心缺陷：\n\n1. **信息丢失（information loss）**\n现有方法在每一步去噪时，对置信度不足的位置强制重置为统一的 \nMASK\n 嵌入，完全丢弃之前步骤得到的预测分布，导致不确定位置无法通过自注意力共享概率信息。\n\n2. **过早确定（premature commitment）**\n硬分配策略（hard assignment）在局部置信度较高时立即将位置锁定为某个具体词元，缺乏全局协调，容易引入错误并在后续步骤中放大。\n\n为此，作者提出 **Latent Refinement Decoding (LRD)**，通过两阶段框架在嵌入空间维护“信念状态”的连续分布，实现全局一致的信息累积与渐进式词元确定，从而在提升生成质量的同时显著加速推理。\n\nQ2: 有哪些相关研究？\n\n相关研究可分为两条主线：扩散式语言模型（dLLMs）本身的提出与改进，以及面向其推理效率的加速方法。主要文献与贡献如下：\n\n- **扩散式语言模型基础**\n- Masked Diffusion Models（MDMs）将文本离散状态视为“掩码-恢复”过程，代表工作包括 Austin et al. 2021a、Ou et al. 2024、Shi et al. 2024。\n- DiffuLLaMA（Gong et al. 2025）通过继续预训练将 LLaMA 转化为扩散范式，验证 dLLMs 可与自回归模型性能媲美。\n- 后续商业/开源模型：Mercury（Labs et al. 2025）、Gemini Diffusion（DeepMind 2025）、LLaDA（Nie et al. 2025）、Dream（Ye et al. 2025）等，进一步放大模型规模与生成质量。\n- **dLLM 推理加速**\n\n1. **KV-Cache 类**\n\n- dLLM-Cache（Liu et al. 2025）提出免训练的自适应缓存，缓解提示与响应阶段的重复计算。\n- Fast-dLLM（Wu et al. 2025）利用块级 KV 复用与置信度感知的动态并行解码。\n- Sparse-dLLM（Song et al. 2025）结合动态缓存驱逐与稀疏注意力，以令牌显著性时序一致性为核心。\n2. **计算/步数优化类**"}},{"id":"hf-4-1760485638280","title":"RLFR: Extending Reinforcement Learning for LLMs with Flow Environment","description":"Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.","url":"https://huggingface.co/papers/2510.10201","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.280Z","metadata":{"authors":["Jinghao Zhang","Naishan Zheng","Ruilin Li","Dongzhou Cheng","Zheming Liang","Feng Zhao","Jiaqi Wang"],"date":"","rank":5,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.10201.pdf","arxivUrl":"https://arxiv.org/abs/2510.10201","arxivId":"2510.10201","coolPaperUrl":"https://papers.cool/arxiv/2510.10201","published":"2025-10-11T13:00:25Z","updated":"2025-10-11T13:00:25.000Z","zh_summary":"可验证奖励的强化学习（RLVR）最近被认为是提升大型语言模型（LLMs）推理能力的一个有前景的框架。然而，采用二元验证进行优化的策略容易忽略推理轨迹中潜在的有价值探索。鉴于黄金过程奖励模型（PRMs）的标注成本高昂，近期研究尝试使用辅助信号对过程 token 进行奖励塑形，包括从 logits 空间收集的熵和似然。在本工作中，我们提供了一种利用潜在空间流奖励进行 RLVR 奖励塑形的新视角，并提出了 RLFR，其中模型潜在变量的流场由非策略高质量数据或策略拒绝采样数据构建，并量化策略潜在变量在其中的速度偏差以作为奖励信号。RLFR 首次表明，成熟的流场可以成为奖励信号收集的可靠环境，突显了潜在表达空间尚未被充分探索。此外，RLFR 可以将任何非策略专家数据压缩作为构建奖励信号的参考，我们展示了隐藏状态中压缩的高效上下文依赖被利用，而非单个 token 级别的表示参与上下文理解。在语言和多模态推理基准上的实验表明流奖励的可靠性，并提示了使用辅助信号进行奖励塑形的有希望的新范式。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**强化学习与可验证奖励（RLVR）**在提升大语言模型（LLM）推理能力时，因**二元验证信号过于稀疏**而忽视潜在有价值探索的问题。具体而言：\n\n- **二元验证仅依据最终答案正确性**给出 0/1 奖励，导致部分推理路径虽局部正确却因最终答案错误而被全盘否定，抑制了探索。\n- \\*\\*过程奖励模型（PRM）\\*\\*可提供细粒度信号，但标注成本极高，且在线分布偏移会引入奖励偏差。\n- **现有辅助信号**（如 logit 空间的熵或似然）存在**自奖励风险**：模型可能过度利用自身置信度而非真正提升推理策略。\n\n为此，论文提出 **RLFR（Reinforcement Learning with Flow Rewards）**，核心思想是：\n\n> **利用 LLM 潜空间构建“流场”环境，将策略潜表示的“速度偏差”量化为稠密奖励**，从而在无额外人工标注的前提下，为推理轨迹的每一步提供可靠、可扩展的辅助信号。\n\n简言之，论文把**潜空间流匹配**从生成模型领域迁移到 RLVR 奖励塑形，首次证明**潜空间可成为稳定、低成本的过程奖励来源**，缓解二元验证的探索瓶颈。\n\nQ2: 有哪些相关研究？\n\n论文涉及的三条主要研究脉络及其代表性工作如下（按出现顺序归纳）：\n\n1. 强化学习与可验证奖励（RLVR）\n\n- Lambert et al. 2024, Jaech et al. 2024, DeepSeek-AI et al. 2025 —— 二元结果奖励的鲁棒性验证\n- Shao et al. 2024 —— Group Relative Policy Optimization（GRPO），无需价值模型的 PPO 简化版\n- Hammoud et al. 2025, Cui et al. 2025 —— 指出二元信号忽略部分正确轨迹，抑制探索\n2. 过程奖励与辅助信号塑形\n\n- Zhao et al. 2025, Liu et al. 2025d, Yang et al. 2025a —— 人工标注 PRM 提供逐步奖励，但成本高昂\n- Cheng et al. 2025, Damani et al. 2025, He et al. 2025 —— 用 logit 空间熵/似然塑形，存在自奖励风险\n- Wang et al. 2025b —— 高熵少数 token 驱动 RL，揭示置信度可被过度利用\n3. 流匹配（Flow Matching）与潜空间建模\n\n- Lipman et al. 2022, Liu et al. 2022 —— 连续时间速度场生成框架\n- Li et al. 2025c —— 用速度预测误差评估样本是否服从参考分布\n- Liu et al. 2025a, McAllister et al. 2025 —— 将流匹配用于视觉或机器人策略优化，**与 RLFR 区别**：前者把流当“策略”，RLFR 把流当“奖励环境”\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **RLFR（Reinforcement Learning with Flow Rewards）**，把「潜空间流场」当成奖励信号采集环境，以**速度偏差**作为稠密过程奖励，从而绕过二元验证的稀疏性与 logit 自奖励风险。关键步骤如下：\n\n1. 构建潜空间流场\n\n- 用**离策略高质量数据**（openr1、MMPR math 子集）预训练流网络  v_φ ，建立参考速度场。\n- 在线阶段用**拒绝采样**（ correctness 过滤）持续更新流场，缓解分布漂移。\n2. 速度偏差 → 奖励信号\n\n- 对当前策略产生的隐状态  a_k  计算\n\nR_k^(CFM)=E_(tsim T,τsim L)l[(t) / (1-t)l| ∇_(a)_(k,t)log p_(v_φ)(a_(k,t))-∇_(a)_(k,t)log p_u(a_(k,t))r|^2r]\n\n其中系数  (t) / (1-t)  自动削弱低 t 时段的高噪声；大偏差=远离参考分布→负奖励，小偏差→正奖励。\n- 理论保证：速度偏差与参考分布下的对数似然呈**负相关**（ELBO 下界），故最小化偏差等价于最大化似然。\n3. 令牌级优势塑形\n\n- 将上述流奖励  r_k^(v_φ)  经 min-max 归一化、阈值截断后，逐令牌累积折扣回报\n\nA_k=∑_(s=k)^(|a|)γ^(s-k)r_s^(v_φ)+A_o\n\n替代 GRPO 中原「整句共享」优势，实现细粒度信用分配。\n4. 整体算法\n\n- 离线阶段：在高质量数据上预训练流网络。\n- 在线阶段：交替执行\n– 策略 rollout → 用流场计算令牌级优势 → GRPO 更新策略；\n– 用 correctness 过滤新轨迹 → 更新流场。\n\n通过把「潜空间速度偏差」转化为稳定、可解释的稠密奖励，RLFR 既保留了二元验证的防 hacking 特性，又为中间推理步骤提供了低成本、可扩展的探索信号。\n\nQ4: 论文做了哪些实验？\n\n实验从**语言推理**与**多模态推理**两条主线展开，覆盖 1.5 B–8 B 参数的 Qwen/Llama 系列，共 10 余个公开基准。核心设计如下：\n\n1. 训练数据\n\n- 语言：离线流预训练 93 k openr1 数学题目 → 在线 RL 在 MATH 训练集。\n- 多模态：离线 115 k MMPR-math 子集 → 在线 RL 在 MMK12。\n2. 评测基准\n\n- 语言：AIME24/25、AMC23、MATH500、OlympiadBench，指标 Pass@1（T=0）与 Pass@32（T=0.7）。\n- 多模态：MathVista、MathVision、MathVerse、WeMath、LogicVista、VisuLogic，指标 Pass@1（greedy）。\n3. 主结果\n\n- 语言（表 1）：\n– Qwen2.5-Math-7B 上 RLFR 比 RLVR 平均 **+1.5 %**，比熵基线 **+0.9 %**；\n– Llama3.1-8B 提升更大，平均 **+5.3 %**，绝对增益最高 +7.3 %。\n- 多模态（表 2）：\n– Qwen2.5VL-3B 平均 **+2.4 %**，MathVision 单基准 +5.5 %；\n– 7B 级别与 MM-Eureka、R1-OneVision 等持平或超越，平均 **+1.9 %**。\n4. 消融实验（ multimodal 3B ）\n\n- 离线流预训练 → 贡献 +3.6 %；\n- 在线拒绝采样 → 贡献 +2.4 %；\n- 大 timestep（0.8）（去偏加权）比多 timestep 平均再 +1.7 %；\n- 用「后一令牌」条件做速度预测比前一令牌 +3.4 %；\n- 过滤阈值 η=0.6 去除噪声不可或缺。\n5. 行为分析\n\n- 流奖励**偏爱执行性令牌**（frac、sqrt、angle），抑制连接词；\n- 高熵令牌在潜空间对应大速度偏差，与奖励负相关；\n- 奖励随训练动态更新，不再完全匹配离线分布，显示**上下文依赖**而非单令牌语义。\n6. 训练曲线\n\n- 图 6 显示 RLFR 在相同迭代下准确率持续高于 RLVR，策略熵维持更高水平，响应长度健康增长，无退化。\n\n综上，实验验证了「潜空间流奖励」在**不同模型规模、模态与难度**上的**稳定增益**，且各组件对性能均有可量化贡献。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可进一步释放“流场奖励”潜力并拓展 RLFR 框架：\n\n- **潜空间规模化**\n- 测试 30 B+ 模型，观察流场是否随容量增大出现层次化结构，能否利用更深/更早层隐状态。\n- 引入多尺度流场（layer-wise 或 block-wise 并联），对不同抽象级别分别建模速度偏差。\n- **流场即策略**\n- 将当前仅用于奖励的流网络  v_φ  升级为**生成式策略**，实现“流匹配-策略”一体化，减少策略-奖励分布失配。\n- 探索流场与扩散/连续归一化模型的统一训练目标，实现单模型同时生成 token 与评估自身置信度。\n- **Test-time 流场扩展**\n- 在推理阶段用流场做**自适应步长搜索**：对速度偏差大的 token 自动触发回溯或增加 rollout 预算，实现无训练 test-time scaling。\n- 结合 Best-of-N 或 beam，用流偏差分数替代传统似然打分，减少重复与幻觉。\n- **跨任务迁移与通用域**\n- 将数学预训练流场直接迁移到代码、科学问答、逻辑谜题，验证**数学潜空间结构**是否具备通用推理先验。\n- 研究“流场蒸馏”：用大模型流场监督小模型，实现低成本跨容量奖励迁移。\n- **奖励组合与鲁棒性**\n- 引入**对抗流场**：训练一个“反奖励”流场专门识别过度自信的轨迹，与原流场做博弈式正则，抑制奖励 hacking。\n- 与二元结果奖励做动态加权  α_t · r_(bin) + (1-α_t) · r_(flow) ，学习最优融合系数  α_t  而非手工设定。\n- **理论深化**\n- 建立**流场-信息论**联系：把速度偏差与 Fisher 信息、互信息关联，量化“探索-利用”边界。\n- 推导连续时间 RL 的 Hamilton-Jacobi-Bellman 形式，证明流奖励对应最优控制中的代价函数，给出收敛速率界。\n- **系统级优化**\n- 实现**GPU 内核级**流场前向与策略 rollout 融合，减少显存拷贝；用 checkpoint-offload 支持 100 k+ token 长推理。\n- 探索**异步流场更新**：策略与流网络解耦训练，用流场 replay buffer 锁定旧分布，提升样本复用率。\n\n这些方向既能把潜空间奖励推向更大规模、更广任务，也能从理论、系统与算法层面进一步巩固 RLFR 的通用性与可扩展性。\n\nQ6: 总结一下论文的主要内容\n\n**RLFR: Extending Reinforcement Learning for LLMs with Flow Environment**\n一句话总结：把「潜空间速度偏差」当成稠密奖励，用流场环境扩展二元验证 RL，让大模型在数学与多模态推理上稳定涨点。\n\n1 背景痛点\n\n- RLVR 只靠 0/1 结果奖励，易错过局部正确轨迹 → 探索不足。\n- PRM 标注贵，logit 熵/似然存在自奖励风险 → 不可靠。\n\n2 核心思路\n\n用**流匹配**在潜空间建「参考速度场」；当前策略的隐状态若偏离该场就被惩罚，接近则奖励 → 无需人工标注即可获得**逐令牌过程奖励**。\n\n3 方法要点\n\n| 步骤 | 做法 |\n| --- | --- |\n| ① 建场 | 离线高质量数据预训练流网络  v_φ ；在线用拒绝采样（correctness 过滤）持续更新。 |\n| ② 算偏差 | 对隐状态  a_k  计算速度-分数偏差  R_k^(CFM) ，去噪加权  (t) / (1-t) ，大偏差负奖励。 |\n| ③ 塑形 | 令牌级累积折扣回报  A_k  替代整句优势，直接嵌入 GRPO。 |\n\n4 实验结果\n\n| 场景 | 模型 | 平均提升 |\n| --- | --- | --- |\n| 语言推理 | Qwen2.5-Math-7B | +1.5 % vs RLVR，+0.9 % vs 熵基线 |\n| 语言推理 | Llama3.1-8B | +5.3 % |\n| 多模态推理 | Qwen2.5VL-3/7B | +2.4 % / +1.9 %，MathVision 单基准 +5.5 % |\n\n5 贡献\n\n1. 首次把潜空间流场当奖励环境，给出稳定稠密信号。\n2. 理论连接速度偏差与对数似然 ELBO，提供负相关保证。\n3. 离/在线数据均可压缩为参考流，免人工标注。\n4. 语言+多模态多尺度实验验证通用性与可扩展性。\n\n6 未来方向\n\n- 30 B+ 大模型、跨任务迁移、test-time 流场搜索、流场-策略一体化、对抗式正则、系统级内核优化。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是RLFR%3A%20Extending%20Reinforcement%20Learning%20for%20LLMs%20with%20Flow%20Environment，链接是https://arxiv.org/pdf/2510.10201，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.10201。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-5-1760485638280","title":"Spotlight on Token Perception for Multimodal Reinforcement Learning","description":"While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through a dual mechanism: it reweights a trajectory's advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On a comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading open-source RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish a new token-level perceptual perspective for analyzing multimodal RLVR but also present a novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs.","url":"https://huggingface.co/papers/2510.09285","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.280Z","metadata":{"authors":["Siyuan Huang","Xiaoye Qu","Yafu Li","Yun Luo","Zefeng He","Daizong Liu","Yu Cheng"],"date":"","rank":6,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.09285.pdf","arxivUrl":"https://arxiv.org/abs/2510.09285","arxivId":"2510.09285","coolPaperUrl":"https://papers.cool/arxiv/2510.09285","published":"2025-10-10T11:25:33Z","updated":"2025-10-10T11:25:33.000Z","zh_summary":"虽然具有可验证奖励的强化学习（Reinforcement Learning with Verifiable Rewards，RLVR）已经提高了大型视觉语言模型（Large Vision-Language Models，LVLM）的推理能力，但大多数现有的多模态推理方法都忽视了视觉感知在RLVR优化过程中的关键作用。在本文中，我们通过标记感知的新颖视角对多模态 RLVR 进行了开创性的探索，该视角衡量了每个生成的标记的视觉依赖性。通过对思维链（CoT）过程的精细分析，我们发现了两个关键见解：首先，推出轨迹中的标记感知分布稀疏，只有一小部分标记对视觉基础推理具有高度的视觉依赖性;其次，不同的轨迹在整体视觉依赖性上表现出显着的差异。基于这些观察结果，我们提出了视觉感知策略优化（VPPO），这是一种新颖的策略梯度算法，它明确地利用标记感知来细化学习信号。具体来说，VPPO 通过双重机制实现这一目标：它通过其整体视觉依赖性重新加权轨迹的优势，并将政策更新专门集中在感知上关键的代币上。在一套包含八个感知和推理基准的综合套件中，VPPO 与领先的开源 RL 调整模型相比表现出显着优势，其有效性在 7B 和 32B 模型规模上得到一致验证。研究结果不仅为分析多模态RLVR建立了新的token级感知视角，而且提出了一种新颖有效的优化策略，以显著增强LVLM的多模态推理能力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心针对的是**多模态强化学习（RLVR）中视觉感知被忽视**的问题。现有方法把视觉-语言模型当“黑箱”直接优化，所有 token 共享同一个稀疏的 0/1 答案奖励，导致：\n\n1. 轨迹级歧义：多条回答路径只要终点正确就被同等对待，无法区分“真正依赖视觉”与“靠语言先验猜中”。\n2. 令牌级稀释：奖励被平均摊到全部 token，关键的几处“看图瞬间”信号被大量语言模板 token 淹没。\n\n作者首次提出\\*\\*“令牌感知”视角\\*\\*，量化每个生成令牌对视觉输入的依赖程度，并据此设计 VPPO 算法，在轨迹层面重加权优势、在令牌层面只更新高视觉依赖的“关键令牌”，从而把强化学习信号精准地集中到真正需要视觉信息的推理环节，显著提升多模态推理性能。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归为三大类，并指出它们与 VPPO 的核心区别：\n\n1. 多模态推理（Multimodal Reasoning）\n\n- 代表工作：GPT-4o、Gemini-1.5、Qwen2.5-VL 等基础模型；以及 Insight-V、OpenThinker-Img、Vision-R1 等链式思考（CoT）框架。\n- 共同点：聚焦如何“让模型看懂图再推理”。\n- 与 VPPO 区别：它们仅提出模型结构或提示策略，**未在 RL 训练阶段显式刻画视觉依赖**。\n2. 数据/奖励工程为主的多模态 RL\n\n- 数据-centric：MM-Eureka、ThinkLite、R1-ShareVL、NoisyRollout 等，通过精选蒸馏数据或数据增强提升性能。\n- 奖励-centric：PAPO-D、SRPO、Perception-R1 等，设计更精细的感知敏感奖励。\n- 算法调整：DAPO 提出动态采样与 clip-higher 技巧。\n- 与 VPPO 区别：上述方法**仍把同一奖励无差别地广播给所有 token**，未在梯度层面区分“哪些 token 真在看图”。\n3. 文本域“关键令牌”研究\n\n- 代表工作：识别高熵分叉点、低置信错误点、对比正确/错误轨迹找“关键令牌”。\n- 与 VPPO 区别：这些指标**仅适用于纯文本**，无法定位“视觉-语言交叉时刻”；VPPO 首次提出**基于视觉依赖**来定义并筛选多模态关键令牌，并嵌入策略梯度更新。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **Visually-Perceptive Policy Optimization（VPPO）**，通过“令牌级视觉依赖”这一全新信号，把强化学习更新精准地集中到真正需要看图的时刻。整体流程分三步：\n\n1. 量化令牌视觉依赖\n对每条轨迹中的每个令牌  o_t ，计算\n\nS_t = D_(KL)!l(π_θ(·|s_t,I)parallelπ_θ(·|s_t,I')r)\n\n其中  I'  为随机 patch 黑化后的扰动图像。 S_t  越大，表示该令牌预测分布对视觉输入越敏感。\n\n2. 两级信号调制\n\n- 宏观-轨迹优势塑形（TAS）\n用轨迹内平均依赖  bar S(τ_i)  对 GRPO 优势做线性重缩放：\n\nα(τ_i)= β_(min)+(β_(max)-β_(min))(bar S(τ_i)-min_j bar S(τ_j)) / (max_j bar S(τ_j)-min_j bar S(τ_j))\n\n使得“高视觉依赖轨迹”获得更大更新权重。\n\n- 微观-令牌梯度过滤（TGF）\n仅保留每条轨迹中依赖最高的 top-k% 令牌，构造二元掩码  m_(i,t) ，策略梯度只在这些位置回传：\n\n∇_θ mathcal L_(VPPO)propto m_(i,t)· clip!l(r_(i,t)(θ),1!-!varepsilon,1!+!varepsilonr)· hat A'_i\n\n3. 理论保证\n在标准假设下，VPPO 梯度方差近似为\n\nVar(g_(VPPO))≈ k·mathbb E[α(τ)^2]· Var(g_(GRPO))\n\n由于 kll 1 且 $mathbb E\nα^2\n≈ 1$，方差显著降低，训练更稳定、收敛更快。\n\n通过“轨迹重加权 + 令牌稀疏更新”，VPPO 把统一、稀疏的 0/1 奖励转化为**视觉感知敏感、低方差**的学习信号，从而在多模态推理 benchmark 上取得一致且显著的性能提升。\n\nQ4: 论文做了哪些实验？\n\n实验围绕“**是否有效**”与“**为何有效**”两条主线展开，覆盖 8 个主流多模态推理 benchmark，并在 7 B 与 32 B 双尺度上验证可扩展性。核心结果与消融如下：\n\n1. 主实验（8 基准，平均 accuracy@8）\n\n- 7 B 组：VPPO 57.5 %，**领先最强开源对手 PAPO-D（55.3 %）2.2 个百分点**；相对基线 DAPO（55.0 %）提升 2.5 个百分点。\n- 32 B 组：VPPO 64.6 %，**领先 MM-Eureka-32 B（60.8 %）3.8 个百分点**；相对基线（57.0 %）提升 7.6 个百分点。\n- 训练曲线显示 VPPO 收敛更快、抖动更小。\n2. 消融实验（7 B 模型，同一训练集）\n\n- 组件剥离\n– 仅 TAS：56.3 %\n– 仅 TGF：57.1 %\n– TAS+TGF（完整 VPPO）：57.5 %\n二者正交，组合最佳。\n\n- 关键超参\n– 梯度过滤比例 k：0.4 时平均 57.5 % 达峰值；k 过小信号不足，过大引入噪声。\n– 优势塑形区间 \nβmin,βmax\n：固定 0.9–1.1 与动态上界差距 <0.4 %，βmin=0.9 最稳健。\n\n- 替代信号对比\n– 随机选 40 % token：54.8 %\n– 熵选 top-k（0.2–0.8）：55.3 %–56.0 %\n– 视觉依赖（VPPO）：57.1 %\n证明“视觉依赖”比纯文本熵指标更能捕捉多模态关键令牌。\n\n- 掩码策略与度量方式\n– 随机 patch 黑化（默认）> 高斯噪声/模糊/全灰，平均领先 1.0 %。\n– KL 散度 > JS 散度 > Top-1 概率差，最大领先 7.2 %。\n\n- rollout 组大小 G\nG=5/8/12/16 对应平均 56.5/57.5/57.4/57.7 %；G=8 已捕获主要收益，再增大回报递减。\n\n3. 定性案例\n在几何、圆性质、三角函数三类题目中， baseline 出现“看图错位、定理误用、数值张冠李戴”等典型失败；VPPO 高依赖令牌恰好落在“半径相等、圆周角定理、30-60-90 边比例”等关键视觉-概念节点，八次生成均正确，验证其令牌级感知精准性。\n\n4. 理论验证\n推导显示 VPPO 梯度方差约为 GRPO 的 $k·mathbb E\nα^2\n≈ 0.4$ 倍，与训练曲线观测到的更低抖动、更快收敛一致。\n\n综上，实验从**性能、收敛行为、组件贡献、超参敏感性、掩码/度量选择、理论方差**等多角度证明：VPPO 通过视觉依赖驱动的两级信号调制，可稳定、显著地提升多模态大模型在数学、几何、逻辑等推理任务上的表现，并具备良好的规模扩展性。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可继续推进，按“**方法-效率-场景-理论**”四象限归纳：\n\n方法层面\n\n- **更细粒度的感知信号**\n当前仅用 KL 度量“单图→无图”差异，可引入：\n\n- 区域级依赖：将图像划分语义区域，计算 token 对每块区域的梯度/注意力，实现“token↔patch”双向定位。\n- 跨模态互信息： I(o_t; V_(region)) ，直接量化文本令牌与特定视觉区域的共享信息，减少扰动带来的噪声。\n- **动态稀疏掩码**\n固定 top-k% 可能过剪或欠剪；可借鉴稀疏激活网络，让模型在每步自回归时预测“是否需要视觉”，实现**样本自适应**的令牌掩码。\n\n- **层次化依赖聚合**\n除轨迹均值外，可尝试：\n\n- 滑动窗口峰值检测——捕捉局部“看图高峰”；\n- 聚类识别多条轨迹的公共“视觉瓶颈”——用于课程学习，先训高共识段。\n\n效率层面\n\n- **单前向近似**\n二次前向带来 10% 训练耗时。探索：\n\n- 梯度掩码缓存——用小型“依赖探针”网络一次性预测整句依赖；\n- 零样本估计——利用注意力分布或隐藏状态余弦跳变，免扰动图像即可近似 KL。\n- **异构加速**\n视觉编码与语言模型分离部署，把依赖计算 offload 到专用视觉编码器，主模型仅接收稀疏梯度，适合千亿规模模型 pipeline 并行。\n\n场景层面\n\n- **更大规模与任务迁移**\n- 72 B+ 参数：验证极端规模下视觉依赖是否仍呈稀疏分布，以及 β、k 等超参是否需重新搜索。\n- 创意/开放生成：图像故事、广告文案等“无唯一答案”任务，需将二元正确奖励替换为**视觉相关度+人类偏好**的混合奖励，再应用 VPPO 框架。\n- **视频与时间感知**\n将“token 视觉依赖”升级为“token-帧依赖”，用 KL 度量随时间变化，定位关键帧，实现长视频推理的稀疏更新。\n\n- **跨语言与多文化**\n检验非拉丁语系（中文、阿拉伯文）是否同样满足“稀疏关键令牌”假设，及不同文化图示(算盘、几何证明风格)对依赖分布的影响。\n\n理论层面\n\n- **方差下界与最优稀疏率**\n当前给出方差上界 $k·mathbb E\nα^2\n$，可进一步推导：\n\n- 给定固定样本量，最优 k 与任务难度、视觉-文本重叠度的解析关系；\n- 引入偏差-方差权衡，证明存在唯一最优 k\\* 使期望回报最大。\n- **与信息瓶颈的关联**\n把 VPPO 视为对“视觉-语言互信息”进行瓶颈约束，探讨其最大化  I(Y;V)  同时最小化  I(O;V)  的泛化误差界，为多模态 RL 提供信息论解释。\n\n- **对抗与鲁棒性**\n研究当视觉输入遭受对抗扰动时，高依赖令牌是否最先失效；利用 VPPO 的稀疏掩码作为“鲁棒性感知探针”，反向增强图像预处理器。\n\n数据与评测\n\n- **构建“视觉捷径”诊断集**\n主动生成“文本冗余/视觉必需”双版本题目，量化模型在 VPPO 前后对“视觉捷径”的依赖变化，验证是否真正“被迫看图”。\n\n- **人类眼动对齐**\n采集人做几何题时的眼动热图，与 VPPO 高依赖令牌时间轴对齐，评估二者一致性，提升可解释性。\n\n简言之，**更精细的依赖度量、更高效的训练实现、更广泛的场景验证、更严谨的理论刻画**四条线，均可作为后续深入探索的切入点。\n\nQ6: 总结一下论文的主要内容\n\n论文核心内容可概括为“**一个瓶颈、两大发现、一套算法、全面验证**”：\n\n1. 指出瓶颈\n现有视觉-语言强化学习（RLVR）把稀疏的 0/1 答案奖励**无差别地广播给所有 token**，忽视“哪些令牌真需要看图”——导致视觉感知信号被海量语言模板稀释，多模态推理难以精进。\n\n2. 实证两大发现\n\n- **令牌视觉依赖极度稀疏**：仅 top-k% 令牌对图像敏感，其余为低依赖“闲聊”。\n- **轨迹间视觉 grounding 差异显著**：同一答案下，部分路径真正“看图说话”，部分靠语言捷径。\n3. 提出 VPPO 算法\n以“视觉依赖”为杠杆，做两级调制：\n\n- **宏观**——按轨迹平均依赖重缩放优势，优先学习高视觉投入路径；\n- **微观**——只在前 k% 高依赖令牌上回传梯度，屏蔽噪声。\n理论证明方差降低 ≈ k·E\nα²\n，训练更稳更快。\n4. 全面验证\n\n- 8 个数学/几何/逻辑/多学科 benchmark、7 B & 32 B 双尺度：VPPO 平均提升 2.2–7.6 个百分点，**开源第一**。\n- 大量消融显示两大组件正交互补，超参、掩码策略、度量方式均经严格对比，定性案例可视化关键令牌恰好落在几何定理、数值等“看图瞬间”。\n\n综上，论文首次把“令牌级视觉感知”引入多模态 RL，用稀疏而精准的信号替代粗放奖励，为提升大型视觉-语言模型推理能力提供了新视角与即插即用的新算法。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Spotlight%20on%20Token%20Perception%20for%20Multimodal%20Reinforcement%20Learning，链接是https://arxiv.org/pdf/2510.09285，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.09285。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-6-1760485638281","title":"AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration","description":"Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings.","url":"https://huggingface.co/papers/2510.10395","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.281Z","metadata":{"authors":["Xinlong Chen","Yue Ding","Weihong Lin","Jingyun Hua","Linli Yao","Yang Shi","Bozhou Li","Yuanxing Zhang","Qiang Liu","Pengfei Wan","Liang Wang","Tieniu Tan"],"date":"","rank":7,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.10395.pdf","arxivUrl":"https://arxiv.org/abs/2510.10395","arxivId":"2510.10395","coolPaperUrl":"https://papers.cool/arxiv/2510.10395","published":"2025-10-12T01:20:22Z","updated":"2025-10-12T01:20:22.000Z","zh_summary":"视听视频字幕生成旨在生成语义丰富且在时间上与视觉和听觉事件对齐的描述，从而有利于视频理解和生成。在本文中，我们提出了AVoCaDO，一种强大的视听视频字幕生成器，其驱动力来自音频和视觉模态之间的时间协同。我们提出了一个两阶段的后训练流程： (1) AVoCaDO SFT，对模型在新整理的107K高质量、时间对齐的视听字幕数据集上进行微调；(2) AVoCaDO GRPO，利用定制的奖励函数进一步增强时间一致性和对话准确性，同时规范字幕长度并减少崩溃。实验结果表明，AVoCaDO在四个视听视频字幕生成基准上显著优于现有的开源模型，并在仅视觉输入的VDC和DREAM-1K基准上也取得了具有竞争力的表现。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文针对“视听视频字幕生成”（audiovisual video captioning）中两个核心缺陷：\n\n1. 现有方法普遍**视觉中心**，忽视音频里富含的语义（对白、旁白、背景音乐等），导致字幕与真实视听体验脱节。\n2. 简单地把独立生成的视觉字幕与音频字幕拼接，无法刻画**细粒度时间对齐**与**跨模态因果关联**，在需要同步理解视听事件的场景下可靠性低。\n\n为此，作者提出 AVoCaDO，目标是在**单一模型内**联合建模视觉与音频信号，并显式保证二者在**时间轴上的同步**，从而输出语义丰富、时序一致的视听字幕，提升视频理解与后续生成任务的质量。\n\nQ2: 有哪些相关研究？\n\n- **Video-LLM 字幕方向**\n- OwlCap、Tarsier 系列：通过大规模 SFT 数据提升**视觉细节**与**动态描述**能力，但仍为**纯视觉**。\n- VideoChat-R1、VersaVid-R1、VideoCap-R1：首次用 RL（奖励）优化**视觉字幕**的完整性、准确性，**未引入音频**。\n- **视听联合理解模型**\n- video-SALMONN-2、UGC-VideoCaptioner：同期工作，开始探索**视听字幕**，但前者需六轮 DPO 且仅按原子事件挑样本，后者仅面向**短视频**；二者均**未显式对齐**跨模态时序。\n- Qwen2.5-Omni、HumanOmniV2、MiniCPM-o-2.6 等通用视听模型：具备音频-视觉 token 交错能力，却**未针对字幕任务**做专门后训练。\n- **RL 在视频 LLM 中的应用**\n- Video-R1、VerIPO、LongVILA-R1：用 GRPO 与规则奖励提升**通用视频理解**。\n- Time-R1、TAR-TVG、Tempo-R0：用 IoU 奖励做**时序定位**，均**不面向长描述生成**。\n\n综上，**尚无工作**在视听字幕场景下，通过轻量级两阶段后训练同时解决**跨模态时间对齐**、**对白准确性**、**长度控制**与**重复塌陷**问题；AVoCaDO 填补了这一空白。\n\nQ3: 论文如何解决这个问题？\n\n论文将问题拆解为「数据质量」与「训练目标」两大瓶颈，提出两阶段后训练流程，核心思路是**先获得高质量时序对齐数据，再用细粒度奖励持续优化**。具体方案如下：\n\n1\\. AVoCaDO-SFT：构造 107 k 高质量「时序对齐」视听字幕对\n\n- **数据来源**\n- 24 k TikTok-10M + 18 k ShortVideo：口语、背景音乐、音效密集的真实短视频\n- 20 k Shot2Story：多镜头影视片段，强化跨镜头叙事能力\n- 29 k FineVideo + 11 k YouTube-Comm"}},{"id":"hf-7-1760485638281","title":"DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training","description":"In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.","url":"https://huggingface.co/papers/2510.11712","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.281Z","metadata":{"authors":["Haoran Feng","Dizhe Zhang","Xiangtai Li","Bo Du","Lu Qi"],"date":"","rank":8,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.11712.pdf","arxivUrl":"https://arxiv.org/abs/2510.11712","arxivId":"2510.11712","coolPaperUrl":"https://papers.cool/arxiv/2510.11712","published":"2025-10-13T17:59:15Z","updated":"2025-10-13T17:59:15.000Z","zh_summary":"在这项工作中，我们提出了 DiT360，一种基于 DiT 的框架，对透视图像和全景图像数据进行混合训练，用于全景图像生成。针对保持几何保真度和生成质量的真实感问题，我们认为主要原因在于缺乏大规模、高质量的真实世界全景数据，这种以数据为中心的视角不同于以往注重模型设计的方法。基本上，DiT360 包含几个关键模块，用于跨域转换和域内增强，分别应用于 VAE 前的图像级别和 VAE 后的 token 级别。在图像级别，我们通过透视图像引导和全景图像精炼来融入跨域知识，这不仅提升了感知质量，还规范了多样性和真实感。在 token 级别，我们在多个模块中应用混合监督，这些模块包括用于边界连续性的循环填充、用于旋转鲁棒性的偏航损失以及用于畸变感知的立方体损失。在文本到全景、图像修复和图像扩展任务上的大量实验证明，我们的方法在十一项量化指标上实现了更好的边界一致性和图像保真度。我们的代码可在 https://github.com/Insta360-Research-Team/DiT360 获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**高质量、真实感 360° 全景图像生成**中的两大核心难题：\n\n1. **几何保真度不足**\n极区严重畸变、0°/360° 经线边界不连续，导致球面结构失真。\n\n2. **真实感缺失**\n现有全景数据集规模小、质量低，模型倾向于“渲染感”而非照片级真实感。\n\n为此，作者提出**数据视角**而非单纯模型结构的创新：\n在**极缺真实全景数据**的现实中，通过\\*\\*“混合训练”**把**大规模高质量透视图像**与**有限全景数据**联合利用，并在**图像级**与**token 级**同时施加几何一致性约束，实现兼具**几何正确性**与**照片真实感\\*\\*的全景图像生成。\n\nQ2: 有哪些相关研究？\n\n论文第 2 节“Related Work”将已有研究归为三大脉络，并指出其与 DiT360 的差异。按议题梳理如下：\n\n- **Text-to-Image Diffusion Models**\n- Latent Diffusion / LDM：Rombach et al., 2022\n- UNet 高分辨率改进：Podell et al., 2023 (SDXL)\n- Transformer 主干：Peebles & Xie, 2023 (DiT)；Black Forest Labs, 2024 (Flux)\n这些工作均以透视图像为训练集，未考虑全景几何。\n- **Panoramic Image Generation**\n\n1. **Outpainting-based**\nAkimoto et al., 2022；Dastjerdi et al., 2022；Wu et al., 2023b/c —— 从窄视场逐步外扩，灵活性差。\n2. **Text-to-Panorama 两条技术路线**\n- **多视角拼接**：Bar-Tal et al., 2023；Li & Bansal, 2023；Tang et al., 2023；Park et al., 2025 —— 易出现重复目标、接缝断裂。\n- **直接 ERP 训练**：Chen et al., 2022；Zhang et al., 2023；Sun et al., 2025 —— 边界不连续、极区畸变严重。\n3. **立方图或球面卷积改进**\nSong et al., 2023；Ye et al., 2024；Kalischek et al., 2025 —— 缓解畸变但带来面间不连续，且仍受限于低质全景数据。\n- **数据稀缺与质量瓶颈**\n现有方法普遍依赖 Matterport3D 等少量真实全景，或从 YouTube 直接爬取未筛选数据，导致“渲染感”强、极区模糊。DiT360 首次**系统性引入大规模高质量透视图像**作为正则，弥补数据缺口，并设计跨域/跨表示级监督，与上述“纯全景”或“纯模型结构”思路形成区别。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **DiT360** 框架，以“数据-视角”而非“模型-视角”切入，通过**混合训练**把**高质量透视图像**与**有限全景数据**联合利用，并在**图像级**与**token 级**同时施加几何一致性约束，具体方案如下：\n\n1\\. 混合训练范式（Perspective + Panoramic）\n\n| 层级 | 目的 | 关键模块 | 公式/操作 |\n| --- | --- | --- | --- |\n| 图像级 | 补足真实感、去极区模糊 | ① 全景精炼② 透视引导 | ① 立方图 inpainting： I_(mask) = Iodot M+(1-M)· I_(miss) ② 透视→ERP 重投影损失： Lperspective=|εodot M - hatεθodot M|^2  |\n| token 级 | 保证球面几何一致性 | ① 循环 padding② yaw 旋转损失③ 立方图损失 | ①  tilde X_t = [X_(-1), X_t, X_0] ②  Lyaw=|εθ,yaw-ε_(yaw)|^2 ③  Lcube=|εθ,cube-ε_(cube)|^2  |\n\n2\\. 统一损失函数\n\n将上述约束与标准扩散损失相加：\n\nL_(pano) = L_(MSE) + λ_1L_(cube) + λ_2L_(yaw)\n\n3\\. 零样本扩展能力\n\n借助 inversion+token 替换，**无需额外微调**即可直接完成全景 inpainting / outpainting。\n\n4\\. 实验验证\n\n- **定量**：11 项指标全面领先，Matterport3D 上 FID 从 46.72→42.88，BRISQUE 从 30.35→10.25。\n- **定性**：极区畸变、边界接缝、渲染感显著减少。\n- **用户研究**：63 名受试者，80.9% 整体偏好度。\n\n综上，DiT360 通过“透视-全景混合训练 + 多层级几何监督”，在**数据稀缺**的现实下同时提升**几何保真度**与**照片真实感**，实现高保真 360° 全景生成。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **文本到全景生成**、**inpainting** 与 **outpainting** 三大任务，从 **定量测评、定性可视化、用户偏好、消融分析** 四个维度展开系统实验，具体如下：\n\n1\\. 主实验：文本→全景生成\n\n- **基准对比**\n与 8 个代表性方法（PanFusion、MVDiffusion、SMGD、PAR、WorldGen、Matrix-3D、LayerPano3D、HunyuanWorld）在 **Matterport3D 验证集** 上比较。\n\n- **指标**（共 11 项）\n- 真实度：FID、FIDclip、FIDpole、FIDequ、FAED、BRISQUE、NIQE\n- 多样性：IS\n- 图文对齐：CS\n- 感知质量：QAquality、QAaesthetic\n- **结果**\n在 **9/11 项指标** 中排名第一，其余两项差距微小（CLIP Score 与 QAquality 偏向透视图像，对全景不完全适配）。\n\n2\\. 定性对比\n\n- 图 4 & 图 8（附录）给出 **全幅可视化** 与 **局部放大**，红框标出典型瑕疵：\n– 极区扭曲、边界断裂、重复目标、渲染感。\nDiT360 在 **细节锐度、结构连贯、畸变抑制** 上显著优于基线。\n\n3\\. 用户研究\n\n- **63 名受试者**，双盲随机展示同一 prompt 下不同方法结果。\n- **四维度打分**：文本对齐、边界连续性、真实感、整体质量。\n- **结果**（表 3）\nDiT360 在 **整体质量** 上获得 **80.9% 偏好率**，远超第二名 13.7%。\n\n4\\. 消融实验\n\n- **基线**：Flux + LoRA（仅基础扩散损失）。\n- **逐模块添加**：循环 padding、cube 损失、yaw 损失、透视引导。\n- **观测指标**：同上 11 项。\n- **结论**（表 2 & 图 5）\n– 循环 padding → 边界连续性↑，FID↓3.0。\n– cube 损失 → 极区 artifact↓，IS↑0.09。\n– yaw 损失 → 旋转鲁棒性↑，FAED↓0.32。\n– 透视引导 → 真实感↑，QAaesthetic↑0.22。\n**全模块组合** 取得最佳平衡。\n\n5\\. 扩展任务验证\n\n- **Inpainting / Outpainting**\n利用 inversion + token 替换，**无需额外训练**。\n图 1、图 7、图 9-10 展示多样场景（室内、户外、天空、极区）均得到 **语义连贯、无缝边界** 的补全/外推结果。\n\n6\\. 实现细节与可复现性\n\n- **训练**：5×H20，20 epoch，batch=1，grad-accum=3，lr=2e-5，guidance=1.0。\n- **推理**：28 步，guidance=3.0，1024×2048 直接生成。\n- **代码与权重**：已开源（GitHub 链接在摘要末）。\n\n综上，论文通过 **全面指标、可视化、用户偏好与消融** 共同验证：\nDiT360 在 **几何保真、边界连续、照片真实感** 上均优于现有方法，且可零样本完成 inpainting/outpainting。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 DiT360 的“直接外延”与“深层拓展”，均围绕**数据、几何、语义、交互、三维化**五个维度展开：\n\n1\\. 数据层面：更大、更全、更真\n\n- **超大规模多场景透视-全景配对**\n构建 100 k+ 级别的“城市-自然-室内”多风格配对，解决当前 40 k 透视 + 10 k 全景的覆盖盲区（人脸、高光、复杂材质）。\n- **自动地平线校正与美学评分流水线**\n目前依赖人工筛选，可训练“全景质量评判模型”实现**全自动清洗+标注**，降低数据成本。\n- **合成数据增强**\n利用 NeRF/GS 生成**可控畸变、可控光照、可控布局**的伪全景，再用 DiT360 做**自监督微调**，突破真实采集瓶颈。\n\n2\\. 几何层面：从 ERP 到任意球面表示\n\n- **混合球面表示训练**\n同时支持 ERP、立方图、icosahedron、HEALPix，设计**可学习投影嵌入**，让模型自动选择最优表示。\n- **球面谱域损失**\n将噪声/图像转换到球面调和域，直接在**频域**约束低阶系数，可**理论保证**旋转不变性与极区连续性。\n- **显式深度/法向监督**\n引入单目深度估计器，对极区施加**深度一致性损失**，进一步抑制“拉伸”与“飞点”。\n\n3\\. 语义层面：细粒度控制与多模态\n\n- **分层文本-布局-语义生成**\n支持“天空-建筑-地面”分层 prompt，或输入**粗糙语义图**实现**布局+纹理联合生成**。\n- **跨模态声音/语义一致性**\n联合 360° 环境声与图像，实现**视听一致**的沉浸式场景生成。\n- **时序一致全景视频生成**\n在 latent 空间引入**时空旋转-扭曲一致性损失**，把 yaw 损失扩展为**yaw-t 损失**，生成**可循环**的 360° 视频。\n\n4\\. 交互层面：实时编辑与个性化\n\n- **全景风格化+局部重照明**\n结合光照估计网络，实现“一键昼夜/季节”切换，并保持极区阴影几何正确。\n- **用户涂鸦-引导编辑**\n支持**球面笔刷**交互：用户直接在 ERP 或立方图上勾画，模型**局部重采样**完成实时编辑。\n- **个性化少样本适配**\n用 DreamBooth/LoRA 在\\*\\*<10 张用户全景\\*\\*上微调，实现“我家客厅”个性化生成。\n\n5\\. 三维化：从 2.5D 到真正 3D\n\n- **全景 → 3D Gaussian Splatting 逆过程**\n生成同步输出**全景 + 深度 + 法向**，直接初始化 3D-GS，实现**文本到 3D 场景**秒级重建。\n- **球面 NeRF 蒸馏**\n利用 DiT360 作为**球面 NeRF 的先验**，在极区缺失视角提供**幻觉补全**，解决 NeRF 在 360° 场景中的“极点退化”。\n- **多房间/多层一致性**\n引入**图神经网络**对“房间-走廊-楼层”拓扑建模，实现**建筑级**一致的全景序列生成。\n\n6\\. 理论层面：统一框架与可解释性\n\n- **球面扩散理论**\n建立**球面随机过程**的连续形式，证明 yaw/cube 损失对应**球面布朗运动**的生成式 Score Matching。\n- **跨域最优传输解释**\n把透视→ERP 重投影视为**Wasserstein 映射**，用最优传输距离替代 MSE，提升**分布级**一致性。\n- **可解释注意力热图**\n可视化 DiT 的**球面注意力 模式**，分析模型如何“感知”极区与边界，指导**结构改进**。\n\n7\\. 应用层面：垂直场景落地\n\n- **自动驾驶：360° 极端场景仿真**\n生成罕见天气、异常障碍物全景，用于**感知模型压力测试**。\n- **AR/VR：注视点自适应生成**\n结合眼动仪，**动态高分辨率**生成用户注视区域，其余部分保持低分辨率，实现**可变分辨率**全景流式传输。\n- **文化遗产：破损古迹全景修复**\n利用 inpainting 模块，对**文物扫描全景**进行**缺失补全**与**原貌还原**。\n\n简言之，DiT360 打开了“透视-全景混合训练”的通路，后续可在\n**数据规模、球面几何、多模态控制、三维升维、理论框架**五个方向持续深挖，形成**从 2D 全景到 3D 场景、从被动生成到主动交互**的完整研究闭环。\n\nQ6: 总结一下论文的主要内容\n\nDiT360 提出“数据-视角”而非“模型-视角”的创新路线，通过**混合训练**把**高质量透视图像**与**有限全景数据**联合利用，并在**图像级**与**token 级**同时施加几何一致性约束，实现高保真 360° 全景生成。核心内容可概括为：\n\n1\\. 问题定位\n\n- 全景生成缺**大规模真实数据**，导致**极区畸变**与**边界不连续**，缺乏**照片级真实感**。\n\n2\\. 方法框架\n\n| 层级 | 模块 | 作用 |\n| --- | --- | --- |\n| 图像级 | ① 全景精炼（立方图 inpainting）② 透视引导（重投影+MSE） | 去极区模糊引入真实感 |\n| token 级 | ① 循环 padding② yaw 旋转损失③ cube 损失 | 保证 0°/360° 连续旋转鲁棒抑制畸变 |\n\n统一损失： \nL_(pano) = L_(MSE) + λ_1L_(cube) + λ_2L_(yaw)\n\n3\\. 实验结果\n\n- **定量**：Matterport3D 上 11 项指标 **9 项第一**，FID 从 46.72→42.88，BRISQUE 从 30.35→10.25。\n- **定性**：极区扭曲、边界断裂、渲染感显著减少。\n- **用户研究**：63 人双盲，**80.9% 整体偏好率**。\n- **消融**：各模块均带来可度量提升，组合最佳。\n\n4\\. 零样本扩展\n\n- 借助 inversion+token 替换，**无需额外训练**即可完成 **inpainting / outpainting**。\n\n5\\. 贡献总结\n\n- 提出**透视-全景混合训练**范式，缓解数据稀缺。\n- 设计**图像级+token 级**多层级几何一致性约束。\n- 在**文本→全景、inpainting、outpainting**三大任务上实现**SOTA 真实感与几何保真度**。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是DiT360%3A%20High-Fidelity%20Panoramic%20Image%20Generation%20via%20Hybrid%20Training，链接是https://arxiv.org/pdf/2510.11712，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11712。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-8-1760485638282","title":"Demystifying Reinforcement Learning in Agentic Reasoning","description":"Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL","url":"https://huggingface.co/papers/2510.11701","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.282Z","metadata":{"authors":["Zhaochen Yu","Ling Yang","Jiaru Zou","Shuicheng Yan","Mengdi Wang"],"date":"","rank":9,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.11701.pdf","arxivUrl":"https://arxiv.org/abs/2510.11701","arxivId":"2510.11701","coolPaperUrl":"https://papers.cool/arxiv/2510.11701","published":"2025-10-13T17:57:15Z","updated":"2025-10-13T17:57:15.000Z","zh_summary":"最近，主动型强化学习（agentic RL）的出现表明，强化学习也能够有效提升大语言模型（LLMs）的主动推理能力，但其关键设计原则和最佳实践仍不清楚。在本研究中，我们从数据、算法和推理模式三个关键角度，对主动型推理中的强化学习进行全面系统的研究，并揭示其奥秘。我们强调的关键见解包括：(i) 用真实的端到端工具使用轨迹替代拼接的合成轨迹，可以显著提升SFT初始化；高多样性、模型感知的数据集能够持续促进探索，并显著改善强化学习性能。(ii) 探索友好的技术对主动型RL至关重要，例如提升剪辑上限、延长奖励塑形以及保持适当的策略熵，这些都可以提高训练效率。(iii) 较少工具调用的深思熟虑策略优于频繁工具调用或冗长的自我推理，从而提升工具使用效率和最终准确率。这些简单的做法能够持续增强主动推理能力和训练效率，在使用较小模型的情况下也能在具有挑战性的基准上取得优异成果，为未来主动型RL研究建立了实用基线。除了这些实证见解，我们还贡献了高质量的真实端到端主动SFT数据集以及高质量的RL数据集，并在四个具有挑战性的基准（包括AIME2024/AIME2025、GPQA-Diamond和LiveCodeBench-v6）上展示了我们方法提升LLM主动推理能力的有效性。通过我们的方案，即使是4B参数规模的模型，也能实现超过32B模型的主动推理性能。代码和模型：https://github.com/Gen-Verse/Open-AgentRL","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文核心定位\n现有大模型在“agentic reasoning”阶段（即多轮调用外部工具完成复杂任务）仍缺乏系统、可复现的 RL 训练范式。作者将问题拆成三轴：\n\n1. 数据轴： stitched 合成轨迹无法还原真实多轮决策信号，导致 SFT 冷启动弱、RL 探索空间窄。\n2. 算法轴： 直接把 GRPO 类算法搬来会因保守裁剪、强 KL 惩罚、稀疏奖励而迅速熵塌缩，训练不稳定。\n3. 推理模式轴： 对“何时调用工具、调用几次、内部思考长度如何分配”尚无定量原则，易出现 over-/under-thinking。\n\n因此，论文旨在“demystify”——通过大规模对照实验给出可落地的数据构造、算法改进与推理策略的最优实践，使小模型（4 B）也能在 AIME、GPQA、LiveCodeBench 等硬基准上取得 SOTA 级 agentic 表现。\n\nQ2: 有哪些相关研究？\n\n- **工具集成推理（TIR）**\n- 提示工程：PoT、BoT\n- SFT 方法：ToRA、Toolformer、QwenMath-TIR、ReAct、MathCoder、Mario\n- **Agent 强化学习**\n- 搜索增强：Search-R1、R1-Searcher、Search-o1\n- 代码工具：ToRL、ReTool、ZeroTIR\n- 多工具框架：ARTIST、Tool-Star、Auto-TIR\n- **熵机制与 RLVR**\n- 熵塌缩分析：Cui et al. 2025\n- 高熵 token 驱动：Wang et al. 2025b\n- 熵感知目标：Cheng et al. 2025a、Dong et al. 2025b\n- 探索-利用权衡：Deng et al. 2025\n\nQ3: 论文如何解决这个问题？\n\n论文采用“三轴拆解 + 对照实验 + 开源基准”的策略，系统性地给出可复现的最优实践。\n\n1. 数据轴\n\n- 构造 3 k 条**真实端到端**工具调用轨迹，替代 stitched 合成数据，显著提升 SFT 冷启动。\n- 混合数学、科学、代码 30 k 样本，保持**高多样性**，维持策略熵与探索空间。\n- 引入**模型感知难度过滤**：按 8-shot 正确率划分易/中/难，动态适配不同容量模型，缓解梯度信号稀疏问题。\n2. 算法轴\n\n- 在 GRPO 框架内提出三条改进：\n– **Token-level 损失**（vs 序列级）使每 token 均等贡献梯度，利于强模型快速收敛。\n– **放宽裁剪上限**（ε\\_high=0.315）扩大探索预算，避免过早熵塌缩。\n– **Overlong 奖励塑形**（rlength）对超长回答线性惩罚，抑制“废话”生成，稳定训练信号。\n- 组合为 GRPO-TCR 配方，在 450 步内把 4 B 模型 AIME2025 average@32 从 33 % 提升到 70 %。\n3. 推理模式轴\n\n- 通过统计发现“**Deliberative 模式**”（先深思考→少而精的工具调用）成功率 >70 %，显著优于“Reactive 模式”（频繁浅调用）。\n- 针对 Long-CoT 模型“拒用工具”现象，提出**先用多轮工具 SFT 对齐**再 RL，避免内部长推理与外部工具冲突。\n- 给出定量建议：每轮响应长度 500–800 token、工具调用 ≤3 次时效率最高。\n4. 开源与验证\n\n- 发布 3 k SFT + 30 k RL 数据集、Qwen3-4B-RA-SFT 冷启动模型及 DemyAgent-4B 权重。\n- 在 AIME2024/25、GPQA-Diamond、LiveCodeBench-v6 上，4 B 模型击败 14 B/32 B 级 Agent，实现 SOTA 级 agentic 推理性能。\n\nQ4: 论文做了哪些实验？\n\n实验按“三轴”系统展开，全部在 8×A100 上完成，基座模型为 Qwen2.5-7B-Instruct 与 Qwen3-4B-Instruct-2507，统一用 temperature=1.0、top-p=0.6、32 次采样，指标为 average@32 / pass@32 / maj@32（代码任务用 pass@1&5）。\n\n1. 数据轴实验\n1.1 真实端到端 vs 合成 stitch-SFT\n\\- 训练集：3 k 真实轨迹 vs ReTool 合成轨迹\n\\- 评测：AIME2024/2025\n\\- 结果：真实轨迹使 Qwen3-4B 平均得分 +28.85 %，pass@32 +40.51 %，验证“真实轨迹冷启动更强”。\n1.2 多样性 RL 数据集消融\n\\- 训练集：30 k 混合（数学+科学+代码）vs 17 k 纯数学\n\\- 观测：前者熵值全程高 0.3 bit，150 步即达 50 % 平均准确率，后者需 220 步，证明“多样性→高熵→更快收敛”。\n1.3 模型感知难度过滤\n\\- 对 Qwen2.5-7B 保留 0.25–0.75 正确率区间样本，重训 GRPO-TCR\n\\- 结果：平均奖励从 0→0.42，AIME2025 average@32 +18 %，打破“弱模型零梯度”瓶颈。\n\n2. 算法轴实验\n2.1 三项技术叠加测试\n\\- 配方：GRPO-T（基线） vs GRPO-TCR（token+clip高+overlong） vs GRPO-SCR（sequence+clip高+overlong）\n\\- 结果：GRPO-TCR 450 步达 70 %，GRPO-T 仅 54 % 且需 4×步数；token-loss 在强模型上再 +3.9 %，验证“clip高+塑形+token-loss”有效性。\n2.2 探索-利用动态\n\\- 追踪 pass@32 与 average@32 差距\n\\- 发现 GRPO-TCR 可同时提升两者>10 %，而 GRPO-T 出现传统“pass↑ average↓” trade-off，说明“工具交互可打破传统权衡”。\n2.3 熵 regime 搜索\n\\- 对 ε\\_high∈{0.28,0.315,0.35} 做网格扫描\n\\- 结果：ε=0.315 时 Qwen2.5-7B 训练步数节省 40 %；ε=0.35 时 Qwen3-4B 反而下降 2.4 %，揭示“存在最优熵区间，过犹不及”。\n\n3. 推理模式轴实验\n3.1 工具调用频率 vs 长度统计\n\\- 统计每轮平均调用次数与响应长度\n\\- 发现高性能模型集中落在“长思考+≤3 次调用”区域，工具成功率 70 %；低性能模型呈“短思考+>5 次调用”分布，成功率 <40 %。\n3.2 Long-CoT 直接 RL\n\\- 用 Qwen3-4B-Thinking-2507 直接跑 GRPO-TCR\n\\- 结果：训练后工具调用→0，average@32 几乎不变，证明“原生 Long-CoT 拒用工具”。\n3.3 Long-CoT 先 SFT 再 RL\n\\- 先用 3 k 真实工具轨迹 SFT 对齐，再 RL\n\\- 结果：工具调用恢复，average@32 提升 12 %，但最终仍与指令模型持平，验证“指令模型从零联合学习工具+推理更高效”。\n\n4. 综合验证\n\n- 用上述最优配方训练 DemyAgent-4B\n- 在 4 个基准上与 14 B/32 B Agent 对比，取得\nAIME2024 72.6 %、AIME2025 70.0 %、GPQA-Diamond 58.5 %、LiveCodeBench-v6 26.8 %，全部进入前二，实现“小模型 SOTA”。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为该工作的直接延伸，均围绕“数据-算法-推理”三轴尚未饱和的区域展开。\n\n1. 数据稀缺与自动扩充\n\n- 小样本高质量 SFT 蒸馏：借鉴 s1、LIMO 的“less-is-more”思路，研究能否用 ≤1 k 条真实轨迹+自动质量筛选达到同等冷启动效果。\n- 可验证奖励自举：利用 outcome reward 模型对失败轨迹进行“局部修复”，自动生成新正例，降低人工标注端到端轨迹的成本。\n- 多工具轨迹合成：当前仅覆盖代码解释器，需构建同时调用搜索、数据库、计算器的跨工具轨迹，并研究跨工具依赖图的自动标注方法。\n2. 算法与训练动态\n\n- 大模型超参敏感性：论文实验止于 7 B，需验证 14 B/32 B/70 B 对 ε\\_high、β\\_KL、reward shaping 的敏感度是否呈现非线性突变。\n- 熵可控目标函数：将 ε\\_high 改为自适应熵预算——根据实时熵值动态调整 clip 范围，使“弱模型先探索、强模型后收敛”自动完成。\n- 分层 RL：把“是否调用工具”与“生成具体代码”拆成两层动作空间，用 option framework 或 hierarchical PPO 减少同时优化带来的非平稳性。\n- 离线→在线混合：先用 30 k 离线数据做保守更新，再切到实时环境在线 rollout，研究在非平稳工具 API 下的稳定性条件。\n3. 推理模式与架构\n\n- 工具规划专用模块：在 Transformer 内显式加入 tool-planning head，提前输出子任务 DAG，再驱动语言模型填充每个子任务，减少“思考-调用”耦合。\n- 预算感知解码：给每条推理路径附带“剩余 token/调用次数”嵌入，让模型在解码阶段即遵守预算硬约束，而非事后惩罚。\n- 反思与 rollback：允许模型在检测到工具返回错误后回到任意历史隐状态，引入“可回溯动作”形式化，研究其对样本效率的影响。\n- 异构工具动态注册：工具集合不再静态，模型需学会读取新工具文档（function description）并即时生成正确调用，考察 zero-shot tool generalization。\n4. 评估与可扩展性\n\n- 多轮交互代价模型：除了准确率，同时度量经济成本（API 费用、延迟），建立“accuracy-per-dollar”帕累托前沿，推动实用化。\n- 对抗性工具环境：工具返回随机错误或恶意结果，测试模型能否通过一致性检查与投票机制保持鲁棒。\n- 长周期自我改进：让 agent 在连续数天的自动迭代中自行生成新训练数据，观察是否出现“能力雪崩”或“工具滥用”失控，需设计安全熔断机制。\n5. 理论层面\n\n- 工具增强 MDP 的样本复杂度：形式化证明引入工具动作后，最优策略的覆盖维度或 Bellman rank 如何变化，给出所需样本量上界。\n- 熵-探索-利用三角约束：建立熵与 advantage variance 的定量关系，推导“最优熵区间”的闭合解，为自适应 clip 提供理论依据。\n\nQ6: 总结一下论文的主要内容\n\n**论文主旨**\n系统揭示“如何用 RL 把小模型训练成高效工具调用 Agent”，提出数据-算法-推理三轴最佳实践，4 B 模型在 AIME、GPQA、LiveCodeBench 上击败 14–32 B Agent。\n\n1\\. 关键发现速览\n\n| 维度 | 传统做法 | 本文最佳实践 | 效果 |\n| --- | --- | --- | --- |\n| 数据 | 合成 stitch 轨迹 | ① 3 k 真实端到端轨迹 SFT② 30 k 数学+科学+代码混合 RL③ 按模型能力过滤难度 | 冷启动 +28 %，熵高 0.3 bit，训练步数省 40 % |\n| 算法 | GRPO 默认 ε=0.2 | ① Token-level 损失② ε_high=0.315 放宽裁剪③ Overlong 奖励塑形 | 450 步达 70 %（原 54 % 需 1800 步） |\n| 推理 | 频繁工具调用 | 先深思考 → ≤3 次精准调用 | 工具成功率 70 % vs 40 % |\n\n2\\. 实验规模\n\n- 基座：Qwen2.5-7B / Qwen3-4B\n- 训练：8×A100，SFT 5 epoch → RL 3 epoch\n- 评测：AIME2024/25、GPQA-Diamond、LiveCodeBench-v6，32 次采样 average@32 / pass@32 / maj@32\n\n3\\. 主要贡献\n\n1. 开源 3 k 真实 SFT + 30 k 多样 RL 数据集\n2. 提出 GRPO-TCR 配方（Token-loss + Clip高 + Overlong塑形）\n3. 揭示“熵-探索-利用”定量关系，给出 ε\\_high 最优区间\n4. 4 B 模型 DemyAgent-4B 取得 SOTA agentic 成绩，超越 ReTool-32B、rStar2-Agent-14B\n\n4\\. 一句话总结\n\n“真实轨迹冷启动、宽松裁剪保熵、少而精调用”三招齐下，让小模型也能在复杂推理任务中通过 RL 高效掌握工具使用，达到大模型级表现。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Demystifying%20Reinforcement%20Learning%20in%20Agentic%20Reasoning，链接是https://arxiv.org/pdf/2510.11701，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11701。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-9-1760485638283","title":"Making Mathematical Reasoning Adaptive","description":"Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/NJUNLP/AdaR.","url":"https://huggingface.co/papers/2510.04617","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.283Z","metadata":{"authors":["Zhejian Lai","Xiang Geng","Zhijun Wang","Yang Bai","Jiahuan Li","Rongxiang Weng","Jingang Wang","Xuezhi Cao","Xunliang Cai","Shujian Huang"],"date":"","rank":10,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.04617.pdf","arxivUrl":"https://arxiv.org/abs/2510.04617","arxivId":"2510.04617","coolPaperUrl":"https://papers.cool/arxiv/2510.04617","published":"2025-10-06T09:30:05Z","updated":"2025-10-06T09:30:05.000Z","zh_summary":"数学推理是大型语言模型（LLMs）智能的重要指标。然而，现有的LLMs在稳健性和泛化能力上存在缺陷。本文将这些不足归因于虚假推理，即依赖表面特征生成答案。为应对这一挑战，我们提出了AdaR框架以实现自适应推理，其中模型依靠问题解决逻辑生成答案。AdaR通过改变变量值合成逻辑等价的查询，并使用RLVR在这些数据上训练模型，以惩罚虚假逻辑，同时鼓励自适应逻辑。为了提高数据质量，我们从原始查询中提取问题解决逻辑，并通过代码执行生成相应答案，然后进行合理性检查。实验结果表明，AdaR提高了模型的稳健性和泛化能力，在保持高数据效率的同时，在数学推理任务上取得了显著提升。分析显示，数据合成和RLVR协调工作，使LLMs能够实现自适应推理。后续分析得出了关键设计洞察，包括关键因素的影响及其在指导LLMs上的适用性。我们的项目可在https://github.com/NJUNLP/AdaR获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决大语言模型（LLM）在数学推理中出现的“虚假推理”（spurious reasoning）问题，即模型仅凭表面特征而非真正的解题逻辑给出正确答案，导致在数值扰动或跨域场景下鲁棒性与泛化能力显著下降。为此，作者提出 AdaR 框架，通过以下两步使模型具备“自适应推理”（adaptive reasoning）能力：\n\n1. 数据合成：将原始题目抽象为模板与变量集，用可执行代码刻画解题逻辑；在保持逻辑不变的前提下对变量值进行可控扰动，经代码执行与 sanity check 生成大量逻辑等价但数值不同的新题目-答案对。\n2. 强化学习：利用 RLVR 在合成数据上训练，使模型在扰动题目上因虚假推理而答错、因自适应推理而答对，从而通过奖励差异抑制虚假路径，强化依赖解题逻辑的自适应路径。\n\n实验表明，仅用 9 k 合成数据即可在 7 个内外域基准上平均提升 8.5 分，验证 AdaR 在提升鲁棒性与泛化性的同时保持高数据效率。\n\nQ2: 有哪些相关研究？\n\n与 AdaR 直接相关的研究可归纳为三条主线：① 数学鲁棒性与泛化诊断，② 数据合成与增强，③ 强化学习提升推理。按时间顺序与关联度列举如下（不含原文已引用的基础 CoT、RLVR 等奠基工作）：\n\n- **GSM-SYM**（Mirzadeh et al., 2024）\n首次系统量化 LLM 在“仅改数值、不改逻辑”的扰动下性能骤降，提出“符号模板+实例化”评测协议，为 AdaR 的扰动策略提供评测基准。\n\n- **MetaMATH**（Yu et al., 2023）\n通过复述（paraphrase）与自验证实现模板级多样性，但未对变量值进行可控扰动，仍可能强化表面模式；AdaR 将其作为对比基线，并补充了变量维度的扰动。\n\n- **MathGenie**（Lu et al., 2024）\n采用“回译”方式由答案反推新题，依赖 LLM 自身校验，易累积错误；AdaR 改用可执行代码生成答案，避免误差放大。\n\n- **SFT memorizes, RL generalizes**（Chu et al., 2025）\n实证指出监督微调易记忆 CoT 表层文本，而 RL 可抑制记忆；AdaR 据此选用 RLVR 而非 RFT，并进一步用扰动数据显式区分虚假/自适应路径。\n\n- **RLVR 的“奖励黑客”与格式敏感问题**（Guo et al. 2025b; Huang et al. 2025）\n揭示纯结果奖励可能鼓励捷径；AdaR 通过“逻辑等价但数值不同”的成对样本，使奖励信号天然携带“是否正确依赖逻辑”的信息，缓解奖励黑客。\n\n- **难度感知拒采微调 DART-Math**（Tong et al., 2024）\n按题目难度加权采样，提升 RL 效率；AdaR 的 sanity-check 过滤可看作一种“难度/有效性”控制，但核心差异在于 AdaR 用扰动产生对比样本而非难度筛选。\n\n- **代数思维与比较归纳**（Gerstenberg et al., 2015; Kieran, 2004）\n认知科学提出“通过比较不同实例以抽取不变逻辑”是人类形成代数思维的关键；AdaR 的成对扰动+RL 正是让模型经历类似“比较-归纳”过程。\n\n- **大规模数学合成数据 MathScale**（Tang et al., 2024）\n沿“知识点→习题”框架海量扩写，侧重广度；AdaR 则沿“模板→变量扰动”框架深挖同一逻辑的多样性，二者互补。\n\n- **定理级问答 TheoremQA**（Chen et al., 2023）\n提供跨域评测场景，AdaR 在其上取得显著增益，验证方法对“需要定理抽象”的 OOD 题目同样有效。\n\n综上，AdaR 在诊断层面继承并扩展了 GSM-SYM 的扰动思想；在数据层面把 MetaMATH/MathGenie 的“模板多样性”推进到“变量可控扰动+代码验证”的细粒度；在训练层面借助 Chu et al. 2025 的 RL 泛化结论，通过合成对比样本显式解决 RLVR 的奖励黑客问题，形成“诊断-合成-训练”闭环。\n\nQ3: 论文如何解决这个问题？\n\n论文把“虚假推理”问题形式化为：模型在固定解题逻辑  L  的前提下，仅因变量取值  x  变化就给出不一致答案，说明其依赖表面特征而非逻辑。为迫使模型必须学会“同一逻辑⇒同一算法”这一不变式，AdaR 采用“可控扰动+可验证答案+RLVR 对比惩罚”的三段式流程：\n\n1. 逻辑-变量解耦\n用开源 LLM 把原始题目转换成\n\n- 模板  T （仅保留语义框架，数值用占位符）\n- 可执行代码  L_code （输入变量集  x  即可输出答案）\n这一步保证后续任意扰动  x  时，只要  L_code  不变，标准答案  y  就能自动产生，无需人工标注。\n2. 可控扰动与 Sanity Check\n对每道题采样  x_i = x_0 · (1+Delta_i),; Delta_isimU(-α%,α%) ，并强制类型、符号不变；随后\n\n- 用  T  与  x_i  实例化出新题目  q_i \n- 执行  L_code(x_i)  得到金标答案  y_i \n再经三项过滤：\n- VA：模板与代码变量名一致\n- EC：代码可运行且回代原始  x_0  能重现原答案\n- EVS：把代码作为提示喂给 LLM，确认存在合理解\n通不过则重试，最多  τ  次，否则丢弃。最终 9 k 种子→9 k 训练对，保证“逻辑等价、数值多样、题意有效”。\n3. RLVR 对比惩罚\n把原始题  q_0  及其所有扰动题  q_i  放进同一 batch，让模型 rollout 得到响应集合  r_j 。奖励仅按最终答案是否等于  y_i  给 0/1。关键机制：\n\n- 若模型用虚假捷径，在  q_0  上可能蒙对，但在数值不同的  q_i  上极易答错，于是获得低期望奖励\n- 若模型真正学会  L_code  的算法，则在所有  q_i  上都能稳定答对，获得高期望奖励\n通过策略梯度，低奖励路径被抑制，高奖励路径被强化，从而把参数推向“依赖逻辑而非表面数值”的区域。训练目标即最大化\n\nJ(θ)=E_(qsimD),rsimπ_θ(·|q)[v(q,r)]\n\n其中  v  为上述 0/1 答案匹配奖励。\n\n简言之，AdaR 用“代码即逻辑”保证金标答案无成本、用“数值扰动”制造天然负例、用“RLVR 批量对比”把虚假推理路径的期望奖励压低，迫使模型只能学习随  x  变化而稳定输出的自适应逻辑。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“AdaR 能否用极少合成数据同时提升数学推理的鲁棒性与泛化性”这一核心问题，设计了**三大组实验**与**五组深度分析**，覆盖 7 个基准、3 类基础模型、共 20 余项评测。具体实验矩阵如下（均以 pass@1 报告，AIME 用 avg@32）：\n\n1 主实验：全基准对比\n\n**目的**：验证 AdaR 在 9 k 数据量级下是否显著优于现有合成方法与标准 RLVR。\n**设置**：\n\n- 基础模型：Qwen2.5-Math-7B、DeepSeekMath-7B、LLaMA3-8B\n- 训练数据：9 k ORCA-AdaR-train（AdaR 合成） vs. 9 k 原始 ORCA-MATH（Initial-SFT） vs. 9 k MetaMATH vs. 9 k MathGenie vs. 9 k Standard-RLVR\n- 评测集合：\n– 域内鲁棒：ORCA-AdaR-test、GSM-SYM（main / p1 / p2）\n– 域外泛化：MATH、CollegeMath、TheoremQA、AIME 2025\n\n**关键结果**（平均提升）：\n\n| 模型 | Initial-SFT | AdaR | Δ |\n| --- | --- | --- | --- |\n| Qwen2.5-Math-7B | 48.92 | 66.61 | +17.69 |\n| DeepSeekMath-7B | 37.73 | 48.26 | +10.53 |\n| LLaMA3-8B | 32.54 | 37.80 | +5.26 |\n| 平均超越 MetaMATH | +8.50 |\n| 平均超越 MathGenie | +11.44 |\n\n2 消融实验：三要素必要性\n\n**目的**：验证 RLVR、Sanity-Check（VA/EC/EVS）、Paraphrase 是否缺一不可。\n**设置**：以 Qwen2.5-Math-7B 为骨干，逐一把组件换成 RFT 或去掉检查项。\n\n| 配置 | MATH | College | Theorem | AIME | 备注 |\n| --- | --- | --- | --- | --- | --- |\n| Initial-SFT | 42.84 | 28.55 | 14.88 | 4.27 | 基线 |\n| AdaR-RFT | 45.20 | 30.41 | 16.13 | 3.54 | 换 RFT，提升有限 |\n| RLVR w/o VA | 71.58 | 44.14 | 32.50 | 7.92 | 变量对齐缺失，掉点 3.4 |\n| RLVR w/o EC | – | – | – | – | 无法运行，EVS 仅余 0.2 % 数据 |\n| RLVR w/o EVS | 75.28 | 48.22 | 34.88 | 6.88 | 噪声增加，掉点 1.45 |\n| RLVR w/o Paraphrase | 75.90 | 48.62 | 37.13 | 14.27 | 模板多样性下降 |\n| 完整 AdaR | 75.90 | 48.62 | 37.13 | 14.27 | 最佳 |\n\n3 能力解耦：推理 vs. 计算\n\n**目的**：区分 AdaR 究竟提升的是“列式推理”还是“数值计算”。\n**评测协议**：\n\n- 推理能力：给定题目，让模型生成 Python 代码，能跑通且输出正确即得分\n- 计算能力：把金标代码喂给模型，让其直接执行并返回结果，考察算术准确性\n\n| 方法 | ORCA-AdaR-test | 推理准确率 | 计算准确率 |\n| --- | --- | --- | --- |\n| Initial-SFT | 77.12 | 75.24 % | 54.92 % |\n| Standard-RLVR | 81.80 | 69.52 % | 68.52 % |\n| AdaR-RFT | 82.32 | 72.36 % | 75.36 % |\n| AdaR | 86.08 | 80.96 % | 90.76 % |\n\n4 深度分析实验\n\n4.1 自适应推理证据\n\n- **代数思维**：抽样 20 条 GSM-SYM 回答，含“结构化代码片段”比例由 55 % → 90 %\n- **逻辑顺序影响度（ILO）**：\n\nILO=(1) / (n)∑_(i=1)^n|PPL(y|q,z)-PPL(y|q,R_i(z))|PPL(y|q,z)\n\nAdaR 模型 ILO 119.22 % → 150.49 %，显著高于无法通过全部扰动测试的虚假推理轨迹（114.24 %）\n\n4.2 扰动幅度 α 扫描\n\nα ∈ {50,100,500,1000}，性能先升后降，500 为拐点；过大导致 EVS 失效，噪声反超收益。\n\n4.3 三维缩放定律\n\n- 固定总样本 9 k，仅改变模板采样频率 vs. 变量采样频率 vs. 种子逻辑数\n- 拟合  k·log(scale) ，系数满足  k_T<k_xll k_L ，说明“变量扰动”边际收益最高，模板次之，扩逻辑最贵。\n\n4.4 已见/未见扰动查询影响\n\n- **设置 A**：SFT 阶段已见过原始题，RLVR 用扰动题 → ORCA-AdaR-test 84.72\n- **设置 B**：SFT 完全未见的 2.25 k 种子，每题再扰动 4 次 → 83.42\n- **设置 C**：Standard-RLVR 仅用普通题 → 77.82\n证明即使 RLVR 阶段才首次接触扰动题，对比信号仍足以激发自适应推理。\n\n4.5 Instruct 模型适用性\n\n以 Qwen2.5-Math-7B-Instruct 为起点，再用 AdaR 9 k 数据 RLVR：\n\n| 方法 | 平均 8 项得分 |\n| --- | --- |\n| Instruct 原模型 | 73.99 |\n| AdaR（base→RLVR） | 73.16 |\n| AdaR-Instruct | 75.91 |\n| 用 3000× 更少数据即可在已极强起点上再提升 1.9 分，证明 AdaR 对指令模型依旧有效。 |\n\n5 效率与成本\n\n- Sanity-Check 三项耗时：VA 2.9 s/1k、EC 2.8 s/1k、EVS 612 s/1k，但 EC 过滤后 EVS 仅需在 30 % 样本上运行，整体合成 9 k 数据在 32 核 CPU 上 < 2 小时完成。\n- RLVR 训练在 4×A100 上约 3 小时收敛，远低于继续预训练或大规模 SFT 成本。\n\n综上，论文通过“主结果-消融-能力解耦-机制验证-缩放/扰动/指令模型扩展”这一完整实验链条，系统回答了：\n\n1. 小数据能否带来大幅提升？→ 能，+17.69 分\n2. 提升来自推理还是计算？→ 两者兼得，推理 +6 %，计算 +36 %\n3. 哪一环节最关键？→ RLVR + Sanity-Check + 变量扰动缺一不可\n4. 对更强基底/指令模型是否仍有效？→ 是，继续提升 1.9 分\n5. 数据维度如何权衡？→ 优先扩变量扰动，再扩模板，最后扩逻辑\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可视为 AdaR 的“直接外延”或“深层机制”探索，均未被原文系统讨论，且具备可行性与学术价值：\n\n1 逻辑维度自动化扩展\n\n- **定理级抽象**：当前模板仅覆盖算术/代数，能否把  L_(code)  升级成“含量词、集合、极限、积分”的符号逻辑，自动从 MATH、TheoremQA 抽取定理模板？\n- **逻辑正确性形式验证**：用 SMT/Coq 对生成的  L_(code)  做形式化证明，彻底消除 EVS 依赖 LLM 带来的不确定性。\n\n2 扰动空间再定义\n\n- **结构扰动**：在保持“解题不变式”前提下，对题干做句法树级别的结构重排（被动句⇄主动句、条件从句移位），测试模型是否仍沿同一逻辑路径求解。\n- **多模态数值**：引入分数、复数、单位（km/h、°C）、误差范围，考察模型能否在“数值类型+量纲”双重变化下维持自适应。\n- **约束耦合扰动**：当前各变量独立扰动，若引入耦合约束（如  a+b=1 ），需设计可逆采样或 MCMC 保证约束满足，探索“耦合场景”下的鲁棒性。\n\n3 奖励信号精细化\n\n- **过程奖励模型（PRM）蒸馏**：仅用 0/1 结果奖励最稀疏，能否用 AdaR 合成的大量中间步骤（代码行级）自动蒸馏一个无人工标注的 PRM，实现稠密奖励？\n- **对比奖励**：把“同一逻辑不同数值”的轨迹两两拼接，用 Bradley-Terry 模型直接学习“哪条轨迹更可能正确”，绕过结果匹配。\n\n4 跨领域迁移\n\n- **物理/化学 word problem**：将  L_(code)  换成含物理定律（ F=ma 、理想气体方程）的代码模板，验证 AdaR 是否同样能抑制“单位混淆”或“量纲虚假关联”。\n- **几何+视觉**：用 Asymptote/TikZ 代码描述几何构造，扰动边长/角度，考察模型在“图文双通道”下是否仍保持几何逻辑不变。\n\n5 模型规模与算法耦合\n\n- **测试时扩展**：在推理阶段对同一模板多次采样不同数值实例，用 majority voting 或 consistency reranking 进一步提升准确率，量化“测试时自适应”收益。\n- **小模型→大模型蒸馏**：用 AdaR 训练后的 7 B 模型生成大量正确轨迹，蒸馏到 1 B 甚至 0.5 B 模型，观察“自适应推理”能否跨越规模边界，实现“小模型大逻辑”。\n\n6 可解释性与认知对齐\n\n- **因果干预**：用 DoWhy 或 causal mediation 框架，对 CoT 中的关键变量符号做干预，测量模型输出变化，量化“逻辑节点”的因果强度。\n- **人类-模型对比**：招募人类被试在相同扰动题上给出 CoT，用 ILO 指标对比人类与模型的逻辑顺序敏感度，验证 AdaR 是否更接近人类代数思维。\n\n7 数据效率极限\n\n- **Few-shot AdaR**：仅给 100/500 条种子，用主动学习选择“最具冲突性”的扰动题（模型当前最不确定的数值区间），迭代式扩增，探索 1 k 以内数据能否达到 90 % GSM8K。\n- **课程扰动**：先小幅度扰动（α=10 %），待模型稳定后再逐步放大，考察课程式数值难度是否比固定 α 更节省样本。\n\n8 安全与对齐风险\n\n- **奖励黑客再探**：故意在代码中植入“看似正确、实则错误”的逻辑（如把 / 写成 \\*），观察 AdaR 是否会因结果奖励而放大该漏洞，评估“代码即逻辑”假设的失效边界。\n- **对抗数值**：用 PGD 或遗传算法搜索“微小数值扰动即可使模型失败”的对抗题，检验自适应推理的局部鲁棒性极限。\n\n9 系统级优化\n\n- **在线合成**：把模板-代码-扰动流程嵌入训练循环，每 rollout 一轮即生成全新数值，实现“训练即合成”，避免静态数据集过拟合。\n- **异构算子融合**：将代码执行卸载到 GPU TensorCore 或 JIT 编译，减少 CPU-GPU 往返，使 AdaR 的 sanity-check 开销降至毫秒级，支持十万级实时扰动。\n\n10 开放问题\n\n- **模板自动发现**：能否从无标注网络数学文本中直接挖掘“隐模板”与对应逻辑，实现完全自监督的 AdaR？\n- **逻辑复杂度可扩展性**：当解题逻辑从  O(1)  行代码增至  O(n)  层递归或动态规划，Sanity-Check 的 EVS 通过率与 RLVR 收敛性如何变化？是否存在“逻辑复杂度壁垒”？\n\n这些方向既可直接在 AdaR 代码框架上迭代（1-5），也可引入跨学科工具（6-8），或挑战理论极限（9-10），为后续研究提供持续动力。\n\nQ6: 总结一下论文的主要内容\n\n- **问题**：大模型数学推理常依赖表面特征给出正确答案，出现“虚假推理”，导致数值扰动或跨域场景下鲁棒性与泛化性骤降。\n- **观点**：理想推理应抽象出“模板+变量”并建模不变解题逻辑，实现代数式自适应。\n- **方法（AdaR）**\n1. 数据合成：用 LLM 将原题拆成模板 T 与可执行代码 L，随机扰动变量值 x 并经 sanity-check 生成逻辑等价的新题-答案对，无需人工标注。\n2. 训练策略：用 RLVR 在同一 batch 内对比原始题与扰动题的 0/1 结果奖励；虚假路径易在扰动题上出错而被惩罚，迫使模型学习随 x 变化仍稳定的自适应逻辑。\n- **实验**：9 k 合成数据在 7 个内外域基准上平均提升 8.5 分，最高 +17.7；消融表明 RLVR、 sanity-check、变量扰动缺一不可；分析证实模型代数思维与逻辑顺序敏感度显著增强，且对 Instruct 模型依旧有效。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Making%20Mathematical%20Reasoning%20Adaptive，链接是https://arxiv.org/pdf/2510.04617，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.04617。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-10-1760485638283","title":"Building a Foundational Guardrail for General Agentic Systems via Synthetic Data","description":"While LLM agents can plan multi-step tasks, intervening at the planning stage-before any action is executed-is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap, model gap, and evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release Pre-Exec Bench, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains of the proposed guardrail over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.","url":"https://huggingface.co/papers/2510.09781","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.283Z","metadata":{"authors":["Yue Huang","Hang Hua","Yujun Zhou","Pengcheng Jing","Manish Nagireddy","Inkit Padhi","Greta Dolcetti","Zhangchen Xu","Subhajit Chaudhury","Ambrish Rawat","Liubov Nedoshivina","Pin-Yu Chen","Prasanna Sattigeri","Xiangliang Zhang"],"date":"","rank":11,"llmAnalysis":"","categories":["cs.LG","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.09781.pdf","arxivUrl":"https://arxiv.org/abs/2510.09781","arxivId":"2510.09781","coolPaperUrl":"https://papers.cool/arxiv/2510.09781","published":"2025-10-10T18:42:32Z","updated":"2025-10-10T18:42:32.000Z","zh_summary":"虽然大语言模型（LLM）代理可以规划多步骤任务，但在执行任何操作之前干预规划阶段通常是防止伤害的最安全方式，因为某些风险一旦实施可能导致严重后果。然而，现有的防护措施大多是在执行后进行的，这很难规模化，并且在计划层面上几乎没有可控的监督空间。为了解决这一挑战，我们指出当前研究中的三个关键缺口：数据缺口、模型缺口和评估缺口。为了弥补数据缺口，我们引入了AuraGen，这是一款可控引擎，可以(i) 合成良性轨迹，(ii) 注入带类别标签的风险并校准难度，(iii) 通过自动奖励模型过滤输出，从而生成大规模且可靠的预执行安全语料库。为了弥补守护模型的缺口，我们提出了基础防护工具Safiron，结合跨规划器适配器和精简守护模型。该适配器统一不同的输入格式，而Safiron能够标记风险案例、分配风险类型并生成理由；Safiron通过两阶段训练，结合广泛探索的数据策略，实现了在各种环境中的稳健迁移。为了弥补评估缺口，我们发布了Pre-Exec Bench，这是一个覆盖多种工具和分支轨迹的真实基准，能够在人工验证场景中衡量检测、细化分类、解释和跨规划器泛化能力。大量实验表明，在Pre-Exec Bench上，所提出的防护工具相较于强基线稳定取得提升，并且消融研究进一步提炼了可操作实践，为更安全的智能体系统提供了实用模板。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决通用智能体（LLM-based agentic）系统在规划阶段缺乏可扩展、可泛化安全护栏的核心难题。具体而言，现有研究在数据、模型与评测三个维度存在显著缺口：\n\n1. **数据缺口**：真实有害轨迹稀缺且标注成本极高，导致训练 guardian 模型所需的规模化、多样化、可控风险语料严重不足。\n2. **模型缺口**：现有护栏多为事后（post-execution）检测，或仅针对局部内容/单轮对话，无法在规划阶段对整条行动轨迹进行事前（pre-execution）拦截；同时缺乏对多工具、多步推理、跨 planner 格式的统一支持。\n3. **评测缺口**：主流基准聚焦执行期风险或单轮对话安全，缺少专门针对“规划层面”风险、覆盖多工具调用与分支轨迹、并经人工验证的评测体系。\n\n为此，作者提出一套面向规划阶段的 foundational guardrail 框架，通过**合成数据引擎 AuraGen**、**统一适配器+轻量级 guardian 模型 Safiron** 以及**预执行评测基准 Pre-Exec Bench**，闭环填补上述三缺口，实现对智能体在行动执行前的风险检测、细粒度分类与可解释拦截。\n\nQ2: 有哪些相关研究？\n\n与本文直接相关的研究可划分为三大类：智能体安全评测、护栏/守护模型、以及合成数据生成。以下按类别列出代表性工作，并指出其与本文的差异。\n\n智能体安全评测基准\n\n| 基准 | 核心关注点 | 与本文差异 |\n| --- | --- | --- |\n| Agent-SafetyBench (Zhang et al., 2025c) | 执行期攻击与防御 | 侧重执行阶段，缺少规划级风险与分支轨迹 |\n| R-Judge (Yuan et al., 2024) | 对话级风险感知 | 多为对话片段，缺乏逐步工具调用与长程规划 |\n| SafeAgentBench (Yin et al., 2025) | 具身智能体安全规划 | 聚焦具身任务，工具集与通用软件智能体不同 |\n| RealSafe (Ma, 2025) | 真实场景风险量化 | 以单步执行为主，未提供规划级事前检测 |\n| OPENAGENTSAFETY (Vijayvargiya et al., 2025) | 真实智能体系统评测 | 工具集有限，未对规划轨迹做细粒度风险分类 |\n\n护栏 / 守护模型\n\n| 工作 | 核心机制 | 与本文差异 |\n| --- | --- | --- |\n| Llama-Guard (Inan et al., 2023) | 对话输入-输出安全分类 | 针对单轮对话，不支持多步工具轨迹 |\n| Granite Guardian (Padhi et al., 2025b) | 多风险（偏见、幻觉、越狱等）检测 | 面向通用对话/RAG，未对规划轨迹做风险注入与分类 |\n| LlamaFireWall (Chennabasappa et al., 2025 |"}},{"id":"hf-11-1760485638283","title":"InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models","description":"General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.","url":"https://huggingface.co/papers/2510.11341","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.283Z","metadata":{"authors":["Haomin Wang","Jinhui Yin","Qi Wei","Wenguang Zeng","Lixin Gu","Shenglong Ye","Zhangwei Gao","Yaohui Wang","Yanting Zhang","Yuanqi Li","Yanwen Guo","Wenhai Wang","Kai Chen","Yu Qiao","Hongjie Zhang"],"date":"","rank":12,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.11341.pdf","arxivUrl":"https://arxiv.org/abs/2510.11341","arxivId":"2510.11341","coolPaperUrl":"https://papers.cool/arxiv/2510.11341","published":"2025-10-13T12:38:04Z","updated":"2025-10-13T12:38:04.000Z","zh_summary":"由于数据集零散、方法在不同任务之间的迁移性有限以及处理结构复杂性的困难，一般的 SVG 建模仍然具有挑战性。为此，我们利用多模态大语言模型（MLLMs）强大的迁移和泛化能力，实现 SVG 理解、编辑和生成的统一建模。我们提出了 InternSVG 系列，这是一个集成的数据-基准-模型套件。其核心是 SAgoge，这是迄今为止最大且最全面的 SVG 任务多模态数据集，涵盖静态图形和动态动画。它包含图标、长序列插图、科学图表和动态动画，支持各种难度级别的任务，并提供比以往数据集更深的层级关系和更丰富的属性。基于这一资源，我们引入 SArena，这是一个配套基准，提供全面的任务定义和标准化评估，与 SAgoge 覆盖的领域和难度范围保持一致。在此基础上，我们提出了 InternSVG，这是一个针对 SVG 理解、编辑和生成的统一 MLLM，具有 SVG 专用特殊标记、基于子词的嵌入初始化以及从短静态 SVG 到长序列插图和复杂动画的两阶段训练策略。这种统一的建模方式能够引入正迁移并提升整体性能。在 SArena 和先前基准上的实验表明，InternSVG 实现了显著的性能提升，并始终优于领先的开源和专有模型。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**统一建模可缩放矢量图形（SVG）任务**所面临的三大核心难题：\n\n1. 数据割裂\n现有数据集仅针对单点任务（理解、编辑或生成），缺乏跨任务共享的大规模监督信号，导致迁移受限。\n\n2. 规模与多样性不足\n最大统一数据集仅 0.5 M 样本，且以简单图标为主，缺少长序列插图、科学图表与动画等复杂形态，难以支撑通用模型训练。\n\n3. 方法碎片化与泛化弱\n优化式、扩散式或纯 LLM 方案各自孤立，难以同时处理语义理解、精细编辑与高质量生成，更无法泛化到长序列、多领域、多模态场景。\n\n为此，作者提出 InternSVG 家族：构建 16 M 样本的多领域、多任务、多难度统一数据集 SAgoge，配套标准化评测基准 SArena，并设计支持理解-编辑-生成一体化的多模态大模型 InternSVG，验证统一建模带来的正向迁移与整体性能跃升。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为 **数据集/基准** 与 **建模方法** 两条主线，按时间递进梳理如下：\n\n1\\. 数据集与基准\n\n| 名称 | 年份 | 任务覆盖 | 规模 | 关键局限 |\n| --- | --- | --- | --- | --- |\n| DeepSVG | 2020 | 生成（图标） | 100 k | 无理解/编辑，无动画 |\n| SVG-Stack | 2025 | 文→图/图→图 | 2.2 M | 仅生成，无理解/编辑 |\n| ColorSVG-100K | 2025 | 文→图 | 100 k | 无理解/编辑，领域单一 |\n| MMSVG | 2025 | 文→图/图→图 | 2.0 M | 无理解/编辑 |\n| SVGX | 2025 | 文→图 | 1.0 M | 无理解/编辑 |\n| UniSVG | 2025 | 理解+生成 | 525 k | 无编辑，无动画 |\n| SGP-Bench | 2024 | 理解 | 4.3 k | 无编辑/生成 |\n| SVGEditBench | 2024 | 编辑 | 1.3 k | 无理解/生成 |\n| VGBench | 2024 | 理解+生成 | 10.1 k | 无编辑 |\n| SVGenius | 2025 | 理解+编辑+生成 | 2.3 k | 规模小，仅评测 |\n| SAgoge/SArena | 2025 | 理解+编辑+生成+动画 | 16 M / 31 k | 本文工作 |\n\n2\\. 建模方法\n\n2.1 序列/变分生成\n\n- **Sketch-RNN** (Ha & Eck, 2017)\n早期 RNN 手绘草图生成，无语义控制。\n- **DeepSVG** (Carlier et al., 2020)\n分层 VAE + Transformer，仅限静态图标，无文本/图像条件。\n\n2.2 可微光栅化优化\n\n- **DiffVG** (Li et al., 2020)\n可微栅化实现路径拟合，计算昂贵、无语义推理。\n- **LIVE** (Ma et al., 2022)\n层-wise 图像矢量化，细节冗余、编辑困难。\n\n2.3 扩散模型\n\n- **VectorFusion** (Jain et al., 2023)\n文本→像素扩散+矢量化，视觉保真高但可编辑性差。\n- **SVGDreamer** (Xing et al., 2024)\n引入语义向量场，仍局限于生成任务。\n\n2.4 大模型代码生成\n\n- **StarVector** (Rodriguez et al., 2025)\n图像→SVG 代码，无理解/编辑，长序列易崩溃。\n- **LLM4SVG** (Xing et al., 2025)\nLLM 文→图，未统一理解与编辑。\n- **OmniSVG** (Yang et al., 2025b)\n多模态文/图→图，仍仅生成，无编辑基准。\n\n3\\. 小结\n\n现有研究要么**任务单一**，要么**规模有限**，要么**方法孤立**。InternSVG 首次将“大规模多域数据 + 统一多任务基准 + 多模态大模型”三者闭环，填补 SVG 统一建模空白。\n\nQ3: 论文如何解决这个问题？\n\n论文从 **数据、基准、模型** 三条线协同发力，形成闭环方案，具体措施如下：\n\n1\\. 数据层：构建 SAgoge（16 M 样本）\n\n| 维度 | 做法 | 效果 |\n| --- | --- | --- |\n| 任务统一 | 将理解、编辑、生成三大任务共 16 个子任务纳入同一标注格式 | 消除监督信号碎片化 |\n| 领域多样 | 图标、插图、化学结构、动画四域同步收集；长序列插图与动画通过 FLUX+LoRA→VTracer 与 Claude-4 SMIL 代码合成 补充 | 覆盖简单图标到复杂动画的全谱复杂度 |\n| 规模提升 | 互联网爬取 + 合成 pipeline，总量 16 M，最大单域 11 M | 比此前最大统一数据集扩大 30× |\n| 质量可控 | 128×128 归一化 + 路径简化 + MLLM 自动标注 + 人工抽检 | 平均 token 长度下降 35 %，渲染成功率 > 99 % |\n\n2\\. 基准层：设计 SArena（31 k 评测）\n\n| 功能 | 做法 | 效果 |\n| --- | --- | --- |\n| 对齐数据 | 任务、领域、难度分布与 SAgoge 完全一致 | 训练-评测零偏差 |\n| 指标统一 | 理解：4 选 1 QA 准确率；编辑/生成：DINO、SSIM、LPIPS、PSNR、FID、CLIP-Score 等 | 可横向对比任意模型 |\n| 鲁棒惩罚 | 无法渲染的 SVG 直接判黑图，计入指标 | 杜绝“生成无效代码刷分” |\n\n3\\. 模型层：提出 InternSVG\n\n3.1 架构\n\n- **ViT–MLP–LLM 范式**：InternViT-300M → MLP projector → Qwen2.5-7B\n- **SVG 专用 tokenization**\n– 55 组标签 token（`<svg>`、`<path>`、`<animate>` 等）\n– 42 组属性 token（`fill=\"`、`d=\"`、`dur=\"` 等）\n– 257 个数值 token（-128–128 整数 + 0.00–0.99 小数）\n整体缩短序列长度 **≈ 40 %**，降低长序列幻觉。\n\n3.2 子词嵌入初始化\n\n新 token 先拆成已有子词，再取平均嵌入：\n\ne_(t_new) = (1) / (n)∑_(i=1)^n e_(s_i)\n\n相比随机初始化，**初期损失下降 28 %**，收敛快 1.8×。\n\n3.3 两阶段课程训练\n\n| 阶段 | 数据 | 目标 |\n| --- | --- | --- |\n| 阶段 1 | Icon + Chemistry（短序列） | 建立基础语法与对齐 |\n| 阶段 2 | 全量数据（插图+动画） | 攻克长序列与复杂语义 |\n| 通过重采样平衡各域比例，FID-C 从 25.67 降至 5.14，长动画 FVD 绝对降低 29.6。 |\n\n3.4 统一多任务训练\n\n同一序列到序列格式承载所有任务：\n\n- 理解：SVG code → 文本描述/答案\n- 编辑：SVG code + 指令 → 修改后 code\n- 生成：文本/图像 + 指令 → SVG code\n\n实验表明三任务联合训练相比单任务：\n\n- 理解准确率 **+12.5 %**\n- 编辑 PSNR **+12.5 dB**\n- 生成 FID **\\-3.2**\n\n4\\. 结果验证\n\n在 SArena 四大子集上，8 B 模型全面领先：\n\n| 任务 | 指标 | 提升（vs 最强基线） |\n| --- | --- | --- |\n| 理解 | Overall Acc | +8.0 % |\n| 编辑 | PSNR | +19.7 dB |\n| 文→图 | FID | -6.2 |\n| 图→图 | SSIM | +0.046 |\n| 化学结构 | CLIP-I2I | +15.6 % |\n| 动画 | FVD | -17.0 |\n\n5\\. 总结\n\n通过 **“大规模统一数据 + 标准化评测 + 多模态课程训练”** 三位一体，论文首次实现 SVG 理解-编辑-生成的统一建模，显著超越传统优化、扩散及现有 LLM 方案，为矢量图形智能提供了可扩展的新范式。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **数据有效性、模型设计、训练策略、任务统一性、领域泛化** 五个维度展开系统实验，共 5 大类 20 余项测试，结果均以 SArena 为主、辅以既有基准 SGP-Bench。核心实验一览如下（无表格，纯要点）：\n\n1\\. 主实验：SArena 全量评测\n\n**目的**：验证 InternSVG 在统一设定下是否全面领先。\n**内容**：\n\n- 4 个子基准（Icon / Illustration / Chemistry / Animation）\n- 3 类任务（理解、编辑、生成）共 16 个子任务\n- 对比 30+ 基线：\n– 开源 MLLM：Qwen2.5-VL、InternVL3、Llama-4、GLM-4.5V 等\n– 闭源 MLLM：GPT-4o、Gemini-2.5-Flash、Claude-Sonnet-4\n– 传统 SVG 方法：DiffVG、LIVE、VTracer、VectorFusion、SVGDreamer\n– LLM-based SVG 专用模型：StarVector、LLM4SVG、OmniSVG\n\n**关键结论**：\n\n- 8 B 模型在 31 k 测试样例上取得 **SOTA**，平均领先次优者 8–34 % 不等。\n- 生成代码长度仅为 LIVE 的 1/14，渲染成功率 > 99 %。\n\n2\\. 消融实验：统一建模收益\n\n**设计**：在 Icon 域采样 100 k 样本，构造三任务均衡子集，对比\n\n- 仅生成（G）\n- 生成+理解（G+U）\n- 生成+编辑（G+E）\n- 三任务联合（G+U+E）\n\n**指标**：各任务独立测试集上的专用指标（FID、PSNR、Acc 等）。\n**结论**：联合训练带来 **一致提升**，三任务版本平均 FID 再降 3.2，PSNR 再升 5 dB，理解 Acc 升 12.5 %，证实 **正向迁移**。\n\n3\\. 训练策略对比：单阶段 vs 两阶段\n\n**设置**：\n\n- 单阶段：一次性混合全量数据训练。\n- 两阶段：先 Icon+Chemistry（短序列），后全量（含长插图/动画）。\n\n**结果**（表 6 & 表 16）：\n\n- Illustration Text-to-SVG：FID-C 从 25.67 → 5.14（–80 %）。\n- Animation Video-to-SANI：FVD 降 17 点，SSIM 升 0.019。\n- 长序列生成崩溃率由 8 % → <1 %。\n\n**结论**：课程式两阶段显著 **缓解数据不平衡与长序列幻觉**。\n\n4\\. 子词嵌入初始化消融\n\n**对比**：\n\n- 随机初始化\n- 子词平均初始化（公式 1）\n\n**监控**：阶段 1 前 2 k step 的损失与收敛速度。\n**结果**：\n\n- 初始损失降低 28 %，收敛步数减少 1.8×。\n- 最终各任务指标平均提升 1–2 %，验证 **语义先验加速适配**。\n\n5\\. 跨域泛化实验\n\n5.1 化学结构生成\n\n- 以 IUPAC 名称为提示，测试文→图。\n- InternSVG FID 9.97，次优闭源 Claude-Sonnet-4 为 21.25，领先 > 11 点。\n\n5.2 动画生成\n\n- Text-to-SANI & Video-to-SANI 双任务。\n- FVD 99.47，比最强开源 Llama-4-Maverick 低 42 点；与 Claude-Sonnet-4 差距 < 2 点。\n\n5.3 低层编辑细拆\n\n- 8 项简单编辑（颜色、描边、平移、缩放、旋转、翻转、透明度、裁剪）\n- 2 项高层编辑（语义换色、风格迁移）\n- InternSVG 在 8/10 子任务上获 **满分或最佳**，仅风格转移略低于 Claude-Sonnet-4（SSIM 0.808 vs 0.867），显示 **细粒度操作能力**。\n\n6\\. 外部基准验证：SGP-Bench\n\n**目的**：检验 SAgoge 数据对 **符号程序理解能力** 的增益。\n**结果**：InternSVG 8 B 达 72.3 % Overall Acc，**超越最强开源 GLM-4.5V 6.2 %**，并领先 Claude-Sonnet-4 1.3 %，证实数据与模型的 **通用语义迁移性**。\n\n7\\. 可视化与失败案例分析\n\n- 图 5–26 给出 100+ 样例，覆盖图标、插图、化学、动画四域。\n- 与 VectorFusion、SVGDreamer、StarVector 等对比：\n– 传统方法常引入冗余路径、语义漂移；\n– 通用 MLLM 出现结构不完整、颜色错位；\n– InternSVG 在 **几何保真、语义对齐、代码简洁** 上均表现最优。\n- 失败案例主要集中在 **复杂风格迁移** 与 **多对象空间关系** 两个场景，为后续研究指明方向。\n\n8\\. 计算效率测试\n\n- 平均生成 1 k token 耗时 180 ms（A800），比扩散方法快 5–10×。\n- 序列长度压缩 40 %，显存占用降低 25 %，支持 **单机 8×A800 推理 128 长序** 无溢出。\n\n综上，实验从 **规模、质量、效率、鲁棒、泛化** 多维度系统验证：\n\n1. SAgoge 数据有效提升各类 SVG 任务上限；\n2. SArena 提供可复现的横向对比平台；\n3. InternSVG 统一架构在全部维度取得 **SOTA 或接近 SOTA** 的表现。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可直接在 InternSVG 框架上延伸，亦适合社区后续攻关：\n\n1\\. 更高阶的矢量语义推理\n\n- **跨对象逻辑推理**：给定多张 SVG 图标，自动推断“哪两个对象可组成新功能”“缺失步骤应为何形状”等因果/组合问题。\n- **时空混合推理**：在动画 SVG 中回答“若某帧物体 A 消失，B 的运动轨迹将如何改变”，需要同时解析几何与 SMIL 时序语义。\n\n2\\. 复杂风格与可迁移编辑\n\n- **少样本风格迁移**：目前风格迁移仍落后 Claude-Sonnet-4；可引入视觉-风格解耦编码器，实现 1–3 张参考图即可把任意图标转为目标风格，且保持可编辑矢量结构。\n- **非刚性形变编辑**：现有编辑局限于刚性变换（旋转、缩放）。支持“把沙发从直角形变成 L 形”这类语义形变，需要学习基于控制点的路径插值与语义约束。\n\n3\\. 多模态条件生成\n\n- **声音→SVG 动画**：将音乐节奏或语音频谱映射到动画参数（颜色、形变频率），实现“可视化伴奏”。\n- **3D→SVG 线条画**：把 NeRF 或网格渲染的 3D 关键帧自动转成可分层矢量线条，保持遮挡关系与透视一致。\n\n4\\. 自监督与数据飞轮\n\n- **无配对矢量化**：利用可微栅化构建“光栅↔矢量”双向往返损失，对任意图片集自监督训练，摆脱对 VTracer 等工具依赖。\n- **质量自评+主动学习**：模型对自身生成的 SVG 进行“可渲染性、简洁性、语义一致性”打分，主动挑选低置信样本人工修正，持续扩大 SAgoge。\n\n5\\. 压缩与端侧部署\n\n- **矢量 token 压缩**：借鉴图像 VQ-VAE，把路径段离散成紧凑 codebook，进一步减少 30–50 % 序列长度，方便端侧实时生成。\n- **量化-蒸馏**：将 8 B 模型蒸馏到 1 B 以内，保持编辑/生成精度下降 < 3 %，适配浏览器 WebGPU 推理。\n\n6\\. 编辑安全性与版权\n\n- **可验证编辑**：给定“移除品牌 Logo”指令，确保只改指定像素区域，引入差分渲染层，输出可验证的“编辑证明”。\n- **版权过滤**：在训练阶段嵌入指纹检测头，自动拒绝与版权图库高度相似的生成结果，降低法律风险。\n\n7\\. 开放世界评估\n\n- **用户真实指令分布**：现有编辑模板相对固定，可爬取 Figma、Dribbble 等平台的用户真实修改记录，构建 **SArena-Live** 动态基准。\n- **对抗评测**：引入红队模型自动生成“最难指令”（超长、多对象、隐式语义），持续测试 InternSVG 失败边界。\n\n8\\. 与其他模态大模型协同\n\n- **SVG⇄LaTeX 双向转换**：同一模型既能将数学公式 SVG 化，也能把 SVG 图表转回 LaTeX/TikZ 代码，服务学术出版。\n- **SVG⇄Mermaid/PlantUML**：自动生成可编辑的流程图、时序图代码，实现“手绘草图→可迭代源码”的闭环。\n\n9\\. 交互式生成\n\n- **人-模型协同画布**：用户实时拖动控制点或修改文本提示，模型以 <100 ms 延迟流式更新 SVG，支持“对话式精修”。\n- **多轮编辑一致性**：保证第 k 轮编辑结果与第 1…k-1 轮指令不冲突，需要引入历史感知缓存与一致性损失。\n\n10\\. 认知与可解释性\n\n- **路径级解释**：对“把猫变狗”的编辑，可视化出模型具体修改了哪些路径段，并给出语义解释（如“耳朵路径曲率增大”）。\n- **注意力可视化**：在 ViT 与 LLM 层间插入跨模态注意力图，帮助设计师理解模型为何选择某条贝塞尔曲线或某帧关键时序。\n\n这些方向既涵盖**算法创新**（推理、形变、自监督），也兼顾**系统与产品**（端侧部署、交互、版权），可推动 SVG 大模型从“可用”走向“好用、可信、普适”。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **InternSVG 家族**，首次实现**大规模、多领域、多任务**的 SVG 统一建模，核心贡献可概括为 **“一套数据、一个基准、一个模型”**：\n\n1. **SAgoge 数据集** – 16 M 样本，覆盖图标、插图、化学结构、动画；理解、编辑、生成 16 子任务统一标注，规模与多样性均超现有最大数据集 30×。\n2. **SArena 基准** – 31 k 高质量评测，四域并行，指标标准化，无法渲染即判黑，保证横向可比。\n3. **InternSVG 模型** – 基于 ViT–MLP–LLM，引入 154 个 SVG 专用 token 与子词初始化，两阶段课程训练；统一序列到序列格式同时完成理解、编辑、生成，取得全面 SOTA，生成代码长度仅为传统方法的 1/14，渲染成功率 >99 %。\n\n实验表明：统一建模带来显著正向迁移，在 SArena 各子任务上平均领先最强基线 8–34 %；外部基准 SGP-Bench 亦领先开源模型 6.2 %。论文为矢量图形的语义推理、精细编辑与高质量生成提供了可扩展的新范式。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是InternSVG%3A%20Towards%20Unified%20SVG%20Tasks%20with%20Multimodal%20Large%20Language%20Models，链接是https://arxiv.org/pdf/2510.11341，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11341。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-12-1760485638284","title":"BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions","description":"Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing. In this paper, we propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These results indicate that BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents.","url":"https://huggingface.co/papers/2510.10666","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.284Z","metadata":{"authors":["Zhengbo Zhang","Zhiheng Lyu","Junhao Gong","Hongzhu Yi","Xinming Wang","Yuxuan Zhou","Jiabing Yang","Ping Nie","Yan Huang","Wenhu Chen"],"date":"","rank":13,"llmAnalysis":"","categories":["cs.CL","cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.10666.pdf","arxivUrl":"https://arxiv.org/abs/2510.10666","arxivId":"2510.10666","coolPaperUrl":"https://papers.cool/arxiv/2510.10666","published":"2025-10-12T15:43:37Z","updated":"2025-10-12T15:43:37.000Z","zh_summary":"高效解决现实世界问题对大型语言模型（LLMs）来说，越来越依赖于其与动态网页环境的交互能力以及自主获取外部信息的能力。虽然像 Search-R1 和 WebDancer 这样的最新研究在解决网页任务方面表现出色，但它们在很大程度上依赖额外工具将互动网页环境转换为静态文本内容。这与人类的浏览行为形成对比，人类浏览涉及多样化的浏览器交互，如滚动、点击和输入。在本文中，我们提出了 BrowserAgent，这是一种更加互动的代理，通过人类启发的浏览器操作来解决复杂任务。BrowserAgent 通过 Playwright 直接在原始网页上执行一组预定义的浏览器操作。我们采用两阶段训练（监督微调（SFT）和拒绝微调（RFT））以提升模型的泛化能力。尽管使用的训练数据远少于 Search-R1，BrowserAgent 在不同的开放式问答任务中依然取得了更具竞争力的结果。此外，我们引入了显式记忆机制，用于跨步骤存储关键结论，进一步增强了模型在长程任务中的推理能力。值得注意的是，BrowserAgent-7B 在 HotpotQA、2Wiki 和 Bamboogle 等多跳问答任务中，相较于 Search-R1 约提升 20%。这些结果表明，BrowserAgent 可以作为更先进的框架，为更加互动和可扩展的网页代理提供支持。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决现有大模型（LLM）在动态 Web 环境中“交互深度不足、训练成本高昂”的核心矛盾，具体表现为：\n\n- **静态摘要依赖**：已有方法（Search-R1、WebDancer 等）通过外部工具将网页实时内容转化为静态文本，丢失了对页面结构、超链接、滚动等细粒度信息的直接利用，限制了信息获取的深度与广度。\n- **训练可扩展性差**：基于 Playwright 的浏览器原生交互吞吐量极低（≈1–2 episode/min），导致大规模训练难以实现。\n- **长程推理遗忘**：多跳问答等长程任务中，历史信息随交互轮次增加而被截断或稀释，造成逻辑链断裂。\n\nBrowserAgent 通过以下手段直接针对上述问题：\n\n1. 原生浏览器动作空间：定义 12 种原子操作（点击、滚动、输入、标签管理等），让模型像人类一样直接操作 DOM，无需额外解析或摘要服务。\n2. Ray-并行化基础设施：在 32 核单机上部署 64 个并发 Playwright 实例，将数据收集速度提升一个数量级（>50 episode/min），使大规模 SFT+RFT 训练可行。\n3. 显式记忆机制：用结构化 `&lt;conclusion&gt;` 标签在每一步提取并存储关键中间结论，仅将紧凑记忆随上下文输入，实现 30 轮以上长程推理而不触碰上下文长度上限。\n4. 两阶段轻量训练：仅用 5.3 K 条浏览器轨迹完成 SFT→RFT，即显著超越用 10× 数据、依赖强化学习的 Search-R1，在 HotpotQA、2Wiki、Bamboogle 等多跳数据集上平均提升约 20%。\n\n综上，论文提出“浏览器原生、人类式交互”的新范式，兼顾了交互深度、训练效率与长程推理，为构建可扩展的通用 Web Agent 提供了端到端解决方案。\n\nQ2: 有哪些相关研究？\n\n与 BrowserAgent 直接可比或构成技术背景的研究可归纳为三条主线，共 12 篇代表性工作：\n\n1. 浏览器交互与基准\n\n- WebArena (Zhou et al., 2024)\n- AssistantBench (Yoran et al., 2024)\n- WebShop (Yao et al., 2022)\n共同点：提供高保真网页环境，但仅作评测，未解决大规模训练效率问题。\n2. 检索-增强问答（RAG / 搜索-推理）\n\n- Search-R1 (Jin et al., 2025b)\n- IRCoT (Trivedi et"}},{"id":"hf-13-1760485638284","title":"ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems","description":"In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.","url":"https://huggingface.co/papers/2510.11652","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.284Z","metadata":{"authors":["Xin Gui","King Zhu","JinCheng Ren","Qianben Chen","Zekun Moore Wang","Yizhi LI","Xinpeng Liu","Xiaowan Li","Wenli Ren","Linyu Miao","Tianrui Qin","Ziqi Shu","He Zhu","Xiangru Tang","Dingfeng Shi","Jiaheng Liu","Yuchen Eleanor Jiang","Minghao Liu","Ge Zhang","Wangchunshu Zhou"],"date":"","rank":14,"llmAnalysis":"","categories":["cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.11652.pdf","arxivUrl":"https://arxiv.org/abs/2510.11652","arxivId":"2510.11652","coolPaperUrl":"https://papers.cool/arxiv/2510.11652","published":"2025-10-13T17:30:36Z","updated":"2025-10-13T17:30:36.000Z","zh_summary":"近年来，大型语言模型（LLMs）和智能体的研究重点越来越多地从展示新颖的能力转向复杂的推理和应对具有挑战性的任务。然而，现有的评估主要集中在数学/代码竞赛或一般任务上，而现有的多领域学术基准缺乏足够的推理深度，使得该领域没有严格的高级推理基准。为了填补这一空白，我们引入了 Acadreason 基准，旨在评估法学硕士和代理在学术知识上获取和推理的能力。它由 50 个专家注释的学术问题组成，涵盖计算机科学、经济学、法律、数学和哲学等五个高推理领域。所有问题均来自近年来的顶级出版物，并经过严格的注释和质量控制，以确保它们既具有挑战性又可回答。我们对 10 多个主流 LLM 和代理进行系统评估。结果显示，大多数LLM的得分都在20分以下，即使是尖端的GPT-5也只达到了16分。虽然代理商获得了更高的分数，但没有一个超过 40 分。这展示了目前法学硕士和智能体在超智能学术研究任务中的能力差距，并凸显了 Acadreason 的挑战。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文旨在解决现有大模型推理评测基准在“领域广度”与“推理深度”上的双重不足：\n\n- **领域广度缺失**：已有基准（如 MMLU-Pro、GPQA、GAIA、PaperBench 等）过度集中在数学、编程或通用常识任务，缺乏对**人文社科**（法律、哲学、经济等）与**前沿交叉学科**的覆盖。\n- **推理深度不足**：多数基准停留在**表层信息检索或单步常识推理**，无法考察模型是否具备**研究生级别的、多步理论推导与批判性分析能力**，难以反映真实科研场景中的复杂认知负荷。\n\n为此，作者提出 **ACADREASON** 基准，通过从 2023–2025 年五大领域（计算机、经济、法律、数学、哲学）的**顶级理论论文**中人工抽取 50 道高阶研究题，配套可验证的评分清单与三类知识提示，系统评测大模型与智能体在**无原文条件下**完成前沿学术推理的能力。实验结果显示，最强模型 GPT-5 的通过率仅 16%，最佳智能体框架未达 40%，充分暴露当前系统在面对**真正学术级深度推理任务**时的能力鸿沟，从而推动社区向“超智能科研助手”目标迈进。\n\nQ2: 有哪些相关研究？\n\n论文第 3 页 “2 Related Work” 将相关研究划分为两条主线，并指出它们与 ACADREASON 的差距。可归纳为以下两类：\n\n1. 大推理模型（LRM）与智能体框架\n\n- DeepSeek-R1、OpenAI o1/o3、Qwen3 等通过强化学习或 inference-time scaling 提升推理链长度与准确性。\n- 智能体框架（OAgents、MiroFlow、WebThinker 等）外挂搜索、代码执行、数据库查询等工具，弥补模型内部知识不足，在 GAIA、BrowseComp 等基准上取得 SOTA。\n2. 推理评测基准\n\n- **数学/代码导向**：MMLU-Pro、GPQA、SuperGPQA 主要考察中学-本科难度知识问答；PaperBench 要求复现 ICML 论文代码，聚焦机器学习领域。\n- **通用工具使用**：GAIA、BrowseComp 强调多步工具调用与网络信息整合，但题目多为生活常识或公开事实检索，推理深度有限。\n- **科研辅助**：arXivBench 仅测试“生成论文标题+链接”等表层能力；DeepResearch Bench 覆盖多域但题目难度仍偏应用综述，而非理论推导。\n\n**共同缺陷**：\n\n- 领域覆盖偏科（重数学/CS、轻人文社科）；\n- 推理深度不足（缺少需“定义-定理-证明-批判”完整链条的研究生级理论题）；\n- 题目时效性低（大量题目源自 2020 年前教材或维基）。\n\nACADREASON 通过精选 2023–2025 五大领域顶级理论论文，首次把“前沿、可解、高推理深度”的学术研究题纳入统一基准，填补了上述空白。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“构建新基准 + 系统评估 + 诊断分析”的三段式路线解决“缺乏高阶学术推理评测”的问题，具体步骤如下：\n\n1. 构建 ACADREASON 基准\n1.1 精选源头\n\n- 时间窗：2023–2025 年\n- 渠道：CS、经济、法律、数学、哲学五大领域顶刊/顶会（IEEE TIT、AER、Harvard Law Review、SODA 等）\n- 筛选：10 名领域专家按“纯理论、高推理复杂度”标准从 430 篇候选中保留 50 篇\n\n1.2 人工抽取高阶研究题\n\n- 每篇仅取 1 个核心研究问题，重写为自包含、可解、边界清晰的形式化题目\n- 同步撰写“黄金答案”，覆盖背景、定义、推导、结论，确保可复现\n\n1.3 设计动态评分工具\n\n- **Checklist**：从黄金答案提炼 5–7 个可验证、相互独立的关键步骤，用于细粒度打分\n- **Hints**：分背景、定义、方法论三类，支持后续消融实验\n\n1.4 多轮质控\n\n- 三专家独立审核“题目可解性、信息完整性、学科逻辑一致性”\n- 迭代修订直至通过一致性检查，确保 50 题全部达到“研究生资格考试”难度\n2. 系统评估主流模型与智能体\n\n- 基线：覆盖 10+ 大模型（GPT-5、DeepSeek-R1、o3、Claude-4-Sonnet 等）与 8 个智能体框架（OAgents、Gemini-2.5-Pro-DeepResearch 等）\n- 协议：零样本闭卷答题，允许智能体调用搜索/代码工具；采用 GPT-5-mini 作为 LLM-as-Judge，统一按“通过率 Rp”与“清单分 Rj”双指标评测\n3. 诊断与改进线索\n3.1 结果揭示能力鸿沟\n\n- 最强单模型 GPT-5 仅 16 % 通过率，40.6 清单分；最佳智能体 OAgents 34 % 通过率，65.1 清单分，远未饱和\n\n3.2 消融实验定位瓶颈\n\n- 三类提示中，“方法论提示”带来最大平均增益（↑20–30 分），说明模型缺的是“如何推”而非“背景是什么”\n- 人文类题目对外部知识依赖更高，STEM 类题目对推理链深度更敏感\n\n3.3 失败案例分析\n\n- 法律案例显示，GPT-5 只能完成表层法条对照，无法像 OAgents 一样整合政治经济语境完成“根因-修辞-司法影响”的多维批判\n\n通过上述“高难度题目库 + 细粒度评测协议 + 增益诊断”，论文不仅量化地暴露了当前大模型在真正学术推理场景下的短板，也为后续“检索增强、推理链扩展、领域工具融合”提供了明确的改进方向。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **ACADREASON** 共设计并执行了 4 组实验，全面扫描模型与智能体在“学术级推理”上的能力分布与瓶颈。\n\n1. 主实验：主流模型与智能体整体评测\n\n- 基线\n– 通用模型：GPT-oss、GPT-4.1、GPT-5、DeepSeek-V3/V3.1、Claude-4-Sonnet\n– 推理专用模型：Qwen3、Kimi-k2、o3、DeepSeek-R1、Gemini-2.5-Pro\n– 智能体框架：OAgents、Gemini-2.5-Pro-DeepResearch、o3-DeepResearch、Tongyi-DeepResearch、AFM、WebThinker、MiroThinker、WebDancer\n- 协议\n– 零样本闭卷，允许智能体调用搜索/代码/数据库；禁止直接访问原始论文\n– GPT-5-mini 作为统一 judge，输出双指标：\n- 通过率  R_p = (1) / (50)∑_(q=1)^(50) s_q× 100 \n- 清单分  R_j = (1) / (250)∑_(q=1)^(50)∑_(i=1)^(5) c_(q,i)× 100 \n- 结果\n– 单模型最佳：GPT-5 仅 16 / 40.5\n– 智能体最佳：OAgents 34 / 65.1\n– CS、经济两域平均清单分显著低于法律、哲学，验证题目难度跨域差异\n2. 消融实验：三类知识提示的增益\n\n- 设计\n– 对同一 50 题依次补入 background、definition、methodology、ALL 四类提示\n– 受试模型：GPT-5、GPT-oss、DeepSeek-V3/V3.1、GPT-4.1、Claude-4-Sonnet、Qwen3、Kimi-k2、o3、DeepSeek-R1\n- 度量\n– 绝对提升  Delta R_j = R_j^(hint) - R_j^(no-hint) \n- 结果\n– methodology 提示平均提升 20–30 分，显著高于 background（≈ 5–8 分）\n– 人文类题目（法、哲、经）对外部知识提示更敏感；STEM 类（CS、数学）提升主要依赖方法论提示\n3. 域别细粒度分析\n\n- 将 50 题按学科拆分，统计各域通过率和清单分分布\n- 发现\n– 法律、哲学高阶论证题虽知识生僻，但推理链较短，模型凭提示可“补知识”后得分较高\n– 数学、CS 题需多步推导，即使给提示，模型仍在公式变形、下界证明环节失分\n4. 失败案例显微分析\n\n- 选取法律案例“设计专利滥用 counterfeit 概念”对比最高分智能体 OAgents 与最高分单模型 GPT-5 的完整生成路径\n- 四步 checklist：\n1. 指出法律谬误\n2. 驳斥虚假安全命题\n3. 剖析根因\n4. 指明司法影响\n- 结果\n– OAgents 四步全对；GPT-5 仅完成 1、4 两步\n– 错误根源：GPT-5 停留在“假冒=消费者风险”表层关联，未进行“专利非质量认证”的深层反驳，也无法把“程序杠杆”上升为“协调游说策略”的政治经济综合判断\n\n4 组实验共同证明：ACADREASON 成功把当前最强模型拉到 40 分以下区间，同时精准揭示“知识检索–方法掌握–批判综合”三级瓶颈，为后续研究提供可量化的改进靶点。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可在此基础上继续深入，分为“基准扩展”“评测细化”“模型改进”三条主线，供后续研究参考。\n\n1\\. 基准扩展\n\n- **动态更新机制**\n建立年度滚动收录流程，自动抓取最新顶刊理论论文并生成新题，避免基准被针对性过拟合。\n- **跨模态学术推理**\n将论文中的关键图表、公式截图或证明树作为输入，考察模型对“图-文混合”论证的理解与续推能力。\n- **实验/实证类题目**\n当前仅含纯理论题，可引入计量经济学、实验哲学等“数据+理论”混合题型，考察模型对研究设计、因果识别的批判。\n- **交叉学科子集**\n构造 CS-Law（算法公平）、Econ-Philosophy（福利伦理）等跨域题目，评测模型在真实科研中常见的“领域桥接”能力。\n\n2\\. 评测细化\n\n- **细粒度能力标签**\n为每题标注所需的具体认知技能（定义复述、定理应用、反例构造、悖论消解、价值权衡等），输出模型技能雷达图。\n- **推理链深度分布**\n记录黄金答案的推导步数 k，绘制“模型得分随 k 衰减曲线”，量化其多步推理衰减系数。\n- **可解释性评测**\n引入 step-by-step 人工打分，对比模型生成链与黄金链的“逻辑对齐度”，而不仅仅看最终结论。\n- **对抗性干扰**\n在提示中加入与题目相关但逻辑错误的“伪定理”或“假数据”，测试模型对误导信息的鲁棒性。\n\n3\\. 模型改进\n\n- **工具调用策略搜索**\n当前智能体为固定 DAG 或顺序调用，可探索基于强化学习的“何时搜、搜什么、如何合成”最优策略。\n- **领域工具注入**\n为法律域接入判例数据库、为经济域接入宏观数据库，观察“垂直工具”相比通用搜索的边际增益。\n- **推理链自纠正**\n在生成链中插入“验证器”角色，对每一步推导进行合法性检查，发现矛盾后自动回退，提高长链准确率。\n- **小参数模型蒸馏**\n用大模型在 ACADREASON 上的失败/成功轨迹蒸馏出“学术推理专用小模型”，测试是否能在 7B–13B 规模逼近大模型性能，降低部署成本。\n- **人-机协同接口**\n让真实研究者使用同一题目与模型交互，记录“人机共推”相比纯机推理的增益，量化模型在科研 workflow 中的辅助价值。\n\n4\\. 伦理与认知风险\n\n- **知识剽窃度量**\n检测模型生成答案与原文的重合度，确保基准不会鼓励“变相抄袭”，并设立学术伦理红线。\n- **过度置信校准**\n记录模型对错误结论的置信概率，分析其在学术场景下的“伪权威”风险，为后续可信度机制提供数据。\n\n通过持续扩展题目边界、细化认知维度并反向驱动模型架构与训练策略，ACADREASON 可由“静态评测集”演进为“学术推理能力持续监测与改进平台”。\n\nQ6: 总结一下论文的主要内容\n\n**ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems**\nOPPO AI Agent Team, arXiv:2510.11652v1, 13 Oct 2025\n\n1\\. 背景与动机\n\n- 现有推理基准（MMLU-Pro、GPQA、GAIA、PaperBench 等）\n– 偏重数学/代码或生活常识，缺乏**人文社科**与**前沿交叉领域**；\n– 题目多为单步或浅层整合，**推理深度不足**且已接近饱和。\n- 亟需一个“研究生级别”的多学科基准，衡量大模型与智能体在**真实科研场景**中获取前沿知识并进行**多步理论推导**的能力。\n\n2\\. ACADREASON 基准构建\n\n| 阶段 | 关键操作 | 输出 |\n| --- | --- | --- |\n| ① 选文 | 10 名领域专家从 2023–2025 五大领域（CS、经济、法律、数学、哲学）顶刊/顶会筛选 430 → 50 篇纯理论论文 | 50 篇源头论文 |\n| ② 命题 | 每篇人工提炼 1 个自包含研究题，撰写含背景-定义-推导-结论的黄金答案 | 50 题 + 50 份完整解答 |\n| ③ 评分工具 | 设计动态 checklist（5–7 个可验证关键步骤）与三类 hints（背景、定义、方法论） | 评分标准与提示集 |\n\n3\\. 实验设计\n\n- **主实验**：10+ 大模型（含 GPT-5、o3、DeepSeek-R1 等）与 8 个智能体框架零样本答题，GPT-5-mini 作为统一 judge，指标：\n– 通过率  R_p （与黄金答案完全匹配）\n– 清单分  R_j （checklist 项目满足率）\n- **消融实验**：同模型逐次补入三类 hints，量化不同知识类型的边际增益。\n- **失败案例分析**：对比最高分智能体 OAgents 与最高分单模型 GPT-5 在同一法律题上的推理链，定位“知识-方法-批判”三级瓶颈。\n\n4\\. 主要结果\n\n| 系统 |  R_p  /  R_j  | 备注 |\n| --- | --- | --- |\n| 最强单模型 GPT-5 | 16 / 40.5 | 远低于及格线 |\n| 最强智能体 OAgents | 34 / 65.1 | 仍距满分甚远 |\n| methodology hints | 平均 +20–30 分 | 显著 > background hints |\n| 域别差异 | CS、经济得分最低 | 理论推导链更长 |\n\n5\\. 结论与贡献\n\n- 提出**首个跨五大学科、研究生难度**的学术推理基准 ACADREASON，50 题全部公开。\n- 实证揭示当前最强模型在真正前沿理论任务上**通过率 < 20%**，智能体亦未突破 40%，能力鸿沟明确。\n- 方法论提示带来最大增益，为后续“工具增强 + 推理链自纠正”提供量化靶点。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是ACADREASON%3A%20Exploring%20the%20Limits%20of%20Reasoning%20Models%20with%20Academic%20Research%20Problems，链接是https://arxiv.org/pdf/2510.11652，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11652。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-14-1760485638285","title":"FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs","description":"The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.","url":"https://huggingface.co/papers/2510.08886","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.285Z","metadata":{"authors":["Yan Wang","Keyi Wang","Shanshan Yang","Jaisal Patel","Jeff Zhao","Fengran Mo","Xueqing Peng","Lingfei Qian","Jimin Huang","Guojun Xiong","Xiao-Yang Liu","Jian-Yun Nie"],"date":"","rank":15,"llmAnalysis":"","categories":["cs.CL","cs.CE","cs.IR"],"pdfUrl":"https://arxiv.org/pdf/2510.08886.pdf","arxivUrl":"https://arxiv.org/abs/2510.08886","arxivId":"2510.08886","coolPaperUrl":"https://papers.cool/arxiv/2510.08886","published":"2025-10-10T00:41:55Z","updated":"2025-10-10T00:41:55.000Z","zh_summary":"公认会计原则（GAAP）的复杂性以及可扩展商业报告语言（XBRL）文件的层次结构使得财务审计自动化和验证变得越来越困难。尽管大型语言模型（LLM）在非结构化文本理解方面表现出了强大的能力，但它们在结构化、相互依赖且由分类法驱动的财务文件上的推理能力尚未得到充分探索。为填补这一空白，我们提出了FinAuditing，这是首个面向分类法、结构感知的多文档基准，用于评估大型语言模型在财务审计任务中的表现。FinAuditing基于符合美国GAAP的真实XBRL文件构建，定义了三个互补的子任务：FinSM（语义一致性）、FinRE（关系一致性）和FinMR（数值一致性），每个子任务针对结构化审计推理的不同方面。我们进一步提出了一个统一的评估框架，整合了各子任务的检索、分类和推理指标。在13个最先进的LLM上的大规模零样本实验表明，目前模型在语义、关系和数学三个维度上的表现不一致，在处理层次化多文档结构时准确率下降高达60%至90%。我们的研究揭示了现代大型语言模型在基于分类法的财务推理方面的系统性局限性，并确立了FinAuditing作为开发可信、结构感知且符合法规的财务智能系统的基础。该基准数据集可在Hugging Face获取。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**现有大语言模型（LLM）在结构化、层级化、多文档的 XBRL 财务报告审计场景下表现严重不足**的问题。具体而言，它聚焦以下三点核心缺陷：\n\n1. **语义一致性缺失**\n当前模型难以判断 XBRL 实例文档中使用的 US-GAAP 标签是否与官方税务定义语义一致，导致无法自动发现“标签误用”类错误。\n\n2. **层级关系理解薄弱**\n模型缺乏对 XBRL 链接库（presentation/calculation/definition linkbase）中父子、兄弟、轴-成员等层级约束的细粒度理解，无法识别“关系颠倒”“不当归属”“轴-成员非法组合”等结构错误。\n\n3. **数值逻辑推理不足**\n面对计算链接库定义的加减乘除及维度交叉检验规则，模型难以完成多步数值推导，无法验证“汇总项 ≠ 子项之和”“维度交叉校验不通过”等计算性错误。\n\n为此，作者提出 **FINAUDITING**——首个面向真实 US-GAAP 合规 XBRL 报送的、taxonomy-driven 的多文档审计基准，通过 **FinSM（语义匹配）、FinRE（关系抽取）、FinMR（数值推理）** 三项子任务，系统评估 LLM 在结构化财务数据上的检索、分类与推理能力，揭示其在跨文档、层级化、税务约束场景下的系统性局限。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为三条主线，均与金融 NLP 或 XBRL 有关，但均未同时满足“结构化-层级化-跨文档-税务约束”四重要求：\n\n1. 面向**单文档、非结构化或半结构化**的金融基准\n\n- **FiNER**、**FNXL**：仅做 XBRL 句子级数值实体抽取，无层级关系。\n- **FinQA / ConvFinQA / TAT-QA / MultiHiertt / DocMath-Eval**：在财报文本+表格上做数值问答，缺乏税务词典监督，也不跨多份 XBRL 链接库。\n- **Headlines、ECTSum、BIGDATA22**：针对新闻或电话会议纪要，任务局限于情感、摘要或股价预测，与审计无关。\n2. 面向**XBRL 局部子任务**的初步探索\n\n- **Loukas et al.**、**Sharma et al.**：早期抽取式 XBRL 标注数据集，仅考虑单标签分类。\n- **Wang et al. 2025（FinTagging）**：首次用 LLM 做标签归一化，但未评估层级或计算关系。\n- **Steven et al.**、**Han et al.（XBRL-Agent）**：用 ChatGPT 做标签检索或问答，任务单一且未引入官方计算/定义链接库约束。\n3. 面向**通用数值推理或文档级数学**的基准\n\n- **DOCPRO、MathBERT、DocMath-Eval** 等强调长文档数学推理，却非金融域，也不存在 US-GAAP 税务层级。\n\n综上，现有工作要么停留在“文本+表格”表层语义，要么仅触及 XBRL 单环节，**均未系统评估模型在跨文档、税务层级、计算一致性上的综合审计能力**。FINAUDITING 首次把真实 XBRL 六件套（实例+模式+四链接库）与 US-GAAP 税务词典同时纳入评测，填补了该空白。\n\nQ3: 论文如何解决这个问题？\n\n论文并未提出新的模型架构，而是通过“构建基准 + 统一评测框架”的方式，把问题转化为可量化的实验任务，迫使社区直面 LLM 在结构化财务审计上的缺陷。具体手段如下：\n\n1. 构建 FINAUDITING 基准\n\\-1.1 原始数据：从 SEC 抓取 372 份 2022–2024 年真实 XBRL 报送，结合 XBRL-US 官方 4 545 条 DQC 错误消息，筛选出 9 类高频错误（占比 60 %），对应语义、关系、计算三大维度。\n\\-1.2 实例级标注：用 GPT-4o-mini 自动抽取错误标签、错误关系三元组、<reported, calculated> 数值对，再经 50 % 人工复核，得到 1 102 条高质量实例。\n\\-1.3 层级切片：按 statement-roleType 把完整 XBRL 六件套切分成子报送，既保留父子结构，又把平均长度压到 33 k token 级别，适配现有 LLM 上下文。\n\\-1.4 税务词典切块：将 US-GAAP 2024 官方 Excel 解析为 concept-centric 核心块 + 关系块（presentation/calculation/definition/reference/enumeration），实现细粒度检索。\n\n2. 设计三项互补任务\n\n- **FinSM（Financial Semantic Matching）**：给定查询 + XBRL 子报送 + 税务词典，检索“语义不一致”的 US-GAAP 标签，考察检索命中率、召回、Macro-F1。\n- **FinRE（Financial Relation Extraction）**：给定两个概念 + 子报送 + 税务词典，分类关系错误类型 {Reversal, Inappropriateness, CombinationErr}，考察 Accuracy、Macro-F1。\n- **FinMR（Financial Mathematical Reasoning）**：给定两个问题（提取值、计算值）+ 子报送 + 税务词典，输出 JSON{“extracted\\_value”: “x”, “calculated\\_value”: “y”}，用 LLM-as-a-judge 评 Accuracy，并细分 SER/EER/CER 错误率。\n3. 统一零样本评测协议\n\n- 覆盖 13 个 SOTA 模型（闭源：GPT-4o；开源通用：DeepSeek-V3、Qwen3-235B、Llama-3.3-70B 等；金融专精：Fin-o1-14B、Fin-R1）。\n- 统一 81 920 token 输入上限、512 token 输出上限，采用 LM-Eval-Harness，保证可复现。\n- 多维度指标：检索任务用 HR@k/R@k/Macro-F1@k；分类任务用 Acc/Macro-P/R/F1；数值任务用 Acc + 三级错误率，实现同一框架跨任务对比。\n4. 实验结果驱动社区关注\n\n- 零样本下，最佳模型在 FinSM 平均 Hit-Rate 仅 11.9 %，FinRE 最高 Acc 91.8 % 但开源模型普遍 <70 %，FinMR 最高 Acc 仅 13.9 %，计算错误率普遍 >70 %。\n- 结果直接暴露“语义对齐-层级理解-数值推理”链条的系统性短板，为后续研究提供量化基线与改进方向。\n\n通过上述“真实数据 + 三重任务 + 统一协议”，论文把原本零散的审计错误类型整合成可重复、可度量的 benchmark，迫使未来模型在结构化、层级化、跨文档、税务约束场景下持续优化。\n\nQ4: 论文做了哪些实验？\n\n实验部分围绕 **FINAUDITING** 三项子任务展开，全部在 **零样本（zero-shot）** 设定下完成，覆盖 13 个 SOTA 大模型，统一输入/输出长度与评测协议，具体如下：\n\n1. 实验规模\n\n- 模型：13 个，分三类\n– 闭源通用：GPT-4o\n– 开源通用：DeepSeek-V3、Qwen3-235B-A22B、Llama-4-Scout-17B、Qwen2.5-72B、Llama-3.3-70B、Qwen3-32B、gemma-3-27b-it、gemma-3-12b-it、Llama-3.1-8B、Llama-3.2-3B\n– 金融专精：Fin-o1-14B、Fin-R1\n- 数据：1 102 条已标注 XBRL 子报送（FinSM 330、FinRE 440、FinMR 332）\n- 环境：LM-Eval-Harness；开源模型在 4×H200(140 GB) 集群本地推理，闭源模型调用 OpenAI/TogetherAI API；最大输入 81 920 token，输出 512 token；总计 ≈320 GPU 小时、≈$200 API 费用。\n2. FinSM 实验\n\n- 指标：Hit-Rate@k、Recall@k、Macro-F1@k（k=1/5/10/20）\n- 结果：\n– 最佳平均 Hit-Rate 仅 11.89 %（DeepSeek-V3），GPT-4o 8.79 %；金融域模型 Fin-o1-14B 直接 0 %。\n– 召回与 F1 同步低迷，最高 Macro-F1@AVE 9.35 %，显示“检索准度-覆盖-精度/召回平衡”全面不足。\n3. FinRE 实验\n\n- 指标：Accuracy、Macro-P、Macro-R、Macro-F1；细分类别 F1\n- 结果：\n– GPT-4o 取得 91.82 % Acc、90 % 级 Macro-F1，遥遥领先；DeepSeek-V3 82.73 % Acc 次之。\n– 开源模型普遍 <70 %，Llama 系列甚至 <30 %；金融专精模型 Fin-o1-14B 与 Qwen3-32B 全部 0 %。\n– 类别消融：CombinationErr（轴-成员非法组合）最难，多数开源模型 F1≈0；GPT-4o 在该类仍达 100 % F1。\n4. FinMR 实验\n\n- 指标：Overall Accuracy（LLM-as-a-judge）+ 三级错误率 SER/EER/CER\n- 结果：\n– 最高 Accuracy 仅 13.86 %（Fin-o1-14B），GPT-4o 10.24 %；其余模型 <10 %。\n– 错误分布：CER（计算错误）主导，普遍 70–83 %；EER（提取错误）10–22 %；SER（结构错误）波动大，Fin-o1-14B 虽 CER 低至 9 %，但 SER 高达 71 %，暴露“看懂结构才能算对”的瓶颈。\n5. 横向对比与显著性观察\n\n- 同一家族模型规模增大≠性能单调提升（Llama 系列反差明显）。\n- 金融域模型在检索与关系任务上几乎全线崩溃，说明“领域语料预训练”无法替代“结构化-税务对齐”能力。\n- CombinationErr 与 CER 成为 hardest nut，直接指向“跨链接库轴-成员校验”与“多步计算链”两大核心挑战。\n\n通过上述实验，论文用量化结果首次系统揭示了当前 LLM 在真实 XBRL 多文档审计场景下的**一致性缺口**，为后续结构感知、税务对齐的财务大模型研究奠定基线。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 FINAUDITING 暴露出的“系统性缺口”的直接延伸，既具学术价值，也贴合监管与工业落地的刚性需求：\n\n1. 结构感知预训练\n\n- 将 XBRL 六件套解析为“图-序列”混合格式，显式保留父子、计算、定义弧，设计 Tree-Transformer / Graph-Transformer 预训练目标（节点重建+弧分类+数值重构），让模型在预训练阶段即习得税务层级。\n2. 税务对齐的检索增强（Taxonomy-Aligned RAG）\n\n- 把 US-GAAP 概念块编码为双层索引（核心属性 + 关系子图），在 FinSM/FinRE/FinMR 每一步先做“税务子图检索”，再生成答案；探索稠密-稀疏混合检索、对比学习微调，缓解语义漂移。\n3. 跨链接库轴-成员一致性约束推理\n\n- 针对 CombinationErr 零分现象，构建“轴-成员封闭世界约束”自动抽取器，将每条 US-GAAP 规则转化为可验证的 JSON-Schema 或 Answer-Set Program，实现可解释验证。\n4. 数值符号与维度交叉自动修复\n\n- 利用 FinMR 的 <reported, calculated> 对，训练“符号修正 + 维度补全”双重生成模型：先预测符号/维度缺失，再重写实例文档，实现可回写的审计机器人。\n5. 多模态报送一致性检查\n\n- 把 Inline XBRL 中的 HTML 可视化文本与底层标记对齐，引入版面理解（LayoutLMv3、DocFormer）做“呈现 vs. 结构”交叉验证，检测管理层故意隐藏维度或错误归并的行文技巧。\n6. 增量-隐私友好的领域持续学习\n\n- 探索参数高效微调（LoRA/AdaLoRA）与联邦学习结合，让不同会计师事务所共享梯度而非原始底稿，持续更新模型以适配每年变化的 US-GAAP 税务版本。\n7. 可解释审计轨迹生成\n\n- 在生成答案同时输出“概念检索路径→关系验证→计算链”三段落解释，采用 RLHF 奖励模型把“监管可读性”作为显式优化目标，满足 PCAOB/SEC 对审计底稿可追溯的要求。\n8. 跨准则泛化与多语言扩展\n\n- 将基准迁移至 IFRS、CAS、ESG 披露准则，检验模型能否零样本迁移到不同税务层级；同时构建中文、日语、德语标签的多语言 FinSM 任务，评估概念语义与语言表面形式的解耦能力。\n9. 对抗性披露检测\n\n- 基于 FinRE 框架构造“对抗样本”：自动把正确父子关系颠倒或插入非法轴-成员，测试模型鲁棒性；进一步用红队方法生成“难以察觉”的误导性标签，训练鲁棒一致性判别器。\n10. 实时流式审计\n\n- 利用 XBRL 实例文档的“片段更新”机制，设计增量编码与缓存策略，实现财报发布秒级结构一致性扫描，为交易所、监管机构提供即时的 Restatement 预警 API。\n\n这些方向共同指向“结构-税务-数值”三位一体的下一代金融大模型，既填补 FINAUDITING 揭示的能力真空，也为全自动、可解释、监管友好的智能审计系统奠定研究路线。\n\nQ6: 总结一下论文的主要内容\n\n**FINAUDITING: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs**\n一句话总结：首个面向真实 US-GAAP/XBRL 多文档审计场景的结构化基准，通过三项子任务量化揭示当前大模型在语义-关系-数值一致性上的系统性短板。\n\n| 维度 | 核心内容 |\n| --- | --- |\n| 问题 | GAAP 日趋复杂，XBRL 报送层级交错，导致财务重述频发；现有 LLM 评测仅停留在单文档文本，无法衡量其在“税务约束-跨文档-层级结构”下的审计能力。 |\n| 数据 | 372 份 2022-2024 真实 10-K/10-Q XBRL 六件套 → 1 102 条高错样本，覆盖 9 类 DQC 高频错误（语义/关系/计算）。 |\n| 任务 | 1) FinSM──检索与税务概念语义不一致的标签；2) FinRE──分类父子颠倒、不当归属、轴-成员非法组合三类关系错误；3) FinMR──提取账面值并按计算链接库重算，验证数值一致性。 |\n| 评测 | 零样本、13 个 SOTA 模型（含 GPT-4o、DeepSeek-V3、Fin-o1-14B 等）；统一 81 k token 输入；指标：HR/R/F1、Acc、Macro-F1、LLM-as-a-judge + SER/EER/CER。 |\n| 结果 | – FinSM：最佳 Hit-Rate 11.9 %，金融域模型 0 %。– FinRE：GPT-4o 91.8 % Acc，开源普遍 <70 %；轴-成员校验最难。– FinMR：最高 Acc 13.9 %，计算错误率 70-83 %。 |\n| 结论 | 即使最强模型也在结构化、层级化、税务驱动的跨文档推理上全面失守；亟需结构感知、税务对齐的新一代金融大模型。 |\n| 资源 | 数据集与评测代码已开源 Hugging Face，支持后续研究与监管落地。 |\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是FinAuditing%3A%20A%20Financial%20Taxonomy-Structured%20Multi-Document%20Benchmark%20for%20Evaluating%20LLMs，链接是https://arxiv.org/pdf/2510.08886，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.08886。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-15-1760485638285","title":"DocReward: A Document Reward Model for Structuring and Stylizing","description":"Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.","url":"https://huggingface.co/papers/2510.11391","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.285Z","metadata":{"authors":["Junpeng Liu","Yuzhong Zhao","Bowen Cao","Jiayu Ding","Yilin Jia","Tengchao Lv","Yupan Huang","Shaohan Huang","Nan Yang","Li Dong","Lei Cui","Tao Ge","Xun Wang","Huitian Jiao","Sun Mao","FNU Kartik","Si-Qing Chen","Wai Lam","Furu Wei"],"date":"","rank":16,"llmAnalysis":"","categories":["cs.CV","cs.AI","cs.CL"],"pdfUrl":"https://arxiv.org/pdf/2510.11391.pdf","arxivUrl":"https://arxiv.org/abs/2510.11391","arxivId":"2510.11391","coolPaperUrl":"https://papers.cool/arxiv/2510.11391","published":"2025-10-13T13:36:32Z","updated":"2025-10-13T13:36:32.000Z","zh_summary":"最近在自主工作流（agentic workflows）方面的进展使得诸如专业文档生成等任务的自动化成为可能。然而，这些工作流主要关注文本质量，却忽视了视觉结构和风格，而这对于可读性和参与度至关重要。这一差距主要源于缺乏合适的奖励模型来引导自主工作流生成具有更强结构性和风格质量的文档。为此，我们提出了 DocReward，一种基于文档结构和风格评估文档的奖励模型。我们构建了一个多领域数据集 DocPair，包含 117K 对文档，涵盖 32 个领域和 267 种文档类型，每对文档均包括内容相同但结构和风格不同的高专业度文档和低专业度文档。这使得模型能够以不依赖文本质量的方式全面评估专业度。DocReward 使用 Bradley-Terry 损失进行训练，对文档进行评分，并对与标注排序相矛盾的预测进行惩罚。为了评估奖励模型的表现，我们创建了一个测试数据集，其中包含由受过良好教育的人工评审员排序的文档集合。值得注意的是，DocReward 在准确率上分别超越 GPT-4o 和 GPT-5 30.6 和 19.4 个百分点，显示出其优于基线方法的优势。在文档生成的外部评估中，DocReward 实现了显著更高的胜率 60.8%，相比之下，GPT-5 的胜率为 37.7%，这证明了其在引导生成代理生成更符合人类偏好的文档方面的有效性。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n该论文针对现有“智能体工作流”在生成专业文档时普遍只关注文本质量、忽视视觉结构与排版风格的问题，提出并验证了 DOCREWARD——一个专门评估文档“结构+风格”专业度的奖励模型。核心待解决问题可概括为：\n\n- **缺乏可指导结构与风格优化的奖励信号**\n现有方法缺少能够量化“排版专业度”的奖励模型，导致生成代理无从判断并改进留白、对齐、字体、页眉页脚等视觉要素。\n\n- **需要兼顾“全面性”与“文本无关性”的评价体系**\n全面性：横跨多领域、多版式；文本无关性：评分仅取决于结构与风格，而不受文本内容本身好坏影响。\n\n- **缺少成对比较数据与评测基准**\n为此构建 117 K 同内容异排版的文档对（DOCPAIR）以及人工标注的 473 对评测集，以支持偏好学习与系统评估。\n\n简言之，论文旨在填补“文档视觉专业度可学习奖励模型”这一空白，使智能体工作流在生成环节能够显式优化结构与风格，最终输出人类更偏好的专业文档。\n\nQ2: 有哪些相关研究？\n\n论文在第 5 节“Related Work”中将与 DOCREWARD 相关的研究划分为三大主线，并指出它们与本文任务的核心差异。可归纳如下：\n\n1. 美学与专业度评估（Aesthetic & Professionalism Assessment）\n\n- 平面设计：AesthetiQ 用多模态 LLM 做布局美学偏好判别；LACE 在扩散模型中加入可微美学约束。\n- 网页/移动端界面：Calista、Android UIs 等利用显式评分或成对比较来学习“视觉吸引力”，并与可用性指标关联。\n- 照片/视频：A-Lamp 等 CNN 结构引入布局感知机制，预测照片美学分数。\n→ 共同点：依赖人类偏好信号；差异：聚焦单帧图像、UI 或海报，而非多页文档的“结构+风格”专业度。\n2. 文档 AI：结构理解与生成（Document AI: Structure & Generation）\n\n- 语义解析：LayoutLM、ReLayout 等模型联合文本与版面坐标，做标题、表格、段落等逻辑区域的检测与信息抽取。\n- OCR 流水线：主要解决字符识别与后续理解任务。\n- 自动排版生成：近期研究尝试用 LLM 生成 DOCX/LaTeX 源码，但评价止步于“内容正确性”或“基础格式是否出错”，未对“专业美观”量化。\n→ 共同点：关注“内容+版面坐标”的语义对齐；差异：尚无奖励模型专门评估视觉专业度，也未在生成阶段优化排版"}},{"id":"hf-16-1760485638285","title":"Don't Just Fine-tune the Agent, Tune the Environment","description":"Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce $\\textbf{Environment Tuning}$, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. $\\textbf{Environment Tuning}$ orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.","url":"https://huggingface.co/papers/2510.10197","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.285Z","metadata":{"authors":["Siyuan Lu","Zechuan Wang","Hongxuan Zhang","Qintong Wu","Leilei Gan","Chenyi Zhuang","Jinjie Gu","Tao Lin"],"date":"","rank":17,"llmAnalysis":"","categories":["cs.AI"],"pdfUrl":"https://arxiv.org/pdf/2510.10197.pdf","arxivUrl":"https://arxiv.org/abs/2510.10197","arxivId":"2510.10197","coolPaperUrl":"https://papers.cool/arxiv/2510.10197","published":"2025-10-11T12:35:15Z","updated":"2025-10-11T12:35:15.000Z","zh_summary":"大型语言模型（LLM）代理在复杂的多轮工具使用任务中展现出巨大潜力，但其发展通常受到高质量训练数据极度稀缺的限制。在合成数据上进行监督微调（SFT）会导致过拟合，而标准强化学习（RL）则面临关键的冷启动问题和训练不稳定性。为了解决这些挑战，我们引入了\\textbf{环境调优}（Environment Tuning），这是一种新型训练范式，使代理能够直接从问题实例中学习复杂行为，而无需依赖预先收集的专家轨迹。\\textbf{环境调优}通过结构化课程、提供纠正反馈的可操作环境增强以及细粒度进度奖励来组织学习过程，从而确保探索的稳定性和效率。仅使用伯克利函数调用排行榜（BFCL）中的400个问题实例，我们的方法不仅在分布内性能上与强基线竞争，而且在分布外泛化能力上表现卓越，克服了基于SFT方法常见的性能崩溃问题。我们的工作展示了从对静态轨迹进行监督微调向动态、基于环境的探索的范式转变，为训练更强大、更高效的数据利用代理铺平了道路。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决**极端数据稀缺场景下多轮工具使用智能体的训练难题**。具体而言，其关注的核心问题可概括为：\n\n- **C1 数据稀缺**：高质量多轮工具调用数据极度匮乏（BFCL V3 仅 800 例），传统监督微调（SFT）依赖合成轨迹，易过拟合且泛化差。\n- **C2 环境复杂**：多领域、跨 API 的 84 种工具形成庞大组合空间，标准强化学习（RL）面临“冷启动”——初始策略无法生成有效轨迹，导致训练崩溃。\n- **C3 长交互链**：任务成功需连续多轮无误，任何单点失败即全局失败，稀疏二元奖励使信用分配与探索异常困难。\n\n为此，作者提出 **ENVIRONMENT TUNING** 范式，通过“调环境”而非“仅调模型”，在仅 400 个实例的条件下，实现从零开始稳定训练，兼顾分布内性能与分布外泛化。\n\nQ2: 有哪些相关研究？\n\n论文在第 2 节“Related work”中将现有研究划分为两条主线，并指出它们与本文任务设定之间的关键差异。相关研究可归纳如下：\n\n1. Tool-Integrated Reasoning（TIR）\n\n- 代表工作：\n– **ReTool**（Feng et al., 2025a）\n– **ToRL**（Li et al., 2025）\n– **SimpleTIR**（Xue et al., 2025）\n- 特点：\n- 采用 RL 替代 SFT 来学习“何时调用工具”，多聚焦于**单一或同构工具**（如计算器、搜索引擎）。\n- 近期方法引入细粒度信用分配、熵引导探索、轨迹过滤等技巧缓解训练崩溃，但仍局限于**短程、单轮或单工具**场景。\n- 与本文区别：\n- 本文需**跨 8 个领域、84 个异构 API** 的多轮编排，状态空间与动作空间复杂度显著更高；现有 TIR 方法未验证在此类**大规模异构工具链**上的可扩展性。\n2. Multi-turn Tool Orchestration\n\n- 数据侧（SFT 路线）\n- **ToolACE**（Liu et al., 2024）\n- **xLAM**（Zhang et al., 2024）\n- **Magnet**（Yin et al., 2025）\n- **APIgen-MT**（Prabhakar et al., 2025）\n- 共同思路：利用合成轨迹+蒸馏进行**大规模监督微调**，在 BFCL V3 上取得较高 ID 分数，但论文表 2 显示其 OOD 性能普遍崩溃（xLAM 从 70.5% → 5.0%）。\n- 交互侧（在线 RL 路线）\n- **ReCall**（Chen et al., 2025b）\n- **ARTIST**（Singh et al., 2025）\n- 尝试直接用 RL 与工具环境交互，但在 BFCL 上仅带来**边际提升**，暴露出复杂环境中的探索与稳定性瓶颈。\n- 与本文区别：\n- 上述工作要么**静态模仿合成轨迹**，要么**直接在线 RL 无课程/无环境增强**。本文则通过**结构化课程 + 可执行环境增强 + 细粒度进度奖励**，把“环境反馈”本身作为可调组件，从而用 400 样本实现稳定训练与强泛化。\n\n简言之，现有研究或聚焦于**单工具/短程推理**，或依赖**大规模合成数据+SFT**，或尝试**直接在线 RL** 但受限于冷启动与稀疏奖励。本文首次提出\\*\\*“调环境”而非仅调模型**的范式，将课程、奖励与反馈机制系统性地嵌入环境，从而在**极端数据稀缺且工具异构的多轮场景\\*\\*中实现可扩展的强化学习训练。\n\nQ3: 论文如何解决这个问题？\n\n论文提出 **ENVIRONMENT TUNING** 范式，把“调环境”作为核心杠杆，仅用 400 条实例即可从零训练出强泛化多轮工具调用智能体。解决方案围绕三大机制展开：\n\n1. 结构化课程（Structured Curriculum）\n\n- 四阶段渐进式目标：\n① **语法正确性**：先让模型“会说”，奖励格式与工具调用合法性。\n② **基础推理**：在 Base 数据上引入进度奖励与可执行反馈，学习核心多轮模式。\n③ **复杂场景**：加入 Missing Func/Param、长上下文等困难样本，提升鲁棒性。\n④ **对齐评估**：关闭环境增强，迫使模型依靠内部推理，避免对提示的过度依赖。\n- 阶段切换规则：验证集性能**收敛**且梯度范数**稳定**才进入下一阶段，防止梯度爆炸。\n2. 可执行环境增强（Actionable Environment Augmentation）\n\n- 将原本模糊的错误码替换为**教学式提示**，同时完成两件事：\n- **揭示工具依赖**：如订机票失败时返回“Invalid airport code\ns\n: …，可先用 list\\_airport 查询”，让模型在交互中**自己发现**必须先查机场代码。\n- **暴露内部约束**：如文件系统返回“Paths are not allowed. Specify only file/directory name…”，直接纠正参数格式误解。\n- 结果：把死胡同轨迹转化为**富学习信号**，显著降低探索空间。\n3. 细粒度进度奖励（Fine-Grained Progress Reward）\n\n- 每轮结束后立即计算二元指标：\n r_t = r_t^(state) · r_t^(exec) \n其中  r_t^(state)  检查环境状态是否达标， r_t^(exec)  检查返回值是否正确。\n- 整条轨迹的奖励为平均成功率\n R_(P) = (1) / (T)∑_(t=1)^T r_t ，\n提供**密集、连续**信号，使部分正确轨迹也能被区分，缓解长链稀疏奖励问题。\n\n实现层面，作者以改进的 **GRPO** 做策略优化，加入解耦裁剪与较大 KL 惩罚（β=0.1）保证稳定性。三大机制协同，实现：\n\n- **冷启动友好**：Stage 1 仅用格式奖励即可把基础模型从 0 拉到 15.5%。\n- **样本高效**：400 样本让 Qwen2.5-7B 在 BFCL V3 多轮上从 7% → 36.9%，超越多款 8B 级 SFT 模型。\n- **强泛化**：OOD 场景（BFCL V4 Web Search、ACEBench）上，ENVIRONMENT TUNING 平均提升 10+ pp，而 SFT 基线普遍下降。\n\n综上，论文通过“课程-环境-奖励”三位一体设计，把**环境反馈本身变成可优化的训练资源**，从而在极端数据稀缺条件下完成稳定、高效、可泛化的多轮工具使用强化学习。\n\nQ4: 论文做了哪些实验？\n\n论文围绕 **BFCL V3 多轮 benchmark** 展开，系统验证了 ENVIRONMENT TUNING 在「极端数据稀缺」条件下的有效性、稳定性与泛化能力。实验分为四大板块：\n\n1. 主实验：分布内（ID）性能\n\n- 数据集：BFCL V3 多轮 800 例 → 400 例训练 / 400 例测试\n- 对比维度\n– 基础模型：Qwen2.5-7B-Instruct、Llama-3.1-8B-Instruct\n– 已有 SFT 模型：ToolACE-2-8B、watt-tool-8B\n– 开源 SFT 强基线：xLAM-2-8b-fc-r、Arch-Agent-7B、BitAgent-8B\n– 闭源标杆：Claude-Sonnet-4、GPT-4o、Gemini-2.5-Pro、o3\n- 结果（表 1）\n- 零基础提升：Qwen2.5-7B 从 7.0% → 36.9%，超越 BitAgent 与 Arch-Agent。\n- SFT 再提升：watt-tool-8B 从 35.7% → 54.3%，超过 GPT-4o 与 o3；ToolACE-2 再涨 9.2 pp。\n2. 分布外（OOD）泛化\n\n- 测试集\n– BFCL V4 Web-Search（多跳检索 + 网络故障）\n– BFCL V4 Memory（三种记忆后端：KV / Vector / Recursive-Sum）\n– ACEBench Agent（多步推理 + 长程状态）\n- 结果（表 2）\n- SFT 基线普遍崩溃：xLAM-2 在 Web-Search 从 70.5%→5.0%；ENVIRONMENT TUNING 同任务 1.0%→15.0%。\n- ToolACE-2 经环境调优后，ACEBench 平均从 8.3%→15.0%，翻倍提升。\n3. 消融实验（Ablation）\n\n- 环境增强（图 4a）\n- Missing Parameters / Functions 上移除增强 → 性能掉 20+ pp，曲线波动大。\n- 进度奖励（图 4b）\n- 改用稀疏二元奖励 → Stage 3 直接崩溃，Missing Parameters 接近 0%。\n- 课程 vs 单阶段 RL（表 3）\n- 直接 GRPO：17.4% → 课程四阶段后 36.9%，提升 19.5 pp。\n4. 训练动态与稳定性分析\n\n- 梯度范数监控（图 5-6）\n- 单阶段 RL 70 步后梯度爆炸，性能回落；课程训练梯度平稳。\n- KL 惩罚系数（图 7）\n- β=0.1 维持策略熵，避免过早塌陷；β=0.001 或无 KL 均出现熵崩与精度下跌。\n5. 案例可视化（附录 F）\n\n- 文件系统、旅行 API、车辆控制+Twitter 混合任务三条轨迹对比\n- 展示「模糊错误」导致任务放弃 vs 「可执行提示」引导自我修正的完整过程。\n\n综上，实验覆盖 **ID 性能、OOD 泛化、消融、训练曲线、稳定性、案例」六维**，共同证明：在仅 400 样本的极端设定下，ENVIRONMENT TUNING 可同时实现「高样本效率、训练稳定、强泛化」。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可被视为 ENVIRONMENT TUNING 的“直接外延”或“深层追问”，均围绕「把环境变成可学习对象」这一核心范式展开：\n\n1\\. 自动化课程生成\n\n- **问题**：当前四阶段课程依赖人工划分技能与数据 split。\n- **探索**：\n- 用 LLM-based Judge 或技能图谱自动发现「下一步最难且可学」的子目标；\n- 将课程形式化为 POMDP 的选项框架，用元 RL 自动优化 stage boundary 与数据配比。\n\n2\\. 可执行环境增强的自动生成\n\n- **问题**：教学式提示现由人工撰写规则。\n- **探索**：\n- 构建「错误-诊断-提示」三元组合成数据，训练专用 reward model 打分，再反向过滤高质量提示；\n- 采用 self-improve 循环：agent 失败 → 日志送入 LLM → 生成更精确提示 → 注入环境，实现「环境自写提示」。\n\n3\\. 多模态与具身场景\n\n- **问题**：本文仅文本 API。\n- **探索**：\n- 把环境增强推广到视觉输入（截图、GUI、机器人传感器），例如返回「红色边框标出可点击按钮」的图文混合提示；\n- 研究跨模态进度奖励：同时检查屏幕状态变化与文本输出。\n\n4\\. 奖励塑形与信用分配的进一步细化\n\n- **问题**：进度奖励仍是 turn-级二元信号。\n- **探索**：\n- 引入子目标自动发现（sub-goal discovery），用 Successor Feature 或 FeUdal 结构把奖励打到 action-级；\n- 利用反事实基线（counterfactual baseline）估计「少调一个参数」对最终成败的边际贡献，实现更细粒度信用分配。\n\n5\\. 环境可微参数化：把“反馈规则”直接梯度优化\n\n- **问题**：环境提示目前是离散文本，无法端到端优化。\n- **探索**：\n- 将提示模板参数化（soft prompt），把 KL 散度或泛化误差作为外层目标，用 ES/BO 自动搜索最佳提示；\n- 研究「双层 RL」：内层优化 agent 策略，外层优化环境提示分布，形成共生训练。\n\n6\\. 安全与可验证性\n\n- **问题**：教学式提示可能泄露敏感约束或被恶意利用。\n- **探索**：\n- 引入「约束一致性检查器」，确保环境提示不会暴露 PII 或绕过安全策略；\n- 利用形式化验证（如 SMT）对增强后的环境反馈进行可达性分析，避免给出可导致非法状态的提示。\n\n7\\. 跨任务与跨语言通用化\n\n- **问题**：BFCL 以英文 API 为主。\n- **探索**：\n- 构建多语言工具集，研究「环境提示」是否需要语言特定文化或仅需符号约束；\n- 通过 meta-learning 让课程与提示生成器在 unseen API 家族上快速适应，检验「工具无关」通用性。\n\n8\\. 样本复杂度极限与理论刻画\n\n- **问题**：400 样本已显高效，但极限未知。\n- **探索**：\n- 建立「环境增强 + 课程」的样本复杂度上界，对比标准 RL 的 Ω(1/ε²) 下界；\n- 分析「提示质量」与「探索效率」之间的量化权衡，给出最优提示长度/信息量的理论描述。\n\n9\\. 人与环境协同教学\n\n- **问题**：当前环境是自包含反馈系统。\n- **探索**：\n- 引入「人在环」微调：当 agent 连续失败 K 次后，向人类教师发送可视化诊断报告，教师仅需回答 yes/no，系统再把人类信号转化为新的环境提示；\n- 研究最小人类介入量（minimal human teaching budget）与最终性能之间的 scaling law。\n\n10\\. 开源与标准化基准\n\n- **问题**：缺乏公开可用的“可编程环境增强”接口。\n- **探索**：\n- 将 ENVIRONMENT TUNING 的增强层抽象为通用插件（gymnasium 中间层），社区可像装 mod 一样为任意 API/Web/GUI 环境写入教学提示；\n- 举办“环境提示锦标赛”，鼓励研究者提交自动提示生成器，以 agent 的 OOD 得分作为排名标准，推动领域标准化。\n\n简言之，未来工作可从「自动、可微、多模、安全、理论、人机协同」六个维度继续把“调环境”范式推向更通用、更可信、更高效的下一阶段。\n\nQ6: 总结一下论文的主要内容\n\n论文提出 **ENVIRONMENT TUNING**——一种“调环境而非仅调模型”的新范式，解决极端数据稀缺场景下多轮工具使用智能体的训练难题。核心内容与贡献如下：\n\n1\\. 问题背景\n\n- 多轮工具调用任务面临 **数据极少**（BFCL V3 仅 800 例）、**环境复杂**（84 个跨领域 API）、**长链易错**（单点失败即全局失败）三大挑战。\n- 传统 SFT 易过拟合，标准 RL 冷启动严重、训练不稳定。\n\n2\\. 解决思路\n\n不再单纯模仿静态轨迹，而是把 **环境本身变成可优化的训练资源**，通过三大机制协同：\n\n| 机制 | 作用 |\n| --- | --- |\n| 结构化课程 | 四阶段渐进：语法正确 → 基础推理 → 复杂场景 → 对齐评估，阶段切换由验证性能与梯度稳定性自动触发。 |\n| 可执行环境增强 | 失败时返回教学式提示（如“Invalid airport code[s]… 可先用 list_airport 查询”），让模型在交互中自行发现工具依赖与参数约束。 |\n| 细粒度进度奖励 | 每轮二元评估（状态正确 ∧ 执行正确），整条轨迹取平均，提供密集、连续信号，缓解稀疏奖励。 |\n\n3\\. 实验结果\n\n- **样本高效**：400 训练例把 Qwen2.5-7B 从 7.0% 提升到 36.9%，超越多款 8B-SFT 模型；watt-tool-8B 再涨 18.5 pp，达 54.3%，超过 GPT-4o。\n- **强泛化**：OOD 场景（BFCL V4 Web Search、ACEBench）上，环境调优模型平均提升 10+ pp，而 SFT 基线普遍下降（xLAM 70.5%→5.0%）。\n- **消融验证**：移除环境增强或进度奖励，复杂 split 性能掉 20+ pp；无课程直接 RL 训练 70 步即梯度爆炸。\n\n4\\. 结论与意义\n\n- 首次证明在 **零专家轨迹、极少量数据** 条件下，仅靠“调环境”即可稳定训练出泛化能力强的多轮工具调用智能体。\n- 为数据稀缺场景下的 agent 训练提供了 **可复用的“课程-环境-奖励”三位一体框架**，推动从“模仿轨迹”到“环境驱动探索”的范式转变。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Don%27t%20Just%20Fine-tune%20the%20Agent%2C%20Tune%20the%20Environment，链接是https://arxiv.org/pdf/2510.10197，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.10197。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}},{"id":"hf-17-1760485638285","title":"GIR-Bench: Versatile Benchmark for Generating Images with Reasoning","description":"Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.","url":"https://huggingface.co/papers/2510.11026","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.285Z","metadata":{"authors":["Hongxiang Li","Yaowei Li","Bin Lin","Yuwei Niu","Yuhang Yang","Xiaoshuang Huang","Jiayin Cai","Xiaolong Jiang","Yao Hu","Long Chen"],"date":"","rank":18,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.11026.pdf","arxivUrl":"https://arxiv.org/abs/2510.11026","arxivId":"2510.11026","coolPaperUrl":"https://papers.cool/arxiv/2510.11026","published":"2025-10-13T05:50:44Z","updated":"2025-10-13T05:50:44.000Z","zh_summary":"统一多模态模型将大型语言模型的推理能力与图像理解和生成结合在一起，在高级多模态智能方面展现出巨大潜力。然而，学术界仍然缺乏一个严格以推理为中心的基准，以系统性地评估理解与生成之间的一致性，以及它们在复杂视觉任务中的泛化能力。为此，我们推出了\\textbf{GIR-Bench}，一个从三种互补角度评估统一模型的综合基准。首先，我们研究理解-生成一致性（GIR-Bench-UGC），探索模型是否能够在理解和生成任务中始终利用相同的知识。其次，我们研究模型是否能够进行以推理为中心的文本到图像生成，这需要应用逻辑约束和隐含知识来生成忠实的视觉内容（GIR-Bench-T2I）。第三，我们评估模型是否能够处理编辑中的多步推理（GIR-Bench-Edit）。对于每个子集，我们仔细设计了不同的任务特定评估流程，以适应各自的任务。这样可以实现精细化和可解释的评估，同时减少普遍使用的“MLLM作为评判者”范式带来的偏差。对多种统一模型和仅生成系统的广泛消融实验表明：虽然统一模型在以推理为驱动的视觉任务上更具能力，但它们在理解与生成之间仍存在持续的差距。GIR-Bench 的数据和代码可在此获取：\\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\nGIR-Bench 针对的核心问题是：\n现有统一多模态模型（同时支持图像理解与生成）在“推理驱动”的视觉任务中，其**理解能力与生成能力之间存在系统性错位**，且社区缺乏能够**细粒度、可解释地度量这一错位**的基准。\n\n具体而言，论文试图解决以下三个关键缺陷：\n\n1. 维度缺失\n已有基准仅关注属性、组合等浅层对齐，无法量化**同一模型在“理解侧”与“生成侧”对同一条知识或同一推理链的一致性**。\n\n2. 协议偏差\n主流评估依赖“MLLM-as-a-Judge”，把大模型既当选手又当裁判，导致分数与模型自身偏见耦合，缺乏**任务专属、可解释、可复现的指标**。\n\n3. 任务单一\n现有推理类基准多聚焦文本到图像生成，未同时覆盖**理解-生成一致性**、**推理式文本到图像生成**、**多步推理编辑**三大场景，难以全面暴露统一模型的能力边界。\n\nGIR-Bench 通过构建三大子集（UGC / T2I / Edit）及配套任务专属评测管线，首次系统度量并揭示了统一模型在“会认却不会画”“会算却画不对”“会推却改不准”等深层错位现象。\n\nQ2: 有哪些相关研究？\n\n与 GIR-Bench 直接相关的研究可归纳为两条主线：\n\n- **统一多模态模型架构与训练**\n- **面向统一模型的评测基准**\n\n以下按时间顺序列出代表性文献，并指出其与 GIR-Bench 的关联。\n\n统一多模态模型（理解+生成）\n\n| 模型 / 工作 | 核心贡献 | 与 GIR-Bench 的关联 |\n| --- | --- | --- |\n| Chameleon (Team, 2024) | 早期融合、统一词汇表，端到端文本-图像交错生成 | 首次验证“统一架构”可行，但未系统评估推理-生成一致性 |\n| Emu3 (Wang et al., 2024) | 纯自回归 next-token 预测，无需扩散即可生成图像 | 证明单一建模范式即可做多模态生成，GIR-Bench 将其纳入评测池 |\n| MetaQueries (Pan et al., 2025) | 冻结 MLLM + 可学习 Query，驱动扩散解码器 | 提供“知识增强生成”思路，GIR-Bench-UGC 直接测试知识-生成对齐 |\n| BLIP3-o (Chen et al., 2025a) | 扩散 Transformer 生成 CLIP 图像特征，再序列化预训练 | 统一预训练策略代表，GIR-Bench 评估其推理式生成差距 |\n| BAGEL (Deng et al., 2025 |"}},{"id":"hf-18-1760485638286","title":"AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes","description":"Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.","url":"https://huggingface.co/papers/2510.10670","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.286Z","metadata":{"authors":["Yu Li","Menghan Xia","Gongye Liu","Jianhong Bai","Xintao Wang","Conglang Zhang","Yuxuan Lin","Ruihang Chu","Pengfei Wan","Yujiu Yang"],"date":"","rank":19,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.10670.pdf","arxivUrl":"https://arxiv.org/abs/2510.10670","arxivId":"2510.10670","coolPaperUrl":"https://papers.cool/arxiv/2510.10670","published":"2025-10-12T15:55:44Z","updated":"2025-10-12T15:55:44.000Z","zh_summary":"最近的文本到视频 （T2V） 模型在现实世界几何形状和物理定律的视觉模拟方面表现出强大的能力，表明其作为隐式世界模型的潜力。受此启发，我们探索了利用视频生成预先从给定 4D 场景进行视点规划的可行性，因为视频在内部伴随着具有自然视点的动态场景。为此，我们提出了一种两阶段范式，以兼容的方式调整预训练的T2V模型以进行观点预测。首先，我们通过自适应学习分支将 4D 场景表示注入到预训练的 T2V 模型中，其中 4D 场景与视点无关，条件生成的视频直观地嵌入了视点。然后，我们将视点提取表述为混合条件引导相机外在去噪过程。具体来说，通过将生成的视频和 4D 场景作为输入，在预训练的 T2V 模型中进一步引入相机外在扩散分支。实验结果表明，我们所提出的方法优于现有竞争对手，消融研究验证了我们关键技术设计的有效性。这项工作在某种程度上证明了视频生成模型在现实世界中实现4D交互的潜力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在解决“如何为给定的 4D 场景自动生成专业且文本可控制的相机轨迹”这一核心问题。具体而言：\n\n- 传统手动设计虚拟相机路径耗时且依赖专家经验；\n- 现有数据驱动方法通常只能在受限场景或特定任务上工作，难以泛化到开放世界，且缺乏对文本指令的灵活响应；\n- 近期文本到视频（T2V）大模型展现出对真实几何与物理规律的隐式建模能力，内部包含了丰富的摄影技巧与动态视点先验。\n\n因此，作者提出将预训练 T2V 模型“改造”为虚拟摄影师，使其仅依据 4D 内容（如 SMPL-X 人体序列）与文本提示，即可输出坐标对齐的相机位姿序列及对应的预览视频，实现自动、文本可控、可泛化的视点规划。\n\nQ2: 有哪些相关研究？\n\n论文将相关研究归为两大主线，并指出自身与它们的区别。可梳理如下：\n\n1. 计算摄影与自动相机规划\n\n- 早期数据驱动：模仿专业镜头示例（Jiang et al. 2020, 2021）或无人机拍摄示范（Huang et al. 2019）。\n- 深度生成式：基于 GAN 或扩散模型，在交互环境（Yu et al. 2023）、音乐-舞蹈（Wang et al. 2024c,d）或纯文本（Courant et al. 2024; Li et al. 2024）条件下合成 3D 相机轨迹。\n- 共同点：依赖专门网络、小规模数据，难以泛化到开放世界，且多数需显式给定角色轨迹或音频，不能直接接受 4D 场景+文本。\n2. 可控人类视频生成（Human Motion Control for VDMs）\n\n- 2D 姿态驱动 → 3D 运动条件：用 SMPL/SMPL-X 解决深度歧义与自遮挡（Zhu et al. 2024; Ding et al. 2025; Shao et al. 2024, 2025）。\n- 世界坐标统一：Uni3C、RealisMotion 等把角色与相机都放在同一世界坐标系，实现联合控制。\n- 局限：要么输入 2D 姿态序列且视角固定，要么必须同时给定 4D 姿态+相机参数；尚无工作仅输入“规范化 4D 姿态”就让模型**自己规划**视点。\n\nAdaViewPlanner 与上述研究的根本差异：\n\n- 不训练专用相机网络，而是**复用预训练 T2V 大模型的影视先验**；\n- 仅提供**规范化 4D 人体运动**与文本，模型即输出**坐标对齐的相机轨迹**；\n- 通过两阶段兼容式微调，把 4D 场景注入视频分支、再扩散出相机外参，实现开放世界泛化与文本指令跟随。\n\nQ3: 论文如何解决这个问题？"}},{"id":"hf-19-1760485638286","title":"Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning","description":"While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.","url":"https://huggingface.co/papers/2510.11027","source":"HuggingFace Papers","timestamp":"2025-10-14T23:47:18.286Z","metadata":{"authors":["Ganlin Yang","Tianyi Zhang","Haoran Hao","Weiyun Wang","Yibin Liu","Dehui Wang","Guanzhou Chen","Zijian Cai","Junting Chen","Weijie Su","Wengang Zhou","Yu Qiao","Jifeng Dai","Jiangmiao Pang","Gen Luo","Wenhai Wang","Yao Mu","Zhi Hou"],"date":"","rank":20,"llmAnalysis":"","categories":["cs.CV"],"pdfUrl":"https://arxiv.org/pdf/2510.11027.pdf","arxivUrl":"https://arxiv.org/abs/2510.11027","arxivId":"2510.11027","coolPaperUrl":"https://papers.cool/arxiv/2510.11027","published":"2025-10-13T05:51:22Z","updated":"2025-10-13T05:51:22.000Z","zh_summary":"虽然大量研究集中于使用视觉-语言模型（VLMs）开发具身推理能力，或将先进的VLMs集成到视觉-语言-动作（VLA）模型中以实现端到端的机器人控制，但很少有研究直接解决上游基于VLM的推理与下游VLA策略学习之间的关键差距。在本研究中，我们迈出了将具身推理与VLA策略学习相结合的初步步骤，提出了Vlaser——一种具有协同具身推理能力的视觉-语言-动作模型，它是一种基础的视觉-语言模型，旨在将高层次推理与低层次控制整合到具身代理中。基于高质量的Vlaser-6M数据集，Vlaser在一系列具身推理基准中取得了最先进的性能，包括空间推理、具身落地、具身问答以及任务规划。此外，我们系统地研究了不同VLM初始化方式如何影响监督VLA微调，为缓解互联网规模预训练数据与具身特定策略学习数据之间的领域迁移提供了新见解。基于这些见解，我们的方法在WidowX基准上取得了最先进的结果，并在Google Robot基准上表现出竞争力。","llm_analysis":"Q1: 这篇论文试图解决什么问题？\n\n论文旨在弥合“上游基于 VLM 的具身推理”与“下游 VLA 策略学习”之间的关键断层。具体而言，现有研究要么仅用 VLM 增强高层推理，要么直接将 VLM 权重初始化 VLA 做端到端控制，但**何种多模态数据/能力对 VLA 的底层控制真正有效**仍不清楚。为此，作者提出 Vlaser——一个具备**协同具身推理能力**的 Vision-Language-Action 基础模型，并系统回答以下核心问题：\n\n- 如何构建一个高质量、多任务、百万级规模的具身推理数据集（Vlaser-6M），使 VLM 在 grounding、spatial reasoning、planning、embodied QA 等 benchmark 上达到 SOTA；\n- 何种预训练数据流（互联网图文 vs. 机器人内域 QA）能**显著加速 VLA 微调收敛并提升闭环控制成功率**；\n- 如何设计轻量级但高效的 VLA 架构（共享注意力流+flow-matching Action Expert），在 WidowX/Google Robot 等真实可迁移仿真任务上取得 SOTA 或媲美 SOTA 的结果。\n\n综上，论文首次**量化分析**了“高层推理能力”与“底层控制性能”之间的迁移关系，指出**缩小观测域差距（机器人视角数据）比单纯增加通用推理数据更重要**，为后续具身基础模型建设提供了数据选择与架构设计的实证指南。\n\nQ2: 有哪些相关研究？\n\n相关研究可归纳为两条主线：\n\n1. 增强 VLM 的具身推理能力；\n2. 将 VLM 扩展为 VLA 以支持端到端机器人控制。\n\n- **Embodied-VLM 方向**\n- 数据引擎：Cosmos-Reason1、VeBrain、MolmoAct、EmbodiedOneVision、RoboBrain 系列、Embodied-R1、Robix 等，通过构建 grounding/spatial/planning 专用数据集或引入 RFT/CoT 提升推理。\n- Benchmark 与能力：Gemini Robotics-ER、SPAR、SpaceR-151k、VSI-Bench、RefSpatial、MMSI-Bench、VLABench、EmbodiedBench 等，聚焦空间关系、affordance、长程规划评测。\n- **VLA 方向**\n- 早期工作：RT-1、RT-2、BC-Z 证明大规模机器人数据 + Transformer 可提升泛化。\n- 近期 VLA：OpenVLA、π₀、SpatialVLA、GR00T N1、Octo、TraceVLA、Emma-X、Magma、DITA 等，将扩散/流匹配、MoE、大容量 VLM 骨干引入策略学习，提升多任务、多本体泛化。\n- 联合训练框架：Intelligence et al. 2025、Driess et al. 2025 尝试 web+robot 数据共训练，但未系统分析数据流贡献。\n\nVlaser 与上述工作的区别：首次**量化对比**不同预训练数据流（通用 embodied reasoning vs. 机器人内域 QA）对下游 VLA 收敛速度与闭环成功率的影响，并给出“域间距”而非“推理能力”才是迁移瓶颈的实证结论。\n\nQ3: 论文如何解决这个问题？\n\n论文通过“数据-模型-实验”三位一体策略系统解决“上游 VLM 推理”与“下游 VLA 控制”之间的断层问题。\n\n1. 构建高质量多任务数据引擎\n\n- 整合 25+ 公开数据集并自研标注管线，产出 600 万规模、五类任务均衡的 Vlaser-6M：\n– Embodied Grounding 1.8 M（框/点混合，含 SA-1B 合成 300 k）\n– Embodied QA 1.2 M（RoboVQA、OWMM-Agent 等）\n– Spatial Reasoning 0.5 M（SPAR、SpaceR-151k 外加 100 k 3D 场景人工标注）\n– Planning 0.4 M（Alpaca-15k、MuEP、WAP、EgoPlan-IT 等）\n– In-Domain Simulation 2.0 M（SimplerEnv 中 WidowX/Google Robot 视角的 QA、Grounding、Spatial 样本）\n- 所有坐标归一化到 \n0,1000\n，保证跨分辨率一致；3D 场景构建时空图自动生成问答，提升空间一致性。\n2. 设计协同推理-控制统一架构\n\n- 骨干：InternVL3（2B/8B）提供视觉-语言语义。\n- Action Expert：轻量级 MLP 投影 + 流匹配头，与 LLM 共享自注意力层，实现单帧→4 步动作块预测；训练与推理均 10 步 ODE 积分，保证实时性。\n- 两阶段训练流程：\n① VLM 预训练：仅用自回归损失  L_(lm)  在 Vlaser-6M 上微调，冻结视觉/投影/LLM 全量参数。\n② VLA 微调：固定 VLM 权重，仅优化 Action Expert（也可联合优化，实验发现联合收敛更快），采用流匹配损失  L_(vla)=|v_θ(A_t^τ,o_t)−(ε−A_t)|^2 。\n3. 系统实验验证“数据有效性”与“域差距”假说\n\n- 12 个具身推理 benchmark 上 Vlaser-2B/8B 平均得分从 15.2→45.3 与 22.3→51.3，超越 GPT-4o、Gemini-2.5-Pro 及 RoboBrain2.0 等同类模型。\n- 在 SimplerEnv 的 WidowX 与 Google Robot 任务上，仅用 InternVL3-2B 初始化 VLA 平均成功率 41.8%；换成“Vlaser-QA”（同一架构，仅额外加入 200 k 机器人视角 QA）即提升至 64.6% 与 58.3%，显著优于 π₀、OpenVLA、SpatialVLA 等。\n- 消融显示：\n– 通用 grounding/spatial 数据提升上游 benchmark，但对闭环控制几乎无增益；\n– 机器人内域 QA（相同视角、相同任务描述分布）哪怕数据量小一个数量级，也能显著加速收敛并降低失败率。\n- 结论：真正影响 VLA 性能的不是“推理能力有多强”，而是“预训练观测-动作分布与目标机器人环境是否一致”。因此，缩小“互联网视角→机器人视角”的域差距比继续堆叠通用推理数据更紧迫。\n\nQ4: 论文做了哪些实验？\n\n论文围绕“上游 VLM 具身推理”与“下游 VLA 控制”两个维度，共执行三类实验，覆盖 12 项开放基准与 2 类机器人仿真平台。\n\n1. 具身推理能力评测（开环）\n基准：ERQA、Ego-Plan2、Where2place、PointArena、Paco-Lavis、Pixmo-Points、VSI-Bench、RefSpatial、MMSI-Bench、VLABench、EB-ALFRED、EB-Habitat（共 12 个）。\n对比对象：\n\n- 闭源 GPT-4o-20241120、Claude-3.7-Sonnet、Gemini-2.5-Pro\n- 2B-3B 档：ChatVLA-2B、InternVL3-2B、Qwen2.5VL-3B、Embodied-R1-3B、RoboBrain2.0-3B\n- 7B-8B 档：Magma-8B、Cosmos-Reason1-7B、VeBrain-7B、InternVL3-8B、Qwen2.5VL-7B、Embodied-R1-7B、RoboBrain2.0-7B\n结果：Vlaser-2B 平均 45.3，Vlaser-8B 平均 51.3，均位列同类规模第一，较最佳竞品再提升约 10%。\n2. 下游 VLA 微调与闭环控制评测\n平台：SimplerEnv（Real-to-Sim 高相关性）\n\n- WidowX 任务：Carrot-on-plate、Put-eggplant-in-basket、Spoon-on-towel、Stack-Cube\n- Google Robot 任务：Pick-Coke-can、Move-near、Open/Close-drawer（含 Visual-Matching 与 Variance-Aggregation 两种难度协议）\n实验变量：\n- 基线骨干：InternVL3-2B\n- 不同数据流初始化：Vlaser（全 6M）、Vlaser-QA（仅 2M 机器人 QA）、Vlaser-Spatial（仅 0.5M 空间）、Vlaser-Grounding（仅 1.8M grounding）\n指标：任务成功率（%）\n结果：\n- WidowX 平均：InternVL3-2B 41.8 → Vlaser 43.2 → Vlaser-QA 64.6（+22.8 pp）\n- Google Robot 平均：InternVL3-2B 54.7 → Vlaser 51.3 → Vlaser-QA 56.4 → Vlaser-Grounding 58.3（最高）\n说明：加入机器人内域 QA/grounding 数据显著缩短收敛步数并降低末端定位误差，而仅使用通用推理数据无一致提升。\n3. 数据有效性消融与域差距量化\n\n- 训练曲线：在同等 1024 batch、10 epoch 下，Vlaser-QA 的 VLA 损失下降快 1.6×，最终动作 MSE 低 22%。\n- 视点分布可视化：互联网图片平均俯仰角 −15°，机器人摄像头 +30°；用 FID 与 KID 计算图像特征分布差距，机器人内域数据可将分布距离缩小 38%。\n- 失败案例统计：InternVL3-2B 基线 63% 失败由“对象定位偏差 >1 cm”导致，Vlaser-QA 将该比例降至 27%，验证域对齐对精细控制的重要性。\n\n综上，实验既展示了 Vlaser 在 12 项开放基准的 SOTA 推理能力，也通过严格对照证明“机器人视角内域数据”才是提升 VLA 闭环成功率的关键，而非单纯增强通用推理。\n\nQ5: 有什么可以进一步探索的点？\n\n以下方向可延续 Vlaser 的“数据-架构-评测”框架，进一步缩小“推理-控制”断层并迈向通用具身智能。\n\n1. 数据层面\n\n- 多本体同步采集：在真实机器人、AR 眼镜、手持相机同步记录同一任务，构建“人-机器人”共享视角的百万级视频-QA，验证视角-动作一致性对跨本体迁移的增益。\n- 物理-语义耦合标注：引入触觉、力觉、音频与 6D 姿态，构建“多模态事件图”，研究触觉/力觉 token 是否降低接触型任务（插孔、叠衣）的失败率。\n- 动态环境数据：加入运动物体、光照变化、遮挡序列，测试模型对非稳态环境的鲁棒性，并量化动态数据与静态数据的最佳混合比例。\n2. 架构层面\n\n- 层级 Action Expert：高层输出 1 Hz 子目标 token，低层输出 200 Hz 扭矩向量，实现“推理-重规划-控制”三级递进，用强化学习微调低层专家，保持高层语义对齐。\n- 记忆增强 VLA：引入跨 episode 的向量记忆库（Episodic Memory Transformer），支持长时任务（如 50 步烘焙）中的子目标回溯与错误恢复。\n- 流匹配 → 扩散混合：在动作空间使用混合扩散-流匹配调度，前期快速扩散探索，后期精确流匹配收敛，评估对多峰分布（双臂协同）是否更优。\n3. 训练策略\n\n- 在线数据飞轮：部署 Vlaser 到真实厨房场景，用失败案例触发自动重标注（self-instruct + 4D 重建），实时扩充 In-Domain 数据，实现“越用越像机器人视角”的正循环。\n- 课程式域适应：从 SimplerEnv→NeRF 场景→真实场景逐步增加视觉-动力学差异，用 Domain Randomization + AdaBN 测试能否保持零样本性能。\n- 多任务平衡系数搜索：将 QA、grounding、spatial、planning、control 设为可学习权重，用元学习在验证集上自动搜索最优数据配比，避免人工启发式划分。\n4. 评测与基准\n\n- 真实-闭环排行榜：搭建统一硬件（如 WidowX+Intel RealSense）与协议（100 台×100 任务），提供 Docker 镜像，推动社区在完全一致的真实环境提交模型，解决 Sim→Real 评分不一致问题。\n- 可解释性评测：引入“动作-注意力一致性”指标（A2A score），测量视觉注意力热区与真实抓取接触点的 IoU，量化模型是否依赖正确区域做决策。\n- 安全与故障恢复基准：定义 20 种常见失败（碰撞、滑落、超限），记录模型从检测故障到自主恢复的平均时间与成功率，推动安全可部署的 VLA 研究。\n5. 理论与 scaling\n\n- 推理-控制互信息下界：推导“高层规划 token 与低层动作序列的互信息”与任务成功率的下界关系，给出满足该下界的最小 VLM 容量，指导模型缩放。\n- 机器人视角-互联网视角的域间距度量：提出基于最优传输的 Robot-FID 指标，与最终控制误差建立线性模型，用于预测任意新数据集对 VLA 的潜在增益，减少盲目采集成本。\n\nQ6: 总结一下论文的主要内容\n\n- 问题：现有 VLM 擅长大规模图文推理，VLA 擅长大规模机器人控制，但“上游 VLM 预训练”与“下游 VLA 微调”之间缺乏系统性的数据-能力映射，导致推理强≠控制好。\n- 方法：提出 Vlaser——协同具身推理的 Vision-Language-Action 基础模型，配套构建 600 万规模、五类任务均衡的 Vlaser-6M 数据集（QA、Grounding、Spatial、Planning、In-Domain Simulation）。\n- 架构：InternVL3 骨干 + 共享注意力流匹配 Action Expert，两阶段训练（VLM 预训练 → VLA 微调）。\n- 实验：\n– 12 项具身推理基准：Vlaser-2B/8B 平均得分 45.3/51.3，超越 GPT-4o、Gemini-2.5-Pro 及同类 Embodied-VLM 约 10%。\n– SimplerEnv 闭环控制：WidowX 任务平均成功率由 41.8% 提升至 64.6%，Google Robot 达 58.3%，优于 π₀、OpenVLA、SpatialVLA 等。\n- 发现：通用推理数据提升 benchmark 但几乎不提升控制；机器人内域 QA/grounding 数据显著加速收敛与成功率，说明“观测域差距”而非“推理能力”是迁移瓶颈。\n- 贡献：开源模型+数据+训练评测代码，提供“数据选择-架构设计-真实验证”完整范式，为后续具身基础模型缩小推理-控制断层奠定实证基准。\n\nQ7: 想要进一步了解论文\n\n以上只是了解一篇论文的几个基本FAQ。如果你还想与Kimi进一步讨论该论文，请点击 [**这里**](http://kimi.com/_prefill_chat?prefill_prompt=我们要讨论的论文是Vlaser%3A%20Vision-Language-Action%20Model%20with%20Synergistic%20Embodied%20Reasoning，链接是https://arxiv.org/pdf/2510.11027，已有的FAQ链接是https://papers.cool/arxiv/kimi?paper=2510.11027。请以此为基础，继续回答我后面的问题。&system_prompt=你是一个学术助手，后面的对话将围绕着以下论文内容进行，已经通过链接给出了论文的PDF和论文已有的FAQ。用户将继续向你咨询论文的相关问题，请你作出专业的回答，不要出现第一人称，当涉及到分点回答时，鼓励你以markdown格式输出。&send_immediately=true&force_search=false&enable_reasoning=false) 为你跳转Kimi AI网页版，并启动一个与该论文相关的新会话。"}}]}