<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文精读 ViT | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="3.ViT论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.11929 源码链接：https:&#x2F;&#x2F;github.com&#x2F;rwightman&#x2F;py 1.标题 + 简介An image is worth 16*16 words 每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches —&gt; an image is worth 16 * 16 words">
<meta property="og:type" content="article">
<meta property="og:title" content="论文精读 ViT">
<meta property="og:url" content="https://wdndev.github.io/paper_reading/2.3.ViT/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="3.ViT论文链接：https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2010.11929 源码链接：https:&#x2F;&#x2F;github.com&#x2F;rwightman&#x2F;py 1.标题 + 简介An image is worth 16*16 words 每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches —&gt; an image is worth 16 * 16 words">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2023-06-14T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:10.700Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="PaperReading">
<meta property="article:tag" content="CV">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/paper_reading/2.3.ViT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文精读 ViT',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">论文精读 ViT</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-06-14T16:00:00.000Z" title="Created 2023-06-15 00:00:00">2023-06-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:10.700Z" title="Updated 2025-11-02 07:46:10">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>18min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文精读 ViT"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="3-ViT"><a href="#3-ViT" class="headerlink" title="3.ViT"></a>3.ViT</h1><p>论文链接：<a href="https://link.zhihu.com/?target=https://arxiv.org/abs/2010.11929" title="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a></p>
<p>源码链接：<a href="https://link.zhihu.com/?target=https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py" title="https://github.com/rwightman/py">https://github.com/rwightman/py</a></p>
<h1 id="1-标题-简介"><a href="#1-标题-简介" class="headerlink" title="1.标题 + 简介"></a>1.标题 + 简介</h1><p><strong>An image is worth 16*16 words</strong></p>
<p>每一个方格都是 16 * 16 大小，图片有很多 16 * 16 方格 patches —&gt; an image is worth 16 * 16 words</p>
<p>ViT：过去一年，CV 最有影响力的工作</p>
<ul>
<li>推翻了 2012 Alexnet 提出的 CNN 在 CV 的统治地位</li>
<li>有足够多的预训练数据，NLP 的 Transformer 搬运到 CV，效果很好</li>
<li>打破 CV 和 NLP 的壁垒，给 CV、多模态 <strong>挖坑</strong></li>
</ul>
<p>ViT效果有多好，CV 任务刷榜，paperwithcode网站&#x20;</p>
<ol>
<li>霸榜 ImageNet （基于 ViT）</li>
<li>&#x20;COCO ,目标检测（Swin Transformer ICCV 21 best paper：多尺度的 ViT ）的模型</li>
</ol>
<p>下图中的的四种情况 ViT 都能处理</p>
<p>遮挡、数据分布的偏移（纹理的去除）、鸟头部+对抗的patch、图片打散重新排列组合</p>
<p><img src="image/image_mzEpAZYXpH.png" alt=""></p>
<h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a><strong>2.摘要</strong></h1><p>Transformer 在 NLP 是基本操作，但 transformer 在 CV 的应用有限。</p>
<p>CV 里的 attention 是怎么用的呢？attention + CNN, or attention 替换 CNN components 但依然保持 CNN 整体结构。</p>
<p>本文怎么看 CV 里的 attention? attention 不用和 CNN 绑在一起，和 transformer 结合，在 CV 领域 大杀四方。</p>
<h1 id="3-引言"><a href="#3-引言" class="headerlink" title="3.引言"></a><strong>3.引言</strong></h1><h2 id="3-1-self-attention"><a href="#3-1-self-attention" class="headerlink" title="3.1 self attention"></a>3.1 self attention</h2><p>self-attention 架构， esp Transformers，是 NLP 必选模型。主流方式是 BERT 提出的，<strong>大规模数据集预训练，在特定领域的小数据集做微调</strong>。 Transformer 的 计算高效和可扩展性，1000亿参数都还没有性能饱和的现象。</p>
<h2 id="3-2-Transformer-应用在-CV-的难点"><a href="#3-2-Transformer-应用在-CV-的难点" class="headerlink" title="3.2 Transformer 应用在 CV 的难点"></a>3.2 Transformer 应用在 CV 的<strong>难点</strong></h2><p>计算像素的 self-attention，序列长，维度爆炸</p>
<p>Trnasformer 的计算复杂度是 序列长度 n 的 平方 $O(n^2)$</p>
<p>224 分辨率的图片，有 50176 个像素点，（2d 图片 flatten）序列长度是 BERT 的近 100 倍。</p>
<p><strong>CV 如何用 attention 呢？</strong></p>
<h3 id="（1）CNN-结构-self-attention-or-attention-替代卷积"><a href="#（1）CNN-结构-self-attention-or-attention-替代卷积" class="headerlink" title="（1）CNN 结构 + self-attention or attention 替代卷积"></a>（1）CNN 结构 + self-attention or attention 替代卷积</h3><p>降低序列长度的方式：CVPR Wang et al. 2018, Local Network, 网络中的特征图 输入 Transformer</p>
<p>ResNet 50 最后一个 stage, res4 的 feature map 14 * 14， 196</p>
<h3 id="（2）stand-alone-attention-孤立自注意力"><a href="#（2）stand-alone-attention-孤立自注意力" class="headerlink" title="（2）stand-alone attention 孤立自注意力"></a>（2）stand-alone attention 孤立自注意力</h3><p>用 local window 局部小窗口 控制 transformer 的计算复杂度，有点像卷积，卷积也有 locality，局部窗口卷积。</p>
<h3 id="（3）axial-attention-轴注意力"><a href="#（3）axial-attention-轴注意力" class="headerlink" title="（3）axial attention 轴注意力"></a>（3）axial attention 轴注意力</h3><p>2 个 1d 顺序操作，降低计算复杂度，图片的序列长度 n = H * W。</p>
<p>2d 矩阵 拆分为 2个1d 向量，先在 H 高度 dimension 做一次 self-attention，再 W 宽度 dimension 做一次 self-attention</p>
<p><strong>replacing the convolutions entirely 好不好呢？</strong></p>
<p>理论高效，但硬件无法加速 —\&gt; 此类模型都还没有太大。</p>
<p>本段（第二段）总结：在大规模的图像识别上，ResNet 还是效果最好的。</p>
<h2 id="3-3-ViT"><a href="#3-3-ViT" class="headerlink" title="3.3 ViT"></a>3.3 ViT</h2><h3 id="（1）ViT做法"><a href="#（1）ViT做法" class="headerlink" title="（1）ViT做法"></a>（1）ViT做法</h3><p><strong>现状：</strong> attention 已经在 CV 领域有应用，甚至也有 attention 替代卷积的操作</p>
<p>标准 Transformer 直接应用于图片，做最少的修改，不做任何针对视觉任务的特定的改变。</p>
<p>把图片划分成很多 patches，每个 patch 元素 是 16 * 16，序列长度 14 * 14 = 196个元素</p>
<p>每一个 patch 经过一个 FC layer（fully connected layer）得到一个 linear embedding，patches 的 linear embeddings 是 Transformer 的输入。</p>
<p>一个 224 * 224 图片 变成一个 196 个的 16 * 16 图片块（words in NLP）。</p>
<p><img src="image/image_xD9SovxAY_.png" alt=""></p>
<h3 id="（2）为什么-transformer-的训练是-supervised-fashion？"><a href="#（2）为什么-transformer-的训练是-supervised-fashion？" class="headerlink" title="（2）为什么 transformer 的训练是 supervised fashion？"></a>（2）<strong>为什么 transformer 的训练是 supervised fashion？</strong></h3><p>NLP 的 Transformer 无监督训练 by language model LM or mask language model MLM；</p>
<p>CV 任务的 benchmark 使用有监督训练。</p>
<p><strong>ViT 把 CV 任务当成 NLP 任务，模型使用的是 BERT, Transformer encoder 简洁框架</strong>。</p>
<h3 id="（3）Transformer-in-CV，之前有人做吗？"><a href="#（3）Transformer-in-CV，之前有人做吗？" class="headerlink" title="（3）Transformer in CV，之前有人做吗？"></a>（3）<strong>Transformer in CV，之前有人做吗？</strong></h3><p>ICLR 2020 从输入图片里抽取 2 * 2 patches。 2 * 2 size enough：CIFAR-10 32 * 32 图片，16 * 16 会过大。 抽好 patch 之后，在 patches 上 做 self-attention。 —&gt; 技术上的 Vision Transformer</p>
<p><strong>ViT 和 ICLR 2 * 2 patches 的区别？</strong></p>
<ul>
<li>ViT证明了 大规模数据集预训练 （NLP 常用）之后的 Transformer，不需要做 针对视觉任务的 修改，比最好的 CNNs 效果差不多 or 甚至更好。</li>
<li>2 * 2 patches applicable only to small-resolution images, ViT handles medium-resolution images as well.</li>
<li>ViT 告诉大家，Transformer 在 vision 领域能拓展到有多好。large 数据集 + large 模型，transformer 能否取代 CNN 地位？</li>
</ul>
<h3 id="（4）ViT-任何情况比CNN都强吗？"><a href="#（4）ViT-任何情况比CNN都强吗？" class="headerlink" title="（4）ViT 任何情况比CNN都强吗？"></a>（4）ViT 任何情况比CNN都强吗？</h3><p>mid-sized datasets ImageNet without strong regularization，ViT 比 ResNet of comparable size 弱几个点。</p>
<p><strong>Transformer 比 CNN 少 inductive biases 归纳偏置</strong></p>
<p>CNN 有 locality 和 translation equivariance 归纳偏置，—&gt; CNN 有 很多先验信息 —&gt; 需要较少的数据去学好一个模型。</p>
<p>Transformer 没有这些先验信息，只能 从图片数据里，自己学习对 视觉世界 的感知。</p>
<h4 id="inductive-biases-归纳偏置：先验知识-or-提前的假设"><a href="#inductive-biases-归纳偏置：先验知识-or-提前的假设" class="headerlink" title="inductive biases 归纳偏置：先验知识 or 提前的假设"></a>inductive biases 归纳偏置：先验知识 or 提前的假设</h4><p>CNN 的 inductive biases 是 locality（相似特征） 和  translation equaivariance（平移不变性 ）。</p>
<ul>
<li>locality: CNN用滑动窗口在图片上做卷积。假设是图片相邻的区域有相似的特征。i.e., 桌椅在一起的概率大，距离近的物品 相关性越强。</li>
<li>translation equaivariance：$f (g(x)) = g( f(x) )$，f 和 g 函数的顺序不影响结果。</li>
<li>f：卷积 g：平移; 无论先做平移 g 还是先做卷积 f , 最后结果一样。</li>
</ul>
<h4 id="怎么验证-Transformer-无-inductive-bias-的假设？"><a href="#怎么验证-Transformer-无-inductive-bias-的假设？" class="headerlink" title="怎么验证 Transformer 无 inductive bias 的假设？"></a><strong>怎么验证 Transformer 无 inductive bias 的假设？</strong></h4><p>在 1400万(ImageNet-21K) - 3000 万(JFT-300)得到图片数据集上预训练 trumps inductive bias, ViT +足够训练数据，CV SOTA。</p>
<p>VTAB 融合了 19 个数据集，检测模型的稳健性，ViT的 robustness 也很好。</p>
<h2 id="3-4引言总结"><a href="#3-4引言总结" class="headerlink" title="3.4引言总结"></a>3.4引言总结</h2><p>第一段：Transformer 在 NLP 扩展的很好，没有因为大模型和大数据集而饱和，performance 一直有提升，Transformer 在 CV 里能不能也有大幅度的提升呢？</p>
<p>第二段：前人工作。这么好的 idea 有哪些人做过呢？<strong>要讲清楚自己的工作和 related works 的区别</strong>。之前的工作是 CNN + attention 或者 attention 替代 convolutions，没有工作将 transformer 用到 CV 领域，没有得到很好的扩展效果。</p>
<p>第三段：Vision Transformer 是 standard Transformer with the fewest possible modifications。图片变成 16 * 16 的像素块 patches，经过 一个 fc layer 得到的 linear embeddings 输入 transformer。<strong>ViT 融合了 CV 和 NLP 领域</strong>。</p>
<p>第四+五段：show 结果，足够多的数据集，ViT 能 SOTA</p>
<h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a><strong>4.结论</strong></h1><p>直接 用 NLP 的 Transformer 来处理图片，<strong>和其它 self-attention in CV 的工作不同</strong>：除了 将图片转成 16 * 16 patches + 位置编码 之外，没有额外引入 图像特有的 inductive bias</p>
<p><strong>没有图片的 inductive bias 的好处是什么？</strong></p>
<p>不需要对 vision 领域的了解，不需要 domain knowledge，直接把 图片理解成 a sequence of patches, i.e., 一个句子里的很多单词。</p>
<p><strong>新问题</strong> —— CV 除了 image classfication 其他的任务，行不行呢？分割、检测</p>
<ul>
<li>DETR (Carion et al. 2020) 目标检测的力作，改变了目标检测 出框的方式。ViT 做其它 CV 任务应该效果也很好。</li>
<li>ViT-FRCNN 检测 detection</li>
<li>SETR 分割 segmentation</li>
<li>（3个月后）Swin Transformer 融合 Transformer 和多尺度设计</li>
</ul>
<p>Transformer 是 CV 领域的一个通用的骨干网络 backbone</p>
<p>另外一个未来工作方向，<strong>自监督的预训练方式</strong>。NLP 大的 transformer 模型使用 自监督 预训练，ViT有 initial experiments 证明 自监督预训练也可以，但和有监督的训练有差距 still large gap。</p>
<p><strong>ViT 挖坑</strong>：</p>
<ul>
<li>视觉领域 CV</li>
<li>多模态，一个 transformer 处理 CV 和 NLP问题</li>
</ul>
<h1 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a><strong>5.相关工作</strong></h1><h2 id="5-1-Transformer"><a href="#5-1-Transformer" class="headerlink" title="5.1 Transformer"></a>5.1 Transformer</h2><p>Transformer 在 NLP 领域的应用：BERT, GPT</p>
<ul>
<li>Transformer 先在大规模语料库上做预训练，再根据具体的任务数据集进行微调。</li>
<li>BERT： denosing mask挖词、完形填空，把masked的词预测出来</li>
<li>GPT：language modelling, 预测下一个词 next word prediction</li>
<li>自监督的训练方式：完形填空 or 预测下一个词，人为设定。语料句子是完整的，去掉某些词（完形填空） or 最后词（预测下一个词）</li>
</ul>
<h2 id="5-2-self-attention-在视觉领域的应用"><a href="#5-2-self-attention-在视觉领域的应用" class="headerlink" title="5.2 self-attention 在视觉领域的应用"></a>5.2 self-attention 在视觉领域的应用</h2><p>用图片的所有像素做Transformer会产生复杂度爆炸的问题，Transformer的复杂度是输入维度的平方（$O(n^2 )$）</p>
<p>self-attention to each image with approximations：</p>
<ul>
<li>不用整张图，只用 local neighborhoods，降低序列长度</li>
</ul>
<p>sparse transformer</p>
<ul>
<li>全局注意力的近似</li>
<li>只对 稀疏的点 做注意力</li>
</ul>
<p>scale attention by applying attention in blocks of varying size</p>
<ul>
<li>把自注意力用到不同大小的 blocks</li>
<li>in the extreme case only along individual axes 极端情况，只关心轴， axial self-attention，横轴 + 纵轴</li>
</ul>
<p>小结：以上 self-attention + CV 效果不错，但工程实现加速很难。可在 cpu gpu跑，但大规模训练不行。</p>
<h2 id="5-3-和-ViT-最相似的工作：-x20"><a href="#5-3-和-ViT-最相似的工作：-x20" class="headerlink" title="5.3 和 ViT 最相似的工作：&#x20;"></a>5.3 <strong>和 ViT 最相似的工作：</strong>&#x20;</h2><h3 id="（1）ICLR-2020-2-2-patches-for-CIFAR-10-32-32-图片"><a href="#（1）ICLR-2020-2-2-patches-for-CIFAR-10-32-32-图片" class="headerlink" title="（1）ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 图片"></a>（1）ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 图片</h3><p><strong>ViT 胜在哪里:</strong> 更大的 patches 16 *16 + 更大的训练数据集</p>
<p>CV 中 检测、分类、视频处理、多模态 self-attention with CNNs</p>
<h3 id="（2）image-GPT"><a href="#（2）image-GPT" class="headerlink" title="（2）image GPT"></a><strong>（2）image GPT</strong></h3><p>GPT 是 NLP 的生成模型，image GPT 无监督预训练，生成模型。</p>
<p>image GPT 也用了 transformer 图片（降低分辨率和 color space）。用训练好的 image GPT or 直接把 image GPT 当成特征提取器， ImageNet 准确率 72%；ViT ImageNet 准确率 88.5%</p>
<h3 id="（3）最近爆火的-MAE"><a href="#（3）最近爆火的-MAE" class="headerlink" title="（3）最近爆火的 MAE"></a><strong>（3）最近爆火的 MAE</strong></h3><p>在 BEiT 或 MAE 论文之前，生成式网络 在 CV 比 判别式网络 弱很多。</p>
<p>MAE 生成式模型 在 ImageNet-1k 做训练，比判别式模型好。分类，目标检测  （transfer learning）都有不错的效果。</p>
<h1 id="6-ViT模型"><a href="#6-ViT模型" class="headerlink" title="6.ViT模型"></a><strong>6.ViT模型</strong></h1><p>ViT 尽可能使用 original Transformer，享受 Transformer 的高效实现。</p>
<h2 id="6-1-Vision-Transformer"><a href="#6-1-Vision-Transformer" class="headerlink" title="6.1 Vision Transformer"></a><strong>6.1 Vision Transformer</strong></h2><h3 id="（1）Model"><a href="#（1）Model" class="headerlink" title="（1）Model"></a>（1）Model</h3><p><img src="image/image_0cloNkz9xq.png" alt=""></p>
<p><img src="image/image_Un5thXW-QB.png" alt=""></p>
<p><strong>ViT 对 图片的操作</strong>： 划分 patches，flatten patches 的线性投影 + patches 的位置信息，得到输入 transformer 的 tokens</p>
<p><strong>如何分类</strong>：借鉴 BERT，插入一个特殊的字符 <code>* [CLS]</code>，用于分类。<code>* [CLS]</code>也有 position embedding, 0(永远是0)。<code>* [CLS]</code> 输入 一个通用的 MLP Head，得到 Class，cross-entropy 损失函数训练模型。</p>
<p>图像示例（维度变化），以224*224输入图像为例：</p>
<ul>
<li>**图片 **$X$： <code>224 * 224 * 3</code>(RGB, 3 channels)</li>
<li>$ patches  $**数 **$N$： <code>224 ^ 2 / 16 ^ 2 = 14 ^ 2 = 196</code></li>
<li><strong>每一个 patch 的维度</strong>：<code>16 * 16 * 3 (RGB, 3 channels) = 768</code></li>
<li>Linear Projection 全连接层 $E$输入：<code>768( 不变，patch 计算而来 ) * D(embedding_dim， 768 或 更大)</code></li>
<li>**Linear Projection 全连接层 **$E$<strong>输出</strong>：<code>X * E = patches (196 patches 个数 * 768 每个 patch 的维度) * E ( 768 * D ) = 196 * D (768)</code></li>
<li>position embedding向量维度： <code>1 * 768</code>，通过sum加到输入信息中；</li>
<li>进入Transformer Encoder维度：196 * 768(图片对应的 tokens) 拼接 concatenate [CLS] token (1 * 768) = <code>197 * 768</code></li>
<li>ViT base MLA： <code>12 heads</code>；</li>
<li><strong>MLP</strong>：放大 <code>4</code> 倍，再缩小到原维度大小</li>
<li>Transfomer encoder 输入输出维度一致，可以直接叠加 <code>L</code> 个</li>
</ul>
<p><img src="image/image_HTcwHJy9-L.png" alt=""></p>
<h3 id="（2）ViT和CNN"><a href="#（2）ViT和CNN" class="headerlink" title="（2）ViT和CNN"></a>（2）ViT和CNN</h3><p><strong>class token：证明 标准的 transformer 做视觉，没问题！</strong></p>
<p>ViT 除了标准的 transformer，关键部分是 怎么对图片进行预处理 和 怎么对图片最后的输出进行后处理。</p>
<p>控制和 NLP 的差异：使用 BERT 的 CLS，CLS 在 NLP 理解为 一个全局的对句子理解的特征；ViT 的 CLS 理解为 一个图像的整体特征。</p>
<ul>
<li>CLS token + MLP (tanh acitvation) == 分类</li>
<li>CV 通常的 全局特征：(i.e., Res50)  feature map (14 * 14) —&gt; GAP globally average-pooling 全局平均池化 —&gt; a flatten vector 全局的图片特征向量 —&gt; MLP 分类</li>
<li>CLS 可用 GAP global average pooling 替换</li>
<li><strong>CV 的 CLS GAP 和 NLP 的 CLS 效果差异不大</strong>**，<strong>为了和原始Transformer保持统一</strong>。** 但CLS-Token 和 GAP 的 适用参数 不一样。</li>
</ul>
<p><strong>位置编码：</strong> 1d、2d、 relative编码都相差不大，为了和原始Transformer保持一致，使用1D编码。</p>
<ul>
<li>猜测相差不大原因：ViT 直接作用于 14 * 14 patches，而不是 224 * 224 像素。较少数量的 patches 之间的相对位置信息，容易学到。</li>
</ul>
<h3 id="（3）Inductive-bias"><a href="#（3）Inductive-bias" class="headerlink" title="（3）Inductive bias"></a><strong>（3）Inductive bias</strong></h3><p><strong>CNN 的 inductive bias</strong>： locality 局部性、 translation equivalence 平移等变性。在 CNN 模型每一层都有所体现，模型的先验知识从头到尾，贯穿整个模型。</p>
<p><strong>ViT 比 CNN 的 inductive bias 少, only MLP</strong></p>
<p><strong>ViT 的 inductive bias in images</strong>：图片 切成 patches；+ position embedding（随机初始化，没有携带 2d 位置信息）</p>
<p>ViT 的 patches 块的 2d 位置信息 + spatial relations 图像块之间的场景信息，都需要重新学。所以 <strong>ViT 没有很多 inductive bias</strong>，在中小型数据集训练 ViT 效果不如 CNN。</p>
<h3 id="（4）Hybrid-architecture"><a href="#（4）Hybrid-architecture" class="headerlink" title="（4）Hybrid architecture"></a>（4）<strong>Hybrid architecture</strong></h3><p>Transformer：全局建模能力强</p>
<p>CNN： data-efficient 不用那么多训练数据</p>
<p>前 CNN + 后 Transformer —&gt; Hybrid archtecture</p>
<p><strong>不同的图片预处理方式</strong>：不划分 patches，采用 CNN (Res50 的 feature map 14 * 14 = 196)，过全连接层 <strong>E</strong> Linear projections 得到图片的 embedding</p>
<p><strong>ViT 的图片预处理方式</strong>**：** 把一张图划分成 patches，直接过全连接层 FC</p>
<p><img src="image/image_E2mWR1qp4h.png" alt=""></p>
<h2 id="6-2-Fine-tuning-and-higher-resolution"><a href="#6-2-Fine-tuning-and-higher-resolution" class="headerlink" title="6.2 Fine-tuning and higher resolution"></a><strong>6.2 Fine-tuning and higher resolution</strong></h2><h3 id="（1）预训练好的-ViT-可以在更大尺寸的图片上微调吗？"><a href="#（1）预训练好的-ViT-可以在更大尺寸的图片上微调吗？" class="headerlink" title="（1）预训练好的 ViT 可以在更大尺寸的图片上微调吗？"></a><strong>（1）预训练好的 ViT 可以在更大尺寸的图片上微调吗？</strong></h3><p>if patch size 不变 16 * 16，更大尺寸的图片 —&gt; 序列长度的增加；Transformer 理论上，可以处理任意长度。* <em>But，</em> *<strong>提前训练好的 position embedding 可能失效</strong>。</p>
<h3 id="2-patches-数增多，如何使用已预训练好的位置编码吗？-​"><a href="#2-patches-数增多，如何使用已预训练好的位置编码吗？-​" class="headerlink" title="** (2) patches 数增多，如何使用已预训练好的位置编码吗？** ​"></a>** (2) patches 数增多，如何使用已预训练好的位置编码吗？** ​</h3><p>不可以。但可以使用2d 插值，增加位置编码的长度，但效果会掉点，是临时解决方案，ViT 微调时的一个局限。</p>
<ul>
<li>2d 插值，torch 的 interpolate 函数实现；但也不是任意长度增加都能保持效果。</li>
</ul>
<p>ViT 用了图片 2d 结构 的 inductive bias 地方：resolution adjustment 尺寸改变 和 patch extraction 抽 patches</p>
<h1 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a><strong>7.实验</strong></h1><p>对比 <code>ResNet</code>, <code>ViT</code>, <code>Hybrid ViT</code> (CNN 特征图，不是图片直接 patch 化) 的 representation learning capabilities 表征学习能力。</p>
<p>为了了解每个模型预训练好 到底需要多少数据，在不同大小的数据集上预训练，然后在很多 benchmark tasks 做测试。</p>
<p>考虑模型预训练的计算成本时，ViT performs very favourably 表现很好， SOTA + fewer resource 训练时间更少；</p>
<p>ViT 的自监督训练，可行，效果也还不错，有潜力；一年之后，MAE 用自监督训练 ViT 效果很好。</p>
<p>数据集：</p>
<ul>
<li><code>ImageNet-1K</code>: 1000 classes, 1.3M images</li>
<li><code>ImageNet-21K</code>: 21000 classes, 14M images</li>
<li><code>JFG-300</code>: 303M images Google 不开源</li>
<li>下游任务：分类 CFIAR etc.</li>
</ul>
<h2 id="7-1-ViT-model-variants"><a href="#7-1-ViT-model-variants" class="headerlink" title="7.1 ViT model variants"></a>7.1 ViT model variants</h2><p><img src="image/image_96dem1nNZT.png" alt=""></p>
<p>模型变体 = (Base, Large, Hugh) + (patch size 表示)</p>
<p>ViT-L/16 使用 Large 参数 和 patch 16 * 16 输入</p>
<h2 id="7-2-结果"><a href="#7-2-结果" class="headerlink" title="7.2 结果"></a>7.2 结果</h2><p>和 CNN 的工作 BiT-L, Noisy Student 做对比</p>
<ul>
<li><strong>BiT-L:</strong> CNN比较大的模型，ViT论文作者团队自己的工作</li>
<li><strong>Noisy Student:</strong> ImageNet 当时表现最好的方法。用 伪标签 pseudo-label 去 self-training</li>
</ul>
<p><img src="image/image_aM_NJtUFRn.png" alt=""></p>
<p>ViT-H/14 训练比 ViT-H/16 贵，效果和 BiT-L 差不多，优势不明显。<strong>怎么突出 ViT 的好呢？</strong></p>
<p><strong>ViT 训练更便宜</strong>。TPUv3 天数：ViT-H/14 2.5K, BiT-L 9.9K, Noisy Student 12.3K</p>
<p><strong>ViT 优点：效果好 + 训练快</strong></p>
<h2 id="7-3-结果分析"><a href="#7-3-结果分析" class="headerlink" title="7.3 结果分析"></a>7.3 <strong>结果分析</strong></h2><h3 id="（1）Vision-Transformer-到底需要多少数据才能训练好？"><a href="#（1）Vision-Transformer-到底需要多少数据才能训练好？" class="headerlink" title="（1）Vision Transformer 到底需要多少数据才能训练好？"></a>（1）Vision Transformer 到底需要多少数据才能训练好？</h3><p>图中灰色区域 ResNet 的效果，圆圈 ViT 的效果</p>
<p><img src="image/image_o0i2Wm3-gE.png" alt=""></p>
<p>如果想用 ViT，至少需要 ImageNet-21K 14M 大小的数据集</p>
<ul>
<li>小于整个数据量，CNN 更合适，更好的利用 inductive bias，ViT 没有特别多 inductive bias 需要更多数据训练。</li>
</ul>
<p>数据集规模比 ImageNet-21K 更大时，Vision Transformer 效果更好，因为可扩展性 scaling 更好。</p>
<h3 id="（2）Linear-few-shot-evaluation"><a href="#（2）Linear-few-shot-evaluation" class="headerlink" title="（2）Linear few-shot evaluation"></a><strong>（2）Linear few-shot evaluation</strong></h3><p><img src="image/image_pv4zY639Ju.png" alt=""></p>
<p><strong>linear evalution</strong>：把 ViT 预训练好的模型 直接作为 特征提取器，不 fine-tune，+ 一个 logistic regression 得到分类结果。</p>
<p><strong>Few-shot</strong>：5-shot，在 ImageNet 做 linear evaluation 时，每类图片随机选取 5 个 samples，evaluation 很快，做 消融实验。</p>
<p>linear few-shot evaluation 采用 JFT 数据集 10M, 30M, 100M, 300M。来自同一个数据集，数据没有 distribution gap，模型的效果更能体现 Vision Transformer 本身特质。</p>
<p>ViT 图中 效果 和 上图差不多。</p>
<p><strong>如何用 ViT 做小样本学习，未来研究方向之一。</strong>&#x20;</p>
<h3 id="（3）用-ViT-比-CNNs-便宜"><a href="#（3）用-ViT-比-CNNs-便宜" class="headerlink" title="（3）用 ViT 比 CNNs 便宜"></a>（3）<strong>用 ViT 比 CNNs 便宜</strong></h3><p>大家的印象：Transformer 又大又贵，很难训练</p>
<p><img src="image/image_XOqmsgtP7e.png" alt=""></p>
<ul>
<li><strong>average-5</strong>：ImageNet-real, Pets, Flower, CIFAR10, CIFAR100 平均</li>
<li>ImageNet 单独的对比</li>
</ul>
<p><strong>同等计算复杂度：ViT 比 ResNet 效果好，印证了 ViT 训练更便宜</strong></p>
<p><strong>Q：Hybrid 模型，CNN 抽取出来的特征，能不能帮助 Transformer 更好的学习呢？</strong></p>
<ul>
<li>小模型，Hybrid 模型吸收 CNN 和 Transformer 的优点，效果好。不需要很多的数据预训练，达到 Transformer 的效果</li>
<li>大模型，Hybrid 模型 和 Transformer 差不多，甚至不如 Transformer 模型。</li>
</ul>
<h2 id="7-4-Inspecting-Vision-Transformer"><a href="#7-4-Inspecting-Vision-Transformer" class="headerlink" title="7.4 Inspecting Vision Transformer"></a><strong>7.4 Inspecting Vision Transformer</strong></h2><p>可视化分析 ViT 内部表征 internal representations: <strong>Patch embedding, position embedding</strong></p>
<h3 id="（1）-Linear-projection-E"><a href="#（1）-Linear-projection-E" class="headerlink" title="（1）** Linear projection E **"></a>（1）** Linear projection E **</h3><p><strong>ViT 第一层 Linear projection E 学到了什么？</strong></p>
<p>embed RGB value 前 28 个主成分</p>
<p><img src="image/image_P_ocxXsixV.png" alt=""></p>
<p>Vision Transformer 和 CNN 学到的很像，类似 gabor filter 有颜色、纹理， 可以做 plausible basis functions，可以描述每个图像块的底层信息 a low-dimensional representation of the fine structure within each patch.</p>
<h3 id="（2）Position-embedding"><a href="#（2）Position-embedding" class="headerlink" title="（2）Position embedding"></a>（2）<strong>Position embedding</strong></h3><p><strong>Position embedding 能学到一些表示位置距离的信息</strong></p>
<p><img src="image/image_1XvQ8mrrd2.png" alt=""></p>
<ul>
<li>patch 自己本身 相似度高 黄色 1</li>
<li>学到了距离的概念</li>
<li>(4, 4) 黄色中心点，越边缘，相似度越低，颜色越蓝</li>
<li>学到了 行 和 列 的距离规则</li>
<li>同行同列，颜色条 的表示</li>
</ul>
<p>虽然是 1d 的 position embedding，但已经学到了 2d 的图像位置概念；所以换成 2d position 提升不多。</p>
<h3 id="（3）Self-attention-有没有起作用？"><a href="#（3）Self-attention-有没有起作用？" class="headerlink" title="（3）Self-attention 有没有起作用？"></a>（3）<strong>Self-attention 有没有起作用？</strong></h3><p>用 Transformer 的原因：自注意力 能模拟长距离的关系。</p>
<ul>
<li>NLP 一个很长的句子里，开头的一个词和结尾的一个词 可能互相有关联。</li>
<li>CV 里 很远的两个像素点之间 也能做自注意力。</li>
</ul>
<p><strong>ViT 的 self-attention 是不是 很远的像素点也能有交互？</strong></p>
<p>ViT-L/16 有 24 层（横坐标值），五颜六色的点：transformer 每层 multi-head 的heads，ViT-L 16 heads, 每一列有 16 个点</p>
<p><img src="image/image_RBoM5A-PX8.png" alt=""></p>
<p>纵轴是 mean attention distance</p>
<p>$d<em>{ab} = l</em>{ab} <em> A_{ab} = ab 两点 pixel 之间的距离差 </em> ab 两点之间的attention ~weights$</p>
<p>d_ab 的大小，反映模型能不能注意到很远的 2 个 pixels</p>
<ul>
<li>self-attention 刚开始能注意到 10 - 110 pixels</li>
<li>self-attention 刚开始就注意到全局的信息；CNN 刚开始第一层的感受野 receptive filed 很小，只能看到附近的 pixel</li>
</ul>
<p>网络加深，模型学到的特征越来越 high level，越来越有语义信息，像素的自注意力距离 越来越远，不是靠邻近的像素点做判断。</p>
<p><strong>证明 自注意力 有学到 很远距离的 pixel 信息，</strong> ViT 最后一层 output 的 token 的 self-attention 折射（逆向映射）回 原来的输入图片。ViT 真的学到了一些概念：狗、飞机</p>
<p><img src="image/image_gAoexe0iVB.png" alt=""></p>
<p>Globally 全局来说，输出的 token 是融合全局的特征信息，ViT 模型可以关注到 和 classfication 分类相关的图像区域。</p>
<h2 id="7-5-self-supervision"><a href="#7-5-self-supervision" class="headerlink" title="7.5 self-supervision"></a><strong>7.5 self-supervision</strong></h2><p>如何用 自监督 的方式 训练一个 vision transformer？</p>
<p>因为 NLP 的 transformer 都是用 large scale self-supervised pre-training <strong>大规模、自监督</strong> 的方式预训练的。</p>
<p><strong>NLP 的 自监督</strong>：BERT <strong>完形填空</strong> Mask language model，GPT 生成，<strong>预测下一个词</strong> by language model</p>
<p>ViT 借鉴 BERT，创建一个专属于 vision 的目标函数，<strong>masked patch prediction</strong>。一张图片的某些 patches 随机抹掉，ViT 重建缺失的patches</p>
<p><strong>Note：从 模型、目标函数上，CV 和 NLP 的大一统。</strong></p>
<p>但是，ViT-B/16 with masked patch prediction 在 ImageNet ~80% 准确率。~80% 比 从头训练 ViT 好 2%，比 supervised pre-training 低 4%。</p>
<h1 id="8-评论"><a href="#8-评论" class="headerlink" title="8.评论"></a><strong>8.评论</strong></h1><h2 id="8-1-写作"><a href="#8-1-写作" class="headerlink" title="8.1 写作"></a>8.1 写作</h2><p>写作：简洁明了、有轻有重（重要结果放正文），图表清晰。</p>
<p>内容：Vision Transformer 挖了一个大坑：各个角度的分析，提升 or 推广</p>
<p>task 任务角度：ViT 只做了分类，检测、分割、其它领域的任务 future work</p>
<p>ViT 结构的角度：</p>
<ul>
<li>改刚开始的 tokenization</li>
<li>改 transformer block, i.e., self-attention 换成 MLP works</li>
<li>MetaFormer 认为 transformer work 的原因是 transformer 的架构，不是 transformer 某些特殊的算子</li>
<li>MetaFormer，self-attention 改成 （不能学习的）pooling 池化操作；甚至改成 Identity，不用注意力</li>
<li>改 目标函数：有监督、or 不同的自监督训练方式</li>
</ul>
<h2 id="8-2-ViT"><a href="#8-2-ViT" class="headerlink" title="8.2 ViT"></a>8.2 ViT</h2><p>ViT 的大坑：</p>
<ul>
<li><strong>打通了 CV 和 LP 之间的鸿沟</strong></li>
<li>挖了一个更大的<strong>多模态</strong>的坑</li>
<li>视频、音频、基于 touch 的信号</li>
<li>各种 modality 的信号都可以拿来用</li>
</ul>
<p><strong>CNN, self-attention, MLP 鹿死谁手？</strong> 犹未可知，期待下一个改进的 vision transformer</p>
<ul>
<li>一个简洁、高效、通用的视觉骨干网络 CV backbone，甚至完全不用任何标注信息</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/paper_reading/2.3.ViT/">https://wdndev.github.io/paper_reading/2.3.ViT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/PaperReading/">PaperReading</a><a class="post-meta__tags" href="/tags/CV/">CV</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/paper_reading/2.4.Swin%20Transformer/" title="论文精读 Swin Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-01</div><div class="title">论文精读 Swin Transformer</div></div></a></div><div><a href="/paper_reading/2.1.Transformer/" title="论文精读 Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-31</div><div class="title">论文精读 Transformer</div></div></a></div><div><a href="/paper_reading/2.2.BERT/" title="论文精读 BERT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-06</div><div class="title">论文精读 BERT</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#3-ViT"><span class="toc-text">3.ViT</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E6%A0%87%E9%A2%98-%E7%AE%80%E4%BB%8B"><span class="toc-text">1.标题 + 简介</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%91%98%E8%A6%81"><span class="toc-text">2.摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-%E5%BC%95%E8%A8%80"><span class="toc-text">3.引言</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1-self-attention"><span class="toc-text">3.1 self attention</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-Transformer-%E5%BA%94%E7%94%A8%E5%9C%A8-CV-%E7%9A%84%E9%9A%BE%E7%82%B9"><span class="toc-text">3.2 Transformer 应用在 CV 的难点</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89CNN-%E7%BB%93%E6%9E%84-self-attention-or-attention-%E6%9B%BF%E4%BB%A3%E5%8D%B7%E7%A7%AF"><span class="toc-text">（1）CNN 结构 + self-attention or attention 替代卷积</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89stand-alone-attention-%E5%AD%A4%E7%AB%8B%E8%87%AA%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">（2）stand-alone attention 孤立自注意力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89axial-attention-%E8%BD%B4%E6%B3%A8%E6%84%8F%E5%8A%9B"><span class="toc-text">（3）axial attention 轴注意力</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-ViT"><span class="toc-text">3.3 ViT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89ViT%E5%81%9A%E6%B3%95"><span class="toc-text">（1）ViT做法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%B8%BA%E4%BB%80%E4%B9%88-transformer-%E7%9A%84%E8%AE%AD%E7%BB%83%E6%98%AF-supervised-fashion%EF%BC%9F"><span class="toc-text">（2）为什么 transformer 的训练是 supervised fashion？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Transformer-in-CV%EF%BC%8C%E4%B9%8B%E5%89%8D%E6%9C%89%E4%BA%BA%E5%81%9A%E5%90%97%EF%BC%9F"><span class="toc-text">（3）Transformer in CV，之前有人做吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89ViT-%E4%BB%BB%E4%BD%95%E6%83%85%E5%86%B5%E6%AF%94CNN%E9%83%BD%E5%BC%BA%E5%90%97%EF%BC%9F"><span class="toc-text">（4）ViT 任何情况比CNN都强吗？</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#inductive-biases-%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE%EF%BC%9A%E5%85%88%E9%AA%8C%E7%9F%A5%E8%AF%86-or-%E6%8F%90%E5%89%8D%E7%9A%84%E5%81%87%E8%AE%BE"><span class="toc-text">inductive biases 归纳偏置：先验知识 or 提前的假设</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%80%8E%E4%B9%88%E9%AA%8C%E8%AF%81-Transformer-%E6%97%A0-inductive-bias-%E7%9A%84%E5%81%87%E8%AE%BE%EF%BC%9F"><span class="toc-text">怎么验证 Transformer 无 inductive bias 的假设？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4%E5%BC%95%E8%A8%80%E6%80%BB%E7%BB%93"><span class="toc-text">3.4引言总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E7%BB%93%E8%AE%BA"><span class="toc-text">4.结论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">5.相关工作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#5-1-Transformer"><span class="toc-text">5.1 Transformer</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-2-self-attention-%E5%9C%A8%E8%A7%86%E8%A7%89%E9%A2%86%E5%9F%9F%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-text">5.2 self-attention 在视觉领域的应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-3-%E5%92%8C-ViT-%E6%9C%80%E7%9B%B8%E4%BC%BC%E7%9A%84%E5%B7%A5%E4%BD%9C%EF%BC%9A-x20"><span class="toc-text">5.3 和 ViT 最相似的工作： </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89ICLR-2020-2-2-patches-for-CIFAR-10-32-32-%E5%9B%BE%E7%89%87"><span class="toc-text">（1）ICLR 2020 2 * 2 patches for CIFAR-10 32 * 32 图片</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89image-GPT"><span class="toc-text">（2）image GPT</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%9C%80%E8%BF%91%E7%88%86%E7%81%AB%E7%9A%84-MAE"><span class="toc-text">（3）最近爆火的 MAE</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-ViT%E6%A8%A1%E5%9E%8B"><span class="toc-text">6.ViT模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-Vision-Transformer"><span class="toc-text">6.1 Vision Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89Model"><span class="toc-text">（1）Model</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89ViT%E5%92%8CCNN"><span class="toc-text">（2）ViT和CNN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Inductive-bias"><span class="toc-text">（3）Inductive bias</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%884%EF%BC%89Hybrid-architecture"><span class="toc-text">（4）Hybrid architecture</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-Fine-tuning-and-higher-resolution"><span class="toc-text">6.2 Fine-tuning and higher resolution</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84-ViT-%E5%8F%AF%E4%BB%A5%E5%9C%A8%E6%9B%B4%E5%A4%A7%E5%B0%BA%E5%AF%B8%E7%9A%84%E5%9B%BE%E7%89%87%E4%B8%8A%E5%BE%AE%E8%B0%83%E5%90%97%EF%BC%9F"><span class="toc-text">（1）预训练好的 ViT 可以在更大尺寸的图片上微调吗？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-patches-%E6%95%B0%E5%A2%9E%E5%A4%9A%EF%BC%8C%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8%E5%B7%B2%E9%A2%84%E8%AE%AD%E7%BB%83%E5%A5%BD%E7%9A%84%E4%BD%8D%E7%BD%AE%E7%BC%96%E7%A0%81%E5%90%97%EF%BC%9F-%E2%80%8B"><span class="toc-text">** (2) patches 数增多，如何使用已预训练好的位置编码吗？** ​</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E5%AE%9E%E9%AA%8C"><span class="toc-text">7.实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-ViT-model-variants"><span class="toc-text">7.1 ViT model variants</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-%E7%BB%93%E6%9E%9C"><span class="toc-text">7.2 结果</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-3-%E7%BB%93%E6%9E%9C%E5%88%86%E6%9E%90"><span class="toc-text">7.3 结果分析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89Vision-Transformer-%E5%88%B0%E5%BA%95%E9%9C%80%E8%A6%81%E5%A4%9A%E5%B0%91%E6%95%B0%E6%8D%AE%E6%89%8D%E8%83%BD%E8%AE%AD%E7%BB%83%E5%A5%BD%EF%BC%9F"><span class="toc-text">（1）Vision Transformer 到底需要多少数据才能训练好？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89Linear-few-shot-evaluation"><span class="toc-text">（2）Linear few-shot evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E7%94%A8-ViT-%E6%AF%94-CNNs-%E4%BE%BF%E5%AE%9C"><span class="toc-text">（3）用 ViT 比 CNNs 便宜</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-4-Inspecting-Vision-Transformer"><span class="toc-text">7.4 Inspecting Vision Transformer</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89-Linear-projection-E"><span class="toc-text">（1）** Linear projection E **</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89Position-embedding"><span class="toc-text">（2）Position embedding</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89Self-attention-%E6%9C%89%E6%B2%A1%E6%9C%89%E8%B5%B7%E4%BD%9C%E7%94%A8%EF%BC%9F"><span class="toc-text">（3）Self-attention 有没有起作用？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-5-self-supervision"><span class="toc-text">7.5 self-supervision</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-%E8%AF%84%E8%AE%BA"><span class="toc-text">8.评论</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-%E5%86%99%E4%BD%9C"><span class="toc-text">8.1 写作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-ViT"><span class="toc-text">8.2 ViT</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>