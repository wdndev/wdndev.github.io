<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>论文精读 BERT | 37.2° Blog</title><meta name="author" content="Firefly"><meta name="copyright" content="Firefly"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="2.BERT论文链接：1810.04805.pdf (arxiv.org) 解读视频：BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili 参考代码：personal&#x2F;bert_torch at main · wdndev&#x2F;personal · GitHub 1.题目+作者“BERT” (Devlin 等, 2018, p. 1) 自然语言中中近三年最重要的文章；在计算机视觉里面很早就能">
<meta property="og:type" content="article">
<meta property="og:title" content="论文精读 BERT">
<meta property="og:url" content="https://wdndev.github.io/paper_reading/2.2.BERT/index.html">
<meta property="og:site_name" content="37.2° Blog">
<meta property="og:description" content="2.BERT论文链接：1810.04805.pdf (arxiv.org) 解读视频：BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili 参考代码：personal&#x2F;bert_torch at main · wdndev&#x2F;personal · GitHub 1.题目+作者“BERT” (Devlin 等, 2018, p. 1) 自然语言中中近三年最重要的文章；在计算机视觉里面很早就能">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://wdndev.github.io/img/wdn_icon.png">
<meta property="article:published_time" content="2023-06-05T16:00:00.000Z">
<meta property="article:modified_time" content="2025-11-01T23:46:10.696Z">
<meta property="article:author" content="Firefly">
<meta property="article:tag" content="Transformer">
<meta property="article:tag" content="PaperReading">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://wdndev.github.io/img/wdn_icon.png"><link rel="shortcut icon" href="/img/wdn_icon.png"><link rel="canonical" href="https://wdndev.github.io/paper_reading/2.2.BERT/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search/.xml","preload":false,"top_n_per_article":1,"unescape":false,"languages":{"hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: true,
    post: true
  },
  runtime: 'days',
  dateSuffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: {"limitCount":100,"languages":{"author":"Author: Firefly","link":"Link: ","source":"Source: 37.2° Blog","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: true,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '论文精读 BERT',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-02 07:46:10'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/background.css"><meta name="generator" content="Hexo 7.3.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/wdn_icon.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">565</div></a><a href="/tags/"><div class="headline">Tags</div><div class="length-num">24</div></a><a href="/categories/"><div class="headline">Categories</div><div class="length-num">15</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img fixed" id="page-header"><nav id="nav"><span id="blog-info"><a href="/" title="37.2° Blog"><span class="site-name">37.2° Blog</span></a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search" href="javascript:void(0);"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Content</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/paper_reading/pr_content"><i class="fa-fw fas fa-newspaper"></i><span> Paper</span></a></li><li><a class="site-page child" href="/llms/llms_idx"><i class="fa-fw fa-regular fa-bookmark"></i><span> LLMs</span></a></li><li><a class="site-page child" href="/jupyter"><i class="fa-fw fa-solid fa-file"></i><span> Jupyter</span></a></li><li><a class="site-page child" href="/dsa/dsa_idx"><i class="fa-fw fas fa-tree"></i><span> Algorithm</span></a></li><li><a class="site-page child" href="/program_language/pl_idx"><i class="fa-fw fas fa-code"></i><span> PLs</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-calendar-days"></i><span> Daily</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/daily/github/index"><i class="fa-fw fas fa-arrow-trend-up"></i><span> Github</span></a></li><li><a class="site-page child" href="/daily/weibo/index"><i class="fa-fw fas fa-brands fa-weibo"></i><span> Weibo</span></a></li><li><a class="site-page child" href="/daily/hf/index"><i class="fa-fw fas fa-face-smile"></i><span> HF</span></a></li><li><a class="site-page child" href="/daily/domain/index"><i class="fa-fw fas fa-book-open"></i><span> Arxiv</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">论文精读 BERT</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-06-05T16:00:00.000Z" title="Created 2023-06-06 00:00:00">2023-06-06</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-11-01T23:46:10.696Z" title="Updated 2025-11-02 07:46:10">2025-11-02</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/PaperReading/">PaperReading</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">7k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>21min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="论文精读 BERT"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div><article class="post-content" id="article-container"><h1 id="2-BERT"><a href="#2-BERT" class="headerlink" title="2.BERT"></a>2.BERT</h1><p>论文链接：<a target="_blank" rel="noopener" href="https://arxiv.org/pdf/1810.04805.pdf" title="1810.04805.pdf (arxiv.org)">1810.04805.pdf (arxiv.org)</a></p>
<p>解读视频：<a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1PL411M7eQ/?spm_id_from=333.337.search-card.all.click\&amp;vd_source=6bc8f793c75740c7bcfb8e281f986a8e" title="BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili">BERT 论文逐段精读【论文精读】_哔哩哔哩_bilibili</a></p>
<p>参考代码：<a target="_blank" rel="noopener" href="https://github.com/wdndev/personal/tree/main/bert_torch" title="personal/bert_torch at main · wdndev/personal · GitHub">personal/bert_torch at main · wdndev/personal · GitHub</a></p>
<h1 id="1-题目-作者"><a href="#1-题目-作者" class="headerlink" title="1.题目+作者"></a>1.题目+作者</h1><p>“BERT” (Devlin 等, 2018, p. 1) 自然语言中中近三年最重要的文章；在计算机视觉里面很早就能够在一个大的数据集（比如说ImageNet）上训练出一个CNN模型，用这个模型可以用来处理一大片的机器视觉任务，来提升他们的性能；但是在自然语言处理里面，在BERT之前一直没有一个深的神经网络使得它训练好之后能够帮处理一大片的NLP任务，在NLP中很多时候还是对每个任务构造自己的神经网路，然后再做训练；<strong>BERT的出现使得我们能够在一个大的数据集上面训练好一个比较深的神经网络，然后应用在很多的NLP任务上面，这样既简化了NLP任务的训练，又提升了它的性能，所以BERT和它之后的一系列工作使得自然语言处理在过去三年中有了质的飞跃</strong>；</p>
<ul>
<li><strong>pre-training</strong>：在一个数据集上训练好一个模型，这个模型主要的目的是用在另外一个任务上面，所以如果另外一个任务叫training的话，那么在大的数据集上训练的这个任务（模型）就叫做pre-training，即training之前的任务</li>
<li><strong>deep</strong>：更深的神经网络</li>
<li><strong>bidirectional</strong>：双向的</li>
<li><strong>transformers</strong>：</li>
<li><strong>language understanding</strong>：transformer主要是用在机器翻译这个小任务上，这里使用的是一个更加广义的词，就是对语言的理解</li>
</ul>
<h1 id="2-摘要"><a href="#2-摘要" class="headerlink" title="2.摘要"></a>2.摘要</h1><p>BERT是一个新的语言表示模型，BERT的名字来自于：</p>
<ul>
<li>Bidirectional</li>
<li>Encoder</li>
<li>Representation</li>
<li>Transformer</li>
</ul>
<p>它的意思是transformer这个模型双向的编码器表示，这四个词跟标题是不一样的</p>
<p><strong>它的想法是基于ELMo</strong></p>
<ul>
<li>ELMo来自于芝麻街中人物的名字，芝麻街是美国的一个少儿英语学习节目</li>
<li>BERT是芝麻街中另外一个主人公的名字</li>
<li>这篇文章和之前的ELMo开创了NLP的芝麻街系列文章</li>
</ul>
<p><strong>BERT和最近的一些语言的表示模型有所不同</strong></p>
<ul>
<li>Peters 引用的是ELMo</li>
<li>Radfor 引用的是GPT</li>
</ul>
<p><strong>BERT和ELMo、GPT的区别：</strong></p>
<ul>
<li>BERT是设计用来训练深的双向表示，<strong>使用的是没有标号的数据，再联合左右的上下文信息，</strong>因为这样的设计导致训练好的BERT只用加一个额外的输出层，就可以在很多NLP的任务（比如问答、语言推理）上面得到一个不错的结果，而且不需要对任务做很多特别的架构上的改动</li>
<li>GPT考虑的是单向（用左边的上下文信息去预测未来），BERT同时使用了左侧和右侧的信息，它是双向的（Bidirectional）</li>
<li>ELMO用的是一个基于RNN的架构，BERT用的是transformer，所以ELMo在用到一些下游任务的时候需要对架构做一点点调整，但是BERT相对比较简单，和GPT一样只需要改最上层就可以了</li>
</ul>
<h1 id="3-Introduction"><a href="#3-Introduction" class="headerlink" title="3.Introduction"></a>3.Introduction</h1><h2 id="3-1自然语言任务和预训练"><a href="#3-1自然语言任务和预训练" class="headerlink" title="3.1自然语言任务和预训练"></a>3.1自然语言任务和预训练</h2><p>在语言模型中，预训练可以用来提升很多自然语言的任务</p>
<p>自然语言任务包括两类</p>
<ul>
<li><strong>句子层面的任务（sentence-level）</strong>：主要是用来建模句子之间的关系，比如说对句子的情绪识别或者两个句子之间的关系</li>
<li><strong>词元层面的任务（token-level）</strong>：包括实体命名的识别（对每个词识别是不是实体命名，比如说人名、街道名），这些任务需要输出一些细腻度的词元层面上的输出</li>
</ul>
<p>预训练在NLP中已经流行了有一阵子了，在计算机视觉里面已经用了很多年了，同样的方法用到自然语言上面也不会很新，但是在介绍BERT的时候，很有可能会把NLP做预训练归功于BERT，BERT不是第一个提出来的而是BERT让这个方法出圈了让后面的研究者跟着做自然语言的任务</p>
<h2 id="3-2-预训练任务的策略"><a href="#3-2-预训练任务的策略" class="headerlink" title="3.2 预训练任务的策略"></a>3.2 预训练任务的策略</h2><p>在使用预训练模型做特征表示的时候，一般有两类策略</p>
<ul>
<li>一个策略是<strong>基于特征</strong>的，代表作是ELMo，对每一个下游的任务构造一个跟这个任务相关的神经网络，它<strong>使用的RNN的架构</strong>，然后将预训练好的这些表示（比如说词嵌入也好，别的东西也好）作为一个额外的特征和输入一起输入进模型中，希望这些特征已经有了比较好的表示，所以导致模型训练起来相对来说比较容易，这也是NLP中使用预训练模型最常用的做法（把学到的特征和输入一起放进去作为一个很好的特征表达）</li>
<li>另一个策略是<strong>基于微调</strong>的，这里举的是GPT的例子，就是把预训练好的模型放在下游任务的时候不需要改变太多，只需要改动一点就可以了。这个模型预训练好的参数会在下游的数据上再进行微调（所有的权重再根据新的数据集进行微调）</li>
</ul>
<p>上述两个途径在预训练的时候都是使用一个相同的目标函数，都是使用一个单向的语言模型（给定一些词去预测下一个词是什么东西，说一句话然后预测这句话下面的词是什么东西，属于一个预测模型，用来预测未来，所以是单向的）</p>
<h2 id="3-3-现有技术的限制"><a href="#3-3-现有技术的限制" class="headerlink" title="3.3 现有技术的限制"></a>3.3 现有技术的限制</h2><p>本文的主要想法：现在这些技术会有局限性，特别是做预训练的表征的时候，主要的问题是标准的语言模型是单向的，这样就导致在选架构的时候会有局限性</p>
<ul>
<li>在GPT中使用的是一个从左到右的架构（在看句子的时候只能从左看到右），这样的坏处在于如果要做句子层面的分析的话，比如说要判断一个句子层面的情绪是不是对的话，从左看到右和从右看到左都是合法的，另外，就算是词元层面上的一些任务，比如QA的时候也是看完整个句子再去选答案，而不是一个一个往下走</li>
</ul>
<p>因此<strong>如果将两个方向的信息都放进去的话，应该是能够提升这些任务的性能的</strong></p>
<h2 id="3-4-BERT"><a href="#3-4-BERT" class="headerlink" title="3.4 BERT"></a>3.4 BERT</h2><p>提出了BERT，BERT是用来减轻之前提到的语言模型是一个单向的限制，<strong>使用的是一个带掩码的语言模型（masked language model）</strong>，这个语言模型是受Cloze任务的启发（引用了一篇1953年的论文）</p>
<ul>
<li>这个带掩码的语言模型<strong>每一次随机地选一些资源，然后将它们盖住，目标函数就是预测被盖住的字</strong>，等价于将一个句子挖一些空完后进行完形填空；</li>
<li>跟标准的语言模型从左看到右的不同之处在于：<strong>带掩码的语言模型是允许看到左右的信息的</strong>（相当于看完形填空的时候不能只看完形填空的左边，也需要看完形填空的右边），这样的话它允许训练深的双向的transformer模型；</li>
<li>在带掩码的语言模型之外还训练了一个任务，<strong>预测下一个句子</strong>，核心思想是给定两个句子，然后判断这两个句子在原文里面是相邻的，还是随机采样得来的，这样就让模型学习了句子层面的信息；</li>
</ul>
<h2 id="3-5-文章的贡献"><a href="#3-5-文章的贡献" class="headerlink" title="3.5 文章的贡献"></a>3.5 文章的贡献</h2><ol>
<li>展示了双向信息的重要性，GPT只用了单向，之前有的工作只是很简单地把一个从左看到右的语言模型和一个从右看到左的语言模型简单地合并到一起，类似于双向的RNN模型（contact到一起），这个模型在双向信息的应用上更好</li>
<li>假设有一个比较好的预训练模型就不需要对特定任务做特定的模型改动。BERT是第一个在一系列的NLP任务上（包括在句子层面上和词元层面上的任务）都取得了最好的成绩的基于微调的模型</li>
<li>代码和模型全部放在：<a target="_blank" rel="noopener" href="https://github.com/google-research/bert" title="https://github.com/google-research/bert">https://github.com/google-research/bert</a></li>
</ol>
<h1 id="4-结论"><a href="#4-结论" class="headerlink" title="4.结论"></a>4.结论</h1><p>最近一些实验表明，使用无监督的预训练是非常好的，这样使得资源不多（训练样本比较少的任务也能够享受深度神经网络）,本文主要的工作就是把前人的工作扩展到深的双向的架构上，使得同样的预训练模型能够处理大量的不同的自然语言任务</p>
<p>简单概括一下：本文之前的两个工作一个叫<strong>ELMo</strong>，它使用了双向的信息但是它网络架构比较老，用的是RNN，另外一个工作是<strong>GPT</strong>，它用的是transformer的架构，但是它只能处理单向的信息，因此本文将ELMo双向的想法和GPT的transformer架构结合起来就成为了BERT</p>
<ul>
<li><strong>具体的改动是在做语言模型的时候不是预测未来，而是变成完形填空</strong></li>
</ul>
<h1 id="5-相关工作"><a href="#5-相关工作" class="headerlink" title="5.相关工作"></a>5.相关工作</h1><h1 id="6-BERT"><a href="#6-BERT" class="headerlink" title="6.BERT"></a>6.BERT</h1><h2 id="6-1-pre-training-and-fine-tuning"><a href="#6-1-pre-training-and-fine-tuning" class="headerlink" title="6.1 pre-training and fine-tuning"></a>6.1 pre-training and fine-tuning</h2><p>BERT中有两个步骤：</p>
<ul>
<li><strong>预训练</strong>：在预训练中，这个模型是在一个没有标号的数据集上训练的</li>
<li><strong>微调</strong>：在微调的时候同样是用一个BERT模型，但是它的权重被初始化成在预训练中得到的权重，所有的权重在微调的时候都会参与训练，用的是有标号的数据</li>
</ul>
<p>每一个下游的任务都会创建一个新的BERT模型，虽然它们都是用最早预训练好的BERT模型作为初始化，但是每个下游任务都会根据自己的数据训练好自己的模型。</p>
<p>虽然预训练和微调不是BERT独创的，在计算机视觉中用的比较多，但是作者还是做了一个简单的介绍（在写论文的时候遇到一些技术需要使用的时候，而且可能应该所有人都知道，最好不要一笔带过，论文是需要自洽的，后面的人读过来可能不太了解这些技术，但是这些技术又是论文中方法不可缺少的一部分的话，最好还是能够做一个简单的说明）</p>
<p><img src="image/image_tuUrpi62qn.png" alt=""></p>
<ul>
<li>预训练的时候输入是一些没有标号的句子对</li>
<li>这里是在一个没有标号的数据上训练出一个BERT模型，把他的权重训练好，对下游的任务来说，对每个任务创建一个同样的BERT模型，但是它的权重的初始化值来自于前面预训练训练好的权重，对于每一个任务都会有自己的有标号的数据，然后对BERT继续进行训练，这样就得到了对于某一任务的BERT版本。</li>
</ul>
<blockquote>
<p>[CLS]是在每个输入示例前添加的特殊符号，[SEP]是特殊的分隔符标记（例如，分隔问题/答案）</p>
</blockquote>
<h2 id="6-2-模型架构"><a href="#6-2-模型架构" class="headerlink" title="6.2 模型架构"></a>6.2 模型架构</h2><p>BERT模型就是一个多层的双向transformer编码器，而且它是直接基于原始的论文和它原始的代码，没有做改动</p>
<h3 id="（1）三个参数"><a href="#（1）三个参数" class="headerlink" title="（1）三个参数"></a>（1）三个参数</h3><ul>
<li>L：transformer块的个数</li>
<li>H：隐藏层的大小</li>
<li>A：自注意力机制中多头的头的个数</li>
</ul>
<h3 id="（2）两个模型"><a href="#（2）两个模型" class="headerlink" title="（2）两个模型"></a>（2）两个模型</h3><ul>
<li>BERT base：它的选取是使得跟GPT模型的参数差不多，来做一个比较公平的比较</li>
<li>BERT large：用来刷榜</li>
<li><strong>BERT中的模型复杂度和层数是一个线性关系，和宽度是一个平方的关系</strong></li>
</ul>
<h3 id="（3）怎样把超参数换算成可学习参数的大小"><a href="#（3）怎样把超参数换算成可学习参数的大小" class="headerlink" title="（3）怎样把超参数换算成可学习参数的大小"></a>（3）<strong>怎样把超参数换算成可学习参数的大小</strong></h3><p><img src="image/image_yZqmWRzAGX.png" alt=""></p>
<p>模型中可学习参数主要来自两块</p>
<ul>
<li><strong>嵌入层</strong>：就是一个矩阵，输入是字典的大小（假设是30k），输出等于隐藏单元的个数（假设是H）</li>
<li><strong>transformer块</strong>：transformer中有两部分：一个是自注意力机制（它本身是没有可学习参数的，但是对多头注意力的话，他会把所有进入的K、V、Q分别做一次投影，每一次投影的维度是等于64的，因为有多个头，头的个数A乘以64得到H，所以进入的话有key、value、q，他们都有自己的投影矩阵，这些投影矩阵在每个头之间合并起来其实就是H<em>H的矩阵了，同样道理拿到输出之后也会做一次投影，他也是一个H</em>H的矩阵，所以对于一个transformer块，他的自注意力可学习的参数是H的平方乘以4），一个是后面的MLP（MLP里面需要两个全连接层，第一个层的输入是H，但是它的输出是4<em>H，另外一个全连接层的输入是4</em>H，输出是H，所以每一个矩阵的大小是H*4H，两个矩阵就是H的平方乘以8），这两部分加起来就是一个transformer块中的参数，还要乘以L（transformer块的个数）</li>
</ul>
<p>所以总参数的个数就是30k乘以H（这部分就是嵌入层总共可以学习的参数个数）再加上L层乘以H的平方再乘以12</p>
<h2 id="6-3-输入和输出"><a href="#6-3-输入和输出" class="headerlink" title="6.3 输入和输出"></a>6.3 输入和输出</h2><h3 id="（1）输入形式"><a href="#（1）输入形式" class="headerlink" title="（1）输入形式"></a>（1）输入形式</h3><p>对于下游任务的话，有些任务是处理一个句子，有些任务是处理两个句子，所以为了使BERT模型能够处理所有的任务，它的输入既可以是一个句子，也可以是一个句子对</p>
<ul>
<li>这里的一个句子是指一段连续的文字，不一定是真正的语义上的一段句子</li>
<li>输入叫做一个序列，可以是一个句子，也可以是两个句子</li>
<li>这和之前文章里的transformer是不一样的：transformer在训练的时候，他的输入是一个序列对，因为它的编码器和解码器分别会输入一个序列，但是BERT只有一个编码器，所以为了使它能够处理两个句子，就需要把两个句子变成一个序列</li>
</ul>
<h3 id="（2）序列的构成"><a href="#（2）序列的构成" class="headerlink" title="（2）序列的构成"></a><strong>（2）序列的构成</strong></h3><p>这里使用的切词的方法是WordPiece，核心思想是：</p>
<ul>
<li>假设按照空格切词的话，一个词作为一个token，因为数据量相对比较大，所以会导致词典大小特别大，可能是百万级别的，那么根据之前算模型参数的方法，如果是百万级别的话，就导致整个可学习参数都在嵌入层上面</li>
<li>WordPiece是说假设一个词在整个里面出现的概率不大的话，那么应该把它切开看它的一个子序列，它的某一个子序列很有可能是一个词根，这个词很有可能出现的概率比较大话，那么就只保留这个子序列就行了。这样的话，可以把一个相对来说比较长的词切成很多一段一段的片段，而且这些片段是经常出现的，这样的话就可以用一个相对来说比较小的词典就能够表示一个比较大的文本了</li>
</ul>
<p>切好词之后如何将两个句子放在一起</p>
<ul>
<li><strong>序列的第一个词永远是一个特殊的记号**</strong><code>[CLS]</code>**，CLS表示classification，这个词的作用是BERT希望最后的输出代表的是整个序列的信息（比如说整个句子层面的信息），因为BERT使用的是transformer的编码器，所以它的自注意力层中每一个词都会去看输出入中所有词的关系，就算是词放在第一的位置，它也是有办法能够看到之后的所有词</li>
</ul>
<p>把两个句子合在一起，但是因为要做句子层面的分类，所以需要区分开来这两个句子，这里有两个办法：</p>
<ol>
<li>在每一个句子后面放一个特殊的词：<code>[SEP]</code>表示separate</li>
<li>学一个嵌入层来表示这个句子到底是第一个句子还是第二个句子</li>
</ol>
<p><code>[CLS]</code>是第一个特殊的记号表示分类，中间用一个特殊的记号<code>[SEP]</code>分隔开，每一个token进入BERT得到这个token的embedding表示（对BERT来讲，就是输入一个序列，然后得到另外一个序列），最后transformer块的输出就表示这个词元的BERT表示，最后再添加额外的输出层来得到想要的结果。</p>
<h3 id="（3）embedding"><a href="#（3）embedding" class="headerlink" title="（3）embedding"></a>（3）embedding</h3><p>对于每一个词元进入BERT的向量表示，它是这个词元本身的embedding加上它在哪一个句子的embedding再加上位置的embedding，如下图所示：</p>
<p><img src="image/image_oJD6S2AS3W.png" alt=""></p>
<ul>
<li>上图演示的是BERT的嵌入层的做法，即由一个词元的序列得到一个向量的序列，这个向量的序列会进入transformer块</li>
<li>上图中每一个方块是一个词元</li>
<li><strong>token embedding</strong>：这是一个正常的embedding层，对每一个词元输出它对应的向量</li>
<li><strong>segment embedding</strong>：表示是第一句话还是第二句话</li>
<li><strong>position embedding</strong>：输入的大小是这个序列的最大长度，它的输入就是每个词元这个序列中的位置信息（从零开始），由此得到对应的位置的向量</li>
<li>最终就是每个词元本身的嵌入加上在第几个句子的嵌入再加上在句子中间的位置嵌入</li>
<li>在transformer中，位置信息是手动构造出来的一个矩阵，但是在BERT中不管是属于哪个句子，还是具体的位置，它对应的向量表示都是通过学习得来的</li>
</ul>
<h2 id="6-4-预训练"><a href="#6-4-预训练" class="headerlink" title="6.4 预训练"></a>6.4 预训练</h2><p>在预训练的时候，主要有两个东西比较关键</p>
<ul>
<li>目标函数</li>
<li>用来做预训练的数据</li>
</ul>
<h3 id="（1）TASK-1：Masked-LM"><a href="#（1）TASK-1：Masked-LM" class="headerlink" title="（1）TASK #1：Masked LM"></a>（1）TASK #1：Masked LM</h3><p>对于输入的词元序列，如果词元序列是由WordPiece生成的话，那么它有15%的概率会随机替换成掩码，但是对于特殊的词元（第一个词元和中间的分割词元不做替换），如果输入序列长度是1000的话，那么就要预测150个词。</p>
<p><img src="image/image_cn80GZGzOT.png" alt=""></p>
<p>这里也会存在问题：因为<strong>在做掩码的时候会把词元替换成一个特殊的token（[MASK]），在训练的时候大概会看到15%的词元，但是在微调的时候是没有的</strong>，因为在微调的时候不用这个目标函数，所以没有mask这个东西，导致在预训练和微调的时候所看到的数据会有多不同。</p>
<p><strong>解决方法</strong>：对这15%的被选中作为掩码的词有80%的概率是真的将它替换成这个特殊的掩码符号（[MASK]），还有10%的概率将它替换成一个随机的词元（其实是加入了一些噪音），最后有10%的概率什么都不干，就把它存在那里用来做预测（附录中有例子）。</p>
<h3 id="（2）TASK-2：Next-Sentence-Prediction（NSP）"><a href="#（2）TASK-2：Next-Sentence-Prediction（NSP）" class="headerlink" title="（2）TASK #2：Next Sentence Prediction（NSP）"></a>（2）TASK #2：Next Sentence Prediction（NSP）</h3><p>在问答（QA）和自然语言推理（NLI）中都是句子对，如果让它学习一些句子层面的信息也不错，具体来说，一个输入序列里面有两个句子：a和b，有50的概率b在原文中间真的是在a的后面，还有50%的概率b就是随机从别的地方选取出来的句子，这就意味着有50%的样本是正例（两个句子是相邻的关系），50%的样本是负例（两个句子没有太大的关系），加入这个目标函数能够极大地提升在QA和自然语言推理的效果（附录中有例子）</p>
<p><img src="image/image_u5BdxsXmyY.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">Input = [CLS] the man went to [MASK] store [SEP]</span><br><span class="line">he bought a gallon [MASK] milk [SEP]</span><br><span class="line">Label = IsNext</span><br><span class="line">Input = [CLS] the man [MASK] to the store [SEP]</span><br><span class="line">penguin [MASK] are flight <span class="comment">##less birds [SEP] # 两个## 表示后面这个词less跟在flight后，二者是同一个词被切成两半</span></span><br><span class="line">Label = NotNext</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>预训练过程很大程度上遵循现存文献的语言模型预训练，使用BooksCorpus（800M个单词）和English Wikipedia（25亿个单词）。为了获取长的连续序列，使用文档级的语料（文章）比使用像Billion Word Benchmark这样无序的句子级语料更为重要。</p>
<h3 id="（3）预训练数据"><a href="#（3）预训练数据" class="headerlink" title="（3）预训练数据"></a>（3）预训练数据</h3><p>使用了两个数据集</p>
<ul>
<li>BooksCorpus</li>
<li>English Wikipedia</li>
</ul>
<p>应该使用文本层面的数据集，即数据集里面是一篇一篇的文章而不是一些随机打乱的句子，因为transformer确实能够处理比较长的序列，所以对于整个文本序列作为数据集效果会更好一些。</p>
<h2 id="6-5-微调"><a href="#6-5-微调" class="headerlink" title="6.5 微调"></a>6.5 微调</h2><p>BERT和一些基于编码器解码器的架构有什么不同</p>
<ul>
<li>transformer是编码器解码器架构</li>
<li>因为把整个句子对都放在一起放进去了，所以自注意力能够在两端之间相互能够看到，但是在编码器解码器这个架构中，编码器一般是看不到解码器的东西的，所以BERT在这一块会更好一点，但是实际上也付出了一定的代价（不能像transformer一样能够做机器翻译）</li>
</ul>
<p>在做下游任务的时候会根据任务设计任务相关的输入和输出，好处是模型其实不需要做大的改动，主要是怎么样把输入改成所要的那个句子对</p>
<ul>
<li>如果真的有两个句子的话就是句子a和b</li>
<li>如果只有一个句子的话，比如说要做一个句子的分类，b就没有了</li>
</ul>
<p>然后根据下游的任务要求，要么是拿到第一个词元对应的输出做分类或者是拿到对应的词元的输出做所想要的输出，不管怎么样都是在最后加一个输出层，然后用一个softnax得到想要的标号</p>
<p>跟预训练比微调相对来说比较便宜，所有的结果都可以使用一个TPU跑一个小时就可以了，使用GPU的话多跑几个小时也行</p>
<h1 id="7-实验"><a href="#7-实验" class="headerlink" title="7.实验"></a>7.实验</h1><h2 id="7-1-GLUE"><a href="#7-1-GLUE" class="headerlink" title="7.1 GLUE"></a><strong>7.1 GLUE</strong></h2><p>它里面包含了多个数据集，是一个句子层面的任务</p>
<p>BERT就是把第一个特殊词元[CLS]的最后的向量拿出来，然后学习一个输出层w，放进去之后用softmax就能得到标号，这就变成了一个很正常的多分类问题了</p>
<p>下图表示了在这个分类任务上的结果</p>
<p><img src="image/image_MwJx_plxkW.png" alt=""></p>
<ul>
<li>average表示在所有数据集上的平均值，它表示精度，越高越好</li>
<li>可以发现就算是BERT就算是在base跟GPT可学习参数差不多的情况下，也还是能够有比较大的提升</li>
</ul>
<h2 id="7-2-SQuAD-v1-1"><a href="#7-2-SQuAD-v1-1" class="headerlink" title="7.2 SQuAD v1.1"></a><strong>7.2 SQuAD v1.1</strong></h2><p>斯坦福的一个QA数据集</p>
<p>QA任务是说给定一段话，然后问一个问题，需要在这段话中找出问题的答案（类似于阅读理解），答案在给定的那段话中，只需要把答案对应的小的片段找出来就可以了（找到这个片段的开始和结尾）</p>
<ul>
<li>就是对每个词元进行判断，看是不是答案的开头或者答案的结尾</li>
</ul>
<p>具体来说就是学两个向量S和E，分别对应这个词元是答案开始的概率和答案最后的概率，它对每个词元（也就是第二句话中每个词元）的S和Ti相乘，然后再做softmax，就会得到这个段中每一个词元是答案开始的概率，公式如下图所示，同理也可以得出是答案末尾的概率:</p>
<script type="math/tex; mode=display">
P_i = \frac{e^{S\cdot T_i}}{\sum_j e^{S\cdot T_j}}</script><ul>
<li>Ti表示第 i 个输入词元对应的最后一个隐藏向量</li>
</ul>
<p><strong>注意：在做微调的时候的参数设置</strong></p>
<ul>
<li>使用了3个epoch，扫描了3遍数据</li>
<li>学习率是5e-5</li>
<li>batchsize是32</li>
</ul>
<p>用BERT做微调的时候结果非常不稳定，同样的参数、同样的数据集，训练十遍，可能会得到不同的结果。最后发现3其实是不够的，可能多学习几遍会好一点</p>
<p>BERT用的优化器是adam的不完全版，当BERT要训练很长时间的时候是没有影响的，但是如果BERT只训练一小段时间的话，它可能会带来影响（将这个优化器换成adam的正常版就可以解决这个问题了）</p>
<h1 id="8-Ablation-Studies"><a href="#8-Ablation-Studies" class="headerlink" title="8.Ablation Studies"></a>8.Ablation Studies</h1><p>介绍了BERT中每一块最后对结果的贡献</p>
<h2 id="8-1-Effect-of-Pre-training-Tasks"><a href="#8-1-Effect-of-Pre-training-Tasks" class="headerlink" title="8.1 Effect of Pre-training Tasks"></a>8.1 Effect of Pre-training Tasks</h2><p><img src="image/image_Bk66lmk49Z.png" alt=""></p>
<ul>
<li><strong>No NSP</strong>：假设去掉对下一个句子的预测</li>
<li>L<strong>TR &amp; No NSP</strong>：使用一个从左看到右的单向的语言模型（而不是用带掩码的语言模型），然后去掉对下一个句子的预测</li>
<li><strong>+ BiLSTM</strong>：在上面加一个双向的LSTM</li>
</ul>
<p>从结果来看，去掉任何一部分，结果都会打折扣</p>
<h2 id="8-2-Effect-of-Model-Size"><a href="#8-2-Effect-of-Model-Size" class="headerlink" title="8.2 Effect of Model Size"></a>8.2 Effect of Model Size</h2><p>BERT base中有1亿的可学习参数，BERT large中有3亿可学习的参数，相对于之前的transformer，可学习参数数量的提升还是比较大的。</p>
<p>当模型变得越来越大的时候，效果会越来越好，这是第一个展示将模型变得特别大的时候对语言模型有较大提升的工作。</p>
<p>虽然现在GPT3已经做到1000亿甚至在向万亿级别发展，但是在三年前，BERT确实是开创性地将一个模型推到如此之大，引发了之后的模型大战</p>
<h2 id="8-3-Feature-based-Approach-with-BERT"><a href="#8-3-Feature-based-Approach-with-BERT" class="headerlink" title="8.3 Feature-based Approach with BERT"></a>8.3 Feature-based Approach with BERT</h2><p>假设不用BERT做微调而是把BERT的特征作为一个静态特征输进去会怎样</p>
<p>结论是效果确实没有微调好，所有用BERT的话应该用微调</p>
<h1 id="9-评论"><a href="#9-评论" class="headerlink" title="9.评论"></a><strong>9.评论</strong></h1><p><strong>写作</strong></p>
<ul>
<li>先写了BERT和GPT的区别</li>
<li>然后介绍了BERT模型</li>
<li>接下来是在各个实验上的设置</li>
<li>最后对比结果，结果非常好</li>
</ul>
<p>这篇文章认为本文的最大贡献就是<strong>双向性</strong>（写文章最好能有一个明确的卖点，有得有失，都应该写出来）</p>
<ul>
<li>但是今天来看，这篇文章的贡献不仅仅只有双向性，还有其它东西</li>
<li>从写作上来说，至少要说选择双向性所带来的不好的地方是什么，做一个选择，会得到一些东西，也会失去一些东西：和GPT比，BERT用的是编码器，GPT用的是解码器，得到了一些好处，但是也有坏处（比如做机器翻译和文本摘要比较困难，做生成类的东西就没那么方便了）</li>
<li>分类问题在NLP中更加常见，所以NLP的研究者更喜欢用BERT，会更容易一些</li>
</ul>
<p>BERT所提供的是一个完整的解决问题的思路，符合了大家对于深度学习模型的期望：在一个很大的数据集上训练好一个很深很宽的模型，这个模型拿出来之后可以用在很多小问题上，通过微调可以全面提升这些小数据上的性能</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="https://wdndev.github.io">Firefly</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wdndev.github.io/paper_reading/2.2.BERT/">https://wdndev.github.io/paper_reading/2.2.BERT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a><a class="post-meta__tags" href="/tags/PaperReading/">PaperReading</a></div><div class="post_share"><div class="social-share" data-image="/img/wdn_icon.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/paper_reading/2.1.Transformer/" title="论文精读 Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-31</div><div class="title">论文精读 Transformer</div></div></a></div><div><a href="/paper_reading/2.3.ViT/" title="论文精读 ViT"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-15</div><div class="title">论文精读 ViT</div></div></a></div><div><a href="/paper_reading/2.4.Swin%20Transformer/" title="论文精读 Swin Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-01</div><div class="title">论文精读 Swin Transformer</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#2-BERT"><span class="toc-text">2.BERT</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-%E9%A2%98%E7%9B%AE-%E4%BD%9C%E8%80%85"><span class="toc-text">1.题目+作者</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#2-%E6%91%98%E8%A6%81"><span class="toc-text">2.摘要</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#3-Introduction"><span class="toc-text">3.Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#3-1%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E4%BB%BB%E5%8A%A1%E5%92%8C%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-text">3.1自然语言任务和预训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-2-%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E7%9A%84%E7%AD%96%E7%95%A5"><span class="toc-text">3.2 预训练任务的策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-%E7%8E%B0%E6%9C%89%E6%8A%80%E6%9C%AF%E7%9A%84%E9%99%90%E5%88%B6"><span class="toc-text">3.3 现有技术的限制</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-4-BERT"><span class="toc-text">3.4 BERT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-5-%E6%96%87%E7%AB%A0%E7%9A%84%E8%B4%A1%E7%8C%AE"><span class="toc-text">3.5 文章的贡献</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#4-%E7%BB%93%E8%AE%BA"><span class="toc-text">4.结论</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#5-%E7%9B%B8%E5%85%B3%E5%B7%A5%E4%BD%9C"><span class="toc-text">5.相关工作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#6-BERT"><span class="toc-text">6.BERT</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#6-1-pre-training-and-fine-tuning"><span class="toc-text">6.1 pre-training and fine-tuning</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-2-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-text">6.2 模型架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E4%B8%89%E4%B8%AA%E5%8F%82%E6%95%B0"><span class="toc-text">（1）三个参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E4%B8%A4%E4%B8%AA%E6%A8%A1%E5%9E%8B"><span class="toc-text">（2）两个模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E6%80%8E%E6%A0%B7%E6%8A%8A%E8%B6%85%E5%8F%82%E6%95%B0%E6%8D%A2%E7%AE%97%E6%88%90%E5%8F%AF%E5%AD%A6%E4%B9%A0%E5%8F%82%E6%95%B0%E7%9A%84%E5%A4%A7%E5%B0%8F"><span class="toc-text">（3）怎样把超参数换算成可学习参数的大小</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-3-%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA"><span class="toc-text">6.3 输入和输出</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89%E8%BE%93%E5%85%A5%E5%BD%A2%E5%BC%8F"><span class="toc-text">（1）输入形式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89%E5%BA%8F%E5%88%97%E7%9A%84%E6%9E%84%E6%88%90"><span class="toc-text">（2）序列的构成</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89embedding"><span class="toc-text">（3）embedding</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-4-%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="toc-text">6.4 预训练</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%881%EF%BC%89TASK-1%EF%BC%9AMasked-LM"><span class="toc-text">（1）TASK #1：Masked LM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%882%EF%BC%89TASK-2%EF%BC%9ANext-Sentence-Prediction%EF%BC%88NSP%EF%BC%89"><span class="toc-text">（2）TASK #2：Next Sentence Prediction（NSP）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%EF%BC%883%EF%BC%89%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE"><span class="toc-text">（3）预训练数据</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-5-%E5%BE%AE%E8%B0%83"><span class="toc-text">6.5 微调</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#7-%E5%AE%9E%E9%AA%8C"><span class="toc-text">7.实验</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#7-1-GLUE"><span class="toc-text">7.1 GLUE</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#7-2-SQuAD-v1-1"><span class="toc-text">7.2 SQuAD v1.1</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#8-Ablation-Studies"><span class="toc-text">8.Ablation Studies</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#8-1-Effect-of-Pre-training-Tasks"><span class="toc-text">8.1 Effect of Pre-training Tasks</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-2-Effect-of-Model-Size"><span class="toc-text">8.2 Effect of Model Size</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#8-3-Feature-based-Approach-with-BERT"><span class="toc-text">8.3 Feature-based Approach with BERT</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#9-%E8%AF%84%E8%AE%BA"><span class="toc-text">9.评论</span></a></li></ol></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2023 - 2025 By Firefly</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">簡</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container').forEach(node => {
            if (node.hasAttribute('display')) {
              btf.wrap(node, 'div', { class: 'mathjax-overflow' })
            } else {
              btf.wrap(node, 'span', { class: 'mathjax-overflow' })
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/katex/dist/katex.min.css"><script src="https://cdn.jsdelivr.net/npm/katex/dist/contrib/copy-tex.min.js"></script><script>(() => {
  document.querySelectorAll('#article-container span.katex-display').forEach(item => {
    btf.wrap(item, 'div', { class: 'katex-wrap'})
  })
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'oe7vzWxH80qwJJjWslYTCViT-gzGzoHsz',
      appKey: 'k89nSbK0BTbmzmpQottRHvNI',
      avatar: 'monsterid',
      serverURLs: 'https://oe7vzwxh.lc-cn-n1-shared.com',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Valine' === 'Valine' || !true) {
  if (true) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script async src="/js/title.js"></script><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="true"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">Search</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  Loading the Database</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div><hr/><div class="no-result" id="local-search-results"></div><div id="local-search-stats-wrap"></div></div></div><div id="search-mask"></div><script src="/js/search/local-search.js"></script></div></div><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"jsonPath":"/live2dw/assets/wanko.model.json"},"display":{"position":"left","width":180,"height":360,"hOffset":0,"vOffset":-100},"mobile":{"show":true},"react":{"opacity":0.85},"log":false});</script></body></html>